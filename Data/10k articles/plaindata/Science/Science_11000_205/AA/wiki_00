{"id": "35483592", "url": "https://en.wikipedia.org/wiki?curid=35483592", "title": "A42 road (England)", "text": "A42 road (England)\n\nThe A42 is a major trunk road in the East Midlands region of the United Kingdom. It links junction 23A of the M1 motorway to junction 11 of the M42 motorway. The A42 is in effect a continuation of the M42, and its junctions are numbered accordingly.\n\nIt is built to a similar standard to the M42, being a grade separated dual carriageway. The Measham and Ashby-de-la-Zouch bypass section was opened in August 1989 at a cost of £33m.\n\nThe A42 was built by the UK Government in 1989 to link the northern section of the M42 to the M1. Although it is not designated as motorway, and has no hard shoulder, the road is fully grade separated and runs with two lanes each way, the same as the M42 to the south.\n\nThe original planned line of the M42 saw it joining the M1 further to the north, crossing what is now the A50 Derby Southern Bypass and meeting the M1 north of Bardills Island (A52/M1 interchange).\n\n\nThe current road is the second incarnation of the A42. The original (1923) route was Reading to Birmingham via Oxford. The whole road disappeared in 1935 - the section from Reading to Shillingford became part of the A329, Shillingford to Oxford became part of the A423 and Oxford to Birmingham became part of the A34. In 1993 the A423 was itself renumbered, with the section formerly the A42 becoming part of the A4074 from Reading to Oxford. The modern M42 does interchange with the former A42 at junction 4 near Solihull: Stratford Road now being numbered A34 to the north of the junction and A3400 to the south.\n\n\n"}
{"id": "32263504", "url": "https://en.wikipedia.org/wiki?curid=32263504", "title": "Abell 2744", "text": "Abell 2744\n\nAbell 2744, nicknamed Pandora's Cluster, is a giant galaxy cluster resulting from the simultaneous pile-up of at least four separate, smaller galaxy clusters that took place over a span of 350 million years. The galaxies in the cluster make up less than five percent of its mass. The gas (around 20 percent) is so hot that it shines only in X-rays. Dark matter makes up around 75 percent of the cluster's mass.\n\nThis cluster also shows a radio halo along with several other Abell clusters. It has a strong central halo, along with an extended tail, which could either be relic radiation, or an extension of the central halo.\n\nRenato Dupke, a member of the team that discovered the Cluster, explained the origin of the name in an interview: \"We nicknamed it ‘\"Pandora's Cluster\"’ because so many different and strange phenomena were unleashed by the collision.\"\n\n\n"}
{"id": "28495100", "url": "https://en.wikipedia.org/wiki?curid=28495100", "title": "Affective haptics", "text": "Affective haptics\n\nAffective haptics is the emerging area of research which focuses on the study and design of devices and systems that can elicit, enhance, or influence the emotional state of a human by means of sense of touch. The research field is originated with the Dzmitry Tsetserukou and Alena Neviarouskaya papers on affective haptics and real-time communication system with rich emotional and haptic channels. Driven by the motivation to enhance social interactivity and emotionally immersive experience of users of real-time messaging, virtual, augmented realities, the idea of reinforcing (intensifying) own feelings and reproducing (simulating) the emotions felt by the partner was proposed.\nFour basic haptic (tactile) channels governing our emotions can be distinguished: (1) physiological changes (e.g., heart beat rate, body temperature, etc.), (2) physical stimulation (e.g., tickling), (3) social touch (e.g., hug, handshake), (4) emotional haptic design (e.g., shape of device, material, texture).\n\nAccording to James-Lange theory, the conscious experience of emotion occurs after the cortex receives signals about changes in physiological state. Researchers argued that feelings are preceded by certain physiological changes. Thus, when we see a venomous snake, we feel fear, because our cortex has received signals about our racing heart, knocking knees, etc. Damasio distinguishes primary and secondary emotions. Both involve changes in bodily states, but the secondary emotions are evoked by thoughts. Recent empirical studies support non-cognitive theories of nature of emotions. It was proven that we can easily evoke our emotions by something as simple as changing facial expression (e.g., smile brings on a feeling of happiness).\n\nHuman emotions can be easily evoked by different cues, and the sense of touch is one of the most emotionally charged channels. Affective haptic devices produce different senses of touch including kinesthetic and coetaneous channels. Kinesthetic stimulations, which are produced by forces exerted on the body, are sensed by mechanoreceptors in the tendons and muscles. On the other hand, mechanoreceptors in the skin layers are responsible for the perception of cutaneous stimulation. Different types of tactile corpuscles allow us sensing thermal property of the object, pressure, vibration frequency, and stimuli location.\n\nOnline interactions rely heavily on vision and hearing, so a substantial need exists for mediated social touch. Of the forms of physical contact, hugging is particularly emotionally charged; it conveys warmth, love, and affiliation. Recently, researchers have made several attempts to create a hugging device providing some sense of physical co-presence over a distance. HaptiHug’s key feature is that it physically reproduces the human-hug pattern, generating pressure simultaneously on each user’s chest and back. The idea to realistically reproduce hugging is in integration of active-haptic device HaptiHug and pseudo-haptic touch simulated by hugging animation. Thus, high immersion into the physical contact of partners while hugging is achieved.\n\nAffection goes further than hugging alone. People in long distance relationships are faced with a lack of physical intimacy on a day-to-day basis. Haptic technology allows for kinesthetic and tactile interface design. The field of digital co-presence envelops Teledildonics as well. The Kiiroo SVir is a good example of an Adult CyberToy that incorporates tactile input, by means of a surface that is touch capacitive, and an inside that is kinesthetic in nature. 12 rings contract, pulse and vibrate according to the movements one's partner makes in real time. The SVir mimics the actual motion of its OPue counterpart and is also compatible with other SVir models.\nThe SVir enables women to have intercourse with their partner through use of the OPue interactive vibrator no matter how great the distance that separates the couple.\n\nDifferent types of devices can be used to produce the physiological changes. Of the bodily organs, the heart plays a particularly important role in our emotional experience. The heart imitator HaptiHeart produces special heartbeat patterns according to emotion to be conveyed or elicited (sadness is associated with slightly intense heartbeat, anger with quick and violent heartbeat, fear with intense heart rate). False heart beat feedback can be directly interpreted as a real heart beat, so it can change the emotional perception. HaptiButterfly reproduces “butterflies in your stomach” (the fluttery or tickling feeling felt by people experiencing love) through arrays of vibration motors attached to the user’s abdomen. HaptiShiver sends “shivers up and down your spine” through a row of vibration motors. HaptiTemper sends “chills up and down your spine” through both cold airflow from a fan and the cold side of a Peltier element. HaptiTemper is also intended for simulation of warmth on the human skin to evoke either pleasant feeling or aggression\n\nHaptiTickler directly evokes joy by tickling the user’s ribs. It includes four vibration motors reproducing stimuli similar to human finger movements\n\nRecent findings show that attractive things make people feel good, which in turn makes them think more creatively. The concept of emotional haptic design was proposed. The core idea is to make user to feel an affinity for the device through:\n\nAffective computing can be used to measure and to recognize emotional information in systems and devises employing affective haptics. Emotional information is extracted by using such techniques as speech recognition, natural language processing, facial expression detection, and measurement of physiological data.\n\nPossible applications are as follows:\nAffective Haptics is in the vanguard of emotional telepresence, technology that lets users feel emotionally as if they were present and communicating at a remote physical location. The remote environment can be real, virtual, or augmented.\n\nThe philosophy behind the iFeel_IM! (intelligent system for Feeling enhancement powered by affect sensitive Instant Messenger) is “I feel [therefore] I am!”. In the iFeel_IM! system, great importance is placed on the automatic sensing of emotions conveyed through textual messages in 3D virtual world Second Life (artificial intelligence), the visualization of the detected emotions by avatars in virtual environment, enhancement of user’s affective state, and reproduction of feeling of social touch (e.g., hug) by means of haptic stimulation in a real world. The control of the conversation is implemented through the Second Life object called EmoHeart attached to the avatar’s chest. In addition to communication with the system for textual affect sensing (Affect Analysis Model), EmoHeart is responsible for sensing symbolic cues or keywords of ‘hug’ communicative function conveyed by text, and for visualization of ‘hugging’ in Second Life. The iFeel_IM! system considerably enhance emotionally immersive experience of real-time messaging.\n\nIn order to build a social interface, Réhman et al.\n\nTo produce movie-specific tactile stimuli influencing the viewer’s emotions to the viewer’s body, the wearable tactile jacket was developed by Philips researchers. The motivation was to increase emotional immersion in a movie-viewing. The jacket contains 64 vibration motors that produce specially designed tactile patterns on the human torso.\n\n\n\n"}
{"id": "37942128", "url": "https://en.wikipedia.org/wiki?curid=37942128", "title": "Alipogene tiparvovec", "text": "Alipogene tiparvovec\n\nAlipogene tiparvovec (marketed under the trade name Glybera) is a gene therapy treatment designed to reverse lipoprotein lipase deficiency (LPLD), a rare inherited disorder which can cause severe pancreatitis. In July 2012, the European Medicines Agency recommended it for approval (the first recommendation for a gene therapy treatment in either Europe or the United States), and the recommendation was endorsed by the European Commission in November 2012.\n\nThe drug is administered via a series of injections into the leg muscles—as many as 60, all in one session. It is a one-time treatment intended to last at least ten years.\n\nGlybera gained infamy as the \"million-dollar drug,\" causing its manufacturer, uniQure, to remove the drug after two years on the European market and just a single patient treated. As of 2018, only 31 people worldwide have ever been administered Glybera, and uniQure has no plans to sell the drug in the US or Canada.\n\nGlybera was developed over a period of decades by researchers at the University of British Columbia. In 1986, Michael R. Hayden and John Kastelein began research at UBC, confirming the hypothesis that LPLD was caused by a gene mutation. Years later, in 2002, Hayden and Colin Ross successfully performed gene therapy on test mice to treat LPLD; their findings were featured on the September 2004 cover of \"Human Gene Therapy\". Hayden next succeeded in treating cats in the same manner, with the help of Boyce Jones.\n\nMeanwhile, Kastelein—who had, by 1998, become an international expert in lipid disorders—co-founded Amsterdam Molecular Therapeutics (AMT), which took over Hayden's research with the aim of releasing the drug in Europe.\n\nSince LPLD is a rare condition (prevalence worldwide 1–2 per million), related clinical tests and trials have involved unusually small cohort sizes. The first main trial (CT-AMT-011-01) involved just 14 subjects, and by 2015, a total of 27 individuals had been involved in Phase III testing. The second phase of testing focused on subjects living along the Saguenay River in Quebec, where LPLD affects people at the highest rate in the world (up to 200 per million), due to the founder effect.\n\nAfter over two years of testing and lobbying by AMT, Glybera was approved in Europe in 2012. However, after spending millions of euros on Glybera's approval, AMT went bankrupt and its assets were acquired by uniQure N.V..\n\nAlipogene tiparvovec was expected to cost around $1.6 million per treatment in 2012—revised to $1 million in 2015—making it the most expensive medicine in the world at the time. However, replacement therapy, a similar treatment, can cost over $300,000 per year, for life.\n\nIn 2015, uniQure dropped its plans for approval in the US and exclusively licensed rights to sell the drug in Europe to Chiesi Farmaceutici for €31 million.\n\nAs of 2016, only one person had been treated with the drug, a German woman.\n\nIn April 2017, Chiesi quit selling Glybera and uniQure announced that it would not pursue the renewal of the marketing authorization in Europe when it was scheduled to expire that October, due to lack of demand. Afterwards, the three remaining doses in Chiesi's inventory were administered to three patients for €1 each.\n\nThe adeno-associated virus serotype 1 (AAV1) viral vector delivers an intact copy of the human lipoprotein lipase (LPL) gene to muscle cells. The LPL gene is not inserted into the cell's chromosomes but remains as free floating DNA in the nucleus. The injection is followed by immunosuppressive therapy to prevent immune reactions to the virus.\n\nData from the clinical trials indicates that fat concentrations in blood were reduced between 3 and 12 weeks after injection, in nearly all patients. The advantages of AAV include apparent lack of pathogenicity, delivery to non-dividing cells, and much smaller risk of insertion compared to retroviruses, which show random insertion with accompanying risk of cancer. AAV also presents very low immunogenicity, mainly restricted to generating neutralizing antibodies, and little well defined cytotoxic response. The cloning capacity of the vector is limited to replacement of the virus's 4.8 kilobase genome.\n\n"}
{"id": "915997", "url": "https://en.wikipedia.org/wiki?curid=915997", "title": "Allais effect", "text": "Allais effect\n\nThe Allais effect is the alleged anomalous behavior of pendulums or gravimeters which is sometimes purportedly observed during a solar eclipse. The effect was first reported as an anomalous precession of the plane of oscillation of a Foucault pendulum during the solar eclipse of June 30, 1954 by Maurice Allais, a French polymath who went on to win the Nobel Prize in Economics. Allais reported another observation of the effect during the solar eclipse of October 2, 1959 using the paraconical pendulum he invented. This study earned him the 1959 Galabert Prize of the French Astronautical Society and made him a laureate of the U.S. Gravity Research Foundation for his 1959 memoir on gravity. The veracity of the Allais effect remains controversial among the scientific community, as its testing has frequently met with inconsistent or ambiguous results over more than five decades of observation.\n\nMaurice Allais emphasized the \"dynamic character\" of the effects he observed:\nBesides Allais' own experiments, related research about a possible effect of the Moon's shielding, absorption or bending of the Sun's gravitational field during a solar eclipse have been conducted by scientists around the world. Some observations have yielded positive results, seemingly confirming that minute but detectable variations in the expected behavior of devices dependent on gravity do indeed occur within the umbra of an eclipse, but others have failed to detect any noticeable effect.\n\nRomanian physicist Gheorghe Jeverdan \"et al.\" observed the Allais effect and the so-called \"Jeverdan-Rusu-Antonescu effect\" or \"Jeverdan effect\" (i.e. the change in the oscillation period of a pendulum during an eclipse) while monitoring a Foucault pendulum during the solar eclipse of February 15, 1961. The authors made two hypotheses regarding their observation: during an eclipse, the Moon exerts a screening effect on the gravitational attraction of the Sun so that the attraction of the Earth is indirectly increased, a phenomenon that could also be studied with tides. If the hypothesis of the screening effect is wrong, another explanation could be that the variation of the Earth's gravity might be considered as a result of the diffraction of gravitational waves. Erwin Saxl and Mildred Allen similarly reported strong anomalous changes in the period of a torsion pendulum during the solar eclipse of March 7, 1970 and concluded that \"gravitational theory needs to be modified\".\n\nDr. Leonid Savrov of the Sternberg Astronomical Institute built a dedicated paraconical pendulum to test the Allais effect during the solar eclipse of July 11, 1991 in Mexico and the eclipse of November 3, 1994 in Brazil. While he could not observe Allais' claim that there is a diurnal periodicity in the motion of a paraconical pendulum, he did, however, write: \"The most interesting result of the Mexico and Brazil experiments is the increase of rotational velocity of the pendulum oscillation plane in the direction of the Foucault effect during the eclipse. It seems that we have some kind of special effect.\"\n\nVarious other experiments using atomic clocks and gravimeters instead of pendulums also recorded significant anomalous gravitational effects which can neither be caused by a tidal effect or drift of the gravimeters, nor by high-frequency noise which has special patterns. These experiments were set up by different teams during solar eclipses in China in 1992, India in 1995, and China in 1997.\n\nDutch physicist Chris Duif, who surveys the field of gravitational anomalies in general, concludes that the question remains open because Allais observations do not satisfy conventional explanations, and that such investigations should be pursued, in view of their relatively inexpensive nature and the enormous implications if genuine anomalies are actually confirmed, but the article was self-published and has not undergone any peer review.\n\nResults confirming observation of the Allais and Jeverdan-Rusu-Antonescu effects during the annular solar eclipse of September 22, 2006 were presented the following year by a Romanian team, with a quantization of the behavior of the paraconical pendulum. During the solar eclipse of August 1, 2008, a Ukrainian team and two Romanian teams worked together hundreds of kilometers apart with different apparatuses: five independent miniature torsion balances for the Ukrainian team, two independent short ball-borne pendulums for a Romanian team and a long Foucault pendulum for the third team. All three teams detected unexplained and mutually correlated disturbances. The same teams repeated a dual experiment during the annular solar eclipse of January 26, 2009, this time outside of the umbra, with the same significant correlation between the behavior of light torsion balances and a Foucault pendulum. They also registered similar anomalies using a Foucault pendulum and a very light torsion balance, both located underground in a disused salt mine with minimal interference, during the partial solar eclipse of June 1, 2011.\n\nLouis B. Slichter, using a gravimeter during the solar eclipse of February 15, 1961 in Florence, Italy, failed to detect an associated gravitational signal.\n\nDuring the solar eclipse of July 22, 1990, no anomalous period increase of a torsion pendulum was detected independently by a team in Finland and another team in Belomorsk, USSR.\n\nThe total solar eclipse of August 11, 1999 had been a good opportunity to solve a 45-year mystery, thanks to an international collaboration. NASA's Marshall Space Flight Center first inquired about experimental protocols to Maurice Allais, in order to coordinate ahead of the event a worldwide effort to test the Allais effect between observatories and universities over seven countries (United States, Austria, Germany, Italy, Australia, England and four sites in the United Arab Emirates). The lead supervisor then stated: \"The initial interpretation of the record points to three possibilities: a systematic error, a local effect, or the unexplored. To eliminate the first two possibilities, we and several other observers will use different kinds of measuring instruments in a distributed global network of observing stations.\" However, after the eclipse, Allais criticized the experiments in his final NASA report, writing the period of observation was \"much too short […] to detect anomalies properly\". Moreover, the lead supervisor left NASA shortly thereafter with the gathered data and the NASA study has never been published.\n\nFurther observations conducted by the team led by Xin-She Yang appear to have yielded much weaker evidence of anomalies than their first 1997 study. The authors first posited a more conventional explanation based on temperature changes causing ground tilting, but later suggested that this explanation was unlikely. A possible yet controversial explanation was finally proposed by the same author and Tom Van Flandern which conjectured that the anomaly is due to the gravitational effect of an increased air density spot in the upper atmosphere created by cooling winds during the solar eclipse. They conclude there have been \"no unambiguous detections <nowiki>[of an Allais effect]</nowiki> within the past 30 years when consciousness of the importance of <nowiki>[experimental]</nowiki> controls was more widespread.\" They point out that \"the gravitation anomaly discussed here is about a factor of 100,000 too small to explain the Allais excess pendulum precession […] during eclipses\" and from this conclude that the original Allais anomaly was merely due to poor controls.\n\nEight gravimeters and two pendulums were deployed across six monitoring sites in China for the solar eclipse of July 22, 2009. Although one of the scientists involved described in an interview having observed the Allais effect, no result has been published in any academic journal. An automated Foucault pendulum was also used during the solar eclipse of July 11, 2010 in Argentina, with no evidence of a precession change of the pendulum's oscillation plane (< 0.3 degree per hour).\n\nMaurice Allais states that the eclipse effect is related to a gravitational anomaly that is inexplicable in the framework of the currently admitted theory of gravitation, without giving any explanation of his own. Allais' explanation for another anomaly (the lunisolar periodicity in variations of the azimuth of a pendulum) is that space evinces certain anisotropic characteristics, which he ascribes to motion through an aether which is partially entrained by planetary bodies.\n\nHis hypothesis leads to a speed of light dependent on the moving direction with respect to a terrestrial observer, since the Earth moves within the aether but the rotation of the Moon induces a \"wind\" of about 8 km/s. Thus Allais rejects Einstein's interpretation of the Michelson–Morley experiment and the subsequent verification experiments of Dayton Miller.\n\nIn particular, the Michelson–Morley experiment did not give a zero speed difference, but at most 8 km/s, without being able to detect any regularity. This difference was therefore interpreted as due to measurement uncertainties. Similarly, Miller's experiments corroborated these results over a long period of time, but Miller could not explain the source of the irregularities. At the time, temperature problems were invoked to explain the cause, as concluded by Robert S. Shankland. By re-analyzing the data from this experiment, Allais reported a periodicity using sidereal time rather than civil time used by Miller (daytime sidereal variation of the speed of light over a period of 23 hours 56 minutes with an amplitude of about 8 km/s).\n\nApplying the Titius–Bode law to the Earth–Moon system, which he generalizes to aether, Allais calculates a \"wind\" of 7.95 km/s, which is comparable to the values found by the experiments of Michelson and Miller. Hence Allais deduces that the aether turns with the stars, as proposed by the aether drag hypothesis, and is not fixed as Hendrik Lorentz thought when inventing his famous transformation and his ether theory. But the majority of scientists at the end of the 19th century imagined that such an aether crossed the Earth so that the rotation of the Earth around the Sun would cause an important variation of 30 km/s. Consequently, since the third postulate on which special relativity is based is the constancy of the speed of light in a vacuum, Allais considers it unfounded. In order to measure a change in the speed of light, one would have to get back to the definition of the 1960 meter, since confidence in the theory of relativity nowadays is such that current metrology uses constancy of the speed of light as an axiom.\n\nAllais summarized his experimental work in English in his 1999 memoir on behalf of NASA. He detailed his aether hypothesis in the books \"L'Anisotropie de l'Espace\", published in 1997, and \"L'Effondrement de la Théorie de la Relativité\", published in 2004. A book on Allais' scientific legacy has been edited in English in 2011, yet his aether hypothesis has not gained significant traction among mainstream scientists. Nevertheless, after Allais' death in 2010, experiments on the Allais effect continue.\n\n\n"}
{"id": "26537401", "url": "https://en.wikipedia.org/wiki?curid=26537401", "title": "Anthropophilia", "text": "Anthropophilia\n\nIn parasitology, anthropophilia, from the Greek ἅνθρωπος (anthrōpos, \"human being\") and φιλία (philia, \"friendship\" or \"love\"), is a preference of a parasite or dermatophyte for humans over other animals. The related term endophilia refers specifically to a preference for being in human habitats, especially inside dwellings. The term \"zoophilia\", in this context, describes animals which prefer non-human animals for nourishment.\n\nMost usage of the term \"anthropophilia\" refers to hematophagous insects (see \"Anopheles\") that prefer human blood over animal blood (zoophily, but see other meanings of zoophily). Examples other than haematophagy include geckoes that live close to humans, pied crows (\"Corvus albus\"), cockroaches, and many others. In the study of malaria and its disease vectors, researchers make the distinction between anthropophilic mosquitoes and other types as part of disease eradiction efforts.\n\nAnthropic organisms are organisms that show anthropophily, where the adjective \"synanthropic \" refers to organisms that live close to human settlements and houses, and eusynathropic to those that live within human housing.\n"}
{"id": "37545351", "url": "https://en.wikipedia.org/wiki?curid=37545351", "title": "Book of Vermilion Fish", "text": "Book of Vermilion Fish\n\nBook of Vermilion Fish () is the first monograph on goldfish in the world, written by Chinese writer Zhang Qiande () (1577－1643) in 1596 during the Ming dynasty.\n\n\n"}
{"id": "26919900", "url": "https://en.wikipedia.org/wiki?curid=26919900", "title": "Cardinal gem", "text": "Cardinal gem\n\nCardinal gems are gemstones which have traditionally been considered precious above all others. The classification of the cardinal gems dates back to antiquity, and was largely determined by ceremonial or religious use and rarity. The term has largely fallen out of use.\n\nThe five traditional cardinal gems are:\n"}
{"id": "3286366", "url": "https://en.wikipedia.org/wiki?curid=3286366", "title": "Cognitive specialization", "text": "Cognitive specialization\n\nCognitive specialization suggests that certain behaviors, often in the domain of social communication, are passed on to offspring and refined to be maximally beneficial by the process of natural selection. Specializations serve an adaptive purpose for an organism by allowing the organism to be better suited for its habitat. Over time, specializations often become essential to the species' continued survival. Cognitive specialization in humans has been thought to underlie the acquisition, development, and evolution of language, theory of mind, and specific social skills such as trust and reciprocity. These specializations are considered to be critical to the survival of the species, even though there are successful individuals who lack certain specializations, including those diagnosed with autism spectrum disorder or who lack language abilities. Cognitive specialization is also believed to underlie adaptive behaviors such as self-awareness, navigation, and problem solving skills in several animal species such as chimpanzees and bottlenose dolphins.\n\nFirst studied as an adaptive mechanism specific to humans, cognitive specialization has since evolved to encompass many behaviors in the social realm. Organisms have evolved over millions of years to become well-adapted to their habitats; this requires becoming specialized in behaviors that improve an organism’s likelihood of survival and reproduction. Not to be confused with functional specialization, which examines the specific parts of the brain that are engaged during specific behaviors or processes, cognitive specialization is focused on characteristics of the mind (an internal entity), which in turn affects external behaviors. Most of these specializations are thought to have developed in areas of the neocortex unique to humans. The most significant cognitive specializations among humans include theory of mind and language acquisition and production, while non-human animals may specialize in foraging behavior, self-awareness, or other adaptive abilities.\n\nSocial communication is critical to effective human interaction, and has evolved over time to support the complex exchange of ideas. Some social behaviors, such as helping and altruism, are largely unique to humans and are instrumental in ensuring the survival of the species. Evolutionary psychologists Leda Cosmides and John Tooby argue that the human mind contains \"specialized mechanisms\" that were designed by natural selection to facilitate social communication and exchange. Without this specialized \"algorithm\", Cosmides and Tooby claim, social exchange among humans would be closer to that of our closest evolutionary ancestors, the great apes. In addition to humans' broad abilities supporting positive social interaction, Stone et al. (2002) put forth evidence for more specific specializations including \"cheater detection\" and \"precautionary reasoning,\" both of which appear to serve strong adaptive purposes by allowing humans to share resources with only those who are likely to share with them in the future, and avoid sharing resources with untrustworthy individuals. Overall, the adaptiveness of social communication has been examined in children, adults, and older adults, across cultures, and in neuropsychiatric populations.\n\nIf social behavior is to be considered a cognitive specialization unique to human neural architecture, it should be present in every human society. To provide cross-cultual evidence that cognitive adaptations specifically support social communication, Sugiyama, Tooby, and Cosmides investigated social reasoning in a tribe in the Ecuadorian Amazon. The Shiwiar, who are a hunter-horticulturalist group previously unexposed to the presented psychological stimuli, were \"as highly proficient\" in determining who cheated in a given situation as their counterparts in the United States. This performance indicates that social communication, at least in the domain of cheater detection, is not determined by one's culture. According to Sugiyama, Tooby, and Cosimdes, the social \"algorithms\" discussed above are present in both Western and non-Western populations, providing strong evidence for the universality of such a skill.\n\nTheory of mind, or the ability to attribute mental states to other people, is thought to be a cognitive specialization unique to humans, with a few possible exceptions discussed below. Theory of mind is thought to be critical in social cognition and communication because it allows us to distinguish between accidental and purposeful actions, to make judgments about others' internal states, and to determine how another's thoughts may differ from our own. The acquisition of theory of mind in humans mostly takes place during early childhood, and is thought to be fully developed by the early school years. Theory of mind research in chimpanzees by social psychologists David Premack and Guy Woodruff in 1978 brought it to the forefront of psychological inquiry, though true theory of mind is only thought to exist in humans. This phenomenon has been analyzed in many fields, and it is thought to be among the most beneficial specializations for survival of the human species, due to its facilitation of cooperation and interpersonal relationships.\n\nTheory of mind appears to be lacking in children with autism spectrum disorders, and this deficit is thought to be a major contributor to frequent impairments in some areas of social understanding in people with autism. The fact that a developmental delay in (or absence of) theory of mind can impair social functioning—a skill imperative in the survival of the human species—is argued to be evidence for theory of mind as an adaptive cognitive specialization. Understanding that others may be thinking different thoughts than I am (colloquially, \"putting oneself in another person's shoes\") allows humans to communicate effectively and to live in large social groups. This adaptability is what makes theory of mind a cognitive specialization, rather than just another byproduct of human evolution: humankind has unique and beneficial communication skills, and this is partially due to our ability to recognize that other people may not think or know the same things we do.\n\nThough some (including Bates et al.) have argued that language arose as a byproduct of the evolution of humans' general cognitive abilities, Steven Pinker argues that it is, on its own, an adaptive mechanism. Drawing on existing literature and theory, he proposes several types of evidence for this claim, including the universality and ontogeny of language. Pinker also uses the double dissociation between general intelligence and language to argue for language as a specific adaptation. Those who lose language capabilities due to traumatic brain injury or stroke but maintain many other cognitive abilities exemplify Pinker's idea that language and general cognition are not always perfectly overlapping in human behavior. Using language \"multiplies the benefit of knowledge\" in multiple domains, including technology, tool use, and intentions of ourselves and others.\n\nArbib puts forth a hypothesis that mirror neurons in the primate brain were a precursor to language abilities in humans. Without these neurons in Broca's area in humans (which is analogous to F5 in monkeys), Arbib claims, we could not have evolved a specialization for language—which is used to explain why non-human animals do not have linguistic capabilities. In addition, Meguerditchian and Vauclair have argued that our evolutionary ancestors' communicative gestures (such as threat gestures and \"food begs\" among baboons) established a foundation on which to build human language skills. This behavior was selected for, built upon, and modified, leading to the capabilities humans have today. Early theories explained early language as an adaptive way to communicate during a hunt, but recent research has focused on ecological theories that incorporate social demands; or, as Flinn et al. put it, a \"social arms race\" against non-human primates. As a behavior selected for over the long term, with many successful \"intermediary stages,\" human language differs from all other social behaviors among chimpanzees, which are thought to be more gradual in their evolutionary development. Further evidence for language as a cognitive specialization includes Ferreira et al.'s finding that some parts of language (for instance, syntax) can be spared in amnesia, while other abilities (like memory retention) are drastically reduced. This and similar dissociations support the theory that specific neural architecture, which has evolved over time, supports language function.\n\nLinguist Noam Chomsky proposed a biological component of language, which he termed Universal Grammar. According to Chomsky, an essential part of language processing is hard-wired into the human brain. This allows language to be produced with or without specific linguistic instruction (which is closely associated with the poverty of the stimulus argument). All humans—and only humans—have this biological trait, but building blocks of universal grammar have been reported in other species. Jackendoff argues that Universal Grammar is itself a \"pre-existing cognitive specialization\": rather than needing explicit instruction on how to speak their native language, or having vocabulary and syntactical rules of a specific language present in their brains from birth, children seem to be genetically pre-disposed to \"learn\" language. Complementary to the connection made between area F5 in macaques' brains, the theory of Universal Grammar allows for an evolutionary perspective on language use as a cognitive specialization. There is some controversy, however, on whether or not Universal Grammar can have evolved by standard Darwinian evolutionary principles, or must be explained using different mechanisms.\n\nAccording to Nowak and Sigmund, language is essential to human life as we know it. Without the ability to verbally communicate with members of our social group, there would be no reciprocity (that is, returning of favors), and no way to cooperate with one another for a greater good. Some have argued that unique aspects of human language have evolved for unexpectedly beneficial reasons, besides simply asking for help or sharing information about the world. Gossip, viewed by many to be a superfluous aspect of human communication, may even serve an adaptive purpose. The spread of information about other people, even if it is malicious, may serve as an indicator of social intelligence and a way to deter illicit behaviors. Though gossip likely helps some humans and hinders others' social standing, it appears to be an overall benefit of the ability to produce verbal language. Without an overall specialization for language (including such sub-specializations as gossip), linguists argue, humans would not be able to share information efficiently and effectively.\n\nWatson et al. provide support for a specific specialization in language-dependent humor. Its adaptive value has both extrinsic and intrinsic components: humor facilitates social bonding if shared extrinsically, and provides pleasure if enjoyed in one's own mind. In addition, Johnson-Frey (2003) proposed a unique human specialization for tool use. According to Johnson-Frey, humans' ability to use tools is based on complex cognitive mechanisms, not just advanced sensorimotor skills. Rather than it being considered a purely physical specialization based only in motor areas of the brain, Johnson-Frey argues that tool use should be classified as a cognitive phenomenon due to its foundation in cognition. On a more philosophical level, Boyer (2003) argues that \"religious thought and behavior\" is a specialization that originally developed as a by-product of brain function, and its adaptive purposes led to its continued evolution by natural selection. Krueger et al. (2007) have argued that trust, which may form the foundation for helping and altruism and thus the basis of human social interaction, is also a cognitive specialization.\n\nHumankind’s closest ancestors, the great apes, have evolved a number of specialized behaviors: orangutans are specialists at climbing trees, while chimpanzees and gorillas have evolved to walk on their knuckles. However, in considering non-behavioral specializations, Penn et al. (2008) argue that the \"profound continuity\" Charles Darwin noted between human and non-human animals in the biological domain is matched by a \"profound discontinuity\" between human and non-human animal minds. In contrast, in addition to cognitive-behavioral adaptations, it is possible that chimpanzees have acquired more socially advanced skills through natural selection, including self-recognition (indicated by chimpanzees' established ability to pass the \"mirror test\"). This task—in which a successful trial is simply one in which an animal recognizes itself in a mirror—is thought to be a basic building block of theory of mind development. Rhesus monkeys have also been shown to realize when they remember certain events and items, which is considered to be an instrumental building block in the formation of social relationships, as one must remember who owes him favors, who he can trust, and who he should avoid in order to prosper in the community.\n\nMore recent evidence has shown that cognitive specialization is not just present in primates: domesticated dogs may show signs of understanding human behavior and communication, indicating a social-cognitive specialization that is argued to make them more likely to receive food, shelter and love from their human owners. Being receptive to human behavioral indicators and responding accordingly has allowed dogs to survive and thrive as a species. Bottlenose dolphins and elephants have also been shown to pass the \"mirror test\" explained above. This indication of some elementary self-awareness provides more evidence for foundational theory of mind skills in organisms throughout the animal kingdom. Ants, bees, and other insects have also evolved behaviors consistent with various specializations, including advanced navigational skills and several basic social communication abilities. Adaptive cognitive evolution has been examined in pigeons' ability to group objects (which is argued to support their processing of and adaptation to novel environments), problem solving and \"creative\" tool modification among rooks, and tool use in crows.\n\n\n\n"}
{"id": "48623020", "url": "https://en.wikipedia.org/wiki?curid=48623020", "title": "Concise Encyclopedia of Supersymmetry and Noncommutative Structures in Mathematics and Physics", "text": "Concise Encyclopedia of Supersymmetry and Noncommutative Structures in Mathematics and Physics\n\nConcise Encyclopedia of Supersymmetry and Noncommutative Structures in Mathematics and Physics is a fundamental authoritative text in specialized areas of contemporary mathematics and physics. This is an English language reference work which consists of around 800 original articles by around 280 scientists under the guidance of 23 Advisory Board members.\n\nThe first edition was published by Kluwer Academic Publishers in 2004, when Kluwer was a part of Springer. A second printing with corrections was issued in 2005 by Springer Science & Business Media.\n\n\"Concise Encyclopedia of Supersymmetry\" contains articles devoted to supergravity, M-theory, quantum gravity, quantum groups, noncommutative geometry and other topics related to supersymmetry.\nThe articles are of the following kinds: 1) short review; 2) term/notion definition; 3) biography. \nThe detailed Subject Index consists of 31 four-column oversized pages and is of three level, which makes easier navigation through\nthe volume. A useful list of 116 abbreviations helps to understand special articles on supersymmetry also in other sources. \nThe book can serve as a working instrument for professionals and PhD students.\n\n\n\n\n"}
{"id": "4403422", "url": "https://en.wikipedia.org/wiki?curid=4403422", "title": "Congelation ice", "text": "Congelation ice\n\nCongelation ice is ice that forms on the bottom of an established ice cover.\n\nOn seawater, congelation ice is ice that forms on the bottom of an established sea ice cover, usually in the form of platelets which coalesce to form solid ice.\n\nOnly the water freezes to ice, the salt from the seawater is concentrated into brine, some of which is contained in pockets in the new ice. Due to the brine pockets, congelation ice on seawater is neither as hard nor as transparent as fresh water ice.\n\nOn the surface of lakes, or other bodies of still freshwater, \"congelation ice\" is often called \"black Ice\". This ice has frozen without many air bubbles trapped inside, making it transparent. Its transparency reveals the colour, usually black, of the water beneath it, hence the name. This is in contrast to \"snow ice\", sometimes called \"slush ice\", which is formed when slush (water saturated snow) refreezes. Snow ice is white due to the presence of air bubbles.\n\nBlack ice grows downward from the bottom of the existing ice surface. The growth rate of the ice is proportional to the rate that heat is transferred from the water below the ice surface to the air above the ice surface. The total ice thickness can be approximated from Stefan's equation.\n\nBlack ice is very hard, strong and smooth, which makes it ideal for ice skating, skate sailing, ice yachting and some other ice sports.\n\nThin, clear ice also has acoustic properties which are useful to tour skaters. Skating on clear ice radiates a tone whose frequency depends on the thickness of the ice.\n"}
{"id": "2363839", "url": "https://en.wikipedia.org/wiki?curid=2363839", "title": "Danger Room", "text": "Danger Room\n\nThe Danger Room is a fictional training facility appearing in American comic books published by Marvel Comics. The facility is depicted as built for the X-Men as part of the various incarnations of the X-Mansion.\n\nAn obstacle course in which the X-Men train appears in \"The X-Men\" #1 (September 1963), but the Danger Room is never mentioned by name. The name \"Danger Room\" is first used in \"The X-Men\" #2 (November 1963). According to X-Men writer/editor/co-creator Stan Lee, \"the Danger Room was Jack Kirby's idea. I thought it was great because we could always open with an action sequence if we needed to.\"\n\nIn the early books it was filled with traps, projectile firing devices, flamethrowers, and mechanical dangers such as presses, collapsing walls and the like intended to challenge the trainee. Meanwhile, an observer is in the overhanging control booth managing the room's mechanisms to oversee the exercise while manually ensuring the subject's safety. Later the Danger Room was upgraded with machines and robots for the X-Men to fight against.\n\nAfter befriending the Shi'ar the X-Men rebuilt the Danger Room with Shi'ar hard-light holographic technology. These upgrades were largely added by Dr. Hank McCoy (\"Beast\"). The Danger Room is located in the X-Mansion; every destruction of the latter led to a rebuilding, and usually upgrading, of the Danger Room. The training facility has endured extensive damage over the years, usually from X-Men training or X-Men going rogue, as Colossus did during The Muir Island Saga. Supervillains have dealt critical damage to it as well as taking over the facility, especially Arcade. The security and safety protocols that ensure the safety of anyone using the Danger Room have frequently been disrupted, tampered with by villains, failed, or have been completely negated in all the years of its use, each time happening more frequently as the room began to get more and more upgraded.\n\nIt is suggested in the \"X-Men Official Guide\" that the objects in the Danger Room are holograms surrounded by force fields, supposedly confirmed in \"Astonishing X-Men\" when a student managed to kill himself by jumping from a holographic cliff face. It is also revealed that the Danger Room can display holograms in only 32-bit color.\n\nIn Joss Whedon's \"Astonishing X-Men\", the Danger Room developed self-awareness. It is mentioned by Wolverine as 'acting twitchy all semester.' The first thing it does is convince Wing, who has recently been depowered by Ord's cure, to kill himself. The next thing it does is take control of an old, broken Sentinel robot and knock out all the psychics within the X-Mansion. During the Sentinel's attack, Cyclops orders Shadowcat and the students to go hide in the Danger Room for safety. While there, they find the corpse of Wing. The Danger Room reanimates the corpse and attacks the students. Apart from Wing, there are no additional fatalities among the students. After being freed from its prison, it takes the form of a woman. She is dubbed Danger and attacks the X-Men. After defeating them, she travels to Genosha to kill Professor Xavier.\n\nXavier was revealed to have known of the Danger Room's sentience and chosen not to reveal it, much to the dismay of the X-Men who seem to view this deception as taking on one of Magneto's former ideals. During the battle on Genosha, she takes control of Cassandra Nova's second Wild Sentinel and engages Beast in a vicious and brutal fight while the other X-Men take on the Wild Sentinel. After Beast destroys her body, \"Danger's\" consciousness was presumed to still exist within the conflicted consciousness of the Wild Sentinel.\n\nLater, Danger reappears in a new humanoid form, similar to her previous one, in which she infiltrated S.W.O.R.D. headquarters to speak with Ord of the Breakworld and offer her assistance.\n\nDanger and Ord both ended up on Breakworld, along with the X-Men and S.W.O.R.D., and after the robot encountered Emma Frost and an unconscious Cyclops, Emma told the robot that despite its supposed enmity it has let the mutants live too often, meaning it hasn't overcome its parent programming, so it cannot kill any mutant. Cyclops recovers, and Emma tells Danger to help the X-Men in Breakworld, and in exchange, it will be given Professor Xavier. Danger later attacks some of Breakworld's inhabitants and sides with the X-Men and S.W.O.R.D.\n\nAfter the Breakworld incident, S.W.O.R.D. takes Danger into custody. When the S.W.O.R.D. headquarters is destroyed during Secret Invasion, Danger escapes and goes to Australia, taking on the form of an anthropologist from Melbourne University. She approaches Rogue in her disguise but is targeted by a low flying Shi'ar salvage spacecraft, where she reveals who she really is and that she is going to use Rogue as a conduit so she can get her revenge on Professor Xavier.\n\nAfter being damaged by the crew, she warps the entire area with her holographic projections of past moments within Rogue's life as well as other famous moments from the X-Men's history. Xavier confronts Danger and she reveals she intended to make Rogue absorb all his powers and memories permanently, leaving him crippled and useless. Xavier reveals that when she first said \"Where am I?\", he consulted her Shi'ar makers who assured him it was not possible she could have gained self-awareness and that he had no way of knowing what she was or would become. He tried to free her but because she was really billions of lines of machine code, he didn't know which lines to erase without lobotomizing her. Because of not knowing what she was capable of, if he had freed her, she might have killed his X-Men because she had the knowledge and power, so instead he did nothing and watched her suffer. Xavier ends her suffering by repairing her. She then sides with Xavier, Rogue, and Gambit and takes out the Shi'ar salvage crew. Afterwards, with Xavier, she helps Rogue gain full control over her powers.\n\nDanger joins Rogue and Gambit to help the X-Men settle the unrest in San Francisco. She saves a child who mistakes her for a Transformer. She comes to aid Rogue and Gambit during a fight with Ares and later departs with them to find Trance and any other students still out during the riots. After locating Trance, they are attacked by Ms. Marvel who manages to seriously damage her shoulder.\n\nAfter the events of Utopia, Danger is seen being repaired by Madison Jeffries when they are attacked by Emplate. Danger tries to defend Madison only to be further damaged by Emplate. After being repaired, at the request of Cyclops, she informs the inhabitants of Utopia who Emplate is and his history. She works alongside Madison, Rogue and the X-Club on erasing Legion's many personalities, and is offered the position of warden of Utopia by Emma Frost, which she accepts because it also allows her to study the best and the worst of what humanity has to offer. Danger's first job was to interrogate Scalphunter, who was forced into bringing five Predator Xs to Utopia. Armor confronts Danger, due to her being responsible for Wing's death. The get into a tussle. Danger was able to discover Armor's weakness and insists that they talk. Armor angrily expresses to Danger that her presence with the X-Men pisses on Wing's memory. Danger tells Armor, that she constantly reboots herself, but can't wipe out Wing's image in her memory. Armor is surprised that Danger can experience feelings of guilt, and she proceeds to tell her about Wing.\n\nAs warden of Utopia, some of the prisoners she guards include Empath, Sebastian Shaw and Donald Pierce. She uses her Virtual Reality technology in an attempt to rehabilitate them. During the invasion from Selene's forces, she detects an energy source, but then malfunctions and crashes. Then, Shinobi Shaw and Harry Leland, who have been resurrected by the T-O virus, appear, ready to kill Shaw for preventing Selene's ascent to godhood. However, Danger is able to adapt to the mass increase and severed Shinobi's hand before he could kill his father.\n\nDanger appears to have been compromised as she is not aware that Donald Pierce has free rein to sabotage Utopia's defenses.\n\nBeast rebuilds the Westchester School at the behest of Wolverine at the aftermath of the Schism event. Built on the ruins of the previous Xavier Institute, the school is rechristened the Jean Grey School for Higher Learning and is built with state of the art Shi'ar Technology, gifted to him by the current Emperor of the Shi'ar Empire Gladiator (Kallark) whose son Kid Gladiator is enrolled (presumably to learn control over his vast powers.) The school incorporates a decentralized Danger Room that is integrated into the entire building itself instead of just one room. This is seen in the first issue of Wolverine and the X-Men when hidden defense weapons are activated in the boys' bathroom. Whether the danger room's holographic functions are fitted throughout the school as well remains to be seen.\n\nDanger reappears in the \"All-New X-Factor\" series. She appears as a prisoner of a member of the Thieves Guild, which Gambit is running, and when he discovers this, he orders her freed. After restoring her memory which was lost due to the imprisonment, he invites her to join the new X-Factor team.\n\nDanger appears in the form of the Blackbird jet plane and is used as the main transport for the original time-displaced X-Men. \n\nDuring the \"Secret Empire\" storyline, Danger takes Jean Grey and Jimmy Hudson to a prison stronghold to rescue their captured teammates. During the ensuing battle, she reveals her true form and projects holograms of the adult X-Men to keep the guards occupied.\n\nAs Danger gained self-awareness and adopted a more humanoid appearance, she has shown enhanced strength and durability, create hard-light holographic projections that can affect entire areas, energy blasts, flight, the ability to rebuild herself after her body is destroyed (at one point even rebuilding herself with butterfly-like wings which are soon destroyed by Beast) and control over other machines. She is able to bring other machines into self-awareness and upload herself into other machines to build and operate new bodies. She also possesses more detailed knowledge of the X-Men and their combat skills than any other source, having trained against them as the Danger Room.\n\nWith the Danger Room gone, the X-Men have resorted to using the empty room to train with their students. Also, the student Prodigy, using the borrowed knowledge from several X-Men, built a \"Danger Cave\" underneath the X-Mansion that is a giant stone room with a large metal circle control center.\n\nThe Danger Cave is similar to the Danger Room, but what makes the Danger Cave unique is that it uses holograms to train the students by re-enacting renowned battles the X-Men were involved in, like Inferno, Broodworld, Planet X, or Onslaught, even going so far as to dress the participants up in what the X-Men wore at that time.\n\nThe Danger Room equivalent in the \"Age of Apocalypse\" reality was the Killing Zone. The facility was located in Mount Wundagore and was apparently destroyed by Nemesis. Later, it was Dazzler who replaced the Danger Room and acted as a one-woman training facility for the fully fledged X-Men.\n\nThe X-Men from the Ultimate Marvel universe also have a Danger Room with similar technology, including smaller holographic training rooms in the hidden safehouses prepared by Professor X. However, these are prone to malfunctions, such as a fight sequence producing Hasidic rabbis instead of ninjas (though to no less of an effect towards the latter intent).\n\n\n\n\n\n"}
{"id": "13333575", "url": "https://en.wikipedia.org/wiki?curid=13333575", "title": "David Pearson (scientist)", "text": "David Pearson (scientist)\n\nDavid Pearson (born 1942) is a British-born Canadian scientist, academic and television personality. He is a professor of earth sciences and science communication at Laurentian University in Sudbury, Ontario.\n\nPearson was educated at the University of Durham, where he took a B.Sc. degree in geology in 1963, then at Imperial College, London, graduating Ph.D. and D.I.C. in 1967.\n\nJoining the teaching staff of Laurentian University in 1969, Pearson was the founding director of Science North, the city's interactive science museum, from 1980 to 1986. He retained an advisory position with Science North after 1986, and returned as science director in 2007 following the departure of Alan Nursall from the institution. He now serves as Senior Science Advisor to Science North and as Laurentian University's co-director of its Science Communication program.\n\nPearson also hosted science-oriented television programming, including TVOntario's \"Understanding the Earth\" and MCTV's syndicated \"The World Around Us\", in the 1980s. He received the Ward Neale Medal from the Geological Association of Canada for promotion of the earth sciences in 2001, and the McNeil Medal for science communication from the Royal Society of Canada in 2003.\n\nPearson was the co-chair of the 2009 Ontario Expert Panel on Climate Change Adaptation.\n\nIn 2016, he was made a member of the Order of Ontario.\n"}
{"id": "32021761", "url": "https://en.wikipedia.org/wiki?curid=32021761", "title": "Dehalococcoidetes", "text": "Dehalococcoidetes\n\nDehalococcoides is a class of Chloroflexi, a phylum of Bacteria. It is also known as the DHC group.\n\nThe name \"Dehalococcoidetes\" is a placeholder name given by Hugenholtz and Stackebrandt, 2004, after \"Dehalococcoides ethenogenes\", a partially described species in 1997, whereas the first species fully described belonging to this class was \"Dehalogenimonas lykanthroporepellens\" by Moe et al. 2009, but no emendations to the name were made.\n\nBoth species, \"Dehalococcoides ethenogenes\" and \"Dehalogenimonas lykanthroporepellens\" are irregular coccus (coccoid) bacteria capable of dehalogenating polychlorinated aliphatic alkanes and alkenes, such as tetrachloroethene, trichloropropane, trichloroethane, dichloropropane, and dichloroethane.\n\nOne of the features of the members of the phylum Chloroflexi is the unusual cell wall structure, which is monoderm but with great variation in presence or structure of the peptidoglycan resulting in many members staining Gram-negative and other Gram-positive.\nBoth species of Dehalococcoidetes stain Gram negative, but they potentially lack peptidoglycan and instead possess pseudopeptidoglycan (S-layer) (resistant to peptidoglycan-attacking antibiotics ampicillin and vancomycin; wheat germ agglutinin does not bind nor does lysozyme work).\n\n"}
{"id": "12193070", "url": "https://en.wikipedia.org/wiki?curid=12193070", "title": "Dye 3", "text": "Dye 3\n\nDye 3 is an ice core site and previously part of the Distant Early Warning (DEW) line, located at (, 2480 masl) in Greenland. As a DEW line base, it was disbanded in years 1990/1991.\n\nAn ice core is a core sample from the accumulation of snow and ice that has re-crystallized and trapped air bubbles over many years. The composition of these ice cores, especially the presence of hydrogen and oxygen isotopes, provides a picture of the climate at the time. Ice cores contain an abundance of climate information.\n\nInclusions in the snow, such as wind-blown dust, ash, bubbles of atmospheric gas and radioactive substances, remain in the ice. The variety of climatic proxies is greater than in any other natural recorder of climate, such as tree rings or sediment layers. These include (proxies for) temperature, ocean volume, precipitation, chemistry and gas composition of the lower atmosphere, volcanic eruptions, solar variability, sea-surface productivity, desert extent and forest fires.\n\nTypical ice cores are removed from an ice sheet such as the ice cap internal to Greenland. Greenland is, by area, the world's largest island. The Greenland ice sheet covers about 1.71 million km and contains about 2.6 million km of ice.\n\nThe 'Greenland ice sheet' () is a vast body of ice covering 1.71 million km, roughly 80% of the surface of Greenland. It is the second largest ice body in the World, after the Antarctic Ice Sheet. The ice sheet is almost 2,400 kilometers long in a north-south direction, and its greatest width is 1,100 kilometers at a latitude of 77°N, near its northern margin. The mean altitude of the ice is 2,135 meters.\n\nThe ice in the current ice sheet is as old as 110,000 years. However,\nit is generally thought that the Greenland Ice Sheet formed in the late Pliocene or early Pleistocene by coalescence of ice caps and glaciers. It did not develop at all until the late Pliocene, but apparently developed very rapidly with the first continental glaciation.\n\nThe ice surface reaches its greatest altitude on two north-south elongated domes, or ridges. The southern dome reaches almost 3,000 metres at latitudes 63°–65°N; the northern dome reaches about 3,290 metres at about latitude 72°N. The crests of both domes are displaced east of the centre line of Greenland. The unconfined ice sheet does not reach the sea along a broad front anywhere in Greenland, so that no large ice shelves occur.\n\nOn the ice sheet, temperatures are generally substantially lower than elsewhere in Greenland. The lowest mean annual temperatures, about −31 °C (−24 °F), occur on the north-central part of the north dome, and temperatures at the crest of the south dome are about −20 °C (−4 °F).\n\nDuring winter, the ice sheet takes on a strikingly clear blue/green color. During summer, the top layer of ice melts leaving pockets of air in the ice that makes it look white. Positioned in the Arctic, the Greenland ice sheet is especially vulnerable to global warming. Arctic climate is now rapidly warming.\n\nDye-2 and 3 were among 58 Distant Early Warning (DEW) Line radar stations built by the United States of America (USA) between 1955 and 1960 across Alaska, Canada, Greenland and Iceland at a cost of billions of dollars.\n\nAfter extensive studies in late 1957, the US Air Force (USAF) selected sites for two radar stations on the ice cap in southern Greenland. The DYE stations were the eastern extension of the DEW Line. DYE-1 was on the West Coast at Holsteinsborg; DYE-4 on the East Coast at Kulusuk. Dye 2 (66°29'30\"N 46°18'19\"W, 2338 masl) was built approximately 100 miles east of Sondrestrom AB and 90 miles south of the Arctic Circle at an altitude of 7,600 feet. Dye 3 was located approximately 100 miles east of Dye 2 and slightly south at an elevation of 8,600 feet, contrary to USAF map above.\n\nThe sites were constructed with materials provided through airlift from LC-130's flying out of Kangerlussuaq, Greenland.\n\nThe new radar sites were found to receive from three to four feet of snow each year. The snow was formed into large drifts by winds constantly blowing as much as 100 mph. To overcome this, the Dye sites were elevated approximately 20 feet above the ice cap surface. Dye 3 was completed in 1960. Due to snow accretion, the station was \"jacked up\" again in the late 1970s, but by the 1990s needed further elevation.\n\nInstead, Dye 3 was closed as a radar station in the years 1990/1991.\n\nToday, it is used as a training site for the 139th Airlift Squadron.\n\nThe Greenland Ice Sheet Project (GISP) was a decade-long project to drill 20 ice cores in Greenland. GISP involved scientists and funding agencies from Denmark, Switzerland and the United States. Besides the U.S. National Science Foundation, funding was provided by the Swiss National Science Foundation and the Danish Commission for Scientific Research in Greenland. The ice cores provide a proxy archive of temperature and atmospheric constituents that help to understand past climate variations.\n\nAnnual field expeditions were carried out to drill intermediate depth cores at various locations on the ice sheet:\n\n\n“On most of the Greenland ice sheet, however, the annual accumulation rate is considerably higher than 0.2 m ice a, and the delta method therefore works thousands of years backwards in time, the only limitation being obliteration of the annual delta cycles by diffusion of the water molecule in the solid ice...” Delta refers to the changing proportion of oxygen-18 in the different seasonal layers. “The main reason for the seasonal delta variations is that, on its travel to the polar regions, a precipitating air mass is generally cooled more in winter than in summer.” “... the annual layer thickness...decreases from 19 cm in 2,000-year-old ice to 2 cm in 10,000-year-old ice due to plastic thinning of the annual layers as they sink towards greater depths.” “... volcanic acids in snow layers deposited shortly after a large volcanic eruption can be detected – as elevated specific conductivities measured on melted ice samples8, or as elevated acidities revealed by an electric current through the solid ice...”\n\nAlthough available GISP data gathered over the earlier seven years, pointed to north-central Greenland as the optimum site location for the first deep drilling, financial restrictions forced the selection of the logistically convenient Dye-3 location.\n\nPreliminary GISP field work started in 1971 at Dye 3 (), where a 372 meter deep, 10.2 cm diameter core was recovered using a Thermal (US) drill type. Three more cores to depths of 90, 93, and 95 m were drilled with different drill types.\n\nFor an intermediate drilling c. 390 m, the drill was installed 25 m below the surface at the bottom of the Dye 3 radar station. Some 740 seasonal δ cycles were counted, indicating that the core reached back to 1231 AD. Evident in this coring was that as melt water seeps through the porous snow, it refreezes somewhere in the cold firn and disturbs the layer sequence.\n\nA second core at Dye 3 was drilled in 1975 with a Shallow (Swiss) drill type to 95 m at 7.6 cm diameter.\n\nA third core at Dye 3 was drilled in 1976 with a Wireline (US) drill type, 10.2 cm diameter, to 93 m.\n\nAnother core at Dye 3 was drilled in 1978 using a Shallow (US) drill type, 10.2 cm diameter, to 90 m.\n\nMeasurements of [SO] and [NO] in firn samples spanning the period 1895–1978 were taken from the Dye 3 1978 core down to 70 m.\n\nIn 1979, the initial Dye-3 deep bedrock drilling was started using a 22.2 cm diameter CRREL thermal (US) coring drill to produce an 18 cm diameter access hole, which was cased, to a depth of 77 m. The large diameter casing was inserted over the porous firn zone to contain the drilling fluid.\n\nAfter working out various logistical and engineering problems related to the development of a more sophisticated drilling rig, drilling to bedrock at Dye 3 began in the summer of 1979 using a new Danish electro-mechanical ice drill yielding a 10.2 cm diameter core. From July to August 1979 using ISTUK, 273 m of core was removed. At the end of the 1980 field season ISTUK had gnawed down to 901 m. In 1981 at a depth of 1785 m dust and conductivity measurements indicated the beginning of ice from the last glaciation. Coring continued and on August 10, 1981, bedrock was reached at a depth of 2038 m. The depth range for the Danish drill was 80–2038 m.\n\nThe Dye 3 site was a compromise: glaciologically, a higher site on the ice divide with smooth bedrock would have been better; logistically, such a site would have been too remote.\n\nThe borehole is 41.5 km east of the local ice divide of the south Greenland ice sheet.\n\nThe Dye 3 cores were part of the GISP and, at 2037 meters, the final Dye 3 1979 core was the deepest of the 20 ice cores recovered from the Greenland ice sheet. The surface ice velocity is 12.5 ma, 61.2° true. At 500 m above bedrock, the ice velocity is ~10 ma, 61.2° true. The ice upstream and downstream from Dye 3 is flowing downhill (-) on ~0.48 % mean slope. The bedrock temperature is −13.22 °C (as of 1984).\n\nThe Dye 3 1979 core is not completely intact and is not undamaged. “Below 600 m, the ice became brittle with increasing depth and badly fractured between 800 and 1,200 m. The physical property of the core progressively improved and below ~1,400 m was of excellent quality.” “The deep ice core drilling terminated in August 1981. The ice core is 2035 m long and has a diameter of 10 cm. It was drilled with less than 6° deviation from vertical, and less than 2 m is missing. The deepest 22 m consists of silty ice with an increasing concentration of pebbles downward. In the depth interval 800 to 1400 m the ice was extremely brittle, and even careful handling unavoidably damaged this part of the core, but the rest of the core is in good to excellent condition.”\n\nThe depth interval 800 to 1400 m would be a period approximately from about two thousand years ago to about five or six thousand years ago.\n\nMelting has been commonplace throughout the Holocene. Summer melting is usually the rule at Dye 3, and there is occasional melting even in north Greenland. All of these meltings disturb the clarity of the annual record to some degree. “An exceptionally warm spell can produce features which extend downwards by percolation, along isolated channels, into the snow of several previous years. This can happen in regions which generally have little or no melting at the snow surface as exemplified during mid July 1954 in north-west Greenland. Such an event could lead to the conclusion that two or three successive years had abnormally warm summers, whereas all the icing formed during a single period which lasted for several days. The location where melt features will have the greatest climactic significance is high in the percolation facies where summer melting is common but deep percolation is minimal. Dye 3 in southern Greenland (65°11’N; 43°50’W) is such a location.”\n\nAs the drill site of Dye 3 receives more than twice as much accumulation as central Greenland, the annual layers are well resolved and relatively thick in the upper parts, making the core ideal for dating the most recent millennia. But, the high accumulation rate has resulted in relatively rapid ice flow (flow-induced layer thinning and diffusion of isotopes), Dye 3 1979 cannot be used for annual layer counting much more than 8 kyr back in time.\n\nCrystal diameters range from ~0.2 cm at 1900 m from bedrock (depth 137 m) to ~0.42 cm vertical diameter (v) and ~0.55 cm horizontal diameter (h) at 300 m above bedrock (depth 1737 m). However, below 300 m crystal diameter decreases rapidly with increasing dust concentration to a minimum of ~0.05 cm at 200 m above bedrock (depth 1837 m), increasing again linearly to ~0.25 cm v and ~0.3 cm h just above bedrock. Crystal diameters remain approximately constant between 1400 and 300 m above bedrock (depths 637–1737 m), with the largest crystals and the largest distortion (~0.55 cm v and ~0.7 cm h) occurring at 1100 m above bedrock (depth 937 m).\n\nThe brittle zone mentioned above under \"Core continuity\" corresponds in Dye 3 1979 with the steady state grain size (crystal size) from ~637 to ~1737 m depth range. This is also the Holocene climatic optimum period.\n\nAs of 1998 the only long record available for Be is from Dye 3 1979. Questions were raised whether all parts of the Dye 3 1979 record reflect the sun activity or are affected by climatic and/or ice dynamics.\n\nThe dust concentration has a peak of ~3 mg/kg at 200 m above bedrock (depth 1837 m), second only to the silty ice (>20 mg/kg) of the bottom 25 m, which has a very high deformation rate.\n\nThe uppermost 1780 m is considered Holocene ice, and the lower portion is considered as deposited during the Wisconsin period.\n\nFrom the δ O profile of the Dye 3 core it is relatively easy to differentiate the post-glacial climatic optimum, portions thereof and earlier: the Pre-Boreal transition, the Allerød, Bølling, Younger Dryas, and Oldest Dryas. In the Dye 3 1979 oxygen isotope record, the Older Dryas appears as a downward peak establishing a small, low-intensity gap between the Bølling and the Allerød.\n\nDuring the transition from the Younger Dryas to the Pre-Boreal, the South Greenland temperature increased by 15 °C in 50 years. At the beginning of this same transition the deuterium excess and dust concentration shifted to lower levels in less than 20 years.\n\nThe post-glacial climatic optimum lasted from ~9000–4000 yrs B.P. as determined from Dye 3 1979 and Camp Century 1963 δ O profiles. Both Dye 3 1979 and Camp Century 1963 cores exhibit the 8.2 ka event and the boundary event separating Holocene I from Holocene II.\n\nSamples from the base of the 2 km deep Dye 3 1979 and the 3 km deep GRIP cores revealed that high-altitude southern Greenland has been inhabited by a diverse array of conifer trees and insects within the past million years.\n\nEllen Mosley-Thompson led a 3-man glaciological team to drill an intermediate depth core at Dye 3, Greenland.\n\nFor a map of the locations of the various Greenland ice cap corings, see ref.\n\nTo investigate the possibility of climatic cooling, scientists drilled into the Greenland ice caps to obtain core samples. The oxygen isotopes from the ice caps suggested that the Medieval Warm Period had caused a relatively milder climate in Greenland, lasting from roughly 800 to 1200. However, from 1300 or so the climate began to cool. By 1420, we know that the \"Little Ice Age\" had reached intense levels in Greenland.\n\nFor most of the arctic ice cores up to 1987, regions of the core with high dust concentrations correlate well with the ice having high deformation rates and small crystal diameters, in both Holocene and Wisconsin ice.\n\nThe Camp Century, Greenland, ice core (cored from 1963–1966) is 1390 m deep and contains climatic oscillations with periods of 120, 940, and 13,000 years.\n\n“Thus in principle dating of the Camp Century ice core by counting annual layers is possible to about the 1,060 m depth, corresponding to 8,300 yr BP according to the time scale which we shall adopt.” “It may be necessary, however, to apply a depth dependent correction to account for ‘lost’ annual oscillations. Even during firnification seasonal δ-oscillations in years with unusually low accumulation may disappear due to mass exchange. Unfortunately, the physical condition (broken or missing pieces) of the Camp Century ice core precludes continuous measurement of seasonal isotope variations for the purpose of dating from the surface downward.”\n\nThe Crête core was drilled in central Greenland (1974) and reached a depth of 404.64 meters, extending back only about fifteen centuries.\n\n\"The first core drilled at Station Milcent in central Greenland covers the past 780 years.\" Milcent core was drilled at 70.3°N, 44.6°W, 2410 masl. The Milcent core (398 m) was 12.4 cm in diameter, using a Thermal (US) drill type, in 1973.\n\nThe Milcent core record only goes back to AD 1174 (Holocene) due to the high accumulation rates.\n\nThe Renland ice core was drilled in 1985. The Renland ice core from East Greenland apparently covers a full glacial cycle from the Holocene into the previous Eemian interglacial. The Renland ice core is 325 m long.\n\nFrom the delta-profile, the Renland ice cap in the Scoresbysund Fiord has always been separated from the inland ice, yet all the delta-leaps revealed in the Camp Century 1963 core recurred in the Renland ice core.\n\nThe Renland core is noted for apparently containing the first Northern Hemisphere record of methanesulfonate (MSA), and having the first continuous record of non-seasalt sulfate.\n\nThe Renland core is also the first to provide a continuous record of ammonium (NH) apparently through the whole glacial period.\n\nThe distribution of Be in the top 40 m of the Renland ice core has been reported and corroborates the Be cyclic fluctuation pattern from Dye 3.\n\nThe Renland core apparently contains ice from the Eemian onward.\n\nGRIP successfully drilled a 3028-metre ice core to the bed of the Greenland Ice Sheet at Summit, Central Greenland from 1989 to 1992 at , 3238 masl.\n\nEight ash layers have been identified in the central Greenland ice core GRIP. Four of the ash layers (Ash Zones I and II, Saksunarvatn and the Settlement layer) originating in Iceland have been identified in GRIP by comparison of chemical composition of glass shards from the ash. The other four have not been correlated with known ash deposits.\n\nThe Saksunarvatn tephra via radiocarbon dating is ca 10,200 years BP.\n\nThe follow-up U.S. GISP2 project drilled at a glaciologically better location on the summit (72°36'N, 38°30'W, 3200 masl). This hit bedrock (and drilled another 1.55 m into bedrock) on July 1, 1993 after five years of drilling. European scientists produced a parallel core in the GRIP project. GISP2 produced an ice core 3053.44 meters in depth, the deepest ice core recovered in the world at the time. The GRIP site was 30 km to the east of GISP2. \"Down to a depth of 2790 m in GISP2 (corresponding to an age of about 110 kyr B.P.), the GISP2 and GRIP records are nearly identical in shape and in many of the details.\"\n\nThe GISP2 time scale is based on counting annual layers primarily by visual stratigraphy.\n\nThe isotopic temperature records show 23 interstadial events correlateable between the GRIP and GISP2 records between 110 and 15 kyr B.P. Ice in both cores below 2790 m depth (records prior to 110 kyr B.P.) shows evidence of folding or tilting in structures too large to be fully observed in a single core.\n\nThe bulk of the GISP2 ice core is archived at the National Ice Core Laboratory in Lakewood, Colorado, United States.\n\nThe drilling site of the North Greenland Ice Core Project (NGRIP) is near the center of Greenland (75.1 N, 42.32 W, 2917 m, ice thickness 3085). Drilling began in 1999 and was completed at bedrock in on July 17, 2003. The NGRIP site was chosen to extract a long and undisturbed record stretching into the last glacial, and it was apparently successful.\n\nUnusually, there is melting at the bottom of the NGRIP core – believed to be due to a high geothermal heat flux locally. This has the advantage that the bottom layers are less compressed by thinning than they would otherwise be: NGRIP annual layers at 105 kyr age are 1.1 cm thick, twice the GRIP thicknesses at equal age.\n\nThe site was chosen for a flat basal topography to avoid the flow distortions that render the bottom of the GRIP and GISP cores unreliable.\n\nIn the upper 80 m of the ice sheet, the firn or the snow gradually compacts to a close packing of ice crystals of typical sizes 1 to 5 mm. Crystal size distributions were obtained from fifteen vertical thin sections of 20 cm × 10 cm (height × width) and a thickness of 0.4 ±0.1 mm of ice evenly distributed in the depth interval 115 – 880 m. Peak sizes with depth were ~1.9 mm 115 m, ~2.2 mm 165 m, ~2.8 mm 220 m, ~3.0 mm 330 m, ~3.2 mm 440 m, ~3.3 mm 605 m, whereas mean sizes with depth were ~1.8 115 m, ~2.2 mm 165 m, ~2.4 mm 220 m, ~2.8 mm ~270 m, ~2.75 mm 330 m, ~2.6 mm ~370 m, ~2.9 mm 440 m, ~2.8 mm ~490 m, ~2.9 mm ~540 m, ~2.9 mm 605 m, ~3.0 ~660 m, ~3.2 mm ~720 m, ~2.9 mm ~770 m, ~2.7 mm ~820 m, ~2.8 mm 880 m. And, here again as with Dye 3, steady state in grain growth was reached and continued through the post-glacial climatic optimum.\n\nThe size distribution of ice crystals changes with depth and approaches the Normal Grain Growth law via competing mechanisms of fragmentation (producing smaller polygonal grains) and grain boundary diffusion (producing larger, vertically compressed, horizontally expanded grains). Although some of the peaks for the deeper distributions appear to be slightly greater, the predicted steady state average grain size is 2.9±0.1 mm.\n\nThe NGRIP record helps to resolve a problem with the GRIP record – the unreliability of the Eemian Stage portion of the record. NGRIP covers 5 kyr of the Eemian, and shows that temperatures then were roughly as stable as the pre-industrial Holocene temperatures were. This is confirmed by sediment cores, in particular MD95-2042.\n\nIn 2003, NGRIP recovered what seem to be plant remnants nearly two miles below the surface, and they may be several million years old.\n\n\"Several of the pieces look very much like blades of grass or pine needles,\" said University of Colorado at Boulder geological sciences Professor James White, an NGRIP principal investigator. \"If confirmed, this will be the first organic material ever recovered from a deep ice-core drilling project,\" he said.\n\n\n"}
{"id": "12274706", "url": "https://en.wikipedia.org/wiki?curid=12274706", "title": "Ed Nather", "text": "Ed Nather\n\nR. Edward Nather (1926 – 2014) was the Rex G. Baker, Jr. and McDonald Observatory Centennial Research Professor Emeritus in Astronomy at UT Austin. He pioneered the fields of asteroseismology of white dwarfs, and observational studies of interacting binary collapsed stars.\nHe served as the director of the Whole Earth Telescope for the first decade of its existence, and achieved internet fame by posting the Story of Mel, a Real Programmer, on Usenet.\n\nNather died on August 13, 2014 in Austin, Texas. He is survived by his wife, Marilane Nather; his children, Kathy Nather Thomas, Kelley Thompson, Wendy Nather, David Nather and Lara Nather; nine grandchildren and two great-grandchildren.\n"}
{"id": "7977203", "url": "https://en.wikipedia.org/wiki?curid=7977203", "title": "Engineering economics", "text": "Engineering economics\n\n\"For the application of engineering economics in the practice of civil engineering please see this article.\"\n\nEngineering Economics, previously known as engineering economy, is a subset of economics concerned with the use and \"...application of economic principles\" in the analysis of engineering decisions. As a discipline, it is focused on the branch of economics known as microeconomics in that it studies the behavior of individuals and firms in making decisions regarding the allocation of limited resources. Thus, it focuses on the decision making process, its context and environment. It is pragmatic by nature, integrating economic theory with engineering practice. But, it is also a simplified application of microeconomic theory in that it avoids a number of microeconomic concepts such as price determination, competition and demand/supply. As a discipline though, it is closely related to others such as statistics, mathematics and cost accounting. It draws upon the logical framework of economics but adds to that the analytical power of mathematics and statistics. \n\nEngineers seek solutions to problems, and the economic viability of each potential solution is normally considered along with the technical aspects.\nFundamentally, engineering economics involves formulating, estimating, and evaluating the economic outcomes when alternatives to accomplish a defined purpose are available.\n\nIn some U.S. undergraduate civil engineering curricula, engineering economics is a required course. It is a topic on the Fundamentals of Engineering examination, and questions might also be asked on the Principles and Practice of Engineering examination; both are part of the Professional Engineering registration process.\n\nConsidering the time value of money is central to most engineering economic analyses. Cash flows are \"discounted\" using an interest rate, except in the most basic economic studies.\n\nFor each problem, there are usually many possible \"alternatives\". One option that must be considered in each analysis, and is often the \"choice\", is the \"do nothing alternative\". The \"opportunity cost\" of making one choice over another must also be considered. There are also non-economic factors to be considered, like color, style, public image, etc.; such factors are termed \"attributes\".\n\n\"Costs\" as well as \"revenues\" are considered, for each alternative, for an \"analysis period\" that is either a fixed number of years or the estimated life of the project. The \"salvage value\" is often forgotten, but is important, and is either the net cost or revenue for decommissioning the project.\n\nSome other topics that may be addressed in engineering economics are inflation, uncertainty, replacements, depreciation, resource depletion, taxes, tax credits, accounting, cost estimations, or capital financing. All these topics are primary skills and knowledge areas in the field of cost engineering.\n\nSince engineering is an important part of the manufacturing sector of the economy, engineering industrial economics is an important part of industrial or business economics. Major topics in engineering industrial economics are:\n\n\nSome examples of engineering economic problems range from value analysis to economic studies. Each of these is relevant in different situations, and most often used by engineers or project managers. For example, engineering economic analysis helps a company not only determine the difference between fixed and incremental costs of certain operations, but also calculates that cost, depending upon a number of variables. Further uses of engineering economics include:\n\n\nEach of the previous components of engineering economics is critical at certain junctures, depending on the situation, scale, and objective of the project at hand. Critical path economy, as an example, is necessary in most situations as it is the coordination and planning of material, labor, and capital movements in a specific project. The most critical of these \"paths\" are determined to be those that have affect upon the outcome both in time and cost. Therefore, the critical paths must be determined and closely monitored by engineers and managers alike. Engineering economics helps provide the Gantt charts and activity-event networks to ascertain the correct use of time and resources.\n\nProper value analysis finds its roots in the need for industrial engineers and managers to not only simplify and improve processes and systems, but also the logical simplification of the designs of those products and systems. Though not directly related to engineering economy, value analysis is nonetheless important, and allows engineers to properly manage new and existing systems/processes to make them more simple and save money and time. Further, value analysis helps combat common \"roadblock excuses\" that may trip up managers or engineers. Sayings such as \"The customer wants it this way\" are retorted by questions such as; has the customer been told of cheaper alternatives or methods? \"If the product is changed, machines will be idle for lack of work\" can be combated by; can management not find new and profitable uses for these machines? Questions like these are part of engineering economy, as they preface any real studies or analyses. \n\nLinear programming is the use of mathematical methods to find optimized solutions, whether they be minimized or maximized in nature. This method uses a series of lines to create a polygon then to determine the largest, or smallest, point on that shape. Manufacturing operations often use linear programming to help mitigate costs and maximize profits or production. \n\nConsidering the prevalence of capital to be lent for a certain period of time, with the understanding that it will be returned to the investor, money-time relationships analyze the costs associated with these types of actions. Capital itself must be divided into two different categories, \"equity capital\" and \"debt capital\". Equity capital is money already at the disposal of the business, and mainly derived from profit, and therefore is not of much concern, as it has no owners that demand its return with interest. Debt capital does indeed have owners, and they require that its usage be returned with \"profit\", otherwise known as interest. The interest to be paid by the business is going to be an expense, while the capital lenders will take interest as a profit, which may confuse the situation. To add to this, each will change the income tax position of the participants. \n\nInterest and Money - Time Relationships comes into play when the capital required to complete a project must be either borrowed or derived from reserves. To borrow brings about the question of interest and value created by the completion of the project. While taking capital from reserves also denies its usage on other projects that may yield more results. Interest in the simplest terms is defined by the multiplication of the principle, the units of time, and the interest rate. The complexity of interest calculations, however, becomes much higher as factors such as compounding interest or annuities come into play. \n\nThe fact that assets and material in the real world eventually wear down, and thence break, is a situation that must be accounted for. Depreciation itself is defined by the decreasing of value of any given asset, though some exceptions do exist. Valuation can be considered the basis for depreciation in a basic sense, as any decrease in \"value\" would be based on an \"original value\". The idea and existence of depreciation becomes especially relevant to engineering and project management is the fact that capital equipment and assets used in operations will slowly decrease in worth, which will also coincide with an increase in the likelihood of machine failure. Hence the recording and calculation of depreciation is important for two major reasons. \n\n\nBoth of these reasons, however, cannot make up for the \"fleeting\" nature of depreciation, which make direct analysis somewhat difficult. To further add to the issues associated with depreciation, it must be broken down into three separate types, each having intricate calculations and implications. \n\n\nCalculation of depreciation also comes in a number of forms; \"straight line, declining balance, sum-of-the-year's,\" and \"service output\". The first method being perhaps the easiest to calculate, while the remaining have varying levels of difficulty and utility. Most situations faced by managers in regards to depreciation can be solved using any of these formulas, however, company policy or preference of individual may affect the choice of model. \n\nCapital budgeting, in relation to engineering economics, is the proper usage and utilization of capital to achieve project objectives. It can be fully defined by the statement; \"... as the series of decisions by individuals and firms concerning how much and where resources will be obtained and expended to meet future objectives.\" This definition almost perfectly explains capital and its general relation to engineering, though some special cases may not lend themselves to such a concise explanation. The actual acquisition of that capital has many different routes, from equity to bonds to retained profits, each having unique strengths and weakness, especially when in relation to income taxation. Factors such as risk of capital loss, along with possible or expected returns must also be considered when capital budgeting is underway. For example, if a company has $20,000 to invest in a number of high, moderate, and low risk projects, the decision would depend upon how much risk the company is willing to take on, and if the returns offered by each category offset this perceived risk. Continuing with this example, if the high risk offered only 20% return, while the moderate offered 19% return, engineers and managers would most likely choose the moderate risk project, as its return is far more favorable for its category. The high risk project failed to offer proper returns to warrant its risk status. A more difficult decision may be between a moderate risk offering 15% while a low risk offering 11% return. The decision here would be much more subject to factors such as company policy, extra available capital, and possible investors. \"In general, the firm should estimate the project opportunities, including investment requirements and prospective rates of return for each, expected to be available for the coming period. Then the available capital should be tentatively allocated to the most favorable projects. The lowest prospective rate of return within the capital available then becomes the minimum acceptable rate of return for analyses of any projects during that period.\"\n\nBeing one of the most important and integral operations in the engineering economic field is the minimization of cost in systems and processes. Time, resources, labor, and capital must all be minimized when placed into any system, so that revenue, product, and profit can be maximized. Hence, the general equation;\n\nformula_1\n\nwhere \"C\" is total cost, \"a b\" and \"k\" are constants, and \"x\" is the variable number of units produced. \n\nThere are a great number of cost analysis formulas, each for particular situations and are warranted by the policies of the company in question, or the preferences of the engineer at hand. \n\nEconomic studies, which are much more common outside of engineering economics, are still used from time to time to determine feasibility and utility of certain projects. They do not, however, truly reflect the \"common notion\" of economic studies, which is fixated upon macroeconomics, something engineers have little interaction with. Therefore, the studies conducted in engineering economics are for specific companies and limited projects inside those companies. At most one may expect to find some feasibility studies done by private firms for the government or another business, but these again are in stark contrast to the overarching nature of true economic studies. Studies have a number of major steps that can be applied to almost every type of situation, those being as follows;\n\n\n\n"}
{"id": "908660", "url": "https://en.wikipedia.org/wiki?curid=908660", "title": "Fach", "text": "Fach\n\nThe German system (; literally \"compartment\" or \"subject of study\", here in the sense of \"vocal specialization\") is a method of classifying singers, primarily opera singers, according to the range, weight, and color of their voices. It is used worldwide, but primarily in Europe, especially in German-speaking countries and by repertory opera houses.\n\nThe ' system is a convenience for singers and opera houses. It prevents singers from being asked to sing roles which they are incapable of performing. Opera companies keep lists of available singers by ' so that when they are casting roles for an upcoming production, they do not inadvertently contact performers who would be inappropriate for the part.\n\nBelow is a list of ' (), their ranges as written on sheet music, and roles generally considered appropriate to each. When two names for the ' are given, the first is in more common use today. Where possible, an English and/or Italian equivalent of each ' is listed; however, not all ' have ready English or Italian equivalents. Note that some roles can be sung by more than one ' and that many singers do not easily fit into a ': for instance some sopranos may sing both ' and ' roles. In addition, roles traditionally more difficult to cast may be given to a voice other than the traditional \"\". For instance, the \"Queen of the Night\" is more traditionally a dramatic coloratura role, but it is difficult to find a dramatic coloratura to sing it (particularly given the extreme range). Therefore, the role is often sung by a lyric coloratura.\n\n\nOne must not mistake the Mozartian dramatic coloratura soprano with the Italian dramatic coloratura soprano. A singer that sings Konstanze, Donna Anna or Fiordiligi can not necessarily sing the Italian dramatic coloratura parts, due to other vocal demands. Imogene, Leonora and Violetta require a dramatic soprano voice and are most often sung by dramatic sopranos with an agile voice that can easily produce coloratura and high notes. Roles like Norma, Lady Macbeth, Odabella or Abigaille are good examples of Italian roles that are not necessarily a coloratura soprano (even though the score calls for coloratura singing), but a full bodied dramatic soprano with a voice that can handle extreme dramatic singing and that is flexible enough to sing coloratura. Giuseppe Verdi wrote many parts like this in his early years.\n\n\n\n\n\nTwo roles mentioned above, Salome and the Marschallin, are relatively high dramatic sopranos and require that the soprano can endure long stretches of very high tessitura. Richard Strauss himself said that Salome should be sung by someone with the flexibility of a dramatic coloratura due to the high tessitura.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3783795", "url": "https://en.wikipedia.org/wiki?curid=3783795", "title": "Future interests (actuarial science)", "text": "Future interests (actuarial science)\n\nFuture interests is the subset of actuarial math that divides enjoyment of property -- usually the right to an income stream either from an annuity, a trust, royalties, or rents -- based usually on the future survival of one or more persons (natural humans, not juridical persons such as corporations).\n"}
{"id": "869876", "url": "https://en.wikipedia.org/wiki?curid=869876", "title": "Geologic modelling", "text": "Geologic modelling\n\nGeologic modelling, Geological modelling or Geomodelling is the applied science of creating computerized representations of portions of the Earth's crust based on geophysical and geological observations made on and below the Earth surface. A Geomodel is the numerical equivalent of a three-dimensional geological map complemented by a description of physical quantities in the domain of interest.\nGeomodelling is related to the concept of Shared Earth Model; \nwhich is a multidisciplinary, interoperable and updatable knowledge base about the subsurface.\n\nGeomodelling is commonly used for managing natural resources, identifying natural hazards, and quantifying geological processes, with main applications to oil and gas fields, groundwater aquifers and ore deposits. For example, in the oil and gas industry, realistic geologic models are required as input to reservoir simulator programs, which predict the behavior of the rocks under various hydrocarbon recovery scenarios. A reservoir can only be developed and produced once; therefore, making a mistake by selecting a site with poor conditions for development is tragic and wasteful. Using geological models and reservoir simulation allows reservoir engineers to identify which recovery options offer the safest and most economic, efficient, and effective development plan for a particular reservoir.\n\nGeologic modelling is a relatively recent subdiscipline of geology which integrates structural geology, sedimentology, stratigraphy, paleoclimatology, and diagenesis;\n\nIn 2-dimensions (2D), a geologic formation or unit is represented by a polygon, which can be bounded by faults, unconformities or by its lateral extent, or crop. In geological models a geological unit is bounded by 3-dimensional (3D) triangulated or gridded surfaces. The equivalent to the mapped polygon is the fully enclosed geological unit, using a triangulated mesh. For the purpose of property or fluid modelling these volumes can be separated further into an array of cells, often referred to as voxels (volumetric elements). These 3D grids are the equivalent to 2D grids used to express properties of single surfaces.\n\nGeomodelling generally involves the following steps: \n\nIncorporating the spatial positions of the major formation boundaries, including the effects of faulting, folding, and erosion (unconformities). The major stratigraphic divisions are further subdivided into layers of cells with differing geometries with relation to the bounding surfaces (parallel to top, parallel to base, proportional). Maximum cell dimensions are dictated by the minimum sizes of the features to be resolved (everyday example: On a digital map of a city, the location of a city park might be adequately resolved by one big green pixel, but to define the locations of the basketball court, the baseball field, and the pool, much smaller pixels - higher resolution - need to be used).\n\nEach cell in the model is assigned a rock type. In a coastal clastic environment, these might be beach sand, high water energy marine upper shoreface sand, intermediate water energy marine lower shoreface sand, and deeper low energy marine silt and shale. The distribution of these rock types within the model is controlled by several methods, including map boundary polygons, rock type probability maps, or statistically emplaced based on sufficiently closely spaced well data.\n\nReservoir quality parameters almost always include porosity and permeability, but may include measures of clay content, cementation factors, and other factors that affect the storage and deliverability of fluids contained in the pores of those rocks. Geostatistical techniques are most often used to populate the cells with porosity and permeability values that are appropriate for the rock type of each cell.\n\nMost rock is completely saturated with groundwater. Sometimes, under the right conditions, some of the pore space in the rock is occupied by other liquids or gases. In the energy industry, oil and natural gas are the fluids most commonly being modelled. The preferred methods for calculating hydrocarbon saturations in a geologic model incorporate an estimate of pore throat size, the densities of the fluids, and the height of the cell above the water contact, since these factors exert the strongest influence on capillary action, which ultimately controls fluid saturations.\n\nAn important part of geologic modelling is related to geostatistics. In order to represent the observed data, often\nnot on regular grids, we have to use certain interpolation techniques. The most widely used technique is kriging\nwhich uses the spatial correlation among data and intends to construct the interpolation via semi-variograms. To reproduce more realistic spatial variability and help assess spatial uncertainty between data, geostatistical simulation based on variograms, training images, or parametric geological objects is often used.\n\nGeologists involved in mining and mineral exploration use geologic modelling to determine the geometry and placement of mineral deposits in the subsurface of the earth. Geologic models help define the volume and concentration of minerals, to which economic constraints are applied to determine the economic value of the mineralization. Mineral deposits that are deemed to be economic may be developed into a mine.\n\nGeomodelling and CAD share a lot of common technologies. Software is usually implemented using object-oriented programming technologies in C++, Java or C# on one or multiple computer platforms. The graphical user interface generally consists of one or several 3D and 2D graphics windows to visualize spatial data, interpretations and modelling output. Such visualization is generally achieved by exploiting graphics hardware. User interaction is mostly performed through mouse and keyboard, although 3D pointing devices and immersive environments may be used in some specific cases. GIS (Geographic Information System) is also a widely used tool to manipulate geological data.\n\nGeometric objects are represented with parametric curves and surfaces or discrete models such as polygonal meshes.\n\nProblems pertainting to Geomodelling cover:\n\nIn the 70's, geomodelling mainly consisted of automatic 2D cartographic techniques such as contouring, implemented as FORTRAN routines communicating directly with plotting hardware. The advent of workstations with 3D graphics capabilities during the 80's gave birth to a new generation of geomodelling software with graphical user interface which became mature during the 90's.\n\nSince its inception, geomodelling has been mainly motivated and supported by oil and gas industry.\n\nSoftware developers have built several packages for geologic modelling purposes. Such software can display, edit, digitise and automatically calculate the parameters required by engineers, geologists and surveyors. Current software is mainly developed and commercialized by oil and gas or mining industry software vendors:\n\n\nMoreover, industry Consortia or companies are specifically working at improving standardization and interoperability of earth science databases and geomodelling software: \n\n\n\n\n"}
{"id": "53467573", "url": "https://en.wikipedia.org/wiki?curid=53467573", "title": "Georg Zoidl", "text": "Georg Zoidl\n\nGeorg Zoidl is a German-Canadian Canada Research Chair for Molecular and Cellular Neuroscience at York University.\n"}
{"id": "5176764", "url": "https://en.wikipedia.org/wiki?curid=5176764", "title": "Hardness comparison", "text": "Hardness comparison\n\nThere are a large number of hardness testing methods available (e.g. Vickers, Brinell, Rockwell, Meyer and Leeb). Although it is impossible in many cases to give an exact conversion, it is possible to give an approximate material-specific comparison table e.g. for steels.\n\n"}
{"id": "22378999", "url": "https://en.wikipedia.org/wiki?curid=22378999", "title": "Haush", "text": "Haush\n\nThe Haush or Manek'enk were an indigenous people, considered the oldest inhabitants of Tierra del Fuego, who spoke the Haush language. Their autonym, or name for themselves was Manek'enk. \nAt the time of European encounter and settlement, they inhabited the far eastern tip of the island on Mitre Peninsula. Land to their west, still in the northeast of Tierra del Fuego, was occupied by the Ona or Selk'nam, a related linguistic and cultural group, but distinct. \n\nThe Haush were nomadic hunters, hunting the guanaco. They used every part of it, making clothing out of the skin. They shared many customs with their neighbors the Selk'nam, including the use of small bows and stone-tipped arrows, making clothing from the skins of animals, and an initiation ritual for male youth called \"hain.\" Their languages, part of the Chonan family, were similar.\n\nSalesian missionaries ministered to the Manek'enk, and worked to preserve their culture and language. Father José María Beauvoir prepared a vocabulary. Lucas Bridges, an Anglo-Argentine born in the region, whose father had been an Anglican missionary in Tierra del Fuego, compiled a dictionary of the Haush language.\n\n\n"}
{"id": "16059132", "url": "https://en.wikipedia.org/wiki?curid=16059132", "title": "Hilbert C*-module", "text": "Hilbert C*-module\n\nHilbert C*-modules are mathematical objects which generalise the notion of a Hilbert space (which itself is a generalisation of Euclidean space), in that they endow a linear space with an \"inner product\" which takes values in a C*-algebra. Hilbert C*-modules were first introduced in the work of Irving Kaplansky in 1953, which developed the theory for commutative, unital algebras (though Kaplansky observed that the assumption of a unit element was not \"vital\"). In the 1970s the theory was extended to non-commutative C*-algebras independently by William Lindall Paschke and Marc Rieffel, the latter in a paper which used Hilbert C*-modules to construct a theory of induced representations of C*-algebras. Hilbert C*-modules are crucial to Kasparov's formulation of KK-theory, and provide the right framework to extend the notion of Morita equivalence to C*-algebras. They can be viewed as the generalization of vector bundles to noncommutative C*-algebras and as such play an important role in noncommutative geometry, notably in C*-algebraic quantum group theory, and groupoid C*-algebras.\n\nLet \"A\" be a C*-algebra (not assumed to be commutative or unital), its involution denoted by *. An inner-product \"A\"-module (or pre-Hilbert \"A\"-module) is a complex linear space \"E\" which is equipped with a compatible right \"A\"-module structure, together with a map\nwhich satisfies the following properties:\n\n\n\n\n\nAn analogue to the Cauchy–Schwarz inequality holds for an inner-product \"A\"-module \"E\":\n\nfor \"x\", \"y\" in \"E\".\n\nOn the pre-Hilbert module \"E\", define a norm by\n\nThe norm-completion of \"E\", still denoted by \"E\", is said to be a Hilbert \"A\"-module or a Hilbert C*-module over the C*-algebra \"A\".\nThe Cauchy–Schwarz inequality implies the inner product is jointly continuous in norm and can therefore be extended to the completion.\n\nThe action of \"A\" on \"E\" is continuous: for all \"x\" in \"E\"\n\nSimilarly, if {\"e\"} is an approximate unit for \"A\" (a net of self-adjoint elements of \"A\" for which \"ae\" and \"e\"\"a\" tend to \"a\" for each \"a\" in \"A\"), then for \"x\" in \"E\"\n\nwhence it follows that \"EA\" is dense in \"E\", and \"x\"1 = \"x\" when \"A\" is unital.\nLet\n\nthen the closure of <\"E\",\"E\"> is a two-sided ideal in \"A\". Two-sided ideals are C*-subalgebras and therefore possess approximate units. One can verify that \"E\"<\"E\",\"E\"> is dense in \"E\". In the case when <\"E\",\"E\"> is dense in \"A\", \"E\" is said to be full. This does not generally hold.\n\nA complex Hilbert space \"H\" is a Hilbert C-module under its inner product, the complex numbers being a C*-algebra with an involution given by complex conjugation.\n\nIf \"X\" is a locally compact Hausdorff space and \"E\" a vector bundle over \"X\" with a Riemannian metric \"g\", then the space of continuous sections of \"E\" is a Hilbert \"C(X)\"-module. The inner product is given by\n\nThe converse holds as well: Every countably generated Hilbert C*-module over a commutative C*-algebra \"A = C(X)\" is isomorphic to the space of sections vanishing at infinity of a continuous field of Hilbert spaces over \"X\".\n\nAny C*-algebra \"A\" is a Hilbert \"A\"-module under the inner product <\"a\",\"b\"> = \"a\"*\"b\". By the C*-identity, the Hilbert module norm coincides with C*-norm on \"A\".\n\nThe (algebraic) direct sum of \"n\" copies of \"A\"\n\ncan be made into a Hilbert \"A\"-module by defining\n\nOne may also consider the following subspace of elements in the countable direct product of \"A\"\n\nEndowed with the obvious inner product (analogous to that of \"A\"), the resulting Hilbert \"A\"-module is called the standard Hilbert module.\n\n\n"}
{"id": "3023266", "url": "https://en.wikipedia.org/wiki?curid=3023266", "title": "Hydathode", "text": "Hydathode\n\nA hydathode is a type of pore, commonly found in angiosperms, that secretes water through pores in the epidermis or leaf margin, typically at the tip of a marginal tooth or serration. Hydathodes occur in the leaves of submerged aquatic plants such as \"Ranunculus fluitans\" as well as herbaceous plants of drier habitats such as \"Campanula rotundifolia\". They are connected to the plant vascular system by a vascular bundle. Hydathodes are commonly seen in water lettuce, water hyacinth, rose, balsam, and many other species.\n\nHydathodes are made of a group of living cells with numerous intercellular spaces filled with water, but few or no chloroplasts, and represent modified bundle-ends. These cells (called \"epithem cells\") open out into one or more sub-epidermal chambers. These, in turn, communicate with the exterior through an open water stoma or open pore. The water stoma structurally resembles an ordinary stoma, but is usually larger and has lost the power of movement. \n\nHydathodes are involved in the process of guttation, in which positive xylem pressure (due to root pressure) causes liquid to exude from the pores. Some halophytes possess glandular trichomes that actively secrete salt in order to reduce the concentration of cytotoxic inorganic ions in their cytoplasm; this may lead to the formation of a white powdery substance on the surface of the leaf.\nHydathodes are of two types:\n\n\n"}
{"id": "2356036", "url": "https://en.wikipedia.org/wiki?curid=2356036", "title": "Inductive reasoning aptitude", "text": "Inductive reasoning aptitude\n\nInductive reasoning aptitude (also called differentiation or inductive learning ability) measures how well a person can identify a pattern within a large amount of data. It involves applying the rules of logic when inferring general principles from a constellation of particulars. \n\nMeasurement is generally done in a timed test by showing four pictures or words and asking the test taker to identify which of the pictures or words does not belong in the set. The test taker is shown a large number of sets of various degrees of difficulty. The measurement is made by timing how many of these a person can properly identify in a set period of time. The test resembles the game 'Which of These Is Not Like the Others'.\n\nInductive reasoning is very useful for scientists, auto mechanics, system integrators, lawyers, network engineers, medical doctors, system administrators and members of all fields where substantial diagnostic or data interpretation work is needed. Inductive reasoning aptitude is also useful for learning a graphical user interface quickly, because highly inductive people are very good at seeing others' categorization schemes. Inductive reasoning aptitude is often counter-productive in fields like sales where tolerance is very important, because highly inductive people tend to be good at seeing faults in others. \n\nHere is an example question:\n\nFind the set of letters that doesn’t belong with the other sets.\n\nThe correct answer is...\n\n→(\"A\", as it is the only set with four letters in sequential order, although set \"D\" arguably differs by both lacking a vowel and being separated from the previous set by more than one intervening letter.)\n\n\n"}
{"id": "51581", "url": "https://en.wikipedia.org/wiki?curid=51581", "title": "Iridology", "text": "Iridology\n\nIridology (also known as iridodiagnosis or iridiagnosis) is an alternative medicine technique whose proponents claim that patterns, colors, and other characteristics of the iris can be examined to determine information about a patient's systemic health. Practitioners match their observations to \"iris charts,\" which divide the iris into zones that correspond to specific parts of the human body. Iridologists see the eyes as \"windows\" into the body's state of health.\n\nIridologists claim they can use the charts to distinguish between healthy systems and organs in the body and those that are overactive, inflamed, or distressed. Iridologists claim this information demonstrates a patient's susceptibility towards certain illnesses, reflects past medical problems, or predicts later health problems.\n\nAs opposed to evidence-based medicine, iridology is not supported by quality research studies and is widely considered pseudoscience. The features of the iris are one of the most stable features on the human body throughout life. The stability of iris structures is the foundation of the biometric technology which uses iris recognition for identification purposes.\n\nIn 1979, Bernard Jensen, a leading American iridologist, and two other iridology proponents failed to establish the basis of their practice when they examined photographs of the eyes of 143 patients in an attempt to determine which ones had kidney impairments. Of the patients, forty-eight had been diagnosed with kidney disease, and the rest had normal kidney function. Based on their analysis of the patients' irises, the three iridologists could not detect which patients had kidney disease and which did not.\n\nIridologists generally use equipment such as a flashlight and magnifying glass, cameras or slit-lamp microscopes to examine a patient's irises for tissue changes, as well as features such as specific pigment patterns and irregular stromal architecture. The markings and patterns are compared to an \"iris chart\" that correlates zones of the iris with parts of the body. Typical charts divide the iris into approximately 80–90 zones. For example, the zone corresponding to the kidney is in the lower part of the iris, just before 6 o'clock. There are minor variations between charts' associations between body parts and areas of the iris.\n\nAccording to iridologists, details in the iris reflect changes in the tissues of the corresponding body organs. One prominent practitioner, Bernard Jensen, described it thus: \"Nerve fibers in the iris respond to changes in body tissues by manifesting a reflex physiology that corresponds to specific tissue changes and locations.\" This would mean that a bodily condition translates to a noticeable change in the appearance of the iris, but this has been disproven through many studies. (See section on Scientific research.) For example, \"acute inflammatory\", \"chronic inflammatory\" and \"catarrhal\" signs may indicate involvement, maintenance, or healing of corresponding distant tissues, respectively. Other features that iridologists look for are \"contraction rings\" and \"Klumpenzellen\", which may indicate various other health conditions, as interpreted in context.\n\nThe first \"explicit\" description of iridological principles such as homolaterality (without using the word \"iridology\") are found in \"Chiromatica Medica\", a famous work published in 1665 and reprinted in 1670 and 1691 by Philippus Meyeus (Philip Meyen von Coburg).\n\nThe first use of the word \"Augendiagnostik\" (\"eye diagnosis\", loosely translated as \"iridology\") began with Ignaz von Peczely, a 19th-century Hungarian physician who is recognised as its founding father. The most common story is that he got the idea for this diagnostic tool after seeing similar streaks in the eyes of a man he was treating for a broken leg and the eyes of an owl whose leg von Peczely had broken many years before. At the First International Iridological Congress, Ignaz von Peczely's nephew, August von Peczely, dismissed this myth as apocryphal, and maintained that such claims were irreproducible.\n\nThe second 'father' to iridology is thought to be Nils Liljequist from Sweden, who greatly suffered from the outgrowth of his lymph nodes. After a round of medication made from iodine and quinine, he observed many differences in the colour of his iris. This observation inspired him to create and publish an atlas in 1893, which contained 258 black and white illustrations and 12 colour illustrations of the iris, known as the Diagnosis of the Eye.\n\nThe German contribution in the field of natural healing is due to a minister Pastor Emanuel Felke, who developed a form of homeopathy for treating specific illnesses and described new \"iris signs\" in the early 1900s. However, Pastor Felke was subject to long and bitter litigation. The Felke Institute in Gerlingen, Germany, was established as a leading center of iridological research and training.\n\nIridology became better known in the United States in the 1950s, when Bernard Jensen, an American chiropractor, began giving classes in his own method. This is in direct relationship with P. Johannes Thiel, Eduard Lahn (who became an American under the name of Edward Lane) and J Haskell Kritzer. Jensen emphasized the importance of the body's exposure to toxins, and the use of natural foods as detoxifiers.\n\nThe majority of medical doctors reject all the claims of all branches of iridology and label them as pseudoscience or even quackery.\n\nCritics, including most practitioners of medicine, dismiss iridology given that published studies have indicated a lack of success for its claims. To date, clinical data does not support correlation between illness in the body and coinciding observable changes in the iris. In controlled experiments, practitioners of iridology have performed statistically no better than chance in determining the presence of a disease or condition solely through observation of the iris.\n\nIt has been pointed out that the premise of iridology is at odds with the fact that the iris does not undergo substantial changes in an individual's life. Iris texture is a phenotypical feature that develops during gestation and remains unchanged after birth. There is no evidence for changes in the iris pattern other than variations in pigmentation in the first year of life and variations caused by glaucoma treatment. The stability of iris structures is the foundation of the biometric technology which uses iris recognition for identification purposes.\n\nWell-controlled scientific evaluation of iridology has shown entirely negative results, with all rigorous double blind tests failing to find any statistical significance to its claims.\n\nIn 2015 the Australian Government's Department of Health published the results of a review of alternative therapies that sought to determine if any were suitable for being covered by health insurance. Iridology was one of 17 therapies evaluated for which no clear evidence of effectiveness was found.\n\nA German study from 1957 which took more than 4,000 iris photographs of more than 1,000 people concluded that iridology was not useful as a diagnostic tool.\n\nIn 1979, Bernard Jensen, a leading American iridologist, and two other iridology proponents failed to establish the basis of their practice when they examined photographs of the eyes of 143 patients in an attempt to determine which ones had kidney impairments. Of the patients, forty-eight had been diagnosed with kidney disease, and the rest had normal kidney function. Based on their analysis of the patients' irises, the three iridologists could not detect which patients had kidney disease and which did not. One iridologist, for example, decided that 88% of the normal patients had kidney disease, while another judged through his iris analysis that 74% of patients who needed artificial kidney treatment were normal.\n\nAnother study was published in the \"British Medical Journal\" which selected 39 patients who were due to have their gall bladder removed the following day, because of suspected gallstones. The study also selected a group of people who did not have diseased gall bladders to act as a control. A group of 5 iridologists examined a series of slides of both groups' irises. The iridologists could not correctly identify which patients had gall bladder problems and which had healthy gall bladders. For example, one of the iridologists diagnosed 49% of the patients with gall stones as having them and 51% as not having them. The author concluded:, \"...this study showed that iridology is not a useful diagnostic aid.\"\n\nEdzard Ernst raised the question in 2000: \"Does iridology work? [...] This search strategy resulted in 77 publications on the subject of iridology. [...] All of the uncontrolled studies and several of the unmasked experiments suggested that iridology was a valid diagnostic tool. The discussion that follows refers to the 4 controlled, masked evaluations of the diagnostic validity of iridology. [...] In conclusion, few controlled studies with masked evaluation of diagnostic validity have been published. None have found any benefit from iridology.\"\n\nA 2005 study tested the usefulness of iridology in diagnosing common forms of cancer. An experienced iridology practitioner examined the eyes of 110 total subjects, of which 68 people had proven cancers of the breast, ovary, uterus, prostate, or colorectum, and 42 for whom there was no medical evidence of cancer. The practitioner, who was unaware of their gender or medical details, was asked to suggest a diagnosis for each person and his results were then compared with each subject's known medical diagnosis. The study conclusion was that \"Iridology was of no value in diagnosing the cancers investigated in this study.\"\n\nIn Canada and the United States, iridology is not regulated or licensed by any governmental agency. Numerous organizations offer certification courses.\n\nMedical errors—treatment for conditions diagnosed via this method which do not actually exist (false positive result) or a false sense of security when a serious condition is not diagnosed by this method (false negative result)—could lead to improper or delayed treatment and even loss of life.\n\n\n"}
{"id": "37558994", "url": "https://en.wikipedia.org/wiki?curid=37558994", "title": "Jean François Aimé Théophile Philippe Gaudin", "text": "Jean François Aimé Théophile Philippe Gaudin\n\nJean François Aimé Théophile Philippe Gaudin or Jean François Aimée Gottlieb Philippe Gaudin (1766 in Longirod, canton de Vaud – 1833) was a Swiss pastor, professor and botanist. He was the author of the monumental \"Flora Helvetica\" in 7 volumes.\n\nHe gave his name to the annual grass genus \"Gaudinia\" belonging to the Poeae.\n\n"}
{"id": "49922564", "url": "https://en.wikipedia.org/wiki?curid=49922564", "title": "Kino Border Initiative", "text": "Kino Border Initiative\n\nKino Border Initiative (KBI) since 2008 has united six Catholic organizations in an effort to be a humanizing presence and to foster bi-national solidarity on the issue of migration on the U.S.-Mexico border through direct assistance and accompaniment, education, research, and advocacy.\n\nIn 2008 and 2009 in Nogales, Arizona, and Nogales, Sonora, Mexico, this work was begun by six organizations: the California Province of the Society of Jesus, Jesuit Refugee Service/USA, the Missionary Sisters of the Eucharist, the Mexican Province of the Society of Jesus, the Diocese of Tucson, and the Archdiocese of Hermosillo. Its purpose is to break down barriers to humane, just, and workable migration, affirming human dignity and solidarity. KBI gives direct assistance and accompanies migrants, educates communities on both sides of the border, and collaborates with networks of research and advocacy to transform policy at all levels of government. It sees its work in continuity with the historic record of the USA in welcoming immigrants, and in support of international standards for the treatment of refugees.\n"}
{"id": "1134909", "url": "https://en.wikipedia.org/wiki?curid=1134909", "title": "List of -ectomies", "text": "List of -ectomies\n\nThe surgical terminology suffix \"-ectomy\" was taken from Greek εκ-τομια = \"act of cutting out\". It means surgical removal of something, usually from inside the body.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes this suffix is used humorously in non-surgical contexts, for example \"popculturectomy\" for a type of editing process on a text.\n"}
{"id": "49855806", "url": "https://en.wikipedia.org/wiki?curid=49855806", "title": "List of computer science journals", "text": "List of computer science journals\n\nBelow is a list of computer science journals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "24435196", "url": "https://en.wikipedia.org/wiki?curid=24435196", "title": "List of craters on Mars: O–Z", "text": "List of craters on Mars: O–Z\n\nThis is a list of craters on Mars. There are hundreds of thousands of impact craters on Mars, but only some of them have names. This list here contains only named Martian craters starting with the letter O – Z \"(see also lists for and )\".\n\nLarge Martian craters (greater than 60 kilometers in diameter) are named after famous scientists and science fiction authors; smaller ones (less than 60 km in diameter) get their names from towns on Earth. Craters cannot be named for living people, and small crater names are not intended to be commemorative – that is, a small crater isn't actually named after a specific town on Earth, but rather its name comes at random from a pool of terrestrial place names, with some exceptions made for craters near landing sites. Latitude and longitude are given as planetographic coordinates with west longitude.\n\n\n"}
{"id": "51137228", "url": "https://en.wikipedia.org/wiki?curid=51137228", "title": "List of geographical brows", "text": "List of geographical brows\n\nBrow is used in the name of several geographical features:\n\n\n"}
{"id": "264569", "url": "https://en.wikipedia.org/wiki?curid=264569", "title": "List of home computers", "text": "List of home computers\n\nThe home computers between 1977 and about 1990 were different from today's uniform and predictable machines. During this time it made economic sense for manufacturers to make microcomputers aimed at the home user. By simplifying the machines, and making use of household items such as television sets and cassette recorders instead of dedicated computer peripherals, the home computer allowed the consumer to own a computer at a fraction of the price of computers oriented to small business. Today, the price of microcomputers has dropped to the point where there's no advantage to building a separate, incompatible series just for home users.\n\nWhile many office-type personal computers were used in homes, in this list a \"home computer\" is a factory-assembled mass-marketed consumer product, usually at significantly lower cost than contemporary business computers. It would have an alphabetic keyboard and a multi-line alphanumeric display, the ability to run both games software as well as application software and user-written programs, and some removable mass storage device (such as cassette tape or floppy disk).\n\nThis list excludes smartphones, personal digital assistants, pocket computers, laptop computers, programmable calculators and pure video game consoles. Single-board development or evaluation boards, intended to demonstrate a microprocessor, are excluded since these were not marketed to general consumers.\n\nPioneering kit and assembled hobby microcomputers which generally required electronics skills to build or operate are listed separately, as are computers intended primarily for use in schools. A hobby-type computer often would have required significant expansion of memory and peripherals to make it useful for the usual role of a factory-made home computer. School computers usually had facilities to share expensive peripherals such as disk drives and printers, and often had provision for central administration.\n\nAttributes are as typically advertised by the original manufacturer. Popular machines inspired third-party sources for adapters, add-on processors, mass storage, and other peripherals.\n\n\"Processor\" indicates the microprocessor chip that ran the system. A few home computers had multiple processors, generally used for input/output devices. Processor speeds were not a competitive point among home computer manufacturers, and typically the processor ran either at its maximum rated speed ( between 1 and 4 MHz for most processor types here), or at some fraction of the television color subcarrier signal, for economy of design. Since a crystal oscillator was necessary for stable color, it was often also used as the microprocessor clock source. Many processors were second-sourced, with different manufacturers making the same device under different part numbers. Variations of a basic part number might have been used to indicate minor variations in speed or transistor type, or might indicate fairly significant alterations to the prototype's capabilities. In the Soviet Bloc countries, manufacturers made functional duplicates of Western microprocessors under different part number series.\n\n\"TV\" indicates the factory configuration produces composite video compatible with a home TV receiver. Some computers came with a built-in RF modulator to allow connection to the TV receiver antenna terminals; others output composite video for use with a free-standing monitor or external RF modulator. Still others had built-in or proprietary monitors. Often a composite video monitor (monochrome or color) would be substituted for the family TV. Some standard types of video controller ICs were popular, but see the very detailed List of home computers by video hardware for a discussion of video capabilities of different models. Memory and TV bandwidth restrictions meant that typical home computers had only a few color choices and perhaps 20 lines of 40 characters of text as an upper limit to their video capabilities. Where the same model was sold in countries using PAL or NTSC television standards, sometimes there would be minor variations in the speed of the processor, because NTSC and PAL use different frequencies for the color information and the crystal for the video system was often also used for the processor clock. \n\nBase mass storage was whatever came in the basic configuration. Some machines had built-in cassette drives or optional external drives, others relied on the consumer to provide a cassette recorder. Cassette recorders had the primary virtue of being widely available as a consumer product at the time. Typically a home computer would generate audio tones to encode data, that could be stored on audio tape through a direct connection to the recorder. Re-loading the data required re-winding the tape. The home computer would contain some circuit such as a phase-locked loop to convert audio tones back into digital data. Since consumer cassette recorders were not made for remote control, the user would have to manually operate the recorder in response to prompts from the computer. Random access to data on a cassette was impossible, since the entire tape would have to be searched to retrieve any particular item. A few manufacturers integrated a cassette tape drive or cassette-like tape mechanism into the console, but these variants were made obsolete by the reduction in cost of floppy diskette drives.\n\nFloppy disk drives were initially very costly compared to the system purchase price. Plug-in ROM cartridges containing game or application software were popular in earlier home computers since they were easier to use, faster, and more reliable than cassette tapes. Once diskette drives became available at low cost, cartridges declined in popularity since they were more expensive to manufacture than reproducing a diskette, and had comparatively small capacity compared to diskettes. A few cartridges contained battery-backed memory that allowed users to save data (for example, game high scores) between uses of the cartridge.\n\nTypically there were several models or variants within a product line, especially to account for different international video standards and power supplies; see the linked articles for variants and consequences of variations. \"Compatibility\" indicates some measure of compatibility with a parent type, however, sometimes incompatibility existed even within a product family. A \"clone\" system has identical hardware and is functionally interchangeable with its prototype; a few clone systems relied on illicit copies of system ROMs to make them functional.\n\nThis type of microcomputer required significant electronics skills to assemble or operate. They were sometimes sold in kit form that required the user to insert and solder components in a printed circuit board. They may have had just blinking lights and toggle switches, or a hexadecimal display and a numeric keypad. While some units were possibly expandable to the \"checkbook balancing/homework typing\" stage, most were intended more for education on the use and application of microprocessors. See also Microprocessor development board, Single-board computer.\n\n\nThese were aimed at the class room, not the living room. Some types were popular in the centrally planned economies of eastern Europe where Western computers were scarce, or in the early days of computer education in Western schools. Popular home computers of the period were fitted with various types of network interfaces to allow sharing of files, large disk drives, and printers, and often allowed a teacher to interact with a student, supervise the system usage, and carry out administrative tasks from a host computer.\n\n\nLogic demonstrators illustrated some of the logical principles of computer circuits, but were incapable of automatic operation or non-trivial calculations. Some were literally cardboard, others used combinations of switches and lamps to show how logical operations worked. Some products demonstrated logical operations purely mechanically. \n\n\n\n"}
{"id": "56067657", "url": "https://en.wikipedia.org/wiki?curid=56067657", "title": "NIROSETI", "text": "NIROSETI\n\nThe NIROSETI (Near-InfraRed Optical Search for Extraterrestrial Intelligence) is an astronomical program to search for artificial signals in the optical (visible) and near infrared (NIR) wavebands of the electromagnetic spectrum. It is the first dedicated near-infrared SETI experiment. The instrument was created by a collaboration of scientists from the University of California, San Diego, Berkeley SETI Research Center at the University of California, Berkeley, University of Toronto, and the SETI Institute. It uses the Anna Nickel 1-m telescope at the Lick Observatory, situated on the summit of Mount Hamilton, east of San Jose, California, USA. The instrument was commissioned (saw its first light) on 15 March 2015 and has been operated for more than 150 nights.\n\nThe NIROSETI project is based on the assumption that hypothetical communicative extraterrestrials may send out pulsed laser signals in the optical, as well as infrared spectrum. Near-infrared offers a possible way for signal transmission since there is a decrease in both interstellar extinction and Galactic background compared to optical wavelengths. The near-infrared bands remain largely unexplored because instruments capable of capturing short pulses of infrared light have only recently become available.\n\nThe NIROSETI instrument makes use of the 1-meter optical Nickel telescope located at the Lick Observatory in California to search for near-infrared (laser) transmissions from extraterrestrial communication or technosignatures. This project was funded by the Bill and Susan Bloomfield Foundation and is based upon a predecessor called Lick Optical SETI instrument, conducted between 2001 and 2006. Professor Shelley Wright leads the team that built and operates the NIROSETI program.\n\nThe NIROSETI instrument employs a new generation of near-infrared (900 to 1700 nm) detectors, cooled at -25 °C, that have a high speed response (>1 GHz) and gain comparable to photomultiplier tubes, while also producing very low noise, and significantly reducing false positives. Its field-of-view is 2.5\"x2.5\" each, and focuses\non detecting short (nanosecond) pulsed laser emissions. The NIROSETI instrument is also being used to study variability of very short natural near-infrared transient stars.\n\nThe NIROSETI survey has been designed for observing several thousand objects over a few years, and commenced full operations on 28 January 2016. During a clear night of observations, about 20 to 30 objects are observed. Because infrared light penetrates farther through gas and dust than visible light, this search will extend to stars thousands of light-years away. The initial target sample is mostly main-sequence and giant stars located within 50 parsecs from Earth, drawn from the Breakthrough Listen program target list.\n\nThe sample of targets also includes 82 galaxies for being the nearest representatives of the five major morphological classes of galaxies (20 spirals, 36 ellipticals, 15 dwarf spheroidals, 9 irregulars, and 2 lenticular galaxies), as well as stars that triggered alarms on other targeted SETI surveys.\n\nA significant drawback is that the extraterrestrial laser signals would need to be transmitted in the direction of the Solar System in order to be detected.\n\n\n"}
{"id": "15450044", "url": "https://en.wikipedia.org/wiki?curid=15450044", "title": "Normalization process theory", "text": "Normalization process theory\n\nNormalization process theory (NPT) is a derivative sociological theory of the implementation, embedding, and integration of new technologies and organizational innovations developed originally from a collective set of learning workshops and included a large number of people including Carl R. May, Tracy Finch, Elizabeth Murray, Anne Rogers, Catherine Pope, Anne Kennedy, Pauline Ong and . The theory is a contribution to the field of science and technology studies (STS), and is the result of a programme of theory building by May and a range of academics from applied social science to medicine. Through three iterations, the theory has built upon the normalization process model previously developed by May et al. to explain the social processes that lead to the routine embedding of innovative health technologies.\n\nNormalization process theory focuses attention on agentic contributions – the things that individuals and groups do to operationalize new or modified modes of practice as they interact with dynamic elements of their environments. It defines the implementation, embedding, and integration as a process that occurs when participants deliberately initiate and seek to sustain a sequence of events that bring it into operation. The dynamics of implementation processes are complex, but normalization process theory facilitates understanding by focusing attention on the mechanisms through which participants invest and contribute to them. It reveals \"the work that actors do as they engage with some ensemble of activities (that may include new or changed ways of thinking, acting, and organizing) and by which means it becomes routinely embedded in the matrices of already existing, socially patterned, knowledge and practices\". These have explored objects, agents, and contexts. In a paper published under a creative commons license, May and colleagues describe how, since 2006, NPT has undergone three iterations.\n\n\nNormalization process theory is regarded as a middle range theory that is located within the 'turn to materiality' in STS. It therefore fits well with the case-study oriented approach to empirical investigation used in STS. It also appears to be a straightforward alternative to actor–network theory in that it does not insist on the agency of non-human actors, and seeks to be explanatory rather than descriptive. However, because normalization process theory specifies a set of generative mechanisms that empirical investigation has shown to be relevant to implementation and integration of new technologies, it can also be used in larger scale structured and comparative studies. Although it fits well with the interpretive approach of ethnography and other qualitative research methods, it also lends itself to systematic review and survey research methods. As a middle range theory, it can be federated with other theories to explain empirical phenomena. It is compatible with theories of the transmission and organization of innovations, especially diffusion of innovations theory, labor process theory, and psychological theories including the theory of planned behavior and social learning theory.\n"}
{"id": "7374187", "url": "https://en.wikipedia.org/wiki?curid=7374187", "title": "Obstacle avoidance", "text": "Obstacle avoidance\n\nIn robotics, obstacle avoidance is the task of satisfying some control objective subject to non-intersection or non-collision position constraints. In unmanned air vehicles, it is a hot topic . What is critical about obstacle avoidance concept in this area is the growing need of usage of unmanned aerial vehicles in urban areas for especially military applications where it can be very useful in city wars. Normally obstacle avoidance is considered to be distinct from path planning in that one is usually implemented as a reactive control law while the other involves the pre-computation of an obstacle-free path which a controller will then guide a robot along.\n\n\n\n"}
{"id": "40593753", "url": "https://en.wikipedia.org/wiki?curid=40593753", "title": "Of Time and Space and Other Things", "text": "Of Time and Space and Other Things\n\nOf Time and Space and Other Things is a collection of seventeen scientific essays by Isaac Asimov. It was the fourth of a series of books collecting essays from \"The Magazine of Fantasy and Science Fiction\". It was first published by Doubleday & Company in 1965.\n\n\n"}
{"id": "169633", "url": "https://en.wikipedia.org/wiki?curid=169633", "title": "Outline of computer science", "text": "Outline of computer science\n\nComputer science (also called computing science) is the study of the theoretical foundations of information and computation and their implementation and application in computer systems. One well known subject classification system for computer science is the ACM Computing Classification System devised by the Association for Computing Machinery.\n\nComputer science can be described as all of the following:\n\n\n\n\nOutline of artificial intelligence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "54179148", "url": "https://en.wikipedia.org/wiki?curid=54179148", "title": "Petr Topychkanov", "text": "Petr Topychkanov\n\nPetr Topychkanov (born October 16, 1981) is a Russian researcher of South Asia, nuclear nonproliferation, arms control, and military industrial cooperation.\n\nPetr Topychkanov received a Bachelor of Arts in South Asian Studies from the Institute of Practical Oriental Studies in 2003. He then attended the Institute of Asian and African Countries, Moscow State University, receiving a Master of Arts in South Asian Studies in 2006 and a Ph.D in 2009. His dissertation was about \"Constructing the Status of Religious Minorities of South Asia in the Second Half of 20th and Beginning of 21st Centuries (Cases of India and Pakistan).\"\n\nPetr Topychkanov served in the 4th Guards Kantemirovskaya Tank Division, Russian Armed Forces from 2003 to 2004. He joined the Carnegie Moscow Center's Nonproliferation Program in 2006, when it was co-chaired by Rose Gottemoeller and Academician Alexei Arbatov. In 2009, Topychkanov worked as a senior researcher at the Center for International Security at the Institute of World Economy and International Relations of the Russian Academy of Sciences. In 2014, he joined the Center's Information Security Problems Group and became an expert at the Russian International Affairs Council. He is also a participant of the Program on Strategic Stability Evaluation at the Georgia Institute of Technology and an associate of the South and Central Asia Project at the York Centre for Asian Research.\n\nIn 2017 Topychkanov was included to the list of the 20 most influential think tank experts in the field of international relations and security in Twitter, according to the Elcano Royal Institute.\n\nCo-author of:\n\nPetr Topychkanov’s articles are frequently published in the press, including Newsweek, India Today, Hindustan Times, Russian Vedomosti, Forbes, RBC Daily, and others.\n"}
{"id": "31698140", "url": "https://en.wikipedia.org/wiki?curid=31698140", "title": "Photoconductive atomic force microscopy", "text": "Photoconductive atomic force microscopy\n\nPhotoconductive atomic force microscopy (PC-AFM) is a variant of atomic force microscopy that measures photoconductivity in addition to surface forces.\n\nMulti-layer photovoltaic cells have gained popularity since mid 1980s. At the time, research was primarily focused on single-layer photovoltaic (PV) devices between two electrodes, in which PV properties rely heavily on the nature of the electrodes. In addition, single layer PV devices notoriously have a poor fill factor. This property is largely attributed to resistance that is characteristic of the organic layer. The fundamentals of pc-AFM are modifications to traditional AFM and focus on the use of pc-AFM in PV characterization. In pc-AFM the major modifications include: a second illumination laser, an inverted microscope and a neutral density filter. These components assist in the precise alignment of the illumination laser and the AFM tip within the sample. Such modifications must complement the existing principals and instrumental modules of pc-AFM so as to minimize the effect of mechanical noise and other interferences on the cantilever and sample.\n\nThe original exploration of the PV effect can be accredited to research published by Henri Becquerel in 1839. Becquerel noticed the generation of a photocurrent after illumination when he submerged platinum electrodes within an aqueous solution of either silver chloride or silver bromide. In the early 20th century, Pochettino and Volmer studied the first organic compound, anthracene, in which photoconductivity was observed. Anthracene was heavily studied due to its known crystal structure and its commercial availability in high-purity single anthracene crystals. The studies of photoconductive properties of organic dyes such as methylene blue were initiated only in the early 1960s owing to the discovery of the PV effect in these dyes. In further studies, it was determined that important biological molecules such as chlorophylls, carotenes, other porphyrins as well as structurally similar phthalocyanines also exhibited the PV effect. Although many different blends have been researched, the market is dominated by inorganic solar cells which are slightly more expensive than organic based solar cells. The commonly used inorganic based solar cells include crystalline, polycrystalline, and amorphous substrates such as silicon, gallium selenide, gallium arsenide, copper indium gallium selenide and cadmium telluride.\n\nWith the high demand of cheap, clean energy sources persistently increasing, organic photovoltaic (OPV) devices (organic solar cells), have been studied extensively to help in reducing the dependence on fossil fuel and containing the emission of green house gases (especially CO, NO, and SO). This global demand for solar energy increased 54% in 2010, while the United States alone has installed more than 2.3 GW of solar energy sources in 2010. Some of the attributes which make OPVs such a promising candidate to solve this problem include their low-cost of production, throughput, ruggedness, and their chemically tunable electric properties along with significant reduction in the production of greenhouse gases. \nFor decades, the researchers have believed that the maximum power conversion efficiency (PCE) would most likely remain below 0.1%. Only in 1979 Tang reported a two-layer, thin-film PV device, which ultimately yielded a power conversion efficiency of 1%. Tang’s research was published in 1986, which allowed others to decipher many of the problems which limited the basic understanding of the process involved in the OPVs. In later years, the majority of the research focused on the composite blend of poly(3-hexylthiopehene) (P3HT) and phenyl-C61-butyric acid methyl ester (PCBM). This, along with the research performed on fullerenes, dictated the majority of studies pertaining to OPV for many years. In more recent research, polymer-based bulk heterojunction solar cells, along with low band-gap donor-acceptor copolymers have been created for PCBM-based OPV devices. These low band-gap donor-acceptor copolymers are able to absorb a higher percentage of the solar spectrum as compared to other high efficiency polymers. These copolymers have been widely researched due to their ability to be tuned for specific optical and electrical properties.\nTo date, the best OPV devices have a maximum power conversion efficiency of approximately 8.13%. This low power conversion efficiency is directly related to discrepancies in the film morphology on the nano-scale level. Explanations of film morphology include recombination and/or trapping of charges, low open circuit voltages, heterogeneous interfaces, grain boundaries, and phase-separated domains. Many of these problems arise from the deficient knowledge of electro-optical properties on the nano-scale level. In numerous studies, it has been observed that heterogeneities in the electrical and optical properties influence device performance. These heterogeneities which occur in OPVs are a result the manufacturing process, such as annealing time, which is explained below. Research has mainly consisted of discovering exactly how this film morphology affects the device performance.\n\nUntil recently, microscopy methods used in the characterization of these OPVs consisted of atomic force microscopy (AFM), transmission electron microscopy (TEM) and scanning transmission X-ray microscopy (STXM). These methods are very useful in the identification of the local morphology on the film surface, but lack the ability to provide fundamental information regarding local photocurrent generation and ultimately on the device performance. To obtain information which links the electrical and optical properties, the use of electrical scanning probe microscopy (SPM) is an active area of research. Electrostatic force microscopy (EFM) and scanning Kelvin probe microscopy (SKPM) have been utilized in the studies of electron injection and charge trapping effects, while scanning tunneling microscopy (STM) and conductive atomic force microscopy (c-AFM) have been used to investigate electron transport properties within these organic semiconductors. \nConductive AFM has been widely used in characterizing the local electric properties in both photovoltaic fullerene blends and organic films, but no reports have shown the use of c-AFM to display the distribution of photocurrents in organic thin films. The most recent variation of SPM devices include (tr-EFM) and photoconductive AFM (pc-AFM) . Both these techniques are capable of obtaining information regarding photo-induced charging rates with nano-scale resolution. The advantage of pc-AFM over tr-ERM is present in the maximum obtainable resolution by each method. pc-AFM can map photocurrent distributions with approximately 20 nm resolution, whereas tr-EFM was only able to obtain between 50–100 nm resolution at this time. Another important factor to note is although the tr-EFM is capable of characterizing thin films within organic solar cells, it is unable to provide the needed information regarding the capacitance gradient nor the surface potential of the thin film.\nThe origin of PC-AFM is due to the work performed by Gerd Binnig and Heinrich Rohrer on STM for which they were awarded the Nobel Prize in physics in 1986. They fabricated an instrument called scanning tunneling microscope (STM) and demonstrated that STM provides surface topography on the atomic scale. This microscopy technique yielded resolutions which were nearly equal to scanning electron microscopy (SEM).\n\nThe fundamental principles of photoconductive atomic force microscopy (pc-AFM) are based on those of traditional atomic force microscopy (AFM) in that an ultrafine metallic tip scans the surface of a material to quantify topological features. \nThe working premises for all types of AFM techniques are largely dependent on the fundamentals of the AFM cantilever, metallic tip, scanning piezo-tube and the feedback loop that transfers information from lasers that guide the motion of the probe across the surface of a sample. The ultra-fine dimensions of the tip and the way the tip scans the surface produces lateral resolutions of 500 nm or less. In AFM, the cantilever and tip functions as a mass on a spring. When a force acts on the spring (cantilever), the spring response is directly related to the magnitude of the force. \"k\" is defined as the force constant of the cantilever.\n\nHooke's law for cantilever motion:\n\nformula_1\n\nThe forces acting on the tip are such that the spring (cantilever) remains soft but responds to the applied force, with a detectable resonant frequency, \"f\". In Hooke's law, \"k\" is the spring constant of the cantilever and \"m\" is defined as the mass acting on the cantilever: the mass of the cantilever itself and the mass of the tip. The relationship between \"f\" and the spring constant is such that \"k\" must be very small in order to make the spring soft. Since \"k\" and \"m\" are in a ratio, the value of \"m\" must also decrease to increase the value of the ratio. Manipulating the values in this way provides the necessary high resonance frequency. A typical \"m\" value has a magnitude of 10 kg and creates an \"f\" of approximately 2 kHz.\n\nExpression for resonant frequency of a spring:\n\nformula_2\n\nSeveral forces affect the behavior of the cantilever: attractive and repulsive Van der Waals forces, and electrostatic repulsion. Changes in these forces are monitored by a guide laser that is reflected off the back of the cantilever and detected by a photodetector. Attractive forces between the atoms on the sample surface and the atom at the AFM tip draw the cantilever tip closer to the surface. When the cantilever tip and the sample surface come within a range of a few angstroms repulsive forces come into play as a result of electrostatic interactions. There is also a force exerted from the cantilever pressing down on the tip. The magnitude of the force exerted by the cantilever is dependent upon the direction of its motion, whether it is attracted or repelled from the sample surface When the tip of the cantilever and the surface come into contact, the single atom at the point of the tip and the atoms on the surface exhibit a Lennard-Jones potential. The atoms exhibit attractive forces until a certain point and then experience repulsion from one another. The term \"r\" is the separation at which the sum of the potentials between the two atoms is zero \n\nForce on AFM tip in terms of Lennard-Jones potential:\n\nformula_3\n\nModifications of this early work have been implemented to perform AFM analysis on both conducting and non-conducting materials. Conductive atomic force microscopy (c-AFM) is one such modification technique. The c-AFM technique operates by measuring fluctuations in current from the biased tip and sample while simultaneously measuring changes in the topographical features. In all techniques of AFM, two modes of operation can be used: contact mode and non-contact mode. In c-AFM resonant contact mode is used to obtain topographical from current that is measured between the biased AFM tip and the sample surface. In this type of operation, the current is measured in the small space between the tip and the sample surface. This quantification is based on the relationship between the current traveling through the sample and layer thickness. In the previous equation, A is the effective emission area at the injecting electrode, q is the electron charge, h is planck’s constant, \"m\" / \"m\" =0.5, which is the effective mass of an electron in the conduction band of a sample, \"d\" is the sample thickness and \"Φ\" is the barrier height. The symbol, \"β\", the field enhancement factor, accounts for the non-planar, geometry of the tip used.\n\nRelationship between conducting current and sample layer thickness:\n\nformula_4\n\nThe accuracies of all AFM techniques rely heavily on a sample scanning tube, the piezo-tube. The piezo-tube scanner is responsible for the direction of tip displacement during a sample analysis, and is dependent on the mode of analysis. The piezo components are either arranged orthogonally or manufactured as a cylinder. In all techniques, sample topography is measured by the movement of the x and y piezos. When performing non-contact mode pc-AFM, the piezo-tube keeps the probe from moving in the x and y direction and measures the photocurrent between the sample surface and conducting tip in the z-direction. \nThe principles of the piezo-tube is dependent upon how the piezo-electric material reacts with an applied voltage to either the interior or exterior of the tube. When voltage is applied to the two electrodes connected to the scanner, the tube will expand or contract causing motion to the AFM tip in the direction of this movement. This phenomenon is illustrated as the piezo-tube becomes displaced by an angle, θ. As the tube is displaced, the sample that, in traditional AFM is fixed to the tube generates lateral translation and rotation relative to the AFM tip, thus movement of the tip is generated in the x and y directions When voltage is applied of the inside of the tube, movement in the z-direction is implemented.\nThe relationship between the movement of the piezo-tube and the direction of the displacement of the AFM tip assumes that the tube is perfectly symmetric. When no voltage is applied to the tube the z-axis bisects the tube, sample and sample stage symmetrically. When a voltage is applied to the exterior of the tube (x and y motion), the expansion of the tube can be understood as a circular arc. In this equation, the \"r\" term indicates the outside radius of the piezo-tube, \"R\" is the curvature radius of the tube with applied voltage, \"θ\" is the bend angle of the tube, \"L\" is the initial length of the tube and \"ΔL\" is the extension of the tube after the voltage is applied. The change in length of the piezo-tube, \"ΔL\", is expressed as the intensity of the electric field applied to the exterior of the tube, the voltage along the x-axis, U, and the thickness of the wall of the tube.\n\nExpressions for bend geometry of piezo-tube:\n\nformula_5\n\nformula_6\n\nLength displacement in terms of exterior electric field:\n\nformula_7\n\nExpression for tube displacement, \"θ\":\n\nformula_8\n\nWith the calculation of \"θ\", the displacement of the probe in the x and z directions can be calculated as:\n\nExpressions for probe displacement in the x- and z-directions:\n\nformula_9\n\nformula_10\n\nAnother fundamental concept of all AFM is the feedback loop. The feedback loop is particularly important in non-contact AFM techniques, particularly in pc-AFM. As previously mentioned, in non-contact mode the cantilever is stationary and the tip does not come into physical contact with the sample surface. The cantilever behaves as a spring and oscillates at its resonance frequency. Topological variance causes the spring-like oscillations of the cantilever to change amplitude and phase in order to prevent the tip from colliding with sample topographies. The non-contact feedback loop is used to control that changes in the oscillations of the cantilever. \nThe application of AFM on non-conducting samples (c-AFM) has in recent years evolved into the modification used for analysis of morphologies on the local scale, particularly morphologies at heterojunctions of multilayered samples. Photoconductive atomic force microscopy (pc-AFM) is particularly prevalent in the development of organic photovoltaic devices (OPV). The fundamental modification of c-AFM to pc-AFM is the addition of an illumination source and an inverted microscope that focuses the laser to a nanometer-scale point directly underneath the conductive AFM tip. The main concept of the illumination laser point is that it must be small enough to fit within the confines of ultra-thin films. These characteristics are achieved by using a monochromatic light source and a laser filter. In the OPV application, applying the illumination laser to the confines of ultra-thin films is further assisted by the recent development of the bulk heterojunction (BHJ) mixture of electron donating and accepting material in the film.\nThe combination of the conductive tip and illumination laser provides photocurrent images with vertical resolutions in the range of 0 to 10 pA when overlaid with the topographical data obtained. Also unique to this modification are the spectra data gathered by comparing the current between the tip and sample to a variety of parameters including: laser wavelength, applied voltage and light intensity. The pc-AFM technique was also reported to detect local surface oxidation at a vertical resolution of 80 nm.\nThe instrumentation involved for pc-AFM is very similar to that necessary for traditional AFM or the modified conductive AFM. The main difference between pc-AFM and other types of AFM instruments is the illumination source that is focused through the inverted microscope objective and the neutral density filter that is positioned adjacent to the illumination source. The technical parameters of pc-AFM are identical to those of traditional AFM techniques. This section will focus on the instrumentation necessary for AFM and then detail the requirements for pc-AFM modification. \nThe main instrumental components to all AFM techniques are the conductive AFM cantilever and tip, the modified piezo components and the sample substrate. The components for photoconductive modification include: the illumination source (532 nm laser), filter and inverted microscope. When modifying traditional AFM for pc application, all components must be combined such that they do not interfere with one another and so that various sources of noise and mechanical interference do not disrupt the optical components.\nIn traditional instrumentation, the stage is a cylindrical piezo-tube scanner that minimizes the effect of mechanical noise. Most cylindrical piezos are between 12 and 24 mm in length and 6 and 12 mm in diameter. The exterior of the piezo-tube is coated with a thin layer of conducting metal so that this region can sustain an electric field. The interior of the cylinder is divided into four regions (x and y regions) by non-conducting metallic strips. Electrical leads are fixed to one end and the exterior wall of the cylinder so that a current can be applied. When a voltage is applied to the exterior, the cylinder expands in x and y direction. Voltage along the interior of the tube causes cylinder expansion in the z-direction and thus movement of the tip in the z-direction. The placement of the piezo tube is dependent upon the type of AFM performed and the mode of analysis. However the z-piezo must always be fixed above the tip and cantilever to control the z-motion. This configuration is most often seen in the c-AFM and pc-AFM modifications to make room for additional instrumental components which are placed below the scanning stage. This is particularly true for pc-AFM, which must have the piezo-components arranged above the cantilever and tip so that the illumination laser can transmit through the sample.\n\nwith applied voltage\n\nIn some configurations, the piezo components can be arranged in a tripod design. In this type of set-up, the x, y and z components are arranged orthogonally to one another with their apex attached to a movable pivot point. Similar to the cylindrical piezo, in the tripod design the voltage is applied to the piezo corresponding to the appropriate direction of tip displacement. In this type of set-up the sample and substrate are mounted on top of the z-piezo component. When the x and y piezo components are in use, the orthogonal design causes them to push against the base of the z-piezo, causing the z-piezo to rotate about a fixed point. Applying voltage to the z-piezo causes the tube to move up and down on its pivot point. \n\nThe other essential components of AFM instrumentation include the AFM tip module, which includes: the AFM tip, the cantilever, and the guiding laser.\nWhen the piezo-tube is positioned above the cantilever and tip, the guiding laser is focused through the tube and onto a mirror that rests on tip of the cantilever. The guiding laser is reflected off of the mirror and detected by a photodetector. The laser senses when the forces acting on the tip change. The reflected laser beam from this phenomenon reaches the detector. The output from this detector acts as a response to the changes in force and the cantilever adjusts the position of the tip, while keeping constant the force that acts on the tip.\n\nThe instrumentation of conductive AFM (c-AFM) has evolved with the desire to measure local electrical properties of materials with high resolutions. The essential components are: the piezo-tube, the guide laser, the conducting tip, and cantilever. Although these components are identical to traditional AFM their configuration is tailored to measuring surface currents on the local scale. \nAs mentioned previously, the piezo-tube can be placed either above or below the sample, depending on the application of the instrumentation. In the case of c-AFM, repulsive contact mode is the predominantly used to obtain electric current images from the surface as the sample moves in the x and y direction. Placing the z-piezo above the cantilever allows for better control of the cantilever and tip during analysis.\nThe material that comprises the conductive tip and cantilever can be customized for a particular application. Metal-coated cantilevers, gold wires, all-metal cantilevers and diamond cantilevers are used. In many cases diamond is the preferred material for cantilever and/or tip because it is an extremely hard material that does not oxidize in ambient conditions. The main difference between the instrumentation of c-AFM and STM is that in c-AFM the bias voltage can be directly applied to the nanostructure (tip and substrate). In STM, on the other hand, the applied voltage must be supported within the vacuum tunneling gap between the STM probe and surface. When the tip is in close contact with the sample surface the application of bias voltage to the tip creates a vacuum gap between the tip and the sample that enables the investigation of electron transport through nanostructures.\n\nThe main components and instrumentation of c-AFM instrumentation are identical to that required for a pc-AFM module. The only modifications are the illumination source, filter and inverted microscope objective that are located beneath the sample substrate. In fact, most pc-AFM instruments are simply modified from existing cp-AFM instrumentation. The first report of this instrumental modification came in 2008. In that paper, Lee and coworkers implemented the aforementioned modifications to examine the resolution of photocurrent imaging. Their design consisted of three main units: a conductive mirror plate, steering mirror and laser source.\nThe main difficulty with the previously existing c-AFM instrumentation is the inability of the technique for characterizing photonic devices. Specifically, it is difficult to measure changes in local and nano-scale electrical properties that result from the photonic effect. The optical illumination component (laser) was added to the c-AFM module in order to make such properties visible. Early in development, the main concerns regarding pc-AFM include: physical configuration, laser disturbance and laser alignment. Although many of these concerns have been resolved pc-AFM modules are still widely modified from c-AFM and traditional AFM instruments.\n\nThe first main concern deals with component configuration and whether or not there is physically enough space for modification in the cramped c-AFM module. The component configuration must be such the addition of the laser illumination component does not cause disturbance to other units. Interaction between the illumination laser and the guiding laser was also a concern. First attempts to address these two issues was to place a prism between the sample tip and the surface such that the prism would allow the illumination laser to reflect at the interface between the prism and the laser and thus be focused to a localized spot on the sample surface. However, lack of space for the prism and the production of multiple light reflections when introducing a prism required a different concept for configuration.\n\nThe module constructed by Lee et al. implemented a tilted mirror plate that was positioned underneath the sample substrate. This conductive mirror was tilted at 45° and successfully reflected the illuminating laser to a focused spot directly underneath the conductive tip. The steering mirror was employed as a means of controlling the trajectory of the laser source, with this addition the position of the reflected beam on the sample could be easily adjusted for placement underneath the AFM tip. The illumination laser source was a diode-pumped solid-state laser system that produced a wavelength of 532 nm and a spot of 1 mm in the sample.\n\nThe addition of the mirror and laser underneath the sample substrate results in a higher scanning level due to raising the sample substrate. This configuration has no effect on any other instrument component and does not affect AFM performance. This result was confirmed by identical topographical images that were taken with and without the placement of the mirror and laser. This particular set-up required the separation of the x, y and z piezo-scanners The separation of piezo-tubes accounts for the elimination of x-z cross-coupling and scanning-size errors, which is common in traditional AFM.\n\nIn addition there was no evidence of laser interferences between the guiding laser and the irradiation laser. The guiding laser, at a wavelength of 650 nm, hits the mirror on the back of the conducting cantilever from vertical trajectory and is reflected away from the cantilever towards the position sensitive photodetector (PSPD). The illumination beam, on the other hand, travels from underneath the sample platform and is reflected into position by the reflecting mirror. The angle of the mirror plate ensures that the beam does not extend past the sample surface.\n\nThe conductive AFM tip was easily aligned over the reflected illumination beam. The laser spot in the sample was reported to be 1mm in size and can be found using the AFM recording device. A convenience of this technique is that laser alignment is only necessary for imaging in the z-direction because the photocurrents are mapped in this direction. Therefore, normal AFM/c-AFM can be implemented for analysis in the x and y directions.\nThe instrumental module proposed by Lee et al. produced spot sizes from the illumination laser of 1 mm in thicknesses. Recent applications have altered Lee’s design in order to decrease spot size while simultaneously increasing the intensity of this laser. Recent instrumentation has replaced the angled mirror with an inverted microscope and a neutral density filter. In this device the x and y piezos, illumination laser and inverted microscopy are confined underneath the sample substrate, while the z-piezo remains above the conductive cantilever. In the applications of Ginger et al. a neutral-density filter is added to increase laser attenuation and the precision of laser alignment is enhanced by the addition of the inverted microscope.\n\nOne of the most common pc-AFM setups incorporates a light source, which emits in the visible spectrum along with an indium tin oxide (ITO) semi-conductive layer (used as the bottom cathode). The use of a gold plated silicon AFM probe is often used as the top anode in pc-AFM studies. This electrode which carries relatively small current within it, is able to generate nano-scale holes within the sample material to which the two electrodes are able to detect the relatively small change in conductance due to the flow from the top electrode to the bottom electrode. The combination of these elements produced laser intensities in the range of 10 to 108 W/m and decreased the size of the laser spot to sub-micrometer dimensions making this technique useful for the application of nm thin OPV films.\nAlthough there is significant insight as to how OPVs work, it is still difficult to relate the device’s functionality to local film structures. This difficulty may be attributed to the minimal current generation at a given point within OPVs. Through pc-AFM, OPV devices can be probed at nano-scale level and can help to increase our fundamental knowledge of mechanisms involved in OPVs at nano-scale level. pc-AFM is capable of gathering information such as the mapping of photocurrents, differences in film morphology, determination of donor-acceptor domains, current density-voltage plots, quantum efficiencies, and approximate charge carrier mobilities. One of the other notable characteristics of pc-AFM is its ability to provide concurrent information regarding the topological and photocurrent properties of the device at nano-scale. Using this concurrent sampling method, the sample handling is minimized and can provide more accurate results. In a study by Pingree et al., pc-AFM was used to measure how spatial deviations in the photocurrent generation developed with different processing techniques. The authors were able to compare these photocurrent variations to the duration of the annealing process. They have concluded that lengthening the annealing time allows for improved nano-scale phase separation as well as created a more ordered device. Actual times for the annealing process vary depending on the properties of the polymers used. The authors have shown that external quantum efficiency (EQE) and power conversion efficiency (PCE) levels reach a maximum at certain annealing times whereas while the electron and hole mobility’s do not show the corresponding trends. Therefore, while lengthening the annealing time can increase the photocurrents within the OPV, there is a practical limit to after which the benefits may not be substantial. Besides functional properties, pc-AFM can also be used to interrogate the composition heterogeneity of OPVs when combined with either Raman or infrared (IR) spectroscopy, and it is especially valuable for studying their degradation.\n\nIn more recent studies, pc-AFM has been employed to gather information regarding the photoactive regions from the use of quantum dots. Because if their relative ease of use, along with size-tunable excitation attributes, quantum dots have commonly been applied as sensitizers in optoelectronic devices. The authors have studied the photoresponse of sub-surface foundations such as buried indium arsenide (InAs) quantum dots through the implementation of pc-AFM. Through the use of pc-AFM, information regarding quantum dot size, as well as the dispersion of quantum dots within the device, can be recorded in a non-destructive manner. This information can then be used to display local variances in photoactivity relating to heterogeneities within the film morphology.\n\nSample preparation of the OPV is of the utmost importance when performing pc-AFM studies. The sampling substrate is recommended to be conductive, as well as transparent, to the light source which is irradiated upon it. Numerous studies have used ITO-coated glass as their conductive substrate. Because of high cost of ITO, however, there have been attempts to utilize other semiconducting layers, such as zinc oxide (ZnO) and carbon nanotubes as an alternative to ITO. Although these semiconductors are relatively inexpensive, high quality ITO layers are still being used extensively for PV applications. Poly(3,4-ethylenedioxythiophene) poly(styrenesulfonate), more commonly known as , is a transparent, polymeric conductive layer which is usually placed between the ITO and the active OPV layer. The PEDOT:PSS is a conductive polymer is stable over various applied charges. In most studies, PEDOT:PSS is spin-coated onto the ITO-coated glass substrates directly after plasma cleaning of the ITO. Plasma cleaning, as well as halo-acid etching, have been shown to improve the surface uniformity and conductivity of the substrate. This PEDOT:PSS layer is then annealed to the ITO prior to spin-coating the OPV layer onto the substrate. Studies by Pingree et al. have shown the direct correlation between annealing time and both peak and average photocurrent generation. Once this OPV film is spin-coated onto the substrate, it is then annealed at temperatures between 70 and 170 °C, for periods up to an hour depending on the procedure as well as OPV being used.\n\nA recently developed OPV system based on tetrabenzoporphryin (BP) and either [6,6]-phenyl-C-butyric acid methyl ester (PCBM) is explained in detail as follows. In this study, the precursor to BP (1,4:8,11:15,18:22,25-tetraethano-29H,31H-tetrabenzo[b,g,l,q]porphyrin (CP) solution is applied as the starting film, and was thermally annealed which caused the CP to convert into BP. The BP:fullerene layer serves as the undoped layer within the device. For surface measurements, the undoped layer is rinsed with a few drops of chloroform and spin-dried until the BP network is exposed at the donor/acceptor interface. For bulk heterojunction characterization, an additional fullerene solution is spin-coated onto the undoped layer, a thin layer of lithium fluoride is then deposited followed by either an aluminum or gold cathode which is thermally annealed to the device. The thin layer of lithium fluoride is deposited to help prevent the oxidation of the device. Controlling the thickness of these layers plays a significant role in the generation of the efficiency of the PV cells. Typically, the thickness of the active layers is usually smaller than 100 nm to produce photocurrents. This dependence on layer thickness is due to the probability that an electron is able to travel distances on the order of exciton diffusion length within the applied electric field. It should be noted that many of the organic semiconductors used in the PV devices are sensitive to water and oxygen. This is due to the likelihood of photo-oxidation which can occur when exposed to these conditions. While the top metal contact can prevent some of this, many studies are either performed in an inert atmosphere such as nitrogen, or under ultra-high vacuum (UHV).\n\nOnce the sample preparation is complete, the sample is placed onto the scanning stage of the pc-AFM module. This scanning stage is used for x-y piezo translation, completely independent of the z-direction while using a z-piezo scanner. The piezo-electric material within this scanner converts a change in the applied potential into mechanical motion which moves the samples with nanometer resolution and accuracy. There are two variations in which the z-piezo scanner functions; one is contact mode while the other is tapping mode.\n\nMany commercial AFM cantilever tips have pre-measured resonant frequencies and force constants which are provided to the customer. As sampling proceeds, the cantilever tip’s position changes, which causes the scanning laser wavelength (650 nm) to deviate from its original position on the detector. The z-piezo scanner then recognizes this deviation and moves vertically to return the laser spot to its set position. This vertical movement by the z-piezo scanner is correlated to a change in voltage. Sampling in contact mode relies upon intermolecular forces between the tip and surface as depicted by Van der Waals force. As the sampling begins, the tip is moved close to the sample which creates a weakly attractive force between them. Another force which is often present in contact mode is capillary force due to hydration on the sample surface. This force is due to the ability of the water to contact the tip, thus creating an undesirable attractive force. Capillary force, along with several other sources of tip contamination, are key factors in the decreased resolution observed while sampling\n\nThere are considerations which need to be taken into account when determining which mode is optimal for sampling for a given application. It has been shown that sampling in contact mode with very soft samples can damage the sample and render it useless for further studies. Sampling in non-contact mode is less destructive to the sample, but the tip is more likely to drift out of contact with the surface and thus it may not record data. Drifting of the tip is also seen due to piezo hysteresis, which causes displacement due to molecular friction and polarization effects due to the applied electric field.\nIt is important to note the correlation between resolution and curvature of tip radius. Early STM tips used by Binning and Rohrer were fairly large, anywhere between some hundred nm to 1 µm in radius. In more recent work, the tip radius of curvature was mentioned as 10–40 nm. By reducing the radius of curvature of the tip, it allows for the enhanced detection of deviations within the OPVs surface morphology. Tips often need to be replaced due to tip rounding, which leads to a decrease in the resolution. Tip rounding occurs due to the loss of outermost atoms present at the apex of the tip which can be a result of excessive force applied or character of the sample.\n\nBecause of the extremely small radius of the AFM tip, the illumination source is allowed to be focused tighter, thus increasing its efficiency. Typical arrangements for pc-AFM contain a low powered, 532 nm laser (2–5 mW) whose beam is reflected off mirrors located beneath the scanning stage. Through the use of a charge-coupled device (CCD), the tip can easily be positioned directly over the laser spot. Xenon arc lamps have also been widely used as illumination sources, but are atypical in recent work. In a study by Coffey et al., lasers of two different wavelengths (532 nm and 405 nm) are irradiated onto the same sample area. With this work, they have shown images with identical contrast which proves that the photocurrent variations are less related to spatial absorbance variation.\n\nCurrent density equation:\n\nformula_11\n\nwhere \"J\" is the current density, \"ε\" is the permittivity of a vacuum, \"ε\" is the relative permeability of the medium, \"µ\" is the mobility of the medium, \"V\" is the applied bias and \"L\" is the film thickness in nanometers. The majority of the organic materials have relative permeability values of ~3 in their amorphous and crystalline states.\nThe range of bias commonly applied is usually limited to between −5 V to +5 V for most studies. This can be achieved by applying a forward bias or reverse bias to the sample through the spotted gold contact. By adjusting this bias, along with the current passing through the cantilever, one can adjust the repulsive/attractive forces between the sample and the tip. When a reverse bias is applied (tip is negative relative to the sample), the tip and the sample experience attractive forces between them. This current density measurement is then combined with the topographical information previously gathered from the AFM tip and cantilever. The resulting image displays the local variations in morphology with the current density measurements superimposed onto of them.\n\nSeveral methods have been employed to help reduce both mechanical and acoustic vibrations within the system. Mechanical vibrations are mainly attributed to traffic in and out of a building Other sources of mechanical vibrations have often been seen in the higher stories of a building due to reduced damping from building supports. This source of vibrational noise is easily controlled through the use of a vibration isolation table. Acoustical vibrations are far more common than mechanical vibrations. This type of vibration is a result of air movement near the instrument such as fans or human voices. Several methods have been developed to help reduce this source of vibration. An easy solution for this is separating the electronic components from the stage. The reason for this separation of components is due to the cooling fans within the electrical devices. While operating, the fans lead to a constant source of vibrational noise within the system. In most cases, other methods still need to be employed to help reduce this source of noise. For instance, the instrument can be placed within a sealed box constructed of acoustic dampening material. Smaller stages also result in less surface area for acoustic vibrations to collide with, thus reducing the noise recorded. A more in depth solution consists of removing all sharp edges on the instrument. These sharp edges can excite resonances within the piezo-electric materials which increase the acoustic noise within the system.\n\n"}
{"id": "617167", "url": "https://en.wikipedia.org/wiki?curid=617167", "title": "Piphilology", "text": "Piphilology\n\nPiphilology comprises the creation and use of mnemonic techniques to remember a span of digits of the mathematical constant . The word is a play on the word \"pi\" itself and of the linguistic field of philology.\n\nThere are many ways to memorize , including the use of \"piems\" (a portmanteau, formed by combining pi and poem), which are poems that represent in a way such that the length of each word (in letters) represents a digit. Here is an example of a piem: \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\" Notice how the first word has three letters, the second word has one, the third has four, the fourth has one, the fifth has five, and so on. In longer examples, 10-letter words are used to represent the digit zero, and this rule is extended to handle repeated digits in so-called Pilish writing. The short story \"Cadaeic Cadenza\" records the first 3834 digits of in this manner, and a 10,000-word novel, \"Not A Wake\", has been written accordingly.\n\nHowever, piems prove to be inefficient for large memorizations of . Other methods include remembering patterns in the numbers (for instance, the year 1971 appears in the first fifty digits of ) and the method of loci (which has been used to memorize to 67,890 digits).\n\nUntil the 20th century, the number of digits of pi which mathematicians had the stamina to calculate by hand remained in the hundreds, so that memorization of \"all\" known digits at the time was possible. In 1949 a computer was used to calculate π to 2000 places, presenting one of the earliest opportunities for a more difficult challenge.\n\nLater computers calculated pi to extraordinary numbers of digits (2.7 trillion as of August 2010), and people began memorizing more and more of the output. The world record for the number of digits memorized has exploded since the mid-1990s, and it stood at 100,000 as of October 2006. The previous record (83,431) was set by the same person (Akira Haraguchi) on July 2, 2005, and the record previous to that (42,195) was held by Hiroyuki Goto.\nAn institution from Germany provides the details of the \"Pi World Ranking\".\n\nThe most common mnemonic technique is to memorize a so-called \"piem\" (a wordplay on \"pi\" and \"poem\") in which the number of letters in each word is equal to the corresponding digit of π. This famous example for 15 digits has several variations, including:\n\nShort mnemonics such as these, of course, do not take one very far down π's infinite road. Instead, they are intended more as amusing doggerel. If even less accuracy suffices, the following examples can be used:\n\nThis second one gives the value of π as 3.1415926535, while the first only brings it to the second five. Indeed, many published poems use truncation instead of one of the several roundings, thereby producing a less-accurate result when the first omitted digit is greater than or equal to five. It is advantageous to use truncation in memorizing if the individual intends to study more places later on, otherwise one will be remembering erroneous digits.\n\nAnother mnemonic is:\nIn this mnemonic the word \"point\" represents the decimal point itself.\n\nYet another example is:\nIn this example, the spelling of Archimedes is normalised to nine. (Although 'Archimedes' is, today, a more correct spelling of the ancient Greek mathematician's name in English, Archimede is also often seen when this mnemonic is given, since Archimède is the more correct spelling in some languages, such as French.)\n\nLonger mnemonics employ the same concept. This example created by Peter M. Brigham incorporates twenty decimal digits:\n\nSome mnemonics, such as this poem which gives the three and the first 20 decimal digits, use the separation of the poem's title and main body to represent the decimal point:\n\nAnother, more poetic version is:\n\nExtensions to 30 or 31 decimals of the same proceed as follows:\nThere are minor variations on the above rhyme, which still allow pi to be worked out correctly. However, one variation replaces the word \"lexicon's\" with \"lesson's\" and in doing so, incorrectly indicates that the 18th digit is seven.\n\nThe logologist Dmitri Borgmann gives the following 30-word poem in his book, \"\": \n\nThe following sonnet is a mnemonic for pi to 75 decimal places in iambic pentameter:\n\nNote that in this example, 10-letter words are used to represent the digit zero.\n\nOther poems use sound as a mnemonic technique, as in the following poem which rhymes with the first 140 decimal places of pi using a blend of assonance, slant rhyme, and perfect rhyme:\n\nNote that \"dreams number us like pi\" corresponds to \"314159,\" and so on. Sound-based mnemonic techniques, unlike pilish, do not require that the letters in each word be counted in order to recall the digits of pi. However, where sound-based mnemonics use assonance, extra care must be taken to distinguish \"nine\" and \"five,\" which contain the same vowel sound. In this example, the author assumes the convention that zero is often called \"O.\"\n\nThe piku follows the rules of conventional haiku (three lines of 5, 7 and 5 syllables), but with the added mnemonic trick that each word contains the same number of letters as the numerals of pi, e.g.\n\nIn 2004, Andrew Huang wrote a song that was a mnemonic for the first fifty digits of pi, titled \"I am the first 50 digits of pi\". The first line is:\nIn 2013, Huang extended the song to include the first 100 digits of pi, and changed the title to \"Pi Mnemonic Song\".\n\nThere are piphilologists who have written texts that encode hundreds or thousands of digits. This is an example of constrained writing, known as \"Pilish\". For example, \"Poe, E.: Near a Raven\" represents 740 digits, \"Cadaeic Cadenza\" encodes 3835, and \"Not A Wake\" extends to 10,000 digits.\n\nIt is also possible to use the rhythm and sound of the spoken digits themselves as a memorization device. The mathematician John Horton Conway composed the following arrangement for the first 100 digits,\n\nwhere the accents indicate various kinds of repetition.\n\nAnother mnemonic system used commonly in the memorization of pi is the Mnemonic major system, where single numbers are translated into basic sounds. A combination of these sounds creates a word, which can then be translated back into numbers. When combined with the Method of loci, this becomes a very powerful memorization tool.\n\nTranslation:\n\nCounting the letters in each word (additionally separated by \"|\"), gives 10 decimal places of : خرد (kherad) = 3, و (va) = 1, دانش (daanesh) = 4, و (va) = 1, آگاهی (aagaahi) = 5, ...\n\nTranslation:\nAn interesting (not math themed) alternative:\n\nTranslation:\n\nAnother alternative:\n\nThis statement yields π to twenty-two decimal places:\n\nEnglish translation that doesn't encode pi:\n\nLooser English translation that encodes pi:\n\nThe following poem composed of alexandrines consists of words each with a number of letters that yields π to 126 decimal places:\n\nTranslation:\n\nAn alternative beginning:\n\nYielding π to 22 decimal places:\n\nTranslation:\n\nThe following piem, giving π to 31 decimal places, is well known in Argentina:\n\nTranslation:\n\nAnother. This piem gives π (correctly rounded) to 10 decimal places. (If you prefer to not round π, then replace \"cosmos\" with \"cielo\".)\n\nTranslation:\n\n\"Níl i mata, a shaoi, eolaíocht nó feidhm.\" (7 decimal places) — \"Wise one, mathematics has neither science nor use.\"\n\nOne of the Romanian versions of Pi poems is:\nThere is another phrase known in Romanian that will help to memorize the number by eight decimal places:\n\"Așa e bine a scrie renumitul și utilul număr.\" — \"This is the way to write the renowned and useful number.\"\n\nIn the Russian language, there is a well-known phrase in the reform of 1917 orthography of old tradition: \"Кто и шутя, и скоро пожелаетъ «Пи» узнать число — ужъ знаетъ.\" (The one who would wish to know the number pi easily and quickly already knows it.)\n\nA more modern rhyme is:\n\nA short approximation is: \"Что я знаю о кругах?\" (What do I know about circles?)\n\nIn addition, there are several nonfolklore verses that simply rhyme the digits of pi \"as is\"; for examples, see .\n\nThe verse of Polish mathematician Witold Rybczyński: \n\nThe verse of Polish mathematician Kazimierz Cwojdziński:\n\nAn occasionally seen verse related to Mundial Argentina and the Polish football team:\n\nOr in Brazilian Portuguese: \n\nA piem written in a more poetic manner:\n\nTranslation:\n\nJapanese piphilology has countless mnemonics based on punning words with numbers. This is especially easy in Japanese because there are two or three ways to pronounce each digit, and the language has relatively few phonemes to begin with. For example, to 31 decimal places:\n\nThis is close to being ungrammatical nonsense, but a loose translation prioritizing word order yields:\n\nJapanese children also use songs built on this principle to memorize the multiplication table.\n\nIt is possible to construct piphilogical poems in Chinese by using homophones or near-homophones of the numbers zero through nine, as in the following well known example which covers 22 decimal places of π. In this example the character meaning \"mountain\" (山 \"shān\") is used to represent the number \"three\" (三 \"sān\"), the character meaning \"I\" (吾 \"wú\") is used to represent the number \"five\" (五 \"wǔ\"), and the characters meaning \"temple\" (寺 \"sì\") and \"die\" (死 \"sǐ\") are used to represent the number \"four\" (四 \"sì\"). Some of the mnemonic characters used in this poem, for example \"kill\" (殺 \"shā\") for \"three\" (三 \"sān\"), \"jug\" (壺 \"hú\") for \"five\" (五 \"wǔ\"), \"happiness\" (樂 \"lè\") for \"six\" (六 \"liù\") and \"eat\" (吃 \"chī\") for \"seven\" (七 \"qī\"), are not very close phonetically in Mandarin/Putonghua.\n\nThis can be translated as:\n\n\"Sen, o alan o çevre bölününce ve sonsuz rakam ile çıkan değişken dizilimli sayısın.\" — \"You are the number with infinite digits in changing order, which is found when the circumference and the area is divided.\"\n\n\"Sám u sebe v hlavě magického pí číslic deset mám.\" (nine decimal places) — \"I have ten digits of magical pi in my head.\"\n\n\"Lín a kapr u hráze prohlídli si rybáře, udici měl novou, jikrnáči neuplovou.\" (12 decimal places) — \"Tench and carp by the dam watched the fisher. He has a new rod, fish will not escape.\"\n\n\"Dej, ó Bože, a číslo Ludolfovo já navždy pomnu, pro větší naplnění moudrosti početní.\" (13 decimal places) — \"Oh God, let me to remember the pi forever, for the increase of mathematical skills.\"\n\n\"Mám ó bože ó velký pamatovat si takový cifer řad, velký slovutný Archimedes, pomáhej trápenému, dej mu moc, nazpaměť nechť odříká ty slavné sice, ale tak protivné nám, ach, číslice Ludolfovy!\" (30 decimal places) — \"Shall I, God oh almighty, remember such a long string of numbers, great and famous Archimedes, help my careworn being, give me the power, to recite by heart all the digits, which may be famous, but also hated by some of us, the digits of Ludolph van Ceulen.\"\n\n\"Čak i Grci i stari Vavilonci su kazali: obime kad deliš krugovim prečnikom dobijaš neophodan nam pi.\" (16 decimal places) — \"Even Greeks and Old Babylonians have told: when dividing circumferences with circle's diameter you obtain the indispensable pi.\"\n\n\"Non è dato a tutti ricordare il numero aureo del sommo filosofo Archimede. Certuni sostengon che si può ricordar tale numero, ma questi soli poi non recitano che un centone insensato.\" (30 decimal places) — \"Not anybody can retain the golden number of the great philosopher Archimedes. There are who claim it is possible to recall this number, but then they just recite a senseless cento\"\n\nThe Katapayadi System of verses is basically a system of code so that things can be defined in a way so that people can remember. The code is as follows:\n\nWith the above key in place, Sri Bharathi Krishna Tirtha in his Vedic Mathematics gives the following verse:\n\nगोपी भाग्य मधुव्रात श्रुङ्गिशो दधिसन्धिग |\nखलजीवित खाताव गलहालारसंधार |\nIf we replace the code from the above table in the above verse, here is what we get.\n31 41 5926 535 89793\n23846 264 33832792\nThat gives us π/10=0.31415926535897932384626433832792 \n\nEven before computers calculated , memorizing a record number of digits became an obsession for some people. The record for memorizing digits of , certified by \"Guinness World Records\", is 70,000 digits, recited in India by Rajveer Meena in 9 hours and 27 minutes on 21 March 2015. On October 3, 2006, Akira Haraguchi, a retired Japanese engineer, claimed to have recited 100,000 decimal places, but the claim was not verified by Guinness World Records. \n\nDavid Fiore was an early record holder for pi memorization. Fiore's record stood as an American record for more than 27 years, which remains the longest time period for an American recordholder. He was the first person to break the 10,000 digit mark.\n\nSuresh Kumar Sharma holds Limca Book of Records for the most decimal places of pi recited by memory. He rattled off 70,030 numbers in 17 hours 14 minutes on October 21, 2015.\n\n\n"}
{"id": "642973", "url": "https://en.wikipedia.org/wiki?curid=642973", "title": "Project Steve", "text": "Project Steve\n\nProject Steve is a list of scientists with the given name Stephen/Steven or a variation thereof (e.g., Stephanie, Stefan, Esteban, etc.) who \"support evolution\". It was originally created by the National Center for Science Education as a \"tongue-in-cheek parody\" of creationist attempts to collect a list of scientists who \"doubt evolution,\" such as the Answers in Genesis' list of scientists who accept the biblical account of the Genesis creation narrative or the Discovery Institute's \"A Scientific Dissent From Darwinism\". The list pokes fun at such endeavors while making it clear that, \"We did not wish to mislead the public into thinking that scientific issues are decided by who has the longer list of scientists!\" It also honors Stephen Jay Gould. The level of support for evolution among scientists is very high. A 2009 poll by Pew Research Center found that \"Nearly all scientists (97%) say humans and other living things have evolved over time.\"\n\nHowever, at the same time the project is a genuine collection of scientists. Despite the list's restriction to only scientists with names like \"Steve\", which in the United States limits the list to roughly 1 percent of the total population, Project Steve is longer and contains many more eminent scientists than any creationist list. In particular, \"Project Steve\" contains many more biologists than the creationist lists, with about 54% of the listed Steves being biologists.\n\nThe \"List of Steves\" webpage provides an updated total of scientist \"Steves\" who have signed the list. , Project Steve has 1,432 signatories.\n\nThe statement that signatories agree to reads:\n\nThere have been some complaints that the statement left out the geological sciences, where evolution is an important principle as well. However, this oversight was noticed too late and it was decided that it would be more effort than it is worth to go back to correct it.\n\nThe project was named in honor of the paleontologist Stephen Jay Gould (1941–2002). It began in 2003, with an official press release on February 16, 2003. The press release was issued at the American Association for the Advancement of Science's 2003 convention in Denver, Colorado, after a lecture by Lawrence Krauss titled \"Scientific Ignorance as a Way of Life: From Science Fiction in Washington to Intelligent Design in the Classroom.\" Krauss made the actual announcement and directed the reporters to NCSE Director Eugenie Scott, who was sitting in the audience in the front row.\n\nThe original goal was to collect the signatures of 100 Steves, but this goal was reached in about 10 days. Both Nobel Prize-winning Steves in science — Steven Weinberg and Steven Chu (who has since served as Secretary of Energy in Barack Obama's Cabinet) — were among the first 100 Steves. Over 200 Steves responded in the first month. As the news of Project Steve spread by word of mouth, ever-increasing numbers of Steves contacted the NCSE, and the list continued to grow.\n\nProject Steve captured the attention of the media. The first media coverage included articles in the \"Washington Times\", \"Science\", the \"Oakland Tribune\" and an interview of NCSE director Eugenie Scott by Australian science journalist and radio broadcaster Robyn Williams for the Australian Broadcasting Corporation’s radio show, \"The Science Show\". \"The Science Show\" arranged for Geoff Sirmai and David Fisher of the Australian musical comedy team \"Comic Roasts\" to write the \"Steve Song\", a parody of the Monty Python song about Spam, for Project Steve. The song had its debut on \"The Science Show\" episode featuring the interview of Scott which aired on Australian Broadcasting Corporation Radio National on March 8, 2003.\n\nCambridge University Lucasian Professor of Mathematics Stephen Hawking was the 300th Steve to sign the list. By the time the announcement was made on April 21, 2003, another five had joined to bring the total number of Steves to 305. By December 26, 2003, St. Stephen's Day, \"Project Steve\" had grown to 400 scientists.\n\nAs Project Steve reached the 400 scientist mark, the NCSE decided to offer a commemorative novelty Project Steve t-shirt. The t-shirt is emblazoned with the proclamation, \"Over _00 Scientists named Steve Agree, Teach Evolution!\" in large letters, where the blank contains the most recent hundreds mark. A list of the current signatories is included in a smaller typeface on the t-shirt as well.\n\nEugenie Scott, Glenn Branch and Nick Matzke published an article in the July/August 2004 issue of the \"Annals of Improbable Research\" (with all the Steves that had signed up to that point listed as co-authors) called \"The Morphology of Steve\" which contained \"the first scientific analysis of the sex, geographic location, and body size of scientists named Steve\". The data were obtained using NCSE's \"pioneering experimental steveometry apparatus\"—the t-shirt.\n\nShortly after the second anniversary of Project Steve in February 2005, 543 Steves had signed the list. A front-page story in the Ottawa Citizen marking this event was published on February 20, 2005. On September 12, 2005, the 600th Steve signed the list. By February 16, 2006, the third anniversary of \"Project Steve\"'s official launch, the Steve-o-meter stood at 700. On April 24, 2007, the list had grown to 800 Steves. In February 2009, the milestone #1000 was assigned to professor of ecology and evolutionary biology Steven P. Darwin (no relation to Charles). Subsequent milestones were #1100 on August 25, 2009, #1200 on April 6, 2012, and #1300 on January 15, 2014.\n\nThere have been articles about \"Project Steve\" in \"The Times\", \"Scientific American\", \"Yale Daily News\", Focus on the Family's \"Family News in Focus\", \"The Guardian\", MIT's \"TechTalk\", and \"The Arizona Republic\", among many others.\n\nWilliam Dembski, fellow of the Discovery Institute, whose \"\"Scientific Dissent from Darwinism\" petition had eight Steves as of July, 2007, has said that:\n\nInspired by Project Steve, and motivated by media coverage of the Discovery Institute's \"Dissent From Darwinism\"\" list, during the \"Kitzmiller v. Dover Area School District\" case, R. Joe Brandon initiated a four-day, word-of-mouth petition of scientists in support of evolution in October 2005. During the four-day drive \"A Scientific Support For Darwinism And For Public Schools Not To Teach Intelligent Design As Science\" gathered 7733 signatures of verifiable scientists. During the four days of the petition, \"A Scientific Support for Darwinism\" received signatures at a rate 697,000% higher than the Discovery Institute's petition, \"A Scientific Dissent from Darwinism\", according to archaeologist R. Joe Brandon.\n\n"}
{"id": "327801", "url": "https://en.wikipedia.org/wiki?curid=327801", "title": "Public Interest Research Group", "text": "Public Interest Research Group\n\nPublic Interest Research Groups (PIRGs) are a federation of U.S. and Canadian non-profit organizations that employ grassroots organizing and direct advocacy with the goal of effecting liberal political change.\n\nThe PIRGs emerged in the early 1970s on U.S. college campuses. The PIRG model was proposed in the book \"Action for a Change\" by Ralph Nader and Donald Ross. Ross helped students across the country set up the first PIRG chapters, then became the director of the New York Public Interest Research Group in 1973.\n\nThe Minnesota Public Interest Research Group, founded in 1971, was the first state PIRG to incorporate. It was followed by Oregon (OSPIRG) and Massachusetts (MASSPIRG). The PIRGs were supportive of container deposit legislation in the United States, popularly called \"bottle bills\".\n\nIn 1982, the PIRGs established the Fund for the Public Interest as its fundraising and canvassing arm.\n\nPIRGs on college campuses have historically been funded with a portion of student activity fees in the form of a labor checkoff. Students may elect to have the fees refunded to them, although many students are unaware that this is the case. This system of PIRG funding has been met with controversy and with a number of legal challenges. In 2014, students at Macalester College in Minnesota voted to end their relationship with MPIRG due to the group's revenue structure, which relied on MPIRG automatically receiving a cut of student activity fees.\n\nU.S. PIRG lobbied for the creation of the Consumer Financial Protection Bureau, an independent U.S. government agency which was founded as a result of the Dodd–Frank Wall Street Reform and Consumer Protection Act in the wake of the late-2000s recession and the financial crisis.\n\nThe PIRGs have worked to make same-sex marriage legal, to increase the minimum wage, to enact increased environmental regulations, to oppose Voter ID laws in the United States, build high speed rail in California, defend solar net metering in California, increase food labeling, expand open educational resources on campus, expand campus food pantries, and ban pesticides linked to colony collapse.\n\nCharity Navigator rated the U.S. PIRG two out of four stars for accountability and transparency, and three out of four stars for financials.\n\nSome PIRGs are members of a larger network of non-profit organizations called the Public Interest Network. In the past, they have also helped to launch a number of other independent public interest non-profits, including:\n\nState PIRGs include:\n\n\n"}
{"id": "29278", "url": "https://en.wikipedia.org/wiki?curid=29278", "title": "Safety engineering", "text": "Safety engineering\n\nSafety engineering is an engineering discipline which assures that engineered systems provide acceptable levels of safety. It is strongly related to industrial engineering/systems engineering, and the subset system safety engineering. Safety engineering assures that a life-critical system behaves as needed, even when components fail.\n\nAnalysis techniques can be split into two categories: qualitative and quantitative methods. Both approaches share the goal of finding causal dependencies between a hazard on system level and failures of individual components. Qualitative approaches focus on the question \"What must go wrong, such that a system hazard may not occur?\", while quantitative methods aim at providing estimations about probabilities, rates and/or severity of consequences.\n\nThe complexity of the technical systems such as Improvements of Design and Materials, Planned Inspections, Fool-proof design, and Backup Redundancy decreases risk and increases the cost. The risk can be decreased to ALARA (as low as reasonably achievable) or ALAPA (as low as practically achievable) levels.\n\nTraditionally, safety analysis techniques rely solely on skill and expertise of the safety engineer. In the last decade model-based approaches have become prominent. In contrast to traditional methods, model-based techniques try to derive relationships between causes and consequences from some sort of model of the system.\n\nThe two most common fault modeling techniques are called failure mode and effects analysis and fault tree analysis. These techniques are just ways of finding problems and of making plans to cope with failures, as in probabilistic risk assessment. One of the earliest complete studies using this technique on a commercial nuclear plant was the WASH-1400 study, also known as the Reactor Safety Study or the Rasmussen Report.\n\nFailure Mode and Effects Analysis (FMEA) is a bottom-up, inductive analytical method which may be performed at either the functional or piece-part level. For functional FMEA, failure modes are identified for each function in a system or equipment item, usually with the help of a functional block diagram. For piece-part FMEA, failure modes are identified for each piece-part component (such as a valve, connector, resistor, or diode). The effects of the failure mode are described, and assigned a probability based on the failure rate and failure mode ratio of the function or component. This quantiazation is difficult for software ---a bug exists or not, and the failure models used for hardware components do not apply. Temperature and age and manufacturing variability affect a resistor; they do not affect software.\n\nFailure modes with identical effects can be combined and summarized in a Failure Mode Effects Summary. When combined with criticality analysis, FMEA is known as Failure Mode, Effects, and Criticality Analysis or FMECA, pronounced \"fuh-MEE-kuh\".\n\nFault tree analysis (FTA) is a top-down, deductive analytical method. In FTA, initiating primary events such as component failures, human errors, and external events are traced through Boolean logic gates to an undesired top event such as an aircraft crash or nuclear reactor core melt. The intent is to identify ways to make top events less probable, and verify that safety goals have been achieved.\n\nFault trees are a logical inverse of success trees, and may be obtained by applying de Morgan's theorem to success trees (which are directly related to reliability block diagrams).\n\nFTA may be qualitative or quantitative. When failure and event probabilities are unknown, qualitative fault trees may be analyzed for minimal cut sets. For example, if any minimal cut set contains a single base event, then the top event may be caused by a single failure. Quantitative FTA is used to compute top event probability, and usually requires computer software such as CAFTA from the Electric Power Research Institute or SAPHIRE from the Idaho National Laboratory.\n\nSome industries use both fault trees and event trees. An event tree starts from an undesired initiator (loss of critical supply, component failure etc.) and follows possible further system events through to a series of final consequences. As each new event is considered, a new node on the tree is added with a split of probabilities of taking either branch. The probabilities of a range of \"top events\" arising from the initial event can then be seen.\n\nTypically, safety guidelines prescribe a set of steps, deliverable documents, and exit criterion focused around planning, analysis and design, implementation, verification and validation, configuration management, and quality assurance activities for the development of a safety-critical system. In addition, they typically formulate expectations regarding the creation and use of traceability in the project. For example, depending upon the criticality level of a requirement, the US Federal Aviation Authority guideline DO-178B/C requires traceability from requirements to design, and from requirements to source code and executable object code for software components of a system. Thereby, higher quality traceability information can simplify the certification process and help to establish trust in the maturity of the applied development process.\n\nUsually a failure in safety-certified systems is acceptable if, on average, less than one life per 10 hours of continuous operation is lost to failure.{as per FAA document AC 25.1309-1A} Most Western nuclear reactors, medical equipment, and commercial aircraft are certified to this level. The cost versus loss of lives has been considered appropriate at this level (by FAA for aircraft systems under Federal Aviation Regulations).\n\nOnce a failure mode is identified, it can usually be mitigated by adding extra or redundant equipment to the system. For example, nuclear reactors contain dangerous radiation, and nuclear reactions can cause so much heat that no substance might contain them. Therefore, reactors have emergency core cooling systems to keep the temperature down, shielding to contain the radiation, and engineered barriers (usually several, nested, surmounted by a containment building) to prevent accidental leakage. Safety-critical systems are commonly required to permit no single event or component failure to result in a catastrophic failure mode.\n\nMost biological organisms have a certain amount of redundancy: multiple organs, multiple limbs, etc.\n\nFor any given failure, a fail-over or redundancy can almost always be designed and incorporated into a system.\n\nThere are two categories of techniques to reduce the probability of failure:\nFault avoidance techniques increase the reliability of individual items (increased design margin, de-rating, etc.).\nFault tolerance techniques increase the reliability of the system as a whole (redundancies, barriers, etc.).\n\nSafety engineering and reliability engineering have much in common, but safety is not reliability. If a medical device fails, it should fail safely; other alternatives will be available to the surgeon. If the engine on a single-engine aircraft fails, there is no backup. Electrical power grids are designed for both safety and reliability; telephone systems are designed for reliability, which becomes a safety issue when emergency (e.g. US \"911\") calls are placed.\n\nProbabilistic risk assessment has created a close relationship between safety and reliability. Component reliability, generally defined in terms of component failure rate, and external event probability are both used in quantitative safety assessment methods such as FTA. Related probabilistic methods are used to determine system Mean Time Between Failure (MTBF), system availability, or probability of mission success or failure. Reliability analysis has a broader scope than safety analysis, in that non-critical failures are considered. On the other hand, higher failure rates are considered acceptable for non-critical systems.\n\nSafety generally cannot be achieved through component reliability alone. Catastrophic failure probabilities of 10 per hour correspond to the failure rates of very simple components such as resistors or capacitors. A complex system containing hundreds or thousands of components might be able to achieve a MTBF of 10,000 to 100,000 hours, meaning it would fail at 10 or 10 per hour. If a system failure is catastrophic, usually the only practical way to achieve 10 per hour failure rate is through redundancy.\n\nWhen adding equipment is impractical (usually because of expense), then the least expensive form of design is often \"inherently fail-safe\". That is, change the system design so its failure modes are not catastrophic. Inherent fail-safes are common in medical equipment, traffic and railway signals, communications equipment, and safety equipment.\n\nThe typical approach is to arrange the system so that ordinary single failures cause the mechanism to shut down in a safe way (for nuclear power plants, this is termed a passively safe design, although more than ordinary failures are covered). Alternately, if the system contains a hazard source such as a battery or rotor, then it may be possible to remove the hazard from the system so that its failure modes cannot be catastrophic. The U.S. Department of Defense Standard Practice for System Safety (MIL–STD–882) places the highest priority on elimination of hazards through design selection.\n\nOne of the most common fail-safe systems is the overflow tube in baths and kitchen sinks. If the valve sticks open, rather than causing an overflow and damage, the tank spills into an overflow. Another common example is that in an elevator the cable supporting the car keeps spring-loaded brakes open. If the cable breaks, the brakes grab rails, and the elevator cabin does not fall.\n\nSome systems can never be made fail safe, as continuous availability is needed. For example, loss of engine thrust in flight is dangerous. Redundancy, fault tolerance, or recovery procedures are used for these situations (e.g. multiple independent controlled and fuel fed engines). This also makes the system less sensitive for the reliability prediction errors or quality induced uncertainty for the separate items. On the other hand, failure detection & correction and avoidance of common cause failures becomes here increasingly important to ensure system level reliability.\n\n\n\n\n"}
{"id": "3404049", "url": "https://en.wikipedia.org/wiki?curid=3404049", "title": "Semantic analysis (linguistics)", "text": "Semantic analysis (linguistics)\n\nIn linguistics, semantic analysis is the process of relating syntactic structures, from the levels of phrases, clauses, sentences and paragraphs to the level of the writing as a whole, to their language-independent meanings. It also involves removing features specific to particular linguistic and cultural contexts, to the extent that such a project is possible. The elements of idiom and figurative speech, being cultural, are often also converted into relatively invariant meanings in semantic analysis. Semantics, although related to pragmatics, is distinct in that the former deals with word or sentence choice in any given context, while pragmatics considers the unique or particular meaning derived from context or tone. To reiterate in different terms, semantics is about universally coded meaning, and pragmatics, the meaning encoded in words that is then interpreted by an audience.\n\nSemantic analysis can begin with the relationship between individual words. This requires an understanding of lexical hierarchy, including hyponymy and hypernymy, meronomy, polysemy, synonyms, antonyms, and homonyms. It also relates to concepts like connotation (semiotics) and collocation, which is the particular combination of words that can be or frequently are surrounding a single word. This can include idioms, metaphor, and simile, like, \"white as a ghost.\"\n\nWith the availability of enough material to analyze, semantic analysis can be used to catalog and trace the style of writing of specific authors.\n\n"}
{"id": "1837640", "url": "https://en.wikipedia.org/wiki?curid=1837640", "title": "The Creator", "text": "The Creator\n\nThe Creator () is a 298.48 carat colored raw diamond, the third largest gem diamond ever found in Russia or the territory of the former USSR (after the 26th Congress of the CPSU and the Alexander Pushkin), and one of the largest in the world as of 2016. It was found at a placer mining factory in the area of the Lower Lena River (Yakutia, Far Eastern Federal District) in 2004 and is owned by the Government of the Sakha Republic, but kept in the Russian Diamond Fund (Moscow Kremlin.)\n\n\n"}
{"id": "53844131", "url": "https://en.wikipedia.org/wiki?curid=53844131", "title": "William Arthur Coles", "text": "William Arthur Coles\n\nWilliam Arthur Coles from the University of California, San Diego, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Topical Group in Plasma Astrophysics in 2006, for \"his major contributions to our understanding of the effect of plasma turbulence on radio wave propagation, and the use of radio propagation measurements to infer properties of remote turbulent plasmas in interplanetary space and the interstellar medium.\"\n"}
{"id": "52620968", "url": "https://en.wikipedia.org/wiki?curid=52620968", "title": "Zhao Er-mi", "text": "Zhao Er-mi\n\nZhao Er-mi (; 1930 – 24 December 2016) was a Chinese herpetologist, born in Chengdu.\n\nHe had been a member of the Chinese Academy of Sciences since 2001. He died at West China Medical Center of Sichuan University on 24 December 2016.\n\nZhao is commemorated in the scientific names of four taxa of reptiles.\n\nAlso, two amphibian species have been named after him.\n"}
