{"id": "30857555", "url": "https://en.wikipedia.org/wiki?curid=30857555", "title": "A Question and Answer Guide to Astronomy", "text": "A Question and Answer Guide to Astronomy\n\nA Question and Answer Guide to Astronomy is a book about astronomy and cosmology, and is intended for a general audience. The book was written by Pierre-Yves Bely, Carol Christian, and Jean-Rene Roy, and published in English by Cambridge University Press in 2010. It was originally written in French. The content within the book is written using a question and answer format. It contains some 250 questions, which \"The Science Teacher\" states each are answered with a \"concise and well-formulated essay that is informative and readable.\" \"The Science Teacher\" review goes on to state that many of the answers given in the book are \"little gems of science writing\". \"The Science Teacher\" summarizes by stating that each question is likely to be thought of by a student, and that \"the answers are informative, well constructed, and thorough\".\n\nThe book covers information about the planets, the Earth, the Universe, practical astronomy, history, and awkward questions such as astronomy in the Bible, UFOs, and aliens. Also covered are subjects such as the Big Bang, comprehension of large numbers, and the Moon illusion.\n\n\n"}
{"id": "5671882", "url": "https://en.wikipedia.org/wiki?curid=5671882", "title": "Abell 3266", "text": "Abell 3266\n\nAbell 3266 is a galaxy cluster in the southern sky. It is part of the Horologium-Reticulum supercluster. The galaxy cluster is one of the largest in the southern sky, and one of the largest mass concentrations in the nearby universe.\n\nThe Department of Physics at the University of Maryland, Baltimore County discovered that a large mass of gas is hurtling through the cluster at a speed of 750 km/s. The mass is billions of solar masses, approximately three (3) million light years in diameter and is the largest of its kind so far discovered (June 2006).\n\n\n"}
{"id": "2095947", "url": "https://en.wikipedia.org/wiki?curid=2095947", "title": "Ahmad (crater)", "text": "Ahmad (crater)\n\nAhmad is a crater in the northern hemisphere of Saturn's moon Enceladus. Ahmad was first discovered in \"Voyager 2\" images but was seen at much higher resolution, though near the terminator, by \"Cassini\". It is located at 58.8° North Latitude, 311.6° West Longitude and is 18.7 kilometers across. The western portion of the crater is largely absent, either buried or disrupted by the eastern margin of Samarkand Sulci. A large, dome-like structure occupies the interior of the crater, caused by infill of material from Samarkand Sulci or from viscous relaxation.\n\nAhmad is named after a hero from \"Arabian Nights\". He brings his father an apple and marries Peri-Banu.\n"}
{"id": "39292620", "url": "https://en.wikipedia.org/wiki?curid=39292620", "title": "Amazon Coin", "text": "Amazon Coin\n\nAmazon Coin is a digital payment method created by Amazon.com. Currently, the coins can only be used to purchase software for Kindle, Kindle Fire, and Android devices from within an app or from the Amazon Appstore.\n\nThe company introduced Amazon Coins via a promotion in the United Kingdom and the United States, consisting of giving free coins to all users of Kindle and Kindle Fire devices. Existing Kindle owners were given Coins valued $5/£4, as were customers who had ordered the Kindle Fire HDX in late 2013, upon receipt of their devices. However, in 2014 the company began allowing all Android users in Germany, the UK, and the US to earn, buy, and spend Amazon Coins via the Amazon Store via Android phones and tablets. Moreover, observes Lance Whitney writing for CNET: \"Shoppers can get discounts when they buy the coins in bulk and earn coins through certain apps\".\n\nIn 2014, with the release of the Fire Phone, Amazon offered app developers 500,000 Amazon Coins for each paid app or app with in-app purchasing developed and optimized for the Fire Phone.\n\nAmazon has called Amazon Coins a \"virtual currency\". However, the Coins operate like other digital gift cards.\n\nOne Amazon Coin is worth one cent. However, like many coupons, they cannot be redeemed for cash. However, they can be transferred to another Amazon account.\n\nPurchased Coins do not expire, but some promotional Coins expire just over one year from the date they are acquired.\n\nWhen a customer buys software with Amazon Coins, the developer is paid in conventional currency.\n"}
{"id": "2991908", "url": "https://en.wikipedia.org/wiki?curid=2991908", "title": "Area compatibility factor", "text": "Area compatibility factor\n\nIn survival analysis, the area compatibility factor, \"F\", is used in indirect standardisation of population mortality rates.\n\nwhere:\n\nThe expression can be thought of as the crude mortality rate for the standard population divided by what the crude mortality rate is for the region being studied, assuming the mortality rates are the same as for the standard population.\n\n\"F\" is then multiplied by the crude mortality rate to arrive at the indirectly standardised mortality rate.\n"}
{"id": "47286488", "url": "https://en.wikipedia.org/wiki?curid=47286488", "title": "Atom (Krauss book)", "text": "Atom (Krauss book)\n\nAtom: An Odyssey from the Big Bang to Life on Earth...and Beyond is the sixth non-fiction book by the American theoretical physicist Lawrence M. Krauss. The text was published on April 1, 2001 by Little, Brown. Krauss won the Science Writing Award (2002) for this book.\n\nIn this book Krauss discusses creating parts of an oxygen atom, the primary atoms of the Big Bang. Then he follows it through the remaining history of the Universe. As time has been passing by, the atom was a part of a supernova and star dust, star and planet systems, and, ultimately, a part of living cells.\n\nIn his review Chris Lavers of \"The Guardian\" stated \"Krauss weaves his cosmic story around the life of a single oxygen atom, from the time it was just a twinkle in the universe's eye to the eventual death of its constituent particles. This denouement may come to pass in some distant part of the cosmos long after we have all passed away, but, if we are really lucky, it may just happen in an enormous tank of minutely scrutinised water currently located down a mineshaft in Japan. If and when it does, physicists the world over will jump up and down with excitement, because they will have learned something truly profound. Exactly what would take too long to explain, which is a relief, because I'm not at all sure I understand it. Read the book and try for yourself... I am in a better position to judge Krauss's geology and biology, subjects he admits he had to learn from scratch before writing \"Atom\". Not only has he mastered them, he often finds lyrical ways of explaining ideas in both fields. Indeed, the standard of writing in \"Atom\" is perhaps even higher than in his 1995 bestseller, \"The Physics of Star Trek\".\"\n\n\n"}
{"id": "31378834", "url": "https://en.wikipedia.org/wiki?curid=31378834", "title": "Burduna language", "text": "Burduna language\n\nBurduna (also spelled \"Boordoona, Budina, Budoona, Buduna, Poodena, Poordoona, Purduma, Purduna\") and Bayungu (Payungu) is an aboriginal language, or pair of languages, of Western Australia. It is spoken in Henry and upper Lyndon rivers region. It is endangered or perhaps extinct.\n"}
{"id": "24514641", "url": "https://en.wikipedia.org/wiki?curid=24514641", "title": "Calceology", "text": "Calceology\n\nCalceology (from Latin \"calcei\" \"shoes\" and , \"-logiā\", \"-logy\") is the study of footwear, especially historical footwear whether as archaeology, shoe fashion history, or otherwise. It is not yet formally recognized as a field of research. Calceology comprises the examination, registration, research and conservation of leather shoe fragments. A wider definition includes the general study of the ancient footwear, its social and cultural history, technical aspects of pre-industrial shoemaking and associated leather trades, as well as reconstruction of archaeological footwear.\n\nAmong the early studies of footwear from European archaeological excavations, Roman period footwear figures prominently followed by medieval period finds. Scientifically based research was first applied to Roman period finds and later for prehistoric and primitive footwear. With the development of the Goubitz notation system, the technical aspects of the recovered shoe fragments could be clearly presented, allowing researchers a coherent scientific base for leather artifact documentation and correct interpretation. The interest in the history of ancient shoe fashion starts in the 17th century. The interpretation of historical socio-cultural attributes shows the importance of footwear in an archaeological context. The reference book for calceological studies covers the chronological span from European prehistory (Neolithic, Bronze and Iron Ages), Roman period, the Middle Ages to the 19th century. Calceological studies outside of Europe include the eastern coast and bays of North America for post-1600 sites, and the North African sites for Egyptian, Roman and Coptic periods.\n\nArchaeological leather artifacts are preserved in stable environments, either in constantly humid, dry or frozen sites. Peat bogs also preserve leather and skin artifacts, but through a re-tanning process. Water-logged archaeological sites provide the necessary conditions for the preservation of vegetable tanned leather. As an organic material, water-logged archaeological leather needs to be stabilized by an appropriate conservation method. Dry conditions may be found in deserts and at high altitudes but also within the walls of medieval and later period buildings where leather shoes were concealed for superstitious reasons. Ice fields, tundra and glaciers can occasionally preserve ancient leather artifacts through constant freezing.\n\nWater-logged finds generally consist of loose components since the threads used to sew the objects together does not survive humid burial. A tracking system should be used for keeping the loose components in order throughout the analysis and conservation processes. For wet archaeological leather, the first step is cleaning gently in water with a small soft brush. Conservation is preferably performed after the documentation phase. Documentation consists of drawings and written notes, photographic records are less useful since blackish leather does not show fine detail well. The first step for the Goubitz notation registration is an exact tracing of the fragment’s outline, usually positioned grain side down, flesh side up. Then symbols that indicate the type of stitches and seams are drawn in their appropriate place inside the outline. Sole constructions (the way in which the upper parts of the shoe is fixed to the sole), fastening method and ensembles of components from the same shoe as well as animal type, leather thickness, folds and creases should be noted. If present, decoration type and technique used should also be recorded.\n\nMost archaeological recovered leather artifacts are parts of footwear and may be combined with wood, fibre or metal parts. The technical details such as shoe construction technique, fastening method and fashion elements are used to establish a typology for a specific find group. Shoe type indicates the kinds of footwear such as boots, shoes, pattens, overshoes, etcetera. Shoe style is the consistent combination of a fastening method, height, fashion and decoration elements on a significant quantity of recovered shoes. Style nomenclature based on find place's name has been partly established for Roman period finds. Due to changes in fashion and the fact that shoes have a limited life span due to use, footwear is a chronologically sensitive material excavation and represents a closely dated chronological source for archeology. The find context, stratigraphic placement and other dating methods contribute to establishing a specific chronology. Further research for comparative parallel examples among the existing archaeological archives (collections, publications, reports) helps to define a relative chronology for the shoe types and styles.\n\n"}
{"id": "308803", "url": "https://en.wikipedia.org/wiki?curid=308803", "title": "Carnot's theorem (thermodynamics)", "text": "Carnot's theorem (thermodynamics)\n\nCarnot's theorem, developed in 1824 by Nicolas Léonard Sadi Carnot, also called Carnot's rule, is a principle that specifies limits on the maximum efficiency any heat engine can obtain. The efficiency of a Carnot engine depends solely on the difference between the hot and cold temperature reservoirs.\n\nCarnot's theorem states: \n\nThe formula for this maximum efficiency is\nwhere \"T\" is the absolute temperature of the cold reservoir, \"T\" is the absolute temperature of the hot reservoir, and the efficiency formula_2 is the ratio of the work done by the engine to the heat drawn out of the hot reservoir.\n\nBased on modern thermodynamics, Carnot's theorem is a result of the second law of thermodynamics. Historically, it was based on contemporary caloric theory and preceded the establishment of the second law.\n\nThe proof of the Carnot theorem is a proof by contradiction, or reductio ad absurdum, as illustrated by the figure showing two heat engines operating between two reservoirs of different temperature. The heat engine with more efficiency (formula_3) is driving a heat engine with less efficiency (formula_4), causing the latter to act as a heat pump. This pair of engines receives no outside energy, and operates solely on the energy released when heat is transferred from the hot and into the cold reservoir. However, if formula_5, then the net heat flow would be backwards, i.e., into the hot reservoir:\n\nIt is generally agreed that this is impossible because it violates the second law of thermodynamics.\n\nWe begin by verifying the values of work and heat flow depicted in the figure. First, we must point out an important caveat: the engine with less efficiency (formula_4) is being driven as a heat pump, and therefore must be a \"reversible\" engine. If the less efficient engine (formula_4) is not reversible, then the device could be built, but the expressions for work and heat flow shown in the figure would not be valid.\n\nBy restricting our discussion to cases where engine (formula_4) has less efficiency than engine (formula_3), we are able to simplify notation by adopting the convention that all symbols, formula_11 and formula_12 represent non-negative quantities (since the direction of energy flow never changes sign in all cases where formula_13). Conservation of energy demands that for each engine, the energy which enters, formula_14, must equal the energy which exits, formula_15:\n\nThe figure is also consistent with the definition of efficiency as formula_18 for both engines:\n\nIt may seem odd that a hypothetical heat pump with low efficiency is being used to violate the second law of thermodynamics, but the figure of merit for refrigerator units is not efficiency, formula_21, but the coefficient of performance (COP),\nwhich is formula_22. A reversible heat engine with low thermodynamic efficiency, formula_21 delivers more heat to the hot reservoir for a given amount of work when it is being driven as a heat pump.\n\nHaving established that the heat flow values shown in the figure are correct, Carnot's theorem may be proven for irreversible and the reversible heat engines.\n\nTo see that every reversible engine operating between reservoirs formula_24 and formula_25 must have the same efficiency, assume that two reversible heat engines have different values of formula_2, and let the more efficient engine (M) drive the less efficient engine (L) as a heat pump. As the figure shows, this will cause heat to flow from the cold to the hot reservoir without any external work or energy, which violates the second law of thermodynamics. Therefore both (reversible) heat engines have the same efficiency, and we conclude that:\n\nThis is an important result because it helps establish the Clausius theorem, which implies that the change in entropy is unique for all reversible processes.,\n\nover all paths (from a to b in \"V-T\" space). If this integral were not path independent, then entropy, S, would lose its status as a state variable.\n\nIf one of the engines is irreversible, it must be the (M) engine, placed so that it reverse drives the less efficient but reversible (L) engine. But if this irreversible engine is more efficient than the reversible engine, (i.e., if formula_28), then the second law of thermodynamics is violated. And, since the Carnot cycle represents a reversible engine, we have the first part of Carnot's theorem:\n\nThe efficiency of the engine is the work divided by the heat introduced to the system or\n\nwhere w is the work done per cycle. Thus, the efficiency depends only on q/q.\n\nBecause all reversible engines operating between the same heat reservoirs are equally efficient, all reversible heat engines operating between temperatures \"T\" and \"T\" must have the same efficiency, meaning the efficiency is a function only of the two temperatures:\nIn addition, a reversible heat engine operating between temperatures \"T\" and \"T\" must have the same efficiency as one consisting of two cycles, one between \"T\" and another (intermediate) temperature \"T\", and the second between \"T\" and \"T\". This can only be the case if\n\nSpecializing to the case that formula_24 is a fixed reference temperature: the temperature of the triple point of water. Then for any \"T\" and \"T\",\n\nTherefore, if thermodynamic temperature is defined by\n\nthen the function \"f\", viewed as a function of thermodynamic temperature, is\nand the reference temperature \"T\" has the value 273.16. (Of course any reference temperature and any positive numerical value could be used—the choice here corresponds to the Kelvin scale.)\n\nIt follows immediately that\nSubstituting Equation back into Equation gives a relationship for the efficiency in terms of temperature:\n\nSince fuel cells and batteries can generate useful power when all components of the system are at the same temperature (formula_34), they are clearly not limited by Carnot's theorem, which states that no power can be generated when formula_35. This is because Carnot's theorem applies to engines converting thermal energy to work, whereas fuel cells and batteries instead convert chemical energy to work. Nevertheless, the second law of thermodynamics still provides restrictions on fuel cell and battery energy conversion.\n"}
{"id": "7344637", "url": "https://en.wikipedia.org/wiki?curid=7344637", "title": "Desert (particle physics)", "text": "Desert (particle physics)\n\nIn the Grand Unified Theory of particle physics, the desert refers to a theorized gap in energy scales, between the TeV scale and the GUT scale, in which no interactions appear.\n\nIt can also be described as a gap in the lengths involved, with no new physics below 10 m (the currently probed length scale) and above 10 m (the GUT length scale).\n\nThe idea of the desert was motivated by the observation of approximate, order of magnitude, gauge coupling unification at the GUT scale. Adding interactions at an intermediate scale generically disrupts the gauge coupling unification.\n\nAll the Standard Model particles were discovered below the energy scale of approximately 10 eV or 1 TeV.\n\nAbove these energies, “desert theories” predict there are no particles to be discovered until reaching the scale of approximately 10 eV.\n\nThe desert theory is attractive because, in such a scenario, measurements of TeV scale physics at the LHC and the near-future ILC will allow extrapolation all the way up to the GUT scale. The downside of the particle desert is that experimental physics will simply have nothing more fundamental to discover, over a very long period of time, which, depending on the rate of increase in experiment energies, might span centuries, millennia or more. Presumably, even if the energy achieved in the LHC, ~ 10 eV, were increased by up to 12 orders of magnitude, this would only result in producing more copious amounts of the particles known today, with no underlying structure being probed. The aforementioned timespan might be shortened by observing the GUT scale through a radical development in accelerator physics, or by a non-accelerator observational technology, such as examining tremendously high energy cosmic ray events, or another, yet undeveloped technology.\n\nAlternatives to the desert exhibit particles and interactions unfolding with every few orders of magnitude increase in the energy scale.\n\nWith the Minimal Supersymmetric Standard Model, adjustment of parameters can make this unification exact. This unification is not unique.\n\nScenarios like the Katoptron model can also lead to exact unification after a similar energetic desert. If neutrino masses are due to a seesaw mechanism, the seesaw scale should lie within the desert.\n"}
{"id": "16773798", "url": "https://en.wikipedia.org/wiki?curid=16773798", "title": "Enki Catena", "text": "Enki Catena\n\nEnki Catena (\"Enki\" from the Assyro-Babylonian principal water god of the Apsu, and \"catena\" from Latin meaning \"chain\") is a crater chain on Ganymede measuring long.\n\nThis chain of 13 craters probably formed by a comet which was pulled into pieces by Jupiter's gravity as it passed too close to the planet. Soon after this breakup, the 13 fragments crashed onto Ganymede in rapid succession. The Enki craters formed across the sharp boundary between areas of bright terrain and dark terrain, delimited by a thin trough running diagonally across the center of this image. The ejecta deposit surrounding the craters appears very bright on the bright terrain. Even though all the craters formed nearly simultaneously, it is difficult to discern any ejecta deposit on the dark terrain. This may be because the impacts excavated and mixed dark material into the ejecta and the resulting mix is not apparent against the dark background.\n"}
{"id": "38812893", "url": "https://en.wikipedia.org/wiki?curid=38812893", "title": "Ergonomic glove", "text": "Ergonomic glove\n\nAn ergonomic glove, also known as a computer glove or support glove, is a stiff glove worn to prevent or remedy carpal tunnel syndrome by holding the wrist in a certain position while typing.\n\nThere are numerous types of ergonomic gloves that vary in design in multiple ways including in their firmness, whether they cover the fingers completely, and if they are intended for heavier physical use.\n\nErgonomic glove design covers many fields with even the gloves worn by healthcare professionals becoming ergonomically designed.\n"}
{"id": "15374", "url": "https://en.wikipedia.org/wiki?curid=15374", "title": "Food irradiation", "text": "Food irradiation\n\nFood irradiation is the process of exposing food and food packaging to ionizing radiation. Ionizing radiation, such as from gamma rays, x-rays or electron beams, is energy that can be transmitted without direct contact to the source of the energy (radiation) capable of freeing electrons from their atomic bonds (ionization) in the targeted food. The radiation can be emitted by a radioactive substance or generated electrically. This treatment is used to improve food safety by extending product shelf-life (preservation), reducing the risk of foodborne illness, delaying or eliminating sprouting or ripening, by sterilization of foods, and as a means of controlling insects and invasive pests. Food irradiation primarily extends the shelf-life of irradiated foods by effectively destroying organisms responsible for spoilage and foodborne illness and inhibiting sprouting. Although consumer perception of foods treated with irradiation is more negative than those processed by other means, because people imagine that the food is radioactive or mutated, all independent research, the U.S. Food and Drug Administration (FDA), the World Health Organization (WHO), the Center for Disease Control and Prevention (CDC), and U.S. Department of Agriculture (USDA) have confirmed irradiation to be safe.\n\nFood irradiation is permitted by over 60 countries, with about 500,000 metric tons of food annually processed worldwide. The regulations that dictate how food is to be irradiated, as well as the food allowed to be irradiated, vary greatly from country to country. In Austria, Germany, and many other countries of the European Union only dried herbs, spices, and seasonings can be processed with irradiation and only at a specific dose, while in Brazil all foods are allowed at any dose.\n\nIrradiation is used to reduce or eliminate the risk of food-borne illnesses, prevent or slow down spoilage, arrest maturation or sprouting and as a treatment against pests. Depending on the dose, some or all of the pathogenic organisms, microorganisms, bacteria, and viruses present are destroyed, slowed down, or rendered incapable of reproduction. Irradiation cannot return spoiled or over-ripe food to a fresh state. If this food was processed by irradiation, further spoilage would cease and ripening would slow down, yet the irradiation would not destroy the toxins or repair the texture, color, or taste of the food. When targeting bacteria, most foods are irradiated to significantly reduce the number of active microbes, not to sterilize all microbes in the product. In this respect it is similar to pasteurization.\n\nIrradiation is used to create safe foods for people at high risk of infection, or for conditions where food must be stored for long periods of time and proper storage conditions are not available. Foods that can tolerate irradiation at sufficient doses are treated to ensure that the product is completely sterilized. This is most commonly done with rations for astronauts, and special diets for hospital patients.\n\nIrradiation is used to create shelf-stable products. Since irradiation reduces the populations of spoilage microorganisms, and because pre-packed food can be irradiated, the packaging prevents recontamination of the final product.\n\nIrradiation is used to reduce post-harvest losses. It reduces populations of spoilage micro-organisms in the food and can slow down the speed at which enzymes change the food, and therefore slows spoilage and ripening, and inhibits sprouting (e.g., of potato, onion, and garlic).\n\nFood is also irradiated to prevent the spread of invasive pest species through trade in fresh vegetables and fruits, either within countries, or trade across international boundaries. Pests such as insects could be transported to new habitats through trade in fresh produce which could significantly affect agricultural production and the environment were they to establish themselves. This \"phytosanitary irradiation\" aims to render any hitch-hiking pest incapable of breeding. The pests are sterilized when the food is treated by low doses of irradiation. In general, the higher doses required to destroy pests such as insects, mealybugs, mites, moths, and butterflies either affect the look or taste, or cannot be tolerated by fresh produce. Low dosage treatments (less than 1000 gray) enables trade across quarantine boundaries and may also help reduce spoilage.\n\nIrradiation reduces the risk of infection and spoilage, does not make food radioactive, and the food is shown to be safe, but it does cause chemical reactions that alter the food and therefore alters the chemical makeup, nutritional content, and the sensory qualities of the food. Some of the potential secondary impacts of irradiation are hypothetical, while others are demonstrated. These effects include cumulative impacts to pathogens, people, and the environment due to the reduction of food quality, the transportation and storage of radioactive goods, and destruction of pathogens, changes in the way we relate to food and how irradiation changes the food production and shipping industries.\n\nThe radiation source supplies energetic particles or waves. As these waves/particles pass through a target material they collide with other particles. Around the sites of these collisions chemical bonds are broken, creating short lived radicals (e.g. the hydroxyl radical, the hydrogen atom and solvated electrons). These radicals cause further chemical changes by bonding with and or stripping particles from nearby molecules. When collisions damage DNA or RNA, effective reproduction becomes unlikely, also when collisions occur in cells, cell division is often suppressed.\n\nIrradiation (within the accepted energy limits, as 10 MeV for electrons, 5 MeV for X-rays [US 7.5 MeV] and gamma rays from Cobalt-60) can not make food radioactive, but it does produce radiolytic products, and free radicals in the food. A few of these products are unique, but not considered dangerous.\n\nIrradiation can also alter the nutritional content and flavor of foods, much like cooking. The scale of these chemical changes is not unique. Cooking, smoking, salting, and other less novel techniques, cause the food to be altered so drastically that its original nature is almost unrecognizable, and must be called by a different name. Storage of food also causes dramatic chemical changes, ones that eventually lead to deterioration and spoilage.\n\nA major concern is that irradiation might cause chemical changes that are harmful to the consumer. Several national expert groups and two international expert groups evaluated the available data and concluded that any food at any dose is wholesome and safe to consume as long as it remains palatable and maintains its technical properties (e.g. feel, texture, or color).\n\nIrradiated food does not become radioactive, just as an object exposed to light does not start producing light. Radioactivity is the ability of a substance to emit high energy particles. When particles hit the target materials they may free other highly energetic particles. This ends shortly after the end of the exposure, much like objects stop reflecting light when the source is turned off and warm objects emit heat until they cool down but do not continue to produce their own heat. To modify a material so that it keeps emitting radiation (induce radiation) the atomic cores (nucleus) of the atoms in the target material must be modified.\n\nIt is impossible for food irradiators to induce radiation in a product. Irradiators emit electrons or photons and the radiation is intrinsically radiated at precisely known strengths (wavelengths for photons, and speeds for electrons). These radiated particles at these strengths can never be strong enough to modify the nucleus of the targeted atom in the food, regardless of how many particles hit the target material, and radioactivity can not be induced without modifying the nucleus.\n\nCompounds known as free radicals form when food is irradiated. Most of these are oxidizers (i.e., accept electrons) and some react very strongly. According to the free-radical theory of aging excessive amounts of these free radicals can lead to cell injury and cell death, which may contribute to many diseases. However, this generally relates to the free radicals generated in the body, not the free radicals consumed by the individual, as much of these are destroyed in the digestive process.\nMost of the substances found in irradiated food are also found in food that has been subjected to other food processing treatments, and are therefore not unique. One family of chemicals (2ACB's) are uniquely formed by irradiation (unique radiolytic products), and this product is nontoxic. When fatty acids are irradiated, a family of compounds called 2-alkylcyclobutanones (2-ACBs) are produced. These are thought to be unique radiolytic products.\nWhen irradiating food, all other chemicals occur in a lower or comparable frequency to other food processing techniques. Furthermore, the quantities in which they occur in irradiated food are lower or similar to the quantities formed in heat treatments.\n\nThe radiation doses to cause toxic changes are much higher than the doses used during irradiation, and taking into account the presence of 2-ACBs along with what is known of free radicals, these results lead to the conclusion that there is no significant risk from radiolytic products.\n\nIonizing radiation can change food quality but in general very high levels of radiation treatment (many thousands of gray) are necessary to adversely change nutritional content, as well as the sensory qualities (taste, appearance, and texture). Irradiation to the doses used commercially to treat food have very little negative impact on the sensory qualities and nutrient content in foods. When irradiation is used to maintain food quality for a longer period of time (improve the shelf stability of some sensory qualities and nutrients) the improvement means that more consumers have access to the original taste, texture, appearance, and nutrients. The changes in quality and nutrition depend on the degree of treatment and may vary greatly from food to food.\n\nThere has been low level gamma irradiation that has been attempted on arugula, spinach, cauliflower, ash gourd, bamboo shoots, coriander, parsley, and watercress. There has been limited information, however, regarding the physical, chemical and/or bioactive properties and the shelf life on these minimally processed vegetables.\n\nThere is some degradation of vitamins caused by irradiation, but is similar to or even less than the loss caused by other processes that achieve the same result. Other processes like chilling, freezing, drying, and heating also result in some vitamin loss.\n\nThe changes in the flavor of fatty foods like meats, nuts and oils are sometimes noticeable, while the changes in lean products like fruits and vegetables are less so. Some studies by the irradiation industry show that for some properly treated fruits and vegetables irradiation is seen by consumers to improve the sensory qualities of the product compared to untreated fruits and vegetables.\n\nWatercress (\"Nasturtium Officinale\") is a rapidly growing aquatic or semi aquatic perennial plant. Because chemical agents do not provide efficient microbial reductions, watercress has been tested with gamma irradiation treatment in order to improve both safety and the shelf life of the product. It is traditionally used on horticultural products to prevent sprouting and post-packaging contamination, delay post-harvest ripening, maturation and senescence.\n\nIn a Food Chemistry food journal, scientists studied the suitability of gamma irradiation of 1, 2, and 5 kGy for preserving quality parameters of the fresh cut watercress at around 4 degrees Celsius for 7 days. They determined that a 2 kGy dose of irradiation was the dose that contained most similar qualities to non-stored control samples, which is one of the goals of irradiation. 2 kGy preserved high levels of reducing sugars and favoured PUFA; while samples of the 5 kGy dose revealed high contents of sucrose and MUFA. Both cases the watercress samples obtained healthier fatty acids profiles. However, a 5kGy dose better preserved the antioxidant activity and total flavonoids.\n\nIf the majority of food was irradiated at high-enough levels to significantly decrease its nutritional content, there would be an increased risk of developing nutritionally-based illnesses if additional steps, such as changes in eating habits, were not taken to mitigate this. Furthermore, for at least three studies on cats, the consumption of irradiated food was associated with a loss of tissue in the myelin sheath, leading to reversible paralysis. Researchers suspect that reduced levels of vitamin A and high levels of free radicals may be the cause. This effect is thought to be specific to cats and has not been reproduced in any other animal. To produce these effects, the cats were fed solely on food that was irradiated at a dose at least five times higher than the maximum allowable dose.\n\nIt may seem reasonable to assume that irradiating food might lead to radiation-tolerant strains, similar to the way that strains of bacteria have developed resistance to antibiotics. Bacteria develop a resistance to antibiotics after an individual uses antibiotics repeatedly. Much like pasteurization plants, products that pass through irradiation plants are processed once, and are not processed and reprocessed. Cycles of heat treatment have been shown to produce heat-tolerant bacteria, yet no problems have appeared so far in pasteurization plants. Furthermore, when the irradiation dose is chosen to target a specific species of microbe, it is calibrated to doses several times the value required to target the species. This ensures that the process randomly destroys all members of a target species. Therefore, the more irradiation-tolerant members of the target species are not given any evolutionary advantage. Without evolutionary advantage, selection does not occur. As to the irradiation process directly producing mutations that lead to more virulent, radiation-resistant strains, the European Commission's Scientific Committee on Food found that there is no evidence; on the contrary, irradiation has been found to cause loss of virulence and infectivity, as mutants are usually less competitive and less adapted.\n\nSome who advocate against food irradiation argue the safety of irradiated food is not scientifically proven because there are a lack of long-term studies in spite of the fact that hundreds of animal feeding studies of irradiated food, including multigenerational studies, have been performed since 1950. Endpoints investigated have included subchronic and chronic changes in metabolism, histopathology, function of most systems, reproductive effects, growth, teratogenicity, and mutagenicity. A large number of studies have been performed; meta-studies have supported the safety of irradiated food.\n\nThe below experiments are cited by food irradiation opponents, but either could not be verified in later experiments, could not be clearly attributed to the radiation effect, or could be attributed to an inappropriate design of the experiment.\n\nThe indirect effects of irradiation are the concerns and benefits of irradiation that are related to how making food irradiation a common process will change the world, with emphasis on the system of food production.\n\nIf irradiation was to become common in the food handling process there would be a reduction of the prevalence of foodborne illness and potentially the eradication of specific pathogens. However, multiple studies suggest that an increased rate of pathogen growth may occur when irradiated food is cross-contaminated with a pathogen, as the competing spoilage organisms are no longer present. This being said, cross contamination itself becomes less prevalent with an increase in usage of irradiated foods.\n\nThe ability to remove bacterial contamination through post-processing by irradiation may reduce the fear of mishandling food which could cultivate a cavalier attitude toward hygiene and result in contaminants other than bacteria. However, concerns that the pasteurization of milk would lead to increased contamination of milk were prevalent when mandatory pasteurization was introduced, but these fears never materialized after adoption of this law. Therefore, it is unlikely for irradiation to cause an increase of illness due to nonbacteria-based contamination.\n\nUp to the point where the food is processed by irradiation, the food is processed in the same way as all other food. To treat the food, they are exposed to a radioactive source, for a set period of time to achieve a desired dose. Radiation may be emitted by a radioactive substance, or by X-ray and electron beam accelerators. Special precautions are taken to ensure the food stuffs never come in contact with the radioactive substances and that the personnel and the environment are protected from exposure radiation.\nIrradiation treatments are typically classified by dose (high, medium, and low), but are sometimes classified by the effects of the treatment (radappertization, radicidation and radurization). Food irradiation is sometimes referred to as \"cold pasteurization\" or \"electronic pasteurization\" because ionizing the food does not heat the food to high temperatures during the process, and the effect is similar to heat pasteurization. The term \"cold pasteurization\" is controversial because the term may be used to disguise the fact the food has been irradiated and pasteurization and irradiation are fundamentally different processes.\n\nTreatment costs vary as a function of dose and facility usage. A pallet or tote is typically exposed for several minutes to hours depending on dose. Low-dose applications such as disinfestation of fruit range between US$0.01/lbs and US$0.08/lbs while higher-dose applications can cost as much as US$0.20/lbs.\n\nFood processors and manufacturers today struggle with using affordable, efficient packaging materials for irradiation based processing. The implementation of irradiation on prepackaged foods has been found to impact foods by inducing specific chemical alterations to the food packaging material that migrates into the food. Cross-linking in various plastics can lead to physical and chemical modifications that can increase the overall molecular weight. On the other hand, chain scission is fragmentation of polymer chains that leads to a molecular weight reduction.\n\nThe radiation absorbed dose is the amount energy absorbed per unit weight of the target material. Dose is used because, when the same substance is given the same dose, similar changes are observed in the target material. The SI unit for dose is grays (Gy or J/kg). Dosimeters are used to measure dose, and are small components that, when exposed to ionizing radiation, change measurable physical attributes to a degree that can be correlated to the dose received. Measuring dose (dosimetry) involves exposing one or more dosimeters along with the target material.\n\nFor purposes of legislation doses are divided into low (up to 1 kGy), medium (1 kGy to 10 kGy), and high-dose applications (above 10 kGy). High-dose applications are above those currently permitted in the US for commercial food items by the FDA and other regulators around the world. Though these doses are approved for non commercial applications, such as sterilizing frozen meat for NASA astronauts (doses of 44 kGy) and food for hospital patients.\n\nGamma irradiation is produced from the radioisotopes cobalt-60 and caesium-137, which are derived by neutron bombardment of cobalt-59 and as a nuclear source by-product, respectively. Cobalt-60 is the most common source of gamma rays for food irradiation in commercial scale facilities as it is water insoluble and hence has little risk of environmental contamination by leakage into the water systems. As for transportation of the radiation source, cobalt-60 is transported in special trucks that prevent release of radiation and meet standards mentioned in the Regulations for Safe Transport of Radioactive Materials of the International Atomic Energy Act. The special trucks must meet high safety standards and pass extensive tests to be approved to ship radiation sources. Conversely, caesium-137, is water soluble and poses a risk of environmental contamination. Insufficient quantities are available for large scale commercial use. An incident where water-soluble caesium-137 leaked into the source storage pool requiring NRC intervention has led to near elimination of this radioisotope.\n\nGamma irradiation is widely used due to its high penetration depth and dose uniformity, allowing for large-scale applications with high through puts. Additionally, gamma irradiation is significantly less expensive than using an X-ray source In most designs, the radioisotope, contained in stainless steel pencils, is stored in a water-filled storage pool which absorbs the radiation energy when not in use. For treatment, the source is lifted out of the storage tank, and product contained in totes is passed around the pencils to achieve required processing.\n\nTreatment of electron beams is created as a result of high energy electrons in an accelerator that generates electrons accelerated to 99% the speed of light. This system uses electrical energy and can be powered on and off. The high power correlates with a higher throughput and lower unit cost, but electron beams have low dose uniformity and a penetration depth of centimeters. Therefore, electron beam treatment works for products that have low thickness.\nX-rays are produced by bombardment of dense target material with high energy accelerated electrons(this process is known as bremsstrahlung-conversion), giving rise to a continuous energy spectrum. Heavy metals, such as tantalum and tungsten, are used because of their high atomic numbers and high melting temperatures.Tantalum is usually preferred versus tungsten for industrial, large-area, high-power targets because it is more workable than tungsten and has a higher threshold energy for induced reactions. Like electron beams, x-rays do not require the use of radioactive materials and can be turned off when not in use. X-rays have high penetration depths and high dose uniformity but they are a very expensive source of irradiation as only 8% of the incident energy is converted into X-rays.\n\nThe cost of food irradiation is influenced by dose requirements, the food's tolerance of radiation, handling conditions, i.e., packaging and stacking requirements, construction costs, financing arrangements, and other variables particular to the situation. Irradiation is a capital-intensive technology requiring a substantial initial investment, ranging from $1 million to $5 million. In the case of large research or contract irradiation facilities, major capital costs include a radiation source, hardware (irradiator, totes and conveyors, control systems, and other auxiliary equipment), land (1 to 1.5 acres), radiation shield, and warehouse. Operating costs include\n\nsalaries (for fixed and variable labor), utilities, maintenance, taxes/insurance, cobalt-60 replenishment, general utilities, and miscellaneous operating costs. Perishable food items, like fruits, vegetables and meats would still require to be handled in the cold chain, so all other supply chain costs remain the same.\n\nNegative connotations associated with the word \"radiation\" are thought to be responsible for low consumer acceptance. Several national expert groups and two international expert groups evaluated the available data and concluded that any food at any dose is wholesome and safe to consume.\n\nIrradiation has been approved by many countries. For example, in the U.S. the FDA has approved food irradiation for over fifty years. However, in the past decade the major growth area is for fruits and vegetables that are irradiated to prevent the spread of pests. In the early 2000s in the US, irradiated meat was common at some grocery stores, but because of lack of consumer demand, it is no longer common. Because consumer demand for irradiated food is low, reducing the spoilage between manufacturer and consumer purchase and reducing the risk of food borne illness is currently not sufficient incentive for most manufacturers to supplement their process with irradiation. Nevertheless, food irradiation does take place commercially and volumes are in general increasing at a slow rate, even in the European Union where all member countries allow the irradiation of dried herbs spices and vegetable seasonings but only a few allow other foods to be sold as irradiated.\n\nAlthough there are some consumers who choose not to purchase irradiated food, a sufficient market has existed for retailers to have continuously stocked irradiated products for years. When labeled irradiated food is offered for retail sale, these consumers buy it and re-purchase it, indicating that it is possible to successfully market irradiated foods, therefore retailers not stocking irradiated foods might be a major bottleneck to the wider adoption of irradiated foods. It is however, widely believed that consumer perception of foods treated with irradiation is more negative than those processed by other means and some industry studies indicate the number of consumers concerned about the safety of irradiated food decreased between 1985 and 1995 to levels comparable to those of people concerned about food additives and preservatives. Even though is it is untrue \"People think the product is radioactive,\" said Harlan Clemmons, president of Sadex, a food irradiation company based in Sioux City, Iowa. Because of these concerns and the increased cost of irradiated foods, there is not a widespread public demand for the irradiation of foods for human consumption. Irradiated food does not become radioactive.\n\nThe Codex Alimentarius represents the global standard for irradiation of food, in particular under the WTO-agreement. Regardless of treatment source, all processing facilities must adhere to safety standards set by the International Atomic Energy Agency (IAEA), Codex Code of Practice for the Radiation Processing of Food, Nuclear Regulatory Commission (NRC), and the International Organization for Standardization (ISO). More specifically, ISO 14470 and ISO 9001 provide in-depth information regarding safety in irradiation facilities.\n\nAll commercial irradiation facilities contain safety systems are designed to prevent exposure of personnel to radiation. The radiation source is constantly shielded by water, concrete, or metal. Irradiation facilities are designed with overlapping layers of protection, interlocks, and safeguards to prevent accidental radiation exposure. Additionally, \"melt-downs\" do not occur in facilities because the radiation source gives off radiation and decay heat; however, the heat is not sufficient to melt any material.\n\nThe provisions of the Codex Alimentarius are that any \"first generation\" product must be labeled \"irradiated\" as any product derived directly from an irradiated raw material; for ingredients the provision is that even the last molecule of an irradiated ingredient must be listed with the ingredients even in cases where the unirradiated ingredient does not appear on the label. The RADURA-logo is optional; several countries use a graphical version that differs from the Codex-version. The suggested rules for labeling is published at CODEX-STAN – 1 (2005), and includes the usage of the Radura symbol for all products that contain irradiated foods. The Radura symbol is not a designator of quality. The amount of pathogens remaining is based upon dose and the original content and the dose applied can vary on a product by product basis.\n\nThe European Union follows the Codex's provision to label irradiated ingredients down to the last molecule of irradiated food. The European Community does not provide for the use of the Radura logo and relies exclusively on labeling by the appropriate phrases in the respective languages of the Member States. The European Union enforces its irradiation labeling laws by requiring its member countries to perform tests on a cross section of food items in the market-place and to report to the European Commission. The results are published annually in the OJ of the European Communities.\n\nThe US defines irradiated foods as foods in which the irradiation causes a material change in the food, or a material change in the consequences that may result from the use of the food. Therefore, food that is processed as an ingredient by a restaurant or food processor is exempt from the labeling requirement in the US. All irradiated foods must include a prominent Radura symbol followed in addition to the statement \"treated with irradiation\" or \"treated by irradiation. Bulk foods must be individually labeled with the symbol and statement or, alternatively, the Radura and statement should be located next to the sale container.\n\nUnder section 409 of the Federal Food, Drug, and Cosmetic Act, irradiation of prepackaged foods requires premarket approval for not only the irradiation source for a specific food but also for the food packaging material. Approved packaging materials include various plastic films, yet does not cover a variety of polymers and adhesive based materials that have been found to meet specific standards. The lack of packaging material approval limits manufacturers production and expansion of irradiated prepackaged foods.\n\nApproved materials by FDA for Irradiation according to 21 CFR 179.45:\nIn 2003, the Codex Alimentarius removed any upper dose limit for food irradiation as well as clearances for specific foods, declaring that all are safe to irradiate. Countries such as Pakistan and Brazil have adopted the Codex without any reservation or restriction.\n\nStandards that describe calibration and operation for radiation dosimetry, as well as procedures to relate the measured dose to the effects achieved and to report and document such results, are maintained by the American Society for Testing and Materials (ASTM international) and are also available as ISO/ASTM standards.\n\nAll of the rules involved in processing food are applied to all foods before they are irradiated.\n\nThe U.S. Food and Drug Administration (FDA) is the agency responsible for regulation of radiation sources in the United States. Irradiation, as defined by the FDA is a \"food additive\" as opposed to a food process and therefore falls under the food additive regulations. Each food approved for irradiation has specific guidelines in terms of minimum and maximum dosage as determined safe by the FDA. Packaging materials containing the food processed by irradiation must also undergo approval. The United States Department of Agriculture (USDA) amends these rules for use with meat, poultry, and fresh fruit.\n\nThe United States Department of Agriculture (USDA) has approved the use of low-level irradiation as an alternative treatment to pesticides for fruits and vegetables that are considered hosts to a number of insect pests, including fruit flies and seed weevils. Under bilateral agreements that allows less-developed countries to earn income through food exports agreements are made to allow them to irradiate fruits and vegetables at low doses to kill insects, so that the food can avoid quarantine.\n\nEuropean law dictates that all member countries must allow the sale of irradiated dried aromatic herbs, spices and vegetable seasonings. However, these Directives allow Member States to maintain previous clearances food categories the EC's Scientific Committee on Food (SCF) had previously approved (the approval body is now the European Food Safety Authority). Presently, Belgium, Czech Republic, France, Italy, Netherlands, Poland, and the United Kingdom allow the sale of many different types of irradiated foods. Before individual items in an approved class can be added to the approved list, studies into the toxicology of each of such food and for each of the proposed dose ranges are requested. It also states that irradiation shall not be used \"as a substitute for hygiene or health practices or good manufacturing or agricultural practice\". These Directives only control food irradiation for food retail and their conditions and controls are not applicable to the irradiation of food for patients requiring sterile diets.\n\nBecause of the Single Market of the EC any food, even if irradiated, must be allowed to be marketed in any other Member State even if a general ban of food irradiation prevails, under the condition that the food has been irradiated legally in the state of origin.\nFurthermore, imports into the EC are possible from third countries if the irradiation facility had been inspected and approved by the EC and the treatment is legal within the EC or some Member state.\n\nAustralia banned irradiated cat food after a national scare where cats suffered from paralyzation after eating a specific brand of highly irradiated catfood for an extended period of time. The suspected culprit was malnutrition from consuming food depleted of Vitamin A by the irradiation process. The incident was linked only to a single batch of one brand's product and no illness was linked to any of that brand's other irradiated batches of the same product or to any other brand of irradiated cat food. This, along with incomplete evidence indicating that the cat food was not sufficiently depleted of Vitamin A makes irradiation a less likely cause. Further research has been able to experimentally induce the paralyzation of cats by via Vitamin A deficiency by feeding highly irradiated food. For more details see the Long term impacts section.\n\nInterlocks and safeguards are mandated to minimize this risk. There have been radiation-related accidents, deaths, and injury at such facilities, many of them caused by operators overriding the safety related interlocks. In a radiation processing facility, radiation specific concerns are supervised by special authorities, while \"Ordinary\" occupational safety regulations are handled much like other businesses.\n\nThe safety of irradiation facilities is regulated by the United Nations International Atomic Energy Agency and monitored by the different national Nuclear Regulatory Commissions. The regulators enforce a safety culture that mandates that all incidents that occur are documented and thoroughly analyzed to determine the cause and improvement potential. Such incidents are studied by personnel at multiple facilities, and improvements are mandated to retrofit existing facilities and future design.\n\nIn the US the Nuclear Regulatory Commission (NRC) regulates the safety of the processing facility, and the United States Department of Transportation (DOT) regulates the safe transport of the radioactive sources.\n\nAs of 2010, the quantities of foods irradiated in Asia, the EU and the US were 285,200, 9,300, and 103,000 tons. Authorities in some countries use tests that can detect the irradiation of food items to enforce labeling standards and to bolster consumer confidence. The European Union monitors the market to determine the quantity of irradiated foods, if irradiated foods are labeled as irradiated, and if the irradiation is performed at approved facilities.\n\nIrradiation of fruits and vegetables to prevent the spread of pest and diseases across borders has been increasing globally. In 2010, 18,446 tonnes of fruits and vegetables were irradiated in six countries for export quarantine control. 97% of this was exported to the United States.\n\nIn total, 103 000 tonnes of food products were irradiated on mainland United States in 2010. The three types of foods irradiated the most were spices (77.7%), fruits and vegetables (14.6%) and meat and poultry (7.77%). 17 953 tonnes of irradiated fruits and vegetables were exported to the mainland United States. Mexico, the United States' state of Hawaii, Thailand, Vietnam and India export irradiated produce to the mainland U.S. Mexico, followed by the United States' state of Hawaii, is the largest exporter of irradiated produce to the mainland U.S.\n\nIn total, 6 876 tonnes of food products were irradiated in European Union countries in 2013; mainly in four member state countries: Belgium (49.4%), the Netherlands (24.4%), Spain (12.7%) and France (10.0%). The two types of foods irradiated the most were frog legs (46%), and dried herbs and spices (25%). There has been a decrease of 14% in the total quantity of products irradiated in the EU compared to the previous year 2012 (7 972 tonnes).\n\nThe U.S. Food and Drug Administration and the U.S. Department of Agriculture have approved irradiation of the following foods and purposes:\n\n\n\n\n\n"}
{"id": "3694774", "url": "https://en.wikipedia.org/wiki?curid=3694774", "title": "Formal science", "text": "Formal science\n\nFormal sciences are formal language disciplines concerned with formal systems, such as logic, mathematics, statistics, theoretical computer science, robotics, information theory, game theory, systems theory, decision theory, and theoretical linguistics. Strictly speaking, formal science is not science but a variety of fundamentally abstract logical systems that are applied to both the natural world and human constructs. Whereas the natural sciences and social sciences seek to characterize physical systems and social systems, respectively, using empirical methods, the formal sciences are language tools concerned with characterizing abstract structures described by sign systems. The formal sciences aid the natural and social sciences by providing information about the structures the latter use to describe the world, and what inferences may be made about them.\n\nFormal sciences began before the formulation of the scientific method, with the most ancient mathematical texts dating back to 1800 BC (Babylonian mathematics), 1600 BC (Egyptian mathematics) and 1000 BC (Indian mathematics). From then on different cultures such as the Indian, Greek, Arab and Persian made major contributions to mathematics, while the Chinese and Japanese, independently of more distant cultures, developed their own mathematical tradition.\n\nBesides mathematics, logic is another example of one of oldest subjects in the field of the formal sciences. As an explicit analysis of the methods of reasoning, logic received sustained development originally in three places: India from the , China in the , and Greece between the and the . The formally sophisticated treatment of modern logic descends from the Greek tradition, being informed from the transmission of Aristotelian logic, which was then further developed by Islamic logicians. The Indian tradition also continued into the early modern period. The native Chinese tradition did not survive beyond antiquity, though Indian logic was later adopted in medieval China.\n\nAs a number of other disciplines of formal science rely heavily on mathematics, they did not exist until mathematics had developed into a relatively advanced level. Pierre de Fermat and Blaise Pascal (1654), and Christiaan Huygens (1657) started the earliest study of probability theory. In the early 1800s, Gauss and Laplace developed the mathematical theory of statistics, which also explained the use of statistics in insurance and governmental accounting. Mathematical statistics was recognized as a mathematical discipline in the early 20th century.\n\nIn the mid-20th century, mathematics was broadened and enriched by the rise of new mathematical sciences and engineering disciplines such as operations research and systems engineering. These sciences benefited from basic research in electrical engineering and then by the development of electrical computing, which also stimulated information theory, numerical analysis (scientific computing), and theoretical computer science. Theoretical computer science also benefits from the discipline of mathematical logic, which included the theory of computation.\n\nAs opposed to empirical sciences (natural and social), the formal sciences do not involve empirical procedures. They also do not presuppose knowledge of contingent facts, or describe the real world. In this sense, formal sciences are both logically and methodologically a priori, for their content and validity are independent of any empirical procedures. \n\nTherefore, straightly speaking, formal science is not a science. It is a formal logical system with its content targeted at the real things, information and thoughts that we experienced. As Francis Bacon pointed out in the 17th century, experimental verification of the propositions must be carried out rigorously and cannot take logic itself as the way to draw conclusions in nature. Formal science is a method that is helpful to science but cannot replace science.\n\nAlthough formal sciences are conceptual systems, lacking empirical content, this does not mean that they have no relation to the real world. But this relation is such that their formal statements hold in all possible conceivable worlds – whereas, statements based on empirical theories, such as, say, general relativity or evolutionary biology, do not hold in all possible worlds, and may eventually turn out not to hold in this world as well. That is why formal sciences are applicable in all domains and useful in all empirical sciences.\n\nBecause of their non-empirical nature, formal sciences are construed by outlining a set of axioms and definitions from which other statements (theorems) are deduced. In other words, theories in formal sciences contain no synthetic statements; all their statements are analytic.\n\n\n\n"}
{"id": "29860765", "url": "https://en.wikipedia.org/wiki?curid=29860765", "title": "Gerhard Haszprunar", "text": "Gerhard Haszprunar\n\nGerhard Haszprunar (born 25 February 1957 in Vienna) is Austrian zoologist and malacologist.\n\nHe is credited with the invention of the modern species naming patrongage model and is a founder of the BIOPAT non-profit organization.\n\n"}
{"id": "18126925", "url": "https://en.wikipedia.org/wiki?curid=18126925", "title": "Hanny's Voorwerp", "text": "Hanny's Voorwerp\n\nHanny's Voorwerp (HsV) is about the size of a small galaxy and has a central hole over 16,000 light years across. In an image taken with the HST, HsV is colored green, a standard false color that is used to represent the presence of several luminous emission lines of glowing oxygen. HsV has been shown to be at the same distance from Earth as the adjacent galaxy IC 2497, which is about 650 million light-years away. \nStar birth is occurring in the region of HsV that faces IC 2497. Radio observations indicate that this is due to an outflow of gas arising from the IC 2497's core which is interacting with a small region of HsV to collapse and form stars. The youngest of these stars are several million years old.\n\nA 40-page comic and associated promotional offers about HsV and the story surrounding it were presented at the 24th Dragon Con in Atlanta on 3 September 2010, as well as first pictures of HsV from the Hubble Space Telescope. The launch was streamed live on UStream.\n\nOne hypothesis suggests that HsV consists of remnants of a small galaxy showing the impact of radiation from a bright quasar event that occurred in the center of IC 2497 about 100,000 years before how it is observed today. The quasar event is thought to have stimulated the bright emission that characterizes HsV. The quasar might have switched off in the last 200,000 years and is not visible in the available images. This might well be due to a process known as AGN feedback.\n\nOne possible explanation for the missing light-source is that illumination from the assumed quasar was a transient phenomenon. In this case, its effects on HsV would be still visible because of the distance of several tens of thousands of light years between HsV and the quasar in the nearby galaxy: HsV would show a \"light echo\" or \"ghost image\" of events that are older than those currently seen in the galaxy.\n\nOn 17 June 2010, a group of researchers at the European VLBI Network (EVN) and the UK’s Multi-Element Radio Linked Interferometer Network (MERLIN), proposed another related explanation. They hypothesized that the light comes from two sources: (1) a supermassive black hole at the center of IC 2497, and (2) light produced by an interaction of an energetic jet from the black hole and the gas surrounding IC 2497.\n\nIn February 2012, W. C. Keel and others published a paper in the \"Monthly Notices of the Royal Astronomical Society\". As a result of the interest in similar ionized clouds for the study of both the history and obscuration of Active Galactic Nucleus (AGN), participants in the Galaxy Zoo (GZ) project carried out a wide search for such clouds using data from the Sloan Digital Sky Survey (SDSS). This search yielded a list of 19 galaxies with AGN-photoionized clouds detected to beyond 10 kiloparsecs from the nuclei. These were nicknamed 'Voorwerpjes' from the Dutch for 'small objects'.\nIn May 2015, W.C. Keel and others published a study in the \"Astrophysical Journal\". This studies 8 of the original 19 Voorwerpjes in greater detail, focusing on 'the host-galaxy properties and origin of the gas.' Among the telescopes used was the 6 meter BTA-6 at the Special Astrophysical Observatory of the Russian Academy of Science.\n\nIn August 2013, F. Schweizer and others published a paper in the \"Astrophysical Journal\". This reports the finding of a Voorwerpje on the outskirts of the well-studied NGC 7252.\n\nIn April 2016, a study was published in the \"Monthly Notices of the Royal Astronomical Society\" using data gathered by the Chandra X-ray Observatory in January 2012. The study found extended soft X-ray emission in IC 2497 which suggested the presence of a bubble or cavity surrounding the AGN. The authors hypothesize that this could be due to the bubble being inflated by the AGN, or by a past luminous quasar.\n\n\n\n"}
{"id": "34684337", "url": "https://en.wikipedia.org/wiki?curid=34684337", "title": "Harry Allan", "text": "Harry Allan\n\nHarry Howard Barton Allan (27 April 1882 – 29 October 1957) was a New Zealand teacher, botanist, scientific administrator and writer.\n\nAllan was born on 27 April 1882 in Nelson, and was educated at Nelson College, and Auckland University College, from where he graduated MA in 1908. For his lengthy botanical study of Mount Peel, he was awarded a Doctor of Science in 1923.\n\nIn the 1948 King's Birthday Honours Allan was appointed a Commander of the Order of the British Empire for services to botany in New Zealand.\n"}
{"id": "16378988", "url": "https://en.wikipedia.org/wiki?curid=16378988", "title": "Hyper Articles en Ligne", "text": "Hyper Articles en Ligne\n\nHyper Articles en Ligne, generally shortened to HAL, is an open archive where authors can deposit scholarly documents from all academic fields. It has a good position in the international web repository ranking.\n\nHAL is run by the \"Centre pour la communication scientifique directe\", a French computing centre, which is part of the French National Centre for Scientific Research, CNRS. Other French institutions, such as INRIA, have joined the system. While it is primarily directed towards French academics, participation is not restricted to them.\n\nDocuments in HAL are uploaded either by one of the authors with the consent of the others or by an authorized person on their behalf. Since 2017 it's also possible to use Dissem.in, a tool for easy and semi-automated deposit.\n\nHAL is a tool for direct scientific communication between academics. A text posted to HAL is normally comparable to that of a paper that an investigator might submit for publication in a peer-reviewed scientific journal or conference proceedings. A document deposited in HAL will not be subjected to any detailed scientific evaluation, but simply a rapid overview, to ensure that it falls within the category defined above.\n\nAn uploaded document does not need to have been published or even to be intended for publication: It may be posted to HAL as long as its scientific content justifies it. But should the article be published, contributors are invited to indicate the relevant bibliographic information and the digital object identifier (DOI).\n\nHAL aims to ensure the long term preservation of the deposited documents that are stored there permanently and will receive a stable web address. Thus, like any publication in a traditional scientific journal, it can be cited in other work.\n\nThe free online access to these documents provided by HAL is intended to promote the best possible dissemination of research work; the intellectual property remains that of the authors.\n\nFor physics, mathematics and other natural science topics, HAL has an automated depositing agreement with arXiv. A similar agreement exists with PubMed Central for biomedical topics.\n\nOver 120 institutions have their own entrance to HAL, called portals. HAL hosts institutional repositories (for universities, research organisms and units) as well as subject repositories ; one example is the Arts and Humanities eprint repository, hprints.\n\nAs an open access repository, HAL complies with the \"Open Archives Initiative\" (OAI-PMH) as well as with the European OpenAIRE project.\n\n\n\n"}
{"id": "1976484", "url": "https://en.wikipedia.org/wiki?curid=1976484", "title": "IEEE Transactions on Information Theory", "text": "IEEE Transactions on Information Theory\n\nIEEE Transactions on Information Theory is a monthly peer-reviewed scientific journal published by the IEEE Information Theory Society. It covers information theory and the mathematics of communications. It was established in 1953 as \"IRE Transactions on Information Theory\". The editor-in-chief is Prakash Narayan (University of Maryland, College Park). As of 2007, the journal allows the posting of preprints on arXiv.\n"}
{"id": "53708683", "url": "https://en.wikipedia.org/wiki?curid=53708683", "title": "Initiative for Open Citations", "text": "Initiative for Open Citations\n\nThe Initiative for Open Citations (I4OC) is a project launched publicly in April 2017, that describes itself as:\n\nIt is intended to facilitate improved citation analysis.\n\nThe citations are stored in Crossref and are made available through the Crossref REST API. They are also available from the OpenCitations Corpus, a database that harvests citation data from Crossref and other sources. The data are considered by the those involved in the Initiative to be in the public domain, and so a CC0 licence is used.\n\nThe stated benefits of this approach are:\n\n\nThe initiative was established in response to a paper on citations in Wikidata, \"Citations needed for the sum of all human knowledge: Wikidata as the missing link between scholarly publishing and linked open data\", given by Dario Taraborelli, head of research at the Wikimedia Foundation, at the eighth Conference on Open Access Scholarly Publishing, in September 2016. At that time, only 1% of papers in Crossref had citations metadata that were freely available. By the time of the public launch, on 6 April 2017, that had risen to 40% as a result of setting up the initiative.\n\nThe founding partners were:\n\n\nAt the time of launch, 64 organisations, including the Wellcome Trust, the Bill And Melinda Gates Foundation and the Alfred P. Sloan Foundation, had endorsed the project and as of May, 2017, Sloan Foundation confirmed it would be providing funding. 29 of these organisations were publishers who had agreed to share their citation metadata openly. These include Springer Nature, Taylor & Francis, and Wiley. A notable exception was Elsevier, who contribute 30% of the citation metadata in Crossref. Elsevier's vice-president of corporate relations, Tom Reller, said:\n\nOn 11 July 2017, the Initiative announced that a further sixteen publishers had signed up. On 8 August 2017, the Initiative released on open letter to stakeholders. The same month, the British Library became a member organisation.\n\n"}
{"id": "658606", "url": "https://en.wikipedia.org/wiki?curid=658606", "title": "Jay C. Buckey", "text": "Jay C. Buckey\n\nJay Clark Buckey, Jr. (born June 6, 1956, in New York City) is an American physician and astronaut who flew aboard one space shuttle mission (STS-90) as a Payload Specialist. Buckey briefly ran for the Democratic nomination to challenge New Hampshire Senator John E. Sununu, a first term Republican, when he was up for re-election in 2008. Buckey withdrew from the race when former Governor Jeanne Shaheen entered the race.\n\nBuckey holds a Bachelor of Science degree in Electrical Engineering from Cornell University (1977) and an M.D. from Cornell in 1981, interning at New York Hospital-Cornell Medical Center and completing his residence at Dartmouth-Hitchcock Medical Center. Currently, Buckey is a Professor of Medicine at the Dartmouth Medical School. He was also a flight surgeon with the U.S. Air Force Reserve for 8 years.\n\nIn 1998 he was a Payload Specialist aboard NASA Space Shuttle flight STS-90 as part of the Neurolab mission from April 17 to May 3, 1998. Aboard the Neurolab Mission, Buckey was the Payload Specialist for the experiment \"Cardiovascular Adaptation to Zero-Gravity\" and assisted with other Spacelab Life Sciences experiments. During the 16-day Spacelab flight, the seven person crew aboard Space Shuttle Columbia served as both experiment subjects and operators for 26 individual life science experiments focusing on the effects of microgravity on the brain and nervous system. The STS-90 flight orbited the Earth 256 times, covered 6.3 million miles, and logged him over 381 hours in space.\n\nIn 2018, Buckey was part of research using virtual reality, at the Australian Antarctic Division’s Mawson Station, wherein the expeditioners used VR headsets to view Australian beach scenes, European nature scenes, and North American nature scenes of forests and urban environments, which were different from the isolation of the whiteness and silence of Antarctica. The research will inform psychological techniques to support long duration spaceflight such as for astronauts going to Mars.\n\n"}
{"id": "43249770", "url": "https://en.wikipedia.org/wiki?curid=43249770", "title": "Knowledge acquisition", "text": "Knowledge acquisition\n\nKnowledge acquisition is the process used to define the rules and ontologies required for a knowledge-based system. The phrase was first used in conjunction with expert systems to describe the initial tasks associated with developing an expert system, namely finding and interviewing domain experts and capturing their knowledge via rules, objects, and frame-based ontologies. \n\nExpert systems were one of the first successful applications of artificial intelligence technology to real world business problems. Researchers at Stanford and other AI laboratories worked with doctors and other highly skilled experts to develop systems that could automate complex tasks such as medical diagnosis. Until this point computers had mostly been used to automate highly data intensive tasks but not for complex reasoning. Technologies such as inference engines allowed developers for the first time to tackle more complex problems. \n\nAs expert systems scaled up from demonstration prototypes to industrial strength applications it was soon realized that the acquisition of domain expert knowledge was one of if not the most critical task in the knowledge engineering process. This knowledge acquisition process became an intense area of research on its own. One of the earlier works on the topic used Batesonian theories of learning to guide the process.\n\nOne approach to knowledge acquisition investigated was to use natural language parsing and generation to facilitate knowledge acquisition. Natural language parsing could be performed on manuals and other expert documents and an initial first pass at the rules and objects could be developed automatically. Text generation was also extremely useful in generating explanations for system behavior. This greatly facilitated the development and maintenance of expert systems.\n\nA more recent approach to knowledge acquisition is a re-use based approach. Knowledge can be developed in ontologies that conform to standards such as the Web Ontology Language (OWL). In this way knowledge can be standardized and shared across a broad community of knowledge workers. One example domain where this approach has been successful is bioinformatics. \n"}
{"id": "22419272", "url": "https://en.wikipedia.org/wiki?curid=22419272", "title": "LRGB", "text": "LRGB\n\nLRGB, short for Luminance, Red, Green and Blue, is a photographic technique used in amateur astronomy for producing good quality color photographs by combining a high-quality black-and-white image with a lower-quality color image.\n\nIn amateur astronomy, it is easier and cheaper to obtain good quality, high signal-to-noise ratio images in black and white. The LRGB method is used to work around this to get good color images. The color information from the color image is combined with the overall brightness from the black-and-white image.\n\nThe theory behind the effectiveness behind LRGB techniques has been related to the way humans see color. The rods in human eyes are sensitive to luminance and spatial data, while the cones are sensitive to color. There are three types of cones: those sensitive to red, those sensitive to green, and those sensitive to blue. Thus, each element of LRGB targets one type of photoreceptor cell in the human eye. \n\n"}
{"id": "6851367", "url": "https://en.wikipedia.org/wiki?curid=6851367", "title": "Langer correction", "text": "Langer correction\n\nThe Langer correction is a correction when the WKB approximation method is applied to three-dimensional problems with spherical symmetry.\n\nWhen applying WKB approximation method to the radial Schrödinger equation\nwhere the effective potential is given by\nthe eigenenergies and the wave function behaviour obtained are different from the real solution.\n\nIn 1937, Rudolph E. Langer suggested a correction\n\nwhich is known as Langer correction or Langer replacement. This is equivalent to inserting a 1/4 constant factor whenever ℓ(ℓ + 1) appears. Heuristically, it is said that this factor arises because the range of the radial Schrödinger equation is restricted from 0 to infinity, as opposed to the entire real line.\n\nBy such a changing of constant term in the effective potential, the results obtained by WKB approximation reproduces the exact spectrum for many potentials.\n\nThat the Langer replacement is correct follows from the WKB calculation of the Coulomb eigenvalues with this replacement which reproduces the well known result. An even more convincing calculation is the derivation of Regge trajectories (and hence eigenvalues) of the radial Schrödinger equation with Yukawa potential by both a perturbation method (with the old formula_4 factor) and independently the derivation by the WKB method (with Langer replacement)-- in both cases even to higher orders. For the perturbation calculation see Müller-Kirsten and for the WKB calculation Boukema.\n\nNote that for 2D systems, as the potential takes the form:\nLanger correction goes:\n\n"}
{"id": "14072268", "url": "https://en.wikipedia.org/wiki?curid=14072268", "title": "Law of primacy in persuasion", "text": "Law of primacy in persuasion\n\nIn persuasive communication, the order of the information's presentation influences opinion formation. The law of primacy in persuasion, otherwise known as a primacy effect, as postulated by Frederick Hansen Lund in 1925 holds that the side of an issue presented first will have greater effectiveness in persuasion than the side presented subsequently. Lund presented college students with a document in support of one side of a controversial issue and then presented a second document which supported the opposite position. He found the document read first had greater influence, regardless of which position it expressed. This empirical evidence was generally accepted until 1950, when Cromwell published findings of the opposite: a recency effect in which arguments presented later had greater effectiveness in persuasion than arguments presented first. It now appears that both primacy and recency effects occur in persuasion.\n\nThere are many different theoretical models proposed to explain the occurrence of primacy and recency effects.\n\nSchultz (1963) developed the \"sensory-variation\" hypothesis for order effects, which suggests that humans seek high activation and will respond to novel stimuli more strongly than to stimuli they are familiar with. Novel stimuli should provide higher activation than familiar information, according to this theory. Shultz developed four postulates from this general hypothesis: \n\nAnderson (1981) theorized that order effects occur due to \"attention decrement\". According to this theory, when the first piece of information is presented about an opinion, people tend to pay less attention to subsequent information that may provide evidence to the contrary opinion. Therefore, people's opinions about something are influenced strongly by the information that they paid attention to, which was the first information presented to them. This model of \"attention decrement\" predicts a primacy effect in opinion formation.\n\nHogarth and Einhorn (1992) proposed the belief-adjustment model to try to predict in which situations order effects would occur and what specific order effect will occur. According to Hogarth and Einhorn, early information forms an initial impression, which is called an anchor. This anchor is then adjusted as new information is processed. This model predicts order effects based on the type of mental processing that is used for the new information. For end-of-sequence processing, or processing that occurs once all of the information has been presented, the model predicts primacy effects. The initial piece of information serves as the anchor, and subsequent pieces of information are aggregated together to adjust the initial piece of information. Therefore, the initial piece of information is weighted more than subsequent pieces of information, leading to a primacy effect.\n\nHowever, when processing changes to step-by-step processing, or processing that occurs after each new piece of information, recency effects are predicted. Each new piece of information received will be processed separately. This new piece of information will then become the new anchor, which forms a new impression. Beliefs are being adjusted with the processing of each new anchor, which leads to more weight being placed to the information most recently received. Therefore, recency effects are predicted to occur when information is processed in a step-by-step manner.\n\nThere are factors that can moderate the occurrence of order effects. Moderating factors affect the likelihood of order effects occurring.\n\nNeed for cognition moderates the occurrence of order effects. Kassin, Reddy, and Tulloch (1990) demonstrated that a juror's need for cognition affects which order effect the juror relies on for their vote. An ambiguous confession was played by one side of the case, and then both sides commented that the confession fit into their narrative of the crime. The side who presented the confession spoke first. Jurors who had high need for cognition were more likely to exhibit a primacy effect, meaning that they believed the confession fit with whichever side presented the confession as evidence. Conversely, jurors who had low need for cognition demonstrated a recency effect, and believed that the confession supported the case for the side who did not submit the confession.\n\nKassin, Reddy, and Tulloch (1990) believed that the reason for this effect was due to the nature of processing that the jurors engaged in. Jurors who were high in need for cognition actively process the information. This active processing leads to agreeing with the initial presentation of the data, and then participating in processing that confirms this agreement. However, people low in need for cognition do not process information, and therefore, rely on the information presented most recently for their opinion.\n\nThis impact of need for cognition is supported by a study conducted by Huagtvedt and Petty (1992). The experimenters played a message for two groups of people, one group that was high in need for cognition and one group that was low in need for cognition. The two groups were determined by their own need for cognition, and were not experimentally manipulated into the two groups. Initially, both groups were equally persuaded by the message. However, the two groups then listened to a weak counter-message, which was not as strong as the evidence for the initial message. The group that had high need for cognition were not persuaded by the weak counter-message, and their opinions were still in line with the initial message. The other group, the low need for cognition group, was persuaded by the new message. The high need for cognition demonstrated a primacy effect, while the low need for cognition demonstrated a recency effect.\n\nChunking interacts with the order of information and need for cognition to moderate the occurrence of order effects. Petty, Tormala, Hawkins, and Wegener (2001) conducted a study which examined the effect chunking has on order effects in people with high and low need for cognition. Participants read arguments for and against an exam policy, and this information was presented as being either chunked or unchunked. When the information was chunked, those who show high need for cognition were prone to primacy effects, while those with low need for cognition were prone to recency effects. However, when the information was presented in an unchunked nature, the opposite results were found. People who had high need for cognition demonstrated recency effects, whereas people who had low need for cognition showed a primacy effect. Therefore, chunking appears to interact with need for cognition to allow for order effects to occur.\n\nA study conducted by Lana (1961) demonstrates a moderating effect of the familiarity of information presented on order effects. In the study, a topic of initial low familiarity was used. Then, the researchers presented a long talk to well familiarize one group with the topic and a short talk to another group to gain little familiarization with the topic. A third group was then presented with no familiarization talk. Each of these three groups was then split into two subgroups, which listened to arguments for and against the topic twelve days later. One subgroup would listen to the argument for the topic first and then the argument against the topic, and the second subgroup would listen to the argument against it first. These groups then filled out a Likert scale questionnaire on their opinions on the topic. The results show that prior familiarization with a topic increased the likelihood of a primacy effect. Therefore, those in the long familiarization group had an opinion on the topic that coincided with which argument they heard first, regardless of the actual stance. However, no prior familiarization led to a recency effect to be demonstrated. Therefore, the group that was given no familiarization talk demonstrated opinions that coincided with the argument presented last to them.\n\nA study conducted by Lana (1963) demonstrates that the controversiality of the topic can have a moderating effect on order effects. College students and high school students read arguments for and against a controversial topic and a noncontroversial topic, and were then asked to fill out an opinion questionnaire about the topics. For college students, the controversial topic exhibited a primacy effect. College students did not exhibit any order effects for a noncontroversial topic. The college students were persuaded more by the argument they encountered first for a controversial issue, but were not influenced by the order of the presentation of arguments for noncontroversial topics. However, for high school students, no order effects were exhibited for either the controversial or noncontroversial topics. Therefore, the controversy of a topic appears to affect the role order effects play for some age groups in persuasion.\n\nSmith, Greenlees, and Manley (2009) found that order effects can occur in assessment of sports ability. The researchers had participants watch a video composed of an ultimate Frisbee player performing certain skills. Two videos were shown, either in descending ability or increasing ability. The participants were then asked to make assessments of the overall ability of the players and three aspects of their ability. However, the assessments occurred at differing times in the video. One group made the assessments at the end of the video, one group made delayed assessments at the end of the video, a third group made the assessments after each skill in the video, and a fourth group that made the overall assessment after each skill but then made the assessment of the ability after viewing the entire video. Results indicate that a primacy effect was exhibited in each of the conditions, except for the third group, which did not demonstrate any order effects. The assessment of ability tended to agree with the initial ability level shown. However, the group that made assessments in an extended step-by-step manner were not influenced by the order the abilities were shown. Therefore, a primacy effect can occur in ability assessment, unless extended step-by-step processing is employed.\n\nIn a study conducted by Garnefeld and Steinhoff (2013), order effects were demonstrated for opinions regarding service encounters. Four groups received daily descriptions of a hypothetical hotel stay over the course of five days. One group had a very positive experience on the first day, and then a neutral to slightly positive experience for the rest of the days. The second group had a negative experience on the first day, and then a neutral to slightly positive experience for the rest of the stay. Groups three and four experienced a neutral experience for the first four days, with group three then having a positive experience on the last day and group four having a negative experience on the last day. Each group was then tested for customer satisfaction regarding their hypothetical stay. Garnefeld and Steinhoff found that the timing of positive or negative occurrences is what affected satisfaction. For negative events, a recency effect was demonstrated, meaning that negative events which occurred at the end of the stay affected customer satisfaction more than negative events at the beginning of the stay. For positive events, a primacy effect was demonstrated, which means that positive events that occurred at the beginning of the stay affected customer satisfaction more than positive events at the end of the stay. Therefore, the timing of particular types of events in extended service encounters predicts the effect of the event on satisfaction.\n\nOrder effects may be used to influence a patient to receive an effective treatment that aligns with their values. One study, conducted by Bansback, Li, Lynd, and Bryan (2014), demonstrates that a primacy effect will influence the decision for treatment. The researchers presented three groups with information about sleep apnea treatments. The three groups were based on the order in which the information was presented. One group received information in an order unrelated to their values, while two groups received information ordered based on their values. One of the groups received information that aligned with their values first, and the other group received information that aligned with their values last. The researchers found that patients were more likely to choose the treatment that aligned with their values when this information was presented first, thus demonstrating a primacy effect for information on treatments that align with a patient's values. The order in which patients receive information appears to influence which treatment option they choose.\n\nIn a study conducted by Panagopoulos (2010), order effects were found in terms of voter mobilization. Calls were made to residents of an American city at different times before an election. Some residents received a call 4 weeks prior to an election, some received a call two weeks prior to an election, some received a call three days before an election, and some residents did not receive a call. The call made was a nonpartisan attempt to mobilize people to vote. Results indicate that the timing of the call may not have an effect on the general voter population, but that the timing can affect certain populations and get certain populations to vote at higher rates. According to Panagopoulos, high-propensity voters, voters who typically turn out in higher numbers, voted at higher percentages when they received the call four weeks prior to the election, demonstrating a primacy effect. However, for lower propensity voters, voters who typically do not vote, calls made three days prior to the election were more effective at getting this population to vote, demonstrating a recency effect. Therefore, for voter mobilization, propensity to vote appears to be a moderating variable influencing the effect of timing of a mobilization call.\n\n"}
{"id": "319611", "url": "https://en.wikipedia.org/wiki?curid=319611", "title": "Life history (sociology)", "text": "Life history (sociology)\n\nThe method was first used when interviewing indigenous peoples of the Americas and specifically Native American leaders who were asked by an interviewer to describe their lives with an insight as to what it was like to be that particular person. The purpose of the interview was to capture a living picture of a disappearing (as such) people/way of life.\n\nLater the method was used to interview criminals and prostitutes in Chicago. Interviewers looked at social and police-records, as well as the society in general, and asked subjects to talk about their lives. The resulting report discussed (i) Chicago at that particular time; (ii) how the subject viewed their own life (i.e. `how it was like to be this particular person') and (iii) how society viewed the subject and whether they would be incarcerated, receive help, perform social work, etc.\nThe landmark of the life history method was developed in the 1920s and most significantly embodied in The Polish Peasant in Europe and America by W.I Thomas and Florian Znaniecki.The authors employed a Polish immigrant to write his own life story which they then interpreted and analyzed. According to Martin Bulmer, it was \"the first systematically collected sociological life history\".\n\nThe approach later lost momentum as quantitative methods became more prevalent in American sociology. The method was revived in the 1970s, mainly through the efforts of French sociologist Daniel Bertaux and Paul Thompson whose life history research focused on such professions as bakers and fishermen. Major initiatives of the life history method were undertaken also in Germany, Italy, and Finland.\n\nIn the German context, the life history method is closely associated with the development of biographical research and biographical-narrative interviews. The narrative interview as a method for conducting open narrative interviews in empirical social research was developed in Germany around 1975. It borrowed concepts from phenomenology (Alfred Schütz), symbolic interactionism (George Herbert Mead), ethnomethodology (Harold Garfinkel), and sociology of knowledge (Karl Mannheim). The development and improvement of the method are closely connected to German sociologist Fritz Schütze, part of the Bielefeld Sociologist’s Working Group, which maintained close academic cooperation with American sociolinguists and social scientists such as Erving Goffman, Harvey Sacks, John Gumpertz, and Anselm Strauss. The analysis of life histories was further developed by the biographical case reconstruction method of German sociologist Gabriele Rosenthal for the analysis of life \"history\" and life \"story\". Rosenthal differentiates between the level of analysis of the \"narrated\" life story (\"erzählte Lebensgeshichte\") and the \"experienced\" life history (\"erlebte Lebensgeschichte\").\n\nIn both cases, the one doing the interview should be careful not to ask \"yes or no\"-questions, but to get the subject to tell \"the story of his or her life\", in his or her own words. This is called the \"narrative\" method. It is common practice to begin the interview with the subject's early childhood and to proceed chronologically to the present. Another approach, dating from the Polish Peasant, is to ask participants to write their own life stories. This can be done either through competitions (as in Poland, Finland or Italy) or by collecting written life stories written spontaneously. In these countries, there are already large collections of life stories, which can be used by researchers.\n\n"}
{"id": "27550952", "url": "https://en.wikipedia.org/wiki?curid=27550952", "title": "List of Indian IT companies", "text": "List of Indian IT companies\n\nThis is a list of notable companies in the information technology sector based in India. Top 10 companies are listed in descending order of their market capitalization, and other companies are listed alphabetically, grouped by the cities in which they are headquartered. Certain companies have main offices in more than one city, in which case they are listed under each, but minor offices and resources are not listed. Foreign companies are only listed if they have one of their main offices in India.\n"}
{"id": "411523", "url": "https://en.wikipedia.org/wiki?curid=411523", "title": "List of Intel chipsets", "text": "List of Intel chipsets\n\nThis article provides a list of motherboard chipsets made by Intel, divided into three main categories: those that use the PCI bus for interconnection (the 4xx series), those that connect using specialized \"hub links\" (the 8xx series), and those that connect using PCI Express (the 9xx series). The chipsets are listed in chronological order.\n\nEarly IBM XT-compatible mainboards did not have a chipset yet, but relied instead on a collection of discrete TTL chips by Intel:\nTo integrate the functions needed on a mainboard into a smaller amount of ICs, Intel licensed the ZyMOS POACH chipset for its Intel 80286 and Intel 80386SX processors (the 82230/82231 High Integration AT-Compatible Chip Set). This chipset can be used with an 82335 High-integration Interface Device to provide support for the Intel 386SX.\n\nList of early Intel chipset includes:\n\n\nWhile not an actual Intel chipset bug, the Mercury and Neptune chipsets could be found paired with RZ1000 and CMD640 IDE controllers with data corruption bugs. L2 caches are direct-mapped with SRAM tag RAM, write-back for 430FX, HX, VX, and TX.\n\nSummary:\n\nAll Chipsets listed in the table below: \n Remapping of PCIE/APIC memory ranges not supported, some physical memory might not be accessible (e.g. limited to 3.5 GB or similar).\n\nSummary:\n\n Remapping of PCIE/APIC memory ranges not supported, some physical memory might not be accessible (e.g. limited to 3.5 GB or similar).\n\nAll Core 2 Duo chipsets support the Pentium Dual-Core and Celeron processors based on the Core architecture. Support for all NetBurst based processors was officially dropped starting with the Bearlake chipset family. However, some motherboards still support the older processors.\n\nSummary:\n\n\n\nThe Nehalem microarchitecture moves the memory controller into the processor. For high-end Nehalem processors, the X58 IOH acts as a bridge from the QPI to PCI Express peripherals and DMI to the ICH10 southbridge. For mainstream and lower-end Nehalem processors, the integrated memory controller (IMC) is an entire northbridge (some even having GPUs), and the PCH (Platform Controller Hub) acts as a southbridge.\n\nNot listed below is the 3450 chipset (see Xeon chipsets) which is compatible with Nehalem mainstream and high-end processors but does not claim core iX-compatibility. With either a Core i5 or i3 processor, the 3400-series chipsets enable the ECC functionality of unbuffered ECC memory. Otherwise these chipsets do not enable unbuffered ECC functionality.\n\nThe Cougar Point Intel 6 series chipsets with stepping B2 were recalled due to a hardware bug that causes their 3 Gbit/s Serial ATA to degrade over time until they become unusable. Stepping B3 of the Intel 6 series chipsets will have the fix for this. The Z68 chipset which supports CPU overclocking and use of the integrated graphics does not have this hardware bug. The Z68 also added support for transparently caching hard disk data on to solid-state drives (up to 64 GB), a technology called Smart Response Technology.\n\nChipsets supporting LGA 1156 CPUs (Lynnfield and Clarkdale).\n\nChipsets supporting LGA 1155 CPUs (Sandy Bridge and Ivy Bridge). The PCIe 2.0 lanes from the PCH ran at 5 GT/s in this series, unlike in the previous LGA 1156 chips.\n\n\nChipsets that support LGA 1150 CPUs are listed below. Haswell and Haswell Refresh CPUs are supported by all listed chipsets; however, a BIOS update is usually required for 8-Series \"Lynx Point\" motherboards to support Haswell Refresh CPUs. Broadwell CPUs are supported only by 9-Series chipsets, which are usually referred to as \"Wildcat Point\".\n\nThe C1 stepping of the \"Lynx Point\" chipset contains a bug a system could lose connectivity with USB devices plugged into USB 3.0 ports provided by the chipset if the system enters the S3 sleep mode.\n\nChipsets supporting LGA 1366, LGA 2011, and LGA 2011-v3 CPUs.\n\n\nAll Core I series mobile chipsets are integrated south bridge.\n\n\nThe 100 Series chipsets (codenamed \"Sunrise Point\"), for Skylake processors using the LGA 1151 socket, were released in the third quarter of 2015.\n\nThe 200 Series chipsets (codenamed \"Union Point\") were introduced along with Kaby Lake processors, which also use the LGA 1151 socket; these were released in the first quarter of 2017.\n\nWhile Coffee Lake shares the same socket as Skylake and Kaby Lake, this revision of LGA 1151 is electrically \"incompatible\" with 100 and 200 series CPUs.\n\nThe 300 Series chipsets were introduced along with Coffee Lake processors, which use the LGA 1151 socket; the enthusiast model was released in the last quarter of 2017, the rest of the line will be released in 2018.\n\nChipset C232 does not support CPU integrated GPUs.\n\nChipsets supporting LGA 2066 socket for Skylake-X processors and Kaby Lake-X processors.\n\n"}
{"id": "47095423", "url": "https://en.wikipedia.org/wiki?curid=47095423", "title": "List of baseball players who underwent Tommy John surgery", "text": "List of baseball players who underwent Tommy John surgery\n\nTommy John surgery (TJS), known in medical practice as ulnar collateral ligament (UCL) reconstruction, is a surgical graft procedure in which the ulnar collateral ligament in the medial elbow is replaced with either a tendon from elsewhere from the patient's own body, or the use of a tendon from the donated tissue from a cadaver. The procedure is common among collegiate and professional athletes in several sports, most notably baseball.\n\nThe procedure was first performed in 1974 by orthopedic surgeon Dr. Frank Jobe, then a Los Angeles Dodgers team physician. The surgery is named after Tommy John, the first recipient of the surgery. John won 288 games in his career, which was extended by the surgery. Many players since have had Tommy John surgery.\n"}
{"id": "31613741", "url": "https://en.wikipedia.org/wiki?curid=31613741", "title": "List of reagents", "text": "List of reagents\n\nThis is a list of inorganic and organic reagents commonly used in chemistry.\n\nReagents are \"substances or compounds that are added to a system in order to bring about a chemical reaction or are added to see if a reaction occurs.\" Some reagents are just a single element. However, most processes require reagents made of chemical compounds. Some of the most common ones are listed below. These are some of the chemical reagants, there are many more.\n\n"}
{"id": "56427192", "url": "https://en.wikipedia.org/wiki?curid=56427192", "title": "List of things named after Freeman Dyson", "text": "List of things named after Freeman Dyson\n\nThings named after Freeman Dyson:\nGordon Freeman\n"}
{"id": "768163", "url": "https://en.wikipedia.org/wiki?curid=768163", "title": "Little Joe 6", "text": "Little Joe 6\n\nThe Little Joe 6 was a Launch Escape System test of the Mercury spacecraft, conducted as part of the U.S. Mercury program. The mission used a boilerplate Mercury spacecraft. The mission was launched October 4, 1959, from Wallops Island, Virginia. The Little Joe 6 flew to an apogee of and a range of . The mission lasted 5 minutes 10 seconds. Maximum speed was and acceleration was 5.9 g (58 m/s²). Payload .\n\n"}
{"id": "45418224", "url": "https://en.wikipedia.org/wiki?curid=45418224", "title": "Mathematics and Plausible Reasoning", "text": "Mathematics and Plausible Reasoning\n\nPolya begins Volume I with a discussion on induction, not the mathematical induction, as a way of guessing new results. He shows how the chance observations of a few results of the form 4 = 2 + 2, 6 = 3 + 3, 8 = 3 + 5, 10 = 3 + 7, etc., may prompt a sharp mind to formulate the conjecture that every even number greater than 4 can be represented as the sum of two odd prime numbers. This is the well known Goldbach's conjecture. The first problem in the first chapter is to guess the rule according to which the successive terms of the following sequence are chosen: 11, 31, 41, 61, 71, 101, 131, . . . In the next chapter the techniques of generalization, specialization and analogy are presented as possible strategies for plausible reasoning. In the remaining chapters, these ideas are illustrated by discussing the discovery of several results in various fields of mathematics like number theory, geometry, etc. and also in physical sciences.\n\nThis volume attempts to formulate certain patterns of plausible reasoning. The relation of these patterns with the calculus of probability are also investigated. Their relation to mathematical invention and instruction are also discussed. The following are\nsome of the patterns of plausible inference discussed by Polya.\n\n"}
{"id": "213243", "url": "https://en.wikipedia.org/wiki?curid=213243", "title": "Mercury-Atlas 9", "text": "Mercury-Atlas 9\n\nMercury-Atlas 9 was the final manned space mission of the U.S. Mercury program, launched on May 15, 1963 from Launch Complex 14 at Cape Canaveral, Florida. The spacecraft, named Faith 7, completed 22 Earth orbits before splashing down in the Pacific Ocean, piloted by astronaut Gordon Cooper, then an Air Force major. The Atlas rocket was No. 130-D, and the Mercury spacecraft was No. 20. This mission marks the last time an American was launched alone to conduct an entirely solo orbital mission.\n\n\n\nThe flight of \"Sigma 7\" had been so nearly perfect that some at NASA thought America should quit while it was ahead and make MA-8 the last Mercury mission, and not risk the chance of future disaster. NASA had pushed the first-generation Mercury hardware far enough, and taking more chances on another longer mission was not warranted; instead, they should move on to the Gemini program.\n\nManned Spacecraft Center officials, however, believed that the Mercury team should be given the chance to test man in space for a full day. In addition, all of the Soviet single-seat Vostok spacecraft launched after Vostok 1 lasted for more than a day, thus the Mercury 9 flight would bring the Mercury spacecraft up to the same level as that of the Soviets.\n\nIn September 1962, NASA concluded negotiations with McDonnell to modify four Mercury spacecraft (#12, #15, #17 and #20) to a configuration that supported a one-day mission. Such changes to the spacecraft included the removal of the periscope, a redundant set of thrusters, and the addition of extra batteries and oxygen tanks.\n\nIn November 1962, Gordon Cooper was chosen to pilot the MA-9 mission and Alan Shepard was picked as backup.\n\nOn April 22, 1963, Atlas booster 130-D and Mercury spacecraft #20 were stacked on the launch pad at Launch Complex 14.\n\nBecause MA-9 would orbit over nearly every part of the world from 32.5 degrees north to 32.5 degrees south, a total of 28 ships, 171 aircraft, and 18,000 servicemen were assigned to support the mission.\n\nThe Atlas booster used for MA-9 sported several technical improvements, most notably an enhanced propulsion system with a hypergolic igniter that would eliminate the need for hold-down time at launch to prevent rough combustion. With seven successful Mercury launches in a row, the failures of the early days seemed like a distant memory by early 1963 and NASA officials had a high degree of confidence in the Atlas that overshadowed its still spotty launch record. At the first meeting of senior MSFC officials for the year (January 4), Walter Williams noted that the Air Force had yet to provide an explanation for two Atlas F failures during the second half of 1962. Until the investigation committees released their findings and cleared the Atlas D of guilt by association, Cooper's flight could be delayed. During the seven months between Schirra and Cooper's flights, there were five failures of Atlas D vehicles (one of them an Atlas-Agena, the rest operational ICBM tests). NASA did not let its guard down on the Atlas, despite the recent high degree of success enjoyed by Project Mercury.\n\nWhen Atlas 130D received its factory rollout on January 30, it was found to have damaged wiring and had to be sent back for repairs. At his first press conference on February 8, Gordon Cooper admitted to not knowing much about the booster problems and focused instead on the enhancements made to his Mercury capsule. The numerous added equipment and consumables for the day-long mission boosted the weight of Faith 7 considerably; it now weighed over 3000 pounds.\n\nOn March 15, the Atlas was rolled out of the factory a second time and passed tests with flying colors; Convair engineers expressed confidence that this \"was their best bird yet\". Aside from the new propulsion system, the booster received some slight modifications to the engine offsets to counteract the potentially dangerous roll that occurred during Schirra's launch. The booster also sported an improved calibration of the propellant utilization system.\n\nThe upgraded MA-2 engines featured baffled injector heads and a hypergolic igniter, eliminating any concerns of rough combustion or the need for hold-down time prior to liftoff. As such, the RCC (Rough Combustion Cutoff) sensors on 130D were operated open loop and for qualitative purposes only.\n\nCooper's decision to name his capsule \"Faith 7\" was based on the faith he had in the Atlas booster and Mercury capsule to carry out the mission successfully, although it was reported in \"The Washington Post\" that some NASA officials were skeptical of the idea.\n\nWhen Cooper boarded Faith 7 at 6:36 AM on the morning of May 14, he found a little gift that had been left for him. Alan Shepard, knowing that Cooper would have a new version of the urine containment device that Shepard did not have on his Mercury-Redstone 3 flight (forcing him to relieve himself during a long countdown hold), had left behind a toilet plunger as a joke. Instructions on the handle said, \"Remove Before Launch\". The gift did not make the trip. Neither did Cooper that day. Various problems with radar in Bermuda and the diesel engine that rolled back the gantry caused the launch to be cancelled until May 15.\n\nAt 8:04:13 a.m. EST, May 15, 1963, \"Faith 7\" was launched from Launch Complex 14. At T+60 seconds, the Atlas started its pitch program. Shortly afterward, MA-9 passed through max Q. At T+2 minutes 14 seconds Cooper felt BECO (Booster Engine Cutoff) and staging. The two Atlas booster engines had been left behind. The Launch Escape Tower was then jettisoned. At T+3 minutes the cabin pressure sealed at . Cooper reported, \"Faith 7 is all go.\"\n\nAt about T+5 minutes was SECO (Sustainer Engine Cutoff) and \"Faith 7\" entered orbit at 17,547 mph (7,844 m/s). After the spacecraft separated and turned around to orbit attitude, Cooper watched his Atlas booster lag behind and tumble for about eight minutes. Over Zanzibar on the first orbit, he learned that the orbital parameters were good enough for at least 20 orbits. As the spacecraft passed over Guaymas, Mexico still on the first orbit, capsule communicator Gus Grissom told Cooper the ground computers said he was \"go for seven orbits\".\n\nAtlas performance was overall excellent. The upgraded propulsion system worked well, with slightly above nominal booster engine thrust. Measurable propellant slosh occurred from T+55 to T+120 seconds, caused by slightly lower than nominal autopilot gains. The flight trajectory was slightly more lofted than nominal due to the DC voltage in the booster electrical system being about 0.7 volts above normal, this was counteracted by the higher than nominal booster engine performance.\n\nAt the start of the third orbit, Cooper checked his list of 11 experiments that were on his schedule. His first task was to eject a six-inch (152 mm) diameter sphere, equipped with xenon strobe lights, from the nose of the spacecraft. This experiment was designed to test his ability to spot and track a flashing beacon in orbit. At T+3 hours 25 minutes, Cooper flipped the switch and heard and felt the beacon detach from the spacecraft. He tried to see the flashing light in the approaching dusk and on the nightside pass, but failed to do so. On the fourth orbit, he did spot the beacon and saw it pulsing. Cooper reported to Scott Carpenter on Kauai, Hawaii, \"I was with the little rascal all night.\" He also spotted the beacon on his fifth and sixth orbits.\n\nAlso on the sixth orbit, at about T+9 hours, Cooper set up cameras, adjusted the spacecraft attitude and set switches to deploy a tethered balloon from the nose of the spacecraft. It was a PET film balloon painted fluorescent orange, inflated with nitrogen and attached to a nylon line from the antenna canister. A strain gauge in the antenna canister would measure differences in atmospheric drag between the perigee and the apogee. Cooper tried several times to eject the balloon, but it failed to eject.\n\nCooper passed Schirra's orbital record on the seventh orbit while he was engaged in radiation experiments. After 10 hours, the Zanzibar tracking station informed Cooper the flight was a go for 17 orbits. Cooper was orbiting the Earth every 88 minutes 45 seconds at an inclination of 32.55 degrees to the equator.\n\nHis scheduled rest period was during orbits 9 through 13. He had a dinner of powdered roast beef mush and some water, took pictures of Asia and reported the spacecraft condition. Cooper was not sleepy and during orbit 9 took some of the best photos made during his flight. He took pictures of the Tibetan highlands and of the Himalayas.\n\nDuring the flight Cooper reported that he could see roads, rivers, small villages, and even individual houses if the lighting and background conditions were right. This was later confirmed by the two-man Gemini crews that later flew (of which Cooper was included). Cooper slept intermittently the next six hours, during orbits 10 through 13. He woke from time to time and took more pictures, taped status reports, and kept adjusting his spacesuit temperature control which kept getting too hot or too cold.\n\nOn his fourteenth orbit, Cooper took an assessment of the spacecraft condition. The oxygen supply was sufficient. The peroxide fuel for attitude control was 69 percent in the automatic tank and 95 percent in the manual one. On the fifteenth orbit, he spent most of the time calibrating equipment and synchronizing clocks.\n\nWhen he entered night on the sixteenth orbit, Cooper pitched the spacecraft to slowly follow the plane of the ecliptic. Through the spacecraft window he viewed the zodiacal light and night airglow layer. He took pictures of these two \"dim light\" phenomena from Zanzibar, across the Earth's nightside, to Canton Island. The pictures were later found to have been overexposed, but they still contained valuable data.\n\nAt the start of the 17th orbit while crossing Cape Canaveral, Florida, Cooper transmitted slow scan black and white television pictures to Mercury Control. The picture showed a ghostly image of the astronaut. In the murky picture, a helmet and hoses could be seen. It was the first time an American astronaut had sent back television images from space.\n\nOn the 17th and 18th orbits, Cooper took infrared weather photos and moonset Earth-limb pictures. He also resumed Geiger counter measurements of radiation. He sang during orbits 18 and 19, and marveled at the greenery of Earth. It was nearing thirty hours since liftoff.\n\nOn the 19th orbit, the first sign of trouble appeared when the spacecraft 0.05 g (0.5 m/s²) light came on. However, this turned out to be a faulty indicator, and the spacecraft was not reentering. On the 20th orbit, Cooper lost all attitude readings. The 21st orbit saw a short-circuit occur in the bus bar serving the 250 volt main inverter. This left the automatic stabilization and control system without electric power.\n\nOn the 21st orbit, John Glenn on board the tracking ship \"Coastal Sentry Quebec\" near Kyūshū, Japan, helped Cooper prepare a revised checklist for retrofire. Due to the system malfunctions, many of the steps would have to be done manually. Only Hawaii and Zanzibar were in radio range on this last orbit, but communications were good. Cooper noted that the carbon dioxide level was rising in the cabin and in his spacesuit. He told Carpenter as he passed over Zanzibar, \"Things are beginning to stack up a little.\" Throughout the problems, Cooper remained cool, calm and collected.\n\nCooper did not experience much of an appetite during the flight and ate only because it was scheduled. The food containers and water dispenser system proved unwieldy and he was not able to properly prepare freeze-dried food packages, so he limited his consumption to cubed food and bite-sized sandwiches. Cooper found the cubed food largely unpalatable, which contributed to his lack of eating. He had no difficulty urinating during the flight and the urine collection system worked well, although transferring urine to storage bags in the cramped capsule proved difficult. Cooper took several naps during the flight, lasting about an hour each. He experienced some discomfort from the pressure suit compressing his knees, which he alleviated by moving his feet slightly upward. An hour and 20 minutes before retrofire, Cooper took a dextroamphetamine tablet to ensure his alertness; he reported not feeling any sleepiness for the remainder of the flight.\n\nAt the end of the 21st orbit, Cooper again contacted Glenn on the \"Coastal Sentry Quebec\". He reported the spacecraft was in retro attitude and holding manually. The checklist was complete. Glenn gave a ten-second countdown to retrofire. Cooper kept the spacecraft aligned at a 34° pitchdown angle and manually fired the retrorockets on \"Mark!\".\n\nCooper had drawn lines on the window to stay aligned with constellations as he flew the craft. He later said he used his wristwatch to time the burn and his eyes to maintain attitude.\n\nFifteen minutes later \"Faith 7\" landed just four miles (6 km) from the prime recovery ship, the carrier USS \"Kearsarge\". This was the most accurate landing to date, despite the lack of automatic controls. \"Faith 7\" landed southeast of Midway Island, in the Pacific Ocean. This would be near .\n\nSplashdown was at 34 hours 19 minutes 49 seconds after liftoff. The spacecraft tipped over in the water momentarily, then righted itself. Helicopters dropped rescue swimmers and relayed Cooper's request of an Air Force officer for permission to be hoisted aboard the Navy's carrier. Permission was granted. Forty minutes later the explosive hatch blew open on the deck of the \"Kearsarge\". Cooper stepped out of \"Faith 7\" to a warm greeting.\n\nPostflight medical examination of Cooper found that he was slightly dehydrated and experienced a degree of orthostatic hypotension from being seated in the capsule an entire day, but other than that no significant effects from the flight were noted.\n\nAfter the MA-9 mission, there was another debate about whether to fly one more Mercury flight, Mercury-Atlas 10 (MA-10). It was proposed as a three-day, 48-orbit mission to be flown by Alan Shepard in October 1963. In the end, NASA officials decided it was time to move on to Project Gemini and MA-10 never flew.\n\nThe Mercury program had fulfilled all of its goals.\n\nThe \"Faith 7\" spacecraft is currently displayed at Space Center Houston, Houston, Texas.\n\n\n\n"}
{"id": "54055503", "url": "https://en.wikipedia.org/wiki?curid=54055503", "title": "Modern Sign Language communication", "text": "Modern Sign Language communication\n\nSign Language communication is a system of communication using visual gestures and signs, as used by deaf people. A unique property of Sign Language is that it is not easy to put on paper due to a lack of formal acceptance, and is impossible to be mediated over the phone or in a plain email. Because of its three-dimensional nature, it can only be transferred in the form of timed two or three dimensional snapshots, also known as a video. The video itself has to be large and fast enough to capture sign language movements sufficiently.\n\nIn the late 1990s, it became possible for deaf people to use electronic communication tools as another channel for Sign Language. Initially, there weren't many choices prior to the advent of more interfaces (e.g. APIs like ). With these, however, deaf people can perform electronic sign language exchanges independently without the need for a third person in the middle, i.e. a Video relay service.\n"}
{"id": "14345328", "url": "https://en.wikipedia.org/wiki?curid=14345328", "title": "Multimedia University Engineering Society Overseas Research Programme", "text": "Multimedia University Engineering Society Overseas Research Programme\n\nMESCORP, originally an acronym for Multimedia University Engineering Society Overseas Research Programme, is an undergraduate-based research work which involves a field study initiated by Multimedia University's engineering students. MESCORP addresses the dire need of qualified IT-knowledgeable engineers in Malaysia by supplementing soft skills such as initiative, creative, innovative qualities to the technical skills acquired through practical applications of theories in laboratory experiments.\n\nMultimedia University Engineering Society Overseas Research Programme (MESCORP) was founded in 2002 by the former dean of the Faculty of Engineering (FOE), Ir. Professor Dato' Dr Chuah Hean Teik, under the patronage of the Chancellor YABhg. Tun Dr. Siti Hasmah Hj. Mohamad Ali(Chancellor 2002-2012).\n\nAwards received:\nEver since 2002, MESCORP has researched and published journals regarding the topics concerned and organized annual conferences to disseminate their findings to the public. MESCORP's track record is as follows:\n\n\n"}
{"id": "3842607", "url": "https://en.wikipedia.org/wiki?curid=3842607", "title": "NIST-7", "text": "NIST-7\n\nNIST-7 was the atomic clock used by the United States from 1993 to 1999. It was one of a series of Atomic Clocks at the National Institute of Standards and Technology. The caesium beam clock served as the nation's primary time and frequency standard during that time period, but it has since been replaced with the more accurate NIST-F1, a caesium fountain atomic clock that neither gains nor loses one second in 100 million years.\n\n"}
{"id": "57847895", "url": "https://en.wikipedia.org/wiki?curid=57847895", "title": "Olbers-Planetarium", "text": "Olbers-Planetarium\n\nThe Olbers-Planetarium is located in the Free Hanseatic City of Bremen, Germany, recording about 25.000 visitors a year. It was named after Heinrich Wilhelm Olbers (1758 – 1840), a doctor and astronomer from Bremen. \n\nThe planetarium is located within the City Unversity of Applied Sciences Bremen. From the main station tram 4 (Arsten), 6 (Flughafen/Airport) and 8 (Huchting) as well as bus 24 (Rablinghausen) pass the stop ‘Wilhelm-Kaisen-Brücke’. It is another 400 meters by foot to reach the planetarium. The planetarium is well connected to the historic town centre which features the Bremen City Hall, the Bremen Cathedral, Böttcherstraße, the Schnoor and the Schlachte promenade.\nThe planetarium offers shows 365 days a year. During the school holidays in Bremen the planetarium is running a holiday programme. All shows are held in German, however it is possible to request shows in a different language. Shows usually last about one hour.\n\nThe planetarium opened its doors on 23 January 1952. Back in the day it was part of the nautical college in Bremen, now the City University of Applied Sciences and its primary cause was to train nautical students in Celestial navigation. The first projector in place was the ZKP 1, which the German Hydrographical Institute, now called the Federal Maritime and Hydrographic Agency of Germany) funded. It was originally developed for the navigational institutions of the Kriegsmarine and Luftwaffe.\n\nIn 1958 the nautical college moved to Werderstraße 73 where the planetarium is still located today. In 1979 a more modern projector, the ZKP 2 was introduced. Dr. Erwin Mücke had applied for funding from the German Research Foundation. The same year they also received a solar system projector, funded by the senat for Education and Science. The old ZKP 1 was sold to the college in Nordenham where it is still in use today. \n\nSince 2015 a hybrid system is in use. The main projector was upgraded with parts from a more recent model.\n\nIn the 1950s ornithological research was conducted at the Olbers-Planetarium. Dr. Franz and Dr. Eleonore Sauer from the Zoological Institute of the University of Freiburg observed the orientation of Warblers using artificial night-skies. \nIn 2017 the Lund University in Sweden published a paper on Dung beetle orientation. The scenes for that particular research project were amongst other places, taken at the planetarium.\n\nThe Walter Stein Observatory is run by the Olbers Society, named in honour of Heinrich Wilhelm Matthias Olbers. In the winter months from October to April, the observatory is open on selected days. The Olbers Society occasionally uses the planetarium for guest talks. The observatory and the planetarium are located in the same building.\n\nIn 2018 the International Astronautical Congress is held in Bremen and the Planetarium is part of many projects as part of the astronautical year.\n\n\n"}
{"id": "59100324", "url": "https://en.wikipedia.org/wiki?curid=59100324", "title": "Paludisphaera", "text": "Paludisphaera\n\nPaludisphaera is an aerobic genus of bacteria from the family of Isosphaeraceae with one known species (\"Paludisphaera borealis\"). \"Paludisphaera borealis\"has been isolated from Sphagnum peat from the Yaroslavl region in Russia.\n"}
{"id": "39205259", "url": "https://en.wikipedia.org/wiki?curid=39205259", "title": "Pascal Chretien", "text": "Pascal Chretien\n\nPascal Chrétien (born 7 April 1968) is a French-Australian commercial helicopter pilot and engineer with degrees in electronics and aerospace who designed, built and test flew the world’s first manned electric helicopter the Solution F/Chretien Helicopter on 12 August 2011. The helicopter, built for the French company Solution F, set a \"Guinness World Record\" and received the \"IDTechEx Electric Vehicles Land Sea & Air\" award.\n\nChrétien has been pioneering electromagnetic transmissions and hybrid propulsion applied to aircraft since 2002 and holds several patents in the field. Chrétien obtained commercial helicopter pilot licences in Canada and Australia, in 1993 and 1994, respectively. He has aerial work and aircraft testing experience.\n"}
{"id": "57494507", "url": "https://en.wikipedia.org/wiki?curid=57494507", "title": "Portrayal of women scientists in film", "text": "Portrayal of women scientists in film\n\nThe portrayal of women scientists in film refers to the way that professional women in science, technology, engineering, and mathematics (STEM) fields are written as film characters. The portrayal of women is a hotly contested talking point in the film community, with feminists saying that women are not accurately depicted in films. Judith Mayne, author and professor of women's studies at Ohio State University, claims that the study of female characters in film began with movements from the 1960s and 1970s in the form of second-wave feminism, the rise of independent films, and the beginning of academic film studies.\n\nScholars offer several different stereotypes that academic women fall under in films. Eva Flicker, professor of sociology specializing in film at the University of Vienna, is a leading scholar on the topic of academic women in film, specifically women in the sciences. Flicker writes that the role of women scientists often reflect that \"job stereotypes are combined with gender stereotypes.\"\n\nNot all scholars believe that objectification is reserved solely for female characters in films. Nöel Carroll, professor of philosophy at Cornell University, writes that men are also strategically blocked for the audience's viewing pleasure. Carroll does not disagree with the notion that women are objectified in film; rather, his stance says that women are not the only ones affected by the erotic gaze of viewers.\n\nGorillas in the Mist (1988)\n\nThe film Gorillas in the Mist based on the book by Dian Fossey appears to show respect for the fact that a woman would rather work on her career than be a stereotypical housewife. The story is about the scientist Fossey that leaves the United States to study gorillas in Rwanda and Uganda. As she bonds with the gorillas, she worries about poachers and devotes her time to protecting the animals. Fossey is shown as an independent woman, breaking the common trope of women being the homemaker.\n\nJurassic Park (1993)\n\nThe 1993 film Jurassic Park, based on the novel by Michael Crichton depicts a paleobotanist, Dr. Ellie Sattler. She is portrayed with great physical ability, allowing her to survive multiple attacks from dinosaurs. She is shown to have extensive knowledge about dinosaurs and plant life throughout the movie.\n\nGravity (2013)\n\nThe 2013 film \"Gravity\", directed by Alfonso Cuarón and starring Sandra Bullock and George Clooney, is often heralded as a breakthrough feminist film due to Bullock's starring role. Some who have critiqued the film have written that \"Gravity\" \"proves that a woman can anchor an action-packed blockbuster that does \"not\" have to include violence, superheroes, weapons and/or huge death tolls.\" While the film's lead is a woman, she gets by with help from her male counterpart, played by Clooney. Bullock's character Ryan Stone is described as \"the very model of the damsel in distress,\" as she can never get out of a situation on her own and must lean on Clooney's character to do the heavy lifting. The role of Bullock's character is thought to be an act of defiant feminism, as she is the lead in a science fiction film, but below the surface viewers find that the film actually subscribes to traditional gender stereotypes and does not at all portray Bullock's character as a true independent woman.\n\nIn terms of intelligence, women are not often portrayed as equal to men. Eva Flicker writes that in science fiction films, men are overwhelmingly portrayed as scientists, making up 82% of all film scientists. After reviewing 60 movies, Flicker has placed the women shown in science fiction films into six categories.\n\nThis type of woman scientist is \"only interested in her work\" and is often depicted having a nondescript appearance and style. As the film progresses, a man saves her and brings out her feminine side, after which she becomes more attractive. However, she loses credibility as an academic and suddenly makes more mistakes than when she did not focus as much on her appearance. Based on this type of story, Flicker concludes that \"femininity and intelligence are mutually exclusive characteristics in a woman's film role.\" An example of this woman in a film can be seen in \"Spellbound\", in the character Dr. Constance Peterson.\n\nThis type of woman works with men in an all male environment. Because of this, she has a \"harsh voice\" and occasionally \"succumbs to an unhealthy lifestyle,\" such as partaking in smoking and drinking, to fit in with the boys. Flicker claims that this type of woman is \"lost somewhere in the middle\" of masculinity and femininity, meaning she is not as sexual a character as other women are, but she is also not on the same level as the men she works with. This decreases her credibility both as a women and as one trying to fit into \"a man's world.\" In the end, her heightened female emotions allow her to contribute to a solution, which is her redeeming quality as a character. The \"male woman\" character appears in the 1970 science fiction film \"Andromeda Strain\".\n\nSeen in \"\", this woman scientist does very little. For the sake of the dramatization of the film, she is crucial, but she does not advance the story and does not contribute much to the solution. Instead, her femininity causes more trouble for the team of scientists. She is young, attractive, and subject to experience womanly emotions which add an extra layer to the existing predicament, forcing the man to solve the problem in order to get the team out of trouble. She is \"naïve in her actions,\" messing up every task that is given to her despite her extensive education and knowledge, while her male counterpart stands in stark contrast and ends up saving the day.\n\nThe \"evil plotter\" woman is young and very beautiful, and she uses her feminine charm to trick the men into doing what she wants. She has an ulterior motive, which is on the opposite end of the spectrum from what the rest of the team is trying to accomplish. She is the character that the audience and the other characters despise by the end of the movie because she is devilishly smart and knows how to use her scientific knowledge and sexual prowess for evil. This bombshell character type was portrayed by Alison Doody as Dr. Elsa Schneider in \"Indiana Jones and The Last Crusade\".\n\nThis role of female scientist encompasses many feminine stereotypes portrayed in movies. In this role the woman is subordinate to her male counterpart, who is either her father or her lover. She is smart and capable, but her secondary role does not allow her to demonstrate her abilities. Flicker writes that when this woman plays the role of lover to the male scientist, \"her work place is limited to the bed.\" She is only good for sexual satisfaction, not for the degree she earned. The assistant role is seen in the female Dr. Medford in the film \"Them!\", as she is portrayed alongside an older gentleman of the same name.\n\nThis type of woman scientist is intelligent, attractive, and somewhat independent. Flicker says that she \"has appropriated some male traits,\" such as losing herself in her work. She is both sexual and smart, and she manages to exhibit both qualities in the film. Despite this, she is still subordinate to the men on her team, and depends on them and their work in order to gain respect. She is the most progressive of the woman scientist types, but she lacks her own form of independence and still must rely on a sexual relationship with a man in order to be seen as someone. The \"lonely heroine\" type is best seen in Jodie Foster's portrayal of Elleonore Arroway in the film \"Contact\".\nFlicker argues that women are often pigeonholed into these six limiting roles when written in films. Each of these roles places the female scientist character on the sidelines, and does not allow her to be on the same level as her male counterpart(s). Despite the fact that the women in these roles are educated, and often just as educated as the men on their team, they are used primarily as assistants and sexual characters. Producers strategically write women's roles for the male gaze, often making the female characters use their \"weapons of a woman,\" such as sex appeal, to be attractive to male characters and viewers alike.\n\nFeminist film theorist Laura Mulvey writes that in film, women are passive objects of the male gaze. Mulvey writes that movies fulfill \"a primordial wish for pleasurable looking,\" and that male audiences are largely catered to in the film industry. In her analysis of film, she states that the lead woman in a film often falls in love with her male counterpart, and when she does, she only exists as a character to please him. Through the male character's ownership of the woman, the men in the audience find themselves owning her as well.\n\nMale gaze is a big factor to consider when looking at female scientists and how they are portrayed in films. Typically women are viewed as sexual objects for pleasure of males who view these films. This has a direct effect on how people interpret women scientists and their role in movies. Instead of being portrayed as superheroes, they begin to obtain a reputation based on sexual appeal.\n\nConsidering superhero films, Amy Shackelford mentions how the male gaze is applied to hypersexualize the female character, further misinterpreting women in the media through visual depiction. Looking at these particular films, screenwriters have a difficult time accomplishing the task of writing female characters. Shackelford also states that it seems like the only way these screenwriters know how to portray female characters in superhero films power is to hypersexualize them.\n\nJudith Mayne supports Laura Mulvey's view. She writes that \"most feminist film theory and criticism of the last decade\" has been in written in response to Mulvey's 1975 assessment, \"Visual Pleasure and Narrative Cinema.\" She argues that understanding the often sexist portrayal of women in film requires \"an understanding of patriarchy as oppressive and as vulnerable.\" Mayne goes deeper in her argument claiming that feminist film theory inspired feminist documentaries that are \"aimed at rejecting stereotyped images of women.\" This criticism also opens the question about \"the notion of woman as 'image.'\"\n\nLaw professor Sarah Eschholz and her colleagues Jana Bufkin and Jenny Long write that in film, women are often young, and female characters are rarely played by middle aged or older women. Often the only role available to these women is that of the mother, who is not meant to be a leading character. They write that \"females' primary societal value is based on physical appearance and youthful beauty.\" According to their assessment, men are valued at all ages, and arguably more so as they age and become wiser. Most women in film are 35 years old or younger, while their male costars are often older. Despite women in film having impressive credentials and extensive educations, they are often reduced to objects for looking, due to a reluctance to hire an older, less attractive woman for a major role.\n\nIn the traditional husband and wife family, women are often portrayed as the second in command. Their husbands take on the role of family head and get to maintain a bachelor level of freedom, which allows them to work and spend time out with the guys. Eschholz, Bufkin, and Long report on studies that show female characters are more likely to be married and have a family than male characters. Men have the freedom to work and be protagonists through their actions, while their wives or girlfriends are forced to take a back seat in the story in order to care for the family.\n\nNöel Carroll references Mulvey's pivotal paper on psychoanalysis and visual pleasure in his writing, and plays devil's advocate to her claim that women are the only subjects of gaze. Carroll acknowledges and agrees with Mulvey's assessment that women in film are strategically placed for the male gaze despite the role of their actual character. Carroll states, \"Women in Hollywood film are staged and blocked for the purpose of male erotic contemplation and pleasure.\" However, Carroll adds that men in films are also strategically placed for the purpose of pleasure. He cites such examples as Sylvester Stallone and Arnold Schwarzenegger, big bodybuilding actors \"whose scenes are blocked and staged precisely to afford spectacles of bulging pectorals and other parts.\" Similarly to actresses, male actors are also heralded for their facial attractiveness and are sometimes lauded exclusively for being attractive. As an example, Carroll brings in Leslie Howard, an actor in \"Of Human Bondage\" and \"Gone With the Wind,\" who was highly successful in the industry despite being \"staggeringly ineffectual.\" According to Carroll, being subject to the erotic gaze of the audience is not an exclusively female burden; rather, both sexes fall prey to Hollywood camera angles that best show off their bodies.\n\nKristin Thompson, an American film theorist, analyzes the film \"Laura.\" In her analysis, she claims that the main character, Laura, was written to embody the role of \"passive visual object,\" which Mulvey and Flicker claim is an extremely prevalent role of women in film. However, Thompson, like Carroll, does not believe that this passive role is limited to women in the industry. Thompson claims that Mulvey's assessment stating that women are used as objects and men cannot handle being the subject of gaze is \"common but not universal\" in the film industry. She claims that men are also presented in flashy ways in film, and gives the example of Howard Keel in the film \"Show Boat.\" Her analysis aligns with Carroll's conclusion that both sexes must be the subject of the audience's gaze, and that objectification is all-encompassing.\n\nStudies have shown that female scientists are either underrepresented or misrepresented as film characters. As Eva Flicker writes, film has a way of taking social realities and expressing women in . The media formats then are able to influence the audience by creating a mirror of metaphors, myths, opinions and a social memory resulting in stereotypes. The ways that women scientists are portrayed in films have contributed to viewers holding certain stereotypes of scientists. A 2007 meta-analysis by Jocelyn Steinke of Western Michigan University and colleagues looked at gender stereotyping by children who have been exposed to images of scientists through films, television shows, and books. One study reported on consisted of elementary school students taking the Draw-a-Scientist-Test, or DAST. The results showed that out of the over 4,000 children who participated in the DAST, only 28 girls drew female scientists. Another study of 1,137 Korean students between the ages of eleven and fifteen found that 74 percent of them drew male scientists, while only 16 percent had depicted female scientists. The study includes the result that gender stereotypes can be the product of the environment a person possess and can influence how a person views themselves and others around them. As social media platforms are being used more and more each day by adolescences, the reinforcement of cultural norms is at an all time high. Even before young women reach adolescence, social media platforms transform their images of women into the different stereotypes of dependent, emotional and less capable beings.\n\n"}
{"id": "10989188", "url": "https://en.wikipedia.org/wiki?curid=10989188", "title": "Retinalophototroph", "text": "Retinalophototroph\n\nA retinalophototroph is a phototroph that uses either bacteriorhodopsin or proteorhodopsin for the light-driven production of proton-motive force.\n"}
{"id": "449743", "url": "https://en.wikipedia.org/wiki?curid=449743", "title": "Rush hour", "text": "Rush hour\n\nA rush hour (American English, British English) or peak hour (Australian English) is a part of the day during which traffic congestion on roads and crowding on public transport is at its highest. Normally, this happens twice every weekday; once in the morning and once in the afternoon or evening, the times during which the most people commute. The term is often used for a period of peak congestion that may last for more than one hour.\nThe term is very broad, but often refers specifically to private automobile transportation traffic, even when there is a large volume of cars on a road but not a large number of people, or if the volume is normal but there is some disruption of speed. By analogy to vehicular traffic, the term Internet rush hour has been used to describe periods of peak data network usage, resulting in delays and slower delivery of data packets.\n\nThe name is sometimes a misnomer, as the peak period often lasts more than one hour and the \"rush\" refers to the volume of traffic, not the speed of its flow. Rush hour may be 6–10 am (6:00–10:00) and 4–8 pm (16:00–20:00). Peak traffic periods may vary from city to city, from region to region, and seasonally.\n\nThe frequency of public transport service is usually higher in the rush hour, and longer trains or larger vehicles are often used. However, the increase in capacity is often less than the increased number of passengers, due to the limits on available vehicles, staff and, in the case of rail transport, track capacity including platform length. The resulting crowding may force many passengers to stand, and others may be unable to board. If there is inadequate capacity, this can make public transport less attractive, leading to higher car use and partly shifting the congestion to roads.\n\nTransport demand management, such as road pricing or a congestion charge, is designed to induce people to alter their travel timing to minimize congestion. Similarly, public transport fares may be higher during peak periods; this is often presented as an off peak discount for single fares. Season tickets or multi-ride tickets, sold at a discount, are commonly used in rush hours by commuters, and may or may not reflect rush hour fare differentials.\n\nStaggered hours have been promoted as a means of spreading demand across a longer time span, for example in Rush Hour (1941 film) and by the International Labour Office.\n\nIn Australia, Sydney, Brisbane and Melbourne and in New Zealand, Auckland and Christchurch are usually the most congested cities in the morning 6–9am, and 4:30–7pm. In Melbourne the Monash Freeway, which connects Melbourne's suburban sprawl, to the city is usually heavily congested each morning and evening. In Perth, Mitchell Freeway, Kwinana Freeway and various arterial roads are usually congested between peak hours, making movement between suburbs and the city quite slow.\n\nEfforts to minimise traffic congestion during peak hour vary on a state by state and city by city basis.\n\nIn Melbourne, congestion is managed by means including:\n\nIn Sydney, congestion is managed by many means including:\n\nTraffic congestion is managed through the Traffic Management Centre via a network of Closed Circuit TV's, with operators able to change the timing of traffic signals to reduce wait times\n\nIn São Paulo, Brazil, each vehicle is assigned a certain day of the week in which it cannot travel the roads during rush hour (7–10 am and 5–8 pm). The day of the week for each vehicle is derived from the last digit in the licence plate number and the rule is enforced by traffic police. This policy is aimed at reducing the number of vehicles on the roads and encouraging the use of buses, subway and the urban train systems.\n\nIn Toronto, rush hour typically lasts from 8:00-9:00 in the morning and later from 2:00 PM until at least 7:30-8:00 PM. Montreal, however, has rush hour times from 6:30-8:30 AM and 3:30-6:00 PM.\n\nIn the cities of Edmonton and Calgary, rush hour typically lasts from 7:00-9:00 AM and begins again at 2:30-6:00 PM. The overwhelming traffic causes significant delays on freeways and commuter routes, most notably being Anthony Henday Drive in Edmonton, where the province has committed to widening, and Deerfoot Trail in Calgary.\n\nChina is home to some of the busiest subway networks in the world. Despite aggressive expansion of rapid transit networks in the past decade, rapid urban population growth has put heavy demand on urban transport. Some systems routinely restrict station entrances and transfer passages to prevent the network from being overwhelmed. For example, 96 subway stations in the Beijing Subway have entry restrictions at some point of the day. The Guangzhou Metro has 51 stations with passenger flow restrictions.\n\nIn the \"pico y placa\" (peak and license plate) program in Bogotá, drivers of non-commercial automobiles are prevented from driving them during rush hours on certain days of the week. The vehicles barred each day are determined by the last digit of their license plate. The measure is mandatory and those who break it are penalized. The digits banned each day are rotated every year.\n\nIn the capital city of Athens the rush hours are usually 7–10am and 4–7pm. During these periods there is congestion in the Athens Mass Transit System, most notably in buses and metro, as well as road traffic. The 6-car trains of Athens Metro carries almost 1.5 million passengers during a typical week day.\n\nIn Japan, the proportion of rail transportation is high compared with the use of automobiles. Rail transport accounts for 27% of all passenger transport in Japan (other examples: Germany (7.7%), United Kingdom (6.4%), United States (0.6%)). In the Greater Tokyo Area and the Keihanshin metropolitan area there is a dense rail network and frequent service, which accounts for more than half of the passenger transport; most people in the area commute by public transport without using cars.\n\nRailways in the Greater Tokyo Area are severely congested. This is gradually being improved by increasing rail capacity and expanding Home Liner and bi-level Green car (First-class) services. But it is still common on major lines in Tokyo for more than 3,000 passengers to be packed in a 10-car train, and about 100,000 passengers to be transported per hour (usually, the maximum capacity of double-track commuter rail in Japan is 10-car trains at two-minute intervals).\n\nIn road transport, Expressways of Japan is operated by on a beneficiaries-pay principle which imposes expensive toll fees, having the effect of reducing road traffic. Electronic toll collection (ETC) is widespread and discounts during low-traffic periods has been introduced to disperse traffic over a wider period than the rush hour. Road pricing is being considered but has not been introduced, partly because the expressway fee is already very high.\n\nFor trains in the Netherlands there is an off-peak discount available, giving a 40% discount. Its validity starts at 9am (until 4am the next morning) on weekdays, and all day at weekends and in July and August. In the case of a group of up to four people, all get the discount even if only one has a pass.\n\nRail passes not requiring an additional ticket come in two versions: for a fixed route, and for the whole network. Both are mainly used by commuters. No off-peak discount version of these passes is offered since there is insufficient demand; commuters usually cannot avoid the rush hour.\n\nInside Metro Manila, the Unified Vehicular Volume Reduction Program, popularly known as the \"number coding scheme\", is implemented by the Metropolitan Manila Development Authority. The program stipulates that vehicles are prohibited from plying all roads within the metropolis, depending on the last digit of their license plates and on the day of the week.\n\nThe vehicles are banned from 7am to 7pm. Unlike the public vehicles, the private vehicles have a five-hour window exception which runs from 10am to 3pm. However, the cities of Makati and San Juan do not implement the five-hour window.\n\nThis table shows the license plates with numbers ending with its corresponding days:\n\nExempted from the program are motorcycles, school buses, shuttle buses, ambulances, fire engines, police cars, military vehicles, those carrying a person needing immediate medical attention, and vehicles with diplomatic license plates.\n\nOn the other hand, in other places, there are certain policies the municipal or city government are proposing or has implemented for the whole municipality or city.\n\nWhile most schools are open, peak hours in rapid transit trains on Manila Metro Rail Transit System and Manila Light Rail Transit System, and in commuter trains on Philippine National Railways are 6-9am and 4-8pm.\n\nIn Singapore, there is a free travel scheme before 7:45am and 50 cent discount between 7.45am and 8.00am, which applies only if you exit and not enter at the 18 CBD stations. This is an attempt to encourage commuters' travel on the MRT outside the crowded weekday morning peak. Electronic Road Pricing is intended to discourage driving between 7.30am and 8pm. In addition, employees were given travel incentives through Travel Smart programme. Peak hours are defined as follows: 7:30-9:30am and 5pm-8pm, with different times for terminal stations.\n\nIn London, Peak Day Travelcards allow travel at all hours. Off-peak Day Travelcards are 20-50% cheaper but are valid for travel only after 9:30am and on weekends. This is an attempt to encourage commuters' travel on the London Underground, Docklands Light Railway, buses, and trams outside of the crowded weekday morning peak. There is a similar system on Transport (Bus and Tyne and Wear Metro) in the Newcastle upon Tyne area. In London, congestion charges are intended to discourage driving 7am–6pm.\n\nIn Manchester, the Metrolink light rail system offers single, return and 'Metromax' daysaver tickets at a reduced price when they are purchased after 9:30am. This incentive is designed to lure passengers into avoiding the daily crowded conditions at Metrolink stations during rush hour.\n\nFor 16–25 Railcard holders, the offer of one-third off ticket prices is valid only after 10am (unless a minimum fare is paid) or weekends. This restriction does not apply in July and August, the main summer holiday season.\n\nFor other Railcards, other restrictions apply; for example, the Family Railcard and Network Railcard cannot be used for peak journeys within London and south-east England.\n\nEfforts to manage transportation demand during rush hour periods vary by state and by metropolitan area. In some states, freeways have designated lanes that become HOV (High-Occupancy Vehicle aka car-pooling) only during rush hours, while open to all vehicles at other times. In others, such as the Massachusetts portion of I-93, travel is permitted in the breakdown lane during this time. Several states use ramp meters to regulate traffic entering freeways during rush hour. Transportation officials in Colorado and Minnesota have added value pricing to some urban freeways around Denver, the Twin Cities, and Seattle, charging motorists a higher toll during peak periods.\n\nTransit agencies – such as Metro North serving New York City, WMATA serving Washington DC, and BART serving San Francisco – often charge riders a higher \"peak fare\" for travel during the morning and evening rush hour.\n\nMorning rush hour times can range 6–10am in cities like New York. Some New York commuters try to be on the road by at least 6am because traffic gets heavy between 6:30 and 9:30am. Many train commuters leave early to get the best seats on the trains, because by 7am the trains are packed with passengers standing or those who cannot get on. Los Angeles, California has several rush hours, including a midnight rush for night workers. Bus and train service (such as Metrolink) in Los Angeles are limited and tend to be underused, but their use is increasing. In the Chicago area people use Metra Trains, the 'L', and buses.\n\nIn Northeast Ohio, near Cleveland, morning rush hour is 7–9am, with the peak 7:30–8:30am. Because of Cleveland's compact size, most people can be in Downtown Cleveland within 10–45 minutes. The Greater Cleveland Regional Transit Authority runs buses every half hour and some routes have non-stop freeway buses that run during rush hour.\n\nThere is also an afternoon rush hour. For example, in the New York City area, the afternoon rush hour can begin as early as 3pm and last until 7pm. Some people who live in Connecticut but work in New York often do not arrive home until 7pm or later. On the other hand, in a smaller city like Cleveland, the afternoon rush hour takes place in a more literal sense such that heavy traffic congestion typically only occurs between 5 and 6pm. Usually the RTA in Cleveland has an afternoon rush hour schedule like the morning.\nThe city of Philadelphia is known for its very dangerous Schuylkill Expressway, much of which predates the 1956 introduction of the Interstate Highway System. One of the busiest highways in the country (and state of Pennsylvania) and with the road being highly over capacity, it has become notorious for its chronic congestion, especially during rush hour. Rush hour in Philadelphia is usually as early as 6am, with many in the Delaware Valley using the Schuylkill to reach Central Philadelphia and some of Philadelphia's western suburbs. The rugged terrain, limited riverfront space covered by the route and narrow spans of bridges passing over the highway have largely stymied later attempts to upgrade or widen the highway. An average 163,000 vehicles use the road daily in Philadelphia County, and an average of 109,000 use the highway in Montgomery County. Its narrow lane and left shoulder configuration, left lane entrances and exits (nicknamed \"merge or die\"), common construction activity and generally congested conditions have led to many accidents, critical injuries and fatalities, leading to the highway's humorous nickname of the \"Surekill Expressway\" or in further embellishment, \"Surekill Distressway\".\n\nBoston and the larger Greater Boston region are notorious for traffic congestion due to the region's high population density, outmoded highway system, and economic growth resulting in a high concentration of corporations with large offices located along major expressways and urban loops (including Route 128, MassPike, I-93, and I-495). Despite the region's compact nature, inbound traffic becomes very heavy on all expressways as early as 6am on a typical weekday morning, making an inbound drive from the suburbs as long as 75 minutes. Improvements brought by the infamous Big Dig project temporarily improved expressway traffic within Boston's city limits, but traffic congestion soon returned, also appearing in areas such as the rapidly-developing Seaport District area of South Boston.\n\nCities such as Atlanta, Houston, Austin, Boston, Chicago, Honolulu, New York, Los Angeles, Philadelphia, San Francisco, and Washington DC, to name a few, are ranked as having the worst traffic in the country. Los Angeles also has the highest amount of time spent in congestion, followed by Honolulu and Washington DC.\n\nThe term \"third rush hour\" has been used to refer to a period of the midday in which roads in urban and suburban areas become congested due to a large number of people taking lunch breaks using their vehicles. These motorists often frequent restaurants and fast food locations, where vehicles crowding the entrances cause traffic congestion. Active retirees, who travel by automobile to engage in many midday activities, also contribute to the midday rush hour. Areas which have large school-age populations may also experience added congestion due to the large number of school buses and kiss-and-ride traffic that flood the roads after lunch, but before the evening rush hour. In many European countries (e.g. Germany, Austria, Hungary) the schools are only half-day and many people work only half-time too. This causes a third rush hour around 12:30–2pm, which diverts some traffic from the evening rush hour, thus leaving the morning rush hour the most intense period of the day.\n\nAnother usage of \"third rush hour\" can be to describe congestion later at night (generally between 10-11pm and 2-3am the next morning, particularly on Thursdays, Fridays, and Saturdays) of people returning home from nights spent out at restaurants, bars, nightclubs, casinos, concerts, amusement parks, movie theaters, and sporting events. At other times (such as evenings and weekends), additional periods of congestion can be the result of various special events, such as sports competitions, festivals, or religious services. Out-of-the-ordinary congestion can be the result of an accident, construction, long holiday weekends, or inclement weather.\n"}
{"id": "48347082", "url": "https://en.wikipedia.org/wiki?curid=48347082", "title": "Scope (logic)", "text": "Scope (logic)\n\nIn logic, the scope of a quantifier or a quantification is the range in the formula where the quantifier \"engages in\". It is put right after the quantifier, often in parentheses. Some authors describe this as including the variable put right after the forall or exists symbol. In the formula , for example, (or ) is the scope of the quantifier (or ).\n\nA variable in the formula is free, if and only if it does not occur in the scope of any quantifier for that variable. A term is free for a variable in the formula (i.e. free to substitute that variable that occurs free), if and only if that variable does not occur free in the scope of any quantifier for any variable in the term.\n\n"}
{"id": "718855", "url": "https://en.wikipedia.org/wiki?curid=718855", "title": "Self-organized criticality", "text": "Self-organized criticality\n\nIn physics, self-organized criticality (SOC) is a property of dynamical systems that have a critical point as an attractor. Their macroscopic behavior thus displays the spatial and/or temporal scale-invariance characteristic of the critical point of a phase transition, but without the need to tune control parameters to a precise value, because the system, effectively, tunes itself as it evolves towards criticality.\n\nThe concept was put forward by Per Bak, Chao Tang and Kurt Wiesenfeld (\"BTW\") in a paper \npublished in 1987 in \"Physical Review Letters\", and is considered to be one of the mechanisms by which complexity arises in nature. Its concepts have been enthusiastically applied across fields as diverse as geophysics, physical cosmology, evolutionary biology and ecology, bio-inspired computing and optimization (mathematics), economics, quantum gravity, sociology, solar physics, plasma physics, neurobiology and others.\n\nSOC is typically observed in slowly driven non-equilibrium systems with extended degrees of freedom and a high level of nonlinearity. Many individual examples have been identified since BTW's original paper, but to date there is no known set of general characteristics that \"guarantee\" a system will display SOC.\n\nSelf-organized criticality is one of a number of important discoveries made in statistical physics and related fields over the latter half of the 20th century, discoveries which relate particularly to the study of complexity in nature. For example, the study of cellular automata, from the early discoveries of Stanislaw Ulam and John von Neumann through to John Conway's Game of Life and the extensive work of Stephen Wolfram, made it clear that complexity could be generated as an emergent feature of extended systems with simple local interactions. Over a similar period of time, Benoît Mandelbrot's large body of work on fractals showed that much complexity in nature could be described by certain ubiquitous mathematical laws, while the extensive study of phase transitions carried out in the 1960s and 1970s showed how scale invariant phenomena such as fractals and power laws emerged at the critical point between phases.\nHowever, the term Self-Organized Criticality was firstly introduced by Bak, Tang and Wiesenfeld's 1987 paper which clearly linked together these factors: a simple cellular automaton was shown to produce several characteristic features observed in natural complexity (fractal geometry, pink (1/f) noise and power laws) in a way that could be linked to critical-point phenomena. Crucially, however, the paper emphasized that the complexity observed emerged in a robust manner that did not depend on finely tuned details of the system: variable parameters in the model could be changed widely without affecting the emergence of critical behavior (hence, \"self-organized\" criticality). Thus, the key result of BTW's paper was its discovery of a mechanism by which the emergence of complexity from simple local interactions could be \"spontaneous\"—and therefore plausible as a source of natural complexity—rather than something that was only possible in the lab (or lab computer) where it was possible to tune control parameters to precise values. The publication of this research sparked considerable interest from both theoreticians and experimentalists, and important papers on the subject are among the most cited papers in the scientific literature.\n\nDue to BTW's metaphorical visualization of their model as a \"sandpile\" on which new sand grains were being slowly sprinkled to cause \"avalanches\", much of the initial experimental work tended to focus on examining real avalanches in granular matter, the most famous and extensive such study probably being the Oslo ricepile experiment. Other experiments include those carried out on magnetic-domain patterns, the Barkhausen effect and vortices in superconductors. Early theoretical work included the development of a variety of alternative SOC-generating dynamics distinct from the BTW model, attempts to prove model properties analytically (including calculating the critical exponents), and examination of the necessary conditions for SOC to emerge. One of the important issues for the latter investigation was whether conservation of energy was required in the local dynamical exchanges of models: the answer in general is no, but with (minor) reservations, as some exchange dynamics (such as those of BTW) do require local conservation at least on average. In the long term, key theoretical issues yet to be resolved include the calculation of the possible universality classes of SOC behavior and the question of whether it is possible to derive a general rule for determining if an arbitrary algorithm displays SOC.\n\nAlongside these largely lab-based approaches, many other investigations have centered around large-scale natural or social systems that are known (or suspected) to display scale-invariant behavior. Although these approaches were not always welcomed (at least initially) by specialists in the subjects examined, SOC has nevertheless become established as a strong candidate for explaining a number of natural phenomena, including: earthquakes (which, long before SOC was discovered, were known as a source of scale-invariant behavior such as the Gutenberg–Richter law describing the statistical distribution of earthquake sizes and the Omori law describing the frequency of aftershocks, and where models that displayed SOC were proposed and analyzed prior to the BTW 87 paper;); solar flares; fluctuations in economic systems such as financial markets (references to SOC are common in econophysics); landscape formation; forest fires; landslides; epidemics; neuronal avalanches in cortex; 1/f noise in the amplitude envelope of electrophysiological signals; and biological evolution (where SOC has been invoked, for example, as the dynamical mechanism behind the theory of \"punctuated equilibria\" put forward by Niles Eldredge and Stephen Jay Gould). These \"applied\" investigations of SOC have included both attempts at modelling (either developing new models or adapting existing ones to the specifics of a given natural system) and extensive data analysis to determine the existence and/or characteristics of natural scaling laws.\n\nIn addition, SOC has been linked to computation. Recently, it has been found that the avalanches from an SOC process, like the BTW model, make effective patterns in a random search for optimal solutions on graphs. \nAn example of such an optimization problem is graph coloring. The SOC process apparently helps the optimization from getting stuck in a local optimum without the use of any annealing scheme, as suggested by previous work on extremal optimization.\n\nThe recent excitement generated by scale-free networks has raised some interesting new questions for SOC-related research: a number of different SOC models have been shown to generate such networks as an emergent phenomenon, as opposed to the simpler models proposed by network researchers where the network tends to be assumed to exist independently of any physical space or dynamics. While many single phenomena have been shown to exhibit scale free properties over narrow ranges, by far the most extensive data concern the solvent-accessible surface areas of globular proteins, plotted for modular segments centered on a specific amino acid, against lengths L, with 9 <= L <= 35.\nBecause this fractal linearity holds for all 20 amino acids over the same range, it exists independently of any model. It can be regarded as the basic mechanism of life. These studies quantify the differential geometry of proteins, and resolve many evolutionary puzzles regarding the biological emergence of complexity.\n\nDespite the considerable interest and research output generated from the SOC hypothesis there remains no general agreement with regards to its mathematical mechanisms. Bak Tang and Wiesenfeld based their hypothesis on the behavior of their sandpile model. However, this model was subsequently shown to actually generate 1/f noise rather than 1/f noise.\nOther simulation models were proposed later that could produce true 1/f noise, and experimental sandpile models were observed to yield 1/f noise. In addition to the nonconservative theoretical model mentioned above, other theoretical models for SOC have been based upon information theory, \nmean field theory,\nand cluster formation. A continuous model of self-organised criticality is proposed by using tropical geometry.\n\nIn chronological order of development:\n\n\n\n\n"}
{"id": "46788584", "url": "https://en.wikipedia.org/wiki?curid=46788584", "title": "Space Shuttle recovery convoy", "text": "Space Shuttle recovery convoy\n\nThe Space Shuttle recovery convoy was a fleet of ground vehicles, many of which were specially-designed for their purpose, staged at the landing site of a Space Shuttle orbiter which assist the crew in egress and safe the vehicle and its payload after landing. Some vehicles and equipment which were very specific to the shuttle program were retired and either sold at auction or transferred to museums for public display. The majority of convoy vehicles were stored in buildings near the Shuttle Landing Facility.\n\n"}
{"id": "7932827", "url": "https://en.wikipedia.org/wiki?curid=7932827", "title": "Spatial multiplexing", "text": "Spatial multiplexing\n\nSpatial multiplexing (often abbreviated SM or SMX) is a transmission technique in MIMO wireless communication, Fibre-optic communication and other communications technologies to transmit independent and separately encoded data signals, known as \"streams\". Therefore, the space dimension is reused, or multiplexed, more than one time. \n\n=Wireless communications=\nIf the transmitter is equipped with formula_1 antennas and the receiver has formula_2 antennas, the maximum spatial multiplexing order (the number of streams) is,\n\nif a linear receiver is used. This means that formula_4 streams can be transmitted in parallel, ideally leading to an formula_4 increase of the spectral efficiency (the number of bits per second per Hz that can be transmitted over the wireless channel). The practical multiplexing gain can be limited by spatial correlation, which means that some of the parallel streams may have very weak channel gains.\n\nIn an open-loop MIMO system with formula_1 transmitter antennas and formula_2 receiver antennas, the input-output relationship can be described as\nwhere formula_9 is the formula_10 vector of transmitted symbols, formula_11 are the formula_12 vectors of received symbols and noise respectively and formula_13 is the formula_14 matrix of channel coefficients. An often encountered problem in open loop spatial multiplexing is to guard against instance of high channel correlation and strong power imbalances between the multiple streams. One such extension which is being considered for DVB-NGH systems is the so-called \"enhanced Spatial Multiplexing (eSM)\" scheme.\n\nA closed-loop MIMO system utilizes Channel State Information (CSI) at the transmitter. In most cases, only partial CSI is available at the transmitter because of the limitations of the feedback channel. In a closed-loop MIMO system the input-output relationship with a closed-loop approach can be described as\nwhere formula_16 is the formula_17 vector of transmitted symbols, formula_11 are the formula_19 vectors of received symbols and noise respectively, formula_13 is the formula_21 matrix of channel coefficients and formula_22 is the formula_23 linear precoding matrix. \n\nA precoding matrix formula_22 is used to precode the symbols in the vector to enhance the performance. The column dimension formula_4 of formula_22 can be selected smaller than formula_1 which is useful if the system requires formula_28 streams because of several reasons. Examples of the reasons are as follows: either the rank of the MIMO channel or the number of receiver antennas is smaller than the number of transmit antennas.\n\n"}
{"id": "48044910", "url": "https://en.wikipedia.org/wiki?curid=48044910", "title": "The Beginnings of Western Science", "text": "The Beginnings of Western Science\n\nThe Beginnings of Western Science, subtitled \"The European Scientific Tradition in Philosophical, Religious, and Institutional Context, 600 B.C. to A.D. 1450\" (1992 edition) or \"The European Scientific Tradition in Philosophical, Religious, and Institutional Context, Prehistory to A.D. 1450\" (2007 edition), is an introductory book on the history of science by David C. Lindberg. The book focuses on what is called Western science, prominently covering Greek, Roman, Islamic and Mediaeval European science, while the 2007 second edition expands on precursors to Greek science, such as Mesopotamian and Egyptian science, and on Islamic science. \"Western science\" is defined as scientific enquiry done in Greek, Latin or Arabic according to reviewer William A. Wallace.\n\n\"The Beginnings of Western Science\" focuses on the theoretical dimension of classical and mediaeval science, giving less attention to technology and scientific craft. Lindberg defends a moderate view on the continuity thesis, accepting continuity between early modern science and mediaeval antecedents but also identifying a scientific revolution in the cosmology and metaphysics behind science. Vivian Nutton states that Lindberg emphasizes discontinuities over continuities between mediaeval and renaissance science.\n\nThe book has found wide acclaim and is considered a good general introduction to the history of science in the West before the Renaissance.\n\nThe expanded edition begins with debates over definitions of science. Next, possible precursors to science are treated in the forms of knowledge prehistoric societies and theories about the notions of truth that would apply in such cultures. The section about Egyptian and Babylonian science starts with the differences in their types of arithmetic. Lindberg states that the Babylonian notation is superior because of its greater parsimony. Babylonian astrology, with the important development of horoscopic astrology, is mentioned as a major contribution of Babylonian civilization. Egyptian and Babylonian medicine are described as systemizations of diagnoses, which often included non-naturalistic formulations.\n\nGreek science first covers the philosophies of the pre-Socratics, Plato and Aristotle, after which the narrative continues with the Hellenistic philosophical schools, the Academy, the Lyceum, the Epicureans and the Stoics. Major figures in mathematics are Euclid and Archimedes, and Eudoxus and Claudius Ptolemy (discussed as part of Hellenistic astronomy) in astronomy.\n\nA central theme underlying the narrative of \"The Beginnings of Western Science\" is that classical and mediaeval science must be treated relative to their context, according to their own standards. F. Jamil Ragep also states that the rejection of presentism is an important theme in the book.\n\nSeveral reviewers commented on the book's take on science and religion. Vivian Nutton mentions that one point that Lindberg makes is that religious traditions have influenced the development of science. Jamil Ragep notes that the warfare thesis is rejected.\n\nMethodology was another topic that commenters singled out. Angela Smith identifies the discussion of the methodological importance of mathematics and experimentation in science as a recurrent theme throughout \"The Beginnings of Western Science\". Jamil Ragep says that the notion that Aristotelian methodology was a positive contribution to science is tacitly accepted in the book.\n\nAccording to Smith, other themes in \"The Beginnings of Western Science\" are problems with definitions of science and \"the philosophical implications of different concepts of change\". Nutton maintains that Lindberg portrays science as a tradition that \"may be no less difficult\" to maintain than to create.\n\nPeter Harrison called the first edition \"[t]he best general introduction to classical and medieval science.\" Vivian Nutton considered the work \"solid and accurate\", but lamented Lindberg's traditional choice of subject matter. He besides thought that his limited inclusion of late mediaeval scientists, particularly in the field of medicine, hurt his argument to stress discontinuity with the renaissance era. William A. Wallace considered \"The Beginnings of Western Science\" \"most welcome\", but also stated he had reservations about the omission of several details he considered necessary for an introduction and criticized Lindberg's decision to comment on the continuity thesis while his book doesn't include renaissance science. F. Jamil Ragep criticizes the book's first edition as Eurocentric. He notes that it claimed that Islamic science declined in the thirteenth and fourteenth centuries and hardly anything remained by the fifteenth century. The book also didn't emphasize the role of Babylonian science, especially in mathematics. Richard C. Dales also thought that Lindberg undervalued Babylonian, but also Egyptian science. He in addition critiques Lindberg's description of twelfth-century science until the translation movement. His overall verdict is nonetheless that \"The Beginnings of Western Science\" is a \"triumph\" and \"an authoritative account of Western science from its beginnings to the height of medieval scientific achievement\".\n\nAngela Smith considered the second edition of \"The Beginnings of Western Science\" a substantial success and thought it \"a fundamental and reliable resource for many years to come\". She appraised the improvements in the treatment of Islamic science to include recent scholarship in particular. Jamil Ragep calls it \"a fine book, the culmination of a century of distinguished research on premodern European science\" but also contends that the second edition still has a Eurocentric bias. He argues for inclusion of Chinese, Indian and Central-Asian influences on Western science.\n\n\n"}
{"id": "37329515", "url": "https://en.wikipedia.org/wiki?curid=37329515", "title": "Thermales", "text": "Thermales\n\nThermales is an order of bacteria belonging to the Deinococcus–Thermus phylum. They are particularly resistant to heat, and live in the benthic zone of the Gulf of Mexico.\n"}
{"id": "21852557", "url": "https://en.wikipedia.org/wiki?curid=21852557", "title": "Why Beauty Is Truth", "text": "Why Beauty Is Truth\n\nWhy Beauty Is Truth: A History of Symmetry is a 2007 book by Ian Stewart.\n\nFollowing the life and work of famous mathematicians from antiquity to the present, Stewart traces mathematics' developing handling of the concept of symmetry. One of the very first takeaways, established in the preface of this book, is that it dispels the idea of the origins of symmetry in geometry, as is often the first context in which the term is introduced. This book, through its chapters, establishes its origins in algebra, more specifically group theory.\n\nThe topics covered are:\n\n\n—\"Plus Magazine\"\n"}
