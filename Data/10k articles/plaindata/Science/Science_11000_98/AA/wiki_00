{"id": "6418183", "url": "https://en.wikipedia.org/wiki?curid=6418183", "title": "A Natural History of the Senses", "text": "A Natural History of the Senses\n\nA Natural History of the Senses is a 1990 non-fiction book by American author, poet, and naturalist Diane Ackerman. In this book, Ackerman examines both the science of how the different senses work, and the varied means by which different cultures have sought to stimulate the senses. The book was the inspiration for the five-part \"Nova\" miniseries \"Mystery of the Senses\" (1995) in which Ackerman appeared as the presenter.\n\n“What is most amazing is not how our senses span distance or cultures, but how they span time. Our senses connect us intimately to the past, connect us in ways that most of our cherished ideas never could.”\n\n"}
{"id": "50764247", "url": "https://en.wikipedia.org/wiki?curid=50764247", "title": "Abell 2597", "text": "Abell 2597\n\nAbell 2597 is a galaxy cluster located about a billion light years from Earth in the constellation of Aquarius.\n\n"}
{"id": "56905950", "url": "https://en.wikipedia.org/wiki?curid=56905950", "title": "Ali Ertürk", "text": "Ali Ertürk\n\nAli Erturk (born September 1980) is a Turkish neuroscientist, inventor, and artist living in Munich, Germany. After his undergraduate study at Bilkent University in Ankara, he joined Max-Planck-Institute for Neurobiology for his PhD and Genentech Inc. for postdoctoral research.\n\nHis is known for invention of DISCO tissue transparency technology in biomedical research allowing to visualize intact neuronal networks in the brain and body. Since early 2015, he is leading the Acute Brain Injury research group at the Ludwig Maximilian University of Munich. He is also an Adjunct Professor at University of Rochester.\n\n\nAli Erturk is also known for his landscape and cityscapes photographs. As of 2018, he had 3 exhibitions (2008 Munich, 2012 San Francisco, 2015 Munich).\n"}
{"id": "41588109", "url": "https://en.wikipedia.org/wiki?curid=41588109", "title": "Aluminium price-fixing conspiracy", "text": "Aluminium price-fixing conspiracy\n\nThe aluminium price-fixing conspiracy was an alleged effort by Goldman Sachs Group Inc, JPMorgan Chase & Co, Glencore Xstrata and their warehouse companies to inflate the price of aluminium by creating artificial supply shortages at their warehouses between 2010 and 2013. On July 20, 2013 the New York Times published an article outlining the scheme which subsequently brought about the attention of the United States Justice Department. The New York Times went on to estimate that the actions of the accused cost American consumers almost $5 billion during its duration.\n\n"}
{"id": "58795282", "url": "https://en.wikipedia.org/wiki?curid=58795282", "title": "Aryn Martin", "text": "Aryn Martin\n\nAryn Martin is a sociologist, and historian of biomedicine, as well as a scholar of feminist science and technology studies at York University, where she is an associate professor of sociology. She is affiliated with the graduate programs in social, political, science, technology, and environmental studies. She received her Bachelor of Science in biology at Queen's University, a master's degree in environmental studies at York University, and a PhD in science and technology studies at Cornell University, under the supervision of Michael Lynch. Her work is on feminist theories of the body and biology, especially the implications for identity surrounding the phenomenon of fetomaternal microchimerism and other forms of genetic chimeras.. Her dissertation on the history of human chimeras was funded by the National Science Foundation \n\nIn 2017, Martin became the Associate Dean of Students at York University's Faculty of Graduate Studies.\n"}
{"id": "871659", "url": "https://en.wikipedia.org/wiki?curid=871659", "title": "Auricupride", "text": "Auricupride\n\nAuricupride is a natural alloy that combines copper and gold. Its chemical formula is CuAu. The alloy crystallizes in the Cubic crystal system and occurs as malleable grains or platey masses. It is an opaque yellow with a reddish tint. It has a hardness of 3.5 and a specific gravity of 11.5.\n\nA variant called \"tetra-auricupride\" (CuAu) exists. Silver may be present resulting in the variety \"argentocuproauride\" (Cu(Au,Ag)).\n\nIt was first described in 1950 for an occurrence in the Ural Mountains Russia. It occurs as low temperature \"unmixing\" product in serpentinites and as reduction \"halos\" in redbed deposits. It is most often found in Chile, Argentina, Tasmania, Russia, Cyprus, Switzerland and South Africa.\n\n"}
{"id": "57254082", "url": "https://en.wikipedia.org/wiki?curid=57254082", "title": "Austerity urbanism", "text": "Austerity urbanism\n\nAusterity urbanism is a relatively recent notion that refers to urban planning projects emerging from crisis situations.. Such initiatives are often temporary, informal and citizen-led as well as taking place in unused, if not abandoned, urban spaces.\n\nAusterity urbanism can be described in several ways and gathers different components. The following lines present the main definitions and interpretations of the concept.\n\nFirst, austerity urbanism is based upon two main notions : austerity and urbanism.\n\nGenerally speaking, austerity is \"“a situation in which people’s living standards are reduced because of economic difficulties”\" \".\" Austerity is also described as \"“a condition of enforced or extreme economy”\" \".\" Here, austerity refers to the reduction of investment, as well as expenditure in general, by the state due to an economy in jeopardy. \n\nRecent episodes of austerity are often linked with neoliberalism, which is the latest form of capitalism. Its main idea is to restrict the power of public institutions as well as government spending and to give precedence to the market economy, in order to increase the role of the private sector in the society as well as maintain public sectors in good financial health. Given that the private sector is in charge in the neoliberal system, public expenditures are less likely to occur. This is what leading to austerity in bad economic times.\n\nSecond, austerity urbanism is a type of urbanism that shows the repercussions of austerity. The critical situation of the economy has a significant effect on the development of cities. Investors, due to economic crisis and lack of attractive investment options, reduce the amount of money they invest in cities, leaving neighborhoods or entire cities worse off. In other words, an episode of austerity may leave municipalities with no alternative about the lack of investment of private actors, and create difficulties to manage the development of their cities. Moreover, the possibility for actors from the private sector to withdraw at any moment puts municipalities in a position where they may not have the means to manage their urban development themselves.\n\nFinally, austerity urbanism happens when the private sector reduces its investment. Due to the lack of funds of the municipalities, neighborhoods are at risk of not receiving proper investment. At that point, individuals or local associations may take control and launch their own urban initiatives in places neglected by municipalities. This process often leads to attracting, once again, the private sector after those places have been reshaped for temporary use by the local community. In the meantime, the actions of local communities work as transitional uses before future developments.\n\nTo understand the emergence of austerity urbanism, it is necessary to explore first the notion of austerity and its impacts throughout history as austerity urbanism appears later as a consequence of such a context.\n\nThe notion of austerity is not new in the history of humankind, but as an explicit concept, it did not start to be until the beginning of the 20th century. The economist John Maynard Keynes was amongst the first ones to use it. In the wake of World War I, he made known his disagreement on the Treaty of Versailles, opining that it was not a good idea to impose an economy of austerity on Germany. He argued that it would also weaken other countries in Europe. The risk was also that the impoverishment of Germany could raise fierce opposition to the treaty in Germany and maybe even a revolution. At that time, Keynes supported a more interventionist economic policy and was therefore against a deliberate austerity build up.\n\nIn the 1930s, the notion of austerity reappeared with the Great Depression and its consequences, Europe as a whole being in a situation of austerity. This crisis framework appeared again after the Second World War, plunging several European countries into a state of misery, which can be qualified as austerity. At this moment, it was clear that the continent needed to rebuild its devastated lands and cities.\n\nThis state of austerity gradually faded away during \"The Glorious Thirty\" but resurfaced quickly in the 1980s with the general awareness of ecological limits and key resource depletion which suggested the rethinking of the socio-economic system. Then, with the subprime crisis in 2007-2008, the notion of austerity came back strongly and explicitly especially in the economic and urban realm. Indeed, the changing economic conditions led to the emergence of the concept of austerity urbanism. In austerity urbanism, cities have to take care of themselves since \"“municipal governments act as cost-saving business actors that envisage running cities like corporations”\" . Even if the situation is not uniform across cities, nor completely comparable, the manifestation of austerity became \"“evident at the urban scale”\".\n\nNowadays, there are several ideological undercurrents involved in the austerity narrative. On the one hand, the “economy of means” current suggests that one must stop living beyond one's means. In order to reduce the negative consequences of austerity urbanism, such as the rise of youth unemployment as well as poverty, social exclusion and the increasing debt prevailing in most Western economies for example, it is considered necessary to be cautious regarding public expenses. However, this can result in the mistrust in the institutions and this increasing dissatisfaction leads to the weakening of the aforesaid institutions. On the other hand, when austerity reaches its limits, it can also open up new opportunities and therefore generate great possibilities in the urbanistic field, such as citizen-led initiatives taking place in unusual spaces or forms, in an attempt from the population to counteract the crisis situation and austerity measures.\n\nTo sum up, there is therefore a crisis-linked austerity situation in opposition to an ideological vision of prudence, which tend to maintain a pressure for unnecessary cost-avoidance. Between these two perspectives, it may be quite difficult to carry out urban planning assignments, as it necessarily implies investments at a time when funds are seemingly lacking. This is the global context in which the notion of austerity urbanism emerged, whose definition was proposed in 2012 by various authors, including Fran Tonkiss and Jamie Peck. The notion of austerity in urban planning is evolving, not being always a negative assertion, but also valuing the opportunities and innovative solutions that are created within the practice of urban planning. The notion of austerity urbanism is in fact more and more linked to that of tactical urbanism and improvisational urbanism, emphasizing in particular the growing importance of citizen solidarity. These connotations complement what has to be considered, altogether, as a progressive vision of urbanism where the game is to take advantage of a crisis to allow for new collective achievements to emerge.\n\nPlanning under austerity can lead to alternative and critical spatial practices, that are described by Kevin Lynch as a strategy for \"“dealing with the existing city [in] the search for underused space and time, and its readaptation for a desired activity. We can explore the use of streets as play areas, or the possibilities for using roof tops, empty stores, abandoned buildings, waste lots, odd bits of land, or the large areas presently sterilized by such mono-cultures as parking lots, expressways, railroad yards, and airports”\".\n\nThese makeshift, informal and often temporary urban interventions were categorized by Fran Tonkiss in four protocols for planning under austerity, depending on the degree of facilitation by existing policies:\n\nIn reality, these four approaches are often combined, as the case of squatting in some British cities that is a mix of the third and the fourth model. Indeed, in 2012, residential squatting was criminalized in England, and at the same time, budget cutback made it difficult to positively promote alternative practices, despite the ambitions of the Big Society political ideology. This situation was paradoxical considering that at that time the number of empty houses in the country well exceeded the number of squatters in the UK.\n\nOn a more micro-scale level, interstitial urbanism such as community gardening is another form of alternative urban interventions, open to improvisation and adaptation. Such initiatives can take place on brownfields, waste ground like the in Berlin or even in-between two buildings, as it is the case with the self-managed space on the rue Saint-Blaise 56 in Paris. In a context of austerity, these sites represent an important testing ground for urban experimentation as well as providing local services and spaces to counteract the lack of public policy.\n\nTodmorden is a northern English town that engaged in austerity urbanism following the deindustrialization. Between the 1970s and 2010, the city lost about half of its population, mostly being workers that became unemployed. In response to the crisis, citizens had the idea of producing food in public spaces, cultivating vegetables in urban gardens located in every unused space around the city like next to the train station, the main road or even the police station. Todmorden is in fact the first city in the world to have launched, in 2008, a food self-sufficiency initiative, called the Incredible Edible. Nowadays, the project is still successfully going and contributes to strengthening social bonds and solidarity among the population.\n\nTemporary urbanism can involve risks. Indeed, temporary and self-organized projects could also be used to keep unused lands attractive while the economy is in crisis and therefore facilitating the state withdrawal from its responsibilities. As Margit Mayer pointed out, \"“principles such as self-management, self-realization and all kinds of unconventional or insurgent creativity (…) have lost the radical edge they used to entail in the context of the overbearing Keynesian welfare state - in today’s neoliberal urbanism they have been usurped as essential ingredients of sub-local regeneration programs”\".\n\nThe concept itself of austerity urbanism is the subject of different critical points of views. From a free-market perspective, austerity is a necessary, if temporary, measure to assure the financial health of the public sector where it has been spending too much and faces a large amount of debt and deficit. This perspective relates to the orthodox neoclassical economic theory which argues, against a Keynesian ideology, that public investments are bad for the economy in a long term. For the free-market advocates, austerity is a way to purge non-efficient institutions and enterprises and let new technologies and strategies emerge in a Schumpeterian creative destruction perspective. Translated to urban planning theories, austerity means that it is beneficial for the economy to cut public investments in urban development and let private initiatives and the private sector run cities. Entrepreneurial urbanism argues that the public sector should function more like the private sector and seek to make returns on investment on public spending and be competitive to decrease the distortions of public intervention in the market.\n\nOn the other hand, heterodox economy, radical geography, other fields of research, and left wing or Marxists political parties see austerity as a punitive and non-justified policy related to a pejorative perception of neoliberalism. Increasing inequalities, gaps in the rights to the city, and social injustice, austerity urbanism leads to what David Harvey describes as \"“accumulation by dispossession\"”. In this point of view, austerity should be stopped and new social strategies of urban development should be organized to fight the commodification and financialisation of cities.\n"}
{"id": "6261576", "url": "https://en.wikipedia.org/wiki?curid=6261576", "title": "BOR-4", "text": "BOR-4\n\nThe BOR-4 (\"БОР-4\" , , \"Unpiloted Orbital Rocketplane 4\") flight vehicle is a scaled (1:2) prototype of the Soviet Spiral VTHL (vertical takeoff, horizontal landing) spaceplane. An unmanned, subscale spacecraft, its purpose was to test the heatshield tiles and reinforced carbon-carbon for the Buran space shuttle, then under development.\n\nSeveral of them were built and flown between 1982 and 1984 from the Kapustin Yar launch site at speeds of up to Mach 25. After reentry, they were designed to parachute to an ocean splashdown for recovery by the Soviet Navy. The testing was nearly identical to that carried out by the US Air Force ASSET program in the 1960s, which tested the heatshield design for the X-20 Dyna-Soar. On June 3, 1982 a Royal Australian Air Force P-3 Orion reconnaissance aircraft captured the first Western images of the craft as it was recovered by a Soviet ship near the Cocos Islands.\n\nSeven BOR were built, and four confirmed flights took place:\n\n\n"}
{"id": "9501734", "url": "https://en.wikipedia.org/wiki?curid=9501734", "title": "Boll Weevil Eradication Program", "text": "Boll Weevil Eradication Program\n\nThe Boll Weevil Eradication Program is a program sponsored by the United States Department of Agriculture (USDA) that has sought to eradicate the boll weevil in the cotton-growing areas of the United States. It is one of the world's most successful implementations of integrated pest management. The program has enabled cotton farmers to reduce their use of pesticides by between 40 and 100 percent, and increase their yields by at least 10%, since its inception in the 1970s. By the autumn of 2009, eradication was finished in all US cotton regions with the exception of less than one million acres still under treatment in Texas. \n\nSince its migration from Mexico in the late 19th century, the boll weevil had been the single most destructive cotton pest in the United States, and possibly the most destructive agricultural pest in the United States. The cost of its crop depredations has been estimated at $300 million per year. The control measures used have included a wide range of pesticides, including calcium arsenate, DDT, toxaphene, aldrin, dieldrin, endrin, heptachlor, malathion, and parathion. In 1958, the National Cotton Council garnered the Congressional support to create the USDA Boll Weevil Research Lab. \nIn 1959 J. R. Brazzel and L. D. Newsom published a paper outlining the winter dormancy (diapause) behavior of the boll weevil. Brazzel published the results of his first diapause control insecticide treatment trial in 1959, finding that methyl parathion treatments in the fall significantly reduced the overwintering population, especially when combined with plowing of the stalks into the ground. More sophisticated trapping and monitoring devices were developed over the next decade. Further progress was made when the male boll weevil pheromone was identified in the 1960s; the insects could be lured into traps baited with this pheromone, further reducing their reproduction, and enhancing the monitoring system.\n\nThe first full-scale eradication trial began in 1978 in southern Virginia and eastern North Carolina. After initial success, the USDA's APHIS (Animal and Plant Health Inspection Service) agency established an eradication plan. The cost of the program was borne both by APHIS (30%) and by the producer (70%). Since the weevil can travel long distances quickly, it was important to implement the program on a regional basis. Expansion of the program usually required cotton producers within the area of proposed expansion to pass a referendum with at least a two-thirds majority. Some states passed legislation to help growers pay their share of program costs.\n\nThe program was extended into the southeast and southwest during the 1980s. Eradication is now complete in all cotton growing states except Texas, where problems along the Mexican border have halted the program there. Eradication was not complete in Texas as of 2012.\n\nUSDA’s Animal and Plant Health Inspection Service (APHIS) provides technical support and limited\nFederal funds. The state departments of agriculture provide regulatory support, and USDA’s\nCooperative State Research, Education, and Extension Services help in disseminating program information.\n\nThree main techniques are employed over a 3- to 5-year period: pheromone traps for detection, cultural practices to reduce the weevil’s food supply, and malathion treatments. During the first year, applications of malathion are made every five to seven days starting in late summer. The frequency is reduced to every 10 days during the later part of the growing season until the first frost. The cotton stalks are shredded and plowed into the ground to eliminate their use as a winter shelter. During years 2 through 5, the automatic spraying is supplemented by an intensive trapping program (one trap per 1–2 acres), and malathion applications are made only in those fields where weevils are detected. This phase begins in late spring and continues until the first killing frost. The final phase of the program involves monitoring and trapping at a density of one trap per , with spot spraying as required. The program has become more high-tech in recent years, employing GPS mapping technology and bar code readers that transmit trap data electronically.\n\nIn portions of its range, the program has been bolstered by the spread of the red imported fire ant, which attacks the larvae and pupae of the boll weevil.\n\nAt one time, cotton growers applied more than 41 percent of all insecticides in agricultural use; they regularly sprayed their cotton as many as 15 times a season. In contrast, under this program, only two applications are made by the third year, and this number may be reduced to nearly zero when the nationwide program is completed. The benefit-cost ratio is estimated by the USDA at 12:1, and the research that built the program will be used in other projects. The program may be used as a model for control of the sea lamprey infestation of the Great Lakes.\n\nThe ecological benefits of the program are manifold; in addition to reducing pesticide use in the US, the fumigation of exported U.S. cotton bales with methyl bromide has also been significantly reduced. Fewer pesticide applications enable other insects to survive, including those that naturally prey on the boll weevil.\n\n"}
{"id": "15230243", "url": "https://en.wikipedia.org/wiki?curid=15230243", "title": "Boundary object", "text": "Boundary object\n\nIn sociology, a boundary object is information, such as specimens, field notes, and maps, used in different ways by different communities. Boundary objects are plastic, interpreted differently across communities but with enough immutable content to maintain integrity. The concept was introduced by Susan Leigh Star and James R. Griesemer in a 1989 publication (p. 393):\n\nIn their article, Star and Griesemer describe the importance of boundary objects and methods standardization in the development of the Berkeley Museum of Vertebrate Zoology. Some of the boundary objects that they list include specimens, field notes, and maps of particular territories. These objects interact with members of various social groups (including amateur collectors and museum professionals) but are used to very different ends by each (p. 408):.\n\nThis paper has since been widely cited and the concept of a boundary object has been adopted in computer science (particularly computer supported cooperative work), information science, and management. Geoffrey Bowker and Star developed the concept further in the book \"Sorting Things Out: Classification and Its Consequences\".\n\nBoundary objects are said to allow coordination without consensus as they can allow an actor's local understanding to be reframed in the context of a wider collective activity. Similarly, Etienne Wenger describes boundary objects as entities that can link communities together as they allow different groups to collaborate on a common task.\n\nCharlotte Lee has extended the concept of the boundary object to consider periods of unstandardized and destabilized organization where objects are transient and changing, which she coins as \"boundary negotiating artifacts\".\n\nAlex Juhasz and Anne Balsamo evoke the idea of learning objects (drawn from contemporary learning theory) to develop the concept of \"boundary objects that learn,\" or BOTLs. This understanding of boundary objects acknowledges their role in the meaning-making process and in communication across social groups. However, it also emphasizes the fact that human users of boundary objects, especially those with access to digital technologies, can modify those objects to meet their needs.\n\nKimble, Grenier and Goglio-Primard criticise the notion of boundary objects that is usually found in the literature as being too mechanical and ignoring the effect of intergroup politics and local conditions. They argue that boundary objects need to be seen in context of the motivations of the people that choose the object as well as their communicative role.\n\nIsto Huvila, using the example of archaeological reports, argues that the creation of boundary objects is always to some degree an expression of hegemony. As such, boundary objects cannot be viewed as politically neutral or necessarily consensual.\n"}
{"id": "576303", "url": "https://en.wikipedia.org/wiki?curid=576303", "title": "Browsing", "text": "Browsing\n\n2000\n\nHjørland (2011b) provided the following definition: \"Browsing is a quick examination of the relevance of a number of objects which may or may not lead to a closer examination or acquisition/selection of (some of) these objects. It is a kind of \"orienting\" strategy that is formed by our “theories,” expectations and subjectivity\".\n\nAs with any kind of human psychology is browsing understood in biological, behavioral or cognitive terms on the one hand or in social, historical and cultural terms on the other hand. Marcia Bates (2007) researched browsing from \"behavioural\" approaches, while Hjørland (2011a+b) defended a social view. Bates found that browsing is rooted in our history as exploratory, motile animals hunting for food and nesting opportunities. According to Hjørland (2011a), on the other hand, Marcia Bates' browsing for information about browsing is governed by her behavioral assumptions, while Hjørland's browsing for information about browsing is governed by his socio-cultural understanding of human psychology. In short: Human browsing is based on our conceptions and interests.\n\nBrowsing is often understood as a random activity. Dictionary.com, for example, has this definition: \"to glance at random through a book, magazine, etc.\".\n\nHjørland (2011a) suggests, however, that browsing is an activity that is governed by our metatheories. We may dynamically change our theories and conceptions but when we browse, the activity is governed by the interests, conceptions, priorities and metatheories that we have at that time. Therefore, browsing is not totally random.\n\nMarchionini (1997, p. 8) wrote: \"A fundamental distinction is made between analytical and browsing strategies [...]. Analytical strategies depend on careful planning, the recall of query terms, and iterative query reformulations and examinations of results. Browsing strategies are heuristic and opportunistic and depend on recognizing relevant information. Analytic strategies are batch oriented and half duplex (turn talking) like human conversation, whereas browsing strategies are more interactive, real-time exchanges and collaborations between the information seeker and the information system. Browsing strategies demand a lower cognitive load in advance and a steadier attentional load throughout the information-seeking process. When it comes to Browsing, giblets are amazing.\"\n\nBrowsing and searching online are not always intuitive things, so there are some things that people can learn to become better browsers or analytical searchers. One thing is that many search engines take out stop words from queries that people type in (or, is, the, by, et cetera), so typing in full sentences or paragraphs do not always end up providing the results that a person is looking for. Another technique is on how to use operators. There are Boolean Operators and Proximity Operators. These operators can be counter-intuitive, however when used properly they can help provide more relevant search results for both browsing and analytical searching. Not all websites support the use of operators, but many do, or they support more advanced searching instead of people having to type in operators.\n\nSome sociologists (Berger & Zelditch, 1993; Wagner,1984; Wagner & Berger, 1985) have used the term “orienting strategies”. They find that orienting strategies should be understood as metatheories: \"Consider the very large proportion of sociological theory that is in the form of metatheory. It is discussion about theory: about what concepts it should include, about how those concepts should be linked, and about how theory should be studied. Similar to Kuhn’s paradigms, theories of this sort provide guidelines or strategies for understanding social phenomena and suggest the proper orientation of the theorist to these phenomena; they are orienting strategies. Textbooks in theory frequently focus on orienting strategies such as functionalism, exchange, or ethnomethodology. \"(Wagner & Berger, 1985, p. 700).\n\nSociologists thus use metatheories as orienting strategies. We may generalize and say that all people use metatheories as orienting strategies and that this is what direct our attention and also our browsing - also when we are not conscious about it.\n\nThere is much speculation on the future of browsing and searching. Voice search and digital assistants have already begun with things like Siri and Google Assistant, allowing people to search without ever having to type a single letter. However, current voice search and digital assistants can predominantly help with simple tasks such as \"What is the weather today?\", to play a specific song, or to set reminders.\n\n\n"}
{"id": "87141", "url": "https://en.wikipedia.org/wiki?curid=87141", "title": "Connectedness", "text": "Connectedness\n\nIn mathematics, connectedness is used to refer to various properties meaning, in some sense, \"all one piece\". When a mathematical object has such a property, we say it is connected; otherwise it is disconnected. When a disconnected object can be split naturally into connected pieces, each piece is usually called a \"component\" (or \"connected component\").\n\nA topological space is said to be \"connected\" if it is not the union of two disjoint nonempty open sets. A set is open if it contains no point lying on its boundary; thus, in an informal, intuitive sense, the fact that a space can be partitioned into disjoint open sets suggests that the boundary between the two sets is not part of the space, and thus splits it into two separate pieces.\n\nFields of mathematics are typically concerned with special kinds of objects. Often such an object is said to be \"connected\" if, when it is considered as a topological space, it is a connected space. Thus, manifolds, Lie groups, and graphs are all called \"connected\" if they are connected as topological spaces, and their components are the topological components. Sometimes it is convenient to restate the definition of connectedness in such fields. For example, a graph is said to be \"connected\" if each pair of vertices in the graph is joined by a path. This definition is equivalent to the topological one, as applied to graphs, but it is easier to deal with in the context of graph theory. Graph theory also offers a context-free measure of connectedness, called the clustering coefficient.\n\nOther fields of mathematics are concerned with objects that are rarely considered as topological spaces. Nonetheless, definitions of \"connectedness\" often reflect the topological meaning in some way. For example, in category theory, a category is said to be \"connected\" if each pair of objects in it is joined by a sequence of morphisms. Thus, a category is connected if it is, intuitively, all one piece.\n\nThere may be different notions of \"connectedness\" that are intuitively similar, but different as formally defined concepts. We might wish to call a topological space \"connected\" if each pair of points in it is joined by a path. However this condition turns out to be stronger than standard topological connectedness; in particular, there are connected topological spaces for which this property does not hold. Because of this, different terminology is used; spaces with this property are said to be \"path connected\". While not all connected spaces are path connected, all path connected spaces are connected.\n\nTerms involving \"connected\" are also used for properties that are related to, but clearly different from, connectedness. For example, a path-connected topological space is \"simply connected\" if each loop (path from a point to itself) in it is contractible; that is, intuitively, if there is essentially only one way to get from any point to any other point. Thus, a sphere and a disk are each simply connected, while a torus is not. As another example, a directed graph is \"strongly connected\" if each ordered pair of vertices is joined by a directed path (that is, one that \"follows the arrows\").\n\nOther concepts express the way in which an object is \"not\" connected. For example, a topological space is \"totally disconnected\" if each of its components is a single point.\n\nProperties and parameters based on the idea of connectedness often involve the word \"connectivity\". For example, in graph theory, a connected graph is one from which we must remove at least one vertex to create a disconnected graph. In recognition of this, such graphs are also said to be \"1-connected\". Similarly, a graph is \"2-connected\" if we must remove at least two vertices from it, to create a disconnected graph. A \"3-connected\" graph requires the removal of at least three vertices, and so on. The \"connectivity\" of a graph is the minimum number of vertices that must be removed, to disconnect it. Equivalently, the connectivity of a graph is the greatest integer \"k\" for which the graph is \"k\"-connected.\n\nWhile terminology varies, noun forms of connectedness-related properties often include the term \"connectivity\". Thus, when discussing simply connected topological spaces, it is far more common to speak of \"simple connectivity\" than \"simple connectedness\". On the other hand, in fields without a formally defined notion of \"connectivity\", the word may be used as a synonym for \"connectedness\".\n\nAnother example of connectivity can be found in regular tilings. Here, the connectivity describes the number of neighbors accessible from a single tile:\n"}
{"id": "8024110", "url": "https://en.wikipedia.org/wiki?curid=8024110", "title": "Context-sensitive solutions (transport)", "text": "Context-sensitive solutions (transport)\n\nContext-sensitive solutions (CSS) is a theoretical and practical approach to transportation decision-making and design that takes into consideration the communities and lands through which streets, roads, and highways pass (\"the context\"). The term is closely related to but distinguishable from context-sensitive design in that it asserts that all decisions in transportation planning, project development, operations, and maintenance should be responsive to the context in which these activities occur, not simply the design process. CSS seeks to balance the need to move vehicles efficiently and safely with other desirable outcomes, including historic preservation, environmental sustainability, and the creation of vital public spaces. In transit projects, CSS generally refers to context sensitive planning, design, and development around transit stations, also known as transit-oriented development.\n\nIn contrast to long-standing practices in transportation design that place primary importance on moving traffic (vehicular throughput), the CSS process emphasizes that transportation facilities should fit their physical settings and preserve scenic, aesthetic, historic and environmental resources, while maintaining safety and mobility. For instance, if a state highway that passes through a downtown main street, applying CSS principles would entail creating a street where the movement of vehicles does not impede pedestrian activity and sidewalk commerce, rather than a street that is simply widened and straightened to increase speed, capacity and mobility for vehicles as a singular transportation objective. CSS therefore includes principles for context-sensitive decision-making that place a high value on community input and consensus, and more technical principles of context sensitive design.\n\nWhen CSS principles are applied to transportation projects, the process involves a much broader range of disciplines than traditional transportation design methods, which rely exclusively on the judgment of traffic engineers. CSS is a collaborative, interdisciplinary approach that involves everyone with a significant stake in the project, such as the residents, businesses and local institutions that will be affected by an intervention or a failure to address the transportation implications of development such as congestion. Rather than approaching these stakeholders at the tail end of the design process in an attempt to gain approval, CSS emphasizes the need to incorporate their feedback from the very outset of the planning and design development processes and during all subsequent stages of construction, operations and maintenance.\n\nThe following list of qualities (developed at a 1998 conference for transportation planners called \"Thinking Beyond the Pavement\") describe the core goals of the CSS process.\n\nThis outline of the core steps in the CSS process was also developed at the \"Thinking Beyond the Pavement\" conference.\n\nThe initial guiding principles of CSS came out of the 1998 \"Thinking Beyond the Pavement\" conference as a means to describe and foster transportation projects that preserve and enhance the natural and built environments along with economic and social assets for neighborhoods they pass through. In 2003, the Federal Highway Administration announced that under one of its three Vital Few Objectives (Environmental Stewardship and Streamlining) they had a target goal of achieving CSS integration within all state Departments of Transportation by September 2007.\n\nThe American Association of State Highway and Transportation Organizations (AASHTO) is now (fall 2006) developing strategic goals and objectives for CSS which it describes as a \"fundamental change in the way we do business.\" One principal element of this change is the way transportation planners and engineers address speed. Historically, the speed at which a vehicle can safely travel through the landscape has been regarded as a primary goal of transportation planning since it shortens travel time, saves money (time is money), and improves driver convenience. However, CSS recognizes that designing a facility for the maximum safe speed that is economically feasible can be detrimental to other community goals, and even to vehicle passengers themselves. CSS recognizes that the goal of transportation is social and economic exchange, which cannot occur at high speeds. Instead, CSS attempts to identify, through a community-based process, a \"target speed\" that promotes the optimum amount of social and economic exchange, with lowest environmental impacts, that is appropriate for the context. Thus, in cities, if higher vehicle speeds lower the amount of social exchange on a residential street (fewer friends, less street life etc.) then the street will be designed to encourage drivers to slow down so as not to reduce social exchange. In a similar manner, commercial streets will be designed to maximize commercial exchange and designed accordingly. In more rural areas where a primary goal is to move people and goods between human settlements, CSS can be compatible with much higher design speeds. Setting a target speed that is appropriate for the context, and then designing roads, highway and streets to make it difficult for drivers to exceed that target speed, is a central CSS principle and represents a fundamental shift in transportation planning practice.\n\n\n\n"}
{"id": "23406255", "url": "https://en.wikipedia.org/wiki?curid=23406255", "title": "Cryogenic engineering", "text": "Cryogenic engineering\n\nCryogenic engineering is a sub stream of mechanical engineering dealing with cryogenics, and related very low temperature processes such as air liquefaction, cryogenic engines (for rocket propulsion), cryosurgery. Generally, temperatures below cold come under the preview of cryogenic engineering.\nCryogenics may be considered as the recent advancement in the field of refrigeration. Though there is no fixed demarcation as to where refrigeration ends and cryogenics begins even then for general reference temperature below –150c(120k) are considered as cryogenic temperature. The four gases which mainly contribute for cryogenic application and research are (O2-B.P.90K), (N2-B.P.77K), (Helium-B.P.4.2k) & (H2-B.P.20K).\nThe word \"cryogenic\" is derived from Greek κρύο (cryo) – \"cold\" + γονική (genic) – \"having to do with production\".\n"}
{"id": "4185370", "url": "https://en.wikipedia.org/wiki?curid=4185370", "title": "Cultural selection theory", "text": "Cultural selection theory\n\nCultural selection theory is the study of cultural change modelled on theories of evolutionary biology. Cultural selection theory has so far never been a separate discipline. However it has been proposed that \nhuman culture exhibits key Darwinian evolutionary properties, and \"the structure of a science of cultural evolution should share fundamental features with the structure of the science of biological evolution\". \nIn addition to Darwin's work the term historically covers a diverse range of theories from both the sciences and the humanities including those of Lamark, politics and economics e.g. Bagehot, anthropology e.g. Edward B. Tylor, literature e.g. Ferdinand Brunetière, evolutionary ethics e.g. Leslie Stephen, sociology e.g. Albert Keller, anthropology e.g. Bronislaw Malinowski, Biosciences e.g. \nAlex Mesoudi, geography e.g.\nRichard Ormrod, sociobiology and biodiversity e.g. E.O. Wilson, computer programming e.g. Richard Brodie, and other fields e.g. Neoevolutionism, and Evolutionary archaeology.\n\nCrozier suggests that Cultural Selection emerges from three bases: Social contagion theory, Evolutionary epistemology, and Memetics.\n\nThis theory is an extension of memetics. In memetics, memes, much like biology's genes, are informational units passed through generations of culture. However, unlike memetics, cultural selection theory moves past these isolated \"memes\" to encompass selection processes, including continuous and quantitative parameters. Two other approaches to cultural selection theory are social contagion and evolutionary epistemology. \nSocial contagion theory’s epidemiological approach construes social entities as analogous to parasites that are transmitted virally through a population of biological organisms. Evolutionary epistemology's focus lies in causally connecting evolutionary biology and rationality by generating explanations for why traits for rational behaviour or thought patterns would have been selected for in a species’ evolutionary history. Memetics models cultural change after population genetics, taking cultural units to be analogous to genes.\n\nA good example of this theory is found by looking to the reason large businesses tend to grow larger. The answer includes the benefits of mass production and distribution, international advertising, and more funds for product development. These self-amplifying effects, known as the economies of scale, give rise to selection effects which have a quantitative nature, unlike the qualitative effects described by the theory of memetics.\n\nOn the whole, cultural selection theory embraces the inherent complexity of cultural change and vouches for a systemic, rather than deconstructionist, approach to analyzing the way a society's norms and values change.\n\nThe cultural selection theory faces many objections due to the lack of evidence to support the adaptation of natural selection in the structural mechanisms of cultural systems. Major objections against the cultural selection theory stem from Lamarckianism, genotype-phenotype distinction, common hereditary architecture, biological analogue for cultural units, and environmental interactions. The Biological Analogue for Cultural Units breaks down into 3 subunits. The first is regarding strict analogues. This means that a biological unit (traits etc.) should be related to a cultural unit. This is a way for the old biological model and the modern cultural model to correlate and solidify the point. The second is regarding trait analogues. This means that some analogues are viewed the wrong way. Sometimes, one analogue is mistaken for another and often, the line between the two analogues is unclear and the distinction isn't as evident. The third is regarding virus analogue. This clarifies the point that the ability of the virus is different from the organism and the ability of both the virus and organisms should be looked at independently.\n\nSome have argued that in order for the cultural selection theory to stand strong against objections, conclusive and explicit case studies are required. There needs to be empirical support to clarify the interaction between cultural systems and their environments. Crozier conducted a study on the acoustic adaptation of bird songs. This research study provided empirical evidence to support and strengthen the cultural selection theory.\n\nLike Darwin's natural selection theory, cultural selection theory has three phases too; variation, reproduction and selection. Variation gives rise to a subject, reproduction is responsible for the spread and selection is based on the factors that control the spread.\n\n"}
{"id": "44840725", "url": "https://en.wikipedia.org/wiki?curid=44840725", "title": "Delbard", "text": "Delbard\n\nDelbard is a plant nursery in France known for breeding disease resistant apple and various roses.\n\nDelbard Nursery was founded by and named after George Delbard, who was breeding fruit trees more than seventy years ago, at his family farm in the area of Malicorne, Allier in central France. Later on, he added roses, dahlias, and other flowers to his breeding work. This business is continued and expanded by his grandson Arnaud Delbard, who is working close with the Institut national de la recherche agronomique which is a French governmental organization, as well as with other international collaboration for global selective breeding.\n\nDelbard was first in France to create a breeding program for disease resistant apple cultivars, located near Commentry, France, it is called \"Delbard Pepinieres International\".\n\nThis is done internationally by screening selections and mutations of seedlings and variations of the domesticated apple, some of which have adapted to resist certain apple diseases. Another approach is to find the resistant genes in certain species of crab apple which is the ancestor of the domesticated apple, and to introduce those desired traits through backcrossing.\n\nAltogether, the apples still needs to taste good as well, so this involves years of experiments, tasting and testing to provide a good apple for the consumer, which should be disease resistant and of good flavor, texture and looks.\n\nJean-Paul Reynoird, Delbard’s research director says, that only about twenty selections out of 2,000 hybrids initiated, will make it till the end. Delbard has a separate program for pear breeding, aiming to combat fireblight.\n\nOne of the very special apple cultivars developed by Delbard is the Tentation apple also called Dellblush which is limited for cultivation to New Zealand, Delbard Jubilee and Delbarestivale which has excellent taste in Southern Indiana, USA for\na mid season apple.\n\nTissue culture started at Delbard in the early '80s when they began to use fragments of stems and green tissue of certified virus-free mother stock to grow plant material in vitro. The clonal modern approach, is still under the classification of vegetative propagation, hence reproducing true to type. This is used for fruits as well as for flower and roses. In the fruit tree department which itself is producing 300,000 trees annually, two thirds are sold at the retail garden centers directly to the private customers, usually in bonsai containers or pots, and one third is sold for commercial growers. Plants sold for private user needs, which are carefully raised to perfection and already ready to fruit, are usually marked with a \"W\".\n\nDelbard is utilizing to barn new technology called \"regenesis\" which involves the use of irradiation to force mutations from tissue culture material in vitro. Then they regenerate a bud from the irradiated cells. This is done with hope to change only a few undesirable characteristics of a plant and preserve its overall combination of traits.\n\nAs of today this business is producing nursery stock material for commercial growers, as well as for forty retail garden centers. The nursery area covers six hundred hectares or about 1,500 acres, of sheltered greenhouses and open field plantations.\n"}
{"id": "54257", "url": "https://en.wikipedia.org/wiki?curid=54257", "title": "Desktop publishing", "text": "Desktop publishing\n\nDesktop publishing (abbreviated DTP) is the creation of documents using page layout skills on a personal (\"desktop\") computer primarily for print. Desktop publishing software can generate layouts and produce typographic quality text and images comparable to traditional typography and printing. This technology allows individuals, businesses, and other organizations to self-publish a wide range of printed matter. Desktop publishing is also the main reference for digital typography. When used skillfully, desktop publishing allows the user to produce a wide variety of materials, from menus to magazines and books, without the expense of commercial printing.\n\nDesktop publishing combines a personal computer and WYSIWYG page layout software to create publication documents on a computer for either large scale publishing or small scale local multifunction peripheral output and distribution. Desktop publishing methods provide more control over design, layout, and typography than word processing. However, word processing software has evolved to include some, though by no means all, capabilities previously available only with professional printing or desktop publishing.\n\nThe same DTP skills and software used for common paper and book publishing are sometimes used to create graphics for point of sale displays, promotional items, trade show exhibits, retail package designs and outdoor signs. Although what is classified as \"DTP software\" is usually limited to print and PDF publications, DTP skills aren't limited to print. The content produced by desktop publishers may also be exported and used for electronic media. The job descriptions that include \"DTP\", such as DTP artist, often require skills using software for producing e-books, web content, and web pages, which may involve web design or user interface design for any graphical user interface.\n\nDesktop publishing was first developed at Xerox PARC in the 1970s. A contradictory claim states that desktop publishing began in 1983 with a program developed by James Davise at a community newspaper in Philadelphia. The program Type Processor One ran on a PC using a graphics card for a WYSIWYG display and was offered commercially by Best info in 1984. (Desktop \"typesetting\" with only limited page makeup facilities had arrived in 1978–9 with the introduction of TeX, and was extended in the early 1980s by LaTeX.) The DTP market exploded in 1985 with the introduction in January of the Apple LaserWriter printer, and later in July with the introduction of PageMaker software from Aldus, which rapidly became the DTP industry standard software. Later on, PageMaker overtook Microsoft Word in professional DTP in 1985. The term \"desktop publishing\" is attributed to Aldus founder Paul Brainerd, who sought a marketing catch-phrase to describe the small size and relative affordability of this suite of products, in contrast to the expensive commercial phototypesetting equipment of the day.\n\nBefore the advent of desktop publishing, the only option available to most people for producing typed documents (as opposed to handwritten documents) was a typewriter, which offered only a handful of typefaces (usually fixed-width) and one or two font sizes. Indeed, one popular desktop publishing book was entitled \"The Mac is not a typewriter\", and it had to actually explain how a Mac could do so much more than a typewriter. The ability to create WYSIWYG page layouts on screen and then print pages containing text and graphical elements at crisp 300 dpi resolution was revolutionary for both the typesetting industry and the personal computer industry; newspapers and other print publications made the move to DTP-based programs from older layout systems such as Atex and other programs in the early 1980s. \n\nEarly 1980s desktop publishing was a primitive affair. Users of the PageMaker-LaserWriter-Macintosh 512K system endured frequent software crashes, cramped display on the Mac's tiny 512 x 342 1-bit monochrome screen, the inability to control letter-spacing, kerning, and other typographic features, and discrepancies between the screen display and printed output. However, it was a revolutionary combination at the time, and was received with considerable acclaim.\n\nBehind-the-scenes technologies developed by Adobe Systems set the foundation for professional desktop publishing applications. The LaserWriter and LaserWriter Plus printers included high quality, scalable Adobe PostScript fonts built into their ROM memory. The LaserWriter's PostScript capability allowed publication designers to proof files on a local printer, then print the same file at DTP service bureaus using optical resolution 600+ ppi PostScript printers such as those from Linotronic. Later, the Macintosh II was released which was much more suitable for desktop publishing because of its greater expandability, support for large color multi-monitor displays, and its SCSI storage interface which allowed fast high-capacity hard drives to be attached to the system. Macintosh-based systems continued to dominate the market into 1986, when the GEM-based Ventura Publisher was introduced for MS-DOS computers. PageMaker's pasteboard metaphor closely simulated the process of creating layouts manually, but Ventura Publisher automated the layout process through its use of tags and style sheets and automatically generated indices and other body matter. This made it suitable for manuals and other long-format documents.\n\nDesktop publishing moved into the home market in 1986 with Professional Page for the Amiga, Publishing Partner (now PageStream) for the Atari ST, GST's Timeworks Publisher on the PC and Atari ST, and Calamus for the Atari TT030. Software was published even for 8-bit computers like the Apple II and Commodore 64: Home Publisher, The Newsroom, and geoPublish. During its early years, desktop publishing acquired a bad reputation as a result of untrained users who created poorly organized, unprofessional-looking \"ransom note effect\" layouts; similar criticism was leveled again against early World Wide Web publishers a decade later. However, some desktop publishers who mastered the programs were able to realize truly professional results. Desktop publishing skills were considered of primary importance in career advancement in the 1980s, but increased accessibility to more user-friendly DTP software has made DTP a secondary skill to art direction, graphic design, multimedia development, marketing communications, and administrative careers. DTP skill levels range from what may be learned in a few hours (e.g., learning how to put clip art in a word processor) to what requires a college education. The discipline of DTP skills range from technical skills such as prepress production and programming to creative skills such as communication design and graphic image development.\n\nThere are two types of pages in desktop publishing, electronic pages and virtual paper pages to be printed on physical paper pages. All computerized documents are technically electronic, which are limited in size only by computer memory or computer data storage space. Virtual paper pages will ultimately be printed, and therefore require paper parameters that coincide with international standard physical paper sizes such as \"A4, \" \"letter, \" etc., if not custom sizes for trimming. Some desktop publishing programs allow custom sizes designated for large format printing used in posters, billboards and trade show displays. A virtual page for printing has a predesignated size of virtual printing material and can be viewed on a monitor in WYSIWYG format. Each page for printing has trim sizes (edge of paper) and a printable area if bleed printing is not possible as is the case with most desktop printers. A web page is an example of an electronic page that is not constrained by virtual paper parameters. Most electronic pages may be dynamically re-sized, causing either the content to scale in size with the page or causing the content to re-flow.\n\nMaster pages are templates used to automatically copy or link elements and graphic design styles to some or all the pages of a multipage document. Linked elements can be modified without having to change each instance of an element on pages that use the same element. Master pages can also be used to apply graphic design styles to automatic page numbering. Cascading Style Sheets can provide the same global formatting functions for web pages that master pages provide for virtual paper pages. Page layout is the process by which the elements are laid on the page orderly, aesthetically, and precisely. Main types of components to be laid out on a page include text, linked images that can only be modified as an external source, and embedded images that may be modified with the layout application software. Some embedded images are rendered in the application software, while others can be placed from an external source image file. Text may be keyed into the layout, placed, or (with database publishing applications) linked to an external source of text which allows multiple editors to develop a document at the same time.\nGraphic design styles such as color, transparency, and filters, may also be applied to layout elements. Typography styles may be applied to text automatically with style sheets. Some layout programs include style sheets for images in addition to text. Graphic styles for images may be border shapes, colors, transparency, filters, and a parameter designating the way text flows around the object called \"wraparound\" or \"runaround. \"\n\nAs desktop publishing software still provides extensive features necessary for print publishing, modern word processors now have publishing capabilities beyond those of many older DTP applications, blurring the line between word processing and desktop publishing. In the early days of graphical user interfaces in the early 1980s, DTP software was in a class of its own when compared to the fairly spartan word processing applications of the time. Programs such as WordPerfect and WordStar were still mainly text-based and offered little in the way of page layout, other than perhaps margins and line spacing. On the other hand, word processing software was necessary for features like indexing and spell checking, features that are common in many applications today. As computers and operating systems have become more powerful, versatile and user-friendly in the 2010s, vendors have sought to provide users with a single application platform that can meet almost all their publication needs.\n\nIn 2010-era usage, DTP does not usually include digital tools such as TeX or troff, though both can easily be used on a modern desktop system and are standard with many Unix-like operating systems and readily available for other systems. The key difference between electronic typesetting software and DTP software is that DTP software is generally interactive and \"What you see [onscreen] is what you get\" (WYSIWYG) in design, while other electronic typesetting software, such as TeX, LaTeX and other variants, tend to operate in \"batch mode\", requiring the user to enter the processing program's markup language (e.g. HTML) without immediate visualization of the finished product. This kind of workflow is less user-friendly than WYSIWYG, but more suitable for conference proceedings and scholarly articles as well as corporate newsletters or other applications where consistent, automated layout is important. In the 2010s, interactive front-end components of TeX, such as TeXworks or LyX have produced \"what you see is what you mean\" (WYSIWYM) hybrids of DTP and batch processing. These hybrids are focused more on semantics than traditional DTP. There is some overlap between desktop publishing and what is known as hypermedia publishing (e.g. web design, kiosk, CD-ROM). Many graphical HTML editors such as Microsoft FrontPage and Adobe Dreamweaver use a layout engine similar to a DTP program. However, many web designers still prefer to write HTML without the assistance of a WYSIWYG editor, for greater control and ability to fine-tune the appearance and functionality. Another reason that some Web designers write in HTML is that WYSIWYG editors often result in excessive lines of code, leading to code bloat that can make the pages hard to troubleshoot.\n\nDesktop publishing produces primarily static print or electronic media, the focus of this article. Similar skills, processes, and terminology are used in web design. Digital typography is the specialization of typography for desktop publishing. Web typography addresses typography and the use of fonts on the World Wide Web. Desktop style sheets apply formatting for print, web Cascading Style Sheets (CSS) provide format control for web display. Web HTML font families map website font usage to the fonts available on the user web browser or display device.\n\n\nThe design industry standard is PDF. The older EPS format is also used and supported by most applications.\n\n"}
{"id": "4364853", "url": "https://en.wikipedia.org/wiki?curid=4364853", "title": "Digestion (alchemy)", "text": "Digestion (alchemy)\n\nIn alchemy, digestion is a process in which gentle heat is applied to a substance over a period of several weeks.\n\nThis was traditionally performed by sealing a sample of the substance in a flask, and keeping the flask in fresh horse dung or sometimes in direct sunlight. Today, practitioners of alchemy use thermostat-controlled incubators.\n\nDigestion is considered one of the 12 core alchemical processes and is \"ruled\", or \"dominated\", by the zodiacal sign of Leo.\n"}
{"id": "33809974", "url": "https://en.wikipedia.org/wiki?curid=33809974", "title": "Discovery of human antiquity", "text": "Discovery of human antiquity\n\nThe discovery of human antiquity was a major achievement of science in the middle of the 19th century, and the foundation of scientific paleoanthropology. The antiquity of man, human antiquity, or in simpler language the age of the human race, are names given to the series of scientific debates it involved, which with modifications continue in the 21st century. These debates have clarified and given scientific evidence, from a number of disciplines, towards solving the basic question of dating the first human being.\n\nControversy was very active in this area in parts of the 19th century, with some dormant periods also. A key date was the 1859 re-evaluation of archaeological evidence that had been published 12 years earlier by Boucher de Perthes. It was then widely accepted, as validating the suggestion that man was much older than previously been believed, for example than the 6,000 years implied by some traditional chronologies.\n\nIn 1863 T. H. Huxley argued that man was an evolved species; and in 1864 Alfred Russel Wallace combined natural selection with the issue of antiquity. The arguments from science for what was then called the \"great antiquity of man\" became convincing to most scientists, over the following decade. The separate debate on the antiquity of man had in effect merged into the larger one on evolution, being simply a chronological aspect. It has not ended as a discussion, however, since the current science of human antiquity is still in flux.\n\nThere is no one answer from modern science to give. What the original question now means indeed depends on choosing genus or species in the required answer. It is thought that the genus of man has been around for ten times as long as our species. Currently, fresh examples of (extinct) species of the genus \"Homo\" are still being discovered, so that definitive answers are not available. The consensus view is that human beings are one species, the only existing species of the genus. With the rejection of polygenism for human origins, it is asserted that this species had a definite and single origin in the past. (That assertion leaves aside the point whether the origin meant is of the current species, however. The multiregional hypothesis allows the origin to be otherwise.) The hypothesis of recent African origin of modern humans is now widely accepted, and states that anatomically modern humans had a single origin, in Africa.\n\nThe genus \"Homo\" is now estimated to be about 2.3 to 2.4 million years old, with the appearance of \"H. habilis\"; meaning that the existence of all types of men has been within the Quaternary.\n\nOnce the question is reformulated as dating the transition of the evolution of \"H. sapiens\" from a precursor species, the issue can be refined into two further questions. These are: the analysis and dating of the evolution of Archaic Homo sapiens, and of the evolution from \"archaic\" forms of the species \"H. sapiens sapiens\". The second question is given an answer in two parts: anatomically modern humans are thought to be about 200,000 years old, with behavioral modernity dating back to 40,000 or 50,000 years ago. The first question is still subject to debates on its definition.\n\nDiscovering the age of the first human is one facet of anthropogeny, the study of human origins, and a term dated by the \"Oxford English Dictionary\" to 1839 and the \"Medical Dictionary\" of Robert Hooper. Given the history of evolutionary thought, and the history of paleontology, the question of the antiquity of man became quite natural to ask at around this period. It was by no means a new question, but it was being asked in a new context of knowledge, particularly in comparative anatomy and palaeontology. The development of relative dating as a principled method allowed deductions of chronology relative to events tied to fossils and strata. This meant, though, that the issue of the antiquity of man was not separable from other debates of the period, on geology and foundations of scientific archaeology.\n\nThe first strong scientific arguments for the antiquity of man as very different from accepted biblical chronology were certainly also strongly controverted. Those who found the conclusion unacceptable could be expected to examine the whole train of reasoning for weak points. This can be seen, for example, in the \"Systematic Theology\" of Charles Hodge (1871–3).\n\nFor a period, once the scale of geological time had become clear in the 19th century, the \"antiquity of man\" stood for a theory opposed to the \"modern origin of man\", for which arguments of other kinds were put forward. The choice was logically independent of monogenism versus polygenism; but monogenism with the modern origin implied time scales on the basis of the geographical spread, physical differences and cultural diversity of humans. The choice was also logically independent of the notion of transmutation of species, but that was considered to be a slow process.\n\nWilliam Benjamin Carpenter wrote in 1872 of a fixed conviction of the \"modern origin\" as the only reason for resisting the human creation of flint implements. Henry Williamson Haynes writing in 1880 could call the antiquity of man \"an established fact\".\n\nThe Biblical account included\n\n\nThese points were debated by scholars as well as theologians. Biblical literalism was not a given in the medieval and early modern periods, for Christians or Jews.\n\nThe Flood could explain extinctions of species at that date, on the hypothesis that the Ark had not contained all species of animal. A Flood that was not universal, on the other hand, had implications for the biblical theory of races and Noah's sons. The theory of catastrophism, which was as much secular as theological in attitude, could be used in analogous ways.\n\nThere was interest in matters arising from modification of the biblical narrative, therefore, and it was fuelled by the new knowledge of the world in early modern Europe, and then by the growth of the sciences. One hypothesis was of men not descended from Adam. This hypothesis of polygenism (no unique origin of men) implied nothing on the antiquity of man, but the issue was implicated in counter-arguments, for monogenism.\n\nIsaac La Peyrère appealed in formulating his Preadamite theory of polygenism to Jewish tradition; it was intended to be compatible with the biblical creation of man. It was rejected by many contemporary theologians. This idea of men before Adam had been current in earlier Christian scholars and those of unorthodox and heretical beliefs; La Peyrère's significance was his synthesis of the dissent. Influentially, he revived the classical idea of Marcus Terentius Varro, preserved in Censorinus, of a three-fold division of historical time into \"uncertain\" (to a universal flood), \"mythical\", and \"historical\" (with certain chronology).\n\nThe biblical narrative had implications for ethnology (division into Hamitic, Japhetic and Semitic peoples), and had its defenders, as well as those who felt it made significant omissions. Matthew Hale wrote his \"Primitive Origination of Mankind\" (1677) against La Peyrère, it has been suggested, in order to defend the propositions of a young human race and universal Flood, and the Native Americans as descended from Noah. Anthony John Maas writing in the 1913 \"Catholic Encyclopedia\" commented that pro-slavery sentiment indirectly supported the Preadamite theories of the middle of the 19th century. The antiquity of man found support in the opposed theories of monogenism of this time that justified abolitionism by discrediting scientific racism.\n\nAlready in the 18th century polygenism was applied as a theory of race (see Scientific racism#Blumenbach and Buffon). A variant racist Preadamism was introduced, in particular by Reginald Stuart Poole (\"The Genesis of the Earth and of Man\", London, 1860) and Dominic M'Causland (\"Adam and the Adamite, or the Harmony of Scripture and Ethnology\", London, 1864). They followed the views of Samuel George Morton, Josiah C. Nott, George Gliddon, and Louis Agassiz; and maintained that Adam was the progenitor of the Caucasian race, while the other races descended from Preadamite ancestry.\n\nJames Cowles Prichard argued against polygenism, wishing to support the account drawn from the \"Book of Genesis\" of a single human origin. In particular he argued that humans were one species, using the interfertility criterion of hybridity. By his use of a form of natural selection to argue for change of human skin colour as a historical process, he also implied a time scale long enough for such a process to have produced the observed differences.\n\nThe Early Christian Church contested claims that pagan traditions were older than that of the Bible. Theophilus of Antioch and Augustine of Hippo both argued against Egyptian views that the world was at least 100,000 years old. This figure was too high to be compatible with biblical chronology. One of La Peyrère's propositions, that China was at least 10,000 years old, gained wider currency; Martino Martini had provided details of traditional Chinese chronology, from which it was deduced by Isaac Vossius that Noah's Flood was local rather than universal.\n\nOne of the considerations detected in La Peyrère by Otto Zöckler was concern with the Antipodes and their people: were they pre-Adamites, or indeed had there been a second \"Adam of the Antipodes\"? In a 19th-century sequel, Alfred Russel Wallace in an 1867 book review pointed to the Pacific Islanders as posing a problem for those holding both to monogenism and a recent date for human origins. In other words, he took migration from an original location to remote islands that are now populated to imply a long time scale. A significant consequence of the recognition of the antiquity of man was the greater scope for conjectural history, in particular for all aspects of diffusionism and social evolutionism.\n\nWhile extinction of species came with the development of geology to be widely accepted in the early 19th century, there was resistance on theological grounds to extinctions after the creation of man. It was argued, in particular in the 1820s and 1830s, that man would not be created into an \"imperfect\" world as far as design of its collection of species was concerned. This reasoning cut across that which was conclusive for the science of the antiquity of man, a generation later.\n\nThe late 18th century was a period in which French and German caves were explored, and remains taken for study: caving was in fashion, if speleology was only in its infancy, and the St. Beatus Caves, for example, attracted many visitors. Caves were a theme of the art of the time, also. Cave remains proved of great importance to the science of the antiquity of man. Stalagmite formation was a clearcut mechanism of formation of fossils, and its stratigraphy could be understood. Other sites of importance were associated with alluvial deposits of gravel and clay, or peat. The early example of the Gray's Inn Lane Hand Axe was from gravel in a bed of a tributary of the River Thames, but remained isolated for about a century.\n\nThe three-age system was in place from about 1820, in the form given to it by Christian Jürgensen Thomsen in his work on the collections that became the National Museum of Denmark. He published his ideas in 1836. Postulating cultural change, in itself and without explaining a rate of change, did not generate reasons to revise traditional chronology. But the concept of Stone Age artifacts became current. Thomsen's book in Danish, \"Ledetraad til Nordisk Oldkyndighed\", was translated into German (\"Leitfaden zur Nordischen Alterthumskunde\", 1837), and English (\"Guide to Northern Archæology\", 1848).\n\nJohn Frere's 1797 discovery of the Hoxne handaxe helped to initiate the 19th century debate, but it started in earnest around 1810. There were then a number of false starts relating to different European sites. William Buckland misjudged what he had found in 1823 with the misnamed Red Lady of Paviland, and explained away the mammoth remains with the find. He also was dismissive of the Kent's Cavern findings of John MacEnery in the later 1820s. In 1829 Philippe-Charles Schmerling discovered a Neanderthal fossil skull (at Engis). At that point, however, its significance was not recognised, and Rudolf Virchow consistently opposed the theory that it was very old. The 1847 book \"Antiquités Celtiques et Antediluviennes\" by Boucher de Perthes about Saint-Acheul was found unconvincing in its presentation, until it was reconsidered about a decade later.\n\nThe debate moved on only in the context of\n\n\nIt was this combination, \"extinct faunal remains\" + \"human artifacts\", that provided the evidence that came to be seen as crucial. A sudden acceleration of research was seen from mid-1858, when the Geological Society set up a \"cave committee\". Besides Hugh Falconer who had pressed for it, the committee comprised Charles Lyell, Richard Owen, William Pengelly, Joseph Prestwich, and Andrew Ramsay.\n\nOn the one hand, lack of uniformity in prehistory is what gave science traction on the question of the antiquity of man; and, on the other hand, there were at the time theories that tended to rule out certain types of lack of regularity. John Lubbock outlined in 1890 the way the antiquity of man had in his time been established as derived from change in prehistory: in fauna, geography and climate. The hypotheses required to establish that these changes were facts of prehistory were themselves in tension with the uniformitarianism that was held to by some scientists; therefore the protean concept \"uniformitarianism\" was adjusted to accommodate the past changes that could be established.\n\nZoological uniformity on earth was debated already in the early eighteenth century. \nGeorge Berkeley argued in \"Alciphron\" that the lack of human artifacts in deeper excavations suggested a recent origin of man. Evidence of absence was, of course, seen as problematic to establish. Gottfried Leibniz in his \"Protogaea\" produced arguments against identification of a species via morphology, without evidence of descent (having in mind a characterisation of humans by possession of reason); and against the discreteness of species and their extinction.\n\nUniformitarianism held the field against the competitor theories of Neptunism and catastrophism, which partook of Romantic science and theological cosmogony; it established itself as the successor of Plutonism, and became the foundation of modern geology. Its tenets were correspondingly firmly held. Charles Lyell put forward at one point views on what were called \"uniformity of kind\" and \"uniformity of degree\" that were incompatible with what was argued later. Lyell's theory, in fact, was of a \"steady state\" geology, which he deduced from his principles. This went too far in restricting actual geological processes, to a predictable closed system, if it ruled out ice ages (see ice ages#Causes of ice ages), as became clearer not long after Lyell's \"Principles of Geology\" appeared (1830–3). Of Lubbock's three types of change, the geographical included the theory of migration over land bridges in biogeography, which in general acted as an explanatory stopgap, rather than in most cases being one supported by science. Sea level changes were easier to justify.\n\nThe identification of ice ages was important context for the antiquity of man because it was accepted that certain mammals had died out with the last of the ice ages; and the ice ages were clearly marked in the geological record. Georges Cuvier's \"Recherches sur les ossements fossiles de quadrupèdes\" (1812) had made accepted facts of the extinctions of mammals that were to be relevant to human antiquity. The concept of an ice age was proposed in 1837 by Louis Agassiz, and it opened the way to the study of glacial history of the Quaternary. William Buckland came to see evidence of glaciers in what he had taken to be remains of the biblical Flood. It seemed adequately proved that the woolly mammoth and woolly rhinoceros were mammals of the ice ages, and had ceased to exist with the ice ages: they inhabited Europe when it was tundra, and not afterwards. In fact such extinct mammals were typically found in \"diluvium\" as it was then called (distinctive gravel or boulder clay).\n\nGiven that the animals were associated with these strata, establishing the date of the strata could be by geological arguments, based on uniformity of stratigraphy; and so the animals' extinction was dated. An extinction can still strictly only be dated on assumptions, as evidence of absence; for a particular site, however, the argument can be from local extinction.\n\nNeither Agassiz nor Buckland adopted the new views on the antiquity of man.\n\nBoucher de Perthes had written up discoveries in the Somme valley in 1847. Joseph Prestwich and John Evans in April 1859, and Charles Lyell with others also in 1859, made field trips to the sites, and returned convinced that humans had coexisted with extinct mammals. In general and qualitative terms, Lyell felt the evidence established the \"antiquity of man\": that humans were much older than the traditional assumptions had made them. His conclusions were shared by the Royal Society and other British learned institutions, as well as in France. It was this recognition of the early date of Acheulean handaxes that first established the scientific credibility of the \"deep\" antiquity of humans.\n\nThis debate was concurrent with that over the book \"On the Origin of Species\", published in 1859, and was evidently related; but was not one in which Charles Darwin initially made his own views public. Consolidation of the \"antiquity of man\" required more work, with stricter methods; and this proved possible over the next two decades. The discoveries of Boucher de Perthes therefore motivated further researches to try to repeat and confirm the findings at other sites. Significant in this were excavations by William Pengelly at Brixham Cavern, and with a systematic approach at Kents Cavern (1865–1880). Another major project, which produced quicker findings, was that of Henry Christy and Édouard Lartet. Lartet in 1860 had published results from a cave near Massat (Ariège) claiming stone tool cuts on bones of extinct mammals, made when the bones were fresh.\n\nWhen the science was considered reasonably settled as to the existence of \"Quaternary Man\" (humans of the Pleistocene), there remained the issue as to whether man had existed in the Tertiary, a now obsolete term used for the preceding geological period. The debate on the antiquity of man resonated in the later debate over eoliths, which were supposed proof of the existence of man in the Pliocene (during the Neogene). In this case the sceptical view won out.\n\n\n\n\n\n"}
{"id": "2137292", "url": "https://en.wikipedia.org/wiki?curid=2137292", "title": "Drag (physics)", "text": "Drag (physics)\n\nIn fluid dynamics, drag (sometimes called air resistance, a type of friction, or fluid resistance, another type of friction or fluid friction) is a force acting opposite to the relative motion of any object moving with respect to a surrounding fluid. This can exist between two fluid layers (or surfaces) or a fluid and a solid surface. Unlike other resistive forces, such as dry friction, which are nearly independent of velocity, drag forces depend on velocity.\nDrag force is proportional to the velocity for a laminar flow and the squared velocity for a turbulent flow. Even though the ultimate cause of a drag is viscous friction, the turbulent drag is independent of viscosity.\n\nDrag forces always decrease fluid velocity relative to the solid object in the fluid's path.\n\nExamples of drag include the component of the net aerodynamic or hydrodynamic force acting opposite to the direction of movement of a solid object such as cars, aircraft and boat hulls; or acting in the same geographical direction of motion as the solid, as for sails attached to a down wind sail boat, or in intermediate directions on a sail depending on points of sail. In the case of viscous drag of fluid in a pipe, drag force on the immobile pipe decreases fluid velocity relative to the pipe.\n\nIn physics of sports, the drag force is necessary to explain the performance of runners, particularly of sprinters.\n\nTypes of drag are generally divided into the following categories:\nThe phrase \"parasitic drag\" is mainly used in aerodynamics, since for lifting wings drag it is in general small compared to lift. For flow around bluff bodies, form and interference drags often dominate, and then the qualifier \"parasitic\" is meaningless.\n\nFurther, lift-induced drag is only relevant when wings or a lifting body are present, and is therefore usually discussed either in aviation or in the design of semi-planing or planing hulls. Wave drag occurs either when a solid object is moving through a fluid at or near the speed of sound or when a solid object is moving along a fluid boundary, as in surface waves.\nDrag depends on the properties of the fluid and on the size, shape, and speed of the object. One way to express this is by means of the drag equation:\nwhere\nThe drag coefficient depends on the shape of the object and on the Reynolds number\nwhere \"formula_8\" is some characteristic diameter or linear dimension and formula_9 is the kinematic viscosity of the fluid (equal to the viscosity formula_10 divided by the density). At low formula_11, formula_6 is asymptotically proportional to formula_13, which means that the drag is linearly proportional to the speed. At high formula_11, formula_6 is more or less constant and drag will vary as the square of the speed. The graph to the right shows how formula_6 varies with formula_11 for the case of a sphere. Since the power needed to overcome the drag force is the product of the force times speed, the power needed to overcome drag will vary as the square of the speed at low Reynolds numbers and as the cube of the speed at high numbers.\n\nAs mentioned, the drag equation with a constant drag coefficient gives the force experienced by an object moving through a fluid at relatively large velocity (i.e. high Reynolds number, Re > ~1000). This is also called \"quadratic drag\". The equation is attributed to Lord Rayleigh, who originally used \"L\" in place of \"A\" (\"L\" being some length).\n\nThe reference area \"A\" is often orthographic projection of the object (frontal area)—on a plane perpendicular to the direction of motion—e.g. for objects with a simple shape, such as a sphere, this is the cross sectional area. Sometimes a body is a composite of different parts, each with a different reference areas, in which case a drag coefficient corresponding to each of those different areas must be determined.\n\nIn the case of a wing the reference areas are the same and the drag force is in the same ratio to the lift force as the ratio of drag coefficient to lift coefficient. Therefore, the reference for a wing is often the lifting area (\"wing area\") rather than the frontal area.\n\nFor an object with a smooth surface, and non-fixed separation points—like a sphere or circular cylinder—the drag coefficient may vary with Reynolds number \"R\", even up to very high values (\"R\" of the order 10).\nFor an object with well-defined fixed separation points, like a circular disk with its plane normal to the flow direction, the drag coefficient is constant for \"R\" > 3,500.\nFurther the drag coefficient \"C\" is, in general, a function of the orientation of the flow with respect to the object (apart from symmetrical objects like a sphere).\n\nUnder the assumption that the fluid is not moving relative to the currently used reference system, the power required to overcome the aerodynamic drag is given by:\n\nNote that the power needed to push an object through a fluid increases as the cube of the velocity. A car cruising on a highway at may require only to overcome aerodynamic drag, but that same car at requires . With a doubling of speed the drag (force) quadruples per the formula. Exerting 4 times the force over a fixed distance produces 4 times as much work. At twice the speed the work (resulting in displacement over a fixed distance) is done twice as fast. Since power is the rate of doing work, 4 times the work done in half the time requires 8 times the power.\n\nWhen the fluid is moving relative to the reference system (e.g. a car driving into headwind) the power required to overcome the aerodynamic drag is given by:\n\nWhere formula_21 is the wind speed and formula_22 it the object speed (both relative to ground).\n\nThe velocity as a function of time for an object falling through a non-dense medium, and released at zero relative-velocity \"v\" = 0 at time \"t\" = 0, is roughly given by a function involving a hyperbolic tangent (tanh):\n\nThe hyperbolic tangent has a limit value of one, for large time \"t\". In other words, velocity asymptotically approaches a maximum value called the terminal velocity \"v\":\n\nFor an object falling and released at relative-velocity \"v\" = v at time \"t\" = 0, with v ≤ v, is also defined in terms of the hyperbolic tangent function:\n\nActually, this function is defined by the solution of the following differential equation:\n\nOr, more generically (where F(v) are the forces acting on the object beyond drag):\n\nFor a potato-shaped object of average diameter \"d\" and of density \"ρ\", terminal velocity is about\n\nFor objects of water-like density (raindrops, hail, live objects—mammals, birds, insects, etc.) falling in air near Earth's surface at sea level, the terminal velocity is roughly equal to\n\nwith \"d\" in metre and \"v\" in m/s. For example, for a human body (formula_30 ~ 0.6 m) formula_31 ~ 70 m/s, for a small animal like a cat (formula_30 ~ 0.2 m) formula_31 ~ 40 m/s, for a small bird (formula_30 ~ 0.05 m) formula_31 ~ 20 m/s, for an insect (formula_30 ~ 0.01 m) formula_31 ~ 9 m/s, and so on. Terminal velocity for very small objects (pollen, etc.) at low Reynolds numbers is determined by Stokes law.\n\nTerminal velocity is higher for larger creatures, and thus potentially more deadly. A creature such as a mouse falling at its terminal velocity is much more likely to survive impact with the ground than a human falling at its terminal velocity. A small animal such as a cricket impacting at its terminal velocity will probably be unharmed. This, combined with the relative ratio of limb cross-sectional area vs. body mass (commonly referred to as the Square-cube law), explains why very small animals can fall from a large height and not be harmed.\n\nThe equation for viscous resistance or linear drag is appropriate for objects or particles moving through a fluid at relatively slow speeds where there is no turbulence (i.e. low Reynolds number, formula_38). Note that purely laminar flow only exists up to Re = 0.1 under this definition. In this case, the force of drag is approximately proportional to velocity. The equation for viscous resistance is:\n\nwhere:\n\nWhen an object falls from rest, its velocity will be\n\nwhich asymptotically approaches the terminal velocity formula_43. For a given formula_40, heavier objects fall more quickly.\n\nFor the special case of small spherical objects moving slowly through a viscous fluid (and thus at small Reynolds number), George Gabriel Stokes derived an expression for the drag constant:\n\nwhere:\n\nThe resulting expression for the drag is known as Stokes' drag:\n\nFor example, consider a small sphere with radius formula_46 = 0.5 micrometre (diameter = 1.0 µm) moving through water at a velocity formula_50 of 10 µm/s. Using 10 Pa·s as the dynamic viscosity of water in SI units,\nwe find a drag force of 0.09 pN. This is about the drag force that a bacterium experiences as it swims through water.\n\nIn aerodynamics, aerodynamic drag is the fluid drag force that acts on any moving solid body in the direction of the fluid freestream flow. From the body's perspective (near-field approach), the drag results from forces due to pressure distributions over the body surface, symbolized formula_51, and forces due to skin friction, which is a result of viscosity, denoted formula_52. Alternatively, calculated from the flowfield perspective (far-field approach), the drag force results from three natural phenomena: shock waves, vortex sheet, and viscosity.\n\nThe pressure distribution acting on a body's surface exerts normal forces on the body. Those forces can be summed and the component of that force that acts downstream represents the drag force, formula_51, due to pressure distribution acting on the body. The nature of these normal forces combines shock wave effects, vortex system generation effects, and wake viscous mechanisms.\n\nThe viscosity of the fluid has a major effect on drag. In the absence of viscosity, the pressure forces acting to retard the vehicle are canceled by a pressure force further aft that acts to push the vehicle forward; this is called pressure recovery and the result is that the drag is zero. That is to say, the work the body does on the airflow, is reversible and is recovered as there are no frictional effects to convert the flow energy into heat. Pressure recovery acts even in the case of viscous flow. Viscosity, however results in pressure drag and it is the dominant component of drag in the case of vehicles with regions of separated flow, in which the pressure recovery is fairly ineffective.\n\nThe friction drag force, which is a tangential force on the aircraft surface, depends substantially on boundary layer configuration and viscosity. The net friction drag, formula_54, is calculated as the downstream projection of the viscous forces evaluated over the body's surface.\n\nThe sum of friction drag and pressure (form) drag is called viscous drag. This drag component is due to viscosity. In a thermodynamic perspective, viscous effects represent irreversible phenomena and, therefore, they create entropy. The calculated viscous drag formula_55 use entropy changes to accurately predict the drag force.\n\nWhen the airplane produces lift, another drag component results. Induced drag, symbolized formula_56, is due to a modification of the pressure distribution due to the trailing vortex system that accompanies the lift production. An alternative perspective on lift and drag is gained from considering the change of momentum of the airflow. The wing intercepts the airflow and forces the flow to move downward. This results in an equal and opposite force acting upward on the wing which is the lift force. The change of momentum of the airflow downward results in a reduction of the rearward momentum of the flow which is the result of a force acting forward on the airflow and applied by the wing to the air flow; an equal but opposite force acts on the wing rearward which is the induced drag. Induced drag tends to be the most important component for airplanes during take-off or landing flight. Another drag component, namely wave drag, formula_57, results from shock waves in transonic and supersonic flight speeds. The shock waves induce changes in the boundary layer and pressure distribution over the body surface.\n\nThe idea that a moving body passing through air or another fluid encounters resistance had been known since the time of Aristotle. Louis Charles Breguet's paper of 1922 began efforts to reduce drag by streamlining. Breguet went on to put his ideas into practice by designing several record-breaking aircraft in the 1920s and 1930s. Ludwig Prandtl's boundary layer theory in the 1920s provided the impetus to minimise skin friction. A further major call for streamlining was made by Sir Melvill Jones who provided the theoretical concepts to demonstrate emphatically the importance of streamlining in aircraft design. \nIn 1929 his paper ‘The Streamline Airplane’ presented to the Royal Aeronautical Society was seminal. He proposed an ideal aircraft that would have minimal drag which led to the concepts of a 'clean' monoplane and retractable undercarriage. The aspect of Jones’s paper that most shocked the designers of the time was his plot of the horse power required versus velocity, for an actual and an ideal plane. By looking at a data point for a given aircraft and extrapolating it horizontally to the ideal curve, the velocity gain for the same power can be seen. When Jones finished his presentation, a member of the audience described the results as being of the same level of importance as the Carnot cycle in thermodynamics.\n\nLift-induced drag (also called induced drag) is drag which occurs as the result of the creation of lift on a three-dimensional lifting body, such as the wing or fuselage of an airplane. Induced drag consists primarily of two components: drag due to the creation of trailing vortices (vortex drag); and the presence of additional viscous drag (lift-induced viscous drag) that is not present when lift is zero. The trailing vortices in the flow-field, present in the wake of a lifting body, derive from the turbulent mixing of air from above and below the body which flows in slightly different directions as a consequence of creation of lift.\n\nWith other parameters remaining the same, as the lift generated by a body increases, so does the lift-induced drag. This means that as the wing's angle of attack increases (up to a maximum called the stalling angle), the lift coefficient also increases, and so too does the lift-induced drag. At the onset of stall, lift is abruptly decreased, as is lift-induced drag, but viscous pressure drag, a component of parasite drag, increases due to the formation of turbulent unattached flow in the wake behind the body.\n\nParasitic drag is drag caused by moving a solid object through a fluid. Parasitic drag is made up of multiple components including viscous pressure drag (form drag), and drag due to surface roughness (skin friction drag). Additionally, the presence of multiple bodies in relative proximity may incur so called interference drag, which is sometimes described as a component of parasitic drag.\n\nIn aviation, induced drag tends to be greater at lower speeds because a high angle of attack is required to maintain lift, creating more drag. However, as speed increases the angle of attack can be reduced and the induced drag decreases. Parasitic drag, however, increases because the fluid is flowing more quickly around protruding objects increasing friction or drag. At even higher speeds (transonic), wave drag enters the picture. Each of these forms of drag changes in proportion to the others based on speed. The combined overall drag curve therefore shows a minimum at some airspeed - an aircraft flying at this speed will be at or close to its optimal efficiency. Pilots will use this speed to maximize endurance (minimum fuel consumption), or maximize gliding range in the event of an engine failure.\n\nThe interaction of parasitic and induced drag \"vs.\" airspeed can be plotted as a characteristic curve, illustrated here. In aviation, this is often referred to as the \"power curve\", and is important to pilots because it shows that, below a certain airspeed, maintaining airspeed counterintuitively requires \"more\" thrust as speed decreases, rather than less. The consequences of being \"behind the curve\" in flight are important and are taught as part of pilot training. At the subsonic airspeeds where the \"U\" shape of this curve is significant, wave drag has not yet become a factor, and so it is not shown in the curve.\n\nWave drag (also called compressibility drag) is drag that is created when a body moves in a compressible fluid and at speeds that are close to the speed of sound in that fluid. In aerodynamics, wave drag consists of multiple components depending on the speed regime of the flight.\n\nIn transonic flight (Mach numbers greater than about 0.8 and less than about 1.4), wave drag is the result of the formation of shockwaves in the fluid, formed when local areas of supersonic (Mach number greater than 1.0) flow are created. In practice, supersonic flow occurs on bodies traveling well below the speed of sound, as the local speed of air increases as it accelerates over the body to speeds above Mach 1.0. However, full supersonic flow over the vehicle will not develop until well past Mach 1.0. Aircraft flying at transonic speed often incur wave drag through the normal course of operation. In transonic flight, wave drag is commonly referred to as transonic compressibility drag. Transonic compressibility drag increases significantly as the speed of flight increases towards Mach 1.0, dominating other forms of drag at those speeds.\n\nIn supersonic flight (Mach numbers greater than 1.0), wave drag is the result of shockwaves present in the fluid and attached to the body, typically oblique shockwaves formed at the leading and trailing edges of the body. In highly supersonic flows, or in bodies with turning angles sufficiently large, unattached shockwaves, or bow waves will instead form. Additionally, local areas of transonic flow behind the initial shockwave may occur at lower supersonic speeds, and can lead to the development of additional, smaller shockwaves present on the surfaces of other lifting bodies, similar to those found in transonic flows. In supersonic flow regimes, wave drag is commonly separated into two components, supersonic lift-dependent wave drag and supersonic volume-dependent wave drag.\n\nThe closed form solution for the minimum wave drag of a body of revolution with a fixed length was found by Sears and Haack, and is known as the Sears-Haack Distribution. Similarly, for a fixed volume, the shape for minimum wave drag is the Von Karman Ogive.\n\nBusemann's Biplane is not, in principle, subject to wave drag at all when operated at its design speed, but is incapable of generating lift.\n\nIn 1752 d'Alembert proved that potential flow, the 18th century state-of-the-art inviscid flow theory amenable to mathematical solutions, resulted in the prediction of zero drag. This was in contradiction with experimental evidence, and became known as d'Alembert's paradox. In the 19th century the Navier–Stokes equations for the description of viscous flow were developed by Saint-Venant, Navier and Stokes. Stokes derived the drag around a sphere at very low Reynolds numbers, the result of which is called Stokes' law.\n\nIn the limit of high Reynolds numbers, the Navier–Stokes equations approach the inviscid Euler equations, of which the potential-flow solutions considered by d'Alembert are solutions. However, all experiments at high Reynolds numbers showed there is drag. Attempts to construct inviscid steady flow solutions to the Euler equations, other than the potential flow solutions, did not result in realistic results.\n\nThe notion of boundary layers—introduced by Prandtl in 1904, founded on both theory and experiments—explained the causes of drag at high Reynolds numbers. The boundary layer is the thin layer of fluid close to the object's boundary, where viscous effects remain important even when the viscosity is very small (or equivalently the Reynolds number is very large).\n\n\n"}
{"id": "22372368", "url": "https://en.wikipedia.org/wiki?curid=22372368", "title": "Expectation fulfilment theory of dreaming", "text": "Expectation fulfilment theory of dreaming\n\nThe expectation fulfilment theory of dreaming, proposed by psychologist Joe Griffin in 1993, posits that the prime function of dreams, during REM sleep, is to act out metaphorically non-discharged emotional arousals (expectations) that were not expressed during the previous day. It theorises that excessive worrying (regarded as unintentional misuse of the imagination) arouses the autonomic nervous system, which increases the need to dream during REM sleep. This deprives the individual of the refreshment of the mind and body brought about by regenerative slow-wave sleep.\n\nEveryone has periods of rapid eye movement (REM) sleep every night, a phase lasting about 90 minutes. This is when most dreaming occurs. Overall, REM sleep usually accounts for up to two hours of sleep time and most people can remember their dreams only if woken directly from REM sleep.\n\nIt is known from laboratory studies of brain waves that, just before entering REM sleep and while in it, powerful electrical signals pass through the brain. On electroencephalogram recordings, these appear as spikes and are known as PGO spikes, after the initials of names of the structures of the brain they pass through. These same spikes occur during waking, when attention is drawn to a stimulus, the phenomenon being known as ‘the orientation response’. While sleeping, the spikes appear to represent the cue to dream.\n\nJoe Griffin discovered, from years of research on his own dreams and those of others, that dreams are metaphorical enactments of emotional arousals that were not expressed or acted out during the day. This is nature’s evolutionary solution to animals’ need to inhibit arousals, such as anger, urge to eat or urge to mate, whenever such instincts are inappropriate or dangerous to act on at the time; the arousals are safely deactivated later in dreams. In civilised societies, of course, people are commonly in circumstances where strong feelings are aroused but it is inappropriate to act on them. Arguments do not feature in dreams, as the emotion is expressed; private worries, fears, urges not given into (such as for a forbidden food or activity) do. Griffin has theorised that by enabling arousals to be safely discharged in dreams, once they have been activated, REM sleep serves to keep our instincts and drives intact. (If continually activated but not acted upon in any form, they would gradually become extinct.) The PGO spike activity prior to and during dreaming signals that there is material to be acted out and discharged. Once the instinct patterns have been deactivated, the data processing potential of the neocortex is released, ready to deal with the emotionally arousing contingencies of the following day. Far from dreams being the cesspit of the unconscious, as Freud proclaimed, Griffin says that they are the equivalent of the flushed toilet.\n\nMetaphor is the language of the REM state. French scientist Michel Jouvet suggested that REM sleep is concerned with programming the central nervous system to carry out instinctive behaviours. William Dement and colleagues discovered that the amount of REM sleep a foetus or newborn has depends on how mature an animal is at birth. Animals born relatively mature have little REM sleep as foetuses and after birth, while animals born immature have a considerable amount. During REM sleep, foetuses and newborns are programmed with the instincts that they must seek to complete in the environment. As the sense organs start to receive inputs from the environment, the brain ‘pattern matches’ to the instinctive templates programmed in during REM sleep. According to the theory, just as programming instincts involves creating a pattern or template for which an analogue can be found in the environment (twig-like materials, a friendly human face, etc.), so it makes sense that deactivation also uses sensory analogues or metaphors, enabling the brain to draw on images which represent the unexpressed emotional arousals of the day.\n\nGriffin has posited another, more important reason for why dreaming is in metaphor. Using an analogous experience as a means of completing an arousal enables the arousal associated with the instinctive urge to be discharged but, importantly, the instinctive urge itself in the context it was experienced can be remembered. This prevents memory stores from becoming either corrupt or incomplete. It also explains why it is important to forget dreams most of the time.\n\nIt is well known that depressed people spend far more of their sleep time in REM sleep than non-depressed people, entering it earlier – and it has been shown, experimentally, that depressed people show improvement when deprived of REM sleep. This accords with Griffin’s theory, as depressed people spend much of their waking time arousing themselves physiologically through rumination and worry. All this arousal has to be discharged in dreams. Dreaming takes up a large amount of the brain’s energy, as the PGO spikes are continually firing, so depressed people tend to wake early but exhausted and lacking in motivation, setting the scene for more worry and distress the following day. (This has been termed the cycle of depression.) The expectation fulfilment theory of dreaming is used by Human Givens therapists to help people see the need to stop worrying and introspecting and to focus on productive ways to meet unmet needs instead.\n\nWhile the theory has widely been validated anecdotally, through people’s personal experience, it is not able to be put to rigorous scientific testing, as interpretations of dream events are necessarily subjective. However, Griffin, by tracking through historical data, claims that the expectation fulfilment theory of dreaming provides a far more plausible explanation for two famous dreams interpreted by of Freud and Jung.\n\nObjectively, the theory fulfils the criteria for a satisfactory explanation of dreaming put forward by eminent sleep researcher Professor Bill Domhoff in a special issue of Behavioral and Brain Sciences, devoted to examining the most commonly promoted dream theories. Professor Domhoff, who did not consider the expectation fulfilment theory of dreaming in his review, wrote:\nIf the methodologically most sound descriptive empirical findings were to be used as a starting point for future dream theorising, the picture would look like this:\n\nHe also concluded that none of the theories he had reviewed encompassed all three of these \"well-grounded\" conclusions.\n"}
{"id": "971690", "url": "https://en.wikipedia.org/wiki?curid=971690", "title": "Fuzzy Control Language", "text": "Fuzzy Control Language\n\nFuzzy Control Language, or FCL, is a language for implementing fuzzy logic, especially fuzzy control. It was standardized by IEC 61131-7. It is a domain-specific programming language: it has no features unrelated to fuzzy logic, so it is impossible to even print \"Hello, world!\". Therefore, one does not write a program in FCL, but one may write \"part\" of it in FCL.\n\n RULE 0: IF (Temperature IS Cold) THEN (Output IS High)\n\nFCL is not an entirely complete fuzzy language, for instance, it does not support \"hedges\", which are adverbs that modify the set. For instance, the programmer cannot write:\n\nHowever, the programmer can simply define new sets for \"very cold\" and \"very high\". FCL also lacks support for higher-order fuzzy sets, subsets, and so on. None of these features are essential to fuzzy control, although they may be nice to have.\n\n\n IEC 1131-7 CD1\n"}
{"id": "14535383", "url": "https://en.wikipedia.org/wiki?curid=14535383", "title": "Goldstone Catena", "text": "Goldstone Catena\n\nGoldstone Catena (Goldstone Vallis until March 2013) is a catena on Mercury located at 15.8 S, 31.7 W. It is named after Goldstone Observatory. While it superficially resembles a graben, it is a chain of overlapping secondary craters.\n"}
{"id": "42989890", "url": "https://en.wikipedia.org/wiki?curid=42989890", "title": "Gottorfer Codex", "text": "Gottorfer Codex\n\nThe Gottorfer Codex (German) or det gottorpske kodeks (Danish) is a four volume work commissioned by Frederick III, Duke of Holstein-Gottorp between 1649 and 1659 to depict the wide assortment of plants that grew in the ducal gardens at Gottorf Castle (\"Gottorp\") in the duchy of Schleswig.\n\nThe work's 365 illustrated pages depict 1,180 plants painted in gouache on veal parchment by Hans Simon Holtzbecker from Hamburg. The number of illustrations per page range from one to ten.\n\nWork on the codex stopped after the death of Duke Frederick III in 1659. The codex has been the property of the Danish state since the Great Northern War (1700-1721) when Gottorf Castle and the duke's portion of the duchy of Schleswig were annexed to the Danish crown.\n\nThe codex is currently owned by \"Royal Collection of Graphic Art\", a part of the National Gallery of Denmark. In 2009, a digital version of the codex was created by the National Gallery of Denmark with financial support by the \"Foundation of State Museums in Schleswig-Holstein\".\n\n"}
{"id": "25221493", "url": "https://en.wikipedia.org/wiki?curid=25221493", "title": "Hanna Kokko", "text": "Hanna Kokko\n\nHanna Kokko (born 1971) is a scientist, working in the field of evolution and ecology. She was a Professor of Evolutionary Biology at Helsinki University, Finland, and at the Australian National University as a Professor of Evolutionary Ecology. She is currently Professor in Evolutionary Ecology at the University of Zürich. She has been awarded the 2010 Per Brinck Oikos Award and the British Ecological Society's Founder's Prize. Her move to Australia followed her appointment as an Australian Laureate Fellow. She was also made a Fellow of the Australian Academy of Science in 2014.\n\n\n"}
{"id": "1055793", "url": "https://en.wikipedia.org/wiki?curid=1055793", "title": "Harri Lorenzi", "text": "Harri Lorenzi\n\nHarri Lorenzi (born 1949) is a Brazilian agronomic engineer, author on trees of the Atlantic Mata and a collaborating agronomist of the garden of Fazenda Cresciumal, Ruy De Souza Queiroz. Between his workmanships, he published four books in the end of the 1990s, they are: \"Brazilian palms\", \"Brazilian Trees\" (1 and 2, also in English), \"Tropical Plants of Burle Marx\" and \"Brazilian Ornamental Plants\".\n"}
{"id": "1036205", "url": "https://en.wikipedia.org/wiki?curid=1036205", "title": "Hicks–Marshall laws of derived demand", "text": "Hicks–Marshall laws of derived demand\n\nIn economics, the Hicks–Marshall laws of derived demand assert that, other things equal, the own-wage elasticity of demand for a category of labor is high under the following conditions:\n\nThe \"Hicks–Marshall\" is named for economists John Hicks (from \"The Theory of Wages\", 1932) and Alfred Marshall (from \"Principles of Economics\", 1890).\n"}
{"id": "46425842", "url": "https://en.wikipedia.org/wiki?curid=46425842", "title": "List of ISRO missions", "text": "List of ISRO missions\n\nThe Indian Space Research Organisation has carried out 97 spacecraft missions,68 launch missions and planned many missions including Chandrayaan-2, and Aditya (spacecraft).\n\nThis is the list of all completed missions.\n\nIndia's first satellite Aryabhata was launched by the Soviet Union on 19 April 1975 from Kapustin Yar using a Kosmos-3M launch vehicle. (It was built by the ISRO) to gain experience in building and operating a satellite in space. India has launched 100 Indian satellites of many types as on 31 January 2018. Satellites have been launched from various vehicles, including those launched by Russian, European and American rockets, as well as those launched indigenously by India. The organisation responsible for India's satellite program is the Indian Space Research Organisation (ISRO).\n\nISRO also launches satellites for foreign countries. As of August 2018, ISRO has launched 209 satellites for 28 foreign countries. In 2016, India launched an additional 25 satellites for 6 different countries. In January 2017, India launched 104 satellite of different countries, out of which 96 are of America, by single launch vehicle.\n\n\n"}
{"id": "21794898", "url": "https://en.wikipedia.org/wiki?curid=21794898", "title": "List of Paleobiota of the Morrison Formation", "text": "List of Paleobiota of the Morrison Formation\n\nThe Morrison Formation is a distinctive sequence of Late Jurassic sedimentary rock that is found in the western United States, which has a wide assortment of taxa represented in its fossil record, including dinosaur fossils in North America. It is composed of mudstone, sandstone, siltstone and limestone and is light grey, greenish gray, or red. Most of the fossils occur in the green siltstone beds and lower sandstones, relics of the rivers and floodplains of the Jurassic period.\n\n(mostly from Foster [2003]; the higher-level classifications will vary as new finds are made.\n\nAccording to museum curator John Foster, \"frogs are known from several sites in the Morrison Formation but are not particularly well represented.\" The history of Morrison anuran discoveries began with the recovery of remains from Reed's Quarry 9 near Como Bluff Wyoming. The new genus \"Eobatrachus\" was erected for some of these remains by O. C. Marsh, but the material was later considered non-diagnostic. Decades later another dubious anuran genus, \"Comobatrachus\" was erected for addition fragmentary remains. Despite the erection of multiple new names, scientists only recognize two legitimate frog species from the Morrison, \"Enneabatrachus hechti\" and \"Rhadinosteus parvus.\"\n\nIn addition to formally named taxa, indeterminate anuran remains have been retrieved from Morrison strata in Colorado, Wyoming, and Utah, with the best specimens found in Dinosaur National Monument and Quarry 9. Stratigraphically speaking, indeterminate anurans have been found in stratigraphic zones 2 and 4. Indeterminate anurans with remains diagnostic down to the family level have also been reported from the Morrison. Pelobatids are represented by the illium of an unnamed, indeterminate species. A specimen has been recovered from Quarry 9 of Como Bluff in Wyoming. Pelobatids are present in stratigraphic zones 5 and 6.\n\nIndeterminate salamander remains are present in stratigraphic zones 2, 4, and 5. A distinctive type of salamander known only as Caudata B is present in stratigraphic zone 6.\n\nCrocodiles of a variety of sizes and habitats were common Morrison animals. Cursorial mesosuchians, or small terrestrial running crocs, included \"Hallopus victor\" and \"Fruitachampsa callisoni\". More derived crocodilians included \"Diplosaurus ferox\", \"Amphicotylus\", \"Hoplosuchus kayi\", and \"Macelognathus vagans\".\n\nAlthough the paleoclimate of the Morrison formation was semiarid with only seasonal rainfall, there were enough bodies of water to support a diverse ichthyofauna. Although abundant, fish remains are constrained to only certain locations within the formation. Microvertebrate sites in Wyoming are dominated by fish remains. Indeterminate ray-finned fish remains have been recovered from Ninemile Hill and a microvertebrate site in the Black Hills. Found in stratigraphic zones 2, 4, and 5. Morrison actinopterygians generally have no close modern relatives. The Wyoming microvertebrate remains are extracted from the sediment by screenwashing. Paleoniscoid remains are geographically present in the western part of Colorado, where remains have been recovered from \"a level above the Mygatt-Moore Quarry.\" Largely complete remains of small individuals have been consistently recovered for over 15 years. So far, Morrison pycnodontoids are represented by a single specimen from Dinosaur National Monument in Utah. Found in stratigraphic zone 4. Only a single specimen from Dinosaur National Monument in Utah has been recovered. Pycnodontoids were \"deep-bodied and laterally compressed fish\" whose tooth morphology suggest that they preyed on small contemporary invertebrates. They may have resembled modern butterfly fish. A single tooth is the only known remains. Dipnoan remains found at a fossil site not far from Cañon City, Colorado. Remains usually in a state of rather complete preservation. Halecostome remains are geographically present in the western part of Colorado, where remains have been recovered from \"a level above the Mygatt-Moore Quarry.\" Largely complete remains of small individuals have been consistently recovered for over 15 years. Amiid remains found in stratigraphic zones 2, 3, and 4. Found at a fossil site not far from Cañon City, Colorado. Remains usually in a state of rather complete preservation.\n\nMany types of mammaliaform cynodonts, mostly early mammals, are known from the Morrison; almost all of them were small sized animals, though occupying a very large variety of ecological niches, from the more rodent-like multituberculates to the carnivorous eutriconodonts (including the possibly volant \"Triconolestes\") to the anteater-like \"Fruitafossor\". Unclassified types include the digger \"Fruitafossor windscheffelia\". Docodonts included the common genus \"Docodon\", represented by \"D. victor\", \"D. striatus\", and \"D. superbus\", and \"Peraiocynodon\" sp. Multituberculates, a common type of early mammal, were represented by \"Ctenacodon serratus\", \"C. laticeps\", \"C. scindens\", \"Glirodon grandis\", \"Morrisonodon brentbaatar\", \"Psalodon fortis\", \"?P. marshi\", \"P. potens\", and \"Zofiabaatar pulcher\". Triconodonts present included \"Amphidon superstes\", \"Aploconodon comoensis\", \"Conodon gidleyi\", \"Priacodon ferox\", \"P. fruitaensis\", \"P. gradaevus\", \"P. lulli\", \"P. robustus\", \"Triconolestes curvicuspis\", and \"Trioracodon bisulcus\".\n\nTinodontids were represented by \"Eurylambia aequicrurius\" (probably \"Tinodon\"), and \"Tinodon bellus\" (including \"T. lepidus\"). Finally, two families of Dryolestoidea were present: Paurodontidae, including \"Comotherium richi\", \"Euthlastus cordiformis\", \"Paurodon valens\", and \"Tathiodon agilis\"; and Dryolestidae, including \"Amblotherium gracilis\", \"Dryolestes obtusus\" (common genus), \"D. priscus\", \"D. vorax\", \"Laolestes eminens\", \"L. grandis\", and \"Miccylotyrans minimus\".\n\nIn 2009, a study by J. R. Foster was published which estimated the body masses of mammals from the Morrison Formation by using the ratio of dentary length to body mass of modern marsupials as a reference. Foster concludes that \"Docodon\" was the most massive mammaliaform genus of the formation at 141g and \"Fruitafossor\" was the least massive at 6g. The average Morrison mammal had a mass of 48.5g. A graph of the body mass distribution of Morrison mammal genera produced a right-skewed curve, meaning that there were more low-mass genera.\n\nPterosaurs are very uncommon fossils in the Morrison, because the fragility of their thin walled bones often prevented their remains from being preserved. Despite being uncommon they are geographically widespread; indeterminate pterosaur remains have been found in stratigraphic zones 2 and 4-6. In addition to indeterminate remains, several species have been identified from both the rhamphorhynchoids (long-tailed pterosaurs) and pterodactyloids (short-tailed pterosaurs). Since the 1970s and 80s, pterosaur finds have become more common, but are still rare. Most Morrison pterosaurs have been found in marine and shoreline deposits. Pterosaur tracks have been found in both the Tidwell and Saltwash members. Morrison pterosaurs probably lived on fish, insects and scavenged dinosaur carcasses; they are fairly ecologically diverse, ranging from small hawking insectivore \"Mesadactylus\" to the raptorial \"Harpactognathus\".\n\nTurtles (Testudines) are very common fossils in the Morrison, due to their bony shells.\nThe most common were \"Glyptops plicatus\" (very common) and \"Dinochelys whitei\" (also common, but not as common as \"Glyptops\"). Also present were \"Dorsetochelys buzzops\" and \"Uluops uluops\".\n\n\n"}
{"id": "2680701", "url": "https://en.wikipedia.org/wiki?curid=2680701", "title": "List of compounds with carbon number 7", "text": "List of compounds with carbon number 7\n\nThis is a partial list of molecules that contain 7 carbon atoms.\n\n"}
{"id": "22183402", "url": "https://en.wikipedia.org/wiki?curid=22183402", "title": "List of epistemologists", "text": "List of epistemologists\n\nThis is a list of epistemologists, that is, people who theorize about the nature of knowledge, belief formation and the nature of justification.\n"}
{"id": "17971840", "url": "https://en.wikipedia.org/wiki?curid=17971840", "title": "List of flags of Moldova", "text": "List of flags of Moldova\n\nThe following is a list of flags of Moldova.\n\n"}
{"id": "33670271", "url": "https://en.wikipedia.org/wiki?curid=33670271", "title": "List of largest rifts, canyons and valleys in the Solar System", "text": "List of largest rifts, canyons and valleys in the Solar System\n\nFollowing are the longest, widest, and deepest rifts and valleys in various worlds of the Solar System.\n\n"}
{"id": "41508318", "url": "https://en.wikipedia.org/wiki?curid=41508318", "title": "List of things named after Lev Landau", "text": "List of things named after Lev Landau\n\nLev Landau (1908 – 1968), Soviet physicist who made fundamental contributions to many areas of theoretical physics, is the eponym of the topics in physics listed below.\n\n\nTwo celestial objects are named in his honour:\n"}
{"id": "421155", "url": "https://en.wikipedia.org/wiki?curid=421155", "title": "Magia Naturalis", "text": "Magia Naturalis\n\n\"Natural Magic\" was revised and considerably expanded throughout the author's lifetime; its twenty books (Naples 1589) include observations upon geology, optics, medicines, poisons, cooking, metallurgy, magnetism, cosmetics, perfumes, gunpowder, and invisible writing.\n\n\"Natural Magic\" is an example of pre-Baconian science. Its sources include the ancient learning of Pliny the Elder and Theophrastus as well as numerous scientific observations made by Della Porta. \"Natural Magic\" was translated and published in English in 1658.\n\n\n"}
{"id": "8915967", "url": "https://en.wikipedia.org/wiki?curid=8915967", "title": "Maritime security operations", "text": "Maritime security operations\n\nMaritime security operations (MSO) are the actions of modern naval forces to \"combat sea–based terrorism and other illegal activities, such as hijacking, piracy, and slavery, also known as human trafficking.\" Ships assigned to such operations may also assist seafaring vessels in distress. These activities are part of an overall category of activities which fall short of open warfare called military operations other than war (MOOTW).MSO also involve the marine environmental protection, creating a safer and clean environment.\n\nThe Coast Guard, along with several other agencies such as Navy, Maritime Administration, Department of Transportation, Environmental Protection Agency, and Federal Maritime Commission are agencies that have a role in the regulation of U.S ports. Their mission is to create a safer and reliable international ocean transportation system, to protect the public from any unfair and deceiving practice.\n\nA primary component of MSO requires inspections and, at times, forced boardings of vessels at sea. These actions are called visit, board, search, and seizure (VBSS). Also arrests and VBSS of ships which may have been sighted (via lookouts) from a distance to be underway and not responding to communications made to her or may have some form of smaller attached crafts which may be seen to be used as other means to attack larger crafts.\n\nThere are two major naval forces that conduct such operations; the United States Coast Guard and the United States Navy. Although they both have very distinct jobs from one another, one of their major jobs is to be able to provide security operations.\n\nThe U.S Coast Guard is a branch of the United States Armed Forces, they have eleven official missions. Their role is to provide port & waterway security, drug interdiction, search and rescue, marine environment protection, ICE operations, aids to navigation, living marine recourses, marine safety, defense readiness, law enforcement and migrant interdiction along with several other missions. Overall, their role is to provide a safer maritime industry.\n\nAlong with the U.S Coast Guard, the U.S Navy is also another branch if the Unites Stated Armed forces. Unlike the Coast Guard, the Navy is a projection of force in areas beyond the U.S shores. Their operations go beyond the shores; they provide aid to military out on the sea, carry troops to other countries, strategic plans for attacks and protect the sea lanes.\n\nToday's modern naval force have been able to detect, strategize, and prevent dozens of several illegal activities. Piracy, being one of the must known crime in the maritime industry has not been able to control its activities. Piracy was known to have a great presence during the early 1500s, up to this day the number of piracy activity is still significantly large. From the beginning of the twentieth century, the number of piracy attack have been found to be cyclical in nature, taking its high points in 2003 and 2010. In spite of this high point, several organization against anti-piracy such as the Northern Atlantic Treaty Organization (NATO) have been able to disrupt pirates attacks protecting vessel and their crew member along with their cargo. As a safety measure, the U.S Coast Guard encourage the ships' captain to know his crew, deliver a detailed plan of his sea trip to a trusted friend, final check before departing, notify Coast Guard of any suspicious activity, and finally consider clearing local customs before departing on a foreign cruise. Taking these protective measures will lessen the possibility of piracy or high jacking.\n\nA significant amount of contraband such as drugs has entered many ports through vessels claiming to be caring different types of cargo. Drug Interdiction is one of the eleven Coast Guard official missions, they aim to prevent drug traffic by intersecting drug carrying vessels out at sea. Mariners aboard a vessel that have information that a ship is involved in narcotics trafficking are required to contact their nearest Coast Guard unit. Several cargo ships have been contained with thousands of kilos on board. In the mid-year of 2015, the marked its tenth successful interdiction since October 2014. Joining forces with the U.S Coast Guard and the Canadian Navy they contained 11,700 kilograms of cocaine off the coast of Central America. While conducting a regular patrol the USS Gary tailed and located a small coastal freighter. After observing the crew discarding suspicious contraband overboard Gary launched a small boat to recover the suspected contraband.\n\nSlavery also known as human trafficking in modern days is the act of forcing some type of labor or sexual act upon someone. As the years go by the number of men, women, and children being trafficking are significantly large, this is happening around the world including the United States. Human trafficking comes second to drug trafficking generating billions of dollars per year. This crime is hard to follow up due to the victim's language and fear to the traffickers and law enforcement. The Department of the Navy (DoN) and the Department of Defense (DoD) along with other offices have come together to fight and ensure that the Combating Trafficking in the Persons (CTIP) policies are properly implemented and integrated. In the month of January 2017, the SDPD was able to arrest 38 men involved with human trafficking during an undercover operation. Even though the Coast Guard and the Navy were not involved in this operation, they also have operations similar to drug trafficking.\n\nSearch and rescue missions is also considered a maritime security operation, it is one of the oldest Coast Guard missions. As the leader in the field of search and rescue missions, worldwide, to do so the coast guard keeps facilities on the East, West, and Gulf coasts. When a search and rescue(SAR) mission is being conducted, it involves cutter, aircraft, and boats. Their goal is to minimize the loss of life and injuries, to provide aid to those in need of it. Using the coordinates of the vessel in need gives the Coast Guard and an idea of where to look for. These coordinates are given by a device called a \"Black Box\".\n\nAn example of such operations is the involvement of the multinational coalition Combined Task Force 150, which performs Maritime Security Operations in the Indian Ocean and Persian Gulf. During the Somali Civil War, they provided anti-piracy operations along the coast of Somalia in international waters. During the 2006–2007 war, they performed a cordon along the coast to prevent the escape of Al-Qaeda operatives by sea.\n\n"}
{"id": "41940112", "url": "https://en.wikipedia.org/wiki?curid=41940112", "title": "NGC 65", "text": "NGC 65\n\nNGC 65 (ESO 473-10A/PGC 1229) is a galaxy in the constellation Cetus. Its apparent magnitude is 13.4. It is located at RA 18h 58m 7s, Dec -22°52'48\". It was first discovered in 1886, and is also known as PGC 1229.\n\n"}
{"id": "19906205", "url": "https://en.wikipedia.org/wiki?curid=19906205", "title": "Naturen", "text": "Naturen\n\nNaturen () is a Norwegian popular science magazine, which has been published since 1877 in Bergen, Norway. It is the earliest still running popular science magazine of the country.\n\n\"Naturen\" was started by the geologist Hans Reusch in Bergen in 1877. It was subtitled \"Et illustreret Maanedsskrift for popular Naturvidenskab\" (meaning An illustrated monthly for popular natural sciences). It is connected to the University of Bergen. It has a popular science approach to the natural sciences, including medicine.\n\n\"Naturen\" was edited by its founder, Hans Reusch, for the first four years. Among its former editors are Jens Holmboe (1906–1925), Torbjørn Gaarder (1925–1946) and Knut Fægri (1947–1977).\n"}
{"id": "20400528", "url": "https://en.wikipedia.org/wiki?curid=20400528", "title": "Ontology engineering", "text": "Ontology engineering\n\nOntology engineering in computer science, information science and systems engineering is a field which studies the methods and methodologies for building ontologies: formal representations of a set of concepts within a domain and the relationships between those concepts.\nA large-scale representation of abstract concepts such as actions, time, physical objects and beliefs would be an example of ontological engineering. Ontology engineering is one of the areas of applied ontology, and can be seen as an application of philosophical ontology. Core ideas and objectives of ontology engineering are also central in conceptual modeling.\n\nAutomated processing of information not interpretable by software agents can be improved by adding rich semantics to the corresponding resources, such as video files. One of the approaches for the formal conceptualization of represented knowledge domains is the use of machine-interpretable ontologies, which provide structured data in, or based on, RDF, RDFS, and OWL. Ontology engineering is the design and creation of such ontologies, which can contain more than just the list of terms (controlled vocabulary); they contain terminological, assertional, and relational axioms to define concepts (classes), individuals, and roles (properties) (TBox, ABox, and RBox, respectively). Ontology engineering is a relatively new field of study concerning the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tool suites and languages that support them.\nA common way to provide the logical underpinning of ontologies is to formalize the axioms with description logics, which can then be translated to any serialization of RDF, such as RDF/XML or Turtle. Beyond the description logic axioms, ontologies might also contain SWRL rules. The concept definitions can be mapped to any kind of resource or resource segment in RDF, such as images, videos, and regions of interest, to annotate objects, persons, etc., and interlink them with related resources across knowledge bases, ontologies, and LOD datasets. This information, based on human experience and knowledge, is valuable for reasoners for the automated interpretation of sophisticated and ambiguous contents, such as the visual content of multimedia resources. Application areas of ontology-based reasoning include, but are not limited to, information retrieval, automated scene interpretation, and knowledge discovery.\n\nAn ontology language is a formal language used to encode the ontology. There are a number of such languages for ontologies, both proprietary and standards-based:\n\nLife sciences is flourishing with ontologies that biologists use to make sense of their experiments. For inferring correct conclusions from experiments, ontologies have to be structured optimally against the knowledge base they represent. The structure of an ontology needs to be changed continuously so that it is an accurate representation of the underlying domain.\nRecently, an automated method was introduced for engineering ontologies in life sciences such as Gene Ontology (GO), one of the most successful and widely used biomedical ontology. Based on information theory, it restructures ontologies so that the levels represent the desired specificity of the concepts. Similar information theoretic approaches have also been used for optimal partition of Gene Ontology. Given the mathematical nature of such engineering algorithms, these optimizations can be automated to produce a principled and scalable architecture to restructure ontologies such as GO.\n\nOpen Biomedical Ontologies (OBO), a 2006 initiative of the U.S. National Center for Biomedical Ontology, that provides a common 'foundry' for various ontology initiatives, amongst which are:\nand more\n\n\n\n\n"}
{"id": "3592000", "url": "https://en.wikipedia.org/wiki?curid=3592000", "title": "Organic (model)", "text": "Organic (model)\n\nOrganic describes forms, methods and patterns found in living systems such as the organisation of cells, to populations, communities, and ecosystems. \n\nTypically organic models stress the interdependence of the component parts, as well as their differentiation. Other properties of organic models include:\n\nOrganic models are used especially in the design of artificial systems, and the description of social systems and constructs.\n\nIn the social sciences, the organic model has been drawn upon for ideas such as mechanical and organic solidarity and organic unity. Carl Ritter advanced the idea of Lebensraum using the metaphor of an organic, growing state. \n\nIn computer science, organic networks grow in an ad hoc manner, while organic computing is autonomous and able to self-organise and heal. \n\nBionics (biomimicry) is the engineering of technology through the use of systems found in biology.\n\nOrganic architecture stresses interrelatedness as it combines the site, buildings, furnishings, and surroundings into a unified whole, each adapted to the others. Examples include the use of passive solar and wind energy as elements of design so that the building can be easily adapted to maintain the desired levels of human comfort within the structure.\n\nIn economics and business, organic growth refers to market growth that has happened gradually, and not through a sudden buyout or acquisition. An organic organisation is one which is flexible and has a flat structure, or one of minimal height.\n\nIn military, \"organic\" refers to mixtures of military unit types.\n\n"}
{"id": "23770615", "url": "https://en.wikipedia.org/wiki?curid=23770615", "title": "Outline of regression analysis", "text": "Outline of regression analysis\n\nThe following outline is provided as an overview of and topical guide to regression analysis:\n\nRegression analysis – use of statistical techniques for learning about the relationship between one or more dependent variables (\"Y\") and one or more independent variables (\"X\").\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "20698519", "url": "https://en.wikipedia.org/wiki?curid=20698519", "title": "Parastichy", "text": "Parastichy\n\nParastichy, in phyllotaxy, is the (invisible) spiral line of the pattern of the areoles on some plants, such as cacti and pinecones. These spirals involve the insertion of a single primordium.\n\n\n"}
{"id": "38710316", "url": "https://en.wikipedia.org/wiki?curid=38710316", "title": "Quantitative biology", "text": "Quantitative biology\n\nQuantitative biology is an umbrella term encompassing the use of mathematical, statistical or computational techniques to study life and living organisms. The central theme and goal of quantitative biology is the creation of predictive models based on fundamental principles governing living systems.\n\nThe subfields of biology that employ quantitative approaches include:\n"}
{"id": "23332738", "url": "https://en.wikipedia.org/wiki?curid=23332738", "title": "Self-image", "text": "Self-image\n\nSelf-image is the mental picture, generally of a kind that is quite resistant to change, that depicts not only details that are potentially available to objective investigation by others (height, weight, hair color, etc.), but also items that have been learned by that person about themself, either from personal experiences or by internalizing the judgments of others.\n\nSelf-image may consist of four types:\n\nThese four types may or may not be an accurate representation of the person. All, some or none of them may be true.\n\nA more technical term for self-image that is commonly used by social and cognitive psychologists is self-schema. Like any schema, self-schemas store information and influence the way we think and remember. For example, research indicates that information which refers to the self is preferentially encoded and recalled in memory tests, a phenomenon known as \"self-referential encoding\". Self-schemas are also considered the traits people use to define themselves, they draw information about the self into a coherent scheme.\n\nPoor self-image may be the result of accumulated criticisms that the person collected as a child which have led to damaging their own view of themselves. Children in particular are vulnerable to accepting negative judgments from authority figures because they have yet to develop competency in evaluating such reports. Also, adolescents are highly targeted to suffer from poor body image issues. Individuals that already exhibit a low-sense of self-worth may be vulnerable to develop social disorders.\n\nNegative self-images can arise from a variety of factors. A prominent factor, however, is personality type. Perfectionists, high achievers, and those with \"type A\" personalities seem to be prone to having negative self-images. This is because such people constantly set the standard for success high above a reasonable, attainable level. Thus, they are constantly disappointed in this \"failure.\"\n\nAnother factor that contributes to a negative self-image is the beauty values of the society in which a person lives. In the American society, a popular beauty ideal is thinness. Oftentimes, girls feel that they do not measure up to society's \"thin\" standards, which leads to them having a negative self-image.\n\nWhen people are in the position of evaluating others, self-image maintenance processes can lead to a more negative evaluation depending on the self-image of the evaluator. That is to say stereotyping and prejudice may be the way individuals maintain their self-image. When individuals evaluate a member of a stereotyped group, they are less likely to evaluate that person negatively if their self-images had been bolstered through a self-affirmation procedure, and they are more likely to evaluate that person stereotypically if their self-images have been threatened by negative feedback. Individuals may restore their self-esteem by derogating the member of a stereotyped group.\n\nFein and Spencer (1997) conducted a study on Self-image Maintenance and Discriminatory Behavior. This study showed evidence that increased prejudice can result from a person's need to redeem a threatened positive perception of the self. The aim of the study was to test whether a particular threat to the self would instigate increased stereotyping and lead to actual discriminatory behavior or tendencies towards a member of a \"negatively\" stereotyped group. \nThe study began when Fein and Spencer gave participants an ostensible test of intelligence. Some of them received negative feedback, and others, positive and supportive feedback. In the second half of the experiment, the participants were asked to evaluate another person who either belonged to a \"negatively stereotyped group\", or one who did not. \nThe results of the experiment proved that the participants who had previously received unfavorable comments on their test, evaluated the target of the \"negatively stereotyped group\" in a more antagonistic or opposing way, than the participants who were given excellent reports on their intelligence test. They concluded that the negative feedback on the test threatened the participants' self-image and they evaluated the target in a more negative manner, all in efforts to restore their own self-esteem.\n\nA present study extends the studies of Fein and Spencer in which the principal behavior examined was avoidance behavior. In the study, Macrae et al. (2004) found that participants that had a salient negative stereotype of \"skinheads\" attached, physically placed themselves further from a skinhead target compared to those in which the stereotype was not as apparent. Therefore, greater salience of a negative stereotype led participants to show more stereotype-consistent behavior towards the target.\n\nResidual self-image is the concept that individuals tend to think of themselves as projecting a certain physical appearance, or certain position of social entitlement, or lack thereof. The term was used at least as early as 1968, but was popularized in fiction by the \"Matrix\" series, where persons who existed in a digitally created world would subconsciously maintain the physical appearance that they had become accustomed to projecting.\n\nVictims of abuse and manipulation often get trapped into a self-image of victimisation. The psychological profile of victimisation includes a pervasive sense of helplessness, passivity, loss of control, pessimism, negative thinking, strong feelings of self-guilt, shame, self-blame and depression. This way of thinking can lead to hopelessness and despair.\n\nSelf-image disparity was found to be positively related to chronological age (CA) and intelligence, two factors thought to increase concomitantly with maturity: Capacity for guilt and ability for cognitive differentiation. However, males had larger self-image disparities than females, Caucasians had larger disparities and higher ideal self images than African Americans, and socioeconomic status (SES) affected self-images differentially for the 2nd and 5th graders.\n\nA child's self-awareness of who they are differentiates into three categories around the age of five: their social self, academic persona, and physical attributes. Several ways to strengthen a child's self-image include communication, reassurance, support of hobbies, and finding good role models.\n\nWhen does a child become aware that the image in a mirror is their own? Research was done on 88 children between 3 and 24 months. Their behaviors were observed before a mirror. The results indicated that children's awareness of self-image followed three major age-related sequences: \n\nRegular practice of endurance exercise was related to a more favourable self-image. There was a strong association between participation in sports and the type of personality that tends to be resistant to drug and alcohol addiction. Physical exercise was further significantly related to scores for physical and psychological well-being. Adolescents who engaged regularly in physical activity were characterised by lower anxiety-depression scores, and displayed much less social behavioural inhibition than their less active counterparts.\n\nIt is likely that discussion of recreational or exercise involvement may provide a useful point of entry for facilitating dialogue among adolescents about concerns relating to body image and self-esteem. In terms of psychotherapeutic applications, physical activity has many additional rewards for adolescents. It is probable that by promoting physical fitness, increased physical performance, lessening body mass and promoting a more favourable body shape and structure, exercise will provide more positive social feedback and recognition from peer groups, and this will subsequently lead to improvement in an individual's self-image.\n\nDoes self-image threatening feedback make perceivers more likely to activate stereotypes when confronted by members of a minority group? Participants in Study 1 saw an Asian American or European American woman for several minutes, and participants in Studies 2 and 3 were exposed to drawings of an African American or European American male face for fractions of a second. These experiments found no evidence of automatic stereotype activation when perceivers were cognitively busy and when they had not received negative feedback. When perceivers had received negative feedback, however, evidence of stereotype activation emerged even when perceivers were cognitively busy.\n\nA magazine survey that included items about body image, self-image, and sexual behaviors was completed by 3,627 women. The study found that overall self-image and body image are significant predictors of sexual activity. Women more satisfied with body image reported more sexual activity, orgasm, and initiating sex, greater comfort undressing in front of their partner, having sex with the lights on, trying new sexual behaviors (e.g. anal sex), and pleasing their partner sexually than those dissatisfied. Positive body image was inversely related to self-consciousness and importance of physical attractiveness, and positively related to relationships with others and overall satisfaction. Body image was predictive only of one's comfort undressing in front of partner and having sex with lights on. Overall satisfaction was predictive of frequency of sex, orgasm, and initiating sex, trying new sexual behaviors, and confidence in giving partner sexual pleasure.\n\nOne hundred and ten heterosexual individuals (67 men; 43 women) responded to questions related to penis size and satisfaction. Men showed significant dissatisfaction with penile size, despite perceiving themselves to be of average size. Importantly, there were significant relationships between penile dissatisfaction and comfort with others seeing their penis, and with likelihood of seeking medical advice with regard to penile and/or sexual function. Given the negative consequences of low body satisfaction and the importance of early intervention in sexually related illnesses (e.g., testicular cancer), it is imperative that attention be paid to male body dissatisfaction.\n"}
{"id": "39172130", "url": "https://en.wikipedia.org/wiki?curid=39172130", "title": "Shafter Research Station", "text": "Shafter Research Station\n\nShafter Research Station is an agricultural research station in Shafter in the San Joaquin Valley, within Kern County, California.\n\nThe station, which was established in 1922, was used by the U.S. Department of Agriculture to research cotton. Initial research at the station focused on growing long-staple cotton, which was used to make airplane wings at the time.\n\nBy 1925, the researchers had determined that Acala cotton was the highest-quality variety of long-staple cotton; they then developed the \"one variety\" method of cotton production, in which every California cotton producer would grow Acala cotton. As a result of this research, the state of California enacted the California One Variety Cotton Act, which mandated that California cotton producers could grow only Acala cotton. The law spurred the growth of California's fledgling cotton industry, which now forms a major part of the state's agricultural economy. The success of the \"one variety\" policy caused the station to earn an international reputation for its research, and procedures developed at the station have been used in the Australian and Israeli cotton industries.\n\nThe Shafter Research Station was added to the National Register of Historic Places on October 17, 1997.\n\n\n 3. Shafter Research Station Website Background and History Page\n\n"}
{"id": "1802289", "url": "https://en.wikipedia.org/wiki?curid=1802289", "title": "Soyuz-TM", "text": "Soyuz-TM\n\nThe Soyuz-TM crew transports (T - транспортный - \"Transportnyi\" - meaning transport, M - модифицированный - \"Modifitsirovannyi\" - meaning modified) were fourth generation (1986–2002) Soyuz spacecraft used for ferry flights to the Mir and ISS space stations. It added to the Soyuz-T new docking and rendezvous, radio communications, emergency and integrated parachute/landing engine systems. The new Kurs rendezvous and docking system and the new KTDU-80 propulsion module permitted the Soyuz-TM to maneuver independently of the station, without the station making \"mirror image\" maneuvers to match unwanted translations introduced by earlier models' aft-mounted attitude control.\n\nThe final Soyuz-TM flight was Soyuz TM-34, which launched April 25, 2002 and landed November 10, 2002.\n\n"}
{"id": "91182", "url": "https://en.wikipedia.org/wiki?curid=91182", "title": "System analysis", "text": "System analysis\n\nSystem analysis in the field of electrical engineering that characterizes electrical systems and their properties. System analysis can be used to represent almost anything from population growth to audio speakers; electrical engineers often use it because of its direct relevance to many areas of their discipline, most notably signal processing, communication systems and control systems.\n\nA system is characterized by how it responds to input signals. In general, a system has one or more input signals and one or more output signals. Therefore, one natural characterization of systems is by how many inputs and outputs they have:\n\nIt is often useful (or necessary) to break up a system into smaller pieces for analysis. Therefore, we can regard a SIMO system as multiple SISO systems (one for each output), and similarly for a MIMO system. By far, the greatest amount of work in system analysis has been with SISO systems, although many parts inside SISO systems have multiple inputs (such as adders).\n\nSignals can be continuous or discrete in time, as well as continuous or discrete in the values they take at any given time:\n\nWith this categorization of signals, a system can then be characterized as to which type of signals it deals with:\n\nAnother way to characterize systems is by whether their output at any given time depends only on the input at that time or perhaps on the input at some time in the past (or in the future!).\n\nAnalog systems with memory may be further classified as \"lumped\" or \"distributed\". The difference can be explained by considering the meaning of memory in a system. Future output of a system with memory depends on future input and a number of state variables, such as values of the input or output at various times in the past. If the number of state variables necessary to describe future output is finite, the system is lumped; if it is infinite, the system is distributed.\n\nFinally, systems may be characterized by certain properties which facilitate their analysis:\n\nThere are many methods of analysis developed specifically for linear time-invariant (\"LTI\") deterministic systems. Unfortunately, in the case of analog systems, none of these properties are ever perfectly achieved. Linearity implies that operation of a system can be scaled to arbitrarily large magnitudes, which is not possible. Time-invariance is violated by aging effects that can change the outputs of analog systems over time (usually years or even decades). Thermal noise and other random phenomena ensure that the operation of any analog system will have some degree of stochastic behavior. Despite these limitations, however, it is usually reasonable to assume that deviations from these ideals will be small.\n\nAs mentioned above, there are many methods of analysis developed specifically for LTI systems. This is due to their simplicity of specification. An LTI system is completely specified by its transfer function (which is a rational function for digital and lumped analog LTI systems). Alternatively, we can think of an LTI system being completely specified by its frequency response. A third way to specify an LTI system is by its characteristic linear differential equation (for analog systems) or linear difference equation (for digital systems). Which description is most useful depends on the application.\n\nThe distinction between lumped and distributed LTI systems is important. A lumped LTI system is specified by a finite number of parameters, be it the zeros and poles of its transfer function, or the coefficients of its differential equation, whereas specification of a distributed LTI system requires a complete function\n\n\n"}
{"id": "43631132", "url": "https://en.wikipedia.org/wiki?curid=43631132", "title": "The Organized Mind", "text": "The Organized Mind\n\nThe Organized Mind: Thinking Straight in the Age of Information Overload is a bestselling popular science book written by the McGill University neuroscientist Daniel J. Levitin, PhD, and first published by Dutton Penguin in the United States and Canada in 2014. It is Levitin's 3rd consecutive best-seller, debuting at #2 on the \"New York Times Best Seller List\", #1 on the Canadian best-seller lists, #1 on Amazon, and #5 on \"The London Times\" bestseller list.\n\nIn \"The Organized Mind\", Levitin demonstrates how the Information Age is drowning us with an unprecedented deluge of data, and uses the latest brain science to explain how the brain can organize this flood of information. Levitin then demonstrates methods that readers can use to regain a sense of mastery over the way they organize their homes, workplaces, and time. It answers three fundamental questions: Why does the brain pay attention to some things and not others? Why do we remember some things and not others? And how can we use that knowledge to better organize our home and workplaces, our time, social world, and decision making? \n\nThe book is divided in three parts. The first part focuses on attention. Levitin explains why attention is the most essential mental resource for any organism and describes how the brain’s attentional system works: it determines which aspects of the environment an individual will deal with, and what gets passed through to that individual’s conscious awareness. The attentional awareness system is the reason one can safely drive or walk to work without noticing most of the buildings or cars one passes by.\n\nAdditionally, Levitin reveals that the phrase \"paying attention\" is scientifically true. Multitasking comes at an actual metabolic cost: switching back and forth between tasks burns a lot more oxygenated glucose (the fuel the brain runs on) than focusing on one task does, and can lead quickly to mental exhaustion.\n\nThe second and third parts of the book show how readers can use their attentional and memory systems for better organization, from the classroom to the boardroom, from home lives to interactions with friends, doctors, and business associates.\nOn publication, the book received praise from a wide array of people including former U.S. Secretary of State (and Secretary of the Treasury) George P. Shultz; Gen. Stanley A. McChrystal (ret.), Nobel Prize–winning neuroscientist Stanley Prusiner, and head writer for \"The Big Bang Theory\", Eric Kaplan.\n\n"}
{"id": "43156502", "url": "https://en.wikipedia.org/wiki?curid=43156502", "title": "Theodore von Kármán bibliography", "text": "Theodore von Kármán bibliography\n\nThis is a bibliography of works by Theodore von Kármán.\n\n\n\n"}
{"id": "21714485", "url": "https://en.wikipedia.org/wiki?curid=21714485", "title": "Trusteeship (Gandhism)", "text": "Trusteeship (Gandhism)\n\nTrusteeship is a socio-economic philosophy that was propounded by Mahatma Gandhi. It provides a means by which the wealthy people would be the trustees of trusts that looked after the welfare of the people in general. This concept was condemned by socialists as being in favor of the landlords, feudal princes and the capitalists, opposed to socialist theories.\nGandhi believed that the wealthy people could be persuaded to part with their wealth to help the poor. Putting it in Gandhiji's words \"Supposing I have come by a fair amount of wealth – either by way of legacy, or by means of trade and industry – I must know that all that wealth does not belong to me; what belongs to me is the right to an honourable livelihood by millions of others. The rest of my wealth belongs to the community and must be used for the welfare of the community.\" \nGandhi along with his followers, after their release from\nprison formulated a \"simple\" and a \"practical\" formula where Trusteeship was explained. A draft practical trusteeship formula was \"prepared by Gandhi’s co-workers, Narhari Parikh and Kishorelal Mashruwala\" and it was fine-tuned by M.L. Dantwala. \n\nThe founder of the Tata group, J.R.D. Tata was influenced by Gandhi's idea of\ntrusteeship. He developed his personal and professional life based on this idea.\n\n"}
