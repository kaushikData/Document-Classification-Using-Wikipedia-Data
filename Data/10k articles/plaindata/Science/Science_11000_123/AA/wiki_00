{"id": "27150875", "url": "https://en.wikipedia.org/wiki?curid=27150875", "title": "ADITYA (tokamak)", "text": "ADITYA (tokamak)\n\nADITYA is a medium size tokamak installed at the Institute for Plasma Research in India. \n\nIt has a major radius of 0.75 metres and a minor radius of 0.25 metres. The maximum field strength is 1.2¬†tesla produced by 20 toroidal field coils spaced symmetrically in the toroidal direction. It is operated by two power supplies, a capacitor bank and the APPS (ADITYA pulse power supply). \n\nThe typical plasma parameters during capacitor bank discharges are: I ~30¬†kA, shot duration ~25¬†ms, central electron temperature ~100¬†eV and core plasma density ~10¬†m and the typical parameters of APPS operation is ~100¬†kA plasma current, ~ 100¬†ms duration, central electron temp. ~300¬†eV and ~3x10¬†m core plasma density. \n\nVarious diagnostics used in ADITYA include electric and magnetic probes, microwave interferometry, Thomson scattering and charge exchange spectroscopy.\n\n"}
{"id": "30511763", "url": "https://en.wikipedia.org/wiki?curid=30511763", "title": "AIXI", "text": "AIXI\n\nAIXI is a theoretical mathematical formalism for artificial general intelligence.\nIt combines Solomonoff induction with sequential decision theory.\nAIXI was first proposed by Marcus Hutter in 2000 and the results below are proved in Hutter's 2005 book \"Universal Artificial Intelligence\".\n\nAIXI is a reinforcement learning agent;\nit maximizes the expected total rewards received from the environment.\nIntuitively, it simultaneously considers every computable hypothesis.\nIn each time step, it looks at every possible program and evaluates how many rewards that program generates depending on the next action taken.\nThe promised rewards are then weighted by the subjective belief that this program constitutes the true environment.\nThis belief is computed from the length of the program: longer programs are considered less likely, in line with Occam's razor. \nAIXI then selects the action that has the highest expected total reward in the weighted sum of all these programs.\n\nThe AIXI agent interacts sequentially with some (stochastic and unknown to AIXI) environment formula_1.\nIn step \"t\", the agent outputs an action formula_2 and\nthe environment responds with an observation formula_3 and a reward formula_4 distributed according to the conditional probability\nformula_5.\nThen this cycle repeats for \"t + 1\".\nThe agent tries to maximize cumulative future reward formula_6 for a fixed lifetime \"m\".\n\nGiven a current time \"t\" and history formula_7,\nthe action AIXI outputs is defined as\n\nwhere \"U\" denotes a monotone universal Turing machine, and\n\"q\" ranges over all programs on the universal machine \"U\".\n\nThe parameters to AIXI are the universal Turing machine and the agent's lifetime \"m\".\nThe latter dependence can be removed by the use of discounting.\n\nAIXI's performance is measured by the expected total number of rewards it receives.\nAIXI has been proven to be optimal in the following ways.\n\n\nIt was later shown by Hutter and Jan Leike that balanced Pareto optimality is subjective and that any policy can be considered Pareto optimal, which they describe as undermining all previous optimality claims for AIXI.\n\nHowever, AIXI does have limitations. It is restricted to maximizing rewards based on percepts as opposed to external states. It also assumes it interacts with the environment solely through action and percept channels, preventing it from considering the possibility of being damaged or modified. Colloquially, this means that it doesn't consider itself to be contained by the environment it interacts with. It also assumes the environment is computable. Since AIXI is incomputable (see below), it assigns zero probability to its own existence.\n\nLike Solomonoff induction, AIXI is incomputable.\nHowever, there are computable approximations of it.\nOne such approximation is AIXI\"tl\",\nwhich performs at least as well as the provably best time \"t\" and space \"l\" limited agent.\nAnother approximation to AIXI with a restricted environment class is MC-AIXI(FAC-CTW),\nwhich has had some success playing simple games such as partially observable Pac-Man.\n\n\n"}
{"id": "21026017", "url": "https://en.wikipedia.org/wiki?curid=21026017", "title": "ARCADE", "text": "ARCADE\n\nAbsolute Radiometer for Cosmology, Astrophysics, and Diffuse Emission (ARCADE) is a program which utilizes high-altitude balloon instrument package intended to measure the heating of the universe by the first stars and galaxies after the big bang and search for the signal of relic decay or annihilation. In July 2006 a strong residual radio source was found using the radiometer, approximately six times what is predicted by theory. This phenomenon is known as \"space roar\" and remains an unsolved problem in astrophysics.\n\nARCADE has been funded by the NASA's Science Mission Directorate under the Astronomy and Physics Research and Analysis Suborbital Investigation program. The program is composed of a team led by Alan Kogut of NASA's Goddard Space Flight Center. ARCADE was launched from NASA's Columbia Scientific Balloon Facility in Palestine, Texas, conducted under the auspices of the Balloon Program Office at Wallops Flight Facility. The balloon flew to an altitude of , viewing about 7% of the sky during its observations.\n\nThe instrument is designed to detect radiation at centimeter wavelengths. The craft contained seven radiometers which were cooled to using liquid helium, with the intent to measure temperature differences as small as 1/1000 of a degree against a background which is only . The optics in the instrument package were placed near the top of the dewar flask which cooled them in order to prevent the instruments from seeing the walls of the container, thereby simplifying the processing of the observational data. This design choice necessitated the use of superfluid pumps in order to drench the radiometers in liquid helium. The design also utilized heaters in order to create a cloud of helium gas, in place of using a (relatively warm) window, which also simplified processing of the observational data.\n\nIn 2011, the ARCADE 2 researchers reported, \"Correcting for instrumental systematic errors in measurements such as ARCADE 2 is always a primary concern. We emphasize that we detect residual emission at 3 GHz with the ARCADE 2 data, but the result is also independently detected by a combination of low-frequency data and FIRAS.\"\n\n\n"}
{"id": "27197456", "url": "https://en.wikipedia.org/wiki?curid=27197456", "title": "A Passage to Infinity", "text": "A Passage to Infinity\n\n\n\n"}
{"id": "2731947", "url": "https://en.wikipedia.org/wiki?curid=2731947", "title": "Anaeroplasmatales", "text": "Anaeroplasmatales\n\nAnaeroplasmatales is an order of mollicute bacteria which are generally found in the rumens of cattle and sheep. The only family in the order is the family Anaeroplasmataceae.\n\nMembers of the order Anaeroplasmatales can appear as different shapes at different times in their lifecycles. Cells which are 16‚Äì18 hours old tend to be spherical. When the cells are older, they can take on various shapes. Anaeroplasmatales are not motile. Anaeroplasmatales cannot grow in the presence of oxygen. They do grow at a temperature of 37¬†¬∞C on microbiological media, where they form irregular-shaped colonies with a \"fried-egg\" appearance, similar to other mycoplasmas. Anaeroplasmatales are negative by Gram stain.\n\nThe order Anaeroplasmatales was created in 1987 to encompass the family Anaeroplasmataceae which itself was created to hold the anaerobic mycoplasmas \"Anaeroplasma\" and \"Asteroleplasma\".\n\n"}
{"id": "28913422", "url": "https://en.wikipedia.org/wiki?curid=28913422", "title": "Anuchin Glacier", "text": "Anuchin Glacier\n\nAnuchin Glacier () is a glacier draining southward to Lake Unter-See in the northern part of the Gruber Mountains, Queen Maud Land. It was discovered, and plotted from air photos, by the Third German Antarctic Expedition, 1938‚Äì39. It was mapped from air photos and from surveys by the Sixth Norwegian Antarctic Expedition, 1956‚Äì60, and remapped by the Soviet Antarctic Expedition, 1960‚Äì61, and named after Dmitry Nikolayevich Anuchin, Soviet geographer.\n\n"}
{"id": "7626743", "url": "https://en.wikipedia.org/wiki?curid=7626743", "title": "Atomic ratio", "text": "Atomic ratio\n\nThe atomic ratio is a measure of the ratio of atoms of one kind (i) to another kind (j). A closely related concept is the atomic percent (or at.%), which gives the percentage of one kind of atom relative to the total number of atoms. The molecular equivalents of these concepts are the molar fraction, or molar percent.\n\nMathematically, the \"atomic percent\" is\n\nwhere \"N\" are the number of atoms of interest and \"N\" are the total number of atoms, while the \"atomic ratio\" is\n\nFor example, the \"atomic percent\" of hydrogen in water (HO) is , while the \"atomic ratio\" of hydrogen to oxygen is .\n\nAnother application is in radiochemistry, where this may refer to isotopic ratios or isotopic abundances. Mathematically, the \"isotopic abundance\" is\nwhere \"N\" are the number of atoms of the isotope of interest and \"N\" is the total number of atoms, while the \"atomic ratio\" is\n\nFor example, the \"isotopic ratio\" of deuterium (D) to hydrogen (H) in heavy water is roughly (corresponding to an \"isotopic abundance\" of 0.00014%).\n\nIn laser physics however, the \"atomic ratio\" may refer to the doping ratio or the doping fraction.\n\n\n"}
{"id": "5725168", "url": "https://en.wikipedia.org/wiki?curid=5725168", "title": "Bhaskaracharya Pratishthana", "text": "Bhaskaracharya Pratishthana\n\nBhaskaracharya Pratishthana is a research and education institute for mathematics in Pune, India, founded by noted Indian-American mathematician professor Shreeram Abhyankar.\nThe institute is named after the great ancient Indian Mathematician Bhaskaracharya (Born in 1114 A.D.). Bhaskaracharya Pratishthana is a Pune, India, based institute founded in 1976. It has researchers working in many areas of mathematics, particularly in algebra and number theory.\n\nSince 1990, the Pratishthana has been the nodal center for the \"Regional Mathematics Olympiad\" under the National Board of Higher Mathematics. Pratishthana is noted for its mathematics olympiad training programs for high-school students.\n\nPratishthana publishes the mathematics periodical \"Bona Mathematica\" and has published texts in higher and olympiad mathematics.\n\nRecently, in Pratishthana, two projects supported by MHRD have been started. One is on e-learning and other on the use of free open source software in Mathematics education (FOOSME). In the e-learning project video broadcasting of online Maths lectures is being done. The topics are Ring Theory and Complex Analysis. These lectures are at Undergraduate levels. The FOSSME project is all about exploring FOSS for Maths Education. There were 3 national level workshops held on FOSS for Maths education, Scilab, Advanced and LaTeX Advanced.\n\n"}
{"id": "27544928", "url": "https://en.wikipedia.org/wiki?curid=27544928", "title": "Bogen Glacier", "text": "Bogen Glacier\n\nBogen Glacier () is a small glacier on the north side of Drygalski Fjord between Trendall Crag and Hamilton Bay, at the southeast end of South Georgia. It was named by the UK Antarctic Place-Names Committee in 1979 after Arne Bogen, Norwegian sealer working in South Georgia after 1950; Master of the sealing vessel \"Albatross\" and Station Foreman, Grytviken.\n\n"}
{"id": "1612600", "url": "https://en.wikipedia.org/wiki?curid=1612600", "title": "Christopher Aikman", "text": "Christopher Aikman\n\nChristopher Aikman (born 1943) is a Canadian astrophysicist who spent most of his career (from 1968 to 1997) at the Dominion Astrophysical Observatory (DAO) in Saanich, British Columbia, Canada.\n\nAn early interest in Astronomy led Aikman to join the Royal Astronomical Society of Canada Quebec Centre in 1958, at the age of 15. He received a B.Sc. from Bishop's University in 1965 and a M.Sc. from the University of Toronto in 1968. His Master's work focused on microwave observations of H II regions and the planetary nebula IC 1795.\n\nIn 1968 joined the staff of the Dominion Astrophysical Observatory. His initial research was on the spectroscopy of chemically peculiar stars, whose surface compositions differ markedly from that of the Sun, with the aim of understanding the origin of their anomalies. From 1991, he conducted a program of tracking Earth approaching asteroids with the historic telescope built by John S. Plaskett, but the project was cancelled in 1997.\n\nHe was the Canadian representative on the Spaceguard Foundation, a group concerned with assessing the asteroid impact threat to Earth. An incidental product of this research was the discovery of four asteroids between 1994 and 1998 (as credited by the Minor Planet Center).\n\nIn 1983, Aikman won the Royal Astronomical Society of Canada Service Award for his significant contributions to RASC Victoria Centre.\n\n"}
{"id": "544294", "url": "https://en.wikipedia.org/wiki?curid=544294", "title": "City Beautiful movement", "text": "City Beautiful movement\n\nThe City Beautiful Movement was a reform philosophy of North American architecture and urban planning that flourished during the 1890s and 1900s with the intent of introducing beautification and monumental grandeur in cities. The movement, which was originally associated mainly with Chicago, Cleveland, Detroit, and Washington, D.C., promoted beauty not only for its own sake, but also to create moral and civic virtue among urban populations. Advocates of the philosophy believed that such beautification could promote a harmonious social order that would increase the quality of life, while critics would complain that the movement was overly concerned with aesthetics at the expense of social reform; Jane Jacobs referred to the movement as an \"architectural design cult.\"\n\nThe movement began in the United States in response to crowding in tenement districts, a consequence of high birth rates, increased immigration and internal migration of rural populations into cities. The movement flourished for several decades, and in addition to the construction of monuments, it also achieved great influence in urban planning that endured throughout the 20th century, particularly in regard to United States public housing projects. The \"Garden City\" movement in Britain influenced the contemporary planning of some newer suburbs of London, and there was cross-influence between the two aesthetics, one based in formal garden plans and urbanization schemes and the other, with its \"semi-detached villas\" evoking a more rural atmosphere.\n\nThe particular architectural style of the movement borrowed mainly from the contemporary Beaux-Arts and neoclassical architectures, which emphasized the necessity of order, dignity, and harmony.\n\nThe first large-scale elaboration of the City Beautiful occurred during the World's Columbian Exposition of 1893 in Chicago. The planning of the exposition was directed by architect Daniel Burnham, who hired architects from the eastern United States, as well as the sculptor Augustus Saint-Gaudens, to build large-scale Beaux-Arts monuments that were vaguely classical with uniform cornice height. The exposition displayed a model city of grand scale, known as the \"White City\", with modern transport systems and no poverty visible. The exposition is credited with resulting in the large-scale adoption of monumentalism for American architecture for the next 15 years. Richmond, Virginia's Monument Avenue is one expression of this initial phase.\n\nThe popularization begun by the World Columbian Exposition was increased by the Louisiana Purchase Exposition in St. Louis in 1904. The commissioner of architects selected Franco-American architect Emmanuel Louis Masqueray to be Chief of Design of the fair. In this position, which Masqueray held for three years, he designed the following fair buildings in the prevailing Beaux Arts mode: the Palace of Agriculture; the cascades and colonnades; the Palace of Forestry, Fish, and Game; the Palace of Horticulture; and the Palace of Transportation. All these were widely emulated in civic projects across the United States. Masqueray resigned soon after the fair opened in 1904, having been invited by Archbishop John Ireland of St. Paul to Minnesota to design a new cathedral for the city in the fair's Beaux-Arts style. Other celebrated architects of the fair's buildings, notably Cass Gilbert, who designed the Saint Louis Art Museum, originally the fair's Palace of the Fine Arts, similarly employed City Beautiful ideas from the exposition throughout their careers.\n\nAn early use of the City Beautiful ideal with the intent of creating social order through beautification was the McMillan Plan, (1902) named for Michigan Senator James McMillan. The plan emerged from the Senate Park Commission's redesigning of the monumental core of Washington, D.C. to commemorate the city's centennial and to fulfill unrealized aspects of the city plan of Pierre Charles L'Enfant a century earlier.\n\nThe Washington planners, who included Burnham, Saint-Gaudens, Charles McKim of McKim, Mead, and White, and Frederick Law Olmsted, Jr., visited many of the great cities of Europe. They hoped to make Washington monumental and green like the European capitals of the era; they believed that state-organized beautification could lend legitimacy to government during a time of social disturbance in the United States. The essence of the plan surrounded the United States Capitol with monumental government buildings to replace \"notorious slum communities\". At the heart of the design was the creation of the National Mall and eventually included Burnham's Union Station. The implementation of the plan was interrupted by World War I but resumed after the war, culminating in the construction of the Lincoln Memorial in 1922.\n\nThe success of the City Beautiful philosophy in Washington, D.C., is credited with influencing subsequent plans for beautification of many other cities, including Chicago, Baltimore, Cleveland (The Mall), Columbus (with the axis along State Street from the Ohio State Capitol building east to the Metropolitan Library and west to the Scioto River), Des Moines, Denver, Detroit (the Cultural Center, Belle Isle and Outer Drive), Madison (with the axis from the capitol building through State Street and to the University of Wisconsin campus), Montreal, New York City (notably the Manhattan Municipal Building), Philadelphia (the Benjamin Franklin Parkway museum district between Philadelphia City Hall and the Philadelphia Museum of Art), Pittsburgh (the Schenley Farms district in the Oakland neighborhood of parks, museums, and universities), San Antonio, Texas (San Antonio River development), San Francisco (manifested by its Civic Center), and the Washington State Capitol Campus in Olympia and the University of Washington's Rainier Vista in Seattle. In Wilmington, Delaware, it inspired the creation of Rodney Square and the surrounding civic buildings. In New Haven, John Russell Pope developed a plan for Yale University that eliminated substandard housing and relocated the urban poor to the peripheries. Kansas City, Missouri, and Dallas, Texas, undertook the installation of parkways and parks under the influence of the movement, and Coral Gables, Florida would be an example of a city consistent with the City Beautiful philosophy.\n\nDaniel Burnham's 1909 Plan of Chicago is considered one of principal documents of the City Beautiful movement. The plan featured a dynamic new civic center, axial streets, and a lush strip of parkland for recreation alongside the city's lakefront. Of these, only the lakefront park was implemented to any significant degree.\n\nIn 1913, the City of Chicago appointed a Commission with a mandate to ‚Äúmake Chicago Beautiful.‚Äù As part of the plan, the Pennsylvania Union Railroad Depot was to be moved to the west side of the City and replaced with a new modern depot. The West Side Property Owner‚Äôs Association was among those who objected. As reported by the Chicago Tribune, the Association‚Äôs attorney, Sidney Adler of Loeb & Adler, said, ‚ÄúAs I saw the beautiful picture of the city beautiful we will have fountains in West Madison Street, with poets and poetesses walking along Clinton, and the simple minded residents of the west side, after work is done, will take their gondolas and row on the limpid bosom of the Chicago River idlely strumming guitars.‚Äù \n\nPlanned out as a suburb of Miami, Florida in the early 1920s by George Edgar Merrick during the Florida land boom of the 1920s, Coral Gables was developed entirely upon the City Beautiful movement, with obelisks, fountains, and monuments seen in street roundabouts, parks, city buildings and around the city. Today, Coral Gables is one of Miami's most expensive suburban communities, long known for its strict zoning regulations which preserve the City Beautiful elements along with its Mediterranean Revival architecture style, which is prevalent throughout the city. Coral Gables has many parks and a heavy tree canopy with an urban forest planted largely in the 1920s.\n\nIn Denver, Colorado, Mayor Robert W. Speer endorsed City Beautiful planning, with a plan for a Civic Center, disposed along a grand esplanade that led to the Colorado State Capitol. The plan was partly realized, on a reduced scale, with the Greek amphitheater, Voorhies Memorial and the Colonnade of Civic Benefactors, completed in 1919. The Andrew Carnegie Foundation funded the Denver Public Library (1910), which was designed as a three-story Greek Revival temple with a colossal Ionic colonnade across its front; inside it featured open shelves, an art gallery and a children's room. Monuments and vistas were an essential feature of City Beautiful urban planning: in Denver, Paris-trained American sculptor Frederick MacMonnies was commissioned to design a monument marking the end of the Smoky Hill Trail. The bronze Indian guide he envisaged was vetoed by the committee and replaced with an equestrian Kit Carson.\n\nHarrisburg's movement of beautification and improvement was one of the earliest and most successful urban reform movements in the country. It began when local minded residents became convinced that their city was unattractive, unhealthy, and filthy, and lacked the appearance and facilities appropriate to its status as Pennsylvania's state capital. The causes of the city's defects were well known: industrialization in the previous half century had left the city poorly planned with unpaved streets and undeveloped water management systems. Residents of Harrisburg suffered disease and illnesses caused by the lack of good filtration systems that could filter the sewage dumped by populations further up the Susquehanna River. A disastrous fire that consumed the state capitol in 1897 had spawned new conversation about the suitability of Harrisburg as a state capital.\n\nThe improvement campaign was sparked by a riveting speech of conservationist Mira Lloyd Dock to the Harrisburg Board of Trade on December 20, 1900. Dock wanted to publicly challenge the horrific conditions in Harrisburg, and set out to gain public sentiment in support of changing them. \nDock‚Äôs speech was titled ‚ÄúThe City Beautiful‚Äù or ‚ÄúImprovement Work at Home and Abroad‚Äù, and this was the starting point for Harrisburg‚Äôs city improvements. Dock‚Äôs contemporary and closest ally in her drive for urban beautification was J. Horace McFarland, who was the president of the American Civic Association. With McFarland and Dock working together, they were able to push the process of municipal improvement in Harrisburg by convincing prominent community leaders to donate money, and by gathering the support of the majority of citizens. In April 1901, the Harrisburg Telegraph, a city newspaper, published a front-page article on the city‚Äôs problems, which stressed Dock‚Äôs message of beautification and recreation, paved streets, clean water, a city hall, land for parks, and a covered sewer interceptor along the river. The following February 1901, the population voted in favor of a bond issue that funded $1.1 million in new constructions and city planning. These improvements, combined with a new state capitol building in 1906, quickly transformed Harrisburg into a proud modern city by 1915.\n\nIn Memphis, the City Beautiful Commission was officially established by a city ordinance on July 1, 1930, making it the first and oldest beautification commission in the nation. It was the brainchild of the mayor, Mr. E. H. Crump. The first Commission was appointed and had operating expenses of $1,500. A small office was set up in The Nineteenth Century Club. Mrs. E. G. Willingham was chosen as chairman and Mrs. William B. Fowler served as vice chairman. In 1935, the Riverside Drive project was dedicated. Costing nearly $1,000,000 (largely WPA funds) the City Beautiful Commission landscaped the bluffs with crape myrtle, redbuds, magnolias, dogwoods and Paul Scarlet roses. White roses were planted at each guardrail post. In 1936, Mrs. William B. Fowler became chairman of the City Beautiful Commission and served for many years. City Beautiful grew under her leadership and soon had to relocate to larger headquarters. Through the efforts of City Beautiful, Memphis gained the title of cleanest city in Tennessee in 1940, 1941, 1942, 1943, 1944, 1945 and 1946. Memphis also received the Ernest T. Trigg ‚ÄúNation‚Äôs Cleanest City‚Äù award in 1948, 1949, 1950 and 1951. During this time, volunteers were organized into Wards and Block Clubs with Ward Chairmen and Block Captains. The City Beautiful staff grew to include 30 inspectors by 1954 who worked through these organizations to identify and improve eyesores. Memphis participated with the headquartered in Washington, D.C. In 1978, the Commission was reorganized, eliminating the field inspectors. In February 1989, the Commission moved to its present location at The Massey House in Victorian Village, Memphis.\n\nIn the 1920s, Palos Verdes Estates, California was established as a master planned community by noted American landscape architect, Frederick Law Olmsted. The community was designed as a \"City Beautiful.\" Among its earliest structures were the buildings comprising Malaga Cove Plaza which were designed in a Mediterranean Revival style popular with the City Beautiful movement.\n\nBoth European and North American cities provided models for the Australian City Beautiful movement. A combination of elements about 1900 also influenced the movement:\n\nHowever, City Beautiful was not solely concerned with aesthetics. The term ‚Äòbeautility‚Äô derived from the American city beautiful philosophy, which meant that the beautification of a city must also be functional. Beautility, including the proven economic value of improvements, influenced Australian town planning.\n\nThere were no formal city beautiful organisations that led this movement in Australia; rather it was influenced by communications among professionals and bureaucrats, in particular architect-planners and local government reformers. In the early Federation era some influential Australians were determined that their cities be progressive and competitive. Adelaide was used as an Australian example of the ‚Äúbenefits of comprehensive civic design‚Äù with its ring of parklands. Beautification of the city of Hobart, for example, was considered a way to increase the city‚Äôs popularity as a tourist destination.\n\nWalter Burley Griffin incorporated City Beautiful principles for his design for Canberra. Griffin was influenced by Washington ‚Äúwith grand axes and vistas and a strong central focal point‚Äù with specialised centres and, being a landscape architect, used the landscape to complement this layout. John Sulman, however, was Australia's \"leading proponent\" of the City Beautiful movement and, in 1921, wrote the book \"An Introduction to Australian City Planning\". Both the City Beautiful and the Garden City philosophies were represented by Sulman‚Äôs ‚Äúgeometric or contour controlled‚Äù designs of the circulatory road systems in Canberra. The widths of pavements were also reduced and vegetated areas were increased, such as planted road verges.\n\nMelbourne‚Äôs grid plan was considered dull and monotonous by some people, and so the architect William Campbell designed a blueprint for the city. The main principle behind this were diagonal streets, providing sites for new and comprehensive architecture and for special buildings. The designs of Paris and Washington were major inspirations for this plan.\n\nWorld War I prolonged the City Beautiful movement in Australia, as more memorials were erected than in any other country. Although City Beautiful, or artistic planning, became a part of comprehensive town planning, the Great Depression of the 1930s largely ended this fashion. Now, however, in Australia, many streets are tree-lined and streetscapes and skylines are regulated. This was largely a result of the City Beautiful philosophy.\n\n\nNotes\n"}
{"id": "4320083", "url": "https://en.wikipedia.org/wiki?curid=4320083", "title": "Clausius theorem", "text": "Clausius theorem\n\nThe Clausius theorem (1855) states that a system exchanging heat with external reservoirs and undergoing a cyclic process, is one that ultimately returns a system to its original state,\n\nwhere formula_2 is the infinitesimal amount of heat absorbed by the system from the reservoir and formula_3 is the temperature of the external reservoir (surroundings) at a particular instant in time. In the special case of a reversible process, the equality holds. The reversible case is used to introduce the entropy state function. This is because in a cyclic process the variation of a state function is zero. In words, the Clausius statement states that it is impossible to construct a device whose sole effect is the transfer of heat from a cool reservoir to a hot reservoir. Equivalently, heat spontaneously flows from a hot body to a cooler one, not the other way around. The generalized \"inequality of Clausius\" \n\nfor an infinitesimal change in entropy \"S\" applies not only to cyclic processes, but to any process that occurs in a closed system. \n\nThe Clausius theorem is a mathematical explanation of the second law of thermodynamics. It was developed by Rudolf Clausius who intended to explain the relationship between the heat flow in a system and the entropy of the system and its surroundings. Clausius developed this in his efforts to explain entropy and define it quantitatively. In more direct terms, the theorem gives us a way to determine if a cyclical process is reversible or irreversible. The Clausius theorem provides a quantitative formula for understanding the second law.\n\nClausius was one of the first to work on the idea of entropy and is even responsible for giving it that name. What is now known as the Clausius theorem was first published in 1862 in Clausius' sixth memoir, \"On the Application of the Theorem of the Equivalence of Transformations to Interior Work\". Clausius sought to show a proportional relationship between entropy and the energy flow by heating (Œ¥\"Q\") into a system. In a system, this heat energy can be transformed into work, and work can be transformed into heat through a cyclical process. Clausius writes that \"The algebraic sum of all the transformations occurring in a cyclical process can only be less than zero, or, as an extreme case, equal to nothing.\" In other words, the equation\n\nwith ùõø\"Q\" being energy flow into the system due to heating and \"T\" being absolute temperature of the body when that energy is absorbed, is found to be true for any process that is cyclical and reversible. Clausius then took this a step further and determined that the following relation must be found true for any cyclical process that is possible, reversible or not. This relation is the \"Clausius inequality\".\n\nNow that this is known, there must be a relation developed between the Clausius inequality and entropy. The amount of entropy S added to the system during the cycle is defined as\n\nIt has been determined, as stated in the second law of thermodynamics, that the entropy is a state function: It depends only upon the state that the system is in, and not what path the system took to get there. This is in contrast to the amount of energy added as heat (ùõø\"Q\") and as work (ùõø\"W\"), which may vary depending on the path. In a cyclic process, therefore, the entropy of the system at the beginning of the cycle must equal the entropy at the end of the cycle, formula_8, regardless of whether the process is reversible or irreversible. In the irreversible case, entropy will be created in the system, and more entropy must be extracted than was added formula_9 in order to return the system to its original state. In the reversible case, no entropy is created and the amount of entropy added is equal to the amount extracted.\n\nIf the amount of energy added by heating can be measured during the process, and the temperature can be measured during the process, the Clausius inequality can be used to determine whether the process is reversible or irreversible by carrying out the integration in the Clausius inequality.\n\nThe temperature that enters in the denominator of the integrand in the Clausius inequality is actually the temperature of the external reservoir with which the system exchanges heat. At each instant of the process, the system is in contact with an external reservoir.\nBecause of the Second Law of Thermodynamics, in each infinitesimal heat exchange process between the system and the reservoir, the net change in entropy of the \"universe\", so to say, is formula_10.\n\nWhen the system takes in heat by an infinitesimal amount formula_11(formula_12), for the net change in entropy formula_13 in this step to be positive, the temperature of the \"hot\" reservoir formula_14 needs to be slightly greater than the temperature of the system at that instant. \nIf the temperature of the system is given by formula_15 at that instant, then formula_16, and formula_17 forces us to have:\n\nThis means the magnitude of the entropy \"loss\" from the reservoir, formula_19 is less than the magnitude of the entropy gain formula_20(formula_12) by the system:\n\nSimilarly, when the system at temperature formula_22 expels heat in magnitude formula_23 (formula_24) into a colder reservoir (at temperature formula_25) in an infinitesimal step, then again, for the Second Law of Thermodynamics to hold, we would have, in an exactly similar manner:\n\nHere, the amount of heat 'absorbed' by the system is given by formula_27(formula_28), signifying that heat is transferring from the system to the reservoir, with formula_29. The magnitude of the entropy gained by the reservoir, formula_30 is greater than the magnitude of the entropy loss of the system formula_31\n\nSince the total change in entropy for the system is 0 in a cyclic process, if we add all the infinitesimal steps of heat intake and heat expulsion from the reservoir, signified by the previous two equations, with the temperature of the reservoir at each instant given by formula_3, we would have,\n\nIn particular.\n\nHence, we proved the Clausius Theorem.\n\nWe summarize the following, (the inequality in the third statement below, being obviously guaranteed by the second law of thermodynamics,which is the basis of our calculation), \n\nFor a reversible cyclic process, there is no generation of entropy in each of the infinitesimal heat transfer processes, and thus we would have the equality\n\nThus, the Clausius inequality is a consequence of applying the second law of thermodynamics at each infinitesimal stage of heat transfer, and is thus in a sense a weaker condition than the Second Law itself.\n\n\n"}
{"id": "7216822", "url": "https://en.wikipedia.org/wiki?curid=7216822", "title": "Contact order", "text": "Contact order\n\nThe contact order of a protein is a measure of the locality of the inter-amino acid contacts in the protein's native state tertiary structure. It is calculated as the average sequence distance between residues that form native contacts in the folded protein divided by the total length of the protein. Higher contact orders indicate longer folding times, and low contact order has been suggested as a predictor of potential downhill folding, or protein folding that occurs without a free energy barrier. This effect is thought to be due to the lower loss of conformational entropy associated with the formation of local as opposed to nonlocal contacts.\n\nRelative contact order (CO) is formally defined as:\nwhere \"N\" is the total number of contacts, Œî\"Si,j\" is the sequence separation, in residues, between contacting residues \"i\" and \"j\", and \"L\" is the total number of residues in the protein. The value of contact order typically ranges from 5% to 25% for single-domain proteins, with lower contact order belonging to mainly helical proteins, and higher contact order belonging to proteins with a high beta-sheet content.\n\nProtein structure prediction methods are more accurate in predicting the structures of proteins with low contact orders. This may be partly because low contact order proteins tend to be small, but is likely to be explained by the smaller number of possible long-range residue-residue interactions to be considered during global optimization procedures that minimize an energy function. Even successful structure prediction methods such as the Rosetta method overproduce low-contact-order structure predictions compared to the distributions observed in experimentally determined protein structures.\n\nThe percentage of the natively folded contact order can also be used as a measure of the \"nativeness\" of folding transition states. Phi value analysis in concert with molecular dynamics has produced transition-state models whose contact order is close to that of the folded state in proteins that are small and fast-folding. Further, contact orders in transition states as well as those in native states are highly correlated with overall folding time.\n\nIn addition to their role in structure prediction, contact orders can themselves be predicted based on a sequence alignment, which can be useful in classifying the fold of a novel sequence with some degree of homology to known sequences.\n"}
{"id": "2701185", "url": "https://en.wikipedia.org/wiki?curid=2701185", "title": "Correlation trading", "text": "Correlation trading\n\nIn finance, correlation trading is a strategy in which the investor gets exposure to the average correlation of an index. \n\nThe key to correlation trading is being able to predict when future realized correlation amongst the stocks of a particular index will be greater or less than the \"implied\" correlation level derived from derivatives on the index and its single stocks. One observation related to correlation trading is the principle of diversification, which implies that the volatility of a portfolio of securities is less than (or equal to) the average volatility of all the securities in that portfolio (based on Modern portfolio theory). The lower the correlation among the individual securities, the lower the overall volatility of the entire portfolio. This is due to the way in which variances behave when summing correlated random variables.\n\nTo sell correlation, investors can:\n\nIn practice, exchange-traded funds (ETF's) are sometimes chosen instead of indices.\n\n"}
{"id": "18865539", "url": "https://en.wikipedia.org/wiki?curid=18865539", "title": "Creation Evidence Museum", "text": "Creation Evidence Museum\n\nThe Creation Evidence Museum of Texas, originally Creation Evidences Museum, is a creationist museum in Glen Rose in Somervell County in central Texas, USA. Founded in 1984 by Carl Baugh for the purpose of researching and displaying exhibits that support creationism, it portrays the Earth as six thousand years old and humans coexisting with dinosaurs, disputing that the Earth is approximately 4.5 billion years old and dinosaurs became extinct 65.5 million years before human beings arose.\n\nThe Creation Evidence Museum was founded by Carl Baugh, a young Earth creationist, after he came to Glen Rose in 1982 to research claims of fossilized human footprints alongside dinosaur footprints in the limestone banks of the Paluxy River, near Dinosaur Valley State Park. He claims to have excavated 475 dinosaur footprints and 86 human footprints, which form the basis of the Creation Evidence Museum as well as other exhibits. Baugh, who does not have an accredited degree, remains the director and main speaker for CEM.\n\nIn 2001 Baugh and Creation Evidence Museum were featured on \"The Daily Show\" where Baugh likened human history to \"The Flintstones\" and the show poked fun at his claims about the hyperbaric biosphere, pterodactyl expeditions, and dinosaurs.\n\nThe Creation Evidence Museum sponsors continuing paleontological and archaeological excavations among other research projects, including a hunt for living pterodactyls in Papua New Guinea, and expeditions to Israel. Materials from the museum have been recommended by the National Council on Bible Curriculum in Public Schools, but the NCBCPS curriculum has been deemed \"unfit for use in public school classrooms.\"\n\nOne of the museum's projects is a \"hyperbaric biosphere\", a chamber which the museum hopes will reproduce the atmospheric conditions that these creationists postulate for Earth before the Great Flood, and enable them to grow dinosaurs. Baugh says that these conditions made creatures live longer, and get larger, smarter and nicer. He claims that tests under these conditions have tripled the lifespan of fruit-flies, and detoxified copperhead snakes. A much larger version is under construction in the new building.\n\nIn 2008, a descendant of a family that provided many original Paluxy River dinosaur tracks in the 1930s claimed that her grandfather had faked many of them, including the Alvis Delk Cretaceous Footprint. Zana Douglas, the granddaughter of George Adams, explained that during the 1930s depression her grandfather and other residents of Glen Rose made money by making moonshine and selling \"dinosaur fossils\". The faux fossils brought $15 to $30 and when the supply ran low, they \"just carved more, some with human footprints thrown in.\"\n\nAll of the creationist exhibits have been strongly criticized as incorrectly identified dinosaur prints, other fossils, or outright forgeries. The second floor balcony of the museum features prominently a high statue of Dallas Cowboys football coach Tom Landry.\n\nDisplays in the Creation Evidence Museum include:\n\nIn 1982‚Äì1984, several scientists, including J.R. Cole, L.R. Godfrey, R.J. Hastings, and Steven Schafersman, examined Baugh's purported \"mantracks\" as well as others provided by creationists in Glen Rose. In the course of the examination \"Baugh contradicted his own earlier reports of the locations of key discoveries\" and many of the supposed prints \"lacked characteristics of human footprints.\" After a three-year investigation of the tracks and Baugh's specimens, the scientists concluded there was no evidence for any of Baugh's claims or any \"dinosaur-man tracks\".\n\nYoung Earth creationist organizations such as Answers in Genesis and Creation Ministries International have criticized Baugh's claims saying he \"muddied the water for many Christians ... People are being misled.\" Don Batten, of Creation Ministries International wrote: \"Some Christians will try to use Baugh's 'evidences' in witnessing and get 'shot down' by someone who is scientifically literate. The ones witnessed to will thereafter be wary of all creation evidences and even more inclined to dismiss Christians as nut cases not worth listening to.\" Answers in Genesis lists the \"Paluxy tracks\" as arguments \"we think creationists should NOT use\" [emphasis in original]. The old Earth creationist organization Answers In Creation also reviewed Baugh's museum and concluded \"the main artifacts they claim show a young earth reveal that they are deceptions, and in many cases, not even clever ones.\"\n\nThe \"Burdick track\" and \"fossilized finger\" were featured on the controversial NBC program \"The Mysterious Origins of Man\", aired in 1996 and hosted by Charlton Heston. Creationist Ken Ham criticized the production in the February 1996 Answers in Genesis newsletter in a review titled \"Hollywood's 'Moses' Undermines Genesis.\" Ham attacked Baugh's claims, saying, \"According to leading creationist researchers, this evidence is open to much debate and needs much more intensive research. One wonders how much of the information in the program can really be trusted!\"\n\n\n"}
{"id": "5229661", "url": "https://en.wikipedia.org/wiki?curid=5229661", "title": "Crustose", "text": "Crustose\n\nCrustose is a habit of some types of algae and lichens in which the plant grows tightly appressed to a substrate, forming a biological layer of the adhering organism. \"Crustose\" adheres very closely to the substrates at all points. \"Crustose\" is found on rocks and tree bark. Some species of marine algae of the Rhodophyta, in particular members of the order Corallinales, family Corallinaceae, subfamily Melobesioideae with cell walls containing calcium carbonate grow to great depths in the intertidal zone, forming crusts on various substrates. The substrate can be rocks throughout the intertidal zone, or, as in the case of the Corallinales, reef-building corals, and other living organisms including plants, such as mangroves and animals such as shelled molluscs. The coralline red algae are major members of coral reef communities, cementing the corals together with their crusts. Among the brown algae, the order Ralfsiales comprises two families of crustose algae.\n\nMany lichens grow close to the surface of rocks, tree trunks, and other substrata, and are referred to as crustose lichens. Crustose organisms can be detrimental to engineered structures when found on buildings, coastal structures, and ships.\n\nThere are different types of Crustose lichens, including endolithic, endophloidic and leprose. Endolithic lichens are immersed in the outer layer of rocks with their bodies above the surface. Endophloidic ones are located in or on plant tissue. Leprose lichens consist of crusts without a layered structure.\n\nCrustose lichens have learned to adapt to their environment, with the shells helping with adaptation to dry and drought resistant climates. Crustose lichens have been found in deserts, ice free parts of Antarctica, and in the Alpine and Arctic regions.\n\nCrustose can come in a variety of colors such as yellow, orange, red, gray and green. These colors tend to be bright and vibrant. \n\nCrustose is similar to other lichens because they share a similar internal morphology. The lichen's body is formed from filaments of the fungal partner. The density of these filaments determines the layers within the lichen. \n"}
{"id": "22168509", "url": "https://en.wikipedia.org/wiki?curid=22168509", "title": "Does God Play Dice?", "text": "Does God Play Dice?\n\nDoes God Play Dice: The New Mathematics of Chaos is a non-fiction book about chaos theory written by British mathematician Ian Stewart. The book was initially published by Blackwell Publishing in 1989.\n\nIn this book, Stewart explains chaos theory to an audience presumably unfamiliar with it. As the book progresses the writing changes from simple explanations of chaos theory to in-depth, rigorous mathematical study. Stewart covers mathematical concepts such as differential equations, resonance, nonlinear dynamics, and probability. The book is illustrated with diagrams and graphs of mathematical concepts and equations when applicable.\n\nThe back of the book, and a summary of its content, reads, \"The science of chaos is forcing scientists to rethink Einstein's fundamental assumptions regarding the way the universe behaves. Chaos theory has already shown that simple systems, obeying precise laws, can nevertheless act in a random manner. Perhaps God plays dice within a cosmic game of complete law and order. Does God Play Dice? reveals a strange universe in which nothing may be as it seems. Familiar geometric shapes such as circles and ellipses give way to infinitely complex structures known as fractals, the fluttering of a butterfly's wings can change the weather, and the gravitational attraction of a creature in a distant galaxy can change the fate of the solar system.\"\n\nThe title of the book is a reference to a famous quote by Albert Einstein.\n\n\n"}
{"id": "12978543", "url": "https://en.wikipedia.org/wiki?curid=12978543", "title": "Edwin T. Layton", "text": "Edwin T. Layton\n\nEdwin Thomas Layton (April 7, 1903¬†‚Äì April 12, 1984) was a rear admiral in the United States Navy, who is most noted for his work as an intelligence officer during and before World War II.\n\nEdwin Thomas Layton was born on April 7, 1903 in Nauvoo, Illinois as a son of George E. Layton and his wife Mary C.Layton. Layton attended the United States Naval Academy at Annapolis, Maryland and graduated in 1924. Layton spent next five years with the Pacific Fleet aboard the battleship and destroyer .\n\nIn 1929, Layton was one of a small number of naval officers selected to go to Japan for language training. Significantly, on his voyage to Japan he met another young naval officer, Joseph J. Rochefort, assigned to the same duty. Both became intelligence officers, Rochefort specializing in decryption efforts, Layton in using intelligence information in war planning. Layton and Rochefort, both of whom were in Pearl Harbor, worked closely together in the months before the attack, among other things trying to work out aspects of the larger international context which Washington had decided would be handled by Washington alone, and even more closely after the war began, especially in the month before the Battle of Midway. They both made signal contributions to that victory.\n\nLayton was assigned to the American Embassy in Tokyo as a naval attach√© where he remained for three years. While in Japan, he met Admiral Isoroku Yamamoto on several occasions. The last four months he spent in Peiping (Beijing), China, as assistant naval attach√© at the American Legation.\n\nHis linguistic ability and fluency in Japanese proved to be assets as his career progressed, even more so as World War II began in Europe.\n\nDuring the 1930s, Layton served two tours of duty in the Navy Department's Office of Naval Intelligence, in 1933 and again in from 1936 to 1937, but he also saw sea duty. He had a three-year stint in the battleship Pennsylvania where he received commendations for gunnery excellence. In 1937, he returned to Tokyo for two years as assistant naval attach√© at the American Embassy. This was followed by a one-year tour of duty as Commanding Officer of (AG-19).\n\nExactly one year to the day before the attack on Pearl Harbor, Layton became Combat Intelligence Officer on the staff of Admiral Husband E. Kimmel, Commander-in-Chief of the United States Pacific Fleet, which had recently been moved from its base in San Diego, California to Pearl Harbor¬†‚Äî over the objections of Admiral James O. Richardson, whom Kimmel replaced. Layton was in charge of all intelligence in the Pacific Ocean area.\n\nLayton was a champion of using code-breaking information in war planning operations and had strong supporters in both Admiral Kimmel and Admiral Nimitz.\n\nLayton's book describes how Kimmel and his army counterpart at Pearl Harbor, General Walter C. Short, the commanders there, were scapegoats for failures by higher-ups in Washington, D.C. Layton blames Admiral Richmond K. Turner in particular for monopolizing naval intelligence in Washington that would have alerted Kimmel and his staff to the imminence of attack and to the fact that Pearl Harbor could be a target of that attack.\n\nLayton's argument is detailed and comprehensive, but in general, he maintains that although Washington was reading the highest level Japanese diplomatic code, Purple, little of this was ever made available to the field commanders, other than to MacArthur in the Philippines, who failed to act, not only on the Purple data, but even after he knew that Pearl Harbor had been attacked. The diplomatic information that they were denied not only contained data about the imminence of war, but also included messages sent from Honolulu to Tokyo by Takeo Yoshikawa, the spy sent to observe and report daily on the exact positions of ships in the harbor, using a grid system that was obviously designed for the purpose of targeting torpedoes and bombs. Those above Turner, including his boss, the Chief of Naval Operations, Admiral Stark, and even General Marshall, also come in for blame, though some details are still missing from the official record.\n\nForrest Biard, another naval linguist, one who was in the last group to be sent to Japan for language studies, worked for the Rochefort HYPO team as soon as he left Japan in 1941. HYPO was located in a basement, called \"The Dungeon\" by team members. In a speech to the National Cryptologic Museum Foundation, Biard describes Layton as the sixth member of the five-member team (Joseph J. Rochefort, Joe Finnegan, Alva B. Lasswell, Wesley A. Wright, Thomas Dyer) who produced the information that was vital to winning the Battle of Midway, following the Battle of the Coral Sea. He also characterizes Layton:\n\nDuring May 1942, in particular, Layton and the Rochefort team were battling Washington as much as the Japanese ‚Äì both as to where the next attack would occur and as to when it would occur. Washington said Port Moresby or the Aleutians in mid-June; Rochefort/Layton said Midway, first week in June. The story of how Rochefort's team prevailed is told in the Rochefort article, and in much greater detail in Layton's book. Nimitz deserves the highest praise for realizing that their analysis was sounder, something for which Layton deserves a very great deal of credit, and for risking the wrath of his boss in Washington, Admiral King, something for which Nimitz alone deserves a very great deal of credit. (Selecting Admiral Raymond Spruance to replace the hospitalized Admiral Halsey was also the right move, as was the earlier decision to retain Kimmel's intelligence officers.)\n\nLayton remained on the staff of the Pacific Fleet until February 1945, followed by a three-year tour of duty as Commander of the U.S. Naval Net Depot at Tiburon, California. During this time, Admiral Nimitz, as a mark of his recognition of Layton's contributions, invited him to Tokyo Bay when the Japanese formally surrendered on September 2, 1945.\n\nIntelligence work occupied Layton again, in the form of a two-year assignment as the first Director of the Naval Intelligence School in Washington D.C.\n\nStarting in 1950, Layton spent six months as Intelligence Officer on the staff of the Commandant, Fourteenth Naval District in Hawaii. His evaluative skills and keen interpretation of events were vital during the early stages of the conflict. In 1951, for a two-year period, he assumed his old position of Fleet Intelligence Officer on the staff of the Commander-in-Chief, Pacific Fleet.\n\nIn 1953, with the war over, he was assigned to the staff of the Joint Chiefs where he was Assistant Director for Intelligence, then Deputy Director. His last duty before retirement was Director of the Naval Intelligence School at the Naval Receiving Station, Washington, D.C.\n\nLayton retired in 1959. He went to work for the Northrop Corporation as Director of Far East Operations in Tokyo, Japan, 1959 to 1963. He retired from Northrop in 1964 and moved to Carmel, California. Not until the 1980s were many of the documents about Pearl Harbor and Midway declassified. His book, \"And I Was There: Pearl Harbor and Midway ‚Äî Breaking the Secrets\" was written with co-authors Roger Pineau and John Costello and was published in 1985, the year after Layton died. As appears in the books' acknowledgments, his wife, Miriam, assisted the publication of the book by not only encouraging the admiral to bring his story to print, but also by giving his collaborators access to his research notes and papers after his death.\n\nIn the 2019 film \"Midway\", he will be portrayed by actor Patrick Wilson.\n\nHere is the ribbon bar of Rear Admiral Edwin T. Layton:\n\nThe Naval War College in Newport, Rhode Island, honored Layton in the 1960s by naming the Chair of Naval Intelligence after him.\n\n"}
{"id": "2390915", "url": "https://en.wikipedia.org/wiki?curid=2390915", "title": "Electron spectrometer", "text": "Electron spectrometer\n\nIn an electron spectrometer, an incoming beam of electrons is bent with electric or magnetic fields. As higher energy electrons will be bent less by the beam, this produces a spatially distributed range of energies.\n\nElectron spectrometers are used on a range of scientific equipment, including particle accelerators, transmission electron microscopes, and astronomical satellites.\n\nAn electrostatic electron spectrometer uses the electric field, which cause electrons to move along field gradients, whereas magnetic devices cause electrons to move at right angles to the field. Magnetic fields will act in a direction perpendicular to the electron propagation, thereby conserving velocity, whereas electrostatic fields will cause electrons to move along the field gradient, which may change electron energies if the component of the direction of propagation and field gradients are not perpendicular. Owing to these effects, sector based designs are commonly used in electron spectrometers.\n\nThe effective potential in the solution of motion in a magnetic or electric system with rotational symmetry leads to radial focusing onto a mean radius. By superposition of a quadrupole field axial focusing is possible while weakening the radial focusing, until the astigmatism vanishes. By breaking the rotational symmetry a bit and varying the electrostatic potential along the mean path of the spherical aberration is minimized.\n\nThe spectrometer can use entrance and exit slits or use a small source, which only emits into specific angle and a small detector. Photoelectron spectra from single crystals exhibit a dependency on the emission angle, so that the entrance slit is needed. All the electrons from an isotopic source may be sucked off and focused into a directed beam (much like in an electron gun), which can then be analyzed. A position sensitive detector can detect the energy along one direction and depending on the additional optics lateral resolution or one angle along the other direction.\n\nElectrostatic spectrometers preserve the spin, which can be resolved afterwards.\n\n"}
{"id": "4870889", "url": "https://en.wikipedia.org/wiki?curid=4870889", "title": "Equine herpesvirus 3", "text": "Equine herpesvirus 3\n\nEquine herpesvirus 3 (EHV-3) is a virus of the family \"Herpesviridae\" that affects horses. It causes a disease known as equine coital exanthema. The disease is spread through direct and sexual contact and possibly through flies carrying infected vaginal discharge. EHV-3 has an incubation period of as little as two days. Signs of the disease include pustules and ulcerations of the vagina, penis, prepuce, and perineum. Lesions may also be seen on the lips and teats. Usually the only symptom seen is a decreased libido in stallions. The lesions heal within two weeks. As with other herpes viruses, the virus remains latent in the host for life. Carrier animals can sometimes be identified by spots of pigment loss on black skin in the genital region.\nEHV-3 is best prevented by taking note of present clinical signs and keeping infected horses isolated and breeding stock from sexual contact with other horses. Antibiotic ointments should be used on the lesions to prevent secondary bacterial infections and hasten the healing process. It is also important to use disposable gloves and instruments in veterinary exams as the virus can be spread by using contaminated equipment.\n"}
{"id": "10649523", "url": "https://en.wikipedia.org/wiki?curid=10649523", "title": "Eye (magazine)", "text": "Eye (magazine)\n\n\"For the Toronto-based weekly see \"Eye Weekly\".\"\nEye magazine, the international review of graphic design, is a quarterly print magazine on graphic design and visual culture.\n\nFirst published in London in 1990, \"Eye\" was founded by Rick Poynor, a prolific writer on graphic design and visual communication. Poynor edited the first twenty-four issues (1990-1997). Max Bruinsma was the second editor, editing issues 25‚Äì32 (1997‚Äì1999), before its current editor John L. Walters took over in 1999. Stephen Coates was art director for issues 1-26, Nick Bell was art director from issues 27-57, and Simon Esterson has been art director since issue 58.\n\nFrequent contributors include Phil Baines, Steven Heller, Steve Hare, Richard Hollis, Robin Kinross, Jan Middendorp, J. Abbott Miller, John O‚ÄôReilly, Rick Poynor, Alice Twemlow, Kerry William Purcell, Steve Rigley, Adrian Shaughnessy, David Thompson, Christopher Wilson and many others.\n\nOther contributors have included Nick Bell (creative director from issues 27-57), Gavin Bryars, Anne Burdick, Brendan Dawes, Simon Esterson (art director since issue 58), Malcolm Garrett, Anna Gerber, Jonathan Jones, Emily King, Ellen Lupton, Russell Mills, Quentin Newark, Tom Phillips, Robin Rimbaud, Stefan Sagmeister, Sue Steward, Erik Spiekermann, Teal Triggs, Val Williams and Judith Williamson.\n\nThe magazine has had five publishers: Wordsearch, Emap, Quantum Publishing, Haymarket Brand Media and Eye Magazine Ltd., formed in April 2008 after a management buyout.\n\n\n\n"}
{"id": "49396186", "url": "https://en.wikipedia.org/wiki?curid=49396186", "title": "First observation of gravitational waves", "text": "First observation of gravitational waves\n\nThe first direct observation of gravitational waves was made on 14 September 2015 and was announced by the LIGO and Virgo collaborations on 11 February 2016. Previously, gravitational waves had only been inferred indirectly, via their effect on the timing of pulsars in binary star systems. The waveform, detected by both LIGO observatories, matched the predictions of general relativity for a gravitational wave emanating from the inward spiral and merger of a pair of black holes of around 36 and 29 solar masses and the subsequent \"ringdown\" of the single resulting black hole. The signal was named GW150914 (from \"Gravitational Wave\" and the date of observation 2015-09-14). It was also the first observation of a binary black hole merger, demonstrating both the existence of binary stellar-mass black hole systems and the fact that such mergers could occur within the current age of the universe.\n\nThis first direct observation was reported around the world as a remarkable accomplishment for many reasons. Efforts to directly prove the existence of such waves had been ongoing for over fifty years, and the waves are so minuscule that Albert Einstein himself doubted that they could ever be detected. The waves given off by the cataclysmic merger of GW150914 reached Earth as a ripple in spacetime that changed the length of a 4-km LIGO arm by a thousandth of the width of a proton, proportionally equivalent to changing the distance to the nearest star outside the Solar System by one hair's width. The energy released by the binary as it spiralled together and merged was immense, with the energy of \"c\" solar masses ( joules or foes) in total radiated as gravitational waves, reaching a peak emission rate in its final few milliseconds of about watts ‚Äì a level greater than the combined power of all light radiated by all the stars in the observable universe.\n\nThe observation confirms the last remaining directly undetected prediction of general relativity and corroborates its predictions of space-time distortion in the context of large scale cosmic events (known as strong field tests). It was also heralded as inaugurating a new era of gravitational-wave astronomy, which will enable observations of violent astrophysical events that were not previously possible and potentially allow the direct observation of the very earliest history of the universe. The second observation of gravitational waves was made on 26 December 2015 and announced on 15 June 2016. Four more observations were made in 2017, including GW170817, the first observed merger of binary neutron stars, which was also observed in electromagnetic radiation.\n\nAlbert Einstein originally predicted the existence of gravitational waves in 1916, on the basis of his theory of general relativity. General relativity interprets gravity as a consequence of distortions in space-time, caused by mass. Therefore, Einstein also predicted that events in the cosmos would cause \"ripples\" in space-time ‚Äì distortions of space-time itself ‚Äì which would spread outward, although they would be so minuscule that they would be nearly impossible to detect by any technology foreseen at that time. It was also predicted that objects moving in an orbit would lose energy for this reason (a consequence of the law of conservation of energy), as some energy would be given off as gravitational waves, although this would be insignificantly small in all but the most extreme cases.\n\nOne case where gravitational waves would be strongest is during the final moments of the merger of two compact objects such as neutron stars or black holes. Over a span of millions of years, binary neutron stars, and binary black holes lose energy, largely through gravitational waves, and as a result, they spiral in towards each other. At the very end of this process, the two objects will reach extreme velocities, and in the final fraction of a second of their merger a substantial amount of their mass would theoretically be converted into gravitational energy, and travel outward as gravitational waves, allowing a greater than usual chance for detection. However, since little was known about the number of compact binaries in the universe and reaching that final stage can be very slow, there was little certainty as to how often such events might happen.\n\nGravitational waves can be detected indirectly ‚Äì by observing celestial phenomena caused by gravitational waves ‚Äì or more directly by means of instruments such as the Earth-based LIGO or the planned space-based LISA instrument.\n\nEvidence of gravitational waves was first deduced in 1974 through the motion of the double neutron star system PSR B1913+16, in which one of the stars is a pulsar that emits electro-magnetic pulses at radio frequencies at precise, regular intervals as it rotates. Russell Hulse and Joseph Taylor, who discovered the stars, also showed that over time, the frequency of pulses shortened, and that the stars were gradually spiralling towards each other with an energy loss that agreed closely with the predicted energy that would be radiated by gravitational waves. For this work, Hulse and Taylor were awarded the Nobel Prize in Physics in 1993. Further observations of this pulsar and others in multiple systems (such as the double pulsar system PSR J0737-3039) also agree with General Relativity to high precision.\n\nDirect observation of gravitational waves was not possible for the many decades after they were predicted due to the minuscule effect that would need to be detected and separated from the background of vibrations present everywhere on Earth. A technique called interferometry was suggested in the 1960s and eventually technology developed sufficiently for this technique to become feasible.\n\nIn the present approach used by LIGO, a laser beam is split and the two halves are recombined after travelling different paths. Changes to the length of the paths or the time taken for the two split beams, caused by the effect of passing gravitational waves, to reach the point where they recombine are revealed as \"beats\". Such a technique is extremely sensitive to tiny changes in the distance or time taken to traverse the two paths. In theory, an interferometer with arms about 4¬†km long would be capable of revealing the change of space-time ‚Äì a tiny fraction of the size of a single proton ‚Äì as a gravitational wave of sufficient strength passed through Earth from elsewhere. This effect would be perceptible only to other interferometers of a similar size, such as the Virgo, GEO 600 and planned KAGRA and INDIGO detectors. In practice at least two interferometers would be needed, because any gravitational wave would be detected at both of these but other kinds of disturbance would generally not be present at both, allowing the sought-after signal to be distinguished from noise. This project was eventually founded in 1992 as the Laser Interferometer Gravitational-Wave Observatory (LIGO). The original instruments were upgraded between 2010 and 2015 (to Advanced LIGO), giving an increase of around 10 times their original sensitivity.\n\nLIGO operates two gravitational-wave observatories in unison, located apart: the LIGO Livingston Observatory () in Livingston, Louisiana, and the LIGO Hanford Observatory, on the DOE Hanford Site () near Richland, Washington. The tiny shifts in the length of their arms are continually compared and significant patterns which appear to arise synchronously are followed up to determine whether a gravitational wave may have been detected or if some other cause was responsible.\n\nInitial LIGO operations between 2002 and 2010 did not detect any statistically significant events that could be confirmed as gravitational waves. This was followed by a multi-year shut-down while the detectors were replaced by much improved \"Advanced LIGO\" versions.¬†¬†In February 2015, the two advanced detectors were brought into engineering mode, in which the instruments are operating fully for the purpose of testing and confirming they are functioning correctly before being used for research, with formal science observations due to begin on 18 September 2015.\n\nThroughout the development and initial observations by LIGO, several \"blind injections\" of fake gravitational wave signals were introduced to test the ability of the researchers to identify such signals. To protect the efficacy of blind injections, only four LIGO scientists knew when such injections occurred, and that information was revealed only after a signal had been thoroughly analyzed by researchers. On 14 September 2015, while LIGO was running in engineering mode but without any blind data injections, the instrument reported a possible gravitational wave detection. The detected event was given the name GW150914.\n\nGW150914 was detected by the LIGO detectors in Hanford, Washington state, and Livingston, Louisiana, USA, at 09:50:45 UTC on 14 September 2015. The LIGO detectors were operating in \"engineering mode\", meaning that they were operating fully but had not yet begun a formal \"research\" phase (which was due to commence three days later on 18 September), so initially there was a question as to whether the signals had been real detections or simulated data for testing purposes before it was ascertained that they were not tests.\n\nThe chirp signal lasted over 0.2 seconds, and increased in frequency and amplitude in about 8 cycles from 35¬†Hz to 250¬†Hz. The signal is in the audible range and has been described as resembling the \"chirp\" of a bird; astrophysicists and other interested parties the world over excitedly responded by imitating the signal on social media upon the announcement of the discovery. (The frequency increases because each orbit is noticeably faster than the one before during the final moments before merging.)\n\nThe trigger that indicated a possible detection was reported within three minutes of acquisition of the signal, using rapid ('online') search methods that provide a quick, initial analysis of the data from the detectors. After the initial automatic alert at 09:54 UTC, a sequence of internal emails confirmed that no scheduled or unscheduled injections had been made, and that the data looked clean. After this, the rest of the collaborating team was quickly made aware of the tentative detection and its parameters.\n\nMore detailed statistical analysis of the signal, and of 16 days of surrounding data from 12 September to 20 October 2015, identified GW150914 as a real event, with an estimated significance of at least 5.1 sigma or a confidence level of 99.99994%. Corresponding wave peaks were seen at Livingston seven milliseconds before they arrived at Hanford. Gravitational waves propagate at the speed of light, and the disparity is consistent with the light travel time between the two sites. The waves had traveled at the speed of light for more than a billion years.\n\nAt the time of the event, the Virgo gravitational wave detector (near Pisa, Italy) was offline and undergoing an upgrade; had it been online it would likely have been sensitive enough to also detect the signal, which would have greatly improved the positioning of the event. GEO600 (near Hannover, Germany) was not sensitive enough to detect the signal. Consequently, neither of those detectors was able to confirm the signal measured by the LIGO detectors.\n\nThe event happened at a luminosity distance of megaparsecs (determined by the amplitude of the signal), or billion light years, corresponding to a cosmological redshift of (90% credible intervals). Analysis of the signal along with the inferred redshift suggested that it was produced by the merger of two black holes with masses of times and times the mass of the Sun (in the source frame), resulting in a post-merger black hole of solar masses. The mass‚Äìenergy of the missing solar masses was radiated away in the form of gravitational waves.\n\nDuring the final 20 milliseconds of the merger, the power of the radiated gravitational waves peaked at about or 526dBm ‚Äì 50 times greater than the combined power of all light radiated by all the stars in the observable universe.\n\nAcross the 0.2-second duration of the detectable signal, the relative tangential (orbiting) velocity of the black holes increased from 30% to 60% of the speed of light. The orbital frequency of 75¬†Hz (half the gravitational wave frequency) means that the objects were orbiting each other at a distance of only 350¬†km by the time they merged. The phase changes to the signal's polarization allowed calculation of the objects' orbital frequency, and taken together with the amplitude and pattern of the signal, allowed calculation of their masses and therefore their extreme final velocities and orbital separation (distance apart) when they merged. That information showed that the objects had to be black holes, as any other kind of known objects with these masses would have been physically larger and therefore merged before that point, or would not have reached such velocities in such a small orbit. The highest observed neutron star mass is two solar masses, with a conservative upper limit for the mass of a stable neutron star of three solar masses, so that a pair of neutron stars would not have had sufficient mass to account for the merger (unless exotic alternatives exist, for example, boson stars), while a black hole-neutron star pair would have merged sooner, resulting in a final orbital frequency that was not so high.\n\nThe decay of the waveform after it peaked was consistent with the damped oscillations of a black hole as it relaxed to a final merged configuration. Although the inspiral motion of compact binaries can be described well from post-Newtonian calculations, the strong gravitational field merger stage can only be solved in full generality by large-scale numerical relativity simulations.\n\nIn the improved model and analysis, the post-merger object is found to be a rotating Kerr black hole with a spin parameter of , i.e. one with 2/3 of the maximum possible angular momentum for its mass.\n\nThe two stars which formed the two black holes were likely formed about 2 billion years after the Big Bang with masses of between 40 and 100 times the mass of the Sun.\n\nGravitational wave instruments are whole-sky monitors with little ability to resolve signals spatially. A network of such instruments is needed to locate the source in the sky through triangulation. With only the two LIGO instruments in observational mode, GW150914's source location could only be confined to an arc on the sky. This was done via analysis of the ms time-delay, along with amplitude and phase consistency across both detectors. This analysis produced a credible region of 150 deg with a probability of 50% or 610 deg with a probability of 90% located mainly in the Southern Celestial Hemisphere, in the rough direction of (but much farther than) the Magellanic Clouds.\n\nFor comparison, the area of the constellation Orion is 594 deg.\n\nThe Fermi Gamma-ray Space Telescope reported that its Gamma-Ray Burst Monitor (GBM) instrument detected a weak gamma-ray burst above 50 keV, starting 0.4 seconds after the LIGO event and with a positional uncertainty region overlapping that of the LIGO observation. The Fermi team calculated the odds of such an event being the result of a coincidence or noise at 0.22%. However a gamma ray burst would not have been expected, and observations from the INTEGRAL telescope's all-sky SPI-ACS instrument indicated that any energy emission in gamma-rays and hard X-rays from the event was less than one millionth of the energy emitted as gravitational waves, which \"excludes the possibility that the event is associated with substantial gamma-ray radiation, directed towards the observer.\" If the signal observed by the Fermi GBM was genuinely astrophysical, INTEGRAL would have indicated a clear detection at a significance of 15 sigma above background radiation. The AGILE space telescope also did not detect a gamma-ray counterpart of the event.\n\nA follow-up analysis by an independent group, released in June 2016, developed a different statistical approach to estimate the spectrum of the gamma-ray transient. It concluded that Fermi GBM's data did not show evidence of a gamma ray burst, and was either background radiation or an Earth albedo transient on a 1-second timescale. A rebuttal of this follow-up analysis, however, pointed out that the independent group misrepresented the analysis of the original Fermi GBM Team paper and therefore misconstrued the results of the original analysis. The rebuttal reaffirmed that the false coincidence probability is calculated empirically and is not refuted by the independent analysis.\n\nBlack hole mergers of the type thought to have produced the gravitational wave event are not expected to produce gamma-ray bursts, as stellar-mass black hole binaries are not expected to have large amounts of orbiting matter. Avi Loeb has theorised that if a massive star is rapidly rotating, the centrifugal force produced during its collapse will lead to the formation of a rotating bar that breaks into two dense clumps of matter with a dumbbell configuration that becomes a black hole binary, and at the end of the star's collapse it triggers a gamma-ray burst. Loeb suggests that the 0.4 second delay is the time it took the gamma-ray burst to cross the star, relative to the gravitational waves.\n\nThe reconstructed source area was targeted by follow-up observations covering radio, optical, near infra-red, X-ray, and gamma-ray wavelengths along with searches for coincident neutrinos. However, because LIGO had not yet started its science run, notice to other telescopes was delayed.\n\nThe ANTARES telescope detected no neutrino candidates within ¬±500 seconds of GW150914. The IceCube Neutrino Observatory detected three neutrino candidates within ¬±500 seconds of GW150914. One event was found in the southern sky and two in the northern sky. This was consistent with the expectation of background detection levels. None of the candidates were compatible with the 90% confidence area of the merger event. Although no neutrinos were detected, the lack of such observations provided a limit on neutrino emission from this type of gravitational wave event.\n\nObservations by the Swift Gamma-Ray Burst Mission of nearby galaxies in the region of the detection, two days after the event, did not detect any new X-ray, optical or ultraviolet sources.\n\nThe announcement of the detection was made on 11 February 2016 at a news conference in Washington, D.C. by David Reitze, the executive director of LIGO, with a panel comprising Gabriela Gonz√°lez, Rainer Weiss and Kip Thorne, of LIGO, and France A. C√≥rdova, the director of NSF. Barry Barish delivered the first presentation on this discovery to a scientific audience simultaneously with the public announcement.\n\nThe initial announcement paper was published during the news conference in \"Physical Review Letters\", with further papers either published shortly afterwards or immediately available in preprint form.\n\nIn May 2016, the full collaboration, and in particular Ronald Drever, Kip Thorne, and Rainer Weiss, received the Special Breakthrough Prize in Fundamental Physics for the observation of gravitational waves. Drever, Thorne, Weiss, and the LIGO discovery team also received the Gruber Prize in Cosmology. Drever, Thorne, and Weiss were also awarded the 2016 Shaw Prize in Astronomy and the 2016 Kavli Prize in Astrophysics. Barish was awarded the 2016 Enrico Fermi Prize from the Italian Physical Society (Societ√† Italiana di Fisica). In January 2017, LIGO spokesperson Gabriela Gonz√°lez and the LIGO team were awarded the 2017 Bruno Rossi Prize.\n\nThe 2017 Nobel Prize in Physics was awarded to Rainer Weiss, Barry Barish and Kip Thorne \"for decisive contributions to the LIGO detector and the observation of gravitational waves\".\n\nThe observation was heralded as inaugurating a revolutionary era of gravitational-wave astronomy. Prior to this detection, astrophysicists and cosmologists were able to make observations based upon electromagnetic radiation (including visible light, X-rays, microwave, radio waves, gamma rays) and particle-like entities (cosmic rays, stellar winds, neutrinos, and so on). These have significant limitations - light and other radiation may not be emitted by many kinds of objects, and can also be obscured or hidden behind other objects. Objects such as galaxies and nebulae can also absorb, re-emit, or modify light generated within or behind them, and compact stars or exotic stars may contain material which is dark and radio silent, and as a result there is little evidence of their presence other than through their gravitational interactions.\n\nOn 15 June 2016, the LIGO group announced an observation of another gravitational wave signal, named GW151226. The Advanced LIGO is predicted to detect five more black hole mergers like GW150914 in its next observing campaign, and then 40 binary star mergers each year, in addition to an unknown number of more exotic gravitational wave sources, some of which may not be anticipated by current theory.\n\nPlanned upgrades are expected to double the signal-to-noise ratio, expanding the volume of space in which events like GW150914 can be detected by a factor of ten. Additionally, Advanced Virgo, KAGRA, and a possible third LIGO detector in India will extend the network and significantly improve the position reconstruction and parameter estimation of sources.\n\nLaser Interferometer Space Antenna (LISA) is a proposed space based observation mission to detect gravitational waves. With the proposed sensitivity range of LISA, merging binaries like GW150914 would be detectable about 1000 years before they merge, providing for a class of previously unknown sources for this observatory if they exist within about 10 megaparsecs. LISA Pathfinder, LISA's technology development mission, was launched in December 2015 and it demonstrated that the LISA mission is feasible.\n\nA current model predicts LIGO will detect approximately 1000 black hole mergers per year after it reaches full sensitivity planned for 2020.\n\nThe masses of the two pre-merger black holes provide information about stellar evolution. Both black holes were more massive than previously discovered stellar-mass black holes, which were inferred from X-ray binary observations. This implies that the stellar winds from their progenitor stars must have been relatively weak, and therefore that the metallicity (mass fraction of chemical elements heavier than hydrogen and helium) must have been less than about half the solar value.\n\nThe fact that the pre-merger black holes were present in a binary star system, as well as the fact that the system was compact enough to merge within the age of the universe, constrains either binary star evolution or dynamical formation scenarios, depending on how the black hole binary was formed. A significant number of black holes must receive low natal kicks (the velocity a black hole gains at its formation in a core-collapse supernova event), otherwise the black hole forming in a binary star system would be ejected and an event like GW would be prevented. The survival of such binaries, through common envelope phases of high rotation in massive progenitor stars, may be necessary for their survival. The majority of the latest black hole model predictions comply with these added constraints.\n\nThe discovery of the GW merger event increases the lower limit on the rate of such events, and rules out certain theoretical models that predicted very low rates of less than 1¬†Gpcyr (one event per cubic gigaparsec per year). Analysis resulted in lowering the previous upper limit rate on events like GW150914 from ~140¬†Gpcyr to ¬†Gpcyr.\n\nMeasurement of the waveform and amplitude of the gravitational waves from a black hole merger event makes accurate determination of its distance possible. The accumulation of black hole merger data from cosmologically distant events may help to create more precise models of the history of the expansion of the universe and the nature of the dark energy that influences it.\n\nThe earliest universe is opaque since the cosmos was so energetic then that most matter was ionized and photons were scattered by free electrons. However, this opacity would not affect gravitational waves from that time, so if they occurred at levels strong enough to be detected at this distance, it would allow a window to observe the cosmos beyond the current visible universe. Gravitational-wave astronomy therefore may some day allow direct observation of the earliest history of the universe.\n\nThe inferred fundamental properties, mass and spin, of the post-merger black hole were consistent with those of the two pre-merger black holes, following the predictions of general relativity. This is the first test of general relativity in the very . No evidence could be established against the predictions of general relativity.\n\nThe opportunity was limited in this signal to investigate the more complex general relativity interactions, such as tails produced by interactions between the gravitational wave and curved space-time background. Although a moderately strong signal, it is much smaller than that produced by binary-pulsar systems. In the future stronger signals, in conjunction with more sensitive detectors, could be used to explore the intricate interactions of gravitational waves as well as to improve the constraints on deviations from general relativity.\n\nThe speed of gravitational waves (v) is predicted by general relativity to be the speed of light (c). The extent of any deviation from this relationship can be parameterized in terms of the mass of the hypothetical graviton. The graviton is the name given to an elementary particle that would act as the force carrier for gravity, in quantum theories about gravity. It is expected to be massless if, as it appears, gravitation has an infinite range. (This is because the more massive a gauge boson is, the shorter is the range of the associated force; as with the infinite range of electromagnetism, which is due to the massless photon, the infinite range of gravity implies that any associated force-carrying particle would also be massless.) If the graviton were not massless, gravitational waves would propagate below lightspeed, with lower frequencies (∆í) being slower than higher frequencies, leading to dispersion of the waves from the merger event. No such dispersion was observed. The observations of the inspiral slightly improve (lower) the upper limit on the mass of the graviton from Solar System observations to , corresponding to or a Compton wavelength (Œª) of greater than km, roughly 1 light-year. Using the lowest observed frequency of 35¬†Hz, this translates to a lower limit on v such that the upper limit on 1-\"v\" /\"c\" is ~¬†.\n\n\n"}
{"id": "12751687", "url": "https://en.wikipedia.org/wiki?curid=12751687", "title": "God Created the Integers", "text": "God Created the Integers\n\nGod Created the Integers: The Mathematical Breakthroughs That Changed History is an anthology, edited by Stephen Hawking, of \"excerpts from thirty-one of the most important works in the history of mathematics.\" \n\nThe title of the book is a reference to a quotation attributed to mathematician Leopold Kronecker, who once wrote that \"God made the integers; all else is the work of man.\" \n\nThe works are grouped by author and ordered chronologically. Each section is prefaced by notes on the mathematician's life and work. The anthology includes works by the following mathematicians:\n\nSelections from the works of Euler, Bolyai, Lobachevsky and Galois, which are included in the second edition of the book (published in 2007), were not included in the first edition.\n"}
{"id": "13811284", "url": "https://en.wikipedia.org/wiki?curid=13811284", "title": "Green to Gold (book)", "text": "Green to Gold (book)\n\nGreen to Gold: How Smart Companies Use Environmental Strategy to Innovate, Create Value, and Build Competitive Advantage is a 2006 book on sustainability by Daniel C. Esty and Andrew S. Winston and published by Yale University Press.\n\nThe book argues that pollution control and natural resource management have become critical elements of marketplace success and explains how leading-edge companies have folded environmental thinking into their core business strategies.\n"}
{"id": "29427482", "url": "https://en.wikipedia.org/wiki?curid=29427482", "title": "Gunter Faure", "text": "Gunter Faure\n\nGunter Faure is a leading geochemist who currently holds the position of Professor Emeritus in the School of Earth Science of Ohio State University. He obtained his PhD from the Massachusetts Institute of Technology in 1961.\n\n"}
{"id": "28121639", "url": "https://en.wikipedia.org/wiki?curid=28121639", "title": "Handbook of Australian Soils", "text": "Handbook of Australian Soils\n\nThe Handbook of Australian Soils is a soil classification system developed for Australian soils. The first edition was published in 1968 and is based on the great soil group classification system published by J. A. Prescott in 1931. It has since been superseded by the Australian Soil Classification.\n"}
{"id": "26692522", "url": "https://en.wikipedia.org/wiki?curid=26692522", "title": "History of metamaterials", "text": "History of metamaterials\n\nThe history of metamaterials begins with artificial dielectrics in microwave engineering as it developed just after World War II. Yet, there are seminal explorations of artificial materials for manipulating electromagnetic waves at the end of the 19th century.\nHence, the history of metamaterials is essentially a history of developing certain types of manufactured materials, which interact at radio frequency, microwave, and later optical frequencies.\n\nAs the science of materials has advanced, photonic materials have been developed which use the photon of light as the fundamental carrier of information. This has led to photonic crystals, and at the beginning of the new millennium, the proof of principle for functioning metamaterials with a negative index of refraction in the microwave range at 10.5 Gigahertz. This was followed by the first proof of principle for metamaterial cloaking (shielding an object from view), also in the microwave range, about six years later. However, a cloak that can conceal objects across the entire electromagnetic spectrum is still decades away. Many physics and engineering problems need to be solved.\n\nNevertheless, negative refractive materials have led to the development of metamaterial antennas and metamaterial microwave lenses for miniature wireless system antennas which are more efficient than their conventional counterparts. Also, metamaterial antennas are now commercially available. Meanwhile, subwavelength focusing with the superlens is also a part of present-day metamaterials research.\n\nClassical waves transfer energy without transporting matter through the medium (material). For example, waves in a pond do not carry the water molecules from place to place; rather the wave's energy travels through the water, leaving the water molecules in place. Additionally, charged particles, such as electrons and protons create electromagnetic fields when they move, and these fields transport the type of energy known as electromagnetic radiation, or light. A changing magnetic field will induce a changing electric field and vice versa‚Äîthe two are linked. These changing fields form electromagnetic waves. Electromagnetic waves differ from mechanical waves in that they do not require a medium to propagate. This means that electromagnetic waves can travel not only through air and solid materials, but also through the vacuum of space.\n\nThe \"history of metamaterials\" can have a variety starting points depending on the properties of interest. Related early wave studies started in 1904 and progressed through more than half of the first part of the twentieth century. This early research included the relationship of the phase velocity to group velocity and the relationship of the wave vector and Pointing vector.\n\nIn 1904 the possibility of negative phase velocity accompanied by an anti-parallel group velocity were noted by Horace Lamb (book: \"Hydrodynamics\") and Arthur Schuster (Book: \"Intro to Optics\"). However both thought practical achievement of these phenomena were not possible. In 1945 Leonid Mandelstam (also \"Mandel'shtam\") studied the anti-parallel phase and group advance in more detail. He is also noted for examining the electromagnetic characteristics of materials demonstrating negative refraction, as well as the first left-handed material concept. These studies included negative group velocity. He reported that such phenomena occurs in a crystal lattice. This may be considered significant because the metamaterial is a man made crystal lattice (structure). In 1905 H.C. Pocklington also studied certain effects related to negative group velocity.\n\nV.E. Pafomov (1959), and several years later, the research team V.M. Agranovich and V.L. Ginzburg (1966) reported the repercussions of negative permittivity, negative permeability, and negative group velocity in their study of crystals and excitons.\n\nV.G. Veselago's 1967 paper is considered the theoretical work that began metamaterial research. However, physical experimentation did not occur until 33 years after the paper's publication due to lack of available materials and lack of sufficient computing power. It was not until the 1990s that materials and computing power became available to artificially produce the necessary structures. Veselago also predicted a number of electromagnetic phenomena that would be reversed including the refractive index. In addition, he is credited with coining the term \"left handed material\" for the present day metamaterial because of the anti-parallel behavior of the wave vector and other electromagnetic fields. Moreover, he noted that the material he was studying was a double negative material, as certain metamaterials are named today, because of the ability to simultaneously produce negative values for two important parameters, e.g. permitivity and permeability. In 1968, his paper was translated and published in English.\n\nLater still, developments in nanofabrication and subwavelength imaging techniques are now taking this work into optical wavelengths.\n\nIn the 19th century Maxwell's equations united all previous observations, experiments, and established propositions pertaining to electricity and magnetism into a consistent theory, which is also fundamental to optics. Maxwell's work demonstrated that electricity, magnetism and even light are all manifestations of the same phenomenon, namely the electromagnetic field.\n\nLikewise, the concept of using certain constructed materials as a method for manipulating electromagnetic waves dates back to the 19th century. Microwave theory had developed significantly during the latter part of the 19th century with the cylindrical parabolic reflector, dielectric lens, microwave absorbers, the cavity radiator, the radiating iris, and the pyramidal electromagnetic horn.\nThe science involving microwaves also included round, square, and rectangular waveguides precluding Sir Rayleigh's published work on waveguide operation in 1896. Microwave optics, involving the focusing of microwaves, introduced quasi-optical components, and a treatment of microwave optics was published in 1897 (by Righi).\n\nJagadish Chandra Bose was a scientist involved in original microwave research during the 1890s. As officiating professor of physics at Presidency College he involved himself with laboratory experiments and studies involving refraction, diffraction and polarization, as well as transmitters, receivers and various microwave components.\n\nHe connected receivers to a sensitive galvanometer, and developed crystals to be used as a receiver. The crystals operated in the shortwave radio range. Crystals were also developed to detect both white and ultraviolet light. These crystals were patented in 1904 for their capability to detect electromagnetic radiation. Furthermore, it appears that his work also anticipated the existence of p-type and n-type semiconductors by 60 years.\n\nFor the general public in 1895, Bose was able to remotely ring a bell and explode gunpowder with the use of electromagnetic waves. In 1896, it was reported that Bose had transmitted electromagnetic signals over almost a mile. In 1897, Bose reported on his microwave research (experiments) at the Royal Institution in London. There he demonstrated his apparatus at wavelengths that ranged from 2.5 centimeters to 5 millimeters.\n\nIn 1898, Jagadish Chandra Bose conducted the first microwave experiment on twisted structures. These twisted structures match the geometries that are known as artificial chiral media in today's terminology. By this time, he had also researched double refraction (birefringence) in crystals. Other research included polarization of electric field \"waves\" that crystals produce. He discovered this type of polarization in other materials including a class of dielectrics.\n\nIn addition, chirality as optical activity in a given material is a phenomenon that has been studied since the 19th century. By 1811, a study of quartz crystals revealed that such crystaline solids rotate the polarization of polarized light denoting optical activity. By 1815, materials other than crystals, such as oil of turpentine were known to exhibit chirality. However, the basic cause was not known. Louis Pasteur solved the problem (chirality of the molecules) originating a new discipline known as stereochemistry. At the macroscopic scale, Lindman applied microwaves to the problem with wire spirals (wire helices) in 1920 and 1922.\n\nKarl F. Lindman, from 1914 and into the 1920s, studied artificial chiral media formed by a collection of randomly oriented small spirals. He was written about by present-day metamaterials scientists: Ismo V. Lindell, Ari H. Sihvola, and Juhani Kurkijarvi.\n\nMuch of the historic research related to metamaterials is weighted from the view of antenna beam shaping within microwave engineering just after World War II. Furthermore, metamaterials appear to be historically linked to the body of research pertaining to artificial dielectrics throughout the late 1940s, the 1950s and the 1960s. The most common use for artificial dielectrics throughout prior decades has been in the microwave regime for antenna beam shaping. The artificial dielectrics had been proposed as a low cost and lightweight \"tool\". Research on artificial dielectrics, other than metamaterials, is still ongoing for pertinent parts of the electromagnetic spectrum.\n\nPioneering works in microwave engineering on artificial dielectrics in microwave were produced by Winston E. Kock, Seymour Cohn, John Brown, and Walter Rotman. \"Periodic\" artificial structures were proposed by Kock, Rotman, and Sergei Schelkunoff. There is also an extensive reference list that is focused on the properties of artificial dielectrics in the 1990 book, \" Field theory of guided waves\" by R.E. Collin.\n\nSchelkunoff achieved notice for contributions to antenna theory and electromagnetic wave propagation.\n\"Magnetic particles made of capacitively loaded loops were also suggested by Sergei Schelkunoff in 1952 (who was a senior colleague of Winston Kock at\nBell Labs at the time). However, Schelkunoff suggested these particles as a means of synthesizing high permeability (and not negative) values\nbut he recognized that such high permeability artificial dielectrics would be quite dispersive.\"\n\nW.E. Kock proposed metallic and wire lenses for antennas. Some of these are the metallic delay lens, parallel-wire lens, and the wire mesh lens. In addition, he conducted analytical studies regarding the response of customized metallic particles to a quasistatic electromagnetic radiation. As with the current large group of researchers conveying the behavior of metamaterials, Kock noted behaviors and structure in artificial materials that are similar to metamaterials.\n\nHe employed particles, which would be of varying geometric shape; spheres, discs, ellipsoids and prolate or oblate spheroids, and would be either isolated or set in a repeating pattern as part of an array configuration. Furthermore, he was able to determine that such particles behave as a dielectric medium. He also noticed that the permittivity \"Œµ\" and permeability \" Œº\" of these particles can be purposely tuned, but not independently.\n\nWith metamaterials, however, local values for both Œµ and Œº are designed as part of the fabrication process, or analytically designed in theoretical studies. Because of this process, individual metamaterial inclusions can be independently tuned.\n\nWith artificial dielectrics Kock was able to see that any value for permittivity and permeability, arbitrarily large or small, can be achieved, and that this included the possibility of negative values for these parameters. The optical properties of the medium depended solely on the particles‚Äô geometrical shape and spacing, rather than on their own intrinsic behavior. His work also anticipated the split-ring resonator, a fabricated periodic structure that is a common workhorse for metamaterials.\n\nKock, however, did not investigate the simultaneous occurrence of negative values of Œµ and Œº, which has become one of the first achievements defining modern metamaterials. This was because research in artificial materials was oriented toward other goals, such as creating plasma media at RF or microwave frequencies related to the overarching needs of NASA and the space program at that time.\n\nWalter Rotman and R.F. Turner advanced microwave beam shaping systems with a lens that has three perfect focal points; two symmetrically located off-axis and one on-axis. They published the design equations for the improved straight-front-face lens, the evaluation of its phase control capabilities, scanning capabilities, and the demonstrated fabrication techniques applicable to this type of design.\nRotman invented other periodic structures that include many types of surface wave antennas: the trough waveguide, the channel waveguide, and the sandwich wire antenna.\n\n\"At frequencies of a few hundred gigahertz and lower, electrons are the principle particles which serve as the workhorse of devices. On the other hand,\nat infrared through optical to ultraviolet wavelengths, the photon is the fundamental particle of choice.\"\nThe word 'photonics' appeared in the late 1960s to describe a research field whose goal was to use light to perform functions that traditionally fell within the typical domain of electronics, such as telecommunications, information processing, among other processes. The term \"photonics\" more specifically connotes:\n\nHence, as photonic materials are used, the photons, rather than electrons, become the fundamental carriers of information. Furthermore, the photon appears to be a more effiecient carrier of information, and materials that can process photonic signals are both in use and in further development. Additionally, developing photonic materials will lead to further miniaturization of components.\n\nIn 1987 Eli Yablonovitch proposed controlling spontaneous emissions and constructing physical zones in periodic dielectrics that forbid certain wavelengths of electromagnetic radiation. These capabilities would be built into three-dimensional periodic dielectric structures (artificial dielectric). He noted that controlling spontaneous emission is desirable for semiconductor processes.\n\nHistorically, and conventionally, the function or behavior of materials can be altered through their chemistry. This has long been known. For example, adding lead changes the color or hardness of glass. However, at the end of the 20th century this description was expanded by John Pendry, a physicist from Imperial College in London. In the 1990s he was consulting for a British company, \"Marconi Materials Technology\", as a condensed matter physics expert. The company manufactured a stealth technology made of a radiation-absorbing carbon that was for naval vessels. However, the company did not understand the physics of the material. The company asked Pendry if he could understand how the material worked.\n\nPendry discovered that the radiation absorption property did not come from the molecular or chemical structure of the material, i.e. the carbon per se. This property came from the long and thin, physical shape of the carbon fibers. He realized rather than conventionally altering a material through its chemistry, as lead does with glass, the behavior of a material can be altered by changing a material‚Äôs internal structure on a very fine scale. The very fine scale was less than the wavelength of the electromagnetic radiation that is applied. The theory applies across the electromagnetic spectrum that is in use by today's technologies. The radiations of interest are from radio waves, and microwaves, through infrared to the visible wavelengths. Scientists view this material as \"beyond\" conventional materials. Hence, the Greek word \"meta\" was attached, and these are called metamaterials.\n\nAfter successfully deducing and realizing the carbon fiber structure, Pendry further proposed that he try to change the magnetic properties of a non-magnetic material, also by altering its physical structure. The material would not be intrinsically magnetic, nor inherently susceptible to being magnetized. Copper wire is such a non-magnetic material. He envisioned fabricating a non-magnetic composite material, which could mimic the movements of electrons orbiting atoms. However, the structures are fabricated on a scale that is magnitudes larger than the atom, yet smaller than the radiated wavelength.\n\nHe envisioned and hypothesized miniature loops of copper wire set in a fiberglass substrate could mimic the action of electrons but on a larger scale. Furthermore, this composite material could act like a slab of iron. In addition, he deduced that a current run through the loops of wire results in a magnetic response.\n\nThis metamaterial idea resulted in variations. Cutting the loops results in a magnetic resonator, which acts like a switch. The switch, in turn, would allow Pendry to determine or alter the magnetic properties of the material simply by choice. At the time, Pendry didn't realize the significance of the two materials he had engineered. By combining the electrical properties of Marconi‚Äôs radar-absorbing material with his new man-made magnetic material he had unwittingly placed in his hands a new way to manipulate electromagnetic radiation. In 1999, Pendry published his new conception of artificially produced magnetic materials in a notable physics journal. This was read by scientists all over the world, and it \"stoked their imagination\".\n\nIn 1967, Victor Veselago produced an often cited, seminal work on a theoretical material that could produce extraordinary effects that are difficult or impossible to produce in nature. At that time he proposed that a reversal of Snell's law, an extraordinary lens, and other exceptional phenomena can occur within the laws of physics. This theory lay dormant for a few decades. There were no materials available in nature, or otherwise, that could physically realize Veselago's analysis. Not until thirty-three years later did the properties of this material, a metamaterial, became a subdiscipline of physics and engineering.\n\nHowever, there were certain observations, demonstrations, and implementations that closely preceded this work. Permittivity of metals, with values that could be stretched from the positive to the negative domain, had been studied extensively. In other words, negative permittivity was a known phenomenon by the time the first metamaterial was produced. Contemporaries of Kock were involved in this type of research. The concentrated effort was led by the US government for researching interactions between the ionosphere and the re-entry of NASA space vehicles.\n\nIn the 1990s, Pendry et al. developed sequentially repeating thin wire structures, analogous to crystal structures. These extended the range of material permittivity. However, a more revolutionary structure developed by Pendry et al. was a structure that could control magnetic interactions (permeability) of the radiated light, albeit only at microwave frequencies. This sequentially repeating, split ring structure, extended material magnetic parameters into the negative. This lattice or periodic, \"magnetic\" structure was constructed from non-magnetic components.\n\nHence, in electromagnetic domain, a negative value for permittivity and permeability occurring simultaneously was a requirement to produce the first metamaterials. These were beginning steps for proof of principle regarding Veselago's original 1967 proposal.\n\nIn 2000, a team of UCSD researchers produced and demonstrated metamaterials, which exhibited unusual physical properties that were never before produced in nature. These materials obey the laws of physics, but behave differently from normal materials. In essence these \"negative index metamaterials\" were noted for having the ability to reverse many of the physical properties that govern the behavior of ordinary optical materials. One of those unusual properties is the capability to reverse, for the first time, the Snell's law of refraction. Until this May 2000 demonstration by the UCSD team, the material was unavailable. Advances during the 1990s in fabrication and computation capabilities allowed these first metamaterials to be constructed. Thus, testing the \"new\" metamaterial began for the effects described by Victor Veselago 30 years earlier, but only at first in the microwave frequency domain. Reversal of group velocity was explicitly announced in the related published paper.\nThe super lens or superlens is a practical structure based on John Pendry's work describing a perfect lens that can focus all four fourier components. Pendry's paper described a theoretical novel lens that could capture images below the diffraction limit by employing the negative refractive index behavior. The super lens is a practical realization of this theory. It is a working lens that can capture images below the diffraction limit while realizing the inefficiencies of real materials. This means that although there are losses, enough of an image is returned that makes this research useful and successful.\n\nUlf Leonhardt was born in East Germany, and presently occupies the theoretical physics chair at the University of St. Andrews in Scotland, and is considered one the leaders in the science of creating an invisibility cloak. Around 1999, Leonhardt began work on how to build a cloaking device with a few other colleagues. Leonhardt stated that at the time invisibility was not considered fashionable. He then wrote a theoretical study entitled \"Optical Conformal Mapping\". The first sentence sums up the objective: \"An invisibility device should guide light around an object as if nothing were there.\"\n\nIn 2005, he sent the paper to three notable scientific journals, Nature, Nature Physics, and Science. Each journal, in turn, rejected the paper. In 2006, Physical Review Letters rejected the paper for publication, as well. However, according to the PRL assessment, one of the anonymous reviewers noted that (he or she ) had been to two meetings in the previous months with John Pendry's group, who were also working on a cloaking device. From the meetings, the reviewer also became aware of a patent that Pendry and his colleagues were supposed to file. Leonhardt was at the time unaware of the Pendry group's work. Because of the Pendry meetings Leonhardt's work was not really considered new physics by the reviewer and, therefore, did not merit publication in Physical Review Letters.\n\nLater in 2006, \"Science\" (the journal) reversed its decision and contacted Leonhardt to publish his paper because it had just received a theoretical study from Pendry‚Äôs team entitled \"Controlling Electromagnetic Fields\". \"Science\" considered both papers strikingly similar and published them both in the same issue of Science Express on May 25, 2006. The published papers touched off research efforts by a dozen groups to build cloaking devices at locations around the globe, which would test out the mathematics of both papers.\n\nOnly months after the submission of notable invisibility cloak theories, a practical device was built and demonstrated by David Schurig and David Smith, engineering researchers of Duke University (October 2006). It was limited to the microwave range so the object was not invisible to the human eye. However, it demonstrated proof of principle.\n\nThe original theoretical papers on cloaking opened a new science discipline called transformation optics.\n\n\n\n\n"}
{"id": "53999565", "url": "https://en.wikipedia.org/wiki?curid=53999565", "title": "Holotomography", "text": "Holotomography\n\nHolotomography (HT) is a laser technique to measure three-dimensional refractive index (RI) tomogram of a microscopic sample such as biological cells and tissues. Because the RI can serve as an intrinsic imaging contrast for transparent or phase objects, measurements of RI tomograms can provide label-free quantitative imaging of microscopic phase objects. In order to measure 3-D RI tomogram of samples, HT employs the principle of holographic imaging and inverse scattering. Typically, multiple 2D holographic images of a sample are measured at various illumination angles, employing the principle of interferometric imaging. Then, a 3D RI tomogram of the sample is reconstructed from these multiple 2D holographic images by inversely solving light scattering in the sample.\n\nThe principle of HT is very similar to X-ray computed tomography (CT) or CT scan. CT scan measures multiple 2-D X-ray images of a human body at various illumination angles, and a 3-D tomogram (X-ray absorptivity) is then retrieved via the inverse scattering theory. Both the X-ray CT and laser HT shares the same governing equation ‚Äì Helmholtz equation, the wave equation for a monochromatic wavelength. HT is also known as optical diffraction tomography.\n\nThe applications of HT includes \nHT provides 3D dynamic images of live cells and thin tissues without using exogenous labeling agents such as fluorescence proteins or dyes. HT enables quantitative live cell imaging, and also provides quantitative information such as cell volume, surface area, protein concentration. HT provides following advantages over conventional 3D microscopic techniques. \nHowever, 3D RI tomography does not provide molecular specificity. Generally, the measured RI information cannot be directly related to information about molecules or proteins, except for notable cases such as gold nanoparties or lipid droplets that exhibit distinctly high RI values compared to cell cytoplams.\n\nHT provide various quantitative imaging capability, providing morphological, biochemical, and mechanical properties of individuals cells. 3D RI tomography directly provides morphological properties including volume, surface area, and sphericity (roundness) of a cell. Local RI value can be translated into biochemical information or cytoplasmic protein concentration, because the RI of a solution is linearly proportional to its concentration. In particular, for the case of red blood cells, RI value can be converted into hemoglobin concentration. Measurements of dynamic cell membrane fluctuatino, which can also be obtained with a HT instrument, provides information about cellular deformability. Furthermore, these various quantitative parameters can be obtained at the single cell level, allowing correlative analysis between various cellular parameters. HT has been utilized for the study of red blood cells, white blood cells, malaria infection, bebasia infection, blood storage, and diabetes.\n\nThe first theoretical proposal was presented by Emil Wolf, and the first experimental demonstration was shown by Fercher et al. From 2000s, HT techniques had been extensively studied and applied to the field of biology and medicine, by several research groups including MIT spectroscopy laboratory (PI: the late Michael S. Feld). Both the technical developments and applications of HT have been significantly advanced. During the mid 2010s, first commercial HT companies Nanolive and Tomocube were founded.\n\n"}
{"id": "38448488", "url": "https://en.wikipedia.org/wiki?curid=38448488", "title": "Index of physics articles (W)", "text": "Index of physics articles (W)\n\nThe index of physics articles is split into multiple pages due to its size.\n\nTo navigate by individual letter use the table of contents below.\n\n"}
{"id": "42621590", "url": "https://en.wikipedia.org/wiki?curid=42621590", "title": "International Society for Extracellular Vesicles", "text": "International Society for Extracellular Vesicles\n\nThe International Society for Extracellular Vesicles (ISEV) is an international scientific organization that focuses on the study of extracellular vesicles (EVs), including exosomes, microvesicles, oncosomes, and other membrane-bound particles that are released from cells. Established in 2011, the society is a non-profit entity and a 501(c)(3) corporation in the United States. ISEV is governed by an executive committee. The current president of ISEV (2016-2018) is Andrew Hill; the founding president (2011-2016) is Jan L√∂tvall. The society publishes the \"Journal of Extracellular Vesicles\", the only journal devoted to the topic.\n\nAs part of its mission to promote scientific research and education, ISEV hosts an annual meeting and educational event as well as numerous workshops on defined EV-related topics. ISEV also co-sponsors or endorses related events.\n\n\n\n\nTo disseminate research in the field, in 2012, ISEV established the peer-reviewed and open access \"Journal of Extracellular Vesicles\", (J Extracell Vesicles or JEV). JEV is included in PubMed and Scopus. The editors-in-chief are Clotilde Th√©ry (Institut Curie), Peter Quesenberry (Brown University) and Yong Song Gho (Pohang University of Science and Technology), and the founding editor was Jan L√∂tvall (University of Gothenburg). In addition to research and review articles, the journal periodically publishes position papers of the society that are meant to advance standardization efforts.\n\nIn 2014, the Executive Committee of ISEV published a set of minimal information guidelines for publication of EV studies. These requirements, known as \"MISEV,\" mirror similar initiatives in the fields of microarray (MIAME) and proteomics (MIAPE) analysis, among others. In 2017, the ISEV board announced intent to update these requirements with community involvement.\n\nAmong the educational initiatives of ISEV is a massive open online course (MOOC) on the Basics of Extracellular Vesicles, launched in 2016 and available through Coursera. Coordinated by Cecilia L√§sser, the MOOC is co-sponsored by the University of Gothenburg, Pohang University of Science and Technology, and the University of California, Irvine.\n\n"}
{"id": "49183925", "url": "https://en.wikipedia.org/wiki?curid=49183925", "title": "Intracapsular fracture", "text": "Intracapsular fracture\n\nAn intracapsular fracture is a bone fracture located within the joint capsule.\n\nExamples of intracapsular fractures includes:\n"}
{"id": "28111321", "url": "https://en.wikipedia.org/wiki?curid=28111321", "title": "Irish Planning Institute", "text": "Irish Planning Institute\n\nThe Irish Planning Institute (IPI) is an all-island professional body representing professional planners engaged in physical and environmental planning in the Republic of Ireland and Northern Ireland. \n\nThe IPI works with both its members and other built environment professionals to promote and improve the quality of planning, to represent the views of the planning profession and to contribute to education and environmental awareness in the wider community.\n\nThis is achieved through the hosting of conferences and CPD Events for its members; awarding, recognising and publishing best practice; making submissions on behalf of the planning profession on national policies, governance and other relevant publications and papers; and by representing the values of planning and planning professionals in the general media.\n\nThe Irish Planning Institute is an active member of the European Council of Spatial Planners and has held the Presidency of this organisation. Through this involvement, the Institute not only represents Irish planning interests abroad but also forges important links with sister institutions in Europe. It is also a member of the Global Planners Network.\n\nThe IPI inspects and accredits on invitation a number of planning courses in universities and other third level educational institutions across the island of Ireland. The Institute is also a nominating body to Seanad √âireann and to An Bord Plean√°la.\n\nThe Irish Planning Institute lobbies central Government on new legislation and on planning policy at national, regional and local levels. It also from time to time issues statements on current topics of public interest and debate.\n\nThe IPI publishes \"Plean√°il\" which is the only technical publication on planning theory and professional planning practice published in Ireland. \n\nThrough its annual National Planning Conference, the Institute offers a major forum for the debate of planning and related topics. It also organises the National Planning Awards every two years and a series of CPD events throughout the year to promote best practice and to inform our membership regarding new approaches and policies.\n\nThe Irish Planning Institute has six categories of membership: Fellowship, Corporate, Graduate, Student, Affiliate and Honorary. \n\nFellowship is generally awarded to professionally qualified planners who have considerable professional planning experience and who have made an important contribution to the development of the Institute. \n\nCorporate Membership, which is the main class of membership, is open to professionally qualified planners those who have reached a standard of knowledge, skill and professional experience necessary to engage in planning in Ireland, in the public sector or in private practice. This class of membership is also open to qualified professional planners who are full members of the member Institutes of the European Council of Spatial Planners, and to recently qualified planners from other countries, subject to appropriate post-qualification experience, under the provisions of the European Directive on Mutual Recognition of Professional Qualifications.\n\nGraduate Membership is available to those who have acquired an IPI accredited planning qualification and, under certain circumstances, those who have been awarded planning qualifications which have not been accredited by the IPI.\n\nPersons engaged in a programme of formal study leading to an IPI accredited planning qualification from a planning school are eligible for Student Membership of the Institute.\n\nAffiliate Membership can be awarded to persons who have a professional qualification that is related to planning, who have been engaged in approved research or practice related to planning and who have been engaged in approved research or practice relating to planning, and who have made a special contribution to planning in Ireland.\n\nHonorary Membership is generally awarded as an honour on an individual whose special interests have been deemed by the Institute to have made an outstanding contribution to planning.\n\nThe Council of the IPI, which is the governing body of the Institute, is directly elected by the Members by postal ballot.\n\nThe aims of the Planning Institute are: \n\n\nThere are a number of regional and sectoral branches within the Institute. The Regional Branches are: the Cork Branch, the Midland Branch, the Mid-West Branch, the Western Branch, the South Eastern Branch, the Greater Dublin Area Branch and the Northern Branch (covering members in Northern Ireland). The sectoral Branch is the Private Practice Branch, covering members working in private practice throughout the island of Ireland.\n\nThe Irish Planning Institute\n\n"}
{"id": "42307766", "url": "https://en.wikipedia.org/wiki?curid=42307766", "title": "Jean-Pierre Gosse", "text": "Jean-Pierre Gosse\n\nJean-Pierre Gosse is a biologist who discovered a type of angelfish called Pterophyllum leopoldi \n"}
{"id": "42285920", "url": "https://en.wikipedia.org/wiki?curid=42285920", "title": "Laser microprobe mass spectrometer", "text": "Laser microprobe mass spectrometer\n\nA laser microprobe mass spectrometer (LMMS), also laser microprobe mass analyzer (LAMMA), laser ionization mass spectrometer (LIMS), or laser ionization mass analyzer (LIMA) is a mass spectrometer that uses a focused laser for microanalysis. It employs local ionization by a pulsed laser and subsequent mass analysis of the generated ions.\n\nIn laser microprobe mass analysis, a highly focused laser beam is pulsed on a micro sample usually with a volume of approximately 1 microliter. The resulting ions generated by this laser are then analyzed with time-of-flight mass spectrometry to give composition, concentration, and in the case of organic molecules structural information.\n\nUnlike other methods of microprobe analysis which involve ions or electrons, the LMMS microproble fires an ultraviolet pulse in order to create ions. As a result, this method is much better at detecting qualitatively rather than quantitatively.\n\nLMMS is relatively simple to operate compared to other methods. Furthermore, its strength lies in its ability to analyze biological materials to detect certain compounds (such as metals or organic materials).\n\nLAMMA is particular about the sample which is used. The sample must be small and thin. Ionization of too much material results in a large microplasma whose time spread and ion energy distribution entering the mass spectrometer can result in undesired peak deformation.\n\n"}
{"id": "13549151", "url": "https://en.wikipedia.org/wiki?curid=13549151", "title": "List of Mexican flags", "text": "List of Mexican flags\n\nThe following is a list of flags used in the United Mexican States.\n\n"}
{"id": "2675160", "url": "https://en.wikipedia.org/wiki?curid=2675160", "title": "List of compounds with carbon number 24", "text": "List of compounds with carbon number 24\n\nThis is a partial list of molecules that contain 24 carbon atoms.\n\n"}
{"id": "2655502", "url": "https://en.wikipedia.org/wiki?curid=2655502", "title": "Lists of biologists by author abbreviation", "text": "Lists of biologists by author abbreviation\n\nThe following are lists of biologists by author abbreviation:\n"}
{"id": "26386980", "url": "https://en.wikipedia.org/wiki?curid=26386980", "title": "Mars Rafikov", "text": "Mars Rafikov\n\nMars Zakirovich Rafikov (, 29 September 1933 ‚Äì 23 July 2000) was a Soviet cosmonaut who was dismissed from the Soviet space program for disciplinary reasons.\n\nSenior Lieutenant Rafikov, age 26, was selected as one of the original 20 cosmonauts on March 7, 1960 along with Yuri Gagarin.\n\nOn March 24, 1962, Rafikov was dismissed from the cosmonaut corps, officially for \"a variety of offenses, including womanizing and 'gallivanting' in Moscow restaurants, and so forth\". Other cosmonauts (notably Gagarin) had exhibited similar behavior, but could not be officially disciplined because of their stature and international reputation. Gherman Titov later suggested, though, that the real reason for his dismissal was because he and his wife had divorced.\n\nHe remained in the military, serving as a pilot in the Afghanistan war.\n\nTo protect the image of the space program, efforts were made to cover up the reason for Rafikov's dismissal. His image, like that of others who were dismissed, was airbrushed out of cosmonaut photos. This airbrushing led to speculation about \"lost cosmonauts\" even though the actual reasons were often mundane.\n\n"}
{"id": "30268354", "url": "https://en.wikipedia.org/wiki?curid=30268354", "title": "Megaflora", "text": "Megaflora\n\nMegaflora (from Greek ŒºŒ≠Œ≥Œ±œÇ \"megas\" \"large\" and New Latin \"flora\" \"plant life\") refers to an exceptionally large plant species. Examples of megaflora include the Sequoioideae of California and a number of extinct plant species from the Mesozoic.\n\n\n\n\n\n\n"}
{"id": "46728121", "url": "https://en.wikipedia.org/wiki?curid=46728121", "title": "Modti inc.", "text": "Modti inc.\n\nModti Inc. is a Finnish technology company headquartered in Joensuu, Finland, that designs, develops, and sells its shape programmable hardware technology. As of May 17, 2015, its main product is Segment, an electrically driven, paper-thin, and flexible actuator bending mechanism tailored for use in emerging flexible technologies.\n\nModti was officially founded by Shane H. Allen and Iouri Kotorov on August 13, 2013 to develop and sell its shape programmable hardware technology. Originally registered as MottiFilm Inc. and was renamed Modti Inc. on August 13, 2014 due to its name giving the incorrect impression of being a movie company.\n\nPartners have included Joensuu Business Incubator, Karelia University of Applied Sciences, Startup Sauna, FAU Technology Business Incubator and Joensuu Entrepreneurship Society.\n\n\n\n"}
{"id": "19091272", "url": "https://en.wikipedia.org/wiki?curid=19091272", "title": "M‚Äìsigma relation", "text": "M‚Äìsigma relation\n\nThe M‚Äìsigma (or M\"‚Äì\"œÉ) relation is an empirical correlation between the stellar velocity dispersion \"œÉ\" of a galaxy bulge and the mass M of the supermassive black hole at its center.\n\nThe M‚ÄìœÉ relation was first presented in 1999 during a conference at the Institut d'astrophysique de Paris in France. The proposed form of the relation, which was called the \"Faber‚ÄìJackson law for black holes\", was\nwhere formula_2 is the solar mass. Publication of the relation in a refereed journal, by two groups, took place the following year.\nOne of many recent studies, based on the growing sample of published black hole masses in nearby galaxies, gives\n\nEarlier work demonstrated a relationship between galaxy luminosity and black hole mass, which nowadays has a comparable level of scatter. The \"M\"‚Äì\"œÉ\" relation is generally interpreted as implying some source of mechanical feedback between the growth of supermassive black holes and the growth of galaxy bulges, although the source of this feedback is still uncertain.\n\nDiscovery of the M‚ÄìœÉ relation was taken by many astronomers to imply that supermassive black holes are fundamental components of galaxies. Prior to about 2000, the main concern had been the simple detection of black holes, while afterward the interest changed to understanding the role of supermassive black holes as a critical component of galaxies. This led to the main uses of the relation to estimate black hole masses in galaxies that are too distant for direct mass measurements to be made, and to assay the overall black hole content of the Universe.\n\nThe tightness of the M‚ÄìœÉ relation suggests that some kind of feedback acts to maintain the connection between black hole mass and stellar velocity dispersion, in spite of processes like galaxy mergers and gas accretion that might be expected to increase the scatter over time.\nOne such mechanism was suggested by Joseph Silk and Martin Rees in 1998. These authors proposed a model in which supermassive black holes first form via collapse of giant \ngas clouds before most of the bulge mass has turned into stars. The black holes created in this way would then accrete and radiate, driving a wind which acts back on the accretion flow.\nThe flow would stall if the rate of deposition of mechanical energy into the infalling gas was large enough to unbind the protogalaxy in one crossing time. The Silk and Rees model predicts \na slope for the M‚ÄìœÉ relation of Œ±=5, which is approximately correct. However, the predicted normalization of the relation is too small by about a factor of one thousand. The reason is that there is far more energy released in the formation of a supermassive black hole than is needed to completely unbind the stellar bulge.\n\nA more successful feedback model was first presented by Andrew King at the University of Leicester in 2003. In King's model, feedback occurs through momentum transfer, rather than energy transfer as in the case of Silk & Rees's model. A \"momentum-driven flow\" is one in which the gas cooling time is so short that essentially all the energy in the flow is in the form of bulk motion. In such a flow, most of the energy released by the black hole is lost to radiation, and only a few percent is left to affect the gas mechanically. King's model predicts a slope of Œ±=4 for the M‚ÄìœÉ relation, and the normalization is exactly correct; it is roughly a factor c/œÉ ‚âà 10 times larger than in Silk & Rees's relation.\n\nBefore the M‚ÄìœÉ relation was discovered in 2000, a large discrepancy existed between black hole masses derived using three techniques.\n\"Direct,\" or dynamical, measurements based on the motion of stars or gas near the black hole seemed to give masses that averaged ~1% of the bulge mass (the \"Magorrian relation\"). Two other techniques‚Äîreverberation mapping in active galactic nuclei, and the Soltan argument, which computes the cosmological density in black holes needed to explain the quasar light‚Äîboth gave a mean value of M/M that was a factor ~10 smaller than implied by the Magorrian relation. The M‚ÄìœÉ relation resolved this discrepancy by showing that most of the direct black hole masses published prior to 2000 were significantly in error, presumably because the data on which they were based were of insufficient quality to resolve the black hole's dynamical sphere of influence. The mean ratio of black hole mass to bulge mass in big early-type galaxies is now believed to be approximately 1:200, and increasingly smaller as one moves to less massive galaxies.\n\nA common use of the \"M\"‚Äì\"œÉ\" relation is to estimate black hole masses in distant galaxies using the easily measured quantity œÉ. Black hole masses in thousands of galaxies have been estimated in this way. The M‚ÄìœÉ relation is also used to calibrate so-called secondary and tertiary mass estimators, which relate the black hole mass to the strength of emission lines from hot gas in the nucleus or to the velocity dispersion of gas in the bulge.\n\nThe tightness of the \"M\"‚Äì\"œÉ\" relation has led to suggestions that \"every\" bulge must contain a supermassive black hole. However, the number of galaxies in which the effect of the black hole's gravity on the motion of stars or gas is unambiguously seen is still quite small. It is unclear whether the lack of black hole detections in many galaxies implies that these galaxies do not contain black holes; or that their masses are significantly below the value implied by the M‚ÄìœÉ relation; or that the data are simply too poor to reveal the presence of the black hole.\n\nThe smallest supermassive black hole with a well-determined mass has . The existence of black holes in the mass range (\"intermediate-mass black holes\") is predicted by the M‚ÄìœÉ relation in low-mass galaxies, and the existence of intermediate-mass black holes has been reasonably well established in a number of galaxies that contain active galactic nuclei, although the values of M in these galaxies are very uncertain.\nNo clear evidence has been found for ultra-massive black holes with masses above , although this may be an expected consequence of the observed upper limit to œÉ.\n\n"}
{"id": "241742", "url": "https://en.wikipedia.org/wiki?curid=241742", "title": "Outline of organic chemistry", "text": "Outline of organic chemistry\n\nThe following outline is provided as an overview of and topical guide to organic chemistry:\n\nOrganic chemistry ‚Äì scientific study of the structure, properties, composition, reactions, and preparation (by synthesis or by other means) of carbon-based compounds, hydrocarbons, and their derivatives. These compounds may contain any number of other elements, including hydrogen, nitrogen, oxygen, the halogens as well as phosphorus, silicon, and sulfur.\n\n\nCurrent trends (as of 2008) in organic chemistry include:\n\n\n\n\"See also List of organic reactions\n\n\n"}
{"id": "394392", "url": "https://en.wikipedia.org/wiki?curid=394392", "title": "Selection bias", "text": "Selection bias\n\nSelection bias is the bias introduced by the selection of individuals, groups or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed. It is sometimes referred to as the selection effect. The phrase \"selection bias\" most often refers to the distortion of a statistical analysis, resulting from the method of collecting samples. If the selection bias is not taken into account, then some conclusions of the study may not be accurate.\n\nThere are many types of possible selection bias, including:\n\nSampling bias is systematic error due to a non-random sample of a population, causing some members of the population to be less likely to be included than others, resulting in a biased sample, defined as a statistical sample of a population (or non-human factors) in which all participants are not equally balanced or objectively represented. It is mostly classified as a subtype of selection bias, sometimes specifically termed \"sample selection bias\", but some classify it as a separate type of bias.\nA distinction of sampling bias (albeit not a universally accepted one) is that it undermines the external validity of a test (the ability of its results to be generalized to the rest of the population), while selection bias mainly addresses internal validity for differences or similarities found in the sample at hand. In this sense, errors occurring in the process of gathering the sample or cohort cause sampling bias, while errors in any process thereafter cause selection bias.\n\nExamples of sampling bias include self-selection, pre-screening of trial participants, discounting trial subjects/tests that did not run to completion and migration bias by excluding subjects who have recently moved into or out of the study area.\n\n\n\n\n\n\"Attrition bias\" is a kind of selection bias caused by attrition (loss of participants), discounting trial subjects/tests that did not run to completion. It is closely related to the survivorship bias, where only the subjects that \"survived\" a process are included in the analysis or the failure bias, where only the subjects that \"failed\" a process are included. It includes \"dropout\", \"nonresponse\" (lower response rate), \"withdrawal\" and \"protocol deviators\". It gives biased results where it is unequal in regard to exposure and/or outcome. For example, in a test of a dieting program, the researcher may simply reject everyone who drops out of the trial, but most of those who drop out are those for whom it was not working. Different loss of subjects in intervention and comparison group may change the characteristics of these groups and outcomes irrespective of the studied intervention.\n\nData are filtered not only by study design and measurement, but by the necessary precondition that there has to be someone doing a study. In situations where the existence of the observer or the study is correlated with the data, observation selection effects occur, and anthropic reasoning is required.\n\nAn example is the past impact event record of Earth: if large impacts cause mass extinctions and ecological disruptions precluding the evolution of intelligent observers for long periods, no one will observe any evidence of large impacts in the recent past (since they would have prevented intelligent observers from evolving). Hence there is a potential bias in the impact record of Earth. Astronomical existential risks might similarly be underestimated due to selection bias, and an anthropic correction has to be introduced.\n\nIn the general case, selection biases cannot be overcome with statistical analysis of existing data alone, though Heckman correction may be used in special cases. An assessment of the degree of selection bias can be made by examining correlations between exogenous (background) variables and a treatment indicator. However, in regression models, it is correlation between \"unobserved\" determinants of the outcome and \"unobserved\" determinants of selection into the sample which bias estimates, and this correlation between unobservables cannot be directly assessed by the observed determinants of treatment.\n\nSelection bias is closely related to:\n\n"}
{"id": "2773801", "url": "https://en.wikipedia.org/wiki?curid=2773801", "title": "Self-experimentation", "text": "Self-experimentation\n\nSelf-experimentation refers to the special case of single-subject research in which the experimenter conducts the experiment on themself. Usually this means that a single person is the designer, operator, subject, analyst, and user or reporter of the experiment.\n\nCurrent champions of self-experimentation include the late Seth Roberts, Tim Ferriss, and a sprawling community of Quantified Self\n\nHuman scientific self-experimentation principally (though not necessarily) falls into the fields of medicine and psychology. Self-experimentation has a long and well-documented history in medicine which continues to the present day.\n\nFor example, after failed attempts to infect piglets in 1984, Barry Marshall drank a petri dish of the Helicobacter pylori from a patient, and soon developed gastritis, achlorhydria, stomach discomfort, nausea, vomiting, and halitosis. The results were published in 1985 in the \"Medical Journal of Australia,\" and is among the most cited articles from the journal. He was awarded the Nobel Prize in Physiology or Medicine in 2005.\n\nEvaluations have been presented in the context of clinical trials and program evaluations.\n\nThe self-experimental approach has long and often been applied to practical psychological problems. Benjamin Franklin recorded his self-experiment of successively devoting his attention for a week to one of thirteen \"Virtues\", \"leaving the other Virtues to their ordinary Chance, only marking every Evening the Faults of the Day.\"\n\nIn psychology, the best-known self-experiments are the memory studies of Hermann Ebbinghaus, which established many basic characteristics of human memory through tedious experiments involving nonsense syllables.\n\nIn \"Self-change: Strategies for solving personal problems\", M. J. Mahoney suggested that self-experimentation be used as a method of psychological treatment, and recommended that clients be taught basic scientific methods, in order that the client become a \"personal scientist.\"\n\nSeveral popular and well-known sweeteners were discovered by deliberate or sometimes accidental tasting of reaction products. Sucralose was discovered by a scientist mishearing the instruction to \"test\" the compounds as to \"taste\" the compounds. Fahlberg noticed a sweet taste on his fingers and associated the taste with his work in the chemistry labs at Johns Hopkins; out of that taste test came Saccharin. Cyclamate was discovered when a chemist noticed a sweet taste on his cigarette that he had set down on his bench. Aspartame was also discovered accidentally when chemist Schlatter tasted a sweet substance that had stuck to his hand. Acesulfame potassium is another sweetener discovered when a chemist tasted what he had made.\n\nLeo Sternbach, the inventor of Librium and Valium, tested chemicals that he made on himself, saying in an interview, \"I tried everything. Many drugs. Once, in the sixties, I was sent home for two days. It was an extremely potent drug, not a Benzedrine. I slept for a long time. My wife was very worried.\"\n\nSwiss chemist Albert Hofmann first discovered the psychedelic properties of LSD five years after its creation, when he accidentally absorbed a small amount of the drug through his fingertips. Days later, he intentionally self-experimented with it.\n\nCharles Dalziel studied the effects of electricity on animals and humans, and wrote \"The Effects of Electric Shock on Man\", a book in which he explains the effects of different amounts of electricity on human subjects. He carried out experiments on human subjects, including himself. He also invented the ground-fault circuit interrupter or GFCI, based on his understanding of electric shock in humans.\n\nIn 1998 a British scientist, Kevin Warwick, became the first human being to test an RFID as an implant to control surrounding technology. In 2002 he went on to have an array of 100 electrodes fired into the median nerve of his left arm during a two-hour neurosurgical operation. With this implant in place, over a three-month period, he conducted a number of experiments, including the first direct electronic communication between the nervous systems of two humans.\n\nThe 21st century has seen a revival of self-experimentation. With the late Seth Roberts having a popular blog with many interesting findings of himself and others. Bestselling author Tim Ferriss claims to be a self-experimenter to the extreme. The Quantified Self community has many members that do many kinds of experiments and meet-ups to report about them, along with yearly conferences discussing findings, methods and other aspects.\nAlexander Shulgin devoted his entire career to self experimentation, publishing his results in the widely acclaimed books PiHKAL and TiHKAL. Dr. Zee, a self-proclaimed Neo-Shulginist, is also known for a career of self experimentation.\n\nExamples in classic fiction include the tales of \"The Invisible Man\" and \"Dr. Jekyll and Mr. Hyde\". In each case the scientist's unorthodox theories lead to permanent change and ultimately to self-destruction.\n\nSelf-experimentation is a common trait amongst mad scientists and evil geniuses in more contemporary fiction and is part of the creation story of many comic book supervillains, and some superheroes. For example, the Spider-Man villain The Lizard lost his arm in a war (other versions vary), and experimented with reptilian DNA to try to grow it back; however, the therapy caused him to mutate into a half-human-half-reptile creature. The Fantastic Four were created when the Four were testing Reed Richards's new prototype rocket and were exposed to cosmic rays, giving them super powers. Other cases include the Man-Bat, the Ultra-Humanite, the Green Goblin, and the animated Justice League version of Cheetah.\n\nThe deepest example for self-experimentation was shown in Ang Lee's \"Hulk\" (2003), where David Banner experimented on himself to improve on his own limits, which is also why his son had many different abilities.\n\n"}
{"id": "1101984", "url": "https://en.wikipedia.org/wiki?curid=1101984", "title": "Soft systems methodology", "text": "Soft systems methodology\n\nSoft systems methodology (SSM) is an approach to organizational process modeling (business process modeling) and it can be used both for general problem solving and in the management of change. It was developed in England by academics at the University of Lancaster Systems Department through a ten-year action research program.\n\nThe methodology was developed from earlier systems engineering approaches, primarily by Peter Checkland and colleagues such as Brian Wilson. The primary use of SSM is in the analysis of complex situations where there are divergent views about the definition of the problem. These situations are \"soft problems\" such as: How to improve health services delivery? How to manage disaster planning? When should mentally disordered offenders be diverted from custody? What to do about homelessness amongst young people?\n\nIn such situations even the actual problem to be addressed may not be easy to agree upon. \nTo intervene in such situations the \"soft systems\" approach uses the notion of a \"system\" as an interrogative device that will enable debate amongst concerned parties. In its 'classic' form the methodology consists of seven steps, with initial appreciation of the problem situation leading to the modelling of several human activity systems that might be thought relevant to the problem situation. By discussions and exploration of these, the decision makers will arrive at accommodations (or, exceptionally, at consensus) over what kind of changes may be systemically desirable and feasible in the situation. Later explanations of the ideas give a more sophisticated view of this systemic method, and give more attention to locating the methodology in respect to its philosophical underpinnings. It is the earlier classical view which is most widely used in practice.\n\nThere are several hundred documented examples of the successful use of SSM in many different fields, ranging from ecology, to business and military logistics. It has been adopted by many organizations and incorporated into other approaches: in the 1990s for example it was the recommended planning tool for the UK government's SSADM system development methodology.\n\nThe general applicability of the approach has led to some criticisms that it is functionalist, non-emancipatory or supports the status quo and existing power structures; this is a claim that users would deny, arguing that the methodology itself can be none of these, it is the user of the methodology that may choose to employ it in such a way.\n\nThe methodology has been described in several books and many academic articles.\n\nSSM remains the most widely used and practical application of systems thinking, and other systems approaches such as critical systems thinking have incorporated many of its ideas.\n\n7-stage representation of SSM:\n\nIn 1975, David Smyth, a researcher in Checkland's department, observed that SSM was most successful when the Root Definition included certain elements. These elements, captured in the mnemonic CATWOE, identified the people, processes and environment that contribute to a situation, issue or problem that required analyzing.\n\nThis is used to prompt thinking about what the business is trying to achieve. Business Perspectives help the Business Analyst to consider the impact of any proposed solution on the people involved. There are six elements of CATWOE\n\nA human activity system can be defined as 'notional system (i.e. not existing in any tangible form) where human beings are undertaking some activities that achieve some purpose' (Patching, 1990).\n\n\n\n\n"}
{"id": "4521831", "url": "https://en.wikipedia.org/wiki?curid=4521831", "title": "Supervisory control", "text": "Supervisory control\n\nSupervisory control is a general term for control of many individual controllers or control loops, such as within a distributed control system. It refers to a high level of overall monitoring of individual process controllers, which is not necessary for the operation of each controller, but gives the operator an overall plant process view, and allows integration of operation between controllers. \n\nA more specific use of the term is for a Supervisory Control and Data Acquisition system or SCADA, which refers to a specific class of system for use in process control, often on fairly small and remote applications such as a pipeline transport, water distribution, or wastewater utility system station.\n\nSupervisory control often takes one of two forms. In one, the controlled machine or process continues autonomously. It is observed from time to time by a human who, when deeming it necessary, intervenes to modify the control algorithm in some way. In the other, the process accepts an instruction, carries it out autonomously, reports the results and awaits further commands. With manual control, the operator interacts directly with a controlled process or task using switches, levers, screws, valves etc., to control actuators. This concept was incorporated in the earliest machines which sought to extend the physical capabilities of man. In contrast, with automatic control, the machine adapts to changing circumstances and makes decisions in pursuit of some goal which can be as simple as switching a heating system on and off to maintain a room temperature within a specified range. Sheridan defines supervisory control as follows: \"in the strictest sense, supervisory control means that one or more human operators are intermittently programming and continually receiving information from a computer that itself closes an autonomous control loop through artificial effectors to the controlled process or task environment.\"\n\nRobotics applications have traditionally aimed for automatic control. Automatic control requires sensing and responding appropriately to all combinations of circumstances which can present problems of overwhelming complexity. A supervisory control scheme offers the prospect of solving the automation problem incrementally and leaving those problems unsolved to be handled by the human supervisor.\n\nCommunications delay does not have the same impact on this control scheme. All time critical feedback occurs at the slave where the delays are negligible. Instability is thus avoided without modifying the feedback loop. Communications delay, in this case, slows the rate at which an operator can assign tasks to the slave and determine whether those tasks have been successfully carried out.\n\n\n"}
{"id": "145245", "url": "https://en.wikipedia.org/wiki?curid=145245", "title": "Swarthmore College", "text": "Swarthmore College\n\nSwarthmore College (, ) is a private liberal arts college in Swarthmore, Pennsylvania. Founded in 1864, Swarthmore was one of the earliest coeducational colleges in the United States. It was established to be a college \"...under the care of Friends, at which an education may be obtained equal to that of the best institutions of learning in our country.\" By 1906, Swarthmore had dropped its religious affiliation and became officially non-sectarian.\n\nSwarthmore is a member of the Tri-College Consortium along with Bryn Mawr and Haverford College, a cooperative academic arrangement between the three schools. Swarthmore is also affiliated with the University of Pennsylvania through the Quaker Consortium, which allows for students to cross-register for classes at all four institutions. Swarthmore offers over 600 courses a year in more than 40 areas of study, including an ABET accredited engineering program which culminates with a Bachelor of Science in engineering. Swarthmore has a variety of sporting teams with a total of 22 Division III Varsity Intercollegiate Sports Teams and competes in the Centennial Conference, a group of private colleges in Pennsylvania and Maryland.\n\nDespite the school's small size, Swarthmore alumni have attained prominence in a broad range of fields. Graduates include five Nobel Prize winners (as of 2016, the third-highest number of Nobel Prize winners per graduate in the U.S.), 11 MacArthur Foundation fellows, 30 Rhodes Scholars, 27 Truman Scholars, 10 Marshall Scholars, 201 Fulbright Grantees, and many noteworthy figures in law, art, science, academia, business, politics, and other fields. Swarthmore also counts 49 alumni as members of the National Academies of Science, Engineering and Medicine.\n\nSwarthmore is currently ranked 3rd best liberal arts college in the country by U.S. News and World Report.\n\nThe name \"Swarthmore\" has its roots in early Quaker history. In England, Swarthmoor Hall near the town of Ulverston, Cumbria, (previously in Lancashire) was the home of Thomas and Margaret Fell in 1652 when George Fox, (1624‚Äì1691), fresh from his epiphany atop Pendle Hill in 1651, came to visit. The visitation turned into a long association, as Fox persuaded Thomas and Margaret Fell of his views. Swarthmoor was used for the first meetings of what became known as the Religious Society of Friends (later colloquially labeled \"The Quakers\").\n\nThe College was founded in 1864 by a committee of Quakers who were members of the Philadelphia, New York and Baltimore Yearly Meetings of the Religious Society of Friends. Edward Parrish (1822‚Äì1872) was its first president. Lucretia Mott (1793‚Äì1880) and Martha Ellicott Tyson (1795‚Äì1873) were among those Friends, who insisted that the new college of Swarthmore be coeducational. Edward Hicks Magill, the second president, served for 17 years. His daughter, Helen Magill, (1853‚Äì1944), was in the first class to graduate in 1873; in 1877, she was the first woman in the United States to earn a Doctor of Philosophy degree, (Ph.D.); hers was in Greek from Boston University in Boston, Massachusetts.\n\nIn the early 1900s, the College had a major collegiate American football program during the formation period of the soon-to-be nationwide sport, (playing Navy, (Annapolis), Princeton, Columbia, and other larger schools) and an active fraternity and sorority life. The 1921 appointment of Frank Aydelotte as President began the development of the school's current academic focus, particularly with his vision for the Honors program based on his experience as a Rhodes Scholar.\n\nDuring World War II, Swarthmore was one of 131 colleges and universities nationally that took part in the V-12 Navy College Training Program, which offered students a path to a U.S. Navy commission.\n\nWolfgang K√∂hler, Hans Wallach and Solomon Asch were noted psychologists who became professors at Swarthmore, a center for Gestalt psychology. Both Wallach, who was Jewish, and K√∂hler, who was not, had left Nazi Germany because of its discriminatory policies against Jews. K√∂hler came to Swarthmore in 1935 and served until his retirement in 1958. Wallach came in 1936, first as a researcher, and also teaching from 1942 until 1975. Asch, who was Polish-American and had immigrated as a child to the US in 1920, joined the faculty in 1947 and served until 1966, conducting his noted conformity experiments at Swarthmore.\n\nThe 1960s and 1970s saw the construction of new buildings ‚Äì the Sharples Dining Hall in 1964, the Worth Health Center in 1965, the Dana/Hallowell Residence Halls in 1967, and the Lang Music Building in 1973. They also saw a 1967 review of the college initiated by President Courtney Smith, a 1969 black protest movement, in which African-American students conducted an eight-day sit-in in the admissions office to demand increased black enrollment, and the establishment of the Black Cultural Center (1970) and the Women's Resource Center (1974). The Environmental Studies program and the Intercultural Center were established in 1992, and in 1993 the Lang Performing Arts Center was opened; the Kohlberg Hall was then established in 1996 and a renovation of the Trotter hall was undertaken in 1997. In 1999 the college began purchasing renewable energy credits in the form of wind power, and in the 2002‚Äì2003 academic year it constructed its first green roof. In 2008, Swarthmore's first mascot, Phineas the Phoenix, made its debut.\n\nSwarthmore's Oxbridge tutorial-inspired Honors Program allows students to take double-credit seminars from their third year and often write honors theses. Seminars are usually composed of four to eight students. Students in seminars will usually write at least three ten-page papers per seminar, and often one of these papers is expanded into a 20‚Äì30 page paper by the end of the seminar. At the end of their final year, Honors students take oral and written examinations conducted by outside experts in their field. Usually one student in each discipline is awarded \"Highest Honors\"; others are either awarded \"High Honors\" or \"Honors\"; rarely, a student is denied Honors altogether by the outside examiner. Each department usually has a grade threshold for admission to the Honors program.\n\nUncommon for a liberal arts college, Swarthmore has an engineering program in which at the completion of four years' work, students are granted a B.S. in Engineering. Other notable programs include minors in peace and conflict studies, cognitive science, and interpretation theory.\n\nSwarthmore has a total undergraduate student enrollment of 1,620 (for the 2016‚Äì2017 year) and 187 faculty members (99% with a terminal degree), for a student-faculty ratio of 8:1. The small college offers more than 600 courses a year in over 40 courses of study. Swarthmore has a reputation as a very academically oriented college, with 66% of students participating in undergraduate research or independent creative projects, and 90% of graduates eventually attending graduate or professional school.\n\nSome sources, including \"Greene's Guides\", have termed Swarthmore one of the \"Little Ivies\". In its 2013 college ranking, the national news magazine, \"U.S. News & World Report\" ranked Swarthmore as the third-best liberal arts college in the nation, behind Williams and Amherst. In its most recent 2018 rankings, Swarthmore is ranked third, behind Williams, Amherst and tied with Wellesley and Bowdoin. Since the inception of the \"U.S. News\" rankings, Amherst, Williams, and Swarthmore are the only colleges to have been ranked for the number one liberal arts college. Swarthmore has been ranked the number one liberal arts college in the country a total of six times.\n\nIn its 2016 ranking of U.S. colleges and universities, \"Forbes\" magazine ranked Swarthmore twenty-third in the nation. In the March/April 2007 edition of \"Foreign Policy\" magazine, a ranking of the top twenty institutions for the study of international relations placed Swarthmore as the highest-ranked undergraduate-only institution, coming in at 15. The only other undergraduate-focused programs to make the list were Dartmouth and Williams, although neither school is exclusively undergraduate.\n\nSwarthmore ranks 10th in \"The Wall Street Journal\"'s 2004 survey of feeder schools to top ranked business, medical, and law schools. Swarthmore ranked fourth among all institutions of higher education in the United States as measured by the percentage of graduates who went on to earn Ph.D.s between 2002‚Äì2011.\n\nSwarthmore ranked tenth among all colleges and sixth for liberal arts colleges only in the number of schools that selected it as a peer institution. Swarthmore selected Amherst, Bowdoin, Carleton, Davidson, Haverford, Middlebury, Oberlin, Pomona, Trinity, Wesleyan, and Williams as schools of comparable academic quality.\n\nIn 2009, 2010, 2011, and 2013, Swarthmore was named the #1 \"Best Value\" private college by \"The Princeton Review\". Overall selection criteria included more than 30 factors in three areas: academics, costs and financial aid. Swarthmore was also placed on \"The Princeton Review's\" Financial Aid Honor Roll along with twelve other institutions for receiving the highest possible rating in its ranking methodology.\n\nThe college is considered by \"U.S. News & World Report\" as \"most selective\", with 10.7% accepted of the 9,383 applicants during the 2016‚Äì2017 admissions cycle. The number of applicants was the highest in the college's history and among the highest overall of any liberal arts college. The college saw increases in the number of underrepresented students, first generation college students, and international students. The college reports that \"Twenty-five percent of the admitted students are among the first generation in their family to attend college\" and \"Of the admitted students attending high schools reporting class rank, 94 percent are in the top decile\". The 2018 admission statistics are only partially released; it is known so far that 10,749 applicants resulted in 1,016 admits for an admit rate of 9.45%.\n\nIn 2012, \"The Princeton Review\" gave Swarthmore a 99 out of 99 on their Admissions Selectivity Rating. In the November 2003 selectivity ranking for undergraduate programs, \"The Atlantic\" magazine ranked Swarthmore as the only liberal arts college to make the top ten institutions, placing Swarthmore in tenth place.\n\nAt Swarthmore, 16% of earners of undergraduate degrees immediately enter graduate or professional school, and within five years of graduation 77% of alumni enter these programs. Alumni of the school earn graduate degrees most commonly at institutions that include University of California-Berkeley, University of Michigan, Harvard, Columbia, New York University, University of Pennsylvania, Cambridge University, Oxford University, Johns Hopkins, Stanford, and Yale. At graduate programs, the most common fields for Swarthmore graduates to enter are humanities, math & physical sciences, life sciences, and social sciences.\n\nPayScale reports that Swarthmore graduates have an average starting salary of $61,300 and an average mid-career salary of $130,900, making their salaries 39th highest among all colleges and universities, and 10th among liberal arts colleges alone.\n\nThe cost of tuition, student activity fees, room, and board for the 2017‚Äì2018 academic year was $65,774 (tuition alone was $50,424). One hundred percent of admitted students' demonstrated need is offered by the college. In total, 56% of the student body receives financial aid, and the average financial aid award was $50,361 during the 2017‚Äì18 year. As a need-blind school, Swarthmore makes admission decisions and financial aid decisions independently.\n\nSwarthmore's endowment at the end of the 2016 fiscal year was $1,746,962,000. Endowment per student was $1,078,371 for the same year, one of the highest rates in the country. Operating revenue for the 2016 fiscal year was $148,086,000, over 50% of which was provided by the endowment. Swarthmore ended a $230 million capital campaign on October 6, 2006, when President Bloom declared the project completed, three months ahead of schedule. The campaign, christened the \"Meaning of Swarthmore\", had been underway officially since the fall of 2001. 87% of the college's alumni participated in the effort.\n\nAt the end of 2007, the Swarthmore Board of Managers approved the decision for the college to eliminate student loans from all financial aid packages. Instead, additional aid scholarships will be granted.\n\nThe campus consists of , based on a north-south axis anchored by Parrish Hall, which houses numerous administrative offices and student lounges, as well as two floors of student housing. The fourth floor houses campus radio station WSRN-FM as well as the weekly student newspaper, \"The Phoenix\". Many acres are wooded and include trails.\n\nFrom the SEPTA Swarthmore commuter train station and the \"ville\" or borough of Swarthmore to the south, the oak-lined Magill Walk leads north up a hill to Parrish. The campus is coterminous with the grounds of the Scott Arboretum, cited by some as a main staple of the campus's renowned beauty. In 2011, Travel+Leisure named Swarthmore one of the most beautiful college campuses in the United States.\n\nThe majority of the buildings housing classrooms and department offices are located to the north of Parrish, as are Kyle and Woolman dormitories. McCabe Library is to the east of Parrish, as are the dorms of Willets, Mertz, Worth, The Lodges, Alice Paul, and David Kemp. To the west are the dorms of Wharton, Dana, Hallowell, and Danawell, along with the Scott Amphitheater, an open wooded outdoor amphitheater, in which graduations and college collections (meetings) are held. The Crum Woods extend westward from the main campus, and many buildings on the forest side of the campus incorporate views of the woods. South of Parrish are Sharples dining hall, the two non-residential fraternities (Phi Psi and Delta Upsilon), and various other buildings. Palmer, Pittenger, and Roberts dormitories are south of the railroad station, as are the athletic facilities, while the Mary Lyon dorm is off-campus to the southwest.\n\nThe College has three main libraries (McCabe Library, the Cornell Library of Science and Engineering, and the Underhill Music and Dance Library) and seven other specialized collections. Since 1923, McCabe library has been a Federal Depository library for selected U.S. Government documents.\n\nFriends Historical Library was established in 1871 to collect, preserve, and make available archival, manuscript, printed, and visual records concerning the Religious Society of Friends (Quakers) from their origins mid-seventeenth century to the present. Besides the focus on Quaker history, the holdings are a significant research collection for the regional and local history of the middle-Atlantic region of the United States and the history of American social reform. Quakers played prominent roles in almost every major reform movement in American history, including abolition, African-American history, Indian rights, women's rights, prison reform, humane treatment of the mentally ill, and temperance. The collections also reflect the significant role Friends played in the development of science, technology, education, and business in Britain and America. The Library also maintains the Swarthmore College Archives and the papers of the Swarthmore Historical Society.\n\nWithin the archives is what was formerly known as the Jane Addams Peace Collection and later called the Swarthmore College Peace Collection (SCPC). The SCPC includes papers from Jane Addams' collection. and material from over 59 different countries. The Nobel Peace Prize, awarded to Addams, is part of the collection. The SCPC states that \"Well over fifty percent of all the holdings in the Peace Collection concern women's activism around the world.\" The SCPC was started when a member of the board of managers discovered that Addams was burning her old papers, and convinced her to donate them instead to the Friends Historical Library. After World War II, the librarian at Princeton University, Julian P. Boyd, appraised the papers in the SCPC's collection and found that they were of \"rare historic value\".\n\nFounded in 2000, the Swarthmore Mock Trial team placed 10th at the 2000 American Mock Trial Association (AMTA) National Championship Tournament and was awarded \"Best New School\". Dennis Cheng '01 was awarded the prestigious \"Spirit of AMTA\" award in 2000. Swarthmore's team placed 2nd at the 2001 AMTA National Championship Tournament. The Swarthmore Mock Trial program has also won numerous accolades and boasted a team of over 25 members for the 2013‚Äì2014 season. The 2010‚Äì2011 competitive season resulted in all three teams competing at Regional Championships, two teams going on to Opening Round Championships, and one team qualifying and competing at the 2011 National Championships held in Des Moines, Iowa, where the team placed 15th in their division. Other successes included placing first at the Philadelphia Regional competition in February 2011, and winning the University of Massachusetts Amherst's invitational tournament in February 2014.\n\nThe Amos J. Peaslee Debate Society, named after a former United States Ambassador to Australia, is one of the few independently endowed organizations on campus. Members of the Society debate on the American Parliamentary Debate Association (APDA) circuit in addition to traveling abroad to Britain, Canada, and the World Universities Debating Championship for British Parliamentary Style tournaments. The team has won four APDA national championships, including one as recently as 2017. It has also won Team of the Year two times and Speaker of the Year once. In 2017, it was ranked as the top liberal arts debate program in the country.\nTwo Greek organizations exist on the campus in the form of fraternities, Delta Upsilon and local Phi Psi, a former chapter of Phi Kappa Psi. A third, Phi Sigma Kappa fraternity, maintained a chapter on campus from 1906 to 1991 and continues strong alumni involvement.\n\nSororities were abandoned in the 1930s following student outrage about discrimination within the sorority system, and leading to a 79-year ban. However, in September 2012, the college announced that the ban on sororities would be reversed as of the 2013 term, citing Title IX regulations. The four women who helped overturn the ban subsequently spearheaded the reestablishment of a Kappa Alpha Theta chapter the following spring. The announcement sparked controversy on campus; a petition seeking a referendum to continue the ban was dismissed, again citing a legal opinion that to disallow the sorority chapter would be a violation of Title IX regulations. The sorority admitted its first pledge class in the Spring of 2013. A further non-binding referendum was later distributed, but by then the controversy had cooled: Of the six items on the referendum, only one passed, which asked \"Do you support admitting students of all genders to sororities and fraternities?\" No action was taken on the referendum.\n\nSwarthmore has a variety of sporting teams with a total of 22 NCAA Division III varsity intercollegiate sports teams. 40 percent of Swarthmore students play intercollegiate or club sports. Varsity teams include badminton, baseball, basketball, cross country, field hockey, golf, lacrosse, soccer, softball, swimming, tennis, track and field and volleyball. The football team was controversially eliminated in 2000, along with wrestling and, initially, badminton. The Board of Managers cited lack of athletes on campus and difficulty of recruiting as reasons for terminating the programs. Swarthmore also offers a number of club sport options, including men's and women's rugby, ultimate frisbee, volleyball, fencing, squash, and quidditch.\n\nSwarthmore is a charter member of the Centennial Conference, a group of private colleges in Pennsylvania and Maryland.\n\nSwarthmore's varsity women's soccer team has won the Centennial Conference Championship three times (2014, 2017, 2018).\n\nBased on federal campus safety data for 2014, Swarthmore College was the third highest in the nation in \"total reports of rape per 1,000 students\" on its main campus, with 11 reports of rape per 1,000 students.\n\nSwarthmore has two main student news publications. The weekly newspaper, \"The Phoenix\", is published nearly every Thursday. Founded in 1881, the paper began putting stories online in 1995. Two thousand copies are distributed across the college campus and to the Borough of Swarthmore. The newspaper is printed by Bartash Printing in Philadelphia.\n\nThere are a number of magazines at Swarthmore, most of which are published semiannually at the end of each semester. One is \"Spike\", Swarthmore's humor magazine, founded in 1993. The others are literary magazines, including \"Nacht\", which publishes long-form non-fiction, fiction, poetry, and artwork; \"Small Craft Warnings\", which publishes poetry, fiction and artwork; \"Scarlet Letters\", which publishes women's literature; \"Enie\", for Spanish literature; \"OURstory\", for literature relating to diversity issues; \"Bug-Eyed Magazine\", a very limited-run science fiction/fantasy magazine published by Psi Phi, formerly known as Swarthmore Warders of Imaginative Literature (SWIL); \"Remappings\" (formerly \"CelebrASIAN\"), published by the Swarthmore Asian Organization; \"Alchemy\", a collection of academic writings published by the Swarthmore Writing Associates; \"Mjumbe\", published by the Swarthmore African-American Student Society; and a magazine for French literature. An erotica magazine, \"!\" (pronounced \"bang\") was briefly published in 2005 in homage to an earlier publication, \"Untouchables\". Most of the literary magazines print approximately 500 copies, with around 100 pages. There is also a new photography magazine, \"Pun/ctum\", which features work from students and alumni.\n\nThe collegiate a cappella groups include Sixteen Feet, the College's oldest group (founded in 1981), as well as its first and only all-male group. Grapevine is its corresponding all-female group (founded in 1983), and Mixed Company is a co-ed group. Chaverim is a co-ed group that includes students from the Tri-College Consortium and draws on music from cultures around the world for its repertoire. Lastly, OffBeat was founded in the fall of 2013 as a co-ed group. The groups, self-run as volunteer clubs with college support, travel to other schools to participate in concerts. Once every semester, all of the school's a cappella groups collaborate for a joint concert called Jamboree, which includes visiting groups from other colleges and universities.\n\nWSRN 91.5 FM is the college radio station. It has a mix of indie, rock, hip-hop, folk, world, jazz, and classical music, as well as a number of radio talk shows. At one time, WSRN had a significant news department, and covered events such as the 1969 black protest movement extensively. In the 1990s, WSRN centered its programming on the immensely popular \"Hank and Bernie Show\", starring undergraduates Hank Hanks and Bernie Bernstein. Hank and Bernie conducted wide-ranging and entertaining interviews of sports stars and cultural icons such as Lou Piniella, Mark Grace, Jake Plummer, Greg Ostertag, Andy Karich and Mark \"the Bird\" Fidrych, and also engaged the Swarthmore community in discussions on campus issues and current events. Upwards of 90 percent of the Swarthmore community would tune in to the Hank and Bernie Show and many members of the surrounding villages and towns would also listen and call in. Many archived recordings of musical and spoken word performances exist, such as the once-annual Swarthmore Folk Festival. Today WSRN focuses virtually exclusively on entertainment, though it has covered significant news developments such as the athletic cuts in 2000 and the effects of the September 11 attacks on campus. War News Radio and The Sudan Radio Project (formerly the Darfur Radio Project) do broadcast news on WSRN, however. Currently, the longest running show in WSRN's lineup is \"O√≠do al Tambor\", which focuses on news and music from Latin America. The show has been running non-stop, on Sundays from 4:00 to 6:00¬†p.m., since September 2006. After its members graduated in December 2009, the show's concept was revived by the show \"Rayuela\", which has been running since September 2009.\nSwarthmore College students are eligible to participate in the local emergency department, the Swarthmore Fire and Protective Association. They are trained as firefighters and as emergency medical technicians (EMTs) and are qualified on both the state and national level. The fire department responds to over 200 fire calls and almost 800 EMS calls a year. A fire horn is located within the Swarthmore campus and its sound has become a fixture of campus life. Students affectionately refer to the noise as the call of the Fire Moose or the Space Whale, although the names themselves are in a constant state of evolution.\n\nSwarthmore College Computer Society (SCCS) is a student-run volunteer organization independent of the official ITS department of the college. In addition to operating a set of servers that provide e-mail accounts, Unix shell login accounts, server storage space, and webspace to students, professors, alumni, and other student-run organizations, SCCS hosts over 100 mailing lists used by various student groups, and over 130 organizational websites, including the website of the student newspaper, \"The Daily Gazette\". SCCS also provides a computer lab and gaming room, located in Clothier basement beneath Essie Mae's snack bar.\n\nIn September 2003, the SCCS servers survived a Slashdotting while hosting a copy of the Diebold memos on behalf of the student group Free Culture Swarthmore, then known as the Swarthmore Coalition for the Digital Commons. SCCS staff promptly complied with the relevant DMCA takedown request received by the college's ITS department.\n\nSCCS was noted in PC Magazine's article \"Top 20 Wired Colleges\" as one of the reasons for ranking Swarthmore #4 on that list. During the 2004‚Äì2005 school year, the SCCS Media Lounge served as the early home of War News Radio, a weekly webcast run by Swarthmore students and providing news about the Iraq war, providing resources, space, and technical support for the project in its infancy.\n\nThree SCCS-related papers have been accepted for publication at the USENIX Large Installation System Administration (LISA) Conference, one of which was awarded Best Paper.\n\nSwarthmore's alumni include five Nobel Prize winners (second highest number of Nobel Prize winners per graduate in the U.S.), including the 2006 Physics laureate John C. Mather (1968), the 2004 Economics laureate Edward Prescott (1962) and the 1972 Chemistry laureate Christian B. Anfinsen (1937). Swarthmore also has 11 MacArthur Foundation fellows (second highest per graduate and ninth highest overall of any college or university in the U.S.), and hundreds of prominent figures in law, art, science, business, politics, and other fields.\n"}
{"id": "29398139", "url": "https://en.wikipedia.org/wiki?curid=29398139", "title": "University of Edinburgh School of Physics and Astronomy", "text": "University of Edinburgh School of Physics and Astronomy\n\nThe University of Edinburgh School of Physics and Astronomy is the physics department of the University of Edinburgh. The school was formed in 1993 by a merger of the Department of Physics - called the Department of Natural Philosophy until the late 1960s - and the Department of Astronomy, both at the University of Edinburgh. The school is part of the University's College of Science and Engineering.\n\nInstitutions and research groups based within the school include:\n\nThe school is housed in the James Clerk Maxwell Building on the University's King's Buildings campus.\n\nNotable physicists associated with the University have included Joseph Black, Max Born, Peter Higgs, James Clerk Maxwell and Peter Guthrie Tait. Five winners of the Nobel Prize in Physics are associated with the University (Edward Victor Appleton, Charles Glover Barkla, Max Born, Igor Tamm and Peter Higgs).\n\n\n"}
{"id": "1235779", "url": "https://en.wikipedia.org/wiki?curid=1235779", "title": "Writing center", "text": "Writing center\n\nMany educational institutions maintain a writing center that provides students with free assistance on their papers, projects, reports, multimodal documents, web pages, et cetera from consultants. A key goal of any writing center is helping writers to learn. Typical services include help with the purpose, structure, function of writing, and are geared toward writers of various levels and fields of study. Formats may include one-on-one tutoring, group tutoring, and workshop settings. Some services include drop-in, appointment, and weekly services. Writing centers attempt to provide non-proscriptive and non-corrective response, instead relying on a fuller explanation of why a piece of writing may fail to fulfill the writer's aims. The goal is to help a writer learn to address the various exigences that they may encounter with the realization that no writing is decontextualized‚Äîit always addresses a specific audience.\n\nA writing center usually offers individualized conferencing whereby the writing tutor offers his or her feedback on the piece of writing at hand; a writing tutor's main function is to discuss how the piece of writing might be revised. However, the tutor usually does not proofread nor edit the student's work. Instead, the tutor facilitates the student's attempts to revise his or her own work by conversing with the student about the topic at hand, discussing principles and processes of writing, modeling rhetorical and syntactical moves for the student to apply, and assisting the student in identifying patterns of grammatical error in his or her writing. This is based upon a large body of theory discussed in a later paragraph.\n\nHistorically, writing centers in American universities began appearing as \"writing labs\" in the early 20th century. Elizabeth Boquet and Stephen North point to the origins of the writing laboratory as first a method, not a place, where \"the key characteristic of which appears to have been that all work was to be done during class time\". This was to allow the student to compose with the teacher present, able to help with any revisions or questions the student may have. However, as class sizes and universities grew, Writing Centers began to develop as university institutions, often conceived of as an editing service for students.\n\nSome institutions also offer an Online Writing Lab (OWL), which generally attempts to follow the model of writing center tutoring in an online environment. These environments have been said to be a step toward a new model of writing centers, a model known as Multiliteracy Centers. Another environment that could fall under this category is a physical space known as a digital studio.\n\nWhile some institutions do not have writing centers, a number offer similar support through entities which have a wider remit (scope). These might typically be called Academic Skills Units or Learning Development Groups. Writing centers may be part of a writing studies department or stand-alone.\n\nA recent movement in writing centers has been to provide services for the non-academic community. Such services include one-to-one response for out-of-school writers, and workshops on a wide variety of topics. Such writing centers are often identified as community writing centers.\n\nWriting centers are not exclusively a post-secondary phenomenon. Some high schools have successfully created writing centers similar to the model in higher education.\n\nIn many cases, writing center directors or writing program administrators (WPAs) are responsible for conducting writing center assessment, and must communicate these results to academic administration and various stakeholders. Assessment is seen as beneficial for writing centers because it challenges them to assume the professional and ethical behaviors important not just for writing centers but for all higher education.\n\nNo longer strictly an American phenomenon, writing centers have spread in other world regions as well. The European Association for the Teaching of Academic Writing (EATAW) is in part concerned with the study and advancement of writing centers in European universities. The International Writing Centers Association (IWCA) is committed to supporting writing centers from around the world, with current regional associations in Europe and proposed associations in the Middle East, South Africa, and the Far East.\n\nWriters served by these writing centers may vary depending on the setting. A university writing center may service undergraduate writers at a separate writing center than graduate writers, or combine the two. A department may choose to have its own writing center for students in that major. Community college writing centers have programs serving enrolled students, GED students, or members from the community. High school writing centers service enrolled students only.\n\nDepending on the writing center and the target population, consultants may be undergraduate peer consultants, graduate consultants, graduate peer consultants, staff consultants, or faculty consultants. The consultants may be working for pay or for college credit. If the writing center offers workshop or group tutoring sessions, staff, experienced undergraduates, or graduates may serve in an unofficial or official TA capacity. Writing center research has examined what effect each type of consultant has upon the writer seeking help.\n\nFaculty, students, staff, and administrators often viewed writing centers as places for remediation. At their best, however, they are places where all students, including the best ones, can get better, a place (according to Karen Head), \"that returns to the ideal of a safe space for active debate and discourse about the best ways to communicate in a variety of modes.\"\n\nList here\n\nLinks to further reading\n\n\n"}
