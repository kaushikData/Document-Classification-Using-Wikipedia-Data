{"id": "6869444", "url": "https://en.wikipedia.org/wiki?curid=6869444", "title": "A574 road", "text": "A574 road\n\nThe A574 is a road in England, running through the borough of Warrington before terminating at the end of Butts Bridge in Leigh. The route covers a distance of approximately and links Warrington town centre with the outer suburbs of Birchwood and Sankey.\n\nThe road passes through the following districts of Warrington and Leigh (in route order):\n\nBeing in the new part of Warrington, the road is renowned for its numerous roundabouts—26 in all, the first one less than from the start and the last about from the terminus.\n"}
{"id": "2370084", "url": "https://en.wikipedia.org/wiki?curid=2370084", "title": "A702 road", "text": "A702 road\n\nThe A702 is a major road in Scotland, that runs from Edinburgh to St. John's Town of Dalry in Dumfries and Galloway. It is the last section of a sometimes recommended route by Google Maps between London and Edinburgh, the English and Scottish capitals, which now follows the M1, M6, A74(M) and A702 roads, rather than the more direct A1 and A1(M) which runs all the way.\n\nThe A702 begins as a minor street heading north as Ponton Street from its junction with West Tolcross, then turning east into Fountainbridge, and south into Earl Grey Street where it overlaps with the A700. As at 2013 it is not possible to drive this section continuously due to opposing one-way systems.\n\nIt starts as a primary route at the Tollcross junction in Edinburgh, and continues south until it meets the Edinburgh City Bypass (A720) on the city's outskirts. In the city is known as Home Street, Leven Street, Bruntsfield Place, Morningside Road, Comiston Road and finally Biggar Road. It continues in a south-westerly direction through the Pentland Hills to Biggar, before following the Clyde Valley. The route is a major commuter route for residents of Carlops, West Linton and Biggar who work in and around the Edinburgh area. The road passes through the villages of Coulter and Lamington, before meeting the A73 road at a new roundabout junction, and then shortly afterwards meets junction 13 of the M74. From Abington, it continues as a non-primary route, and overlaps the A76 road for 1½ miles until Thornhill. From Thornhill, the road winds south-west still, through the village of Moniaive, until eventually reaching St. John's Town of Dalry. Here, it terminates at its junction with the A713 road.\n\nThe A702 from junction 13 of the M74 is the recommended route to Edinburgh, whereas the windier A7 road is signed as the Tourist Route to Edinburgh. The route attracts a large number of lorries and heavy goods vehicles and even though it includes some reasonably straight sections suitable for overtaking it can be a slow route from the M74 to Edinburgh. One other possible route to Edinburgh for those prepared to enjoy their driving more is the A701 road via Moffat.\n\nThe stretch of the A702 between Abington and Edinburgh was formerly part of the Euroroute system, of route E32. The E32 also ran northwards out of Edinburgh along the A90/M90 as far as Perth.\n\n"}
{"id": "47158945", "url": "https://en.wikipedia.org/wiki?curid=47158945", "title": "Alain Berton", "text": "Alain Berton\n\nAlain-Edgard Berton (1912-1979) was a French chemical engineer who specialized in toxicology and in the analysis of air components in industrial environments. In the late 1950s he invented the \"Osmopile\", a measuring device, dubbed \"the first artificial nose,\" which initiated, through the use of highly sensitive galvanic cells, the electrochemical analysis of air to detect dangerous components.\n\nAlain Berton was born in Coro Coro in Bolivia on 27 August 1912. He was the son of Adrien Berton, a mining engineer, and Justine Rodriguez. He was educated at the Lycée Hoche in Versailles, and became a chemical engineer at the Chemical Institute of the University of Paris in 1933. From 1935 to 1937 he was a Ramsay Fellow at the Institute of Technology in London, in the laboratory of Prof. William Lawrence Bragg, at the Royal Institution. He began his career in 1938 at the French National Centre for Scientific Research as a \"boursier\" (fellow) in Georges Urbain's laboratory (dedicated to war chemical studies, protection against poison gas). Following Georges Urbain's death that same year, he was assigned to Paul Lebeau's laboratory as \"chargé de recherche\" (researcher). From 1959 till 1969 he was head of research. In parallel, from 1959 to 1978, he was head of the Toxicology Laboratory for the Regional Social Security Fund in Paris.\n\nBy the end of the war in 1944, post war recovery started: Alain Berton's work on the application of absorption and emission spectroscopy in the ultraviolet and infrared, and within the frame of concerns about labor force protection, the specific dosage of atmospheric pollutants became of vital interest in factories to effectively detect and remedy industrial pollution. Thus, in the 1950s, based on the method of gas chromatography analysis by low temperature followed by pyrolysis, he managed to isolate chlorinated substances and acid vapors components in the air. He was able to individualize traces of gas and vapors by using ultra-sensitive galvanic batteries and galvanic microcell detectors. He presented his research in the preamble to the convention of the Analytical Chemistry Group in 1958. Alain Berton named his invention \"the Osmopile,\" later nicknamed \"the sniffing cells\" by the scientific journal \"Atomes\". The first \"artificial nose\" was thus born. His invention was adopted and developed in the US and went around the world with a report from the Associated Press dated December 8, 1958.\n\nBerton’s Osmopile was marketed by Jouan, a laboratory equipment manufacturer founded in the 1940s by a researcher from the Pasteur Institute and acquired in 2003 by Thermo Electron. The Osmopile device was modernized over time and used in the fight against industrial pollution.\nThrough his invention, Alain Berton proved to be an ecology pioneer.\n\n\nAlain Berton was awarded the Medal of the International Bureau of Analytical Chemistry (BICA)- International fight against chemical weapons. led by .\n\n\nThe information on this page is partially translated from the equivalent page in French licensed under the Creative Commons/Attribution Sharealike . History of contributions can be checked here:\n"}
{"id": "43332292", "url": "https://en.wikipedia.org/wiki?curid=43332292", "title": "Association of International Research and Development Centers for Agriculture", "text": "Association of International Research and Development Centers for Agriculture\n\nThe Association of International Research and Development Centers for Agriculture (AIRCA) is an international, non-profit alliance focused on increasing food security by supporting smallholder agriculture and rural enterprise within healthy, sustainable and climate-smart landscapes.\n\nAIRCA unites nine international agricultural research and development centers which focus on a diverse mix of commodities, crops and issues including tropical agriculture, vegetable production, bamboo and rattan, insect pests, fertilizer use, underutilized crops, biosaline agriculture and sustainable development in mountains.\n\nThe members of AIRCA address sustainable agriculture and the environment at the landscape level. The centers create solutions that take into account the diversity of interactions among people and the environment, agricultural and non-agricultural systems, the crossing of national boundaries and other factors that represent the entire context of agriculture.\n\nAIRCA members serve more than 60 countries comprising over 70% of the world’s population from across the Americas, Africa and the Asia-Pacific region. The broad alliance has collective access to a wide variety of crops and ecosystems.\n\nThe combined resources of AIRCA can be used to achieve 10 of the 17 Sustainable Development Goals (SDGs) established by the United Nations in 2015.\n\nAIRCA member organizations work with crops of high economic, social, nutritional and ecological value. This complements and contrasts with the work of the Food and Agriculture Organization (FAO) and the CGIAR who work primarily in staple crops.\n\nAIRCA does have some overlap with the CGIAR, however the CGIAR generally focuses more on agricultural research while AIRCA has more of a concentration on the implementation of agricultural research and development communication. AIRCA has a strong orientation toward problem solving at a system level, rather than a focus on a single commodity.\n\nAIRCA was launched on 2 March 2012 at the headquarters of the Food and Agriculture Organization (FAO) in Rome, Italy.\n\nIt was publicly presented in Punta del Este, Uruguay, on 30 October 2012 during the second Global Conference on Agricultural Research for Development.\n\n"}
{"id": "227982", "url": "https://en.wikipedia.org/wiki?curid=227982", "title": "Atomic force microscopy", "text": "Atomic force microscopy\n\nAtomic force microscopy (AFM) or scanning force microscopy (SFM) is a very-high-resolution type of scanning probe microscopy (SPM), with demonstrated resolution on the order of fractions of a nanometer, more than 1000 times better than the optical diffraction limit.\n\nAFM is a type of scanning probe microscopy (SPM), with demonstrated resolution on the order of fractions of a nanometer, more than 1000 times better than the optical diffraction limit. The information is gathered by \"feeling\" or \"touching\" the surface with a mechanical probe.\nPiezoelectric elements that facilitate tiny but accurate and precise movements on (electronic) command enable precise scanning.\n\nThe AFM has three major abilities: force measurement, imaging, and manipulation.\n\nIn force measurement, AFMs can be used to measure the forces between the probe and the sample as a function of their mutual separation. This can be applied to perform force spectroscopy, to measure the mechanical properties of the sample, such as the sample's Young's modulus, a measure of stiffness.\n\nFor imaging, the reaction of the probe to the forces that the sample imposes on it can be used to form an image of the three-dimensional shape (topography) of a sample surface at a high resolution. This is achieved by raster scanning the position of the sample with respect to the tip and recording the height of the probe that corresponds to a constant probe-sample interaction (see section topographic imaging in AFM for more details). The surface topography is commonly displayed as a pseudocolor plot.\n\nIn manipulation, the forces between tip and sample can also be used to change the properties of the sample in a controlled way. Examples of this include atomic manipulation, scanning probe lithography and local stimulation of cells.\n\nSimultaneous with the acquisition of topographical images, other properties of the sample can be measured locally and displayed as an image, often with similarly high resolution. Examples of such properties are mechanical properties like stiffness or adhesion strength and electrical properties such as conductivity or surface potential. In fact, the majority of SPM techniques are extensions of AFM that use this modality.\n\nThe major difference between atomic force microscopy and competing technologies such as optical microscopy and electron microscopy is that AFM does not use lenses or beam irradiation. Therefore, it does not suffer from a limitation in spatial resolution due to diffraction and aberration, and preparing a space for guiding the beam (by creating a vacuum) and staining the sample are not necessary.\n\nThere are several types of scanning microscopy including scanning probe microscopy (which includes AFM, scanning tunneling microscopy (STM) and near-field scanning optical microscope (SNOM/NSOM), STED microscopy (STED), and scanning electron microscopy and Electrochemical AFM, EC-AFM). Although SNOM and STED use visible, infrared or even terahertz light to illuminate the sample, their resolution is not constrained by the diffraction limit.\n\nFig. 3 shows an AFM, which typically consists of the following features. Numbers in parentheses correspond to numbered features in Fig. 3. Coordinate directions are defined by the coordinate system (0).\nThe small spring-like cantilever (1) is carried by the support (2). Optionally, a piezoelectric element (typically made of a ceramic material) (3) oscillates the cantilever (1). The sharp tip (4) is fixed to the free end of the cantilever (1). The detector (5) records the deflection and motion of the cantilever (1). The sample (6) is mounted on the sample stage (8). An xyz drive (7) permits to displace the sample (6) and the sample stage (8) in x, y, and z directions with respect to the tip apex (4). Although Fig. 3 shows the drive attached to the sample, the drive can also be attached to the tip, or independent drives can be attached to both, since it is the relative displacement of the sample and tip that needs to be controlled. Controllers and plotter are not shown in Fig. 3.\n\nAccording to the configuration described above, the interaction between tip and sample, which can be an atomic scale phenomenon, is transduced into changes of the motion of cantilever which is a macro scale phenomenon. Several different aspects of the cantilever motion can be used to quantify the interaction between the tip and sample, most commonly the value of the deflection, the amplitude of an imposed oscillation of the cantilever, or the shift in resonance frequency of the cantilever (see section Imaging Modes).\n\nThe detector (5) of AFM measures the deflection (displacement with respect to the equilibrium position) of the cantilever and converts it into an electrical signal. The intensity of this signal will be proportional to the displacement of the cantilever.\n\nVarious methods of detection can be used, e.g. interferometry, optical levers, the piezoresistive method, the piezoelectric method, and STM-based detectors (see section \"AFM cantilever deflection measurement\".).\n\n\"Note: The following paragraphs assume that 'contact mode' is used (see section Imaging Modes). For other imaging modes, the process is similar, except that 'deflection' should be replaced by the appropriate feedback variable.\"\n\nWhen using the AFM to image a sample, the tip is brought into contact with the sample, and the sample is raster scanned along an x-y grid (fig 4). Most commonly, an electronic feedback loop is employed to keep the probe-sample force constant during scanning. This feedback loop has the cantilever deflection as input, and its output controls the distance along the z axis between the probe support (2 in fig. 3) and the sample support (8 in fig 3). As long as the tip remains in contact with the sample, and the sample is scanned in the x-y plane, height variations in the sample will change the deflection of the cantilever. The feedback then adjusts the height of the probe support so that the deflection is restored to a user-defined value (the setpoint). A properly adjusted feedback loop adjusts the support-sample separation continuously during the scanning motion, such that the deflection remains approximately constant. In this situation, the feedback output equals the sample surface topography to within a small error.\n\nHistorically, a different operation method has been used, in which the sample-probe support distance is kept constant and not controlled by a feedback (servo mechanism). In this mode, usually referred to as 'constant height mode', the deflection of the cantilever is recorded as a function of the sample x-y position. As long as the tip is in contact with the sample, the deflection then corresponds to surface topography. The main reason this method is not very popular anymore, is that the forces between tip and sample are not controlled, which can lead to forces high enough to damage the tip or the sample. It is however common practice to record the deflection even when scanning in 'constant force mode', with feedback. This reveals the small tracking error of the feedback, and can sometimes reveal features that the feedback was not able to adjust for.\n\nThe AFM signals, such as sample height or cantilever deflection, are recorded on a computer during the x-y scan. They are plotted in a pseudocolor image, in which each pixel represents an x-y position on the sample, and the color represents the recorded signal.\n\nThe AFM was invented by IBM scientists in 1982. The precursor to the AFM, the scanning tunneling microscope (STM), was developed by Gerd Binnig and Heinrich Rohrer in the early 1980s at IBM Research - Zurich, a development that earned them the 1986 Nobel Prize for Physics. Binnig invented the atomic force microscope and the first experimental implementation was made by Binnig, Quate and Gerber in 1986.\n\nThe first commercially available atomic force microscope was introduced in 1989. The AFM is one of the foremost tools for imaging, measuring, and manipulating matter at the nanoscale.\n\nThe AFM has been applied to problems in a wide range of disciplines of the natural sciences, including solid-state physics, semiconductor science and technology, molecular engineering, polymer chemistry and physics, surface chemistry, molecular biology, cell biology, and medicine.\n\nApplications in the field of solid state physics include (a) the identification of atoms at a surface, (b) the evaluation of interactions between a specific atom and its neighboring atoms, and (c) the study of changes in physical properties arising from changes in an atomic arrangement through atomic manipulation.\n\nIn molecular biology, AFM can be used to study the structure and mechanical properties of protein complexes and assemblies. For example, AFM has been used to image microtubules and measure their stiffness.\n\nIn cellular biology, AFM can be used to attempt to distinguish cancer cells and normal cells based on a hardness of cells, and to evaluate interactions between a specific cell and its neighboring cells in a competitive culture system. AFM can also be used to indent cells, to study how they regulate the stiffness or shape of the cell membrane or wall.\n\nIn some variations, electric potentials can also be scanned using conducting cantilevers. In more advanced versions, currents can be passed through the tip to probe the electrical conductivity or transport of the underlying surface, but this is a challenging task with few research groups reporting consistent data (as of 2004).\n\nThe AFM consists of a cantilever with a sharp tip (probe) at its end that is used to scan the specimen surface. The cantilever is typically silicon or silicon nitride with a tip radius of curvature on the order of nanometers. When the tip is brought into proximity of a sample surface, forces between the tip and the sample lead to a deflection of the cantilever according to Hooke's law. Depending on the situation, forces that are measured in AFM include mechanical contact force, van der Waals forces, capillary forces, chemical bonding, electrostatic forces, magnetic forces (see magnetic force microscope, MFM), Casimir forces, solvation forces, etc. Along with force, additional quantities may simultaneously be measured through the use of specialized types of probes (see scanning thermal microscopy, scanning joule expansion microscopy, photothermal microspectroscopy, etc.).\nThe AFM can be operated in a number of modes, depending on the application. In general, possible imaging modes are divided into static (also called \"contact\") modes and a variety of dynamic (non-contact or \"tapping\") modes where the cantilever is vibrated or oscillated at a given frequency.\n\nAFM operation is usually described as one of three modes, according to the nature of the tip motion: contact mode, also called static mode (as opposed to the other two modes, which are called dynamic modes); tapping mode, also called intermittent contact, AC mode, or vibrating mode, or, after the detection mechanism, amplitude modulation AFM; non-contact mode, or, again after the detection mechanism, frequency modulation AFM.\n\nIt should be noted that despite the nomenclature, repulsive contact can occur or be avoided both in amplitude modulation AFM and frequency modulation AFM, depending on the settings.\n\nIn contact mode, the tip is \"dragged\" across the surface of the sample and the contours of the surface are measured either using the deflection of the cantilever directly or, more commonly, using the feedback signal required to keep the cantilever at a constant position. Because the measurement of a static signal is prone to noise and drift, low stiffness cantilevers (i.e. cantilevers with a low spring constant, k) are used to achieve a large enough deflection signal while keeping the interaction force low. Close to the surface of the sample, attractive forces can be quite strong, causing the tip to \"snap-in\" to the surface. Thus, contact mode AFM is almost always done at a depth where the overall force is repulsive, that is, in firm \"contact\" with the solid surface.\n\nIn ambient conditions, most samples develop a liquid meniscus layer. Because of this, keeping the probe tip close enough to the sample for short-range forces to become detectable while preventing the tip from sticking to the surface presents a major problem for contact mode in ambient conditions. Dynamic contact mode (also called intermittent contact, AC mode or tapping mode) was developed to bypass this problem. Nowadays, tapping mode is the most frequently used AFM mode when operating in ambient conditions or in liquids.\n\nIn \"tapping mode\", the cantilever is driven to oscillate up and down at or near its resonance frequency. This oscillation is commonly achieved with a small piezo element in the cantilever holder, but other possibilities include an AC magnetic field (with magnetic cantilevers), piezoelectric cantilevers, or periodic heating with a modulated laser beam. The amplitude of this oscillation usually varies from several nm to 200 nm. In tapping mode, the frequency and amplitude of the driving signal are kept constant, leading to a constant amplitude of the cantilever oscillation as long as there is no drift or interaction with the surface. The interaction of forces acting on the cantilever when the tip comes close to the surface, Van der Waals forces, dipole-dipole interactions, electrostatic forces, etc. cause the amplitude of the cantilever's oscillation to change (usually decrease) as the tip gets closer to the sample. This amplitude is used as the parameter that goes into the electronic servo that controls the height of the cantilever above the sample. The servo adjusts the height to maintain a set cantilever oscillation amplitude as the cantilever is scanned over the sample. A \"tapping AFM\" image is therefore produced by imaging the force of the intermittent contacts of the tip with the sample surface.\n\nAlthough the peak forces applied during the contacting part of the oscillation can be much higher than typically used in contact mode, tapping mode generally lessens the damage done to the surface and the tip compared to the amount done in contact mode. This can be explained by the short duration of the applied force, and because the lateral forces between tip and sample are significantly lower in tapping mode over contact mode.\nTapping mode imaging is gentle enough even for the visualization of supported lipid bilayers or adsorbed single polymer molecules (for instance, 0.4 nm thick chains of synthetic polyelectrolytes) under liquid medium. With proper scanning parameters, the conformation of single molecules can remain unchanged for hours, and even single molecular motors can be imaged while moving.\n\nWhen operating in tapping mode, the phase of the cantilever's oscillation with respect to the driving signal can be recorded as well. This signal channel contains information about the energy dissipated by the cantilever in each oscillation cycle. Samples that contain regions of varying stiffness or with different adhesion properties can give a contrast in this channel that is not visible in the topographic image. Extracting the sample's material properties in a quantitative manner from phase images, however, is often not feasible.\n\nIn non-contact atomic force microscopy mode, the tip of the cantilever does not contact the sample surface. The cantilever is instead oscillated at either its resonant frequency (frequency modulation) or just above (amplitude modulation) where the amplitude of oscillation is typically a few nanometers (<10 nm) down to a few picometers. The van der Waals forces, which are strongest from 1 nm to 10 nm above the surface, or any other long-range force that extends above the surface acts to decrease the resonance frequency of the cantilever. This decrease in resonant frequency combined with the feedback loop system maintains a constant oscillation amplitude or frequency by adjusting the average tip-to-sample distance. Measuring the tip-to-sample distance at each (x,y) data point allows the scanning software to construct a topographic image of the sample surface.\n\nNon-contact mode AFM does not suffer from tip or sample degradation effects that are sometimes observed after taking numerous scans with contact AFM. This makes non-contact AFM preferable to contact AFM for measuring soft samples, e.g. biological samples and organic thin film. In the case of rigid samples, contact and non-contact images may look the same. However, if a few monolayers of adsorbed fluid are lying on the surface of a rigid sample, the images may look quite different. An AFM operating in contact mode will penetrate the liquid layer to image the underlying surface, whereas in non-contact mode an AFM will oscillate above the adsorbed fluid layer to image both the liquid and surface.\n\nSchemes for dynamic mode operation include frequency modulation where a phase-locked loop is used to track the cantilever's resonance frequency and the more common amplitude modulation with a servo loop in place to keep the cantilever excitation to a defined amplitude. In frequency modulation, changes in the oscillation frequency provide information about tip-sample interactions. Frequency can be measured with very high sensitivity and thus the frequency modulation mode allows for the use of very stiff cantilevers. Stiff cantilevers provide stability very close to the surface and, as a result, this technique was the first AFM technique to provide true atomic resolution in ultra-high vacuum conditions.\n\nIn amplitude modulation, changes in the oscillation amplitude or phase provide the feedback signal for imaging. In amplitude modulation, changes in the phase of oscillation can be used to discriminate between different types of materials on the surface. Amplitude modulation can be operated either in the non-contact or in the intermittent contact regime. In dynamic contact mode, the cantilever is oscillated such that the separation distance between the cantilever tip and the sample surface is modulated.\n\nAmplitude modulation has also been used in the non-contact regime to image with atomic resolution by using very stiff cantilevers and small amplitudes in an ultra-high vacuum environment.\n\nImage formation is a plotting method that produces a color mapping through changing the x-y position of the tip while scanning and recording the measured variable, i.e. the intensity of control signal, to each x-y coordinate. The color mapping shows the measured value corresponding to each coordinate. The image expresses the intensity of a value as a hue. Usually, the correspondence between the intensity of a value and a hue is shown as a color scale in the explanatory notes accompanying the image.\n\nOperation mode of image forming of the AFM are generally classified into two groups from the viewpoint whether it uses z-Feedback loop (not shown) to maintain the tip-sample distance to keep signal intensity exported by the detector. The first one (using z-Feedback loop), said to be \"constant XX mode\" (XX is something which kept by z-Feedback loop).\n\nTopographic image formation mode is based on abovementioned \"constant XX mode\", z-Feedback loop controls the relative distance between the probe and the sample through outputting control signals to keep constant one of frequency, vibration and phase which typically corresponds to the motion of cantilever (for instance, voltage is applied to the Z-piezoelectric element and it moves the sample up and down towards the Z direction.\n\nDetails will be explained in the case that especially \"constant df mode\"(FM-AFM) among AFM as an instance in next section.\n\nWhen the distance between the probe and the sample is brought to the range where atomic force may be detected, while a cantilever is excited in its natural eigen frequency (f), a phenomenon occurs that the resonance frequency (f) of the cantilever shifts from its original resonance frequency (natural eigen frequency). In other words, in the range where atomic force may be detected, the frequency shift (df=f-f) will be observed. So, when the distance between the probe and the sample is in the non-contact region, the frequency shift increases in negative direction as the distance between the probe and the sample gets smaller.\n\nWhen the sample has concavity and convexity, the distance between the tip-apex and the sample varies in accordance with the concavity and convexity accompanied with a scan of the sample along x-y direction (without height regulation in z-direction) . As a result, the frequency shift arises. The image in which the values of the frequency obtained by a raster scan along the x-y direction of the sample surface are plotted against the x-y coordination of each measurement point is called a constant-height image.\n\nOn the other hand, the df may be kept constant by moving the probe upward and downward (See (3) of FIG.5) in z-direction using a negative feedback (by using z-feedback loop) while the raster scan of the sample surface along the x-y direction . The image in which the amounts of the negative feedback (the moving distance of the probe upward and downward in z-direction) are plotted against the x-y coordination of each measurement point is a topographic image. In other words, the topographic image is a trace of the tip of the probe regulated so that the df is constant and it may also be considered to be a plot of a constant-height surface of the df.\n\nTherefore, the topographic image of the AFM is not the exact surface morphology itself, but actually the image influenced by the bond-order between the probe and the sample, however, the topographic image of the AFM is considered to reflect the geographical shape of the surface more than the topographic image of a scanning tunnel microscope.\n\nAnother major application of AFM (besides imaging) is force spectroscopy, the direct measurement of tip-sample interaction forces as a function of the gap between the tip and sample (the result of this measurement is called a force-distance curve). For this method, the AFM tip is extended towards and retracted from the surface as the deflection of the cantilever is monitored as a function of piezoelectric displacement. These measurements have been used to measure nanoscale contacts, atomic bonding, Van der Waals forces, and Casimir forces, dissolution forces in liquids and single molecule stretching and rupture forces. Furthermore, AFM was used to measure, in an aqueous environment, the dispersion force due to polymer adsorbed on the substrate. Forces of the order of a few piconewtons can now be routinely measured with a vertical distance resolution of better than 0.1 nanometers. Force spectroscopy can be performed with either static or dynamic modes. In dynamic modes, information about the cantilever vibration is monitored in addition to the static deflection.\n\nProblems with the technique include no direct measurement of the tip-sample separation and the common need for low-stiffness cantilevers, which tend to 'snap' to the surface. These problems are not insurmountable. An AFM that directly measures the tip-sample separation has been developed. The snap-in can be reduced by measuring in liquids or by using stiffer cantilevers, but in the latter case a more sensitive deflection sensor is needed. By applying a small dither to the tip, the stiffness (force gradient) of the bond can be measured as well.\n\nForce spectroscopy is used in biophysics to measure the mechanical properties. of living material (such as tissue or cells) or detect structures of different stiffness buried into the bulk of the sample using the stiffness tomography. Another application was to measure the interaction forces between from one hand a material stuck on the tip of the cantilever, and from another hand the surface of particles either free or occupied by the same material. From the adhesion force distribution curve, a mean value of the forces has been derived. It allowed to make a cartography of the surface of the particles, covered or not by the material.\n\nThe AFM can be used to image and manipulate atoms and structures on a variety of surfaces. The atom at the apex of the tip \"senses\" individual atoms on the underlying surface when it forms incipient chemical bonds with each atom. Because these chemical interactions subtly alter the tip's vibration frequency, they can be detected and mapped. This principle was used to distinguish between atoms of silicon, tin and lead on an alloy surface, by comparing these 'atomic fingerprints' to values obtained from large-scale density functional theory (DFT) simulations.\n\nThe trick is to first measure these forces precisely for each type of atom expected in the sample, and then to compare with forces given by DFT simulations. The team found that the tip interacted most strongly with silicon atoms, and interacted 24% and 41% less strongly with tin and lead atoms, respectively. Thus, each different type of atom can be identified in the matrix as the tip is moved across the surface.\n\nAn AFM probe has a sharp tip on the free-swinging end of a cantilever that is protruding from a holder. The dimensions of the cantilever are in the scale of micrometers. The radius of the tip is usually on the scale of a few nanometers to a few tens of nanometers. (Specialized probes exist with much larger end radii, for example probes for indentation of soft materials.) The cantilever holder, also called holder chip – often 1.6 mm by 3.4 mm in size – allows the operator to hold the AFM cantilever/probe assembly with tweezers and fit it into the corresponding holder clips on the scanning head of the atomic force microscope.\n\nThis device is most commonly called an \"AFM probe\", but other names include \"AFM tip\" and \"cantilever\" (employing the name of a single part as the name of the whole device). An AFM probe is a particular type of SPM (scanning probe microscopy) probe.\n\nAFM probes are manufactured with MEMS technology. Most AFM probes used are made from silicon (Si), but borosilicate glass and silicon nitride are also in use. AFM probes are considered consumables as they are often replaced when the tip apex becomes dull or contaminated or when the cantilever is broken. They can cost from a couple of tens of dollars up to hundreds of dollars per cantilever for the most specialized cantilever/probe combinations.\n\nJust the tip is brought very close to the surface of the object under investigation, the cantilever is deflected by the interaction between the tip and the surface, which is what the AFM is designed to measure. A spatial map of the interaction can be made by measuring the deflection at many points on a 2D surface.\n\nSeveral types of interaction can be detected. Depending on the interaction under investigation, the surface of the tip of the AFM probe needs to be modified with a coating. Among the coatings used are gold – for covalent bonding of biological molecules and the detection of their interaction with a surface, diamond for increased wear resistance and magnetic coatings for detecting the magnetic properties of the investigated surface. Another solution exists to achieve high resolution magnetic imaging : having the probe equip with a microSQUID. The AFM tips is fabricated using silicon micro machining and the precise positioning of the microSQUID loop is done by electron beam lithography.\n\nThe surface of the cantilevers can also be modified. These coatings are mostly applied in order to increase the reflectance of the cantilever and to improve the deflection signal.\n\nThe forces between the tip and the sample strongly depend on the geometry of the tip. Various studies were exploited in the past years to write the forces as a function of the tip parameters.\n\nAmong the different forces between the tip and the sample, the water meniscus forces are highly interesting, both in air and liquid environment. Other forces must be considered, like the Coulomb force, van der Waals forces, double layer interactions, solvation forces, hydration and hydrophobic forces.\n\nWater meniscus forces are highly interesting for AFM measurements in air. Due to the ambient humidity, a thin layer of water is formed between the tip and the sample during air measumements. The resulting capillary force gives rise to a strong attractive force that pulls the tip onto the surface. In fact, the adhesion force measured between tip and sample in ambient air of finite humidity is usually dominated by capillary forces. As a consequence, it is difficult to pull the tip away from the surface. For soft samples including many polymers and in particular biological materials, the strong adhesive capillary force gives rise to sample degradation and destruction upon imaging in contact mode. Historically, these problems were an important motivation for the development of dynamic imaging in air (e.g. 'tapping mode'). During tapping mode imaging in air, capillary bridges still form. Yet, for suitable imaging conditions, the capillary bridges are formed and broken in every oscillation cycle of the cantilever normal to the surface, as can be inferred from a analysis of cantilever amplitude and phase vs. distance curves. As a consequence, destructive shear forces are largely reduced and soft samples can be investigated.\n\nIn order to quantify the equilibrium capillary force, it is necessary to start from the Laplace equation for pressure:\n\nformula_1\n\nwhere γ is the surface energy and r and r are defined in the figure.\n\nThe pressure is applied on an area of \n\nformula_2\n\nwhere d, θ , and h are defined in the figure.\n\nThe force which pulles together the two surfaces is\n\nformula_3\n\nThe same formula could also be calculated as a function of relative humidity.\n\nGao calculated formulas for different tip geometries. As an example, the forse decreases by 20% for a conical tip with respect to a spherical tip.\n\nWhen these forces are calculated, a difference must be made between the wet on dry situation and the wet on wet situation.\n\nFor a spherical tip, the force is:\n\nformula_4 for dry on wet\n\nformula_5for wet on wet\n\nwhere θ is the contact angle of the dry sphere and φ is the immersed angle, as shown in the figure Also R,h and D are illustrated in the same figure.\n\nFor a conical tip, the formula becomes:\n\nformula_6 for dry on wet\n\nformula_7 for wet on wet\n\nwhere δ is the half cone angle and r and h are parameters of the meniscus profile.\n\nThe most common method for cantilever-deflection measurements is the beam-deflection method. In this method, laser light from a solid-state diode is reflected off the back of the cantilever and collected by a position-sensitive detector (PSD) consisting of two closely spaced photodiodes, whose output signal is collected by a differential amplifier.\nAngular displacement of the cantilever results in one photodiode collecting more light than the other photodiode, producing an output signal (the difference between the photodiode signals normalized by their sum), which is proportional to the deflection of the cantilever. The sensitivity of the beam-deflection method is very high, a noise floor on the order of 10 fm Hz can be obtained routinely in a well-designed system. Although this method is sometimes called the 'optical lever' method, the signal is not amplified if the beam path is made longer. A longer beam path increases the motion of the reflected spot on the photodiodes, but also widens the spot by the same amount due to diffraction, so that the same amount of optical power is moved from one photodiode to the other. The 'optical leverage' (output signal of the detector divided by deflection of the cantilever) is inversely proportional to the numerical aperture of the beam focusing optics, as long as the focused laser spot is small enough to fall completely on the cantilever. It is also inversely proportional to the length of the cantilever.\n\nThe relative popularity of the beam-deflection method can be explained by its high sensitivity and simple operation, and by the fact that cantilevers do not require electrical contacts or other special treatments, and can therefore be fabricated relatively cheaply with sharp integrated tips.\n\nMany other methods for beam-deflection measurements exist. \n\nAFM scanners are made from piezoelectric material, which expands and contracts proportionally to an applied voltage. Whether they elongate or contract depends upon the polarity of the voltage applied. Traditionally the tip or sample is mounted on a 'tripod' of three piezo crystals, with each responsible for scanning in the \"x\",\"y\" and \"z\" directions. In 1986, the same year as the AFM was invented, a new piezoelectric scanner, the tube scanner, was developed for use in STM. Later tube scanners were incorporated into AFMs. The tube scanner can move the sample in the \"x\", \"y\", and \"z\" directions using a single tube piezo with a single interior contact and four external contacts. An advantage of the tube scanner compared to the original tripod design, is better vibrational isolation, resulting from the higher resonant frequency of the single element construction, in combination with a low resonant frequency isolation stage. A disadvantage is that the \"x\"-\"y\" motion can cause unwanted \"z\" motion resulting in distortion. Another popular design for AFM scanners is the flexure stage, which uses separate piezos for each axis, and couples them through a flexure mechanism.\n\nScanners are characterized by their sensitivity, which is the ratio of piezo movement to piezo voltage, i.e., by how much the piezo material extends or contracts per applied volt. Because of differences in material or size, the sensitivity varies from scanner to scanner. Sensitivity varies non-linearly with respect to scan size. Piezo scanners exhibit more sensitivity at the end than at the beginning of a scan. This causes the forward and reverse scans to behave differently and display hysteresis between the two scan directions. This can be corrected by applying a non-linear voltage to the piezo electrodes to cause linear scanner movement and calibrating the scanner accordingly. One disadvantage of this approach is that it requires re-calibration because the precise non-linear voltage needed to correct non-linear movement will change as the piezo ages (see below). This problem can be circumvented by adding a linear sensor to the sample stage or piezo stage to detect the true movement of the piezo. Deviations from ideal movement can be detected by the sensor and corrections applied to the piezo drive signal to correct for non-linear piezo movement. This design is known as a 'closed loop' AFM. Non-sensored piezo AFMs are referred to as 'open loop' AFMs.\n\nThe sensitivity of piezoelectric materials decreases exponentially with time. This causes most of the change in sensitivity to occur in the initial stages of the scanner's life. Piezoelectric scanners are run for approximately 48 hours before they are shipped from the factory so that they are past the point where they may have large changes in sensitivity. As the scanner ages, the sensitivity will change less with time and the scanner would seldom require recalibration, though various manufacturer manuals recommend monthly to semi-monthly calibration of open loop AFMs.\n\nAFM has several advantages over the scanning electron microscope (SEM). Unlike the electron microscope, which provides a two-dimensional projection or a two-dimensional image of a sample, the AFM provides a three-dimensional surface profile. In addition, samples viewed by AFM do not require any special treatments (such as metal/carbon coatings) that would irreversibly change or damage the sample, and does not typically suffer from charging artifacts in the final image. While an electron microscope needs an expensive vacuum environment for proper operation, most AFM modes can work perfectly well in ambient air or even a liquid environment. This makes it possible to study biological macromolecules and even living organisms. In principle, AFM can provide higher resolution than SEM. It has been shown to give true atomic resolution in ultra-high vacuum (UHV) and, more recently, in liquid environments. High resolution AFM is comparable in resolution to scanning tunneling microscopy and transmission electron microscopy. AFM can also be combined with a variety of optical microscopy and spectroscopy techniques such as fluorescent microscopy of infrared spectroscopy, giving rise to scanning near-field optical microscopy, nano-FTIR and further expanding its applicability. Combined AFM-optical instruments have been applied primarily in the biological sciences but have recently attracted strong interest in photovoltaics and energy-storage research, polymer sciences, nanotechnology and even medical research.\n\nA disadvantage of AFM compared with the scanning electron microscope (SEM) is the single scan image size. In one pass, the SEM can image an area on the order of square millimeters with a depth of field on the order of millimeters, whereas the AFM can only image a maximum scanning area of about 150×150 micrometers and a maximum height on the order of 10-20 micrometers. One method of improving the scanned area size for AFM is by using parallel probes in a fashion similar to that of millipede data storage.\n\nThe scanning speed of an AFM is also a limitation. Traditionally, an AFM cannot scan images as fast as an SEM, requiring several minutes for a typical scan, while an SEM is capable of scanning at near real-time, although at relatively low quality. The relatively slow rate of scanning during AFM imaging often leads to thermal drift in the image making the AFM less suited for measuring accurate distances between topographical features on the image. However, several fast-acting designs were suggested to increase microscope scanning productivity including what is being termed videoAFM (reasonable quality images are being obtained with videoAFM at video rate: faster than the average SEM). To eliminate image distortions induced by thermal drift, several methods have been introduced.\n\nAFM images can also be affected by nonlinearity, hysteresis, and creep of the piezoelectric material and cross-talk between the \"x\", \"y\", \"z\" axes that may require software enhancement and filtering. Such filtering could \"flatten\" out real topographical features. However, newer AFMs utilize real-time correction software (for example, feature-oriented scanning) or closed-loop scanners, which practically eliminate these problems. Some AFMs also use separated orthogonal scanners (as opposed to a single tube), which also serve to eliminate part of the cross-talk problems.\n\nAs with any other imaging technique, there is the possibility of image artifacts, which could be induced by an unsuitable tip, a poor operating environment, or even by the sample itself, as depicted on the right. These image artifacts are unavoidable; however, their occurrence and effect on results can be reduced through various methods.\nArtifacts resulting from a too-coarse tip can be caused for example by inappropriate handling or de facto collisions with the sample by either scanning too fast or having an unreasonably rough surface, causing actual wearing of the tip.\nDue to the nature of AFM probes, they cannot normally measure steep walls or overhangs. Specially made cantilevers and AFMs can be used to modulate the probe sideways as well as up and down (as with dynamic contact and non-contact modes) to measure sidewalls, at the cost of more expensive cantilevers, lower lateral resolution and additional artifacts.\n\nThe latest efforts in integrating nanotechnology and biological research have been successful and show much promise for the future. Since nanoparticles are a potential vehicle of drug delivery, the biological responses of cells to these nanoparticles are continuously being explored to optimize their efficacy and how their design could be improved. Pyrgiotakis et al. were able to study the interaction between CeO and FeO engineered nanoparticles and cells by attaching the engineered nanoparticles to the AFM tip. Studies have taken advantage of AFM to obtain further information on the behavior of live cells in biological media. Real-time atomic force spectroscopy (or nanoscopy) and dynamic atomic force spectroscopy have been used to study live cells and membrane proteins and their dynamic behavior at high resolution, on the nanoscale. Imaging and obtaining information on the topography and the properties of the cells has also given insight into chemical processes and mechanisms that occur through cell-cell interaction and interactions with other signaling molecules (ex. ligands). Evans and Calderwood used single cell force microscopy to study cell adhesion forces, bond kinetics/dynamic bond strength and its role in chemical processes such as cell signaling. \nScheuring, Lévy, and Rigaud reviewed studies in which AFM to explore the crystal structure of membrane proteins of photosynthetic bacteria.\nAlsteen et al. have used AFM-based nanoscopy to perform a real-time analysis of the interaction between live mycobacteria and antimycobacterial drugs (specifically isoniazid, ethionamide, ethambutol, and streptomycine), which serves as an example of the more in-depth analysis of pathogen-drug interactions that can be done through AFM.\n\n\n"}
{"id": "53479461", "url": "https://en.wikipedia.org/wiki?curid=53479461", "title": "Australian Geoscience Data Cube", "text": "Australian Geoscience Data Cube\n\nThe Australian Geoscience Data Cube (AGDC) is an approach to storing, processing and analyzing large collections of earth observation data. The technology is designed to meet challenges of national interest by being agile and flexible with vast amounts of layered grid data.\n\nThe AGDC reduces processing time of traditional image analysis by calibrating, pre-computing known extents, pixel alignment and storing metadata in a cell lattice structure. The temporal-pixel aligned data can often be analysed faster across space and time dimensions than previous scene based techniques. This allows the AGDC to be flexible in tackling future challenges and improve analysis times on \"every-increasing\" data repositories of earth observation.\n\nThe AGDC has also been used internationally to allow countries to maintain ecologically sustainable programs and reduce the difficultly curve of utilizing Remote Sensing data.\n\nThe AGDC was originally conceived by Geoscience Australia but is now maintained in a partnership between Geoscience Australia, Commonwealth Scientific and Industrial Research Organisation (CSIRO) and National Computational Infrastructure National Facility (Australia) (NCI). This is made possible by the funding from the partnership and a number of organisations such as National Collaborative Research Infrastructure Strategy (NCRIS).\n\nThe data processed in the cube is made \"analysis ready\" before being ingested and indexed into the AGDC. Analysis ready data is pre-processed data that has applied corrections for instrument calibration (gains and offsets), geolocation (spatial alignment) and radiometry (solar illumination, incidence angle, topography, atmospheric interference). The ingestion process manages the translation of datasets into the storage units while maintaining a database index. The data within the storage and index can be accessed via API calls often compiled within code such as Python (programming language).\n\nExample\n\n\"s2a_l1c = dc.load(product='s2a_level1c_granule',x=(147.36, 147.41), y=(-35.1, -35.15), measurements=['04','03','02'], output_crs='EPSG:4326', resolution=(-0.00025,0.00025))\"\n\n\n\nThe AGDC code base is situated in GitHub as an open repository. The core code base moved to the \"Open Data Cube\" in early 2017 as part of an international collaboration. Whilst the code base is the \"Open Data Cube\", individual cubes exist as their own right such as the AGDC on the National Computational Infrastructure National Facility (Australia) (NCI) using the High-Performance Computing Cluster HPCC. The core code can be installed on personal computers or public computers (using git) and has many unit tests.\n\nDocumentation for the code base exists on Read the Docs.\n\nThe AGDC is designed to meet nationally significant challenges such as the following.\n\n\nThe AGDC won the 2016 Content Platform of the Year award from Geospatial World Forum.\n"}
{"id": "23045765", "url": "https://en.wikipedia.org/wiki?curid=23045765", "title": "Berthold Ribbentrop", "text": "Berthold Ribbentrop\n\nBerthold Ribbentrop was a pioneering forester from Germany who worked in India with Sir Dietrich Brandis and others. He is said to have inspired Rudyard Kipling's character of Muller in \"In the Rukh\" (1893), one of the earliest of his \"Jungle Book\" stories.\n\nBerthold Ribbentrop was Inspector-general of Forests to the Government of India from 1885. In 1900 he wrote \"Forestry in British India\", in which he wrote that he was coming to the end of his career. He described the early lack of forestry expertise among the British administrators of India, and wrote\n"}
{"id": "10669579", "url": "https://en.wikipedia.org/wiki?curid=10669579", "title": "C-Repeat Binding Factor", "text": "C-Repeat Binding Factor\n\nC-Repeat Binding Factors (CBFs) are transcription factors in plants involved in response to low temperature. \nAlso known as Dehydration Response Element Binding factors (DREBs), they are a subfamily of AP2 DNA binding domain transcription factors.\n\n"}
{"id": "7003", "url": "https://en.wikipedia.org/wiki?curid=7003", "title": "Cauchy distribution", "text": "Cauchy distribution\n\nThe Cauchy distribution, named after Augustin Cauchy, is a continuous probability distribution. It is also known, especially among physicists, as the Lorentz distribution (after Hendrik Lorentz), Cauchy–Lorentz distribution, Lorentz(ian) function, or Breit–Wigner distribution. The Cauchy distribution formula_1 is the distribution of the \"x\"-intercept of a ray issuing from formula_2 with a uniformly distributed angle. It is also the distribution of the ratio of two independent normally distributed random variables if the denominator distribution has mean zero.\n\nThe Cauchy distribution is often used in statistics as the canonical example of a \"pathological\" distribution since both its expected value and its variance are undefined. (But see the section \"Explanation of undefined moments\" below.) The Cauchy distribution does not have finite moments of order greater than or equal to one; only fractional absolute moments exist. The Cauchy distribution has no moment generating function.\n\nIn mathematics, it is closely related to the Poisson kernel, which is the fundamental solution for the Laplace equation in the upper half-plane. In spectroscopy, it is the description of the shape of spectral lines which are subject to homogeneous broadening in which all atoms interact in the same way with the frequency range contained in the line shape. Many mechanisms cause homogeneous broadening, most notably collision broadening.\n\nIt is one of the few distributions that is stable and has a probability density function that can be expressed analytically, the others being the normal distribution and the Lévy distribution.\n\nFunctions with the form of the density function of the Cauchy distribution were studied by mathematicians in the 17th century, but in a different context and under the title of the witch of Agnesi. Despite its name, the first explicit analysis of the properties of the Cauchy distribution was published by the French mathematician Poisson in 1824, with Cauchy only becoming associated with it during an academic controversy in 1853. As such, the name of the distribution is a case of Stigler's Law of Eponymy. Poisson noted that if the mean of observations following such a distribution were taken, the mean error did not converge to any finite number. As such, Laplace's use of the Central Limit Theorem with such a distribution was inappropriate, as it assumed a finite mean and variance. Despite this, Poisson did not regard the issue as important, in contrast to Bienaymé, who was to engage Cauchy in a long dispute over the matter.\n\nThe Cauchy distribution has the probability density function (PDF)\n\nwhere formula_4 is the location parameter, specifying the location of the peak of the distribution, and formula_5 is the scale parameter which specifies the half-width at half-maximum (HWHM), alternatively formula_6 is full width at half maximum (FWHM). formula_5 is also equal to half the interquartile range and is sometimes called the probable error. Augustin-Louis Cauchy exploited such a density function in 1827 with an infinitesimal scale parameter, defining what would now be called a Dirac delta function.\n\nThe maximum value or amplitude of the Cauchy PDF is formula_8, located at formula_9.\n\nIt is sometimes convenient to express the PDF in terms of the complex parameter formula_10\n\nThe special case when formula_12 and formula_13 is called the standard Cauchy distribution with the probability density function\n\nIn physics, a three-parameter Lorentzian function is often used:\nwhere formula_16 is the height of the peak. The three-parameter Lorentzian function indicated is not, in general, a probability density function, since it does not integrate to 1, except in the special case where formula_17\n\nThe cumulative distribution function of the Cauchy distribution is:\n\nand the quantile function (inverse cdf) of the Cauchy distribution is\nIt follows that the first and third quartiles are formula_20, and hence the interquartile range is formula_6.\n\nFor the standard distribution, the cumulative distribution function simplifies to arctangent function formula_22:\n\nThe entropy of the Cauchy distribution is given by:\n\nThe derivative of the quantile function, the quantile density function, for the Cauchy distribution is:\n\nThe differential entropy of a distribution can be defined in terms of its quantile density, specifically:\n\nThe Cauchy distribution is the maximum entropy probability distribution for a random variate formula_27 for which \n\nor, alternatively, for a random variate formula_27 for which \n\nIn its standard form, it is the maximum entropy probability distribution for a random variate formula_27 for which \n\nThe Cauchy distribution is an example of a distribution which has no mean, variance or higher moments defined. Its mode and median are well defined and are both equal to formula_4.\n\nWhen formula_34 and formula_35 are two independent normally distributed random variables with expected value 0 and variance 1, then the ratio formula_36 has the standard Cauchy distribution.\n\nIf formula_37 is a formula_38 positive-semidefinite covariance matrix with strictly positive diagonal entries, then for independent and identically distributed formula_39 and any random formula_40-vector formula_41 independent of formula_27 and formula_43 such that formula_44 and formula_45 (defining a categorical distribution) it holds that\n\nIf formula_47 are independent and identically distributed random variables, each with a standard Cauchy distribution, then the sample mean formula_48 has the same standard Cauchy distribution. To see that this is true, compute the characteristic function of the sample mean:\n\nwhere formula_50 is the sample mean. This example serves to show that the hypothesis of finite variance in the central limit theorem cannot be dropped. It is also an example of a more generalized version of the central limit theorem that is characteristic of all stable distributions, of which the Cauchy distribution is a special case.\n\nThe Cauchy distribution is an infinitely divisible probability distribution. It is also a strictly stable distribution.\n\nThe standard Cauchy distribution coincides with the Student's \"t\"-distribution with one degree of freedom.\n\nLike all stable distributions, the location-scale family to which the Cauchy distribution belongs is closed under linear transformations with real coefficients. In addition, the Cauchy distribution is closed under linear fractional transformations with real coefficients. In this connection, see also McCullagh's parametrization of the Cauchy distributions.\n\nLet formula_27 denote a Cauchy distributed random variable. The characteristic function of the Cauchy distribution is given by\n\nwhich is just the Fourier transform of the probability density. The original probability density may be expressed in terms of the characteristic function, essentially by using the inverse Fourier transform:\n\nThe \"n\"th moment of a distribution is the \"n\"th derivative of the characteristic function evaluated at formula_54. Observe that the characteristic function is not differentiable at the origin: this corresponds to the fact that the Cauchy distribution does not have well-defined moments higher than the zeroth moment.\n\nIf a probability distribution has a density function formula_55, then the mean, if it exists, is given by\n\nWe may evaluate this two-sided improper integral by computing the sum of two one-sided improper integrals. That is,\nfor an arbitrary real number formula_58.\n\nFor the integral to exist (even as an infinite value), at least one of the terms in this sum should be finite, or both should be infinite and have the same sign. But in the case of the Cauchy distribution, both the terms in this sum (2) are infinite and have opposite sign. Hence (1) is undefined, and thus so is the mean.\n\nNote that the Cauchy principal value of the Cauchy distribution is\n\nwhich is zero. On the other hand, the related integral\n\nis \"not\" zero, as can be seen easily by computing the integral. This again shows that the mean (1) can not exist.\n\nVarious results in probability theory about expected values, such as the strong law of large numbers, fail to hold for the Cauchy distribution.\n\nThe Cauchy distribution does not have finite moments of any order. Some of the higher raw moments do exist and have a value of infinity, for example the raw second moment:\n\nBy re-arranging the formula, one can see that the second moment is essentially the infinite integral of a constant (here 1). Higher even-powered raw moments will also evaluate to infinity. Odd-powered raw moments, however, are undefined, which is distinctly different from existing with the value of infinity. The odd-powered raw moments are undefined because their values are essentially equivalent to formula_62 since the two halves of the integral both diverge and have opposite signs. The first raw moment is the mean, which, being odd, does not exist. (See also the discussion above about this.) This in turn means that all of the central moments and standardized moments are undefined, since they are all based on the mean. The variance—which is the second central moment—is likewise non-existent (despite the fact that the raw second moment exists with the value infinity).\n\nThe results for higher moments follow from Hölder's inequality, which implies that higher moments (or halves of moments) diverge if lower ones do.\n\nBecause the parameters of the Cauchy distribution do not correspond to a mean and variance, attempting to estimate the parameters of the Cauchy distribution by using a sample mean and a sample variance will not succeed. For example, if an i.i.d. sample of size \"n\" is taken from a Cauchy distribution, one may calculate the sample mean as:\n\nAlthough the sample values formula_64 will be concentrated about the central value formula_4, the sample mean will become increasingly variable as more observations are taken, because of the increased probability of encountering sample points with a large absolute value. In fact, the distribution of the sample mean will be equal to the distribution of the observations themselves; i.e., the sample mean of a large sample is no better (or worse) an estimator of formula_4 than any single observation from the sample. Similarly, calculating the sample variance will result in values that grow larger as more observations are taken.\n\nTherefore, more robust means of estimating the central value formula_4 and the scaling parameter formula_5 are needed. One simple method is to take the median value of the sample as an estimator of formula_4 and half the sample interquartile range as an estimator of formula_5. Other, more precise and robust methods have been developed For example, the truncated mean of the middle 24% of the sample order statistics produces an estimate for formula_4 that is more efficient than using either the sample median or the full sample mean. However, because of the fat tails of the Cauchy distribution, the efficiency of the estimator decreases if more than 24% of the sample is used.\n\nMaximum likelihood can also be used to estimate the parameters formula_4 and formula_5. However, this tends to be complicated by the fact that this requires finding the roots of a high degree polynomial, and there can be multiple roots that represent local maxima. Also, while the maximum likelihood estimator is asymptotically efficient, it is relatively inefficient for small samples. The log-likelihood function for the Cauchy distribution for sample size formula_74 is:\n\nMaximizing the log likelihood function with respect to formula_4 and formula_5 produces the following system of equations:\n\nNote that\n\nis a monotone function in formula_5 and that the solution formula_5 must satisfy\n\nSolving just for formula_4 requires solving a polynomial of degree formula_85, and solving just for formula_5 requires solving a polynomial of degree formula_74 (first for formula_88, then formula_4). Therefore, whether solving for one parameter or for both parameters simultaneously, a numerical solution on a computer is typically required. The benefit of maximum likelihood estimation is asymptotic efficiency; estimating formula_4 using the sample median is only about 81% as asymptotically efficient as estimating formula_4 by maximum likelihood. The truncated sample mean using the middle 24% order statistics is about 88% as asymptotically efficient an estimator of formula_4 as the maximum likelihood estimate. When Newton's method is used to find the solution for the maximum likelihood estimate, the middle 24% order statistics can be used as an initial solution for formula_4.\n\nA random vector formula_94 is said to have the multivariate Cauchy distribution if every linear combination of its components formula_95 has a Cauchy distribution. That is, for any constant vector formula_96, the random variable formula_97 should have a univariate Cauchy distribution. The characteristic function of a multivariate Cauchy distribution is given by:\n\nwhere formula_99 and formula_100 are real functions with formula_99 a homogeneous function of degree one and formula_100 a positive homogeneous function of degree one. More formally:\n\nfor all formula_105.\n\nAn example of a bivariate Cauchy distribution can be given by:\nNote that in this example, even though there is no analogue to a covariance matrix, formula_107 and formula_108 are not statistically independent.\n\nWe also can write this formula for complexe variable. Then the probability density function of complexe cauchy is :\n\nformula_109\n\nAnalogous to the univariate density, the multidimensional Cauchy density also relates to the multivariate Student distribution. They are equivalent when the degrees of freedom parameter is equal to one. The density of a formula_110 dimension Student distribution with one degree of freedom becomes:\n\nProperties and details for this density can be obtained by taking it as a particular case of the multivariate Student density.\n\nwhere formula_58, formula_126, formula_127 and formula_128 are real numbers.\n\nThe Cauchy distribution is the stable distribution of index 1. The Lévy–Khintchine representation of such a stable distribution of parameter formula_132 is given, for formula_133 by:\n\nwhere\n\nand formula_136 can be expressed explicitly. In the case formula_137 of the Cauchy distribution, one has formula_138.\n\nThis last representation is a consequence of the formula\n\n\nIn nuclear and particle physics, the energy profile of a resonance is described by the relativistic Breit–Wigner distribution, while the Cauchy distribution is the (non-relativistic) Breit–Wigner distribution.\n\n\n\n\n"}
{"id": "8957", "url": "https://en.wikipedia.org/wiki?curid=8957", "title": "DARPA", "text": "DARPA\n\nThe Defense Advanced Research Projects Agency (DARPA) is an agency of the United States Department of Defense responsible for the development of emerging technologies for use by the military.\n\nOriginally known as the Advanced Research Projects Agency (ARPA), the agency was created in February 1958 by President Dwight D. Eisenhower in response to the Soviet launching of Sputnik 1 in 1957. By collaborating with academic, industry, and government partners, DARPA formulates and executes research and development projects to expand the frontiers of technology and science, often beyond immediate U.S. military requirements.\n\nDARPA-funded projects have provided significant technologies that influenced many non-military fields, such as computer networking and the basis for the modern Internet, and graphical user interfaces in information technology.\n\nDARPA is independent of other military research and development and reports directly to senior Department of Defense management. DARPA has about 220 employees, of whom approximately 100 are in management.\n\nThe name of the organization first changed from its founding name ARPA to DARPA in March 1972, momentarily changing back to ARPA in February 1993, only to revert to DARPA in March 1996.\n\nCurrently, DARPA's mission statement is \"to make pivotal investments in breakthrough technologies for national security\". \n\nThe creation of the Advanced Research Projects Agency (ARPA) was authorized by President Dwight D. Eisenhower in 1958 for the purpose of forming and executing research and development projects to expand the frontiers of technology and science, and able to reach far beyond immediate military requirements, the two relevant acts being the Supplemental Military Construction Authorization (Air Force) (Public Law 85-325) and Department of Defense Directive 5105.15, in February 1958. Its creation was directly attributed to the launching of Sputnik and to U.S. realization that the Soviet Union had developed the capacity to rapidly exploit military technology. Initial funding of ARPA was $520 million. ARPA's first director, Roy Johnson, left a $160,000 management job at General Electric for an $18,000 job at ARPA. Herbert York from Lawrence Livermore National Laboratory was hired as his scientific assistant.\n\nJohnson and York were both keen on space projects, but when NASA was established later in 1958 all space projects and most of ARPA's funding were transferred to it. Johnson resigned and ARPA was repurposed to do \"high-risk\", \"high-gain\", \"far out\" basic research, a posture that was enthusiastically embraced by the nation's scientists and research universities. ARPA's second director was Brigadier General Austin W. Betts, who resigned in early 1961. He was succeeded by Jack Ruina who served until 1963. Ruina, the first scientist to administer ARPA, managed to raise its budget to $250 million. It was Ruina who hired J. C. R. Licklider as the first administrator of the Information Processing Techniques Office, which played a vital role in creation of ARPANET, the basis for the future Internet.\n\nAdditionally, the political and defense communities recognized the need for a high-level Department of Defense organization to formulate and execute R&D projects that would expand the frontiers of technology beyond the immediate and specific requirements of the Military Services and their laboratories. In pursuit of this mission, DARPA has developed and transferred technology programs encompassing a wide range of scientific disciplines that address the full spectrum of national security needs.\n\nFrom 1958 to 1965, ARPA's emphasis centered on major national issues, including space, ballistic missile defense, and nuclear test detection. During 1960, all of its civilian space programs were transferred to the National Aeronautics and Space Administration (NASA) and the military space programs to the individual services.\n\nThis allowed ARPA to concentrate its efforts on the Project Defender (defense against ballistic missiles), Project Vela (nuclear test detection), and Project AGILE (counterinsurgency R&D) programs, and to begin work on computer processing, behavioral sciences, and materials sciences. The DEFENDER and AGILE programs formed the foundation of DARPA sensor, surveillance, and directed energy R&D, particularly in the study of radar, infrared sensing, and x-ray/gamma ray detection.\n\nARPA at this point (1959) played an early role in Transit (also called NavSat) a predecessor to the Global Positioning System (GPS). \"Fast-forward to 1959 when a joint effort between DARPA and the Johns Hopkins Applied Physics Laboratory began to fine-tune the early explorers’ discoveries. TRANSIT, sponsored by the Navy and developed under the leadership of Dr. Richard Kirschner at Johns Hopkins, was the first satellite positioning system.\"\n\nDuring the late 1960s, with the transfer of these mature programs to the Services, ARPA redefined its role and concentrated on a diverse set of relatively small, essentially exploratory research programs. The agency was renamed the Defense Advanced Research Projects Agency (DARPA) in 1972, and during the early 1970s, it emphasized direct energy programs, information processing, and tactical technologies.\n\nConcerning information processing, DARPA made great progress, initially through its support of the development of time-sharing (all modern operating systems rely on concepts invented for the Multics system, developed by a cooperation among Bell Labs, General Electric and MIT, which DARPA supported by funding Project MAC at MIT with an initial two-million-dollar grant).\n\nDARPA supported the evolution of the ARPANET (the first wide-area packet switching network), Packet Radio Network, Packet Satellite Network and ultimately, the Internet and research in the artificial intelligence fields of speech recognition and signal processing, including parts of Shakey the robot. DARPA also funded the development of the Douglas Engelbart's NLS computer system and The Mother of All Demos; and the Aspen Movie Map, which was probably the first hypermedia system and an important precursor of virtual reality.\n\nThe Mansfield Amendment of 1973 expressly limited appropriations for defense research (through ARPA/DARPA) only to projects with direct military application. Some contend that the amendment devastated American science, since ARPA/DARPA was a major funding source for basic science projects of the time; the National Science Foundation never made up the difference as expected.\n\nThe resulting \"brain drain\" is also credited with boosting the development of the fledgling personal computer industry. Some young computer scientists left the universities to startups and private research laboratories such as Xerox PARC.\n\nBetween 1976 and 1981, DARPA's major projects were dominated by air, land, sea, and space technology, tactical armor and anti-armor programs, infrared sensing for space-based surveillance, high-energy laser technology for space-based missile defense, antisubmarine warfare, advanced cruise missiles, advanced aircraft, and defense applications of advanced computing. These large-scale technological program demonstrations were joined by integrated circuit research, which resulted in submicrometer electronic technology and electron devices that evolved into the Very-Large-Scale Integration (VLSI) Program and the Congressionally-mandated charged particle beam program.\n\nMany of the successful programs were transitioned to the Services, such as the foundation technologies in automatic target recognition, space based sensing, propulsion, and materials that were transferred to the Strategic Defense Initiative Organization (SDIO), later known as the Ballistic Missile Defense Organization (BMDO), now titled the Missile Defense Agency (MDA).\n\nDuring the 1980s, the attention of the Agency was centered on information processing and aircraft-related programs, including the National Aerospace Plane (NASP) or Hypersonic Research Program. The Strategic Computing Program enabled DARPA to exploit advanced processing and networking technologies and to rebuild and strengthen relationships with universities after the Vietnam War. In addition, DARPA began to pursue new concepts for small, lightweight satellites (LIGHTSAT) and directed new programs regarding defense manufacturing, submarine technology, and armor/anti-armor.\n\nOn October 28, 2009 the agency broke ground on a new facility in Arlington, Virginia a few miles from the Pentagon.\n\nIn fall 2011, DARPA hosted the 100-Year Starship Symposium with the aim of getting the public to start thinking seriously about interstellar travel.\n\nOn June 5, 2016, NASA and DARPA announced that it planned to build new X-planes with NASA's plan setting to create a whole series of X planes over the next 10 years.\n\nIn July 2016, it was announced that DARPA would bring a group of top-notch computer security experts to search for security vulnerabilities and create a fix that patches those vulnerabilities and it is called the Cyber Grand Challenge (CGC).\n\nIn June 2018, DARPA leaders demonstrated a number of new technologies that were developed within the framework of the GXV-T program. The goal of this program is to create a lightly armored combat vehicle of not very large dimensions, which, due to maneuverability and other tricks, can successfully resist modern anti-tank weapon systems.\n\nDARPA has six technical offices that manage the agency's research portfolio, and two additional support offices that manage special projects and transition efforts. All offices report to the DARPA director:\n\n\nA 1991 reorganization created several offices which existed throughout the early 1990s:\nReorganization in 2010 merged two offices:\n\nA list of DARPA's active and archived projects is available on the agency's website. Because of the agency's fast pace, programs constantly start and stop based on the needs of the U.S. government. Structured information about some of the DARPA's contracts and projects is publicly available.\n\n"}
{"id": "42868443", "url": "https://en.wikipedia.org/wiki?curid=42868443", "title": "Desulfurobacterium thermolithotrophum", "text": "Desulfurobacterium thermolithotrophum\n\nDesulfurobacterium thermolithotrophum is a species of autotrophic, sulphur-reducing bacterium isolated from a deep-sea hydrothermal vent. It is the type species of its genus, being thermophilic, anaerobic, Gram-negative, motile and rod-shaped, with type strain BSA (= DSM 11699).\n\n\n"}
{"id": "3156210", "url": "https://en.wikipedia.org/wiki?curid=3156210", "title": "Dollar glut", "text": "Dollar glut\n\nThe dollar glut is a term for the accumulation of American dollars outside of the United States as a reserve currency, contrasted with the dollar gap, which led to the creation of the Marshall Plan following World War II. The eventual shift to a dollar glut forced the end of the gold standard in the United States and led to the collapse of the Bretton Woods system.\n\nThe stability of the Bretton Woods system came to depend upon the ability of the US government to exchange dollars for gold at $35 an ounce. The American ability to fulfill this commitment began to diminish as the postwar dollar shortage was transformed into an overabundance of dollars, also known as the dollar glut.\n"}
{"id": "53851281", "url": "https://en.wikipedia.org/wiki?curid=53851281", "title": "Engelier", "text": "Engelier\n\nEngelier is a 310-mile (500-kilometers) large crater on Saturn's moon Iapetus.\n"}
{"id": "49636283", "url": "https://en.wikipedia.org/wiki?curid=49636283", "title": "Florencite-(Sm)", "text": "Florencite-(Sm)\n\nFlorencite-(Sm) is a very rare mineral of the plumbogummite group (alunite supergroup) with simplified formula SmAl(PO)(OH). Samarium in florencite-(Sm) is substituted by other rare earth elements, mostly neodymium. It does not form separate crystals, but is found as zones in florencite-(Ce), which is cerium-dominant member of the plumbogummite group. Florencite-(Sm) is also a samarium-analogue of florencite-(La) (lanthanum-dominant) and waylandite (bismuth-dominant), both being aluminium-rich minerals.\n\nFlorencite-(Sm) was revealed in quartz veins in the Maldynyrd Range, Subpolar Urals, Russia. It associates with xenotime-(Y).\n\nFlorencite-(Sm) has admixtures of neodymium, and small amounts of cerium, gadolinium, sulfur, strontium, praseodymium, calcium, lanthanum, europium, and silicon.\n"}
{"id": "31226470", "url": "https://en.wikipedia.org/wiki?curid=31226470", "title": "Frank D. Fletcher", "text": "Frank D. Fletcher\n\nFrank D. Fletcher was a sailor, mainly known for his time as Chief Officer of the SY \"Aurora\" during the 1911-1914 Australasian Antarctic Expedition, under Captain John King Davis. Fletcher replaced N. C. Toutcher—who had been Chief Officer during \"Aurora\"<nowiki>'</nowiki>s first voyage of the expedition—for the second Antarctic voyage, and the spring and winter sub-Antarctic voyages of 1912 and 1913. In his 1962 book \"High latitude\", Davis described Fletcher as \"a most efficient and conscientious officer and seaman who at first sight might have been taken for the prototype of the perfect 'Bucko', that semi-legendary figure sometimes described as having 'a jaw like a sea boot'.\" Fletcher left the \"Aurora\" in 1913 to join a coastal shipping company, presumably in New Zealand, where he was discharged.\n\nExpedition Commander Douglas Mawson named Fletcher Island, near Commonwealth Bay in Antarctica, after him.\n"}
{"id": "24188246", "url": "https://en.wikipedia.org/wiki?curid=24188246", "title": "Glossary of gastropod terms", "text": "Glossary of gastropod terms\n\nThe following is a glossary of common English language and scientific terms used in the description of gastropods.\n\nAcephalous. Headless.\n\nAcinose. Full of small bulgings; resembling the kernel in a nut.\n\nAculeate. Very sharply pointed, as the teeth on the radula of some snails.\n\nAcute. Sharp or pointed, as the spire of a shell, or the lip of a shell.\n\nAcuminate. Long and tapering, as the spire of some shells.\n\nAdmedian. Next to the central object, as the lateral teeth on the lingual membrane.\n\nAfferent. To bring in; when relating to a vessel or duct, indicating that it brings in its contents.\n\nAmoeboid. Shaped like an amoeba, a small animalcule.\n\nAmorphous. Without distinct form.\n\nAmphibious. Inhabiting both land and water.\n\nAmphidetic. With the ligament on both sides of the umbones.\n\nAnalogue. A likeness between two objects when otherwise they are totally different, as the wing of a bird and the wing of a butterfly.\n\nAnastomosing. Coming together.\n\nAnnular. Made up of rings.\n\nAnterior. The front or fore end.\n\nAquatic. Inhabiting the water.\n\nArborescent. Branching like a tree.\n\nArched. Bowed or bent in a curve.\n\nArcti-spiral. Tightly coiled, as some spiral shells.\n\nAsphyxiating. Causing suspended animation; apparent death.\n\nAssimilation. Act of converting one substance into another, as the changing of food-stuffs into living bodies.\n\nAsymmetrical. Not symmetrical.\n\nAtrium.\n\nAtrophied. Wasted away.\n\nAttenuate. Long and slender, as in some shells.\n\nAuditory. Connected with the hearing.\n\nAuricled. Eared, or with ear-like appendages.\n\nBasal. The bottom or lower part.\n\nBiangulate. With two angles.\n\nBicuspid or bicuspidate. Having two cusps.\n\nBifid. Having two arms or prongs.\n\nBifurcated. Having two branches.\n\nBilateral. With two sides.\n\nBilobed. With two lobes.\n\nBlood sinus.\n\nBulbous. Swollen.\n\nCalcareous. Composed of carbonate of lime.\n\nCallosity. A hardened and raised bunch, as the callus on the columella of some shells.\n\nCallus. A deposit of shelly matter.\n\nCampanulate. Formed like a bell.\n\nCanaliculate. Resembling a canal, as the deep sutures in some shells.\n\nCancellated. Formed of cross-bars, as the longitudinal and spiral lines which cross in some shells.\n\nCardiac pouch. Containing the heart and placed near the umb'ones of the shell.\n\nCarinate. Keeled. With keel.\n\nCartilaginous. Like cartilage.\n\nCaudal. Tail-like, or with a tail-like appendage.\n\nCellular. Made up of cells.\n\nCerebral. Pertaining to the brain.\n\nChanneled. Grooved or formed like a channel.\n\nChitinous. Formed of chitin, as the radulas of gastropods.\n\nCiliary. By means of cilia.\n\nCiliated. Having cilia.\n\nCilium (plural cilia). A lash; used to designate the hairs on the mantle, gills, etc.\n\nClavate. Club-shaped.\n\nCoarctate. Pressed together, narrowed.\n\nConcave. Excavated, hollowed out.\n\nConchiolin.\n\nConic. Shaped like a cone.\n\nConnective. A part connecting two other parts, as a muscle connecting two parts of the body, or a nerve connecting two ganglia.\n\nConstricted. Narrowed.\n\nContractile. Capable of being contracted or drawn in, as the tentacle of a snail.\n\nConvex. Bulged out, as the whorls of some snails.\n\nConvoluted. Rolled together.\n\nCordate. Heart-shaped.\n\nCorneous. Horn-like, as the opercula of some gastropods.\n\nCorrugated. Roughened by wrinkles.\n\nCostate. Having rib-like ridges.\n\nCrenulate. Wrinkled on the edges.\n\nCrescentic. Like a crescent.\n\nCylindrical. Like a cylinder.\n\nDecollated. Cut off, as the apex of some shells.\n\nDecussated. With spiral and longitudinal lines intersecting, as the sculpture of some shells.\n\nDeflexed. Bent downward, as the last whorl in some snails.\n\nDentate. With points or nodules resembling teeth, as the aperture of some snails.\n\nDenticulate. Finely dentate.\n\nDepressed. Flattened, as the spire in some snails.\n\nDextral. Right-handed.\n\nDigitiform. Finger-like.\n\nDilated. Expanded in all directions, as the aperture of a shell.\n\nDimorphism. With two forms or conditions.\n\nDioecious. Having the sexes in two individuals, one male and one female.\n\nDistal. The farthest part from an object.\n\nDiscoidal. Shaped like a flat disk.\n\nDiverticulum. A pouch or hole, as the pouch containing the radula, or that containing the dart in helices.\n\nDormant. In a state of torpor or sleep.\n\nDorsal. The back. In gastropods the opposite to the aperture.\n\nEctocone. The outer cusp on the teeth of the radula.\n\nEdentulous. Without teeth or folds, as the aperture in some gastropods.\n\nEfferent. Carrying out.\n\nElliptical. With an oval form.\n\nElongated. Drawn out, as the spire of a shell.\n\nEmarginate. Bluntly notched.\n\nEncysted. Enclosed in a cyst.\n\nEntocone. The inner cusp on the teeth of the radula.\n\nEntire. With even, unbroken edges, as the aperture of some shells.\n\nEpiphallus. A portion of the vas deferens which becomes modified into a tube-like organ and is continued beyond the apex of the penis; it frequently bears a blind duct, or flagellum.\n\nEpithelium. All tissues bounding a free surface.\n\nEquidistant. Equally spaced, as the spiral lines on some snail shells.\n\nEquilibrating. Balancing equally.\n\nEroded. Worn away, as the epidermis on some shells.\n\nErosive. Capable of erosion.\n\nExcavated. Hollowed out, as the columella of some snails.\n\nExcurrent. Referring to the siphon which carries out the waste matter of the body.\n\nExoskeleton. The outer skeleton; all shells are exoskeletons.\n\nExserted. Brought out.\n\nExpanded. Spread out, as the lip of some shells.\n\nFalcate. Scythe-shaped.\n\nFasciculus. A little bundle.\n\nFlagellate. Animals with a flagellum or lash.\n\nFlexuous. Formed in a series of curves or turnings, as the columella in some shells.\n\nFlocculent. Clinging together in bunches.\n\nFluviatile. Living in running streams.\n\nFusiform. Thick in the middle and tapering at each end.\n\nGelatinous. Like jelly, as the eggs of some mollusks.\n\nGibbous. Very much rounded, as the whorls in some snails.\n\nGlandular. Like a gland.\n\nGlobose. Rounded.\n\nGranulated. Covered with little grains.\n\nGravid. A female mollusk with ovaries distended with young.\n\nGregarious. Living in colonies.\n\nGular. Relating to the windpipe or palate. In mollusks, referring to the innermost part of the aperture.\n\nHabitat. Locality of a species.\n\nHasmolymph. Molluscan blood.\n\nHeliciform. In form like \"Helix\".\n\nHemispherical. Half a sphere.\n\nHerbivorous. Subsisting upon vegetable food.\n\nHermaphrodite. Having the sexes united in the same individual.\n\nHibernation. The act of hibernating or going to sleep for the winter months.\n\nHirsute. Covered with hairs, as some snails.\n\nHispid. Same as hirsute.\n\nHomologous. Having the same position or value, as the wing of a bird and of a bat.\n\nHyaline. Glassy.\n\nImperforate. Not perforated or umbilicated.\n\nImpressed. Marked by a furrow, as the impressed spiral lines on some gastropod shells.\n\nIncrassate. Thickened.\n\nIncurved. Leaned or bent over, as the apex in some snails.\n\nIndented. Notched.\n\nInflected. Turned in, as the teeth of some snails.\n\nInhalent. Same as incurrent.\n\nInoperculate. Without an operculum.\n\nIntercostate. Between the ribs or ridges.\n\nInvaginate. One part bending into another, as the tentacles of some land snails.\n\nInvertible. Capable of being inverted, or drawn in, as the eye-peduncles of a land snail.\n\nJuvenile.\n\nKeeled. With a more or less sharp projection at the periphery.\n\nLamellated. Covered with scales.\n\nLamelliform. Having the form of scales.\n\nLaminated. Consisting of plates or scales laid over each other.\n\nLanceolate. Gradually tapering to a point.\n\nLateral. Pertaining to the side.\n\nLatticed. (See decussated.)\n\nLobulate. Composed of lobes.\n\nLongitudinal. The length of a shell.\n\nLunate. Shaped like a half moon, as the aperture in some shells.\n\nMalleated. Appearing as though hammered.\n\nManducatory. Relating to the apparatus for masticating food. In snails, the jaws and radula.\n\nMedian. Middle, as the middle tooth on the radula.\n\nMesocene. The middle cusp on the teeth of the radula.\n\nMonoecius. Having the sexes united in the same individual.\n\nMultifid. Made up of many lobes or projections, as the cusps on some radulae.\n\nMultispiral. Consisting of many whorls, as some fresh-water snails.\n\nNacreous. Pearly or iridescent.\n\nNepionic. The second stage of the embryonic shell, as the glochidium.\n\nNotched. Nicked or indented, as the anterior canal of some gastropods.\n\nNucleus. The first part or beginning, as the apex in a gastropod shell.\n\nNucleated. Having a nucleus.\n\nObconic. In the form of a reversed cone.\n\nOblique. Slanting, as the aperture of some shells when not parallel to the longitudinal axis.\n\nObovate. Reversed ovate, as some shells when the diameter is greater near the upper than at the lower part.\n\nObtuse. Dull or blunt, as the apex of some gastropods.\n\nOlfactory. Pertaining to the smell.\n\nOlivaceous. Colored like an olive.\n\nOrganism. An organized being, or living object made up of organs.\n\nOvate. Egg-shaped.\n\nOvately conic. Shaped like an egg, but with a somewhat conic apex, as some gastropods.\n\nOviparous. Bringing forth young in an egg which is hatched after it is laid.\n\nOvisac. A pouch in which the eggs or embryos are contained.\n\nOvoviviparous. In this case the young are formed in an egg but are hatched inside the parent.\n\nPallial lung.\n\nPapillose. Covered with many little bulgings or pimples.\n\nParallel. Having the same relative distance in all parts, as when the spiral lines in univalve shells are the same distance apart all the way around.\n\nPatelliform. Shaped like a flattened-out cone, as an \"Ancylus\".\n\nPatulous. Open and spreading, as the aperture in some gastropods.\n\nPaucispiral. Only slightly spiral, as some opercula.\n\nPectinate. Like the teeth of a comb, as the gills of some mollusks.\n\nPedal. Pertaining to the foot.\n\nPedunculated. Supported on a stem or stalk, as the eyes of land snails.\n\nPellucid. Transparent or clear, as the shells of some snails; e. g. \"Vitrea\".\n\nPenultimate. The whorl before the last in gastropod shells.\n\nPericardium. The chamber containing the heart.\n\nPeriostracum. The epidermal covering of some shells.\n\nPervious. Very narrowly open, as the umbilicus in some snails.\n\nPhytophagus. Vegetable-feeding.\n\nPilose. Covered with hairs.\n\nPinnate. Branched like a feather, as the gills of some mollusks.\n\nPlaited. Folded.\n\nPlanispiral shell.\n\nPlanorboid. Flat and orb-like, as some snails.\n\nPleurae. Relating to the side of a body.\n\nPlexus. A network of vessels, as the form of the lungs in snails.\n\nPlicated. Made up of folds.\n\nPlumose. Resembling plumes.\n\nPolygonal. Having many angles.\n\nPorcellanous. Like porcelain.\n\nPrismatic. Like a prism.\n\nProdissoconch. The embryonic shell.\n\nProtoconch. The embryonic shell.\n\nProtract. To push out.\n\nProtractor pedis. The foot protractor muscle.\n\nProtrusile. Capable of being pushed out.\n\nProximal. The nearest end of an object.\n\nPulsation. A throb, as the throbbing of the heart.\n\nPupiform. Like a pupa; one of the stages in the development of an insect.\n\nPustulate. Covered with pustules or little pimples.\n\nPustulose. Same as pustulate.\n\nPyramidal. Having the form of a pyramid.\n\nPyriform. Shaped like a pear.\n\nReflected. Bent backward, as the lip in some snails.\n\nReflexed. Same as Reflected.\n\nRenal. Relating to the kidneys.\n\nReticulated. Resembling a network, as when the longitudinal and spiral lines cross in a snail.\n\nRetractile. Capable of being drawn in, as the eye peduncles in land snails.\n\nRetractor pedis. Foot retractor muscle.\n\nRevolving lines. Spiral lines on a snail shell which run parallel with the sutures.\n\nRhombic. Having four sides, the angles being oblique.\n\nRhomboid. Four-sided, but two of the sides being longer than the others.\n\nRimate. Provided with a very small hole or crack, as some snails in which the umbilicus is very narrowly open.\n\nRoundly lunate. Rounder than lunate (which see).\n\nRostriform. In the form of a rostrum.\n\nRudimentary. Not fully formed; imperfect.\n\nRugose. Rough or wrinkled, as parts of some shells.\n\nSacculated. Somewhat like a sac, or composed of sac-like parts.\n\nScalar. Resembling a ladder.\n\nSecreted. Produced or deposited from the blood or glands, as the shell material in mollusks.\n\nSemicircular. Half round or circular, as the aperture in some snails.\n\nSemidentate. Half toothed, as the parietal wall in some land snails.\n\nSemielliptic. Half elliptical.\n\nSemiglobose. Half, or not quite globose.\n\nSemilunate. Half lunate.\n\nSemioval. Half, or not quite oval.\n\nSerrated. Notched, like the teeth on a saw.\n\nSerriform. In the form of series.\n\nSessile. Attached without a stem, as the eyes in some water snails.\n\nShouldered. Ridged, as the whorls in some snails.\n\nSigmoid. Shaped like the letter S.\n\nSiliceous. Made up of silex.\n\nSinistral. Having the aperture on the left side.\n\nSinuous. Curved in and out, as the edge of some bivalves and the lips of some snails.\n\nSpatulate. In the form of a spatula, a flat-bladed instrument used by druggists in pulverizing drugs.\n\nSpherical. Shaped like a sphere.\n\nSpiral. Wound about a central cavity, as the whorls of snails.\n\nStriated. Marked by lines or striae.\n\nSubangulated. Moderately angled.\n\nSubcarinated. Moderately carinated.\n\nSubcentral. Not quite in the center.\n\nSubcircular. Not quite circular.\n\nSubconical. Moderately conical.\n\nSubcylindrical. Moderately cylindrical.\n\nSubequal. Not quite equal.\n\nSubexcavated. A little excavated.\n\nSubfusiform. Moderately fusiform.\n\nSubglobose. Moderately globose.\n\nSubglobular. Moderately globular.\n\nSubhyaline. Moderately glassy.\n\nSubimperforate. Not much perforated.\n\nSuboblong. Moderately oblong.\n\nSubobsolete. Almost disappearing.\n\nSubovate. Nearly ovate.\n\nSubparallel. Almost parallel.\n\nSubperforated. Almost perforated.\n\nSubquadrate. Almost four-sided.\n\nSubreflected. Moderately turned back.\n\nSubrotund. Moderately round.\n\nSubspiral. Moderately spiral.\n\nSubtriangulate. Moderately or almost triangular.\n\nSubtrigonal. Moderately three-angled.\n\nSubtruncate. Moderately cut off.\n\nSubumbilicated. Moderately umbilicated.\n\nSulcated. Grooved.\n\nSulcus. A longitudinal furrow.\n\nSuperanal. Above the anus.\n\nSupra-peripheral. Above the periphery.\n\nSymmetrical. Alike on both sides or uniform in all parts.\n\nTerrestrial. Living on the land.\n\nTestaceous. Composed of shelly matter.\n\nTorsion. A twisting around.\n\nTortuous. Twisted or winding.\n\nTorpid. Half unconscious or asleep, as a snail during hibernation.\n\nTranslucent. Not quite transparent; light is seen through the thin edges of the object.\n\nTransparent. Objects may be seen through the substance.\n\nTransverse. Referring to the form of a shell when it is wider than high.\n\nTricuspidate. Having three cusps.\n\nTrifid. Having three branches.\n\nTrigonal. Having three angles.\n\nTrilobate. Having three lobes.\n\nTripartite. Divided into three parts, as the foot of some snails.\n\nTruncate. Having the end cut off squarely.\n\nTuberculate. Covered with tubercles or rounded knobs.\n\nTurbinate. Having the form of a top.\n\nTurriculated. Having the form of a tower.\n\nTurreted. Having the form of a tower.\n\nUmbilicated. Having an opening in the base of the shell.\n\nUndulated. Having undulations or waves.\n\nUnivalve. Having the shell composed of a single piece, as a snail.\n\nVaricose. Swollen or enlarged.\n\nVascular. Containing or made up of blood vessels.\n\nVermiform. Formed like a worm.\n\nVentral. The lower border or side.\n\nVentricose. Swollen or inflated on the ventral side.\n\nVibratile. Moving from side to side.\n\nVitreous. Resembling glass, as some snails.\n\n\nThis article include public domain text from Baker, \"The Mollusca of the Chicago area\", 1898-1902.\n"}
{"id": "30290563", "url": "https://en.wikipedia.org/wiki?curid=30290563", "title": "How I Killed Pluto and Why It Had It Coming", "text": "How I Killed Pluto and Why It Had It Coming\n\nHow I Killed Pluto and Why It Had It Coming is the 2010 memoir by Mike Brown, the American astronomer most responsible for the reclassification of the former planet Pluto from planet to dwarf planet.\n\nThe memoir is an account of the events surrounding the redefinition of the term \"planet\" that eventually changed the status of Pluto. It chronicles the discovery of Eris, a dwarf planet then mistakenly thought to be larger than Pluto, located within the scattered disc, beyond Neptune's orbit. The replaying of events includes the adversarial challenging of long-held scientific beliefs between some of the world's leading astronomers, and the eventual 2006 International Astronomical Union's vote that removed Pluto from the list of Solar System planets.\n\nReviews of the book have been generally positive, with James Kennedy of \"The Wall Street Journal\" calling the book a \"brisk\" and \"enjoyable ... chronicle\" of the tale of the search for new planets and the eventual demotion of Pluto from planetary status. Janet Maslin of the \"New York Times\" called it a \"short, eager-to-please research memoir\".\n\n"}
{"id": "9849057", "url": "https://en.wikipedia.org/wiki?curid=9849057", "title": "Human Proteinpedia", "text": "Human Proteinpedia\n\nHuman Proteinpedia is a portal for sharing and integration of human proteomic data. It allows research laboratories to contribute and maintain protein annotations. Human Protein Reference Database (HPRD) integrates data, that is deposited in Human Proteinpedia along with the existing literature curated information at the context of an individual protein. In essence, researchers can add new data to HPRD by registering to Human Proteinpedia. The data deposited in Human Proteinpedia is freely available for download. Emphasizing the importance of proteomics data disposition to public repositories, \"Nature Methods\" recommends Human Proteinpedia in their editorial. More than 70 labs participate in this effort.\n\nData pertaining to post-translational modifications, protein–protein interactions, tissue expression, expression in cell lines, subcellular localization and enzyme substrate relationships can be submitted to Human Proteinpedia.\n\nProtein annotations present in Human Proteinpedia are derived from a number of platforms such as\n\n\nThis portal that allows adding of protein information was developed as a collaborative effort between the laboratory of Dr. Akhilesh Pandey at Johns Hopkins University and the Institute of Bioinformatics\n\n\nAny investigator who fulfills the following criteria can contribute data:\n\ni) provides experimentally derived data, and,\n\nii) is willing to share data, and,\n\niii) is willing to be listed as the 'contributor' of the data\n\n\nAnonymous contributions are not allowed. Contributor details should be clearly presented while contributing data.\n\n\nPredictions of any type are not allowed. Contributed data should be derived experimentally and should be accompanied with experimental evidence.\n\n\nThe data are not subjected to peer review and the actual experimental data (raw or processed) should be provided.\n\n\nIn cases where a given entry is documented as erroneous, we will consult with the contributing group(s) about deleting the entry.\n\n"}
{"id": "23551125", "url": "https://en.wikipedia.org/wiki?curid=23551125", "title": "Iron stress repressed RNA", "text": "Iron stress repressed RNA\n\nIron stress repressed RNA (IsrR) is a cis-encoded antisense RNA which regulates the expression of the photosynthetic protein isiA. IsiA expression is activated by the Ferric uptake regulator protein (Fur) under iron stress conditions. IsiA enhances photosynthesis by forming a ring around photosystem I which acts as an additional antenna complex.\n\nIsrR is abundant when there is a sufficient iron concentration. IsrR is encoded for within the opposite stand of isiA gene and contains a conserved stem loop secondary structure. Under sufficient iron conditions IsrR binds to its complementary region which corresponds to the central third of the isiA mRNA. The resulting duplex RNA is then targeted for degradation. This allows the antisense RNA to act as a reversible switch that responds to changes in environmental conditions to modulate the expression of the isiA protein.\n\nIsrR was originally identified within cyanobacteria but may be conserved throughout a number of photosynthetic species from multiple kingdoms. At present, IsrR is the only non coding RNA identified that has a regulatory role on photosynthetic proteins.\n\n"}
{"id": "13139979", "url": "https://en.wikipedia.org/wiki?curid=13139979", "title": "Julius Victor Carus", "text": "Julius Victor Carus\n\nJulius Victor Carus (July 25, 1823 – March 10, 1903) was a German zoologist, comparative anatomist and entomologist.\n\nCarus was born in Leipzig. He served as curator of the Museum of Comparative Anatomy at Oxford University from 1849 to 1851, and as professor of comparative anatomy and director of the Zoological Museum at the University of Leipzig in 1853.\n\nCarus was an early supporter of Darwinism. With Charles Darwin's approval he became his German translator. In 1875, Carus issued a German edition of Darwin's collected works.\n\n\nTranslation of French Wikipedia\n"}
{"id": "53159121", "url": "https://en.wikipedia.org/wiki?curid=53159121", "title": "Karl Escherich", "text": "Karl Escherich\n\nKarl Leopold Escherich (18 September 1871 – 22 November 1951) was a German entomologist and professor of zoology. Known as a pioneer of applied entomology and expert in termites, he was rector of the University of Munich from 1933 to 1936.\n\nEscherich was born in Schwandorf, Bavaria, to parents Hermann N. Escherich and Katharina von Stengel. His older brother Georg Escherich would become a noted forester and politician.\n\nHe studied medicine in Munich and Würzburg, graduating in 1893. After gaining his postdoctoral qualification in Strasbourg in 1901, he received a professorship at the Department of Forest Zoology in Tharandt in 1907, which had been orphaned since the death of Hinrich Nitsche. In 1914 he joined the Chair of Applied Zoology at the Ludwig-Maximilians-University of Munich, where he succeeded August Pauly. In 1917 he was elected a member of the Academy of Sciences Leopoldina.\n\nAfter a 1911 trip to the United States, he conceived a plan to redesign applied entomology in Germany after the American model. In 1913 he co-founded the German Society for Applied Entomology.\n\nKarl Escherich was one of the few forestry academics who took part in the early Hitler movement of the inflation period. He joined the Nazi Party in 1921, and participated in the 1923 Munich Putsch. In 1924, he still participated in the election campaign for the \"Volkischer Block\", but remained remote from the new NSDAP.\n\nFor his research, Karl Escherich received the Goethe Medal for Art and Science.\n\nTo commemorate its founder, the German Society for Applied Entomology awards the Escherich Medal for outstanding achievements in the field of entomology.\n"}
{"id": "53913187", "url": "https://en.wikipedia.org/wiki?curid=53913187", "title": "Land change modeling", "text": "Land change modeling\n\nLand change models (LCMs) describe, project, and explain changes in and the dynamics of land use and land-cover. LCMs are a means of understanding ways that humans are changing the Earth's surface in the past, present, in forecasting land change into the future.\n\nLand change models are valuable in development policy, helping guide more appropriate decisions for resource management and the natural environment at a variety of scales ranging from a small piece of land to the entire spatial extent. Moreover, developments within land-cover, environmental and socio-economic data (as well as within technological infrastructures) have increased opportunities for land change modeling to help support and influence decisions that affect human-environment systems, as national and international attention increasingly focuses on issues of global climate change and sustainability.\n\nThese are key concepts and various other terminologies necessary to understand the topic of land change modeling. \n\nChanges in land systems have consequences for climate and environmental change on every scale. Therefore, decisions and policies in relation to land systems are very important for reacting these changes and working towards a more sustainable society and planet.\n\nLand change models are significant in their ability to help guide the land systems to positive societal and environmental outcomes at a time when attention to changes across land systems is increasing.\n\nA plethora of science and practitioner communities have been able to advance the amount and quality of data in land change modeling in the past few decades. That has influenced the development of methods and technologies in model land change. The multitudes of land change models that have been developed are significant in their ability to address land system change and useful in various science and practitioner communities.\n\nFor the science community, land change models are important in their ability to test theories and concepts of land change and its connections to human-environment relationships, as well as explore how these dynamics will change future land systems without real-world observation.\n\nLand change modeling is useful to explore spatial land systems, uses, and covers. Land change modeling can account for complexity within dynamics of land use and land cover by linking with climactic, ecological, biogeochemical, biogeophysical and socioeconomic models. Additionally, LCMs are able to produce spatially explicit outcomes according to the type and complexity within the land system dynamics within the spatial extent. Many biophysical and socioeconomic variables influence and produce a variety of outcomes in land change modeling.\n\nA notable property of all land change models is that they have some irreducible level of uncertainty in the model structure, parameter values, and/or input data. For instance, one uncertainty within land change models is a result from temporal non-stationarity that exists in land change processes, so the further into the future the model is applied, the more uncertain it is. Another uncertainty within land change models are data and parameter uncertainties within physical principles (i.e., surface typology), which leads to uncertainties in being able to understand and predict physical processes.\n\nFurthermore, land change model design are a product of both decision-making and physical processes. Human-induced impact on the socio-economic and ecological environment is important to take into account, as it is constantly changing land cover and sometimes model uncertainty. To avoid model uncertainty and interpret model outputs more accurately, a model diagnosis is used to understand more about the connections between land change models and the actual land system of the spatial extent. The overall importance of model diagnosis with model uncertainty issues is its ability to assess how interacting processes and the landscape are represented, as well as the uncertainty within the landscape and its processes.\n\nA machine-learning approach uses land-cover data from the past to try to assess how land will change in the future, and works best with large datasets. There are multiple types of machine-learning and statistical models - a study in western Mexico from 2011 found that results from two outwardly similar models were considerably different, as one used a neural network and the other used a simple weights-of-evidence model.\n\nA cellular land change model uses maps of suitability for various types of land use, and compares areas that are immediately adjacent to one another to project changes into the future. Variations in the scale of cells in a cellular model can have significant impacts on model outputs.\n\nEconomic models are built on principles of supply and demand. They use mathematical parameters in order to predict what land types will be desired and which will be discarded. These are frequently built for urban areas, such as a 2003 study of the highly dense Pearl River Delta in southern China.\n\nAgent-based models try to simulate the behavior of many individuals making independent choices, and then see how those choices affect the landscape as a whole. Agent-based modeling can be complex - for instance, a 2005 study combined an agent-based model with computer-based genetic programming to explore land change in the Yucatan peninsula of Mexico.\n\nMany models do not limit themselves to one of the approaches above - they may combine several in order to develop a fully comprehensive and accurate model.\n\nScientists use LCMs to build and test theories in land change modeling for a variety of human and environmental dynamics. Land change modeling has a variety of implementation opportunities in many science and practice disciplines, such as in decision-making, policy, and in real-world application in public and private domains. The science disciplines use LCMs to formalize and test land change theory, and the explore and experiment with different scenarios of land change modeling. The practical disciplines use LCMs to analyze current land change trends and explore future outcomes from policies or actions in order to set appropriate guidelines, limits and principles for policy and action. Research and practitioner communities may study land change to address topics related to land-climate interactions, water quantity and quality, food and fiber production, and urbanization, infrastructure, and the built environment.\n\nOne improvement for land change modeling can be made through better data and integration with available data and models. Improved observational data can influence modeling quality. Finer spatial and temporal resolution data that can integrate with socioeconomic and biogeophysical data can help land change modeling couple the socioeconomic and biogeological modeling types. Land change modelers should value data at finer scales. Fine data can give a better conceptual understanding of underlying constructs of the model and capture additional dimensions of land use. It is important to maintain the temporal and spatial continuity of data from airborne-based and survey-based observation through constellations of smaller satellite coverage, image processing algorithms, and other new data to link satellite-based land use information and land management information. It is also important to have better information on land change actors and their beliefs, preferences, and behaviors to improve the predictive ability of models and evaluate the consequences of alternative policies.\n\nOne important improvement for land change modeling can be made though better aligning model choices with model goals. It is important to choose the appropriate modeling approach based on the scientific and application contexts of the specific study of interest. For example, when someone needs to design a model with policy and policy actors in mind, they may choose an agent-based model. Here, structural economic or agent-based approaches are useful, but specific patterns and trends in land change as with many ecological systems may not be as useful. When one needs to grasp the early stages of problem identification, and thus needs to understand the scientific patterns and trend of land change, machine learning and cellular approaches are useful.\n\nLand Change Modeling should also better integrate positive and normative approaches to explanation and prediction based on evidence-based accounts of land systems. It should also integrate optimization approaches to explore the outcomes that are the most beneficial and the processes that might produce those outcomes.\n\nIt is important to better integrate data across scales. A models design is based on the dominant processes and data from a specific scale of application and spatial extent. Cross-scale dynamics and feedbacks between temporal and spatial scales influences the patterns and processes of the model. Process like tele-coupling, indirect land use change, and adaption to climate change at multiple scales requires better representation by cross-scale dynamics. Implementing these processes will require a better understanding of feedback mechanisms across scales.\n\nAs there is continuous reinvention of modeling environments, frameworks, and platforms, land change modeling can improve from better research infrastructure support. For example, model and software infrastructure development can help avoid duplication of initiatives by land change modeling community members, co-learn about land change modeling, and integrate models to evaluate impacts of land change. Better data infrastructure can provide more data resources to support compilation, curation, and comparison of heterogeneous data sources. Better community modeling and governance can advance decision-making and modeling capabilities within a community with specific and achievable goals. Community modeling and governance would provide a step towards reaching community agreement on specific goals to move modeling and data capabilities forward.\n\nA number of modern challenges in land change modeling can potentially be addressed through contemporary advances in cyberinfrastructure such as crowd-source, “mining” for distributed data, and improving high-performance computing. Because it is important for modelers to find more data to better construct, calibrate, and validate structural models, the ability to analyze large amount of data on individual behaviors is helpful. For example, modelers can find point-of-sales data on individual purchases by consumers and internet activities that reveal social networks. However, some issues of privacy and propriety for crowdsourcing improvements have not yet been resolved.\n\nThe land change modeling community can also benefit from Global Positioning System and Internet-enabled mobile device data distribution. Combining various structural-based data-collecting methods can improve the availability of microdata and the diversity of people that see the findings and outcomes of land change modeling projects. For example, citizen-contributed data supported the implementation of Ushahidi in Haiti after the 2010 earthquake, helping at least 4,000 disaster events. Universities, non-profit agencies, and volunteers are needed to collect information on events like this to make positive outcomes and improvements in land change modeling and land change modeling applications. Tools such as mobile devices are available to make it easier for participants to participate in collecting micro-data on agents. Google Maps uses cloud-based mapping technologies with datasets that are co-produced by the public and scientists. Examples in agriculture such as coffee farmers in Avaaj Otalo showed use of mobile phones for collecting information and as an interactive voice.\n\nCyberinfrastructure developments may also increase the ability of land change modeling to meet computational demands of various modeling approaches given increasing data volumes and certain expected model interactions. For example, improving the development of processors, data storage, network bandwidth, and coupling land change and environmental process models at high resolution.\n\nAn additional way to improve land change modeling is through improvement of model evaluation approaches. Improvement in sensitivity analysis is needed to gain a better understand of the variation in model output in response to model elements like input data, model parameters, initial conditions, boundary conditions, and model structure. Improvement in pattern validation can help land change modelers make comparisons between model outputs parameterized for some historic case, like maps, and observations for that case. Improvement in uncertainty sources is needed to improve forecasting of future states that are non-stationary in processes, input variables, and boundary conditions. One can explicitly recognize stationarity assumptions and explore data for evidence in non-stationarity to better acknowledge and understand model uncertainty to improve uncertainty sources. Improvement in structural validation can help improve acknowledgement and understanding of the processes in the model and the processes operating in the real world through a combination of qualitative and quantitative measures.\n\n"}
{"id": "11187755", "url": "https://en.wikipedia.org/wiki?curid=11187755", "title": "Lill pin", "text": "Lill pin\n\nLill pins are pins used by entomologists and botanists.\n\nThey are short pins, with a head. Entomologists use them for holding down setting paper, and for pinning name labels inside collection cases. Botanists use them for mounting specimen envelopes containing bryophyte specimens.\n\n"}
{"id": "57217656", "url": "https://en.wikipedia.org/wiki?curid=57217656", "title": "List of Rhodesian periodicals", "text": "List of Rhodesian periodicals\n\nThis is a list of periodicals published in Rhodesia (today Zimbabwe). It includes periodicals published in Southern Rhodesia, before Rhodesia declared independence.\n"}
{"id": "2960687", "url": "https://en.wikipedia.org/wiki?curid=2960687", "title": "List of Ukrainian mathematicians", "text": "List of Ukrainian mathematicians\n\nThis a list of the best known Ukrainian mathematicians. This list includes some Polish, pre-revolutionary Russian and Soviet mathematicians who lived or worked in Ukraine.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "22258844", "url": "https://en.wikipedia.org/wiki?curid=22258844", "title": "List of abbreviations for diseases and disorders", "text": "List of abbreviations for diseases and disorders\n\nThis is a list of acronyms and initials related to diseases (infectious or non-infectious) and medical disorders.\n\n\n"}
{"id": "18568", "url": "https://en.wikipedia.org/wiki?curid=18568", "title": "List of algorithms", "text": "List of algorithms\n\nThe following is a list of algorithms along with one-line descriptions for each.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "11673441", "url": "https://en.wikipedia.org/wiki?curid=11673441", "title": "List of deadly earthquakes since 1900", "text": "List of deadly earthquakes since 1900\n\nThe following list compiles known earthquakes that have caused one or more fatalities since 1900. The list incorporates high quality earthquake source (i.e., origin time, location and earthquake magnitude) and fatality information from several sources.\n\nEarthquake locations are taken from the Centennial Catalog and the updated Engdahl, van der Hilst and Buland earthquake catalog, which is complete to December 2005. From January 2006, earthquake locations are from the United States Geological Survey’s Preliminary Determination of Epicenters (PDE) monthly listing. Preferred magnitudes are moment magnitudes taken from the Global Centroid Moment Tensor Database and its predecessor, the Harvard Centroid Moment Tensor Database. Where these magnitude estimates are unavailable, the preferred magnitude estimate is taken from the Centennial Catalog and the PDE.\n\nFive columns of fatality estimates are provided. The first two columns are derived from the PDE monthly catalog and indicate deaths resulting from earthquake shaking only (i.e., from partial or total building collapse), and total fatalities resulting from earthquake shaking and secondary effects, such as tsunami, landslide, fire, liquefaction or other factors (e.g., heart failure). Where these secondary effects are reported, they are indicated by “T”, “L”, “F” or “Lq”, respectively. Fatality estimates in the PDE are generally obtained from official sources (e.g., local or national government officials, humanitarian agencies, emergency management agencies, etc.) or media reports within days to weeks after the earthquake. The PDE catalog is not updated if more detailed information becomes available after its final publication, usually four months after the earthquake.\n\nThe third fatality column is taken from the Utsu catalog of deadly earthquakes, and generally represents the total deaths resulting from an earthquake. The Utsu catalog is complete up until late 2003. The fourth column is derived from the Emergency Events Database (EM-DAT). EM-DAT has been developed and maintained by the Centre for Research on the Epidemiology of Disasters at the Brussels campus of the University of Louvain, Belgium and is a global, multi-hazard (e.g., earthquake, cyclone, drought, flood, volcano, extreme temperatures, etc.) database of human impacts and economic losses. Earthquake source parameters in the EM-DAT are often absent, incomplete, or erroneous. Consequently, several events may be missed in the automated catalog associations. Furthermore, where the impact of an earthquake spans political boundaries, database entries are often subdivided by country. For significant events, the observed fatalities are aggregated and manually associated.\n\nThe final fatality column is for other sources of shaking deaths and indicates improved fatality estimates from official reports and detailed scholarly studies, where available.\n\nThe death tolls presented below vary widely in quality and in many cases are estimates only, particularly for the most catastrophic events that result in high fatalities. Note that in some cases, fatalities have been documented, but no numerical value of deaths is given. In these cases, fatality estimates are left blank. Many of the events listed with no numerical value are aftershocks where additional fatalities are aggregated with the main shock.\n\nFor death tolls of other natural disasters or significant historical earthquakes that predate 1900, see:\n\n\n"}
{"id": "892306", "url": "https://en.wikipedia.org/wiki?curid=892306", "title": "List of doping cases in sport", "text": "List of doping cases in sport\n\nThe following is an incomplete list of sportspeople who have been involved in doping offences. It contains those who have been found to have, or have admitted to having, taken illegal performance-enhancing drugs, prohibited recreational drugs or have been suspended by a sports governing body for failure to submit to mandatory drug testing.\n\n"}
{"id": "6785009", "url": "https://en.wikipedia.org/wiki?curid=6785009", "title": "List of volcanoes in the United States", "text": "List of volcanoes in the United States\n\nA list of volcanoes in the United States and its territories.\n\n\n"}
{"id": "768177", "url": "https://en.wikipedia.org/wiki?curid=768177", "title": "Little Joe 5", "text": "Little Joe 5\n\nLittle Joe 5 was an unmanned atmospheric test flight of the Mercury spacecraft, conducted as part of the U.S. Mercury program. The objective was to test a production Mercury capsule (#3) and the Launch Escape System during an ascent abort at maximum dynamic pressure. The mission was launched November 8, 1960, from Wallops Island, Virginia. Sixteen seconds after liftoff, the escape rocket and the tower jettison rocket both fired prematurely. Furthermore, the booster, capsule and escape tower failed to separate as intended. The entire stack was destroyed on impact with the Atlantic Ocean. The Little Joe 5 flew to an apogee of 10.1 miles (16.2 km) and a range of 13 miles (20.9 km). Some capsule and booster debris was recovered from the ocean floor for post flight analysis.\n\n\n"}
{"id": "1313004", "url": "https://en.wikipedia.org/wiki?curid=1313004", "title": "Lotka's law", "text": "Lotka's law\n\nLotka's law, named after Alfred J. Lotka, is one of a variety of special applications of Zipf's law. It describes the frequency of publication by authors in any given field. It states that the number of authors making formula_1 contributions in a given period is a fraction of the number making a single contribution, following the formula formula_2 where \"a\" nearly always equals two, i.e., an approximate inverse-square law, where the number of authors publishing a certain number of articles is a fixed ratio to the number of authors publishing a single article. As the number of articles published increases, authors producing that many publications become less frequent. There are 1/4 as many authors publishing two articles within a specified time period as there are single-publication authors, 1/9 as many publishing three articles, 1/16 as many publishing four articles, etc. Though the law itself covers many disciplines, the actual ratios involved (as a function of 'a') are discipline-specific.\nThe general formula says:\n\nor\n\nwhere \"X\" is the number of publications, \"Y\" the relative frequency of authors with \"X\" publications, and \"n\" and formula_5 are constants depending on the specific field (formula_6).\n\nSay 100 authors write at least one article each over a specific period, we assume for this table that C=100 and n=2. Then the number of authors writing portions of any particular articles in that time period is describe as in the following table:\nThat would be a total of 294 articles with 155 writers with an average of 1.9 articles for each writer.\n\nThis is an empirical observation rather than a necessary result. This form of the law is as originally published and is sometimes referred to as the \"discrete Lotka power function\".\n\n\n\n\n"}
{"id": "1274313", "url": "https://en.wikipedia.org/wiki?curid=1274313", "title": "Lunar orbit rendezvous", "text": "Lunar orbit rendezvous\n\nLunar orbit rendezvous (LOR) is a key concept for efficiently landing humans on the Moon and returning them to Earth. It was utilized for the Project Apollo missions in the 1960s and 1970s. In a LOR mission, a main spacecraft and a smaller lunar lander travel to lunar orbit. The lunar lander then independently descends to the surface of the Moon, while the main spacecraft remains in lunar orbit. After completion of the mission there, the lander returns to lunar orbit to rendezvous and re-dock with the main spacecraft, then is discarded after transfer of crew and payload. Only the main spacecraft returns to Earth.\nLunar orbit rendezvous was first known to be proposed in 1919 by Soviet engineer Yuri Kondratyuk, as the most economical way of sending a human on a round-trip journey to the Moon.\nThe most famous example involved Apollo CSM and Apollo LM, where they were both sent to a Translunar flight in a single rocket stack. However, variants where the landers and main spacecraft travel separately, such as the lunar landing plan proposed for Shuttle-Derived Heavy Lift Launch Vehicle and Golden Spike, are also considered as Lunar Orbit rendezvous.\n\nThe main advantage of LOR is the spacecraft payload saving, due to the fact that the propellant necessary to return from lunar orbit back to Earth need not be carried as dead weight down to the Moon and back into lunar orbit. This has a multiplicative effect, because each pound of \"dead weight\" propellant used later has to be propelled by more propellant sooner, and also because increased propellant requires increased tankage weight. The resultant weight increase would also require more thrust for lunar landing, which means larger and heavier engines.\n\nAnother advantage is that the lunar lander can be designed for just that purpose, rather than requiring the main spacecraft to also be made suitable for a lunar landing. Finally, the second set of life support systems that the lunar lander requires can serve as a backup for the systems in the main spacecraft.\n\nLunar-orbit rendezvous was considered risky as of 1962, because space rendezvous had not been achieved, even in Earth orbit. If the LEM could not reach the CSM, two astronauts would be stranded with no way to get back to Earth or survive re-entry into the atmosphere. The fear proved to be unfounded, as rendezvous was successfully demonstrated in 1965 and 1966 on six Project Gemini missions with the aid of radar and on-board computers. It was also successfully done each of the eight times it was tried on Apollo missions.\n\nWhen the Apollo Moon landing program was started in 1961, it was assumed that the three-man Command and Service Module combination (CSM) would be used for takeoff from the lunar surface, and return to Earth. It would therefore have to be landed on the Moon by a larger rocket stage with landing gear legs, resulting in a very large spacecraft (in excess of ) to be sent to the Moon.\n\nIf this were done by direct ascent (on a single launch vehicle), the rocket required would have to be extremely large, in the Nova class. The alternative to this would have been Earth orbit rendezvous, in which two or more rockets in the Saturn class would launch parts of the complete spacecraft, which would rendezvous in Earth orbit before departing for the Moon. This would possibly include a separately launched Earth departure stage, or require on-orbit refueling of the empty departure stage.\n\nTom Dolan proposed the alternative of lunar orbit rendezvous, which had been studied and promoted by Jim Chamberlin and Owen Maynard at the Space Task Group in 1960 early Apollo feasibility studies. This mode allowed a single Saturn V to launch the CSM to the Moon with a smaller Lunar Excursion Module (LEM). When the combined spacecraft reaches lunar orbit, one of the three astronauts remains with the CSM, while the other two enter the LEM, undock and descend to the surface of the Moon. They then use the ascent stage of the LEM to rejoin the CSM in lunar orbit, then discard the LEM and use the CSM for the return to Earth. This method was brought to the attention of NASA Associate Administrator Robert Seamans by Langley Research Center engineer John C. Houbolt, who led a team to develop it.\n\nBesides requiring less payload, the ability to use a lunar lander designed just for that purpose was another advantage of the LOR approach. The LEM's design gave the astronauts a clear view of their landing site through observation windows approximately above the surface, as opposed to being on their backs in a Command Module lander, at least above the surface, able to see it only through a television screen.\n\nDeveloping the LEM as a second manned vehicle provided the further advantage of redundant critical systems (electrical power, life support, and propulsion), which enabled it to be used as a \"lifeboat\" to keep the astronauts alive and get them home safely in the event of a critical CSM system failure. This was envisioned as a contingency, but not made a part of the LEM specifications. As it turned out, this capability proved invaluable in 1970, when just such a critical failure occurred on the Apollo 13 mission when an oxygen tank failure disabled the Service Module.\n\nDr. John Houbolt would not let the advantages of LOR be ignored. As a member of Lunar Mission Steering Group, Houbolt had been studying various technical aspects of space rendezvous since 1959 and was convinced, like several others at Langley Research Center, that LOR was not only the most feasible way to make it to the Moon before the decade was out, it was the only way. He had reported his findings to NASA on various occasions but felt strongly that the internal task forces (to which he made presentations) were following arbitrarily established \"ground rules.\" According to Houbolt, these ground rules were constraining NASA's thinking about the lunar mission—and causing LOR to be ruled out before it was fairly considered.\n\nIn November 1961, Houbolt took the bold step of skipping proper channels and writing a private letter, nine pages long, directly to Robert C. Seamans, the associate administrator. \"Somewhat as a voice in the wilderness,\" Houbolt protested LOR's exclusion. \"Do we want to go to the Moon or not?\" the Langley engineer asked. \"Why is Nova, with its ponderous size simply just accepted, and why is a much less grandiose scheme involving rendezvous ostracized or put on the defensive? I fully realize that contacting you in this manner is somewhat unorthodox,\" Houbolt admitted, \"but the issues at stake are crucial enough to us all that an unusual course is warranted.\"\n\nIt took two weeks for Seamans to reply to Houbolt's extraordinary letter. The associate administrator agreed that \"it would be extremely harmful to our organization and to the country if our qualified staff were unduly limited by restrictive guidelines.\" He assured Houbolt that NASA would in the future be paying more attention to LOR than it had up to this time.\n\nIn the following months, NASA did just that, and to the surprise of many both inside and outside the agency, the dark horse candidate, LOR, quickly became the front runner. Several factors decided the issue in its favor. First, there was growing disenchantment with the idea of direct ascent due to the time and money it was going to take to develop a diameter Nova rocket, compared to the diameter Saturn V. Second, there was increasing technical apprehension over how the relatively large spacecraft demanded even by Earth-orbit rendezvous would be able to maneuver to a soft landing on the Moon. As one NASA engineer who changed his mind explained: The business of eyeballing that thing down to the Moon really didn't have a satisfactory answer. The best thing about LOR was that it allowed us to build a separate vehicle for landing.\n\nThe first major group to break camp in favor of LOR was Robert Gilruth's Space Task Group, which was still located at Langley but was soon to move to Houston. The second to come over was the Von Braun team at the Marshall Space Flight Center in Huntsville, Alabama. Then these two powerful groups of converts, along with the original true believers at Langley, persuaded key officials at NASA Headquarters, notably Administrator James Webb, who had been holding out for direct ascent, that LOR was the only way to land on the Moon by 1969. With the key players inside NASA lined up behind the concept, Webb approved LOR in July 1962. The decision was officially announced at a press conference on July 11, 1962. President Kennedy's science adviser, Jerome Wiesner, remained firmly opposed to LOR.\n\n\n"}
{"id": "726653", "url": "https://en.wikipedia.org/wiki?curid=726653", "title": "Marcel Minnaert", "text": "Marcel Minnaert\n\nMarcel Gilles Jozef Minnaert (12 February 1893 – 26 October 1970) was a Dutch astronomer of Belgian origin. He was born in Bruges and died in Utrecht.\n\nMinnaert obtained a PhD in biology at Ghent University in 1914. Later he obtained also a PhD in physics from Utrecht University, under the supervision of Leonard Ornstein.\n\nHe was a supporter of the Flemish movement during World War I and endorsed the replacement of French by Dutch during the German occupation of Belgium. He worked as \"lector fysica\" at the new Flemish University of Ghent, which was made possible by the support of the German occupation forces, and was viewed as connivance with the enemy by the reestablished Belgian authorities. Because of this, he was sentenced after the war in absence to 15 years of forced labor. However, Minnaert had anticipated this outcome by fleeing Belgium in time.\n\nIn 1918, he found a position at Utrecht University in the Netherlands, \ninitially to do photometric research. In Utrecht, he became interested in astronomy, and he became a pioneer of solar research. He specialized in spectroscopy and the study of stellar atmospheres and invented the spectroscopic curve of growth.\n\nMinnaert was also interested in bubbles and musical nature of the sounds made by running water. In 1933 he published a solution for the acoustic resonance frequency of a single bubble in water, the so-called Minnaert resonance.\n\nIn 1937, he was appointed director of the stellar observatory \"Sonnenborgh\" in Utrecht and full professor in astronomy at the university. In 1940, he published his famous Utrecht Atlas of the solar spectrum. In 1941, he invented the Minnaert function, which is used in optical measurements of celestial bodies.\n\nDuring the German occupation of the Netherlands in World War II, he was imprisoned by the \nGermans because of his left-wing, anti-fascist sympathies. During his incarceration, he taught physics and astronomy to his fellow prisoners. After the War, he was one of the founders of the Mathematisch Centrum in Amsterdam.\n\nIn 1946 he became member of the Royal Netherlands Academy of Arts and Sciences.\n\nOne of his interests was the effects of the atmosphere on light and images. His classic book on this subject was originally published in Dutch in 1937 as \"De natuurkunde van 't vrije veld. Licht en kleur in het landschap\". It was released in English translation as:\nand in a new translation with color photographs as:\n\nAwards\nNamed after him\n\n\n"}
{"id": "383405", "url": "https://en.wikipedia.org/wiki?curid=383405", "title": "Mercury-Atlas 5", "text": "Mercury-Atlas 5\n\nMercury-Atlas 5 was an American unmanned spaceflight of the Mercury program. It was launched on November 29, 1961 with Enos, a chimpanzee, aboard. The craft orbited the Earth twice and splashed down about south of Bermuda.\n\nBy November 1961, the Soviet Union had launched Yuri Gagarin and Gherman Titov into orbit during the Vostok 1 and Vostok 2 manned orbital flights while the United States had managed only suborbital ones. At that time NASA was still debating placing a chimpanzee in orbit as part of the Mercury-Atlas subprogram, with NASA headquarters questioning the wisdom of the Manned Spacecraft Center launching another unmanned Mercury mission. \n\nThe NASA Public Affairs Office issued a press release stating \"The men in charge of Project Mercury have insisted on orbiting the chimpanzee as a necessary preliminary checkout of the entire Mercury program before risking a human astronaut.\" prior to the flight.\n\nThe flight used Mercury capsule #9 and Atlas #93-D. On February 24, 1961 spacecraft # 9 arrived at Cape Canaveral. It took 40 weeks of preflight preparation. This was the longest preparation time in the Mercury program. The mission of spacecraft #9 kept changing. It was first configured for a suborbital instrumented flight, then for a suborbital chimpanzee flight, then a three-orbit instrumented mission, and finally for the orbital flight that Enos flew.\n\nMA-4's successful flight in September had renewed confidence in the Atlas's reliability, and although an Atlas E test carrying a monkey was lost in a launch failure that November, NASA officials assured the public that it was a different model of booster than the Atlas D used for the Mercury program and that that accident had no relevance here.\nAlthough MA-4 had performed well, there was still some concern about high vibration levels during the first 20 seconds of liftoff, so the autopilot on MA-5's booster was modified slightly to correct this problem.\n\nAtlas 93D arrived at CCAS on August 12 and was erected on LC-14 October 6. Prelaunch preparation proceeded relatively smoothly, with a number of minor repairs, including a potentially serious problem with the vernier engines not being bolted in place tightly, which could have resulted in damage to the airframe during launch.\n\nThe Range Safety system on Atlas 93D was modified so that a manual cutoff command could be sent to the sustainer engine. This was to prevent the capsule from being accidentally boosted into a higher than planned orbit if engine over-acceleration occurred.\n\nA more compact all-solid state telemetry unit replaced the bulky vacuum tube-based package used previously.\n\nAtlas 93D was the second D-series Atlas to contain the new SMRD (Spin Motor Rotation Detection System), designed to ensure proper gyroscope operation prior to launch.\n\nMA-5 was planned as a close approximation of the upcoming MA-6 manned orbital mission. Mercury-Atlas 5 would be launched from Complex 14 at Cape Canaveral on a heading 72.51 degrees east of north. Orbital insertion of the Mercury spacecraft would occur from Cape Canaveral. The altitude would be and the speed would be . Retrofire was planned to take place at 4 hours, 32 minutes, and 26 seconds after launch. The spacecraft would land 21 minutes and 49 seconds after retrofire. Reentry temperatures were expected to reach on the heatshield, on the antenna housing, on the cylindrical section, and on the conical section. The spent Atlas sustainer engine was expected to reenter the atmosphere after 9⅓ orbits.\n\nSpacecraft #9 had originally been intended to fly on MA-4, but Spacecraft #8 was used instead after having been recycled from the failed MA-3 launch. #9 was the second of the \"Mark II\" Mercury capsules with a larger square window and explosive bolt hatch, while the older \"Mark I\" capsule had small port windows and a heavy locking mechanism. Gus Grissom's flight on MR-4 had used a Mark II capsule, but it was necessary to test it on a proper orbital mission to ensure that the large window could handle the much higher heat of reentry there.\n\nOn October 29, 1961, three chimps and 12 medical specialists moved into quarters at the Cape to prepare for the flight. The name given to Enos, the chimp selected to fly the MA-5 mission, in Hebrew means \"man\". Enos's backups were (in order of possible call-up) Duane, Jim, Rocky, and Ham, (the Mercury-Redstone 2 veteran). Enos was from Cameroon, Africa, (originally called Chimp # 81), and was purchased by the USAF on April 3, 1960. He was about 5 years old at the time of flight and weighed just under 40 pounds (18 kilograms).\n\nOn November 29, 1961, about five hours before launch, Enos and his spacesuit-couch were inserted in the spacecraft. During the countdown, various holds took 2 hours and 38 minutes. Liftoff came at 15:08 UTC. The Atlas launched the MA-5 spacecraft into an orbit of . The modified autopilot apparently worked as vibration remained within comfortable levels and the boost phase was uneventful. All Atlas systems performed excellently and there were no performance deviations of any significance. BECO occurred at T+130 seconds and SECO/VECO at T+300 seconds, followed by capsule separation two seconds later.\n\nThe turnaround and damping maneuver consumed of the of control fuel aboard. The spacecraft used less fuel than the MA-4 did during the same maneuver. MA-5 assumed its planned 34-degree orbital attitude and after that, through the first orbit the thrusters used only of fuel to maintain a correct position.\n\nAt the end of the first orbit, ground controllers noticed the capsule clock was 18 seconds too fast. As it passed over Cape Canaveral a command was sent to update the clock to the correct time. The Mercury Control Center at Cape Canaveral received information that all spacecraft systems were in good condition.\n\nAs the MA-5 passed over the Atlantic tracking ship at the beginning of the second orbit, indications were received that inverter temperatures were rising. The environmental control system malfunction was also confirmed by Canary Island trackers. Abnormal heating had occurred on earlier flights; in such cases, inverters had continued working or had been switched to standby. There was no alarm at Mercury Control. When the spacecraft reached Muchea, Australia, high thruster signals and capsule motion excursions were detected. Other data indicated that the 34° orbit mode was being maintained. When the MA-5 crossed the tracking station at Woomera, Australia, attitude control problems were not detected, so earlier reports were discounted.\n\nAs the MA-5 capsule reached the Canton Island station, Mercury Control realized that the attitude control system was malfunctioning. A metal chip in a fuel supply line had caused one of the clockwise roll thrusters to fail. The failed thruster allowed the spacecraft to drift from its normal attitude. This drift caused the automatic stabilization and control system to correct the spacecraft attitude. The spacecraft would swing back into the normal 34° orbital attitude, and the sequence would start again. The spacecraft repeated this drift and correction process nine times before retrofire. It did it once more between retrofire and the receipt of the 0.05 \"g\" (0.49 m/s²) light telemetry signal. The remaining thrusters used of fuel to keep the spacecraft properly aligned during the second orbit. Each loss of attitude cost over of fuel as compared with the entire first orbit consumption of only .\n\nIn addition to the attitude control problems, the environmental control system started having problems during the second orbit. The couch-suit circuit temperature rapidly rose from . This was an indication that the heat exchanger was freezing. The rise in suit temperature caused Enos' body temperature to rise to , then to . The medical observers began to worry about the chimp's condition. At , his body temperature stabilized. This indicated that the environmental system had started to function again. The cooling system seemed to correct itself, but the attitude problems continued.\n\nAs the spacecraft neared Hawaii on its second orbit, medical monitors were willing to let Enos continue the flight for a third orbit. However, the engineering team were concerned about the stuck thruster causing high fuel consumption, so they advised terminating the flight before the capsule ran out of attitude control gas.\n\nFlight Director Christopher Kraft alerted the Hawaii controllers to be ready to initiate retrofire to bring the spacecraft down in the Pacific, if necessary. He also alerted controllers at Point Arguello, California, to be ready to initiate retrofire as MA-5 passed over their position. He allowed the spacecraft to continue to its normal second orbit retrofire position near California. Twelve seconds before the retrofire point was reached for the normal second-orbit Atlantic primary recovery point, Kraft decided to bring Enos back to Earth. Arnold Aldrich, the chief flight controller at Point Arguello executed the command.\n\nThere was one more attitude control excursion early in reentry; after that, the rest of reentry and recovery were uneventful. The destroyers and and a P5M aircraft were waiting for the spacecraft at Station 8, the predicted landing point. Three hours and 13 minutes after launch and nine minutes before splashdown, the aircraft spotted the spacecraft at an altitude of descending on its main parachute. The information was relayed to \"Stormes\" and \"Compton\", who were away. The spacecraft recovery aids were all functioning, except for the SARAH beacon. During the descent, the aircraft continued to circle and report landing events. It remained in the area until \"Stormes\" arrived, an hour and 15 minutes after the landing. \"Stormes\" hauled Enos and his spacecraft aboard. On the deck of \"Stormes\", the MA-5 hatch was blown explosively. It was released from outside the capsule by pulling a lanyard. Blowing the hatch caused the spacecraft \"picture\" window to crack.\n\nThe spacecraft and Enos were both found to have survived the mission in good condition, although the chimp had removed all of the medical electrodes and the urine collection device from his body.\n\nOn November 4, 1962, Enos died of dysentery caused by shigellosis, which was resistant to antibiotics of the time. He had been under constant observation for two months before his death. Pathologists at Holloman reported that they found no symptom that could be attributed or related to his space flight a year before.\n\nThe Mercury spacecraft and Atlas booster had now been qualified to carry a human into orbit.\n\n"}
{"id": "1582555", "url": "https://en.wikipedia.org/wiki?curid=1582555", "title": "Metaviridae", "text": "Metaviridae\n\nMetaviridae are a family of viruses which exist as retrotransposons in a eukaryotic host’s genome. They are closely related to retroviruses: Metaviridae share many genomic elements with retroviruses, including length, organization, and genes themselves. This includes genes that encode reverse transcriptase, integrase, and capsid proteins. The reverse transcriptase and integrase proteins are needed for the retrotransposon activity of the virus. In some cases, virus-like particles can be formed from capsid proteins.\n\nSome assembled Metaviridae particles can penetrate and infect previously uninfected cells. An example of this is the gypsy, a retroelement found in the \"Drosophila melanogaster\" genome. The ability to infect other cells in determined by the presence of the retroviral \"env\" genes which encode coat proteins.\n\nMetaviridae are split into the following genera:\n\nFamilies \"Metaviridae\", \"Belpaoviridae\", \"Pseudoviridae\", \"Retroviridae\", and \"Caulimoviridae\" constitute the order \"Ortervirales\".\n\n"}
{"id": "43490575", "url": "https://en.wikipedia.org/wiki?curid=43490575", "title": "Michael J. Quigley", "text": "Michael J. Quigley\n\nMichael J. Quigley is a lieutenant commander in the United States Navy assigned to the Office of the Secretary of Defense in the Pentagon. He is a former member of the United States Army and an officer in the United States Navy Reserve, once a senior high-value detainee interrogator, who now addresses the human rights implications of counter-terrorism operations and the USA's use of torture and other inhumane and coercive interrogation techniques.\nQuigley enlisted in the United States Army in 1989, becoming a military policeman after a short time as a foot soldier.\nAfter transferring from the military police corps to military intelligence, Quigley became one of the Army's most skilled and experienced interrogators and experts in counter-terrorism. In 1998 Quigley served as an advisor to Senator George Mitchell, when Mitchell was assisting in the negotiations that ended the United Kingdom's strife with the Irish Republican Army—resulting in the \"Good Friday Peace Accords\".\n\nWhile serving in Iraq, during Operation Enduring Freedom, Quigley left the Army and accepted a direct commission into the United States Navy Reserve in August 2003. \nFollowing his deployment in Iraq Quigley accepted a job as a civilian counter-terrorism analyst at the Defense Intelligence Agency. Quigley was part of a team assigned to interrogate the \"high-value detainees\" at Guantanamo, including Abd'l Hadi al-Iraqi, a senior leader of al-Qaeda, and formerly a captive held by the CIA.\nIn 2009, based on his extensive experience as an interrogator, Quigley was seconded to aid the United States Senate Intelligence Committee as it conducted an inquiry into the CIA's use of torture and other experimental interrogation techniques.\nIn June 2013, Quigley became a Senior Fellow at Human Rights First.\n\nAs a specialist in interrogation and counter-terrorism Quigley's has briefed subcommittees of the United States Congress.\n\nNotably, Quigley also took a leadership role in organizing volunteers from the US military to go and aid victims of Hurricane Sandy, a destructive hurricane that struck the coast of New Jersey in 2012.\n\nQuigley is a member of the Baker Street Irregulars literary society dedicated to the study of Sherlock Holmes, Dr. Watson, Sir Arthur Conan Doyle, and the Victorian world.\n"}
{"id": "55116320", "url": "https://en.wikipedia.org/wiki?curid=55116320", "title": "Middlebackite", "text": "Middlebackite\n\nMiddlebackite is an organic mineral with the formula CuCO(OH). It was first discovered within a boulder from the Iron Monarch quarry in South Australia in June 1990. Peter Elliott from the University of Adelaide, Australia, identified the structure of the mineral 25 years later. He determined its crystal structure through single-crystal X-ray diffraction using synchrotron radiation. Elliot named the mineral for the Middleback Range where it originated. In 2018 middlebackite was found in Val di Fiemme, Italy, during researches that brought to the discovery of a new mineral named fiemmeite.\n\n "}
{"id": "4111503", "url": "https://en.wikipedia.org/wiki?curid=4111503", "title": "Mining simulator", "text": "Mining simulator\n\nA mining simulator is a system used to replicate elements of mining operations, for training or efficiency analysis. Mining simulation application can range from pure statistical analysis, to scale models, all the way to replica cabins of mining machinery mounted on pneumatic actuators surrounded by screens displaying three-dimensional imagery. These simulators rely on physics engines and geodata to accurately simulate the dynamics of the environment.\n\n"}
{"id": "2213763", "url": "https://en.wikipedia.org/wiki?curid=2213763", "title": "Numerical taxonomy", "text": "Numerical taxonomy\n\nNumerical taxonomy is a classification system in biological systematics which deals with the grouping by numerical methods of taxonomic units based on their character states. It aims to create a taxonomy using numeric algorithms like cluster analysis rather than using subjective evaluation of their properties. The concept was first developed by Robert R. Sokal and Peter H. A. Sneath in 1963 and later elaborated by the same authors. They divided the field into phenetics in which classifications are formed based on the patterns of overall similarities and cladistics in which classifications are based on the branching patterns of the estimated evolutionary history of the taxa.\n\nAlthough intended as an objective method, in practice the choice and implicit or explicit weighting of characteristics is influenced by available data and research interests of the investigator. What was made objective was the introduction of explicit steps to be used to create dendrograms and cladograms using numerical methods rather than subjective synthesis of data.\n\n"}
{"id": "43200369", "url": "https://en.wikipedia.org/wiki?curid=43200369", "title": "Peregrine (journal)", "text": "Peregrine (journal)\n\nPeregrine: American Immigration in the 21st Century is an online journal published by the Hoover Institution as part of its Conte Initiative on Immigration Reform. It is focused on understanding immigration to the United States and identifying optimal immigration policy for the contemporary United States. The journal is edited by economist Tim Kane and relies on contributions from Hoover's Working Group on Immigration Reform, co-chaired by Kane and Edward Lazear.\n\nThe first issue of \"Peregrine\", issue 1401, was published in late June 2014. \n\nThe news release by the Hoover Institution about the launch was published on \"CNBC\", and Herald Online\n"}
{"id": "22261471", "url": "https://en.wikipedia.org/wiki?curid=22261471", "title": "Regenerative economic theory", "text": "Regenerative economic theory\n\nRegenerative economics is an economic system that works to regenerate capital assets. A capital asset is an asset that provides goods and/or services that are required for, or contribute to, our well being. In standard economic theory, one can either “regenerate” one's capital assets or consume them until the point where the asset cannot produce a viable stream of goods and/or services. What sets regenerative economics apart from standard economic theory is that it takes into account and gives hard economic value to the principal or original capital assets — the earth and the sun. We cannot do much to affect the sun although we can value access to the sun in such areas where access can be influenced. Therefore, most of Regenerative Economics focuses on the earth and the goods and services it supplies.\n\nRegenerative economics is completely comfortable within the capitalist economic framework. Recognizing the earth as the original capital asset places the true value on the human support system known as the environment. Not having this original value properly recognized has created the unsustainable economic condition referred to as uneconomic growth, a phrase coined by leading ecological economist and steady-state theorist Herman Daly, as stated in the book Reshaping the Built Environment. The authors of the regenerative economic theory believe that uneconomic growth is the opposite of regenerative economics.\n\n"}
{"id": "20287671", "url": "https://en.wikipedia.org/wiki?curid=20287671", "title": "Self-affirmation", "text": "Self-affirmation\n\nSelf-affirmation theory is a psychological theory that focuses on how individuals adapt to information or experiences that are threatening to their self-concept. Claude Steele originally popularized self-affirmation theory in the late 1980s, and it remains a well-studied theory in social psychological research.\n\nSelf-affirmation theory contends that if individuals reflect on values that are personally relevant to them, they are less likely to experience distress and react defensively when confronted with information that contradicts or threatens their sense of self.\n\nExperimental investigations of self-affirmation theory suggest that self-affirmation can help individuals cope with threat or stress and that it might be beneficial for improving academic performance, health, and reducing defensiveness.\n\nThere are four main principles of self-affirmation theory.\n\nSelf-affirmation theory proposes that individuals are driven to protect their self-integrity. According to self-affirmation theory, self-integrity is one's concept of oneself as a good, moral person, who acts in ways that are in accord with cultural and social norms. Steele purported that the self is made up of different domains: roles, values, and belief systems. Roles include responsibilities a person has, such as being a parent, friend, student, or professional. Values are aspirations people live in accordance to, including things like living healthfully and treating others with respect. Belief systems include the ideologies to which a person ascribes, such as religious or political beliefs. Self-integrity can take many forms. For example, self-integrity can take the form of being independent, intelligent, a helpful member of a society, part of a family, and/or part of a group. Threats against a person's self-integrity are events or messages that imply an individual is not good or adequate in a personally relevant domain. Self-affirmation theory suggests that when individuals face threat to one of these domains, they are motivated to maintain a positive global image of themselves.\n\nSelf-affirmation theory purports that when individuals are faced with information that threatens their self-integrity, the response to this information is often defensive in nature. Defensive reactions attempt to minimize the threat in order to preserve the sense of self. Examples of defensive reactions include denial, avoiding the threat, and changing one's appraisal of the event in order to make it less threatening.\n\nInstead of having one self-concept (e.g., I am a good parent), self-affirmation theory posits that individuals flexibly define who they are using various roles (e.g., I am a good parent, child, and worker). Having a flexible sense of self allows individual to offset weaknesses in one domain, by highlighting the strengths in another domain. That is, if someone perceives threat to one domain, he or she can accommodate this threat by upholding a value in another domain. Self-affirmations can come from many sources. Having a flexible self-concept allows people to adapt in the face of threat.\n\nIt is theorized that engaging in activities that promote the values, beliefs, and roles that are central to an individual's identity can promote self-integrity. Promoting one's values can affirm the individual and reduce the perceived threat. Engaging in such actions when facing threat serves to remind individuals of the broad, principal values by which they define themselves and their lives. This change in perspective shifts attention away from the threat in one domain of the self toward a larger context of who they are. It is thought that when people operate from this broader perspective, they react less defensively to the threat, which allows them to act more effectively. Self-Affirmations can occur by both reflecting on a personally relevant value, belief, or role, as well as engaging in an activity that might evoke a personally relevant value, such as spending time with family if that is personally relevant.\n\nTaken together, the four principles suggest that when confronted with information that threatens self-concept, the person experiences distress and is subsequently motivated to self-defense. However, the defensive reaction can impede more adaptive ways to solve the problem (like engaging in problem-solving or changing unhealthy behaviors). It is thought that affirming an important value unrelated to the threat helps individuals to shift their perspective to their broader life context. Having a broad viewpoint diminishes the perceived threat, allowing individuals to act less defensively and more effectively.\n\nIn laboratory experiments, psychologists induce self-affirmation in participants in order to investigate the influence of self-affirmation on individuals' well-being. There are two chief methods used to self-affirm participants in experimental studies.\n\nOne of the most common ways to induce self-affirmation is to have participants write about a personally relevant value. In order to do this, participants rank a list of values from most important to least important to them. The list typically includes the following value domains, business, art/music/theater, social life/relationships, science/pursuit of knowledge, religion/morality, and government/politics. Participants then write about the value they ranked as most important and how it is meaningful to them for 10 minutes.\n\nThe Allport-Vernon-Lindzey Values Scale is another measures that is administered to participants to induce self-affirmation in the laboratory. Participants choose one of two answers after reading a statement about a value. An example item from the religion scale would be “The more important study for mankind is (a) mathematics (b) theology”. This allows researchers to see where an individual's personal interests and values lie. It is thought that answering questions about a value domain that they find important will make that value more salient to them, resulting in self-affirmation. There are six different value domains on the Allport-Vernon-Lindzey Values scale: religion, theoretical, aesthetic, political, social, and economic.\n\nExtant empirical research demonstrates that self-affirmations can be beneficial in reducing individuals' stress response as well as their defensiveness toward threats.\n\nIn one study investigating the effects of self-affirmation on stress response, undergraduate participants completed the Trier Social Stress Test, a standardized laboratory paradigm used to induce stress in participants. In the Trier Social Stress task, participants are asked to give a short speech in front of a panel of judges who do not give any comments or positive feedback to the participant. Following the speech, participants must complete a mental arithmetic task, in which they count backwards from 2,083 in increments of 13 while being told by the judges to go faster. Prior to completing the Trier Social Stress Task, half of the participants completed a variation of values list self-affirmation task. Participants who completed the values list had significantly lower stress response than individuals in the control condition, as indicated by a lower cortisol response in affirmed participants compared to participants who did not complete the self-affirmation condition.\n\nIn a different experiment, undergraduate students completed difficult problem solving puzzles in the presence of an evaluator. Participants also reported how much chronic stress they endured over the past month. Prior to completing the problem-solving puzzles half of the participants completed a values essay self-affirmation task. For the individuals who did not complete the self-affirmation task, low-stress participants performed significantly better than high-stress participants. For individuals in the self-affirmation condition, high-stress individuals performed equally as well as low-stress individuals. Findings suggest that self-affirmation buffered against the negative effects of stress on problem-solving performance.\n\nFor another experiment, undergraduates were recruited to participate in a research study two weeks prior to completing a mid-term exam. All participants collected urine samples for 24 hours two weeks prior to their midterm (baseline) as well as for the 24 hours prior to their midterm examination so levels their catecholamine levels could be measured. Catecholamine levels are thought to be high when individuals are experiencing higher stress. Half of participants completed two values essays in the two weeks leading up to their midterm examination. Participants who did not complete the self-affirmation condition demonstrated increased catecholamine response from baseline to their midterm exam. However, participants who completed the two values essays did not show an increase in catecholamine levels from baseline to their midterm.\n\nPreliminary research results suggest that self-affirmation can protect against the negative consequences of stress. More research is needed to understand \"how\" self-affirmations decrease stress responses.\n\nResults from studies provide support for the idea that when individuals complete an activity that affirms their self-integrity they are less defensive and more accepting of information that is potentially threatening. However, more research is needed to better understand \"why\" people are more open-minded after they have completed an affirmation task.\n\nStudies have examined the effects of self-affirmation on the academic performance of historically marginalized groups such as African American and Latino American students, who face a multitude of daily threats in the school environment. Seventh grade students took part in a two-year study. Half of the students completed a values essay about their most important value approximately seven to eight times over the course of two academic years, while the other half wrote a values essay about why their least important value might be of value to someone else. The study tracked the students' grades for three years. Ethnic minority students in the self-affirmation condition received higher grade point averages than the ethnic minority students who wrote about why their least important value might be important to someone else. There was no effect of self-affirmation in white students. Findings suggest that for students who face daily, repeated stressors at school, self-affirmation buffers against worsening school performance.\n\nSimilarly, values affirmation decreased the achievement gap for college students from low socioeconomic status and for women in introductory physics courses. These findings suggest that self-affirmation can have a buffering effect on academic achievement for groups who face the most threat.\n\nWomen concerned with their weight were recruited for a study. Concern with weight has similar effects of stress in that it can cause psychological distress, poor eating, and weight gain. Half of the women completed a values essay. Self-affirmed participants had lost more weight, had lower body mass index, and smaller waist circumference than non-affirmed women.\n\nPatients with end stage renal disease participated in a study assessing the effects of self-affirmation on adherence to phosphate binders that facilitate control of phosphate levels. Poor phosphate control in this population can be dangerous and life-threatening. There was a significant improvement in serum phosphate levels for the affirmed patients compared to the non-affirmed patients, suggesting better adherence to phosphate binders.\n\nIndividualist and collectivist cultures place different levels of importance on belonging to in-groups, and it is thought that this may vary the effects of self-affirmation. One study investigated the effects of self-affirmation on reducing cognitive dissonance. This study found that self-affirmed participants from individualist cultures saw reductions in cognitive dissonance, whereas self-affirmed participants from collectivist cultures did not experience a reduction in cognitive dissonance. Another study examined the effect on individuals from individualist and collectivist backgrounds of writing a values essay about a value that was important to the participant compared to a value that was important to the participant's family. The authors found reduced cognitive dissonance for participants from collectivist cultures who wrote about values important to them and their families, and found reduced cognitive dissonance for participants from individualist cultures who wrote about a value important to just them.\n\nBenefits from self-affirmation are thought to primarily occur when the perceived threat is in an area of importance to the individual. For example, in the experiment detailed above in which coffee drinkers read an article about caffeine consumption and increased risk of breast cancer, self-affirmation only reduced defensiveness in individuals who were heavy coffee drinkers and not in occasional coffee drinkers. Because an article on the risks associated with caffeine consumption might not pose the same threat to occasional coffee drinkers as to heavy coffee drinkers, self-affirmation likely does not provide the same benefit for occasional coffee drinkers. Thus, the importance of the threatened domain to one's self-integrity is thought to influence the effectiveness of self-affirmations.\n\nResearch has not yet identified the underlying mechanisms of how self-affirmation buffers against stress and reduces defensiveness. However, it is believed that there is not just one factor responsible for the effects of self-affirmation, but rather many. To date, increasing positive emotions and self-esteem have been investigated as mechanisms of self-affirmation, but the findings are mixed. Some studies have found that positive mood brings about similar reductions in defensiveness as self-affirmations. In contrast, several studies fail to detect any effect of self-affirmation on mood, suggesting self-affirmation does not operate via increases in positive mood. Similarly, results on the effects of self-affirmation on self-esteem are also mixed. Some studies have observed increases in self-esteem following self-affirmation, whereas other have found no effect on self-esteem. More research is needed to better understand \"how\" self-affirmation can provide benefit to individuals.\n"}
{"id": "17908527", "url": "https://en.wikipedia.org/wiki?curid=17908527", "title": "Sergei Alphéraky", "text": "Sergei Alphéraky\n\nSergei Nikolaevich Alphéraky (1850–1918) (sometimes Alphéraki or Alferaki) was a Russian ornithologist and entomologist who specialised in Lepidoptera.\n\nSergei Alphéraky was born into the noble Greek family of Alferakis and was the brother of composer Achilles Alferaki. His father Nikos Alferakis owned the Alferaki Palace in Taganrog. Sergei studied at Moscow University (1867–1869), then with Otto Staudinger in Dresden (1871–1873). On his return to Russia he worked on the Lepidoptera of the Taganrog, Rostov-on-Don region. He also collected in the North Caucasus. After that he devoted himself to the insects, especially Lepidoptera, of Central Asia. He worked on the Lepidoptera collected by Nikolai Przhevalsky in Tibet held by the Zoological Museum of the Russian Academy of Science and those collected by Grigorij Nikolaevich Potanin in China and Mongolia in the same institution. Later he studied the collections made by Alfred Otto Herz in Amur, Korea and Kamchatka, and those of Nicholas Mikhailovich Romanoff (Grand Duke Nicholas Mikhailovich), a friend from his two years at Moscow University. He was an honorary member of both the Russian Entomological Society and the Royal Entomological Society of London.\n\nPartial list\n\n\n"}
{"id": "14811845", "url": "https://en.wikipedia.org/wiki?curid=14811845", "title": "The Pleasure of Finding Things Out", "text": "The Pleasure of Finding Things Out\n\nThe Pleasure of Finding Things Out is a collection of short works from American physicist Richard Feynman, including interviews, speeches, lectures, and printed articles. Among these is his famous 1959 lecture \"There's Plenty of Room at the Bottom\", his report on the Space Shuttle \"Challenger\" disaster, and his speech on scientific integrity in which he coined the term \"cargo cult science\". The original foreword was written by Freeman Dyson. \n\n"}
{"id": "765388", "url": "https://en.wikipedia.org/wiki?curid=765388", "title": "Timeline of the Lewis and Clark Expedition", "text": "Timeline of the Lewis and Clark Expedition\n\nThis is the timeline of the Lewis and Clark Expedition through the American West (1803–1806).\n\n\n\n\n\n"}
{"id": "56637979", "url": "https://en.wikipedia.org/wiki?curid=56637979", "title": "Timeline of the open-access movement", "text": "Timeline of the open-access movement\n\nThe following is a timeline of the international movement for open access to scholarly communication.\n\n\n\n\n\n"}
{"id": "58742599", "url": "https://en.wikipedia.org/wiki?curid=58742599", "title": "Venetia Burney Student Dust Counter", "text": "Venetia Burney Student Dust Counter\n\nThe Venetia Burney Student Dust Counter (VBSDC) is a scientific instrument aboard the unmanned \"New Horizons\" space probe that is designed to detect dust impacts in outer space. VBSDC is the first planetary science instrument to built by students. The dust counter was launched in 2006, and named later that year after Venetia Burney. The detector works when dust strikes films of polarized polyvinylidene fluoride (PVDF), which generates an electrical charge. The dust is then detected over the course of the \"New Horizons\" spacecraft flight out of the solar system and passed Pluto.\n\nIn 2010, VBSDC collected data on dust past 18 AU 1.67 billion miles, which is the distance the \"Pioneer 10\" and \"Pioneer 11\" dust counters stopped working. Five other spacecraft have carried dust detectors beyond the asteroid belt including \"Pioneer 10\", \"Pioneer 11\", \"Ulysses\" (heliocentric orbit out to the distance of Jupiter), \"Galileo\" (Jupiter Orbiter), and \"Cassini\" (Saturn orbiter). The \"Voyager 1\" and \"Voyager 2\" spacecraft did detect dust by using data from the Plasma Wave instrument, but did not have dedicated dust detection instruments. The Pioneer dust detectors stopped working at 18 AU.\n\nThe impacts of the dust is calculated to provide the mass and the velocity of the dust. One of the natural structures of the solar system the VBSDC is design to detect, is the Zodiacal cloud.\n\nThe detector is designed to detect dust between 10 and 10 in mass and between 0.5 and 10 micro(μ) meters in size. By early 2012, the dust counter and produced data along a 23 AU path out the solar system. By 2008 it had taken dust measurements between 1.2 and 11 AU.\n\nThe direction of the dust impact is calculated by noting what direction the instrument is facing due to the orientation of the spacecraft. The instrument has very low power consumption.\n\nExamples of periods of Measurements: \n\nData collected in the inner Solar system was compared to similar data from \"Galileo\" and \"Ulysses\" spacecraft.\n\nVBSDC recorded the first measurements of sub-micron space dust in the outer solar system. In the outer solar system VBSDC recorded an average flux of dust of grain size larger than 2 × 10 grams of 2.5 × 10 m s.\n\nIn June 2006 the student dust counter was named in honor of Venetia Phair (née Burney) who came up with the name Pluto in the 1930s as a girl, and she was given a plaque related to this naming in December, 2006. Venetia suggested the name Pluto after the discovery of the new planet by Clyde Tombaugh in 1930 at Lowell Observatory. The name Pluto was selected in a vote by the observatory's astronomers.\n\nPreviously the instrument was called the \"Student Dust Counter\".\n\n\n"}
{"id": "24950820", "url": "https://en.wikipedia.org/wiki?curid=24950820", "title": "War and Peace in the Global Village", "text": "War and Peace in the Global Village\n\nWar and Peace in the Global Village by Marshall McLuhan and Quentin Fiore is a collage of images and text that illustrates the effects of electronic media and new technology on man. Marshall McLuhan used James Joyce's \"Finnegans Wake\" as a major inspiration for this study of war throughout history as an indicator as to how war may be conducted in the future. (1st Ed.: Bantam, NY; reissued by Gingko Press, 2001 ),\n\nJoyce's \"Wake\" is claimed to be a gigantic cryptogram which reveals a cyclic pattern for the whole history of man through its Ten Thunders. Each \"thunder\" below is a 100-character portmanteau of other words to create a statement he likens to an effect that each technology has on the society into which it is introduced. In order to glean the most understanding out of each, the reader must break the portmanteau into separate words (and many of these are themselves portmanteaus of words taken from multiple languages other than English) and speak them aloud for the spoken effect of each word. There is much dispute over what each portmanteau truly denotes.\n\nMcLuhan claims that the ten thunders in \"Wake\" represent different stages in the history of man:\n"}
{"id": "32857704", "url": "https://en.wikipedia.org/wiki?curid=32857704", "title": "Wayne Maddison", "text": "Wayne Maddison\n\nWayne Paul Maddison , is a professor and Canada Research Chair at the departments of zoology and botany at the University of British Columbia, and the Scientific Director of the Beaty Biodiversity Museum.\n\nHis research concerns the phylogeny, biodiversity, and evolution of jumping spiders (Salticidae), of which he has discovered new species and genera.\n\nHe has also done research in phylogenetic theory, developing and perfecting various methods used in comparative biology, such as character state inference in internal nodes through maximum parsimony, squared-change parsimony, or character correlation through the concentrated changes test or pairwise comparisons. In collaboration with David R. Maddison, he worked on the Mesquite open-source phylogeny software, the MacClade program, and the Tree of Life Web Project.\n\nHis research has led him to discover new species of jumping spiders in Sarawak and Papua New Guinea.\n\n\n"}
