{"id": "3378234", "url": "https://en.wikipedia.org/wiki?curid=3378234", "title": "A299 road", "text": "A299 road\n\nThe A299, better known as the Thanet Way, is a major road in the county of Kent, England, and runs from Brenley Corner near Faversham (where it merges into the M2) to Ramsgate via Whitstable and Herne Bay. It is predominantly used for freight traffic to Ramsgate Harbour and local traffic to Thanet, and is long. It also provides access to Manston Airport.\n\nThe A299 was originally allocated to the original road from Faversham to Herne Bay, via Graveney, Seasalter and Whitstable. Most of the modern route was constructed in the early 1930s as an unemployment relief project. Prior to this, all traffic from the west to the Isle of Thanet had to go via Canterbury.\n\nThe A299 was upgraded between 1989 and 1997 to dual carriageway for almost its entire length. Whitstable and Herne Bay were bypassed, with the old road becoming the A2990, and the A299 to the west of Whitstable and east of Herne Bay received an online upgrade. \n\nThe original Thanet Way project, and hence the A299, used to end near Monkton, where the A253 once continued to Ramsgate. This section of road has been renumbered as an extension of the A299 to Ramsgate Harbour. \n\nThe section from Minster roundabout to the Lord of the Manor roundabout was diverted on to a new dual carriageway during 2009-12, constructed as part of East Kent access phase 2 to provide better access to Manston Airport and the Pfizer complex at Sandwich. It was opened by Norman Baker on 23 May 2012.\n\n\n"}
{"id": "40748267", "url": "https://en.wikipedia.org/wiki?curid=40748267", "title": "ADAPT – Association for International and Comparative Studies in Labour and Industrial relations", "text": "ADAPT – Association for International and Comparative Studies in Labour and Industrial relations\n\nThe Association for International and Comparative Studies in Labour and Industrial Relations (ADAPT) is a non-profit organization that conducts and supports academic research in the field of labour and industrial relations.\n\nADAPT works in collaboration with national and international partners. It was founded in 2000 by Professor Marco Biagi. and is based in Bergamo, Italy. ADAPT is engaged in research in collaboration with a network of national and international partners. \nADAPT’s publications by ADAPT University Press are both online and in print: The Journal of Industrial Relations Law (in Italian), Labour Studies Book Series which is published by Cambridge Scholars Publishing, working papers, International Bulletin, and E-Journal of International and Comparative Labour Studies (in English and Spanish).\n\nADAPT offers two international PhD programs: The first is PhD in Labour Law at the University of Modena and Reggio Emilia in collaboration with the “Marco Biagi” Center for International and Comparative Studies and the University of Modena and Reggio Emilia and the second is PhD in Human Capital Formation and Labour Relations in collaboration with the University of Bergamo. Since their launch in 2007, the two PhD programs have enrolled 248 students and have graduated 138 PhDs. Since 2007, ADAPT has secured for its doctoral candidates 86 private scholarships through its partners and 85 public scholarships granted by the Italian Ministry of Higher Education and Research under an agreement.\n"}
{"id": "2097796", "url": "https://en.wikipedia.org/wiki?curid=2097796", "title": "Aladdin (crater)", "text": "Aladdin (crater)\n\nAladdin is a crater in the northern hemisphere of Saturn's moon Enceladus. Aladdin was first discovered in \"Voyager 2\" images. It is located at 60.7° North Latitude, 26.7° West Longitude and is 37.4 kilometers across. It is located near the craters Ali Baba and Samad. Aladdin has a large dome in its interior, suggesting the crater has undergone some viscous relaxation.\n\nAladdin is named after a famous hero from \"Arabian Nights\" who finds a magic lamp.\n"}
{"id": "58857377", "url": "https://en.wikipedia.org/wiki?curid=58857377", "title": "Alison Goate", "text": "Alison Goate\n\nAlison M. Goate is a professor of neuroscience and director of the Centre on Alzheimer's Disease at Icahn School of Medicine at Mount Sinai, New York City. She was previously professor of genetics in psychiatry, professor of genetics, and professor of neurology at Washington University in St. Louis.\n\nFor her contributions to research on Alzheimer's disease she has received the Potamkin Prize (1993), the Metlife Foundation Award (1994), and a Lifetime Achievement Award from the Alzheimer's Association (2015). She is a fellow of the American Association for the Advancement of Science.\n"}
{"id": "45382053", "url": "https://en.wikipedia.org/wiki?curid=45382053", "title": "Amalgaviridae", "text": "Amalgaviridae\n\nAmalgaviridae is a family of double-stranded RNA viruses that has one genus: \"Amalgavirus\". Members of both the family and the genus are referred to as amalgaviruses. There are currently four recognized species of the family: \"Blueberry latent virus\", \"Rhododendron virus A\", \"Southern tomato virus\", the type species of \"Amalgavirus\", and \"Vicia cryptic virus M\". The family and genus are called \"amalga\", from \"amalgam\", due to the viruses possessing characteristics of both partitiviruses and totiviruses, indicating a likely genetic relation to those two families. Members of this family infect plants and are transmitted vertically via seeds. Their genomes are monopartite, about 3.5 kilobases in length, and contain two partially overlapping open reading frames, encoding the RNA-dependent RNA polymerase (RdRp) and a putative capsid protein.\n\nIt has been suggested that amalgaviruses have evolved via recombination between viruses with double-stranded and negative-strand RNA genomes. Specifically, phylogenetic analyses have shown that the amalgavirus RdRps form a sister clade to the corresponding proteins of partitiviruses (Partitiviridae) which have segmented (bipartite) dsRNA genomes and infect plants, fungi and protists. By contrast, the putative capsid protein of amalgaviruses is homologous to the nucleocapsid proteins of negative-strand RNA viruses of the genera Phlebovirus (Bunyaviridae) and Tenuivirus.\n"}
{"id": "51058214", "url": "https://en.wikipedia.org/wiki?curid=51058214", "title": "Aurora Green Diamond", "text": "Aurora Green Diamond\n\nThe Aurora Green Diamond is a vivid green diamond with VS2 clarity. In May 2016, the Aurora Green became the largest vivid green diamond to ever sell at auction. The record was previous held by a 2.54 carat Fancy Vivid Green VS1 diamond that was sold by Sotheby’s on November 17, 2009 for $1.22 million per carat according to the Diamond Investment & Intelligence Center. On May 31, 2016, the diamond, which was originally owned by Scarselli Diamonds was sold by Christie's for a record price per carat of $3.3 million to Chinese jewelry company Chow Tai Fook, totaling $16.8 million.\n\n– The largest Fancy Vivid Green Diamond ever to be offered at auction.\n– The most expensive Green Diamond in the world to be sold at auction.\n– The highest per carat price ever sold for any Green Diamond in the world at auction.\n– The most expensive Green Diamond to be sold in Asia.\n\n"}
{"id": "14582368", "url": "https://en.wikipedia.org/wiki?curid=14582368", "title": "Background-oriented schlieren technique", "text": "Background-oriented schlieren technique\n\nBackground-oriented schlieren (BOS) is a novel technique for flow visualization of density gradients in fluids using the Gladstone–Dale relation between density and refractive index of the fluid.\n\nBOS simplifies the visualization process by eliminating the need for the use of expensive mirrors, lasers and knife-edges. In its simplest form, BOS makes use of simple background patterns of the form of a randomly generated dot-pattern, an inexpensive strobe light source and a high speed digital camera.\n\nIn its initial stages of implementation, it is mostly being used as a qualitative visualization method. Further research into developing the technique will enable quantitative visualization of fluid flows, in various applications including cryogenic flows, supersonic and hypersonic flows, biomedical device flow visualization etc.\n\nBright light sources such as the sun and moon can also be used to visualize shock wave patterns around aircraft, a technique named Background Oriented Schlieren using Celestial Objects.\n\n\n"}
{"id": "54030155", "url": "https://en.wikipedia.org/wiki?curid=54030155", "title": "Barry Ache", "text": "Barry Ache\n\nBarry W. Ache is an American neuroscientist currently a Distinguished Professor of Biology and Neuroscience at Whitney Laboratory for Marine Bioscience, University of Florida.\n"}
{"id": "34049370", "url": "https://en.wikipedia.org/wiki?curid=34049370", "title": "Bovine stool associated circular virus", "text": "Bovine stool associated circular virus\n\nBovine stool associated circular virus is a single stranded DNA virus with a circular genome that was isolated from bovine stool.\n\nThis virus was isolated from cows that appeared to be healthy. It has also been isolated from healthy pig stool.\n\nThe genome is a single stranded circular DNA molecule 2600 bases in length. It has two open reading frames encoding a replicase and capsid protein. The reading frames are arranged in opposite orientations on the genome. A stem loop is present between the 3' ends of the open reading frames. This is like chimpanzee stool associated circular virus and unlike any other known circular DNA virus.\n\nThis virus appears to form a clade with the chimpanzee stool associated circular virus. Their relationship with other viruses is not yet known. The closest relations appear to be the \"Nanoviridae\" but further work is required to clarify this point.\n"}
{"id": "42501804", "url": "https://en.wikipedia.org/wiki?curid=42501804", "title": "Cognitive behavioral training", "text": "Cognitive behavioral training\n\nCognitive behavioral training (CBTraining), sometimes referred to as structured cognitive behavioral training, (SCBT) is a regimented cognitive-behavioral process that uses a systematic, highly structured workshop-style approach to break down and replace dysfunctional emotionally dependent behaviors. The roots of CBTraining lie in cognitive behavioral therapy (CBT), and like CBT the basic premise of CBTraining is that behavior is inextricably related to beliefs, thoughts and emotions. However, CBTraining is delivered in a highly structured, regimented format and combines several other behavioral change theories and methodologies in addition to CBT. Cognitive behavioral training is currently used primarily in the behavioral health industry and in criminal psychology.\n\nAlthough CBTraining employs some similar foundational concepts that define Cognitive Behavioral Therapy, there are some fundamental differences between CBTraining and CBT, both in philosophy and in application.\nCBTraining is training, not therapy. This is a critical distinction: unlike typical forms and applications of CBT, CBTraining is a process that is absolutely finite. That is to say, when a person begins taking a structured cognitive behavioral training course or program, there is already in place a set schedule – and therefore a predetermined end date – to training. Among one other main difference is the goal for a change in \nIn CBT, as with most therapy, the patient plays a large role in determining the direction of the therapy, including the intensity and duration. An CBTraining course, or program, is usually broken up into a series of progressive, strategically ordered sessions – each one with a particular focus. CBTraining unfolds step by step, guiding the participant through the process of retraining the brain and dissolving the emotionally dependent thinking. Participants of SCBT take \"classes,\" are quizzed, and are given \"homework\". Much of the homework in CBTraining is based on writing therapy; as people write their emotional experience, their sense of trauma and stress is diminished. CBTraining is not tailored to the individual, it is tailored to the specific dependency it addresses. Cognitive behavioral training aims to create rapid change in the \"students,\" is given in some sort of group or class form, and is governed by a prearranged pathway to change.\nCognitive Behavioral Skills Training has been used to teach and firmly implant social behavioral modification in children. Here is a concise explanation of a main difference between cognitive behavioral therapy and cognitive behavioral skills training: \"...cognitive behavioral therapy focuses on how beliefs affect mood...skills training focuses on practicing pragmatic skills of living...\" CBTraining aims for swift and permanent implantation of new skills in living.\n\nCognitive behavioral training (CBTraining) is a cognitive-based process designed with the aim to systematically break down emotionally driven dependencies and behaviors, replacing them with behaviors that are based on rational choice. Like Cognitive Behavioral Therapy (CBT), the philosophy of CBTraining is rooted to one essential notion: feeling and thought are inextricably linked.\n\nCBTraining contends that in any emotionally dependent relationship, people make emotional decisions rather than rational choices. When an emotionally dependent relationship occurs, it creates a belief-induced emotional state. When someone is in this state, they are incapable of making rational choices for more than a short period of time due to an emotion-driven subconscious process that overrides their conscious mind. This phenomenon explains why an overeater cannot resist having another bite... and another... and another, despite the fact that they are not physically hungry and they know that overeating is unhealthy.\n\nCBTraining was born out of cognitive behavioral therapy (CBT) and utilizes well established theories of psychology and treatment, including: bibliotherapy, acceptance and commitment therapy, behavioral self-control theory, behavioral economic theory, self-determination theory, and motivational interviewing. Each of these methods plays a role in CBTraining, at varying stages of completion, in the common endeavor to bring a new sense of cognizant awareness in the participant.\n\nCognitive Behavioral Training, applied in a structured way (CBTraining), has also been used to deal effectively with women dealing with the stressors of having breast cancer (e.g., changing thoughts about stressors ) in studies done at the University of Miami.\n\nIn addressing addictive behavior and other potentially destructive behavior compelling to the participant, CBTraining uses an approach of urge conditioning/desensitization. This approach stands in stark contrast to what is commonly most instinctive to people (urge avoidance), and seems counter-intuitive at first. The approach of urge desensitization has been applied to patients with gambling addictions, and research has shown it to be effective.\nWhen a person is trying to quit smoking, for instance, the instinct is to remove all smoking paraphernalia from his presence. While this \"out of sight, out of mind\" approach seems to make sense, it does nothing to actually deal with the emotionally driven urge to smoke. CBTraining contends that, in fact, this white-knuckled method of willpower only lends the urge more power. With urge conditioning/desensitization, the participant is asked to actually create urges in order to become comfortable with having them. As the strategic format of CBTraining unfolds through the training, the urges are methodically stripped of their power. In acknowledging \"urge\", says CBTraining, you are actually acknowledging your power to choose.\n\nFurther distinguishing CBTraining from its closely related psychological predecessors is the inclusion of the concept of \"Training\" in place of \"Therapy\". CBTraining is a planned, intricately designed and systematically applied regimen that is purposely finite. CBTraining begins with a specific goal, and is constructed as a time-specific road map to achieving the goal. As a person undergoes the training, CBTraining aims to change not only this person's self-talk but his/her self-image at a subconscious level, so that the change in behavior is intrinsically motivated and willpower never really plays a part in it.\n\nAlong with CBT, CBTraining also owes some debt to Albert Ellis's rational emotive behavior therapy (REBT), formerly known as Rational Emotive Therapy. REBT is classified as a form of CBT, and is anchored by the belief that a person is \"affected emotionally by his/her perspective and attitude about outside things.\" As with CBTraining, REBT incorporates Positive Self-Image Psychology.\nLou Ryan, a pioneer in the creation, development, and practical application of CBTraining, worked for some time under the guidance of Albert Ellis. In the early 1980s, Ryan, who was well-versed in Ellis's theories and philosophies, met Ellis in Hawaii after a series of seminars. Ellis recognized his own impact in Ryan's CBTraining programs, and played a peripheral part in some of the development.\nAccording to Ryan when asked about the evolutional process of his particular philosophy: \"I had collected a potpourri of different types of approaches, programs, and philosophies that I had come across through the years. To me, what seemed to be missing was vision. Vision for what people want from life is motivating but can often be lost in the daily trudge of life.\" About his time with Ellis, Ryan said: \"I had met him at a conference in Hawaii. We met afterwards, and I told him what I was doing, that there were similarities between what we were doing. I asked him if he would review what I had put together, and for his input... he coached me on a few things. It occurred to me to add a structure to all of these philosophies.\"\nAlso involved in some of the early development of SCBT were John E. Martin and Scott T. Waters. In a written critical evaluation of SCBT] is the strategic and effective use of techniques that exclusively target people who are ready to change now.\" They went on to say, in the same report, \"...the rigidity of language and pacing might result in negative feelings towards a Training that gives so little choice itself.\"\n\nCBT has been used most notably in two different spectrums: health and wellness, and criminal psychology (with the intention to minimize recidivism).\n\nCBTraining has been established to some degree in changing emotionally addictive behaviors related to tobacco. There is evidence that cognitive group behavioral training may be beneficial for patients with type 1 diabetes in their self-care. SCBT has been used to help people with diabetes manage their disease, with the primary goal being maintained lifestyle changes to slow or halt the progression of the disease. San Diego based SelfHelpWorks Inc. has incorporated SCBT as a core component of its behavioral training courses for smoking cessation, alcohol management, weight management, diabetes management, and stress management. Case studies within the American workplace indicate the efficacy of CBTraining, however as of yet academic studies in this promising area are still relatively sparse.\nCognitive Behavioral Training has been used, combined with diet and exercise to reduce body weight and raise physical capacity in health care workers.\n\nCBTraining has been perhaps most documented in its efficacy with addiction-related issues that involve substances such as food, alcohol and tobacco. Since the philosophy of CBTraining is that feeling is related to thought, and thought can lead to belief, it assumes that addiction in itself is at least partially dependent on the continued dysfunctional and oft-practiced thoughts and beliefs of the addict. Much of CBTraining is the encouragement to expand your \"comfort zone\" until the discomfort of physical withdrawal is no longer bothersome. For instance, when seasoned smokers decide to quit on their own, they typically consider it a matter of willpower, following the natural instinct to avoid cigarettes and \"white knuckle\" it. By contrast, CBTraining focuses on the cognitive aspects. It systematically applies a combination of techniques that makes the smoker consciously aware of the urge to smoke, explains the emotionally charged thought process that is really causing the urges, and brings the smoker to the point of purposefully creating urges while choosing the benefits of not smoking over the consequences of smoking in a way that no longer causes discomfort.\n\nSCBT contends that high levels of stress and anxiety are the practiced, subconscious choice of the mind. If stress is a feeling related to thought, and if thinking thoughts that produce stress is an unconscious habit, then this unconscious habit must first be made conscious to the patient. Once this is accomplished, the power to change thought, and hence feeling, is feasible.\nIn a nine-week \"brief structured cognitive behavioral intervention\" designed for family caregivers for persons diagnosed with Alzheimer's disease, there was a reduction in stress and anxiety – \"Moreover, these reductions in anxiety were maintained through a six-week follow-up period...\"\n\nSCBT has been used in helping diabetics manage their disease. Since diabetes is a disease that needs to be managed to a large degree by the patient himself/herself, much is dependent on the patient's lifestyle. In addition to producing a shift in mindset from living like a victim to living normally, SCBT has been used to help change pre-existing aversions to healthy living that are known to exacerbate the diabetic condition or, in the case of type 2 diabetes, may even have brought it on in the first place.\n\nIn all areas CBTraining is applied to, there is maintained a key concept that when one is constantly aware of the power to choose, one will not be vulnerable to feelings of deprivation.\n\nA training program was developed and applied with the specific aim to increase self-control and reduce aggression in young people. Students were divided into six classes, with five students comprising each class.\n\nThere was also a study regarding the effectiveness of CBTraining in hyperactive boys in anger-inducing situations \n\nCognitive-behavioral programs developed for criminal offenders tend to focus on either cognitive deficits or cognitive distortions. Numerous studies have been conducted in correctional settings testing the effectiveness of cognitive-behavioral techniques at reducing recidivism. Highly individualized one-on-one cognitive-behavioral therapy, provided by clinical psychologists or other mental health workers, is simply not practical on a large scale within our prison system. Therefore cognitive-behavioral therapies in correctional settings consist of highly structured treatments that are typically delivered to groups of 8 to 12 individuals in classroom-like settings.\n\nIn the use of structured cognitive programs for convicted criminals to reduce recidivism, the method was in person, and in group form. There is no evidence that, when targeting this particular issue, SCBT has been used in any other format to date.\n\nAlthough studies have been limited, initial data indicates that success with SCBT is largely dependent on the active, cooperative participation of the patient. The number one factor in inefficacy is failure to complete the training.\nThis essentially means that CBTraining, as it is presented in internet form, is geared towards participants who, in relation to the stages-of-change theory, are in the preparation and action stages. In other words, CBTraining will be most effective when applied to people with a high motivation to change. This is not necessarily a bad thing, but it is an indicator of limitations. Simply put, SCBT is not likely to be effective treatment in someone who has no intention to change.\n"}
{"id": "25262062", "url": "https://en.wikipedia.org/wiki?curid=25262062", "title": "Critical security studies", "text": "Critical security studies\n\nCritical security studies is an academic discipline within security studies which rejects mainstream approaches to security\n\nSome use the term critical security studies to refer to all approaches that are critical of mainstream/orthodox realist approaches. Others see critical security studies as a distinct approach in its own right committed to emancipatory theory. Those representing the latter view are usually referred to as the Welsh School (security studies).\n"}
{"id": "3230759", "url": "https://en.wikipedia.org/wiki?curid=3230759", "title": "David Ashton (botanist)", "text": "David Ashton (botanist)\n\nDavid Hungerford Ashton OAM (6 July 1927 – 22 November 2005) was an Australian botanist and ecologist. He was the world expert on \"Eucalyptus regnans\" forests, claimed to be the most important timber species in Australia.\n\nAshton was born in Melbourne. He received his Bachelor of Science in 1949, and a PhD in 1957. He taught for thirty years at the University of Melbourne, from 1962 to 1992, influencing several generations of Victorian botanists and foresters. His professional expertise ranged from angiosperms, pteridophytes, bryophytes, lichens and fungi. He was also able to synthesise many biological problems ecologically, especially in Mountain Ash forests including geology, plant and animal species interactions, the effects of fire and climate, insect and seed dispersal. He wrote more than 200 scientific articles in over 20 publications.\n\nSince 2000, 'The David Ashton Biodiversity and Ecosystems Award' has been awarded annually for the best Victorian ecological research.\n\nHe was awarded the Medal of the Order of Australia in 2001, \"For service to the science of plant ecology, particularly in the areas of forest regeneration, conservation and management.\" \n\n"}
{"id": "52906381", "url": "https://en.wikipedia.org/wiki?curid=52906381", "title": "David L. Jones (botanist)", "text": "David L. Jones (botanist)\n\nDavid Lloyd Jones (born 1944) is an Australian horticultural botanist and the author of a large number of books and papers, especially on Australian orchids.\n\nJones was born in Victoria and in his youth, was a student at Burnley Horticultural College, then the University of Melbourne, graduating with the degree of Bachelor of Science in Agriculture. He was employed for 14 years by the Victorian Department of Agriculture where he helped develop programs involving the nutrient requirements of Australian native plants. He later owned several commercial nurseries and in 1972 his first description of an orchid, \"Pterostylis aestiva\" was published, then in 1978, his first book, \"Australian Ferns and Fern Allies\", written with Stephen Clemesha was published. In 1987 Jones worked first as a horticultural research officer at the Australian National Botanic Gardens in Canberra and a year later began an intensive study of the taxonomy of Australian plant groups, especially orchids. From 1994 he worked as a research scientist in the Orchid Research Group at the Centre for Plant Biodiversity Research until his retirement in 2007.\n\nJones facilitated a cooperation with groups of both professional and amateur botanists which has led to the description of many new species. Jones has travelled extensively and visited many remote areas of Australia in the search for new orchid species. He is the author of more than 350 papers describing new species of orchids, 18 of cycads as well as of other groups, including the fern genus \"Revwattsia\" and has published many books on Australian plants. He is described by fellow botanist, Mark Alwin Clements as \"without doubt one of the outstanding botanists of our time\" and by Rob Cross and Roger Spencer, horticultural botanists at the Royal Botanic Gardens, Melbourne, as \"probably the most prolific horticultural botanist that Australia has produced.\"\n\n"}
{"id": "3912555", "url": "https://en.wikipedia.org/wiki?curid=3912555", "title": "David White (geologist)", "text": "David White (geologist)\n\nCharles David White (July 1, 1862 – February 7, 1935), who normally went by his middle name, was an American geologist, born in Palmyra, New York.\n\nHe graduated from Cornell University in 1886, and in 1889 became a member of the United States Geological Survey. Eventually, he rose to be chief geologist.\n\nIn 1903 he became an associate curator of paleobotany at the Smithsonian Institution. He wrote numerous papers on geological and paleontological subjects.\n\nThe David White House, his home for 15 years, is a U.S. National Historic Landmark.\n\nHe made one of the most comprehensive studies on the Glossopteris Flora, the main component of the fossil deposits of mineral coal in Brazil.\n\nDavid White won the Thompson Medal in 1931 and the Walcott Medal in 1934. He was president of the Geological Society of America in 1923. He \"himself considered that his structure-carbon ratio for the occurrence of oil and gas was his greatest scientific achievement.\"\n\n"}
{"id": "44595037", "url": "https://en.wikipedia.org/wiki?curid=44595037", "title": "Desulfurobacteriaceae", "text": "Desulfurobacteriaceae\n\nThe Desulfurobacteriaceae family are bacteria belonging to the Aquificae phylum.\n\n\n"}
{"id": "23736226", "url": "https://en.wikipedia.org/wiki?curid=23736226", "title": "Dulong–Petit law", "text": "Dulong–Petit law\n\nThe Dulong–Petit law, a thermodynamic law proposed in 1819 by French physicists Pierre Louis Dulong and Alexis Thérèse Petit, states the classical expression for the molar specific [heat capacity] of certain chemical elements. Experimentally the two scientists had found that the heat capacity per weight (the mass-specific heat capacity) for a number of elements was close to a constant value, \"after\" it had been multiplied by a number representing the presumed relative atomic weight of the element. These atomic weights had shortly before been suggested by John Dalton and modified by Jacob Berzelius.\n\nIn modern terms, Dulong and Petit found that the heat capacity of a mole of many solid elements is about 3\"R\", where \"R\" is the modern constant called the universal gas constant. Dulong and Petit were unaware of the relationship with \"R\", since this constant had not yet been defined from the later kinetic theory of gases. The value of 3\"R\" is about 25 joules per kelvin, and Dulong and Petit essentially found that this was the heat capacity of certain solid elements per mole of atoms they contained.\n\nThe modern theory of the heat capacity of solids states that it is due to lattice vibrations in the solid and was first derived in crude form from this assumption by Albert Einstein in 1907. The Einstein solid model thus gave for the first time a reason why the Dulong–Petit law should be stated in terms of the classical heat capacities for gases.\n\nAn equivalent statement of the Dulong–Petit law in modern terms is that, regardless of the nature of the substance, the specific heat capacity c of a solid element (measured in joule per kelvin per kilogram) is equal to 3R/\"M\", where \"R\" is the gas constant (measured in joule per kelvin per mole) and \"M\" is the molar mass (measured in kilogram per mole). Thus, the heat capacity per mole of many elements is 3\"R\".\n\nThe initial form of the Dulong–Petit law was:\n\nwhere c is the specific heat, \"M\" the atomic weights accepted in that day, and K is a new constant which we know today is about 3R.\n\nIn modern terms the mass m divided by atomic weight \"M\" gives the number of moles N.\n\nTherefore, using uppercase C for the \"total\" heat capacity, and lowercase c for the specific heat capacity c :\n\nor\n\nTherefore, the heat capacity of most solid crystalline substances is 3R per mole of substance.\n\nDulong and Petit did not state their law in terms of the gas constant \"R\" (which was not then known). Instead, they measured the values of heat capacities (per weight) of substances and found them smaller for substances of greater atomic weight as inferred by Dalton and other early atomists. Dulong and Petit then found that when multiplied by these atomic weights, the value for the heat capacity (which would now be the heat capacity \"per mole\" in modern terms) was nearly constant, and equal to a value which was later recognized to be 3 \"R\".\n\nIn other modern terminology, the dimensionless heat capacity is equal to 3.\n\nFor chemical applications the Dulong-Petit Law is approximately given by the relation:\n\nDespite its simplicity, Dulong–Petit law offers fairly good prediction for the specific heat capacity of many elementary solids with relatively simple crystal structure at high temperatures. This is because in the classical theory of heat capacity, the heat capacity of solids approaches a maximum of 3\"R\" per mole of atoms because full vibrational-mode degrees of freedom amount to 3 degrees of freedom per atom, each corresponding to a quadratic kinetic energy term and a quadratic potential energy term. By the equipartition theorem, the average of each quadratic term is \"kT\", or \"RT\" per mole (see derivation below). Multiplied by 3 degrees of freedom and the two terms per degree of freedom, this amounts to 3\"R\" per mole heat capacity.\n\nThe Dulong–Petit law fails at room temperatures for light atoms bonded strongly to each other, such as in metallic beryllium and in carbon as diamond. Here, it predicts higher heat capacities than are actually found, with the difference due to higher-energy vibrational modes not being populated at room temperatures in these substances.\n\nIn the very low (cryogenic) temperature region, where the quantum mechanical nature of energy storage in all solids manifests itself with larger and larger effect, the law fails for all substances. For crystals under such conditions, the Debye model, an extension of the Einstein theory that accounts for statistical distributions in atomic vibration when there are lower amounts of energy to distribute, works well.\n\nA system of vibrations in a crystalline solid lattice can be modelled by considering harmonic oscillator potentials along each degree of freedom. Then, the free energy of the system can be written as\n\nwhere the index \"α\" sums over all the degrees of freedom. In the 1907 Einstein model (as opposed to the later Debye model) we consider only the high-energy limit:\n\nThen\n\nand we have\n\nDefine \"geometric mean frequency\" by\n\nwhere \"M\" measures the total number of degrees of freedom of the system.\n\nThus we have\n\nUsing energy\n\nwe have\n\nThis gives specific heat\n\nwhich is independent of the temperature.\n\n\n"}
{"id": "25137682", "url": "https://en.wikipedia.org/wiki?curid=25137682", "title": "EMC effect", "text": "EMC effect\n\nThe EMC effect is the surprising observation that the cross section for deep inelastic scattering from an atomic nucleus is different from that of the same number of free protons and neutrons (collectively referred to as nucleons). From this observation, it can be inferred that the quark momentum distributions in nucleons bound inside nuclei are different from those of free nucleons. This effect was first observed in 1983 at CERN by the European Muon Collaboration, hence the name \"EMC effect\". It was unexpected, since the average binding energy of protons and neutrons inside nuclei is insignificant when compared to the energy transferred in deep inelastic scattering reactions that probe quark distributions. While over 1000 scientific papers have been written on the topic and numerous hypotheses have been proposed, no definitive explanation for the cause of the effect has been confirmed. Determining the origin of the EMC effect is one of the major unsolved problems in the field of nuclear physics.\n\nProtons and neutrons, collectively referred to as nucleons, are the constituents of atomic nuclei, and nuclear matter such as that in neutron stars. Protons and neutrons themselves are composite particles made up of quarks and gluons, a discovery made at SLAC in the late 1960s using deep inelastic scattering (DIS) experiments (1990 Nobel Prize). In the DIS reaction, a probe (typically an accelerated electron) scatters from an individual quark inside a nucleon. By measuring the cross section of the DIS process, the distribution of quarks inside the nucleon can be determined. These distributions are effectively functions of a single variable, known as Bjorken-x, which is a measure of the momentum fraction of the struck quark. Experiments using DIS from protons by electrons and other probes have allowed physicists to measure the proton's quark distribution over a wide range of Bjorken-x, i.e. the probability of finding a quark with momentum fraction x in the proton. Experiments using deuterium and helium-3 targets have similarly allowed physicists to determine the quark distribution of the neutron.\n\nIn 1983, the European Muon Collaboration published results from an experiment conducted at CERN in which the DIS reaction was measured for high-energy muon scattering from iron and deuterium targets. It was expected that the cross section for DIS from iron divided by that from deuterium, and scaled by a factor of 28 (The iron-56 nucleus has 28 times more nucleons than deuterium) would be approximately 1. Instead, the data (fig. 1) showed a decreasing slope in the region of 0.3 < x < 0.7, reaching a minimum of 0.85 at the largest values of x. This decreasing slope is a hallmark of the EMC effect. The slope of this cross section ratio between 0.3 < x < 0.7 is often referred to as the \"size of the EMC effect\" for a given nucleus.\n\nSince that landmark discovery, the EMC effect has been measured over a wide-range of nuclei, at several different laboratories, and with multiple different probes. Notable examples include the E139 experiment at SLAC, which measured the EMC effect in He, Be, C, Al, Ca, Fe, Ag, and Au, and found that the EMC effect increases with nuclear size. The E03-103 experiment at Jefferson Lab focused on high-precision measurements of light nuclei and found that the size of the effect scales with local nuclear density rather than average nuclear density. \n\nThe EMC effect is surprising because of the difference in energy scales between nuclear binding and deep inelastic scattering. Typical binding energies for nucleons in nuclei are on the order of 10 megaelectron volts (MeV). Typical energy transfers in DIS are on the order of several gigaelectron volts (GeV). Nuclear binding effects were therefore believed to be insignificant when measuring quark distributions. A number of hypotheses for the cause of the EMC effect have been offered. While many older hypotheses, such as Fermi motion (see Fig. 2), nuclear pions, and others have been ruled out by electron scattering or Drell-Yan data, modern hypotheses generally fall into two viable categories: mean-field modification, and short-range correlated pairs.\n\nThe mean-field modification hypothesis suggests that the nuclear environment leads to a modification of nucleon structure. As an illustration, consider that the average density inside a nuclear matter is approximately 0.16 nucleons per fm. If nuclei were hard-spheres, their radius would be approximately 1.1 fm, leading to a density of only 0.13 nucleons per fm, assuming ideal close-packing. Nuclear matter is dense, and the close proximity of nucleons may allow quarks in different nucleons to interact directly, leading to nucleon modification. Mean-field models predict that all nucleons experience some degree of structure modification, and they are consistent with the observation that the EMC effect increases with nuclear size, scales with local density, and saturates for very large nuclei. Furthermore, mean-field models also predict a large \"polarized EMC effect,<nowiki>\"</nowiki> a large modification of the spin-dependent g structure function for nuclei relative to that of their constituent protons and neutrons. This prediction will be tested experimentally as part of the Jefferson Lab 12-GeV program.\n\nRather than all nucleons experiencing some modification, the short-range correlations hypothesis predicts that most nucleons at any one time are unmodified, but some are substantially modified. The most heavily modified nucleons are those in temporary short-range correlated (SRC) pairs. It has been observed that approximately 20% of nucleons (in medium and heavy nuclei) at any given moment are part of short-lived pairs with significant spatial overlap with a partner nucleon. The nucleons in these pairs then recoil apart with large back-to-back momenta of several hundred MeV/\"c\", larger than the nuclear Fermi momentum, making them the highest-momentum nucleons in the nucleus. In the short-range correlations hypothesis, the EMC effect stems from large modification of these high-momentum SRC nucleons. This is supported by the observation that the size of the EMC effect in different nuclei correlates linearly with the density of SRC pairs. This hypothesis predicts increasing modification as a function of nucleon momentum, which will be tested using recoil-tagging techniques in upcoming experiments at Jefferson Lab.\n"}
{"id": "4435025", "url": "https://en.wikipedia.org/wiki?curid=4435025", "title": "Free-return trajectory", "text": "Free-return trajectory\n\nA free-return trajectory is a trajectory of a spacecraft traveling away from a primary body (for example, the Earth) where gravity due to a secondary body (for example, the Moon) causes the spacecraft to return to the primary body without propulsion (hence the term \"free\").\n\nThe first spacecraft to use a free-return trajectory was the Soviet Luna 3 mission in October 1959. It used the moon's gravity to send it back towards the earth so that the photographs it had taken of the far side of the moon could be downloaded by radio.\n\nSymmetrical free-return trajectories were studied by Arthur Schwaniger of NASA in 1963 with reference to the Earth–Moon system. He studied cases in which the trajectory at some point crosses at a right angle the line going through the centre of the Earth and the centre of the moon, and also cases in which the trajectory crosses at a right angle the plane containing that line and perpendicular to the plane of the moon's orbit. In both scenarios we can distinguish between:\n\nIn both the circumlunar case and the cislunar case, the craft can be moving generally from west to east around the earth (co-rotational), or from east to west (counter-rotational).\n\nFor trajectories in the plane of the Moon's orbit with small periselenum radius (close approach of the Moon), the flight time for a cislunar free-return trajectory is longer than for the circumlunar free-return trajectory with the same periselenum radius. Flight time for a cislunar free-return trajectory decreases with increasing periselenum radius, while flight time for a circumlunar free-return trajectory increases with periselenum radius.\n\nThe speed at a perigee of 6555 km from the centre of the earth for trajectories passing between 2000 and 20 000 km from the moon is between 10.84 and 10.92 km/s regardless of whether the trajectory is cislunar or circumlunar or whether it is co-rotational or counter-rotational.\n\nUsing the simplified model where the orbit of the Moon around the Earth is circular, Schwaniger found that there exists a free-return trajectory in the plane of the orbit of the Moon which is periodic. After returning to low altitude above the Earth (the perigee radius is a parameter, typically 6555 km) the spacecraft would start over on the same trajectory. This periodic trajectory is counter-rotational (it goes from east to west when near the Earth). It has a period of about 650 hours (compare with a sidereal month, which is 655.7 hours, or 27.3 days). Considering the trajectory in an inertial (non-rotating) frame of reference, the perigee occurs directly under the Moon when the Moon is on one side of the Earth. Speed at perigee is about 10.91 km/s. After 3 days it reaches the Moon's orbit, but now more or less on the opposite side of the Earth from the Moon. After a few more days, the craft reaches its (first) apogee and begins to fall back toward the Earth, but as it approaches the Moon's orbit, the Moon arrives, and there is a gravitational interaction. The craft passes on the near side of the Moon at a radius of 2150 km (410 km above the surface) and is thrown back outwards, where it reaches a second apogee. It then falls back toward the Earth, goes around to the other side, and goes through another perigee close to where the first perigee had taken place. By this time the Moon has moved almost half an orbit and is again directly over the craft at perigee. Other cislunar trajectories are similar but do not end up in the same situation as at the beginning, so cannot repeat.\n\nThere will of course be similar trajectories with periods of about two sidereal months, three sidereal months, and so on. In each case, the two apogees will be further and further away from Earth. These were not considered by Schwaniger.\n\nThis kind of trajectory can occur of course for similar three-body problems; this problem is an example of a circular restricted three-body problem.\n\nWhile in a true free-return trajectory no propulsion is applied, in practice there may be small mid-course corrections or other maneuvers.\n\nA free-return trajectory may be the initial trajectory to allow a safe return in the event of a systems failure; this was applied in the Apollo 8, Apollo 10, and Apollo 11 lunar missions. In such a case a free return to a suitable reentry situation is more useful than returning to near the Earth, but then needing propulsion anyway to prevent moving away from it again. Since all went well, these Apollo missions did not have to take advantage of the free return and inserted into orbit upon arrival at the Moon.\n\nDue to the landing-site restrictions that resulted from constraining the launch to a free return that flew by the Moon, subsequent Apollo missions, starting with Apollo 12 and including the ill-fated Apollo 13, used a hybrid trajectory that launched to a highly elliptical Earth orbit that fell short of the Moon with effectively a free return to the atmospheric entry corridor. They then performed a mid-course maneuver to change to a trans-Lunar trajectory that was not a free return. This retained the safety characteristics of being on a free return upon launch and only departed from free return once the systems were checked out and the lunar module was docked with the command module, providing back-up maneuver capabilities. In fact, within hours after the accident, Apollo 13 used the lunar module to maneuver from its planned trajectory to a circumlunar free-return trajectory. Apollo 13 was the only Apollo mission to actually turn around the Moon in a free-return trajectory (however, two hours after perilune, propulsion was applied to speed the return to Earth by 10 hours and move the landing spot from the Indian Ocean to the Pacific Ocean).\n\nA free-return transfer orbit to Mars is also possible. As with the Moon, this option is mostly considered for manned missions. Robert Zubrin, in his book \"The Case for Mars\", discusses various trajectories to Mars for his mission design Mars Direct. The Hohmann transfer orbit can be made free-return. It takes 250 days (0.68 years) in the transit to Mars, and in the case of a free-return style abort without the use of propulsion at Mars, 1.5 years to get back to Earth, at a total delta-v requirement of 3.34 km/s. Zubrin advocates a slightly faster transfer, that takes only 180 days to Mars, but 2 years back to Earth in case of an abort. This route comes also at the cost of a higher delta-v of 5.08 km/s. Zubrin claims that even faster routes have a significantly higher delta-v cost and free-return duration (e.g. transfer to Mars in 130 days takes 7.93 km/s delta-v and 4 years on the free return), and thus advocates for the 180-day transfer even if more efficient propulsion systems, that are claimed to enable faster transfers, should materialize. A free return is also the part of various other mission designs, such as Mars Semi-Direct and Inspiration Mars.\n\nThere also exists the option of two- or three-year free-returns that do not rely on the gravity of Mars, but are simply transfer orbits with periods of 2 or 1.5 years, respectively. A two-year free return means from Earth to Mars (aborted there) and then back to Earth all in 2 years. The entry corridor (range of permissible path angles) for landing on Mars is limited, and experience has shown that the path angle is hard to fix (e.g. +/- 0.5 deg). This limits entry into the atmosphere to less than 9 km/s. On this assumption, a two-year return is not possible for some years, and for some years a delta-v kick of 0.6 to 2.7 km/s at Mars may be needed to get back to Earth.\n\nNASA published the Design Reference Architecture 5.0 for Mars in 2009, advocating a 174-day transfer to Mars, which is close to Zubrin's proposed trajectory. It cites a delta-v requirement of approximately 4 km/s for the trans-Mars injection, but does not mention the duration of a free return to Earth.\n\n"}
{"id": "47278268", "url": "https://en.wikipedia.org/wiki?curid=47278268", "title": "Future Circular Collider", "text": "Future Circular Collider\n\nThe Future Circular Collider (FCC) study aims at developing conceptual designs for a post-LHC particle accelerator research infrastructure in a global context, with an energy significantly above that of previous circular colliders (SPS, Tevatron, LHC).\n\nThe FCC study explores the feasibility of different particle collider scenarios with the aim of significantly expanding the current energy and luminosity frontiers. It aims to complement existing technical designs for linear electron/positron colliders (ILC and CLIC).\n\nThe study has an emphasis on proton/proton (hadron) and electron/positron (lepton) colliders while a hadron/lepton scenario is also examined. The study explores the potential of hadron and lepton circular colliders, performing an in-depth analysis of infrastructure and operation concepts and considering the technology research and development programmes that are required to build and operate a future circular collider. A conceptual design is planned to be delivered before the end of 2018, in time for the next update of the European Strategy for Particle Physics.\n\nThe study hosted by CERN has been initiated as a direct response to the high-priority recommendation of the updated European Strategy for Particle Physics, published in 2013:\n\n\"CERN should undertake design studies for accelerator projects in a global context, with emphasis on proton-proton and electron-positron high-energy frontier machines. These design studies should be coupled to a vigorous accelerator R&D programme, including high-field magnets and high-gradient accelerating structures, in collaboration with national institutes, laboratories and universities worldwide.\"\n\nThis is in line with the recommendations of the United States’ Particle Physics Project Prioritization Panel (P5) and of the International Committee for Future Accelerators (ICFA).\n\nThe discovery of the Higgs boson at the LHC, together with the absence so far of any phenomena beyond the Standard Model in collisions at centre of mass energies up to 8 TeV, has triggered an interest in future colliders to push the energy and precision frontiers. A future “energy frontier” collider at 100 TeV is a “discovery machine”, reaching out to so far unknown territories. \"New physics\" seen at such a machine could explain observations such as the prevalence of matter over antimatter and non-zero neutrino masses.\n\nThe LHC has greatly advanced our understanding of matter and the Standard Model (SM), however it cannot confirm every aspect of the SM nor explore other key questions about the Universe. To find out more about dark matter, the matter/antimatter asymmetry, or the origin of neutrino masses, extended energy and mass reach and higher precision are necessary.\n\nThe Future Circular Collider (FCC) study develops options for potential high-energy frontier circular colliders at CERN for the post-LHC era that will open up new horizons in the field of fundamental physics. Among other things, it plans to study dark matter, which accounts for approximately 25% of the visible universe, make precise measurements of the Higgs boson, and explore beyond the Standard Model theories.\n\nA Future Circular Collider could open a window for exploring the unknown 95% of the Universe: What is dark matter? Are there supersymmetric particles? Are there other fundamental interactions?\n\nThe discovery of the Higgs boson was a milestone in the long-standing effort to complete the Standard Model of Particle Physics, the theory that describes the laws governing most of the known Universe. Yet the Standard Model cannot explain several observations, such as:\n\n\nExpanding our understanding of the fundamental laws of nature requires the energy frontier to be pushed further. Reaching this goal within the 21st century in an efficient and reliable way calls for a larger circular collider.\n\nThe FCC, with its high precision and high energy reach, could extend the search of new particles and interactions well beyond the LHC, that may hold the key to understanding those unexplained phenomena.\n\nThe FCC Study puts an emphasis on proton/proton high-energy and electron/positron high-intensity frontier machines. A hadron/lepton interaction scenario is also examined.\n\nA 100 TeV hadron collider in a 100 km long tunnel defines therefore the overall infrastructure for the FCC study. The development of baseline designs for an energy-frontier hadron collider and a luminosity-frontier electron/positron collider forms the core of the study.\n\nThe FCC study will identify the technological advancements required for reaching the new energy and intensity frontiers and will make the first step by performing technology feasibility assessments. The study will provide an analysis of the infrastructure and operation cost while cost optimization, greater efficiency and reliable operation are key parameters in the study.\n\nThe FCC study explores the physics cases for all collider scenarios in a coordinated way that embraces discovery and precision physics. Scientists and engineers are working on the detector concepts needed to address the physics questions in each of the scenarios (hh, ee, he). The work programme includes experiment and detector concept studies to allow new physics to be explored. Detector technologies will be based on experiment concepts, the projected collider performances and the physics cases. Innovative technologies have to be developed in diverse fields, such as cryogenics, superconductivity, material science, and computer science, including new data processing and data management concepts.\n\nCreativity and innovation are needed to develop the physics case, meet the required accelerator parameters and realise unprecedented experiments.\n\nThe FCC Study will develop and evaluate three accelerator concepts for its conceptual design report.\n\nA future energy-frontier hadron collider will be able to discover force carriers of new interactions up to masses of around 30 TeV. In such a machine, the discovery reach for dark matter particles will extend well beyond the TeV region, while supersymmetric partners of quarks and gluons can be produced at masses up to 15-20 TeV and the search for a possible substructure inside quarks can be extended down to distance scales of 10 m. Billions of Higgs bosons and trillions of top quarks will be produced, creating new opportunities for the study of rare decays and flavour physics, which tremendously benefit from higher collision energies.\n\nA hadron collider will also extend the study of Higgs and gauge boson interactions to energies well above the TeV scale, providing a way to analyse in detail the mechanism underlying the breaking of the electroweak symmetry.\n\nThe FCC-hh collider offers an opportunity to push the exploration of the collective structure of matter at the most extreme density and temperature conditions to new frontiers through the study of heavy-ion collisions.\n\nA lepton collider with centre-of-mass collision energies between 90 and 350 GeV is considered a potential intermediate step towards the realisation of the hadron facility. Clean experimental conditions have given e+e− storage rings a strong record of accomplishment both for measuring known particles with the highest precision and for exploring the unknown.\n\nMore specifically, high luminosity and improved handling of lepton beams would create the opportunity to measure the properties of the Z, W, Higgs, and top particles, as well as the strong interaction, with unequalled accuracy.\n\nThe unique measurements of invisible or exotic decays of the Higgs and Z bosons would offer discovery potential for dark matter or heavy neutrinos. In effect, the FCC-ee could enable profound investigations of electroweak symmetry breaking and open a broad indirect search for new physics over several orders of magnitude in energy or couplings.\n\nWith the huge energy provided by the 50 TeV proton beam and the potential availability of an electron beam with energy of the order of 60 GeV, new horizons open up for the physics of deep inelastic scattering. The FCC-he collider would be both a high-precision Higgs factory and a powerful microscope that could discover new particles, study quark/gluon interactions, and examine possible further substructure of matter in the world. With such a programme, accompanied by unprecedented measurements of strong and electroweak interaction phenomena, the hadron/electron collider is another unique complement to the exploration of nature at high energies.\n\nAs the development of a next generation particle accelerator requires state-of-the-art technology, the FCC Study has undertaken extensive R&D to create the equipment and machines that are needed for the realization of the project. Technology R&D relies on interdisciplinary synergies, taking into account the experience from past and present accelerator projects.\n\nThe foundations for these advancements are being laid in a focused R&D programmes:\n\n\nNumerous other technologies are needed for reliable, sustainable and efficient operation. The effective interplay of different science and technology domains (accelerator physics, high-field magnets, cryogenics, vacuum, civil engineering, material science, superconductors) is the key to success.\n\nAdvancing technology and engineering for high-energy particle physics and accelerators creates significant benefits for society. These innovations lead to new products and services that change our daily lives.\n\nThe rich and diverse FCC R&D programme attracts significant interest from students of different fields ranging from material sciences, chemistry, mechanical and electrical engineering, over computer science, mathematics, reliability engineering to technology management, international business management, entrepreneurship, patent and innovation management, international law and social sciences.\n\nHigh-field superconducting magnets are a key enabling technology for a frontier hadron collider. To steer a 50 TeV beam over a 100 km tunnel, 16-Tesla dipoles will be necessary, twice the strength of the magnetic field of the LHC.\n\nThe magnet R&D aims to extend the range of operation of accelerator magnets based on Low Temperature Superconductors (LTS) up to 16 T and explore the technological challenges inherent to the use of High Temperature Superconductors (HTS) for accelerator magnets in the 20 T range.\n\nThe beams that move in a circular accelerator lose a percentage of their energy, approximately 4.4%, at every turn. To maintain their energy, a system of radiofrequency constantly provides 50 MW to each beam.\n\nAt a 45 GeV system, the percentage of energy loss is expected to increase and the superconducting RF cavities will have to deliver a total of 100 MW of acceleration power.\n\nLiquefaction of gas is a power-intensive operation that requires state-of-the-art cryogenic technology. The future lepton and the hadron colliders would make intensive use of low-temperature superconducting devices, operated at 4.5 K and 1.8 K, requiring very large-scale distribution, recovery, and storage of cryogenic fluids.\n\nAs a result, the cryogenic systems that have to be developed correspond to two to four times the presently deployed systems and require increased availability and maximum energy efficiency. Any further improvements in cryogenics are expected to find wide applications in medical imaging techniques.\n\nThe cryogenic beam vacuum system for an energy-frontier hadron collider must absorb an energy of 50 W per meter at cryogenic temperatures. To protect the magnet cold bore from the head load, the vacuum system needs to be resistant against electron cloud effects, highly robust, and stable under superconducting quench conditions.\n\nIt should also allow fast feedback in presence of impedance effects. New composite materials have to be developed to achieve these unique thermo-mechanics and electric properties for collimation systems. Such materials could also be complemented with the ongoing exploration of thin-film NEG coating that is used in the internal surface of the copper vacuum chambers.\n\nFinally, a 100 TeV hadron collider requires efficient and robust collimators, as 100 kW of hadronic background is expected at the interaction points. Moreover, fast self-adapting control systems with sub-millimeter collimation gaps are necessary to prevent irreversible damage of the machine and manage the 180 GJ stored in each magnet.\n\nTo address these challenges, the FCC Study searches for designs that can withstand the large energy loads with acceptable transient deformation and no permanent damage. Novel composites with unprecedented thermo-mechanical and electric properties will be investigated in cooperation with the FP7 HiLumi LHC DS and EuCARD2 programmes.\n\nThe Large Hadron Collider at CERN with its High Luminosity upgrade is the world’s primary instrument for exploring the energy frontier until 2035. This defines the time window for preparing a post-LHC high-energy physics research infrastructure.\n\nLEP and LHC have shown that a time-frame of 30 years is appropriate for the design and construction of a large accelerator complex and particle detectors. The significant lead time calls for a coordinated global effort. The goal is to ensure the seamless continuation of the world’s particle physics programme after the LHC era.\n\nThe FCC study hosted by CERN is an international collaboration of more than 70 institutes from all over the world.\n\nThis prepares the ground for geographically well-balanced contributions, leveraging the competences of world experts in the numerous areas concerned. It also ensures that the entire worldwide scientific community is involved from the very start of the endeavour.\n\nBringing together physics, experiments, accelerator concepts and technology R&D within a single study will result in a coherent and consistent design for a future large-scale research infrastructure.\n\nThe experience from the operation of LEP and LHC and the unique opportunity to test the novel technologies in the High Luminosity LHC provide a solid basis for assessing the feasibility of a post-LHC particle accelerator.\nThe study will deliver a Conceptual Design Report (CDR) by the end of 2018, in time for the next European Strategy for Particle Physics.\n\nThe FCC Study is hosted by CERN, also a Participant in the Collaboration, as a direct response to the recommendation made in the update of the European Strategy for Particle Physics 2013, adopted by the Organisation's Council. The Study is governed by three bodies: the International Collaboration Board (ICB), the International Steering Committee (ISC), and the International Advisory Committee (IAC).\n\nThe ICB reviews the resource needs of the Study and finds matches within the Collaboration. It so channels the contributions from the Participants of the Collaboration aiming at a geographically well-balanced and topically complementary network of contributions The ISC is the supervisory and main governing body for the execution of the Study and acts on behalf of the Collaboration.\n\nIt is responsible for the proper execution and implementation of the decisions of the ICB, deriving and formulating the strategic scope, individual goals and the work programme of the Study. Its work is facilitated by the Coordination Group, the main executive body of the project, which coordinates the individual work packages and performs the day-to-day management of the Study.\n\nFinally, the IAC reviews the scientific and technical progress of the Study and shall submit scientific and technical recommendations to the International Steering Committee to assist and facilitate major technical decisions.\n\nFollowing the first run of the LHC, there is consensus in the scientific community that the results will have to be complemented by a collider that can study the discoveries in greater detail by producing different kinds of collisions. Several studies for future particle accelerators, both linear and circular, are therefore carried out.\n\nThe Compact Linear Collider (CLIC) examines the feasibility of a high-energy (up to 3 TeV), high-luminosity lepton (electron/positron) collider, while the International Linear Collider is a similar project, planned to have a collision energy of 500 GeV. In 2013, the two studies formed an organisational partnership, the Linear Collider Collaboration (LCC) to coordinate and advance the global development work for the linear collider. Concerning the LHC, a high-luminosity upgrade is planned to extend its operation lifetime into the mid-2030s. The upgrade will facilitate the detection of rare processes and improve statistically marginal measurements.\n\n\n"}
{"id": "58028753", "url": "https://en.wikipedia.org/wiki?curid=58028753", "title": "Home Energy Resources Unit", "text": "Home Energy Resources Unit\n\nThe Home Energy Resources Unit or Home Energy Recovery Unit (HERU) is a prototype kitchen appliance which is claimed to use household waste to generate energy.\n\nHousehold waste, both organic and inorganic, is placed into a main chamber. Plastic and cardboard packaging as well as food waste can be inserted as fuel, and the waste does not need to be sorted or separated. It then goes through an eight-hour cycle. The chamber containing the waste is heated to 100° celsius to evaporate water and remove oxygen. The temperature is then increased and pyrolysis takes place. Oxygen is then re-introduced to allow incineration of the pyrolised waste for water heating. After eight hours, a tankful of stored hot water is produced, and all plastic, cardboard and organic waste is reduced to gas and ash. The gas is then used in a domestic boiler, and the ash can be flushed safely into an ordinary sewage system. Any glass or metal waste is left in a clean state.\n\nThe invention won a grant from the UK Government's Innovate UK Energy Game Changer Fund. The technology won the 2017 Environment and Sustainability Group Prize of the Institution of Mechanical Engineers.\n"}
{"id": "54710618", "url": "https://en.wikipedia.org/wiki?curid=54710618", "title": "Hypothalamic–pituitary–somatotropic axis", "text": "Hypothalamic–pituitary–somatotropic axis\n\nThe hypothalamic–pituitary–somatotropic axis (HPS axis), or hypothalamic–pituitary–somatic axis, also known as the hypothalamic–pituitary–growth axis, is a hypothalamic–pituitary axis which includes the secretion of growth hormone (GH; somatotropin) from the somatotropes of the pituitary gland into the circulation and the subsequent stimulation of insulin-like growth factor 1 (IGF-1; somatomedin-1) production by GH in tissues such as, namely, the liver. Other hypothalamic–pituitary hormones such as growth hormone-releasing hormone (GHRH; somatocrinin), growth hormone-inhibiting hormone (GHIH; somatostatin), and ghrelin () are involved in the control of GH secretion from the pituitary gland. The HPS axis is involved in postnatal human growth. Individuals with growth hormone deficiency or Laron syndrome ( insensitivity) show symptoms like short stature, dwarfism and obesity, but are also protected from some forms of cancer. Conversely, acromegaly and gigantism are conditions of GH and IGF-1 excess usually due to a pituitary tumor, and are characterized by overgrowth and tall stature.\n\n"}
{"id": "5921892", "url": "https://en.wikipedia.org/wiki?curid=5921892", "title": "Infrared spectroscopy correlation table", "text": "Infrared spectroscopy correlation table\n\nAn infrared spectroscopy correlation table (or table of infrared absorption frequencies) is a list of absorption peaks and frequencies, typically reported in wavenumber, for common types of molecular bonds and functional groups. In physical and analytical chemistry, infrared spectroscopy (IR spectroscopy) is a technique used to identify chemical compounds based on the way infrared radiation is absorbed by the compound.\n\nThe absorptions in this range do not apply only to bonds in organic molecules. IR spectroscopy is useful when it comes to analysis of inorganic compounds (such as metal complexes or fluoromanganates) as well.\n\nTables of vibrational transitions of stable and transient molecules are also available.\n\n"}
{"id": "1782155", "url": "https://en.wikipedia.org/wiki?curid=1782155", "title": "Jim Blinn", "text": "Jim Blinn\n\nJames F. Blinn (born 1949) is an American computer scientist who first became widely known for his work as a computer graphics expert at NASA's Jet Propulsion Laboratory (JPL), particularly his work on the pre-encounter animations for the Voyager project, his work on the Carl Sagan documentary series \"\", and the research of the Blinn–Phong shading model.\n\nHe is credited with formulating Blinn's Law, which asserts that rendering time tends to remain constant, even as computers get faster. Animators prefer to improve quality, rendering more complex scenes with more sophisticated algorithms, rather than using less time to do the same work as before.\n\nIn 1970, he received his bachelor's degree in physics and communications science, and later a master's degree in engineering from the University of Michigan. In 1978 he received a Ph.D. in computer science from the College of Engineering at the University of Utah.\n\nBlinn devised new methods to represent how objects and light interact in a three-dimensional virtual world, like environment mapping and bump mapping. He is well known for creating animation for three television series: Carl Sagan's \"\"; \"Project MATHEMATICS!\"; and the pioneering instructional graphics in \"The Mechanical Universe\". His simulations of the Voyager spacecraft visiting Jupiter and Saturn have been seen widely.\n\nBlinn was affiliated with the Jet Propulsion Laboratory at the California Institute of Technology until 1995. Thereafter, he joined Microsoft Research, where he was a graphics fellow until his retirement in 2009. Blinn also worked at the New York Institute of Technology during the summer of 1976.\n\n\nFrom 1987 to 2007, Blinn wrote a column for IEEE Computer Graphics & Applications called \"Jim Blinn's Corner\". He wrote a total of 83 columns, most of which were reprinted in these books:\n\n\n\n"}
{"id": "873522", "url": "https://en.wikipedia.org/wiki?curid=873522", "title": "Konstantin Kozeyev", "text": "Konstantin Kozeyev\n\nKonstantin Mirovich Kozeyev () is a retired Russian cosmonaut.\n\nKozeyev was born in Korolyov, Moscow Oblast, Russian SFSR on December 1, 1967. He is a graduate student from Moscow Aviation Technology Institute and was selected as a cosmonaut on February 9, 1996. He flew as Flight Engineer on Soyuz TM-33 in 2001.\n\nKozeyev is divorced and has no children.\n"}
{"id": "26705239", "url": "https://en.wikipedia.org/wiki?curid=26705239", "title": "List of forests of South Africa", "text": "List of forests of South Africa\n\nList of forests of South Africa, among other terms used it usually means \"an area with a high density of trees\" are wood, woodland, wold, weald and holt. Unlike forest, these are all derived from Old English and were not borrowed from another language.\n"}
{"id": "24193312", "url": "https://en.wikipedia.org/wiki?curid=24193312", "title": "List of ornithology awards", "text": "List of ornithology awards\n\nThis is a list of ornithology awards:\n\n"}
{"id": "17155367", "url": "https://en.wikipedia.org/wiki?curid=17155367", "title": "List of political and geographic subdivisions by total area from 0.1 to 250 square kilometers", "text": "List of political and geographic subdivisions by total area from 0.1 to 250 square kilometers\n"}
{"id": "26526088", "url": "https://en.wikipedia.org/wiki?curid=26526088", "title": "Ludwig Haberlandt", "text": "Ludwig Haberlandt\n\nLudwig Haberlandt (1 February 1885 – 22 July 1932) is known as a father of hormonal contraception. In 1921 he carried out experiments on rabbits and he demonstrated a temporary hormonal contraception in a female by transplanting ovaries from a second, pregnant, animal.\n\nHis father was the eminent botanist, Gottlieb Haberlandt, plant tissue culture theorist and visionary; his grandfather was the European 'soybean' pioneer and trailblazer Friedrich J. Haberlandt.\n\nIn 1930 he began clinical trials after successful production of a hormonal preparation, Infecundin®, by the G. Richter Company in Budapest, Hungary. He ended his 1931 book, \"Die hormonale Sterilisierung des weiblichen Organismus\", with a visionary claim: 'Unquestionably, practical application of the temporary hormonal sterilization in women would markedly contribute to the ideal in human society already enunciated a generation earlier by Sigmund Freud (1898). Theoretically, one of the greatest triumphs of mankind would be the elevation of procreation into a voluntary and deliberate act.' He was hounded for his views on reproductive biology up to his death from either suicide, or heart attack.\n\n"}
{"id": "47621038", "url": "https://en.wikipedia.org/wiki?curid=47621038", "title": "Malta Environment and Planning Authority", "text": "Malta Environment and Planning Authority\n\nThe Malta Environment and Planning Authority (MEPA) was the national agency responsible for the environment and planning in Malta. The national agency was established to regulate the environment and planning on the Maltese islands of Malta, Gozo and other small islets of the Maltese archipelago. MEPA is bound to follow the regulations of the Environment Protection Act (2001) and the Development Planning Act (1992) of the Laws of Malta. The national agency is also responsible for the implementation of Directives, Decisions and Regulations under the EU Environmental Acquis as Malta is a member of the European Union, while considering other recommendations and opinion of the Union. The Authority employs over 420 government workers, from a wide range of educational backgrounds, all within their merit of profession. The Officio delle Case was the planning authority during the Order of St. John.\n\nMEPA acts as the national representation under a number of international environmental conventions and multilateral agreements.\nThese include information supported by the Aarhus Convention:\n\nThe Agency is governed by a board of professional, whose responsibility is to provide strategic guidance within, laid by the laws of Malta. The board comprising a maximum of 15 personnel led by the Executive Chairman, Perit Vincent Cassar. Members within the board, include two representative members of the Parliament of Malta, who are knowledgeable and experienced about matters relating to the environment and development such ranging from: commercial, industrial and social affairs. A number of appointed boards and committees provide strategic guidance or expert advise to the directorates to ensure that the organization fulfils its functions and responsibilities efficiently and effectively, in line with legal obligations.\n\nMEPA’s operational functions and responsibilities are carried out by the work of four main structures, namely:\n\n\nThe Chief Executive Officer is responsible for the implementation of the aims and supervise and control the Directorates. The CEO and other directors, are responsible for developing the necessary strategies. The Planning Directorate processes environment and planning applications. It is responsible for enforcement, policy development and plan making, transport planning and research and other.\n\nThe Enforcement Directorate is responsible for both Development Control and Environmental Protection and for supporting the Authority in enforcement campaigns including Direct Action, enforcement, surveillance and actions as necessary to ensure compliance with the building development permits and to protect the environment to help achieve a sustainable environmental improvement. The Environment Protection Directorate advises Government on environmental standards and policies, draws up plans and provides a licensing regime to safeguard and monitor the environment and controls the activities having environmental impact. The Directorate for Corporate Services, is responsible for Human Resources, Information Technology, Mapping and Land-surveying, support services and Finance. There are a number of boards and committees, which provide strategic guidance for the Directorates to ensure the organization fulfils its functions and responsibilities efficiently and effectively, in line with its legal obligations.\n\nHere is an incomplete list of graded property by MEPA according to category. The below property are named according to the official name used by MEPA and not as how they are known among the public.\n\nGrade 1:\nBuilding at this grade have great historical and architectonical values and may not be altered\nGrade 2:\nBuilding at this grade have historical and architectonical value and can have moderate alterations\nGrade 3:\nBuilding at this grade are not considered important and can be demolished\n"}
{"id": "5411659", "url": "https://en.wikipedia.org/wiki?curid=5411659", "title": "Marine ecosystem", "text": "Marine ecosystem\n\nMarine ecosystems are among the largest of Earth's aquatic ecosystems and are distinguished by waters that have a high salt content. These systems contrast with freshwater ecosystems, which have a lower salt content. Marine waters cover more than 70% of the surface of the Earth and account for more than 97% of Earth's water supply and 90% of habitable space on Earth. Marine ecosystems include nearshore systems, such as the salt marshes, mudflats, seagrass meadows, mangroves, rocky intertidal systems and coral reefs. They also extend outwards from the coast to include offshore systems, such as the surface ocean, pelagic ocean waters, the deep sea, oceanic hydrothermal vents, and the sea floor. Marine ecosystems are characterized by their associated biological community of interacting organisms and their physical environment.\n\nAccording to NOAA, salt marshes are defined as \"coastal wetlands that are flooded and drained by salt water brought in by the tides\". These marshy grounds are able to prevent flooding as well as help maintain water quality by absorbing rainwater and runoff that comes through the area.\n\nMangroves are a compilation of different mangrove tree species living together near the coastline to create a forest. These mangrove forests have intricate root systems that provide habitat to many species and act as a buffer to soil erosion.\n\nIntertidal zones are the areas that are visible during low tide and covered up by saltwater during high tide. In these zones simple organisms can be found in tide pools. These areas also have a higher salinity because salt is left diarrhea has occurred.\n\nEstuaries occur where there is a noticeable change in salinity between saltwater and freshwater sources, for example, the confluence between a river and an ocean. Many organisms rely on this fragile ecosystem at least once during their life cycle.\n\nThe National Geographic Society defines lagoons as a \"shallow body of water protected from a larger body of water (usually the ocean) by sandbars, barrier islands, or coral reefs.\" There are two different types of lagoons: coastal lagoons and atoll lagoons.\n\nCoral reefs are one of the most well-known marine ecosystems within the world. The largest being that of the Great Barrier Reef. These reefs are composed of large coral colonies of a variety of species living together. The corals form multiple symbiotic relationships with the organisms around them.\n\nThe deep sea contains up to 95% of the space occupied by living organisms. Combined with the sea floor (or benthic zone), these two areas have yet to be fully explored and have their organisms documented.\n\nIn addition to providing many benefits to the natural world, marine ecosystems also provide social, economic, and biological ecosystem services to humans. Pelagic marine systems regulate the global climate, contribute to the water cycle, maintain biodiversity, provide food and energy resources, and create opportunities for recreation and tourism. Economically, marine systems support billions of dollars worth of capture fisheries, aquaculture, offshore oil and gas, and trade and shipping.\n\nEcosystem services fall into multiple categories, including supporting services, provisioning services, regulating services, and cultural services.\n\nAlthough marine ecosystems provide essential ecosystem services, these systems face various threats. \n\nCoastal marine systems experience growing population pressure and nearly 40% of people in the world live within 100km of the coast. Humans often aggregate near coastal habitats to take advantage of ecosystem services. For example, coastal capture fisheries from mangrove and coral reef habitats is estimated to be worth a minimum of $34 billion per year. Yet, many of these habitats are either marginally protected or not protected. Mangrove area has declined worldwide by more than one-third since 1950, and 60% of the world's coral reefs are now immediately or directly threatened. Human development, aquaculture, and industrialization often lead to the destruction, replacement, or degradation of coastal habitats.\n\nMoving offshore, pelagic marine systems are directly threatened by overfishing. Global fisheries landings peaked in the late 1980s, but are now declining, despite increasing fishing effort. Fish biomass and average trophic level of fisheries landing are decreasing, leading to declines in marine biodiversity. In particular, local extinctions have led to declines in large, long-lived, slow-growing species, and those that have narrow geographic ranges . Biodiversity declines can lead to associated declines in ecosystem services.\n\n\n\n\n\n"}
{"id": "2789297", "url": "https://en.wikipedia.org/wiki?curid=2789297", "title": "Neutron economy", "text": "Neutron economy\n\nNeutron economy is defined as the ratio of an adjoint weighted average of the excess neutron production divided by an adjoint weighted average of the fission production.\n\nThe distribution of neutron energies in a nuclear reactor differs from the fission neutron spectrum due to the slowing down of neutrons in elastic and inelastic collisions with fuel, coolant and construction material. Neutrons slow down in elastic and inelastic collisions, until they are absorbed via Neutron capture or lost by leakage. Neutron economy is the balanced account, in a reactor, of the neutrons created and the neutrons lost through absorption by non-fuel elements, resonance absorption by fuel, and leakage while fast and thermal energy ranges.\n\nHeavy Water is an extremely efficient moderator. As a result, reactors using heavy water, such as the CANDU also have a high neutron economy.\n\nThe quantity that indicates how much the neutron economy is out of balance is given the term Reactivity. If a reactor is exactly critical - that is, the neutron production is exactly equal to neutron destruction - then the reactivity is zero. If the reactivity is positive - then the reactor is supercritical. If the reactivity is negative - then the reactor is subcritical.\n\nHowever the term neutron economy is used not just for the instantaneous reactivity of a reactor but also to describe the overall efficiency of a nuclear reactor design.\n\n"}
{"id": "1436399", "url": "https://en.wikipedia.org/wiki?curid=1436399", "title": "Operational level of war", "text": "Operational level of war\n\nIn the field of military theory, the operational level of war (also called the operational art, as derived from , or the operational warfare) represents the level of command that connects the details of tactics with the goals of strategy.\n\nIn Joint U.S. military doctrine, operational art is \"the cognitive approach by commanders and staffs—supported by their skill, knowledge, experience, creativity, and judgment—to develop strategies, campaigns, and operations to organize and employ military forces by integrating ends, ways, and means.\" It correlates political needs and military power. Operational art is defined by its military-political scope, not by force size, scale of operations or degree of effort. Likewise, operational art provides theory and skills, and the operational level permits doctrinal structure and process.\n\nDuring the 18th and early 19th centuries, the synonymous term grand tactics (or, less frequently, \"maneuver tactics\") was often used to describe the manoeuvres of troops not tactically engaged, while in the late 19th century to the First World War and through the Second World War, the term minor strategy was used by some military commentators. Confusion over terminology was brought up in professional military publications, that sought to identify \"...slightly different shades of meaning, such as tactics, major tactics, minor tactics, grand strategy, major strategy, and minor strategy\". The term was not widely used in the United States or Britain before 1980–1981, when it became much discussed and started to enter military doctrines and officer combat training courses.\n\nOperational art comprises four essential elements: time, space, means and purpose. Each element is found in greater complexity at the operational level than at the tactical or strategic level. This is true, in part, because operational art must consider and incorporate more of the strategic and tactical levels than those levels must absorb from the operational level. Although much can be gained by examining the four elements independently, it is only when they are viewed together that operational art reveals its intricate fabric.\n\nThe challenge of operational art is to establish a four-element equilibrium that permits the optimal generation and application of military power in achieving the political goal. Viewing time, space, means and purpose as a whole requires great skill in organizing, weighing and envisioning masses of complex, often contradictory factors. These factors often exist for extended periods, over great distances and with shifting mixes of players, systems and beliefs, pursuing political goals which may or may not be clear, cogent or settled. Compounding factors, such as the opponent's actions, create further ambiguity.\n\nThe operational-level strategist possesses numerous tools to frame and guide their thinking, but chief among these are mission analysis and end state. Mission analysis answers the question \"What is to be accomplished?\" Through mission analysis, the operational-level planner fuses political aims and military objectives. In so doing, the planner determines what application of military force will create military power to achieve the political purpose. Subordinate processes here include defining objectives and centers of gravity, but excessive dependence on analytical mechanisms can create false security. The final test rewards success, not the quality of the argument. Conversely, the planner cannot hope to \"feel\" a way to victory—complexity demands an integration of thought and effort.\n\nEnd state answers the question \"What will constitute success?\" The campaign end state is not merely a desired status quo of the military goal. It also establishes a touchstone for the tactical, operational and strategic levels. The end state manifests the intended results of military power and exposes any limitations. Indeed, an achievable end state may require the employment of nonmilitary elements of national power. As such, it recognizes that military power alone may not be capable of attaining political success.\n\nAn operational-level strategy must continually inventory and weigh time, space, means and purpose, extrapolating from them outcomes and likelihood. To accomplish this, practitioners need both skill and theory, experience and knowledge. At the operational level, skills and experience must usually be developed indirectly, through formal training, military history and real-world practicum.\n\nSuccess at the tactical level is no guarantee of success at the operational level: mastery of operational art demands strategic skills. Without a strong grounding in the theory and application of operational art, a successful tactician has little hope of making the demanding leap from tactics. The operational level strategist must see clearly and expansively from the foxhole into the corridors of national or coalition authority. They must be aware of the plausibility and coherence of strategic aims, national will and the players who decide them. Successful operational art charts a clear, unbroken path from the individual soldier's efforts to the state or coalition's goals.\n\nWhile the emerging corpus of operational art and the establishment of a specifically operational level of war are relatively new, in practice operational art has existed throughout recorded history. Peoples and commanders have long pursued political goals through military actions, and one can examine campaigns of any period from the existential perspective of operational art. Current schools of thought on the operational art share the fundamental view that military success can be measured only in the attainment of political-strategic aims, and thus historians can analyze any war in terms of operational art.\n\nIn the case of World War II analysis, the Wehrmacht did not use the operational level as a formal doctrinal concept during the campaigns of 1939–1945. While personnel within the German forces knew of operational art, awareness and practice was limited principally to general-staff trained officers. Nevertheless, the existential nature of operational art means that examining a campaign or an operation against political aims is valid irrespective of the doctrine or structures of the period. Thus the elements of operational art—time, space, means and purpose—can illuminate thoughts and actions of any era, regardless of the prevailing contemporary doctrine or structure.\n\n\n"}
{"id": "57040686", "url": "https://en.wikipedia.org/wiki?curid=57040686", "title": "Pelolinea", "text": "Pelolinea\n\nPelolinea is a bacteria genus from the family of Anaerolineaceae with one known species (\"Pelolinea submarina\"). \"Pelolinea submarina\" has been isolated from marine sediments from the Shimokita Peninsula.\n"}
{"id": "6097857", "url": "https://en.wikipedia.org/wiki?curid=6097857", "title": "Peter Dayan", "text": "Peter Dayan\n\nPeter Samuel Dayan was until Sep. 30th, 2018 a professor of the Gatsby Charitable Foundation Computational Neuroscience Unit at University College London.. Currently, he is a director at the Max Planck Institute for Biological Cybernetics. He is co-author of \"Theoretical Neuroscience\", a textbook on Computational neuroscience. He is known for applying Bayesian methods from machine learning and artificial intelligence to understand neural function, and is particularly recognized for having related neurotransmitter levels to prediction errors and Bayesian uncertainties. He co-authored a paper on Q-learning with Chris Watkins, and provided a proof of convergence of TD(λ) for arbitrary λ.\n\nDayan studied mathematics at the University of Cambridge and then continued for a PhD in artificial intelligence at the University of Edinburgh on statistical learning with and David Wallace, focusing on associative memory and reinforcement learning.\n\nAfter his PhD, Dayan held postdoctoral research positions with Terry Sejnowski at the Salk Institute and Geoffrey Hinton at the University of Toronto. He then took up an assistant professor position at the Massachusetts Institute of Technology, and moved to the Gatsby Computational Neuroscience Unit at University College London in 1998, becoming professor and director in 2002. He stepped down as director in 2017. In September 2018, the Max Planck Society announced his appointment as a director at the Max Planck Institute for Biological Cybernetics.\n\nDayan was elected a Fellow of the Royal Society (FRS) in 2018. He was awarded the Rumelhart Prize in 2012 and The Brain Prize in 2017.\n"}
{"id": "1598092", "url": "https://en.wikipedia.org/wiki?curid=1598092", "title": "Philosophy of social science", "text": "Philosophy of social science\n\n<onlyinclude>\nThe philosophy of social science is the study of the logic, methods, and foundations of social sciences such as psychology, economics, and political science. Philosophers of social science are concerned with the differences and similarities between the social and the natural sciences, causal relationships between social phenomena, the possible existence of social laws, and the ontological significance of structure and agency.\n</onlyinclude>\n\nComte first described the epistemological perspective of positivism in \"The Course in Positive Philosophy\", a series of texts published between 1830 and 1842. These texts were followed by the 1848 work, \"A General View of Positivism\" (published in English in 1865). The first three volumes of the \"Course\" dealt chiefly with the physical sciences already in existence (mathematics, astronomy, physics, chemistry, biology), whereas the latter two emphasised the inevitable coming of social science. Observing the circular dependence of theory and observation in science, and classifying the sciences in this way, Comte may be regarded as the first philosopher of science in the modern sense of the term. For him, the physical sciences had necessarily to arrive first, before humanity could adequately channel its efforts into the most challenging and complex \"Queen science\" of human society itself. His \"View of Positivism\" would therefore set-out to define, in more detail, the empirical goals of sociological method.\n\nComte offered an account of social evolution, proposing that society undergoes three phases in its quest for the truth according to a general 'law of three stages'. The idea bears some similarity to Marx's view that human society would progress toward a communist peak. This is perhaps unsurprising as both were profoundly influenced by the early Utopian socialist, Henri de Saint-Simon, who was at one time Comte's teacher and mentor. Both Comte and Marx intended to develop, scientifically, a new secular ideology in the wake of European secularisation.\n\nThe early sociology of Herbert Spencer came about broadly as a reaction to Comte. Writing after various developments in evolutionary biology, Spencer attempted (in vain) to reformulate the discipline in what we might now describe as socially Darwinistic terms (although Spencer was a proponent of Lamarckism rather than Darwinism).\n\nThe modern academic discipline of sociology began with the work of Émile Durkheim (1858–1917). While Durkheim rejected much of the detail of Comte's philosophy, he retained and refined its method, maintaining that the social sciences are a logical continuation of the natural ones into the realm of human activity, and insisting that they may retain the same objectivity, rationalism, and approach to causality. Durkheim set up the first European department of sociology at the University of Bordeaux in 1895. In the same year he argued, in \"The Rules of Sociological Method\" (1895): \"[o]ur main goal is to extend scientific rationalism to human conduct... What has been called our positivism is but a consequence of this rationalism.\" Durkheim's seminal monograph \"Suicide\" (1897), a case study of suicide rates amongst Catholic and Protestant populations, distinguished sociological analysis from psychology or philosophy.\n\nThe positivist perspective, however, has been associated with 'scientism'; the view that the methods of the natural sciences may be applied to all areas of investigation, be it philosophical, social scientific, or otherwise. Among most social scientists and historians, orthodox positivism has long since fallen out of favor. Today, practitioners of both social and physical sciences recognize the distorting effect of observer bias and structural limitations. This scepticism has been facilitated by a general weakening of deductivist accounts of science by philosophers such as Thomas Kuhn, and new philosophical movements such as critical realism and neopragmatism. Positivism has also been espoused by 'technocrats' who believe in the inevitability of social progress through science and technology. The philosopher-sociologist Jürgen Habermas has critiqued pure instrumental rationality as meaning that scientific-thinking becomes something akin to ideology itself.\n\nDurkheim, Marx, and Weber are more typically cited as the fathers of contemporary social science. In psychology, a positivistic approach has historically been favoured in behaviourism.\n\nIn any discipline, there will always be a number of underlying philosophical predispositions in the projects of scientists. Some of these predispositions involve the nature of social knowledge itself, the nature of social reality, and the locus of human control in action. Intellectuals have disagreed about the extent to which the social sciences should mimic the methods used in the natural sciences. The founding positivists of the social sciences argued that social phenomena can and should be studied through conventional scientific methods. This position is closely allied with scientism, naturalism and physicalism; the doctrine that all phenomena are ultimately reducible to physical entities and physical laws. Opponents of naturalism, including advocates of the \"verstehen\" method, contended that there is a need for an interpretive approach to the study of human action, a technique radically different from natural science. The fundamental task for the philosophy of social science has thus been to question the extent to which positivism may be characterized as 'scientific' in relation to fundamental epistemological foundations. These debates also rage \"within\" contemporary social sciences with regard to subjectivity, objectivity, intersubjectivity and practicality in the conduct of theory and research. Philosophers of social science examine further epistemologies and methodologies, including realism, critical realism, instrumentalism, functionalism, structuralism, interpretivism, phenomenology, and post-structuralism.\n\nThough essentially all major social scientists since the late 19th century have accepted that the discipline faces challenges that are different from those of the natural sciences, the ability to determine causal relationships invokes the same discussions held in science meta-theory. Positivism has sometimes met with caricature as a breed of naive empiricism, yet the word has a rich history of applications stretching from Comte to the work of the Vienna Circle and beyond. By the same token, if positivism is able to identify causality, then it is open to the same critical rationalist non-justificationism presented by Karl Popper, which may itself be disputed through Thomas Kuhn's conception of epistemic paradigm shift.\n\nEarly German hermeneuticians such as Wilhelm Dilthey pioneered the distinction between natural and social science ('Geisteswissenschaft'). This tradition greatly informed Max Weber and Georg Simmel's antipositivism, and continued with critical theory. Since the 1960s, a general weakening of deductivist accounts of science has grown side-by-side with critiques of \"scientism\", or 'science \"as ideology\"'. Jürgen Habermas argues, in his \"On the Logic of the Social Sciences\" (1967), that \"the positivist thesis of unified science, which assimilates all the sciences to a natural-scientific model, fails because of the intimate relationship between the social sciences and history, and the fact that they are based on a situation-specific understanding of meaning that can be explicated only hermeneutically … access to a symbolically prestructured reality cannot be gained by observation alone.\" \"Verstehende\" social theory has been the concern of phenomenological works, such as Alfred Schütz \"Phenomenology of the Social World\" (1932) and Hans-Georg Gadamer's \"Truth and Method\" (1960). Phenomenology would later prove influential in the subject-centred theory of the post-structuralists.\n\nThe mid-20th-century linguistic turn led to a rise in highly philosophical sociology, as well as so-called \"postmodern\" perspectives on the social acquisition of knowledge. One notable critique of social science is found in Peter Winch's Wittgensteinian text \"The Idea of Social Science and its Relation to Philosophy\" (1958). Michel Foucault provides a potent critique in his archaeology of the human sciences, though Habermas and Richard Rorty have both argued that Foucault merely replaces one such system of thought with another.\n\nOne underlying problem for the social psychologist is whether studies can or should ultimately be understood in terms of the meaning and consciousness behind social action, as with folk psychology, or whether more objective, natural, materialist, and behavioral facts are to be given exclusive study. This problem is especially important for those within the social sciences who study qualitative mental phenomena, such as consciousness, associative meanings, and mental representations, because a rejection of the study of meanings would lead to the reclassification of such research as non-scientific. Influential traditions like psychodynamic theory and symbolic interactionism may be the first victims of such a paradigm shift. The philosophical issues lying in wait behind these different positions have led to commitments to certain kinds of methodology which have sometimes bordered on the partisan. Still, many researchers have indicated a lack of patience for overly dogmatic proponents of one method or another.\n\nSocial research remains extremely common and effective \"in practise\" with respect to political institutions and businesses. Michael Burawoy has marked the difference between public sociology, which is focused firmly on practical applications (though see e.g. Thibodeaux, 2016), and \"academic\" or \"professional\" sociology, which involves dialogue amongst other social scientists and philosophers.\n\nStructure and agency forms an enduring debate in social theory: \"Do social structures determine an individual's behaviour or does human agency?\" In this context 'agency' refers to the capacity of individuals to act independently and make free choices, whereas 'structure' refers to factors which limit or affect the choices and actions of individuals (such as social class, religion, gender, ethnicity, and so on). Discussions over the primacy of structure or agency relate to the very core of social ontology (\"What is the social world made of?\", \"What is a cause in the social world, and what is an effect?\"). One attempt to reconcile postmodern critiques with the overarching project of social science has been the development, particularly in Britain, of critical realism. For critical realists such as Roy Bhaskar, traditional positivism commits an 'epistemic fallacy' by failing to address the ontological conditions which make science possible: that is, structure and agency itself.\n\n\n\n\n\n\n"}
{"id": "19278463", "url": "https://en.wikipedia.org/wiki?curid=19278463", "title": "Political opportunity", "text": "Political opportunity\n\nPolitical opportunity theory, sometimes also known as the political process theory or political opportunity structure, is an approach of social movements heavily influenced by political sociology. It argues that success or failure of social movements is primarily affected by political opportunities. Social theorists Peter Eisinger, Sidney Tarrow, David Meyer and Doug McAdam are considered among the most prominent supporters of this theory.\n\nThree vital components for movement formation are:\n\nPolitical opportunity theory argues that the actions of the activists are dependent on the existence – or lack – of a specific political opportunity. There are various definitions of political opportunity, but Meyer (2004) stresses that of Tarrow (1998):\nFrom these three components emerges what proponent Doug McAdam terms cognitive liberation, the ability for those active in political protest to recognize their collective strength and take advantage of political opportunities as they become available to them. As political opposition to the movement's demands weakens, members may feel a collective sense of symbolic efficacy, the capacity to enact significant change within the political arena. This opens up significant opportunities for movements to both recruit members and mobilize under a concentrated and effective cycle of demands.\n\nOver time these broad socioeconomic processes develop, maintain and cause decline within the movement. A movement, once developed, may be affected by the level of social control placed on it, which in turn affects its ability to mobilize and maintain members. For when the movement's demands are portrayed as underdeveloped or unattractive they risk losing or failing to receive support from outside institutions.\n\nFurthermore, movements may be affected by oligarchization, when a class of individuals within the movement work to ensure the maintenance of the movement itself rather than a continual push for collective goals, or cooptation, when outside support is garnered for the movement at the same time as it is forced to sacrifice its goals to meet the demands of these supporting institutions. This in turn may lead to the loss of indigenous support, and along with it many of the supporting grassroots organizations that were able to quickly mobilize members at the onset of the movement.\n\nMeyer (2004) credits Eisinger (1973) with first use of the political opportunity theory framed in such a way (traces of which, of course, go further back). Eisinger asked why in the 1960s the level of riots about race and poverty varied between different places in the United States and notes that lack of visible openings for participation of repressed or discouraged dissident made riots more likely. Thus the inability to legally air grievances was the political opportunity which led to organization and mobilization of movements expressing their grievances by rioting.\n\nMeyer (2004) in his overview of political opportunity theory noted that this broader context can affect:\n\nA key advantage of the theory is that it explains why social movements emerge and/or increase their activity at a given time. When there are no political opportunities, simply having grievances (organizational consciousness) and resources will not be enough. Only when all three of these components are present, the movement has a chance to succeed.\n\nAlong with social movement research, Thibodeaux argues that political opportunity structures should be used when analyzing the importance and meaning of social problems.\n\nWithin the structure and agency debate, actions of activists (agents) can only be understood when seen in the broader context of political opportunities (structure). The term structure has been used to characterize political opportunities in older scholarship. A political opportunity structure has been defined as the circumstances surrounding a political landscape. However, Tarrow – who has used this term in his earlier publications himself – now argues it is misleading, as most opportunities need to be perceived, and are situational, not structural. Political opportunity structures are prone to change and can alter in days (or last for decades). Demographics and socioeconomic factors create \"structure\" which affects political actors.\n\nOne side model based on the political opportunity theory is known as the political mediation model. The political mediation model focuses on how the political context affects the strategic choices of the political actors. This model goes beyond looking at whether the movements just succeeded or failed, and analyzes other consequences, including unintentional, as well as collective benefits.\n\nOpposite of political opportunity is a political constraint.\n\nThe political process model has been criticized both structurally and conceptually. Critics suggest that political process theorists utilize overly broad definitions as to what constitute political opportunities, and those definitions vary widely based on the historical context of the social movement itself. Furthermore, as political process theory frames movements as legally or politically detached from the state, it ignores movements that form out of cultural solidarity or do not directly stand in opposition to extant rules or regulations. Critics contend that theorists place too great an emphasis on the role of social networks while often almost entirely ignoring the cultural underpinnings that allow these networks to form and subsist.\nIn response to some criticisms, Doug McAdam, Sidney Tarrow and Charles Tilly proposed the Dynamics of Contention research program, which focuses on identifying mechanisms to explain political opportunities, rather than relying on an abstract structure.\n\nMoveOn.org is an organization that started in 1998 which still operates to this day. MoveOn.org is a progressive organization that specifically centers around political issues. MoveOn.org allows viewers to start their own petitions, a form of collective behavior, which could potentially start a social movement of its own. MoveOn.org also includes other petitions and political articles and video clips on the front page for people to sign and view as a mechanism for people to assemble over a similar issue, perpetuating the concept of solidarity, or a sense of collective identity over political discourse. MoveOn.org can also be applied to resource mobilization theory due to the fact that MoveOn.org is a site that is meant to assemble people, which adds to the strength and success of the organization.\n\n\n"}
{"id": "24178036", "url": "https://en.wikipedia.org/wiki?curid=24178036", "title": "Punched card input/output", "text": "Punched card input/output\n\nA computer punched card reader or just computer card reader is a computer input device used to read computer programs in either source or executable form and data from punched cards. A computer card punch is a computer output device that punches holes in cards. Sometimes computer punch card readers were combined with computer card punches and, later, other devices to form multifunction machines.\nIt is a input device and also an output device.\nMost early computers, such as the ENIAC, and the IBM NORC, provided for punched card input/output. Card readers and punches, either connected to computers or in off-line card to/from magnetic tape configurations, were ubiquitous through the mid-1970s.\n\nPunched cards had been in use since the 1890s; their technology was mature and reliable. Card readers and punches developed for punched card machines were readily adaptable for computer use. Businesses were familiar with storing data on punched cards and keypunch machines were widely employed. Punched cards were a better fit than other 1950s technologies, such as magnetic tape, for some computer applications as individual cards could easily be updated without having to access a computer.\n\nThe standard measure of speed is \"cards per minute\", abbreviated CPM: The number of cards which can be read or punched in one minute. Card reader models vary from 300 to around 2,000 CPM. If all columns of an 80 column card encode information this translates to approximately 2,500 characters per second (CPS).\n\nCards may be read using mechanical \"brushes\" that make an electrical contact for a hole, and no contact if no punch, or photoelectric sensors that function similarly. Timing relates the signals to the position on the card. Cards may be read serially, column by column, or in parallel, row by row.\n\nCard punches necessarily run more slowly to allow for the mechanical action of punching, up to around 300 CPM or 400 characters per second.\n\nSome card devices offer the ability to \"interpret\", or print a line on the card displaying the data that is punched. Typically this slows down the punch operation. Many punches read the card just punched and compare its actual contents to the original data punched, to protect against punch errors. Some devices allow data to be read from a card and additional information to be punched into the same card.\n\nReaders and punches include a \"hopper\" for input cards and one or more \"stackers\" for cards read or punched. A function called \"stacker select\" allows the controlling computer to choose which stacker a card just read or punched will be placed into.\n\n\nDocumation Inc., of Melbourne, Florida, made card readers for minicomputers in the 1970s:\n\nTheir card readers have been used in elections, including the 2000 \"Chads\" election in Florida.\n\n\nFor some computer applications, binary formats were used, where each hole represented a single binary digit (or \"bit\"), every column (or row) is treated as a simple bitfield, and every combination of holes is permitted. For example, the IBM 711 card reader used with the 704/709/7090/7094 series scientific computers treated every row as two 36-bit words, ignoring 8 columns. (The specific 72 columns used were selectable using a plugboard control panel, which is almost always wired to select columns 1–72.) Sometimes the ignored columns (usually 73–80) were used to contain a sequence number for each card, so the card deck could be sorted to the correct order in case it was dropped. An alternative format, used by the IBM 704's IBM 714 native card reader, is referred to as Column Binary or Chinese Binary, and used 3 columns for each 36-bit word. Later computers, such as the IBM 1130 or System/360, used every column. The IBM 1401's card reader could be used in Column Binary mode, which stored two characters in every column, or one 36-bit word in three columns when used as input device for other computers. However, most of the older card punches were not intended to punch more than 3 holes in a column. The \"multipunch\" key is used to produce binary cards, or other characters not on the keyboard.\nAs a prank, in binary mode, cards could be punched where every possible punch position had a hole. Such \"lace cards\" lacked structural strength, and would frequently buckle and jam inside the machine.\n\n"}
{"id": "681628", "url": "https://en.wikipedia.org/wiki?curid=681628", "title": "Quasinormal mode", "text": "Quasinormal mode\n\nQuasinormal modes (QNM) are the modes of energy dissipation of a perturbed object or field, \"i.e.\" they describe perturbations of a field that decay in time.\n\nA familiar example is the perturbation (gentle tap) of a wine glass with a knife: the glass begins to ring, it rings with a set, or superposition, of its natural frequencies — its modes of sonic energy dissipation. One could call these modes \"normal\" if the glass went on ringing forever. Here the amplitude of oscillation decays in time, so we call its modes \"quasi-normal\". To a high degree of accuracy, quasinormal ringing can be approximated by\n\nwhere formula_2 is the amplitude of oscillation,\nformula_3 is the frequency, and\nformula_4 is the decay rate. The quasinormal\nfrequency is described by two numbers,\n\nor, more compactly\n\nHere, formula_8 is what is commonly referred to as the\nquasinormal mode frequency. It is a complex number with two pieces of information: real part is the temporal oscillation; imaginary part is the temporal, exponential decay.\n\nIn certain cases the amplitude of the wave decays quickly, to follow the decay for a longer time one may plot formula_9\n\nIn theoretical physics, a quasinormal mode is a formal solution of linearized differential equations (such as the linearized equations of general relativity constraining perturbations around a black hole solution) with a complex eigenvalue (frequency).\n\nBlack holes have many quasinormal modes (also: ringing modes) that describe the exponential decrease of asymmetry of the black hole in time as it evolves towards the perfect spherical shape.\n\nRecently, the properties of quasinormal modes have been tested in the context of the AdS/CFT correspondence. Also, the asymptotic behavior of quasinormal modes was proposed to be related to the Immirzi parameter in loop quantum gravity, but convincing arguments have not been found yet.\n\nIn computational biophysics, quasinormal modes, also called quasiharmonic modes, are derived from diagonalizing the matrix of equal-time correlations of atomic fluctuations.\n\n"}
{"id": "50093614", "url": "https://en.wikipedia.org/wiki?curid=50093614", "title": "Research Data Alliance", "text": "Research Data Alliance\n\nThe Research Data Alliance (RDA) is a research community organization started in 2013 by the European Commission, the American National Science Foundation and National Institute of Standards and Technology, and the Australian Department of Innovation. Its mission is to build the social and technical bridges to enable open sharing of data. The RDA vision is researchers and innovators openly sharing data across technologies, disciplines, and countries to address the grand challenges of society. The RDA is a major recipient of support in the form of grants from its constituent members' governments.\n\nAs of July 2018, the RDA has over 7,100 individual members from 137 countries.\n\nThe RDA's main vehicle for outputs are 18-month long working groups that generate recommendations aimed at the RDA community. In addition to working groups, interest groups with no fixed lifetime can produce either informal or \"supported\" outputs which carry some degree of RDA endorsement. \n\nThe RDA organises two major plenary conferences a year that are often co-located within other international data sharing initiatives such as the 12th RDA plenary being part of \"International Data Week, 2018\" in Gaborone, co-organised by RDA, the ICSU World Data System (WDS), the ICSU Committee on Data for Science and Technology (CODATA), University of Botswana (UoB) and the Academy of Science of South Africa (ASSAf). RDA aims to spread the plenary meetings across many of its members' locales with recent plenaries being held in Berlin, Montréal, Barcelona, Denver, Tokyo, Paris, San Diego, Amsterdam, Dublin, Washington DC and Gothenburg, Sweden. \n\nThe RDA provides national data sharing organisations, such as the Australian National Data Service (ANDS), an \"influence over the kinds of data sharing environments that Australian researchers will work with when they collaborate with international colleagues\". The RDA is partnered with many major international data initiatives such as DataCite and frequently forms joint working groups with them, such as with the World Data System.\n"}
{"id": "22307545", "url": "https://en.wikipedia.org/wiki?curid=22307545", "title": "Reverse computation", "text": "Reverse computation\n\nReverse computation is a software application of the concept of reversible computing.\n\nBecause it offers a possible solution to the heat problem faced by chip manufacturers, reversible computing has been extensively studied in the area of computer architecture. The promise of reversible computing is that the amount of heat loss for reversible architectures would be minimal for significantly large numbers of transistors. Rather than creating entropy (and thus heat) through destructive operations, a reversible architecture conserves the energy by performing other operations that preserve the system state.\n\nThe concept of reverse computation is somewhat simpler than reversible computing in that reverse computation is only required to restore the \"equivalent\" state of a software application, rather than support the reversibility of the set of all possible instructions. Reversible computing concepts have been successfully applied as \"reverse computation\" in software application areas such as database design, checkpointing and debugging, and code differentiation.\n\nBased on the successful application of Reverse Computation concepts in other software domains, Chris Carothers, Kalyan Perumalla and Richard Fujimoto suggest the application of reverse computation to reduce state saving overheads\nin \"parallel discrete event simulation\" (PDES). They define an approach based on reverse event codes (which can be automatically generated), and\ndemonstrate performance advantages of this approach over\ntraditional state saving for fine-grained applications (those\nwith a small amount of computation per event).\nThe key property that reverse computation\nexploits is that a majority of the operations that modify the\nstate variables are “constructive” in nature. That is, the undo\noperation for such operations requires no history. Only the\nmost current values of the variables are required to undo the\noperation. For example, operators such as ++, ––, +=, -=, *=\nand /= belong to this category. Note, that the *= and /= operators\nrequire special treatment in the case of multiply or divide\nby zero, and overflow / underflow conditions. More complex\noperations such as circular shift (swap being a special case),\nand certain classes of random number generation also belong\nhere.\n\nOperations of the form a = b, modulo and bit-wise computations\nthat result in the loss of data, are termed to be destructive.\nTypically these operations can only be restored using\nconventional state-saving techniques. However, we observe\nthat many of these destructive operations are a consequence\nof the arrival of data contained within the event being\nprocessed. For example, in the work of Yaun, Carothers, et al., with large-scale TCP simulation, the last-sent time records\nthe time stamp of the last packet forwarded on a router logical process.\nThe swap operation makes this operation reversible.\n\nIn 1985 Jefferson introduced the optimistic synchronization protocol, which was utilized in parallel discrete event simulations, known as Time Warp. To date, the technique known as \"Reverse Computation\" has only been applied in software for optimistically synchronized, parallel discrete event simulation.\n\nIn December 1999, Michael Frank graduated from the University of Florida. His doctoral thesis focused on reverse computation at the hardware level, but included descriptions of both an instruction set architecture and a high level programming language (R) for a processor based on reverse computation.\nIn 1998 Carothers and Perumalla published a paper for the Principles of Advanced and Distributed Simulation workshop as part of their graduate studies under Richard Fujimoto, introducing technique of Reverse Computation as an alternative rollback mechanism in optimistically synchronized parallel discrete event simulations (Time Warp). In 1998, Carothers became an associate professor at Rensselaer Polytechnic Institute. Working with graduate students David Bauer and Shawn Pearce, Carothers integrated the Georgia Tech Time Warp design into Rensselaer’s Optimistic Simulation System (ROSS), which supported only reverse computation as the rollback mechanism. Carothers also constructed RC models for BitTorrent at General Electric, as well as numerous network protocols with students (BGP4, TCP Tahoe, Multicast). Carothers created a course on Parallel and Distributed Simulation in which students were required to construct RC models in ROSS.\n\nAround the same time, Perumalla graduated from Georgia Tech and went to work at the Oak Ridge National Laboratory (ORNL). There he built the uSik simulator, which was a combined optimistic / conservative protocol PDES. The system was capable of dynamically determining the best protocol for LPs and remapping them during execution in response to model dynamics. In 2007 Perumalla tested uSik on Blue Gene/L and found that, while scalability is limited to 8K processors for pure Time Warp implementation, the conservative implementation scales to 16K available processors. Note that benchmarking was performed using PHOLD with a constrained remote event rate of 10%, where the timestamp of events was determined by an exponential distribution with a mean of 1.0, and an additional lookahead of 1.0 added to each event. This was the first implementation of PDES on Blue Gene using reverse computation.\n\nFrom 1998 to 2005 Bauer performed graduate work at RPI under Carothers, focusing solely on reverse computation. He developed the first PDES system solely based on reverse computation, called Rensselaer’s Optimistic Simulation System (ROSS). for combined shared and distributed memory systems. From 2006 to 2009 Bauer worked under E.H. Page at Mitre Corporation, and in collaboration with Carothers and Pearce pushed the ROSS simulator to the 131,072 processor Blue Gene/P (Intrepid). This implementation was stable for remote event rates of 100% (every event sent over the network). During his time at RPI and MITRE, Bauer developed the network simulation system ROSS.Net that supports semi-automated experiment design for black-box optimization of network protocol models executing in ROSS. A primary goal of the system was to optimize multiple network protocol models for execution in ROSS. For example, creating an LP layering structure to eliminate events being passed between network protocol LPs on the same simulated machine optimizes simulation of TCP/IP network nodes by eliminating zero-offset timestamps between TCP and IP protocols. Bauer also constructed RC agent-based models for social contact networks to study the effects of infectious diseases, in particular pandemic influenza, that scale to hundreds of millions of agents; as well as RC models for Mobile ad-hoc networks implementing functionality of mobility (proximity detection) and highly accurate physical layer electromagnetic wave propagation (Transmission Line Matrix model).\n\nThere has also been a recent push by the PDES community into the realm of continuous simulation. For example, Fujimoto and Perumalla, working with Tang et al. have implemented an RC model of particle-in-cell and demonstrated excellent speedup over continuous simulation for models of light as a particle. Bauer and Page demonstrated excellent speedup for an RC Transmission Line Matrix model (P.B. Johns, 1971), modeling light as a wave at microwave frequencies. Bauer also created an RC variant of SEIR that generates enormous improvement over continuous models in the area of infectious disease spread. In addition, the RC SEIR model is capable of modeling multiple diseases efficiently, whereas the continuous model explodes exponentially with respect to the number of combinations of diseases possible throughout the population.\n\n"}
{"id": "27380463", "url": "https://en.wikipedia.org/wiki?curid=27380463", "title": "Sensitivity (control systems)", "text": "Sensitivity (control systems)\n\nThe controller parameters are typically matched to the process characteristics and since the process may change, it is important that the controller parameters are chosen in such a way that the closed loop system is not sensitive to variations in process dynamics. One way to characterize sensitivity is through the nominal sensitivity peak formula_1:\n\nformula_2\n\nwhere formula_3 and formula_4 denote the plant and controller's transfer function in a basic closed loop control system using unity negative feedback. \n\nThe sensitivity function formula_5, which appears in the above formula also describes the transfer function from external disturbance to process output. In fact, assuming an additive disturbance \"n\" after the output \n\nof the plant, the transfer functions of the closed loop system are given by\n\nformula_6\n\nHence, lower values of formula_7 suggest further attenuation of the external disturbance. The sensitivity function tells us how the disturbances are influenced by feedback. Disturbances with frequencies such that formula_8 is less than one are reduced by an amount equal to the distance to the critical point formula_9 and disturbances with frequencies such that formula_8 is larger than one are amplified by the feedback.\n\nIt is important that the largest value of the sensitivity function be limited for a control system and it is common to require that the maximum value of the sensitivity function, formula_1, be in a range of 1.3 to 2.\n\nThe quantity formula_1 is the inverse of the shortest distance from the Nyquist curve of the loop transfer function to the critical point formula_9. A sensitivity formula_1 guarantees that the distance from the critical point to the Nyquist curve is always greater than formula_15 and the Nyquist curve of the loop transfer function is always outside a circle around the critical point formula_16 with the radius formula_15, known as the sensitivity circle. formula_1 defines the maximum value of the sensitivity function and the inverse of formula_1 gives you the shortest distance from the open-loop transfer function formula_20 to the critical point formula_16.\n\n"}
{"id": "2155264", "url": "https://en.wikipedia.org/wiki?curid=2155264", "title": "Skepticality", "text": "Skepticality\n\nSkepticality is the official podcast of The Skeptics Society's \"Skeptic\" magazine. Beginning in May 2005, the podcast explores rational thought, skeptical ideas, and famous myths from around the world and throughout history. Each episode is an audio magazine featuring regular segments by contributors who are specialized in specific areas of critical thought followed by featured content which is usually in the form of an interview with a researcher, author, or individual who is helping promote skeptical thought and/or science in an effective way. It has featured interviews with James Randi, and scientists, such as authors and astronomers Phil Plait and Neil deGrasse Tyson, Greg Graffin from \"Bad Religion\", Adam Savage from the \"MythBusters\", songwriter Jill Sobule, author Ann Druyan and science communicator Bill Nye.\n\n\"Skepticality\" is co-hosted by Derek Colanduno and \"Swoopy\" Robynn McCarthy.\n\nThe concept and the name \"Skepticality\" were created in May 2005 by Robynn McCarthy and Derek Colanduno, after the two became friends in Las Vegas. At the time, Colanduno was working at a national Sports Radio network and a privately owned Alternative Rock Station (KEDG) during the overnight shift. Skepticality gained notability on September 7, 2005 during a keynote address, when Apple CEO Steve Jobs mentioned it as one of the top nine podcasts at the iTunes Music Store. On August 14, 2006, \"Skepticality\" became \"Skeptic\" magazine's official podcast.\n\n\nThe show has a number of guests who have been featured on more than one show. Amongst them are James Randi, Phil Plait, Pamela Gay and skeptical musician George Hrab.\n\nIn an interview with Derek during the June 1, 2006 episode of \"Slacker Astronomy\", the naming of Asteroids 106545 Colanduno and 106537 McCarthy was announced to the world. The asteroids were named in homage to the hosts of \"Skepticality\" by their discoverer the late Jeff Medkeff, who said, \"My naming of these asteroids for you is a token of my esteem for you and your accomplishments.\"\n\nIn 2007, \"Skepticality\" was recognized for excellence in podcasting with the Best Speculative Fiction News Podcast award at the Parsec Awards and Best Science Podcast award at the Podcast Peer Awards, selected by registered fellow podcasters. Both presentations were made at Dragon*Con 2007 in Atlanta, Georgia. \n\nOn November 22, 2007, the \"Skepticality\" podcast was listed as \"Site of the Week\" on SciFi.com's \"Sci Fi Weekly\".\n\nOn August 9, 2008, \"Skepticality\" was named \"Podcast of the Week\" by \"The Times\".\n\nIn April 2014, \"Skepticality\" received the Ockham Award at QED for Best Podcast. The award was accepted on behalf of Derek and Swoopy by Susan Gerbic.\n"}
{"id": "27120022", "url": "https://en.wikipedia.org/wiki?curid=27120022", "title": "The Australian Sociological Association", "text": "The Australian Sociological Association\n\nThe Australian Sociological Association (TASA) organisation of sociologists throughout Australia. TASA was founded in 1963 as the Sociological Association of Australia and New Zealand (SAANZ). In 1988 with the New Zealand branch splitting off into the Sociological Association of Aotearoa (New Zealand), the association changed its name to TASA.\n\nTASA holds an annual conference, and publishes since 1965 the (formerly, Australian and New Zealand) \"Journal of Sociology\", the premiere journal for Australian sociological research.\n\nMembership of TASA stood at about 400 in the early 1900s.\n\n\n"}
{"id": "19689038", "url": "https://en.wikipedia.org/wiki?curid=19689038", "title": "The Johns Hopkins Science Review", "text": "The Johns Hopkins Science Review\n\nThe Johns Hopkins Science Review is a US television series about science that was produced at Johns Hopkins University in Baltimore, Maryland from 1948-1955. Starting in 1950, the series aired on the DuMont Television Network until the network's demise in 1955. The series' creator was Lynn Poole, who wrote or co-wrote most of its episodes and acted as the on-camera host. In 2002, Patrick Lucanio and Gary Coville wrote that, \"In retrospect, Lynn Poole created one of those unique series that allowed television to fulfill its idealized mission as both an educational and an entertainment medium.\"\n\nThe original series was followed by three related series produced by Poole at Johns Hopkins University: \"Tomorrow\" (1955), \"Tomorrow's Careers\" (1955-1956), and \"Johns Hopkins File 7\" (1956-1960). Johns Hopkins University ended its production of television series in 1960.\n\nThe original series aired from March 9, 1948, to March 6, 1955. Initially, the show was broadcast only in the Baltimore area. Starting with the December 17, 1948, episode, shows were broadcast by CBS from stations along the east coast.\n\nAs was typical in the early days of television broadcasting, each show was broadcast live from a studio at Johns Hopkins University. Each week's show involved one or more guests, often from the Johns Hopkins faculty and staff. Poole acted as the host and interviewer. The guest might show how a scientific apparatus such an electron microscope or an oscilloscope worked, or would briefly explain scientific ideas to the viewers. In the December 5, 1950, episode, the live broadcast of a fluoroscope screen was used by doctors in New York and Chicago to diagnose the injuries to a machinist in the hospital in Baltimore. In the April 21, 1952, episode, a scientist drank a solution containing the radioactive isotope of iodine, and then followed its progress in his own body with a Geiger counter. The guests were sometimes national figures like Wernher von Braun (October 20, 1952), George Gamov, and Harold Urey. The show famously showed a live birth and gave instructions to women viewers about breast self-examination.\n\nEach half-hour episode was broadcast from WAAM in Baltimore. The series moved to the DuMont Television Network in November 1949 through station WMAR. The program aired Tuesdays at 8:30 pm EST during the 1950-51 season, Mondays at 8:30pm EST during the 1951-52 season, and Wednesdays at 8pm EST during the 1952-53 season. According to the 1953-54 United States network television schedule, the show remained in the Wednesday at 8pm EST slot for the 1953-54 season.\n\nThe series would win the network Peabody Awards in 1950 (honorable mention) and 1952.\n\nA spin-off program, \"Johns Hopkins File 7\", aired on a syndicated basis from 1956 to 1960. Like the \"Review\", \"File 7\" was broadcast by WAAM and featured host Lynn Poole.\n\nApproximately 303 episodes of the original series were made. There are records of 238 episodes, and kinescope films from 186 episodes, stored in Special Collections of the Milton Eisenhower Library at Johns Hopkins University. This means it has the most surviving episodes of any DuMont Network program. The earliest surviving kinescope is from November 21, 1950. At least three episodes survive at the UCLA Film and Television Archive.\n\nIn addition, Johns Hopkins University has records and films of the three successor series.\n\n\n\n"}
{"id": "24538247", "url": "https://en.wikipedia.org/wiki?curid=24538247", "title": "The Magic Cauldron (essay)", "text": "The Magic Cauldron (essay)\n\n\"The Magic Cauldron\" is an essay by Eric S. Raymond on the open-source economic model. It can be read freely online and was published in his book \"The Cathedral and Bazaar\" in 1999.\n\nThe essay analyzes the economic models that Raymond believes can sustain an open-source project in four steps:\n\n\n"}
{"id": "50899649", "url": "https://en.wikipedia.org/wiki?curid=50899649", "title": "Tommaso Perelli", "text": "Tommaso Perelli\n\nTommaso Perelli (1704–1783) was an Italian astronomer.\n\nBorn into a noble family of Arezzo, Perelli was encouraged by his father to study law at the University of Pisa, but Guido Grandi (1671–1742), an abbot who was teaching mathematics there, steered him toward science. When his father died, Perelli abandoned the study of law for good. He decided to get a degree in physics and medicine. He studied astronomy and medicine at the University of Bologna, and Greek literature at the University of Padua. He was then appointed by the Tuscan government to the chair in astronomy at the University of Pisa, where he became a noted astronomer and hydraulics expert. He was the first to identify the hill of Arcetri, near Galileo's (1564–1642) last home, Villa Il Gioiello, as ideal location for astronomical observations.\n\n"}
{"id": "32125771", "url": "https://en.wikipedia.org/wiki?curid=32125771", "title": "Vietnamese studies", "text": "Vietnamese studies\n\nVietnamese studies (Việt Nam học) in general is the study of Vietnam and things related to Vietnam. It refers, especially, to the study of modern Vietnamese and literature, history, ethnology, and the philological approach, respectively.\n\nThe specialist in this area is called a Vietnamist.\n\nSeveral major universities in the United States offer a Vietnamese studies major or program, including the University of Houston, University of California, and Yale University. Some colleges, such as Hobart and William Smith Colleges, offer a study abroad exchange program. The Vietnam National University, Hanoi has a comprehensive Institute of Vietnamese Studies and Development Sciences. The Tokyo University of Foreign Studies also offers a program in the field. In Central Europe, the oldest university, Charles University in Prague also offers a study programme focused on Vietnamese studies, called Ethnology with Vietnamese\n\nIn Germany, Vietnamese Studies (Vietnamistik) has been taught at the Asien-Afrika-Institut of the University of Hamburg since 1982. Instructor Vũ Duy Từ teaches Vietnamese language and culture, from 1984 to 1999 he was accepted as a professor at this university. Vietnamese studies that took place in 1970 at Berlin's Humboldt University, four years after German unification was entered with the Institute of Oriental Studies and Studies in Southeast Asia. In 1998, the position of professor of Vietnamese studies was canceled. From 2002 until now Jörg Thomas Engelbert is professor of the field at the University of Hamburg.\n\n\n"}
{"id": "5188717", "url": "https://en.wikipedia.org/wiki?curid=5188717", "title": "Vladimir Prokhorovich Amalitskii", "text": "Vladimir Prokhorovich Amalitskii\n\nVladimir Prokhorovich Amalitskii (1860–1917) (alternative spelling: Amalitzky) was a Russian Paleontologist and Professor at Warsaw University, who was involved in the discovery and excavation of the famous Late Permian fossil vertebrate fauna from the North Dvina River, Arkhangelsk District, Northern European Russia.\n\nHe made a number of studies of the fossil remains of amphibians and reptiles from Northern Russia.\n\nHe died in December 28, 1917 in Kislovodsk city, Russia.\n\n\n"}
{"id": "49166662", "url": "https://en.wikipedia.org/wiki?curid=49166662", "title": "Wolf Creek Research Basin", "text": "Wolf Creek Research Basin\n\nWolf Creek is in the Yukon Territory, Canada.\n\nA research station at Wolf Creek Research Basin has been providing continuous data since 1992 for hydrological data. There are three meteorological stations, a groundwater monitoring well since 2003, as well as specialized instrumentation running continuously for specific projects.\n\nIt is an important location for the study of snow and climate in the region.\n"}
{"id": "764228", "url": "https://en.wikipedia.org/wiki?curid=764228", "title": "Youngs River", "text": "Youngs River\n\nThe Youngs River is a tributary of the Columbia River, approximately long, in northwest Oregon in the United States. It drains part of the Northern Oregon Coast Range in the extreme northwest corner of state, entering the Columbia via Youngs Bay just approximately from its mouth.\n\nIt rises in a remote section of the mountains of central Clatsop County, north of Saddle Mountain State Natural Area. It flows generally northwest, passing over Youngs River Falls. The falls were encountered in 1806 by a hunting party of the Lewis and Clark Expedition from nearby Fort Clatsop and documented in William Clark's journals. It broadens in a large estuary and enters the south end of Youngs Bay on the Columbia at Astoria. It receives the Klaskanine River from the east approximately south of Astoria. It receives the Wallooskee River from the east approximately south of Astoria.\n\nNamed tributaries of Youngs River from source to mouth are Fall Creek and South Fork Youngs River, then Fox, Osgood, Rock, Bayney, Wawa, and Moosmoos creeks followed by the Klaskanine River. Below that come Cooperage, Battle Creek, Tucker, Casey, Binder, and Cook sloughs followed by the Wallooskee River. Further downstream are Crosel, Brown, and Craig creeks followed by Knowland Slough and the Lewis and Clark River.\n\nAbout from the mouth of the river are Youngs River Falls, a tall waterfall.\n\n"}
