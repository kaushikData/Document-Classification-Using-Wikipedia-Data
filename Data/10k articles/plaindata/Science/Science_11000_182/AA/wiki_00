{"id": "8217126", "url": "https://en.wikipedia.org/wiki?curid=8217126", "title": "A329 road", "text": "A329 road\n\nThe A329 is an east-west road in Southern England that runs from Wentworth in Surrey to Thame in Oxfordshire. The A329 starts at the A30 in Surrey and passes through Ascot, Bracknell, Wokingham, Earley, Reading, Purley, Pangbourne, Lower Basildon, Streatley, Moulsford and Wallingford. It connects to junction 7 of the M40 before finally ending at a junction with the A418 at Thame.\n\nThe A329 originally (in 1922) terminated at Reading. In the 1930s it was extended to Shillingford on part of the route of the former A42. After the M40 motorway was opened in the 1970s it was extended to Thame on the former route of the B4013. In 1993, when the new Winterbrook Bridge enabled traffic to by-pass Wallingford, the section between Wallingford and Shillingford was downgraded to become an unclassified road.\n\nMost of the road is single carriageway, apart from the section through Bracknell, and in central Reading. In Reading the road is dualled where it forms the Inner Distribution Road. In Bracknell the A329 originally ran along the High Street (now pedestrianised) and continued west along the Wokingham Road (now B3408). Today the original London Road is dualled for a short section approaching the Met Office Roundabout. The dual carriageway then by-passes north of the pedestrianised town centre to the Giroscope Roundabout, where it crosses the former route, heading south west to the Twin Bridges roundabout and along the dual Berkshire Way to meet the old route at Amen Corner. Here the A329 continues into Wokingham on the original route, while the A329(M) by-passes to the north.\n\n"}
{"id": "31926884", "url": "https://en.wikipedia.org/wiki?curid=31926884", "title": "Adelaide River virus", "text": "Adelaide River virus\n\nAdelaide River virus is a negative-sense single-stranded RNA virus of the family \"Rhabdoviridae\". The virus's primary hosts are all bovine, including domestic Water Buffalo, and cape buffalo.\n\n"}
{"id": "4401063", "url": "https://en.wikipedia.org/wiki?curid=4401063", "title": "Alexander Rose (geologist)", "text": "Alexander Rose (geologist)\n\nAlexander Rose (1781 – 1860) of Edinburgh was a wood and ivory turner, following in the footsteps of his father, John, who came from Cromarty. He developed an interest in minerals and began a mineral collection, becoming a dealer in minerals. He later became a lecturer in geology and mineralogy at Queen's College, Edinburgh and was eventually nominated as a Fellow of the Royal Scottish Academy.\n\nHe was educated at the Royal High School and in 1816, he married Isabella Boyne. They had three sons and six daughters.\n\nIn 1834, eleven of his students set up the Edinburgh Geological Society, of which Rose became President for eleven years until 1846.\n\nHe retired from active work in 1856 and died four years later.\n\nRecently, he has come to fame again as the notorious 'Alex the Geologist' in the math problem books written by Phillips Exeter Academy. The series of problems, featured throughout all the math levels, has Alex located in the desert, a certain distance from a road and a certain distance from his camp. Generally, the goal is to calculate the fastest route back to his camp. \n\n"}
{"id": "2263034", "url": "https://en.wikipedia.org/wiki?curid=2263034", "title": "American Solar Challenge", "text": "American Solar Challenge\n\nThe American Solar Challenge (ASC), previously known as the North American Solar Challenge and Sunrayce, is a solar car race across the United States. In the race, teams from colleges and universities throughout North America design, build, test, and race solar-powered vehicles in a long distance road rally-style event. ASC is a test of teamwork, engineering skill, and endurance that stretches across thousands of miles of public roads.\n\nASC 2016 took place from July 30 – August 6, on a 1,975 mile (3,178 km) route from Brecksville, Ohio to Hot Springs, South Dakota.\n\n\nOriginally called \"Sunrayce USA\", the first race was organized and sponsored by General Motors in 1990 in an effort to promote automotive engineering and solar energy among college students. At the time, GM had just won the inaugural World Solar Challenge in Australia in 1987; rather than continue actively racing, it instead opted to sponsor collegiate events.\n\nSubsequent races were held in 1993, 1995, 1997 and 1999 under the name \"Sunrayce [year]\" (e.g. Sunrayce 93). In 2001, the race was renamed \"American Solar Challenge\" and was sponsored by the United States Department of Energy and the National Renewable Energy Laboratory. Beginning in 2005, its name changed again to \"North American Solar Challenge\", in order to reflect the border crossing into Canada and the addition of co-sponsor Natural Resources Canada. The name was changed back to ASC in 2010.\n\nAfter the 2005 race, the U.S. Department of Energy discontinued its sponsorship, resulting in no scheduled race for 2007. Sponsorship was taken over for NASC 2008 by Toyota. The American Solar Challenge is now governed by the \"Innovators Educational Foundation\".\n\nThe original, Sunrayce USA route started at Disney World in Orlando, Florida and ended at the General Motors Technical Center in Warren, Michigan. The winner of the first race was the University of Michigan Solar Car Team's \"Sunrunner\", with an average speed of , followed by Western Washington University's \"Viking XX\".\n\nOverall Standings\n\nSunrayce 93 was held June 20–26, 1993. The race route covered over starting in Arlington, TX and ending in Minneapolis, Minnesota. The first place car was \"Maize & Blue\" from the University of Michigan followed by the \"Intrepid\" from Cal Poly Pomona.\n\nOverall Standings\n\nSunrayce 95 ran along a route from Indianapolis, Indiana to Golden, Colorado. Massachusetts Institute of Technology's \"Manta\" won the race with an average speed of , followed by the University of Minnesota's \"Aurora II\" just 18 minutes behind.\n\nOverall Standings\n\nSunrayce 1997 followed a familiar route from Indianapolis, Indiana to a finish line in Colorado Springs, Colorado.\nCalifornia State University-Los Angeles's \"Solar Eagle III\" won the nine-day Sunrayce 97. \"Solar Eagle III\" averaged , followed by MIT's \"Manta GT\" in second place.\n\nOverall Standings\n\nSunrayce 99, running from Washington, D.C., to Orlando, Florida, was notable for its lack of sunshine. The University of Missouri-Rolla's \"Solar Miner II\" won the race with an average speed of . The car from Queen's University placed second.\n\nOverall Standings\n\nIn 2001, the race changed its name to the American Solar Challenge and followed a new route from Chicago, Illinois to Claremont, California along much of the old U.S. Route 66. The University of Michigan won the overall race and the Open Class with a total elapsed time of 56 hours, 10 minutes, and 46 seconds, followed by the University of Missouri-Rolla. The University of Arizona team won the Stock Class event.\n\nOverall Standings\n\nThe 2003 American Solar Challenge also followed U.S. Route 66. \"Solar Miner IV\" from the University of Missouri-Rolla won the race overall, as well as the Open Class, followed by the University of Minnesota's \"Borealis II\". The Stock Class was won by the \"Prairie Fire GT\" from North Dakota State University.\n\nOverall Standings\n\nThe 2005 race, renamed the North American Solar Challenge, was both the longest and most hotly contested race in the history of the event. The route covered , taking the teams from Austin, Texas in the United States to Calgary, Alberta in Canada. The race was won by the \"Momentum\" from the University of Michigan with an average speed of . The University of Minnesota's \"Borealis III\" followed in second place less than 12 minutes behind, with an average speed of . The lead teams often drove (the maximum allowed), but were slowed by rain in Kansas and headwinds in Canada. Stanford University's \"Solstice\" won the Stock Class, followed in second place by the \"Beam Machine\" from The University of California, Berkeley.\n\nOverall Standings\n\nThe 2008 North American Solar Challenge took place on July 13–22, 2008, mostly along the 2005 route from Dallas, Texas to Calgary, Alberta. The University of Michigan's \"Continuum\" won the race with a total elapsed time of 51 hours, 41 minutes, and 53 seconds, marking that school's fifth victory. \"Ra 7\" from Principia College followed in second place.\n\nAs many of the top cars were bumping up against the race speed limit in the 2005 event, race rules were changed for 2008 order to improve safety and limit performance. Open class cars are now only allowed 6 square meters of active cell area, and upright seating is required for both open and stock class cars. The changes were carried over from the 2007 World Solar Challenge.\n\nOverall Standings\n\nThe 2010 race, renamed the American Solar Challenge, ran June 20–26, 2010. The University of Michigan finished in first place, followed by the University of Minnesota's \"Centaurus II\" in 2nd place and team Bochum from Germany in 3rd. The race route was entirely within the United States for the first time since 2003.\n\nOverall Standings\n\nOnly four teams finished the 2012 American Solar Challenge, a 1600-mile race from Rochester, NY to St. Paul, MN, under solar power alone. The University of Michigan's \"Quantum\" won the overall competition, over 10 hours ahead of the 2nd place team. The 2nd, 3rd, and 4th place teams were only an hour apart from each other. In order: Iowa State University's \"Hyperion\", Principia College's \"Ra7s\", and the University of California, Berkeley's \"Impulse\".\n\nOverall Standings\n\nThe 2014 American Solar Challenge reverted to the familiar south-north race route starting in Austin, Texas, and finishing in Minneapolis, Minnesota. The University of Michigan's \"Quantum\" once again took 1st place, followed by University of Minnesota's \"Centaurus III\". Both teams had brought back their cars from the 2012 event.\n\nOverall Standings\n\nThe 2016 American Solar Challenge ran from Brecksville, Ohio to Hot Springs, South Dakota from July 30 to August 6, 2016. ASC partnered with the National Park Service, and the route included stages and checkpoints at 9 national parks, historic sites, or partner properties throughout the Midwest. The University of Michigan's \"Aurum\" won the overall competition, by a margin of over 11 hours. In second place was the Dunwoody College of Technology team in partnership with Zurich University of Applied Sciences. The University of Minnesota's \"Eos I\" made history as the first Cruiser Class vehicle to ever compete in ASC.\n\nOverall Standings\n\nThe 2018 American Solar Challenge will be the first to include a Cruiser Class, featuring more practical multi-occupant Solar Vehicles. Regulations have been released. The event will run 9 days from Omaha, Nebraska to Bend, Oregon.\n\n\n\n"}
{"id": "28913839", "url": "https://en.wikipedia.org/wiki?curid=28913839", "title": "Arneb Glacier", "text": "Arneb Glacier\n\nArneb Glacier or Hallett Glacier () is a glacier long and wide, situated in a cliff-walled bay between Hallett Peninsula and Redcastle Ridge and flowing northwest into Edisto Inlet as a floating ice tongue. It was named by the New Zealand Geological Survey Antarctic Expedition, 1957–58, for , which in the 1957 season carried the buildings and stores for the establishment of Hallett Station and revisited the station in subsequent seasons.\n\n"}
{"id": "40781", "url": "https://en.wikipedia.org/wiki?curid=40781", "title": "Beam divergence", "text": "Beam divergence\n\nIn electromagnetics, especially in optics, beam divergence is an angular measure of the increase in beam diameter or radius with distance from the optical aperture or antenna aperture from which the beam emerges. The term is relevant only in the \"far field\", away from any focus of the beam. Practically speaking, however, the far field can commence physically close to the radiating aperture, depending on aperture diameter and the operating wavelength.\n\nBeam divergence is often used to characterize electromagnetic beams in the optical regime, for cases in which the aperture from which the beam emerges is very large with respect to the wavelength. However, it is also used in the radio frequency (RF) band for cases in which the antenna is very large relative to a wavelength.\n\nBeam divergence usually refers to a beam of circular cross section, but not necessarily so. A beam may, for example, have an elliptical cross section, in which case the orientation of the beam divergence must be specified, for example with respect to the major or minor axis of the elliptical cross section.\n\nThe divergence of a beam can be calculated if one knows the beam diameter at two separate points far from any focus (\"D\", \"D\"), and the distance (\"l\") between these points. The beam divergence, formula_1, is given by \n\nIf a collimated beam is focused with a lens, the diameter formula_3 of the beam in the rear focal plane of the lens is related to the divergence of the initial beam by \nwhere \"f\" is the focal length of the lens. Note that this measurement is valid only when the beam size is measured at the rear focal plane of the lens, i.e. where the focus would lie for a truly collimated beam, and not at the actual focus of the beam, which would occur behind the rear focal plane for a divergent beam.\n\nLike all electromagnetic beams, lasers are subject to divergence, which is measured in milliradians (mrad) or degrees. For many applications, a lower-divergence beam is preferable. Neglecting divergence due to poor beam quality, the divergence of a laser beam is proportional to its wavelength and inversely proportional to the diameter of the beam at its narrowest point. For example, an ultraviolet laser that emits at a wavelength of 308 nm will have a lower divergence than an infrared laser at 808 nm, if both have the same minimum beam diameter. The divergence of good-quality laser beams is modeled using the mathematics of Gaussian beams.\n\nGaussian laser beams are said to be diffraction limited when their radial beam divergence formula_5 is close to the minimum possible value, which is given by \n\nwhere formula_7 is the laser wavelength and formula_8 is the radius of the beam at its narrowest point, which is called the \"beam waist\". This type of beam divergence is observed from optimized laser cavities. Information on the diffraction-limited divergence of a coherent beam is inherently given by the N-slit interferometric equation.\n\n\n"}
{"id": "2803006", "url": "https://en.wikipedia.org/wiki?curid=2803006", "title": "Behzad Ghorbani", "text": "Behzad Ghorbani\n\nBehzad Ghorbani ( March 26, 1971, Tehran) is an Iranian zoologist and the first Iranian planarialogist. He graduated from the University of Tehran and Shahid Beheshti University (The National University of Iran). In 1997, he identified two new species (\"Dugesia iranica\" and \"Dugesia persica\"), that were found in the Karaj River.\n\n\n"}
{"id": "20006473", "url": "https://en.wikipedia.org/wiki?curid=20006473", "title": "Cavallo's multiplier", "text": "Cavallo's multiplier\n\nCavallo's multiplier was an early electrostatic influence machine, invented in 1795 by the Anglo-Italian natural philosopher Tiberius Cavallo. Its purpose was to multiply, or amplify, a small electric charge to a level where it was detectable by the insensitive electroscopes of the day. Repeated operation of the device could produce voltages high enough to generate sparks.\n\nCavallo described his machine in his 1795 \"Treatise on Electricity\". He had examined Bennet's charge doubler of 1787 and found it wanting in several regards, notably in its inconsistent operation and tendency to retain the charge from an earlier experiment. Cavallo resolved to build a better device. His machine consisted of four metal plates supported on a wooden board by posts, of which three were insulating and one conducting.\n\nThe charge to be multiplied was applied to the first of these (plate \"A\"), which stood on an insulating post. A moveable insulated metal plate (\"B\") was brought close to \"A\" (though not permitted to touch it), and then grounded. The charge on \"A\" caused charge separation on \"B\" due to electrostatic induction. Plate \"B\" was then moved away, breaking its earth connection. Since \"B\" was insulated, it acquired and retained a small charge opposite in sign to the charge on \"A\". Plate \"B\" was transferred by means of an insulating rod to be brought into electrical contact with the third metal plate \"C\" which was insulated. Since both \"B\" and \"C\" were conducting, \"B\" would transfer a portion of its charge to \"C\". To maximise the transferred charge, \"C\" was placed in close proximity to a final metal plate \"D\", which was earthed.\n\nThe experimenter would move Plate \"B\" repeatedly back and forth, placing it near to \"A\" and earthed at one end of its motion, and then into contact with \"C\" at the other. With each cycle, charge was drawn from the Earth and added to \"C\". After a suitable number of cycles, the grounded plate \"D\" would be removed, and the electrostatic potential on \"C\" would rise to approximately the potential of \"A\" multiplied by the number of operations.\n\nCavallo termed his device a multiplier, though 'addition' was perhaps a more accurate description of its operation, as the charge on \"C\" was accumulated by successive additions.\n\nWilson's machine, described by its inventor in \"Nicholson's Journal\" in August 1804, was a development on this concept which simultaneously operated two Cavallo's multipliers by means of a pair of reciprocating levers. One side would accumulate the charge of the other, and since the two accumulating plates were connected together by means of a wire, Wilson's machine was a true multiplier, rather than an addition machine. The charge would thus accumulate more rapidly than Cavallo's multiplier and the machine could generate high voltages in a short period of time. It moreover was self-exciting, needing no initial charge to operate, as the small initial charge acquired from contact electrification was enough to start the accumulation process.\n\n"}
{"id": "11277734", "url": "https://en.wikipedia.org/wiki?curid=11277734", "title": "Dynamic aperture", "text": "Dynamic aperture\n\nIn acoustics, dynamic aperture is analogous to aperture in photography. The arrays in side-scan sonar can be programmed to transmit just a few elements at a time or all the elements at once. The more elements transmitting, the narrower the beam and the better the resolution.\n\nThe ratio of the imaging depth to the aperture size is known as the F-number. Dynamic aperture is keeping this number constant by growing the aperture with the imaging depth until the physical aperture cannot be increased. A modern medical ultrasound machine has a typical F-number of 0.5.\n\nSide Scan Sonar systems produce images by forming angular “beams”. Beam width is determined by length of the sonar array, narrower beams resolve finer detail. Longer arrays with narrower beams provide finer spatial resolution. \n"}
{"id": "18004239", "url": "https://en.wikipedia.org/wiki?curid=18004239", "title": "European Observation Network for Territorial Development and Cohesion", "text": "European Observation Network for Territorial Development and Cohesion\n\nThe European Observation Network for Territorial Development and Cohesion, briefly ESPON, is a European funded programme under the objective of \"European Territorial Cooperation\" of the Cohesion Policy of the European Union. It is co-funded by the European Regional Development Fund - Interreg. \n\nThe mission of the programme is to support policy development in relation to the aim of territorial cohesion and a harmonious development of the European territory. Firstly it provides comparable information, evidence, analyses and scenarios on territorial dynamics and secondly it reveals territorial capital and potentials for the development of regions and larger territories thus contributing to European competitiveness, territorial cooperation and a sustainable and balanced development.\n\nThe current ESPON 2020 Programme is carried through by 28 European Union Member States as well as Iceland, Liechtenstein, Norway and Switzerland and the European Commission.\n\n\n"}
{"id": "10515", "url": "https://en.wikipedia.org/wiki?curid=10515", "title": "Extrasensory perception", "text": "Extrasensory perception\n\nExtrasensory perception or ESP, also called sixth sense or second sight, includes claimed reception of information not gained through the recognized physical senses, but sensed with the mind. The term was adopted by Duke University psychologist J. B. Rhine to denote psychic abilities such as intuition, telepathy, psychometry, clairaudience, and clairvoyance, and their trans-temporal operation as precognition or retrocognition.\n\nParapsychology is the study of paranormal psychic phenomena, including ESP. Parapsychology has been criticized for continuing investigation despite being unable to provide convincing evidence for the existence of any psychic phenomena after more than a century of research. The scientific community rejects ESP due to the absence of an evidence base, the lack of a theory which would explain ESP and the lack of positive experimental results; it considers ESP to be pseudoscience.\n\nIn the 1930s, at Duke University in North Carolina, J. B. Rhine and his wife Louisa E. Rhine conducted investigation into extrasensory perception. While Louisa Rhine concentrated on collecting accounts of spontaneous cases, J. B. Rhine worked largely in the laboratory, carefully defining terms such as ESP and \"psi\" and designing experiments to test them. A simple set of cards was developed, originally called Zener cards – now called ESP cards. They bear the symbols circle, square, wavy lines, cross, and star; there are five cards of each in a pack of 25.\n\nIn a telepathy experiment, the \"sender\" looks at a series of cards while the \"receiver\" guesses the symbols. To try to observe clairvoyance, the pack of cards is hidden from everyone while the receiver guesses. To try to observe precognition, the order of the cards is determined after the guesses are made. Later he used dice to test for psychokinesis.\n\nThe parapsychology experiments at Duke evoked criticism from academics and others who challenged the concepts and evidence of ESP. A number of psychological departments attempted to repeat Rhine's experiments with failure. W. S. Cox (1936) from Princeton University with 132 subjects produced 25,064 trials in a playing card ESP experiment. Cox concluded \"There is no evidence of extrasensory perception either in the 'average man' or of the group investigated or in any particular individual of that group. The discrepancy between these results and those obtained by Rhine is due either to uncontrollable factors in experimental procedure or to the difference in the subjects.\" Four other psychological departments failed to replicate Rhine's results.\n\nIn 1938, the psychologist Joseph Jastrow wrote that much of the evidence for extrasensory perception collected by Rhine and other parapsychologists was anecdotal, biased, dubious and the result of \"faulty observation and familiar human frailties\". Rhine's experiments were discredited due to the discovery that sensory leakage or cheating could account for all his results such as the subject being able to read the symbols from the back of the cards and being able to see and hear the experimenter to note subtle clues.\n\nIn the 1960s parapsychologists became increasingly interested in the cognitive components of ESP, the subjective experience involved in making ESP responses, and the role of ESP in psychological life. This called for experimental procedures that were not limited to Rhine's favored forced-choice methodology. Such procedures have included dream telepathy experiments, and the ganzfeld experiments (a mild sensory deprivation procedure).\n\nThe scientific consensus does not view extrasensory perception as a real phenomenon. Skeptics have pointed out that there is no viable theory to explain the mechanism behind ESP, and that there are historical cases in which flaws have been discovered in the experimental design of parapsychological studies.\n\nThere are many criticisms pertaining to experiments involving extrasensory perception, particularly surrounding methodological flaws. These flaws are not unique to a single experimental design, and are effective in discrediting much of the positive research surrounding ESP. Many of the flaws seen in the Zener cards experiment are present in the Ganzfeld experiment as well. First is the stacking effect, an error that occurs in ESP research. Trial-by-trial feedback given in studies using a “closed” ESP target sequence (e.g., a deck of cards) violates the condition of independence used for most standard statistical tests. Multiple responses for a single target cannot be evaluated using statistical tests that assume independence of responses. This increases likelihood of card counting and in turn, increases the chances for the subject to guess correctly without using ESP. Another methodological flaw involves cues through sensory leakage. For example, when the subject receives a visual cue. This could be the reflection of a Zener card in the holder’s glasses. In this case, the subject is able to guess the card correctly because they can see it in the reflection, not because of ESP. Finally, poor randomization of target stimuli could be happening. Poor shuffling methods can make the orders of the cards easier to predict, or the cards could’ve been marked and manipulated, again, making it easier to predict which cards come next.\nThe results of a meta-analysis found that when these errors were corrected and accounted for, there was still no significant effect of ESP. Many of the studies only appeared to have significant occurrence of ESP, when in fact, this result was due to the many methodological errors in the research.\n\n\n\n"}
{"id": "51207248", "url": "https://en.wikipedia.org/wiki?curid=51207248", "title": "Farid Matuk", "text": "Farid Matuk\n\nFarid Matuk is an American poet and educator, born to a Peruvian father and Syrian mother in Peru. He writes in both English and Spanish, and his Spanish translations have appeared in \"Kadar Koli,\" \"Translation Review\", \"Mandorla\", and \"Bombay Gin\". His poems have appeared in \"Denver Quarterly\", \"Flag + Void\", \"Iowa Review\", and \"Poetry\" and abroad in \"White Wall Review\" (Canada), \"Critical Quarterly\" (UK), and \"Poem: International English Language Quarterly\" (UK). He is currently Associate Professor of English and Creative Writing at the University of Arizona. His book \"This Isa Nice Neighborhood (Letter Machine, 2010)\" was the recipient of an Honorable Mention in the 2011 Arab American Book Awards. and was included in The Poetry Society of America's New American Poets series. \"My Daughter La Chola (Ahsata, 2013)\" received an Honorable Mention in the 2014 Arab American Book Awards. My Daughter La Chola was also named among the best books of 2013 by \"The Volta\" and by The Poetry Foundation while selections from its pages have been anthologized in \"The Best American Experimental Poetry, 2014\", \"The &Now Awards: The Best Innovative Writing Vol. 3\", and in \"Angels of the Americlypse: An Anthology of New Latino@ Writing\". He serves as poetry editor for \"Fence\" and on the editorial board for the Creative Writing Studies book series at Bloomsbury. Matuk is the recipient of both the Ford Fellowship and Fulbright Fellowship. The University of Arizona Press published his second full-length collection, \"The Real Horse\", in 2018.\n"}
{"id": "29566848", "url": "https://en.wikipedia.org/wiki?curid=29566848", "title": "Foregut fermentation", "text": "Foregut fermentation\n\nForegut fermentation is a form of digestion that occurs in the foregut of some animals. It has evolved independently in several groups of mammals, and also in the hoatzin bird.\n\nForegut fermentation is employed by ruminants and pseudoruminants, some rodents and some marsupials. It has also evolved in colobine monkeys and in sloths.\n\n"}
{"id": "1234368", "url": "https://en.wikipedia.org/wiki?curid=1234368", "title": "Fractional quantum Hall effect", "text": "Fractional quantum Hall effect\n\nThe fractional quantum Hall effect (FQHE) is a physical phenomenon in which the Hall conductance of 2D electrons shows precisely quantised plateaus at fractional values of formula_1. It is a property of a collective state in which electrons bind magnetic flux lines to make new quasiparticles, and excitations have a fractional elementary charge and possibly also fractional statistics. The 1998 Nobel Prize in Physics was awarded to Robert Laughlin, Horst Störmer, and Daniel Tsui \"for their discovery of a new form of quantum fluid with fractionally charged excitations\" However, Laughlin's explanation was a phenomenological guess and only applies to fillings formula_2 where formula_3 is an odd integer. The microscopic origin of the FQHE is a major research topic in condensed matter physics.\n\nThe fractional quantum Hall effect (FQHE) is a collective behaviour in a two-dimensional system of electrons. In particular magnetic fields, the electron gas condenses into a remarkable liquid state, which is very delicate, requiring high quality material with a low carrier concentration, and extremely low temperatures. As in the integer quantum Hall effect, the Hall resistance undergoes certain quantum Hall transitions to form a series of plateaus. Each particular value of the magnetic field corresponds to a filling factor (the ratio of electrons to magnetic flux quanta)\n\nwhere p and q are integers with no common factors. Here \"q\" turns out to be an odd number with the exception of two filling factors 5/2 and 7/2. The principal series of such fractions are\n\nand\n\nThere were several major steps in the theory of the FQHE.\n\n\nThe FQHE was experimentally discovered in 1982 by Daniel Tsui and Horst Störmer, in experiments performed on gallium arsenide heterostructures developed by Arthur Gossard. Tsui, Störmer, and Laughlin were awarded the 1998 Nobel Prize for their work.\n\nFractionally charged quasiparticles are neither bosons nor fermions and exhibit anyonic statistics. The fractional quantum Hall effect continues to be influential in theories about topological order. Certain fractional quantum Hall phases appear to have the right properties for building a topological quantum computer.\n\nExperiments have reported results that specifically support the understanding that there are fractionally-charged quasiparticles in an electron gas under FQHE conditions.\n\nIn 1995, the fractional charge of Laughlin quasiparticles was measured directly in a quantum antidot electrometer at Stony Brook University, New York. In 1997, two groups of physicists at the Weizmann Institute of Science in Rehovot, Israel, and at the Commissariat à l'énergie atomique laboratory near Paris, detected such quasiparticles carrying an electric current, through measuring quantum shot noise.\nBoth of these experiments have been confirmed with certainty.\n\nA more recent experiment, which measures the quasiparticle charge extremely directly, appears beyond reproach.\n\nThe FQH effect shows the limits of Landau's symmetry breaking theory. Previously it was long believed that the symmetry breaking theory could explain all the important concepts and essential properties of all forms of matter. According to this view the only thing to be done is to apply the symmetry breaking theory to all different kinds of phases and phase transitions.\nFrom this perspective, we can understand the importance of the FQHE discovered by\nTsui, Stormer, and Gossard.\n\nDifferent FQH states all have the same symmetry\nand cannot be described by symmetry breaking theory.\nThus FQH states represent new states of matter that contain a\ncompletely new kind of order—topological order.\nFor example, properties once deemed isotropic for all materials may be anisotropic in 2D planes. The existence of FQH liquids indicates that there is a whole\nnew world beyond the paradigm of symmetry breaking, waiting to be explored. \nThe FQH effect opened up a new chapter in condensed matter physics.\nThe new type of orders represented by FQH states greatly enrich our\nunderstanding of quantum phases and quantum phase transitions.\nThe associated fractional charge, fractional statistics, non-Abelian statistics,\nchiral edge states, etc. demonstrate the power and the fascination of emergence in many-body systems.\n\n\n"}
{"id": "9351532", "url": "https://en.wikipedia.org/wiki?curid=9351532", "title": "Froude–Krylov force", "text": "Froude–Krylov force\n\nIn fluid dynamics, the Froude–Krylov force—sometimes also called the Froude–Kriloff force—is a hydrodynamical force named after William Froude and Alexei Krylov. The Froude–Krylov force is the force introduced by the unsteady pressure field generated by \"undisturbed\" waves. The Froude–Krylov force does, together with the diffraction force, make up the total non-viscous forces acting on a floating body in regular waves. The diffraction force is due to the floating body disturbing the waves.\n\nThe Froude–Krylov force can be calculated from:\n\nwhere\n\nIn the simplest case the formula may be expressed as the product of the wetted surface area (A) of the floating body, and the dynamic pressure acting from the waves on the body:\n\nThe dynamic pressure, formula_7, close to the surface, is given by:\n\nwhere\n\n"}
{"id": "22712749", "url": "https://en.wikipedia.org/wiki?curid=22712749", "title": "Gaseous detection device", "text": "Gaseous detection device\n\nThe gaseous detection device-GDD is a method and apparatus for the detection of signals in the gaseous environment of an environmental scanning electron microscope (ESEM) and all scanned beam type of instruments that allow a minimum gas pressure for the detector to operate.\n\nIn the course of development of the ESEM, the detectors previously employed in the vacuum of a scanning electron microscope (SEM) had to be adapted for operation in gaseous conditions. The backscattered electron (BSE) detector was adapted by an appropriate geometry in accordance with the requirements for optimum electron beam transmission, BSE distribution and light guide transmission. However, the corresponding secondary electron (SE) detector (Everhart-Thornley detector) could not be adapted, because the high potential required would cause a catastrophic breakdown even with moderate increase of pressure, such as low vacuum. Danilatos (1983) overcame this problem by using the environmental gas itself as the detector, by virtue of the ionizing action of various signals. With appropriate control of electrode configuration and bias, detection of SE was achieved. A comprehensive survey dealing with the theory and operation of GDD has been published, from which the majority of the material presented below has been used.\n\nThe GDD is in principle an adaptation of techniques for particle detection used in nuclear physics and astronomy. The adaptation involves the parameters required for the formation of images in the conditions of an electron microscope and in the presence of gas inside the specimen chamber. The signals emanating from the beam specimen-interaction, in turn, interact with the surrounding gas in the form of gaseous ionization and excitation. The type, intensity and distribution of signal-gas interactions vary. It is fortunate that generally the time-constant of these interactions is compatible with the time-constant required for the formation of images in the ESEM. The establishment of this compatibility constitutes the basis of the invention of GDD and the leap from particle physics to electron microscopy. The dominant signal-gas interactions are those by the BSE and SE, as they are outlined below.\n\nIn its simplest form, the GDD involves one or more electrodes biased with a generally low voltage (e.g. up to 20 V), which is sufficient to collect the ionization current created by whatever sources. This is much the same as an ionization chamber in particle physics. The size and location of these electrodes determine the detection volume in the gas and hence the type of signal detected. The energetic BSE traverse a long distance, whereas the SE travel a much shorter lateral distance mainly by way of diffusion in the gas. Correspondingly, an electrode placed further away from the beam axis will have a predominantly BSE component in comparison to the predominant SE component collected by an electrode placed close to the axis. The precise proportion of signal mix and intensity depends on the additional parameters of gas nature and pressure in conjunction with electrode configurations and bias, bearing in mind that there is no abrupt physical distinction between SE and BSE, apart from the conventional definition of the 50 eV boundary between them.\n\nIn another form, the GDD involves one or more electrodes as above but biased with a generally high voltage (e.g. 20–500 V). The processes involved are the same as in the low voltage case with the addition of an amplification of signal along the principle of a proportional amplifier as used in particle physics. That is, all slow electrons in the gas emanating either from the ionizing BSE or directly from the specimen (i.e. the SE) are multiplied in an avalanche form. The energy imparted on the traveling slow electrons by the external electrode field is sufficient to ionize the gas molecules through successive (cascade) collisions. The discharge is controlled in proportion by the applied electrode bias below the breakdown point. This form of detection is referred as ionization-GDD.\n\nParallel to the ionization, there is also excitation of the gas in both cases above. The gaseous photons are produced both by BSE and SE both directly and by cascade avalanche with the ionization electrons. These photons are detected by appropriate means, like photo-multipliers. By positioning Light tubes strategically, using filters and other light optics means, the SE can again be separated from the BSE and corresponding images formed. This form of detection is referred as scintillation-GDD.\n\nThe principles outlined above are best described by considering plane electrodes biased to form a uniform electric field, such as shown in the accompanying . The electron beam striking the specimen at the cathode effectively creates a point source of SE and BSE. The distribution of slow electrons emitted from a point source inside a gas acted upon by a uniform field is given from the equations (low field):\n\nformula_1 with formula_2\n\nwhere R is the fraction of SE that arrives at the anode inside radius r, V the potential difference between the electrodes placed at distance d, k is Boltzmann’s constant, T the absolute gas temperature, e the electron charge and ε is the ratio of the thermal (agitation and kinetic) energy of the electrons divided by the thermal energy of the host gas; I is the corresponding current collected by the anode inside r, δ is the SE yield coefficient and I the incident electron beam current. This provides the spatial distribution of the initial electrons SE as they are acted upon by the uniform electric field that moves them from the cathode to the anode, while the electrons also diffuse away due to thermal collisions with the gas molecules. Plots are provided in the accompanying , for a set of operating conditions of pressure p and distance d. We note that a 100% collection efficiency is fast approached within a small radius even at moderate field strength. At high bias, a nearly complete collection is achieved within a very small radius, a fact that has favorable design implications.\n\nThe above radial distribution is valid also in the presence of formation of electron avalanches at high electric field, but it must be multiplied by an appropriate gain factor. In its simplest form for parallel electrodes, the gain factor is the exponential in the current equation:\n\nformula_3\n\nwhere α is the first Townsend coefficient. This gives the total signal amplification due to both electrons and ions. The spatial charge distribution and gain factor varies with electrode configuration and geometry and by additional discharge processes described in the referenced theory of the GDD.\n\nThe BSE usually have energies in the kV range so that the much lower electrode bias has only a secondary effect on their trajectory. For the same reason, the finite number of collisions with the gas also results in a second order deflection from their trajectory they would have in vacuum. Therefore, their distribution is practically the same as has been worked out by SEM workers, the variation of which depends on the specimen surface properties (geometry and material composition). For a polished specimen surface the BSE distribution assumes a nearly cosine function but for a rough surface we may take it to be spherical (i.e. uniform in all directions). For brevity, the equations of the second case only are given below. In vacuum, the current distribution from BSE on the electrode is given by\n\nformula_4\n\nwhere η is the BSE yield coefficient.\n\nIn the presence of gas at low electric field the corresponding equations become:\n\nformula_5\n\nwhere S is the ionization coefficient of the gas and p its pressure\n\nFinally, for a high electric field we get\n\nformula_6\n\nFor practical purposes, the BSE predominantly fall outside the volume acted upon by predominantly the SE, while there is an intermediate volume of comparable fraction of the two signals. The interplay of the various parameters involved has been studied in the main, but it also constitutes a new field for further research and development, especially as we move outside the plane electrode geometry.\n\nPrior to practical implementations, it is helpful to consider a more esoteric aspect (principle), namely, the fundamental physical process taking place in the GDD. The signal in the external circuit is a displacement current i created by induction of charge on the electrodes by a moving charge e with velocity υ in the space between them:\n\nformula_7\n\nAt the point in time when the charge arrives at the electrode, there is no current flowing in the circuit since υ=0, only when the charge is in motion between the electrodes do we have a signal current. This is important in the case, for example, when a new electron-ion pair is generated at any point in the space between anode-cathode, say at x distance from the anode. Then, only a fraction ex/d of charge is induced by the electron during its transit to the anode, whilst the remainder fraction of e(d–x)/d charge is induced by the ion during its transit to the cathode. Addition of those two fractions gives a charge equal to the charge of one electron. Thus by counting the electrons arriving at the anode or the ions at the cathode we derive the same figure in current measurement. However, since the electrons have a drift velocity about three orders of magnitude greater (in nanosecond range) than the ions, the induced signal may be separated in two components of different significance when the ion transit time may become greater than the pixel time on the scanned image. The GDD has thus two inherent time-constants, a very short one due to the electrons and a longer one due to the ions. When the ion transit time is greater than the pixel dwell time, the useful signal intensity decreases together with an increase of signal background noise or smearing of image edges due to the ions lagging behind. As a consequence, the above derivations, which include the total electron and ion contributions must be modified accordingly with new equations for the case of fast scanning rates. The electrode geometry can be altered with a view to decrease the ion transit time as can be done with a needle or cylindrical geometry.\n\nThis fundamental approach helps also understand the so-called “specimen absorbed current” mode of detection in the vacuum SEM, which is limited only to conductive specimens. Image formation of non-conductive specimens now possible in the ESEM, can be understood in terms of an induced displacement current in the external circuit via a capacitor-like action with the specimen being the dielectric between its surface and the underlying electrode. Therefore, the (misnomer) \"specimen absorbed current\" per se plays no part in any useful image formation except to dissipate the charge (in conductors), without which insulators cannot be generally imaged in vacuum (except in the rare case when the incident beam current equals the total emitted current).\n\nBy use of a derivation for the Townsend coefficient given by von Engel, the gain factor G, in the case of SE with total current collection I (i.e. for R=1), is found by:\n\nformula_8\n\nwhere A and B are tabulated constants for various gases. In the diagram supplied, we plot the for nitrogen with A=9.0 and B=256.5 valid in the range 75–450 V/(Pa·m) for the ratio E/p. We should note that in ESEM work the product pd<3 Pa·m, since at higher values no useful beam is transmitted through the gas layer to the specimen surface. The gray-shaded area shows the region of GDD operation provided also that the γ processes are very low and do not trigger a breakdown of the proportional amplification. This area contains the maxima of the gain curves, which further re-enforces the successful application of this technology to ESEM. The curves outside the shaded area can be used with beam energy greater than 30 kV, and in future development of environmental or atmospheric transmission scanning electron microscopes employing very high beam energy.\n\nThe diagram showing the constitutes a versatile implementation that includes not only the SE mode but also the BSE and a combination of these. Even if only the SE signal is desirable to use alone, at least one additional concentric electrode is recommended to employ in order to help in the separation from interference of BSE and also from other noise sources such as the skirt electrons scattered out of the primary beam by the gas. This addition may act as a “guard” electrode, and by varying its bias independently from the SE electrode, the image contrast can be controlled purposefully. Alternative control electrodes are used such as a mesh between anode and cathode. A multipurpose array of electrodes below and above the specimen and above the pressure limiting aperture of the ESEM has also been described elsewhere.\n\nThe development of this detector has required devoted electronics circuitry, especially when the signal is picked up by the anode at high bias, because the floating current amplified must be coupled at full bandwidth to the ground amplifier and video display circuits (developed by ElectroScan). An alternative is to bias the cathode with a negative potential and pickup the signal from the anode at floating ground without the need for coupling between amplifier stages. However, this would require extra precaution to protect users from exposure to a high potential at the specimen stage.\n\nA further alternative that has been implemented at the laboratory stage is by the application of a high bias at the anode but by pickup of the signals from the cathode at floating ground, as shown in the . Concentric electrodes (E2, E3, E4) are made on a copper-coated fiberglass printed circuit board (PCB) and a copper wire (E1) is added at the center of the disk. The anode is made again from the same PCB with a conical hole (400 micrometres) to act as a pressure limiting aperture in the ESEM. The exposed fiberglass material inside the aperture cone together with its surface above are coated with silver paint in continuity with the copper material of the anode electrode (E0), which is held at high potential. The cathode electrodes are independently connected to ground amplifiers, which, in fact, can be biased with low voltage directly from the amplifier power supplies in the range of ±15 volts without any further coupling required. On account of the induction mechanism operating behind the GDD, this configuration is equivalent to the previous diagram, except for the inverted signal that is electronically restored. While electrode E0 is held at 250 V, meaningful imaging is done as shown by a with composition of signals from various electrodes at two pressures of supplied air. All images show part of the central copper wire (E1), exposed fiber-glass (FG, middle), and copper (part of E2) with some silver paint used to attach the wire. The close resemblance of (a) with (b) at low pressure and (c) with (d) at high pressure is a manifestation of the principle of equivalence by induction. The purest SE image is (e) and the purest BSE is (h). Image (f) has prevailing SE characteristics, whilst (g) has a comparable contribution of both SE and BSE. Images (a) and (b) are dominated by SE with some BSE contribution, whilst (c) and (d) have comparable contribution by both SE and BSE.\n\nThe very bright areas on the FG material result from genuine high specimen signal yield and not from erratic charging or other artifacts familiar with plastics in vacuum SEM. High yield of edges, oblique incidence, etc. can for the first time be studied from the true surfaces without obstruction in ESEM. Mild charging, if present, may produce stable contrast characteristic of material properties and can be used as a means for studies of the physics of the surfaces. The images presented in this series are reproductions from photographic paper with limited bandwidth, on which attempting to bring up detail in dark areas results in saturating the bright areas and vice versa, whilst a lot more information is usually contained on the negative film. Electronic manipulation of the signal together with modern computer graphics can overcome some old imaging limitations.\n\nAn example of the GDD operating at low voltage is shown with of view of a polished mineral containing aluminum, iron, silicon and some unknown surface impurities. The anode electrode is a single thin wire placed on the side and below the specimen surface, several mm away from it. Image (a) shows predominantly SE contrast at low pressure, whilst (b) shows BSE material contrast at higher pressure. Image (c) shows cathodoluminescence (CL) from the specimen surface by use of water vapor (which does not scintillate), whilst (d) shows additional photon signal by changing the gas to air which scintillates by signal electrons originating from the specimen. The latter appears to be a mixture of CL with SE, but it may also contain additional information from the surface contaminant charging to a varying degree with gas pressure.\n\nThe GDD at high voltage has clear advantages over the low voltage mode, but the latter may be used easily with special applications such as at very high pressures where the BSE produce a high ionization gain from their own high energy, or in cases when the electric field requires shaping to purposeful ends. In general, the detector should be designed to operate at both high and low bias levels including variable negative (electron retarding) bias with important contrast generation.\n\nFurther improvements have been envisaged, such as the use of special electrode materials, gas composition and shaping the trajectory of detection electrons by special electric and magnetic fields (page 91).\n\nThe first commercial implementation of the GDD was carried out by ElectroScan Corporation employing the acronym ESD for “environmental secondary detector”, which was followed by an improved version termed “gaseous secondary electron detector” (GSED). The use of the magnetic field of the objective lens of the microscope has been incorporated in another commercial patent. LEO company (now Carl Zeiss SMT) has used the scintillation mode and the ionization (needle) mode of the GDD on its environmental SEMs at low and also extended pressure range.\n\n"}
{"id": "42220998", "url": "https://en.wikipedia.org/wiki?curid=42220998", "title": "Gene Ontology Term Enrichment", "text": "Gene Ontology Term Enrichment\n\nGene Ontology (GO) term enrichment is a technique for interpreting sets of genes making use of the Gene Ontology system of classification, in which genes are assigned to a set of predefined bins depending on their functional characteristics. For example, the gene FasR is categorized as being a receptor, involved in apoptosis and located on the plasma membrane.\n\nResearchers performing high-throughput experiments that yield sets of genes (for example, genes that are differentially expressed under different conditions) often want to retrieve a functional profile of that gene set, in order to better understand the underlying biological processes. This can be done by comparing the input gene set each of the bins (terms) in the GO – a statistical test can be performed for each bin to see if it is enriched for the input genes. FunRich can also be used for Gene Ontology enrichment analysis.\n\nThe output of the analysis is typically a ranked list of GO terms, each associated with a p-value.\n\nThe Gene Ontology (GO) provides a system for hierarchically classifying genes or gene products to terms organized in a graph structure called an ontology. The terms are grouped into three categories: molecular function (describing the molecular activity of a gene), biological process (describing the larger cellular or physiological role carried out by the gene, coordinated with other genes) and cellular component (describing the location in the cell where the gene product executes its function). Each gene can be described (annotated) with multiple terms. The GO is actively used to classify genes from humans, model organisms and a variety of other species.\n\nUsing the GO it is possible to retrieve the set of terms used to describe any gene, or conversely, given a term, return the set of genes annotated to that term. For the latter query, the hierarchical system of the GO is employed to give complete results. For example, a query for the GO term for nucleus should return genes annotated to the term \"nuclear membrane\".\n\nCertain types of high-throughput experiments (e.g. RNA seq) return sets of genes that are over or under expressed. The GO can be used to functionally profile this set of genes, to determine which GO terms appear more frequently than would be expected by chance when examining the set of terms annotated to the input genes. For example, an experiment may compare gene expression in healthy cells versus cancerous cells. Functional profiling can be used to elucidate the underlying cellular mechanisms associated with the cancerous condition. This is also called term enrichment or term overrepresentation, as we are testing whether a GO term is statistically enriched for the given set of genes.\n\nThere are a variety of methods for performing a term enrichment using GO. Methods may vary according to the type of statistical test applied, the most common being a Fisher's exact test / hypergeometric test. Some methods make use of Bayesian statistics. There is also variability in the type of correction applied for Multiple comparisons, the most common being Bonferroni correction.\n\nMethods also vary in their input – some take unranked gene sets, others ranked gene sets, with more sophisticated methods allowing each gene to be associated with a magnitude (e.g. expression level), avoiding arbitrary cutoffs.\n\n"}
{"id": "28883944", "url": "https://en.wikipedia.org/wiki?curid=28883944", "title": "Han Nijssen", "text": "Han Nijssen\n\nHan Nijssen (1935– 2013) was a Dutch ichthyologist. \n\nNijssen was born in Amsterdam and obtained his PhD at the University of Amsterdam in May 1970 with the dissertation \"Revision of the Surinam catfishes of the genus Corydoras\". Later he was a curator at Zoölogisch Museum in Amsterdam.\nNijssen worked extensively with fish from South America, and was the author of several species, e.g. \"Corydoras weitzmani\" and \"Corydoras xinguensis\".\nCollaborating with Isaäc Isbrücker he described, among others, the group \"Hypancistrus\" and the species \"Hypancistrus zebra\" and \"Corydoras panda\".\nHe also collaborated with Sven O. Kullander.\n\nThe species \"Corydoras nijsseni\" and \"Apistogramma nijsseni\" are named after him.\n\n"}
{"id": "14825318", "url": "https://en.wikipedia.org/wiki?curid=14825318", "title": "Henri Gadeau de Kerville", "text": "Henri Gadeau de Kerville\n\nHenri Gadeau de Kerville (17 December 1858 in Rouen – 26 July 1940 in Bagnères-de-Luchon) was a French zoologist, entomologist, botanist and archeologist best known for his photographs of these subjects and especially for his work \"Les Insectes phosphorescents: notes complémentaires et bibliographie générale (anatomie physiologie et biologie): avec quatre planches chromolithographiées\", Rouen, L. Deshays, 1881.\n\nHe was educated at the Lycée Pierre Corneille in Rouen. He was a member of the \"Société des sciences naturelles et amis du Museum de Rouen\" (1878), the \"Société botanique de France\" (1882) and the \"Société préhistorique française\" (1911). In 1910 he founded a laboratory for experimental speleobiology in Saint-Paër. The \"Société zoologique de France\" and \"Société entomologique de France\" each offers a \"Prix Gadeau de Kerville\" for achievements in their respective fields.\n\nHis scientific collections and photographs brought back from his expeditions are kept at museums in Paris, London, Elbeuf and Rouen.\n\n"}
{"id": "18537302", "url": "https://en.wikipedia.org/wiki?curid=18537302", "title": "Henrik Nikolai Krøyer", "text": "Henrik Nikolai Krøyer\n\nHenrik Nikolai Krøyer (22 March 1799 – 14 November 1870) was a Danish zoologist. \n\nBorn in Copenhagen, he was a brother of the composer Hans Ernst Krøyer. He started studying medicine at the University of Copenhagen in 1817, which he later changed to history and philology. While a student, he was a supporter of the Philhellenic movement, and he participated as a volunteer in the Greek War of Independence along with several fellow students. \n\nUpon his return to Denmark, Krøyer gained an interest in zoology. In 1827, he took the position as assistant teacher in Stavanger, where he met, and later married, Bertha Cecilie Gjesdal. Bertha's sister, Ellen Cecilie Gjesdal, was deemed unfit to bring up her child, so Henrik and Bertha adopted the boy, who took on the name Peder Severin Krøyer, and later became a well-known painter.\n\nKrøyer returned to Copenhagen in 1830 where he was employed as a teacher in natural history at the Military Academy. As the course lacked a textbook, Krøyer wrote and published \" (1833).\n\nDuring his career he often travelled along the coasts of Denmark where he studied marine life, especially fish and crustaceans, and this resulted in his main work \" (\"The Fish of Denmark\", 3 volumes, 1838-1853). During his life visited most of the coasts of Western Europe as well as Newfoundland. But his health eventually deteriorated and in 1869 he had to take his leave of his position of head of the Natural Museum of Copenhagen which he had held since 1847. He gained the title of professor in 1853.\n\n"}
{"id": "30747791", "url": "https://en.wikipedia.org/wiki?curid=30747791", "title": "High-refractive-index polymer", "text": "High-refractive-index polymer\n\nA high-refractive-index polymer (HRIP) is a polymer that has a refractive index greater than 1.50.\n\nSuch materials are required for anti-reflective coating and photonic devices such as light emitting diodes (LEDs) and image sensors. The refractive index of a polymer is based on several factors which include polarizability, chain flexibility, molecular geometry and the polymer backbone orientation.\n\nAs of 2004, the highest refractive index for a polymer was 1.76. Substituents with high molar fractions or high-n nanoparticles in a polymer matrix have been introduced to increase the refractive index in polymers.\n\nA typical polymer has a refractive index of 1.30–1.70, but a higher refractive index is often required for specific applications. The refractive index is related to the molar refractivity, structure and weight of the monomer. In general, high molar refractivity and low molar volumes increase the refractive index of the polymer.\n\nOptical dispersion is an important property of an HRIP. It is characterized by the Abbe number. A high refractive index material will generally have a small Abbe number, or a high optical dispersion. A low birefringence has been required along with a high refractive index for many applications. It can be achieved by using different functional groups in the initial monomer to make the HRIP. Aromatic monomers both increase refractive index and decrease the optical anisotropy and thus the birefringence.\n\nA high clarity (optical transparency) is also desired in a high refractive index polymer. The clarity is dependent on the refractive indexes of the polymer and of the initial monomer.\n\nWhen looking at thermal stability, the typical variables measured include glass transition, initial decomposition temperature, degradation temperature and the melting temperature range. The thermal stability can be measured by thermogravimetric analysis and differential scanning calorimetry. Polyesters are considered thermally stable with a degradation temperature of 410 °C. The decomposition temperature changes depending on the substituent that is attached to the monomer used in the polymerization of the high refractive index polymer. Thus, longer alkyl substituents results in lower thermal stability.\n\nMost applications favor polymers which are soluble in as many solvents as possible. Highly refractive polyesters and polyimides are soluble in common organic solvents such as dichloromethane, methanol, hexanes, acetone and toluene.\n\nThe synthesis route depends on the HRIP type. The Michael polyaddition is used for a polyimide because it can be carried out at room temperature and can used for step-growth polymerization. This synthesis was first succeeded with polyimidothiethers, resulting in optically transparent polymers with high refractive index. Polycondensation reactions are also common to make high refractive index polymers, such as polyesters and polyphosphonates.\n\nHigh refractive indices have been achieved either by introducing substituents with high molar refractions (intrinsic HRIPs) or by combining high-n nanoparticles with polymer matrixes (HRIP nanocomposites).\n\nSulfur-containing substituents including linear thioether and sulfone, cyclic thiophene, thiadiazole and thianthrene are the most commonly used groups for increasing refractive index of a polymer. Polymers with sulfur-rich thianthrene and tetrathiaanthracene moieties exhibit n values above 1.72, depending on the degree of molecular packing.\nHalogen elements, especially bromine and iodine, were the earliest components used for developing HRIPs. In 1992, Gaudiana \"et al.\" reported a series of polymethylacrylate compounds containing lateral brominated and iodinated carbazole rings. They had refractive indices of 1.67–1.77 depending on the components and numbers of the halogen substituents. However, recent applications of halogen elements in microelectronics have been severely limited by the WEEE directive and RoHS legislation adopted by the European Union to reduce potential pollution of the environment.\n\nPhosphorus-containing groups, such as phosphonates and phosphazenes, often exhibit high molar refractivity and optical transmittance in the visible light region. Polyphosphonates have high refractive indices due to the phosphorus moiety even if they have chemical structures analogous to polycarbonates. Shaver \"et al.\" reported a series of polyphosphonates with varying backbones, reaching the highest refractive index reported for polyphosphonates at 1.66. In addition, polyphosphonates exhibit good thermal stability and optical transparency; they are also suitable for casting into plastic lenses.\nOrganometallic components result in HRIPs with good film forming ability and relatively low optical dispersion. Polyferrocenylsilanes and polyferrocenes containing phosphorus spacers and phenyl side chains show unusually high n values (n=1.74 and n=1.72). They might be good candidates for all-polymer photonic devices because of their intermediate optical dispersion between organic polymers and inorganic glasses.\n\nHybrid techniques which combine an organic polymer matrix with highly refractive inorganic nanoparticles could result in high n values. The factors affecting the refractive index of a high-n nanocomposite include the characteristics of the polymer matrix, nanoparticles and\nthe hybrid technology between inorganic and organic components. The refractive index of a nanocomposite can be estimated as formula_1, where formula_2, formula_3 and formula_4 stand for the refractive indices of the nanocomposite, nanoparticle and organic matrix, respectively. formula_5 and formula_6 represent the volume fractions of the nanoparticles and organic matrix, respectively. The nanoparticle load is also important in designing HRIP nanocomposites for optical applications, because excessive concentrations increase the optical loss and decrease the processability of the nanocomposites. The choice of nanoparticles is often influenced by their size and surface characteristics. In order to increase optical transparency and reduce Rayleigh scattering of the nanocomposite, the diameter of the nanoparticle should be below 25 nm. Direct mixing of nanoparticles with the polymer matrix often results in the undesirable aggregation of nanoparticles – this is avoided by modifying their surface. The most commonly used nanoparticles for HRIPs include TiO (anatase, n=2.45; rutile, n=2.70), ZrO (n=2.10), amorphous silicon (n=4.23), PbS (n=4.20) and ZnS (n=2.36). Polyimides have high refractive indexes and thus are often used as the matrix for high-n nanoparticles. The resulting nanocomposites exhibit a tunable refractive index ranging from 1.57 to 1.99.\n\nA microlens array is a key component of optoelectronics, optical communications, CMOS image sensors and displays. Polymer-based microlenses are easier to make and are more flexible than conventional glass-based lenses. The resulting devices use less power, are smaller in size and are cheaper to produce.\n\nAnother application of HRIPs is in immersion lithography. It is a new technique for circuit manufacturing that uses both photoresists and high refractive index fluids. The photoresist needs to have an n value of greater than 1.90. It has been shown that non-aromatic, sulfur-containing HRIPs are the best materials for an optical photoresist system.\n\nLight-emitting diodes (LEDs) are a common solid-state light source. High-brightness LEDs (HBLEDs) are often limited by the relatively low light extraction efficiency due to the mismatch of the refractive indices between the LED material (GaN, n=2.5) and the organic encapsulant (epoxy or silicone, n=1.5). Higher light outputs can be achieved by using an HRIP as the encapsulant.\n"}
{"id": "391260", "url": "https://en.wikipedia.org/wiki?curid=391260", "title": "IBM Information Management System", "text": "IBM Information Management System\n\nIBM Information Management System (IMS) is a joint hierarchical database and information management system with extensive transaction processing capabilities.\n\nIBM designed the IMS with Rockwell and Caterpillar starting in 1966 for the Apollo program, where it was used to inventory the very large bill of materials (BOM) for the Saturn V moon rocket and Apollo space vehicle.\n\nThe first \"IMS READY\" message appeared on an IBM 2740 terminal in Downey, California, on 14 August 1968.\nIn the interim period, IMS has undergone many developments as IBM System/360 technology evolved into the current z/OS and IBM zEnterprise System technologies. For example, IMS now supports the Java programming language, JDBC, XML, and, since late 2005, Web services.\n\nVern Watts was IMS's chief architect for many years. Watts joined IBM in 1956 and worked at IBM's Silicon Valley development labs until his death on April 4, 2009. He had continuously worked on IMS since the 1960s.\nThe IMS Database component stores data using a hierarchical model, which is quite different from IBM's later released relational database, DB2. In IMS, the hierarchical model is implemented using blocks of data known as segments. Each segment can contain several pieces of data, which are called fields. For example, a customer database may have a root segment (or the segment at the top of the hierarchy) with fields such as phone, name, and age. Child segments may be added underneath another segment, for instance, one order segment under each customer segment representing each order a customer has placed with a company. Likewise, each order segment may have many children segments for each item on the order. Unlike other databases, you do not need to define all of the data in a segment to IMS. A segment may be defined with a size of 40 bytes but only define one field that is six bytes long as a key field that you can use to find the segment when performing queries. IMS will retrieve and save all 40 bytes as directed by a program but may not understand (or care) what the other bytes represent. In practice, often all data in a segment may map to a COBOL copybook. Besides DL/I query usage, a field may be defined in IMS so that the data can be hidden from certain applications for security reasons. The database component of IMS can be purchased standalone, without the transaction manager component, and used by systems such as CICS.\n\nThere are three basic forms of IMS hierarchical databases:\n\n\nDEDB performance comes from use of high performance (Media Manager) access method, asynchronous write after commit, and optimized code paths. Logging is minimized because no data is updated on disk until commit, so UNDO (before image) logging is not needed, nor is a backout function. Uncommitted changes can simply be discarded.\nStarting with IMS Version 11, DEDBs can use z/OS 64-bit storage for database buffers.\nDEDBs architecture includes a Unit of Work (UOW) concept which made an effective online reorganization utility simple to implement. This function is included in the base product.\n\n\nFast path DEDBs can only be built atop VSAM. DL/I databases can be built atop either VSAM or OSAM, with some restrictions depending on database organization. Although the maximum size of a z/OS VSAM dataset increased to 128 TB a few years ago, IMS still limits a VSAM dataset to 4 GB (and OSAM to 8 GB). This \"limitation\" simply means that IMS customers will use multiple datasets for large amounts of data. VSAM and OSAM are usually referred to as the access methods, and the IMS \"logical\" view of the database is referred to as the database \"organization\" (HDAM, HIDAM, HISAM, etc.) Internally the data are linked using 4-byte pointers or addresses. In the database datasets (DBDSs) the pointers are referred to as RBAs (relative byte addresses).\n\nCollectively the database-related IMS capabilities are often called IMS DB. IMS DB has grown and evolved over nearly four decades to support myriad business needs. IMS, with assistance from z/OS hardware - the Coupling Facility - supports N-way inter-IMS sharing of databases. Many large configurations involve multiple IMS systems managing common databases, a technique providing for scalable growth and system redundancy in the event of hardware or software failures.\n\nIMS is also a robust transaction manager (IMS TM, also known as IMS DC) — one of the \"big three\" classic transaction managers along with CICS and BEA (now Oracle) Tuxedo. A transaction manager interacts with an end user (connected through VTAM or TCP/IP, including 3270 and Web user interfaces) or another application, processes a business function (such as a banking account withdrawal), and maintains state throughout the process, making sure that the system records the business function correctly to a data store. Thus IMS TM is quite like a Web application, operating through a CGI program (for example), to provide an interface to query or update a database. IMS TM typically uses either IMS DB or DB2 as its backend database. When used alone with DB2 the IMS TM component can be purchased without the IMS DB component.\n\nIMS TM uses a messaging and queuing paradigm. An IMS control program receives a transaction entered from a terminal (or Web browser or other application) and then stores the transaction on a message queue (in memory or in a dataset). IMS then invokes its scheduler on the queued transaction to start the business application program in a message processing region. The message processing region retrieves the transaction from the IMS message queue and processes it, reading and updating IMS and/or DB2 databases, assuring proper recording of the transaction. Then, if required, IMS enqueues a response message back onto the IMS message queue. Once the output message is complete and available the IMS control program sends it back to the originating terminal. IMS TM can handle this whole process thousands (or even tens of thousands) of times per second. In 2013 IBM completed a benchmark on IMS Version 13 demonstrating the ability to process 100,000 transactions per second on a single IMS system.\n\nPrior to IMS, businesses and governments had to write their own transaction processing environments. IMS TM provides a straightforward, easy-to-use, reliable, standard environment for high performance transaction execution. In fact, much of the world's banking industry relies on IMS , including the U.S. Federal Reserve. For example, chances are that withdrawing money from an automated teller machine (ATM) will trigger an IMS transaction. Several Chinese banks have recently purchased IMS to support that country's burgeoning financial industry.\n\nToday IMS complements DB2, IBM's relational database system, introduced in 1982. In general, IMS performs faster than DB2 for the common tasks but may require more programming effort to design and maintain for non-primary duties. Relational databases have generally proven superior in cases where the requirements, especially reporting requirements, change frequently or require a variety of viewpoint \"angles\" outside the primary or original function.\n\nA relational \"data warehouse\" may be used to supplement an IMS database. For example, IMS may provide primary ATM transactions because it performs well for such a specific task. However, nightly copies of the IMS data may be copied to relational systems such that a variety of reports and processing tasks may be performed on the data. This allows each kind of database to focus best on its relative strength.\n\n\n"}
{"id": "2623949", "url": "https://en.wikipedia.org/wiki?curid=2623949", "title": "Iben Browning", "text": "Iben Browning\n\nIben Browning (January 9, 1918 – July 18, 1991) was an American business consultant, author, and \"self-proclaimed climatologist.\" He is most notable for having made various failed predictions of disasters involving climate, volcanoes, earthquakes, and government collapse.\n\nBrowning was born in Edna, Texas, grew up in Jackson County, Texas, and graduated from Southwest Texas State Teachers College in 1937, majoring in both math and physics. During World War II, he served in the U.S. Army Air Corps. Subsequently, he earned an M.A. at the University of Texas at Austin in 1947, and then his doctorate (Ph.D.) the following year at the same school. His doctorate was in zoology, with minors in genetics and bacteriology.\n\nBrowning worked in various scientific fields, including artificial intelligence and bio-engineering, and eventually became interested in long-term weather forecasting and climate changes. He believed that climate fluctuations are caused by changes in the amount of particulate matter in the atmosphere mostly from volcanic activity. He believed that volcanic activity can be triggered by land tidal forces caused by the Moon, Earth's elliptical orbit of the Sun, and the alignment of these three bodies. His climate predictions assumed that the dust thrown into the atmosphere by those eruptions reflects sunlight, which results in climatic cooling. Browning believed that climatic changes, especially cooling, are associated with increased troubles in human society, including famine, revolutions, and war.\n\nAfter founding \"The Browning Newsletter\" in 1974, Browning described his climatic theories and findings in \"Climate and the Affairs of Men\" (1975), which he co-authored with Nels Winkless III. At that time, he believed that Earth had been through a long warm period and was moving into a dangerous cooling phase. He also declared that he had not detected any effect of human activity on the climate.\n\nBrowning received notoriety for his erroneous prediction that a major earthquake would occur on the New Madrid Fault around December 2 and 3, 1990. This prediction had no scientific legitimacy, and was largely ignored by credentialed seismologists, who thought it would give the prediction undeserved attention if they had debunked it in public. In spite of this it was widely reported in the national media, which promoted fear, anxiety, and hysteria among residents of the Mississippi Valley. No earthquake occurred in that area on those dates. A study done by the USGS to understand the causes of the earthquake scare described Browning's methodology as pseudoscience.\n\nBrowning wrote four books, held 90 patents, and served as a climatologist and business consultant to Paine Webber in various scientific and engineering fields. He was married to the former Florence Pinto and had one daughter, Evelyn Browning-Garriss., who succeeded him as editor of 'The Browning Newsletter'. He lived his later years in Albuquerque, New Mexico and died at his home there on July 18, 1991 from a heart attack at the age of 73.\n\nNotes\n"}
{"id": "691486", "url": "https://en.wikipedia.org/wiki?curid=691486", "title": "Indian Council of Social Science Research", "text": "Indian Council of Social Science Research\n\nThe Indian Council of Social Science Research (ICSSR) is the national body overseeing research in the social sciences in India. It was established in New Delhi in 1969. It provides funding to scholars and to a network of twenty-seven research institutes, among them:\n\n\nA large proportion of its budget is spent on its own administration; in 1996–1997, this amounted to 23% of total expenditure. It has been described as an \"oversized, unimaginative and inefficient bureaucrac[y]\".\n\n"}
{"id": "15058321", "url": "https://en.wikipedia.org/wiki?curid=15058321", "title": "Institute for Nuclear Research", "text": "Institute for Nuclear Research\n\nInstitute for Nuclear Research of the Russian Academy of Sciences (INR RAS, ) is a Russian scientific research centre \"for further development of the experimental base and fundamental research activities in the field of atomic nucleus, elementary particle and cosmic ray physics and neutrino astrophysics\".\n\nIt was founded in 1970 by the Decree of the USSR Council of Ministers. Located in Moscow, Russia near the Moscow State University and in Troitsk.\n\nThe Institute is a founder of the Baksan Neutrino Observatory, the Baikal Deep Underwater Neutrino Telescope (Lake Baikal) and the former Artemovskaya Scientific Station (Soledar, Ukraine).\n\nAbout 1,300 specialists including 5 academicians and 2 corresponding members of the RAS, 42 doctors and 160 candidates of science work in the institute.\n\n\n\n"}
{"id": "53839760", "url": "https://en.wikipedia.org/wiki?curid=53839760", "title": "James Robert Beene", "text": "James Robert Beene\n\nJames Robert Beene from the Oak Ridge National Laboratory, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Division of Nuclear Physics in 1991, for \"his outstanding contributions and investigations in heavy-ion nuclear physics, particularly studies of the nuclear giant resonance structures via Coulomb excitations and their subsequent decay via photon and neutron emission with 4-TT detector systems.\"\n"}
{"id": "33623516", "url": "https://en.wikipedia.org/wiki?curid=33623516", "title": "Johan Albert Constantin Löfgren", "text": "Johan Albert Constantin Löfgren\n\nJohan Albert Constantin Löfgren (1854–1918), known as Albert Löfgren or Alberto Löfgren, was head of the botany department of the Rio de Janeiro Botanical Garden around 1905. The plant currently known as \"Schlumbergera opuntioides\" is one of those he first named.\n\nLöfgren was the first director of the Horto Florestal de São Paulo, from 1907 to 1909, now named the Albert Löfgren State Park.\n"}
{"id": "50668886", "url": "https://en.wikipedia.org/wiki?curid=50668886", "title": "Keren Elazari", "text": "Keren Elazari\n\nKeren Elazari (born 1980) is an Israeli-born cyber security analyst and senior researcher at the Tel Aviv University Interdisciplinary Cyber Research Center with an emphasis on hackers and technology and their social implications. Her research interests include issues of hacktivism, information and cyber punk.\n\nShe was born and raised in Tel Aviv.\n\nKeren is a senior researcher at the Tel Aviv University Cyber Security Research Center, \nAs well as adjunct faculty member at Singularity University in California.\nKeren's research topics have been concerning the future of Cyber Security, Cyberpunk, Global Hacktivism, Innovation and Creativity learnt from Hackers, Biohacking and more. Her research works and writings about cyber security have been featured by Scientific American, WIRED, CNN, DEFCON and NATO, among others.\n\nIn 2014 Keren Elazari became the first Israeli woman to give a TED talk. In her talk Elazari claimed that by exposing vulnerabilities, hackers and hacktivists push the Internet to become stronger and healthier, wielding their power to create a better world. Her speech was selected as one of TED's most powerful ideas in 2014.\n\nKeren is the co-author of the book \"Women in Tech: Take Your Career to the Next Level with Practical Advice and Inspiring Stories\", Sasquatch Books, 2016, ASIN B010ZZYJSI\nSee more at Women_In_Tech\n\n\n"}
{"id": "2512617", "url": "https://en.wikipedia.org/wiki?curid=2512617", "title": "List of Aleutian Island volcanoes", "text": "List of Aleutian Island volcanoes\n\n"}
{"id": "29655135", "url": "https://en.wikipedia.org/wiki?curid=29655135", "title": "List of Mississippi state symbols", "text": "List of Mississippi state symbols\n\nThe following is a list of state symbols of the U.S. state of Mississippi, as defined by state statutes.\n"}
{"id": "5088157", "url": "https://en.wikipedia.org/wiki?curid=5088157", "title": "List of SIP response codes", "text": "List of SIP response codes\n\nThe Session Initiation Protocol (SIP) is a signalling protocol used for controlling communication sessions such as Voice over IP telephone calls. SIP is based around request/response transactions, in a similar manner to the Hypertext Transfer Protocol (HTTP). Each transaction consists of a SIP request (which will be one of several request methods), and at least one response.\n\nSIP requests and responses may be generated by any SIP user agent; user agents are divided into clients (UACs), which initiate requests, and servers (UASes), which respond to them. A single user agent may act as both UAC and UAS for different transactions: for example, a SIP phone is a user agent that will be a UAC when making a call, and a UAS when receiving one. Additionally, some devices will act as both UAC and UAS for a single transaction; these are called Back-to-Back User Agents (B2BUAs).\n\nSIP responses specify a three-digit integer response code, which is one of a number of defined codes that detail the status of the request. These codes are grouped according to their first digit as \"provisional\", \"success\", \"redirection\", \"client error\", \"server error\" or \"global failure\" codes, corresponding to a first digit of 1–6; these are expressed as, for example, \"1xx\" for provisional responses with a code of 100–199. The SIP response codes are consistent with the HTTP response codes, although not all HTTP response codes are valid in SIP.\n\nSIP responses also specify a \"reason phrase\", and a default reason phrase is defined with each response code. These reason phrases can be varied, however, such as to provide additional information or to provide the text in a different language.\n\nThe SIP response codes and corresponding reason phrases were initially defined in RFC 3261. That RFC also defines a SIP Parameters Internet Assigned Numbers Authority (IANA) registry to allow other RFC to provide more response codes.\n\nThis list includes all the SIP response codes defined in IETF RFCs and registered in the SIP Parameters IANA registry . This list also includes SIP response codes defined in obsolete SIP RFCs (specifically, RFC 2543), which are therefore not registered with the IANA; these are explicitly noted as such.\n\n\n\n\n\n\n\n"}
{"id": "315474", "url": "https://en.wikipedia.org/wiki?curid=315474", "title": "List of alloys", "text": "List of alloys\n\nThis is a list of named alloys grouped alphabetically by base metal. Within these headings, the alloys are also grouped alphabetically. Some of the main alloying elements are optionally listed after the alloy names.\n\nAluminium also forms complex metallic alloys, like β–Al–Mg, ξ'–Al–Pd–Mn, and T–AlMn.\n\n\n\n\n\n\n\n\nMost iron alloys are steels, with carbon as a major alloying element.\nModern steels are made with varying combinations of alloy metals to fulfill many purposes. Carbon steel, composed simply of iron and carbon, accounts for 90% of steel production. Low alloy steel is alloyed with other elements, usually molybdenum, manganese, chromium, or nickel, in amounts of up to 10% by weight to improve the hardenability of thick sections. High strength low alloy steel has small additions (usually < 2% by weight) of other elements, typically 1.5% manganese, to provide additional strength for a modest price increase.\n\nRecent Corporate Average Fuel Economy (CAFE) regulations have given rise to a new variety of steel known as Advanced High Strength Steel (AHSS). This material is both strong and ductile so that vehicle structures can maintain their current safety levels while using less material. There are several commercially available grades of AHSS, such as dual-phase steel, which is heat treated to contain both a ferritic and martensitic microstructure to produce a formable, high strength steel. Transformation Induced Plasticity (TRIP) steel involves special alloying and heat treatments to stabilize amounts of austenite at room temperature in normally austenite-free low-alloy ferritic steels. By applying strain, the austenite undergoes a phase transition to martensite without the addition of heat. Twinning Induced Plasticity (TWIP) steel uses a specific type of strain to increase the effectiveness of work hardening on the alloy.\n\nCarbon Steels are often galvanized, through hot-dip or electroplating in zinc for protection against rust.\n\nStainless steels contain a minimum of 11% chromium, often combined with nickel, to resist corrosion. Some stainless steels, such as the ferritic stainless steels are magnetic, while others, such as the austenitic, are nonmagnetic. Corrosion-resistant steels are abbreviated as CRES.\n\nSome more modern steels include tool steels, which are alloyed with large amounts of tungsten and cobalt or other elements to maximize solution hardening. This also allows the use of precipitation hardening and improves the alloy's temperature resistance. Tool steel is generally used in axes, drills, and other devices that need a sharp, long-lasting cutting edge. Other special-purpose alloys include weathering steels such as Cor-ten, which weather by acquiring a stable, rusted surface, and so can be used un-painted. Maraging steel is alloyed with nickel and other elements, but unlike most steel contains little carbon (0.01%). This creates a very strong but still malleable steel.\n\nEglin steel uses a combination of over a dozen different elements in varying amounts to create a relatively low-cost steel for use in bunker buster weapons. Hadfield steel (after Sir Robert Hadfield) or manganese steel contains 12–14% manganese which when abraded strain-hardens to form an incredibly hard skin which resists wearing. Examples include tank tracks, bulldozer blade edges and cutting blades on the jaws of life.\n\n\n\n\n\n\n\n\n\nSmCo (cobalt); used for permanent magnets in guitar pickups, headphones, satellite transponders, etc.\n\n\n\n\n\n\n\n\n"}
{"id": "768566", "url": "https://en.wikipedia.org/wiki?curid=768566", "title": "List of important publications in physics", "text": "List of important publications in physics\n\nThis is a list of important publications in physics, organized by field.\n\nSome reasons why a particular publication might be regarded as important:\n\n\n\n\n\n\n\n\nAstrophysics employs physical principles \"to ascertain the nature of the heavenly bodies, rather than their positions or motions in space.\"\n\n\n\n\n\n\"Classical mechanics\" is the system of physics begun by Isaac Newton and his contemporaries. It is concerned with the motion of macroscopic objects at speeds well below the speed of light.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"Condensed matter physics\" deals with the physical properties of condensed phases of matter. These properties appear when atoms interact strongly and adhere to each other or are otherwise concentrated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"The primary sources section of the latter article in particular contains many additional (early) publications of importance in the field.\"\n\nMinkowski relativity papers:\n\n\n\n\n\n\n\n\n\n"}
{"id": "36822737", "url": "https://en.wikipedia.org/wiki?curid=36822737", "title": "List of planning journals", "text": "List of planning journals\n\nThis is a lists articles about academic peer-reviewed journals related to urban, regional, land-use, transportation and environmental planning and to urban studies, regional science. \n\n\n"}
{"id": "39726224", "url": "https://en.wikipedia.org/wiki?curid=39726224", "title": "List of reservoirs by surface area", "text": "List of reservoirs by surface area\n\nThis is a listing of the reservoirs (artificial lakes) in the world with a surface area exceeding . Reservoirs can be formed conventionally, by damming the outlet of a canyon or valley to form a lake; the largest of this type is Ghana's Lake Volta, with a water surface of . Reservoirs can also be formed by damming the outlets of natural lakes to regulate water levels, such as those at Uganda's Owen Falls Dam (Lake Victoria) and Russia's Irkutsk Dam (Lake Baikal). These are included and indicated with light blue cell background.\n\nLarge reservoir area does not necessarily coincide with large volume, as reservoirs with a large area tend to be shallow, such as at Suriname's Brokopondo Reservoir, with an average depth of just . In comparison, Canada's Kinbasket Lake, with an average depth of , has a volume 25 percent greater – but with a surface area of just , does not meet the cutoff for inclusion in this list.\n\n"}
{"id": "56209785", "url": "https://en.wikipedia.org/wiki?curid=56209785", "title": "List of taxa with candidatus status", "text": "List of taxa with candidatus status\n\nThis is a list of taxa with candidatus status.\n\n\n\n\n\nUnless otherwise noted (♦), these entries are from LPSN.\n\n\n\n"}
{"id": "3391863", "url": "https://en.wikipedia.org/wiki?curid=3391863", "title": "Meridian circle", "text": "Meridian circle\n\nThe meridian circle is an instrument for timing of the passage of stars across the local meridian, an event known as a culmination, while at the same time measuring their angular distance from the nadir. These are special purpose telescopes mounted so as to allow pointing only in the meridian, the great circle through the north point of the horizon, the zenith, the south point of the horizon, and the nadir. Meridian telescopes rely on the rotation of the Earth to bring objects into their field of view and are mounted on a fixed, horizontal, east–west axis.\n\nThe similar transit instrument, transit circle, or transit telescope is likewise mounted on a horizontal axis, but the axis need not be fixed in the east–west direction. For instance, a surveyor's theodolite can function as a transit instrument if its telescope is capable of a full revolution about the horizontal axis. Meridian circles are often called by these names, although they are less specific.\n\nFor many years, transit timings were the most accurate method of measuring the positions of heavenly bodies, and meridian instruments were relied upon to perform this painstaking work. Before spectroscopy, photography, and the perfection of reflecting telescopes, the measuring of positions (and the deriving of orbits and astronomical constants) was the major work of observatories.\n\nFixing a telescope to move only in the meridian has advantages in the high-precision work for which these instruments are employed:\n\nThe state of the art of meridian instruments of the late 19th and early 20th century is described here, giving some idea of the precise methods of construction, operation and adjustment employed.\n\nThe earliest transit telescope was not placed in the middle of the axis, but nearer to one end, to prevent the axis from bending under the weight of the telescope. Later, it was usually placed in the centre of the axis, which consisted of one piece of brass or gun metal with turned cylindrical steel pivots at each end. Several instruments were made entirely of steel, which was much more rigid than brass. The pivots rested on V-shaped bearings, either set into massive stone or brick piers which supported the instrument, or attached to metal frameworks on the tops of the piers. The temperature of the bearings was monitored by thermometers.\nThe piers were usually separate from the foundation of the building, to prevent transmission of vibration from the building to the telescope. To relieve the pivots from the weight of the instrument, which would have distorted their shape, each end of the axis was supported by a hook with friction rollers, suspended from a lever supported by the pier, counterbalanced so as to leave only about 10 pounds force (45 N) on each bearing. In some cases, the counterweight pushed up on the bearing from below. The bearings were set nearly in a true east–west line, but fine adjustment was possible by horizontal and vertical screws. A spirit level was used to monitor for any inclination of the axis to the horizon. Eccentricity (an off-center condition) of the telescope's axis was accounted for, in some cases, by providing another telescope through the axis itself. By observing the motion of an artificial star through this axis telescope as the main telescope was rotated, the shape of the pivots, and any wobble of the axis, could be determined.\n\nNear each end of the axis, attached to the axis and turning with it, was a circle or wheel for measuring the angle of the telescope to the horizon. Generally of 3 feet to 3.5 ft diameter, it was divided to 2 or 5 arcminutes, on a slip of silver set into the face of the circle near the circumference. These graduations were read by microscopes, generally four for each circle, mounted to the piers or a framework surrounding the axis, at 90° intervals around the circles. By averaging the four readings the eccentricity (from inaccurate centering of the circles) and the errors of graduation were greatly reduced. Each microscope was furnished with a micrometer screw, which moved crosshairs, with which the distance of the circle graduations from the centre of the field of view could be measured. The drum of the screw was divided to measure single seconds of arc (0.1\" being estimated), while the number of revolutions were counted by a kind of comb in the field of view. The microscopes were placed at such a distance from the circle that one revolution of the screw corresponded to 1 arcminute (1') on the circle. The error was determined occasionally by measuring standard intervals of 2' or 5' on the circle. The periodic errors of the screw were accounted for. On some instruments, one of the circles was graduated and read more coarsely than the other, and was used only in finding the target stars.\n\nThe telescope consisted of two tubes screwed to the central cube of the axis. The tubes were usually conical and as stiff as possible to help prevent flexure. The connection to the axis was also as firm as possible, as flexure of the tube would affect declinations deduced from observations. The flexure in the horizontal position of the tube was determined by two collimators - telescopes placed horizontally in the meridian, north and south of the transit circle, with their objective lenses towards it. These were pointed at one another (through holes in the tube of the telescope, or by removing the telescope from its mount) so that the crosshairs in their foci coincided. The collimators were often permanently mounted in these positions, with their objectives and eyepieces fixed to separate piers. The meridian telescope was pointed to one collimator and then the other, moving through exactly 180°, and by reading the circle the amount of flexure (the amount the readings differed from 180°) was found. Absolute flexure, that is, a fixed bend in the tube, was detected by arranging that eyepiece and objective lens could be interchanged, and the average of the two observations of the same star was free from this error.\n\nParts of the apparatus were sometimes enclosed in glass cases to protect them from dust. These cases had openings for access. Other parts were closed against dust by removable silk covers.\n\nCertain instrumental errors could be averaged out by reversing the telescope on its mounting. A carriage was provided, which ran on rails between the piers, and on which the axis, circles and telescope could be raised by a screw-jack, wheeled out from between the piers, turned 180°, wheeled back, and lowered again.\n\nThe observing building housing the meridian circle did not have a rotating dome, as is often seen at observatories. Since the telescope observed only in the meridian, a vertical slot in the north and south walls, and across the roof between these, was all that was necessary. The building was unheated and kept as much as possible at the temperature of the outside air, to avoid air currents which would disturb the telescopic view. The building also housed the clocks, recorders, and other equipment for making observations.\n\nAt the focal plane, the eye end of the telescope had a number of vertical and one or two horizontal wires (crosshairs). In observing stars, the telescope was first directed downward at a basin of mercury forming a perfectly horizontal mirror and reflecting an image of the crosshairs back up the telescope tube. The crosshairs were adjusted until coincident with their reflection, and the line of sight was then perfectly vertical; in this position the circles were read for the \"nadir point\".\n\nThe telescope was next brought up to the approximate declination of the target star by watching the finder circle. The instrument was provided with a clamping apparatus, by which the observer, after having set the approximate declination, could clamp the axis so the telescope could not be moved in declination, except very slowly by a fine screw. By this slow motion, the telescope was adjusted until the star moved along the horizontal wire (or if there were two, in the middle between them), from the east side of the field of view to the west. Following this, the circles were read by the microscopes for a measurement of the apparent altitude of the star. The difference between this measurement and the nadir point was the \"nadir distance\" of the star. A movable horizontal wire or declination-micrometer was also used.\n\nAnother method of observing the apparent altitude of a star was to take half of the angular distance between the star observed directly and its reflection observed in a basin of mercury. The average of these two readings was the reading when the line of sight was horizontal, the \"horizontal point\" of the circle. The small difference in latitude between the telescope and the basin of mercury was accounted for.\n\nThe vertical wires were used for observing transits of stars, each wire furnishing a separate result. The time of transit over the middle wire was estimated, during subsequent analysis of the data, for each wire by adding or subtracting the known interval between the middle wire and the wire in question. These known intervals were predetermined by timing a star of known declination passing from one wire to the other, the pole star being best on account of its slow motion.\n\nTimings were originally made by an \"eye and ear\" method, estimating the interval between two beats of a clock. Later, timings were registered by pressing a key, the electrical signal making a mark on a strip recorder. Later still, the eye end of the telescope was usually fitted with an \"impersonal micrometer\", a device which allowed matching a vertical crosshair's motion to the star's motion. Set precisely on the moving star, the crosshair would trigger the electrical timing of the meridian crossing, removing the observer's personal equation from the measurement.\n\nThe field of the wires could be illuminated; the lamps were placed at some distance from the piers in order not to heat the instrument, and the light passed through holes in the piers and through the hollow axis to the center, whence it was directed to the eye-end by a system of prisms.\n\nTo determine absolute declinations or polar distances, it was necessary to determine the observatory's colatitude, or distance of the celestial pole from the zenith, by observing the upper and lower culmination of a number of circumpolar stars. The difference between the circle reading after observing a star and the reading corresponding to the zenith was the zenith distance of the star, and this plus the colatitude was the north polar distance. To determine the zenith point of the circle, the telescope was directed vertically downwards at a basin of mercury, the surface of which formed an absolutely horizontal mirror. The observer saw the horizontal wire and its reflected image, and moving the telescope to make these coincide, its optical axis was made perpendicular to the plane of the horizon, and the circle reading was 180° + zenith point.\n\nIn observations of stars refraction was taken into account as well as the errors of graduation and flexure. If the bisection of the star on the horizontal wire was not made in the centre of the field, allowance was made for curvature, or the deviation of the star's path from a great circle, and for the inclination of the horizontal wire to the horizon. The amount of this inclination was found by taking repeated observations of the zenith distance of a star during the one transit, the pole star being the most suitable because of its slow motion.\n\nAttempts were made to record the transits of a star photographically. A photographic plate was placed in the focus of a transit instrument and a number of short exposures made, their length and the time being registered automatically by a clock. The exposing shutter was a thin strip of steel, fixed to the armature of an electromagnet. The plate thus recorded a series of dots or short lines, and the vertical wires were photographed on the plate by throwing light through the objective lens for one or two seconds.\n\nMeridian circles required precise adjustment to do accurate work.\n\nThe rotation axis of the main telescope needed to be exactly horizontal. A sensitive spirit level, designed to rest on the pivots of the axis, performed this function. By adjusting one of the V-shaped bearings, the bubble was centered.\n\nThe line of sight of the telescope needed to be exactly perpendicular to the axis of rotation. This could be done by sighting a distant, stationary object, lifting and reversing the telescope on its bearings, and again sighting the object. If the crosshairs did not intersect the object, the line of sight was halfway between the new position of the crosshairs and the distant object; the crosshairs were adjusted accordingly and the process repeated as necessary. Also, if the rotation axis was known to be perfectly horizontal, the telescope could be directed downward at a basin of mercury, and the crosshairs illuminated. The mercury acted as a perfectly horizontal mirror, reflecting an image of the crosshairs back up the telescope tube. The crosshairs could then be adjusted until coincident with their reflection, and the line of sight was then perpendicular to the axis.\n\nThe line of sight of the telescope needed to be exactly within the plane of the meridian. This was done approximately by building the piers and the bearings of the axis on an east–west line. The telescope was then brought into the meridian by repeatedly timing the (apparent, incorrect) upper and lower meridian transits of a circumpolar star and adjusting one of the bearings horizontally until the interval between the transits was equal. Another method used calculated meridian crossing times for particular stars as established by other observatories. This was an important adjustment, and much effort was spent in perfecting it.\n\nIn practice, none of these adjustments were perfect. The small errors introduced by the imperfections were mathematically corrected during the analysis of the data.\n\nSome telescopes designed to measure star transits are zenith telescopes designed to point straight up at or near the zenith for extreme precision measurement of star positions. They use an altazimuth mount, instead of a meridian circle, fitted with leveling screws. Extremely sensitive levels are attached to the telescope mount to make angle measurements and the telescope has an eyepiece fitted with a micrometer.\n\nThe idea of having an instrument (quadrant) fixed in the plane of the meridian occurred even to the ancient astronomers and is mentioned by Ptolemy, but it was not carried into practice until Tycho Brahe constructed a large meridian quadrant.\n\nMeridian circles have been used since the 18th century to accurately measure positions of stars in order to catalog them. This is done by measuring the instant when the star passes through the local meridian. Its altitude above the horizon is noted as well. Knowing one's geographic latitude and longitude these measurements can be used to derive the star's right ascension and declination.\n\nOnce good star catalogs were available a transit telescope could be used anywhere in the world to accurately measure local longitude and time by observing local meridian transit times of catalogue stars. Prior to the invention of the atomic clock this was the most reliable source of accurate time.\n\nIn the \"Almagest\", Ptolemy describes a meridian circle which consisted of a fixed graduated outer ring and a movable inner ring with tabs that used a shadow to set the Sun's position. It was mounted vertically and aligned with the meridian. The instrument was used to measure the altitude of the Sun at noon in order to determine the path of the ecliptic.\n\nA meridian circle enabled the observer to simultaneously determine right ascension and declination, but it does not appear to have been much used for right ascension during the 17th century, the method of equal altitudes by portable quadrants or measures of the angular distance between stars with an astronomical sextant being preferred. These methods were very inconvenient, and in 1690, Ole Rømer invented the transit instrument.\n\nThe transit instrument consists of a horizontal axis in the direction east and west resting on firmly fixed supports, and having a telescope fixed at right angles to it, revolving freely in the plane of the meridian. At the same time Rømer invented the altitude and azimuth instrument for measuring vertical and horizontal angles, and in 1704, he combined a vertical circle with his transit instrument, so as to determine both co-ordinates at the same time.\n\nThis latter idea was, however, not adopted elsewhere, although the transit instrument soon came into universal use (the first one at Greenwich being mounted in 1721), and the mural quadrant continued until the end of the century to be employed for determining declinations. The advantages of using a whole circle, it being less liable to change its figure and not requiring reversal in order to observe stars north of the zenith, were then again recognized by Jesse Ramsden, who also improved the method of reading off angles by means of a micrometer microscope as described below.\n\nThe making of circles was shortly afterwards taken up by Edward Troughton, who constructed the first modern transit circle in 1806 for Groombridge's observatory at Blackheath, the Groombridge Transit Circle (a meridian transit circle). Troughton afterwards abandoned the idea and designed the mural circle to take the place of the mural quadrant.\n\nIn the United Kingdom, the transit instrument and mural circle continued until the middle of the 19th century to be the principal instrument in observatories, the first transit circle constructed there being that at Greenwich (mounted in 1850). However, on the continent, the transit circle superseded them from the years 1818–1819, when two circles by Johann Georg Repsold and Georg Friedrich von Reichenbach were mounted at Göttingen, and one by Reichenbach at Königsberg. The firm of Repsold and Sons was for a number of years eclipsed by that of Pistor and Martins in Berlin, who furnished various observatories with first-class instruments. Following the death of Martins, the Repsolds again took the lead and made many transit circles. The observatories of Harvard College, Cambridge University and Edinburgh University had large circles by Troughton and Simms.\n\nThe Airy Transit Circles at the Royal Greenwich Observatory (1851) and that at the Royal Observatory, Cape of Good Hope (1855) were made by Ransomes and May of Ipswich. The Greenwich instrument had optical and instrumental work by Troughton and Simms to the design of George Biddell Airy.\n\nA modern-day example of this type of telescope is the 8 inch (~0.2m) Flagstaff Astrometric Scanning Transit Telescope (FASTT) at the USNO Flagstaff Station Observatory. Modern meridian circles are usually automated. The observer is replaced with a CCD camera. As the sky drifts across the field of view, the image built up in the CCD is clocked across (and out of) the chip at the same rate. This allows some improvements: \n\nThe first automated instrument was the Carlsberg Automatic Meridian Circle, which came online in 1984.\n\n\n\n\n"}
{"id": "681596", "url": "https://en.wikipedia.org/wiki?curid=681596", "title": "Northrop Grumman X-47A Pegasus", "text": "Northrop Grumman X-47A Pegasus\n\nThe Northrop Grumman X-47 is a demonstration Unmanned Combat Aerial Vehicle. The X-47 began as part of DARPA's J-UCAS program, and is now part of the United States Navy's UCAS-D program to create a carrier-based unmanned aircraft. Unlike the Boeing X-45, initial Pegasus development was company-funded. The original vehicle carries the designation X-47A Pegasus, while the follow-on naval version is designated X-47B.\n\nThe US Navy did not commit to practical UCAV efforts until mid-2000, when the service awarded contracts of US$2 million each to Boeing and Northrop Grumman for a 15-month concept-exploration program.\n\nDesign considerations for a naval UCAV included dealing with the corrosive salt-water environment, deck handling for launch and recovery, integration with command and control systems, and operation in a carrier's high electromagnetic interference environment. The Navy was also interested in using their UCAVs for reconnaissance missions, penetrating protected airspace to identify targets for the attack waves.\n\nThe Navy went on to give Northrop Grumman a contract for a naval UCAV demonstrator with the designation of \"X-47A Pegasus\", in early 2001. The proof-of-concept X-47A vehicle was built under contract by Burt Rutan's Scaled Composites at the Mojave Spaceport. The Pegasus demonstrator looks like a simple black arrowhead with no vertical tailplane. It has a leading edge sweep of 55 degrees and a trailing edge sweep of 35 degrees. The demonstrator has retractable tricycle landing gear, with a one-wheel nose gear and dual-wheel main gear, and has six control surfaces, including two elevons and four \"inlaids\". The inlaids are small flap structures mounted on the top and bottom of the wing forward of the wingtips.\n\nThe X-47A is powered by a single Pratt & Whitney Canada JT15D-5C small high-bypass turbofan engine with 3,190 lbf (14.2 kN) thrust. This engine is currently in use with operational aircraft such as the Aermacchi S-211 trainer. The engine is mounted on the demonstrator's back, with the inlet on top behind the nose. The inlet duct has a serpentine diffuser to prevent radar reflections off the engine fan. However, to keep costs low, the engine exhaust is a simple cylindrical tailpipe, with no provisions for reducing radar or infrared signature.\n\nThe X-47A's airframe is built of composite materials, with construction subcontracted out to Burt Rutan's Scaled Composites company, which had the expertise and tooling to do the job inexpensively. The airframe consists of four main assemblies, split down the middle with two assemblies on top and two on bottom.\n\nThe X-47A was rolled out on 30 July 2001 and performed its first flight on 23 February 2003 at the US Naval Air Warfare Center at China Lake, California. The flight test program did not involve weapons delivery, but Pegasus does have two weapons bays, one on each side of the engine, that may be each loaded with a single 500 pound (225 kg) dummy bomb to simulate operational flight loads. The Pegasus was also used to evaluate technologies for carrier deck landings, though the demonstrator did not have an arrestor hook. Other issues related to carrier operations involve adding deck tie-downs without compromising stealth characteristics, and designing access panels so that they would not be blown around or damaged by strong winds blowing across the carrier deck. The J-UCAS program was terminated in February 2006 following the US military's Quadrennial Defense Review. The US Air Force and US Navy proceeded with their own UAV programs. The Navy selected Northrop Grumman's X-47B as its Unmanned Combat Air System demonstrator (UCAS-D) program.\n\n\n"}
{"id": "11363457", "url": "https://en.wikipedia.org/wiki?curid=11363457", "title": "Pachycaul", "text": "Pachycaul\n\nPachycauls are plants with a disproportionately thick trunk for their height, and few branches. The word is derived from the Greek \"pachy-\" meaning thick or stout, and Latin \"caulis\" meaning the stem.\n\nExamples occur in the genera\n\n"}
{"id": "5392444", "url": "https://en.wikipedia.org/wiki?curid=5392444", "title": "Permafrost Young Researchers Network", "text": "Permafrost Young Researchers Network\n\nThe Permafrost Young Researcher’s Network (PYRN) is a network formed in 2005 to formally facilitate and strengthen contacts among young scientists in the permafrost community. It arose from the need for an integrated single source of information for specific resources vital to young scientists (fellowships, conference travel funding, position opportunities, etc.). Additionally, the imminence of the International Polar Year (IPY) prompted the need for a visible representation of the young permafrost community at the international level. The Permafrost Young Researchers Network has therefore been formally established within the International Permafrost Association (IPA) framework and has created and maintains means of communication among young researchers involved in permafrost research. It reports on young researchers’ activities to the IPA membership and working parties and represents permafrost scientists and engineers within broader international and national assemblages.\n\nThe PYRN website is hosted by the Arctic Portal . On the site information on conferences, events, job and graduate positions, research and other topics related to permafrost science is available. It distributes an electronic newsletter to the young researchers’ communities related to the aforementioned topics and seeks to promote and publicize research undertaken by young researchers. PYRN membership is now up to 1100+ members with as many as 50 countries involved (11/07/2014). An Expression of Intent to the International Polar Year was submitted and PYRN will take part in this major event under the auspices of the International Permafrost Association.\n\n\n"}
{"id": "1375680", "url": "https://en.wikipedia.org/wiki?curid=1375680", "title": "Pinsk Marshes", "text": "Pinsk Marshes\n\nThe Pinsk Marshes (, \"Pinskiya baloty\"), also known as the Pripet Marshes (, \"Prypiackija baloty\"), the Polesie Marshes, and the Rokitno Marshes, are a vast natural region of wetlands along the forested basin of the Pripyat River and its tributaries from Brest to the west to Mogilev to the northeast and Kiev to the southeast. It is one of the largest wetland areas of Europe. The city of Pinsk is one of the most important in the area.\n\nThe Pinsk Marshes mostly lie within the Polesian Lowland, hence Polesie Marshes (Woodland Marshes), and occupy most of the southern part of Belarus and the north-west of Ukraine. They cover roughly surrounding the sandy lowlands of the dense network of rivers and rivulets forming on both sides of the Pripyat River, one of the main tributaries of the Dnieper. Dense woods are interspersed with numerous marshes, moors, ponds and streams extending west to east and north to south. The marshes undergo substantial changes in size during the year, with melting snows in springtime and autumn rainfall causing extensive flooding as the river overflows. Drainage of the eastern portion began in 1870, and significant areas have been cleared for pasture and farmland.\n\nHistorically, for most of the year, the marshes were virtually impassable to major military forces, which influenced strategic planning of all military operations in the region. In Volume VII of \"Wars of Justitian\", by the Roman historian Procopius, is a story that the early Slavs hid out from predators in the Pripet Marshes by breathing through reeds.\n\nLike most other wetlands in Europe, the Pinsk Marshes were once seen as an unhealthy area and a focus of sickness. In the late 19th century, drainage of the marshes recovered 1.5 million hectares of wetlands.\n\nAt the start of World War I, the marshes separated the Austro-Hungarian Fourth Army from the XII corps; the few roads that traversed the region were narrow and largely unimproved. That \nleft a wide gap, and the Third Army Corps of the Imperial Russian Army poured in before the Austro-Hungarian Second Army's transfer from Serbia was complete. The Russians soon captured the valuable railhead at Lemberg (now Lviv), then in the far east of Austria-Hungary (now part of the western Ukraine), as a result. Throughout the rest of the war, the wetlands remained one of the principal geographic obstacles of the Eastern Front.\n\nThe marshes divided the central and southern theatres of operation during World War II, and they served as a hideout for both Soviet and Polish partisans. At one stage during the war, the German administration planned to drain the marshes, 'cleanse' them of their 'degenerate' inhabitants and repopulate the area with German colonists. Konrad Meyer was the leader in command of the 'Pripet plan'. Hitler scuttled the project late in 1941, as he believed that it might entail Dust Bowl conditions.\n\nGerman racial anthropologist Theodor Poesche had proposed, in the late 19th century, that the Aryan race evolved in the marshes because of the prevalence of albinism.\n\nIn 1942, after an uprising, approximately 1,000 Jews escaped from the Łachwa Ghetto, of whom about 600 were able to take refuge in the Pinsk Marshes.\n\nKnown as \"Pripjet-Sümpfe\" by the Germans, the wetlands were dreaded by the Wehrmacht troops. During the German invasion of the Soviet Union, the Third Reich armies skirted the wetlands, passing through the north or south of it. However, after the debacle of the Eastern Front in 1944, many retreating units such as the 7th, 35th, 134th and 292nd Infantry Divisions had to cut across the marshy areas. They often needed to build tracks with logs over which they could pull light loads in horse-drawn vehicles.\n\nThere was a plan to drain the wetlands during 1952, when the area of the marshes was under Soviet administration.\n\nIn 1986, the region became world-famous because of the Chernobyl disaster; however, the Pripet Marshes should not be confused with the ghost city of Pripyat. That ill-fated community within the Chernobyl Exclusion Zone is located east-southeast of the geographic center of the Pinsk Marshes area.\n"}
{"id": "4150239", "url": "https://en.wikipedia.org/wiki?curid=4150239", "title": "RRS John Biscoe (1956)", "text": "RRS John Biscoe (1956)\n\nThe RRS \"John Biscoe\" was a supply and research vessel used by the British Antarctic Survey between 1956 and 1991.\n\nAn earlier vessel, operated from 1947-56. Both were named after the English explorer John Biscoe, who discovered parts of Antarctica in the early 1830s.\n\n\"John Biscoe II\" was replaced by in 1991. After decommissioning, she was sold and eventually scrapped in 2004 under the name \"Fayza Express\".\n\n\"Biscoe\"'s first visit to Halley Research Station, in 1959/60 was under the veteran captain, Bill Johnston.\n\nFrom 1975, joint Masters of \"John Biscoe\" were Malcolm Phelps and Chris Elliott. Chris Elliott had joined BAS as Third Officer on \"John Biscoe\" in 1967, becoming Second Officer in 1970. He established the successful Offshore Biological Programme cruises and helped superintend the building of replacement . Elliott was awarded the Polar Medal in 2004 and an MBE in 2005. The sea passage between Adelaide Island and Jenny Island is named after Chris Elliott.\n\n"}
{"id": "1174316", "url": "https://en.wikipedia.org/wiki?curid=1174316", "title": "Science by press conference", "text": "Science by press conference\n\nScience by press conference (or science by press release) is the practice by which scientists put an unusual focus on publicizing results of research in the media. The term is usually used disparagingly. It is intended to associate the target with people promoting scientific \"findings\" of questionable scientific merit who turn to the media for attention when they are unlikely to win the approval of the professional scientific community.\n\nPremature publicity violates a cultural value of most of the scientific community, which is that findings should be subjected to independent review with a \"thorough examination by the scientific community\" before they are widely publicized. The standard practice is to publish a paper in a peer-reviewed scientific journal. This idea has many merits, including that the scientific community has a responsibility to conduct itself in a deliberative, non-attention seeking way; and that its members should be oriented more towards the pursuit of insight than fame. Science by press conference in its most egregious forms can be undertaken on behalf of an individual researcher seeking fame, a corporation seeking to sway public opinion or investor perception, or a political or ideological movement.\n\nThe phrase was coined by Spyros Andreopoulos, a public affairs officer at Stanford University Medical School, in a 1980 letter which appeared in the \"New England Journal of Medicine\". Andreopoulos was commenting specifically on the publicity practices of biotechnology startups, including Biogen and Genentech. The journal in which it appeared had implemented a long-standing policy under editor Franz J. Ingelfinger which prohibited seeking publicity for research prior to its submission or publication, informally called the Ingelfinger Rule.\n\n\nThese cases became notorious examples of \"science by press conference\" precisely because they were widely reported in the press, but were later either rebuffed, debunked, or found to be outright fraud.\n\nCompetition for publicity, between scientific institutions or just individual researchers, is considered a driving force behind premature press conferences. Pressure to announce research findings quickly enough to \"avoid losing credit\" for any scientific advances may be enhanced by limited or highly competitive funding.\n\nScience by press conference does not have to involve a groundbreaking announcement. A manufacturer may desire to publicize results of research that suggest their product is safe. Science by press conference does not necessarily have to be directed at the general public. In some cases, it may be directed at a target market like opinion leaders, a specific industry, potential investors, or a specific group of consumers. Biotechnology companies, for example, have financial incentives to utilize premature press conferences to gain favorable media coverage.\n\nIn recent years, sociologists of science have recast discussion about \"science by press conference\". They point to the increasing presence of media conversation across all aspects of culture, and argue that science is subject to many of the same social forces as other aspects of culture. They have described the increased \"medialization\" of science, and suggest that both science and society are changed by this process.\n\nWhile the phrase tends to criticize scientists involved in creating the publicity, it has also been used to assert that the media bear responsibility in many instances. Even well-intentioned scientists can sometimes unintentionally create truth-distorting media firestorms because of journalists' difficulty in remaining critical and balanced, the media's interest in controversy, and the general tendency of science reporting to focus on apparent \"groundbreaking findings\" rather than on the larger context of a research field. Further, when results are released with great fanfare and limited peer review, basic journalism skills require skepticism and further investigation; the fact that they often do not can be seen as a problem with the media as much as with scientists who seek to exploit their power.\n\nA common example of science by press conference occurs when the media report that a certain product or activity affects health or safety. For instance, the media frequently report findings that a certain food causes or prevents a disease. These reports sometimes contradict earlier reports. In some cases, it is later learned that a group interested in influencing opinion had a hand in publicizing a specific report.\n\nThe phrase also condemns different behavior in different fields. For instance, scientists working in fields that put an emphasis on the value of fast dissemination of research, like HIV treatment research, often first and most visibly disseminate research results via conferences or talks rather than through printed publication. In these areas of science, printed publication occurs later in the process of dissemination of results than in some other fields. In the case of HIV, this is partly the result of AIDS activism in which people with AIDS and their allies criticized the slow pace of research. In particular, they characterized researchers who kept quiet before publication as being more interested in their careers than in the well-being of people with AIDS. On the other hand, over-hyped early findings can inspire activists' ire and even their direct and critical use of the phrase \"science by press conference\". AIDS denialist groups have claimed that press conferences announcing findings in HIV and AIDS research, particularly Robert Gallo's April 23, 1984, announcement of the discovery of the probable AIDS virus, inhibited research into non-HIV etiologies of AIDS.\n\nSimilarly, clinical trials and other kinds of important medical research may release preliminary results to the media before a journal article is printed. In this case, the justification can be that clinicians and patients will benefit from the information even knowing that the data are preliminary and require further review. For instance, researchers did not wait to publish journal articles about the SARS outbreak before notifying the media about many of their findings, for obvious reasons.\n\nAnother example might be the termination of a clinical trial because it has yielded early benefit. Publicizing this kind of result has obvious value; a delay of a few months might have terrible consequences when the results concern life-threatening conditions. On the other hand, the latter practice is especially vulnerable to abuse for self-serving ends and thus has drawn criticism similar to that implied by the phrase \"science by press conference\".\n\nThese examples illustrate that the derision in the term \"science by press conference\" does not necessarily reflect an absolute rule to publish before publicizing. Rather, it illustrates the value that publicity should be a byproduct of science rather than its objective.\n"}
{"id": "371255", "url": "https://en.wikipedia.org/wiki?curid=371255", "title": "Sliding mode control", "text": "Sliding mode control\n\nIn control systems, sliding mode control (SMC) is a nonlinear control method that alters the dynamics of a nonlinear system by application of a discontinuous control signal (or more rigorously, a set-valued control signal) that forces the system to \"slide\" along a cross-section of the system's normal behavior. The state-feedback control law is not a continuous function of time. Instead, it can switch from one continuous structure to another based on the current position in the state space. Hence, sliding mode control is a variable structure control method. The multiple control structures are designed so that trajectories always move toward an adjacent region with a different control structure, and so the ultimate trajectory will not exist entirely within one control structure. Instead, it will \"slide\" along the boundaries of the control structures. The motion of the system as it slides along these boundaries is called a \"sliding mode\" and the geometrical locus consisting of the boundaries is called the \"sliding (hyper)surface\". In the context of modern control theory, any variable structure system, like a system under SMC, may be viewed as a special case of a hybrid dynamical system as the system both flows through a continuous state space but also moves through different discrete control modes.\n\nFigure 1 shows an example trajectory of a system under sliding mode control. The sliding surface is described by formula_1, and the sliding mode along the surface commences after the finite time when system trajectories have reached the surface. In the theoretical description of sliding modes, the system stays confined to the sliding surface and need only be viewed as sliding along the surface. However, real implementations of sliding mode control approximate this theoretical behavior with a high-frequency and generally non-deterministic switching control signal that causes the system to \"chatter\" in a tight neighborhood of the sliding surface. In fact, although the system is nonlinear in general, the idealized (i.e., non-chattering) behavior of the system in Figure 1 when confined to the formula_1 surface is an LTI system with an exponentially stable origin.\n\nIntuitively, sliding mode control uses practically infinite gain to force the trajectories of a dynamic system to slide along the restricted sliding mode subspace. Trajectories from this reduced-order sliding mode have desirable properties (e.g., the system naturally slides along it until it comes to rest at a desired equilibrium). The main strength of sliding mode control is its robustness. Because the control can be as simple as a switching between two states (e.g., \"on\"/\"off\" or \"forward\"/\"reverse\"), it need not be precise and will not be sensitive to parameter variations that enter into the control channel. Additionally, because the control law is not a continuous function, the sliding mode can be reached in \"finite\" time (i.e., better than asymptotic behavior). Under certain common conditions, optimality requires the use of bang–bang control; hence, sliding mode control describes the optimal controller for a broad set of dynamic systems.\n\nOne application of sliding mode controller is the control of electric drives operated by switching power converters. Because of the discontinuous operating mode of those converters, a discontinuous sliding mode controller is a natural implementation choice over continuous controllers that may need to be applied by means of pulse-width modulation or a similar technique of applying a continuous signal to an output that can only take discrete states. Sliding mode control has many applications in robotics. In particular, this control algorithm has been used for tracking control of unmanned surface vessels in simulated rough seas with high degree of success.\n\nSliding mode control must be applied with more care than other forms of nonlinear control that have more moderate control action. In particular, because actuators have delays and other imperfections, the hard sliding-mode-control action can lead to chatter, energy loss, plant damage, and excitation of unmodeled dynamics. Continuous control design methods are not as susceptible to these problems and can be made to mimic sliding-mode controllers.\n\nConsider a nonlinear dynamical system described by\nwhere\nis an -dimensional state vector and\nis an -dimensional input vector that will be used for state feedback. The functions formula_5 and formula_6 are assumed to be continuous and sufficiently smooth so that the Picard–Lindelöf theorem can be used to guarantee that solution formula_7 to Equation () exists and is unique.\n\nA common task is to design a state-feedback control law formula_8 (i.e., a mapping from current state formula_7 at time to the input formula_10) to stabilize the dynamical system in Equation () around the origin formula_11. That is, under the control law, whenever the system is started away from the origin, it will return to it. For example, the component formula_12 of the state vector formula_13 may represent the difference some output is away from a known signal (e.g., a desirable sinusoidal signal); if the control formula_10 can ensure that formula_12 quickly returns to formula_16, then the output will track the desired sinusoid. In sliding-mode control, the designer knows that the system behaves desirably (e.g., it has a stable equilibrium) provided that it is constrained to a subspace of its configuration space. Sliding mode control forces the system trajectories into this subspace and then holds them there so that they slide along it. This reduced-order subspace is referred to as a \"sliding (hyper)surface\", and when closed-loop feedback forces trajectories to slide along it, it is referred to as a \"sliding mode\" of the closed-loop system. Trajectories along this subspace can be likened to trajectories along eigenvectors (i.e., modes) of LTI systems; however, the sliding mode is enforced by creasing the vector field with high-gain feedback. Like a marble rolling along a crack, trajectories are confined to the sliding mode.\n\nThe sliding-mode control scheme involves\nBecause sliding mode control laws are not continuous, it has the ability to drive trajectories to the sliding mode in finite time (i.e., stability of the sliding surface is better than asymptotic). However, once the trajectories reach the sliding surface, the system takes on the character of the sliding mode (e.g., the origin formula_17 may only have asymptotic stability on this surface).\n\nThe sliding-mode designer picks a \"switching function\" formula_18 that represents a kind of \"distance\" that the states formula_13 are away from a sliding surface.\nThe sliding-mode-control law switches from one state to another based on the \"sign\" of this distance. So the sliding-mode control acts like a stiff pressure always pushing in the direction of the sliding mode where formula_22.\nDesirable formula_7 trajectories will approach the sliding surface, and because the control law is not continuous (i.e., it switches from one state to another as trajectories move across this surface), the surface is reached in finite time. Once a trajectory reaches the surface, it will slide along it and may, for example, move toward the formula_25 origin. So the switching function is like a topographic map with a contour of constant height along which trajectories are forced to move.\n\nThe sliding (hyper)surface is of dimension formula_26 where is the number of states in formula_13 and is the number of input signals (i.e., control signals) in formula_10. For each control index formula_29, there is an formula_30 sliding surface given by\n\nThe vital part of SMC design is to choose a control law so that the sliding mode (i.e., this surface given by formula_31) exists and is reachable along system trajectories. The principle of sliding mode control is to forcibly constrain the system, by suitable control strategy, to stay on the sliding surface on which the system will exhibit desirable features. When the system is constrained by the sliding control to stay on the sliding surface, the system dynamics are governed by reduced-order system obtained from Equation ().\n\nTo force the system states formula_13 to satisfy formula_33, one must:\n\nNote that because the control law is not continuous, it is certainly not locally Lipschitz continuous, and so existence and uniqueness of solutions to the closed-loop system is \"not\" guaranteed by the Picard–Lindelöf theorem. Thus the solutions are to be understood in the Filippov sense. Roughly speaking, the resulting closed-loop system moving along formula_33 is approximated by the smooth dynamics formula_38 however, this smooth behavior may not be truly realizable. Similarly, high-speed pulse-width modulation or delta-sigma modulation produces outputs that only assume two states, but the effective output swings through a continuous range of motion. These complications can be avoided by using a different nonlinear control design method that produces a continuous controller. In some cases, sliding-mode control designs can be approximated by other continuous control designs.\n\nThe following theorems form the foundation of variable structure control.\n\nConsider a Lyapunov function candidate\n\nwhere formula_39 is the Euclidean norm (i.e., formula_40 is the distance away from the sliding manifold where formula_31). For the system given by Equation () and the sliding surface given by Equation (), a sufficient condition for the existence of a sliding mode is that\nin a neighborhood of the surface given by formula_43.\n\nRoughly speaking (i.e., for the scalar control case when formula_44), to achieve formula_45, the feedback control law formula_46 is picked so that formula_47 and formula_48 have opposite signs. That is,\nNote that\nand so the feedback control law formula_56 has a direct impact on formula_48.\n\nTo ensure that the sliding mode formula_31 is attained in finite time, formula_59 must be more strongly bounded away from zero. That is, if it vanishes too quickly, the attraction to the sliding mode will only be asymptotic. To ensure that the sliding mode is entered in finite time,\nwhere formula_61 and formula_62 are constants.\n\nThis condition ensures that for the neighborhood of the sliding mode formula_63,\nSo, for formula_65,\nwhich, by the chain rule (i.e., formula_67 with formula_68), means\nwhere formula_70 is the upper right-hand derivative of formula_71 and the symbol formula_72 denotes proportionality. So, by comparison to the curve formula_73 which is represented by differential equation formula_74 with initial condition formula_75, it must be the case that formula_76 for all . Moreover, because formula_77, formula_78 must reach formula_79 in finite time, which means that must reach formula_80 (i.e., the system enters the sliding mode) in finite time. Because formula_78 is proportional to the Euclidean norm formula_82 of the switching function formula_47, this result implies that the rate of approach to the sliding mode must be firmly bounded away from zero.\n\nIn the context of sliding mode control, this condition means that\nwhere formula_39 is the Euclidean norm. For the case when switching function formula_47 is scalar valued, the sufficient condition becomes\nTaking formula_88, the scalar sufficient condition becomes\nwhich is equivalent to the condition that\nThat is, the system should always be moving toward the switching surface formula_91, and its speed formula_92 toward the switching surface should have a non-zero lower bound. So, even though formula_47 may become vanishingly small as formula_13 approaches the formula_31 surface, formula_48 must always be bounded firmly away from zero. To ensure this condition, sliding mode controllers are discontinuous across the formula_91 manifold; they \"switch\" from one non-zero value to another as trajectories cross the manifold.\n\nFor the system given by Equation () and sliding surface given by Equation (), the subspace for which the formula_98 surface is reachable is given by\nThat is, when initial conditions come entirely from this space, the Lyapunov function candidate formula_100 is a Lyapunov function and formula_13 trajectories are sure to move toward the sliding mode surface where formula_102. Moreover, if the reachability conditions from Theorem 1 are satisfied, the sliding mode will enter the region where formula_103 is more strongly bounded away from zero in finite time. Hence, the sliding mode formula_91 will be attained in finite time.\n\nLet\nbe nonsingular. That is, the system has a kind of controllability that ensures that there is always a control that can move a trajectory to move closer to the sliding mode. Then, once the sliding mode where formula_106 is achieved, the system will stay on that sliding mode. Along sliding mode trajectories, formula_51 is constant, and so sliding mode trajectories are described by the differential equation\nIf an formula_13-equilibrium is stable with respect to this differential equation, then the system will slide along the sliding mode surface toward the equilibrium.\n\nThe \"equivalent control law\" on the sliding mode can be found by solving\nfor the equivalent control law formula_56. That is,\nand so the equivalent control\nThat is, even though the actual control formula_10 is not continuous, the rapid switching across the sliding mode where formula_31 forces the system to \"act\" as if it were driven by this continuous control.\n\nLikewise, the system trajectories on the sliding mode behave as if\nThe resulting system matches the sliding mode differential equation\n, the sliding mode surface formula_31, and the trajectory conditions from the reaching phase now reduce to the above derived simpler condition. Hence, the system can be assumed to follow the simpler formula_119 condition after some initial transient during the period while the system finds the sliding mode. The same motion is approximately maintained when the equality formula_106 only approximately holds.\n\nIt follows from these theorems that the sliding motion is invariant (i.e., insensitive) to sufficiently small disturbances entering the system through the control channel. That is, as long as the control is large enough to ensure that formula_45 and formula_48 is uniformly bounded away from zero, the sliding mode will be maintained as if there was no disturbance. The invariance property of sliding mode control to certain disturbances and model uncertainties is its most attractive feature; it is strongly robust.\n\nAs discussed in an example below, a sliding mode control law can keep the constraint\nin order to asymptotically stabilize any system of the form\nwhen formula_125 has a finite upper bound. In this case, the sliding mode is where\n(i.e., where formula_127). That is, when the system is constrained this way, it behaves like a simple stable linear system, and so it has a globally exponentially stable equilibrium at the formula_128 origin.\n\n\n\nAlthough various theories exist for sliding mode control system design, there is a lack of a highly effective design methodology due to practical difficulties encountered in analytical and numerical methods. A reusable computing paradigm such as a genetic algorithm can, however, be utilized to transform a 'unsolvable problem' of optimal design into a practically solvable 'non-deterministic polynomial problem'. This results in computer-automated designs for sliding model control. \n\nSliding mode control can be used in the design of state observers. These non-linear high-gain observers have the ability to bring coordinates of the estimator error dynamics to zero in finite time. Additionally, switched-mode observers have attractive measurement noise resilience that is similar to a Kalman filter. For simplicity, the example here uses a traditional sliding mode modification of a Luenberger observer for an LTI system. In these sliding mode observers, the order of the observer dynamics are reduced by one when the system enters the sliding mode. In this particular example, the estimator error for a single estimated state is brought to zero in finite time, and after that time the other estimator errors decay exponentially to zero. However, as first described by Drakunov, a sliding mode observer for non-linear systems can be built that brings the estimation error for all estimated states to zero in a finite (and arbitrarily small) time.\n\nHere, consider the LTI system\nwhere state vector formula_180, formula_181 is a vector of inputs, and output is a scalar equal to the first state of the formula_13 state vector. Let\nwhere\n\nThe goal is to design a high-gain state observer that estimates the state vector formula_13 using only information from the measurement formula_190. Hence, let the vector formula_191 be the estimates of the states. The observer takes the form\nwhere formula_193 is a nonlinear function of the error between estimated state formula_194 and the output formula_190, and formula_196 is an observer gain vector that serves a similar purpose as in the typical linear Luenberger observer. Likewise, let\nwhere formula_198 is a column vector. Additionally, let formula_199 be the state estimator error. That is, formula_200. The error dynamics are then\nwhere formula_202 is the estimator error for the first state estimate. The nonlinear control law can be designed to enforce the sliding manifold\nso that estimate formula_194 tracks the real state formula_12 after some finite time (i.e., formula_206). Hence, the sliding mode control switching function\nTo attain the sliding manifold, formula_48 and formula_47 must always have opposite signs (i.e., formula_142 for essentially all formula_13). However,\nwhere formula_213 is the collection of the estimator errors for all of the unmeasured states. To ensure that formula_142, let\nwhere\nThat is, positive constant must be greater than a scaled version of the maximum possible estimator errors for the system (i.e., the initial errors, which are assumed to be bounded so that can be picked large enough; al). If is sufficiently large, it can be assumed that the system achieves formula_217 (i.e., formula_206). Because formula_219 is constant (i.e., 0) along this manifold, formula_220 as well. Hence, the discontinuous control formula_221 may be replaced with the equivalent continuous control formula_222 where\nSo\nThis equivalent control formula_222 represents the contribution from the other formula_136 states to the trajectory of the output state formula_12. In particular, the row formula_228 acts like an output vector for the error subsystem\nSo, to ensure the estimator error formula_230 for the unmeasured states converges to zero, the formula_231 vector formula_232 must be chosen so that the formula_233 matrix formula_234 is Hurwitz (i.e., the real part of each of its eigenvalues must be negative). Hence, provided that it is observable, this formula_230 system can be stabilized in exactly the same way as a typical linear state observer when formula_228 is viewed as the output matrix (i.e., \"\"). That is, the formula_222 equivalent control provides measurement information about the unmeasured states that can continually move their estimates asymptotically closer to them. Meanwhile, the discontinuous control formula_238 forces the estimate of the measured state to have zero error in finite time. Additionally, white zero-mean symmetric measurement noise (e.g., Gaussian noise) only affects the switching frequency of the control , and hence the noise will have little effect on the equivalent sliding mode control formula_222. Hence, the sliding mode observer has Kalman filter–like features.\n\nThe final version of the observer is thus\nwhere\nThat is, by augmenting the control vector formula_10 with the switching function formula_245, the sliding mode observer can be implemented as an LTI system. That is, the discontinuous signal formula_245 is viewed as a control \"input\" to the 2-input LTI system.\n\nFor simplicity, this example assumes that the sliding mode observer has access to a measurement of a single state (i.e., output formula_190). However, a similar procedure can be used to design a sliding mode observer for a vector of weighted combinations of states (i.e., when output formula_248 uses a generic matrix ). In each case, the sliding mode will be the manifold where the estimated output formula_249 follows the measured output formula_250 with zero error (i.e., the manifold where formula_251).\n\n\n"}
{"id": "14467558", "url": "https://en.wikipedia.org/wiki?curid=14467558", "title": "Surface force", "text": "Surface force\n\nSurface force denoted \"f\" is the force that acts across an internal or external surface element in a material body. Surface force can be decomposed into two perpendicular components: normal forces and shear forces. A normal force acts normally over an area and a shear force acts tangentially over an area.\n\nSince pressure is formula_2, and area is a formula_3,\n\n"}
{"id": "27764", "url": "https://en.wikipedia.org/wiki?curid=27764", "title": "Systems engineering", "text": "Systems engineering\n\nSystems engineering is an interdisciplinary field of engineering and engineering management that focuses on how to design and manage complex systems over their life cycles. At its core, systems engineering utilizes systems thinking principles to organize this body of knowledge. Issues such as requirements engineering, reliability, logistics, coordination of different teams, testing and evaluation, maintainability and many other disciplines necessary for successful system development, design, implementation, and ultimate decommission become more difficult when dealing with large or complex projects. Systems engineering deals with work-processes, optimization methods, and risk management tools in such projects. It overlaps technical and human-centered disciplines such as industrial engineering, mechanical engineering, manufacturing engineering, control engineering, software engineering, electrical engineering, cybernetics, organizational studies, civil engineering and project management. Systems engineering ensures that all likely aspects of a project or system are considered, and integrated into a whole.\n\nThe systems engineering process is a discovery process that is quite unlike a manufacturing process. A manufacturing process is focused on repetitive activities that achieve high quality outputs with minimum cost and time. The systems engineering process must begin by discovering the real problems that need to be resolved, and identifying the most probable or highest impact failures that can occur – systems engineering involves finding solutions to these problems.\n\nThe term \"systems engineering\" can be traced back to Bell Telephone Laboratories in the 1940s. The need to identify and manipulate the properties of a system as a whole, which in complex engineering projects may greatly differ from the sum of the parts' properties, motivated various industries, especially those developing systems for the U.S. Military, to apply the discipline.\n\nWhen it was no longer possible to rely on design evolution to improve upon a system and the existing tools were not sufficient to meet growing demands, new methods began to be developed that addressed the complexity directly. The continuing evolution of systems engineering comprises the development and identification of new methods and modeling techniques. These methods aid in a better comprehension of the design and developmental control of engineering systems as they grow more complex. Popular tools that are often used in the systems engineering context were developed during these times, including USL, UML, QFD, and IDEF0.\n\nIn 1990, a professional society for systems engineering, the \"National Council on Systems Engineering\" (NCOSE), was founded by representatives from a number of U.S. corporations and organizations. NCOSE was created to address the need for improvements in systems engineering practices and education. As a result of growing involvement from systems engineers outside of the U.S., the name of the organization was changed to the International Council on Systems Engineering (INCOSE) in 1995. Schools in several countries offer graduate programs in systems engineering, and continuing education options are also available for practicing engineers.\n\nSystems engineering signifies only an approach and, more recently, a discipline in engineering. The aim of education in systems engineering is to formalize various approaches simply and in doing so, identify new methods and research opportunities similar to that which occurs in other fields of engineering. As an approach, systems engineering is holistic and interdisciplinary in flavour.\n\nThe traditional scope of engineering embraces the conception, design, development, production and operation of physical systems. Systems engineering, as originally conceived, falls within this scope. \"Systems engineering\", in this sense of the term, refers to the distinctive set of concepts, methodologies, organizational structures (and so on) that have been developed to meet the challenges of engineering effective functional systems of unprecedented size and complexity within time, budget, and other constraints. The Apollo program is a leading example of a systems engineering project.\n\nThe use of the term \"systems engineer\" has evolved over time to embrace a wider, more holistic concept of \"systems\" and of engineering processes. This evolution of the definition has been a subject of ongoing controversy, and the term continues to apply to both the narrower and broader scope.\n\nTraditional systems engineering was seen as a branch of engineering in the classical sense, that is, as applied only to physical systems, such as spacecraft and aircraft. More recently, systems engineering has evolved to a take on a broader meaning especially when humans were seen as an essential component of a system. Checkland, for example, captures the broader meaning of systems engineering by stating that 'engineering' \"can be read in its general sense; you can engineer a meeting or a political agreement.\"\n\nConsistent with the broader scope of systems engineering, the Systems Engineering Body of Knowledge (SEBoK) has defined three types of systems engineering: (1) Product Systems Engineering (PSE) is the traditional systems engineering focused on the design of physical systems consisting of hardware and software. (2) Enterprise Systems Engineering (ESE) pertains to the view of enterprises, that is, organizations or combinations of organizations, as systems. (3) Service Systems Engineering (SSE) has to do with the engineering of service systems. Checkland defines a service system as a system which is conceived as serving another system. Most civil infrastructure systems are service systems.\n\nSystems engineering focuses on analyzing and eliciting customer needs and required functionality early in the development cycle, documenting requirements, then proceeding with design synthesis and system validation while considering the complete problem, the system lifecycle. This includes fully understanding all of the stakeholders involved. Oliver et al. claim that the systems engineering process can be decomposed into\nWithin Oliver's model, the goal of the Management Process is to organize the technical effort in the lifecycle, while the Technical Process includes \"assessing available information\", \"defining effectiveness measures\", to \"create a behavior model\", \"create a structure model\", \"perform trade-off analysis\", and \"create sequential build & test plan\".\n\nDepending on their application, although there are several models that are used in the industry, all of them aim to identify the relation between the various stages mentioned above and incorporate feedback. Examples of such models include the Waterfall model and the VEE model.\n\nSystem development often requires contribution from diverse technical disciplines. By providing a systems (holistic) view of the development effort, systems engineering helps mold all the technical contributors into a unified team effort, forming a structured development process that proceeds from concept to production to operation and, in some cases, to termination and disposal. In an acquisition, the holistic integrative discipline combines contributions and balances tradeoffs among cost, schedule, and performance while maintaining an acceptable level of risk covering the entire life cycle of the item.\n\nThis perspective is often replicated in educational programs, in that systems engineering courses are taught by faculty from other engineering departments, which helps create an interdisciplinary environment.\n\nThe need for systems engineering arose with the increase in complexity of systems and projects, in turn exponentially increasing the possibility of component friction, and therefore the unreliability of the design. When speaking in this context, complexity incorporates not only engineering systems, but also the logical human organization of data. At the same time, a system can become more complex due to an increase in size as well as with an increase in the amount of data, variables, or the number of fields that are involved in the design. The International Space Station is an example of such a system.\n\nThe development of smarter control algorithms, microprocessor design, and analysis of environmental systems also come within the purview of systems engineering. Systems engineering encourages the use of tools and methods to better comprehend and manage complexity in systems. Some examples of these tools can be seen here:\n\nTaking an interdisciplinary approach to engineering systems is inherently complex since the behavior of and interaction among system components is not always immediately well defined or understood. Defining and characterizing such systems and subsystems and the interactions among them is one of the goals of systems engineering. In doing so, the gap that exists between informal requirements from users, operators, marketing organizations, and technical specifications is successfully bridged.\n\nOne way to understand the motivation behind systems engineering is to see it as a method, or practice, to identify and improve common rules that exist within a wide variety of systems. Keeping this in mind, the principles of systems engineering – holism, emergent behavior, boundary, et al. – can be applied to any system, complex or otherwise, provided systems thinking is employed at all levels. Besides defense and aerospace, many information and technology based companies, software development firms, and industries in the field of electronics & communications require systems engineers as part of their team.\n\nAn analysis by the INCOSE Systems Engineering center of excellence (SECOE) indicates that optimal effort spent on systems engineering is about 15-20% of the total project effort. At the same time, studies have shown that systems engineering essentially leads to reduction in costs among other benefits. However, no quantitative survey at a larger scale encompassing a wide variety of industries has been conducted until recently. Such studies are underway to determine the effectiveness and quantify the benefits of systems engineering.\n\nSystems engineering encourages the use of modeling and simulation to validate assumptions or theories on systems and the interactions within them.\n\nUse of methods that allow early detection of possible failures, in safety engineering, are integrated into the design process. At the same time, decisions made at the beginning of a project whose consequences are not clearly understood can have enormous implications later in the life of a system, and it is the task of the modern systems engineer to explore these issues and make critical decisions. No method guarantees today's decisions will still be valid when a system goes into service years or decades after first conceived. However, there are techniques that support the process of systems engineering. Examples include soft systems methodology, Jay Wright Forrester's System dynamics method, and the Unified Modeling Language (UML)—all currently being explored, evaluated, and developed to support the engineering decision process.\n\nEducation in systems engineering is often seen as an extension to the regular engineering courses, reflecting the industry attitude that engineering students need a foundational background in one of the traditional engineering disciplines (e.g., aerospace engineering, civil engineering, electrical engineering, mechanical engineering, manufacturing engineering, industrial engineering)—plus practical, real-world experience to be effective as systems engineers. Undergraduate university programs in systems engineering are rare. Typically, systems engineering is offered at the graduate level in combination with interdisciplinary study.\n\nINCOSE maintains a continuously updated Directory of Systems Engineering Academic Programs worldwide. As of 2009, there are about 80 institutions in United States that offer 165 undergraduate and graduate programs in systems engineering. Education in systems engineering can be taken as \"Systems-centric\" or \"Domain-centric\".\nBoth of these patterns strive to educate the systems engineer who is able to oversee interdisciplinary projects with the depth required of a core-engineer.\n\nSystems engineering tools are strategies, procedures, and techniques that aid in performing systems engineering on a project or product. The purpose of these tools vary from database management, graphical browsing, simulation, and reasoning, to document production, neutral import/export and more.\n\nThere are many definitions of what a system is in the field of systems engineering. Below are a few authoritative definitions:\n\n\nThe systems engineering process encompasses all creative, manual and technical activities necessary to define the product and which need to be carried out to convert a system definition to a sufficiently detailed system design specification for product manufacture and deployment. Design and development of a system can be divided into four stages, each with different definitions: \n\nDepending on their application, tools are used for various stages of the systems engineering process:\n\nModels play important and diverse roles in systems engineering. A model can be defined in several\nways, including:\nTogether, these definitions are broad enough to encompass physical engineering models used in the verification of a system design, as well as schematic models like a functional flow block diagram and mathematical (i.e., quantitative) models used in the trade study process. This section focuses on the last.\n\nThe main reason for using mathematical models and diagrams in trade studies is to provide estimates of system effectiveness, performance or technical attributes, and cost from a set of known or estimable quantities. Typically, a collection of separate models is needed to provide all of these outcome variables. The heart of any mathematical model is a set of meaningful quantitative relationships among its inputs and outputs. These relationships can be as simple as adding up constituent quantities to obtain a total, or as complex as a set of differential equations describing the trajectory of a spacecraft in a gravitational field. Ideally, the relationships express causality, not just correlation. Furthermore, key to successful systems engineering activities are also the methods with which these models are efficiently and effectively managed and used to simulate the systems. However, diverse domains often present recurring problems of modeling and simulation for systems engineering, and new advancements are aiming to crossfertilize methods among distinct scientific and engineering communities, under the title of 'Modeling & Simulation-based Systems Engineering'.\n\nInitially, when the primary purpose of a systems engineer is to comprehend a complex problem, graphic representations of a system are used to communicate a system's functional and data requirements. Common graphical representations include:\n\nA graphical representation relates the various subsystems or parts of a system through functions, data, or interfaces. Any or each of the above methods are used in an industry based on its requirements. For instance, the N2 chart may be used where interfaces between systems is important. Part of the design phase is to create structural and behavioral models of the system.\n\nOnce the requirements are understood, it is now the responsibility of a systems engineer to refine them, and to determine, along with other engineers, the best technology for a job. At this point starting with a trade study, systems engineering encourages the use of weighted choices to determine the best option. A decision matrix, or Pugh method, is one way (QFD is another) to make this choice while considering all criteria that are important. The trade study in turn informs the design, which again affects graphic representations of the system (without changing the requirements). In an SE process, this stage represents the iterative step that is carried out until a feasible solution is found. A decision matrix is often populated using techniques such as statistical analysis, reliability analysis, system dynamics (feedback control), and optimization methods.\n\nSystems Modeling Language (SysML), a modeling language used for systems engineering applications, supports the specification, analysis, design, verification and validation of a broad range of complex systems.\n\nLifecycle Modeling Language (LML), is an open-standard modeling language designed for systems engineering that supports the full lifecycle: conceptual, utilization, support and retirement stages.\n\nMany related fields may be considered tightly coupled to systems engineering. The following areas have contributed to the development of systems engineering as a distinct entity:\n\n\n\n\n"}
{"id": "55530844", "url": "https://en.wikipedia.org/wiki?curid=55530844", "title": "Tela Botanica", "text": "Tela Botanica\n\nTela Botanica is a collaborative network of francophone botanists, 80% of whom live in France (about 28 000 enrolled at the end of 2016).\nIt served as a model for the network of entomologists Tela Insecta, which is developing in partnership with Tela Botanica.\n\nGiven the growing importance of the issues related to the protection of the planet's resources and the need for their sustainable exploitation, it is important that botany finds a preponderant place at the crossroads of the many disciplines that deal with knowledge of plants and the world vegetal: floristics, systematics, taxonomy, phytosociology, phytogeography, chorology, plant ecology, ethnobotany, etc.\n\nThe Tela Botanica network was created with the aim of supporting this renewal of botany in the French-speaking world.\n\nThe Tela Botanica network was created and is managed by an association of 1901 law: Association Tela Botanica. This association deposited its statutes on 14 December 1999 in the department of Hérault. Its founding members include three legal entities (the Société botanique de France, the Travel Guardian and ACEMAV) and the initiator of the project is Daniel Mathieu.\n\nThe headquarters of the association is located at the Institute of Botany of Montpellier (University Montpellier 2). In four years, the network doubled its number of registrants, whereas it took seven years to reach the ten thousandth registrant. The 20,000th member joined the association on April 22, 2013. By 2014, the network has approximately 24,000 registered members and approximately 13,000 pages of the site are accessed daily.\n\nIts main objectives are:\n\nThe Tela Botanica network is aimed at all persons, whether natural or legal, interested in the knowledge and protection of the plant world, in an ethic of respect for nature, man and his environment.\nIts operation is based on two essential choices:\n\nAll software and applications developed under the network are licensed under CeCILL. The data and documents are mainly distributed under a free Creative Commons license. A close collaboration is established with the french-speaking botanical portal of Wikipedia.\n\nRegistration for the Tela Botanica network is free of charge. It gives the possibility to use the logistical and technical means of the Network to set up and develop projects, to participate in the different groups animated within the network and to receive by e-mail the weekly newsletter of botanical news. Registration is done online from the Internet and a global mapping system can view the location of the 15,000 registered (as of June 8, 2011) of the network in more than 60 countries.\n\nThe network is run by an NGO which also manages the financial management of the project, with 10 employees (end of 2014), thanks to public (47%) and private (39%) contributions, supplemented by benefits and training 10%) or miscellaneous products (4%), and donations from members of the network, to 3% of resources (€18,000 in 2013). An important but unquantified part comes from the volunteering of the members, who have opportunities for expression, collaborative work and proposal in the forums and other advisory tools and databases and photos put in place by the network (eFlore, Online Notebook, newsletter ...). They can be consulted by the steering committee to obtain an opinion on important choices. They are a key element in the dynamics and life of the network.\nTela Botanica publishes a weekly newsletter on botany, which is broadcast free of charge, exclusively via the Internet. This letter as well as the website are freely accessible and devoid of any publicity.\n\n\n"}
{"id": "11441057", "url": "https://en.wikipedia.org/wiki?curid=11441057", "title": "Transglobe Expedition", "text": "Transglobe Expedition\n\nThe Transglobe Expedition was the first expedition to make a circumpolar navigation, traveling the world \"vertically\" traversing both of the poles using only surface transport.\n\nStarting in 1979 from Greenwich in the United Kingdom, adventurers Sir Ranulph Fiennes and Charles R. Burton went south, arriving at the South Pole on 15 December 1980. Over the next 14 months, they went north again, reaching the North Pole on 11 April 1982. Travelling south once more, they arrived again in Greenwich on 29 August 1982.\n\nOliver Shepard took part in the Antarctic leg of the expedition.\n\nAs part of the expedition, Fiennes and Burton completed the Northwest Passage. They left Tuktoyaktuk on 26 July 1981, in the 18 ft open Boston Whaler and reached Tanquary Fiord, 36 days later, on 31 August 1981. Their journey was the first open boat transit of the Northwest Passage from West to East, and covered around , taking a route through Dolphin and Union Strait following the South coast of Victoria and King William Islands, North, via Franklin Strait and Peel Sound, to Resolute Bay (on the southern side of Cornwallis Island), around the South and East coasts of Devon Island, through Hell Gate (near Cardigan Strait) and across Norwegian Bay to Eureka, Greely Bay and the head of Tanquary Fiord.\n\nBetween Tuktoyaktuk and Tanquary Fiord they travelled at an average speed of around per day.\n\nOnce they reached Tanquary Fiord they had to trek overland, via Lake Hazen, to Alert, before setting up their winter base camp.\n\n"}
{"id": "26972512", "url": "https://en.wikipedia.org/wiki?curid=26972512", "title": "Trees for the Future", "text": "Trees for the Future\n\nTrees for the Future is a Maryland-based nonprofit organization founded in 1989 that helps communities around the world plant trees. Through seed distribution, agroforestry training, and in-country technical assistance, it has empowered rural groups to restore tree cover to their lands, protect the environment and help to preserve traditional livelihoods and cultures for generations.\n\nStarted in 1989 by Grace and Dave Deppner, Trees for the Future works with communities in Central America, South America, Africa and Asia to incorporate tree planting into their agricultural activities.\n\nA 501(c)(3) nonprofit based in Silver Spring, Maryland, Trees for the Future also offers individuals and businesses a form of carbon offset through planting trees.\n\nIts programs help communities replenish their natural resources by providing materials and training to allow farmers to sustainably grow crops for food, fodder, and fuelwood.\n\nTrees for the Future has worked on reforestation efforts in Haiti since 2002. It is currently planting fast-growing Moringa trees which can help restore degraded farmlands. Trees for the Future's Haiti Coordinator, Timote Georges, was featured on Discovery Channel about the organization's work in Haiti working on agroforestry projects to restore degraded land throughout the country.\n\nIn 2008, Trees for the Future helped over 200 farmers plant approximately 250,000 forest and fruit trees in Léogâne, the epicenter of the earthquake, and worked with other rural communities. In 2009, due to the organization's increasing network of people on the ground and organizations supporting its work, the Trees for the Future's program had even more success. The organization's Haiti Coordinator, Timote Georges, continues to work with communities along the Arcadine coast to plant trees.\n\nThe organization to date has planted over 65 million trees worldwide in 30 countries and has served over 11,000 villages around the world. Trees for the Future provides free distance and agroforestry training and education; works in conjunction with over 53 specialists who are experts in agroforestry, community development, sustainable agriculture, land use, livestock management, women in development and youth education; provides in-country seed distribution, and; works on natural resource management.\n\nTrees for the Future created a documentary about its work planting trees called \"50 Million Trees and Counting: Trees for the Future\".\n\nIn February 2010, Maryland Senators Richard Madaleno and Brian Frosh, and Maryland Delegate Al Carr announced Senate resolutions recognizing the 20th anniversary of Trees for the Future and the organization's two decades of global activity restoring degraded lands and cutting carbon emissions through the planting of more than 65 million trees.\n\nTrees for the Future partnered with SodaStream International in 2012 to launch the Replant Our Planet initiative. Sodastream committed to planting ten trees in Brazil for each home beverage carbonation system sold from its Rethink Your Soda product line.\n\nAs of 2017, Trees for the Future is working on 14 projects in Sub-Saharan Africa. The group states that their main goal currently is to create \"Forest Garden Programs\" in Cameroon, Kenya, Senegal, Uganda, and Tanzania.\n\n\n"}
{"id": "34890518", "url": "https://en.wikipedia.org/wiki?curid=34890518", "title": "Windwatt", "text": "Windwatt\n\nA windwatt is a mudflat exposed as a result of wind action on water. They occur especially in the Western Pomerania Lagoon Area National Park on Germany's Baltic Sea coast. The term is German.\n\nUnlike the Wadden Sea along Europe's North Sea coast, the shallow water zones of the Western Pomerania Lagoon Area National Park are largely unaffected by oceanic tides. When there are strong winds in a certain direction, however, water is driven out of the lagoons (the so-called \"bodden\") into the Baltic Sea, so that several particularly shallow areas of mud become exposed and dry out. The water flows back when the wind turns again. \nThese \"windwatts\" are a major source of food for migrating birds in the autumn. For the Crane, which cross Western Pomeranian bodden country during migration, the \"windwatts\" are one of the most important resting areas in Western Europe.\n"}
