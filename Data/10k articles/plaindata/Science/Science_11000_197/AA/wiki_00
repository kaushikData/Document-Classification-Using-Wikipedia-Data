{"id": "2883760", "url": "https://en.wikipedia.org/wiki?curid=2883760", "title": "21 grams experiment", "text": "21 grams experiment\n\nThe 21 grams experiment refers to a scientific study published in 1907 by Duncan MacDougall, a physician from Haverhill, Massachusetts. MacDougall hypothesized that souls have physical weight, and attempted to measure the mass lost by a human when the soul departed the body. MacDougall attempted to measure the mass change of six patients at the moment of death. One of the six subjects lost three-fourths of an ounce (21.3 grams).\n\nMacDougall stated his experiment would have to be repeated many times before any conclusion could be obtained. The experiment is widely regarded as flawed and unscientific due to the small sample size, the methods used, as well as the fact only one of the six subjects met the hypothesis. The case has been cited as an example of selective reporting. Despite its rejection within the scientific community, MacDougall's experiment popularized the concept that the soul has weight, and specifically that it weighs 21 grams.\n\nIn 1901, Duncan MacDougall, a physician from Haverhill, Massachusetts, who wished to scientifically determine if a soul had weight, identified six patients in nursing homes whose death was imminent. Four were suffering from tuberculosis, one from diabetes, and one from unspecified causes. MacDougall specifically chose people who were suffering from conditions that caused physical exhaustion, as he needed the patients to remain still when they died to measure them accurately. When the patients looked like they were close to death, their entire bed was placed on an industrial sized scale that was sensitive within two tenths of an ounce (5.6 grams). On the belief that humans have souls and that animals do not, MacDougall later measured the changes in weight from fifteen dogs after death. MacDougall said he wished to use dogs that were sick or dying for his experiment, though was unable to find any. It is therefore presumed he poisoned healthy dogs.\n\nOne of the patients lost weight but then put the weight back on, and two of the other patients registered a loss of weight at death but a few minutes later lost even more weight. One of the patients lost \"three-fourths of an ounce\" (21.3 grams) in weight, coinciding with the time of death. MacDougall disregarded the results of another patient on the grounds the scales were \"not finely adjusted\", and discounted the results of another as the patient died while the equipment was still being calibrated. MacDougall reported that none of the dogs lost any weight after death.\n\nWhile MacDougall believed that the results from his experiment showed the human soul might have weight, his report, which was not published until 1907, stated the experiment would have to be repeated many times before any conclusion could be obtained.\n\nBefore MacDougall was able to publish the results of his experiments, \"The New York Times\" broke the story in an article titled \"Soul has Weight, Physician Thinks\". MacDougall's results were published in April of the same year in the \"Journal of the American Society for Psychical Research\", and the medical journal \"American Medicine\".\n\nFollowing the publication of the experiment in \"American Medicine\", physician Augustus P. Clarke criticized the experiment's validity. Clarke noted that at the time of death there is a sudden rise in body temperature as the lungs are no longer cooling blood, causing a subsequent rise in sweating which could easily account for MacDougall’s missing 21 grams. Clarke also pointed out that, as dogs do not have sweat glands, they would not lose weight in this manner after death. Clarke's criticism was published in the May issue of \"American Medicine\". Arguments between MacDougall and Clarke debating the validity of the experiment continued to be published in the journal until at least December that year.\n\nMacDougall's experiment has been the subject of considerable skepticism, and he has been accused of both flawed methods and outright fraud in obtaining his results. Noting that only one of the six patients measured supported the hypothesis, Karl Kruszelnicki has stated the experiment is a case of selective reporting, as MacDougall ignored the majority of the results. Kruszelnicki also criticized the small sample size, and questioned how MacDougall was able to determine the exact moment when a person had died considering the technology available at the time. Physicist Robert L. Park has written that MacDougall's experiments \"are not regarded today as having any scientific merit\", and psychologist Bruce Hood wrote that \"because the weight loss was not reliable or replicable, his findings were unscientific\". Professor Richard Wiseman said that within the scientific community, the experiment is confined to a \"large pile of scientific curiosities labelled 'almost certainly not true'\".\n\nAn article by Snopes in 2013 said the experiment was flawed because the methods used were suspect, the sample size was much too small, and the capability to measure weight changes too imprecise, concluding: \"credence should not be given to the idea his experiments proved something, let alone that they measured the weight of the soul as 21 grams.\" The fact that MacDougall likely poisoned and killed fifteen healthy dogs in an attempt to support his research has also been a source of criticism.\n\nIn 1911 \"The New York Times\" reported that MacDougall was hoping to run experiments to take photos of souls, but he appears to not have continued any further research into the area and died in 1920. His experiment has not been repeated.\n\nDespite its rejection within the scientific community, MacDougall's experiment popularized the idea that the soul has weight, and specifically that it weighs 21 grams. Most notably, '21 Grams' was taken as the title of a film in 2003, which references the experiment.\n\nThe concept of a soul weighing 21 grams is mentioned in numerous media, including a 2013 issue of the manga \"Gantz\", a 2013 podcast of \"Welcome to Night Vale\" and the 2015 film \"The Empire of Corpses\". The 2015 song \"21 Grams\" by Niykee Heaton features the line \"I just want your soul in my hands, feel your weight of 21 grams\"; a song of the same name was released in 2017 by the Thundamentals, and Travis Scott references the concept in the song \"No Bystanders\", released in 2018. MacDougall and his experiments are explicitly mentioned in the 1978 documentary film \"Beyond and Back\", and episode five of the first season of \"\". A fictional American scientist named \"Mr. MacDougall\" appears in Gail Carriger's 2009 novel \"Soulless\", as an expert in the weight and measurement of souls.\n\n\n"}
{"id": "50387175", "url": "https://en.wikipedia.org/wiki?curid=50387175", "title": "Amanda Reid (malacologist)", "text": "Amanda Reid (malacologist)\n\nAmanda \"Mandy\" Reid is an Australian malacologist who works as a departmental collection manager at the Australian Museum. She is a published researcher and author. Her work in taxonomy has resulted in the description of many species of velvet worms and cephalopods.\n\nReid is a graduate of Macquarie University, where she obtained a science degree in 1984, completed a Masters in 1990 and went on to complete a PhD in 1996. She has a particular interest in cephalopods, including cuttlefish and bobtail squids. In 2000, she co-authored the book \"A Guide to Squid, Cuttlefish and Octopuses of Australasia\" with Mark Norman. Reid has also previously studied velvet worms. Her work has been published in a number of scientific journals, including \"Invertebrate Taxonomy\", \"Bulletin of Marine Science\", \"Australian Natural History\", \"Zootaxa\" and others. Species described by Reid include \"Sepia tanybracheia\", \"Sepia koilados\" (2000) and Ken's cuttlefish (\"Sepia grahami\") (2001). Reid is member of the Society of Editors (NSW) and Sustainable Population Australia.\n"}
{"id": "10571004", "url": "https://en.wikipedia.org/wiki?curid=10571004", "title": "Biological network inference", "text": "Biological network inference\n\nBiological network inference is the process of making inferences and predictions about biological networks.\n\nA network is a set of nodes and a set of directed or undirected edges between the nodes. Many types of biological networks exist, including transcriptional, signalling and metabolic. Few such networks are known in anything approaching their complete structure, even in the simplest bacteria. Still less is known on the parameters governing the behavior of such networks over time, how the networks at different levels in a cell interact, and how to predict the complete state description of a eukaryotic cell or bacterial organism at a given point in the future. Systems biology, in this sense, is still in its infancy.\n\nThere is great interest in network medicine for the modelling biological systems. This article focuses on a necessary prerequisite to dynamic modeling of a network: inference of the topology, that is, prediction of the \"wiring diagram\" of the network. More specifically, we focus here on inference of biological network structure using the growing sets of high-throughput expression data for genes, proteins, and metabolites. Briefly, methods using high-throughput data for inference of regulatory networks rely on searching for patterns of partial correlation or conditional probabilities that indicate causal influence. Such patterns of partial correlations found in the high-throughput data, possibly combined with other supplemental data on the genes or proteins in the proposed networks, or combined with other information on the organism, form the basis upon which such algorithms work. Such algorithms can be of use in inferring the topology of any network where the change in state of one node can affect the state of other nodes.\n\nGenes are the nodes and the edges are directed. A gene serves as the source of a direct regulatory edge to a target gene by producing an RNA or protein molecule that functions as a transcriptional activator or inhibitor of the target gene. If the gene is an activator, then it is the source of a positive regulatory connection; if an inhibitor, then it is the source of a negative regulatory connection. Computational algorithms take as primary input data measurements of mRNA expression levels of the genes under consideration for inclusion in the network, returning an estimate of the network topology. Such algorithms are typically based on linearity, independence or normality assumptions, which must be verified on a case-by-case basis. Clustering or some form of statistical classification is typically employed to perform an initial organization of the high-throughput mRNA expression values derived from microarray experiments, in particular to select sets of genes as candidates for network nodes. The question then arises: how can the clustering or classification results be connected to the underlying biology? Such results can be useful for pattern classification – for example, to classify subtypes of cancer, or to predict differential responses to a drug (pharmacogenomics). But to understand the relationships between the genes, that is, to more precisely define the influence of each gene on the others, the scientist typically attempts to reconstruct the transcriptional regulatory network. This can be done by data integration in dynamic models supported by background literature, or information in public databases, combined with the clustering results. The modelling can be done by a Boolean network, by Ordinary differential equations or Linear regression models, e.g. Least-angle regression, by Bayesian network or based on Information theory approaches. For instance it can be done by the application of a correlation-based inference algorithm, as will be discussed below, an approach which is having increased success as the size of the available microarray sets keeps increasing \n\nSignal transduction networks (very important in the biology of cancer). Proteins are the nodes and directed edges represent interaction in which the biochemical conformation of the child is modified by the action of the parent (e.g. mediated by phosphorylation, ubiquitylation, methylation, etc.). Primary input into the inference algorithm would be data from a set of experiments measuring protein activation / inactivation (e.g., phosphorylation / dephosphorylation) across a set of proteins. Inference for such signalling networks is complicated by the fact that total concentrations of signalling proteins will fluctuate over time due to transcriptional and translational regulation. Such variation can lead to statistical confounding. Accordingly, more sophisticated statistical techniques must be applied to analyse such datasets.\n\nMetabolite networks. Metabolites are the nodes and the edges are directed. Primary input into an algorithm would be data from a set of experiments measuring metabolite levels.\n\nProtein-protein interaction networks are also under very active study. However, reconstruction of these networks does not use correlation-based inference in the sense discussed for the networks already described (interaction does not necessarily imply a change in protein state), and a description of such interaction network reconstruction is left to other articles.\n\n"}
{"id": "29323491", "url": "https://en.wikipedia.org/wiki?curid=29323491", "title": "Böhnecke Glacier", "text": "Böhnecke Glacier\n\nBöhnecke Glacier () is a steep glacier wide, which flows southeast to the northwest side of Violante Inlet, on the east coast of Palmer Land. It was discovered and photographed from the air in December 1940 by members of the United States Antarctic Service. During 1947 the glacier was photographed from the air by members of the Ronne Antarctic Research Expedition under Finn Ronne, who in conjunction with the Falkland Islands Dependencies Survey (FIDS) charted it from the ground. It was named by FIDS for Gunther Böhnecke, a German oceanographer and a member of the German expedition in the \"Meteor\", 1925–27.\n"}
{"id": "58876253", "url": "https://en.wikipedia.org/wiki?curid=58876253", "title": "Cap snatching", "text": "Cap snatching\n\nThe first step of transcription for some RNA viruses is cap snatching, in which the first 10-20 residues of a host cell RNA is removed (snatched) and used as the 5' cap of the nascent viral RNA.\n\nIn the influenza virus, the cap snatching endonuclease function is contained in the PA subunit of the RNA polymerase.\n\nThe United States FDA approved baloxavir marboxil for treatment of acute uncomplicated influenza.\n"}
{"id": "20134130", "url": "https://en.wikipedia.org/wiki?curid=20134130", "title": "Center for Intercultural Dialogue and Translation", "text": "Center for Intercultural Dialogue and Translation\n\nThe Center for Intercultural Dialogue and Translation (CIDT) is an Egyptian organization founded in 2005 to select, review, and translate Arab media publications The reviews are then published in the electronic magazine \"Arab-West Report\".\n\n"}
{"id": "419859", "url": "https://en.wikipedia.org/wiki?curid=419859", "title": "Classical test theory", "text": "Classical test theory\n\nClassical test theory (CTT) is a body of related psychometric theory that predicts outcomes of psychological testing such as the difficulty of items or the ability of test-takers. It is a theory of testing based on the idea that a person’s observed or obtained score on a test is the sum of a true score (error-free score) and an error score. Generally speaking, the aim of classical test theory is to understand and improve the reliability of psychological tests.\n\n\"Classical test theory\" may be regarded as roughly synonymous with \"true score theory\". The term \"classical\" refers not only to the chronology of these models but also contrasts with the more recent psychometric theories, generally referred to collectively as item response theory, which sometimes bear the appellation \"modern\" as in \"modern latent trait theory\".\n\nClassical test theory as we know it today was codified by Novick (1966) and described in classic texts such as Lord & Novick (1968) and Allen & Yen (1979/2002). The description of classical test theory below follows these seminal publications.\n\nClassical test theory was born only after the following three achievements or ideas were conceptualized: one, a recognition of the presence of errors in measurements, two, a conception of that error as a random variable, and third, a conception of correlation and how to index it. In 1904, Charles Spearman was responsible for figuring out how to correct a correlation coefficient for attenuation due to measurement error and how to obtain the index of reliability needed in making the correction. Spearman's finding is thought to be the beginning of Classical Test Theory by some (Traub, 1997). Others who had an influence in the Classical Test Theory's framework include: George Udny Yule, Truman Lee Kelley, those involved in making the Kuder–Richardson Formulas, Louis Guttman, and, most recently, Melvin Novick, not to mention others over the next quarter century after Spearman's initial findings.\n\nClassical test theory assumes that each person has a \"true score\",\"T\", that would be obtained if there were no errors in measurement. A person's true score is defined as the expected number-correct score over an infinite number of independent administrations of the test. Unfortunately, test users never observe a person's true score, only an \"observed score\", \"X\". It is assumed that \"observed score\" = \"true score\" plus some \"error\":\n\nClassical test theory is concerned with the relations between the three variables formula_1, formula_2, and formula_3 in the population. These relations are used to say something about the quality of test scores. In this regard, the most important concept is that of \"reliability\". The reliability of the observed test scores formula_1, which is denoted as formula_5, is defined as the ratio of true score variance formula_6 to the observed score variance formula_7:\n\nBecause the variance of the observed scores can be shown to equal the sum of the variance of true scores and the variance of error scores, this is equivalent to\n\nThis equation, which formulates a signal-to-noise ratio, has intuitive appeal: The reliability of test scores becomes higher as the proportion of error variance in the test scores becomes lower and vice versa. The reliability is equal to the proportion of the variance in the test scores that we could explain if we knew the true scores. The square root of the reliability is the correlation between true and observed scores.\n\nReliability cannot be estimated directly since that would require one to know the true scores, which according to classical test theory is impossible. However, estimates of reliability can be obtained by various means. One way of estimating reliability is by constructing a so-called \"parallel test\". The fundamental property of a parallel test is that it yields the same true score and the same observed score variance as the original test for every individual. If we have parallel tests x and x', then this means that\n\nand\n\nUnder these assumptions, it follows that the correlation between parallel test scores is equal to reliability (see Lord & Novick, 1968, Ch. 2, for a proof).\n\nUsing parallel tests to estimate reliability is cumbersome because parallel tests are very hard to come by. In practice the method is rarely used. Instead, researchers use a measure of internal consistency known as Cronbach's formula_13. Consider a test consisting of formula_14 items formula_15, formula_16. The total test score is defined as the sum of the individual item scores, so that for individual formula_17\n\nThen Cronbach's alpha equals\n\nCronbach's formula_13 can be shown to provide a lower bound for reliability under rather mild assumptions. Thus, the reliability of test scores in a population is always higher than the value of Cronbach's formula_13 in that population. Thus, this method is empirically feasible and, as a result, it is very popular among researchers. Calculation of Cronbach's formula_13 is included in many standard statistical packages such as SPSS and SAS.\n\nAs has been noted above, the entire exercise of classical test theory is done to arrive at a suitable definition of reliability. Reliability is supposed to say something about the general quality of the test scores in question. The general idea is that, the higher reliability is, the better. Classical test theory does not say how high reliability is supposed to be. Too high a value for formula_13, say over .9, indicates redundancy of items. Around .8 is recommended for personality research, while .9+ is desirable for individual high-stakes testing. These 'criteria' are not based on formal arguments, but rather are the result of convention and professional practice. The extent to which they can be mapped to formal principles of statistical inference is unclear.\n\nReliability provides a convenient index of test quality in a single number, reliability. However, it does not provide any information for evaluating single items. Item analysis within the classical approach often relies on two statistics: the P-value (proportion) and the item-total correlation (point-biserial correlation coefficient). The P-value represents the proportion of examinees responding in the keyed direction, and is typically referred to as \"item difficulty\". The item-total correlation provides an index of the discrimination or differentiating power of the item, and is typically referred to as \"item discrimination\". In addition, these statistics are calculated for each response of the oft-used multiple choice item, which are used to evaluate items and diagnose possible issues, such as a confusing distractor. Such valuable analysis is provided by specially-designed psychometric software.\n\nClassical test theory is an influential theory of test scores in the social sciences. In psychometrics, the theory has been superseded by the more sophisticated models in item response theory (IRT) and generalizability theory (G-theory). However, IRT is not included in standard statistical packages like SPSS, but SAS can estimate IRT models via PROC IRT and PROC MCMC and there are IRT packages for the open source statistical programming language R (e.g., CTT). While commercial packages routinely provide estimates of Cronbach's formula_13, specialized psychometric software may be preferred for IRT or G-theory. However, general statistical packages often do not provide a complete classical analysis (Cronbach's formula_13 is only one of many important statistics), and in many cases, specialized software for classical analysis is also necessary.\n\nOne of the most important or well-known shortcomings of classical test theory is that examinee characteristics and test characteristics cannot be separated: each can only be interpreted in the context of the other. Another shortcoming lies in the definition of reliability that exists in classical test theory, which states that reliability is \"the correlation between test scores on parallel forms of a test\". The problem with this is that there are differing opinions of what parallel tests are. Various reliability coefficients provide either lower bound estimates of reliability or reliability estimates with unknown biases. A third shortcoming involves the standard error of measurement. The problem here is that, according to classical test theory, the standard error of measurement is assumed to be the same for all examinees. However, as Hambleton explains in his book, scores on any test are unequally precise measures for examinees of different ability, thus making the assumption of equal errors of measurement for all examinees implausible (Hambleton, Swaminathan, Rogers, 1991, p. 4). A fourth, and final shortcoming of the classical test theory is that it is test oriented, rather than item oriented. In other words, classical test theory cannot help us make predictions of how well an individual or even a group of examinees might do on a test item.\n\n\nhttps://en.wikipedia.org/wiki/Classical_test_theory\n\n\n\n"}
{"id": "6550894", "url": "https://en.wikipedia.org/wiki?curid=6550894", "title": "Corrody", "text": "Corrody\n\nA corrody was a lifetime allowance of food and clothing, and often shelter and care, granted by an abbey, monastery, or other religious house. While rarely granted in the modern era, corrodies were common in the Middle Ages. They were routinely awarded to the servants and staff of royalty, but could also be purchased with donations of money or land. The corrody is one of the earliest forms of insurance, as it provided security in sustenance and lodging in a time when social welfare was scarce. \n\nThe prices paid for corrodies are unknown. Academic estimates of the annual value of the food allowance (alone), are around £3 per year. This assumes an allowance of one loaf of bread and a gallon of ale, but excludes the cost of accommodation and living expenses. When multiplied by the life expectancy of the era (bearing in mind that corrodies would be granted to the old and infirm) it can be assumed that a lifelong corrody for an average person would cost approximately £100.\n\nA. Bell and C. Sutcliffe, Valuing medieval annuities: were corrodies underpriced?, \"Explorations in Economic History\", 47 (2010), 142-57.\n"}
{"id": "1884746", "url": "https://en.wikipedia.org/wiki?curid=1884746", "title": "Cortex (botany)", "text": "Cortex (botany)\n\nA cortex is the outermost layer of a stem or root in a plant.\n\nIn botany, the cortex is the outermost layer of the stem or root of a plant, bounded on the outside by the epidermis and on the inside by the endodermis. In plants, it is composed mostly of differentiated cells, usually large thin-walled parenchyma cells of the ground tissue system. The outer cortical cells often acquire irregularly thickened cell walls, and are called collenchyma cells. Some of the outer cortical cells may contain chloroplasts. It is responsible for the transportation of materials into the central cylinder of the root through diffusion and may also be used for food storage in the form of starch.\n\n"}
{"id": "11722422", "url": "https://en.wikipedia.org/wiki?curid=11722422", "title": "Cresolene", "text": "Cresolene\n\nCresolene is a dark liquid with a pungent smell made from coal tar used in the 19th and early 20th century as a disinfectant and to treat various ailments such as colds and measles. There was also a special 'Vapo-Cresolene' lamp used to heat the substance so that the fumes could be inhaled\n\n\n"}
{"id": "58472849", "url": "https://en.wikipedia.org/wiki?curid=58472849", "title": "Curve of growth", "text": "Curve of growth\n\nIn astronomy, the curve of growth describes the equivalent width of a spectral line as a function of the column density of the material from which the spectral line is observed.\n"}
{"id": "28623021", "url": "https://en.wikipedia.org/wiki?curid=28623021", "title": "Drought deciduous", "text": "Drought deciduous\n\nDrought deciduous plants are those that drop their leaves during the dry season or periods of drought. Examples include plants of the California coastal sage scrub community, and the leeward Hawaiian dry forest tree, the wiliwili (\"Erythrina sandwicensis\"). This may be contrasted to deciduous plants that drop their leaves during cold periods, or evergreen plants that have green leaves year-round.\n"}
{"id": "19636313", "url": "https://en.wikipedia.org/wiki?curid=19636313", "title": "Economic satiation", "text": "Economic satiation\n\nThe economic principle of satiation is the effect whereby the more of a good one possesses the less one is willing to give up in order to get more of it. This effect is caused by diminishing marginal utility, the effect whereby the consumer gains less utility per unit of a product the more units of a product he or she consumes.\n\nFor example, if someone buys a can of cola they will enjoy it. If they then buy a second one they will enjoy it less, and so forth. It can continue to the point where drinking a can of cola becomes a negative experience, and beyond.\n\n"}
{"id": "50220651", "url": "https://en.wikipedia.org/wiki?curid=50220651", "title": "Elise Hofmann", "text": "Elise Hofmann\n\nElise Hofmann (February 5, 1889 – March 14, 1955 ) was an Austrian paleobotanist and geologist. Born in Vienna, she graduated from the University of Vienna in 1920. She produced over 120 works, including the 1934 book \"Palaeohistologie der Pflanze\" (\"Paleohistology of the Plant\"). She was made correspondent of the Geological Survey of Austria in 1931 and the in 1933. \n"}
{"id": "17318804", "url": "https://en.wikipedia.org/wiki?curid=17318804", "title": "Factor 5 (book)", "text": "Factor 5 (book)\n\nFactor 5: Transforming the Global Economy through 80% Increase in Resource Productivity is a 2009 book by Ernst Ulrich von Weizsäcker and an Australian team at The Natural Edge Project. The book suggests that sustainability can be achieved by improving resource productivity. The book presents examples showing the potential of a factor of five in efficiency improvements for some sectors of the economy, while maintaining the quality of service and well-being.\n\nThe book was made possible through sponsorship from Griffith University, Aachen Foundation, CSIRO, and RPS Group. \"Factor 5\" is the sequel to \"Factor 4\", a 1997 international best seller, which presented 50 examples of resource-saving technologies.\n\n\n"}
{"id": "11442", "url": "https://en.wikipedia.org/wiki?curid=11442", "title": "FidoNet", "text": "FidoNet\n\nFidoNet is a worldwide computer network that is used for communication between bulletin board systems (BBSes). It uses a store-and-forward system to exchange private (email) and public (forum) messages between the BBSes in the network, as well as other files and protocols in some cases.\n\nThe FidoNet system was based on a number of small interacting programs. Only one of these interacted with the BBS system directly and was the only portion that had to be ported to support other BBS software. This greatly eased porting, and FidoNet was one of the few networks that was widely supported by almost all BBS software, as well as a number of non-BBS online services. This modular construction also allowed FidoNet to easily upgrade to new data compression systems, which was important in an era using modem-based communications over telephone links with high long-distance calling charges.\n\nThe rapid improvement in modem speeds during the early 1990s, combined with the rapid decrease in price of computer systems and storage, made BBSes increasingly popular. By the mid-1990s there were almost 40,000 FidoNet systems in operation, and it was possible to communicate with millions of users around the world. Only UUCPNET came close in terms of breadth or numbers; FidoNet's user base far surpassed other networks like BITNET.\n\nThe broad availability of low-cost Internet connections starting in the mid-1990s lessened the need for FidoNet's store-and-forward system, as any system in the world could be reached for equal cost. Direct dialing into local BBS systems rapidly declined. Although FidoNet had shrunk considerably since the late 1990s, it has remained in use even today despite internet connectivity becoming universally available.\n\nThere are two major accounts of the development of the FidoNet, differing only in small details.\n\nAround Christmas 1983, Tom Jennings started work on a new MS-DOS–hosted bulletin board system that would emerge as Fido BBS. Jennings set up the system in San Francisco some time in early 1984. Another early user was John Madil, who was trying to set up a similar system in Baltimore on his Rainbow 100. Fido started spreading to new systems, and Jennings eventually started keeping an informal list of their phone numbers, with Jennings becoming #1 and Madil #2.\n\nJennings released the first version of the FidoNet software in June 1984. In early 1985 he wrote a document explaining the operations of the FidoNet, along with a short portion on the history of the system. In this version, FidoNet was developed as a way to exchange mail between the first two Fido BBS systems, Jennings' and Madil's, to \"see if it could be done, merely for the fun of it\". This was first supported in Fido V7, \"sometime in June 84 or so\".\n\nIn early 1984, Ben Baker was planning on starting a BBS for the newly forming computer club at the McDonnell Douglas automotive division in St. Louis. Baker was part of the CP/M special interest group within the club. He intended to use the seminal, CP/M-hosted, CBBS system, and went looking for a machine to run it on. The club's president told Baker that DEC would be giving them a Rainbow 100 computer on indefinite loan, so he made plans to move the CBBS onto this machine. The Rainbow contained two processors, an Intel 8088 and a Zilog Z80, allowing it to run both MS-DOS and CP/M, with the BBS running on the latter. When the machine arrived, they learned that the Z80 side had no access to the I/O ports, so CBBS could not communicate with a modem. While searching for software that would run on the MS-DOS side of the system, Baker learned of Fido through Madil.\n\nThe Fido software required changes to the serial drivers to work properly on the Rainbow. A porting effort started, involving Jennings, Madil and Baker. This caused all involved to rack up considerable long distance charges as they all called each other during development, or called into each other's BBSes to leave email. During one such call \"in May or early June\", Baker and Jennings discussed how great it would be if the BBS systems could call each other automatically, exchanging mail and files between them. This would allow them to compose mail on their local machines, and then deliver it quickly, as opposed to calling in and typing the message in while on a long-distance telephone connection. Jennings responded by calling into Baker's system that night and uploading a new version of the software consisting of three files: FIDO_DECV6 (the new version of the BBS program itself), FIDONET, and NODELIST.BBS. The new version of FIDO BBS had a timer that caused it to exit at a specified time, normally at night, and as it exited it would run the separate FIDONET program. NODELIST was the list of Fido BBS systems, which Jennings had already been compiling.\n\nThe FIDONET program was what later became known as a \"mailer\". FIDO was modified to use a previously unused numeric field in the message headers to store a \"node number\" for the machine the message should be delivered to. When FIDONET ran, it would search through the email database for any messages with a number in this field. FIDONET collected all of the messages for a particular node number into a file known as a \"message packet\". After all the packets were generated, one for each node, the FIDONET program would look up the destination node's phone number in NODELIST.BBS, and call the remote system. Provided that FIDONET was running on that system, the two systems would handshake and, if this succeeded, the calling system would upload its packet, download a return packet if there was one, and disconnect. FIDONET would then unpack the return packet, place the received messages into the local system's storage, and move onto the next packet. When there were no remaining packet, it would exit, and run the FIDO BBS program.\n\nIn order to lower long distance charges, the mail exchanges were timed to run late at night, normally 4 AM. This would later be known as \"national mail hour\", and, later still, as \"Zone Mail Hour\".\n\nBy June 1984 Version 7 of the system was being run in production, and nodes were rapidly being added to the network. By August there were almost 30 systems in the nodelist, 50 by September, and over 160 by January 1985. As the network grew, the maintenance of the nodelist became prohibitive, and errors were common. In these cases people would start receiving phone calls at 4 AM, from a caller that would say nothing and then hang up. In other cases the system would be listed before it was up and running, resulting in long distance calls that accomplished nothing.\n\nIn August 1984 Jennings handed off control of the nodelist to the group in St. Louis, mostly Ken Kaplan and Ben Baker. Kaplan had come across Fido as part of finding a BBS solution for his company, which worked with DEC computers and had been given a Rainbow computer and a USRobotics 1200bit/s modem. From then on, joining FidoNet required one to set up their system and use it to deliver a netmail message to a special system, Node 51. The message contained various required contact information. If this message was transmitted successfully, it ensured that at least some of the system was working properly. The nodelist team would then reply with another netmail message back to the system in question, containing the assigned node number. If delivery succeeded, the system was considered to be working properly, and it was added to the nodelist. The first new nodelist was published on 21 September 1984.\n\nGrowth continued to accelerate, and by the spring of 1985 the system was already reaching its limit of 250 nodes. In addition to the limits on growth of what was clearly a popular system, nodelist maintenance continued to grow more and more time consuming.\n\nIt was also realized that Fido systems were generally clustered – of the 15 systems running by the start of June 1984, 5 of them were in St. Louis. A user on Jennings's system in San Francisco that addressed emails to different systems in St. Louis would cause calls to be made to each of those BBSes in turn. Local calls were normally free or charged at a low rate. Additionally, the initial call setup, generally the first minute of the call, was normally billed at a higher rate than continuing an existing connection. Therefore, it would make sense to deliver all the messages from all the users in San Francisco to all of the users in St. Louis in a single call. Packets were generally small enough to be delivered within a minute or two, so delivering all the messages in a single call could greatly reduce costs by avoiding multiple first-minute charges. Once delivered, the packet would be broken out into separate packets for local systems, and delivered using multiple local free calls.\n\nThe team settled on the concept of adding a new \"network number\" patterned on the idea of area codes. A complete network address would now consist of the network and node number pair, which would be written with a slash between them. All mail travelling between networks would first be sent to their local \"network host\", someone who volunteered to pay for any long distance charges. That single site would collect up all the netmail from all of the systems in their network, then re-package it into single packets destined to each network. They would then call any required network admin sites and deliver the packet to them. That site would then process the mail as normal, although all of the messages in the packet would be guaranteed to be local calls.\n\nThe network address was placed in an unused field in the Fido message database, which formerly always held a zero. Systems running existing versions of the software already ignored the fields containing the new addressing, so they would continue to work as before; when noticing a message addressed to another node they would look it up and call that system. Newer systems would recognize the network number and instead deliver that message to the network host. To ensure backward compatibility, existing systems retained their original node numbers through this period.\n\nA huge advantage of the new scheme was that node numbers were now unique only within their network, not globally. This meant the previous 250 node limit was gone, but for a variety of reasons this was initially limited to about 1,200. This change also devolved the maintenance of the nodelists down to the network hosts, who then sent updated lists back to Node 51 to be collected into the master list. The St. Louis group now had to only maintain their own local network, and do basic work to compile the global list.\n\nAt a meeting held in Kaplan's living room in St. Louis on 11 April 1985 the various parties hammered out all of the details of the new concept. As part of this meeting, they also added the concept of a \"region\", a purely administrative level that was not part of the addressing scheme. Regional hosts would handle any stragglers in the network maps, remote systems that had no local network hosts. They then divided up the US into ten regions that they felt would have roughly equal populations.\n\nBy May, Jennings had early versions of the new software running. These early versions specified the routing manually through a new ROUTE.BBS file that listed network hosts for each node. For instance, an operator might want to forward all mail to St. Louis through a single node, node 10. ROUTE.BBS would then include a list of all the known systems in that area, with instructions to forward mail to each of those nodes through node 10. This process was later semi-automated by John Warren's NODELIST program. Over time, this information was folded into updated versions of the nodelist format, and the ROUTES file is no longer used.\n\nA new version of FIDO and FIDONET, 10C, was released containing all of these features. On 12 June 1985 the core group brought up 10C, and most Fido systems had upgraded within a few months. The process went much smoother than anyone imagined, and very few nodes had any problems.\n\nSome time during the evolution of Fido, file attachments were added to the system, allowing a file to be referenced from an email message. During the normal exchange between two instances of FIDONET, any files attached to the messages in the packets were delivered after the packet itself had been up or downloaded. It is not clear when this was added, but it was already a feature of the basic system when the 8 February 1985 version of the FidoNet standards document was released, so this was added very early in Fido's history.\n\nAt a sysop meeting in Dallas, the idea was raised that it would be nice if there was some way for the sysops to post messages that would be shared among the systems. In February 1986 Jeff Rush, one of the group members, introduced a new mailer that extracted messages from public forums that the sysop selected, in a manner similar to the way the original mailer handled private messages. The new program was known as a \"tosser/scanner\". The tosser produced a file that was similar (or identical) to the output from the normal netmail scan, however, these files were then compressed and attached to a normal netmail message as an attachment. This message was then sent to a special address on the remote system. After receiving netmail as normal, the scanner on the remote system looked for these messages, unpacked them, and put them into the same public forum on the original system.\n\nIn this fashion, Rush's system implemented a store and forward public message system similar to Usenet, but based on, and hosted by, the FidoNet system. The first such \"echomail\" forum was one created by the Dallas area sysops to discuss business, known as SYSOP. Another called TECH soon followed. Several public \"echos\" soon followed, including GAYNET and CLANG. These spawned hundreds of new echos, and led to the creation of the Echomail Conference List (Echolist) by Thomas Kenny in January 1987. Echomail produced world-spanning shared forums, and its traffic volume quickly surpassed the original netmail system. By the early 1990s, echo mail was carrying over 8 MB of compressed message traffic a day, many times that when uncompressed.\n\nEchomail did not necessarily use the same distribution pathways as normal netmail, and the distribution routing was stored in a separate setup file not unlike the original ROUTES.BBS. At the originating site a header line was added to the message indicating the origin system's name and address. After that, each system that the message traveled through added itself to a growing PATH header, as well as a SEENBY header. SEENBY prevented the message from looping around the network in the case of mis-configured routing information.\n\nEchomail was not the only system to use the file attachment feature of netmail to implement store-and-forward capabilities. Similar concepts were used by online games and other systems as well.\n\nThe evolution towards the net/node addressing scheme was also useful for reducing communications costs between continents, where timezone differences on either end of the connection might also come into play. For instance, the best time to forward mail in the US was at night, but that might not be the best time for European hosts to exchange. Efforts towards introducing a continental level to the addressing system started in 1986.\n\nAt the same time, it was noted that some power users were interested in using FidoNet protocols as a way of delivering the large quantities of echomail to their local machines where it could be read offline. These users did not want their systems to appear in the nodelist - they did not (necessarily) run a bulletin board system and were not publicly accessible. A mechanism allowing netmail delivery to these systems without the overhead of nodelist maintenance was desirable.\n\nIn October 1986 the last major change to the FidoNet network was released, adding \"zones\" and \"points\". Zones represented major geographical areas roughly corresponding to continents. There were six zones in total, North America, South America, Europe, Oceania, Asia, and Africa. Points represented non-public nodes, which were created privately on a BBS system. Point mail was delivered to a selected host BBS as normal, but then re-packaged into a packet for the point to pick up on-demand. The complete addressing format was now codice_1, so a real example might be codice_2. Points were widely used only for a short time, the introduction of offline reader systems filled this role with systems that were much easier to use. Points remain in use to this day, but are less popular than when they were introduced.\n\nAlthough FidoNet supported file attachments from even the earliest standards, this feature tended to be rarely used and was often turned off. File attachments followed the normal mail routing through multiple systems, and could back up transfers all along the line as the files were copied. A solution was offered in the form of \"file requests\", which made file transfers driven by the \"calling\" system and used one-time point-to-point connections instead of the traditional routing. Two such standards became common, \"WaZOO\" and \"Bark\", which saw varying support among different mailers. Both worked in a similar fashion, with the mailer calling the remote system and sending a new handshake packet to request the files.\n\nAlthough FidoNet was, by far, the best known BBS-based network, it was by no means the only one. From 1988 on, PCBoard systems were able to host similar functionality known as RelayNet, while other popular networks included RBBSNet from the Commodore 64 world, and AlterNet. Late in the evolution of the FidoNet system, there was a proposal to allow mail (but not forum messages) from these systems to switch into the FidoNet structure. This was not adopted, and the rapid rise of the internet made this superfluous as these networks rapidly added internet exchange, which acted as a lingua franca.\n\nFidoNet started in 1984 and listed 100 nodes by the end of that year. Steady growth continued through the 1980s, but a combination of factors led to rapid growth after 1988. These included faster and less expensive modems, and rapidly declining costs of hard drives and computer systems in general. By April 1993 the FidoNet nodelist contained over 20,000 systems. At that time it was estimated that each node had, on average, about 200 active users. Of these 4 million users in total, 2 million users commonly used echomail, the shared public forums, while about 200,000 used the private netmail system. At its peak, FidoNet listed approximately 39,000 systems.\n\nThroughout its lifetime, FidoNet was beset with management problems and infighting. Much of this can be traced to the fact that the inter-net delivery cost real money, and the traffic grew more rapidly than decreases caused by improving modem speeds and downward trending long distance rates. As they increased, various methods of recouping the costs were attempted, all of which caused friction in the groups. The problems were so bad that Jennings came to refer to the system as the \"fight-o-net\".\n\nAs modems reached speeds of 28.8 kbit/s, the overhead of the TCP/IP protocols were no longer so egregious and dial-up Internet became increasingly common. By 1995 the bulletin board market was reeling as users abandoned local BBS systems in favour of larger sites and web pages, which could be accessed worldwide for the same cost as accessing a local BBS system. This also made FidoNet less expensive to implement, because inter-net transfers could be delivered over the Internet as well, at little or no marginal cost. But this seriously diluted the entire purpose of the store-and-forward model, which had been built up specifically to address a long-distance problem that no longer existed.\n\nThe FidoNet nodelist started shrinking, especially in areas with widespread availability of internet connections. This downward trend continues, but has levelled out at approximately 2,500 nodes. FidoNet remains popular in areas where Internet access is difficult to come by, or expensive.\n\nThere is now (~2014) a retro movement which is resulting in a slow increase in internet connected BBS and nodes. Telnet, Rlogin, and SSH are being used between systems. This means you can telnet to many BBS worldwide as cheaply as ones next door. Also Usenet and internet mail has been added, along with long file names to many newer versions of BBS software, some being free-ware, resulting in increasing use. The deaf and blind are also able to access this better than the internet as a whole as interfaces for them deal mostly with ASCII text which exists in most BBS. They find it helps them communicate without the complications of pictures and audio in their internet mail and communication in general. Nodelists are no longer declining in all cases.\n\nFidoNet is governed in a hierarchical structure according to FidoNet policy, with designated coordinators at each level to manage the administration of FidoNet nodes and resolve disputes between members. Network coordinators are responsible for managing the individual nodes within their area, usually a city or similar sized area. Regional coordinators are responsible for managing the administration of the network coordinators within their region, typically the size of a state, or small country. Zone coordinators are responsible for managing the administration of all of the regions within their zone. The world is divided into six zones, the coordinators of which elect one of themselves to be the \"International Coordinator\" of FidoNet.\n\nFidoNet was historically designed to use modem-based dial-up (POTS) access between bulletin board systems, and much of its policy and structure reflected this.\n\nThe FidoNet system officially referred only to transfer of \"Netmail\"—the individual private messages between people using bulletin boards—including the protocols and standards with which to support it. A netmail message would contain the name of the person sending, the name of the intended recipient, and the respective FidoNet addresses of each. The FidoNet system was responsible for routing the message from one system to the other (details below), with the bulletin board software on each end being responsible for ensuring that only the intended recipient could read it. Due to the hobbyist nature of the network, any privacy between sender and recipient was only the result of politeness from the owners of the FidoNet systems involved in the mail's transfer. It was common, however, for system operators to reserve the right to review the content of mail that passed through their system.\n\nNetmail allowed for the \"attachment\" of a single file to every message. This led to a series of \"piggyback\" protocols that built additional features onto FidoNet by passing information back and forth as file attachments. These included the automated distribution of files, and transmission of data for inter-BBS games.\n\nBy far the most commonly used of these piggyback protocols was \"Echomail\", public discussions similar to Usenet newsgroups in nature. Echomail was supported by a variety of software that collected up new messages from the local BBSes' public forums (the \"scanner\"), compressed it using ARC or ZIP, attached the resulting archive to a Netmail message, and sent that message to a selected system. On receiving such a message, identified because it was addressed to a particular \"user\", the reverse process was used to extract the messages, and a \"tosser\" put them back into the new system's forums.\n\nEchomail was so popular that for many users, Echomail \"was\" the FidoNet. Private person-to-person Netmail was relatively rare.\n\nFidoNet is politically organized into a tree structure, with different parts of the tree electing their respective coordinators. The FidoNet hierarchy consists of \"zones\", \"regions\", \"networks\", \"nodes\" and \"points\" broken down more-or-less geographically.\n\nThe highest level is the zone, which is largely continent-based:\n\nEach zone is broken down into regions, which are broken down into nets, which consist of individual nodes. Zones 7-4095 are used for \"othernets\"; groupings of nodes which use Fido-compatible software to carry their own independent message areas without being in any way controlled by FidoNet's political structure. Using un-used zone numbers would ensure that each network would have a unique set of addresses, avoiding potential routing conflicts and ambiguities for systems that belonged to more than one network.\n\nFidoNet addresses explicitly consist of a \"zone\" number, a \"network\" number (or region number), and a \"node\" number. They are written in the form Zone:Network/Node. The FidoNet structure also allows for semantic designation of region, host, and hub status for particular nodes, but this status is not directly indicated by the main address.\n\nFor example, consider a node located in Tulsa, Oklahoma, United States with an assigned node number is 918, located in Zone 1 (North America), Region 19, and Network 170. The full FidoNet address for this system would be 1:170/918. The \"region\" was used for administrative purposes, and was only part of the address if the node was listed directly underneath the Regional Coordinator, rather than one of the networks that were used to divide the region further.\n\nFidoNet policy requires that each FidoNet system maintain a \"nodelist\" of every other member system. Information on each node includes the name of the system or BBS, the name of the node operator, the geographic location, the telephone number, and software capabilities. The nodelist is updated weekly, to avoid unwanted calls to nodes that had shut down, with their phone numbers possibly having been reassigned for voice use by the respective telephone company.\n\nTo accomplish regular updates, coordinators of each network maintain the list of systems in their local areas. The lists are forwarded back to the International Coordinator via automated systems on a regular basis. The International Coordinator would then compile a new nodelist, and generate the list of changes (nodediff) to be distributed for node operators to apply to their existing nodelist.\n\nIn a theoretical situation, a node would normally forward messages to a \"hub\". The hub, acting as a distribution point for mail, might then send the message to the Net Coordinator. From there it may be sent through a Regional Coordinator, or to some other system specifically set up for the function. Mail to other zones might be sent through a Zone Gate.\n\nFor example, a FidoNet message might follow the path:\n\nOriginally there was no specific relationship between network numbers and the regions they reside in. In some areas of FidoNet, most notably in Zone 2, the relationship between region number and network number are entwined. For example, 2:201/329 is in Net 201 which is in Region 20 while 2:2410/330 is in Net 2410 which is in Region 24. Zone 2 also relates the node number to the hub number if the network is large enough to contain any hubs. This effect may be seen in the nodelist by looking at the structure of Net 2410 where node 2:2410/330 is listed under Hub 300. This is not the case in other zones.\n\nIn Zone 1, things are much different. Zone 1 was the starting point and when Zones and Regions were formed, the existing nets were divided up regionally with no set formula. The only consideration taken was where they were located geographically in respect to the region's mapped outline. As net numbers got added, the following formula was used.\nRegion number × 20\n\nThen when some regions started running out of network numbers, the following was also used.\n\nRegion number × 200\n\nRegion 19, for instance, contains nets 380-399 and 3800-3999 in addition to those that were in Region 19 when it was formed.\n\nPart of the objective behind the formation of local nets was to implement cost reduction plans by which all messages would be sent to one or more hubs or hosts in compressed form (ARC was nominally standard, but PKZIP is universally supported); one toll call could then be made during off-peak hours to exchange entire message-filled archives with an out-of-town uplink for further redistribution.\n\nIn practice, the FidoNet structure allows for any node to connect directly to any other, and node operators would sometimes form their own toll-calling arrangements on an ad-hoc basis, allowing for a balance between collective cost saving and timely delivery. For instance, if one node operator in a network offered to make regular toll calls to a particular system elsewhere, other operators might arrange to forward all of their mail destined for the remote system, and those near it, to the local volunteer. Operators within individual networks would sometimes have cost-sharing arrangements, but it was also common for people to volunteer to pay for regular toll calls either out of generosity, or to build their status in the community.\n\nThis ad-hoc system was particularly popular with networks that were built on top of FidoNet. Echomail, for instance, often involved relatively large file transfers due to its popularity. If official FidoNet distributors refused to transfer Echomail due to additional toll charges, other node operators would sometimes volunteer. In such cases, Echomail messages would be routed to the volunteers' systems instead.\n\nThe FidoNet system was best adapted to an environment in which local telephone service was inexpensive and long-distance calls (or intercity data transfer via packet-switched networks) costly. Therefore, it fared somewhat poorly in Japan, where even local lines are expensive, or in France, where tolls on local calls and competition with Minitel or other data networks limited its growth.\n\nAs the number of messages in Echomail grew over time, it became very difficult for users to keep up with the volume while logged into their local BBS. \"Points\" were introduced to address this, allowing technically savvy users to receive the already compressed and batched Echomail (and Netmail) and read it locally on their own machines. \n\nTo do this, the FidoNet addressing scheme was extended with the addition of a final address segment, the point number. For instance, a user on the example system above might be given point number 10, and thus could be sent mail at the address 1:170/918.10.\n\nIn real-world use, points are fairly difficult to set up. The FidoNet software typically consisted of a number of small utility programs run by manually edited scripts that required some level of technical ability. Reading and editing the mail required either a \"sysop editor\" program, or a BBS program to be run locally.\n\nIn North America (Zone 1), where local calls are generally free, the benefits of the system were offset by its complexity. Points were used only briefly, and even then only to a limited degree. Dedicated offline mail reader programs such as Blue Wave, Squiggy and Silver Xpress (OPX) were introduced in the mid-1990s, and quickly rendered the point system obsolete. Many of these packages supported the QWK offline mail standard.\n\nIn other parts of the world, especially Europe, this was different. In Europe, even local calls are generally metered, so there was a strong incentive to keep the duration of the calls as short as possible. Point software employs standard compression (ZIP, ARJ, etc.) and so keeps the calls down to a few minutes a day at most. In contrast to North America, pointing saw rapid and fairly widespread uptake in Europe.\n\nMany regions distribute a pointlist in parallel with the nodelist. The pointlist segments are maintained by Net- and Region Pointlist Keepers and the Zone Point List Keeper assembles them into the Zone pointlist. At the peak of FidoNet there were over 120,000 points listed in the Zone 2 pointlist. Listing points is on a voluntary basis and not every point is listed, so how many points there really were is anybody's guess. As of June 2006, there are still some 50,000 listed points. Most of them are in Russia and Ukraine.\n\nFidoNet contained several technical specifications for compatibility between systems. The most basic of all is \"FTS-0001\", with which all FidoNet systems are required to comply as a minimal requirement. FTS-0001 defined:\n\nOther specifications that were commonly used provided for \"echomail\", different transfer protocols and handshake methods (\"e.g.: Yoohoo/Yoohoo2u2, EMSI\"), file compression, nodelist format, transfer over reliable connections such as the Internet (Binkp), and other aspects.\n\nSince computer bulletin boards historically used the same telephone lines for transferring mail as were used for dial-in human users of the BBS, FidoNet policy dictates that at least one designated line of each FidoNet node must be available for accepting mail from other FidoNet nodes during a particular hour of each day.\n\n\"Zone Mail Hour\", as it was named, varies depending on the geographic location of the node, and was designated to occur during the early morning. The exact hour varies depending on the time zone, and any node with only one telephone line is required to reject human callers. In practice, particularly in later times, most FidoNet systems tend to accept mail at any time of day when the phone line is not busy, usually during night.\n\nMost FidoNet deployments were designed in a modular fashion. A typical deployment would involve several applications that would communicate through shared files and directories, and switch between each other through carefully designed scripts or batch files. However, monolithic software that encompassed all required functions in one package is available, such as D'Bridge. Such software eliminated the need for custom batch files and is tightly integrated in operation. The preference of deployment was that of the operator and there were both pros and cons of running in either fashion.\n\nArguably the most important piece of software on a DOS-based Fido system was the \"FOSSIL driver\", which was a small device driver which provided a standard way for the Fido software to talk to the modem. This driver needed to be loaded before any Fido software would work. An efficient FOSSIL driver meant faster, more reliable connections.\n\n\"Mailer software\" was responsible for transferring files and messages between systems, as well as passing control to other applications, such as the BBS software, at appropriate times. The mailer would initially answer the phone and, if necessary, deal with incoming mail via FidoNet transfer protocols. If the mailer answered the phone and a human caller was detected rather than other mailer software, the mailer would exit, and pass control to the BBS software, which would then initialise for interaction with the user. When outgoing mail was waiting on the local system, the mailer software would attempt to send it from time to time by dialing and connecting to other systems who would accept and route the mail further. Due to the costs of toll calls which often varied between peak and off-peak times, mailer software would usually allow its operator to configure the optimal times in which to attempt to send mail to other systems.\n\n\"BBS software\" was used to interact with human callers to the system. BBS software would allow dial-in users to use the system's message bases and write mail to others, locally or on other BBSes. Mail directed to other BBSes would later be routed and sent by the mailer, usually after the user had finished using the system. Many BBSes also allowed users to exchange files, play games, and interact with other users in a variety of ways (i.e.: node to node chat).\n\nA \"scanner/tosser\" application, such as FastEcho, FMail, TosScan and Squish, would normally be invoked when a BBS user had entered a new FidoNet message that needed to be sent, or when a mailer had received new mail to be imported into the local messages bases. This application would be responsible for handling the packaging of incoming and outgoing mail, moving it between the local system's message bases and the mailer's inbound and outbound directories. The scanner/tosser application would generally be responsible for basic routing information, determining which systems to forward mail to.\n\nIn later times, \"message readers\" or \"editors\" that were independent of BBS software were also developed. Often the System Operator of a particular BBS would use a devoted message reader, rather than the BBS software itself, to read and write FidoNet and related messages. One of the most popular editors in 2008 was GoldED+. In some cases FidoNet nodes, or more often FidoNet points, had no public bulletin board attached, and existed only for the transfer of mail for the benefit of the node's operator. Most nodes in 2009 had no BBS access, but only points, if anything.\n\nThe original \"Fido BBS\" software, and some other FidoNet-supporting software from the 1980s, is no longer functional on modern systems. This is for several reasons, including problems related to the Y2K bug. In some cases, the original authors have left the BBS or shareware community, and the software, much of which was closed source, has been rendered abandonware.\n\nSeveral DOS based legacy FidoNet Mailers such as FrontDoor, Intermail, MainDoor and D'Bridge from the early 1990s can still be run today under Windows without a modem, by using the freeware NetFoss Telnet FOSSIL driver, and by using a Virtual Modem such as NetSerial. This allows the mailer to \"dial\" an IP address or hostname via Telnet, rather than dialing a real POTS phone number. There are similar solutions for Linux such as MODEMU (modem emulator) which has limited success when combined with DOSEMU (DOS emulator).\nMail Tossers such as FastEcho and FMail are still used today under both Windows and Linux/DOSEMU.\nThere are several modern Windows based FidoNet Mailers available today with source code, including Argus, Radius, and Taurus. MainDoor is another Windows based Fidonet mailer, which also can be run using either a modem or directly over TCP/IP. Two popular free and open source software FidoNet mailers for Unix-like systems are the binkd (cross-platform, IP-only, uses the binkp protocol) and qico (supports modem communication as well as the IP protocol of ifcico and binkp).\n\nOn the \"hardware\" side, Fido systems were usually well-equipped machines, for their day, with quick CPUs, high-speed modems and 16550 UARTs, which were at the time an upgrade. As a Fidonet system was usually a BBS, it needed to quickly process any new mail events before returning to its 'waiting for call' state. In addition, the BBS itself usually necessitated lots of storage space. Finally, a FidoNet system usually had at least one dedicated phoneline. Consequently, operating a Fidonet system often required significant financial investment, a cost usually met by the owner of the system.\n\nWhile the use of FidoNet has dropped dramatically compared with its use up to the mid-1990s, it is still used in many countries and especially Russia and former republics of the USSR. Some BBSes, including those that are now available for users with Internet connections via telnet, also retain their FidoNet netmail and echomail feeds.\n\nSome of FidoNet's echomail conferences are available via gateways with the Usenet news hierarchy using software like UFGate. There are also mail gates for exchanging messages between Internet and FidoNet. Widespread net abuse and e-mail spam on the Internet side has caused some gateways (such as the former 1:1/31 IEEE fidonet.org gateway) to become unusable or cease operation entirely.\n\n\"FidoNews\" is the newsletter of the FidoNet community. Affectionately nicknamed \"The Snooze\", it is published weekly. It was first published in 1984. Throughout its history, it has been published by various people and entities, including the short-lived International FidoNet Association.\n\n\n"}
{"id": "41827467", "url": "https://en.wikipedia.org/wiki?curid=41827467", "title": "Foodomics", "text": "Foodomics\n\nFoodomics was defined in 2009 as \"a discipline that studies the Food and Nutrition domains through the application and integration of advanced -omics technologies to improve consumer's well-being, health, and knowledge\". Foodomics requires the combination of food chemistry, biological sciences, and data analysis.\n\nThe study of foodomics became under the spotlight after it was introduced in the first international conference in 2009 at Cesena, Italy. Many experts in the field of omics and nutrition were invited to this event in order to find the new approach and possibility in the area of food science and technology. However, research and development of foodomics today are still limited due to high throughput analysis required. The American Chemical Society journal called Analytical Chemistry dedicated its cover to foodomics in December 2012.\n\nFoodomics involves four main areas of omics:\n\nFoodomics greatly helps the scientists in an area of food science and nutrition to gain a better access to data, which is used to analyze the effects of food on human health, etc. It is believed to be another step towards better understanding of development and application of technology and food. Moreover, the study of foodomics leads to other omics sub-disciplines, including nutrigenomics which is the integration of the study of nutrition, gene and omics.\n\nFoodomics approach is used to analyze and establish the links between several substances presented in rosemary and the ability to cure colon cancer cells. There are thousands of chemical compounds in rosemary, but the ones that are able to help cure such disease are Carnosic acid (CA) and Carnosol (CS), which can be obtained by extracting rosemary via SFE. They have the potential to fight against and reduce the proliferation of human HT-29 colon cancer cells.\n\nThe experiment done by inserting rosemary extracts to the mice and collecting RNA and metabolites from each controlled and treated individual indicated that there is a correlation between the compounds used and the percentage of recovery from the cancer. This information is however never achievable without the help of foodomics knowledge as it was used to process data, analyze statistic, and identify biomakers. Foodomics, coupled with transcriptomic data, shows that Carnosic acid leads to the accumulation of an antioxidant, glutothione (GSH). The chemical can be broken down to Cysteinylglycine, a naturally occurring dipeptide and an intermediate in the gamma glutamyl cycle. Moreover, the result from an integration of foodomics, transcriptomics and metabolomics reveals that provoking colon cancer cell compounds, such as N‐acetylputrescine, N‐acetylcadaverine, 5’MTA and γ‐aminobutyric acid, can also be lowered by CA treatment.\n\nThus, foodomics plays an important role in explaining the relationship between deadly disease, like colon cancer, and natural compounds existing in rosemary. Data obtained is useful in reaching another approach for tackling proliferation against cancer cells.\n\nAside from measuring the concentration of protein in meat, calculating bioavailability is another way in determining the total amount of component and quality. The calculation is done when food molecules are digested in various steps. Since human digestion is very complicated, a wide range of analytical techniques are used to obtain the data, including foodomics protocol and an in vitro static simulation of digestion.\n\nThe procedure is divided into 3 stages as the samples are collected from oral, gastric and duodenal digestion in order to study protein digestibility closely and thoroughly. A meat based food, Bresaola, is evaluated because beef muscles are still intact, which can be used to indicate nutritional value.\n\nThe consequences of oral step can be observed at the beginning of the gastric digestion, the first stage. As there is no enzymatic proteolytic activity at this stage, the level of H-NMR, a spectrum used to determine the structure, is still constant because there is no change going on. However, when pepsin takes action, TD-NMR, a special technique used for measuring mobile water population with macromolecular solutes, reveals that progressive unbundling of meat fibers helps pepsin activity to digest. TD-NMR data proves that bolus structure changes considerably during the first part of digestion and water molecules, consequently, leave the spaces inside the myofibrils and fiber bundles. This results in a low level of water that can be detected in duodenal stage. Since digestion is in progress, protein molecules become smaller and molecular weight gets lower, in other words, there is an increase in the spectra total area.\n\n"}
{"id": "30640525", "url": "https://en.wikipedia.org/wiki?curid=30640525", "title": "Human Technology", "text": "Human Technology\n\nHuman Technology is an open-access, peer-reviewed, on-line, international scholarly semiannual journal edited by the Agora Center of the University of Jyväskylä, Finland since 2005.\n\nIts aim is to explore current topics regarding the interaction between people and technology. Peer-reviewed articles in the journal are intended to address the issues and challenges surrounding the role of human interaction with ICT in all areas of society. The journal seeks to edit interdisciplinary research about how applied technology can affect human existence or how it can, for instance, foster personal development and professional competencies changes, and how it can enhance, for example, innovation, e-learning, socio-technical evolutions and communication.\n\n\nProfessor Pertti Saariluoma, founding editor in chief (2005-2011) University of Jyväskylä\n\nProfessor Päivi Häkkinen (2012-2014) University of Jyväskylä\n\n\n\nHuman Technology is listed in the Directory of Open Access Journals (Lund University Libraries), and is cited and/or abstracted in various databases, including: PsycINFO(American Psychological Association).\n\n\n"}
{"id": "41758080", "url": "https://en.wikipedia.org/wiki?curid=41758080", "title": "Identifiers.org", "text": "Identifiers.org\n\nIdentifiers.org is a project providing stable and perennial identifiers for data records used in the Life Sciences. The identifiers are provided in the form of Uniform Resource Identifiers (URIs). Identifiers.org is also a resolving system, that relies on collections listed in the MIRIAM Registry to provide direct access to different instances of the identified records.\n\nThe Identifiers.org URIs are perennial identifiers, that specify at once the data collection, using the namespaces of the Registry, and the record identifier within the collection in the form of a unique resolvable URI. The Identifiers.org resolving system is built upon the information stored in the MIRIAM Registry, which is a database that stores namespaces assigned to commonly used data collections (databases and ontologies) for the Life Sciences. It transforms an Identifiers.org URI into the various URLs leading to the various instances of the record identified by the URI. Identifiers.org is part of the ELIXIR Interoperability Platform.\n\nAn Identifiers.org URI is formed of several parts:\n\n\nThe systems allows a consistent and uniform annotation of datasets. This is turn facilitates data alignment and integration. Identifiers.org URIs are used to encode the metadata in the standard formats of the COMBINE initiative, such as SBML. In particular, databases such as BioModels Database and Reactome export their data in SBML with cross-references encoded using Identifiers.org URIs. These URIs are also used in various semantic web projects such as Bio2RDF, Open PHACTS and the EBI RDF platform\n\nIdentifiers.org URIs have been developed since 2011 as a resolvable version of the MIRIAM identifiers, developed since 2005, which were of a URN form, and not directly resolvable. Identifiers.org URIs are similar to PURLs, albeit providing alternative resolutions for collections with several instances. They are also similar to DOIs, but provide human readable collection names, and re-use the record identifier assigned by the data provider.\n\n\n"}
{"id": "247107", "url": "https://en.wikipedia.org/wiki?curid=247107", "title": "Illustration", "text": "Illustration\n\nAn illustration is a decoration, interpretation or visual explanation of a text, concept or process, designed for integration in published media, such as posters, flyers, magazines, books, teaching materials, animations, video games and films.\n\nThe origin of the word “illustration” is late Middle English (in the sense ‘illumination; spiritual or intellectual enlightenment’): via Old French from Latin \"illustratio(n- )\", from the verb \"illustrate\".\n\nContemporary illustration uses a wide range of styles and techniques, including drawing, painting, printmaking, collage, montage, digital design, multimedia, 3D modelling. Most illustrators work on a freelance basis. \n\nDepending on the purpose, illustration may be expressive, stylised, realistic or highly technical. \n\nSpecialist areas include:\n\nTechnical and scientific illustration communicates information of a technical or scientific nature. This may include exploded views, cutaways, fly-throughs, reconstructions, instructional images, component designs, diagrams. The aim is \"to generate expressive images that effectively convey certain information via the visual channel to the human observer\"\n\nTechnical and scientific illustration is generally designed to describe or explain subjects to a nontechnical audience, so must provide \"an overall impression of what an object is or does, to enhance the viewer's interest and understanding\".\n\nIn contemporary illustration practice, 2D and 3D software is often used to create accurate representations that can be updated easily, and reused in a variety of contexts.\n\nIn the art world, illustration has at times been considered of less importance than graphic design and fine art.\n\nToday, however, due in part to the growth of graphic novel and video game industries, as well as increased use of illustration in magazines and other publications, illustration is now becoming a valued art form, capable of engaging a global market.\n\nOriginal illustration art has been known to attract high prices at auction. The US artist Norman Rockwell's painting \"Breaking Home Ties\" sold in a 2006 Sotheby's auction for USD15.4 million. Many other illustration genres are equally valued, with pinup artists such as Gil Elvgren and Alberto Vargas, for example, also attracting high prices.\n\nHistorically, the art of illustration is closely linked to the industrial processes of printing and publishing.\n\nThe illustrations of medieval codices were known as illuminations, and were individually hand drawn and painted. With the invention of the printing press during the 15th century, books became more widely distributed, often illustrated with woodcuts.\n\n1600s Japan saw the origination of Ukiyo-e, an influential illustration style characterised by expressive line, vivid colour and subtle tones, resulting from the ink-brushed wood block printing technique. Subjects included traditional folk tales, popular figures and every day life. Hokusai’s \"The Great Wave of Kanazawa\" is a famous image of the time.\n\nDuring the 16th and 17th centuries in Europe, the main reproduction processes for illustration were engraving and etching. In 18th Century England, a notable illustrator was William Blake (1757–827), who used relief etching. By the early 19th century, the introduction of lithography substantially improved reproduction quality.\n\nIn Europe, notable figures of the early 19th Century were John Leech, George Cruikshank, Dickens illustrator Hablot Knight Browne, and, in France, Honoré Daumier. All contributed to both satirical and “serious” publications. At this time, there was a great demand for caricature drawings encapsulating social mores, types and classes.\n\nThe British humorous magazine Punch (1841–2002) built on the success of Cruikshank's Comic Almanac (1827–1840) and employed many well-regarded illustrators, including Sir John Tenniel, the Dalziel Brothers, and Georges du Maurier. Although all fine art trained, their reputations were gained primarily as illustrators.\n\nHistorically, Punch was most influential in the 1840s and 1850s. The magazine was the first to use the term \"cartoon\" to describe a humorous illustration and its widespread use led to John Leech being known as the world's first \"cartoonist\". In common with similar magazines such as the Parisian Le Voleur, Punch realised good illustration sold as well as good text. With publication continuing into the 21st Century, Punch chronicles a gradual shift in popular illustration, from reliance on caricature to sophisticated topical observation.\n\nFrom the early 1800s newspapers, mass market magazines, and illustrated books had become the dominant consumer media in Europe and the New World. By the 19th century, improvements in printing technology freed illustrators to experiment with color and rendering techniques. These developments in printing effected all areas of literature from cookbooks, photography and traveling guides, as well as children's books. Also, due to advances in printing, it became more affordable to produce color photographs within books and other materials. By 1900, almost 100 percent of paper was machine-made, and while a person working by hand could produce 60-100lbs of paper per day, mechanization yielded around 1,000lbs per day. Additionally, in the 50 year period between 1846 and 1916, book production increased 400% and the price of books was cut in half.\n\nIn America, this led to a \"golden age of illustration\" from before the 1880s until the early 20th century. A small group of illustrators became highly successful, with the imagery they created considered a portrait of American aspirations of the time. Among the best known illustrators of that period were N.C. Wyeth and Howard Pyle of the Brandywine School, J. C. Leyendecker, Maxfield Parrish, and James Montgomery Flagg.\n\n"}
{"id": "49022982", "url": "https://en.wikipedia.org/wiki?curid=49022982", "title": "Incomplete contracts", "text": "Incomplete contracts\n\nIn economic theory, the field of contract theory can be subdivided in the theory of complete contracts and the theory of incomplete contracts.\n\nThe incomplete contracting paradigm was pioneered by Sanford J. Grossman, Oliver D. Hart, and John H. Moore. In their seminal contributions, Grossman and Hart (1986), Hart and Moore (1990), and Hart (1995) argue that in practice, contracts cannot specify what is to be done in every possible contingency. At the time of contracting, future contingencies may not even be describable. Moreover, parties cannot commit themselves never to engage in mutually beneficial renegotiation later on in their relationship. Thus, an immediate consequence of the incomplete contracting approach is the so-called hold-up problem. Since at least in some states of the world the parties will renegotiate their contractual arrangements later on, they have insufficient incentives to make relationship-specific investments (since a party's investment returns will partially go to the other party in the renegotiations). Oliver Hart and his co-authors argue that the hold-up problem may be mitigated by choosing a suitable ownership structure ex ante (according to the incomplete contracting paradigm, more complex contractual arrangements are ruled out). Hence, the property rights approach to the theory of the firm can explain the pros and cons of vertical integration, thus providing a formal answer to important questions regarding the boundaries of the firm that were first raised by Ronald Coase (1937).\n\nThe incomplete contracting approach has been subject of a still ongoing discussion in contract theory. In particular, some authors such as Maskin and Tirole (1999) argue that rational parties should be able to solve the hold-up problem with complex contracts, while Hart and Moore (1999) point out that these contractual solutions do not work if renegotiation cannot be ruled out. Some authors have argued that the pros and cons of vertical integration can sometimes also be explained in complete contracting models. The property rights approach based on incomplete contracting has been criticized by Williamson (2000) because it is focused on ex ante investment incentives, while it neglects ex post inefficiencies. It has been pointed out by Schmitz (2006) that the property rights approach can be extended to the case of asymmetric information, which may explain ex post inefficiencies. The property rights approach has also been extended by Chiu (1998) and DeMeza and Lockwood (1998), who allow for different ways to model the renegotiations. In a more recent extension, Hart and Moore (2008) have argued that contracts may serve as reference points. The theory of incomplete contracts has been successfully applied in various contexts, including privatization, international trade, management of research & development, allocation of formal and real authority, advocacy, and many others.\n\nThe Nobel Prize in Economics 2016 was awarded to Oliver D. Hart and Bengt Holmström for their contribution to contract theory, including incomplete contracts.\n"}
{"id": "2327473", "url": "https://en.wikipedia.org/wiki?curid=2327473", "title": "John Darsee", "text": "John Darsee\n\nJohn Roland Darsee (born in Huntington, West Virginia) is an American physician and former medical researcher. After compiling an impressive list of publications in reputable scientific journals, he was found to have fabricated data for his publications.\n\nJohn Darsee obtained his undergraduate education at the University of Notre Dame, then went to medical school at Indiana University, where he received a degree in 1974.\n\nDarsee had an excellent reputation as a student and medical researcher. He worked at Emory University from 1974 to 1979, serving as chief medical resident at Grady Memorial Hospital. He then moved to Harvard University, where he worked as research fellow at the Cardiac Research Laboratory. Darsee produced 5 major papers in his first 15 months at Harvard. The head of his lab, cardiologist Eugene Braunwald, considered Darsee the most remarkable of the 130 fellows who had worked in his lab and offered Darsee a faculty position at Harvard in 1981. Some of Darsee's colleagues became concerned about the accuracy of Darsee's results. They went to the lab director, Robert Kroner, with their suspicions. Kroner investigated and found that Darsee had been altering dates on his laboratory work to make a few hours' work appear to be several weeks of data. When informed, Braunwald terminated Darsee's fellowship but did not inform the National Institutes of Health (NIH), which was funding the research, of Darsee's misconduct at the time.\n\nBraunwald and Kroner conducted their own investigation into Darsee's work and found no other evidence of fraud; nor did a committee of Harvard faculty appointed by the Dean of the medical school. However, in October 1981 discrepancies between Darsee's data and those collected by other centers performing similar work triggered a formal investigation by the NIH. The NIH review found that Darsee had committed wide-ranging scientific misconduct, fabricating large amounts of data from experiments which he had never conducted. Harvard's investigation, as well as that of Braunwald and Kroner, were criticized for being inadequately rigorous and for reporting that they had \"fully reviewed\" data which later turned out to be non-existent. Darsee was barred by the NIH from receiving federal research funding for 10 years. Brigham and Women's Hospital, affiliated with Harvard, had to return $122,371 in research funds to NIH. This was the first time an institution was required to return money to NIH because of research fraud.\n\nOver time, more research by Darsee came under fire. Investigations revealed that Darsee had previously used false data between 1966 and 1970, while an undergraduate at the University of Notre Dame. Following the NIH investigation, Harvard retracted 30 of Darsee's papers and abstracts in February 1983. Review of Darsee's earlier work at Emory University led to the retraction of an additional 52 papers and abstracts published during his tenure there. Braunwald drew criticism for lax supervision and for creating \"a hurried pace and emphasis on productivity, coupled with limited interaction with senior scientists\", which contributed to the ease with which Darsee was able to fabricate data. Arnold Relman, editor of \"The New England Journal of Medicine\", also criticized Darsee's coauthors for their unfamiliarity with his work and lack of awareness of the scientific misconduct.\n\nDarsee maintained that he had \"no recollection\" of committing research fraud. He issued an apology which was printed in \"The New England Journal of Medicine\", writing: \"I am deeply sorry for allowing these inaccuracies and falsehoods to be published in the Journal and apologize to the editorial board and readers.\" Darsee asked \"forgiveness for whatever I have done wrong.\"\n\nDarsee subsequently entered a clinical fellowship in critical care at Ellis Hospital in Schenectady, New York. He worked there until June 1983. In 1984 the New York State Board of Regents revoked his license to practice medicine in the state of New York. He is now working as a medical writer and blogger under the name of John Hughes-Darsee and living with his wife, Linda Hughes, a surgical nurse, and 2 children in West Nyack, NY. \n\n\n"}
{"id": "1874251", "url": "https://en.wikipedia.org/wiki?curid=1874251", "title": "Kennedy Space Center Launch Complex 39", "text": "Kennedy Space Center Launch Complex 39\n\nLaunch Complex 39 (LC-39) is a rocket launch site at the John F. Kennedy Space Center on Merritt Island in Florida, United States. The site and its collection of facilities were originally built for the Apollo program, and later modified for the Space Shuttle program. As of 2017, only Launch Complex 39A is active, launching SpaceX's Falcon 9 and Falcon Heavy. Pad 39B is being modified to launch NASA's Space Launch System. A new, smaller pad, 39C was added in 2015 to support smaller launches but has not yet been used.\n\nLaunch Complex 39 is composed of three launch pads—39A, 39B and 39C, a Vehicle Assembly Building (VAB), a Crawlerway used by crawler-transporters to carry Mobile Launcher Platforms between the VAB and the pads, Orbiter Processing Facility buildings, a Launch Control Center which contains the firing rooms, a news facility famous for the iconic countdown clock seen in television coverage and photos, and various logistical and operational support buildings.\n\nSpaceX leases Launch Pad 39A from NASA and has modified the pad to support Falcon Heavy launches in 2017 and beyond.\nNASA began modifying Launch Pad 39B in 2007 to accommodate the now defunct Project Constellation, and is currently preparing it for the Space Launch System with first launch scheduled for December 2019. Pad C was originally planned for Apollo but never built, and would have been a copy of pads 39A and 39B. A smaller pad, designated 39C was constructed from January to June 2015 to accommodate small-class vehicles.\n\nNASA launches from LC-39A and 39B have been supervised from the NASA Launch Control Center (LCC), located from the launch pads. LC-39 is one of several launch sites that share radar and tracking services of the Eastern Test Range.\n\nNorthern Merritt Island was first developed around 1890 when a few wealthy Harvard University graduates purchased and constructed a three-story mahogany clubhouse, very nearly on the site of Pad 39A. During the 1920s, Peter E. Studebaker Jr., son of the automobile magnate, built a small casino at De Soto Beach eight miles (13 km) north of the Canaveral lighthouse.\n\nIn 1948, the Navy transferred the former Banana River Naval Air Station located south of Cape Canaveral, to the Air Force for use in testing captured German V-2 rockets. The site's location on the East Florida coast was ideal for this purpose in that launches would be over the ocean, away from populated areas. This site became the Joint Long Range Proving Ground in 1949, and was renamed Patrick Air Force Base in 1950. The Air Force annexed part of Cape Canaveral to the North in 1951, forming the Air Force Missile Test Center, the future Cape Canaveral Air Force Station (CCAFS). Missile and rocketry testing and development would take place here through the 1950s.\n\nAfter the creation of NASA in 1958, the CCAFS launch pads were used for NASA's civilian unmanned and manned launches, including those of Project Mercury and Project Gemini.\n\nIn 1961, President Kennedy proposed to Congress the goal of landing a man on the Moon by the end of the decade. Congressional approval led to the launch of the Apollo program, which required a massive expansion of NASA operations, including an expansion of launch operations from the Cape to adjacent Merritt Island to the north and west. NASA began acquisition of land in 1962, taking title to by outright purchase and negotiating with the state of Florida for an additional . On July 1, 1962, the site was named the Launch Operations Center.\n\nAt the time, the highest numbered launch pad on CCAFS was Launch Complex 37; when the lunar launch complex was designed, it was designated as Launch Complex 39. It was designed to handle launches of the Saturn V rocket, the largest, most powerful rocket then designed, which would propel Apollo spacecraft to the Moon. Initial plans included four pads (five were considered) evenly spaced apart to avoid damage in the event of an explosion on the pad. Three were scheduled for construction (A-C, to the southeast) and two (D and E, west and north) would have been built at a later date. The numbering of the pads at the time was from north to south, with the northernmost being 39A, and the southernmost being 39C. Pad 39A was never built, and 39C became 39A in 1963. With today's numbering, 39C would have been north of 39B, and 39D would have been due west of 39C. Pad 39E would have been due north of the mid-distance between 39C and 39D, with 39E forming the top of a triangle, and equidistant from 39C and 39D. The Crawlerway was built with the additional pads in mind. This is the reason the Crawlerway turns as it heads to Pad B; continuing straight from that turn would have led to the additional pads.\n\nMonths before launch, the three stages of the Saturn V launch vehicle and the components of the Apollo spacecraft were brought inside the Vehicle Assembly Building (VAB) and assembled in one of four high bays into a -tall space vehicle on one of three Mobile Launchers. Each mobile launcher consisted of a two-story, launch platform with four hold-down arms and a Launch Umbilical Tower (LUT) topped by a crane used to lift the spacecraft into position for assembly. The MLP and unfueled vehicle together weighed .\n\nThe Umbilical Tower contained two elevators and nine retractable swing arms which extended to the space vehicle, to provide access to each of the three rocket stages and the spacecraft for people, wiring and plumbing while the vehicle was on the launch pad, and swung away from the vehicle at launch. Technicians, engineers, and astronauts used the uppermost Spacecraft Access Arm to access the crew cabin. At the end of the arm, the white room provided an environmentally controlled and protected area for astronauts and their equipment to enter the spacecraft.\n\nWhen the stack integration was completed, it was moved the to the pad at a speed of by one of two Crawler-Transporters. Each crawler weighed and was capable of keeping the space vehicle on its Mobile Launcher level while negotiating a 5 percent grade to the pad. At the pad, the MLP was supported by six steel pedestals, plus four additional extensible columns.\n\nAfter the MLP was set in place, the Crawler-Transporter rolled a , Mobile Service Structure (MSS) into place to provide further access for technicians to perform detailed checkout of the vehicle, and necessary umbillical connections to the pad. The MSS contained three elevators, two self-propelled platforms and three fixed platforms, and was rolled back to its parking position shortly before launch.\n\nA flame deflector was slid on rails into place under the launch pedestal. This system allowed for rotation with a second flame deflector, after the first was refurbished after each launch. Each deflector measured high by wide by long and weighed . It deflected the exhaust flame into a trench measuring deep by wide by long.\n\nThe four-story Launch Control Center was located away from Pad A, adjacent to the Vehicle Assembly Building for safety. The third floor had four firing rooms (corresponding to the four high bays in the VAB), each with 470 sets of control and monitoring equipment. The second floor contained telemetry, tracking, instrumentation, and data reduction computing equipment. The LCC was connected to the Mobile Launchers by a high speed data link, and during launch a system of 62 closed-circuit television cameras transmitted to 100 monitor screens in the LCC.\n\nLarge cryogenic tanks located near the pads stored the liquid hydrogen and liquid oxygen (LOX) for the second and third stages of the Saturn V. The highly explosive nature of these chemicals required numerous safety measures at the Launch Complex. The pads were located away from each other. Before tanking operations began and during launch, non-essential personnel were excluded from the danger area.\n\nEach pad had a evacuation tube running from the Mobile Launcher platform to a blast-resistant bunker underground, equipped with survival supplies for 20 persons for 24 hours. There was also a cab/slidewire system running from the tower level to evacuate astronauts and technicians away from the pad.\n\nConnections between the Launch Control Center, mobile launcher platform and space vehicle are made in the Pad Terminal Connection Room (PTCR). The facility was a two-story series of rooms beneath the launch pad, constructed of reinforced concrete located on the west side of the flame trench and was protected by up to of fill dirt.\n\nThe first use of LC-39 came in 1967 with the first Saturn V launch, carrying the unmanned Apollo 4 spacecraft. The second unmanned launch, Apollo 6, also used Pad 39A. With the exception of Apollo 10, which used Pad 39B (due to the \"all-up\" testing resulting in a 2-month turnaround period), all manned Apollo-Saturn V launches, commencing with Apollo 8, used Pad 39A.\n\nA total of thirteen Saturn Vs were launched for Apollo, and the unmanned launch of the Skylab space station in 1973. The mobile launchers were then modified for the shorter Saturn IB rockets, by adding a \"milk-stool\" extension platform to the launch pedestal, so that the S-IVB upper stage and Apollo spacecraft swing arms would reach. These were used for three manned Skylab flights and the Apollo-Soyuz Test Project, since the Saturn IB pads 34 and 37 at Cape Canaveral AFB had been decommissioned.\n\nThe thrust to allow the Space Shuttle to achieve orbit was provided by a combination of the Solid Rocket Boosters (SRBs) and the Space Shuttle Main Engines (SSMEs). The SRBs used solid propellant, hence their name. The SSMEs used a combination of liquid hydrogen and liquid oxygen (LOX) from the External Tank (ET), as the orbiter did not have internal fuel tanks for the SSMEs (they would have had to be as large as the External Tank). The SRBs arrived in segments via rail car from their manufacturing facility in Utah, the External Tank arrived from its manufacturing facility in Louisiana by barge, and the orbiter waited in the Orbiter Processing Facility (OPF). The SRBs were first stacked in the VAB, and then the External Tank was mounted between them. Then, using a massive crane, the orbiter was lowered and connected to the External Tank.\n\nPayload to be installed at the launch pad was independently transported in a payload transportation canister then installed vertically at the Payload Changeout Room. Otherwise, payloads would have already been pre-installed at the Orbiter Processing Facility and transported within the orbiter's cargo bay.\n\nThe original structure of the pads was remodeled for the needs of the Space Shuttle, starting with Pad 39A after the last Saturn V launch, and in 1977 for Pad 39B after the Apollo-Soyuz Test Project in 1975.\n\nEach pad contained a two-piece access tower system, the Fixed Service Structure (FSS) and the Rotating Service Structure (RSS). The FSS permitted access to the Shuttle via a retractable arm and a \"beanie cap\" to capture vented LOX from the External Tank. The RSS contained the Payload Changeout Room, which offered \"clean\" access to the orbiter's payload bay, protection from the elements, and protection in winds up to .\n\nThe FSS on Pad 39A was constructed from most of the umbilical tower of Mobile Launcher 2, while the FSS that was on 39B was constructed from most of the umbilical tower of Mobile Launcher 3.\n\nA Sound Suppression Water System (SSWS) was added to protect the Space Shuttle and its payload from effects of the intense sound wave pressure generated by its engines. An elevated water tank on a tower near each pad stored 300,000 gallons (1.1 Megalitres) of water, which was released onto the Mobile Launcher Platform just before engine ignition. The water muffled the intense sound waves produced by the engines. Due to heating of the water, a large quantity of steam and water vapor was produced during launch.\n\nThe Gaseous Oxygen Vent Arm positioned a hood, often called the \"Beanie Cap,\" over the top of the External Tank (ET) nose cone during fueling. Heated gaseous nitrogen was used there to remove the extremely cold gaseous oxygen that normally vented out of the External Tank. This prevented the formation of ice that could fall and damage the shuttle.\n\nThe Hydrogen Vent Line Access Arm mated the External Tank (ET) Ground Umbilical Carrier Plate (GUCP) to the launch pad hydrogen vent line. The GUCP provided support for plumbing and cables, called umbilicals, that transferred fluids, gases, and electrical signals between two pieces of equipment. While the ET was being fueled, hazardous gas was vented from an internal hydrogen tank through the GUCP, out a vent line to a flare stack where it was burned off at a safe distance. Sensors at the GUCP measured gas level. The GUCP was redesigned after leaks created scrubs of STS-127 and were also detected during attempts to launch STS-119 and STS-133. The GUCP released from the ET at launch and fell away with a curtain of water sprayed across it for protection from flames.\n\nIn an emergency, the launch complex used a slidewire escape basket system for quick evacuation. Assisted by members of the closeout team, the crew would leave the orbiter and ride an emergency basket to the ground at speeds reaching up to . From there, the crew took shelter in a bunker. A modified M113 Armored Personnel Carrier could carry injured astronauts away from the complex to safety.\n\nDuring the launch of Discovery on STS-124 on May 31, 2008, the pad at LC-39A suffered extensive damage, in particular to the concrete trench used to deflect the SRB's flames. The subsequent mishap investigation found that the damage was the result of carbonation of epoxy and corrosion of steel anchors which held the refractory bricks in the trench in place. These had been exacerbated by the fact that hydrochloric acid is an exhaust by-product of the solid rocket boosters.\n\nAfter the launch of Skylab in 1973, Pad 39A was reconfigured for the Space Shuttle, with shuttle launches beginning in 1981 with STS-1, flown by the . After Apollo 10, Pad 39B was kept as a backup launch facility in the case of the destruction of 39A, but saw service for all three Skylab missions, the Apollo-Soyuz test flight, and a contingency Skylab Rescue flight that never became necessary. After the Apollo-Soyuz Test Project, 39B was reconfigured similarly to 39A, but due to additional modifications (mainly to allow the facility to service a modified Centaur-G upper stage), along with budgetary restraints, it was not ready until 1986, and the first shuttle flight to use it was STS-51-L, which ended with the \"Challenger\" disaster. The first return to flight mission STS-26 launched from 39B.\n\nThe last Shuttle launch from Pad 39B was the nighttime launch of STS-116 on December 9, 2006. To support the final Shuttle mission to the Hubble Space Telescope STS-125 launched from Pad 39A in May 2009, \"Endeavour\" was placed on 39B if needed to launch the STS-400 rescue mission.\n\nAfter the completion of STS-125, 39B was converted for the single test flight of the \"Constellation Program\" Ares I-X from Pad 39B on October 28, 2009. This program was later cancelled.\n\nWith the retirement of the Shuttle in 2011,\nand the cancellation of Constellation Program in 2010, the future of the LC-39 pads was uncertain.\nBy early 2011, NASA began informal discussions on use of the pads and facilities by private companies to fly missions for the commercial space market, culminating in a 20-year lease agreement with SpaceX for Pad 39A.\n\nJust like the first 24 shuttle flights, Pad 39A supported the final manifested shuttle flights, starting with STS-117 in June 2007 until the retirement of the shuttle fleet in July 2011. Prior to the SpaceX lease agreement, the pad remained as it was when Atlantis launched on the final shuttle mission on July 8, 2011, complete with a mobile launcher platform.\nIn 1997 the pad was the proposed as the site of the first machine for the film Contact.\n\nTalks for use of the pad were underway between NASA and Space Florida—the State of Florida's economic development agency—as early as 2011, but no deal materialized by 2012 and NASA then pursued other options for removing the pad from the Federal government inventory.\n\nBy early 2013, NASA publicly announced that it would allow commercial launch providers to lease Pad 39A,\nand followed that, in May 2013, with a formal solicitation for proposals for commercial use of Launch Pad 39A.\nThere were two competing bids for the commercial use of the launch complex. SpaceX submitted a bid for exclusive use of the launch complex, while Jeff Bezos' Blue Origin submitted a bid for shared non-exclusive use of the complex such that the launchpad would interface with multiple vehicles, and costs could be shared over the long term. One potential shared user in the Blue Origin plan was United Launch Alliance. Prior to completion of the bid period, and prior to any public announcement by NASA of the results of the process, Blue Origin filed a protest with the U.S. General Accounting Office (GAO) \"over what it says is a plan by NASA to award an exclusive commercial lease to SpaceX for use of mothballed space shuttle launch pad 39A.\" NASA had planned to complete the bid award and have the pad transferred by October 1, 2013, but the protest \"will delay any decision until the GAO reaches a decision, expected by mid-December.\" On December 12, 2013, the GAO denied the protest and sided with NASA, which argued that the solicitation contains no preference on the use of the facility as multi-use or single-use. \"The [solicitation] document merely asks bidders to explain their reasons for selecting one approach instead of the other and how they would manage the facility.\"\n\nOn December 13, 2013, NASA announced that they had selected SpaceX as the new commercial tenant. SpaceX signed the lease agreement on April 14, 2014. SpaceX has been given a 20-year exclusive lease of Pad 39A. SpaceX plans to launch their Falcon 9 and Falcon Heavy from the pad and build a new hangar near it.\nElon Musk, CEO of SpaceX, has stated that he wants to shift most of their NASA launches to Pad 39A, including Commercial Cargo and Crew missions to the International Space Station.\n\nOn April 14, 2014, the privately owned launch service provider SpaceX signed a 20-year lease for Launch Pad 39A. The pad was modified to support launches of both Falcon 9 and Falcon Heavy launch vehicles, which included the construction of a horizontal integration facility, similar to that used at existing SpaceX-leased facilities at Cape Canaveral Air Force Station and Vandenberg Air Force Base – this is a marked difference from the vertical integration process used by NASA's own Apollo and Space Shuttle vehicles at the Launch Complex 39. Additionally new instrumentation and control systems were installed, and substantial new plumbing was added for a variety of rocket liquids and gases.\n\nIn 2015, SpaceX built a large Horizontal Integration Facility (HIF) just outside the perimeter of the existing launch pad in order to house both the Falcon 9, and the Falcon Heavy, rockets, and their associated hardware and payloads, during preparation for flight. Both types of launch vehicles will be transported from the HIF to the launch pad aboard a Transporter Erector (TE) which will ride on rails up the former Crawlerway path. Also in 2015, the launch mount for the Falcon Heavy was constructed on Pad 39A over the existing infrastructure. The work on both the HIF building, and the pad, were substantially complete by late 2015. A rollout test of the new Transporter/Erector (TE) was conducted in November 2015.\n\nSpaceX indicated in February 2016 that they had \"completed and activated Launch Complex 39A\", but still has more work yet to do to support crewed flights. SpaceX originally planned to be ready to accomplish the first launch at pad 39A — a Falcon Heavy — as early as 2015, as they had architects and engineers working on the new design and modifications since 2013. By late 2014, a preliminary date for a wet dress rehearsal of the Falcon Heavy was set for no earlier than July 1, 2015. Due to a failure in a June 2015 Falcon 9 launch, SpaceX had to delay launching the Falcon Heavy in order to focus on the Falcon 9's failure investigation and its return to flight. In early 2016, considering the busy Falcon 9 launch manifest, it became unclear if Falcon Heavy would be the first vehicle to launch from Pad 39A, or if one or more Falcon 9 missions would precede a Falcon Heavy launch. The following months, the Falcon Heavy launch was delayed multiple times and eventually pushed back to February 2018.\n\nThe first SpaceX launch from pad 39A was SpaceX CRS-10 using a Falcon 9 on February 19, 2017; it was the company's 10th cargo resupply mission to the International Space Station, and the first unmanned launch from 39A since Skylab.\n\nWhile SLC-40 was undergoing reconstruction after the loss of the AMOS-6 satellite, all SpaceX's east coast launches were launched from LC-39A until SLC-40 became operational again in December 2017. On May 1, 2017, NROL-76 was the first SpaceX mission for the National Reconnaissance Office with a classified payload.\n\nOn February 6, 2018, LC-39A hosted the successful liftoff of the Falcon Heavy on its maiden launch, carrying Elon Musk's Tesla Roadster car to space.\n\nFuture notable missions include:\n\n\nSpaceX intends to utilize the Fixed Service Structure (FSS) of the Pad 39A launch towers, and will extend it above its existing height, but will not need the Rotating Service Structure (RSS) and thus will remove it. Initial plans in 2014 called for leaving the RSS in place until after the first Falcon Heavy launch. However, plans were changed since then, and work on RSS takedown began in February 2016.\n\nNASA has already removed the Orbiter Servicing Arm - with intent to use the space later to build a museum - and white room by which astronauts entered the Space Shuttle. SpaceX indicated in late 2014 that no additional levels to the FSS would be added in the near term. SpaceX plans to later add at least two additional levels to the FSS, and will utilize the FSS for providing crew access for the Dragon V2 launches.\n\nLaunch vehicles will be assembled horizontally in a hangar near the pad, transferred to the pad, and then lifted atop a launch platform for the remainder of the launch prep and lift off. For military missions from Pad 39A, payloads will be vertically integrated, as that is required per launch contract with the US Air Force. A hammerhead crane is planned to be added to the FSS in order to support US military requirements for vertical payload integration.\n\nPad 39A will be used to host launches of astronauts on the crewed-version of the Dragon space capsule in a public–private partnership with NASA. The NASA plan called for the first NASA crewed missions in 2017.\nSpaceX intends to add \"a crew gantry access arm and white room to allow for crew and cargo ingress to the vehicle. The existing Space Shuttle evacuation slide-wire basket system will also be re-purposed to provide a safe emergency egress for the Dragon crew in the event of an emergency on the pad that does not necessitate using the Crew Dragon’s launch abort system.\"\n\nIn August 2018, SpaceX's Crew Access Arm (CAA) was installed on a new level which was built at the necessary height to enter the Crew Dragon spacecraft atop a Falcon 9 rocket. The next month, in September 2018, the refurbished Space Shuttle Emergency Egress System was raised to this new level.\n\nSince the Ares I-X flight, NASA proceeded with plans to strip Pad 39B of its Flight Service Structure (FSS), returning the location to an Apollo-like \"clean pad\" design for the first time since 1977. This approach will make the pad available to multiple types of vehicles which arrive at the pad with service structures on the mobile launcher platform as opposed to custom structures on the pad. The LH, LOX, and water tanks (used for the sound suppression system) are the only structures left from the Space Shuttle era.\n\n, repairs and modifications to selected facility systems at Launch Complex (LC) 39B for Space Launch System (SLS) processing and launch operations are finishing the first phase of a five-phase project. The second phase of this project is currently budgeted at $89.2 million ($6.1 million in FY 2012, $28.5 million in FY 2013, $9.4 million in FY 2014 and $45.2 million in the outyears).\nIn March 2015, Pad 39B was undergoing modifications to the Catacomb Roof structure so that it can handle the loads from the SLS Block 1B rocket, increasing the load capacity to support the crawler-transporter and vertical rocket from .\n\nIn 2014, NASA announced that it would make Pad 39B available to commercial users during times when it is not needed by the Space Launch System. , NASA has one SLS mission scheduled in 2019, and a second one in 2021.\n\nLaunch Pad 39C is a new facility for smaller launch vehicles built in 2015 within the Launch Complex 39B perimeter.\n\nConstruction of the pad began in January 2015 and was completed in June 2015. Kennedy Space Center Director Robert D. Cabana and representatives from the Ground Systems Development and Operations (GSDO) Program and the Center Planning and Development (CPD) and Engineering Directorates marked the completion of the new pad during a ribbon-cutting ceremony July 17, 2015.\n\n\"As America's premier spaceport, we're always looking for new and innovative ways to meet America's launch needs, and one area that was missing was small class payloads,\" Robert D. Cabana said. \"Using 21st Century funds, we built Pad 39C.\"\n\nGSDO oversaw the project and is working with CPD to grow commercial space efforts at Kennedy.\n\n\"Pad 39C is the latest addition to our portfolio of launch pads,\" said Scott Colloredo, CPD director. \"The small class market is here. The demand for that kind of launcher is increasing. The key here is this is really what a launch site for a small class launcher needs to look like.\"\n\nThe concrete pad measures about wide by about long and could support the combined weight of a fueled launch vehicle, payload and customer-provided launch mount up to about , and an umbilical tower structure, fluid lines, cables and umbilical arms weighing up to about .\n\nGSDO also developed a universal propellant servicing system to provide liquid oxygen and liquid methane fueling capabilities for a variety of small class rockets.\n\n\"This is absolutely great to designate a new pad within the confines of Pad 39B. I'm looking forward to having customers here in the not too distant future, making use of this outstanding facility,\" Robert D. Cabana said\n\nKSC's newest Launch Pad, designated 39C, is designed to accommodate Small Class Vehicles. Located in the southeast area of the Launch Complex 39B perimeter, this new concrete pad measures about wide by about long. Launch Complex 39C will serve as a multi-purpose site allowing companies to test vehicles and capabilities in the smaller class of rockets, making it more affordable for smaller companies to break into the commercial spaceflight market.\n\nAs part of this capability, NASA's Ground Systems Development and Operations Program developed a universal propellant servicing system, which can provide liquid oxygen and liquid methane fueling capabilities for a variety of small class rockets. This system is slated for operational readiness in the summer of 2016.\n\nWith the addition of Launch Complex 39C, KSC can offer the following processing and launching features for companies working with small class vehicles (maximum thrust up to ):\n\nKennedy Space Center (KSC) previous Master Plan recommendations in 1966, 1972, and 1977, noted that an expansion of KSC's vertical launch capacity could occur when the market demand existed. The 2007 Site Evaluation Study recommended an additional vertical launch pad, Launch Complex 49 (LC-49), to be sited to the north of existing LC-39B. \nAs part of the Environmental Impact Study (EIS) process, this area was consolidated from two pads (formerly designated in 1963 plans as 39-C and 39-D) to one that provides greater separation from LC-39B. The area was expanded to accommodate a wider variety of launch azimuths, helping protect against potential overflight concerns of LC-39B. \nThis LC-49 launch facility could accommodate medium to large class launch vehicles.\n\nThe 2007 Vertical Launch Site Evaluation Study concluded that a vertical launch pad could also be sited to the south of 39A and to the north of pad 41 to accommodate small/medium launch vehicles. Designated as Launch Complex 48 (LC-48), this area is best suited to accommodate small to medium class launch vehicles due to its closer proximity to LC-39A and LC-41. Due to the nature of these activities, QD arcs, launch hazard impact limit lines, other safety setback, and exposure limits requirements will be imposed for safe operations. The Proposed Launchpads where published in the Kennedy Space Center Master Plan in 2012.\n\nThe Master Plan also notes a New Vertical Launchpad northwest of LC-39B and a Horizontal Launch Area north of the LC-49 and converting the Shuttle Landing Facility (SLF) and it apron areas into a (2nd) Horizontal Launch Area.\n\nFlorida Today reported that up too 3 landing pad for reusable booster to provide more landing options for SpaceX Falcon 9, Falcon Heavy and Blue Origin New Glenn boosters. The pads would be located east of the Horizontal Launch Area and north of LC 39B those planes are not in line with NASA's KSC Masterplan.\n\n\n"}
{"id": "57695348", "url": "https://en.wikipedia.org/wiki?curid=57695348", "title": "Konstantinos Tsioulkas", "text": "Konstantinos Tsioulkas\n\nKonstantinos Tsioulkas (1845 - 1915) was a Greek educator and writer from Western Macedonia.\n\nTsioulkas was born in 1845 in Gorenchi (now Koresos), Kastoria. Most of the inhabitants of the village had a Slavic first language, but Tsioulkas spoke Greek as his first language. After his graduation from the main school of his village he continued his studies with a scholarship in the Educational Association of Kastoria in the Gymnasium of Kasoria and then he taught in the \"Greek school\" of his birthplace. He opposed the attempts of the Exarchists to hold the Divine Liturgy in Bulgarian and in 1871 he succeeded together with others to drive Konstantinos Darzilovites, who, according to Tsioulkas, was the first to teach the Slavic speakers that they were Bulgarians, off Gorenchi, just before a Bulgarian school was founded. In 1875 he was sent with a scholarship from Bellios a bequest in Athens, where he studied in the Philosophic School of the National University. In 1879 he went to Monastiri (Bitola), where he became the first gymnasiarch of the Greek gymnasium for a decade, until 1889, when he was fired because of the events relating to Pichion.\n\nIn 1907 he published in Athens a 350-page work of his \"Contributions to the bilingualism of the Macedonians by comparing the Slavic-seeming Macedonian language with the Greek one\" (\"Συμβολαί εις την διγλωσσίαν των Μακεδόνων εκ συγκρίσεως της σλαυοφανούς μακεδονικής γλώσσης προς την ελληνικήν\"), in order to fight against the Bulgarian and pan-Slavist propaganda in the region of Macedonia. Tsioulkas, who claimed not to speak any Slavic language, tried by distorting historical linguistics to prove that the language of the Slavic speakers in Ottoman Macedonia wasn't Bulgarian, but an Ancient Greek dialect. Tsioulkas didn't focus on phonology, syntax, or morphology, for which he only devoted 11 pages in total, but mainly on the vocabulary, where it is easy to find words with common, Indo-European, etymology with Greek, in order to show that the basic elements of the Macedonian vocabulary weren't Slavic, but originated from Greek. To prove that the language of the Slavophones was closer to Ancient Greek than Modern Greek, he composed a list of homeric words and compared how many of them survive in the \"people's\" language, meaning demotic, (650) and how many in the \"Slavic-seeming Macedonian\" (1260). The conclusion, for Tsioulkas, was that the \"Slavic-seeming Macedonian [language] was a sister of the Greek [language]\" and the \"Macedonian people\" was native and descended from the Ancient Macedonians.\n\nThis anti-scientific book was the most important in a series of pseudo-linguistic publications that appeared in Greece from the beginning to the middle of the 20th century, whose writers without knowing the dialects they were writing about maintained that the \"mixed\" or \"Slavic-seeming\" dialects of the Slavophones weren't Slavic. No Greek linguist supported Tsioulkas' theory, but the book was republished in 1991, during the Macedonia naming dispute, without negative commentary, but with a commendatory exordium of the former minister Nikolaos Martis. The book is still in circulation today and references to it continue to appear online, resulting in Tsioulkas having unwittingly contributed to the idea that modern Macedonians are descended from Ancient Macedonians and are their true inheritors.\n\n\n"}
{"id": "7748700", "url": "https://en.wikipedia.org/wiki?curid=7748700", "title": "Lab website", "text": "Lab website\n\nA lab(s) website is a specific type of website most commonly dedicated to research and development programs.\n\nRelating to the classic scientific research environment - the laboratory - existing lab websites predominantly fall into two categories, the real-world and the virtual.\n\nReal-world lab sites relate to the activities and research conducted by laboratories existing outside the Internet. In general, these sites tend to offer users a chance to see results of past research, rather than detailed views of contemporary research.\n\nExamples of these types of labs from the aviation world include Boeing’s Phantom Works, which covers the research arm of the Boeing Corporation, and Lockheed Martin's Advanced Development Program, aka Skunk Works.\n\nA number of companies and institutions have created virtual lab websites specifically for research into Internet-based products. \n\nThis type of research environment is seen as both podium and playpen for Internet-borne companies. In many cases, the labs offer visitors a chance to learn more about the company's products currently in development, and increasingly, to actually trial the work in progress. \n\nOne of the best-known examples is Google Labs. Since its inception, Google Labs has resulted in the trial and launch of live products such as Gmail, Google Calendar and Google Videos.\n\nSimilar examples from large web-based companies include Yahoo! Next and Microsoft Live Labs.\n\nOne recent notable addition is Digg Labs, illustrating the Digg social bookmarking community's activities in near real-time. The labs are composed of the swarm and the stack activity displays.\n\nMozilla has added a lab area to its product offering.\n\nVirtual laboratories are not the sole domain of companies and institutions. Some are created by individuals and exist solely as websites.\n\nTraditional print and broadcast media companies have also begun to experiment with dedicating specific areas on their websites to advanced projects. One of the first companies credited with creating its own lab area was Reuters. When founded, the Reuters lab offered a limited number of products for visitors to experiment with, including the news and quotes widget and their mobile service.\n\nThe BBC has created a derivation on the lab idea with their BBC Backstage site. Backstage's slogan \"Use our stuff to build your stuff\" openly invites developers to use the BBC's various feeds and API's to power a new range of non-commercial products and services. The backstage site has allowed the BBC to create a developer network, a location for all those working with the BBC's content to come together and share their ideas and prototypes amongst their peers. The site also contains a blog. \n\n\"The Guardian\" newspaper in the UK has taken the idea of a lab to the next level with its Comment is free product. Created by Ben Hammersley, Comment is Free was made as a fully interactive extension to the Guardian Unlimited’s blogging system. \n\nThe site contains the political and opinion material from both \"The Guardian\" and its sister paper \"The Observer\", as well as work from over 600 separate subject-based experts, selected to write on their topics of knowledge. Users are encouraged to read and comment, and all posts are automatically linked to Technorati to return contextual blogosphere results.\n\nIn November 2006, NEWS.com.au, the breaking news section of News Digital Media launched News Lab, the first media-driven R&D website within News Corporation (N.B. News Corp also operates FIM Lab but this is currently without a website). The site aims to collect users' feedback on new products and amend them accordingly.\n\nWhile some media companies choose to create their own experimental areas, others create dedicated areas to document the efforts of others. \"The Washington Post\"'s blog section, referred to as the Mashington Post records the efforts of Internet users' experimentation with combinations of pre-existing data, referred to as mashups.\n"}
{"id": "38563279", "url": "https://en.wikipedia.org/wiki?curid=38563279", "title": "List of computer-assisted organic synthesis software", "text": "List of computer-assisted organic synthesis software\n\nComputer software for computer-assisted organic synthesis (CAOS) is used in organic chemistry and computational chemistry to facilitate the tasks of designing and predicting chemical reactions. The CAOS problem reduces to identifying a series of chemical reactions which can, from starting materials, produce a desired target molecule. CAOS algorithms typically use two databases: a first one of known chemical reactions and a second one of known starting materials (i.e., typically molecules available commercially). Desirable synthetic plans cost less, have high yield, and avoid using hazardous reactions and intermediates. Typically cast as a planning problem, significant progress has been made in CAOS.\n\nThe following table lists the abilities of the most versatile software packages that show an entry in two or more columns of the table.\n\n"}
{"id": "5193078", "url": "https://en.wikipedia.org/wiki?curid=5193078", "title": "List of food additives", "text": "List of food additives\n\nFood additives are substances added to food to preserve flavor or enhance its taste, appearance, or other qualities.\n\nAdditives are used for many purposes but the main uses are:\n\n\nCaffeine and other GRAS (generally recognized as safe) additives such as sugar and\nsalt are not required to go through the regulation process.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "42961979", "url": "https://en.wikipedia.org/wiki?curid=42961979", "title": "List of medical schools in Israel", "text": "List of medical schools in Israel\n\nThis is a list of medical schools in Israel.\n\n\n\nNone.\n\n\n\n\nNone.\n"}
{"id": "37002880", "url": "https://en.wikipedia.org/wiki?curid=37002880", "title": "List of mites associated with cutaneous reactions", "text": "List of mites associated with cutaneous reactions\n\nThere are a number of mites that can bite human skin and result in a cutaneous reaction, as well as transmit other diseases of or affecting the human integumentary system.\n\n\n"}
{"id": "18596832", "url": "https://en.wikipedia.org/wiki?curid=18596832", "title": "List of political and geographic subdivisions by total area in excess of 1,000,000 square kilometers", "text": "List of political and geographic subdivisions by total area in excess of 1,000,000 square kilometers\n"}
{"id": "56427334", "url": "https://en.wikipedia.org/wiki?curid=56427334", "title": "List of things named after Julian Schwinger", "text": "List of things named after Julian Schwinger\n\nThings named Julian Schwinger:\n\nSchwinger function<br>Schwinger model<br>Schwinger–Dyson equation<br>Schwinger's quantum action principle<br>Rarita–Schwinger equation<br>Lippmann–Schwinger equation<br>Schwinger–Tomonaga equation<br>Schwinger variational principle<br>Schwinger parametrization<br>Schwinger limit<br>Schwinger representation<br>Fock–Schwinger gauge\n"}
{"id": "57483945", "url": "https://en.wikipedia.org/wiki?curid=57483945", "title": "Mainzed", "text": "Mainzed\n\nmainzed ([maɪ̯nt͡sed]; acronym for Mainz Centre for Digitality in the Humanities and Cultural Studies) is a joint initiative of six scientific institutions to promote digital methodology in the humanities and cultural sciences in Mainz, Germany.\nIt was founded in the context of the academic annual celebration of the Academy of Sciences and Literature Mainz on 6 November 2015.\nPartners of mainzed are the Academy of Sciences and Literature Mainz (ADW), the Mainz University of Applied Sciences (HS Mainz), the Institute for Historical Regional Studies at the University of Mainz (IGL), the Johannes Gutenberg University Mainz (JGU), the Leibniz Institute of European History Mainz (IEG) and the Romano-Germanic Central Museum Mainz – Archaeological research institute (RGZM).\n\nmainzed is based on different long-term cooperations between various institutions in Mainz. The research and development institution for digital humanities of the Academy of Sciences and Literature Mainz, \"Digitale Akademie\", was founded in 2009 and is connected to the Institute of Historical Regional Studies at the University of Mainz, the Leibniz Institute of European History and the universities of Mainz. Since 1997, the Romano-Germanic Central Museum Mainz and the i3mainz – Institute for Spatial Information and Surveying Technology have operated the \"Competence Centre for Spatial Information Technology in the Humanities\" at the Mainz University of Applied Sciences. In autumn 2013, the informal \"Network DHMainz\" was created with the help of the Mainz Research Alliance. The network was responsible for the preparation of the Digital Humanities Day 2014 in Mainz where first drafts were made for the continuation of the initiative.\n\nmainzed was founded in order to accompany and practically implement the transformation of the humanities and cultural studies in the course of digitisation in Mainz.\nmainzed works in research, the support of research, qualification and transfer. Furthermore, it constitutes a social research infrastructure by offering a network of scientific exchange with regard to the development of projects and research foci for scientists of all qualification levels.\n\nRange of competences represented in the network:\n\n\nmainzed developed the inter-university master’s degree program \"Digital Methods in the Humanities and Cultural Studies\" in terms of organization and concept. Since 2016, each winter term 24 students have been able to begin the course of studies comprising four semesters provided that they have a bachelor’s degree in the humanities, cultural studies or with a focus in computer science.\nThe head of this degree program and director of mainzed Kai-Christian Bruhn received the academy price of the federal state Rhineland-Palatinate on 5 December 2017 in recognition of his interdisciplinary work in teaching and research.\n\nmainzed is initiator of many events promoting the dialogue with the public. An example of this is the fishbowl discussion about the topic \"digitalität und diversität – die Geisteswissenschaften im Jahr 2026\" that took place in 2016. Mainzed has organised similar annual events with national and international guest lecturers like Mercedes Bunz and Joscha Bach.\n\nmainzed is organised into an executive board composed of the founding director Kai-Christian Bruhn as well as his deputy Klaus Pietschmann, a scientific advisory board with representatives of the partner institutions and an executive office managed by Anne Klammt.\n\n"}
{"id": "100034", "url": "https://en.wikipedia.org/wiki?curid=100034", "title": "Military engineering", "text": "Military engineering\n\nMilitary engineering is loosely defined as the art, science, and practice of designing and building military works and maintaining lines of military transport and military communications. Military engineers are also responsible for logistics behind military tactics. Modern military engineering differs from civil engineering. In the 20th and 21st centuries, military engineering also includes other engineering disciplines such as mechanical and electrical engineering techniques.\n\nAccording to NATO, \"military engineering is that engineer activity undertaken, regardless of component or service, to shape the physical operating environment. Military engineering incorporates support to maneuver and to the force as a whole, including military engineering functions such as engineer support to force protection, counter-improvised explosive devices, environmental protection, engineer intelligence and military search. Military engineering does not encompass the activities undertaken by those 'engineers' who maintain, repair and operate vehicles, vessels, aircraft, weapon systems and equipment.\"\n\nMilitary engineering is an academic subject taught in military academies or schools of military engineering. The construction and demolition tasks related to military engineering are usually performed by military engineers including soldiers trained as sappers or pioneers. In modern armies, soldiers trained to perform such tasks while well forward in battle and under fire are often called combat engineers.\n\nIn some countries, military engineers may also perform non-military construction tasks in peacetime such as flood control and river navigation works, but such activities do not fall within the scope of military engineering.\n\nThe word \"engineer\" was initially used in the context of warfare, dating back to 1325 when \"engine’er\" (literally, one who operates an engine) referred to \"a constructor of military engines\". In this context, \"engine\" referred to a military machine, i. e., a mechanical contraption used in war (for example, a catapult).\n\nAs the design of civilian structures such as bridges and buildings developed as a technical discipline, the term \"civil engineering\" entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the older discipline. As the prevalence of civil engineering outstripped engineering in a military context and the number of disciplines expanded, the original military meaning of the word \"engineering\" is now largely obsolete. In its place, the term \"military engineering\" has come to be used.\n\nThe first civilization to have a dedicated force of military engineering specialists were the Romans, whose army contained a dedicated corps of military engineers known as \"architecti\". This group was pre-eminent among its contemporaries. The scale of certain military engineering feats, such as the construction of a double-wall of fortifications long, in just 6 weeks to completely encircle the besieged city of Alesia in 52 B.C.E., is an example. Such military engineering feats would have been completely new, and probably bewildering and demoralizing, to the Gallic defenders. The best known of these Roman army engineers due to his writings surviving is Vitruvius.\n\nIn ancient times, military engineers were responsible for siege warfare and building field fortifications, temporary camps and roads. The most notable engineers of ancient times were the Romans and Chinese, who constructed huge siege-machines (catapults, battering rams and siege towers). The Romans were responsible for constructing fortified wooden camps and paved roads for their legions. Many of these Roman roads are still in use today.\n\nFor 500 years after the fall of the Roman empire, the practice of military engineering barely evolved in the west. In fact, much of the classic techniques and practices of Roman military engineering were lost. Through this period, the foot soldier (who was pivotal to much of the Roman military engineering capability) was largely replaced by mounted soldiers. It was not until later in the Middle Ages, that military engineering saw a revival focused on siege warfare.\n\nMilitary engineers planned castles and fortresses. When laying siege, they planned and oversaw efforts to penetrate castle defenses. When castles served a military purpose, one of the tasks of the sappers was to weaken the bases of walls to enable them to be breached before means of thwarting these activities were devised. Broadly speaking, sappers were experts at demolishing or otherwise overcoming or bypassing fortification systems.\nWith the 14th-century development of gunpowder, new siege engines in the form of cannons appeared. Initially military engineers were responsible for maintaining and operating these new weapons just as had been the case with previous siege engines. In England, the challenge of managing the new technology resulted in the creation of the Office of Ordnance around 1370 in order to administer the cannons, armaments and castles of the kingdom. Both military engineers and artillery formed the body of this organization and served together until the office's predecessor, the Board of Ordnance was disbanded in 1855.\n\nIn comparison to older weapons, the cannon was significantly more effective against traditional medieval fortifications. Military engineering significantly revised the way fortifications were built in order to be better protected from enemy direct and plunging shot. The new fortifications were also intended to increase the ability of defenders to bring fire onto attacking enemies. Fort construction proliferated in 16th-century Europe based on the \"trace italienne\" design.\nBy the 18th century, regiments of foot (infantry) in the British, French, Prussian and other armies included pioneer detachments. In peacetime these specialists constituted the regimental tradesmen, constructing and repairing buildings, transport wagons, etc. On active service they moved at the head of marching columns with axes, shovels, and pickaxes, clearing obstacles or building bridges to enable the main body of the regiment to move through difficult terrain. The modern Royal Welch Fusiliers and French Foreign Legion still maintain pioneer sections who march at the front of ceremonial parades, carrying chromium-plated tools intended for show only. Other historic distinctions include long work aprons and the right to wear beards.\n\nThe Peninsular War (1808–14) revealed deficiencies in the training and knowledge of officers and men of the British Army in the conduct of siege operations and bridging. During this war low-ranking Royal Engineers officers carried out large-scale operations. They had under their command working parties of two or three battalions of infantry, two or three thousand men, who knew nothing in the art of siegeworks. Royal Engineers officers had to demonstrate the simplest tasks to the soldiers, often while under enemy fire. Several officers were lost and could not be replaced, and a better system of training for siege operations was required. On 23 April 1812 an establishment was authorised, by Royal Warrant, to teach \"Sapping, Mining, and other Military Fieldworks\" to the junior officers of the Corps of Royal Engineers and the Corps of Royal Military Artificers, Sappers and Miners.\n\nThe first courses at the Royal Engineers Establishment were done on an all ranks basis with the greatest regard to economy. To reduce staff the NCOs and officers were responsible for instructing and examining the soldiers. If the men could not read or write they were taught to do so, and those who could read and write were taught to draw and interpret simple plans. The Royal Engineers Establishment quickly became the centre of excellence for all fieldworks and bridging. Captain Charles Pasley, the director of the Establishment, was keen to confirm his teaching, and regular exercises were held as demonstrations or as experiments to improve the techniques and teaching of the Establishment. From 1833 bridging skills were demonstrated annually by the building of a pontoon bridge across the Medway which was tested by the infantry of the garrison and the cavalry from Maidstone. These demonstrations had become a popular spectacle for the local people by 1843, when 43,000 came to watch a field day laid on to test a method of assaulting earthworks for a report to the Inspector General of Fortifications. In 1869 the title of the Royal Engineers Establishment was changed to \"The School of Military Engineering\" (SME) as evidence of its status, not only as the font of engineer doctrine and training for the British Army, but also as the leading scientific military school in Europe.\nThe dawn of the internal combustion engine marked the beginning of a significant change in military engineering. With the arrival of the automobile at the end of the 19th century and heavier than air flight at the start of the 20th century, military engineers assumed a major new role in supporting the movement and deployment of these systems in war. Military engineers gained vast knowledge and experience in explosives. They were tasked with planting bombs, landmines and dynamite.\n\nAt the end of World War I, the standoff on the Western Front caused the Imperial German Army to gather experienced and particularly skilled soldiers to form \"Assault Teams\" which would break through the Allied trenches. With enhanced training and special weapons (such as flamethrowers), these squads achieved some success, but too late to change the outcome of the war. In early WWII, however, the Wehrmacht \"Pioniere\" battalions proved their efficiency in both attack and defense, somewhat inspiring other armies to develop their own combat engineers battalions. Notably, the attack on Fort Eben-Emael in Belgium was conducted by Luftwaffe glider-deployed combat engineers.\n\nThe need to defeat the German defensive positions of the \"Atlantic wall\" as part of the amphibious landings in Normandy in 1944 led to the development of specialist combat engineer vehicles. These, collectively known as Hobart's Funnies, included a specific vehicle to carry combat engineers, the Churchill AVRE. These and other dedicated assault vehicles were organised into the specialised 79th Armoured Division and deployed during Operation Overlord – 'D-Day'.\n\nOther significant military engineering projects of World War II include Mulberry harbour and Operation Pluto.\n\nModern military engineering still retains the Roman role of building field fortifications, road paving and breaching terrain obstacles. A notable military engineering task was, for example, breaching the Suez Canal during the Yom Kippur War.\n\nMilitary engineers can come from a variety of engineering programs. They may be graduates of mechanical, electrical, civil, or industrial engineering.\n\nModern military engineering can be divided into three main tasks or fields: combat engineering, strategic support, and ancillary support. Combat engineering is associated with engineering on the battlefield. Combat engineers are responsible for increasing mobility on the front lines of war such as digging trenches and building temporary facilities in war zones. Strategic support is associated with providing service in communication zones such as the construction of airfields and the improvement and upgrade of ports, roads and railways communication. Ancillary support includes provision and distribution of maps as well as the disposal of unexploded warheads. Military engineers construct bases, airfields, roads, bridges, ports, and hospitals. During peacetime before modern warfare, military engineers took the role of civil engineers by participating in the construction of civil-works projects. Nowadays, military engineers are almost entirely engaged in war logistics and preparedness.\n\nThe NATO Military Engineering Center of Excellence (MilEng CoE) is co-located with the German Army Military Engineer School in Ingolstadt. Prior to becoming a NATO CoE, the institute was known as the Euro NATO Training Engineer Centre (ENTEC) and it was located in Munich. As ENTEC, the institute was mandated to conduct military engineer interoperability training for participating nations. As the MilEng CoE, the institute's mandate has expanded to include doctrine and NATO standardization agreements (STANAGs) related to military engineering.\n\nMilitary engineers are key in all armed forces of the world, and invariably found either closely integrated into the force structure, or even into the combat units of the national troops.\n\nAustralia\n\n\nCanada\n\nDenmark\n\nThe Danish military engineering corps is almost entirely organized into one regiment, simply named \"Ingeniørregimentet\" (\"The Engineering Regiment\").\n\nGermany\n\nFrance\n\nIndia\n\nIndonesia\n\nIreland\n\nIsrael\n\n\nThe Israeli combat engineer Corps motto is \"Rishonim Tamid\" , meaning \"Always first\".\n\n\n\n\nSouth Africa: South African Army Engineer Formation\n\n\n\n\nThe prevalence of military engineering in the United States dates back to the American Revolutionary War when engineers would carry out tasks in the U.S. Army. During the war, they would map terrain to and build fortifications to protect troops from opposing forces. The first military engineering organization in the United States was the Army Corps of Engineers. Engineers were responsible for protecting military troops whether using fortifications or designing new technology and weaponry throughout the United States’ history of warfare. The Army originally claimed engineers exclusively, but as the U.S. military branches expanded to the sea and sky, the need for military engineering sects in all branches increased. As each branch of the United States military expanded, technology adapted to fit their respective needs.\n\n\n\n"}
{"id": "18700697", "url": "https://en.wikipedia.org/wiki?curid=18700697", "title": "Open peer review", "text": "Open peer review\n\nOpen peer review is a process in which names of peer reviewers of papers submitted to academic journals are disclosed to the authors of the papers in question. In some cases, as with the \"BMJ\" and BioMed Central, the process also involves posting the entire pre-publication history of the article online, including not only signed reviews of the article, but also its previous versions and author responses to the reviewers.\n\nThere is no single definition of open peer review, as it is implemented differently by different academic journals, but it has been broadly defined as \"any scholarly review mechanism providing disclosure of author and referee identities to one another at any point during the peer review or publication process\".\n\nPossible advantages to an open peer-review system include reviewers being \"more tactful and constructive\" than they would be if they could remain anonymous. It has also been argued that open review leads to more honest reviewing and prevents reviewers from following their individual agendas, as well as leading to the detection of reviewers' conflicts of interests. Some studies have also found that open peer review is associated with an increase in quality of reviews, although other studies have not found such an association. A study of BioMed Central medical journals, all of which use open peer review, found that reviewers usually did not notice problems or request changes in reporting of the results of randomized trials. The same study found most, but not all, of the requested changes had a positive effect on reporting.\n\nA 1999 study found that open peer review did not affect the quality of reviews or the recommendation regarding whether the paper being reviewed should be published, but that it \"significantly increased the likelihood of reviewers declining to review\". Open review of abstracts tended to lead to bias favoring authors from English-speaking countries and prestigious academic institutions. It has also been argued that open peer review could lead to authors accumulating enemies who try to keep their papers from being published or their grant applications from being successful.\n"}
{"id": "695010", "url": "https://en.wikipedia.org/wiki?curid=695010", "title": "Penny Black (research project)", "text": "Penny Black (research project)\n\nThe Penny Black Project is a Microsoft Research project that tries to find effective and practical ways of fighting spam. Because identifying spams consumes a recipient's time, the idea is to make the sender of emails \"pay\" a certain amount for sending them. The currency or the mode of payment could be CPU cycles, Turing tests or memory cycles. Such a payment would limit spammers' ability to send out large quantities of emails quickly.\n\nThe project's name is derived from the Penny Black, the world's first adhesive stamp used for paid postage.\n\nThe goal of the project is move the e-mail costs from the receiver to the sender. The general idea is that if the sender must prove that they have expended a certain amount of effort specifically for the receiver and the message alone.\n\nThe project aims to devise a method to do this without introducing additional challenge-response mechanisms and third parties, and without requiring extra maintenance and updates, while retaining the current architecture of the e-mail system.\n\nOne of the project's ideas was the \"ticket server\", a credit-based method for validating emails. Tickets would be required to perform actions, such as sending emails. There are three operations the ticket server provides: \"request ticket\", \"cancel ticket\", and \"refund ticket\".\n\nThe server would allow the user to request a ticket in exchange for a proof of work: expending CPU cycles solving hard algorithms with processing power, Turing tests, or even just by paying money. The server could also cancel a ticket. For example, after receiving an email with a ticket, the receiver could request the ticket to be cancelled so it cannot be reused. The person who cancels a ticket also has the option to refund the ticket to the sender. This causes the original sender to regain a new ticket. For example, a user might refund a ticket that came with an email if it was not spam.\n\nUsing this, friendly and trusted emails would have little to no cost as tickets would be frequently refunded. However, spammers would be required to invest either a lot of computing time or money in order to create enough tickets to send large numbers of e-mails.\n\nOne of the more obvious flaws is that this project can not entirely stop spam. It only hopes to slow spam down enough such that it is no longer cost effective for spammers. Using these methods to reduce spam will also require these policies to be universal amongst mail clients.\n\nIntended mass email may also not work as intended. For example, subscribers to a particular email service may end up getting their emails with a significant delay while the email service will also become an increased expenses to the provider.\n\n\n"}
{"id": "24933", "url": "https://en.wikipedia.org/wiki?curid=24933", "title": "Polywater", "text": "Polywater\n\nPolywater was a hypothesized polymerized form of water that was the subject of much scientific controversy during the late 1960s. By 1969 the popular press had taken notice and sparked fears of a \"polywater gap\" in the US.\n\nIncreased press attention also brought with it increased scientific attention, and as early as 1970 doubts about its authenticity were being circulated. By 1973 it was found to be illusory, being just water with any number of common organic compounds contaminating it.\n\nToday, polywater is best known as an example of pathological science.\n\nIn 1961, the Soviet physicist Nikolai Fedyakin, working at the Technological Institute of Kostroma, Russia, performed measurements on the properties of water which had been condensed in, or repeatedly forced through, narrow quartz capillary tubes. Some of these experiments resulted in what was seemingly a new form of water with a higher boiling point, lower freezing point, and much higher viscosity than ordinary water – about that of a syrup.\n\nBoris Derjaguin, director of the laboratory for surface physics at the Institute for Physical Chemistry in Moscow, heard about Fedyakin's experiments. He improved on the method to produce the new water, and though he still produced very small quantities of this mysterious material, he did so substantially faster than Fedyakin did. Investigations of the material properties showed a substantially lower freezing point of −40 °C or less, a boiling point of 150 °C or greater, a density of approx. 1.1 to 1.2 g/cm³, and increased expansion with increasing temperature. The results were published in Soviet science journals, and short summaries were published in \"Chemical Abstracts\" in English, but Western scientists took no notice of the work.\n\nIn 1966, Derjaguin travelled to England for the \"Discussions of the Faraday Society\" in Nottingham. There, he presented the work again, and this time English scientists took note of what he referred to as \"anomalous water\". English scientists then started researching the effect as well, and by 1968 it was also under study in the United States.\n\nBy 1969, the concept had spread to newspapers and magazines. There was fear by the United States military that there was a so-called \"polywater gap\" with the Soviet Union, a popular media term indicating a possible capability \"gap\", or discrepancy, between the US and the USSR, popularized by media hype of the \"bomber gap\", the \"missile gap\", and the \"cruiser gap\", during periods when the USSR appeared to be outstripping the US in numbers of these respective weapons. Likely entered the US vernacular after Watergate, the use of \"gap\" also became widely used.\n\nA scientific furor followed. Some experiments carried\nout were able to reproduce Derjaguin's findings, while others failed. Several theories were advanced to explain the phenomenon. Some proposed it was the cause for increasing resistance on trans-Atlantic phone cables, while others predicted that if polywater were to contact ordinary water, it would convert that water into polywater, echoing the doomsday scenario in Kurt Vonnegut's novel \"Cat's Cradle\". By the 1970s, polywater was well known in the general population.\n\nDuring this time, several people questioned the authenticity of what had come to be known in the West as polywater. The main concern being contamination of the water, but the papers went to great lengths to note the care taken to avoid this. Denis Rousseau and Sergio Porto of Bell Labs carried out infrared spectrum analysis, which showed polywater to be mostly chlorine and sodium.\n\nDenis Rousseau undertook an experiment with his own sweat after playing a handball game at the lab and found it had identical properties. He then published a paper suggesting polywater was nothing more than water with small amounts of biological impurities.\n\nAnother wave of research followed, this time more tightly controlled. Invariably, polywater could no longer be made. Chemical analysis found the samples of polywater to be contaminated with other substances (explaining the changes in melting and boiling points due to colligative properties), and examination of polywater by electron microscopy showed it also contained small particles of various solids – from silica to phospholipids, explaining its greater viscosity.\n\nWhen the experiments which had initially produced polywater were repeated with thoroughly cleaned glassware, the anomalous properties of the resulting water vanished, and even the scientists who had originally advanced the case for polywater agreed it did not exist. This took a few years longer in the Soviet Union, where scientists still clung to the idea.\n\nIn August 1973, Derjaguin and N. V. Churaev published a letter in the journal \"Nature\" in which they wrote; \"these [anomalous] properties should be attributed to impurities rather than to the existence of polymeric water molecules\".\n\nDenis Rousseau used polywater as a classic example of pathological science and has since written on other examples as well.\n\nIt has been suggested that polywater should have been dismissed on theoretical grounds. The laws of thermodynamics predicted that, since polywater had a higher boiling point than ordinary water, it meant it was more stable, and thus all of Earth's water should have turned spontaneously into polywater, instead of just part of it. Richard Feynman remarked that if such a material existed, then an animal would exist that would ingest water and excrete polywater, using the energy released on the process to survive.\n\nThe story \"Polywater Doodle\" by Howard L. Myers (writing under the pseudonym \"Dr. Dolittle\") appeared in the February 1971 issue of \"Analog Science Fiction and Fact\". It features an animal composed entirely of polywater, with the metabolism described by Richard Feynman. (The title of the story is a pun on \"Polly Wolly Doodle\".)\n\nPolywater is the central idea of the 1972 espionage/thriller novel \"A Report from Group 17\" by Robert C. O'Brien. The story revolves around the use of a type of polywater to make people controllable and incapable of independent thought or action.\n\nThe episodes \"The Naked Time\" (\"\") and its sequel, \"The Naked Now\" (\"\") involve forms of polywater intoxication. In the original episode, a scientific research outpost falls victim to polywater, which causes the crew to become so incapacitated that they all died after shutting off environmental controls in the compound. In the sequel, a Starfleet vessel is discovered adrift, its crew frozen in various states due to polywater intoxication.\n\nIn Kurt Vonnegut's novel \"Cat's Cradle\", ice-nine was a form of water that was solid at room temperature, and solidified any water that it contacted, giving it the capability to destroy all life on Earth.\n\n\n"}
{"id": "7548962", "url": "https://en.wikipedia.org/wiki?curid=7548962", "title": "Principles of intelligent urbanism", "text": "Principles of intelligent urbanism\n\nPrinciples of intelligent urbanism (PIU) is a theory of urban planning composed of a set of ten axioms intended to guide the formulation of city plans and urban designs. They are intended to reconcile and integrate diverse urban planning and management concerns. These axioms include environmental sustainability, heritage conservation, appropriate technology, infrastructure-efficiency, placemaking, social access, transit-oriented development, regional integration, human scale, and institutional integrity. The term was coined by Prof. Christopher Charles Benninger.\n\nThe PIU evolved from the city planning guidelines formulated by the \"International Congress of Modern Architecture\" (CIAM), the urban design approaches developed at Harvard's pioneering Urban Design Department under the leadership of Josep Lluis Sert, and the concerns enunciated by Team Ten. It is most prominently seen in plans prepared by Christopher Charles Benninger and his numerous colleagues in the Asian context (Benninger 2001). They form the elements of the planning curriculum at the School of Planning, Ahmedabad, which Benninger founded in 1971. They were the basis for the new capital plan for Thimphu, Bhutan.\n\nAccording to proponents of intelligent urbanism, balance with nature emphasizes the distinction between utilizing resources and exploiting them. It focuses on the thresholds beyond which deforestation, soil erosion, aquifer depletion, siltation and flooding reinforce one another in urban development, saving or destroying life support systems. The principle promotes environmental assessments to identify fragile zones, threatened ecosystems and habitats that can be enhanced through conservation, density control, land use planning and open space design (McCarg: 1975). This principle promotes life cycle building energy consumption and pollutant emission analysis.\n\nThis principle states there is a level of human habitation intensity wherein the resources that are consumed will be replaced through the replenishing natural cycles of the seasons, creating environmental equilibrium. Embedded in the principle is contention that so long as nature can resurge each year; so long as the biomass can survive within its own eco-system; so long as the breeding grounds of fauna and avifauna are safe; so long as there is no erosion and the biomass is maintained, nature is only being utilized.\n\nUnderlying this principle is the supposition that there is a fragile line that is crossed when the fauna, which cross-fertilizes the flora, which sustains the soil, which supports the hillsides, is no longer there. Erosion, siltation of drainage networks and flooding result. After a point of no return, utilization of natural resources will outpace the natural ability of the eco-system to replenish itself. From there on degradation accelerates and amplifies. Deforestation, desertification, erosion, floods, fires and landslides all increase.\n\nThe principle states that blatant \"acts against nature\" include cutting of hillside trees, quarrying on slopes, dumping sewage and industrial waste into the natural drainage system, paving and plinthing excessively, and construction on steep slopes. This urban theory proposes that the urban ecological balance can be maintained when fragile areas are reserved, conservation of eco-systems is pursued, and low intensity habitation precincts are thoughtfully identified. Thus, the principles operate within the balance of nature, with a goal of protecting and conserving those elements of the ecology that nurture the environment. Therefore, the first principle of intelligent urbanism is that urbanization be in balance with nature.\n\nBalance with tradition is intended to integrate plan interventions with existing cultural assets, respecting traditional practices and precedents of style (Spreiregen: 1965). This urban planning principle demands respect for the cultural heritage of a place. It seeks out traditional wisdom in the layout of human settlements, in the order of building plans, in the precedents of style, in the symbols and signs that transfer meanings through decoration and motifs. This principle respects the order engendered into building systems through years of adaptation to climate, to social circumstances, to available materials and to technology. It promotes architectural styles and motifs designed to communicate cultural values.\n\nThis principle calls for orienting attention toward historic monuments and heritage structures, leaving space at the ends of visual axis to “frame\" existing views and vistas. Natural views and vistas demand respect, assuring that buildings do not block major sight lines toward visual assets.\n\nEmbedded in the principle is the concern for unique cultural and societal iconography of regions, their signs and symbols. Their incorporation into the spatial order of urban settings is promoted. Adherents promote the orientation and structuring of urban plans using local knowledge and meaning systems, expressed through art, urban space and architecture.\n\nPlanning decisions must operate within the balance of tradition, aggressively protecting, promoting and conserving generic components and elements of the urban pattern.\n\nAppropriate technology emphasizes the employment of building materials, construction techniques, infrastructural systems and project management which are consistent with local contexts (situation, setting or circumstances). People's capacities, geo-climatic conditions, locally available resources, and suitable capital investments all temper technology. Where there are abundant craftspeople, labour-intensive methods are appropriate. Where there is surplus savings, capital intensive methods are appropriate. For every problem there is a range of potential technologies, which can be applied, and an appropriate fit between technology and other resources must be established. Proponents argue that accountability and transparency are enhanced by overlaying the physical spread of urban utilities and services upon electoral constituencies, such that people’s representatives are interlinked with the urban technical systems needed for a civil society. This principle is in sync with \"small is beautiful\" concepts and with the use of local resources.\n\nThe fourth principle sponsors social interaction through public domains, in a hierarchy of places, devised for personal solace, companionship, romance, domesticity, \"neighborliness,\" community and civic life (Jacobs:1993). According to proponents of intelligent urbanism, vibrant societies are interactive, socially engaging and offer their members numerous opportunities for gathering and meeting one another. The PIU maintain that this can be achieved through design and that society operates within hierarchies of social relations which are space specific. The hierarchies can be conceptualized as a system of social tiers, with each tier having a corresponding physical place in the settlement structure.\n\nA goal of intelligent urbanism is to create places of solitude. These may be in urban forests, along urban hills, beside quiet streams, in public gardens and in parks where one can escape to meditate and contemplate. According to proponents, these are the quiet places wherein the individual consciousness dialogues with the rational mind. Idle and random thought sorts out complexities of modern life and allows the obvious to emerge. It is in these natural settings that the wandering mind finds its measure and its balance. Using ceremonial gates, directional walls and other “silent devices\" these spaces are denoted and divined. Places of the individual cultivate introspection. These spaces may also be the forecourts and interior courtyards of public buildings, or even the thoughtful reading rooms of libraries. Meditation focuses one's thought. Intelligent urbanism creates a domain for the individual to mature through self-analysis and self-realization.\n\nThe axiom insists that in city plans there must be spaces for “beautiful, intimate friendship\" where unfettered dialogue can happen. This principle insists that such places will not exist naturally in a modern urban fabric. They must be a part of the conscientious design of the urban core, of the urban hubs, of urban villages and of neighborhoods, where people can meet with friends and talk out life’s issues, sorrows, joys and dilemmas. This second tier is important for the emotional life of the populace. It sponsors strong mental health within the people, creating places where friendship can unfold and grow.\n\nThere must be spaces for householders, which may be in the form of dwellings for families, or homes for intimate companions, and where young workmates can form a common kitchen. Whatever their compositions, there must be a unique domain for social groups, familiar or biological, which have organized themselves into households. These domestic precincts are where families live and carry out their day-to-day functions of life. This third tier of conviviality is where the individual socializes into a personality.\n\nHousing clusters planned according to this axiom create a variety of household possibilities, which respond to a range of household structures and situations. It recognizes that households transform through the years, requiring a variety of dwellings types that respond to a complex matrix of needs and abilities, which are provided for in city plans.\n\nSmaller household domains must cluster into a higher social domain, the neighborhood social group.\nGood city planning practice sponsors, through design, such units of social space. It is in this fourth tier of social life that public conduct takes on new dimensions and groups learn to live peacefully among one another. It is through neighborhoods that the “social contract\" amongst diverse households and individuals is sponsored. This social contract is the rational basis for social relations and negotiations within larger social groups. Within neighborhoods basic amenities like creches, early learning centers, preventive health care and rudimentary infrastructure are maintained by the community.\n\nThe next social tier, or hierarchy, is the community. Historically, communities were tribes who shared social mores and cultural behavioral patterns. In contemporary urban settings communities are formed of diverse people. But these are people who share the common need to negotiate and manage their spatial settings. In plans created through the principles of intelligent urbanism these are called urban villages. Like a rural village, social bonds are found in the community management of security, common resources and social space. Urban villages will have defined social spaces, services and amenities that need to be managed by the community. According to proponents of intelligent urbanism these urban villages optimally become the administrative wards, and therefore the constituencies, of the elected members of municipal bodies. Though there are no physical barriers to these communities, they have their unique spatial social domain. Intelligent urbanism calls for the creation of dense, walkable zones in which the inhabitants recognize each other’s faces, share common facilities and resources, and often see each other at the village centre. This fifth tier of social space is where one needs initiative to join into various activities. It is intended to promote initiative and constructive community participation. There are opportunities for one to be involved in the management of services, and amenities and to meet new people. They accommodate primary education and recreation areas. Good planning practice promotes the creation of community places, where community-based organizations can manage common resources and resolve common problems.\n\nThe principles of intelligent urbanism call for city level domains. These can be plazas, parks, stadia, transport hubs, promenades, \"passages\" or gallerias. These are social spaces where everyone can go. In many cities one has to pay an entrance fee to access “public spaces\" like malls and museums. Unlike the lower tiers of the social hierarchy, this tier is not defined by any biological, familiar, face-to-face or exclusive characteristic. One may find people from all continents, from nearby districts and provinces and from all parts of the city in such places. By nature these are accessible and open spaces, with no physical, social or economic barriers. According to this principle it is the rules of human conduct that order this domain’s behavior. It is civility, or civilization, which protects and energizes such spaces. At the lower tiers, one meets people through introductions, through family ties, and through neighborhood circumstances.\n\nThese domains would include all freely accessible large spaces. These are places where outdoor exhibits are held, sports matches take place, vegetables are sold and goods are on display. These are places where visitors to the city meander amongst the locals. Such places may stay the same, but the people are always changing. Most significant, these city scale public domains foster public interaction; they sponsor unspoken ground rules for unknown people to meet and to interact. They nurture civic understanding of the strength of diversity, variety, a range of cultural groups and ethnic mixes. It is this higher tier of social space which defines truly urbane environments.\n\nEvery social system has its own hierarchy of social relations and interactions. Intelligent urbanism sees cyberspace as a macro tier of conviviality, but does not discount physical places in forging relationships due to the Internet. These are reflected through a system of ‘places’ that respond to them. Good urban planning practice promotes the planning and design of such ‘places’ as elemental components of the urban structure.\n\nThe principle of efficiency promotes a balance between the consumption of resources such as energy, time and fiscal resources, with planned achievements in comfort, safety, security, access, tenure, productivity and hygiene. It encourages optimum sharing of public land, roads, facilities, services and infrastructural networks, reducing per household costs, while increasing affordability, productivity, access and civic viability.\n\nIntelligent urbanism promotes a balance between performance and consumption. Intelligent urbanism promotes efficiency in carrying out functions in a cost effective manner. It assesses the performance of various systems required by the public and the consumption of energy, funds, administrative time and the maintenance efforts required to perform these functions.\n\nA major concern of this principle is transport. While recognizing the convenience of personal vehicles, it attempts to place costs (such as energy consumption, large paved areas, parking, accidents, negative balance of trade, pollution and related morbidity) on the users of private vehicles.\n\nGood city planning practice promotes alternative modes of transport, as opposed to a dependence on personal vehicles. It promotes affordable public transport. It promotes medium to high-density residential development along with complementary social amenities, convenience shopping, recreation and public services in compact, walkable mixed-use settlements. These compact communities have shorter pipe lengths, wire lengths, cable lengths and road lengths per capita. More people share gardens, shops and transit stops.\n\nThese compact urban nodes are spaced along regional urban transport corridors that integrate the region’s urban nodes, through public transport, into a rational system of growth. Good planning practice promotes clean, comfortable, safe and speedy, public transport, which operates at dependable intervals along major origin and destination paths. Such a system is cheaper, safer, less polluting and consumes less energy.\n\nThe same principle applies to public infrastructure, social facilities and public services. Compact, high-density communities result in more efficient urban systems, delivering services at less cost per unit to each citizen.\n\nThere is an appropriate balance to be found somewhere on the line between wasteful low-density individual systems and over-capitalized mega systems. Individual septic tanks and water bores servicing individual households in low-density fragmented layouts, allow the use of filtered greywater for free irrigation of gardens, but, if not maintained, can cause a local pollution of subterranean aquifer systems. The bores can dramatically lower ground water levels especially during droughts. The vantage of septic tanks an bores is to be managed by the very users, at no cost for the community.\nAlternatively, large-scale, citywide sewerage systems and regional water supply systems are capital intensive and prone to management and maintenance dysfunction, if not corruption or extortion by private companies. Operating costs, user fees and cost recovery expenses are high. There is a balance wherein medium-scale systems, covering compact communities, utilize modern technology, without the pitfalls of large-scale infrastructure systems. This principle of urbanism promotes the middle path with regard to public infrastructure, facilities, services and amenities.\n\nWhen these appropriate facilities and service systems overlap electoral constituencies, the “imagery\" between user performance in the form of payments for services, systems dependability through managed delivery, and official response through effective representation, should all become obvious and transparent.\n\nGood city planning practices promote compact settlements along dense urban corridors, and within populated networks, such that the numbers of users who share costs are adequate to support effective and efficient infrastructure systems. Intelligent urbanism is intended to foster movement on foot, linking pedestrian movement with public transport systems at strategic nodes and hubs. Medium-scale infrastructural systems, whose catchment areas overlap political constituencies and administrative jurisdictions, result in transparent governance and accountable urban management.\n\nIntelligent urbanism encourages ground level, pedestrian oriented urban patterns, based on anthropometric dimensions. Walkable, mixed use urban villages are encouraged over single-function blocks, linked by motor ways, and surrounded by parking lots.\n\nAn abiding axiom of urban planning, urban design and city planning has been the promotion of people friendly places, pedestrian walkways and public domains where people can meet freely. These can be parks, gardens, glass-covered gallerias, arcades, courtyards, street side cafes, river- and hill-side stroll ways, and a variety of semi-covered spaces.\n\nIntelligent urbanism promotes the scale of the pedestrian moving on the pathway, as opposed to the scale of the automobile on the expressway. Intelligent urbanism promotes the ground plan of imaginable precincts, as opposed to the imagery of façades and the monumentality of the section. It promotes the personal visibility of places moving on foot at eye level.\n\nIntelligent urbanism advocates removing artificial barrier and promotes face-to-face contact. Proponents argue that the automobile, single use zoning and the construction of public structures in isolated compounds, all deteriorate the human condition and the human scale of the city.\n\nAccording to PIU proponents, the trend towards urban sprawl can be overcome by developing pedestrian circulation networks along streets and open spaces that link local destinations. Shops, amenities, day care, vegetable markets and basic social services should be clustered around public transport stops, and at a walkable distance from work places, public institutions, high and medium density residential areas. Public spaces should be integrated into residential, work, entertainment and commercial areas. Social activities and public buildings should orient onto public open spaces. These should be the interchange sites for people on the move, where they can also revert into the realm of “slowness,\" of community life and of human interaction.\n\nHuman scale can be achieved through building masses that “step down\" to human scale open spaces; by using arcades and pavilions as buffers to large masses; by intermixing open spaces and built masses sensitively; by using anthropometric proportions and natural materials. Traditional building precedents often carry within them a human scale language, from which a contemporary fabric of build may evolve.\n\nThe focus of intelligent urbanism is the ground plane, pedestrian movement and interaction along movement channels, stems, at crossing nodes, at interactive hubs and within vibrant urban cores. The PIU holds many values in common with Transit Oriented Development, but the PIU goal is not merely to replace the automobile, nor to balance it. These are mundane requirements of planning, which the PIU assumes are found in every design and urban configuration. The PIU goal is to enrich the human condition and to enhance the realm of human possibilities.\n\nIntelligent urbanism conceives of urbanity as a process of facilitating human behavior toward more tolerant, more peaceful, more accommodating and more sensitive modalities of interaction and conflict resolution. Intelligent urbanism recognizes that ‘urbanity’ emerges where people mix and interact on a face-to-face basis, on the ground, at high densities and amongst diverse social and economic groups. Intelligent urbanism nurtures ‘urbanity’ through designs and plans that foster human scale interaction.\n\nThe PIU envisions the city as a vehicle for personal, social, and [economic development], through access to a range of organizations, services, facilities and information providing a variety of opportunities for enhanced employment, economic engagement, education, and recreation. This principle aims to increase access to shelter, health care and human resources development. It aims to increase safety and hygienic conditions. The city is a place of economic opportunity. This is generally said with regard to urban annual net product, enriched urban economic base, sustained employment generation and urban balance of trade. More significantly this is true for the individuals who settle in cities. Moreover, cities are places where individuals can increase their knowledge, skills and sensitivities. Cities provide access to health care and preventive medicine. They provide a great umbrella of services under which the individual can leave aside the struggle for survival, and get on with the finer things of life.\n\nThe PIU sees cities as catalysts for personal definition and self-discovery. In cities people get inspired, build a drive to achieve, discover aspects of their personalities, skills and intellectual curiosity which they use to craft their identity.\n\nThe city provides a range of services and facilities, whose realization in villages are the all-consuming struggle of rural inhabitants. Potable water; sewerage management; energy for cooking, heat and lighting are all piped and wired in; solid waste disposal and storm water drainage are taken for granted. The city offers access through roads, public transit, telephones and the Internet. The peace and security provided by effective policing systems, and the courts of law, are just assumed to be there in the city. Then there are the schools, the recreation facilities, the health services and a myriad of professional services offered in the city market place.\n\nIntelligent urbanism views the city as an opportunity system. Yet these opportunities are not equally distributed. Security, health care, education, shelter, hygiene, and most of all employment, are not equally accessible. Proponents of intelligent urbanism see the city as playing an equalizing role allowing citizens to grow according to their own essential capabilities and efforts. If the city is an institution, which generates opportunities, intelligent urbanism promotes the concept of equal access to opportunities within the urban system.\n\nIntelligent urbanism promotes a guaranteed access to education, health care, police protection, and justice before the law, potable water, and a range of basic services. Perhaps this principle, more than any other, distinguishes intelligent urbanism from other elitist, efficiency oriented urban charters and regimes.\n\nIntelligent urbanism does not say every household will stay in an equivalent house, or travel in the same vehicle, or consume the same amount of electricity.\n\nIntelligent urbanism recognizes the existence of poverty, of ignorance, of ill health, of malnutrition, of low skills, of gender bias and ignorance of the urban system itself. Intelligent urbanism is courageous in confronting these forms of inequality, and backlogs in social and economic development. Intelligent urbanism sees an urban plan, not only as a physical plan, but also as a social plan and as an economic plan.\n\nThe ramifications of this understanding are that the people living in intelligent cities should not experience urban development in “standard doses\". In short, people may be born equal or unequal, but they grow inequitably. An important role of the city is to provide a variety of paths and channels for each individual to set right their own future, against the inequity of their past, or the special challenges they face. According to proponents of this principle this is the most salient aspect of a free society; than even voting rights access to opportunity is the essence of self-liberation and human development (Sen:2000).\n\nAccording to proponents of intelligent urbanism, there will be a variety of problems faced by urbanites and they need a variety of opportunity channels for resolution. If there are ten problem areas where people are facing stresses, like economic engagement, health, shelter, food, education, recreation, transport, etc., there must be a variety of opportunities through which individuals and households can resolve each of these stresses. There must be ten channels to resolve each of ten stresses! If this opportunity matrix is understood and responded to, the city is truly functioning as an opportunity matrix. For example, opportunities for shelter could be through the channels of lodges, rented rooms, studio apartments, bedroom apartments and houses. It could be through the channels of ownership, through a variety of tendencies. It could be through opportunities for self-help, or incremental housing. It could be through the up-gradation of slums. Intelligent urbanism promotes a wide range of solutions, where any stress is felt. It therefore promotes a range of problem statements, options, and variable solutions to urban stresses.\n\nIntelligent urbanism sees cities as processes. Proponents argue that good urban plans facilitate those processes and do not place barriers before them. For example, it does not judge a “slum\" as a blight on society; it sees the possibility that such a settlement may be an opportunity channel for entry into the city. Such a settlement may be the only affordable shelter, within easy access to employment and education, for a new immigrant household in the city. According to intelligent urbanism, if the plan ignores, or destroys such settlements, it is creating a city of barriers and despair wherein a poor family, offering a good service to the city, is denied a modicum of basic needs for survival. Alternatively, if the urban plan recognizes that the “slum\" is a mechanism for self development, a spring-board from which children have access to education, a place which can be up-graded with potable water, basic sanitary facilities, street lights and paving…then it is a plan for opportunity. Intelligent urbanism believes that there are slums of hope and slums of despair. It promotes slums of hope, which contribute, not only to individual opportunities, but also to nation building.\n\nThe opportunity matrix must also respond to young professionals, to skilled, well-paid day laborers, to the upper middle class and to affluent entrepreneurs. If a range of needs, of abilities to pay, of locational requirements, and of levels of development of shelter, is addressed, then opportunities are being created.\n\nIntelligent urbanism believes that private enterprise is the logical provider of opportunities, but that alone it will not be just or effective. The regime of land, left to market forces alone, will create an exclusive, dysfunctional society. Intelligent urbanism believes that there is an essential role for the civil society to intervene in the opportunity matrix of the city.\n\nIntelligent urbanism promotes opportunities through access to:\n\n\nIntelligent urbanism proposes that enterprise can only flourish where a public framework provides opportunities for enterprise. This system of opportunities operates through public investments in economic and social infrastructure; through incentives in the form of appropriate finance, tax inducements, subsidized skill development for workers, and: regulations which protect the environment, safety, hygiene and health. To ensure a stable playing field where one can make an investment with predictable returns, a modicum of regulation is necessary. Proponents argue that it is through government regulations that private investment can be protected from fraud. It is through government regulation that the under-pinning conditions for free enterprise can be protected.\n\nIntelligent urbanism envisions the city as an organic part of a larger environmental, socio-economic and cultural-geographic system, essential for its sustainability. This zone of influence is the region. Likewise, it sees the region as integrally connected to the city. Intelligent urbanism sees the planning of the city and its hinterland as a single holistic process. Proponents argue if one does not recognize growth as a regional phenomenon, then development will play a hopscotch game of moving just a bit further along an arterial roads, further up valleys above the municipal jurisdiction, staying beyond the path of the city boundary, development regulations and of the urban tax regime.\n\nThe region may be defined as the catchment area from which employees and students commute into the city on a daily basis. It is the catchment area from which people choose to visit one city, as opposed to another, for retail shopping and entertainment. Economically the city region may include the hinterland that depends on its wholesale markets, banking facilities, transport hubs and information exchanges. The region needing integration may be seen as the zone from which perishable foods, firewood and building materials supply the city. The economic region can also be defined as the area managed by exchanges in the city. Telephone calls to the region go through the city's telecom exchange; post goes through the city's general post office; money transfers go through the city’s financial institutions and internet data passes electronically through the city’s servers. The area over which “city exchanges\" disperse matter can well be called the city’s economic hinterland or region. Usually the region includes dormitory communities, airports, water reservoirs, perishable food farms, hydro facilities, out-of-doors recreation and other infrastructure that serves the city. Intelligent urbanism sees the integrated planning of these services and facilities as part of the city planning process.\n\nIntelligent urbanism understands that the social and economic region linked to a city also has a physical form, or a geographic character. A hierarchy of watersheds, creating valleys and defining edges of neighborhoods, may define the geographic character. Forest ranges, fauna and avifauna habitats are set within such regions and are connected by natural corridors for movement and cross-fertilization. Within this larger, environmental scenario, one must conceptualize urbanism in terms of watersheds, subterranean aquifer systems, and other natural systems that operate across the entire region. Economic infrastructure, such as roads, hydro basins, irrigation channels, water reservoirs and related distribution networks usually follow the terrain of the regional geography. The region’s geographic portals, and lines of control, may also define defense and security systems deployment.\n\nIntelligent urbanism recognizes that there is always a spillover of population from the city into the region, and that population in the region moves into the city for work, shopping, entertainment, health care and education. With thoughtful planning the region can take pressure off of the city. Traditional and new settlements within the urban region can be enhanced and densified to accommodate additional urban households. There are many activities within the city, which are growing and are incompatible with urban habitat. Large, noisy and polluting workshops and manufacturing units are amongst these. Large wholesale markets, storage sheds, vehicular maintenance garages, and waste management facilities need to be housed outside of the city’s limits in their own satellite enclaves. In larger urban agglomerations a number of towns and cities are clustered around a major urban center forming a metropolitan region.\n\nIntelligent urbanism is not just planning for the present; it is also planning for the distant future. Intelligent urbanism is not Utopian, but futuristic in its need to forecast the scenarios to come, within its own boundaries, and within the boundaries of the distant future.\n\nIntelligent urbanism advocates integrated transport systems comprising walkways, cycle paths, bus lanes, light rail corridors, under-ground metros and automobile channels. A balance between appropriate modes of movement is proposed. More capital intensive transport systems should move between high density nodes and hubs, which interchange with lower technology movement options. These modal split nodes become the public domains around which cluster high density, pedestrian, mixed-use urban villages (Taniguchi:2001).\n\nThe PIU accepts that the automobile is here to stay, but that it should not be made essential by design. A well planned metropolis would densify along mass transit corridors and around major urban hubs. Smaller, yet dense, urban nodes are seen as micro-zones of medium level density, public amenities and pedestrian access. At these points lower level nodal split will occur, such as between bus loops and cycle tracts. The PIU views nodal split points as places of urban conviviality and access to services and facilities. Modal split can be between walking, cycling, driving, and mass transit. Bus loops may feed larger rail-based rapid-movement corridors. Social and economic infrastructure becomes more intensive as movement corridors become more intense.\n\nIntelligent urbanism holds that good practices inherent in considered principles can only be realized through accountable, transparent, competent and participatory local governance, founded on appropriate data bases, due entitlements, civic responsibilities and duties. The PIU promotes a range of facilitative and promotive urban development management tools to achieve appropriate urban practices, systems and forms(Islam:2000). None of the principles or practices the PIU promotes can be implemented unless there is a strong and rational institutional framework to define, channel and legalize urban development, in all of its aspects. Intelligent urbanism envisions the institutional framework as being very clear about the rules and regulations it sponsors and that those using discretion in implementing these measures must do so in a totally open, recorded and transparent manner.\n\nIntelligent urbanism facilitates the public in carrying out their honest objectives. It does not regulate and control the public. It attempts to reduce the requirements, steps and documentation required for citizens to process their proposals.\n\nIntelligent urbanism is also promotive in furthering the interests of the public in their genuine utilization of opportunities. It promotes site and services schemes for households who can construct their own houses. It promotes up-gradation of settlements with inadequate basic services. It promotes innovative financing to a range of actors who can contribute to the city’s development. Intelligent urbanism promotes a limited role for government, for example in “packaging\" large-scale urban development schemes, so that the private sector is promoted to actually build and market urban projects, which were previously built by the government.\n\nIntelligent urbanism does not consider itself naïve. It recognizes that there are developers and promoters who have no long term commitment to their own constructions, and their only concern is to hand over a dwelling, gain their profit and move on. For these players it is essential to have Development Control Regulations, which assure the public that the products they invest in are safe, hygienic, orderly, durable and efficient. For the discerning citizen, such rules also lay out the civil understanding by which a complex society agrees to live together.\n\nThe PIU contends that there must be a cadastral System wherein all of the land in the jurisdiction of cities is demarcated, surveyed, characterized and archived, registering its legal owner, its legal uses, and the tax defaults against it.\n\nThe institutional framework can only operate where there is a Structure Plan, or other document that defines how the land will be used, serviced, and accessed. The Structure Plan tells landowners and promoters what the parameters of development are, which assures that their immediate investments are secure, and that the returns and use of such efforts are predictable. A Structure Plan is intended to provide owners and investors with predictable future scenarios. Cities require efficient patterns for their main infrastructure systems and utilities. According to PIU proponents, land needs to be used in a judicious manner, organizing complementary functions and activities into compact, mixed use precincts and separating out non-compatible uses into their own precincts. In a similar manner, proponents argue it is only through a plan that heritage sites and the environment can be legally protected. Public assets in the form of nature, religious places, heritage sites and open space systems must be designated in a legal plan.\n\nIntelligent urbanism proposes that the city and its surrounding region be regulated by a Structure Plan, or equivalent mechanism, which acts as a legal instrument to guide the growth, development and enhancement of the city.\n\nAccording to proponents, there must be a system of participation by the “Stake Holders\" in the preparation of plans. Public meetings, hearings of objections and transparent processes of addressing objections, must be institutionalized. Intelligent urbanism promotes Public Participation. Local Area Plans must be prepared which address local issues and take into account local views and sentiments regarding plan objectives, configurations, standards and patterns. Such plans lay out the sites of plots showing the roads, public open spaces, amenities areas and conservation sites. Land Pooling assures the beneficiaries from provision of public infrastructure and amenities proportionally contribute and that a few individuals do not suffer from reservations in the plan.\n\nAccording to proponents, there must be a system of Floor Area Ratios to assure that the land and the services are not over pressured. No single plot owner should have more than the determined \"fair share\" of utilization of the access roads, amenities and utilities that service all of the sites. Floor Area Ratios temper this relationship as regulated the manner in which public services are consumed. According to PIU proponents, Transfer of Development Rights benefits land owners whose properties have been reserved under the plan. It also benefits the local authorities that lack the financial resources to purchase lands to implement the Structure Plans. It benefits concentrated, city center project promoters who have to amortize expensive land purchases, by allowing them to purchase the development rights from the owners of reserved lands and to hand over those properties to the plan implementing authority. This allows the local authority to widen roads and to implement the Structure Plan. The local authority then transfers the needed development right to city center promoters.\n\nIntelligent urbanism supports the use of Architectural Guidelines where there is a tradition to preserve and where precedents can be used to specify architectural elements, motifs and language in a manner, which intended to reinforce a cultural tradition. Building designs must respect traditional elements, even though the components may vary greatly to integrate contemporary functions. Even in a greenfield setting Architectural Guidelines are required to assure harmony and continuity of building proportions, scale, color, patterns, motifs, materials and facades.\n\nIntelligent urbanism insists on safety, hygiene, durability and utility in the design and construction of buildings. Where large numbers of people gather in schools, hospitals, and other public facilities that may become emergency shelters in disasters, special care must be exercised. A suitable Building Code is the proposed instrument to achieve these aims.\n\nPIU proponents state that those who design buildings must be professionally qualified architects; those who design the structures (especially of more than ground plus two levels) must be professionally qualified structural engineers; those who build buildings must be qualified civil engineers; and, those who supervise and control construction must be qualified construction managers. Intelligent urbanism promotes the professionalisation of the city making process. While promoting professionalism, intelligent urbanism proposes that this not become a barrier in the development process. Small structures, low-rise structures, and humble structures that do not house many people can be self designed and constructed by the inhabitants themselves. Proponents maintain that there must be recognized Professional Accrediting Boards, or Professional Bodies, to see that urban development employs adequate technical competence.\n\nFinally, there must be legislation creating statutory local authorities, and empowering them to act, manage, invest, service, protect, promote and facilitate urban development and all of the opportunities that a modern city must sponsor.\n\nIntelligent urbanism insists that cities, local authorities, regional development commissions and planning agencies be professionally managed. City Managers can be hired to manage the delivery of services, the planning and management of planned development, the maintenance of utilities and the creation of amenities.\n\nIntelligent urbanism views plans and urban designs and housing configurations as expressions of the people for whom they are planned. The processes of planning must therefore be a participatory involving a range of stakeholders. The process must be a transparent one, which makes those privileged to act as guardians of the people’s will accountable for their decisions and choices. Intelligent urbanism sees urban planning and city governance as the most salient expressions of civility. Intelligent urbanism fosters the evolution of institutional systems that enhance transparency, accountability and rational public decision making.\n\nThough not necessarily related to the principles of intelligent urbanism, there are examples representing all or some of them in urban design theory and practice.\nConcurrently, the recent movements of New Urbanism and New Classical Architecture promote a sustainable approach towards construction, that appreciates and develops smart growth, architectural tradition and classical design. This in contrast to modernist and globally uniform architecture, as well as leaning against solitary housing estates and suburban sprawl. Both trends started in the 1980s. The Driehaus Architecture Prize is an award that recognizes efforts in New Urbanism and New Classical Architecture, and is endowed with a prize money twice as high as that of the modernist Pritzker Prize.\n\n\n"}
{"id": "927051", "url": "https://en.wikipedia.org/wiki?curid=927051", "title": "RYB color model", "text": "RYB color model\n\nRYB (an abbreviation of red–yellow–blue) is a historical set of colors used in subtractive color mixing and is one commonly used set of primary colors. It is primarily used in art and design education, particularly painting.\n\nRYB predates modern scientific color theory, which has determined that cyan, magenta, and yellow are the best set of three colorants to combine, for the widest range of high-chroma colors.\n\nRYB (red–yellow–blue) make up the primary color triad in a standard artist's color wheel. The secondary colors purple–orange–green (sometimes called violet–orange–green) make up another triad. Triads are formed by three equidistant colors on a particular color wheel. Other common color wheels represent the light model (RGB) and the print model (CMYK).\n\nThe first known instance of the RYB triad can be found in the work of Franciscus Aguilonius (1567–1617), although he did not arrange the colors in a wheel.\n\nIn his experiments with light, Isaac Newton recognized that colors could be created by mixing color primaries. In his \"Opticks\", Newton published a color wheel to show the geometric relationship between these primaries. This chart was later confused and understood to apply to pigments as well, though Newton was also unaware of the differences between additive and subtractive color mixing.\n\nThe RYB model was used for printing, by Jacob Christoph Le Blon, as early as 1725.\n\nIn the 18th century, the RYB primary colors became the foundation of theories of color vision, as the fundamental sensory qualities that are blended in the perception of all physical colors and equally in the physical mixture of pigments or dyes. These theories were enhanced by 18th-century investigations of a variety of purely psychological color effects, in particular the contrast between \"complementary\" or opposing hues that are produced by color afterimages and in the contrasting shadows in colored light. These ideas and many personal color observations were summarized in two founding documents in color theory: the \"Theory of Colors\" (1810) by the German poet and government minister Johann Wolfgang von Goethe, and \"The Law of Simultaneous Color Contrast\" (1839) by the French industrial chemist Michel-Eugène Chevreul.\n\nPainters have long used more than three RYB primary colors in their palettes, and at one point considered red, yellow, blue and green to be the \"four\" primaries. Red, yellow, blue and green are still widely considered the four psychological primary colors, though red, yellow and blue are sometimes listed as the \"three\" psychological primaries, with black and white occasionally added as a fourth and fifth.\n\nThe cyan, magenta, and yellow primary colors associated with CMYK printing are sometimes known as \"process blue\", \"process red\" and \"process yellow\".\n\n"}
{"id": "2280917", "url": "https://en.wikipedia.org/wiki?curid=2280917", "title": "Robert Norman", "text": "Robert Norman\n\nRobert Norman was a 16th-century-English mariner, compass builder, and hydrographer who discovered magnetic inclination, the deviation of the Earth's magnetic field from the vertical.\n\nRobert Norman is noted for \"The Newe Attractive\", a pamphlet published in 1581 describing the lodestone (magnet) and practical aspects of navigation. More importantly, it included Norman's discovery of magnetic dip, the incline at an angle from the horizon by a compass needle. This effect is caused by the Earth's magnetic field not running parallel to the planet's surface. Norman demonstrated magnetic dip by creating a compass needle that pivoted on a horizontal axis. The needle tilted at a steep angle relative to the horizon line. \n\nMagnetic inclination and local variations were known before Robert Norman, but his pamphlet had a greater influence than the earlier work.\n\nThe crater Norman on the Moon is named in his honour.\n\n"}
{"id": "39364055", "url": "https://en.wikipedia.org/wiki?curid=39364055", "title": "SACLA", "text": "SACLA\n\nThe SPring-8 Angstrom Compact free electron LAser, referred to as SACLA (pronounced \"さくら (Sa-Ku-Ra)\"), is an free-electron laser (X-FEL) in Japan, embedded in the SPring-8 accelerator and synchrotron complex. When it first came into operation 2011, it was the second X-FEL in the world and the first in Japan.\n\nLike other X-FELs, SACLA uses self-amplified spontaneous emission to achieve extremely high intensities of X-rays. SACLA uses in-vacuum, short-period undulators, which is one of the unique factors in its design that allows it to achieve sub-Ångstrom wavelengths of 0.6 Å at a relatively much shorter distance of 0.7 km, compared to other similar X-FELs like LCLS (2 km) or the European X-FEL (3.4 km). An 8.5 GeV electron beam is used as the source.\n\nSACLA has released a number of animated short films to promote its research capabilities to the public. In July 2013, SACLA released two animated short films titled \"Picotopia\", which discussed the cellular biology, and \"Wasureboshi\", which is about conception.\n\nOn December 3, 2013, another animated short titled \"Mirai Koshi: Harima SACLA\" was released to promote the X-FEL's ability to detect atoms and molecules.\n\n"}
{"id": "387979", "url": "https://en.wikipedia.org/wiki?curid=387979", "title": "Saturn I SA-3", "text": "Saturn I SA-3\n\nSaturn-Apollo 3 (SA-3) was the third flight of the Saturn I launch vehicle, the second flight of Project Highwater, and part of the American Apollo program. The rocket was launched on November 16, 1962, from Cape Canaveral, Florida.\n\nThe Saturn I launch vehicle components were delivered to Cape Canaveral by the barge \"Promise\" on September 19, 1962, but erection of the first-stage booster onto its launch pedestal was delayed until September 21 due to a tropical depression that moved over the Florida peninsula. The dummy second and third stages (S-IV and S-V) and payload were assembled on the booster on September 24. Ballast water was loaded into the dummy stages on October 31, and the RP-1 fuel was loaded on November 14.\n\nFor this launch, Cape Canaveral director Kurt Debus asked Marshall Space Flight Center director Wernher von Braun, who was overseeing the Saturn project, that no outside visitors be allowed on NASA grounds due to the ongoing tensions of the Cuban missile crisis.\n\nSaturn-Apollo 3 was launched at 17:45:02 on November 16, 1962, from Launch Complex 34. The only hold in the countdown sequence was for 45 minutes due to a power failure in ground support equipment. This mission was the first time the Saturn I rocket was launched with a full load of propellant, carrying approximately of fuel.\n\nThe vehicle's four inner H-1 engines shut down at 2 minutes 21.66 seconds after launch and an altitude of , and its four outer engines shut down at 2 minutes 29.09 seconds and ; both sets burned slightly longer than was initially estimated, reaching a maximum velocity of . The vehicle continued to coast to an altitude of and range of , at which point, 4 minutes 52 seconds after launch, officials sent a terminate command to the rocket, setting off several charges which caused the dummy stages of the vehicle to destruct. The first stage remained intact, though uncontrolled, until it impacted the Atlantic Ocean around from its launch site.\n\nThe main objectives of SA-3 were much the same as the previous two Saturn I flights in that it was primarily a test of the first-stage booster (S-I) and its H-1 engines. According to the NASA report \"Results of the Third Saturn 1 Launch Vehicle Test Flight\", SA-3 aimed to test four areas: the booster, the ground support equipment, the vehicle in flight, and Project Highwater.\n\nThe test of the booster involved the propulsion system, structural design, and control systems. The ground support test involved the facilities and equipment used in the launch, including propellant systems, automatic checkout equipment, launch platform, and support towers. The vehicle in flight test measured aeroballistics, which confirmed values of aerodynamic characteristics such as stability and performance; propulsion, which ensured the engines could provide enough thrust to propel the vehicle at the correct velocity and trajectory, as well as provide data on the performance of all eight engines during flight; structural and mechanical, which provided measurements of the vehicle's stress and vibration levels through all phases of flight; and guidance and control, which demonstrated that spacecraft systems could accurately provide orientation and velocity information.\n\nThe fourth objective, Project Highwater, was an experiment previously flown on SA-2. This involved the intentional release of ballast water from the second and third stages which allowed scientists to investigate the nature of Earth's ionosphere, as well as noctilucent clouds and the behavior of ice in space.\n\nFor Project Highwater, tanks in SA-3's dummy upper stages were filled with of water, approximately , which was used to simulate the mass of future Saturn payloads. The water was divided roughly in half between the two dummy stages. When the terminate command was sent to the rocket, primacord charges split both stages longitudinally, instantly releasing its load of water. The experiment was tracked by cameras and other equipment on the ground and in aircraft. Observers at Cape Canaveral reported that the ice cloud was visible for about three seconds and was \"several miles across\".\n\nNASA declared all engineering goals of the flight as achieved, despite occasional issues with telemetry during flight and some measurement data being unusable or only partially usable. Project Highwater on SA-3 was also declared successful, though again, telemetry issues produced questionable results.\n\nThe NASA \"Results\" report states that ten special tests were included in the SA-3 flight, all focused on technologies and procedures intended for use on future Apollo missions.\n\nAs mentioned earlier, SA-3 was the first Apollo flight to carry a full load of propellant, compared to earlier flights that carried approximately 83% of maximum capacity. This had the effect of testing the rocket's reaction to slower acceleration and extended first stage flight time. Also on this mission, the outboard engines were allowed to fire until depletion of the rocket's liquid oxygen (LOX), rather than the timed cut-offs of previous flights.\n\nSA-3 also featured the first use of retrorockets on Apollo hardware. These were the only functional part on SA-3 of what would become the S-I/S-IV stage separation system, which would separate the two stages in later missions. These four small solid rockets were located 90 degrees apart around the top of the S-I stage, with their nozzles aimed up. At 2 minutes 33.66 seconds after launch, the rockets fired for about 2.1 seconds. A minor misalignment of the rockets caused a 4.3 degree per second roll of the vehicle, which caused the spacecraft's ST-90 and ST-124P inertial platforms to fail after 15 degrees of rotation. This was considered incidental to the flight and did not impact mission success.\n\nThe ST-124P inertial platform ('P' for prototype) was a component of the guidance and control system, and contained gyroscopes and accelerometers that fed information to control computers. Once out of the atmosphere, this information provided steering signals to the gimbaled engines. During SA-3, this platform was an inactive component; while functioning and monitored during the flight, it had no control over the vehicle, and was used only to compare performance with the then-standard ST-90 platform, which was also an inactive component for the flight. For this mission, both platforms were located on the interstage between S-I and S-IV; Saturn IB and Saturn V vehicles would have one on the Instrument Unit atop the S-IVB stage.\n\nTwo new transmitters were included on SA-3. The pulse code modulated (PCM) data link transmitted digital data, which would be vital to providing automated spacecraft checkout and launch procedures on future flights. The unit operated with high signal strength, indicating that it would provide very accurate data. An ultra high frequency (UHF) radio link was also tested on SA-3. It would be used to transmit sensor measurements which could not be effectively transmitted at lower frequencies. The system performed satisfactorily, and post-flight documentation indicated engineers may expand its role for future telemetry transmission.\n\nA Block II antenna panel was tested during flight. Located between propellant tanks, it provided stronger and more consistent signal strength than the Block I panel.\n\nTemperature measurements of the S-IV dummy stage and interstage fairing were carried out with eighteen temperature probes, called thermocouples. These were used to detect temperature changes around protuberances on the stage's skin and in the area of the retrorockets during operation. For the S-IV stage, temperatures were within expected levels, though a heating rate around twice that predicted was encountered. On the interstage, during retrorocket firing, a maximum temperature of was seen, indicating something unknown may have caused an abnormally high reading.\n\nA single panel of Block II M-31 heat shield insulation, along with one of the spacecraft's calorimeters, was mounted on the base of the first stage by the engines. This test measured heat flux through the new insulation compared to the material normally used on Saturn I Block I flights.\n\nA dynamic pressure study was conducted for the Centaur program, in which two aluminum panels were mounted to the payload adapter atop the S-V stage and equipped with 11 pressure sensors. This study was performed due to the failure of the first Centaur vehicle flown, suspected to result from an adverse pressure environment around the shoulder of the vehicle. The test found that a very low pressure region formed just behind the shoulder while the vehicle was at Mach 0.7.\n\nFinally, a new umbilical tower and Block II swing arm were used for the first time in preparation for future Block II Saturn I flights.\n"}
{"id": "65791", "url": "https://en.wikipedia.org/wiki?curid=65791", "title": "Shenzhou (spacecraft)", "text": "Shenzhou (spacecraft)\n\nShenzhou ( or ; ) is a spacecraft developed and operated by China using Soyuz technology to support its manned spaceflight program. The name is variously translated as \"divine vessel\", \"divine craft\", or \"divine ship\". Its design resembles the Russian Soyuz spacecraft, but it is larger in size. The first launch was on November 19, 1999 and the first manned launch was on October 15, 2003. In March 2005, an asteroid was named 8256 Shenzhou in honour of the spacecraft.\n\nChina's first efforts at human spaceflight started in 1968 with a projected launch date of 1973. Although China successfully launched an unmanned satellite in 1970, its manned spacecraft program was cancelled in 1980 due to lack of funds.\n\nThe Chinese manned spacecraft program was relaunched in 1992 with Project 921. The Phase One spacecraft followed the general layout of the Russian Soyuz spacecraft, with three modules that could separate for reentry. China signed a deal with Russia in 1995 for the transfer of Soyuz technology, including life support and docking systems. The Phase One spacecraft was then modified with the new Russian technology. The general designer of Shenzhou-1 through Shenzhou-5 was Qi Faren (戚发轫, Apr 26, 1933 -), and from Shenzhou-6 on, the general design was turned over to Zhang Bainan (张柏楠, Jun 23, 1962 -).\n\nThe first unmanned flight of the spacecraft was launched on November 19, 1999, after which Project 921/1 was renamed Shenzhou, a name reportedly chosen by Jiang Zemin. A series of three additional unmanned flights ensued. The first manned launch took place on 15 October 2003 with the Shenzhou 5 mission. The spacecraft has since become the mainstay o the Chinese manned space program, being used for both manned and unmanned missions.\n\nShenzhou consists of three modules: a forward orbital module (轨道舱), a reentry module (返回舱) in the middle, and an aft service module (推进舱). This division is based on the principle of minimizing the amount of material to be returned to Earth. Anything placed in the orbital or service modules does not require heat shielding, and this increases the space available in the spacecraft without increasing weight as much as it would if those modules were also able to withstand reentry. Thus both Soyuz and Shenzhou have more living area with less weight than the Apollo CSM.\n\n\nThe orbital module (轨道舱) contains space for experiments, crew-serviced or operated equipment, and in-orbit habitation. Without docking systems, Shenzhou 1–6 carried different kinds of payload on the top of their orbital modules for scientific experiments.\n\nUp until Shenzhou 8, the orbital module of the Shenzhou was equipped with its own propulsion, solar power, and control systems, allowing autonomous flight. It was possible for Shenzhou to leave an orbital module in orbit for redocking with a later spacecraft, something which the Soyuz cannot do since the only hatch between orbital and reentry modules is a part of reentry module, and orbital module is depressurized after separation. In the future it is possible that the orbital module(s) could also be left behind on the planned Chinese project 921/2 space station as additional station modules.\n\nIn the unmanned test flights launched to date, the orbital module of each Shenzhou was left functioning on orbit for several days after the reentry modules return, and the Shenzhou 5 orbital module continued to operate for six months after launch.\n\n\nThe reentry module (返回舱) is located in the middle section of the spacecraft and contains seating for the crew. It is the only portion of Shenzhou which returns to Earth's surface. Its shape is a compromise between maximizing living space while allowing for some aerodynamic control upon reentry.\n\n\nThe aft service module (推进舱) contains life support and other equipment required for the functioning of Shenzhou. Two pairs of solar panels, one pair on the service module, the other pair on the orbital module, have a total area of over 40 m² (430 ft²), indicating average electrical power over 1.5 kW (\"Soyuz\" have 1.0 kW).\n\n\nAlthough the Shenzhou spacecraft follows the same layout as the Russian Soyuz spacecraft, it is substantially larger than Soyuz. There is enough room to carry an inflatable boat in case of a water landing, whereas Soyuz astronauts must jump into the water and swim. The commander sits in the center seat on both spacecraft. However, the copilot sits in the left seat on Shenzhou and the right seat on Soyuz.\n\n\n\n\n"}
{"id": "44282667", "url": "https://en.wikipedia.org/wiki?curid=44282667", "title": "Symbolic self-completion theory", "text": "Symbolic self-completion theory\n\nThe theory of symbolic self-completion is a psychological theory which holds that individuals seek to acquire and display symbols that are strongly related to what they perceive as the ideal self. For example, relatively effeminate boys who want to appear macho may use products associated with manliness—such as a strong cologne or a silver watch—in hopes of symbolically fulfilling their self-definitions, i.e. becoming manly. Such cases of symbolic self-completion are seen in internet communication, marketing and advertising, and consumer behavior.\n\nThe theory of symbolic self-completion has its origins in the symbolic interactionist school of thought. As expressed by George Mead in \"Mind, Self and Society\", symbolic interactionism suggests that the self is defined by the way that society responds to the individual. This idea helped shape the central ideas put forth in the book \"Symbolic Self-Completion\", which states that individuals tend to define themselves using symbols of accomplishment and that they use symbols to communicate their self-definitions to society. Depending on the area of self-definition to which these symbols pertain, a different self-definition is thus exhibited.\n\n\"Self-definitional symbols\" are the objects that individuals use to communicate their self-definitions to society. Symbols can be both material and non-material, including anything ranging from utterances, behaviors, and socially recognized markers such as material possessions and social status. They are defined as \"any facet of the person that has the potential to signal to others (who understand the symbol as related to the identity) that one possesses the identity in question.\" Because it is through these symbols that individuals build their self-definitions around and communicate them to society, symbols are “the building blocks of self-definition.” Thus, symbols are meaningful to individuals only insofar as they adequately represent individuals’ self-definitions, regarding the status of accomplishment in the areas individuals believe are important to their self-definitions. When individuals lack symbols to express their self-definitions, they seek to “display alternative symbols of attainment.”\n\nResearch has shown that when individuals are deficient in any self-definitional area, this deficiency produces a state of tension and a sense of incompleteness in their self-definitions. Individuals are motivated to reduce this tension by using alternate symbols of accomplishment in the relevant self-definitional area. In the study “Symbolic Self-completion, Attempted Influence, and Self-Deprecation,” Robert Wicklund, Peter Gollwitzer and James Hilton asked participants to 1) write an essay teaching people how to perform an activity important to them and then 2) indicate how many people should be required to revise their essays. Results of this study showed that the fewer the years of education or experience participants seemed to have in their respective activities, the higher the number of people participants thought should be required to revise their essays. The higher this number was, the more participants put themselves in a position to influence others; researchers interpreted this relationship as a means of symbolic compensation for lacking the relevant self-definitional area.\n\nAn additional part of this study asked a group of men to make a statement about their ability in the self-definitional area. Results of this part of the study showed that the less education and experience the men had, the less willing they were to provide a negative evaluation of themselves. This behavior remained consistent, even when the men were told that the attractive female confederate preferred men who were critical of themselves. This finding shows people are willing to self-symbolize even when they know the self-symbolizing behavior will be negatively received by society. Such findings indicate that individuals are more concerned with whether their behaviors will be reflective of their self-definitions than with whether the behaviors induce positive or negative judgments from others. Altogether, these findings indicate that “influencing others, as well as positive self-descriptions, can further the individual’s sense of having a complete self-definition.”\n\nIndividuals' self-definitions can change for purposes of self-esteem protection and maintenance. These changes are likely to occur in consideration of the \"relative performance and the psychological\nsimilarity (closeness) and dissimilarity (distance) of others.\" Certain dimensions of individuals' self-definitions can become \"less self-definitional\" when others who are psychologically similar to them outperforms them in those areas. As such, self-definitions lend themselves to change—however, individuals may opt to strengthen their self-definitions in the face of self-definitional threats. \"Self-definitional threat\" refers to a situation in which individuals feel their identities are uncertain or threatened or they feel insecure in an identity they are committed to. In such cases, individuals are more likely to value symbols that reinforce those identities.\n\nThe study “Reactions to self-discrepant feedback: Feminist attitude and symbolic self-completion” shows how a threat to one’s identity also motivates individuals to engage in symbolic self-completion as a means of reducing the tension it causes. The researchers Rudolf Schiffmann and Doris Nelkenbrecher asked a group of feminist participants to subscribe to a feminist journal after being given feedback on their feminist attitudes. The women who were described as less feminist were more likely to subscribe to the feminist journal as a means of symbolically “completing” their self-definition.\n\nRelated to this study is \"Psychological Antecedents of Conspicuous Consumption\" by Ottmar L. Braun and Wicklund. This study was conducted in six separate studies, the first of which involved interviews of law students and attorneys. This first part of the study found that law students were more likely than practicing attorneys to think that it is important to have the \"outward manifestations of an attorney,\" supporting the idea that individuals \"striving toward a particular identity\" and are \"more inexperienced in that identity realm\" are more prone to claim that they \"can be recognized as belonging to that identity.\"\n\nMore recent studies have shown how symbolic self-completion influences individuals' communication in online media platforms. For example, Cindy and Eddie Harmon-Jones and Brandon Schmeichel have shown how individuals’ need for self-definition affects whether they share symbols of self-definitional attainment online. These researchers examined academic web pages and email signature files to see what types of academic departments and professors were more likely to enlist professional titles. The researchers found that the lower an academic department was ranked within National Research Council Rankings, the higher the number of professional titles the professors in that department displayed in their websites. Similarly, the lower the annual rate of publications and citations professors seemed to have, the higher the number of professional titles they enlisted in their email signatures. These correlations suggest that the enlistment of professional titles online serve as alternate symbols of accomplishment in their self-definitional areas; the more they felt they were lacking in a certain area, the more likely they were to engage in symbolic self-completion online regarding that particular area.\n\nThe theory of symbolic self-completion has direct application in the advertisement industry. The media leads consumers to equate advertised products targeting their feelings of “incompleteness” with self-definitional symbols to make up for that incompleteness. Although the symbols that each consumer ascribes to may be different in every case, these symbols as a whole can nonetheless be used to improve the individual consumers' perception of themselves. The product-symbols give some consumers a sense of completeness, since “self-perceptions are influenced by product use/ownership when the product has a strong user image and the consumer does not have a well formed self-image.” For example, a deodorant advertisement may appeal to a male consumer's self-definitional need for masculinity, by suggesting that he will become more masculine if he uses the deodorant advertised. The Old Spice brand famously uses phrases such as \"smell like a man, man\" when advertising their products. The \"Smell Like A Man, Man\" campaign led to Old Spice becoming the number one brand of deodorant. By displaying these product-symbols, consumers improve their sense of self and feel more confident about how others might perceive them.\n\nBraun and Wicklund suggest that there is a \"compensatory relation between person's security\" and certain kinds of conspicuous consumption, but that these relations cannot exist without individuals' perceived \"incompleteness [of]\" and \"commitment to the identity in question.\" Thus, much of what is colloquially referred to as the “mid-life crisis” can be explained by the theory of symbolic self-completion. A classic example of the mid-life crisis is a 40-year-old man buying a red sports car. The man is unsure as to whether he has made the right choices in his life and if he has been leading a successful career. The man then counters his insecurity by purchasing a material object that functions as a status symbol, something that both he and others will recognize as a mark of success.\n\nThe relationship between self-completion theory and materialism is further shown through individuals' tendency to externalize their concerns about their lives by acquiring symbol-objects that reinforce and improve their self-definitions. In terms of goods/objects as individuals' status symbols, greater emphasis is placed on tangible, material objects, as they can be recognized and understood as status symbols by a wider audience than are intangible and abstract ideas. In the same vein, materialism reinforces symbolic self-completion particularly in a societies that are structure in such a way that the consumption of prestigious objects is seen as the best remedy for insecurity. This reinforcement is due to the fact that in such societies, individuals see material wealth as the best source of comforting reassurance to counter insecurity\n"}
{"id": "30279239", "url": "https://en.wikipedia.org/wiki?curid=30279239", "title": "The Plant List", "text": "The Plant List\n\nThe Plant List is a list of botanical names of species of plants created by the Royal Botanic Gardens, Kew and the Missouri Botanical Garden and launched in 2010. It was intended to be a comprehensive record of all known names of plant species over time.\n\nThere is a complementary project called the International Plant Names Index, in which Kew is also involved. The IPNI aims to provide details of publication and does not aim to determine which are accepted species names. Newly published names are automatically added from IPNI to the World Checklist of Selected Plant Families, a database which underlies the Plant List.\n\nThe Plant List has 1,064,035 scientific plant names of species rank. 350,699 are accepted species names, belonging to 642 plant families and 17,020 plant genera.\nThe Plant List accepts approximately 350,699 unique species, with 470,624 synonyms for those species, which suggests that many species have been referred to under more than one name. , The Plant List has determined that another 243,000 names are 'unresolved', meaning that botanists have so far been unable to determine whether they are a separate species or a duplication of the 350,699 unique species.\n\nWhen The Plant List was launched in 2010 (the International Year of Biodiversity), it attracted media attention for its comprehensive approach. Fox News highlighted the number of synonyms encountered, suggesting that this reflected a \"surprising lack\" of biodiversity on earth.\" The Plant List also attracted attention for building on the work of English naturalist Charles Darwin, who in the 1880s started a plant list called the Index Kewensis (IK).\nKew has added an average of 6,000 species every year since the IK was first published with 400,000 names of species. However, the IK (which by 1913 avoided making taxonomic judgement in its citations) is currently run as part of the IPNI rather than the Plant List.\n\n\n"}
{"id": "8260045", "url": "https://en.wikipedia.org/wiki?curid=8260045", "title": "The Skeptics' Guide to the Universe", "text": "The Skeptics' Guide to the Universe\n\nThe Skeptics' Guide to the Universe is a weekly, 80-minute podcast hosted by Steven Novella, MD, and a panel of \"skeptical rogues\". It is the official podcast of the New England Skeptical Society. The show features discussions of recent scientific developments , and interviews authors, people in the area of science and other famous skeptics. The show also includes discussions of myths, conspiracy theories, pseudoscience, the paranormal, and many general forms of superstition, from the point of view of scientific skepticism. Steven Novella, the host of the show, has been particularly active in debunking pseudoscience in medicine. His activities include opposing the claims of anti-vaccine activists, homeopathy practitioners and individuals denying the link between HIV and AIDS.\n\nThe show is prerecorded via a Skype conference call. Each caller records their own audio and then the locally recorded tracks are mixed together. Steven Novella does the editing and post-production of the show himself. British comedian and skeptic Iszi Lawrence provides voice-over introductions for the show and certain segments. \n\n\n\n\n\n\n\n\n\nOnly since 2010, has the \"Who's That Noisy?\" segment been before the interview; pre-2010, it was just before the \"Skeptical Quote\".\n\nMost podcasts last around 80 minutes but on September 23, 2011 SGU produced a 24-hour-long podcast with contributions by skeptics from around the world. It was referred to as SGU-24.\n\n\n\n\n\n\n\n\nThe show's theme music is \"Theorem\" by the San Francisco rock band, Kineto. The theme was acquired from the Podsafe Music Network. Prior to the November 2, 2005 show, Thomas Dolby's \"She Blinded Me with Science\" was the show's theme.\n\nMany Skeptics' Guide episodes contain interviews. Often the interviews feature well-known scientists or skeptics, for instance Massimo Pigliucci or Joe Nickell. Rarely the guests are proponents of fringe or pseudoscientific views. Notable guests include the following:\n\n\"The Skeptics' Guide\" won the 2009 Podcast Awards in the \"Education\" category, and the 2010, 2011, 2012, and 2014 Podcast Awards in the \"Science\" category.\n\nIt also was a 2014 “Dose of Rationality” Top 10 Podcast,\nand a 2010 Physics.org Best Podcast nominee.\n\nOn July 30, 2013, Dr. Steven Novella announced that the SGU would begin offering membership and airing sponsors. Dr. Novella went on to say that the money raised would go into funding skeptical activities, including but not limited to, development of skeptical educational content and web-series such as \"Occ The Skeptical Caveman.\" The addition of sponsors is not permanent, according to Dr. Novella, they shall be removed \"if 4% of listeners support the SGU through membership at an average of the $8 per month level.\" \nThough membership has begun, the SGU continues to publish a free weekly sponsored podcast. Membership entitles one to an ad-free version of The SGU, extra content, and discounts to NECSS (The Northeast Conference on Science and Skepticism). Membership range from $4/month to $200/month.\n\nA companion podcast, \"The Skeptics' Guide 5x5\" (\"SGU 5x5\" for short), described as \"five minutes with five skeptics,\" consist of single-topic episodes which often delve into specific types of logical fallacy. SGU 5x5 did not appear regularly and there were no episodes between and . There have been no episodes released since .\n\nIn a 2017 interview, SGU team member Evan Bernstein announced in an interview for Skeptical Inquirer with Susan Gerbic that the team had collaborated on a book, and it was expected to be available in the fall of 2018. The book, titled \"The Skeptics' Guide to the Universe: How to Know What's Really Real in a World Increasingly Full of Fake\", became available for pre-order in early 2018, and is due to be released by Grand Central Publishing on October 2. The Amazon.com pre-order advertisement included the following information:\nThe book was released in October, 2018. Publishers Weekly reviewed the book, stating: \"In plain English and cogent prose, Novella makes skepticism seem mighty, necessary, and accessible all at once... Empowering and illuminating, this thinker’s paradise is an antidote to spreading anti-scientific sentiments. Readers will return to its ideas again and again.\" Neil deGrasse Tyson's review says: \"Thorough, informative, and enlightening, \"The Skeptics' Guide to the Universe\" inoculates you against the frailties and shortcomings of human cognition. If this book does not become required reading for us all, we may well see modern civilization unravel before our eyes.\"\n\n"}
{"id": "20083580", "url": "https://en.wikipedia.org/wiki?curid=20083580", "title": "Triose phosphate translocator", "text": "Triose phosphate translocator\n\nThe triose phosphate translocator is an integral membrane protein found in the inner membrane of chloroplasts. It exports triose phosphate (Dihydroxyacetone phosphate) in exchange for inorganic phosphate and is therefore classified as an antiporter. The imported phosphate is then used for ATP regeneration via the light-dependent-reaction; the ATP may then for example be used for further reactions in the Calvin-cycle.The Translocator protein is responsible for exporting all the carbohydrate produced in photosynthesis by plants and therefore most of the carbon in food that one eats has been transported by the triose phosphate translocator.\n"}
{"id": "7139178", "url": "https://en.wikipedia.org/wiki?curid=7139178", "title": "Warren Upham", "text": "Warren Upham\n\nWarren Upham (8 March 1850 – 29 January 1934) was a geologist, archaeologist, and librarian who is best known for his studies of glacial Lake Agassiz. Upham worked as a geologist in New Hampshire before moving in 1879 to Minnesota to study the resources and glacial geology of that state. Upham's first major report on Lake Agassiz was published in 1890 by the Geological Survey of Canada, but the main product of his many years of study (\"The Glacial Lake Agassiz\") was published in 1895 as \"Monograph 25\" of the U.S. Geological Survey's monograph series.\n\nUpham graduated from Dartmouth College in 1871 and worked under Minnesota state geologist Newton H. Winchell. The Minnesota Historical Society published his landmark 735-page volume on place name origins, \"Minnesota Geographic Names: Their Origin and Historic Significance\" in 1920.\n\n"}
{"id": "12401060", "url": "https://en.wikipedia.org/wiki?curid=12401060", "title": "Yoav Sarig", "text": "Yoav Sarig\n\nYoav Sarig (born July 27, 1937) is an Israeli scientist, inventor and agricultural engineer. He is an expert in the field of mechanical harvesting of fruit, and is the holder of several patents for mechanical apparatus for the harvesting and processing of jojoba beans, pecan nuts and pomegranates.\n\nSarig was born in Tel-Aviv. He received his Bachelor of Science degree from the Technion in 1962 and his Master's degree from the Technion in 1972. Sarig was awarded his doctorate from Michigan State University in 1976. The title of his doctoral thesis was \"Deformation analysis of foam-encapsulated apples under impact loading\".\n\nWorking for Israel's Volcani Centre, where he headed the Institute for Agricultural Engineering from 1985 to 1988, Sarig developed apparatus including a mechanical pollinator for Date Palms, an apparatus for separating pomegranate seeds, a mechanical cracker for Macadamia nuts and a machine for harvesting agricultural produce from the ground suitable for Pecan nuts and similar produce.\nIn later years, his interest turned to non-invasive methods for evaluation of agricultural produce - to assess a product's readiness for harvest or its ripeness without harming the fruit. He pioneered the use of Nuclear Magnetic Resonance (NMR) to evaluate the maturity of Avocado fruits, and the use of an \"artificial nose\" utilizing the olfactory response of fresh produce as a consumer-oriented and non-destructive quality evaluation method. He also researched the use of mechanization and automation as an alternative to manual labor in agriculture, and published several articles on the topic, which concluded that as long as cheap immigrant labor (some of it illegal) was available , there is little incentive for farmers to invest in labor-saving agricultural automation.\n\nWhen China in 1992 formally established diplomatic relations with Israel and an Israeli embassy was opened in Beijing, Sarig was appointed Science and Agriculture Counsellor, a role he held for 4 years, from 1993 to 1997. There, Sarig proposed and oversaw the establishment of three major demonstration farms in China, which showcase Israeli agricultural technology, and several training centers which are supported by both the Chinese and Israeli ministries of agriculture. Sarig also realized the significance of conducting a variety of management courses in China, and instituted a program to deliver courses in business, marketing and business management. In recognition of his contributions, he was awarded the title \"Honorary Professor\" by China Agricultural University.\n\nIn 2006, Sarig was awarded the Namir Prize for his work in developing the apparatus for the separation of pomegranate seeds, and the same apparatus also won the American Society of Agricultural and Biological Engineers (ASABE)'s AE50 Award in 2007.\n\nIn recognition of his professional achievements, Sarig was named an ASABE Fellow in 2010.\n\nSarig was awarded several US patents. These included; \n"}
