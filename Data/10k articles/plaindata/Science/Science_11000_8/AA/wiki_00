{"id": "40733699", "url": "https://en.wikipedia.org/wiki?curid=40733699", "title": "Actin nucleation core", "text": "Actin nucleation core\n\nAn actin nucleation core is a protein trimer with three actin monomers. It is called a nucleation core because it leads to the energetically favorable elongation reaction once a tetramer is formed from a trimer. Actin protein dimers and trimers are energetically unfavorable.\n"}
{"id": "16526908", "url": "https://en.wikipedia.org/wiki?curid=16526908", "title": "Albone Glacier", "text": "Albone Glacier\n\nAlbone Glacier () is a deeply entrenched narrow glacier on the east side of Wolseley Buttress flowing southward from Detroit Plateau on Nordenskjöld Coast in Graham Land, Antarctica. It was mapped by the Falkland Islands Dependencies Survey from surveys (1960–61) and was named by the United Kingdom Antarctic Place-Names Committee for Dan Albone, English designer of the Ivel tractor, the first successful tractor with an internal combustion engine.\n\n\n"}
{"id": "35828206", "url": "https://en.wikipedia.org/wiki?curid=35828206", "title": "Angelo Andres", "text": "Angelo Andres\n\nAngelo \"Ginobili\" Andres (24 March 1851, Tirano –16 July 1934, Milan) was an Italian zoologist.\nDr. Angelo Andres studied natural history in Pavia, Leipzig, London and Paris. He became a Professor in Moderna. From 1899–1926 he was director of Museo di Storia Naturale in Parma. He was a friend of Anton Dohrn.\n\nAndres was a supporter of Darwinism and gave anniversary lectures supporting his ideas.\n\n\n"}
{"id": "26466424", "url": "https://en.wikipedia.org/wiki?curid=26466424", "title": "Anthony Bean", "text": "Anthony Bean\n\nAnthony R. Bean (born 1957) is an Australian botanist who works at the Queensland Herbarium and Brisbane Botanic Gardens, Mount Coot-tha.\n"}
{"id": "13602025", "url": "https://en.wikipedia.org/wiki?curid=13602025", "title": "Artemis Chasma", "text": "Artemis Chasma\n\nThe Artemis Chasma is the nearly circular fracture in Venus's surface which almost encloses Artemis Corona. The chasma and its associating corona can be found on the Aphrodite Terra continent, at Latitude 35° South, Longitude 135° East.\n\nIt is named after the Artemis, the Greek virgin goddess of the hunt and the moon, the hills, the forest, birth, virginity, and fertility, who carries a bow and arrow.\n\n"}
{"id": "19519055", "url": "https://en.wikipedia.org/wiki?curid=19519055", "title": "August Victor Paul Blüthgen", "text": "August Victor Paul Blüthgen\n\nAugust Victor Paul Blüthgen (25 July 1880 in Mühlhausen, Thüringen – 2 September 1967 in Naumburg) was a German entomologist who specialised in Hymenoptera.\nHe was a Doctor of Law Jurist and court adviser. Blüthgen described very many new species of Aculeata (bees and wasps).\n\n\n"}
{"id": "3575605", "url": "https://en.wikipedia.org/wiki?curid=3575605", "title": "Bracewell probe", "text": "Bracewell probe\n\nA Bracewell probe is a hypothetical concept for an autonomous interstellar space probe dispatched for the express purpose of communication with one or more alien civilizations. It was proposed by Ronald N. Bracewell in a 1960 paper, as an alternative to interstellar radio communication between widely separated civilizations.\n\nA Bracewell probe would be constructed as an autonomous robotic interstellar space probe with a high level of artificial intelligence, and all relevant information that its home civilization might wish to communicate to another culture. It would seek out technological civilizations—or alternatively monitor worlds where there is a likelihood of technological civilizations arising—and communicate over \"short\" distances (compared to the interstellar distances between inhabited worlds) once it discovered a civilization that meets its contact criteria. It would make its presence known, carry out a dialogue with the contacted culture, and presumably communicate the results of its encounter to its place of origin. In essence, such probes would act as an autonomous local representative of their home civilization and would act as the point of contact between the cultures.\n\nSince a Bracewell probe can communicate much faster, over shorter distances, and over large spans of time, it can communicate with alien cultures more efficiently than radio message exchange might. The disadvantage to this approach is that such probes cannot communicate anything not in their data storage, nor can their contact criteria or policies for communication be quickly updated by their \"base of operations\".\n\nWhile a Bracewell probe need not be a von Neumann probe as well, the two concepts are compatible, and a self-replicating device as proposed by von Neumann would greatly speed up a Bracewell probe's search for alien civilizations.\n\nIt is also possible that such a probe (or system of probes if launched as a von Neumann-Bracewell probe) may outlive the civilization which created and launched it. \n\nThere have been some efforts under the SETA and SETV projects to detect evidence for the visitation of the Solar System by such hypothetical probes, and to signal or activate such an alleged probe that may be lying dormant in local space. Variations in the echo delay times of radio transmissions, known as long delayed echoes, or LDEs, have also been interpreted in Professor Bracewell's 1960 paper as evidence for such probes.\n\nThe near-earth object 1991 VG was initially suggested as a candidate for a Bracewell probe due to its unusual characteristics. In more recent years, however, additional discoveries have accounted for the characteristics of 1991 VG, and it is no longer regarded as anomalous.\n\n\n\n\n"}
{"id": "2884875", "url": "https://en.wikipedia.org/wiki?curid=2884875", "title": "Cape Disappointment State Park", "text": "Cape Disappointment State Park\n\nCape Disappointment State Park (formerly, Fort Canby State Park) is a public recreation area located southwest of Ilwaco, Washington, on the bottom end of Long Beach Peninsula, the northern headlands where the Columbia River meets the Pacific Ocean. The state park's encompass a diverse landscape of old-growth forest, freshwater lakes, freshwater and saltwater marshes, and oceanside tidelands. Park sights include the Lewis and Clark Interpretive Center, North Head Lighthouse, and Cape Disappointment Lighthouse. Cape Disappointment is one of several state parks and sites in Washington and Oregon that make up the Lewis and Clark National and State Historical Parks.\n\nCape Disappointment earned its name when Captain John Meares failed to cross the river bar in 1788. The feat was accomplished in 1792 by American Captain Robert Gray. The Lewis and Clark Expedition arrived at Cape Disappointment in 1805.\n\nIn 1862, during the American Civil War, a camp called Post at Cape Disappointment was established and fortifications existed here from that date to protect the northern approaches to the mouth of the Columbia River from possible attacks by Confederate raiders or foreign fleets. It was garrisoned by Company A, U.S. 9th Infantry Regiment and Company A, 8th Regiment California Volunteer Infantry in the District of Oregon. In 1863, its mate Fort Stevens was established on the south bank of the Columbia River. In 1864, the post was renamed Fort Cape Disappointment. Some Civil War-era fortifications still exist: the Tower (or Right) Battery, Left Battery, and Center Battery.\n\nFort Cape Disappointment was expanded and renamed Fort Canby in 1875. By 1906, when construction finished under the Endicott program, Fort Canby became part of the three-fort Harbor Defenses of the Columbia River as a subpost of Fort Stevens along with Fort Columbia. The fort was further expanded during World War II. After being decommissioned in the years following World War II, the fort was turned over to the state for use as a state park in the early 1950s. Workers with the Civilian Conservation Corps helped restore the fort and improved roads and trails during the 1930s.\n\nThe Lewis and Clark Interpretive Center sits on a cliff that overlooks the confluence of the Columbia River and the Pacific Ocean. There are exhibits about the 1803–1806 Lewis and Clark Expedition from St. Louis, Missouri to the Pacific coast, the park's later history, including the lighthouses, U.S. Coast Guard and military activities, and the area's maritime and natural history.\n\nCape Disappointment State Park offers camping and other overnight accommodations, of hiking trails, watercraft launch sites, picnicking facilities, and tours of the North Head Lighthouse.\n\n"}
{"id": "16868231", "url": "https://en.wikipedia.org/wiki?curid=16868231", "title": "Carl Geyer", "text": "Carl Geyer\n\nCarl Geyer (1796–1841) was a German entomologist who wrote and illustrated various supplements to Jacob Hübner's works on Lepidoptera.\n\nCarl Geyer was by profession an artist. He is not to be confused with Karl Andreas Geyer (1809–1853), a botanist and plant collector.\n\n\n"}
{"id": "36540089", "url": "https://en.wikipedia.org/wiki?curid=36540089", "title": "Cattle creep", "text": "Cattle creep\n\nA cattle creep is a small, field-to-field access for farm animals, usually to allow passage beneath an obstacle such as a road, canal, or railway embankment.\n\nAs they are intended primarily for cattle or other livestock, cattle creeps usually have a low head height and are uncomfortable for humans to use.\n\nOn Dartmoor, in south west England, the term sheep creep is used to describe a purposely constructed gap in the base of a drystone wall, commonly topped with a granite lintel. The gap allows sheep to pass from field to field, but is deliberately too small for cattle or ponies. \n\n\n"}
{"id": "42840970", "url": "https://en.wikipedia.org/wiki?curid=42840970", "title": "Constructor theory", "text": "Constructor theory\n\nConstructor theory is a proposal for a new mode of explanation in fundamental physics, first sketched out by David Deutsch, a quantum physicist at the University of Oxford, in 2012. Constructor theory expresses physical laws exclusively in terms of what physical transformations, or tasks, are possible versus which are impossible, and why. By allowing such counterfactual statements into fundamental physics, it allows new physical laws to be expressed, for instance those of the constructor theory of information.\n\nThe fundamental elements of the theory are tasks, i.e., the abstract specifications of transformations in terms of input/output pairs of attributes. A task is impossible if there is a law of physics that forbids its being performed with arbitrarily high accuracy, and possible otherwise. When it is possible, then a \"constructor\" for it can be built, again with arbitrary accuracy and reliability. A constructor is an entity which can cause the task to occur while retaining the ability to cause it again. Examples of constructors include a heat engine (a thermodynamic constructor), a catalyst (a chemical constructor) or a computer program controlling an automated factory (an example of a programmable constructor).\n\nThe theory was developed by physicists David Deutsch and Chiara Marletto. It draws together ideas from diverse areas including thermodynamics, statistical mechanics, information theory and quantum computation.\n\nQuantum mechanics and all other physical theories are claimed to be \"subsidiary\" theories, and quantum information becomes a special case of \"superinformation\".\n\nChiara Marletto's \"constructor theory of life\" builds on constructor theory.\n\nAccording to Deutsch, current theories of physics based on quantum mechanics do not adequately explain why some transformations between states of being are possible and some are not. For example, a drop of dye can dissolve in water but thermodynamics shows that the reverse transformation, of the dye clumping back together, is effectively impossible.\nWe do not know at a quantum level why this should be so. Constructor theory provides an explanatory framework built on the transformations themselves, rather than the components.\n\nInformation has the property that a given statement might have said something else, and one of these alternatives would not be true. The untrue alternative is said to be \"counterfactual\". Conventional physical theories do not model such counterfactuals. However the link between information and such physical ideas as the entropy in a thermodynamic system is so strong that they are sometimes identified. For example, the area of a black hole's event horizon is a measure both of the hole's entropy and of the information it contains. Constructor theory is an attempt to bridge this gap, providing a physical model which can express counterfactuals, thus allowing the laws of information and computation to be viewed as laws of physics.\n\nIn constructor theory, a transformation or change is described as a \"task\". A \"constructor\" is a physical entity which is able to carry out a given task repeatedly. A task is only possible if a constructor capable of carrying it out exists, otherwise it is impossible. To work with constructor theory everything is expressed in terms of tasks. The properties of information are then expressed as relationships between possible and impossible tasks. Counterfactuals are thus fundamental statements and the properties of information may be described by physical laws.\nIf a system has a set of attributes, the set of permutations of these attributes is seen as a set of tasks. A \"computation medium\" is a system whose attributes permute to always produce a possible task. The set of permutations, and hence of tasks, is a \"computation set\". If it is possible to copy the attributes in the computation set, the computation medium is also an \"information medium\".\n\nInformation, or a given task, does not rely on a specific constructor. Any suitable constructor will serve. This ability of information to be carried on different physical systems or media is described as \"interoperability\", and arises as the principle that the combination of two information media is also an information medium.\nMedia capable of carrying out quantum computations are called \"superinformation media\", and are characterised by specific properties. Broadly, certain copying tasks on their states are impossible tasks. This is claimed to give rise to all the known differences between quantum and classical information.\n\n"}
{"id": "10536559", "url": "https://en.wikipedia.org/wiki?curid=10536559", "title": "Dasar", "text": "Dasar\n\nDasar is an acronym for Darkness Amplification by Stimulated Absorption of Radiation.\n\nIt is a little-used expression describing the anomalous interstellar formaldehyde absorption discovered by Palmer \"et al.\" in 1969.\n\n"}
{"id": "5920634", "url": "https://en.wikipedia.org/wiki?curid=5920634", "title": "Discrete event simulation", "text": "Discrete event simulation\n\nA discrete-event simulation (DES) models the operation of a system as a discrete sequence of events in time. Each event occurs at a particular instant in time and marks a change of state in the system. Between consecutive events, no change in the system is assumed to occur; thus the simulation can directly jump in time from one event to the next.\n\nThis contrasts with continuous simulation in which the simulation continuously tracks the system dynamics over time. Instead of being event-based, this is called an activity-based simulation; time is broken up into small time slices and the system state is updated according to the set of activities happening in the time slice. Because discrete-event simulations do not have to simulate every time slice, they can typically run much faster than the corresponding continuous simulation.\n\nA more recent method is the three-phased approach to discrete event simulation (Pidd, 1998). In this approach, the first phase is to jump to the next chronological event. The second phase is to execute all events that unconditionally occur at that time (these are called B-events). The third phase is to execute all events that conditionally occur at that time (these are called C-events). The three phase approach is a refinement of the event-based approach in which simultaneous events are ordered so as to make the most efficient use of computer resources. The three-phase approach is used by a number of commercial simulation software packages, but from the user's point of view, the specifics of the underlying simulation method are generally hidden.\n\nA common exercise in learning how to build discrete-event simulations is to model a queue, such as customers arriving at a bank to be served by a teller. In this example, the system entities are Customer-queue and Tellers. The system events are Customer-Arrival and Customer-Departure. (The event of Teller-Begins-Service can be part of the logic of the arrival and departure events.) The system states, which are changed by these events, are Number-of-Customers-in-the-Queue (an integer from 0 to n) and Teller-Status (busy or idle). The random variables that need to be characterized to model this system stochastically are Customer-Interarrival-Time and Teller-Service-Time. An agent-based framework for performance modeling of an optimistic parallel discrete event simulator is another example for a discrete event simulation.\n\nIn addition to the logic of what happens when system events occur, discrete event simulations include the following:\n\nA system state is a set of variables that captures the salient properties of the system to be studied. The state trajectory over time S(t) can be mathematically represented by a step function whose value can change whenever an event occurs.\n\nThe simulation must keep track of the current simulation time, in whatever measurement units are suitable for the system being modeled. In discrete-event simulations, as opposed to continuous simulations, time 'hops' because events are instantaneous – the clock skips to the next event start time as the simulation proceeds.\n\nThe simulation maintains at least one list of simulation events. This is sometimes called the \"pending event set\"\nbecause it lists events that are pending as a result of previously simulated event but have yet to be simulated themselves.\nAn event is described by the time at which it occurs and a type, indicating the\ncode that will be used to simulate that event. It is common for the event code to be parametrized, in which case, the event description also contains parameters to the event code.\n\nWhen events are instantaneous, activities that extend over time are modeled as sequences of events. Some simulation frameworks allow the time of an event to be specified as an interval, giving the start time and the end time of each event.\n\nSingle-threaded simulation engines based on instantaneous events have just one current event. In contrast, multi-threaded simulation engines and simulation engines supporting an interval-based event model may have multiple current events. In both cases, there are significant problems with synchronization between current events.\n\nThe pending event set is typically organized as a priority queue, sorted by event time. That is, regardless of the order in which events are added to the event set, they are removed in strictly chronological order. Various priority queue implementations have been studied in the context of discrete event simulation; alternatives studied have included splay trees, skip lists, calendar queues, and ladder queues.\nTypically, events are scheduled dynamically as the simulation proceeds. For example, in the bank example noted above, the event CUSTOMER-ARRIVAL at time t would, if the CUSTOMER_QUEUE was empty and TELLER was idle, include the creation of the subsequent event CUSTOMER-DEPARTURE to occur at time t+s, where s is a number generated from the SERVICE-TIME distribution.\n\nThe simulation needs to generate random variables of various kinds, depending on the system model. This is accomplished by one or more Pseudorandom number generators. The use of pseudo-random numbers as opposed to true random numbers is a benefit should a simulation need a rerun with exactly the same behavior.\n\nOne of the problems with the random number distributions used in discrete-event simulation is that the steady-state distributions of event times may not be known in advance. As a result, the initial set of events placed into the pending event set will not have arrival times representative of the steady-state distribution. This problem is typically solved by bootstrapping the simulation model. Only a limited effort is made to assign realistic times to the initial set of pending events. These events, however, schedule additional events, and with time, the distribution of event times approaches its steady state. This is called \"bootstrapping\" the simulation model. In gathering statistics from the running model, it is important to either disregard events that occur before the steady state is reached or to run the simulation for long enough that the bootstrapping behavior is overwhelmed by steady-state behavior. (This use of the term \"bootstrapping\" can be contrasted with its use in both statistics and computing).\n\nThe simulation typically keeps track of the system's statistics, which quantify the aspects of interest. In the bank example, it is of interest to track the mean waiting times. In a simulation model, performance metrics are not analytically derived from probability distributions, but rather as averages over replications, that is different runs of the model. Confidence intervals are usually constructed to help assess the quality of the output.\n\nBecause events are bootstrapped, theoretically a discrete-event simulation could run forever. So the simulation designer must decide when the simulation will end. Typical choices are \"at time t\" or \"after processing n number of events\" or, more generally, \"when statistical measure X reaches the value x\".\n\nThe main loop of a discrete-event simulation is something like this:\n\n\nWhile (Ending Condition is FALSE) then do the following:\n\n\nSimulation approaches are particularly well equipped to help users diagnose issues in complex environments. The Goal (Theory of Constraints) illustrates the importance of understanding bottlenecks in a system. Only process ‘improvements’ at the bottlenecks will actually improve the overall system. In many organizations bottlenecks become hidden by excess inventory, overproduction, variability in processes and variability in routing or sequencing. By accurately documenting the system inside a simulation model it is possible to gain a bird’s eye view of the entire system.\n\nA working model of a system allows management to understand performance drivers. A simulation can be built to include any number of performance indicators such as worker utilization, on-time delivery rate, scrap rate, cash cycles, and so on.\n\nAn operating theater is generally shared between several surgical disciplines. Through better understanding the nature of these procedures it may be possible to increase the patient throughput.\nExample: If a heart surgery takes on average four hours, changing an operating room schedule from eight available hours to nine will not increase patient throughput. On the other hand, if a hernia procedure takes on average twenty minutes providing an extra hour may also not yield any increased throughput if the capacity and average time spent in the recovery room is not considered.\n\nMany systems improvement ideas are built on sound principles, proven methodologies (Lean, Six Sigma, TQM, etc.) yet fail to improve the overall system. A simulation model allows the user to understand and test a performance improvement idea in the context of the overall system.\n\nSimulation modeling is commonly used to model potential investments. Through modeling investments decision-makers can make informed decisions and evaluate potential alternatives.\n\nDiscrete event simulation is used in computer network to simulate new protocols for different network traffic scenarios before deployment.\n\nSystem modeling approaches:\nComputational techniques:\nSoftware:\nDisciplines:\n\n"}
{"id": "61889", "url": "https://en.wikipedia.org/wiki?curid=61889", "title": "Division (biology)", "text": "Division (biology)\n\nDivision is a taxonomic rank in biological classification that is used differently in zoology and in botany.\n\nIn botany and mycology, \"division\" refers to a rank equivalent to phylum. The use of either term is allowed under the International Code of Botanical Nomenclature, and both are commonly used in scientific literature.\n\nThe main Divisions of land plants, in the order in which they probably evolved, are the Marchantiophyta (liverworts), Anthocerotophyta (hornworts), Bryophyta (mosses), Filicophyta (ferns), Sphenophyta (horsetails), Cycadophyta (cycads), Ginkgophyta (ginkgo)s, Pinophyta (conifers), Gnetophyta (gnetophytes), and the Magnoliophyta (Angiosperms, flowering plants). The flowering plants now dominate terrestrial ecosystems, comprising 80% of vascular plant species.\n\nIn zoology, the term \"division\" is applied to an optional rank subordinate to the infraclass and superordinate to the cohort. A widely used classification (e.g. Carroll 1988) recognises teleost fishes as a Division Teleostei within Class Actinopterygii (the ray-finned fishes). Less commonly (as in Milner 1988), living tetrapods are ranked as Divisions Amphibia and Amniota within the clade of vertebrates with fleshy limbs (Sarcopterygii).\n\n"}
{"id": "2345913", "url": "https://en.wikipedia.org/wiki?curid=2345913", "title": "Dmitrii Knorre", "text": "Dmitrii Knorre\n\nDmitrii G. Knorre (Russian: Дмитрий Георгиевич Кнорре; born July 28, 1926 in Leningrad, Soviet Union) is a chemist and biochemist, a specialist in chemical kinetics of complex reactions, bioorganic chemistry, and molecular biology. He was a Corresponding member of the Academy of Sciences of the USSR since 1968, and an academician since 1981. He was assigned to the Division of Biochemistry, Biophysics, and Chemistry of Physiologically Active Compounds of the Academy and to the Siberian Division since 1981.\n\nHe graduated from the Mendeleev Russian University of Chemistry and Technology in 1947. He worked at the Chemical Physics Institute from 1947 to 1960 when he joined the Siberian Division in a laboratory studying natural polymers and joined the Department of Biochemistry of the Novosibirsk Institute of Organic Chemistry. In 1962, he acted as Head of the Natural Polymers Laboratory of the Organic Chemistry Institute in Novosibirsk that was established in 1958 and whose basic work is in the study of aromatic and heterocyclic chemistry and in natural products. He was named the Founding Director of the Novosibirsk Bioorganic Chemistry Institute. He was elected to the Presidium of the Siberian Division in 1988. From 1967 to 1983, he was a professor at the Faculty of Natural Sciences and held the chair of the\nDepartment of Molecular Biology from 1979. Since 1964 he was a dean of Department of Natural Sciences of Novosibirsk State University for 16 years. He is an honored scientist of the former Soviet Union. He is a laureate of the Prize of the Soviet Council of Ministers in 1987, and the M. M. Shemiakin Prize of the Academy of Sciences of the USSR in 1988.\n"}
{"id": "12235647", "url": "https://en.wikipedia.org/wiki?curid=12235647", "title": "Epipodophyllotoxin", "text": "Epipodophyllotoxin\n\nEpipodophyllotoxins are substances naturally occurring in the root of American Mayapple plant (\"Podophyllum peltatum\").\n\nSome epipodophyllotoxin derivatives are currently used in the treatment of cancer. These include etoposide and teniposide. They act as anti-cancer drugs by inhibiting topoisomerase II.\n\n"}
{"id": "57965303", "url": "https://en.wikipedia.org/wiki?curid=57965303", "title": "Esther Choo", "text": "Esther Choo\n\nEsther Choo is an Emergency Doctor and Associate Professor at the Oregon Health & Science University. She is a popular science communicator who has used social media to talk about racism and sexism in healthcare. She was the president of the Academy of Women in Academic Emergency Medicine and is a member of the American Association of Women Emergency Physicians.\n\nChoo grew up in Cleveland. Her parents emigrated from Korea in the 1960s. She graduated in 1994 with a degree in English from Yale College. She was an intern at The Plain Dealer, a newspaper in Cleveland. She earned a medical degree at Yale University in 2001. She was a resident at Boston Medical Center. In 2009 she returned to education, earning a Master's in Public Health at Oregon Health & Science University.\n\nChoo completed her emergency medicine residency at Boston Medical Center, did a health services research fellowship at Oregon Health & Science University, and later became an associate professor at the Alpert Medical School. She won the 2012 Outstanding Physician Award from the University Emergency Medicine Foundation, the SAEM Young Investigator Award, and the OHSU Emerging Leader Award Since 2016 she has been an Associate Professor at Oregon Health & Science University Hospital. Her research interests include developing effective interventions for women who experience partner violence and substance misuse. In 2018 she was the co-founder of Equity Quotient, a start-up which monitors and addresses equity culture in healthcare organizations. \n\nShe is an advocate for more multiculturalism and diversity in medicine, and does this by celebrating women's doctors. She has written for the blog FemInEM, a resource for women in emergency medicine. Choo was President of the Academy for Women in Academic Emergency Medicine. She was a leader of the Division of Women's Health in Emergency Care at Alpert Medical School, and is President of the non-profit Gender Equity Research Group.\n\nShe started a conversation about racism in medicine on Twitter after the August 12 white supremacist rally in Charlottesville, Virginia.@choo_ek: 1/ We've got a lot of white nationalists in Oregon. So a few times a year, a patient in the ER refuses treatment from me because of my race.The tweet was shared by 25,000 people, including Chelsea Clinton. She appeared on CNN and other news channels. Choo has written for HuffPost NBC THINK, and Self magazine. She started a second viral tweet series in July 2018, when she asked \"\"I'm going to write a book called, \"Is It Gender Bias, Or Do I Just Suck?\" Preview in the posts, below.\"\".\n"}
{"id": "57703499", "url": "https://en.wikipedia.org/wiki?curid=57703499", "title": "Explorer 36", "text": "Explorer 36\n\nExplorer 36 (also called GEOS 2 or GEOS B, acronym to Geodetic Earth Orbiting Satellite) was a U.S. satellite launched as part of the Explorers program, being the second of the two satellites GEOS. Explorer 36 was launched on January 11, 1968 from Vandenberg Air Force Base, with Delta rocket.\n\nExplorer 36 was a gravity-gradient-stabilized, solar cell powered spacecraft that carried electronic and geodetic instrumentation. The geodetic instrumentation systems included:\n\n\nNon-geodetic systems included a laser detector and a Minitrack interferometer beacon. The objectives of the spacecraft were to optimize optical station visibility periods and to provide complementary data for inclination-dependent terms established by the Explorer 29 (GEOS 1) gravimetric studies. The spacecraft was placed into a retrograde orbit to accomplish these objectives. Operational problems occurred in the main power system, optical beacon flash system, and the spacecraft clock, and adjustments in scheduling resulted in nominal operations.\n\n\n"}
{"id": "2245310", "url": "https://en.wikipedia.org/wiki?curid=2245310", "title": "Extension agency", "text": "Extension agency\n\nAn extension agency is an organisation that practises extension, in the context of community development. An example is the \"Cooperative Extension Service\", which aims to assist individuals or groups in defining and achieving their goals in rural communities in the USA.\nExtension agents are trained in the skills of extension, such as communication and group facilitation, and usually in technical areas of the sector they serve (for example agriculture, health, or safety). Agricultural extension agencies promote more profitable and sustainable farming, while health extension agencies promote improved health. \n\nExtension agents are represented by professional organisations such as the Australasia-Pacific Extension Network and publish in journals such as the Journal of Extension.\n"}
{"id": "50899829", "url": "https://en.wikipedia.org/wiki?curid=50899829", "title": "Francesco Spighi", "text": "Francesco Spighi\n\nFrancesco Spighi (18th century) was a Florentine artisan active in the late eighteenth century.\n\nAll we know about him is that he worked for some time as craftsman and cabinet-maker for the Museo di Fisica e Storia Naturale of Florence, producing inlaid-wood furniture and apparatuses for the Physics Cabinet. He was also a part of a revival in piano making in late eighteenth-century Italy, along with revival of piano making in late eighteenth-century Italy by such makers as Giuseppe Zannetti, Vicenzio Sodi, Luigi Vignoli, and Errico Gustadt.\n"}
{"id": "19405922", "url": "https://en.wikipedia.org/wiki?curid=19405922", "title": "Henri Termier", "text": "Henri Termier\n\nProfessor Henri-François-Émile Termier (13 December 1897 – 12 August 1989) was a French geologist. \n\nBorn at Lyon into a scholarly family, he served in the First World War as an artillery officer during which he earned a Croix de guerre. After working as an assistant at the university in Montpellier (1923 - 1925), he became a geologist working for the Service géologique du Maroc (Morocco Mine Service), where he worked until 1940, becoming very famous for his studies of stratigraphy and fossil fauna (he found the first specimens of \"Titanichthys agassizi\"). Later he taught at the university of Algeria (1945) and ten years later he became a chairmain at the Sorbonne. He was married to professor Geneviève Termier, another famous French paleontologist.\n\n\n"}
{"id": "1529187", "url": "https://en.wikipedia.org/wiki?curid=1529187", "title": "INTEGRAL", "text": "INTEGRAL\n\nINTErnational Gamma-Ray Astrophysics Laboratory (INTEGRAL) is a currently operational space telescope for observing gamma rays. It was launched by the European Space Agency into Earth orbit in 2002, and is designed to detect some of the most energetic radiation that comes from space. It was the most sensitive gamma ray observatory in space before NASA's \"Fermi\" was launched in 2008.\n\nINTEGRAL is an ESA mission in cooperation with the Russian Space Agency and NASA. It has had some notable successes, for example in detecting a mysterious 'iron quasar'. It has also had success in investigating gamma-ray bursts and evidence for black holes.\n\nBecause gamma rays and X-rays cannot penetrate Earth's atmosphere, direct observations must be made from space. INTEGRAL was launched from Baikonur spaceport, in Kazakhstan. The 2002 launch aboard a Proton-DM2 rocket achieved a 700 km perigee. The onboard thrusters then raised the perigee out of the residual atmosphere, and the worst regions of the radiation belts. The apogee was trimmed with the thrusters to synchronize with Earth's rotation, and thus, the satellite's ground stations.\n\nINTEGRAL's operational orbit has a period of 72 hours, and has a high eccentricity, with perigee close to the Earth at 10,000 km, within the magnetospheric radiation belt. However, most of each orbit is spent outside this region, where scientific observations may take place. It reaches a furthest distance from Earth (apogee) of 153,000 km. The apogee was placed in the northern hemisphere, to reduce time spent in eclipses, and maximize contact time over the ground stations in the northern hemisphere.\n\nIt is controlled from ESOC in Darmstadt, Germany, ESA's control centre, through ground stations in Belgium (Redu) and California (Goldstone).\n\nFuel usage is within predictions. INTEGRAL has already exceeded its 2.2-year planned lifetime; barring mechanical failures, it should continue to function for six years or more. its mission has been extended to December 2018. Its orbit was adjusted in Jan/Feb 2015 to cause a safe (southern) reentry in Feb 2029. There should be sufficient fuel left for science operations past 2020, if ESA judges that the scientific return of the missions continues justifying its operating costs.\n\nThe spacecraft body (\"service module\") is a copy of the XMM-Newton body. This saved development costs and simplified integration with infrastructure and ground facilities. (An adapter was necessary to mate with the different booster, though.) However, the denser instruments used for gamma rays and hard X-rays make INTEGRAL the heaviest scientific payload ever flown by ESA.\n\nThe body is constructed largely of composites. Propulsion is by a hydrazine monopropellant system, containing 544 kg of fuel in four exposed tanks. The titanium tanks were charged with gas to 24 bar (2.4 MPa) at 30 °C, and have tank diaphragms. Attitude control is via a star tracker, multiple Sun sensors, and multiple momentum wheels. The dual solar arrays, spanning 16 meters when deployed and producing 2.4 kW BoL, are backed up by dual nickel-cadmium battery sets.\n\nThe instrument structure (\"payload module\") is also composite. A rigid base supports the detector assemblies, and an H-shaped structure holds the coded masks approximately 4 meters above their detectors. The payload module can be built and tested independently from the service module, reducing cost.\n\nAlenia Spazio (now Thales Alenia Space Italia) was the spacecraft prime contractor.\n\nFour instruments are coaligned to study a target across several ranges. The coded masks were led by the University of Valencia, Spain.\n\nThe INTEGRAL imager, IBIS (Imager on-Board the INTEGRAL Satellite) observes from 15 keV (hard X-rays) to 10 MeV (gamma rays). Angular resolution is 12 arcmin, enabling a bright source to be located to better than 1 arcmin. A 95 x 95 mask of rectangular tungsten tiles sits 3.2 meters above the detectors. The detector system contains a forward plane of 128 x 128 Cadmium-Telluride tiles (ISGRI- Integral Soft Gamma-Ray Imager), backed by a 64 x 64 plane of Caesium-Iodide tiles (PICsIT- Pixellated Caesium-Iodide Telescope). ISGRI is sensitive up to 1 MeV, while PICsIT extends to 10 MeV. Both are surrounded by passive shields of tungsten and lead.\nThe primary spectrometer aboard INTEGRAL is SPI, the SPectrometer for INTEGRAL. It was conceived and assembled by the French Space Agency CNES. It observes radiation between 20 keV and 8 MeV. SPI consists of a coded mask of hexagonal tungsten tiles, above a detector plane of 19 germanium crystals (also packed hexagonally). The Ge crystals are actively cooled with a mechanical system, and give an energy resolution of 2 keV at 1 MeV.\n\nIBIS and SPI need a method to stop background radiation. The SPI ACS (AntiCoincidence Shield) consists of a mask shield and a detector shield. The mask shield is a layer of plastic scintillator behind the tungsten tiles. It absorbs secondary radiation produced by impacts on the tungsten. The rest of the shield consists of BGO scintillator tiles around the sides and back of the SPI.\n\nThe enormous area of the ACS that results makes it an instrument in its own right. Its all-sky coverage and sensitivity make it a natural gamma-ray burst detector, and a valued component of the IPN (InterPlanetary Network). Recently, new algorithms allow the ACS to act as a telescope, through double Compton scattering. Thus ACS can study objects outside the field of view of the other instruments, with surprising spatial and energy resolution.\n\nDual JEM-X units provide additional information on targets. They observe in soft and hard X-rays, from 3 to 35 keV. Aside from broadening the spectral coverage, imaging is more precise due to the shorter wavelength. Detectors are gas scintillators (xenon plus methane) in a microstrip layout, below a mask of hexagonal tiles.\n\nINTEGRAL mounts an Optical Monitor (OMC), sensitive from 500 to 580 nm. It acts as both a framing aid, and can note the activity and state of some brighter targets.\n\nThe spacecraft also mounts a radiation monitor, INTEGRAL Radiation Environment Monitor (IREM), to note the orbital background for calibration purposes. IREM has an electron and a proton channel, though radiation up to cosmic rays can be sensed. Should the background exceed a preset threshold, IREM can shut down the instruments.\n\n\n"}
{"id": "3118138", "url": "https://en.wikipedia.org/wiki?curid=3118138", "title": "International Academy of Sex Research", "text": "International Academy of Sex Research\n\nThe International Academy of Sex Research (IASR) is a scientific society for researchers in sexology. According to John Bancroft, retired director of the Kinsey Institute, IASR \"can claim...most of the field's leading researchers.\" IASR is unique among sexology organizations in that individuals must be elected to membership, which requires demonstration of substantial contribution to sexology, including the authorship of 10 or more professional publications. Notable members have included Drs. Ray Blanchard, Milton Diamond, Kurt Freund, Richard Green, Leonore Tiefer, Judith Becker, and Ken Zucker. The official journal of IASR is the \"Archives of Sexual Behavior\".\n\nIASR was founded on 1 August 1973. The 53 founding charter members were nominated by the board of the journal \"Archives of Sexual Behavior\", which became the official publication of the IASR. The founding editor of \"Archives of Sexual Behavior\" was Richard Green. The current editor is Kenneth Zucker.\n\nThe first annual IASR meeting was held at the State University of New York at Stony Brook in September 1975. It was hosted by IASR founding president, Richard Green.\n\n\n\n"}
{"id": "58195759", "url": "https://en.wikipedia.org/wiki?curid=58195759", "title": "Jerry Silver", "text": "Jerry Silver\n\nJerry Silver is an American neuroscientist and professor in the Department of Neurosciences at Case Western Reserve University School of Medicine. He is known for his research using rat models to develop treatments for spinal cord injuries. He became a fellow of the American Association for the Advancement of Science in 2011.\n\n"}
{"id": "58713866", "url": "https://en.wikipedia.org/wiki?curid=58713866", "title": "Joshua B. Plotkin", "text": "Joshua B. Plotkin\n\nJoshua B. Plotkin is a computational and evolutionary biology ecologist, mathematician, and professor of biology at the University of Pennsylvania. Plotkin's research includes the study of the evolution of adaptation in populations, virus ecology, genetic drift, protein translation, and social norms.\n\nHe serves on the editorial boards for Science Magazine and Cell Reports.\n\n"}
{"id": "173889", "url": "https://en.wikipedia.org/wiki?curid=173889", "title": "Killer heuristic", "text": "Killer heuristic\n\nIn competitive two-player games, the killer heuristic is a technique for improving the efficiency of alpha-beta pruning, which in turn improves the efficiency of the minimax algorithm. This algorithm has an exponential search time to find the optimal next move, so general methods for speeding it up are very useful. Barbara Liskov created the general heuristic to speed tree search in a computer program to play chess endgames for her Ph.D. thesis.\n\nAlpha-beta pruning works best when the best moves are considered first. This is because the best moves are the ones most likely to produce a \"cutoff\", a condition where the game playing program knows that the position it is considering could not possibly have resulted from best play by both sides and so need not be considered further. I.e. the game playing program will always make its best available move for each position. It only needs to consider the other player's possible responses to that best move, and can skip evaluation of responses to (worse) moves it will not make.\n\nThe killer heuristic attempts to produce a cutoff by assuming that a move that produced a cutoff in another branch of the game tree at the same depth is likely to produce a cutoff in the present position, that is to say that a move that was a very good move from a different (but possibly similar) position might also be a good move in the present position. By trying the \"killer move\" before other moves, a game playing program can often produce an early cutoff, saving itself the effort of considering or even generating all legal moves from a position.\n\nIn practical implementation, game playing programs frequently keep track of two killer moves for each depth of the game tree (greater than depth of 1) and see if either of these moves, if legal, produces a cutoff before the program generates and considers the rest of the possible moves. If a non-killer move produces a cutoff, it replaces one of the two killer moves at its depth. This idea can be generalized into a set of refutation tables.\n\nA generalization of the killer heuristic is the \"history heuristic\". The history heuristic can be implemented as a table that is indexed by some characteristic of the move, for example \"from\" and \"to\" squares or piece moving and the \"to\" square. When there is a cutoff, the appropriate entry in the table is incremented, such as by adding \"d²\" or \"2\" where \"d\" is the current search depth. This information is used when ordering moves.\n\n"}
{"id": "34312509", "url": "https://en.wikipedia.org/wiki?curid=34312509", "title": "Level of analysis", "text": "Level of analysis\n\nThe term \"level of analysis\" is used in the social sciences to point to the location, size, or scale of a research target. \"Level of analysis\" is distinct from the term \"unit of observation\" in that the former refers to a more or less integrated set of relationships while the latter refers to the distinct unit from which data have been or will be gathered. Together, the unit of observation and the level of analysis help define the population of a research enterprise.\n\nAlthough levels of analysis are not necessarily mutually exclusive, there are three general levels into which social science research may fall: micro-level, meso-level or middle-range, and macro-level.\n\nThe smallest unit of analysis in the social sciences is an individual in their social setting. At the micro-level, also referred to as the local level, the research population typically is an individual in their social setting or a small group of individuals in a particular social context. Examples of micro levels of analysis include, but are not limited to, the following individual analysis type approach.\n\nIn general, a meso-level analysis indicates a population size that falls between the micro- and macro-levels, such as a community or an organization. However, meso-level may also refer to analyses that are specifically designed to reveal connections between micro- and macro-levels. It is sometimes referred to as mid-range, especially in sociology. Examples of meso-level units of analysis include, but are not limited to, the following. The\n\nMacro-level analyses generally trace the outcomes of interactions, such as economic or other resource transfer interactions over a large population. It is also referred to as the global level. Examples of macro-level units of analysis include, but are not limited to, the following.\n\nAccording to David Marr, information processing systems must be understood at three distinct yet complementary levels of analysis - an analysis at one level alone is not sufficient.\n\nThe computational level of analysis identifies \"what\" the information processing system does (e.g.: what problems does it solve or overcome) and similarly, why does it do these things.\n\nThe algorithmic/representational level of analysis identifies \"how\" the information processing system performs its computations, specifically, what representations are used and what processes are employed to build and manipulate the representations.\n\nThe physical level of analysis identifies how the information processing system is \"physically realized\" (in the case of biological vision, what neural structures and neuronal activities implement the visual system).\n\nAfter thirty years of the book \"Vision (David Marr. 1982. W. H. Freeman and Company)\", Tomaso Poggio adds one higher level beyond the computational level, that is the learning.\n\nIn political science, level of analysis is generally divided into three categories - individual, state, and international system, however newer discussions of globalization have led to a newer level of analysis to be considered. The three (or four) levels of analysis cannot describe every effect and there is unlimited number of levels between the three primary ones, levels of analysis will help understand how one force in political power affects another. Generally, power is the concept that collects all the analysis together. For example, the struggle for power may be the cause of war, but the struggle for power may originate in the individual human being's lust for power. The lust for power is individual level of analysis, while the struggle for power is systemic level of analysis.\n\nThe individual level of analysis locates the cause of events in individual leaders or the immediate circle of decision makers within a particular country. It focuses on human actors on the world stage identifying the characteristics of human decision making. For example, the cause of World War I is from the particular leaders in power at that time. Kaiser Wilhelm II is considered to be the level from which the cause originated. It may have been his need for power to hide a sense of inferiority, or it may have been his inability to understand the intricacies of statecraft, the way Otto von Bismarck did. Or it may have been his idea about the monarchy and German destiny. All three possibilities are drawn from an individual level of analysis.\n\nThe domestic level of analysis locates causes in the character of the domestic system of specific states. Thus, war is caused by aggressive or warlike states, not by evil, inept, or misguided people or the structure of power in the international system. The failure of domestic institutions may also cause war. In World War I, the internal collapse of the Austro-Hungarian Empire, or the brittle coalition inside Germany of agricultural and industrial interest, such as rye and iron, are often cited as important causes. Domestic level cases may come from various characteristics of the domestic system. Capitalist and socialist economies generate different attitudes and behavior. \n\nThe Muslim and Christian religions or democratic and nondemocratic political ideologies do as well. Stable and failed institutions are domestic level factors affecting state behavior. A great worry today is the existence of failed states, meaning states whose domestic institutions have broken down, such as Somalia. Another worry here is existence of a rogue state, such as North Korea, which may pass nuclear weapons on to terrorists. Any type of state come from the domestic level of analysis, but a failed state usually means an institutional breakdown at domestic level of analysis, whereas a rogue state often implies evil intentions by individual – individual level of analysis.\n\nThe systemic level of analysis explains outcomes from a system wide level that includes all states. It takes into account both the position of states in the international system and their interrelationships. The position of states constitutes the systemic structural level of analysis. This involves the relative distribution of power, such as which state; great, middle, or small power, and geopolitics; such as which state is sea or land power. The interaction of states constitutes the systemic process level of analysis. At this level, we are concerned with which state aligns with which other states and which state negotiates with which other states. Thus, we can explain World War I in terms of the absence of system wide institutions, such as League of Nations, which was not created until after World War I to prevent such wars in the future. However, system wide institution does not always mean harmony among nations, as seen in the World War II. The cause of World War II is seen as the failure of a systemic institution, which led new institutions of the United Nations to carry on reformed legacy of the League of Nations.\n\nGlobal level factors are much like Systemic level factors, however the core difference is that global factors are not necessarily created by states, whereas systemic factors are. Global factors \"can\" be the outcome of individuals, interest groups, states, nonstate actors or even natural conditions – however they cannot be \"traced\" to the actions of any one state or even group of states. An example can be how the internet can shape how policy is formed, through social media or forums – where an idea is formed over time by a group of individuals, but the source is generally hard to determine. An environmental natural example is how global warming can help shape how society views certain policies, or help shape new policies themselves. Droughts caused by rising temperatures can cause global actors to form alliances to help procure critical resources - and as writers such as Peter Gleik and Michael Klare have shown, the possibility of \"Water Wars\" in dry countries in Africa and the Middle East are very possible.\n\n\n\n"}
{"id": "1875327", "url": "https://en.wikipedia.org/wiki?curid=1875327", "title": "List of countries with McDonald's restaurants", "text": "List of countries with McDonald's restaurants\n\nThis is a listing of countries with McDonald's restaurants. McDonald's is the largest chain of fast food restaurants in the world. It has more than 35,000 outlets worldwide. The majority of McDonald's outlets outside of the United States are franchises.\n\nThe biggest temporary McDonald's restaurant in the world was opened during 2012 Summer Olympics in London, which had . The biggest still standing one is probably that at Will Rogers Turnpike.\n\nThe list of countries follows the company's own calculation, and contains several non-sovereign territories.\n\n"}
{"id": "38469902", "url": "https://en.wikipedia.org/wiki?curid=38469902", "title": "List of ecoregions in Illinois", "text": "List of ecoregions in Illinois\n\nThe list of ecoregions in Illinois are lists of terrestrial ecoregions (see also, ecosystem) of the United States' State of Illinois, as defined separately by the United States Environmental Protection Agency (USEPA), and by the World Wildlife Fund. Illinois' ecology is in a land area of ; the state is long and wide and is located between latitude: 36.9540° to 42.4951° N, and longitude: 87.3840° to 91.4244° W, with primarily a humid continental climate.\n\nThe EPA ecoregion classification system has four levels, but only Levels I, III, and IV are shown on this list. Level I divides North America into 15 broad ecoregions (or biomes). Illinois is almost entirely within the Eastern Temperate Forest environment Level I region, although very small sections in its extreme west are in the Great Plains, Level I region. Level IV ecoregions (denoted by numbers and letters) are a further subdivision of Level III ecoregions (denoted by numbers alone). In general, Illinois transitions from the forests, to savannah, to tall grass prairie, and is now largely used for agriculture or urbanized, although in its far south are the forested highlands of the Shawnee Hills and along its major rivers varying topography and biome occurs. Its larger ecoregion areas are 'corn belt' plains, known for rich, thick loess (in its north, center and east, particularly 54a) and the 'rivers and hills' region, which also has large till plains in Illinois' south (72j) and west (72i). \n\nThese forests stretch from eastern Texas and northern Florida to the Adirondacks and Wisconsin. For a general description of these forests, refer to Temperate Deciduous Forest. The standard reference is \"The Deciduous Forest of Eastern North America\".\n\n\n\n"}
{"id": "37356389", "url": "https://en.wikipedia.org/wiki?curid=37356389", "title": "List of psephologists", "text": "List of psephologists\n\nThis is a list of notable psephologists.\n\n\"In surname alphabetical order:\"\n\n\n"}
{"id": "51033165", "url": "https://en.wikipedia.org/wiki?curid=51033165", "title": "List of things named after Felix Bloch", "text": "List of things named after Felix Bloch\n\n\n"}
{"id": "1586886", "url": "https://en.wikipedia.org/wiki?curid=1586886", "title": "Malus (constellation)", "text": "Malus (constellation)\n\nMalus (Latin for \"mast\") was a subdivision of the ancient constellation Argo Navis proposed in 1844 by the English astronomer John Herschel. It would have replaced Pyxis, the compass, which was introduced in the 1750s by Nicolas Louis de Lacaille. Herschel's suggestion was not widely adopted and Malus is not now recognized by astronomers.\n\n\n"}
{"id": "21689632", "url": "https://en.wikipedia.org/wiki?curid=21689632", "title": "Market design", "text": "Market design\n\nIn his 2008, Nemmers Prize lecture, Paul Milgrom gave the following definition of \"Market Design\"\n\nMarket design is a kind of economic engineering, utilizing laboratory research, game theory, algorithms, simulations, and more. Its challenges inspire us to rethink longstanding fundamentals of economic theory.\n\nHe outlined that two broad theoretical and practical efforts defined the field: \"auction theory\" and \"matching theory\". Milgrom has contributed to both and also, in many respects, to their synthesis. A third component of Market design is \"Simplifying Participants’ Message\". \n\nMilgrom advised the FCC on the design of the auction of bandwidth for cellular communications.\n\nEarly research on auctions focused on two special cases: common value auctions in which buyers have private signals of an items true value and private value auctions in which values are identically and independently distributed. Milgrom and Weber (1982) present a much more general theory of auctions with positively related values. Each of \"n\" buyers receives a private signal formula_1 . Buyer \"i\"’s value formula_2 is strictly increasing in formula_1 and is an increasing symmetric function of formula_1. If signals are independently and identically distributed, then buyer \"i\"’s expected value formula_5 is independent of the other buyers’ signals. Thus, the buyers’ expected values are independently and identically distributed. This is the standard private value auction. For such auctions the revenue equivalence theorem holds. That is, expected revenue is the same in the sealed first-price and second-price auctions.\n\nMilgrom and Weber assumed instead that the private signals are “affiliated”. With two buyers, the random variables formula_1 and formula_1 with probability density function formula_8 are affiliated if\nApplying Bayes’ Rule it follows that\nformula_12, for all formula_10 and all formula_11.\n\nRearranging this inequality and integrating with respect to formula_15 it follows that\nIt is this implication of affiliation that is critical in the discussion below.\n\nFor more than two symmetrically distributed random variables, let formula_19 be a set of random variables that are continuously distributed with joint probability density function \"f(v\") . The \"n\" random variables are affiliated if\n\nRevenue Ranking Theorem (Milgrom and Weber)\n\nSuppose each of \"n\" buyers receives a private signal formula_1 . Buyer \"i\"’s value formula_2 is strictly increasing in formula_1 and is an increasing symmetric function of formula_1. If signals are affiliated, the equilibrium bid function in a sealed first-price auction formula_29 is smaller than the equilibrium expected payment in the sealed second price auction.\n\nThe intuition for this result is as follows: In the sealed second-price auction the expected payment of a winning bidder with value \"v\" is based on their own information. By the revenue equivalence theorem if all buyers had the same beliefs, there would be revenue equivalence. However, if values are affiliated, a buyer with value \"v\" knows that buyers with lower values have more pessimistic beliefs about the distribution of values. In the sealed high-bid auction such low value buyers therefore bid lower than they would if they had the same beliefs. Thus the buyer with value \"v\" does not have to compete so hard and bids lower as well. Thus the informational effect lowers the equilibrium payment of the winning bidder in the sealed first-price auction.\n\nEquilibrium bidding in the sealed first- and second-price auctions: We consider here the simplest case in which there are two buyers and each buyer’s value formula_30 depends only on his own signal. Then the buyers’ values are private and affiliated. In the sealed second-price (or Vickrey auction), it is a dominant strategy for each buyer to bid his value. If both buyers do so, then a buyer with value v has an expected payment of\n\nIn the sealed first-price auction, the increasing bid function \"B\"(\"v\") is an equilibrium if bidding strategies are mutual best responses. That is, if buyer 1 has value \"v\", their best response is to bid \"b\" = \"B\"(\"v\") if they believes that their opponent is using this same bidding function. Suppose buyer 1 deviates and bids \"b\" = \"B\"(\"z\") rather than \"B\"(\"v\") . Let U(z) be their resulting payoff. For \"B\"(\"v\") to be an equilibrium bid function, \"U\"(\"z\") must take on its maximum at \"x\" = \"v\".\nWith a bid of \"b\" = \"B\"(\"z\") buyer 1 wins if\nThe win probability is then formula_33 so that buyer 1’s expected payoff is\nTaking logs and differentiating by \"z\",\n\n"}
{"id": "26146516", "url": "https://en.wikipedia.org/wiki?curid=26146516", "title": "Mathematical knowledge management", "text": "Mathematical knowledge management\n\nMathematical knowledge management (MKM) is the study of how society can effectively make use of the vast and growing literature on mathematics. It studies approaches such as databases of mathematical knowledge, automated processing of formulae and the use of semantic information, and artificial intelligence. Mathematics is particularly suited to a systematic study of automated knowledge processing due to the high degree of interconnectedness between different areas of mathematics.\n\n\n"}
{"id": "6876796", "url": "https://en.wikipedia.org/wiki?curid=6876796", "title": "Mathematically Correct", "text": "Mathematically Correct\n\nMathematically Correct was a U.S.-based website created by educators, parents, mathematicians, and scientists who were concerned about the direction of reform mathematics curricula based on NCTM standards. Created in 1997, it was a frequently cited website in the so-called Math wars, and was actively updated until 2003. The website went offline sometime in late 2012 or early 2013 but has been preserved on the Internet Archive.\n\nAlthough Mathematically Correct had a national scope, much of its focus was on advocating against mathematics curricula prevalent in California in the mid-1990s. When California reversed course and adopted more traditional mathematics texts (2001 - 2002), Mathematically Correct changed its focus to reviewing the new text books. Convinced that the choices were adequate, the website went largely dormant.\n\nMathematically Correct maintained a large section of critical articles and reviews for a number of math programs. Most of the program opposed by Mathematically Correct had been developed from research projects funded by the National Science Foundation. Most of these programs also claimed to have been based on the 1989 Curriculum and Evaluation Standards for School Mathematics published by the National Council of Teachers of Mathematics.\n\nMathematically Correct's main point of contention was that, in reform textbooks, traditional methods and concepts have been omitted or replaced by new terminology and procedures. As a result, in the case of the high-school program Core-Plus Mathematics Project, for example, some reports suggest that students may be unprepared for college level courses upon completion of the program. Other programs given poor ratings include programs aimed at elementary school students, such as Dale Seymour Publications (TERC) Investigations in Numbers, Data, and Space and Everyday Learning Everyday Mathematics. \n\nSince Mathematically Correct had reviewed the programs, many have undergone revisions and are now with different publishers. Other programs, such as Mathland have been terminated.\n\n\n\n\n\n"}
{"id": "11584601", "url": "https://en.wikipedia.org/wiki?curid=11584601", "title": "Melanoidin", "text": "Melanoidin\n\nMelanoidins are brown, high molecular weight heterogeneous polymers that are formed when sugars and amino acids combine (through the Maillard reaction) at high temperatures and low water activity. Melanoidins are commonly present in foods that have undergone some form of non-enzymatic browning, such as barley malts (Vienna and Munich), bread crust, bakery products and coffee. They are also present in the wastewater of sugar refineries, necessitating treatment in order to avoid contamination around the outflow of these refineries.\n"}
{"id": "6505575", "url": "https://en.wikipedia.org/wiki?curid=6505575", "title": "Milne model", "text": "Milne model\n\nThe Milne model was a special-relativistic cosmological model proposed by Edward Arthur Milne in 1935. It is mathematically equivalent to a special case of the FLRW model in the limit of zero energy density (in other words, an empty universe) and it obeys the cosmological principle. The Milne model is also similar to Rindler space, a simple re-parameterization of flat Minkowski space.\n\nSince it features both zero energy density and maximally negative spatial curvature, the Milne model is inconsistent with cosmological observations. Cosmologists actually observe the universe's density parameter to be consistent with unity and its curvature to be consistent with flatness.\n\nThe Milne universe is a special case of a more general Friedmann–Lemaître–Robertson–Walker model (FLRW). The Milne solution can be obtained from the more generic FLRW model by demanding that the energy density, pressure and cosmological constant all equal zero and the spatial curvature is negative. From these assumptions and the Friedmann equations it follows that the scale factor must depend on time coordinate linearly.\nSetting the spatial curvature and speed of light to unity the metric for a Milne universe can be expressed with hyperspherical coordinates as:\n\nwhere\nis the metric for a two-sphere and\nis the curvature-corrected radial component for negatively curved space that varies between 0 and formula_4.\n\nThe empty space that the Milne model describes can be identified with the inside of a light cone of an event in Minkowski space by a change of coordinates.\n\nMilne developed this model independent of general relativity but with awareness of special relativity. As he initially described it, the model has no expansion of space, so all of the redshift (except that caused by peculiar velocities) is explained by a recessional velocity associated with the hypothetical \"explosion\". However, the mathematical equivalence of the zero energy density (formula_5) version of the FLRW metric to Milne's model implies that a full general relativistic treatment using Milne's assumptions would result in an increasing scale factor and associated metric expansion of space with the unique feature of a linearly increasing scale factor for all time since the deceleration parameter is uniquely zero for such a model.\n\nEven though the Milne model as a special case of a Friedmann-Robertson-Walker universe is a solution to General relativity, the assumption of zero energy content limits its use as a realistic description of the universe. Besides lacking the capability of describing matter Milne's universe is also incompatible with certain cosmological observations. In particular it makes no prediction of the cosmic microwave background radiation nor the abundance of light elements which are hallmark pieces of evidence that cosmologists agree support Big Bang cosmology over alternatives.\n\nMilne proposed that the universe's density changes in time because of an initial outward explosion of matter. Milne's model assumes an inhomogeneous density function which is Lorentz Invariant (around the event t=x=y=z=0). When rendered graphically Milne's density distribution shows a three-dimensional spherical Lobachevskian pattern with outer edges moving outward at the speed of light. Every inertial body perceives itself to be at the center of the explosion of matter (see observable universe), and sees the local universe as homogeneous and isotropic in the sense of the cosmological principle.\n\nUnless the universe modeled has zero density, Milne's proposal does not follow the predictions of general relativity for the curvature of space caused by global matter distribution, as seen in, for example statistics associated with large-scale structure.\n\nIn order to explain the existence of matter in the universe, Milne proposed a physical explosion of matter which would not affect the universe's geometry. This is in contrast to the metric expansion of space that is the hallmark feature of many of the more famous cosmological models including the Big Bang and Steady State models. Milne's universe shares a superficial similarity to Einstein's static universe in that the metric of space is not time-dependent. Unlike Einstein's initial cosmology, Milne's proposal directly contradicts the Einstein equations for cosmological scales. Special relativity becomes a global property of Milne's universe while general relativity is confined to a local property. The reverse is true for standard cosmological models, and most scientists and mathematicians agree that the latter is self-consistent while the former is mathematically impossible.\n\nEdward Arthur Milne predicted a kind of event horizon through the use of this model: \"The particles near the boundary tend towards invisibility as seen by the central observer, and fade into a continuous background of finite intensity.\" The horizon arises naturally from length contraction seen in special relativity which is a consequence of the speed of light upper bound for physical objects. In Milne's universe, the velocities of objects approach this upper bound while the distance to these objects approaches the speed of light multiplied by the time since the event of the initial explosion of material. Beyond this distance, objects do not lie in the observable part of the Milne universe.\n\nAt the time Milne proposed his model, observations of the universe did not appear to be in a homogeneous form. This, to Milne, was a deficiency inherent in the competing cosmological models which relied on the cosmological principle that demanded a homogeneous universe. “This conventional homogeneity is only definite when the motion of the particles is first prescribed.” With present observations of the homogeneity of the universe on the largest scales seen in the cosmic microwave background and in the so-called \"End of Greatness\", questions about the homogeneity of the universe have been settled in the minds of most observational cosmologists.\n\n"}
{"id": "34652474", "url": "https://en.wikipedia.org/wiki?curid=34652474", "title": "Multipartite", "text": "Multipartite\n\nMultipartite is a class of virus that have segmented nucleic acid genomes. Only a few ssDNA viruses have multipartite genomes, but a lot more RNA viruses have multipartite genomes. An advantage of multipartite genome is its ability to synthesize multiple mRNA strands to avoid the cellular constraint of monocistronicity.\n\nMonopartite\n"}
{"id": "49122038", "url": "https://en.wikipedia.org/wiki?curid=49122038", "title": "Notional election results", "text": "Notional election results\n\nNotional election results are calculations made, usually following boundary changes of electoral districts brought about by population shifts, to determine what election results would have been in previous elections had the newly created boundaries then been in place. Such calculations are used by psephologists and media for the purposes of detecting changing voting patterns and swings from one party to another from one election to the next, which is easier to calculate when no boundary changes have been implemented.\n\nBoundary reviews can favour a particular political party, as notional election calculations have found. In the case of the United Kingdom, the Conservative Party usually benefits slightly due to population shifts away from inner cities towards suburban areas, where the Conservative Party has a stronger vote base. Consequently, representation in inner city areas - usually areas where the Labour Party has its strongest voter base - declines as its population declines.\n\nNotional election result calculations are of particular importance in the United Kingdom because the country is unusual in not releasing official breakdowns in voting data, which enables such data to be applied to electoral areas which are subject to boundary changes. Parliamentary constituencies are based on smaller local government wards, and it is the results from local government elections that are the primary source used to calculate notional results at a constituency level.\n\nProfessors Colin Rallings and Michael Thrasher of Plymouth University have been involved in calculating notional results for general elections since 1992 where periodic boundary changes have taken place. The data compiled from their work has been used by several prominent media companies including television channels for the purposes of comparing one general election result to the last.\n\nNotional results have their limitations in that voting patterns may well have been different had new boundaries been in force at previous elections, because some voters who vote tactically may have chosen to vote differently under different boundaries, depending on which political parties or candidates were most competitive under old boundaries.\n\nOther problems with making notional calculations is that people vote differently between national and local elections, particularly as many councilors enjoy a cross-party personal vote. Local factors can also have a great effect on voting patterns. In addition, a large number of wards are not contested by all the major political parties, some of them electing independent candidates or candidates from residents' associations. Sometimes wards themselves are subject to boundary changes.\n\nOn a national level, notional general election results by constituency have been calculated in the UK on numerous occasions in the past by academics and election experts such as Rallings and Thrasher, when boundary changes have been implemented. \n\nIn the February 1974 general election, notional results from the previous general election in 1970 were calculated on behalf of the BBC in constituencies where major boundary changes took effect.\nIn 1983, extensive boundary changes led to notional general election results being jointly calculated by the BBC and ITN for the previous general election in 1979.\n\nFurther notional results were created for the 1997 general election (to compare with the 1992 general election), in 2005 (to take into account the reduction in the number of Scottish constituencies brought about by the establishment of the Scottish Parliament), and in the 2010 general election (to compare with the 2005 general election).\n"}
{"id": "19838404", "url": "https://en.wikipedia.org/wiki?curid=19838404", "title": "Nuker Team", "text": "Nuker Team\n\nThe Nuker Team was formed to use the Hubble Space Telescope, with its high-resolution imaging and spectroscopy, to investigate the central structure and dynamics of galaxies. The team used the HST to examine supermassive black holes and determined the relationship between a galaxy's central black hole's mass and velocity dispersion. The team continues to conduct research and publish papers on the supermassive black holes of galaxies and clusters. The group was initially formed by Tod R. Lauer, then a first year postdoc. At the first meeting of the group held at Princeton University in June 1985, Sandra Faber was elected the group leader.\n\nThe original members of the Nuker Team include Alan Dressler (OCIW), Sandra Faber (UCO/Lick; First PI), John Kormendy (Texas), Tod R. Lauer (NOAO), Douglas Richstone (Michigan; Present PI), and Scott Tremaine (IAS). Later additions to the team include Ralf Bender (Munchen), Alexei V. Filippenko (Berkeley), Karl Gebhardt (Texas), Richard Green (LBTO), Kayhan Gultekin (Michigan), Luis C. Ho (OCIW), John Magorrian (Oxford), Jason Pinkney (Ohio Northern), and Christos Siopis (Michigan).\n\nThe name \"Nuker\" began as an informal internal reference by members of the team to each other, because they came together to study the nuclei of galaxies using the space telescope. The first use of the name was in a 1989 email from Faber, who addressed her five colleagues as \"Dear Nukers\". As the team began to publish its research, the name came into general use in the scientific community.\n\nThe name \"Nuker\" is also used in reference to the \"Nuker Law\", which is a description of the inner few (~3-10) arcseconds of predominantly nearby (< 30 Mpc) early-type galaxy light-profiles. The Nuker Law was described first by members of the Nuker Team, from which it gets its name.\n\n\n"}
{"id": "43779734", "url": "https://en.wikipedia.org/wiki?curid=43779734", "title": "Onur Güntürkün", "text": "Onur Güntürkün\n\nOnur Güntürkün (born 18 July 1958, in İzmir) is a Turkish-German neuroscientist. He is professor of behavioral neuroscience at Ruhr University Bochum.\nGüntürkün studied psychology at the Ruhr University Bochum from 1975 to 1980 and received his PhD in 1984.\n\n"}
{"id": "16606849", "url": "https://en.wikipedia.org/wiki?curid=16606849", "title": "Operation Toggle", "text": "Operation Toggle\n\nThe United States's Toggle nuclear test series was a group of 28 nuclear tests conducted in 1972-1973. These tests followed the \"Operation Grommet\" series and preceded the \"Operation Arbor\" series.\n"}
{"id": "17917373", "url": "https://en.wikipedia.org/wiki?curid=17917373", "title": "Patterns of Sexual Behavior", "text": "Patterns of Sexual Behavior\n\nPatterns of Sexual Behavior is a 1951 book by anthropologist Clellan S. Ford and ethologist Frank A. Beach, in which the authors integrate information about human sexual behavior from different cultures, and include detailed comparisons across animal species, with particular emphasis on primates. The book received positive reviews and has been called a classic. It provided the foundation for the later research of Masters and Johnson. A revised edition, titled \"Human Sexuality in Four Perspectives\", was published in 1977.\n\nFord and Beach employ a \"cross-cultural correlational method\" in exploring sexual behavior, a statistical approach suitable for distinguishing behavioral trends and making generalizations. They integrate information from 191 cultures: 48 from Oceania, 28 from Eurasia, 33 from Africa, 57 from North America, and 26 from South America. Much of their data was collected in the Human Relations Area Files, a cross-institutional organization co-founded by Ford. They offer information on such topics as \"sexual positions, length (time) of intercourse, locations for intercourse, orgasm experiences, types of foreplay, courting behaviors, frequencies of intercourse [and] methods of attracting a partner.\" They cover homosexuality in both humans and other animals, citing evidence of accepted homosexual behavior in 49 of the 76 cultures for which the relevant data were available. Ford and Beach conclude that there is a \"basic mammalian capacity\" for same-sex behavior.\n\n\"Patterns of Sexual Behavior\" was originally published by Harper & Brothers, New York in 1951. The following year, the work was reprinted (under the title \"Patterns of Sexual Behaviour\") by Eyre and Spottiswoode in London. Metheun published a reprint of the 1951 Harper & Row edition in 1965. In 1977, Frank Beach authored a revised version of the book, entitled \"Human Sexuality in Four Perspectives.\"\n\n\"Patterns of Sexual Behavior\" received positive reviews from Allan R. Holmberg in the \"American Sociological Review\" and Abraham Stone in \"Marriage and Family Living\", and was later discussed by the anthropologist George Murdock in \"American Anthropologist\".\n\nHolmberg described the book as well-written, and credited Ford and Beach with placing \"the study of sex in a broad scientific perspective\" by presenting and analyzing \"an enormous body of data\" on sexual behavior in both humans and non-human animals and placing it in cross-cultural, evolutionary, and physiological perspectives. He described their efforts as having \"important theoretical, methodological, and practical implications\" and believed they showed the merits of a \"cross-disciplinary approach to the problems of human behavior.\" He complimented them for statistically documenting sexual practices and attitudes, and contributing important material on masturbation and homosexuality, suggesting the existence of \"an inherent biological tendency toward such activities.\" He believed their book deserved to be widely read and predicted that it would have a \"healthy impact on attitudes toward sex\" and encourage further research by social scientists. However, he criticized them for providing insufficient discussion of \"the symbolic aspects of sexual behavior\".\n\nStone credited Ford and Beach with examining both biological and social influences on sex, thereby providing an \"essential perspective\" on human sexual behavior. Though he considered their use of the term \"sex behavior\" to refer exclusively to \"behavior involving stimulation and excitation of the sexual organ\" to be narrow, he believed they dealt \"in great detail with a great many aspects of sex conduct and sex contact\" and provided a \"very full presentation of sex behavior from the point of view of anatomy and physiology\".\n\nMurdock described the book as a \"classic\" of its field.\n\nAnne Bolin and Patricia Whelehan, writing in \"Perspectives on Human Sexuality\" (1999), identified \"Patterns of Sexual Behavior\" as a book that was highly influential in the study of sexual behavior, and provided the intellectual foundation for the later research of Masters and Johnson. Andrew Paul Lyons and Harriet Lyons, writing in \"Irregular Connections: A History of Anthropology and Sexuality\" (2004), argued that \"Patterns of Sexual Behavior\" was comprehensive for its time but nevertheless contained a number of self-imposed limitations. Its authors limited their definition of sexual behavior to \"behavior involving stimulation and excitation of the sexual organs,\" and made no attempt to explore sexual symbolism. While acknowledging that their study might have implications for psychology and psychoanalysis, they felt themselves unqualified to explore specific questions pertaining to this field. They claimed to make no judgements of moral value, though their study is considered supportive of sexual relativism. Lyons and Lyons credited them with \"making homosexual behavior more visible and more acceptable within the culture of its time.\"\n"}
{"id": "26777607", "url": "https://en.wikipedia.org/wiki?curid=26777607", "title": "Phage typing", "text": "Phage typing\n\nPhage typing is a method used for detecting single strains of bacteria. It is used to trace the source of outbreaks of infections. The viruses that infect bacteria are called bacteriophages (\"phages\" for short) and some of these can only infect a single strain of bacteria. These phages are used to identify different strains of bacteria within a single species.\n\nA culture of the strain is grown in the agar and dried. A grid is drawn on the base of the Petri dish to mark out different regions. Inoculation of each square of the grid is done by a different phage. The phage drops are allowed to dry and are incubated: The susceptible phage regions will show a circular clearing where the bacteria have been lysed, and this is used in differentiation.\n"}
{"id": "17783528", "url": "https://en.wikipedia.org/wiki?curid=17783528", "title": "Sense of Place", "text": "Sense of Place\n\nSense of place: a response to an environment: the Swan Coastal Plain, Western Australia is a 1972 book by George Seddon. It documents Seddon's struggle to understand the Swan Coastal Plain, a biogeographic region that he initially found harsh and unwelcoming. It includes information on landforms, climate, geology, soils, flora, the Swan River, the coast, offshore islands, wetlands, and urban areas. This information is, however, essentially presented in a literary style; in the words of Mark Tredinnick: \"This is the kind of geography an essayist writes. This is the kind of essay a literate scientist writes. This is a literary natural history.\"\n\nIn Australia it is considered a landmark environmental publication. Among its claims to influence is having given modern currency to the term \"sense of place\". Although Seddon did not coin the phrase, it was this book that introduced the phrase into the fields of landscape and environmental design.\n"}
{"id": "519933", "url": "https://en.wikipedia.org/wiki?curid=519933", "title": "Shikasta", "text": "Shikasta\n\nRe: Colonised Planet 5, Shikasta (often shortened to Shikasta) is a 1979 science fiction novel by Doris Lessing, and is the first book in her five-book \"Canopus in Argos\" series. It was first published in the United States in October 1979 by Alfred A. Knopf, and in the United Kingdom in November 1979 by Jonathan Cape. Shikasta is also the name of the fictional planet featured in the novel.\n\nSubtitled \"Personal, psychological, historical documents relating to visit by Johor (George Sherban) Emissary (Grade 9) 87th of the Period of the Last Days\", \"Shikasta\" is the history of the planet Shikasta (an allegorical Earth) under the influence of three galactic empires, Canopus, Sirius, and their mutual enemy, Puttiora. The book is presented in the form of a series of reports by Canopean emissaries to Shikasta who document the planet's prehistory, its degeneration leading to the \"Century of Destruction\" (the 20th century), and the Apocalypse (World War III).\n\n\"Shikasta\" draws on the Old Testament and is influenced by spiritual and mystical themes in Sufism, an Islamic belief system in which Lessing had taken an interest in the mid-1960s. The book represented a major shift of focus in Lessing's writing, from realism to science fiction, and this disappointed many of her readers. It received mixed reviews from critics. Some were impressed by the scope and vision of the book, with one reviewer calling it \"an audacious and disturbing work from one of the world's great living writers\". Others were critical of the novel's bleakness, that humanity has no free will and that their fate lies in the hands of galactic empires.\n\nThe story of Shikasta is retold in the third book of the \"Canopus\" series, \"The Sirian Experiments\" (1980), this time from the point of view of Sirius. Shikasta reappears in the fourth book in the series, \"The Making of the Representative for Planet 8\" (1982), and the Zones, briefly mentioned in \"Shikasta\", are the subject of the second book in the series, \"The Marriages Between Zones Three, Four and Five\" (1980).\n\nCanopus, a benevolent galactic empire centred at Canopus in the constellation Argo Navis, colonises a young and promising planet they name Rohanda (the fruitful). They nurture its bourgeoning humanoids and accelerate their evolution. When the Natives are ready, Canopus imposes a \"Lock\" on Rohanda that links it via \"astral currents\" to the harmony and strength of the Canopean Empire. In addition to Canopus, two other empires also establish a presence on the planet: their ally, Sirius from the star of the same name, and their mutual enemy, Puttiora. The Sirians confine their activities largely to genetic experiments on the southern continents during Rohanda's prehistory (described in Lessing's third book in the \"Canopus\" series, \"The Sirian Experiments\"), while the Shammat of Puttiora remain dormant, waiting for opportunities to strike.\n\nFor many millennia the Natives of Rohanda prosper in a Canopean induced climate of peaceful coexistence and accelerated development. Then an unforeseen \"cosmic re-alignment\" puts Rohanda out of phase with Canopus which causes the Lock to break. Deprived of Canopus's resources and a steady stream of a substance called SOWF (substance-of-we-feeling), the Natives develop a \"Degenerative Disease\" that puts the goals of the individual ahead of those of the community. The Shammat exploit this disturbance and begin undermining Canopus's influence by infecting the Natives with their evil ways. As Rohanda degenerates into greed and conflict, the Canopeans reluctantly change its name to Shikasta (the stricken). Later in the book, Shikasta is identified as Earth, or an allegorical Earth.\n\nIn an attempt to salvage Canopus's plans for Shikasta and correct the Natives' decline, Canopean emissaries are sent to the planet. Johor is one such emissary, who takes on the form of a Native and begins identifying those individuals who have not degenerated too far and are amenable to his corrective instructions. Johor then sends those he has successfully \"converted\" to spread the word among other Natives, and soon isolated communities begin to return to the pre-Shikastan days. But without the SOWF and Shammat's influence over the Natives, Canopus is fighting a losing battle and the planet declines further. By the Shikastan's 20th century, the planet has degenerated into war and self-destruction. Johor returns, but this time through Zone 6 from which he is born on the planet (incarnated) as a Shikastan, George Sherban. As Sherban grows up, he establishes contact with other Canopeans in disguise and then resumes his work trying to help the Shikastans. But famine and unemployment grow, and anarchy spreads.\n\nOn the eve of World War III, Sherban and other emissaries relocate a small number of promising Shikastans to remote locations to escape the coming nuclear holocaust. He also takes part in the trial of all Europeans for the crimes of colonialism. Europe has been conquered by China, but he persuades people that Europe was not the only offender.\n\nThe war reduces Shikasta's population by 99% and sweeps the planet clean of the \"barbarians\". The Shammat, who set the Shikastans on a course of self-destruction, self-destruct themselves and withdraw from the planet. The Canopeans help the survivors rebuild their lives and re-align themselves with Canopus. With a strengthened Lock and the SOWF flowing freely again, harmony and prosperity return to Shikasta.\n\nIn the mid-1960s Lessing had become interested in Sufism, an Islamic belief system, after reading \"The Sufis\" by Idries Shah. She described \"The Sufis\" as \"the most surprising book [she] had read\", and said it \"changed [her] life\". Lessing later met Shah, who became \"a good friend [and] teacher\". In the early 1970s Lessing began writing \"inner space\" fiction, which included the novels \"Briefing for a Descent into Hell\" (1971) and \"Memoirs of a Survivor\" (1974). In the late 1970s she wrote \"Shikasta\" in which she used many Sufi concepts.\n\n\"Shikasta\" was intended to be a \"single self-contained book\", but as Lessing's fictional universe developed, she found she had ideas for more than just one book, and ended up writing a series of five. \"Shikasta\", and the \"Canopus in Argos\" series as a whole, fall into the category of soft science fiction (\"space fiction\" in Lessing's own words) due to their focus on characterization and social and cultural issues, and the de-emphasis of science and technology. Robert Alter of \"The New York Times\" suggested that this kind of writing belongs to a genre literary critic Northrop Frye called the \"anatomy\", which is \"a combination of fantasy and morality\". Gore Vidal placed Lessing's \"science fiction\" \"somewhere between John Milton and L. Ron Hubbard\".\n\"Shikasta\" represented a major shift of focus for Lessing, influenced by spiritual and mystical themes in Sufism. This switch to \"science fiction\" was not well received by readers and critics. By the late 1970s, Lessing was considered \"one of the most honest, intelligent and engaged writers of the day\", and Western readers unfamiliar with Sufism were dismayed that Lessing had abandoned her \"rational worldview\". George Stade of \"The New York Times\" complained that \"our Grand Mistress of lumpen realism has gone religious on us\". The reaction of reviewers and readers to the first two books in the series, \"Shikasta\" and \"The Marriages Between Zones Three, Four and Five\" (1980), prompted Lessing to write in the Preface to the third book in the series, \"The Sirian Experiments\" (1980):\n\nFurther criticism of the \"Canopus\" series followed, which included this comment by \"New York Times\" critic John Leonard: \"One of the many sins for which the 20th century will be held accountable is that it has discouraged Mrs. Lessing ... She now propagandizes on behalf of our insignificance in the cosmic razzmatazz.\" Lessing replied by saying: \"What they didn't realize was that in science fiction is some of the best social fiction of our time. I also admire the classic sort of science fiction, like \"Blood Music\", by Greg Bear. He's a great writer.\" Lessing said in 1983 that she would like to write stories about red and white dwarves, space rockets powered by anti-gravity, and charmed and coloured quarks, \"[b]ut we can't all be physicists\".\n\nLessing later wrote several essays on Sufism which were published in her essay collection, \"Time Bites\" (2004). She was awarded the 2007 Nobel Prize in Literature, and was described by the Swedish Academy as \"that epicist of the female experience, who with scepticism, fire and visionary power has subjected a divided civilisation to scrutiny\".\n\nLessing dedicated \"Shikasta\" to her father. While she was still a child in Southern Rhodesia (now Zimbabwe) he often used to gaze up at the night sky and say, \"Makes you think – there are so many worlds up there, wouldn't really matter if we did blow ourselves up – plenty more where we came from.\" \"Shikasta\" gave rise to a religious cult in America. Lessing said in an interview that its followers had written to her and asked, \"When are we going to be visited by the gods?\", and she told them that the book is \"not a cosmology. It's an invention\", and they replied, \"Ah, you're just testing us\".\n\nThe name \"Shikasta\" comes from the Persian word شکسته (\"shekasteh\") meaning \"broken\", and is often seen used as the name of the Iranian national style of Persian calligraphy, Shekasteh Nastaʿlīq. In the book, Lessing does not state explicitly that the planet Shikasta is Earth, but many critics believe that its similarities to Earth's history make it clear that Shikasta \"is\" Earth as seen by the Canopeans. Some of the documents in the book written by Shikastans refer to geographical locations and countries on Earth. Other critics, however, interpret Shikasta as an allegorical Earth with parallel histories that deviate from time to time.\n\n\"Shikasta\" has been called an \"anti-novel\", and an \"architectonic novel\". It is the story of the planet Shikasta from the perspective of Canopus and is presented as a case study for \"first-year students of Canopean Colonial Rule\". It contains a series of reports by Canopean emissaries to the planet, extracts from the Canopean reference, \"History of Shikasta\", and copies of letters and journals written by selected Shikastans. The history of Shikasta is monitored by the virtually immortal Canopeans, from Rohanda's prehistory, through to Shikasta's \"Century of Destruction\" (Earth's 20th century), and into Earth's future when the Chinese occupy Europe and World War III breaks out. The book purports to be the \"true\" history of our planet.\n\n\"Shikasta\" alludes to the Old Testament, Gnosticism and Sufism, and draws on several Judeo-Christian themes. Lessing wrote in the book's preface that it has its roots in the Old Testament. Her SOWF (Substance-Of-We-Feeling), the \"spiritual nourishment\" that flows from Canopus to Shikasta, is also a word she invented with a pronunciation similar to \"Sufi\". A reviewer of the book in the \"Los Angeles Times\" said that \"Shikasta\" is a \"reworking of the Bible\", and the Infinity Plus website draws parallels between the Canopeans and their emissaries, and God and his angels from the Old Testament. A \"New York Times\" reviewer wrote that the \"outer space\" where the Canopeans come from is a metaphor for \"religious or inner space\". Thelma J. Shinn, in her book \"Worlds Within Women: Myth and Mythmaking in Fantastic Literature by Women\", described the struggle between Canopus and Shammat, played out on Shikasta, as the \"eternal struggle between good and evil\", and the \"Degenerative Disease\" that strikes Shikasta as a metaphor for the original sin. Lessing said in an interview that the final war (World War III) at the end of the novel is the Apocalypse. Phyllis Sternberg Perrakis wrote in \"The Journal of Bahá’í Studies\" that \"Shikasta\" is the \"symbolic rendering of the coming of a new prophet to an earthlike planet\", and relates it to Bahá’í principles.\n\nPaul Gray wrote in a review in \"Time\" that the documents that make up \"Shikasta\" allow Lessing to stretch the novel out over vast periods of time and shift perspective \"dramatically from the near infinite to the minute\". He said that the book's cohesiveness is its variety, and noted how Lessing interspaces her \"grand designs\" and \"configurations of enormous powers\" with \"passages of aching poignancy\". Gray said that \"Shikasta\" is closer to \"Gulliver's Travels\" and the Old Testament than it is to Buck Rogers, and may disappoint readers interpreting her \"space fiction\" as \"science fiction\". He found Lessing's bleak vision of Earth's history in which she suggests that humans \"could not ... help making the messes they have, that their blunders were all ordained by a small tic in the cosmos\", a little \"unsatisfying\", but added that even if you do not subscribe to her theories, the book can still be enjoyable, \"even furiously engaging on every page\". Gray called \"Shikasta\" \"an audacious and disturbing work from one of the world's great living writers\".\n\nAuthor Gore Vidal wrote in \"The New York Review of Books\" that \"Shikasta\" is a \"work of a formidable imagination\". He said that Lessing is \"a master\" of eschatological writing, but added that while her depictions of a terminal London are \"very real\", as a whole the book is \"never quite real enough\". Vidal also felt that Zone 6, Lessing's alternate plane for the dead, is not as convincing as The Dry Lands in Ursula K. Le Guin’s \"Earthsea trilogy\". He compared the Canopeans and Shammat to Milton's God and Satan in \"Paradise Lost\", but said that while Lucifer's \"overthrow ... of his writerly creator is an awesome thing\", in \"Shikasta\" Lessing's human race with no free will is too passive and of no interest. Vidal attributed this to Lessing's \"surrender\" to the Sufis and the SOWF (Substance-Of-We-Feeling), and not her inability to create good characters.\n\n\"New York Times\" reviewer George Stade said that \"Shikasta\" \"forces us to think about ... what we are, how we got that way and where we are going\", but complained that the book is filled with \"false hopes\", and that the fate of humankind relies on \"theosophical emanations, cosmic influences, occult powers, spiritual visitations and stellar vibrations\". When the SOWF is cut off and the Shikastans degenerate, Lessing \"both indicts and exculpates\" them, implying that humanity is bad, but it is not their fault. While Stade complemented Lessing on the book's satire, and her depictions of Zone 6, which he said \"have the eerie beauty of ancient Gnostic texts\", he \"disapprove[d]\" of the novel as a whole, but added, \"that doesn't mean I didn't enjoy reading it\".\n\nWiting in the \"Los Angeles Times\". M. G. Lord called \"Shikasta\" an \"epic\" and suspected that it may have influenced the Nobel committee when they referred to Lessing as an \"epicist of the female experience\". Thelma J. Shinn wrote in her book, \"Worlds Within Women: Myth and Mythmaking in Fantastic Literature by Women\", that Lessing's history of humanity in \"Shikasta\" is \"pessimistic\" but \"convincing\". \"Infinity Plus\" described \"Shikasta\" as a \"mainstream novel that uses SF ideas\", and said that while Lessing was not able to predict the fall of the Soviet Union and the impact of computers, the novel \"barely seems dated\" because of her \"cunningly non-specific\" approach.\n\nJames Schellenberg writing in \"Challenging Destiny\", a Canadian science fiction and fantasy magazine, was impressed by \"Shikasta\" \"grand sense of perspective\" and the context of humanity set in a \"vaster scale of civilization and right-thinking\". He liked the concept of SOWF as a \"metaphor of community connectedness\", but felt it was an unusual way to build a utopia. The book's fractured storytelling leads to Lessing breaking the \"famous dictum of writing – show, don't tell\", and while that may work in certain circumstances, Schellenberg felt that that approach does not work very well in \"Shikasta\". The online magazine \"Journey to the Sea\" found Lessing's inclusion of stories from the Hebrew Bible \"entertaining and intriguing\", and said she challenges the logical thinker's rejection of these sacred texts, suggesting that it is \"imaginatively possible\" that they could be true.\n\nFollowing Lessing's death in 2013, \"The Guardian\" put \"Shikasta\" in their list of the top five Lessing books.\n\n\n"}
{"id": "25267122", "url": "https://en.wikipedia.org/wiki?curid=25267122", "title": "Silvano Canzoneri", "text": "Silvano Canzoneri\n\nSilvano Canzoneri (4 February 1941 in Corleone – 5 October 1995 in Venice) was an Italian entomologist.\n\nCanzoneri specialized in Coleoptera, especially Tenebrionidae, Diptera and Ephydridae.\n\n"}
{"id": "34226513", "url": "https://en.wikipedia.org/wiki?curid=34226513", "title": "Spyder (software)", "text": "Spyder (software)\n\nSpyder is an open source cross-platform integrated development environment (IDE) for scientific programming in the Python language. Spyder integrates with a number of prominent packages in the scientific Python stack, including NumPy, SciPy, Matplotlib, pandas, IPython, SymPy and Cython, as well as other open source software. It is released under the MIT license.\n\nInitially created and developed by Pierre Raybaut in 2009, since 2012 Spyder has been maintained and continuously improved by a team of scientific Python developers and the community.\n\nSpyder is extensible with first- and third-party plugins, includes support for interactive tools for data inspection and embeds Python-specific code quality assurance and introspection instruments, such as Pyflakes, Pylint and Rope. It is available cross-platform through Anaconda, on Windows with WinPython and Python (x,y), on macOS through MacPorts, and on major Linux distributions such as Arch Linux, Debian, Fedora, Gentoo Linux, openSUSE and Ubuntu.\n\nSpyder uses Qt for its GUI, and is designed to use either of the PyQt or PySide Python bindings. QtPy, a thin abstraction layer developed by the Spyder project and later adopted by multiple other packages, provides the flexibility to use either backend.\n\nFeatures include: \n\n\nAvailable plugins include:\n\n\n\n"}
{"id": "225617", "url": "https://en.wikipedia.org/wiki?curid=225617", "title": "Third law of thermodynamics", "text": "Third law of thermodynamics\n\nThe third law of thermodynamics is sometimes stated as follows, regarding the properties of closed systems in thermodynamic equilibrium:\n\nThis constant value cannot depend on any other parameters characterizing the closed system, such as pressure or applied magnetic field. At absolute zero (zero kelvin) the system must be in a state with the minimum possible energy. Entropy is related to the number of accessible microstates, and there is typically one unique state (called the ground state) with minimum energy. In such a case, the entropy at absolute zero will be exactly zero. If the system does not have a well-defined order (if its order is glassy, for example), then there may remain some finite entropy as the system is brought to very low temperatures, either because the system becomes locked into a configuration with non-minimal energy or because the minimum energy state is non-unique. The constant value is called the residual entropy of the system. The entropy is essentially a state-function meaning the inherent value of different atoms, molecules, and other configurations of particles including subatomic or atomic material is defined by entropy, which can be discovered near 0 Kelvin.\n\nThe Nernst–Simon statement of the third law of thermodynamics concerns thermodynamic processes at a fixed, low temperature:\n\nHere a condensed system refers to liquids and solids.\n\nA classical formulation by Nernst (actually a consequence of the Third Law) is:\nThere also exists a formulation of the Third Law which approaches the subject by postulating a specific energy behavior:\nThe 3rd law was developed by the chemist Walther Nernst during the years 1906–12, and is therefore often referred to as Nernst's theorem or Nernst's postulate. The third law of thermodynamics states that the entropy of a system at absolute zero is a well-defined constant. This is because a system at zero temperature exists in its ground state, so that its entropy is determined only by the degeneracy of the ground state.\n\nIn 1912 Nernst stated the law thus: \"It is impossible for any procedure to lead to the isotherm in a finite number of steps.\"\n\nAn alternative version of the third law of thermodynamics as stated by Gilbert N. Lewis and Merle Randall in 1923:\n\nThis version states not only Δ\"S\" will reach zero at 0 K, but S itself will also reach zero as long as the crystal has a ground state with only one configuration. Some crystals form defects which cause a residual entropy. This residual entropy disappears when the kinetic barriers to transitioning to one ground state are overcome.\n\nWith the development of statistical mechanics, the third law of thermodynamics (like the other laws) changed from a \"fundamental\" law (justified by experiments) to a \"derived\" law (derived from even more basic laws). The basic law from which it is primarily derived is the statistical-mechanics definition of entropy for a large system:\nwhere \"S\" is entropy, \"k\" is the Boltzmann constant, and formula_2 is the number of microstates consistent with the macroscopic configuration. The counting of states is from the reference state of absolute zero, which corresponds to the entropy of S.\n\nIn simple terms, the third law states that the entropy of a perfect crystal of a pure substance approaches zero as the temperature approaches zero. The alignment of a perfect crystal leaves no ambiguity as to the location and orientation of each part of the crystal. As the energy of the crystal is reduced, the vibrations of the individual atoms are reduced to nothing, and the crystal becomes the same everywhere.\n\nThe third law provides an absolute reference point for the determination of entropy at any other temperature. The entropy of a closed system, determined relative to this zero point, is then the \"absolute\" entropy of that system. Mathematically, the absolute entropy of any system at zero temperature is the natural log of the number of ground states times Boltzmann's constant .\n\nThe entropy of a \"perfect\" crystal lattice as defined by Nernst's theorem is zero provided that its ground state is unique, because . If the system is composed of one-billion atoms, all alike, and lie within the matrix of a perfect crystal, the number of combinations of one-billion identical things taken one-billion at a time is Ω = 1. Hence:\n\nThe difference is zero, hence the initial entropy S can be any selected value so long as all other such calculations include that as the initial entropy. As a result, the initial entropy value of zero is selected is used for convenience.\n\nSuppose a system consisting of a crystal lattice with volume V of N identical atoms at T= 0 K, and an incoming photon of wavelength λ and energy ε. \n\nInitially, there is only one accessible microstate :\n\nformula_6 .\n\nLet's assume the crystal lattice absorbs the incoming photon. There is a unique atom in the lattice that interacts and absorbs this photon. So after absorption, there is N possible microstates accessible by the system, each of the microstates corresponding to one excited atom, and the other atoms remaining at ground state.\n\nThe entropy, energy, and temperature of the closed system rises and can be calculated. The entropy change is:\n\nformula_7\n\nFrom the second law of thermodynamics:\n\nformula_8\n\nHence:\n\nformula_9\n\nCalculating entropy change:\n\nformula_10\n\nWe assume N = 3.10 and λ = 1 cm . The energy change of the system as a result of absorbing the single photon whose energy is ε:\n\nformula_11\n\nThe temperature of the closed system rises by:\n\nformula_12\n\nThis can be interpreted as the average temperature of the system over the range from formula_13. A single atom was assumed to absorb the photon but the temperature and entropy change characterizes the entire system.\n\nAn example of a system which does not have a unique ground state is one whose net spin is a half-integer, for which time-reversal symmetry gives two degenerate ground states. For such systems, the entropy at zero temperature is at least k*ln(2) (which is negligible on a macroscopic scale). Some crystalline systems exhibit geometrical frustration, where the structure of the crystal lattice prevents the emergence of a unique ground state. Ground-state helium (unless under pressure) remains liquid.\n\nIn addition, glasses and solid solutions retain large entropy at 0 K, because they are large collections of nearly degenerate states, in which they become trapped out of equilibrium. Another example of a solid with many nearly-degenerate ground states, trapped out of equilibrium, is ice Ih, which has \"proton disorder\".\n\nFor the entropy at absolute zero to be zero, the magnetic moments of a perfectly ordered crystal must themselves be perfectly ordered; from an entropic perspective, this can be considered to be part of the definition of a \"perfect crystal\". Only ferromagnetic, antiferromagnetic, and diamagnetic materials can satisfy this condition. However, ferromagnetic materials do not, in fact, have zero entropy at zero temperature, because the spins of the unpaired electrons are all aligned and this gives a ground-state spin degeneracy. Materials that remain paramagnetic at 0 K, by contrast, may have many nearly-degenerate ground states (for example, in a spin glass), or may retain dynamic disorder (a quantum spin liquid).\n\nThe third law is equivalent to the statement that\n\nThe reason that \"T\"=0 cannot be reached according to the third law is explained as follows: Suppose that the temperature of a substance can be reduced in an isentropic process by changing the parameter \"X\" from \"X\" to \"X\". One can think of a multistage nuclear demagnetization setup where a magnetic field is switched on and off in a controlled way. If there were an entropy difference at absolute zero, \"T\"=0 could be reached in a finite number of steps. However, at \"T\"=0 there is no entropy difference so an infinite number of steps would be needed. The process is illustrated in Fig.1.\n\nA non-quantitative description of his third law that Nernst gave at the very beginning was simply that the specific heat can always be made zero by cooling the material down far enough. A modern, quantitative analysis follows.\n\nSupposed that the heat capacity of a sample in the low temperature region has the form of a power law \"C\"(\"T,X\")=\"C\"T asymptotically as \"T\"→0, and we wish to find which values of α are compatible with the third law. We have\n\nBy the discussion of third law (above), this integral must be bounded as \"T\"→0, which is only possible if α>0. So the heat capacity must go to zero at absolute zero\n\nif it has the form of a power law. The same argument shows that it cannot be bounded below by a positive constant, even if we drop the power-law assumption.\n\nOn the other hand, the molar specific heat at constant volume of a monatomic classical ideal gas, such as helium at room temperature, is given by \"C\"=(3/2)\"R\" with \"R\" the molar ideal gas constant. But clearly a constant heat capacity does not satisfy Eq. (12). That is, a gas with a constant heat capacity all the way to absolute zero violates the third law of thermodynamics. We can verify this more fundamentally by substituting \"C\" in Eq. (4), which yields\n\nIn the limit this expression diverges, again contradicting the third law of thermodynamics.\n\nThe conflict is resolved as follows: At a certain temperature the quantum nature of matter starts to dominate the behavior. Fermi particles follow Fermi–Dirac statistics and Bose particles follow Bose–Einstein statistics. In both cases the heat capacity at low temperatures is no longer temperature independent, even for ideal gases. For Fermi gases\n\nwith the Fermi temperature \"T\" given by\n\nHere \"N\" is Avogadro's number, \"V\" the molar volume, and \"M\" the molar mass.\n\nFor Bose gases\n\nwith \"T\" given by\n\nThe specific heats given by Eq.(14) and (16) both satisfy Eq.(12). Indeed, they are power laws with α=1 and α=3/2 respectively.\n\nEven within a purely classical setting, the density of a classical ideal gas at fixed particle number becomes arbitrarily high as \"T\" goes to zero, so the interparticle spacing goes to zero. The assumption of non-interacting particles presumably breaks down when they are sufficiently close together, so the value of formula_14 gets modified away from its ideal constant value.\n\nThe only liquids near absolute zero are ³He and ⁴He. Their heat of evaporation has a limiting value given by\n\nwith \"L\" and \"C\" constant. If we consider a container, partly filled with liquid and partly gas, the entropy of the liquid–gas mixture is\nwhere \"S\"(T) is the entropy of the liquid and \"x\" is the gas fraction. Clearly the entropy change during the liquid–gas transition (\"x\" from 0 to 1) diverges in the limit of \"T\"→0. This violates Eq.(8). Nature solves this paradox as follows: at temperatures below about 50 mK the vapor pressure is so low that the gas density is lower than the best vacuum in the universe. In other words: below 50 mK there is simply no gas above the liquid.\n\nThe melting curves of ³He and ⁴He both extend down to absolute zero at finite pressure. At the melting pressure, liquid and solid are in equilibrium. The third law demands that the entropies of the solid and liquid are equal at \"T\"=0. As a result, the latent heat of melting is zero and the slope of the melting curve extrapolates to zero as a result of the Clausius–Clapeyron equation.\n\nThe thermal expansion coefficient is defined as\nWith the Maxwell relation\n\nand Eq.(8) with \"X\"=\"p\" it is shown that\n\nSo the thermal expansion coefficient of all materials must go to zero at zero kelvin.\n\n\n J. Wilks The Third Law of Thermodynamic\n\n"}
{"id": "35375904", "url": "https://en.wikipedia.org/wiki?curid=35375904", "title": "Timeline of developments in theoretical physics", "text": "Timeline of developments in theoretical physics\n\nThis page lists important developments in theoretical physics that have either been experimentally confirmed or significantly influence current thinking in modern physics.\n\n"}
