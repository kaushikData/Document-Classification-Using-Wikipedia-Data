{"id": "50899713", "url": "https://en.wikipedia.org/wiki?curid=50899713", "title": "Antonio Santucci", "text": "Antonio Santucci\n\nAntonio Santucci (?–1613) was an Italian astronomer, cosmographer, and scientific instrument maker.\n\nHe was a reader in Mathematics at the University of Pisa during 1599–1612. Santucci was an astronomer and cosmographer to Grand Duke Ferdinand I (1549–1609) and later Cosimo II (1590–1621). An attentive observer of comets, most notably that of 1582, he published in 1611 the first edition of \"Trattato delle comete\", in which he argued that, contrary to the prevailing scientific opinion, comets were not atmospheric phenomena. The following year, he wrote \"Breve discorso sopra il trattato galileiano sulle galleggianti\" (which survives in manuscript at the National Central Library). He also authored a treatise in 1593, commissioned by Ferdinand I, on the mathematical and surveying instruments in the Guardaroba Medicea collection. His monumental armillary spheres are famous. One sphere, made in 1582 for King Philip II of Spain, is now at the Escorial in Madrid; the other, the most famous Santucci's Armillary Sphere, built in 1588–1593 for the Sala delle Matematiche in the Uffizi, is now at the Museo Galileo of Florence.\n"}
{"id": "2641938", "url": "https://en.wikipedia.org/wiki?curid=2641938", "title": "Applied physics", "text": "Applied physics\n\nApplied physics is intended for a particular technological or practical use. It is usually considered as a bridge or connection between physics and engineering.\n\n\"Applied\" is distinguished from \"pure\" by a subtle combination of factors, such as the motivation and attitude of researchers and the nature of the relationship to the technology or science that may be affected by the work. Applied physics is rooted in the fundamental truths and basic concepts of the physical sciences, but is concerned with the utilization of scientific principles in practical devices and systems, and in the application of physics in other areas of science. \n\nIt usually differs from engineering in that an applied physicist may not be designing something in particular, but rather is using physics or conducting physics research with the aim of developing new technologies or solving an engineering problem. This approach is similar to that of applied mathematics. \n\nIn other words, applied physics is rooted in the fundamental truths and basic concepts of the physical sciences but is concerned with the utilization of these scientific principles in practical devices and systems.\n\nApplied physicists can also be interested in the use of physics for scientific research. For instance, the field of accelerator physics can contribute to research in theoretical physics by working with engineers enabling design and construction of high-energy colliders.\n\n"}
{"id": "14289682", "url": "https://en.wikipedia.org/wiki?curid=14289682", "title": "Armenag K. Bedevian", "text": "Armenag K. Bedevian\n\nArmenag K. Bedevian, Effendi, from Armenian descent, author of \"Illustrated Polyglottic Dictionary of Plant Names\", in Latin, Arabic, Armenian, English, French, German, Italian and Turkish Languages, 1936 (with 1711 illustrations), was Director of Gizeh Research Farm, Egypt.\n\nAccording to W. Lawrence Balls, M.A., Sc.D., F.R.S., in the book's preface, \"Mr. Bedevian joined me as agricultural assistant at the old Cotton Field Laboratory in 1913 and when I returned to Egypt in 1927 I found him fairly established in the Ministry of Agriculture as Manager of the Giza Experimental Farm, and also Superintendent of the Cotton Seed Control operations, I found that he was not only in possession of masses of data about the farm, about the ginneries, merchants, and the like, but that he could produce summaries, abstracts, and graphs of that information at short notice in reply to any question I might ask about such things. Thus it was evident that he had a flair for order and system\".\n\n"}
{"id": "23628595", "url": "https://en.wikipedia.org/wiki?curid=23628595", "title": "Bahtinov mask", "text": "Bahtinov mask\n\nThe Bahtinov mask is a device used to focus small astronomical telescopes accurately.\nAlthough masks have long been used as focusing aids, the distinctive pattern was invented by Russian amateur astrophotographer Pavel Bahtinov () in 2005. Precise focusing of telescopes and astrographs is critical to performing astrophotography.\n\nThe telescope is pointed at a bright star, and a mask is placed in front of the telescope's objective (e.g. primary mirror).\n\nThe mask consists of three separate grids, positioned in such a way that the grids produce three angled diffraction spikes at the focal plane of the instrument for each bright image element. As the instrument's focus is changed, the central spike appears to move from one side of the star to the other. In reality, all three spikes move, but the central spike moves in the opposite direction to the two spikes forming the \"X\". Optimal focus is achieved when the middle spike is centered between the other two spikes.\n\nSmall deviations from optimal focus are easily visible. For astrophotography, a digital image can be analyzed by software to locate the alignment of the spikes to sub-pixel resolution.\n\nThe direction of this displacement indicates the direction of the necessary focus correction. Rotating the mask through 180° will reverse the direction of spike movement, so it is easier to use if placed on the telescope with consistent orientation. The mask must be removed after accurate focus is achieved.\n\nThe mask works by replacing the aperture stop of the optical system (normally the circular shape of the objective itself) with a stop which is asymmetric and periodic. Viewing a point source (such as a star) yields a diffraction pattern at the focal plane representing the Fraunhofer diffraction transform of the aperture shape. This pattern normally would be an Airy disk resulting from a circular aperture, but with the mask in place, the pattern exhibits asymmetric spikes representing the transform of the mask pattern's spatial frequency and orientation. A very bright star and very dark sky are required to produce highly contrasted spikes that are clearly visible. The diffraction effect is similar to producing sunstar patterns in landscape photography with ordinary camera lenses, where the mechanical iris of the lens is adjusted to a small polygonal shape with sharp corners.\n\nIn the example below, the central pattern shows good focus. The central spike is noticeably displaced from the central position in the left and right images.\n\n"}
{"id": "52634", "url": "https://en.wikipedia.org/wiki?curid=52634", "title": "Baking", "text": "Baking\n\nBaking is a method of cooking food that uses prolonged dry heat, normally in an oven, but also in hot ashes, or on hot stones. The most common baked item is bread but many other types of foods are baked. Heat is gradually transferred \"from the surface of cakes, cookies, and breads to their centre. As heat travels through, it transforms batters and doughs into baked goods with a firm dry crust and a softer centre\". Baking can be combined with grilling to produce a hybrid barbecue variant by using both methods simultaneously, or one after the other. Baking is related to barbecuing because the concept of the masonry oven is similar to that of a smoke pit.\n\nBecause of historical social and familial roles, baking has traditionally been performed at home by women for day-to-day meals and by men in bakeries and restaurants for local consumption. When production was industrialized, baking was automated by machines in large factories. The art of baking remains a fundamental skill and is important for nutrition, as baked goods, especially breads, are a common and important food, both from an economic and cultural point of view. A person who prepares baked goods as a profession is called a baker.\n\nAll types of food can be baked, but some require special care and protection from direct heat. Various techniques have been developed to provide this protection.\n\nIn addition to bread, baking is used to prepare cakes, pastries, pies, tarts, quiches, cookies, scones, crackers, pretzels, and more. These popular items are known collectively as \"baked goods,\" and are often sold at a bakery, which is a store that carries only baked goods, or at markets, grocery stores, farmers markets or through other venues.\n\nMeat, including cured meats, such as ham can also be baked, but baking is usually reserved for meatloaf, smaller cuts of whole meats, or whole meats that contain stuffing or coating such as bread crumbs or buttermilk batter. Some foods are surrounded with moisture during baking by placing a small amount of liquid (such as water or broth) in the bottom of a closed pan, and letting it steam up around the food, a method commonly known as braising or slow baking. Larger cuts prepared without stuffing or coating are more often roasted, which is a similar process, using higher temperatures and shorter cooking times. Roasting, however, is only suitable for finer cuts of meat, so other methods have been developed to make tougher meat cuts palatable after baking. One of these is the method known as \"en croûte\" (French for \"in a pastry crust\"), which protects the food from direct heat and seals the natural juices inside. Meat, poultry, game, fish or vegetables can be prepared by baking \"en croûte\". Well-known examples include Beef Wellington, where the beef is encased in pastry before baking; pâté en croûte, where the terrine is encased in pastry before baking; and the Vietnamese variant, a meat-filled pastry called pâté chaud. The \"en croûte\" method also allows meat to be baked by burying it in the embers of a fire – a favorite method of cooking venison. In this case, the protective casing (or crust) is made from a paste of flour and water which is thrown out before eating. Salt can also be used to make a protective crust that is not eaten. Another method of protecting food from the heat while it is baking, is to cook it \"en papillote\" (French for \"in parchment\"). In this method, the food is covered by baking paper (or aluminium foil) to protect it while it is being baked. The cooked parcel of food is sometimes served unopened, allowing diners to discover the contents for themselves which adds an element of surprise.\n\nEggs can also be used in baking to produce savoury or sweet dishes. In combination with dairy products especially cheese, they are often prepared as a dessert. For example, although a baked custard can be made using starch (in the form of flour, cornflour, arrowroot, or potato flour), the flavor of the dish is much more delicate if eggs are used as the thickening agent. Baked custards, such as crème caramel, are among the items that need protection from an oven's direct heat, and the \"bain-marie\" method serves this purpose. The cooking container is half submerged in water in another, larger one, so that the heat in the oven is more gently applied during the baking process. Baking a successful soufflé requires that the baking process be carefully controlled. The oven temperature must be absolutely even and the oven space not shared with another dish. These factors, along with the theatrical effect of an air-filled dessert, have given this baked food a reputation for being a culinary achievement. Similarly, a good baking technique (and a good oven) are also needed to create a baked Alaska because of the difficulty of baking hot meringue and cold ice cream at the same time.\n\nBaking can also be used to prepare various other foods such as pizzas, baked potatoes, baked apples, baked beans, some casseroles and pasta dishes such as lasagne.\n\nThe first evidence of baking occurred when humans took wild grass grains, soaked them in water, and mixed everything together, mashing it into a kind of broth-like paste. The paste was cooked by pouring it onto a flat, hot rock, resulting in a bread-like substance. Later, when humans mastered fire, the paste was roasted on hot embers, which made bread-making easier, as it could now be made any time fire was created. The world's oldest oven was discovered in Croatia in 2014 dating back 6500 years ago. The Ancient Egyptians baked bread using yeast, which they had previously been using to brew beer. Bread baking began in Ancient Greece around 600 BC, leading to the invention of enclosed ovens. \"Ovens and worktables have been discovered in archaeological digs from Turkey (Hacilar) to Palestine (Jericho) and date back to 5600 BC.\"\n\nBaking flourished during the Roman Empire. Beginning around 300 B.C., the pastry cook became an occupation for Romans (known as the pastillarium) and became a respected profession because pastries were considered decadent, and Romans loved festivity and celebration. Thus, pastries were often cooked especially for large banquets, and any pastry cook who could invent new types of tasty treats was highly prized. Around 1 AD, there were more than three hundred pastry chefs in Rome, and Cato wrote about how they created all sorts of diverse foods and flourished professionally and socially because of their creations. Cato speaks of an enormous number of breads including; libum (sacrificial cakes made with flour), placenta (groats and cress), spira (modern day flour pretzels), scibilata (tortes), savaillum (sweet cake), and globus apherica (fritters). A great selection of these, with many different variations, different ingredients, and varied patterns, were often found at banquets and dining halls. The Romans baked bread in an oven with its own chimney, and had mills to grind grain into flour. A bakers' guild was established in 168 B.C. in Rome.\n\nEventually, the Roman art of baking became known throughout Europe and eventually spread to eastern parts of Asia. \nBy the 13th century in London, commercial trading, including baking, had many regulations attached. In the case of food, they were designed to create a system \"so there was little possibility of false measures, adulterated food or shoddy manufactures.\" There were by that time twenty regulations applying to bakers alone, including that every baker had to have \"the impression of his seal\" upon each loaf of bread.\n\nBeginning in the 19th century, alternative leavening agents became more common, such as baking soda. Bakers often baked goods at home and then sold them in the streets. This scene was so common that Rembrandt, among others, painted a pastry chef selling pancakes in the streets of Germany, with children clamoring for a sample. In London, pastry chefs sold their goods from handcarts. This developed into a delivery system of baked goods to households and greatly increased demand as a result. In Paris, the first open-air café of baked goods was developed, and baking became an established art throughout the entire world.\n\nBaking eventually developed into a commercial industry using automated machinery which enabled more goods to be produced for widespread distribution. In the United States, the baking industry \"was built on marketing methods used during feudal times and production techniques developed by the Romans.\" Some makers of snacks such as potato chips or crisps have produced baked versions of their snack products as an alternative to the usual cooking method of deep-frying in an attempt to reduce their calorie or fat content. Baking has opened up doors to businesses such as cake shops and factories where the baking process is done with larger amounts in large, open furnaces.\n\nThe aroma and texture of baked goods as they come out of the oven are strongly appealing but is a quality that is quickly lost. Since the flavour and appeal largely depend on freshness, commercial producers have to compensate by using food additives as well as imaginative labeling. As more and more baked goods are purchased from commercial suppliers, producers try to capture that original appeal by adding the label \"home-baked.\" Such attempts seek to make an emotional link to the remembered freshness of baked goods as well as to attach positive associations the purchaser has with the idea of \"home\" to the bought product. Freshness is such an important quality that restaurants, although they are commercial (and not domestic) preparers of food, bake their own products. For example, scones at The Ritz London Hotel \"are not baked until early afternoon on the day they are to be served, to make sure they are as fresh as possible.\"\n\nBaking needs an enclosed space for heating – typically in an oven. The fuel can be supplied by wood, coal, gas, or electricity. Adding and removing items from an oven may be done by hand with an oven mitt or by a peel, a long handled tool specifically used for that purpose.\n\nMany commercial ovens are equipped with two heating elements: one for baking, using convection and thermal conduction to heat the food, and one for broiling or grilling, heating mainly by radiation. Another piece of equipment still used for baking is the Dutch oven. \"Also called a bake kettle, bastable, bread oven, fire pan, bake oven kail pot, tin kitchen, roasting kitchen, \"doufeu\" (French: \"gentle fire\") or \"feu de compagne\" (French: \"country oven\") [it] originally replaced the cooking jack as the latest fireside cooking technology,\" combining \"the convenience of pot-oven and hangover oven.\" \n\nAsian cultures have adopted steam baskets to produce the effect of baking while reducing the amount of fat needed.\n\nEleven events occur concurrently during baking, some of which (such as starch gelatinization) would not occur at room temperature.\n\n\nThe dry heat of baking changes the form of starches in the food and causes its outer surfaces to brown, giving it an attractive appearance and taste. The browning is caused by caramelization of sugars and the Maillard reaction. Maillard browning occurs when \"sugars break down in the presence of proteins. Because foods contain many different types of sugars and proteins, Maillard browning contributes to the flavour of a wide range of foods, including nuts, roast beef and baked bread.\" The moisture is never entirely \"sealed in\"; over time, an item being baked will become dry. This is often an advantage, especially in situations where drying is the desired outcome, like drying herbs or roasting certain types of vegetables.\n\nThe baking process does not require any fat to be used to cook in an oven. When baking, consideration must be given to the amount of fat that is contained in the food item. Higher levels of fat such as margarine, butter, lard, or vegetable shortening will cause an item to spread out during the baking process.\n\nWith the passage of time, breads harden and become stale. This is not primarily due to moisture being lost from the baked products, but more a reorganization of the way in which the water and starch are associated over time. This process is similar to recrystallization and is promoted by storage at cool temperatures, such as in a domestic refrigerator or freezer.\n\nBaking, especially of bread, holds special significance for many cultures. It is such a fundamental part of everyday food consumption that the children's nursery rhyme \"Pat-a-cake, pat-a-cake, baker's man\" takes baking as its subject. Baked goods are normally served at all kinds of parties and special attention is given to their quality at formal events. They are also one of the main components of a tea party, including at nursery teas and high teas, a tradition which started in Victorian Britain, reportedly when Anna Russell, Duchess of Bedford \"grew tired of the sinking feeling which afflicted her every afternoon round 4 o'clock ... In 1840, she plucked up courage and asked for a tray of tea, bread and butter, and cake to be brought to her room. Once she had formed the habit she found she could not break it, so spread it among her friends instead. As the century progressed, afternoon tea became increasingly elaborate.\"\n\nBenedictine Sisters of the Benedectine Monastery of Caltanissetta producing the crocette, they used to be prepared for the Holy Crucifix festivity. This was situated next to the Church of the Holy Cross, from which the sweets take the name.\n\nFor Jews, Matzo is a baked product of considerable religious and ritual significance. Baked matzah bread can be ground up and used in other dishes, such as Gefilte fish, and baked again. For Christians, bread has to be baked to be used as an essential component of the sacrament of the Eucharist. In the Eastern Christian tradition, baked bread in the form of birds is given to children to carry to the fields in a spring ceremony that celebrates the Forty Martyrs of Sebaste.\n\n\n\n"}
{"id": "46773408", "url": "https://en.wikipedia.org/wiki?curid=46773408", "title": "Center for Community and Economic Development", "text": "Center for Community and Economic Development\n\nThe Center for Community and Economic Development (CCED) is an extension of the University of Wisconsin. Faculty and staff of The University of Wisconsin Extension Center have affiliations at the University of Wisconsin-Superior, University of Wisconsin-Madison and the University of Wisconsin-Extension. The Center was founded by Dr. Ron Shaffer in 1990 shortly after publishing \"Community Economics: Economic Structure and Change in Smaller Communities\". In the preface of that book, Shaffer noted, \"There are many citizens in Wisconsin and county extension community development agents whose interest in community economics continually remind me that universities must help them improve their collective economic conditions.\"\n\nFounding director Ron Shaffer of the Center (Appointed by CNRED Program Leader Ayse Somersan) served the CCED from 1990 - July, 2000.\n\n"}
{"id": "51448511", "url": "https://en.wikipedia.org/wiki?curid=51448511", "title": "Coastal Batholith of Peru", "text": "Coastal Batholith of Peru\n\nThe Coastal Batholith of Peru () is a group of hundreds, if not thousands, of individual plutons that crop out near or at the coast of Peru. The batholith runs a length of ca. 1600 km. Most of the plutons of the batholith were intruded in an elongated coast-parallel extensional basin. The magma that formed the batholith's plutons is thought to have originated from the partial melting of hydrated basaltic rocks at the base of the crust during rifting (extension). Subsequently, the rift basin was inverted. During the ascent the magma followed vertical pathways but emplacement was mostly in the form of tabular bodies.\n\nPlutons of the batholith intrude both the deformed strata of Marañón fold and thrust belt and the Casma Group.\n\n"}
{"id": "3655654", "url": "https://en.wikipedia.org/wiki?curid=3655654", "title": "Concise International Chemical Assessment Document", "text": "Concise International Chemical Assessment Document\n\nConcise International Chemical Assessment Documents (CICADs) are published by the World Health Organization within the framework of the International Programme on Chemical Safety (IPCS). They describe the toxicological properties of chemical compounds.\n\nCICADs are prepared in draft form by one or two experts from national bodies such as the US CDC, and then peer reviewed by an international group of experts. They do not constitute the official policy of any of the bodies which contribute to their publication.\n\n"}
{"id": "41009046", "url": "https://en.wikipedia.org/wiki?curid=41009046", "title": "Darwin drift", "text": "Darwin drift\n\nIn fluid dynamics, Darwin drift refers to the phenomenon that a fluid parcel is permanently displaced after the passage of a body through a fluid – the fluid being at rest far away from the body.\n\nConsider a plane of fluid parcels perpendicular to the direction of the body's constant velocity vector, far before the passage of the body. During the passage of the body the fluid parcels move, according to their Lagrangian motion. Far after the passage of the body, the fluid parcels are permanently displaced. The volume between the initial plane of the fluid parcels and the surface consisting of the parcel positions long after the body's passage is called the Darwin drift volume.\n\nThe phenomenon is named after Sir Charles Galton Darwin, who proved in 1953 that the drift volume multiplied with the fluid density equals the added mass of the body, – known as Darwin's theorem.\n\nAs shown by Eames and McIntyre in 1999, Darwin drift (by the passage of a body through a fluid otherwise at rest) and Stokes drift (in the fluid motion associated with surface waves) are closely related.\n\n"}
{"id": "446216", "url": "https://en.wikipedia.org/wiki?curid=446216", "title": "Decision theory", "text": "Decision theory\n\nDecision Theory (or the theory of choice) is the study of the reasoning underlying an agent's choices. Decision theory can be broken into two branches: normative decision theory, which gives advice on how to make the best decisions, given a set of uncertain beliefs and a set of values; and descriptive decision theory, which analyzes how existing, possibly irrational agents actually make decisions.\n\nClosely related to the field of game theory, decision theory is concerned with the choices of individual agents whereas game theory is concerned with interactions of agents whose decisions affect each other. Decision theory is an interdisciplinary topic, studied by economists,\nstatisticians, psychologists, biologists, political and other social scientists, philosophers, and computer scientists.\n\nEmpirical applications of this rich theory are usually done with the help of statistical and econometric methods, especially via the so-called choice models, such as probit and logit models. Estimation of such models is usually done via parametric, semi-parametric and non-parametric maximum likelihood methods.\n\nNormative decision theory is concerned with identifying the best decisions by considering an ideal decision maker who is able to compute with perfect accuracy and is fully rational. The practical application of this prescriptive approach (how people \"ought to\"make decisions) is called decision analysis, and is aimed at finding tools, methodologies and software (decision support systems) to help people make better decisions.\n\nIn contrast, positiveor descriptive decision theory is concerned with describing observed behaviors under the assumption that the decision-making agents are behaving under some consistent rules. These rules may, for instance, have a procedural framework (e.g. Amos Tversky's elimination by aspects model) or an axiomaticframework, reconciling the Von Neumann-Morgenstern axiomswith behavioral violations of the expected utilityhypothesis, or they may explicitly give a functional form for time-inconsistentutility functions(e.g. Laibson's quasi-hyperbolic discounting).\n\nThe prescriptions or predictions about behaviour that positive decision theory produces allow for further tests of the kind of decision-making that occurs in practice. There is a thriving dialogue with experimental economics, which uses laboratory and field experiments to evaluate and inform theory. In recent decades, there has also been increasing interest in what is sometimes called \"behavioral decision theory\" and this has contributed to a re-evaluation of what rational decision-making requires.\n\nThe area of choice under uncertainty represents the heart of decision theory. Known from the 17th century (Blaise Pascal invoked it in his famous wager, which is contained in his \"Pensées\", published in 1670), the idea of expected value is that, when faced with a number of actions, each of which could give rise to more than one possible outcome with different probabilities, the rational procedure is to identify all possible outcomes, determine their values (positive or negative) and the probabilities that will result from each course of action, and multiply the two to give an \"expected value\", or the average expectation for an outcome; the action to be chosen should be the one that gives rise to the highest total expected value. In 1738, Daniel Bernoulli published an influential paper entitled \"Exposition of a New Theory on the Measurement of Risk\", in which he uses the St. Petersburg paradox to show that expected value theory must be normatively wrong. He gives an example in which a Dutch merchant is trying to decide whether to insure a cargo being sent from Amsterdam to St Petersburg in winter. In his solution, he defines a utility function and computes expected utility rather than expected financial value (see for a review).\n\nIn the 20th century, interest was reignited by Abraham Wald's 1939 paper pointing out that the two central procedures of sampling-distribution-based statistical-theory, namely hypothesis testing and parameter estimation, are special cases of the general decision problem. Wald's paper renewed and synthesized many concepts of statistical theory, including loss functions, risk functions, admissible decision rules, antecedent distributions, Bayesian procedures, and minimax procedures. The phrase \"decision theory\" itself was used in 1950 by E. L. Lehmann.\n\nThe revival of subjective probability theory, from the work of Frank Ramsey, Bruno de Finetti, Leonard Savage and others, extended the scope of expected utility theory to situations where subjective probabilities can be used. At the time, von Neumann and Morgenstern’s theory of expected utility proved that expected utility maximization followed from basic postulates about rational behavior.\n\nThe work of Maurice Allais and Daniel Ellsberg showed that human behavior has systematic and sometimes important departures from expected-utility maximization. The prospect theory of Daniel Kahneman and Amos Tversky renewed the empirical study of economic behavior with less emphasis on rationality presuppositions. Kahneman and Tversky found three regularities – in actual human decision-making, \"losses loom larger than gains\"; persons focus more on \"changes\" in their utility-states than they focus on absolute utilities; and the estimation of subjective probabilities is severely biased by anchoring.\n\nIntertemporal choice is concerned with the kind of choice where different actions lead to outcomes that are realised at different points in time. If someone received a windfall of several thousand dollars, they could spend it on an expensive holiday, giving them immediate pleasure, or they could invest it in a pension scheme, giving them an income at some time in the future. What is the optimal thing to do? The answer depends partly on factors such as the expected rates of interest and inflation, the person's life expectancy, and their confidence in the pensions industry. However even with all those factors taken into account, human behavior again deviates greatly from the predictions of prescriptive decision theory, leading to alternative models in which, for example, objective interest rates are replaced by subjective discount rates.\n\nSome decisions are difficult because of the need to take into account how other people in the situation will respond to the decision that is taken. The analysis of such social decisions is more often treated under the label of game theory, rather than decision theory, though it involves the same mathematical methods. From the standpoint of game theory most of the problems treated in decision theory are one-player games (or the one player is viewed as playing against an impersonal background situation). In the emerging field of socio-cognitive engineering, the research is especially focused on the different types of distributed decision-making in human organizations, in normal and abnormal/emergency/crisis situations.\n\nOther areas of decision theory are concerned with decisions that are difficult simply because of their complexity, or the complexity of the organization that has to make them. Individuals making decisions may be limited in resources or are boundedly rational (have finite time or intelligence); in such cases the issue, more than the deviation between real and optimal behaviour, is the difficulty of determining the optimal behaviour in the first place. One example is the model of economic growth and resource usage developed by the Club of Rome to help politicians make real-life decisions in complex situations. Decisions are also affected by whether options are framed together or separately; this is known as the distinction bias. In 2011, Dwayne Rosenburgh explored and showed how decision theory can be applied to complex decisions that arise in areas such as wireless communications.\n\nHeuristicsin decision-making is the ability of making decisions based on unjustified or routine thinking. While quicker than step-by-step processing, heuristic thinking is also more likely to experience fallacies or inaccuracies.The main use for heuristics in our daily routines is to decrease the amount of evaluative thinking we perform when making simple decisions, and instead make them based on unconscious rules and focusing on some aspects of the decision, while ignoring others.One example of common and erroneous thought process that arrises through heuristic thinking is the Gambler's Fallacy. Believing that a isolated random event is affected by previous isolated random events, for example if a coin is flipped to tails for a couple of turns it still carries the same probability of doing so, however intuitively it sounds more likely for it to roll heads soon.This happens because, due to routine thinking, one disregards the probability and concentrate on the ratio of the outcome, meaning, in the long run the ratio of flips should be half for each outcome.Another example is that decision-makers may be biased towards preferring moderate alternatives to extreme ones; the \"Compromise Effect\" operates under a mindset that the most moderate option carries the most benefit. In an incomplete information scenario, like in most daily decisions, the moderate option will look more appealing than either extreme independent of the context, based only on the fact that it gathers characteristics that can be found in either extremes.\n\nA highly controversial issue is whether one can replace the use of probability in decision theory by other alternatives.\n\nAdvocates for the use of probability theory point to:\n\nThe proponents of fuzzy logic, possibility theory, quantum cognition, Dempster–Shafer theory, and info-gap decision theory maintain that probability is only one of many alternatives and point to many examples where non-standard alternatives have been implemented with apparent success; notably, probabilistic decision theory is sensitive to assumptions about the probabilities of various events, while non-probabilistic rules such as minimax are robust, in that they do not make such assumptions.\n\nA general criticism of decision theory based on a fixed universe of possibilities is that it considers the \"known unknowns\", not the \"unknown unknowns\": it focuses on expected variations, not on unforeseen events, which some argue (as in black swan theory) have outsized impact and must be considered – significant events may be \"outside model\". This line of argument, called the ludic fallacy, is that there are inevitable imperfections in modeling the real world by particular models, and that unquestioning reliance on models blinds one to their limits.\n\n"}
{"id": "5464618", "url": "https://en.wikipedia.org/wiki?curid=5464618", "title": "Die Stadtkrone", "text": "Die Stadtkrone\n\nDie Stadtkrone or City Crown is a concept of Urban planning put forward by German expressionist architects, and particularly championed by Bruno Taut in the early part of the 20th century. It was often conceived as an inspirational, crystalline form or something with a homogenous formal vocabulary in the centre of a town, with huge impressive scale, analogous to, but not necessarily Skyscrapers. The physical form were notions of social restructuring with subordination of individuals to the collective good and sometimes ideas of a return to an agrarian existence.\n"}
{"id": "1825382", "url": "https://en.wikipedia.org/wiki?curid=1825382", "title": "Edouard Van Beneden", "text": "Edouard Van Beneden\n\nÉdouard Joseph Louis Marie Van Beneden (5 March 1846 in Leuven – 28 April 1910 in Liège), son of Pierre-Joseph Van Beneden, was a Belgian embryologist, cytologist and marine biologist. He was professor of zoology at the University of Liège. He contributed to cytogenetics by his works on the roundworm \"Ascaris\". In this work he discovered how chromosomes organized meiosis (the production of gametes).\n\nVan Beneden elucidated, together with Walther Flemming and Eduard Strasburger, the essential facts of mitosis, where, in contrast to meiosis, there is a qualitative and quantitative equality of chromosome distribution to daughter cells. (See karyotype).\n\n\nVan Beneden's father, Pierre-Joseph van Beneden (18091894) was also a well-known biologist. He introduced two important terms into evolutionary biology and ecology: mutualism and commensalism.\n\n"}
{"id": "1769811", "url": "https://en.wikipedia.org/wiki?curid=1769811", "title": "Edvard Rusjan", "text": "Edvard Rusjan\n\nEdvard Rusjan (6 June 1886 – 9 January 1911) was a Slovenian flight pioneer and airplane constructor. He died in an airplane crash in Belgrade.\n\nRusjan was born in Trieste, then the major port of Austria-Hungary (now in Italy). His parents were both natives of the Gorizia and Gradisca region: his father, Franc Rusjan, was a Slovene, and his mother, Grazia Cabas, was Friulan. Rusjan spent his childhood and adolescence in Gorizia, in the suburb of Rafut.\n\nIn his youth, he became a professional bicyclist, and designed his own bicycle models, together with his brother Josip Rusjan. He was also member of the Sokol gymnastic association.\n\nHe made his first flight on 25 November 1909, near Gorizia, in Eda I, a biplane of his own design. Eda was Edvard's nickname, given from his mother.\n\nThe flight was covered and reached a height of . On 29 November 1909, he flew at an altitude of . The original design was followed by several improved versions. He moved to a hangar near Miren south of Gorizia.\n\nRusjan first attended a public flight event with the model EDA V on 6 December 1909, when his aircraft broke apart at landing. In June 1910, he tried the model EDA V, which enabled him to fly 40 meters above the ground and overfly the whole Miren Field.\n\nThe brothers Rusjan ran out of finance for the construction of EDA VII. In 1910 Edvard Rusjan met the businessman of Serbian origin Mihajlo Merćep, who offered him financial help for his endeavors. The same year, the brothers moved to Zagreb, Croatia, when they started a project of airplane construction on larger scale. In November 1910, they constructed a new model.\n\nIn January 1911, Edvard and Josip Rusjan went on a promotional tour through the Balkan cities. During a flight in Belgrade, Serbia on 9 January 1911 a strong wind broke a wing of Edvard's aeroplane and it dived into a railway embankment near the Belgrade Fortress in a fatal crash. His funeral was attended by a large crowd of about 14,000 people.\n\nHe is interred at Belgrade's New Cemetery, lot 15, grave 343.\n\nThe Maribor Edvard Rusjan Airport and asteroid 19633 Rusjan are named after him. The commercial and business center \"Eda Center\" in Nova Gorica is dedicated to Rusjan's memory.\n\nOne of the DC-10 in the JAT fleet was named after Edvard Rusjan.\n\n"}
{"id": "57407671", "url": "https://en.wikipedia.org/wiki?curid=57407671", "title": "Frances Elizabeth Potter", "text": "Frances Elizabeth Potter\n\nFrances Elizabeth Potter of Kibworth Beauchamp was the first woman to qualify as a pharmacist in the United Kingdom after the passing of the Pharmacy Act 1868.\n\nShe qualified on 5 February 1869 by taking the ‘modified exam’ and was placed on the first compulsory Register of Chemists and Druggists and Pharmaceutical Chemists in 1870. It was not realised until she appeared to sit the exam that she was a woman. From 1875 until the early 1900s, using her married name of Deacon, she owned a pharmacy in Fleckney, near Market Harborough. She was not, however, able to become a member of the Royal Pharmaceutical Society as women were not admitted until 1879.\n\nShe was not the first woman to be registered, as there were 223 women (of the 11638 registered chemists and druggists) who were already operating pharmacies, typically because they had inherited the business from a father or husband. \n"}
{"id": "2628508", "url": "https://en.wikipedia.org/wiki?curid=2628508", "title": "Generative actor", "text": "Generative actor\n\nA generative actor is an instigator of social change. He or she promotes cultural change by defying cultural normatives. Noted examples include Galileo and Rosa Parks.\n\n"}
{"id": "11808249", "url": "https://en.wikipedia.org/wiki?curid=11808249", "title": "Genome-wide association study", "text": "Genome-wide association study\n\nIn genetics, a genome-wide association study (GWA study, or GWAS), also known as whole genome association study (WGA study, or WGAS), is an observational study of a genome-wide set of genetic variants in different individuals to see if any variant is associated with a trait. GWASs typically focus on associations between single-nucleotide polymorphisms (SNPs) and traits like major human diseases, but can equally be applied to any other genetic variants and any other organisms.\n\nWhen applied to human data, GWA studies compare the DNA of participants having varying phenotypes for a particular trait or disease. These participants may be people with a disease (cases) and similar people without the disease (controls), or they may be people with different phenotypes for a particular trait, for example blood pressure. This approach is known as phenotype-first, in which the participants are classified first by their clinical manifestation(s), as opposed to genotype-first. Each person gives a sample of DNA, from which millions of genetic variants are read using SNP arrays. If one type of the variant (one allele) is more frequent in people with the disease, the variant is said to be \"associated\" with the disease. The associated SNPs are then considered to mark a region of the human genome that may influence the risk of disease.\n\nGWA studies investigate the entire genome, in contrast to methods that specifically test a small number of pre-specified genetic regions. Hence, GWAS is a \"non-candidate-driven\" approach, in contrast to \"gene-specific candidate-driven studies\". GWA studies identify SNPs and other variants in DNA associated with a disease, but they cannot on their own specify which genes are causal.\n\nThe first successful GWAS was published in 2002 studying myocardial infarction. This design study was then implemented in the landmark GWA 2005 study investigating patients with age-related macular degeneration and found two SNPs with significantly altered allele frequency compared to healthy controls. , hundreds or thousands of individuals are tested in a typical GWA study. Over 3,000 human GWA studies have examined over 1,800 diseases and traits, and thousands of SNP associations have been found.\n\nAny two human genomes differ in millions of different ways. There are small variations in the individual nucleotides of the genomes (SNPs) as well as many larger variations, such as deletions, insertions and copy number variations. Any of these may cause alterations in an individual's traits, or phenotype, which can be anything from disease risk to physical properties such as height. Around the year 2000, prior to the introduction of GWA studies, the primary method of investigation was through inheritance studies of genetic linkage in families. This approach had proven highly useful towards single gene disorders. However, for common and complex diseases the results of genetic linkage studies proved hard to reproduce. A suggested alternative to linkage studies was the genetic association study. This study type asks if the allele of a genetic variant is found more often than expected in individuals with the phenotype of interest (e.g. with the disease being studied). Early calculations on statistical power indicated that this approach could be better than linkage studies at detecting weak genetic effects.\n\nIn addition to the conceptual framework several additional factors enabled the GWA studies. One was the advent of biobanks, which are repositories of human genetic material that greatly reduced the cost and difficulty of collecting sufficient numbers of biological specimens for study. Another was the International HapMap Project, which, from 2003 identified a majority of the common SNPs interrogated in a GWA study. The haploblock structure identified by HapMap project also allowed the focus on the subset of SNPs that would describe most of the variation. Also the development of the methods to genotype all these SNPs using genotyping arrays was an important prerequisite.\n\nThe most common approach of GWA studies is the case-control setup, which compares two large groups of individuals, one healthy control group and one case group affected by a disease. All individuals in each group are genotyped for the majority of common known SNPs. The exact number of SNPs depends on the genotyping technology, but are typically one million or more. For each of these SNPs it is then investigated if the allele frequency is significantly altered between the case and the control group. In such setups, the fundamental unit for reporting effect sizes is the odds ratio. The odds ratio is the ratio of two odds, which in the context of GWA studies are the odds of disease for individuals having a specific allele and the odds of disease for individuals who do not have that same allele. When the allele frequency in the case group is much higher than in the control group, the odds ratio is higher than 1, and vice versa for lower allele frequency. Additionally, a P-value for the significance of the odds ratio is typically calculated using a simple chi-squared test. Finding odds ratios that are significantly different from 1 is the objective of the GWA study because this shows that a SNP is associated with disease.\n\nThere are several variations to this case-control approach. A common alternative to case-control GWA studies is the analysis of quantitative phenotypic data, e.g. height or biomarker concentrations or even gene expression. Likewise, alternative statistics designed for dominance or recessive penetrance patterns can be used. Calculations are typically done using bioinformatics software such as SNPTEST and PLINK, which also include support for many of these alternative statistics. Earlier GWAS focused on the effect of individual SNPs. However, the empirical evidence shows that complex interactions among two or more SNPs, epistasis, might contribute to complex diseases. Moreover, the researchers try to integrate GWA data with other biological data such as protein protein interaction network to extract more informative results.\n\nA key step in the majority of GWA studies is the imputation of genotypes at SNPs not on the genotype chip used in the study. This process greatly increases the number of SNPs that can be tested for association, increases the power of the study, and facilitates meta-analysis of GWAS across distinct cohorts. Genotype imputation is carried out by statistical methods that combine the GWAS data together with a reference panel of haplotypes. These methods take advantage of sharing of haplotypes between individuals over short stretches of sequence to impute alleles. Existing software packages for genotype imputation include IMPUTE2 and MaCH.\n\nIn addition to the calculation of association, it is common to take into account any variables that could potentially confound the results. Sex and age are common examples of confounding variables. Moreover, it is also known that many genetic variations are associated with the geographical and historical populations in which the mutations first arose. Because of this association, studies must take account of the geographic and ethnic background of participants by controlling for what is called population stratification. If they fail to do so, these studies can produce false positive results.\n\nAfter odds ratios and P-values have been calculated for all SNPs, a common approach is to create a Manhattan plot. In the context of GWA studies, this plot shows the negative logarithm of the P-value as a function of genomic location. Thus the SNPs with the most significant association stand out on the plot, usually as stacks of points because of haploblock structure. Importantly, the P-value threshold for significance is corrected for multiple testing issues. The exact threshold varies by study, but the conventional threshold is to be significant in the face of hundreds of thousands to millions of tested SNPs. GWA studies typically perform the first analysis in a discovery cohort, followed by validation of the most significant SNPs in an independent validation cohort.\n\nAttempts have been made at creating comprehensive catalogues of SNPs that have been identified from GWA studies. As of 2009, SNPs associated with diseases are numbered in the thousands.\n\nThe first GWA study, conducted in 2005, compared 96 patients with age-related macular degeneration (ARMD) with 50 healthy controls. It identified two SNPs with significantly altered allele frequency between the two groups. These SNPs were located in the gene encoding complement factor H, which was an unexpected finding in the research of ARMD. The findings from these first GWA studies have subsequently prompted further functional research towards therapeutical manipulation of the complement system in ARMD. Another landmark publication in the history of GWA studies was the Wellcome Trust Case Control Consortium (WTCCC) study, the largest GWA study ever conducted at the time of its publication in 2007. The WTCCC included 14,000 cases of seven common diseases (~2,000 individuals for each of coronary heart disease, type 1 diabetes, type 2 diabetes, rheumatoid arthritis, Crohn's disease, bipolar disorder, and hypertension) and 3,000 shared controls. This study was successful in uncovering many new disease genes underlying these diseases.\n\nSince these first landmark GWA studies, there have been two general trends. One has been towards larger and larger sample sizes. In 2018, several genome-wide association studies are reaching a total sample size of over 1 million participants, including 1.1 million in a genome-wide study of educational attainment and a study of insomnia containing 1.3 million individuals. The reason is the drive towards reliably detecting risk-SNPs that have smaller odds ratios and lower allele frequency. Another trend has been towards the use of more narrowly defined phenotypes, such as blood lipids, proinsulin or similar biomarkers. These are called \"intermediate phenotypes\", and their analyses may be of value to functional research into biomarkers. A variation of GWAS uses participants that are first-degree \"relatives\" of people with a disease. This type of study has been named genome-wide association study by proxy (\"GWAX\").\n\nA central point of debate on GWA studies has been that most of the SNP variations found by GWA studies are associated with only a small increased risk of the disease, and have only a small predictive value. The median odds ratio is 1.33 per risk-SNP, with only a few showing odds ratios above 3.0. These magnitudes are considered small because they do not explain much of the heritable variation. This heritable variation is known from heritability studies based on monozygotic twins. For example, it is known that 80-90% of variance in height can be explained by hereditary differences, but GWA studies only account for a minority of this variance.\n\nA challenge for future successful GWA study is to apply the findings in a way that accelerates drug and diagnostics development, including better integration of genetic studies into the drug-development process and a focus on the role of genetic variation in maintaining health as a blueprint for designing new drugs and diagnostics. Several studies have looked into the use of risk-SNP markers as a means of directly improving the accuracy of prognosis. Some have found that the accuracy of prognosis improves, while others report only minor benefits from this use. Generally, a problem with this direct approach is the small magnitudes of the effects observed. A small effect ultimately translates into a poor separation of cases and controls and thus only a small improvement of prognosis accuracy. An alternative application is therefore the potential for GWA studies to elucidate pathophysiology.\n\nOne such success is related to identifying the genetic variant associated with response to anti-hepatitis C virus treatment. For genotype 1 hepatitis C treated with Pegylated interferon-alpha-2a or Pegylated interferon-alpha-2b combined with ribavirin, a GWA study has shown that SNPs near the human IL28B gene, encoding interferon lambda 3, are associated with significant differences in response to the treatment. A later report demonstrated that the same genetic variants are also associated with the natural clearance of the genotype 1 hepatitis C virus. These major findings facilitated the development of personalized medicine and allowed physicians to customize medical decisions based on the patient's genotype.\n\nThe goal of elucidating pathophysiology has also led to increased interest in the association between risk-SNPs and the gene expression of nearby genes, the so-called expression quantitative trait loci (eQTL) studies. The reason is that GWAS studies identify risk-SNPs, but not risk-genes, and specification of genes is one step closer towards actionable drug targets. As a result, major GWA studies by 2011 typically included extensive eQTL analysis. One of the strongest eQTL effects observed for a GWA-identified risk SNP is the SORT1 locus. Functional follow up studies of this locus using small interfering RNA and gene knock-out mice have shed light on the metabolism of low-density lipoproteins, which have important clinical implications for cardiovascular disease.\n\nGWA studies have several issues and limitations that can be taken care of through proper quality control and study setup. Lack of well defined case and control groups, insufficient sample size, control for multiple testing and control for population stratification are common problems. Particularly the statistical issue of multiple testing wherein it has been noted that \"the GWA approach can be problematic because the massive number of statistical tests performed presents an unprecedented potential for false-positive results\". Ignoring these correctible issues has been cited as contributing to a general sense of problems with the GWA methodology. In addition to easily correctible problems such as these, some more subtle but important issues have surfaced. A high-profile GWA study that investigated individuals with very long life spans to identify SNPs associated with longevity is an example of this. The publication came under scrutiny because of a discrepancy between the type of genotyping array in the case and control group, which caused several SNPs to be falsely highlighted as associated with longevity. The study was subsequently retracted, but a modified manuscript was later published.\n\nIn addition to these preventable issues, GWA studies have attracted more fundamental criticism, mainly because of their assumption that common genetic variation plays a large role in explaining the heritable variation of common disease. This aspect of GWA studies has attracted the criticism that, although it could not have been known prospectively, GWA studies were ultimately not worth the expenditure. Alternative strategies suggested involve linkage analysis. More recently, the rapidly decreasing price of complete genome sequencing have also provided a realistic alternative to genotyping array-based GWA studies. It can be discussed if the use of this new technique is still referred to as a GWA study, but high-throughput sequencing does have potential to side-step some of the shortcomings of non-sequencing GWA.\n\nGenotyping arrays designed for GWAS rely on linkage disequilibrium to provide coverage of the entire genome by genotyping a subset of variants. Because of this, the reported associated variants are unlikely to be the actual causal variants. Associated regions can contain hundreds of variants spanning large regions and encompassing many different genes, making the biological interpretation of GWAS loci more difficult. Fine-mapping is a process to refine these lists of associated variants to a credible set most likely to include the causal variant.\n\nFine-mapping requires all variants in the associated region to have been genotyped or imputed (dense coverage), very stringent quality control resulting in high-quality genotypes, and large sample sizes sufficient in separating out highly correlated signals. There are several different methods to perform fine-mapping, and all methods produce a posterior probability that a variant in that locus is causal. Because the requirements are often difficult to satisfy, there are still limited examples of these methods being more generally applied.\n\n\n"}
{"id": "38448353", "url": "https://en.wikipedia.org/wiki?curid=38448353", "title": "Index of physics articles (J)", "text": "Index of physics articles (J)\n\nThe index of physics articles is split into multiple pages due to its size.\n\nTo navigate by individual letter use the table of contents below.\n\n"}
{"id": "20806428", "url": "https://en.wikipedia.org/wiki?curid=20806428", "title": "Institute for Science and Society", "text": "Institute for Science and Society\n\nThe Institute for Science and Society (ISS) was an international centre of excellence in Science and Technology Studies located at the University of Nottingham, UK.\n\nIt was founded in 1998 as the Genetics and Society Unit (GSU) and was later (2001) renamed the Institute for the Study of Genetics, Biorisks and Society (IGBiS) before its remit was expanded in 2006 to cover the social, legal, ethical and cultural implications of any field of science, medicine or technology, at which point it became ISS. The Institute was restructured in 2010 and mostly merged into the School of Sociology and Social Policy.\n\nThe Institute for Science and Society was a multidisciplinary centre drawing on a wide range of social sciences and humanities disciplines to conduct research into cutting edge aspects of the mutual relationship between science and society. Historically, the Institute's agenda was dominated by topics in the bio-sciences, broadly conceived, but it became increasingly involved in partnerships around physical science issues like nanotechnology, sustainable energy use and climate change. \n\nThe Institute is now physically part of the School of Sociology and Social Policy at the University of Nottingham, but works virtually across the University under the University's Priority Group scheme as the Science and Technology Studies Priority Group.\n\nSince its foundation, the Institute has been variously supported by grants and contracts from the Leverhulme and Wellcome Trusts, the Economic and Social Research Council (ESRC), the Natural Environment Research Council (NERC), the Biotechnology and Biological Sciences Research Council (BBSRC), the Medical Research Council (MRC), The Engineering and Physical Sciences Research Council (EPSRC), the European Union and the National Health Service (NHS).\n\nDr. Sujatha Raman and Professor Brigitte Nerlich are the current co-directors of the Institute.\n\n\n"}
{"id": "13451384", "url": "https://en.wikipedia.org/wiki?curid=13451384", "title": "John S. Hougham", "text": "John S. Hougham\n\nJohn Scherer Hougham (28 May 1821 – 31 March 1894), was Purdue University’s first appointed professor, first (unofficial) acting President (March 11, 1874 – June 11, 1874) after Purdue's first President Richard Dale Owen resigned on March 1, 1874, and later an official acting President (November 6, 1875 – April 30, 1876) between the administrations of Abraham C. Shortridge and Emerson E. White.\n\nHougham first graduated from Wabash College, Crawfordsville, Indiana, in 1846, where he was a member of Beta Theta Pi fraternity. He then rose to Professor of Mathematics and Natural Philosophy at Franklin College in Franklin, Indiana (1848 – 1867). During this time he was also a well regarded maker of scientific instruments (see images, right) for educational and professional use in medicine, chemistry, astronomy, and other related fields (e.g., Solar Compass). \"Hougham Street\" in Franklin, IN, adjacent to the Franklin College campus, was named to honor one of the city's most illustrious residents.\n\nAfter those appointments, he was Chairman of Philosophy and Agriculture at Kansas State University (1868 – 1872). He then took an appointment as Professor of Physics and Industrial Mechanics, and Chairman of Agricultural Chemistry at Purdue University (1872 – 1876), serving in those early years of Purdue’s history as an academic \"handyman\" — and for a time acting President for parts of 1874 and 1876 — to John Purdue and the founding Trustees, visiting other universities around the country in search of new ideas and faculty to bring back to his native Indiana. Around 1876, he returned to Kansas State where he spent the remaining years of his academic career.\n\n"}
{"id": "32284850", "url": "https://en.wikipedia.org/wiki?curid=32284850", "title": "KIs-V", "text": "KIs-V\n\nKIs-V is a DNA virus isolated from four human cases of acute hepatitis in Japan. This virus has also been isolated in France.\n\nThe genome has a sequence of 9496 bases and 13 potential genes. The virus is 30–50 nanometers in diameter and is enveloped.\n"}
{"id": "26078035", "url": "https://en.wikipedia.org/wiki?curid=26078035", "title": "Language and Communication Technologies", "text": "Language and Communication Technologies\n\nLanguage and Communication Technologies (LCT; also known as human language technologies or language technology for short) is the scientific study of technologies that explore language and communication. It is an interdisciplinary field that encompasses the fields of computer science, linguistics and cognitive science.\n\nOne of the first problems to be studied in the 1950s, shortly after the invention of computers, was an LCT problem, namely the translation of human languages. The large amounts of funding poured into machine translation testifies to the perceived importance of the field, right from the beginning. It was also in this period that scholars started to develop theories of language and communication based on scientific methods. In the case of language, it was Noam Chomsky who refines the goal of linguistics as a quest for a formal description of language, whilst Claude Shannon and Warren Weaver provided a mathematical theory that linked communication with information.\n\nComputers and related technologies have provided a physical and conceptual framework within which scientific studies concerning the notion of communication within a computational framework could be pursued. Indeed, this framework has been fruitful on a number of levels. For a start, it has given birth to a new discipline, known as natural language processing (NLP), or computational linguistics (CL). This discipline studies, from a computational perspective, all levels of language from the production of speech to the meanings of texts and dialogues. And over the past 40 years, NLP has produced an impressive computational infrastructure of resources, techniques, and tools for analyzing sound structure (phonology), word structure (morphology), grammatical structure (syntax) and meaning structure (semantics). As well as being important for language-based applications, this computational infrastructure makes it possible to investigate the structure of human language and communication at a deeper scientific level than was ever previously possible.\n\nMoreover, NLP fits in naturally with other branches of computer science, and in particular, with artificial intelligence (AI). From an AI perspective, language use is regarded as a manifestation of intelligent behaviour by an active agent. The emphasis in AI-based approaches to language and communication is on the computational infrastructure required to integrate linguistic performance into a general theory of intelligent agents that includes, for example, learning generalizations on the basis of particular experience, the ability to plan and reason about intentionally produced utterances, the design of utterances that will fulfill a particular set of goals. Such work tends to be highly interdisciplinary in nature, as it needs to draw on ideas from such fields as linguistics, cognitive psychology, and sociology. LCT draws on and incorporates knowledge and research from all these fields.\n\nLanguage and communication are so fundamental to human activity that it is not at all surprising to find that Language and Communication Technologies affect all major areas of society, including health, education, finance, commerce, and travel. Modern LCT is based on a dual tradition of symbols and statistics. This means that nowadays research on language requires access to large databases of information about words and their properties, to large scale computational grammars, to computational tools for working with all levels of language, and to efficient inference systems for performing reasoning. By working computationally it is possible to get to grips with the deeper structure of natural languages, and in particular, to model the crucial interactions between the various levels of language and other cognitive faculties.\n\nRelevant areas of research in LCT include:\n\n\nThe increasing interest in the field is proved by the existence of several European Masters in this dynamic research area: Degree programmes of the University of Groningen include Language and Communication Technologies. \n\nErasmus Mundus Masters:\n\n"}
{"id": "4971169", "url": "https://en.wikipedia.org/wiki?curid=4971169", "title": "List of Ace SF double titles", "text": "List of Ace SF double titles\n\nAce Books published 221 science fiction Ace doubles between 1952 and 1973 in tête-bêche format, and a further 40 between 1974 and 1978 in a more traditional format in which the two books are both the same way up.\n\nAce published science fiction, mysteries, and westerns, as well as books not in any of these genres. Collectors of these genres have found the Ace doubles an attractive set of books to collect, because of the unusual appearance of the tête-bêche format. This is particularly true for the science fiction books, for which several bibliographic references have been written (see the References section). The format inspired a further series of sf doubles published by Tor Books between 1988 and 1991, the Tor Double Novels.\n\nBecause the tête-bêche format is part of the attraction for collectors, titles published between 1974 and 1978, which contained two titles by one or two authors but which are not tête-bêche are not regarded by some collectors as true Ace Doubles. The distinction is up to each collector; the books are included in the list given below, with the difference in format noted.\n\nThe list given here gives a date of publication; in all cases this refers to the date of publication by Ace, and not the date of original publication of the novels. The list is complete for science fiction titles. However, D-13, listed in Miscellaneous Ace Doubles, contains one novel, \"Cry Plague!\" by Theodore S. Drachman, which can be regarded as sf, and some science fiction collectors treat this as the first sf Ace Double, even though the novel on the other side is not sf in any way.\n\nFor more information about the history of these titles, see Ace Books, which includes a discussion of the serial numbering conventions used and an explanation of the letter-code system.\n\n\n\n\n\n\n\nSerial number 48245, above, was the last Ace Double published in the tête-bêche format. The remainder of the books listed in this section contain two novels, but are published in the traditional way with a single cover and the text the same way up throughout the book. Another Ace SF double(G-723-Andre Norton-Star Hunter/Voodoo Planet)was published in 1968 as a traditional reprint in the second G series(begun in 1964). G-723 was the only traditionally printed SF double included in that series and thus does not fit under any of the categories listed here.\n\nThe remaining novels do not have complete date information, so they are sorted in numerical order of the serial number. Note that the following are also not in tête-bêche format.\n\n\nThe following references have not been seen but cover the Ace Doubles:\n"}
{"id": "29230973", "url": "https://en.wikipedia.org/wiki?curid=29230973", "title": "List of Russian astronomers and astrophysicists", "text": "List of Russian astronomers and astrophysicists\n\nThis list of Russian astronomers and astrophysicists includes the famous astronomers, astrophysicists and cosmologists from the Russian Empire, the Soviet Union and the Russian Federation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1800613", "url": "https://en.wikipedia.org/wiki?curid=1800613", "title": "List of Unix daemons", "text": "List of Unix daemons\n\nThis is a list of Unix daemons that are found on various Unix-like operating systems. Unix daemons typically have a name ending with a \"d\".\n"}
{"id": "2680698", "url": "https://en.wikipedia.org/wiki?curid=2680698", "title": "List of compounds with carbon number 8", "text": "List of compounds with carbon number 8\n\nThis is a partial list of molecules that contain 8 carbon atoms.\n\n"}
{"id": "89844", "url": "https://en.wikipedia.org/wiki?curid=89844", "title": "List of environmental issues", "text": "List of environmental issues\n\nThis is an alphabetical list of environmental issues, harmful aspects of human activity on the biophysical environment. They are loosely divided into causes, effects and mitigation, noting that effects are interconnected and can cause new effects.\n\n\n\n\n\n"}
{"id": "40780391", "url": "https://en.wikipedia.org/wiki?curid=40780391", "title": "List of medical mnemonics", "text": "List of medical mnemonics\n\nThis is a list of medical mnemonics, categorized and alphabetized.\n\n\nMS MAID:\n\nMonitors (EKG, SpO2, EtCO2, etc.)\n\nSuction\n\nMachine check (according to ASA guidelines)\n\nAirway equipment (ETT, laryngoscope, oral/nasal airway)\n\nIV equipment\n\nDrugs (emergency, inductions, NMBs, etc.)\n\nDOPE:\n\nDisplaced (usually right mainstem, pyreform fossa, etc.)\n\nObstruction (kinked or bitten tube, mucous plug, etc.)\n\nPneumothorax (collapsed lung)\n\nEsophagus\n\nMALES:\n\nMasks\n\nAirways\n\nLaryngoscopes\n\nEndotracheal tubes\n\nSuction/ Stylette, bougie\n\n\"Little Boys Prefer Toys\":\n\nLidocaine\n\nBupivicaine\n\nProcaine\n\nTetracaine\n\n\"Ears, Nose, Hose, Fingers and Toes\"\n\"Digital PEN\" - Digits, Penis, ear, nose.\n\nSPACE DIGS:\n\nSleep disruption\n\nPsychomotor retardation\n\nAppetite change\n\nConcentration loss\n\nEnergy loss\n\nDepressed mood\n\nInterest wanes\n\nGuilt\n\nSuicidal tendencies\n\nDepression: DSM-V Criteria for Major Depressive Disorder\n\n\"SIG E CAPS\":\n\nSleep disturbances\n\nInterest decreased (anhedonia)\n\nGuilt and/or feelings of worthlessness\n\nEnergy decreased\n\nConcentration problems\n\nAppetite/weight changes\n\nPsychomotor agitation or retardation\n\nSuicidal ideation\n\nPrimary: Patient's Psyche improved.\n\nSecondary: Symptom Sympathy for patient.\n\nTertiary: Therapist's gain\n\n\"'Death Always Brings Great Acceptance\":\n\nDenial\n\nAnger\n\nBargaining\n\nGrieving\n\nAcceptance\n\nHERO:\n\nHeterosexual crushes/ Homosexual Experience\n\nEducation regarding short term benefits\n\nRisk taking\n\nOmnipotence\n\nCHAP:\n\nCataplexy\n\nHallucinations\n\nAttacks of sleep\n\nParalysis on waking\n\nSAD PERSONS scale:\n\nSex (Male - Completion, Female - Attempt)\n\nAge (Adolescent or Elderly)\n\nDepression\n\nPrevious attempt\n\nEthanol abuse\n\nRational thinking loss\n\nSocial support problems\n\nOrganised plan\n\nNo spouse\n\nSickness (chronic illness)\n\nDElta waves during DEepest sleep (stages 3 & 4, slow-wave).\n\ndREaM during REM sleep.\n\nPLANE:\n\nPsychogenic: performance anxiety\n\nLibido: decreased with androgen deficiency, drugs\n\nAutonomic neuropathy: impede blood flow redirection\n\nNitric oxide deficiency: impaired synthesis, decreased blood pressure\n\nErectile reserve: can't maintain an erection\n\nMED:\n\nMedicines (propranalol, methyldopa, SSRI, etc.)\n\nEthanol\n\nDiabetes mellitus\n\n2 S's:\n\nSSRIs\n\nSqueezing technique [glans pressure before climax]\n\nMore detail with 2 more S's:\n\nSensate-focus exercises [relieves anxiety]\n\nStop and start method [5-6 rehearsals of stopping stimulation before climax]\n\n\"The Rhythm Nearly Proved Contagious\":\n\nIn increasing order:\n\nThiamine (B1)\n\nRiboflavin (B2)\n\nNiacin (B3)\n\nPyridoxine (B6)\n\nCobalamin (B12)\n\n\"PVT. TIM HALL always argues, never tires\":\n\nPhe\n\nVal\n\nThr\n\nTrp\n\nIle\n\nMet\n\nHis\n\nArg\n\nLue\n\nLys\n\n\n\"Muscles LIVe fast\":\n\nLeucine\n\nIsoleucine\n\nValine\n\nA FOLIC DROP:\n\nAlcoholism\n\nFolic acid antagonists\n\nOral contraceptives\n\nLow dietary intake\n\nInfection with Giardia\n\nCeliac sprue\n\nDilantin\n\nRelative folate deficiency\n\nOld\n\nPregnant\n\nABCD:\n\nAnderson's=Branching enzyme.\n\nCori's=Debranching enzyme.\n\n\n\"Viagra Pills Cause A Major Hardon Tendency\":\n\nVon Gierke's\n\nPompe's\n\nCori's\n\nAnderson's\n\nMcArdle's\n\nHer's\n\nTarui's\n\n0- Store fat, \"winter is coming\"→ glycogen synthase\n\n1- Shit, now you're too fat, lets burn calories with sex... sex sounds like six so→ glucose-6-phosphatase\n\n2- Then take some acid, because that's what you do after sex→ acid maltase\n\n3- Now you are so freaking high you rip all the branches from the Christmas tree→ debranching enzyme\n\n4- Then you think Holy Crap! Why did I do that, so you try to put the branches back on→ branching enzyme\n\n5- After all of this your MUSCLES are so tired from phosphorylation→ m-glycogen phosphorylase\n\n6- To make up for all that PHOSPHORYLATION you get drunk which ruins your liver→ L-glycogen phosphorylase\n\n7- more Points For Killing your liver→ muscle phosphofructokinase (m-PFK1)\n\n9- PHOK I'm a dumbass→ Phosphorylase Kinase (PHOK)\n\nLMNOP:\n\nLasix (furosemide)\n\nMorphine (diamorphine)\n\nNitrates\n\nOxygen (sit patient up)\n\nPulmonary ventilation (if doing badly)\n\nTHE ATRIAL FIBS:\n\nThyroid\n\nHypothermia\n\nEmbolism (P.E.)\n\nAlcohol\n\nTrauma (cardiac contusion)\n\nRecent surgery (post CABG)\n\nIschemia\n\nAtrial enlargement\n\nLone or idiopathic\n\nFever, anemia, high-output states\n\nInfarct\n\nBad valves (mitral stenosis)\n\nStimulants (cocaine, theo, amphet, caffeine)\n\nPET-MAC\n\nP= Pulmonary embolism\n\nE= Esophageal rupture\n\nT= Tension pneumothorax \n\nM= Myocardial infarction \n\nA= Aortic dissection\n\nC= Cardiac tamponade\n\nUnder 8, intubate.\n\n4 C's:\n\nComatose\n\nConvulsing\n\nCorrosive\n\nhydroCarbon\n\nPQRST(EKG waves):\n\nPericardial effusion\n\nQuantity of fluid raised (fluid over load)\n\nRight heart failure\n\nSuperior vena caval obstruction\n\nTricuspid stenosis/Tricuspid regurgitation/Tamponade (cardiac)\n\nDOGASH:\n\nDiamorphine\n\nOxygen\n\nGTN spray\n\nAsprin 300 mg\n\nStreptokinase\n\nHeparin\n\nITCHPAD\n\nInfarction\n\nTension pneumothorax\n\nCardiac tamponade\n\nHypovolemia/Hypothermia/Hypo-,Hyperkalemia/Hypomagnesmia/Hypoxemia\n\nPulmonary embolism\n\nAcidosis\n\nDrug overdose\n\nSOAP ME\n\nSuction\n\nOxygen\n\nAirway Equipment\n\nPositioning\n\nMonitoring & Meds\n\nEtCO2 & other Equipment\n\nRapid Sequence intubation Medications (RSI) (CCRx)\n\nVery Calmly Engage the Respiratory System\n\nVecuronium 0.1 mg/kg\n\nCisatracurium 0.2 mg/kg\n\nEtomidate 0.3 mg/kg\n\nRocuronium 0.6 mg/kg-1.2 mg/kg\n\nSuccinylcholine 1 mg/kg\n\nTV SPARC CUBE:\n\nThirst\n\nVomitting\n\nSweating\n\nPulse weak\n\nAnxious\n\nRespirations shallow/rapid\n\nCool\n\nCyanotic\n\nUnconscious\n\nBP low\n\nEyes blank\n\nRN CHAMPS (Alternatively: \"MR. C.H. SNAP\", or \"NH CRAMPS\"):\n\nRespiratory\n\nNeurogenic\n\nCardiogenic\n\nHemorrhagic\n\nAnaphylactic\n\nMetabolic\n\nPsychogenic\n\nSeptic\n\nBATS:\n\nBerry aneurysm\n\nArteriovenous malformation/Adult polycystic kidney disease\n\nTrauma\n\nStroke\n\nHEAD HEART VESSELS:\n\nCNS causes include HEAD:\n\nHypoxia/Hypoglycemia\n\nEpilepsy\n\nAnxiety\n\nDysfunctional brain stem (basivertebral TIA)\n\nCardiac causes are HEART:\n\nHeart attack\n\nEmbolism (PE)\n\nAortic obstruction (IHSS, AS or myxoma)\n\nRhythm disturbance, ventricular\n\nTachycardia\n\nVascular causes are VESSELS:\n\nVasovagal\n\nEctopic (reminds one of hypovolemia)\n\nSituational\n\nSubclavian steal\n\nENT (glossopharyngeal neuralgia)\n\nLow systemic vascular resistance (Addison's, diabetic vascular neuropathy)\n\nSensitive carotid sinus\n\nP-THORAX\n\nPleuritic pain\n\nTracheal deviation\n\nHyperresonance\n\nOnset sudden\n\nReduced breath sounds (and dyspnea)\n\nAbsent fremitus\n\nX-ray shows collapse\n\nShock, Shock, Shock, Everybody Shock, Little Shock, Big Shock, Momma Shock, Poppa Shock:\n\nShock= Defibrillate\n\nEverybody= Epinephrine\n\nLittle= Lidocaine\n\nBig= Bretylium\n\nMomma= MgSO\n\nPoppa= Pocainamide\n\n4 T's:\n\nTeratoma\n\nThymoma\n\nTesticular-type\n\nT-cell / Hodgkin's lymphoma\n\nRisk is 30% at age 30.\nRisk is 40% at age 40, and so on.\n\nBLAB:\n\nBone\n\nLiver\n\nAdrenals\n\nBrain\n\nABCDEF:\n\nAchalasia\n\nBarret's esophagus\n\nCorrosive esophagitis\n\nDiverticuliis\n\nEsophageal web\n\nFamilial\n\nSPEECH:\n\nSuperior vena cava syndrome\n\nParalysis of diaphragm (Phrenic nerve)\n\nEctopic hormones\n\nEaton-Lambert syndrome\n\nClubbing\n\nHorner syndrome/ Hoarseness\n\nABCDE:\n\nAsymmetry\n\nBorder irregular\n\nColour irregular\n\nDiameter usually > 0.5 cm\n\nElevation irregular\n\nPROGNOSIS:\n\nPresentation (time & course)\n\nResponse to treatment\n\nOld (bad prog.)\n\nGood intervention (i.e. early)\n\nNon-compliance with treatment\n\nOrder of differentiation (>1 cell type)\n\nStage of disease\n\nIll health\n\nSpread (diffuse)\n\n\"Go Look For the Adenoma Please\":\n\nTropic hormones affected by growth tumor are:\n\nGnRH\n\nLSH\n\nFSH\n\nACTH\n\nProlactin function\n\nTo assess abdomen, palpate all 4 quadrants for DR. GERM:\n\nDistension: liver problems, bowel obstruction\n\nRigidity (board like): bleeding\n\nGuarding: muscular tension when touched\n\nEviseration/ Ecchymosis\n\nRebound tenderness: infection\n\nMasses\n\nAEIOU TIPS\n\nAlcohol\n\nEpilepsy, Electrolytes, and Encephalopathy\n\nInsulin\n\nOverdose, Oxygen\n\nUnderdose, Uremia\n\nTrauma, Temperature\n\nInfection\n\nPsychogenic, Poisons\n\nStroke, Shock\n\nOne Two, put on my shoe - S1/2 roots for Achilles Reflex (foot plantarflexion)\n\nThree Four, kick the door - L3/4 roots for Patellar Reflex (knee extension)\n\nFive Six, pick up sticks - C5/6 roots for Brachioradialis and Biceps Brachii Reflexes (elbow flexion)\n\nSeven Eight, shut the gate - C7/8 roots for Triceps Brachii Reflex (elbow extension)\n\nOPQRST (Works well for cardiac, and respiratory patients.)\n\nOnset of the event\n\nProvocation or palliation\n\nQuality of the pain\n\nRegion and radiation\n\nSeverity\n\nTime\n\nVEAL CHOP\n5 P's:\n\nPain\n\nPallor\n\nParesthesia\n\nPulse\n\nParalysis\n\nDCAP-BTLS\n\nDeformities & Discolorations\n\nContusions\n\nAbrasions & Avulsion\n\nPenetrations & Punctures\n\nBurns\n\nTenderness\n\nLacerations\n\nSwelling & Symmetry\n\nOTIS CAMPBELL\n\nOrganophosphates\n\nTricyclic antidepressants\n\nIsoniazid, Insulin\n\nSympathomimetics\n\nCamphor, Cocaine\n\nAmphetamines\n\nMethylxanthines\n\nPCP, Propoxyphene, Phenol, Propranolol\n\nBenzodiazepine withdrawal, Botanicals\n\nEthanol withdrawal\n\nLithium, Lidocaine\n\nLindane, Lead\n\nABCDEFGHI:\n\nAcute renal failure\n\nBrain [increased ICP]\n\nCardiac [inferior MI]\n\nDKA\n\nEars [labyrinthitis]\n\nForeign substances [paracetamol, theo, etc.]\n\nGlaucoma\n\nHyperemesis gravidarum\n\nInfection [pyelonephritis, meningitis]\n\n\"All Patients Take Meds\":\n\nReading from top left:\n\nAortic\n\nPulmonary\n\nTricuspid\n\nMitral\n\nScale types is 3 V's:\n\nVisual response\n\nVerbal response\n\nVibratory (motor) response Scale scores are 4,5,6:\n\nScale of 4: see so much more\n\nScale of 5: talking jive\n\nScale of 6: feels the pricks (if testing motor by pain withdrawal)\n\n\"Assessed Mental State To Be Positively Clinically Unremarkable\":\n\nAppearance and behaviour [observe state, clothing...]\n\nMood [recent spirit]\n\nSpeech [rate, form, content]\n\nThinking [thoughts, perceptions]\n\nBehavioural abnormalities\n\nPerception abnormalities\n\nCognition [time, place, age...]\n\nUnderstanding of condition [ideas, expectations, concerns]\n\nSAMPLE history\n\nSigns and Symptoms\n\nAllergies\n\nMedications\n\nPast medical history, injuries, illnesses\n\nLast meal/intake\n\nEvents leading up to the injury and/or illness\n\nCLORIDE FPP\n\nCharacter: sharp or dull pain\n\nLoccasion: region (joint) of origin\n\nOnset: sudden vs. gradual\n\nRadiation:\n\nIntensity: how severe (scale 1-10), impact on ADLs (activities of daily living), is it getting better, worse or staying the same?\n\nDuration: acute vs. chronic\n\nEvents associated: falls, morning stiffness, swelling, redness, joint clicking or locking, muscle cramps, muscle wasting, movement limitation, weakness, numbness or tingling, fever, chills, trauma (mechanism of injury), occupation activities, sports, repetitive movements\n\nFrequency: intermittent vs. constant, have you ever had this pain before?\n\nPalliative factors: is there anything that makes it better? (rest, activity, meds, heat, cold)\n\nProvocative factors: is there anything that makes it worse? (rest, activity, etc.)\n\nSOCRATES:\n\nSite\n\nOnset\n\nCharacter\n\nRadiation\n\nAlleviating factors/ Associated symptoms\n\nTiming (duration, frequency)\n\nExacerbating factors\n\nSeverity\n\nAlternatively, Signs and Symptoms with the 'S'\n\nPLOTRADIO\n\nPast history\n\nLocation\n\nOnset/offset\n\nType/character (of pain)\n\nRadiation\n\nAggravating/alleviating factors\n\nDuration\n\nIntensity\n\nOther associated symptoms\n\n9 F's:\n\nFat\n\nFeces\n\nFluid\n\nFlatus\n\nFetus\n\nFull-sized tumors\n\nFull bladder\n\nFibroids\n\nFalse pregnancy\n\n12 P's\n\nPsychological (mental) status\n\nPupils: size, symmetry, reaction\n\nPaired ocular movements\n\nPapilloedema\n\nPressure (BP, increased ICP)\n\nPulse and rate\n\nParalysis, Paresis\n\nPyramidal signs\n\nPin prick sensory response\n\nPee (incontinent)\n\nPatellar reflex\n\nPtosis\n\n\"Breakfast is fast, Dinner is slow, both go down\":\n\nBobbing is fast\n\nDipping is slow\n\nIn both, the initial movement is down.\n\n3AM:\n\n3rd nerve palsy\n\nAnti-muscarinic eye drops (e.g. to facilitate fundoscopy)\n\nMyotonic pupil\n\nABC:\n\nAppearance (SOB, pain, etc.)\n\nBehaviour\n\nConnections (drips, inhalers, etc. connected to patient)\n\n\"A VITAMIN C\"\n\nA and C stand for Acquired and Congenital\n\nVITAMIN stands for:\n\nVascular\n\nInflammatory (Infectious and non-Infectious)\n\nTrauma/ Toxins\n\nAutoimmune\n\nMetabolic\n\nIdiopathic\n\nNeoplastic\n\n\"Absent Reflexes Should Get Paediatrics Professors Mad\"\n\nAbsent: Asymmetrical Tonic Neck Reflex\n\nReflexes: Rooting Reflex\n\nShould: Suck Reflex\n\nGet: Grasp Reflex\n\nPaediatrics: Placing Reflex\n\nProfessors: Parachute Reflex\n\nMad: Moro Reflex\n\nBALD CHASM:\n\nBlood pressure (high)\n\nArthritis\n\nLung disease\n\nDiabetes\n\nCancer\n\nHeart disease\n\nAlcoholism\n\nStroke\n\nMental health disorders (depression, etc.)\n\n\"I'm A People Person\"\n\nInspection\n\nAuscultation\n\nPercussion\n\nPalpation\n\nMJ THREADS:\n\nMyocardial infarction\n\nJaundice\n\nTuberculosis\n\nHypertension\n\nRheumatic fever/ Rheumatoid arthritis\n\nEpilepsy\n\nAsthma\n\nDiabetes\n\nStrokes\n\nVAMP THIS:\n\nVices (tobacco, alcohol, other drugs, sexual risks)\n\nAllergies\n\nMedications\n\nPreexisting medical conditions\n\nTrauma\n\nHospitalizations\n\nImmunizations\n\nSurgeries\n\nSOAP:\n\nSubjective: what the patient says.\n\nObjective: what the examiner observes.\n\nAssessment: what the examiner thinks is going on.\n\nPlan: what they intend to do about it\n\nLADDERS:\n\nLiving situation/ Lifestyle\n\nAnxiety\n\nDepression\n\nDaily activities (describe a typical day)\n\nEnvironmental risks/ Exposure\n\nRelationships\n\nSupport system/ Stress\n\n\"6 Students and 3 Teachers go for CAMPFIRE\":\n\nSite, Size, Shape, Surface, Skin, Scar\n\nTenderness, Temperature, Transillumination\n\nConsistency\n\nAttachment\n\nMobility\n\nPulsation\n\nFluctuation\n\nIrreducibility\n\nRegional lymph nodes\n\nEdge\n\n\"I Palpate People's Abdomens\":\n\nInspection\n\nPalpation\n\nPercussion\n\nAuscultation\n\nRETARD HEIGHT:\n\nRickets\n\nEndocrine (cretinism, hypopituitarism, Cushing's)\n\nTurner syndrome\n\nAchondroplasia\n\nRespiratory(suppurative lung disease)\n\nDown syndrome\n\nHereditary\n\nEnvironmental (postirradiation, postinfectious)\n\nIUGR\n\nGI (malabsorption)\n\nHeart (congenital heart disease)\n\nTilted backbone (scoliosis)\n\nsIgn: something I can detect even if patient is unconscious. sYMptom is something only hYM knows about.\n\nINVESTIGATIONS:\n\nIatrogenic\n\nNeoplastic\n\nVascular\n\nEndocrine\n\nStructural/ Mechanical\n\nTraumatic\n\nInflammatory\n\nGenetic/ Congenital\n\nAutoimmune\n\nToxic\n\nInfective\n\nOld age/ Degenerative\n\nNutritional\n\nSpontaneous/ Idiopathic\n\nPAST MIDNIGHT:\n\nPsychological\n\nAutoimmune\n\nSpontaneous/idiopathic\n\nToxic\n\nMetabolic\n\nInflammatory\n\nDegenerative\n\nNeoplastic\n\nInflammatory\n\nGenetic\n\nHematological\n\nTraumatic\n\nVITAMIN CDEF:\n\nVascular\n\nInfective/inflammatory\n\nTraumatic\n\nAutoimmune\n\nMetabolic\n\nIatrogenic/idiopathic\n\nNeoplastic\n\nCongenital\n\nDegenerative/developmental\n\nEndocrine/environmental\n\nFunctional\n\nLMNOP:\n\nLump\n\nMammary changes\n\nNipple changes\n\nOther symptoms\n\nPatient risk factors\n\nSPIKES:\n\nSetting up\n\nPerception\n\nInvitation\n\nKnowledge\n\nEmotions\n\nStrategy and Summary\n\nSt. VITUS'S DANCE:\n\nSydenhams\n\nVascular\n\nIncreased RBC's (polycythemia)\n\nToxins: CO, Mg, Hg\n\nUremia\n\nSLE\n\nSenile chorea\n\nDrugs\n\nAPLA syndrome\n\nNeurodegenerative conditions: HD, neuroacanthocytosis, DRPLA\n\nConception related: pregnancy, OCP's\n\nEndocrine: hyperthyroidism, hypo-, hyperglycemia\n\nDREAMS:\n\nDominantly inherited, mostly\n\nReflexes decreased\n\nEnzymes normal\n\nApathetic floppy baby\n\nMilestones delayed\n\nSkeletal abnormalities\n\nDEMENTIA:\n\nDrugs/Depression\n\nElderly\n\nMulti-infarct/Medication\n\nEnvironmental\n\nNutritional\n\nToxins\n\nIschemia\n\nAlcohol\n\nHEADS:\n\nHypertension/ Hyperlipidemia\n\nElderly\n\nAtrial fib\n\nDiabetes mellitus/ Drugs (cocaine)\n\nSmoking/Sex (male)\n\nHorny PAMELA:\n\nPtosis\n\nAnhydrosis\n\nMiosis\n\nEnophthalmos\n\nLoss of ciliary-spinal reflex\n\nAnisocoria\n\nDANISH:\n\n\n\n\n\n\n\nPinpoint Pupils are caused by oPioids and Pontine Pathology\n\nCAFÉ SPOT:\n\n\n\n\n\n\n\n\nWet, Wobbly, Wacky:\n\n\n\n\nSome Drugs Create Awesome Knockers\n\nSpironolactone\n\nDigitalis\n\nCimetidine\n\nAlcohol\n\nKetoconazole\n\nConduct disorder is seen in Children. Antisocial personality disorder is seen in Adults.\n\nAWESOME:\n\nAffect flat\n\nWeight change (loss or gain)\n\nEnergy, loss of\n\nSad feelings/ Suicide thoughts or plans or attempts/ Sexual inhibition/ Sleep change (loss or excess)/ Social withdrawal\n\nOthers (guilt, loss of pleasure, hopeless)\n\nMemory loss\n\nEmotional blunting\n\nUNHAPPINESS:\n\nUnderstandable (such as bereavement, major stresses)\n\nNeurotic (high anxiety personalities, negative parental upbringing Hypochondriasis)\n\nAgitation (usually organic causes such as dementia)\n\nPseudodementia\n\nPain\n\nImportuniing (whingeing, complaining)\n\nNihilistic\n\nEndogenous\n\nSecondary (i.e. cancer at the head of the pancreas, bronchogenic cancer)\n\nSyndromal\n\n\"The sad tale of Erikson Motors\":\n\n\nMr. Trust and MsTrust had an auto they were ashamed of. She took the initiative to find the guilty party. She found the industry was inferior. They were making cars with dents [identity] and rolling fuses [role confusion]. Mr. N.T. Macy [intimacy] isolated the problem, General TVT absorbed the cost. In the end, they found the tires were just gritty and the should have used de- spare!\n\nASEPTIC:\n\nAppearance \n\nSpeech\n\nEmotion (Objective/Subjective)\n\nPerceptions\n\nThoughts\n\nInsight\n\nCognition\n\nDIG FAST:\n\nDistractibility\n\nIndiscretion (DSM-IV's \"excessive involvement in pleasurable activities\")\n\nGrandiosity\n\nFlight of ideas\n\nActivity increase\n\nSleep deficit (decreased need for sleep)\n\nTalkativeness (pressured speech)\n\nMust have 3 of MANIAC:\n\nMouth (pressure of speech)/ Moodl\n\nActivity increased\n\nNaughty (disinhibition)\n\nInsomnia\n\nAttention (distractability)\n\nConfidence (grandiose ideas)\n\nSLeep terrors and SLeepwalking occur during SLow-wave sleep (stages 3 & 4).NightmaRE occurs during REM sleep (and is REMembered).\n\n\"Depressed Patients Seem Anxious, So Claim Psychiatrists\":\n\nDepression and other mood disorders (major depression, bipolar disorder, dysthymia)\n\nPersonality disorders (primarily borderline personality disorder)\n\nSubstance abuse disorders\n\nAnxiety disorders (panic disorder with agoraphobia, obsessive-compulsive disorder)\n\nSomatization disorder, eating disorders (these two disorders are combined because both involve disorders of bodily perception)\n\nCognitive disorders (dementia, delirium)\n\nPsychotic disorders (schizophrenia, delusional disorder and psychosis accompanying depression, substance abuse or dementia)\n\n4 A's:\n\nAmbivalence\n\nAffective incongruence\n\nAssociative loosening\n\nAutism\n\nWITHDraw IT:\n\n\nWithdrawal\n\nInterest or Important activities given up or reduced\n\nTolerance\n\nHarm to physical and psychosocial known but continue to use\n\nDesire to cut down, control\n\nIntended time, amount exceeded\n\nTime spent too much\n\n\"Pamela Found Our Rotation Particularly Exciting; Very Highly Commended Mainly 'Cus She Arouses\":\n\nPatient details\n\nFilm details\n\nObjects (e.g. lines, electrodes)\n\nRotation\n\nPenetration\n\nExpansion\n\nVessels\n\nHila\n\nCostophrenic angles\n\nMediastinum\n\nCardiothoracic Ratio\n\nSoft tissues and bones\n\nAir (diaphragm, pneumothorax, subcut. emphysema)\n\nPreliminary is ABCDEF:\n\nAP or PA\n\nBody position\n\nConfirm name\n\nDate\n\nExposure\n\nFilms for comparison\n\nAnalysis is ABCDEF:\n\nAirways (hilar adenopathy or enlargement)\n\nBreast shadows/ Bones (rib fractures, lytic bone lesions)\n\nCardiac silhoutte (cardiac enlargement)/ Costophrenic angles (pleural effusions)\n\nDiaphragm (evidence of free air)/ Digestive tract\n\nEdges (apices for fibrosis, pneumothorax, pleural thickening or plaques)/ Extrathoracic tissues\n\nFields (evidence of alveolar filling)/ Failure (alveolar air space disease with prominent vascularity with or without pleural effusions)\n\n\"If you see HOLES on chest X-ray, they are WEIRD\":\n\nWegener's syndrome\n\nEmbolic (pulmonary, septic)\n\nInfection (anaerobes, pneumocystis, TB)\n\nRheumatoid (necrobiotic nodules)\n\nDevelopmental cysts (sequestration)\n\nHistiocytosis\n\nOncological\n\nLymphangioleiomyomatosis\n\nEnvironmental, occupational\n\nSarcoid\n\nAlternatively: L=Left atrial myxoma\n\nCRITOE:\n\nCapitellum\n\nRadial head\n\nInternal epicondyle\n\nTrochlea\n\nOlecranon\n\nExternal epicondyle\n\n\"Blood Can Be Very Bad\":\n\nB lood\n\nCistern\n\nBrain\n\nVentricles\n\nBone\n\nABCD:\n\nAnterior: look for swelling\n\nBones: examine each bone for fractures\n\nCartilage: look for slipped discs\n\nDark spots: ensure not abnormally big, or could mean excess blood\n\nLOSS:\n\nLoss of joint space\n\nOsteopyhtes\n\nSubcondral sclerosis\n\nSubchondral cysts\n\n\"WW 2\" (World War II):\n\nWater is White in a T2 scan.\n\nConversely, a T1 scan shows fat as being whiter.\n\nBREASTS:\n\nBeryllium\n\nRadiation\n\nExtrinsic allergic alveolitis\n\nAnkylosing spondylitis\n\nSarcoidosis\n\nTB\n\nSiliconiosis\n\nLEMON\n\nASTHMA\n\nCAT items: CHEST SEA\n\nTo aid memory, think of the \"chest\" (or lungs) floating in a \"sea\" of yellow sputum, which is commonly seen in COPD.\n\n\nA TEA SHOP\n\n\nA CHEST\n\n\nLMNOP:\n\nLasix\n\nMorphine\n\nNitro\n\nOxygen\n\nPosition/Positive pressure ventilation\n\nThe following may or may not fit properly into one of the above categories. They are being stored in this section either temporarily or permanently. Categorize them if needed.\n\nSLUDGE and the Killer B's:\n\nSalivation\n\nLacrimation\n\nUrination\n\nDiaphoresis, Diarrhea\n\nGastrointestinal cramping\n\nEmesis\n\nBradycardia\n\nBronchospasm\n\nBronchorrhea\n\nalso known as DUMBBELLS\n\nDiarrhea\n\nUrination\n\nMiosis\n\nBradycardia\n\nBronchospasm\n\nEmesis\n\nLacrimation\n\nLoss of muscle strength\n\nSalivation/Sweating\n\nDISCO\n\nDIGITALIS\nISONIAZID\nSPIRILACTINE\nCIMETIDINE, KETOCONQZILE\nOESTROGEN\n\nIsoproterenol\n\nDopamine\n\nEpinephrine\n\nAtropine Sulfate\n\nC3, 4, 5 Keeps the Diaphragm Alive\n\n7 P's\n\nPreparation\n\nPreoxygenation\n\nPretreatment\n\nParalysis with induction\n\nPositioning\n\nPlacement of tube\n\nPostintubation management\n\nFAT RN:\n\nFever\nAnemia\n\nThrombocytopenia\n\nRenal\n\nNeuro changes\n\nSOAP BRAIN MD\n\nSerositis\n\nOral ulcers\n\nArthritis\n\nPhotosensitivity, pulmonary fibrosis\n\nBlood cells\n\nRenal, Raynaud's\n\nANA\n\nImmunologic (anti-Sm, anti-dsDNA)\n\nNeuropsych\n\nMalar rash\n\nDiscoid rash however, not in order of diagnostic importance.\n\n"}
{"id": "23438896", "url": "https://en.wikipedia.org/wiki?curid=23438896", "title": "List of mobile app distribution platforms", "text": "List of mobile app distribution platforms\n\nThis list of mobile app distribution platforms includes digital distribution platforms that are intended to provide mobile apps to mobile devices. For information on each mobile platform and its market share see the operating systems section of the mobile operating system and smartphone. A comparison of development capabilities of each mobile platform can be found in the article mobile development. For cross-platform development see multiple phone web-based application framework. The article mobile software contains other general information.\n\nThese application marketplaces are native to the major mobile operating systems.\n\nThird-party platforms are software distribution platforms which are used as alternatives for operating system native distribution platforms. Independent operating systems are software collections which use their own software distribution, customized user interface, SDK and API (except billing API which is related only to application store).\n"}
{"id": "7120135", "url": "https://en.wikipedia.org/wiki?curid=7120135", "title": "List of volcanoes in Nicaragua", "text": "List of volcanoes in Nicaragua\n\nThis is a list of active and extinct volcanoes in Nicaragua. \n"}
{"id": "7120250", "url": "https://en.wikipedia.org/wiki?curid=7120250", "title": "List of volcanoes in Solomon Islands", "text": "List of volcanoes in Solomon Islands\n\nThis is a list of active and extinct volcanoes in Solomon Islands.\n"}
{"id": "47347642", "url": "https://en.wikipedia.org/wiki?curid=47347642", "title": "MAGIC criteria", "text": "MAGIC criteria\n\nThe MAGIC criteria are a set of guidelines put forth by Robert Abelson in his book \"Statistics as Principled Argument\". In this book he posits that the goal of statistical analysis should be to make compelling claims about the world and he presents the MAGIC criteria as a way to do that. \n\nMAGIC is an acronym for:\n\nSong Qian noted that the MAGIC criteria could be of use to ecologists. \n"}
{"id": "185290", "url": "https://en.wikipedia.org/wiki?curid=185290", "title": "Mars effect", "text": "Mars effect\n\nThe Mars effect is a purported statistical correlation between athletic eminence and the position of the planet Mars relative to the horizon at time and place of birth. This controversial finding was first reported by the French psychologist and \"neo-astrologer\" Michel Gauquelin published in his book \"L'influence des astres\" (\"The Influence of the Stars\", 1955). Gauquelin suggested that a statistically significant number of sports champions were born just after the planet Mars rises or culminates. Gauquelin divided the plane of the ecliptic into twelve sectors, identifying two \"key\" sectors of statistical significance.\n\nGauquelin's work was accepted by the notable psychologist and statistician Hans Eysenck among others but later attempts to validate the data and replicate the effect have produced uneven results, chiefly owing to disagreements over the selection and analysis of the data set. Since the phenomenon in question depends upon the daily rotation of the Earth, the availability and accuracy of time and place of birth data is crucial to such studies, as is the criterion of \"eminence\". Later research claims to explain the Mars effect by selection bias, favouring champions who were born in a \"key sector\" of Mars and rejecting those who were not from the sample.\n\nGauquelin's work was not limited to the Mars effect: his calculations led him first to reject most of the conventions of natal astrology as it is practised in the modern west but he singled out \"highly significant statistical correlations between planetary positions and the birth times of eminently successful people.\" This claim concerned not only Mars but five planets, correlated with eminence in fields broadly compatible with the traditional \"planetary rulerships\" of astrology. However, partly because eminence in sport is more quantifiable, later research, publicity and controversy has tended to single out the \"Mars effect\".\n\nIn 1956 Gauquelin invited the Belgian Comité Para to review his findings but it was not until 1962 that Jean Dath corroborated the statistics Gauquelin had presented and suggested an attempt at duplication using Belgian athletes. By this time Gauquelin had published \"Les Hommes et Les Astres\" (Men and the Stars, 1960), offering further data. The Comité Para tested the Mars effect in 1967 and replicated it, though most of the data (473 of 535) were still collected by Gauquelin himself. The committee, suspecting that the results might have been an artifact, withheld its findings for a further eight years, then cited unspecified “demographic errors” in its findings. Unpublished internal analyses contradicted this and one committee member, Luc de Marré, resigned in protest. In 1983 Abell, Kurtz and Zelen (\"see below\") published a reappraisal, rejecting the idea of demographic errors, saying, “Gauquelin adequately allowed for demographic and astronomical factors in predicting the expected distribution of Mars sectors for birth times in the general population.”\n\nIn 1975 Paul Kurtz's journal \"The Humanist\" published an article on astrology criticizing Gauquelin, to which the latter and his wife Françoise responded. Then Professor Marvin Zelen, a statistician and associate of the recently founded Committee for the Scientific Investigation of Claims of the Paranormal (CSICOP, now known as the Committee for Skeptical Inquiry (CSI)), proposed in a 1976 article in the same periodical that, in order to eliminate any demographic anomaly, Gauquelin randomly pick 100 athletes from his data-set of 2,088 and check the birth/planet correlations of a sample of babies born at the same times and places in order to establish a control group, giving the base-rate (chance) expectation for comparison (The 100 random athletes later expanded into a subsample of 303 athletes).\n\nIn April 1977 CSICOP researcher George O. Abell wrote to Kurtz stating that Zelen's test had come out in the Gauquelins' favour. The Gauquelins also performed the test that Professor Zelen had proposed and carried out and found that the chance Mars-in-key-sector expectation for the general population (i.e., non-champions) was about 17%, significantly less than the 22% observed for athletic champions. However the subsequent article by Zelen, Abell and Kurtz did not clearly state this outcome but rather questioned the original data. In a rebuttal of the Gauquelins' published conclusion, Marvin Zelen analysed the composition, not of the 17,000 non-champions of the control group, but of the 303 champions, splitting this secondary subsample (which was already nearly too small to test 22% vs 17%) by eliminating female athletes, a subgroup that gave the results most favourable to Gauquelin, and dividing the remaining athletes into city/rural sections and Parisian/non-Parisian sections.\n\nBefore and after publication of Zelen's results astronomer and charter CSICOP member Dennis Rawlins, the CSICOP Council's only astronomer at the time, repeatedly objected to the procedure and to CSICOP's subsequent reportage of it. Rawlins privately urged that the Gauquelins' results were valid and the “Zelen test” could only uphold this and that Zelen had diverted from the original purpose of the control test, which was to check the base rate of births with Mars in the \"key\" sectors. It appeared to him that the test had minimised the significance of the Mars/key-sector correlations with athletes by splitting the sample of athletes and that the experimenters, who were supposed to be upholding scientific standards, were actually distorting and manipulating evidence to conceal the result of an ill-considered test.\n\nThe Kurtz-Zelen-Abell analysis had split the sample primarily to examine the randomness of the 303 selected champions, the non-randomness of which Rawlins demonstrated in 1975 and 1977. Zelen's 1976 \"Challenge to Gauquelin\" had stated: \"We now have an objective way for unambiguous corroboration or disconfirmation ... to settle this question\", whereas this aim was now disputed. Rawlins made procedural objections, stating; \"... we find an inverse correlation between size and deviation in the Mars-athletes subsamples (that is, the smaller the subsample, the larger the success) – which is what one would expect if bias had infected the blocking off of the sizes of the subsamples\".\n\nCSICOP also contended, after reviewing the results, that the Gauquelins had not chosen randomly. They had had difficulty finding sufficient same-week and same-village births to compare with champions born in rural areas and so had chosen only champions born in larger cities. The Gauquelins' original total list of about 2,088 champions had included 42 Parisians and their subsample of 303 athletes also included 42 Parisians. Further, Paris is divided into 20 \"arrondissements\", different economic classes and ethnic groups typically inhabiting different arrondissements. The Gauquelins had compared the 42 Parisian champions (who had been born throughout Paris) to non-champions of only one arrondissement. If the 22% correlation was an artifact partly based on factors such as rural recordkeeping, economic, class or ethnic differences in birth patterns, this fact would be blurred by this non-random selection.\n\nAt the same time CSICOP began a study of U.S. athletes in consultation with Zelen, Abell and Rawlins. The results, published in 1979 showed a negative result. Gauquelin contended the KZA group demonstrated an overall preference for mediocre athletes and ignored his criteria of eminence and that they included basketball players and people born after 1950.\n\nIn 1994 the results of a major study undertaken by the Committee for the Study of Paranormal Phenomena (\"Comité pour l’Étude des Phénomènes Paranormaux\", or CFEPP) in France found no evidence whatsoever of a \"Mars Effect\" in the births of athletes. The study had been proposed in 1982 and the Committee had agreed in advance to use the protocol upon which Gauquelin insisted. The CFEPP report was “leaked” to the Dutch newspaper \"Trouw\".\n\nIn 1990 the CFEPP had issued a preliminary report on the study, which used 1,066 French sports champions, giving full data for the 1,066 as well as the names of 373 who fit the criteria but for whom birth times were unavailable, discussing methodology and listing data-selection criteria. In 1996 the report, with a commentary by J. W. Nienhuys and several letters from Gauquelin to the Committee, was published in book form as \"The Mars Effect – A French Test of Over 1,000 Sports Champions\". The CFEPP stated that its experiment showed no effect and concluded that the effect was attributable to bias in Gauquelin’s data selection, pointing to the suggestions made by Gauquelin to the Committee for changes in their list of athletes.\n\nSome researchers argued that Gauquelin did not adjust the statistical significance of the Mars Effect for multiple comparisons and did not address the issue in his publications. Simplified and illustrative showcase argument is explained here: There are 10 celestial bodies and 12 sectors for them to be in. Furthermore, there are 132 combinations of sector pairs and thus 1320 different combinations of a planet with two sectors. There is about a 25% chance to find at least one such combination (of one planet and two sectors) for a random dataset of the same size as Gauquelin’s that would yield a result with apparent statistical significance like the one obtained by Gauquelin. This implies that after adjusting for multiple comparisons, the Mars effect is no longer statistically significant even at the modest significance level of 0.05 and is probably a false positive.\n\nGeoffrey Dean has suggested that the effect may be caused by self reporting of birth dates by parents rather than any issue with the study by Gauquelin. Gauquelin had failed to find the Mars effect in populations after 1950. Dean has put forward the idea that this may be due to increases in doctors reporting the time of birth rather than parents. Information about misreporting was unavailable to Gauquelin at the time. Dean had said that misreporting by 3% of the sample would explain the result.\n\n\n"}
{"id": "4954001", "url": "https://en.wikipedia.org/wiki?curid=4954001", "title": "Membrane biology", "text": "Membrane biology\n\nMembrane biology is the study of the biological and physiochemical characteristics of membranes, with applications in the study of cellular physiology.\n\nMembrane bioelectrical impulses are described by the Hodgkin cycle.\n\nMembrane biophysics is the study of biological membrane structure and function using physical, computational, mathematical, and biophysical methods. A combination of these methods can be used to create phase diagrams of different types of membranes, which yields information on thermodynamic behavior of a membrane and its components. As opposed to membrane biology, membrane biophysics focuses on quantitative information and modeling of various membrane phenomena, such as lipid raft formation, rates of lipid and cholesterol flip-flop, protein-lipid coupling, and the effect of bending and elasticity functions of membranes on inter-cell connections.\n\n"}
{"id": "57720057", "url": "https://en.wikipedia.org/wiki?curid=57720057", "title": "Models of Consciousness", "text": "Models of Consciousness\n\nModels of consciousness are used to illustrate and aid in understanding and explaining distinctive aspects consciousness. Sometimes the models are labeled theories of consciousness. Anil Seth defines such models as those that relate brain phenomena such as fast irregular electrical activity and widespread brain activation to properties of consciousness such as qualia. Seth allows for different types of models including mathematical, logical, verbal and conceptual models. \n\nThe Neural correlates of consciousness (NCC) formalism is used as a major step towards explaining consciousness. The NCC are defined to constitute the minimal set of neuronal events and mechanisms sufficient for a specific conscious percept, and consequently sufficient for consciousness. In this formalism, consciousness is viewed as a state-dependent property of some undefined complex, adaptive, and highly interconnected biological system. \n\nTimothy Leary introduced and Robert Anton Wilson and Antero Alli elaborated the Eight-circuit model of consciousness as hypothesis that \"suggested eight periods [circuits] and twenty-four stages of neurological evolution\".\n\nDaniel Dennett proposed a physicalist, information processing based multiple drafts model of consciousness described more fully in his 1991 book, Consciousness Explained.\n\nThe Dehaene–Changeux model (DCM), also known as the global neuronal workspace or the global cognitive workspace model is a computer model of the neural correlates of consciousness programmed as a neural network. Stanislas Dehaene and Jean-Pierre Changeux introduced this model in 1986. It is associated with Bernard Baars's Global workspace theory for consciousness. \n\nClouding of consciousness, also known as brain fog or mental fog, is a term used in medicine denoting an abnormality in the regulation of the overall level of consciousness that is mild and less severe than a delirium. It is part of an overall model where there's regulation of the \"overall level\" of the consciousness of the brain and aspects responsible for \"arousal\" or \"wakefulness\" and awareness of oneself and of the environment. \n\nElectromagnetic theories of consciousness propose that consciousness can be understood as an electromagnetic phenomenon that occurs when a brain produces an electromagnetic field with specific characteristics. Some electromagnetic theories are also quantum mind theories of consciousness; examples include quantum brain dynamics (QBD).\n\nOrchestrated objective reduction (Orch-OR) model is based on the hypothesis that consciousness in the brain originates from quantum processes inside neurons, rather than from connections between neurons (the conventional view). The mechanism is held to be associated with molecular structures called microtubules. The hypothesis was advanced by Roger Penrose and Stuart Hameroff and has been the subject of extensive debate, \n\nMin proposed in a 2010 paper a Thalamic reticular networking model of consciousness. The model suggests consciousness as a \"mental state embodied through TRN-modulated synchronization of thalamocortical networks\". In this model the thalamic reticular nucleus (TRN) is suggested as ideally suited for controlling the entire cerebral network, and responsible (via GABAergic networking) for synchronization of neural activity.\n\nFunctionalism is a view in the theory of the mind. It states that mental states (beliefs, desires, being in pain, etc.) are constituted solely by their functional role – that is, they have causal relations to other mental states, numerous sensory inputs, and behavioral outputs.\n\nSociology of human consciousness uses the theories and methodology of sociology to explain human consciousness. The theory and its models emphasize the importance of language, collective representations, self-conceptions, and self-reflectivity. It argues that the shape and feel of human consciousness is heavily social.\n"}
{"id": "9542300", "url": "https://en.wikipedia.org/wiki?curid=9542300", "title": "National Undersea Research Center for the North Atlantic and Great Lakes", "text": "National Undersea Research Center for the North Atlantic and Great Lakes\n\nThe National Undersea Research Center for the North Atlantic and Great Lakes (NURC-NA&GL) is one of six undersea centers established by the National Oceanic and Atmospheric Administration's Undersea Research Program. It is co-located with the University of Connecticut’s Department of Marine Sciences. The center's mission includes supporting regional and national oceanography research and promoting awareness of the oceanic ecosystem through educational outreach programs. \n\n"}
{"id": "50145306", "url": "https://en.wikipedia.org/wiki?curid=50145306", "title": "Ong Kok Hai", "text": "Ong Kok Hai\n\nOng Kok Hai (born 1945 in Penang) is a Malaysian microbiologist and Professor of Microbiology at the International Medical University. He was one of the founders of the International Medical University in 1992 and also played a major role in the establishment of the medical schools at the University of Science, Malaysia in 1979 and at the National University of Malaysia. He has been involved in typhoid research, and in 1995 he co-founded the medical biotechnology company Malaysian Bio-Diagnostics Research Sdn Bhd (MBDr), which develops a rapid diagnostic test for typhoid fever used in many typhoid endemic countries.\n\nHe holds a BSc (Hons) in microbiology from the University of Guelph in Canada (1969) and a PhD in medical microbiology from the University of Manchester in the United Kingdom (1977). He lectured at the National University of Malaysia from 1977, before joining the University of Science to start its medical school in 1979.\n\nHis current research focuses on enteric fever and on a rapid antigen detection test for brugia malayi.\n\n"}
{"id": "30443935", "url": "https://en.wikipedia.org/wiki?curid=30443935", "title": "PCR food testing", "text": "PCR food testing\n\nPCR food testing is the engagement of Polymerase Chain Reaction (PCR) technologies for the testing of food for the presence or absence of human pathogens, such as E. coli, Salmonella, Listeria, etc.\n\nPCR test results offers faster, more accurate test result data than traditional microbiological culture methods, which typically require 24 to 48 hours of growth time and often report less than 100% specificity and sensitivity. PCR results can be reported from 30 to 55 minutes after the initiation of the amplification run. PCR test results report very close to 100% specificity and sensitivity.\nFour sample collection sites for PCR food testing can be:\nEach of these sample types can be collected, prepared and PCR tested within a short time for many sample types. Some sample types may require sample enrichment via shortened culture growth periods prior to PCR testing.\n\n"}
{"id": "6867899", "url": "https://en.wikipedia.org/wiki?curid=6867899", "title": "Phytotelma", "text": "Phytotelma\n\nPhytotelma (plural phytotelmata) is a small water-filled cavity in a terrestrial plant. The water accumulated within these plants may serve as the habitat for associated fauna and flora. Often the faunae associated with phytotelmata are unique. Some species also are of great practical significance; for example, immature stages of some mosquitoes, such as some \"Anopheles\" and \"Aedes\" species that are important disease vectors, develop in phytotelmata.\n\nA rich literature in German summarised by Thienemann (1954) developed many aspects of phytotelm biology. Reviews of the subject by Kitching (1971) and Maguire (1971) introduced the concept of phytotelmata to English-speaking readers. A multi-authored book edited by Frank and Lounibos (1983) dealt in 11 chapters with classification of phytotelmata, and with phytotelmata provided by bamboo internodes, banana leaf axils, bromeliad leaf axils, \"Nepenthes\" pitchers, \"Sarracenia\" pitchers, tree holes, and \"Heliconia\" flower bracts.\n\nA classification of phytotelmata by Kitching (2000) recognizes five principal types: bromeliad tanks, certain carnivorous plants such as pitcher plants, water-filled tree hollows, bamboo internodes, and axil water (collected at the base of leaves, petals or bracts); it concentrated on food webs. A review by Greeney (2001) identified seven forms: tree holes, leaf axils, flowers, modified leaves, fallen vegetative parts (e.g. leaves or bracts), fallen fruit husks, and stem rots.\n\nThe word \"phytotelma\" derives from the ancient Greek roots \"phyto-\", meaning 'plant', and \"telma\", meaning 'pond'. Thus, the correct singular is \"phytotelma\".\n\nThe term was coined by L. Varga in 1928.\n\nThe correct pronunciation is \"phytotēlma\" and \"phytotēlmata\" because of the Greek origin (the stressed vowels are here written as \"ē\"). \n\n"}
{"id": "299329", "url": "https://en.wikipedia.org/wiki?curid=299329", "title": "Probabilistic context-free grammar", "text": "Probabilistic context-free grammar\n\nGrammar theory to model symbol strings originated from work in computational linguistics aiming to understand the structure of natural languages. Probabilistic context free grammars (PCFGs) have been applied in probabilistic modeling of RNA structures almost 40 years after they were introduced in computational linguistics.\n\nPCFGs extend context-free grammars similar to how hidden Markov models extend regular grammars. Each production is assigned a probability. The probability of a derivation (parse) is the product of the probabilities of the productions used in that derivation. These probabilities can be viewed as parameters of the model, and for large problems it is convenient to learn these parameters via machine learning. A probabilistic grammar's validity is constrained by context of its training dataset.\n\nPCFGs have application in areas as diverse as natural language processing to the study the structure of RNA molecules and design of programming languages. Designing efficient PCFGs has to weigh factors of scalability and generality. Issues such as grammar ambiguity must be resolved. The grammar design affects results accuracy. Grammar parsing algorithms have various time and memory requirements.\n\nDerivation: The process of recursive generation of strings from a grammar.\nParsing: Finding a valid derivation using an automaton.\n\nParse Tree: The alignment of the grammar to a sequence.\n\nAn example of a parser for PCFG grammars is the pushdown automaton. The algorithm parses grammar nonterminals from left to right in a stack-like manner. This brute-force approach is not very efficient. In RNA secondary structure prediction variants of the Cocke–Younger–Kasami (CYK) algorithm provide more efficient alternatives to grammar parsing than pushdown automata. Another example of a PCFG parser is the Stanford Statistical Parser which has been trained using Treebank.\n\nSimilar to a CFG, a probabilistic context-free grammar can be defined by a quintuple:\n\nwhere \n\nPCFGs models extend context-free grammars the same way as hidden Markov models extend regular grammars.\n\nThe Inside-Outside algorithm is an analogue of the Forward-Backward algorithm. It computes the total probability of all derivations that are consistent with a given sequence, based on some PCFG. This is equivalent to the probability of the PCFG generating the sequence, and is intuitively a measure of how consistent the sequence is with the given grammar. The Inside-Outside algorithm is used in model parametrization to estimate prior frequencies observed from training sequences in the case of RNAs.\n\nDynamic programming variants of the CYK algorithm find the Viterbi parse of a RNA sequence for a PCFG model. This parse is the most likely derivation of the sequence by the given PCFG.\n\nContext-free grammars are represented as a set of rules inspired from attempts to model natural languages. The rules are absolute and have a typical syntax representation known as Backus–Naur form. The production rules consist of terminal formula_2 and non-terminal symbols and a blank formula_3 may also be used as an end point. In the production rules of CFG and PCFG the left side has only one nonterminal whereas the right side can be any string of terminal or nonterminals. In PCFG nulls are excluded. An example of a grammar:\nThis grammar can be shortened using the '|' ('or') character into:\n\nTerminals in a grammar are words and through the grammar rules a non-terminal symbol is transformed into a string of either terminals and/or non-terminals. The above grammar is read as \"beginning from a non-terminal the emission can generate either or or formula_3\".\nIts derivation is: \n\nAmbiguous grammar may result in ambiguous parsing if applied on homographs since the same word sequence can have more than one interpretation. Pun sentences such as the newspaper headline \"Iraqi Head Seeks Arms\" are an example of ambiguous parses.\n\nOne strategy of dealing with ambiguous parses (originating with grammarians as early as Pāṇini) is to add yet more rules, or prioritize them so that one rule takes precedence over others. This, however, has the drawback of proliferating the rules, often to the point where they become difficult to manage. Another difficulty is overgeneration, where unlicensed structures are also generated.\n\nProbabilistic grammars circumvent these problems by ranking various productions on frequency weights, resulting in a \"most likely\" (winner-take-all) interpretation. As usage patterns are altered in diachronic shifts, these probabilistic rules can be re-learned, thus updating the grammar.\n\nAssigning probability to production rules makes a PCFG. These probabilities are informed by observing distributions on a training set of similar composition to the language to be modeled. On most samples of broad language, probabilistic grammars where probabilities are estimated from data typically outperform hand-crafted grammars. CFGs when contrasted with PCFGs are not applicable to RNA structure prediction because while they incorporate sequence-structure relationship they lack the scoring metrics that reveal a sequence structural potential \n\nA weighted context-free grammar (WCFG) is a more general category of context-free grammar, where each production has a numeric weight associated with it. The weight of a specific parse tree in a WCFG is the product (or sum ) of all rule weights in the tree. Each rule weight is included as often as the rule is used in the tree. A special case of WCFGs are PCFGs, where the weights are (logarithms of ) probabilities.\n\nAn extended version of the CYK algorithm can be used to find the \"lightest\" (least-weight) derivation of a string given some WCFG.\n\nWhen the tree weight is the product of the rule weights, WCFGs and PCFGs can express the same set of probability distributions.\n\nEnergy minimization and PCFG provide ways of predicting RNA secondary structure with comparable performance. However structure prediction by PCFGs is scored probabilistically rather than by minimum free energy calculation. PCFG model parameters are directly derived from frequencies of different features observed in databases of RNA structures rather than by experimental \ndetermination as is the case with energy minimization methods.\n\nThe types of various structure that can be modeled by a PCFG include long range interactions, pairwise structure and other nested structures. However, pseudoknots can not be modeled. PCFGs extend CFG by assigning probabilities to each production rule. A maximum probability parse tree from the grammar implies a maximum probability structure. Since RNAs preserve their structures over their primary sequence; RNA structure prediction can be guided by combining evolutionary information from comparative sequence analysis with biophysical knowledge about a structure plausibility based on such probabilities. Also search results for structural homologs using PCFG rules are scored according to PCFG derivations probabilities. Therefore, building grammar to model the behavior of base-pairs and single-stranded regions starts with exploring features of structural multiple sequence alignment of related RNAs. \n\nThe above grammar generates a string in an outside-in fashion, that is the basepair on the furthest extremes of the terminal is derived first. So a string such as formula_9 is derived by first generating the distal 's on both sides before moving inwards:\n\nA PCFG model extendibility allows constraining structure prediction by incorporating expectations about different features of an RNA . Such expectation may reflect for example the propensity for assuming a certain structure by an RNA. However incorporation of too much information may increase PCFG space and memory complexity and it is desirable that a PCFG-based model be as simple as possible.\n\nEvery possible string a grammar generates is assigned a probability weight formula_11 given the PCFG model formula_12. It follows that the sum of all probabilities to all possible grammar productions is formula_13. The scores for each paired and unpaired residue explain likelihood for secondary structure formations. Production rules also allow scoring loop lengths as well as the order of base pair stacking hence it is possible to explore the range of all possible generations including suboptimal structures from the grammar and accept or reject structures based on score thresholds.\n\nRNA secondary structure implementations based on PCFG approaches can be utilized in :\n\nDifferent implementation of these approaches exist. For example, Pfold is used in secondary structure prediction from a group of related RNA sequences, covariance models are used in searching databases for homologous sequences and RNA annotation and classification, RNApromo, CMFinder and TEISER are used in finding stable structural motifs in RNAs.\n\nPCFG design impacts the secondary structure prediction accuracy. Any useful structure prediction probabilistic model based on PCFG has to maintain simplicity without much compromise to prediction accuracy. Too complex a model of excellent performance on a single sequence may not scale. A grammar based model should be able to:\n\nThe resulting of multiple parse trees per grammar denotes grammar ambiguity. This may be useful in revealing all possible base-pair structures for a grammar. However an optimal structure is the one where there is one and only one correspondence between the parse tree and the secondary structure.\n\nTwo types of ambiguities can be distinguished. Parse tree ambiguity and structural ambiguity. Structural ambiguity does not affect thermodynamic approaches as the optimal structure selection is always on the basis of lowest free energy scores. Parse tree ambiguity concerns the existence of multiple parse trees per sequence. Such an ambiguity can reveal all possible base-paired structures for the sequence by generating all possible parse trees then finding the optimal one. In the case of structural ambiguity multiple parse trees describe the same secondary structure. This obscures the CYK algorithm decision on finding an optimal structure as the correspondence between the parse tree and the structure is not unique. Grammar ambiguity can be checked for by the conditional-inside algorithm.\n\nA probabilistic context free grammar consists of terminal and nonterminal variables. Each feature to be modeled has a production rule that is assigned a probability estimated from a training set of RNA structures. Production rules are recursively applied until only terminal residues are left.\n\nA starting non-terminal formula_14 produces loops. The rest of the grammar proceeds with parameter formula_15 that decide whether a loop is a start of a stem or a single stranded region and parameter formula_16 that produces paired bases.\n\nThe formalism of this simple PCFG looks like:\n\nThe application of PCFGs in predicting structures is a multi-step process. In addition, the PCFG itself can be incorporated into probabilistic models that consider RNA evolutionary history or search homologous sequences in databases. In an evolutionary history context inclusion of prior distributions of RNA structures of a structural alignment in the production rules of the PCFG facilitates good prediction accuracy.\n\nA summary of general steps for utilizing PCFGs in various scenarios:\n\nSeveral algorithms dealing with aspects of PCFG based probabilistic models in RNA structure prediction exist. For instance the inside-outside algorithm and the CYK algorithm. The inside-outside algorithm is a recursive dynamic programming scoring algorithm that can follow expectation-maximization paradigms. It computes the total probability of all derivations that are consistent with a given sequence, based on some PCFG. The inside part scores the subtrees from a parse tree and therefore subsequences probabilities given an PCFG. The outside part scores the probability of the complete parse tree for a full sequence. CYK modifies the inside-outside scoring. Note that the term 'CYK algorithm' describes the CYK variant of the inside algorithm that finds an optimal parse tree for a sequence using a PCFG. It extends the actual CYK algorithm used in non-probabilistic CFGs.\n\nThe inside algorithm calculates formula_20 probabilities for all formula_21 of a parse subtree rooted at formula_22 for subsequence formula_23. Outside algorithm calculates formula_24 probabilities of a complete parse tree for sequence from root excluding the calculation of formula_23. The variables and refine the estimation of probability parameters of an PCFG. It is possible to reestimate the PCFG algorithm by finding the expected number of times a state is used in a derivation through summing all the products of and divided by the probability for a sequence given the model formula_11. It is also possible to find the expected number of times a production rule is used by an expectation-maximization that utilizes the values of and . The CYK algorithm calculates formula_27 to find the most probable parse tree formula_28 and yields formula_29.\n\nMemory and time complexity for general PCFG algorithms in RNA structure predictions are formula_30 and formula_31 respectively. Restricting a PCFG may alter this requirement as is the case with database searches methods.\n\nCovariance models (CMs) are a special type of PCFGs with applications in database searches for homologs, annotation and RNA classification. Through CMs it is possible to build PCFG-based RNA profiles where related RNAs can be represented by a consensus secondary structure. The RNA analysis package Infernal uses such profiles in inference of RNA alignments. The Rfam database also uses CMs in classifying RNAs into families based on their structure and sequence information.\n\nCMs are designed from a consensus RNA structure. A CM allows indels of unlimited length in the alignment. Terminals constitute states in the CM and the transition probabilities between the states is 1 if no indels are considered. Grammars in a CM are as follows:\n\nThe model has 6 possible states and each state grammar includes different types of secondary structure probabilities of the non-terminals. The states are connected by transitions. Ideally current node states connect to all insert states and subsequent node states connect to non-insert states. In order to allow insertion of more than one base insert states connect to themselves.\n\nIn order to score a CM model the inside-outside algorithms are used. CMs use a slightly different implementation of CYK. Log-odds emission scores for the optimum parse tree - formula_38 - are calculated out of the emitting states formula_39. Since these scores are a function of sequence length a more discriminative measure to recover an optimum parse tree probability score- formula_40 - is reached by limiting the maximum length of the sequence to be aligned and calculating the log-odds relative to a null. The computation time of this step is linear to the database size and the algorithm has a memory complexity of formula_41.\n\nThe KH-99 algorithm by Knudsen and Hein lays the basis of the Pfold approach to predicting RNA secondary structure. In this approach the parameterization requires evolutionary history information derived from an alignment tree in addition to probabilities of columns and mutations. The grammar probabilities are observed from a training dataset.\n\nIn a structural alignment the probabilities of the unpaired bases columns and the paired bases columns are independent of other columns. By counting bases in single base positions and paired positions one obtains the frequencies of bases in loops and stems.\nFor basepair and an occurrence of formula_42 is also counted as an occurrence of formula_43. Identical basepairs such as formula_44 are counted twice.\n\nBy pairing sequences in all possible ways overall mutation rates are estimated. In order to recover plausible mutations a sequence identity threshold should be used so that the comparison is between similar sequences. This approach uses 85% identity threshold between pairing sequences. \nFirst single base positions differences -except for gapped columns- between sequence pairs are counted such that if the same position in two sequences had different bases the count of the difference is incremented for each sequence.\n\nFor unpaired bases a 4 X 4 mutation rate matrix is used that satisfies that the mutation flow from X to Y is reversible:\nFor basepairs a 16 X 16 rate distribution matrix is similarly generated.\nThe PCFG is used to predict the prior probability distribution of the structure whereas posterior probabilities are estimated by the inside-outside algorithm and the most likely structure is found by the CYK algorithm.\n\nAfter calculating the column prior probabilities the alignment probability is estimated by summing over all possible secondary structures. Any column in a secondary structure formula_48 for a sequence of length such that formula_49 can be scored with respect to the alignment tree and the mutational model . The prior distribution given by the PCFG is formula_50. The phylogenetic tree, can be calculated from the model by maximum likelihood estimation. Note that gaps are treated as unknown bases and the summation can be done through dynamic programming. \n\nEach structure in the grammar is assigned production probabilities devised from the structures of the training dataset. These prior probabilities give weight to predictions accuracy. The number of times each rule is used depends on the observations from the training dataset for that particular grammar feature. These probabilities are written in parenthesis in the grammar formalism and each rule will have a total of 100%. For instance:\n\nGiven the prior alignment frequencies of the data the most likely structure from the ensemble predicted by the grammar can then be computed by maximizing formula_58 through the CYK algorithm. The structure with the highest predicted number of correct predictions is reported as the consensus structure.\n\nPCFG based approaches are desired to be scalable and general enough. Compromising speed for accuracy needs to as minimal as possible. Pfold addresses the limitations of the KH-99 algorithm with respect to scalability, gaps, speed and accuracy. \n\nWhereas PCFGs have proved powerful tools for predicting RNA secondary structure, usage in the field of protein sequence analysis has been limited. Indeed, the size of the amino acid alphabet and the variety of interactions seen in proteins make grammar inference much more challenging. As a consequence, most applications of formal language theory to protein analysis have been mainly restricted to the production of grammars of lower expressive power to model simple functional patterns based on local interactions. Since protein structures commonly display higher-order dependencies including nested and crossing relationships, they clearly exceed the capabilities of any CFG. Still, development of PCFGs allows expressing some of those dependencies and providing the ability to model a wider range of protein patterns.\n\nOne of the main obstacles in inferring a protein grammar is the size of the alphabet that should encode the 20 different amino acids. It has been proposed to address this by using physico-chemical properties of amino acids to reduce significantly the number of possible combinations of right side symbols in production rules: 3 levels of a quantitative property are utilised instead of the 20 amino acid types, e.g. small, medium or large van der Waals volume. Based on such a scheme, PCFGs have been produced to generate both binding site and helix-helix contact site descriptors. A significant feature of those grammars is that analysis of their rules and parse trees can provide biologically meaningful information.\n\n\n"}
{"id": "1065575", "url": "https://en.wikipedia.org/wiki?curid=1065575", "title": "Robert A. Rushworth", "text": "Robert A. Rushworth\n\nRobert Aitken \"Bob\" Rushworth (October 9, 1924 – March 18, 1993) was an American United States Air Force major general, World War II, Korean War and Vietnam War pilot, mechanical and aeronautical engineer, and Air Force test pilot for the North American X-15 program.\n\nRushworth was born in Madison, Maine on October 9, 1924. He attended Madison Memorial High School, where he was a class president for four years, graduating in 1942. After attending Hebron Academy (prep school) from which he graduated in June 1943, and joining the United States Army Air Forces, he studied mechanical engineering at the University of Maine, receiving a Bachelor of Engineering degree in 1951. He also received a Bachelor of Science degree in aeronautical engineering from the U.S. Air Force Institute of Technology in 1954. In 1967, he graduated from the National War College in Washington D.C.\n\nIn September 1944, Rushworth earned his pilot wings and a second lieutenant commission following his aviation cadet training program graduation. His first assignment was with the 12th Combat Cargo Squadron in February 1945, in the China-Burma-India Theater of Operations, where he flew C-47 Skytrain transport combat missions from India throughout the Burma Campaign and C-46 Commandos over the \"Hump\" in the Himalaya Mountains to Shanghai and Beijing. After five years with the Reserve and Air National Guard, during which he received a bachelor's degree from the University of Maine, Rushworth was recalled to active duty in 1951. During Korean War, Rushworth served as a F-80C Shooting Star pilot with the 49th Fighter-Interceptor Squadron at Dow Air Force Base.\n\nFollowing his graduation from the U.S. Air Force Institute of Technology in 1954, Rushworth stayed at Wright-Patterson Air Force Base in Dayton, Ohio. Among his duties was to serve at the Directorate of Flight and All-Weather Testing. There he specialized in the development and flight testing of experimental automatic flight control systems. He graduated from the Air Force Experimental Flight Test Pilot School (Class 56C) in 1957, flying F-101 Voodoos, F-102 Delta Daggers, F-104 Starfighters, F-105 Thunderchiefs, F-106 Delta Darts and other jet fighters. Rushworth was selected for the X-15 program in 1958. He made his first flight on November 4, 1960. Over the next six years, he made 34 flights in the X-15, the most of any pilot. This included a flight to an altitude of 285,000 feet, made on June 27, 1963. This flight above 50 miles qualified Rushworth for Astronaut Wings, though he would have attained that honor sooner had the USAF Man In Space Soonest project proceeded according to plan.\n\nOn a later X-15 flight, he was awarded a Distinguished Flying Cross for successfully landing a North American X-15 after its nose wheel extended while flying at nearly Mach 5. He made his final X-15 flight on July 1, 1966, then returned to regular Air Force duties. He attended F-4 Phantom II Combat Crew Training, and in March 1968 he was sent to Cam Ranh Bay Air Base, Republic of Vietnam, where he was Assistant Deputy Commander for Operations with the 12th Tactical Fighter Wing and flew 189 combat missions.\n\nFollowing his return from Vietnam, from April 1969 to January 1971, he was program director of the AGM-65 Maverick program and in February 1971 became Commander of the newly organized 4950th Test Wing, Aeronautical Systems Division at Wright-Patterson Air Force Base, Ohio. Rushworth then served as Inspector General, Air Force Systems Command, Andrews Air Force Base, Maryland, from May 1973 to February 1974. He also served as the Commander of the Air Force Flight Test Center at Edwards Air Force Base, California, where his responsibilities included the major test programs, including the F-5, A-10, F-15, YF-16, YF-17 and B-1, and as the Commander of the Air Force Test and Evaluation Center at Kirtland Air Force Base, New Mexico.\n\nAt the time of his retirement as a Major General, he was Vice Commander, Aeronautical Systems Division, Air Force Systems Command, at Wright-Patterson Air Force Base, Ohio, where he dealt directly with senior deputies and managers and assisted the management of major acquisition programs such as the F-5, A-10, F-15, F-16 and B-1 as well as numerous modernization programs like the B-52 and C-5. He held this position since October 1976.\n\nRushworth was a member of the Society of Experimental Test Pilots, and in 1975 received the SETP's James H. Doolittle Award for \"outstanding accomplishment in technical management or engineering achievement in aerospace technology\". Rushworth retired on June 1, 1981.\n\nHe is rated a Command Pilot Astronaut and has more than 6,500 flying hours in more than 50 different types of aircraft.\n\nRushworth was married with Joyce Butler, and they had one daughter, Cheri (born March 29, 1957). He died of heart attack in Camarillo, California on March 18, 1993, at the age of 68.\n\n\n\n\n"}
{"id": "7979565", "url": "https://en.wikipedia.org/wiki?curid=7979565", "title": "Serving Through Science", "text": "Serving Through Science\n\nServing Through Science was \"the first regular science-related network series\" and the first educational television series broadcast in the United States. \n\nThe series premiered on the DuMont Television Network on August 15, 1946, and was shown Tuesdays at 9 pm ET. The weekly program starred Dr. Miller McClintock showing short films produced by Encyclopædia Britannica, and was sponsored by U. S. Rubber.\n\nThe last show aired May 27, 1947.\n\nThe series' name was also a slogan used by the sponsor in its advertising.\n\n\n\n"}
{"id": "2002473", "url": "https://en.wikipedia.org/wiki?curid=2002473", "title": "Solid-state nuclear magnetic resonance", "text": "Solid-state nuclear magnetic resonance\n\nSolid-state NMR (SSNMR) spectroscopy is a kind of nuclear magnetic resonance (NMR) spectroscopy, characterized by the presence of anisotropic (directionally dependent) interactions.\n\nA spin interacts with a magnetic or an electric field. Spatial proximity and/or a chemical bond between two atoms can give rise to interactions between nuclei. In general, these interactions are orientation dependent. In media with no or little mobility (e.g. crystals, powders, large membrane vesicles, molecular aggregates), anisotropic interactions have a substantial influence on the behaviour of a system of nuclear spins. In contrast, in a classical liquid-state NMR experiment, Brownian motion leads to an averaging of anisotropic interactions. In such cases, these interactions can be neglected on the time-scale of the NMR experiment.\n\nTwo directionally dependent interactions commonly found in solid-state NMR are the \"chemical shift anisotropy\" (CSA) and the internuclear \"dipolar coupling\". Many more such interactions exist, such as the anisotropic J-coupling in NMR, or in related fields, such as the \"g\"-tensor in electron spin resonance. In mathematical terms, all these interactions can be described using the same formalism.\n\nAnisotropic interactions modify the nuclear spin energy levels (and hence the resonance frequency) of all sites in a molecule, and often contribute to a line-broadening effect in NMR spectra. However, there is a range of situations when their presence can either not be avoided, or is even particularly desired, as they encode structural parameters, such as orientation information, on the molecule of interest.\nHigh-resolution conditions in solids (in a wider sense) can be established using magic angle spinning (MAS), macroscopic sample orientation, combinations of both of these techniques, enhancement of mobility by highly viscous sample conditions, and a variety of radio frequency (RF) irradiation patterns. While the latter allows decoupling of interactions in spin space, the others facilitate averaging of interactions in real space. In addition, line-broadening effects from microscopic inhomogeneities can be reduced by appropriate methods of sample preparation.\n\nUnder decoupling conditions, isotropic interactions can report on the local structure, e.g. by the isotropic chemical shift. In addition, decoupled interactions can be selectively re-introduced (\"recoupling\"), and used, for example, for controlled de-phasing or transfer of polarization to derive a number of structural parameters.\n\nThe residual line width (full width at half max) of C nuclei under MAS conditions at 5–15 kHz spinning rate is typically in the order of 0.5–2 ppm, and may be comparable to solution-state NMR conditions. Even at MAS rates of 20 kHz and above, however, non linear groups (not a straight line) of the same nuclei linked via the homonuclear dipolar interactions can only be suppressed partially, leading to line widths of 0.5 ppm and above, which is considerably more than in optimal solution state NMR conditions. Other interactions such as the quadrupolar interaction can lead to line widths of thousands of ppm due to the strength of the interaction. The first-order quadrupolar broadening is largely suppressed by sufficiently fast MAS, but the second-order quadrupolar broadening has a different angular dependence and cannot be removed by spinning at one angle alone. Ways to achieve isotropic lineshapes for quadrupolar nuclei include spinning at two angles simultaneously (DOR), sequentially (DAS), or through refocusing the second-order quadrupolar interaction with a two-dimensional experiment such as MQMAS or STMAS.\n\nFrom the perspective of solution-state NMR, it can be desirable to reduce motional averaging of dipolar interactions by alignment media. The order of magnitude of these residual dipolar couplings (RDCs) are typically of only a few rad/Hz, but do not destroy high-resolution conditions, and provide a pool of information, in particular on the orientation of molecular domains with respect to each other.\n\nThe dipolar coupling between two nuclei is inversely proportional to the cube of their distance. This has the effect that the polarization transfer mediated by the dipolar interaction is cut off in the presence of a third nucleus (all of the same kind, e.g. C) close to one of these nuclei. This effect is commonly referred to as dipolar truncation. It has been one of the major obstacles in efficient extraction of internuclear distances, which are crucial in the structural analysis of biomolecular structure. By means of labeling schemes or pulse sequences, however, it has become possible to circumvent this problem in a number of ways. Another way of circumventing dipolar truncation in case of rare nuclei like 13C is to study the systems at their natural isotopic abundance utilising DNP assisted solid-state NMR under magic-angle spinning, where the probability of finding a third spin is almost 100 times lower.\n\nThe chemical shielding is a local property of each nucleus, and depends on the external magnetic field.\n\nSpecifically, the external magnetic field induces currents of the electrons in molecular orbitals. These induced currents create local magnetic fields that often vary across the entire molecular framework such that nuclei in distinct molecular environments usually experience unique local fields from this effect.\n\nUnder sufficiently fast magic angle spinning, or in solution-state NMR, the directionally dependent character of the chemical shielding is removed, leaving the isotropic chemical shift.\n\nThe J-coupling or indirect nuclear spin-spin coupling (sometimes also called \"scalar\" coupling despite the fact that J is a tensor quantity) describes the interaction of nuclear spins through chemical bonds.\n\n\"Main article:\" Dipolar coupling (NMR)\n\nNuclear spins exhibit a dipole moment, which interacts with the dipole moment of other nuclei (dipolar coupling). The magnitude of the interaction is dependent on the spin species, the internuclear distance, and the orientation of the vector connecting the two nuclear spins with respect to the external magnetic field \"B\" (see figure). The maximum dipolar coupling is given by the dipolar coupling constant \"d\",\nwhere r is the distance between the nuclei, and γ and γ are the gyromagnetic ratios of the nuclei. In a strong magnetic field, the dipolar coupling depends on the orientation of the internuclear vector with the external magnetic field by\nConsequently, two nuclei with a dipolar coupling vector at an angle of θ=54.7° to a strong external magnetic field, which is the angle where D becomes zero, have zero dipolar coupling. θ is called the magic angle. One technique for removing dipolar couplings, at least to some extent, is magic angle spinning.\n\nNuclei with a spin greater than one-half have a non spherical charge distribution. This is known as a quadrupolar nucleus. A non spherical charge distribution can interact with an electric field gradient caused by some form of non-symmetry (e.g. in a trigonal bonding atom there are electrons around it in a plane, but not above or below it) to produce a change in the energy level in addition to the Zeeman effect. The quadrupolar interaction is the largest interaction in NMR apart from the Zeeman interaction and they can even become comparable in size.\nDue to the interaction being so large it can not be treated to just the first order, like most of the other interactions. This means you have a first and second order interaction, which can be treated separately. The first order interaction has an angular dependency with respect to the magnetic field of formula_3 (the P2 Legendre polynomial), this means that if you spin the sample at formula_4 (~54.74°) you can average out the first order interaction over one rotor period (all other interactions apart from Zeeman, Chemical shift, paramagnetic and J coupling also have this angular dependency). However, the second order interaction depends on the P4 Legendre polynomial, which has zero points at 30.6° and 70.1°. These can be taken advantage of by either using DOR (DOuble angle Rotation) where you spin at two angles at the same time, or DAS (Double Angle Spinning) where you switch quickly between the two angles. Specialized hardware (probe) has been developed for such experiments. A revolutionary advance is Lucio Frydman's multiple quantum magic angle spinning (MQMAS) NMR in 1995 and it has become a routine method for obtaining high resolution solid-state NMR spectra of quadrupolar nuclei. A similar method to MQMAS is satellite transisition magic angle spinning (STMAS) NMR proposed by Zhehong Gan in 2000.\n\nParamagnetic substances are subject to the Knight shift.\n\n\"See also:\" nuclear magnetic resonance or NMR spectroscopy articles for an account on discoveries in NMR and NMR spectroscopy in general.\n\n\"History of discoveries of NMR phenomena, and the development of solid-state NMR spectroscopy:\"\n\nPurcell, Torrey and Pound: \"nuclear induction\" on H in paraffin 1945, at about the same time Bloch \"et al.\" on H in water.\n\n\"Methods and techniques\"\n\nA fundamental RF pulse sequence and building-block in most solid-state NMR experiments starts with cross-polarization (CP) [Waugh \"et al.\"]. It can be used to enhance the signal of nuclei with a low gyromagnetic ratio (e.g. C, N) by magnetization transfer from nuclei with a high gyromagnetic ratio (e.g. H), or as spectral editing method (e.g. directed N→C CP in protein spectroscopy). To establish magnetization transfer, the RF pulses applied on the two frequency channels must fulfill the Hartmann–Hahn condition [Hartmann, 1962], that is, the Larmor frequencies in both rf fields must be identical. Experimental optimization of such conditions is one of the routine tasks in performing a (solid-state) NMR experiment.\n\nCP-MAS is a basic building block of most pulse sequences in solid-state NMR spectroscopy. Given its importance, a pulse sequence employing direct excitation of H spin polarization, followed by CP transfer to and signal detection of C, N or similar nuclei, is itself often referred to as \"CP experiment\", or, in conjunction with MAS, as \"CP-MAS\" [Schaefer and Stejskal, 1976]. It is the typical starting point of an investigation using solid-state NMR spectroscopy.\n\nSpin interactions must be removed (decoupled) to increase the resolution of NMR spectra and isolate spin systems.\n\nA technique that can substantially reduce or remove the chemical shift anisotropy, the dipolar coupling is \"sample rotation\" (most commonly magic angle spinning, but also off-magic angle spinning).\n\n\"Homonuclear RF decoupling\" decouples spin interactions of nuclei that are the same as those being detected. \"Heteronuclear RF decoupling\" decouples spin interactions of other nuclei.\n\nAlthough the broadened lines are often not desired, dipolar couplings between atoms in the crystal lattice can also provide very useful information. Dipolar coupling are distance dependent, and so they may be used to calculate interatomic distances in isotopically labeled molecules.\n\nBecause most dipolar interactions are removed by sample spinning, recoupling experiments are needed to re-introduce desired dipolar couplings so they can be measured.\n\nAn example of a recoupling experiment is the Rotational Echo DOuble Resonance (REDOR) experiment\nwhich also can be the basis of an NMR crystallographic study of e.g. an amorphous solid.\n\nIn contrast to traditional approaches particular in protein NMR, in which the broad lines associated with protons effectively relegate this nucleus to mixing of magnetization, recent developments of hardware (very fast MAS) and reduction of dipolar interactions by deuteration have made protons as versatile as they are in solution NMR. This includes spectral dispersion in multi-dimensional experiments as well as structurally valuable restraints and parameters important for studying the materials' dynamics.\n\nMembrane proteins and amyloid fibrils, the latter related to Alzheimer's disease and Parkinson's disease, are two examples of application where solid-state NMR spectroscopy complements solution-state NMR spectroscopy and beam diffraction methods (e.g. X-ray crystallography, electron microscopy). Solid-state NMR structure elucidation of proteins has traditionally been based on secondary chemical shifts and spatial contacts between heteronuclei. Currently, paramagnetic contact shifts and specific proton-proton distances are also used for higher resolution and longer-range distance restraints.\n\nSolid-state NMR spectroscopy serves as an analysis tool in organic and inorganic chemistry, where is used as a valuable tool to study local dynamics, kinetics, and thermodynamics of a variety of systems.\n\nObjects of SSNMR studies in materials science are inorganic/organic aggregates in crystalline and amorphous states, composite materials, heterogeneous systems including liquid or gas components, suspensions, and molecular aggregates with dimensions on the nanoscale.\n\nIn many cases, NMR is the uniquely applicable method for measurement of porosity, particularly for porous systems containing partially filled pores or for dual-phase systems. SSNMR is one of the most effective technique for molecular-level investigation of interfaces.\n\nStudies of solids by NMR relaxation experiments are special issues based on the following general statements. The experimental decay of macroscopic transverse or longitudinal magnetization follows the exponential law for complete domination of the spin-diffusion mechanism and a single relaxation time characterizes all of the nuclei in rigid solids, even those that are not chemically or structurally equivalent. The spin-diffusion mechanism is typical of systems with nuclei experiencing strong dipolar interactions (protons, fluorine or phosphorus nuclei at relatively small concentrations of paramagnetic centers). For other nuclei with weak dipolar coupling and/or at high concentration of paramagnetic centers, relaxation can be non-exponential following a stretched exponential function, exp(–(τ/T1)) or exp(–(τ/T2)). For paramagnetic solids, the β value of 0.5 corresponds to relaxation via direct electron–nucleus dipolar interactions without spin diffusion, while intermediate values between 0.5 and 1.0 can be attributed to a diffusion-limited mechanism.\n\nNMR can also be applied to art conservation. Different salts and moisture levels can be detected through the use of solid state NMR. However, sampling sizes retrieved from works of art in order to run through these large conducting magnets typically exceed levels deemed acceptable. Unilateral NMR techniques use portable magnets that are applied to the object of interest, bypassing the need for sampling. As such, unilateral NMR techniques prove to be useful in the art conservation world.\n\n\n\"Books and major review articles\"\n\n\n\"References to books and research articles\"\n\n"}
{"id": "29650154", "url": "https://en.wikipedia.org/wiki?curid=29650154", "title": "TWINS", "text": "TWINS\n\nTwo Wide-Angle Imaging Neutral-Atom Spectrometers (TWINS) are a pair of NASA instruments aboard two United States National Reconnaissance Office satellites in Molniya orbits. TWINS was designed to provide stereo images of the Earth's ring current. The first instrument, TWINS-1, was launched aboard USA-184 on 28 June 2006. TWINS-2 followed aboard USA-200 on March 13, 2008.\n\nEach instrument consists of an energetic neutral atom imager and a Lyman alpha detector. The ENA imager provides indirect remote sensing of the ring current ions, and the Lyman alpha detector gives a measure of the neutral hydrogen cloud about the Earth, known as the geocorona. The TWINS prime mission lasted two years, from 2008 to 2010, and has been followed by an extended mission which is ongoing.\n\nLaunched as missions of opportunity aboard classified, non-NASA U.S. government spacecraft, TWINS conducts stereoscopic imaging of Earth's magnetosphere. By imaging the charge exchange neutral atoms over a broad energy range (~1-100 keV) using two identical instruments on two widely spaced high-altitude, high-inclination spacecraft, TWINS enables the 3-dimensional visualization and the resolution of large scale structures and dynamics within the magnetosphere for the first time. In contrast to traditional space experiments, which make measurements at only one point in space, imaging experiments provide simultaneous viewing of different regions of the magnetosphere. Stereo imaging, as done by TWINS, takes the next step of producing 3-D images, and provides a leap ahead in our understanding of the global aspects of the terrestrial magnetosphere.\n\nThe ENA imagers observe energetic neutrals produced from the global magnetospheric ion population, over an energy range of 1 to 100 keV with high angular (4-degree) and time (about 1-minute) resolution. A Lyman-alpha geocoronal imager is used to monitor cold exospheric hydrogen atoms that produce ENAs from ions via charge exchange. Complementing these imagers are detectors that measure the local charged particle environment around the spacecraft.\n\nThe offset in the orbital phases (apogees at different times) of TWINS 1 and TWINS 2 means that in addition to stereo ENA imaging for several hours twice per day, the two TWINS instruments also obtain essentially continuous magnetospheric observations.\n\nThe TWINS instrumentation is essentially the same as the MENA instrument on the IMAGE spacecraft. This instrumentation consists of a neutral atom imager covering the ~1-100 keV energy range with 4°x4° angular resolution and 1-minute time resolution, and a simple Lyman-alpha imager to monitor the geocorona.\n\nTWINS provides stereo imaging of the Earth's magnetosphere, the region surrounding the planet controlled by its magnetic field and containing the Van Allen radiation belts and other energetic charged particles. TWINS enables three-dimensional global visualization of this region, leading to greatly enhanced understanding of the connections between different regions of the magnetosphere and their relation to the solar wind.\n\nRoutine stereo imaging by TWINS began on 15 June 2008, during an extremely weak geomagnetic storm whose Dst index never fell below -40 nT, as compared to a nominal Dst of -100 nT for classification as a storm. During the TWINS prime mission (2008–2010), an extended and unprecedented solar minimum (from solar cycle 23) prevailed, bringing with it very calm magnetospheric conditions ranging from dead quiet to mildly disturbed. During this time period TWINS observed numerous weak storms, roughly once every 27 days (corresponding to the solar rotation period and triggered by solar corotating interaction regions (CIRs). The strongest storm (which was still very mild) observed by TWINS during its prime mission was on 22 July 2009, with Dst reaching a moderate -79 nT. Throughout these extended quiet conditions TWINS images contained ENA signals from both high-altitude (ring current) and low-altitude emission (LAE) regions.\n\nThe TWINS Mission of Opportunity maintains a library of selected storm events.\n\n\n"}
{"id": "20935393", "url": "https://en.wikipedia.org/wiki?curid=20935393", "title": "Tropenhaus Frutigen", "text": "Tropenhaus Frutigen\n\nThe Tropenhaus () in Frutigen, Switzerland, is a commercial project using geothermal energy from hot water flowing out of the Lötschberg base tunnel for the production of exotic fruit, sturgeon meat and caviar in a tropical greenhouse in the Swiss alps. In 2007, the project received the \"Prix Evenir\", the Swiss petroleum industry's CHF 50,000 award for sustainable development.\n\nThe idea for the greenhouse was born in 2002 when it became apparent that the water continuously flowing out of the Lötschberg Base Tunnel could not be directly diverted to the local river, the Kander, as its temperature of would disrupt the biological rhythm of the endangered trout there. Rather than cooling the water artificially, wasting its thermal energy, tunnel engineers founded a start-up company to use the warm water to heat a greenhouse. Construction of the site, which started in May 2008 at a cost of CHF 28 million, was due to be completed at the end of 2009.\n\nA sturgeon farm, one of few in Europe, is the heart of the \"Tropenhaus\". Some 60,000 fish are intended to be grown in 40 outdoor basins. The sturgeons thrive in permanent Siberian summer conditions and are intended to yield 20 tonnes of meat as well as two tonnes of caviar annually. The first sturgeon fillets were sold in local stores in November 2008. The rest of the greenhouses are dedicated to the production of exotic fruit, such as banana, papaya, mango and guava, of which about 10 tons are intended to be grown annually in an area of .\n\nThe \"Tropenhaus\" is also intended to be a tourist destination, with a visitors' centre, a visitors' trail through the installation, a restaurant, and an exhibition room (paid for by a Bernese energy company) showcasing the project's use of renewable energy and sustainability. It is located some or 7 minutes' walk from Frutigen railway station.\n\n"}
{"id": "379609", "url": "https://en.wikipedia.org/wiki?curid=379609", "title": "Vaquita", "text": "Vaquita\n\nThe vaquita (; \"Phocoena sinus\") is a species of porpoise endemic to the northern part of the Gulf of California that is on the brink of extinction. Based on beached skulls found in 1950 and 1951, the scientific description of the species was published in 1958. As of March 2018 only about 12-15 individuals remain. The word \"vaquita\" is Spanish for \"little cow\". Other names include cochito (Spanish for \"little pig\"), desert porpoise, vaquita porpoise, Gulf of California harbor porpoise, Gulf of California porpoise, and gulf porpoise. Since the baiji (\"Lipotes vexillifer\") is thought to have gone extinct in 2006, the vaquita has taken on the title of the most endangered cetacean in the world. It has been listed as critically endangered since 1996. The population was estimated at 600 in 1997, below 100 in 2014, approximately 60 in 2015, around 30 in November 2016, and only 12-15 in March 2018, leading to the conclusion that the species will soon be extinct unless drastic action is taken.\n\nThe population decrease is largely attributed to bycatch from the illegal gillnet fishery for the totoaba, a similarly sized endemic drum that is also critically endangered. The population decline has occurred despite an investment of tens of millions of dollars by the Mexican government in efforts to eliminate the bycatch. A partial gillnet ban was put in place for two years in May 2015; its scheduled expiration at the end of May 2017 spurred a campaign to have it extended and strengthened. On 7 June 2017, an agreement was announced by Mexican president Enrique Peña Nieto to make the gillnet ban permanent and strengthen enforcement. As well as the Mexican government and various environmental organizations, this effort will now also involve the foundations of Mexican businessman Carlos Slim and American actor and environmental activist Leonardo DiCaprio.\n\nA protective housing/captive breeding program, unprecedented for a marine mammal, has been developed and is undergoing feasibility testing, being now viewed as necessary to rescue the species. However, the sea pen housing needed to implement this strategy is not expected to be available until October 2017, which is feared may be too late. Additionally, the ability of the vaquita to survive and reproduce while confined to a sanctuary is uncertain. The Mexican government approved the plan on 3 April 2017, with commencement projected to begin in October 2017.\nIn November 2017, the attempt to capture wild vaquitas for captive breeding and safekeeping was suspended following the death of a female vaquita. The adult female died within hours of being captured. In December 2017, Mexico, the United States and China agreed to take further steps to prevent trade in totoaba bladders. However, the intensifying poaching and the extremely low population make it likely that the species will go extinct unless drastic measures are taken. If the species does go extinct, it will likely be the first cetacean to do so since the baiji.\n\nVaquitas are the smallest and most endangered species of the infraorder Cetacea and are endemic to the northern end of the Gulf of California. The vaquita is somewhat stocky and has a characteristic porpoise shape. The species is distinguishable by the dark rings surrounding their eyes, patches on their lips, and a line that extends from their dorsal fins to their mouths. Their backs are a dark grey that fades to white undersides. As vaquitas mature, the shades of grey lighten. Female vaquitas tend to grow larger than males. On average, females mature to a length of , compared to for males. The lifespan, pattern of growth, seasonal reproduction, and testes size of the vaquita are all similar to that of the harbour porpoise. The flippers are proportionately larger than those of other porpoises, and the fin is taller and more falcated. The skull is smaller and the rostrum is shorter and broader than in other members of the genus.\n\nVaquitas use high-pitched sounds to communicate with one another and for echolocation to navigate through their habitats. They generally feed and swim at a leisurely pace. Vaquitas avoid boats and are very evasive. They rise to breathe with a slow, forward motion and then disappear quickly. This lack of activity at the surface makes them difficult to observe. Vaquitas are usually alone unless they are accompanied by a calf, meaning they are less social than other porpoise species. They may also be more competitive during mating season. They are the only species belonging to the porpoise family that live in warm waters. Vaquitas are non-selective predators.\n\nLike other \"Phocoena\", vaquitas are usually seen singly. If they are seen together, it is usually in small groups of two or three individuals. Less often, groups around ten have been observed, with the most ever seen at once being 40 individuals.\n\nVaquitas tend to forage near lagoons. All of the 17 fish species found in vaquita stomachs can be classified as demersal and or benthic species inhabiting relatively shallow water in the upper Gulf of California. Vaquitas appear to be rather non-selective feeders on crustaceans, small fish, octopuses and squid in this area. Some of the most common prey are teleosts (fish with bony skeletons) such as grunts, croakers, and sea trout. Like other cetaceans, vaquitas may use echolocation to locate prey, particularly as their habitat is often turbid.\n\nLittle is known about the life cycle of vaquitas. Age at sexual maturity, longevity, reproductive cycle and population dynamics estimates have been made, but further research is needed. Most of these estimates come from vaquitas that have been stranded or caught in nets. Some are based on other porpoise species similar to vaquitas.\n\nVaquitas are estimated to live about 20 years in ideal conditions. They mature sexually at 1.3 m long, as early as 3 years old, but more likely at 6. Reproduction occurs during late spring or early summer. Their gestation period is between 10 and 11 months. They have seasonal reproduction, and usually have one calf in March. The inter-birth period, or elapsed time between offspring birth, is between 1 and 2 years. The young are then nursed for about 6 to 8 months until they are capable of fending for themselves.\n\nThe habitat of the vaquita is restricted to the northern area of the Gulf of California, or Sea of Cortez. They live in shallow, murky lagoons along shorelines. They rarely swim deeper than and are known to survive in lagoons so shallow that their backs protrude above the surface. The vaquita is most often sighted in water deep, from the coast, over silt and clay bottoms. They tend to choose habitats with turbid waters, because they have high nutrient content, which is important because it attracts the small fish, squid, and crustaceans on which they feed. They are able to withstand the significant temperature fluctuations characteristic of shallow, turbid waters and lagoons.\n\nThe vaquita is considered the most endangered of 129 extant marine mammal species. It has been classified as one of the top 100 evolutionary distinct and globally endangered (EDGE) mammals in the world. The vaquita is an evolutionarily distinct animal and has no close relatives. These animals represent more, proportionally, of the tree of life than other species, meaning they are top priority for conservation campaigns. The EDGE of Existence Programme is a conservation effort that attempts to help conserve endangered animals that represent large portions of their evolutionary trees. The U.S. government has listed the vaquita as endangered under the Endangered Species Act. It is also listed by the IUCN and the CITES in the category at most critical risk of extinction.\n\nVaquitas have never been hunted directly, but their population is declining, largely because of animals becoming trapped in illegal gillnets intended for capturing the totoaba, a large critically endangered fish of the drum family endemic to the Gulf. A trade in totoaba swim bladders has arisen, driven by demand from China (where they are used in soup, being considered a delicacy and also erroneously thought to have medicinal value), which is greatly exacerbating the problem.\n\nEstimates placed the vaquita population at 567 in 1997. Estimates in the 2000s ranged between 150 to 300.\n\nWith their population dropping as low as 85 individuals in 2014, inbreeding depression has probably begun to affect the fitness of the species, potentially contributing to the population's further decline.\n\nIn 2014, estimates of the species' abundance dropped below 100 individuals. An international vaquita recovery team concluded that the population is decreasing at a rate of 18.5% per year, and \"the species will soon be extinct unless drastic steps are taken immediately.\" Their report recommended that a ban on gillnet fishing be enforced throughout the range of the vaquita, that action be taken to eliminate the illegal fishery for the totoaba, and that with help from the U.S. and China, trade in totoaba swim bladders be halted.\n\nOn 16 April 2015, Enrique Peña Nieto, President of Mexico, announced a program to conserve and protect the vaquita and the similar-sized totoaba, including a two-year ban on gillnet fishing in the area, patrols by the Mexican Navy and financial support to fishermen impacted by the plan. However, some commentators believe the measures fall short of what is needed to ensure the species' survival.\n\nIn early May 2016, the IUCN SSC – Cetacean Specialist Group reported that the vaquita population had dipped to around 60 remaining individuals in 2015. This represents a 92% decline from the 1997 population level. In March 2016 alone, at least three vaquitas drowned after being entangled in gillnets set for totoaba. The report concluded that the gillnet ban would need to be extended indefinitely, with more effective enforcement, if the vaquita is to have any chance of long term survival. Otherwise, the species is likely to become extinct within 5 years.\n\nBy November 2016, according to a report released in February 2017, the population had declined to about 30, and it was judged that capture of some of the remaining vaquitas and conducting a captive breeding program within a secure sanctuary was the only remaining hope for survival. This is despite the fact that porpoises generally fare poorly in captivity. However, the head of the Mexican environmental agency asserted in July 2017 that at least 100 individuals remain.\n\nBy March 2018, an interview with the watchdog group Elephant Action League revealed that based on recordings of vaquita calls from multiple sources, there were likely only a dozen remaining vaquita in the region. According to the interview, despite the recent efforts to curb poaching, dozens of poachers have still been seen fishing every night. It remains unlikely that the population will survive the next totoaba fishing season, which began around the same time the interview was released. In response to the endangerment of the vaquita, a federal judge ordered President Donald Trump to ban the import of gillnet-harvested seafood from the Gulf of California into the United States later in the year.\n\nA survey later in 2018 sighted 6-7 vaquita, possibly about half of the species' current population. However, renewing hopes for the species was the sighting of \"Ana\", a female vaquita previously seen with a newborn calf in 2017. \"Ana\" was also seen with another calf in the 2018 survey, indicating that the small population still has the ability to sustain itself, and that the reproduction rate of the species may be annual rather than biennial as thought before. However, acoustic studies have indicated that only about 15 individuals still exist in a very small rectangular area about 12 by 25 miles; a reduction of about 86% of the species' historic range.\n\nAccidental drowning in gillnets set by fishermen meant for catching totoaba is the primary cause of anthropogenic, incidental mortality for the vaquita. Three fishing villages in the northern Gulf of California are primarily involved in the totoaba fishery and, as a result, most directly involved in threats to the vaquita. San Felipe, in Baja California, and Golfo de Santa Clara and Puerto Peñasco, in Sonora, have a total population of approximately 61,000. Up to 80% of the economy in these towns is associated with the fishing industry. A total of 1771 vessels make up the artisanal fleet that have permits to fish with nets, with the total size of the commercial fishery unknown due to the extent of the black market for totoaba. Around 3,000 individuals are involved in the totoaba industry overall. The total economic impact of the industry for the region is estimated to be approximately $5.4 million USD annually, or $78.5 million Pesos. Socioeconomic surveys of the northern Gulf have suggested that approximately $25 million, if invested in the region through education, equipment buyout, and job placement, could end the vaquita bycatch problem.\n\nStudies performed in El Golfo de Santa Clara, one of the three major ports in which vaquitas live, indicated that gillnet fishing caused about 39 vaquita deaths a year in the late 1990s. This was close to 17% of the whole vaquita population within this port. While these results were not taken from the entire range of habitat in which vaquitas live, it is reasonable to assume that these results can be applied to the whole vaquita population, and in fact may even be a little low. Even with a gillnet ban throughout the vaquita refuge area, which contains 50% of the vaquita's habitat, the population is still in decline, which suggests a complete ban of gillnet use may be the only solution to saving the vaquita population. However, even in the face of all-encompassing gillnet bans, a significant number of Mexican fishermen in El Golfo de Santa Clara continue to use the nets. As many as a third of the area's fishermen are thought to still be using gillnets despite the imposition of bans on their use. Trawl nets commonly used to catch shrimp in the area may also present threats due to their impacts on the Gulf's ecosystem, either directly through bycatch or by indirectly altering the seafloor and associated species (including vaquita prey).\n\nOther potential threats to the vaquita population include habitat alterations and pollutants. The habitat of the vaquita is small and the food supply in marine environments is affected by water quality and nutrient levels. The damming of the upper Colorado River has reduced the flow of fresh water into the gulf, though there is no empirical evidence that the reduced flow from the Upper Colorado River has posed an immediate short-term risk to the species. In addition, the use of chlorinated pesticides may also have a detrimental effect. Despite these possible problems, most of the recovered bodies of vaquitas show no signs of emaciation or environmental stressors, implying that the decline is due almost solely to bycatch. However, these additional hazards may pose a long-term threat.\n\nA 2018 interview indicated that the illegal fishermen may be waiting for the species to go extinct in order to fish with fewer restrictions.\n\nThough the major cause of vaquita porpoise mortality is bycatch in gillnets, as numbers continue to dwindle, new problems will arise that will tend to make recovery more difficult. One such problem is reduced breeding rates. With fewer individuals in the habitat, less contact will occur between the sexes and consequently less reproduction. This may be followed by increased inbreeding and reduced genetic variability in the gene pool, following the bottleneck effect.\n\nWhen inbreeding depression occurs, the population experiences reduced fitness because deleterious recessive genes can manifest in the population. In small populations where genetic variability is low, individuals are more genetically similar. When the genomes of mating pairs are more similar, recessive traits appear more often in offspring. The more related two individuals are in the breeding pair, the more deleterious homozygous genes the offspring will likely have which can greatly lower fitness in the offspring. These secondary impacts of dwindling vaquita numbers are not necessarily a threat yet, but they will become problematic if the population continues to decline. In addition, because porpoise population growth rates are generally low, the vaquita population is unlikely to recover rapidly even after the removal of anthropogenic risk factors to their survival. By some estimates, the maximum potential growth rate for the species is under 4%. However, the sighting of a female with a newborn calf in 2017 and the same female with another newborn in 2018 indicates that the species may have a relatively faster annual growth rate.\n\nRemoval of the vaquita will have a significant ecological impact on the northern Gulf of California. The Gulf of California is considered a large marine ecosystem, due to its high species diversity and large habitat size. With such biodiversity in the region, it is important to consider the potentially harmful effects of drops in the vaquita population on seemingly unrelated species due to apparent competition.\n\nSharks have been determined to be the only predators of vaquitas. Because of its limited number of predator species, the vaquita population is sensitive to small changes in predation from sharks. Although the vaquita accounts for only a small percentage of the diets of sharks in the region, extinction of the vaquita could potentially cause negative effects on shark population sizes. Extinction of the vaquita may also impact the vaquita prey populations in the northern Gulf ecosystem. The disappearance of the vaquita could lead to potential over-population of their prey species such as benthic fishes, squid, and crustaceans.\n\nConservation efforts for the vaquita are mainly focused on fishing restrictions to prevent their bycatch. These fishing restrictions could prove beneficial for the fish in the upper Gulf, as well as the vaquita. As a result of increased restrictions on gillnet use, the populations of the targeted fish and shrimp species will receive protection from overfishing. Historically, numerous commercially fished species have experienced devastating impacts due to overfishing, and the vaquita conservation program may lessen the severity of such devastation in the future. Another solution to prevent vaquita bycatch might be to redesign fishing nets, which could be used to effectively catch fish, but leave the vaquita untouched.\n\nBecause vaquitas are endemic to the Gulf of California, Mexico is leading conservation efforts with the creation of the International Committee for the Recovery of the Vaquita (CIRVA), which has tried to prevent the accidental deaths of vaquitas by outlawing the use of fishing nets within the vaquita's habitat. CIRVA has worked with the CITES, the ESA, and the Marine Mammal Protection Act to make a plan to nurse the vaquita population back to a point at which they can sustain themselves. CIRVA concluded in 2000 that between 39 and 84 individuals are killed each year by such gillnets. To try to prevent extinction, the Mexican government has created a nature reserve covering the upper part of the Gulf of California and the Colorado River delta. CIRVA recommends that this reserve be extended southwards to cover the full known area of the vaquita's range and that trawlers be completely banned from the reserve area.\n\nOn 28 October 2008, Canada, Mexico, and the United States launched the North American Conservation Action Plan (NACAP) for the vaquita, under the jurisdiction of the Commission for Environmental Cooperation, a NAFTA environmental organization. The NACAP is a strategy to support Mexico's efforts to recover the vaquita. Also in 2008, Mexico launched the program PACE-VAQUITA, another effort to help preserve the species. PACE-VAQUITA compensates fishermen who choose one of three alternatives: rent-out, switch-out, and buy-out.\n\nIn the rent-out option, fishermen acquire temporary contractual obligations to carry out conservation efforts. They are paid if they agree to terminate their fishing inside the vaquita refuge area. There is a penalty if fishermen breach the contract which includes getting their vessels taken by the government. The switch-out option provides fishermen with compensation for switching to vaquita-safe harvesting technology. Finally, the buy-back program compensates fisherman for permanently turning in their fishing permits, as well as their respective gear. In 2008, because of how few fisherman were enrolling in the switch-out option, PACE Vaquita added a yearly, short-term option for fishermen, letting them simply rent the vaquita-safe fishing equipment yearly for compensation. Then, in 2010, this option was broken down even further, giving fishermen the option of buying the vaquita-safe net, or paying the yearly rent, but for less compensation. Despite these efforts, the probability that these attempts at conservation will work is slim. Only about a third of fishermen in the area have accepted these terms so far. Some fishermen continue to fish in the protected areas despite the economic alternatives. Even measuring the population size of the vaquita will be difficult as the rarity of the vaquita bycatch will make it difficult to demonstrate the difference these programs are making.\n\nIn November 2014, Greenpeace UK launched a campaign urging its members to write to President Peña Nieto to extend the vaquita reserve to the full range of the species, as well as commence dialogue with the Chinese and US over the commercial transport and consumption of products from species that threaten the vaquita's future, such as the similarly sized totoaba fish which is used in Chinese medicine.\n\nIn May 2015 Mexico authorized an emergency partial gillnet ban (which did not extend to the legal fishery for the curvina, \"Cynoscion othonopterus\") in the area of the vaquita's habitat, in an attempt to halt the decline in population. In December 2015, Sea Shepherd Conservation Society launched Operation Milagro, a direct action campaign to patrol the gulf habitat to protect the endangered vaquita. Sea Shepherd partnered with the Mexican Navy in a joint effort to remove illegal nets, release trapped wildlife, obtain visual evidence of poaching in the area and conduct outreach with local communities and marine biologists. In the fall of 2016, a new international program to locate and remove illegal or abandoned fishing gear from the vaquita's range began work, finding 31 illegal gillnets in 15 days. On April 8, 2017, Sea Shepherd pulled its 200th gillnet from Mexican waters since the start of Operation Milagro III in December 2016.\n\nUnfortunately, the gillnet ban seems to have disproportionately impacted legal fisheries, and had the unintended effect of pushing more local fishermen into the illegal totaoba fishery. This was exacerbated by problems with the program intended to compensate fishermen for the economic consequences of the ban; half of those funds were given to just a few individuals, while others received nothing. This led to the annual rate of population decline increasing from ~34% before the ban to ~50% in the first year afterwards.\n\nSince these measures failed to halt the decline, by February 2017 it was judged that a program placing a portion of the remaining population in protective captivity was needed to save the species. Additional measures considered necessary were extending a permanent gillnet ban to the legal curvina fishery (which can provide cover for the illegal totoaba fishery), improving the enforcement of fisheries regulations and increasing penalties for violations, and accelerated development of alternative, vaquita-friendly fishing gear for local fishermen. The gillnet ban was scheduled to expire at the end of May 2017; as that date approached, a campaign among conservationists to extend the ban gathered force on social media, with celebrities getting involved.\n\nOn 7 June 2017, it was announced by President Peña Nieto that the gillnet ban would be extended and made permanent. There will also be newly strengthened efforts to enforce the ban and prosecute violators. To discourage skirting the rules, fishing at night will be prohibited and monitored entry and exit points will be established for fishing vessels that operate in the protected zone. The agreement was signed by the president as well as the Mexican secretaries for the environment, agriculture and navy. The foundations of Mexican businessman Carlos Slim and American actor and environmental activist Leonardo DiCaprio also pledged to support implementation of the plan.\n\nThe proposal for a captive breeding program must contend with the general greater difficulty of keeping porpoises in captivity relative to dolphins, due to porpoises' sensitivity to disturbance and stress. Success in keeping captive porpoises has only been attained in recent years. The scheme involves the use of trained dolphins of the U.S. Navy to locate the vaquitas, along with aircraft and a spotter vessel with an observation tower. Vaquitas would be captured with a light salmon gillnet. Some of these vaquitas might be satellite-tagged and released for research purposes, while others would be kept captive. The latter vaquitas would be transferred to sea pens along the shore of the gulf, with large pools on land also available for special care if needed. Once success was attained in the campaign to eliminate the threat of gillnets, captive vaquitas could then be released back into the wild.\n\nThis program, called VaquitaCPR (Vaquita Conservation, Protection, and Recovery), began capturing vaquitas from the Gulf in autumn of 2017. However, the initial two attempts resulted in the death of one vaquita. On 6 November 2017, Mexico's environmental minister announced that a female vaquita had been successfully captured and brought to an enclosure, but had died several hours later, evidently due to stress. The breeding program was closed soon after, and in February 2018, a program conceived in 2017 was funded, which would breed totoaba in three dedicated fish farms to reduce the size of the totoaba black market and thus decrease accidental vaquita killings. \n\nRecovery efforts have been deemed very slow and inadequate, with large amounts of poaching still going on, reducing the population to one dozen. Sea Shepherd and Elephant Action League are apparently the only organizations to have a constant presence in monitoring the population. \n\nBased on the extremely small range the species has been reduced to as of 2018, it may actually become easier to conserve the remaining population, although this restricted range makes it more vulnerable to illegal fishing in the case of incursion into this area. Potential suggestions to conserve the species include stationing a permanent military vessel in the area or forming a floating barrier of above-water nets to prevent illegal fishing boats from such incursions. Experts have also called on Mexico's incoming president, Andres Manuel Lopez Obrador, to put forth policies conserving the species. However, Obrador's plans for job-creation (likely including promotion of the fishing industry) may debase efforts to protect the species, and Josefa González, Obrador's pick for the Environment Department, has implied that she sees vaquita conservation as a lost cause.\n\n\n\n"}
{"id": "2448955", "url": "https://en.wikipedia.org/wiki?curid=2448955", "title": "Velocity potential", "text": "Velocity potential\n\nA velocity potential is a scalar potential used in potential flow theory. It was introduced by Joseph-Louis Lagrange in 1788.\n\nIt is used in continuum mechanics, when a continuum occupies a simply-connected region and is irrotational. In such a case,\nwhere denotes the flow velocity. As a result, can be represented as the gradient of a scalar function :\n\nA velocity potential is not unique. If is a constant, or a function solely of the temporal variable, then is also a velocity potential for . Conversely, if is a velocity potential for then for some constant, or a function solely of the temporal variable . In other words, velocity potentials are unique up to a constant, or a function solely of the temporal variable.\n\nIf a velocity potential satisfies Laplace equation, the flow is incompressible ; one can check this statement by, for instance, developing and using, thanks to the Clairaut-Schwarz's theorem, the commutation between the gradient and the laplacian operators.\n\nUnlike a stream function, a velocity potential can exist in three-dimensional flow.\n\nIn theoretical acoustics, it is often desirable to work with the acoustic wave equation of the velocity potential instead of pressure and/or particle velocity . \nSolving the wave equation for either field or field does not necessarily provide a simple answer for the other field. On the other hand, when is solved for, not only is found as given above, but is also easily found – from the (linearised) Bernoulli equation for irrotational and unsteady flow – as\n\n"}
{"id": "56950636", "url": "https://en.wikipedia.org/wiki?curid=56950636", "title": "Vicky Forster", "text": "Vicky Forster\n\nVictoria Jane (Vicky) Forster is a postdoctoral researcher at The Hospital for Sick Children.\n\nForster grew up in Chelmsford, Essex. She was diagnosed with acute lymphoblastic leukemia aged 7. She became interested in scientific research whilst at hospital, and went on to study biomedical science at the Durham University. She graduated from Durham University in 2008. Forster completed a PhD at Newcastle University with James Allan and Olaf Heidenreich. On the day she finished her PhD, she tweeted \"‘Dear Cancer, I beat you aged eight and now I’ve got a PhD in cancer research’,\", and it went viral.\n\nForster used the media attention to praise where she worked, the Northern Institute for Cancer Research, Newcastle upon Tyne. Here she concentrated on leukemia caused by mutations in DNA. Today Forster is a postdoctoral researcher at The Hospital for Sick Children. Her research focusses on the rare genetic disorder biallelic mismatch repair deficiency.\n\nIn 2014 Forster was a British Science Association Media Fellow. That year, she spoke at TED x Jesmond Dene about the legacy of Janet Rowley. She appeared in the science communication project Soapbox Science. Forster was a 2017 TED Global Fellow researching paediatric cancer. Her TED talk, \"What can cancer survivors teach us about cancer treatment\", was in Arusha, Tanzania. She was listed in the 2017 Forbes 30 Under 30. She has written for The Times, The Conversation, Forbes Health and The Guardian. She is a member of the Society of The International Society of Paediatric Oncology.\n"}
{"id": "3758544", "url": "https://en.wikipedia.org/wiki?curid=3758544", "title": "William Summerlin", "text": "William Summerlin\n\nWilliam T. Summerlin (born 1938) is a dermatologist who, as a medical researcher, perpetrated a notorious scientific fraud.\n\nIn 1974, Summerlin was working under immunologist Robert A. Good at Memorial Sloan-Kettering Cancer Center in New York City, conducting research in transplantation immunology. He claimed to have shown that success of skin transplants between genetically unrelated animals was enhanced by culturing the skin in special medium for several weeks. If so, the work had major implications as a means to suppress immunological rejection of transplanted tissues. However, his own and others' attempts to reproduce his original results failed.\n\nThe experimental method involved transplantation of skin from black mice (with black melanocyte pigment cells) to white mice (without melanocytes). Over time, the melanoctyes would naturally tend to migrate out of the transplanted tissue, so as to produce a grayish patch rather than a distinctly black patch.\n\nIn the incident that became notorious, Summerlin was called to a meeting with Good, and took with him the single experimental animal that was the best evidence of transplant success. Noting that the patch had \"grayed\", Summerlin by his own subsequent admission darkened it with a black permanent marker. The mouse was not produced at the meeting with Good. Summerlin's action was discovered when he returned the mouse to animal care technicians, who immediately noticed that the patch could be removed with alcohol. Senior staff and Dr. Good were notified within minutes.\n\nIn the subsequent investigation, it became apparent that the original transplantation experiments were poorly controlled, and that other experiments from Summerlin's lab at Sloan-Kettering were misrepresented in reports and to colleagues. The mouse at the center of the controversy involved transplantation between two genetically similar strains, and hence had a conventional explanation. All of Summerlin's transplant work was ultimately withdrawn as shoddy.\n\nSummerlin later attributed his deceptive behavior to a combination of mental and physical exhaustion, a heavy clinical and experimental workload, and pressure to publicize positive results. Memorial Sloan-Kettering President Dr. Lewis Thomas said that Dr. Summerlin was suffering from a \"serious emotional disturbance\". After the incident, Summerlin reportedly moved to rural Louisiana to practice medicine.\n\nAs a result of the Summerlin incident, the term \"painting the mice\" has become a synonym for research fraud. Author Joseph Hixson wrote a book about the scandal entitled \"The Patchwork Mouse\".\n\n\n\n"}
{"id": "7776394", "url": "https://en.wikipedia.org/wiki?curid=7776394", "title": "Women'sNet", "text": "Women'sNet\n\nWomen'sNet is a networking support programme designed to enable South African women to use the internet to find the people, issues, resources and tools needed for women's social action.\n\nSome of the issues it has foccused on include HIV/AIDS, Beijing+5 in Africa, Women and Human Rights, violence against women, Gender in Parliament, Health, Women and Information and communication technologies (ICTs), Women and Enterprise, and Women and Elections.\n\nThe Women'sNet site offers links to useful websites, a newsletter (launched in April 2006), links to relevant issues, and a directory of organisations.\n\nIn 1998, Women'sNet was established as a project of the Commission of Gender Equality and SANGONeT. In the following year, the first Women'sNet webpages went live, focusing on women and human rights issues in South Africa.\n\nIt also created the first online space for South African women to reflect on, and strategise, in the lead up to the 1999 national elections. In 1999 too, the African Sisters online workshop was held. Women'sNet collaborated with FEMNET to create an online platform for joining regional processes in the lead-up to the Beijing+5 conference.\n\n2001 saw the creation of the first women-run internet cafe for civil society organisations at the [World Conference Against Racism, Xenophobia and related Intolerances], among other activities.\n\nIn 2000, Women'sNet launched a project to combine radio and audio production with women's NGOs efforts, to promote a women's empowerment agenda. It also won the Highway Africa ward for \"Innovative Use of the New Media\".\n\nOther landmarks include SANGONeT Board's confirming Women'sNet's independent status (April 2002), its joining of the international ICT4D network the Association for Progressive Communications, being officially registered as a non-governmental organisation (January 2003), being a core partner in the first Africa-wide Women and Electronic Networking Training (WENT) in Cape Town (2003), launching an on-line up-datable database of South African organisations providing services targeted at girls (March 2003).\n\nIn 2004, it convened a stake-holders meet to consult South African women's NGOs on their information and technology needs; launched the GenderStats website, launching Girls'Net, hosting the first African workshop on free software (or FLOSS) and implementing the Recording Women's Voices project.\n\nOver the past two years (2005 and 2006), Women'sNet has also co-trained South African women to participate in the global cyber-dialogues on the [Beijing+10] review process, and women's NGOs on technology planning and FLOSS. Girl'sNet runs a visual literacy training project with young girls in the Eastern Cape, and has held a photo exhibition in August 2005. Women'sNet also launched the (s)hebytes project and website as a space \"where women and men talk about gender and women's empowerment\". It launched digital story telling workshops \"to train women and girls in story telling -- combining images, audio and text into a short documentary about their lives\".\n\nIn 2005, Women'sNet's executive director [Natasha Primo] become the first women Chair of the [Association of Progressive Communicators].\n\n"}
{"id": "70847", "url": "https://en.wikipedia.org/wiki?curid=70847", "title": "X-ray photoelectron spectroscopy", "text": "X-ray photoelectron spectroscopy\n\nX-ray photoelectron spectroscopy (XPS) is a surface-sensitive quantitative spectroscopic technique that measures the elemental composition at the parts per thousand range, empirical formula, chemical state and electronic state of the elements that exist within a material. Put more simply, XPS is a useful measurement technique because it not only shows what elements are within a film but also what other elements they are bounded to. This means if you have a metal oxide and you want to know if the metal is in a +1 or +2 state, using XPS will allow you to find that ratio. However at most the instrument will only probe 20nm into a sample.\n\nXPS spectra are obtained by irradiating a material with a beam of X-rays while simultaneously measuring the kinetic energy and number of electrons that escape from the top 0 to 10 nm of the material being analyzed. XPS requires high vacuum (\"P\" ~ 10 millibar) or ultra-high vacuum (UHV; \"P\" < 10 millibar) conditions, although a current area of development is ambient-pressure XPS, in which samples are analyzed at pressures of a few tens of millibar.\n\nXPS can be used to analyze the surface chemistry of a material in its as-received state, or after some treatment, for example: fracturing, cutting or scraping in air or UHV to expose the bulk chemistry, ion beam etching to clean off some or all of the surface contamination (with mild ion etching) or to intentionally expose deeper layers of the sample (with more extensive ion etching) in depth-profiling XPS, exposure to heat to study the changes due to heating, exposure to reactive gases or solutions, exposure to ion beam implant, exposure to ultraviolet light.\n\nXPS is used to measure:\n\nXPS can be performed using a commercially built XPS system, a privately built XPS system, or a synchrotron-based light source combined with a custom-designed electron energy analyzer. Commercial XPS instruments in the year 2005 used either a focused 20- to 500-micrometer-diameter beam of monochromatic Al K X-rays, or a broad 10- to 30-mm-diameter beam of non-monochromatic (polychromatic) Al K X-rays or Mg K X-rays. A few specially designed XPS instruments can analyze volatile liquids or gases, or materials at pressures of roughly 1 torr (1.00 torr = 1.33 millibar), but there are relatively few of these types of XPS systems. The ability to heat or cool the sample during or prior to analysis is relatively common.\n\nBecause the energy of an X-ray with particular wavelength is known (for Al K X-rays, \"E\" = 1486.7 eV), and because the emitted electrons' kinetic energies are measured, the electron binding energy of each of the emitted electrons can be determined by using an equation that is based on the work of Ernest Rutherford (1914):\n\nwhere \"E\" is the binding energy (BE) of the electron, \"E\" is the energy of the X-ray photons being used, \"E\" is the kinetic energy of the electron as measured by the instrument and formula_2 is the work function dependent on both the spectrometer and the material. This equation is essentially a conservation of energy equation. The work function term formula_2 is an adjustable instrumental correction factor that accounts for the few eV of kinetic energy given up by the photoelectron as it becomes absorbed by the instrument's detector. It is a constant that rarely needs to be adjusted in practice.\n\nIn 1887, Heinrich Rudolf Hertz discovered but could not explain the photoelectric effect, which was later explained in 1905 by Albert Einstein (Nobel Prize in Physics 1921). Two years after Einstein's publication, in 1907, P.D. Innes experimented with a Röntgen tube, Helmholtz coils, a magnetic field hemisphere (an electron kinetic energy analyzer), and photographic plates, to record broad bands of emitted electrons as a function of velocity, in effect recording the first XPS spectrum. Other researchers, including Henry Moseley, Rawlinson and Robinson, independently performed various experiments to sort out the details in the broad bands.\n\nAfter WWII, Kai Siegbahn and his research group in Uppsala (Sweden) developed several significant improvements in the equipment, and in 1954 recorded the first high-energy-resolution XPS spectrum of cleaved sodium chloride (NaCl), revealing the potential of XPS. A few years later in 1967, Siegbahn published a comprehensive study of XPS, bringing instant recognition of the utility of XPS, which he referred to as \"Electron Spectroscopy for Chemical Analysis\" (\"ESCA\"). In cooperation with Siegbahn, a small group of engineers (Mike Kelly, Charles Bryson, Lavier Faye, Robert Chaney) at Hewlett-Packard in the USA, produced the first commercial monochromatic XPS instrument in 1969. Siegbahn received the Nobel Prize for Physics in 1981, to acknowledge his extensive efforts to develop XPS into a useful analytical tool.\n\nIn parallel with Siegbahn's work, David Turner at Imperial College (and later at Oxford University) in the UK developed ultraviolet photoelectron spectroscopy (UPS) on molecular species using helium lamps.\n\nA typical XPS spectrum is a plot of the number of electrons detected (sometimes per unit time) (\"Y\"-axis, ordinate) versus the binding energy of the electrons detected (\"X\"-axis, abscissa). Each element produces a characteristic set of XPS peaks at characteristic binding energy values that directly identify each element that exists in or on the surface of the material being analyzed. These characteristic spectral peaks correspond to the electron configuration of the electrons within the atoms, e.g., 1\"s\", 2\"s\", 2\"p\", 3\"s\", etc. The number of detected electrons in each of the characteristic peaks is directly related to the amount of element within the XPS sampling volume. To generate atomic percentage values, each raw XPS signal must be corrected by dividing its signal intensity (number of electrons detected) by a \"relative sensitivity factor\" (RSF), and normalized over all of the elements detected. Since hydrogen is not detected, these atomic percentages exclude hydrogen.\n\nTo count the number of electrons during the acquisition of a spectrum with a minimum of error, XPS detectors must be operated under ultra-high vacuum (UHV) conditions because electron counting detectors in XPS instruments are typically one meter away from the material irradiated with X-rays. This long path length for detection requires such low pressures.\n\nXPS detects only those electrons that have actually escaped from the sample into the vacuum of the instrument, and reach the detector. In order to escape from the sample into vacuum, a photoelectron must travel through the sample. Photo-emitted electrons can undergo inelastic collisions, recombination, excitation of the sample, recapture or trapping in various excited states within the material, all of which can reduce the number of escaping photoelectrons. These effects appear as an exponential attenuation function as the depth increases, making the signals detected from analytes at the surface much stronger than the signals detected from analytes deeper below the sample surface. Thus, the signal measured by XPS is an exponentially surface-weighted signal, and this fact can be used to estimate analyte depths in layered materials.\n\nThe main components of a commercially made XPS system include a source of X-rays, an ultra-high vacuum (UHV) stainless steel chamber with UHV pumps, an electron collection lens, an electron energy analyzer, Mu-metal magnetic field shielding, an electron detector system, a moderate vacuum sample introduction chamber, sample mounts, a sample stage, and a set of stage manipulators.\n\nMonochromatic aluminum K-alpha X-rays are normally produced by diffracting and focusing a beam of non-monochromatic X-rays off of a thin disc of natural, crystalline quartz with a <1010> orientation. The resulting wavelength is 8.3386 angstroms (0.83386 nm) which corresponds to a photon energy of 1486.7 eV. Aluminum \"K\"-alpha X-rays have an intrinsic full width at half maximum (FWHM) of 0.43 eV, centered on 1486.7 eV (\"E\"/Δ\"E\" = 3457). For a well optimized monochromator, the energy width of the monochromated aluminum \"K\"-alpha X-rays is 0.16 eV, but energy broadening in common electron energy analyzers (spectrometers) produces an ultimate energy resolution on the order of FWHM=0.25 eV which, in effect, is the ultimate energy resolution of most commercial systems. When working under practical, everyday conditions, high-energy-resolution settings will produce peak widths (FWHM) between 0.4–0.6 eV for various pure elements and some compounds. For example, in a spectrum obtained in 1 minute at a pass energy of 20 eV using monochromated aluminum \"K\"-alpha X-rays, the Ag 3\"d\" peak for a clean silver film or foil will typically have a FWHM of 0.45 eV.\n\nNon-monochromatic magnesium X-rays have a wavelength of 9.89 angstroms (0.989 nm) which corresponds to a photon energy of 1253 eV. The energy width of the non-monochromated X-ray is roughly 0.70 eV, which, in effect is the ultimate energy resolution of a system using non-monochromatic X-rays. Non-monochromatic X-ray sources do not use any crystals to diffract the X-rays which allows all primary X-rays lines and the full range of high-energy Bremsstrahlung X-rays (1–12 keV) to reach the surface. The ultimate energy resolution (FWHM) when using a non-monochromatic Mg \"K\"-alpha source is 0.9–1.0 eV, which includes some contribution from spectrometer-induced broadening.\n\nXPS is routinely used to determine:\n\n\nThe ability to produce chemical state information (as distinguished from merely elemental information) from the topmost few nm of any surface makes XPS a unique and valuable tool for understanding the chemistry of any surface, either as received, or after physical or chemical treatment(s). In this context, \"chemical state\" refers to the local bonding environment of a species in question. The local bonding environment of a species in question is affected by its formal oxidation state, the identity of its nearest-neighbor atom, its bonding hybridization to that nearest-neighbor atom, and in some cases even the bonding hybridization between the atom in question and the next-nearest-neighbor atom. Thus, while the nominal binding energy of the C electron is 284.6 eV (some also use 285.0 eV as the nominal value for the binding energy of carbon), subtle but reproducible shifts in the actual binding energy, the so-called \"chemical shift\", provide the chemical state information referred to here.\n\nChemical-state analysis is widely used for the element carbon. Chemical-state analysis of the surface of carbon-containing polymers readily reveals the presence or absence of the chemical states of carbon shown in bold, in approximate order of increasing binding energy, as: carbide (-C), silicone (-Si-CH), methylene/methyl/hydrocarbon (-CH-CH-, CH-CH-, and -CH=CH-), amine (-CH-NH), alcohol (-C-OH), ketone (-C=O), organic ester (-COOR), carbonate (-CO), monofluoro-hydrocarbon (-CFH-CH-), difluoro-hydrocarbon (-CF-CH-), and trifluorocarbon (-CH-CF), to name but a few examples.\n\nChemical state analysis of the surface of a silicon wafer readily reveals chemical shifts due to the presence or absence of the chemical states of silicon in its different formal oxidation states, such as: n-doped silicon and p-doped silicon (metallic silicon in figure above), silicon suboxide (SiO), silicon monoxide (SiO), SiO, and silicon dioxide (SiO). An example of this is seen in the figure above: High-resolution spectrum of an oxidized silicon wafer in the energy range of the Si 2\"p\" signal.\n\n\n\nDetection limits may vary greatly with the cross section of the photoelectron line of interest and the background signal level which is a function of the matrix material. In general photoelectron cross sections increase with atomic number, while the background is a function of the composition of the matrix material and the binding energy. Background signals generally increase with atomic number of the matrix material and decrease with increasing kinetic energy. For example in the case of gold on silicon where the high cross section Au4f peak is at a higher kinetic energy than the major silicon peaks, it sits on a very low background and detection limits of 1ppm or better may be achieved with reasonable acquisition times. Conversely for silicon on gold, where the modest cross section Si2p line sits on the large background below the Au4f lines, detection limits would be much worse for the same acquisition time.\nDetection limits are often quoted as 0.1–1.0 % atomic percent (0.1 % = 1 part per thousand = 1000 ppm) for practical analyses, but lower limits may be achieved in many circumstances.\n\n\nInstruments accept small (mm range) and large samples (cm range), e.g. wafers. Limiting factor is the design of the sample holder, the sample transfer, and the size of the vacuum chamber. Large samples are laterally moved in x and y direction to analyse a larger area. \n\n\n\nInorganic compounds, metal alloys, semiconductors, polymers, pure elements, catalysts, glasses, ceramics, paints, papers, inks, woods, plant parts, make-up, teeth, bones, human implants, biomaterials, viscous oils, glues, ion modified materials\n\n\n\nThe number of peaks produced by a single element varies from 1 to more than 20. Tables of binding energies (BEs) that identify the shell and spin-orbit of each peak produced by a given element are included with modern XPS instruments, and can be found in various handbooks [citations] and websites. Because these experimentally determined BEs are characteristic of specific elements, they can be directly used to identify experimentally measured peaks of a material with unknown elemental composition.\n\nBefore beginning the process of peak identification, the analyst must determine if the BEs of the unprocessed survey spectrum (0-1400 eV) have or have not been shifted due to a positive or negative surface charge. This is most often done by looking for two peaks that due to the presence of carbon and oxygen.\n\nCharge referencing is needed when a sample suffers either a positive (+) or negative (-) charge induced shift of experimental BEs. Charge referencing is needed to obtain meaningful BEs from both wide-scan, high sensitivity (low energy resolution) survey spectra (0-1100 eV), and also narrow-scan, chemical state (high energy resolution) spectra.\n\nCharge induced shifting causes experimentally measured BEs of XPS peaks to appear at BEs that are greater or smaller than true BEs. Charge referencing is performed by adding or subtracting a \"Charge Correction Factor\" to each of the experimentally measured BEs. In general, the BE of the hydrocarbon peak of the C (1s) XPS signal is used to charge reference (charge correct) all BEs obtained from non-conductive (insulating) samples or conductors that have been deliberately insulated from the sample mount.\n\nCharge induced shifting is normally due to: a modest excess of low voltage (-1 to -20 eV) electrons attached to the surface, or a modest shortage of electrons (+1 to +15 eV) within the top 1-12 nm of the sample caused by the loss of photo-emitted electrons. The degree of charging depends on various factors. If, by chance, the charging of the surface is excessively positive, then the spectrum might appear as a series of rolling hills, not sharp peaks as shown in the example spectrum.\n\nThe C (1s) BE of the hydrocarbon species (moieties) of the \"Adventitious\" carbon that appears on all, air-exposed, conductive and semi-conductive materials is normally found between 284.5 eV and 285.5 eV. For convenience, the C (1s) of hydrocarbon moieties is defined to appear between 284.6 eV and 285.0 eV. A value of 284.8 eV has become popular in recent years. However, some recent reports indicate that 284.9 eV or 285.0 eV represents hydrocarbons attached on metals, not the natural native oxide. The 284.8 eV BE is routinely used as the \"Reference BE\" for charge referencing insulators. When the C (1s) BE is used for charge referencing, then the charge correction factor is the difference between 284.8 eV and the experimentally measured C (1s) BE of the hydrocarbon moieties.\n\nWhen using a monochromatic XPS system together with a low voltage electron flood gun for charge compensation the experimental BEs of the C (1s) hydrocarbon peak is often 4-5 eV smaller than the reference BE value (284.8 eV). In this case, all experimental BEs appear at lower BEs than expected and need to be increased by adding a value ranging from 4 to 5 eV. Non-monochromatic XPS systems are not usually equipped with a low voltage electron flood gun so the BEs will normally appear at higher BEs than expected. It is normal to subtract a charge correction factor from all BEs produced by a non-monochromatic XPS system.\n\nConductive materials and most native oxides of conductors should never need charge referencing. Conductive materials should never be charge referenced unless the topmost layer of the sample has a thick non-conductive film.\n\nThe process of peak-fitting high energy resolution XPS spectra is still a mixture of art, science, knowledge and experience. The peak-fit process is affected by instrument design, instrument components, experimental settings (aka analysis conditions) and sample variables. Most instrument parameters are constant while others depend on the choice of experimental settings.\n\nBefore starting any peak-fit effort, the analyst performing the peak-fit needs to know if the topmost 15 nm of the sample is expected to be a homogeneous material or is expected to be a mixture of materials. If the top 15 nm is a homogeneous material with only very minor amounts of adventitious carbon and adsorbed gases, then the analyst can use theoretical peak area ratios to enhance the peak-fitting process.\n\nVariables that affect or define peak-fit results include:\n\n\nThe full width at half maximum (FWHM) values are useful indicators of chemical state changes and physical influences. That is, broadening of a peak may indicate: a\nchange in the number of chemical bonds contributing to a peak shape, a change in the sample condition (x-ray damage) and/or\ndifferential charging of the surface (localised differences in the charge state of the surface). However, it should be noted that the FWHM also depends on the detector, and can also increase due to the sample getting charged.\n\n\nChemical shift values depend on the degree of electron bond polarization between nearest neighbor atoms. A specific chemical shift is the difference in BE values of one specific chemical state versus the BE of one form of the pure element, or of a particular agreed-upon chemical state of that element. Component peaks derived from peak-fitting a raw chemical state spectrum can be assigned to the presence of different chemical states within the sampling volume of the sample.\n\nA hemispherical electron energy analyzer is generally used for applications where a higher resolution is needed. An ideal hemispherical analyzer consists of two concentric hemispherical electrodes (inner and outer hemispheres) held at proper voltages. It is possible to demonstrate that in such a system, (i) the electrons are linearly dispersed along the direction connecting the entrance and the exit slit, depending on their kinetic energy, while (ii) electrons with the same energy are first-order focused.\nWhen two potentials, formula_4 and formula_5, are applied to the inner and outer hemispheres, respectively, the electric potential and field in the region between the two electrodes can be calculated by solving the Laplace equation:\n\nformula_7\n\nwhere formula_8 and formula_9 are the radii of the two hemispheres. In order for the electrons with kinetic energy E0 to follow a circular trajectory of radius formula_10, the force exerted by the electric field \n(formula_11) must equal the centripetal force (formula_12) along the whole path. After some algebra, the following expression can be derived for the potential:\n\nwhere formula_14 is the energy of the electrons expressed in eV. \nFrom this equation, we can calculate the potential difference between the two hemispheres, which is given by:\n\nThe latter equation can be used to determine the potentials to be applied to the hemispheres in order to select electrons with energy formula_16, the so-called \"pass energy\".\n\nIn fact, only the electrons with energy formula_17 impinging normal to the entrance slit of the analyzer describe a trajectory of radius formula_18 and reach the exit slit, where they are revealed by the detector.\n\nThe instrumental energy resolution of the device depends both on the geometrical parameters of the analyzer and on the angular divergence of the incoming photoelectrons:\n\nwhere formula_20 is the average width of the two slits, and formula_21 is the incidence angle of the incoming photoelectrons.\nThough the resolution improves with increasing formula_22, technical problems related to the size of the analyzer put a limit on the actual value of formula_22.\nAlthough a low pass energy formula_17 improves the resolution, the electron transmission probability is reduced at low pass energy, and the signal-to-noise ratio deteriorates, accordingly.\nThe electrostatic lenses in front of the analyzer have two main purposes: they collect and focus the incoming photoelectrons into the entrance slit of the analyzer, and they decelerate the electrons to the kinetic energy formula_17, in order to increase the resolution.\n\nWhen acquiring spectra in \"sweep\" (or \"scanning\") mode, the voltages of the two hemispheres formula_4 and formula_5 -\nand hence the pass energy- are held fixed; at the same time, the voltage applied to the electrostatic lenses is swept in such a way that each channel counts electrons with the selected kinetic energy for the selected amount of time.\nIn order to reduce the acquisition time per spectrum, the so-called \"snapshot\" (or \"fixed\") mode has been introduced. This mode exploits the relation between the kinetic energy of a photoelectron and its position inside the detector. If the detector energy range is wide enough, and if the photoemission signal collected from all the channels is sufficiently strong, the photoemission spectrum can be obtained in one single shot from the image of the detector.\n\nSince the relevant information, in photoemission spectroscopy, is contained in the kinetic energy distribution of the photoelectrons, a specific device is needed to energy-filter the electrons emitted (or scattered) by the sample.\nElectrostatic monochromators are the most common choice. The older design, a CMA, represents a trade-off between the need for high count rates and high angular/energy resolution. The so-called cylindrical mirror analyzer (CMA) is mostly used for checking the elemental composition of the surface.\nIt consists of two co-axial cylinders placed in front of the sample, the inner one being held at a positive potential, while the outer cylinder is held at a negative potential.\nOnly the electrons with the right energy can pass through this set-up and are detected at the end. The count rates are high but the resolution (both in energy and angle) is poor.\n\nA breakthrough has been actually brought about in the last decades by the development of large scale synchrotron radiation facilities. Here, bunches of relativistic electrons kept on a circular orbit inside a storage ring are accelerated through bending magnets or insertion devices like wigglers and undulators to produce a high brilliance and high flux photon beam. \nThe main advantages of using synchrotron light are\n\nThe highest spectral brightness and narrowest beam energy dispersion is attained by undulators, which consist of periodic array of dipole magnets in which the electrons are forced to wiggle and thus to emit coherent light. Besides the high intensity, energy tunability is one of the most important advantages of synchrotron light compared to the light produced by conventional X-ray sources. In fact, a wide energy range (from the IR to the Hard X-ray region, depending on the energy of the electron bunch) is accessible by changing the undulator gaps between the\narrays. Continuous energy spectra available from a synchrotron radiation source allows selection of photon energies yielding optimum photoionization cross-sections appropriate for probing a particular core level. \nThe high photon flux, in addition, makes it possible to perform XPS experiments also from low density atomic species, such as molecular and atomic adsorbates.\n\nElectrons can be detected using an electron multiplier, usually a channeltron. This device essentially consists of a glass tub with a resistive coating on the inside. A high voltage is applied between the front and the end. An electron which enters the channeltron is accelerated to the wall, where it removes more electrons, in such a way that an electron avalanche is created, until a measurable current pulse is obtained.\n\nWhen a photoemission event takes place, the following energy conservation rule holds:\n\nwhere formula_29 is the photon energy, formula_30 is the electron BE (with respect to the vacuum level) prior to ionization, and formula_31 is the kinetic energy of the photoelectron. If reference is taken with respect to the Fermi level (as it is typically done in photoelectron spectroscopy) formula_30 must be replaced by the sum of the binding energy (BE) relative to the Fermi level, formula_33, and the sample work function, formula_34 .\n\nFrom the theoretical point of view, the photoemission process from a solid can be described with a semiclassical approach, where the electromagnetic field is still treated classically, while a quantum-mechanical description is used for matter.\nThe one—particle Hamiltonian for an electron subjected to an electromagnetic field is given by:\n\nwhere formula_36 is the electron wave function, formula_37 is the vector potential of the electromagnetic field and formula_38 is the unperturbed potential of the solid.\nIn the Coulomb gauge (formula_39), the vector potential commutes with the momentum operator \n(formula_40), so that the expression in brackets in the Hamiltonian simplifies to:\n\nActually, neglecting the formula_42 term in the Hamiltonian, we are disregarding possible photocurrent contributions. Such effects are generally negligible in the bulk, but may become important at the surface.\nThe quadratic term in formula_37 can be instead safely neglected, since its contribution in a typical photoemission experiment is about one order of magnitude smaller than that of the first term .\n\nIn first-order perturbation approach, the one-electron Hamiltonian can be split into two terms, an unperturbed Hamiltonian formula_44, plus an interaction Hamiltonian formula_45, which describes the effects of the electromagnetic field:\n\nIn the time-dependent perturbation theory for harmonic perturbations, the transition rate between the initial state formula_47 and the final state formula_48 is expressed by the Fermi's golden rule:\n\nwhere formula_50 and formula_51 are the eigenvalues of the unperturbed Hamiltonian in the initial and final state, respectively, and formula_29 is the photon energy. The Fermi Golden rule strictly applies only if the perturbation acts on the system for an infinite time. Since in a real system the interaction has a finite duration, the Dirac delta function in the equation above must be replaced by the density of states in the final state, formula_53:\n\nIn a real photoemission experiment the ground state core electron BE cannot be directly probed, because the measured BE \nincorporates both initial state and final state effects, and the spectral linewidth is broadened owing to the finite core-hole lifetime (formula_55).\n\nAssuming an exponential decay probability for the core hole in the time domain (formula_56), the spectral function will have a Lorentzian shape, with a FWHM (Full Width at Half Maximum) formula_57 given by:\n\nFrom the theory of Fourier transforms, formula_57 and formula_55 are linked by the indeterminacy relation:\n\nformula_61\n\nThe photoemission event leaves the atom in a highly excited core ionized state, from which it can decay radiatively (fluorescence) or non-radiatively (typically by \"Auger\" decay).\nBesides Lorentzian broadening, photoemission spectra are also affected by a Gaussian broadening, whose contribution can be expressed by\n\nThree main factors enter the Gaussian broadening of the spectra: the experimental energy resolution, vibrational and inhomogeneous broadening.\nThe first effect is caused by the non perfect monochromaticity of the photon beam -which results in a finite bandwidth- and by the limited resolving power of the analyzer. The vibrational component is produced by the excitation of low energy vibrational modes both in the initial and in the final state. Finally, inhomogeneous broadening can originate from the presence of unresolved core level components in the spectrum.\n\nIn a solid, also inelastic scattering events contribute to the photoemission process, generating electron-hole pairs which show up as an inelastic tail on the high BE side of the main photoemission peak. \nIn some cases, we observe also energy loss features due to plasmon excitations. This can either a final state effect caused by core hole decay, which generates quantized electron wave excitations in the solid (intrinsic plasmons), or it can be due to excitations induced by photoelectrons travelling from the emitter to the surface (extrinsic plasmons).\nDue to the reduced coordination number of first-layer atoms, the plasma frequency of bulk and surface atoms are related by the following equation: \nformula_63,\nso that surface and bulk plasmons can be easily distinguished from each other.\nPlasmon states in a solid are typically localized at the surface, and can strongly affect the electron Inelastic Mean Free Path (IMFP).\n\n\"Vibrational effects\"\n\nTemperature-dependent atomic lattice vibrations, or phonons, can broaden the core level components and attenuate the interference patterns in an XPD (X-Ray Photoelectron Diffraction) experiment. The simplest way to account for vibrational effects is by multiplying the scattered single-photoelectron wave function formula_64 by the Debye-Waller factor:\n\nwhere formula_66 is the squared magnitude of the wave vector variation caused by scattering,\nand formula_67 is the temperature-dependent one-dimensional vibrational mean squared displacement of the formula_68 emitter. In the Debye model, the mean squared displacement is calculated in terms of the Debye temperature, formula_69, as:\n\n\n\n\n"}
