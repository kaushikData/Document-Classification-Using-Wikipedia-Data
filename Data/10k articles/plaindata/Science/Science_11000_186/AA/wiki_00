{"id": "40617271", "url": "https://en.wikipedia.org/wiki?curid=40617271", "title": "2MASS J03552337+1133437", "text": "2MASS J03552337+1133437\n\n2MASS J03552337+1133437 (2MASS J0355+11) is a nearby brown dwarf of spectral type L5Î³, located in constellation Taurus at approximately 29.8 light-years from Earth.\n\n2MASS J0355+11 was discovered in 2006 by Reid \"et al.\"\n\n\n\n"}
{"id": "238493", "url": "https://en.wikipedia.org/wiki?curid=238493", "title": "Adverse pressure gradient", "text": "Adverse pressure gradient\n\nIn fluid dynamics, an adverse pressure gradient occurs when the static pressure increases in the direction of the flow. Mathematically this is expressed as:\nformula_1 for a flow in the positive formula_2-direction.\nThis is important for boundary layers, increasing the fluid pressure is akin to increasing the potential energy of the fluid, leading to a reduced kinetic energy and a deceleration of the fluid. Since the fluid in the inner part of the boundary layer is slower, it is more greatly affected by the increasing pressure gradient. For a large enough pressure increase, this fluid may slow to zero velocity or even become reversed. When flow reversal occurs, the flow is said to be separated from the surface. This has very significant consequences in aerodynamics since flow separation significantly modifies the pressure distribution along the surface and hence the lift and drag characteristics.\n\nTurbulent boundary layers tend to be able to sustain an adverse pressure gradient better than an equivalent laminar boundary layer. The more efficient mixing which occurs in a turbulent boundary layer transports kinetic energy from the edge of the boundary layer to the low momentum flow at the solid surface, often preventing the separation which would occur for a laminar boundary layer under the same conditions. This physical fact has led to a variety of schemes to actually produce turbulent boundary layers when boundary layer separation is dominant at high Reynolds numbers. The dimples on a golf ball, the fuzz on a tennis ball, or the seams on a baseball are good examples. Aeroplane wings are often engineered with vortex generators on the upper surface to produce a turbulent boundary layer.\n\n\n"}
{"id": "57233998", "url": "https://en.wikipedia.org/wiki?curid=57233998", "title": "Andre Walker Hair Typing System", "text": "Andre Walker Hair Typing System\n\nThe Andre Walker Hair Typing System is classification system for hair types created in the 1990s by Oprah Winfrey's stylist Andre Walker. It was originally created to market Walker's line of hair care products but has since been widely adopted as a hair type classification system. The system includes images of each hair type to aid classification. The system has been criticised for an apparent hierarchy which values caucasian hair over other hair types. In 2018 the system was the subject of episodes of the podcasts \"99% Invisible\" and \"The Stoop\".\n\nThe system is split into four types with subtypes labeled A, B and C for some of the types, the system has added new subtypes since its original version to Type 1.\n\n"}
{"id": "18420982", "url": "https://en.wikipedia.org/wiki?curid=18420982", "title": "Artificial gravity in fiction", "text": "Artificial gravity in fiction\n\nArtificial gravity is a common theme in fiction, particularly science fiction.\n\nIn the film \"\", and the same-named novel, a rotating centrifuge in the \"Discovery\" spacecraft provides artificial gravity. The people could walk, run, sit, or sleep along a curving \"floor\" that continually rotates inside the exterior shell of the spacecraft; the entry from the non-rotating part of the ship is through the centrifuge's central hub. To film the effect, a rotating circular set was used with the actors at the bottom; the set turned as the actors walked or jogged along the curved floor. One scene required one actor to be strapped in place while the set (and actor) rotated and the other actor walked towards him. The movie also features a large rotating space station; its full-scale interior set was a section of curved floor that did not rotate.\n\nIn \"\", the 1982 novel and second in a series of four by Arthur C. Clarke, the \"Leonov\" spacecraft does not have the \"luxury\" of gravity, yet in \"\", Peter Hyams' 1984 sci-fi film adaptation and sequel to \"\", Syd Mead's \"Leonov\" spacecraft design in the movie features centrifugal force artificial gravity very similar to the Europa One mission in \"Europa Report\", a 2013 science fiction film. In the \"2010\" movie, a Russian unmanned probe mysteriously disappears near the surface of Europa omitting the novel's premise of a space race to Jupiter with the joint Soviet-American mission following a faster Chinese ship, the \"Tsien\" that must land on Europa to refuel with water from the icy moon. The \"Tsien\" is ambushed and destroyed by an indigenous Europan life-form attracted to lights stranding the only survivor - also strikingly similar to events in \"Europa Report\".\n\nLarry Niven's novel \"Ringworld\" featured a gigantic habitat encircling a star, which created artificial gravity through rotation. Niven also makes a reference to the Coriolis effect when the protagonists see what looks like a giant eye above the horizon. When they get closer, they realise that it is in fact a hurricane, but rotating about an axis parallel to the ground rather than perpendicular to it. Large hurricanes on Earth rotate the way they do due to the Coriolis effect. A number of early Known Space and Man-Kzin Wars stories also make use of rotational gravity, prior to the adoption of \"gravity polarizer\" technology which generates artificial gravity fields.\n\nIn the Gundam universe, gigantic space habitats similar to O'Neill cylinders, called Colonies, are an important aspect to the plot. They spin to generate artificial gravity.\n\nIn the anime \"Cowboy Bebop\", the Bebop possesses a ringed area that generates artificial gravity and is often seen being used (with the rest of the ship not rotating).\n\nThe book \"Rendezvous with Rama\" and the sequels featured an alien construct similar to an O'Neill habitat which was able to generate approximately 0.6g on the intentionally habitable ground section. The plot employed significant use of the difference in strength of artificial gravity as an object approaches the center of the rotating cylinder.\n\nIn the television series \"Babylon 5\", the Earth Alliance made extensive use of rotational gravity in its space stations and some larger military vessels, as well as civilian cruise ships. It has been suggested that the cruise ships would alter their rate of spin gradually en route to match the destination, helping to acclimate the passengers to the new gravity they would find upon arrival. Earlier Earth Force ships using straps and harnesses to hold crew in place, and the Minbari later share the secret of artificial gravity as part of the Interstellar Alliance.\n\nIn the stories based on Sid Meier's \"Alpha Centauri\", the Unity provided artificial gravity by spinning, though the game made allusions to less conventional technologies developed later on.\n\nIn John Varley's \"Gaian\" trilogy (\"Titan\", \"Wizard\", and \"Demon\"), the title world Gaia, being a torus with a diameter of 1300 kilometers, spins at a rate of one revolution per sixty-one minutes, producing an apparent gravity of one-quarter \"g\".\n\nIn Iain M. Banks's Culture series, Orbitals are made ten million kilometres in circumference so that they spin with a rate that gives a natural day/night cycle while the center is in orbit around a star.\n\nIn the game \"\", the main location of the story is an artificial ringworld that creates artificial gravity by computer-controlled rotational spin (inspired by the aforementioned Larry Niven's novel \"Ringworld\" but also uses some form of field or other artificially generated gravity as it is stated in Halo: The Flood, the ring world does not spin nearly fast enough to create the amount of gravity it possesses. \"Halo\" (or \"Installation 04\") is approximately 10,000 km in diameter and is eventually destroyed by the same forces keeping it in operation. A fusion explosion weakens part of the ringworld, and centrifugal forces tear the ring apart.\n\nIn \"The Martian\" and the film of the same title, the \"Hermes\" spacecraft achieves artificial gravity by design; it employs a ringed structure, at whose periphery forces around 40% of Earth's gravity are experienced. Such artificial gravity is similar in strength to the gravity on mars. At the center of the ringed structure, lack of gravity makes the astronauts practically weightless.\n\nIn the \"Expanse\" series by James S. A. Corey, space stations generate artificial gravity by rotating, as do spun-up, hollowed-out asteroids, usually at around 0.3 g. Moving ships under constant thrust also simulate gravity by linear acceleration.\n\nIn the video game \"\", the various large space stations seen throughout the galaxy rotate in order to create artificial gravity for the people who live and work on them. One class of these stations is the Coriolis starport, in reference to the eponymous effect it uses to generate gravity.\n\nIn the movie \"Interstellar (film)\" co-produced by \"Christopher Nolan\", the \"Endurance\" space stations and capsules create artificial gravity by rotating at a certain rotational frequency to simulate gravity. At the outer ring of the structure, the gravity experienced is similar to that on Earth.\n\nIn many science fiction stories, there are artificial gravity generators that create a gravitational field based on a mass that does not exist. It helps the story by creating a more Earth-like spaceship, and in the case of a movie or television program, it helps the production because it is a lot cheaper than the special effects needed to simulate weightlessness.\n\nIn the \"Star Trek\" universe, artificial gravity is achieved by the use of \"gravity plating\" embedded in a starship's deck.\n\nIn Gene Roddenberry's \"Andromeda\", set thousands of years in the future, gravity field generators not only provide gravity for the people inside the ship, but also reduce inertial mass of ships such as the \"Andromeda Ascendant\" to just under a kilogram. This greatly increases the efficiency of their Magneto-Plasma Dynamic Drive, allowing them to go from a stop to percentages of light speed very quickly. It can also be easily manipulated to do things like increase gravity and immobilize intruders (though prepared intruders can use an antigravity harness to prepare for this possibility), and reversed to expel things from the ship.\n\nIn the anime \"Dragon Ball Z\", gravity simulation plays a key part in various characters' training regime. It is also used to demonstrate the characters' increasing strength. For example, when Goku first arrives on King Kai's planet, he is nearly crushed by the gravity, which is ten times that of Earth's. By the end of his visit, nearly a year later, he is able to move at great speed under such conditions. This method of training gradually appears more and more in the universe, and the gravity gets stronger as well. Ten times Earth's gravity goes from a seemingly indomitable level of opposition to nothing, and several hundred times Earth's gravity becomes the standard. Vegeta even had a Gravity Room built into his house.\n\nIn the \"Doctor Who\" story \"The Sontaran Experiment\", a Sontaran used similar technology to make a bar above a human very heavy, so that his friends had to lift it up with as much force as they could to prevent him being crushed. The Sontaran gradually increased the bar's weight as part of an experiment to study not only their physical strength but also their loyalty, as their friend had recently attempted to betray them.\n\nIn the Disney film Treasure Planet, the ship used by the protagonists use a device which stimulates artificial gravity, keeping them firmly on the deck of the ship.\n\nIn the BioWare series \"Mass Effect\", the eponymous \"mass effect\" is responsible for the manipulation of gravity or kinetic forces(if the mass effect field is alternating), caused by subjecting a quantity of fictional \"element zero\" to an electric current. A negative current reduces the mass of anything within the field, a positive current increases mass, and an alternating current will create a barrier force of immense power that can shield or crush anything the force is directed at. Mass effect is used in faster than light travel, artificial gravity on spacecraft, weapon technology, kinetic barriers and shields, and much more. Individuals exposed to element zero are known as \"biotics,\" the nodules of element zero embedded into their nervous systems allow them to use neural impulses to create mass effect fields themselves if the power of the element is amplified with a biotic amp, granting specific types of abilities such as certain types of telekinesis. An individual exposed to this \"eezo\" would often rather cause cancer.\n\nIn the video game \"Dead Space\", artificial gravity plates are used to simulate an Earth-like environment in outer space. In several levels, gravity plating is off and the player has to navigate in weightlessness using 'Zero-Gravity Boots', similar to magnetic boots. Defective gravity plates are also encountered sometimes, which push objects upward rather than downward with great force, killing the player or enemies instantly if they step on them.\n\nIn the 2014 film, \"Guardians of the Galaxy\", Peter Quill uses a device called a \"Gravity Mine\" which creates a powerful short-range artificial gravity field that attracts all nearby objects towards it.\n\n"}
{"id": "25489966", "url": "https://en.wikipedia.org/wiki?curid=25489966", "title": "Association for the Sociology of Religion", "text": "Association for the Sociology of Religion\n\nThe Association for the Sociology of Religion (ASR) is an academic association with more than 700 members worldwide. It publishes a journal, the \"\" and holds meetings at the same venues and times as the American Sociological Association.\n\nThe ASR was founded by Catholic sociologists in Chicago in 1938 as the American Catholic Sociological Society. The organization adopted its present name in 1970, reflecting changes in the Vatican's policy that led to greater openness towards other faiths. It has long since become a base for sociological research on religion without regard to belief, creed, or religious orientation. \n\nThe association publishes a journal, \"Sociology of Religion: A Quarterly Review\", as well as a quarterly newsletter. It is the co-publisher of an annual series entitled \"Religion and the Social Order\". The association provides research grants. \n\nThe ASR, which has over 700 members worldwide, continues its historical practice of holding its meetings at the same venues and times as the American Sociological Association, allowing mutual cross-fertilisation between the two associations. Past presidents of the ASR include David G. Bromley, James T. Richardson, Eileen Barker and Benton Johnson\n\n"}
{"id": "797882", "url": "https://en.wikipedia.org/wiki?curid=797882", "title": "CLIVAR", "text": "CLIVAR\n\nCLIVAR (climate variability and predictability) is a component of the World Climate Research Programme. Its purpose is to describe and understand climate variability and predictability on seasonal to centennial time-scales, identify the physical processes responsible for climate change and develop modeling and predictive capabilities for climate modelling.\n\nThe following is an approximate timeline of CLIVAR and its precedents:\n\n\nCLIVAR has a number of panels and working groups based on the study of climate variability and predictability of different components of the global climate system.\n\nCLIVAR has three global panels:\n\n\nRegional panels focus on specific aspects of the climate system. Since the different regions of the ocean are qualitatively different, and given the important role of the oceans in controlling climate over the interannual, decadal, and centennial scales considered by CLIVAR, the subdivision into panels is largely based on regions of the ocean system. Specifically, the following is the list of regional panels:\n\n\nThere are four national CLIVAR programmes, that run largely autonomously but contribute to the international CLIVAR program:\n\n\n"}
{"id": "49020615", "url": "https://en.wikipedia.org/wiki?curid=49020615", "title": "Casma Group", "text": "Casma Group\n\nThe Casma Group () is a stratigraphic group of Mesozoic-aged sedimentary formations exposed along the coast and within the Cordillera Occidental near Casma, Peru. \n\nThe sediments of the Casma Group reflect deposition in two distinct environments of the West Peruvian Basin. Some sediments were deposited near the Casma Volcanic Arc in an intra-cratonic sedimentary basin that was occasionally connected to the ocean. Other sediments were deposited in what was once a continental platform bounded by reefs and with anoxic pockets.\n\nTogether with the Morro Solar and Imperial Groups, the Casma Group contains clastic volcanosedimentary material derivative of the Mesozoic Casma Volcanic Arc. The folding of the Casma Group sediments is the result of the Cretaceous Mochica Phase of the Andean orogeny. The group is intruded by plutons of the Coastal Batholith of Peru.\n\nThe Casma Group include the following lithologies: gypsum, limestone, marl, tuff, lava flows, sandstone and conglomerate.\n"}
{"id": "20534214", "url": "https://en.wikipedia.org/wiki?curid=20534214", "title": "Children, Youth and Environments", "text": "Children, Youth and Environments\n\nChildren, Youth and Environments is a biannual peer-reviewed academic journal that publishes research articles, in-depth analyses, field reports, and book reviews on research, policy, and practice concerning inclusive and sustainable environments for children and youth worldwide.\n\nFrom 1984 through 1995, it was produced in print through the Children, Environments Research Group at the City University of New York with Roger Hart as editor-in-chief. Since 2016, it has been published online by the University of Cincinnati.\n\nThe journal is abstracted and indexed in:\n"}
{"id": "23161639", "url": "https://en.wikipedia.org/wiki?curid=23161639", "title": "Chrysler SERV", "text": "Chrysler SERV\n\nSERV, short for Single-stage Earth-orbital Reusable Vehicle, was a proposed space launch system designed by Chrysler's Space Division for the Space Shuttle project. SERV was so radically different from the two-stage spaceplanes than almost every other competitor entered into the Shuttle development process that it was never seriously considered for the shuttle program.\n\nSERV was to be a single-stage to orbit spacecraft that would take off from the existing Saturn V complexes and land vertically at Kennedy for re-use. SERV looked like a greatly expanded Apollo capsule, with an empty central core able to carry of cargo. SERV could be launched unmanned for cargo missions, ejecting a cargo capsule and returning to Earth. For manned missions, a separate spaceplane, MURP (Manned Upper-stage Reusable Payload), could be carried atop the vehicle.\n\nNote that the name \"SERV\" was also used by an entirely unrelated NASA project, the \"Space Emergency Re-entry Vehicle\".\n\nIn 1966 the US Air Force started a study effort that explored a variety of manned spacecraft and associated launchers. As the proposals were studied, they broke them down into one of three classes, based on the level of reusability. On the simpler end of the development scale were the \"Class I\" vehicles that placed a spaceplane on top of an existing or modified ICBM-based launcher. \"Class II\" vehicles added partial reusability for some of the launcher components, while the \"Class III\" vehicles were fully reusable. The USAF had already started work on a Class I design in their X-20 Dyna Soar program, which had been cancelled in December 1963, but were interested in the Lockheed Star Clipper Class II design as a possible future development. Nothing ever came of the study effort, as the USAF wound down their interest in manned space programs.\n\nAt the time, NASA was in the midst of winding down the Project Apollo build-out, as the vehicles progressed to flight. Looking into the future, a number of NASA offices started programs to explore manned missions in the 1970s and beyond. Among the many proposals, a permanently manned space station was a favorite. These plans generally assumed the use of the existing Saturn rockets to launch the stations, and even the crews, but the Saturn systems were not set up for the sort of constant supply and crew turnaround being envisioned. The idea of a simple and inexpensive manned launcher, a \"ferry and logistics vehicle\", developed out of the space station studies almost as an afterthought, the first mention of it being in the fiscal year 1967 budgets.\n\nDesign of a low cost, reusable Space Transportation System (STS) started in earnest in December 1967, when George Mueller organized a one-day brainstorming session on the topic. He jump-started the discussion by inviting the USAF to attend, even keeping the original USAF acronym for the project, \"ILRV\". Like the original USAF studies, a small vehicle was envisioned, carrying replacement crews and basic supplies, with an emphasis on low cost of operations and fast turnarounds. Unlike the USAF, however, NASA's Space Task Force quickly decided to move directly to the Class III designs.\n\nNASA envisioned a four-phase program of development for the STS. \"Phase A\" was a series of initial studies to select an overall technology path, and development contracts for proposals were released in 1968 with the proposals expected back in the fall of 1969. A number of designs were presented from a variety of industry partners. Almost universally, the designs were small, fully reusable, and based around delta wing or lifting body spaceplanes.\n\nChrysler Aerospace won contract NAS8-26341 for their entry into the Phase A series, forming a team under Charles Tharratt. Their 1969 report, NASA-CR-148948, outlined the SERV design, preliminary performance measures, and basic mission profiles. This report described a wide cargo bay Tharratt was convinced that SERV offered better flexibility than any of the winged platforms, allowing it to launch both manned and unmanned missions, and being much smaller overall.\n\nWith most of the NASA centers backing one of the winged vehicles, and being dramatically different from any of them, SERV found no supporters within the bureaucracy and was never seriously considered for STS. Additionally, the astronaut corps was adamant that any future NASA spacecraft would have to be manned, so the potentially unmanned SERV won no converts there either.\n\nAn extension contract was offered anyway, producing the final NASA-CR-150241 report on the SERV design that was turned in on 1 July 1971. This differed mostly in minor details, the major change being the reduction of the cargo bay from 23Â feet to in keeping with the rest of the Shuttle proposals.\n\nSERV consisted of a large conical body with a rounded base that Chrysler referred to as a \"modified Apollo design\". The resemblance is due to the fact that both vehicles used blunt body re-entry profiles, which lessen heating load during re-entry by creating a very large shock wave in front of a rounded surface. Tilting the vehicle in relation to the direction of motion changes the pattern of the shock waves, producing lift that can be used to maneuver the spacecraft - in the case of SERV, up to about 100 NM on either side of its ballistic path. To aid lift generation, SERV was \"stepped\", with the lower portion of the cone angled in at about 30 degrees, and the upper portion closer to 45 degrees. SERV was across at the widest point, and tall. Gross lift off weight was just over , about the same as the Saturn V's but more than the Shuttle's .\n\nThe majority of the SERV airframe consisted of aluminum composite honeycomb. The base was covered with screw-on ablative heat shield panels, which allowed for easy replacement between missions. The upper portions of the airframe, which received dramatically lower heating loads, were covered with metal shingles covering a quartz insulation below. Four landing legs extended from the bottom, their \"foot\" forming their portion of heat shield surface when retracted.\n\nA twelve module LH2/LOX aerospike engine was arranged around the rim of the base, covered by movable metal shields. During the ascent the shields would move out from the body to adjust for decreasing air pressure, forming a large altitude compensating nozzle. The module was fed from a set of four cross-linked turbopumps that were designed to run at up to 120% of their nominal power, allowing orbital insertion even if one pump failed immediately after takeoff. The engine as a whole would provide 7,454,000Â lbf (25.8 MN) of thrust, about the same as the S-IC, the first stage of the Saturn V.\n\nAlso arranged around the base were forty jet engines, which were fired just prior to touchdown in order to slow the descent. Movable doors above the engines opened for feed air. Two RL-10's provided de-orbit thrust, so the main engine did not have to be restarted in space. Even on-orbit maneuvering, which was not extensive for the SERV (see below), was provided by small LOX/LH2 engines instead of thrusters using different fuels.\n\nA series of conical tanks around the outside rim of the craft, just above the engines, stored the LOX. LH2 was stored in much larger tanks closer to the center of the craft. Much smaller spherical tanks, located in the gaps below the rounded end of the LOX tanks, held the JP-4 used to feed the jet engines. Orbital maneuvering and de-orbit engines were clustered around the top of the spacecraft, fed by their own tanks interspersed between the LH2. This arrangement of tanks left a large open space in the middle of the craft, 15 by , which served as the cargo hold.\n\nTwo basic spacecraft configurations and mission profiles were envisioned. \"Mode A\" missions flew SERV to a high-altitude parking orbit at inclined at 55 degrees, just below the space station's orbit at . \"Mode B\" missions flew to a low Earth orbit (LEO) inclined at 28.5 degrees, a due-east launch from the Kennedy Space Center. In either case the SERV was paired with a long cargo container in its bay, and optionally combined with a manned spacecraft on top.\n\nThe original proposals used a lifting body spaceplane known as MURP to support manned missions. The MURP was based on the HL-10 design already under study by North American Rockwell as part of their STS efforts. MURP was fitted on top of a cargo container and fairing, which was long overall. In the second version of the study, Chrysler also added an option that replaced MURP with a \"personnel module\", based on the Apollo CSM, which was long when combined with the same cargo container. The original, \"SERV-MURP\", was when combined with SERV, while the new configuration, \"SERV-PM\", was tall. Both systems included an all-aspect abort of the manned portion throughout the entire ascent.\n\nAfter considering all four combinations of mode and module, two basic mission profiles were selected as the most efficient. With SERV-PM the high-earth orbit would be used and the PM would maneuver only a short distance to reach the station. With SERV-MURP, the low Earth orbit would be used and the MURP would maneuver the rest of the way on its own. In either case, the SERV could return to Earth immediately and let the PM or MURP land on their own, or more commonly, wait in the parking orbit for a cargo module from an earlier mission to rendezvous with it for return to Earth. Weight and balance considerations limited the return payload.\n\nBoth configurations delivered of cargo to the space station, although in the PM configuration the overall thrown weights were much lower. If the PM configuration was used with a fairing instead of the capsule, SERV could deliver to LEO, or as much as with an \"Extended Nosecone\". The Extended Nosecone was a long spike with a high fineness ratio that lowered atmospheric drag by creating shock waves that cleared the vehicle body during ascent.\n\nIn addition, Chrysler also outlined ways to support wide loads on the front of SERV. This was the diameter of the S-IC and S-II, the lower stages of the Saturn V. NASA had proposed a wide variety of payloads for the Apollo Applications Program that were based on this diameter that were intended to be launched on the Saturn INT-21. Chrysler demonstrated that they could also be launched on SERV, if weight considerations taken into account. However, these plans were based on the earlier SERV designs with the larger cargo bay. When NASA's loads were adapted to fit to the smaller bay common to all the STS proposals, this option was dropped.\n\nSERV was not expected to remain on orbit for extended periods of time, with the longest missions outlined in the report at just under 48 hours. Typically it would return after a small number of orbits brought its ground track close enough to Kennedy, and abort-once-around missions were contemplated. The vehicle was designed to return to a location within four miles (6Â km) of the touchdown point using re-entry maneuvering, the rest would be made up during the jet-powered descent.\n\nNASA had partnered with Chrysler to build the NASA-designed Saturn IB, at the Michoud Assembly Facility outside New Orleans. Chrysler proposed building SERVs at Michoud as well, delivering them to KSC on the Bay-class ships used to deliver Boeing's S-IC from the same factory. Since the SERV was wider than the ships, it had to be carried slightly tilted in order to reduce its overall width. Pontoons were then added to the side of the ships to protect the spacecraft from spray.\n\nSERVs would be fitted out in the Vehicle Assembly Building (VAB) High Bay, mated with the PM or MURP which were prepared in the Low Bay, and then transported to the LC39 pads on the existing crawler-transporters. The LC39 pads required only minor modifications for SERV use, similar to those needed to launch the Saturn IB. Chrysler proposed building several SERV landing pads between LC39 and the VAB, and a landing strip for the MURP near the existing Space Shuttle landing strip. The SERVs would be returned to the VAB on an enormous flatbed truck. The only other new infrastructure was a set of test stands at the Mississippi Test Operations engine testing complex, near Michoud.\n\nRe-using much of the existing infrastructure lowered overall program costs; total costs were estimated as $3.565 billion, with each SERV costing $350 million in FY1971 dollars, and being rated for 100 flights over a 10-year service life. This was far less expensive than the two-stage flyback proposals entered by most companies, which had peak development costs on the order of $10 billion.\n\nSERV was similar to the later McDonnell Douglas DC-X design. The primary difference between the two was that the DC-X was built to a military mission and required much greater re-entry maneuvering capability. Because of this, the airframe was long and skinny, and the spacecraft re-entered nose-first. Tilting this shape relative to the path of motion generates considerably more lift than the blunt base of SERV, but also subjects the airframe to much higher heating loads.\n\nMore recently, the original SERV layout was used in the Blue Origin Goddard spacecraft. Like the SERV, Goddard did not need the extended crossrange capabilities of a military launcher, and returned to the simpler blunt-base re-entry profile. The similar Kankoh-maru design study also used the same blunt-body VTOL profile.\n\n"}
{"id": "51477128", "url": "https://en.wikipedia.org/wiki?curid=51477128", "title": "ColangÃ¼il Batholith", "text": "ColangÃ¼il Batholith\n\nThe ColangÃ¼il Batholith is a group of plutons in western Argentina between the latitudes of 29 and 31Â° S. The plutons of the batholith were emplaced and cooled in the Late Paleozoic and the Triassic. Runs in a north-south direction. The plutons of the batholith are intruded into volcanic rocks produced by the same plutons plus some earlier deformed basement. The most common rocks in the batholith are granodiorite, granite and leucogranite. The batholith contains also a dyke swarm of north-south trending dykes. Compared to other subduction-related batholiths around the Pacific Ocean ColangÃ¼il Batholith is more felsic. \n\nTogether with the Chilean Coastal Batholith and the Elqui-LimarÃ­ Batholith the ColangÃ¼il Batholith is a remnant of the volcanic arcs that erupted the volcanic material of the Choiyoi Group.\n\nThe batholith is made up six major units:\n"}
{"id": "15292186", "url": "https://en.wikipedia.org/wiki?curid=15292186", "title": "Comparison of GUI testing tools", "text": "Comparison of GUI testing tools\n\nGUI testing tools serve the purpose of automating the testing process of software with graphical user interfaces.\n"}
{"id": "29054294", "url": "https://en.wikipedia.org/wiki?curid=29054294", "title": "Conjugate focal plane", "text": "Conjugate focal plane\n\nIn optics, a conjugate plane or conjugate focal plane of a given plane \"P\", is the plane \"P`\" such that points on \"P\" are imaged on \"P`\". If an object is moved to the point occupied by its image, then the moved object's new image will appear at the point where the object originated. In other words, the object and its image are interchangeable. This comes from the principle of reversibility which states light rays will travel along the originating path if the light's direction is reversed. The points that span conjugate planes are called conjugate points.\n\nIn a telescope, the subject focal plane is at infinity and the conjugate image plane, at which the image sensor is placed, is said to be an infinite conjugate. In microscopy and macro photography, the subject is close to the lens, so the plane at which the image sensor is placed is said to be a finite conjugate. Within a system with relay lenses or eyepieces, there may be planes that are conjugate to the aperture.\n"}
{"id": "6144908", "url": "https://en.wikipedia.org/wiki?curid=6144908", "title": "Consumer's risk", "text": "Consumer's risk\n\nConsumer's risk or Consumer risk is a potential risk found in all consumer-oriented products, that a product not meeting quality standards will pass undetected through the manufacturer's quality control system and enter the consumer marketplace.\n\nAccepting the lot of unsatisfactory quality is a risk for any consumer. The probability of accepting the lot of fraction defectives under sampling inspection plan is called as consumer's risk.\n\n"}
{"id": "47341258", "url": "https://en.wikipedia.org/wiki?curid=47341258", "title": "Data janitor", "text": "Data janitor\n\nA data janitor is a person who works to take big data and condense it into useful amounts of information. Also known as a \"data wrangler,\" a data janitor sifts through data for companies in the information technology industry. A multitude of start-ups rely on large amounts of data, so a data janitor works to help these businesses with this basic, but difficult process of interpreting data.\n\nWhile it is a commonly held belief that data janitor work is fully automated, many data scientists are employed primarily as data janitors. The Information technology industry has been increasingly turning towards new sources of data gathered on consumers, so data janitors have become more commonplace in recent years.\n\nData janitors work in a process that largely consists of four steps: selection and defining relationships, extraction and organization, loading, and interpretation. Data janitors identify sources of data before selecting which data is relevant and find the relationships between the data that will be useful to the company's projects. Next, they structure the data in an effort to extract the information and put it into a format that can be stored in a secure place for the business. Last, the data janitors work with other employees to create visual aids to present to managers and executives who will eventually benefit from the conclusions that can be made from them. In this way, the work of data janitors is integral to the functioning of businesses that rely on large amounts data to function.\n"}
{"id": "46601982", "url": "https://en.wikipedia.org/wiki?curid=46601982", "title": "Distributive number", "text": "Distributive number\n\nIn linguistics, more precisely in traditional grammar, a distributive number is a word that answers \"how many times each?\" or \"how many at a time?\", such as \"singly\" or \"doubly\". They are contrasted with multipliers. In English, this part of speech is rarely used and much less recognized than cardinal numbers and ordinal numbers, but it is clearly distinguished and commonly used in Latin and several Romance languages, such as Romanian.\n\nIn English distinct distributive numbers exist, such as \"singly\", \"doubly\", and \"triply\", and are derived from the corresponding multiplier (of Latin origin, via French) by suffixing \"-y\" (reduction of Middle English \"-lely\" > \"-ly\"). However, this is more commonly expressed periphrastically, such as \"one by one\", \"two by two\"; \"one at a time\", \"two at a time\"; \"in twos\", \"in threes\"; or using a counter word such as \"in groups of two\" or \"two pieces to a â¦\". Examples include \"Please get off the bus one by one so no-one falls.\", \"She jumped up the steps two at a time.\", \"Students worked in the lab in twos and threes.\", \"Students worked in groups of two and three.\", and \"Students worked two people to a team.\"\n\nThe suffixes \"-some\" (as in \"twosome\", \"threesome\") and \"-fold\" (as in \"two-fold\", \"three-fold\") are also used, though also relatively infrequently. For musical groups \"solo\", \"duo\", \"trio\", \"quartet\", etc. are commonly used, and \"pair\" is used for a group of two.\n\nA conspicuous use of distributive numbers is in arity or adicity, to indicate how many parameters a function takes. Most commonly this uses Latin distributive numbers and \"-ary\", as in \"unary\", \"binary\", \"ternary\", but sometimes Greek numbers are used instead, with \"-adic\", as in \"monadic\", \"dyadic\", \"triadic\".\n\nGeorgian, Latin, and Romanian are notable languages with distributive numbers; see Romanian distributive numbers.\n\nIn Japanese numerals, distributive numbers are formed regularly from a cardinal number, a counter word, and the suffix , as in .\n\nIn Turkish, one of the -ar/-er suffixes (chosen according to vowel harmony) are added to the end of a cardinal number, as in \"bir\"er\"\" (one of each) and \"dokuz\"ar\"\" (nine of each). If the number ends with a vowel, a letter Å comes to the middle; as in \"iki\"Åer\"\" (two of each) and \"altÄ±\"Åar\"\" (six of each).\n\n"}
{"id": "26278120", "url": "https://en.wikipedia.org/wiki?curid=26278120", "title": "Donald Schultz", "text": "Donald Schultz\n\nDonald Schultz (born 27 August 1978) is a South African film maker, writer and entertainer who travels the world working with dangerous species.\n\nSchultz was on a show on Animal Planet called \"Wild Recon\" which ran for one season for a total of 10 episodes. It ran under the title \"Venom Hunter\" overseas.\n\nHe was also featured in \"Venom in Vegas\", where he was put in a glass box full of 100 snakes, some venomous, in full view of the Las Vegas public. Occasionally, Schultz is also the guest on Chelsea Lately where he introduces Chelsea to various new animals.\n\nHe also co-hosted \"Animal Intervention\" with Allison Eastwood on Nat Geo Wild ' where he was involved in trying to convince owners of exotic animals, namely big cats, to either re-home or improve the conditions of exotic animals in their care. \"Animal Intervention\".\n\nHe often appears on the Jason Ellis Show on Sirius/XM to discuss Extreme Falling, Animals and tribal rituals among other topics of interest.\n\nSchultz works on a variety of travel, adventure and conservation content for television and other mediums.\n\nSchultz is also an accomplished skydiver and BASE jumper. With over 1500 wingsuit and BASE jumps, he has traveled to various countries on wing suit expeditions.\n\nIn March 2014, Schultz accompanied NITRO CIRCUS to South Africa for 3 LIVE shows in Johannesburg, Durban and Cape Town. While there, the crew filmed 4 episodes due for release on MTV2 in 2014.\n\nIn August 2013, Schultz was charged with a misdemeanor for shipping pet lizards across state lines in 2010. Schultz claimed he was unaware a permit was needed and received community service and a $6000 fine. He said he is going to use the whole experience to learn and further educate people about the need for conservation.\n\n"}
{"id": "419993", "url": "https://en.wikipedia.org/wiki?curid=419993", "title": "Fibrocartilage callus", "text": "Fibrocartilage callus\n\nA fibrocartilage callus is a temporary formation of fibroblasts and chondroblasts which forms at the area of a bone fracture as the bone attempts to heal itself. The cells eventually dissipate and become dormant, lying in the resulting extracellular matrix that is the new bone.\nThe callus is the first sign of union visible on x-rays, usually 3 weeks after the fracture. Callus formation is slower in adults than in children, and in cortical bones than in cancellous bones.\n\n"}
{"id": "3798188", "url": "https://en.wikipedia.org/wiki?curid=3798188", "title": "Filamentous bacteriophage", "text": "Filamentous bacteriophage\n\nA filamentous bacteriophage is a type of bacteriophage, or virus of bacteria, defined by its filament-like or rod-like shape. Filamentous phages usually contain a genome of single-stranded DNA and infect Gram-negative bacteria.\n\n"}
{"id": "54470948", "url": "https://en.wikipedia.org/wiki?curid=54470948", "title": "Gill plate trade", "text": "Gill plate trade\n\nThe gill plate trade is the buying and selling of stingray gill plates for their use in traditional Chinese medicine. The gill plates are harvested from stingrays that are caught intentionally, or caught as bycatch. The plates are sold whole or in a powder form. A single kilogram of the gill plate can be sold for up to $350 USD, though the price varies by ray species.\n\nStingrays are caught by fishing lines or nets. Private fishermen tend to target these ray species due to the high payout from their gill plates. Large fishing operations tend to catch rays as bycatch when fishing for more desirable food fish such as tuna, where they are brought back to shore and sold regardless of the intention to catch these fish. The gill plates are split in two and dried after they are taken from the dead rays. Once dried the plates are sold whole or ground into a powder.\n\nTonics made from stingray gill plates are used in traditional Chinese medicine, but there is no evidence of its effectiveness in any medical treatment.\n\nA few biological factors known as K-selected traits make the stingray more susceptible to population decline. These K-selected traits include but are not limited to: late sexual maturity, long gestation, low and number of offspring. Because the gill plate trade will accept plates from immature and mature rays, the number of rays that are able to reproduce is decreased by the capture of these animals. The population is slow to recover from these losses due to the K-selected traits and as fishing continues the population of stingrays decreases at a faster rate than it increases.\n\nThe gill plate trade effects other marine species in one of two ways. The first is the impact that the fishing has on these other species which inhabit the rays ecosystem is when rays are caught intentionally they are typically caught by longline fishing. Longline fishing involves thousands of baited hooks on one very long fishing line close to the ocean's surface. These hooks draw many species including birds, marine mammals, fish, sharks, and rays. The second impact of stingray fishing and population decline is an ecological effect. The stingrays are keystone predators in coral reef environments, when these species are declined the other reef species are effected through trophic factors, for instance predator populations decrease therefore prey populations increase, and risk effects. Risk effects are the behavioral mannerisms that prey learn to avoid predators, such as where or when to feed without becoming a meal for a predator. Without predators such as stingrays, the behavior of prey species is altered.\n\nIt is currently illegal to trade manta and mobula ray species without proper documentation that it is sustainable trade due to these species having been added to the CITES list. Once a species has been added to the CITES list it is internationally protected in legal trade; however, black market transactions still occur.\n"}
{"id": "19537526", "url": "https://en.wikipedia.org/wiki?curid=19537526", "title": "Hermann Haupt", "text": "Hermann Haupt\n\nHermann Haupt (24 January 1873, Langensalza, Unstrut-Hainich, Thuringia â 2 June 1959, Halle, Saxony-Anhalt) was a German entomologist who worked mainly on Auchenorrhyncha and Hymenoptera. \n\nHe was an intermediate school (Mittelschule) teacher. He described many new species. Hauptâs Hymenoptera and Auchenorrhyncha collections are conserved in the University of Halle-Wittenberg (Geiseltalmuseum Halle) and Biozentum), Staatliches Museum fÃ¼r Tierkunde Dresden (Cicadidae) and Naturkundemuseum Erfurt (other Orders). \n\n"}
{"id": "23817797", "url": "https://en.wikipedia.org/wiki?curid=23817797", "title": "Howard N. Potts Medal", "text": "Howard N. Potts Medal\n\nThe Howard N. Potts Medal was one of The Franklin Institute Awards for science and engineering award presented by the Franklin Institute of Philadelphia, Pennsylvania. It is named for Howard N. Potts. The awards program started in 1824. The first Howard N. Potts Medal was awarded in 1911. After 1991 the Franklin Institute merged many of their historical awards into the Benjamin Franklin Medal.\n\nFollowing people received the \"Howard N. Potts Medal\":\n\n"}
{"id": "16849249", "url": "https://en.wikipedia.org/wiki?curid=16849249", "title": "Hun Kal (crater)", "text": "Hun Kal (crater)\n\nHun Kal is a small crater on Mercury that serves as the reference point for the planet's system of longitude. The longitude of Hun Kal's center is defined as being 20Â° W, thus establishing the planet's prime meridian.\n\nHun Kal was chosen as a reference point since the actual prime meridian was in shadow when \"Mariner 10\" photographed the region, hiding any features near 0Â° longitude from view.\n\nHun Kal is about 1.5Â km in diameter.\n\nThe name \"Hun Kal\" means '20' in the language of the Maya.\n"}
{"id": "6038312", "url": "https://en.wikipedia.org/wiki?curid=6038312", "title": "International Society for Bayesian Analysis", "text": "International Society for Bayesian Analysis\n\nThe International Society for Bayesian Analysis (ISBA) is a society with the goal of promoting Bayesian analysis for solving problems in the sciences and government. It was formally incorporated as a not for profit corporation by economist Arnold Zellner and statisticians Gordon M. Kaufman and Thomas H. Leonard on 10 November 1992. It publishes the electronic journal \"Bayesian Analysis\" and organizes world meetings every other year.\n\nISBA is an \"official partner\" of the Joint Statistical Meetings.\n\nThe president of ISBA is elected annually. Service typically lasts three years, since the offices of President Elect and Past President are also official positions.\n\n\n"}
{"id": "19373062", "url": "https://en.wikipedia.org/wiki?curid=19373062", "title": "JÃ¶rg KÃ¼hn", "text": "JÃ¶rg KÃ¼hn\n\nJÃ¶rg KÃ¼hn (1940â64) was a Swiss artist, naturalist and scientific illustrator who specialized in bird paintings and drawings. He was also a children's book illustrator. He is noted for illustrations that are of a particularly scientific and exact nature.\n\nKÃ¼hn's reputation as a master of the exact portrayal of animals is based on a professional career that lasted barely six years during which he worked prolifically. The legacy is regarded as very valuable containing in excess of one hundred colour plates and several hundred line drawings and innumerable field studies of the animals he was illustrating. His depictive style has echoes of Louis Agassiz Fuertes\n\nHe worked from 1961 to 1964 as a scientific illustrator at the Zoological Museum (Zoologische Museum der Universitat ZÃ¼rich). His work which appeared in scientific publications including Pro Natura, \"Pro Juventute\", World Wildlife Fund, and \"Hallwag-Verlag\" are exact and beautifully drawn. He was also skilled in graphics and taught the skill at the Zoological Museum in ZÃ¼rich.\n\nAs well as scientific publications, he was sometimes employed as an illustrator of books including the childrenâs book \"Der Wald Und Seine Tierre\" (\"The Forest and its Animals\") (1963) where his illustrations included monographs of forest animals.It has run to at least five editions since it was first published in 1963.\n\nHis colour plates include those of threatened wildlife that were commissioned by the World Wildlife Fund and contributions to the \"Handbuch der VÃ¶gel Mitteleuropas\" (\"Handbook of the Birds of Central Europe\").\n\nAlthough JÃ¶rg KÃ¼hn's is chiefly remembered as a wildlife illustrator he was also commissioned to illustrate a number of medical textbooks including for the \"Surgery of the Thyroid and Parathyroid Glands\" \n\nKÃ¼hn died in ZÃ¼rich in October 1964 aged 24 from Hodgkin's lymphoma diagnosed when he was 18 years of age.\n\n\n"}
{"id": "3189536", "url": "https://en.wikipedia.org/wiki?curid=3189536", "title": "Leaving the nest", "text": "Leaving the nest\n\nLeaving the nest or moving out refers to the notion of a young person moving out of the accommodation provided by their guardian, fosterers or parent. Such a move can be motivated by various factors, including a desire for independence, the discovery of a more viable location and/or practicality.\n\nThe age at which young people move out of their previous accommodation has been rising since the turn of the 21st century. Some cultures view leaving the nest as one of the key milestones in the transition from tweenhood or teenhood to becoming an adult besides obtaining employment and getting wedded.\n\nIn some cultures, especially Middle Eastern ones, it is not as socially acceptable for a woman or girl to leave the home to live by herself without a marriage arrangement. Some researchers have suggested that a delay in leaving the nest may result in a decrease in sexual activity.\n"}
{"id": "50286142", "url": "https://en.wikipedia.org/wiki?curid=50286142", "title": "List of .NET libraries and frameworks", "text": "List of .NET libraries and frameworks\n\nThis article contains a list of notable libraries that can be used in .NET languages. This article is not merely a list of other Wikipedia entries, but covers a topic that is important on its own right. While the .NET framework provides a basis for application development, which provides platform independence, language interoperability and extensive framework libraries, the whole development ecosystem around .NET is crucially dependent on a vast number of user libraries that are developed independently of the framework. The value of the framework for developers can not be estimated without having an overview of the user library ecosystem, including issues related to \"platform independence\" and \"licensing\"\n\nStandard Libraries (CLI) (including the Base Class Library (BCL) are not included in this article because they have a separate article.\n\nPrograms written for .NET Framework execute in a software environment known as Common Language Runtime (CLR), an application virtual machine that provides services such as security, memory management, and exception handling. Framework includes a large class library called Framework Class Library (FCL), which together with CLR constitute the .NET framework.\n\nThanks to the hosting virtual machine, different .NET CLI compliant languages can operate on the same kind of data structures. Therefore, all CLI compliant languages can use the Framework Class Library and other .NET libraries that are written in one of the CLI compliant languages. When source code of a CLI compliant language is compiled, the compiler generates a platform independent code in the Common Intermediate Language (also referred to as bytecode), which is stored in CLI assemblies. When a .NET application is executed, the just-in-time compiler (JIT) turns the CIL code into platform specific machine code. To improve performance, the .NET Framework also comes with Native Image Generator (NGEN), which performs ahead-of-time compilation to machine code.\n\nThis architecture has several implications. The framework provides language interoperability (each language can use code written in other languages) across the .NET CLI compliant languages programming languages. Calls from one language to another are exactly the same as would be within a single programming language. If a library is written in one CIL language, it can be used in any other CIL language. Moreover, applications that consist only of pure .NET assemblies, can be transferred to any platform that contains implementation of .NET framework and executed on that platform. For example, an application written for Microsoft .NET on Windows can also be executed on Mono (a cross platform alternative implementation of .NET) on Linux or macOS. Such applications are automatically cross-platform (or platform independent) to the extent to which the .NET framework itself is ported to different platforms.\n\nAbility to transfer applications across platforms is extremely important for software developers, because in this way they can use the same code base for the application on any platform, enabling code reuse and avoiding code duplication. Both lead to reduced development and maintenance costs.\n\nHowever, platform independence is only guaranteed in the described way when none of the assemblies constituting an application depends on any code that is not pure .NET code. Many .NET libraries, however, make use of native libraries (written e.g. in C or C++) or system calls through interoperability mechanisms such as COM interoperability and the P/Invoke feature, which makes possible to call native code and thus call into libraries written in compiled languages (as opposed to managed) such as C or C++.\n\nIn these cases, platform independence of applications written for the .NET framework also depends on the ability to transfer non-.NET libraries, on which application depends, to target platforms. Any additional such library may add significantly to the effort necessary to transfer the application to other platforms. Sometimes, the best solution is to re-implement the parts of application that depend on such a library for each targeted platform. In many cases, the vast majority of the application's code base can be easily transferred across platforms, and only small specific portions of code dependent on problematic libraries must be re-implemented for each platform. Sometimes, a special compatibility layer is introduced that provides a uniform API to the platform dependent parts of code. Then, even if the higher level code heavily depends on platform dependent parts, dependence is resolved through API calls that are the same on all platforms, and the same high level code can still be used on different target platforms.\n\nA notable example is the Windows Forms graphical (GUI) class library. The library provides access to Windows User Interface Common Controls by wrapping the Windows API in managed code. The library is therefore not easily transferred to platforms other than Windows. In spite of that, the cross-platform .NET implementation Mono implements the library. Because of that, an application that depends on Windows Forms, can be ported to other forms by using the Mono runtime on which the Windows Forms library is implemented (beside Windows, this includes Unix and OSX). The Mono's implementation of the library is written in C# in order to avoid Windows dependence.\nMost of the Windows Forms API works on Mono, except for some minor incompatibilities and implementation bugs. However, many .NET libraries that were written on Windows and depend on Windows Forms, also make direct P/Invoke calls straight to the Windows API and bypass the Windows Forms API (this is sometimes done to avoid limitations of the Windows Forms). Such libraries will be broken when transferred to platforms other than Windows, although Windows Forms itself is available on these platforms via Mono implementation. For a GUI class library, the Mono project endorses use of the GTK# library, which is a .NET binding for the Gtk+ toolkit written in C.\n\nGUI libraries are not the only critical area of interest for .NET developers. Other libraries that may be problematic include 3D graphics libraries, sound and video libraries and device dependent libraries in general. While some areas are very well covered by core .NET libraries (such as database connectivity, file I/O, sockets, HTTP, XML manipulation, standard cryptography), the others (such as numerical libraries, general parsing libraries) are easy to implement in pure .NET but may be under represented as compared to availability of corresponding native libraries. For developers of both proprietary and open source software (including free software), licensing information is also critically important. Entries in the list therefore provide information about the scope of the listed libraries, main dependencies (especially when these affect platform dependence), and licensing information.\n\nThe .NET Framework has long supported cross-platform software development. The framework has been designed from the beginning for language interoperability, and parts of it were standardized in open standard (The Common Language Infrastructure and framework's most used programming language, C#). The original framework was first implemented only on Windows operating systems. Microsoft, the framework developer, and its partners, were working towards making their patents that cover some .NET - implemented technologies essential for framework implementation, available under \"reasonable and non-discriminatory terms\", which evolved into several patent promises issued by Microsoft. This made alternative third party implementations of the framework possible such as Mono, Portable.NET, and emulation CrossNet.\n\nIn spite of that, it has long been a concern within the Open source community that using alternative .NET implementations for cross-platform development, especially of free software, is unsafe because of the possibility or Microsoft patent claims against such implementations. The Free Software Foundation's Richard Stallman has openly opposed inclusion of C# implementations in the default installation of GNU/Linux distributions and stated that they (the community) should discourage people from writing programs in C#. Primary concerns were parts of the framework implementations that were not subject to standards and were not explicitly included in Microsoft's patent promises.\n\nThe 2010s saw some significant shifts in Microsoft's approach towards the Open software community. The company open sourced the .NET Compiler Platform (\"Roslyn\") and the ASP.NET in April 2014, and later the .NET Core (open sourced on November 2014) and other software. In February 2016, Microsoft acquired Xamarin, developer of Mono, an open source and cross platform implementation of .NET. On March 31, 2016 Microsoft announced at Microsoft Build that they will completely re-license Mono under the MIT license. Microsoft issued the Patent Promise for Mono stating that they won't assert any \"applicable patents\" against parties that are \"using, selling, offering for sale, importing, or distributing Mono\". It was also announced that the Mono Project was contributed to the .NET Foundation, a nonprofit organization established by Microsoft in March 2014 to improve open-source software development and collaboration around the .NET Framework.\n\nIn light of these developments, a strong open source community has begun to develop around the .NET framework (especially on GitHub), starting a number of libraries and software projects targeting the .NET framework for its cross platform character.\n\nThis section lists a number of notable .NET libraries (both open source and proprietary) arranged by topics.\n\nIn this section, items are not sorted by name and are not divided to open/proprietary software.\n\n.NET Core is an implementation of the core .NET Framework\nMono is a cross-platform implementation of the .NET Framework.\n\nC# Native compiles C# code to machine code.\nSharpLang compiles C# and other .NET languages to native machine code, using LLVM as a backend.\nCosmos is a C# Open Source Managed Operating System, an operating system \"construction kit\".\nFling OS is a C#-based operating system designed for learning low-level development.\nMOSA Project - Managed Operating System Alliance Project - a C# Operating System.\n\nASP.NET is a server-side web application framework designed for web development to produce dynamic web pages. It is the successor to Microsoft's Active Server Pages (ASP) technology built on the Common Language Runtime (CLR). It provides separate patterns for developing web applications ASP.NET MVC, ASP.NET Web API, and ASP.NET Web Pages (a platform using only Razor pages), which have merged into a unified MVC 6.\n\nASP.NET Core is a successor and re-implementation of ASP.NET as a modular web framework, together with other frameworks like Entity Framework. The framework uses the new open-source .NET Compiler Platform (codename \"Roslyn\") and is cross platform.\n\nBlazor is a web UI framework based on C#, Razor, and HTML that runs in the web browsers via WebAssembly. Blazor was designed to simplify the task of building fast single-page applications that run in any browser. It enables web developers to write .NET-based web apps that run client-side in web browsers using open web standards.\nOoui is a web framework for programming interactive user interfaces written in C# that run in a web browser. Ooui can target WebAssembly (WASM), enabling Xamarin.Forms applications to be deployed in WASM and run in-browser without the need for a server/side scripting.\n\nAForge.NET is a computer vision and artificial intelligence library. It implements a number of genetic, fuzzy logic and machine learning algorithms with several architectures of artificial neural networks with corresponding training algorithms.\nALGLIB is a cross-platform open source numerical analysis and data processing library. It consists of algorithm collections written in different programming languages (C++, C#, FreePascal, Delphi, VBA) and has dual licensing - commercial and GPL.\nDiffSharp is an automatic differentiation library for exact and efficient calculation of derivatives. It includes symbolic and numerical differentiation. Released under GPLv3.\nFsAlg is a lightweight linear algebra library that supports generic types, implemented in F#. Released under the BSD License.\nIMSL Numerical Libraries for .NET is a commercial library of mathematical, statistical, data mining, financial and charting classes written in C#.\nNeuronDotNet is a GPL-licensed artificial neural network library entirely written in C#. Because it only depends on the core .NET assemblies, it is easily portable across platforms.\nsuanshu.net by Numerical Method Inc. is a large collection of numerical algorithms including linear algebra, (advanced) optimization, interpolation, Markov model, principal component analysis, time series analysis, hypothesis testing, regressions, statistics, ordinary and partial differential equation solvers.\nMath.NET Numerics aims to provide methods and algorithms for numerical computations in science, engineering and every day use. Covered topics include special functions, linear algebra, probability models, random numbers, interpolation, integral transforms and more. MIT/X11 license.\nMeta.Numerics is a library for advanced scientific computation in the .NET Framework.\nThe NAG Library for .NET is a collection of mathematical and statistical routines for Microsoft .NET.\nNLinear is a generic linear algebra toolkit in C# compatible with Silverlight.\n\nAlea GPU is a framework for developing GPU-accelerated algorithms in F# on .NET and Mono.\nILNumerics.Net is a commercial high performance, typesafe numerical array classes and functions for general math, FFT and linear algebra, aims .NET/mono, 32&64 bit, script-like syntax in C#, 2D & 3D plot controls, efficient memory management. Released under GPLv3 or commercial license.\nMeasurement Studio is a commercial integrated suite UI controls and class libraries for use in developing test and measurement applications. The analysis class libraries provide various digital signal processing, signal filtering, signal generation, peak detection, and other general mathematical functionality.\nMicrosoft Solver Foundation (MSF) is a .NET package for designing and optimizing mathematical models.\nNMath by CenterSpace Software: Commercial numerical component libraries for the .NET platform, including signal processing (FFT) classes, a linear algebra (LAPACK & BLAS) framework, and a statistics package.\nThe Extreme Optimization Numerical Libraries for .NET are a commercial collection of mathematical and statistical classes for Microsoft .NET. It includes a large selection of standard algorithms from matrix factorization, function optimization, numerical integration, K-means clustering, and principal component analysis (PCA).\nsuanshu.net by Numerical Method Inc. is a large collection of numerical algorithms including linear algebra, (advanced) optimization, interpolation, Markov model, principal component analysis, time series analysis, hypothesis testing, regressions, statistics, ordinary and partial differential equation solvers.\n\nNPlot is a free, open source and cross platform charting library for .NET, released under the 3-clause-BSD license. Library includes classes for adding graphs to Windows Forms and ASP.NET, or to generate bitmaps.\nOxyPlot is a cross-platform charting library for .NET and supports WinForms, WPF, Xamarin and UWP platforms, released under the MIT license.\nWebCharts is a web control for creating charts that render as images(png, jpg, gif, etc.).\nZedGraph is a .NET 2D charting library for drawing line, bar, and pie Charts, released under the LGPL license. Library provides a high degree of flexibility, where very many aspects of how graphs will be displayed can be configured.\n\nManufaktura Controls is a set of libraries for drawing music scores in desktop, mobile and web applications.\n\nBlotch3D lets you add real-time 3D graphics with only a few lines of code. It can be built for multiple platforms. Blotch3D sits on top of MonoGame and retains all its features. It is released under the Microsoft Public License.\nHelix Toolkit is a 3D graphics toolkit that builds on and extends 3D capabilities of the WPF. Due to its dependence on WPF, the toolkit is limited to Windows platforms. It is released under the MIT license.\n\nMonoGame is free software used by game developers to make their Windows and Windows Phone games run on other systems. It currently supports OS X, Linux, iOS, Android, PlayStation Mobile, and the OUYA console. On Microsoft platforms it uses SharpDX and DirectX. When targeting non-Microsoft platforms, platform specific capabilities are utilized by the OpenTK library. It is released under the Microsoft Public License.\nOpen Toolkit (OpenTK) is a low-level C# binding for OpenGL, OpenGL ES and OpenAL. It runs on Windows, Linux, Mac OS X, BSD, Android and iOS. It can be used standalone or integrated into a GUI.\n\nAb3d.PowerToys is a framework for .Net 3D with cameras, 3D models, 3D lines, 3D text and more.\nAltSketch is a pure C#, 100% managed Vector Graphics Library. It has integration with GUI systems and Mobile platforms.\nUnity is a cross-platform game engine developed by Unity Technologies and used to develop video games for PC, consoles, mobile devices and websites.\nWindows Presentation Foundation (WPF) is a graphical subsystem for rendering user interfaces, developed by Microsoft. It also contains a 3D rendering engine. In addition, interactive 2D content can be overlaid on 3D surfaces natively. It only runs on Windows operating systems.\n\nAForge.NET is a computer vision and artificial intelligence library.\nIt implements a number of image processing algorithms and filters. It is released under the LGPLv3 and partly GPLv3 license. Majority of the library is written in C# and thus cross-platform. Functionality of AForge.NET has been extended by the Accord.NET library.\n\nAccord.NET is another computer vision and artificial intelligence library, available under the Gnu Lesser Public License, version 2.1. It is mainly written in C#.\n\nAvalonia is a cross-platform XAML-based user interface (UI) framework. It has been inspired by Microsoft's Windows Presentation Foundation (WPF) (which was codenamed Avalon at an early development stage), and beside XAML for definition of widget controls it also features flexible CSS - like styling system (unlike the WPF's styling where styles are stored in the \"Resources\" collection). It supports the following operating systems: Windows (.NET Framework, .NET Core), Linux (GTK), MacOS, Android and iOS. It is released under the MIT license. Avalonia is currently in beta stage.\n\nEto.Forms is a cross platform desktop and mobile user interface framework released under the BSD license.\n\nGtk# are C# wrappers around the underlying GTK+ and GNOME libraries, written in C and available on Linux, MacOS and Windows.\n\nQtSharp are C# bindings for the Qt framework.\n\nXwt is a GUI toolkit that maps API calls to native platform calls of the underlying platform, exposing one unified API across different platforms and making possible for the graphical user interfaces to have native look and feel on different platforms.\n\nWindows Forms is a Microsoft's GUI framework. The original Microsoft implementation runs on Windows operating systems and provides access to Windows User Interface Common Controls by wrapping the Windows API in managed code. The alternative Mono's implementation is open source and cross-platform (it runs on Windows, this includes Unix and OS X). It is mainly compatible with the original implementation but not completely. The library is written in C# in order to avoid Windows dependence.\nWindows Presentation Foundation (or WPF) is a graphical subsystem for rendering user interfaces in Windows-based applications by Microsoft. It is based on DirectX and employs XAML, an XML-based language, to define and link various interface elements. WPF applications can be deployed as standalone desktop programs or hosted as an embedded object in a website.\nXamarin.Forms is a cross-platform UI toolkit for development of native user interfaces that can be run on iOS, Android, and Universal Windows Platform apps. \n\nNWebsec - Security headers for ASP.NET applications.\nIdentityManager\nIdentityServer\nSKGL - Serial Key Generating Library\nSSH.NET is client-side library for SSH, SCP and SFTP. It does not contain third party dependencies and is released under the BSD License.\n\nFsCheck is a random testing framework for testing .NET programs automatically. It is a port of Haskell's QuickCheck.\nNUnit is an open source unit testing framework for .NET, written in C# and thus cross-platform. It is one of many programs in the xUnit family. Licensed under MIT License.\n\nMicrosoft Unit Testing Framework is integrated in Visual Studio. It is only available on Windows platforms.\n\nThe .NET framework natively provides utilities for object-relational mapping through ADO.NET, a part of .NET stack since .NET 1.0. In addition, a number of third-party object-relational libraries have been emerged, especially in earlier years of the .NET development, in order to fill some perceived gaps of the framework. \n\nAs the framework has evolved, additional object-relational tools were added, such as the Entity Framework included with the .NET Framework 3.5. LINQ to SQL was also introduced with .NET 3.5. This somehow reduced significance and popularity of third-party object-relational libraries. \n\nEntity Framework is an open source object-relational mapping (ORM) framework for ADO.NET. It was a part of .NET Framework, but since Entity framework version 6 it is separated from .NET framework.\n\nNConstruct Lite is a desktop and web rapid application development tool and environment for .NET Framework, containing an extensive library for ORM.\n\nNHibernate is an object-relationar mapper for the .NET platform.\nSQLProvider is an ORM-like-tool for F# language and SQL databases.\n\nDataObjects.NET is an object-relational mapper and business logic layer development framework for .NET projects. The framework focuses on non-trivial domain models with deep inheritance and composite objects, and on code-first, test-driven development. It is a proprietary library that comes with source code, and community edition of the library is available for free.\n\nThe .NET framework comes with a wide set of utilities for serialization of objects. The framework provides native object serialization to and from XML, JSON and binary streams. In spite of that, there are numerous third-party libraries that support serialization to target formats and working with these formats.\n\nJson.NET is a JSON framework for the .NET platform. Beside serialization and deserialization of arbitrary objects, is features numerous utilities for working with JSON documents. One can compose the object structures that map to JSON documents, save them to strings, streams or files, reload object structure from streams, traverse the structures, find parts by using JSON Path (an analogue to XPath in XML), convert JSON documents to XML documents, etc.\n\nThis Section contains collections of libraries (either general or specialized) and other resources for building applications based on the .NET framework.\n\n.NET Libraries for Numerical Analysis at The Math Forum.\n\n\nGeneral:\nNumerical libraries:\nData:\n"}
{"id": "3958869", "url": "https://en.wikipedia.org/wiki?curid=3958869", "title": "List of conservation organisations", "text": "List of conservation organisations\n\nThis is a list of conservation organisations, which are organisations that primarily deal with the conservation of various ecosystems.\nCave Conservancies are land trusts specialized in caves and karst features.\n\n"}
{"id": "53989897", "url": "https://en.wikipedia.org/wiki?curid=53989897", "title": "List of investigational hormonal agents", "text": "List of investigational hormonal agents\n\nThis is a list of investigational hormonal agents, or hormonal agents that are currently under development for clinical use but are not yet approved. \"Chemical/generic names are listed first, with developmental code names, synonyms, and brand names in parentheses.\"\n\n\n\n\n\n\n\n\n\n"}
{"id": "12169395", "url": "https://en.wikipedia.org/wiki?curid=12169395", "title": "List of lakes in Ohio", "text": "List of lakes in Ohio\n\nThe following is a list of lakes in Ohio. According to the Ohio Department of Natural Resources, there are approximately 50,000 lakes and small ponds in the U.S. state of Ohio, with a total surface area of about . About 2,200 of these lakes are or greater, with a total surface area of . These 2,200 include both public and private lakes. The United States Environmental Protection Agency estimated (from an electronic file generated from 1:100,000 scale maps) that Ohio has 5,130 lakes totaling . The difference in the number of lakes estimated by USEPA and ODNR is likely related to numerous small ponds (high number, small acreage) not detected on the 1:100,000 scale maps.\n\nOne of the five Great Lakes is partially within Ohio.\n\nThere are 110 natural inland lakes in Ohio with a surface area of or larger.\n\nThere are 113 artificial inland lakes in or partially within Ohio, with a surface area greater than .\n345 USGS GNIS officially-named lakes in Ohio\n\n1,255 USGS GNIS officially-named reservoirs in Ohio\n\n\n\n"}
{"id": "147337", "url": "https://en.wikipedia.org/wiki?curid=147337", "title": "List of relativistic equations", "text": "List of relativistic equations\n\nFollowing is a list of the frequently occurring equations in the theory of special relativity.\n\nTo derive the equations of special relativity, one must start with two postulates:\n\n\nFrom these two postulates, all of special relativity follows.\n\nIn the following, the relative velocity \"v\" between two inertial frames is restricted fully to the \"x\"-direction, of a Cartesian coordinate system.\n\nThe following notations are used very often in special relativity:\n\n\nwhere Î² = \"v/c\" and \"v\" is the relative velocity between two inertial frames.\n\nFor two frames at rest, Î³ = 1, and increases with relative velocity between the two inertial frames. As the relative velocity approaches the speed of light, Î³ â â.\n\n\nIn this example the time measured in the frame on the vehicle, \"t\", is known as the proper time. The proper time between two events - such as the event of light being emitted on the vehicle and the event of light being received on the vehicle - is the time between the two events in a frame where the events occur at the same location. So, above, the emission and reception of the light both took place in the vehicle's frame, making the time that an observer in the vehicle's frame would measure the proper time.\n\n\nThis is the formula for length contraction. As there existed a proper time for time dilation, there exists a proper length for length contraction, which in this case is \"\". The proper length of an object is the length of the object in the frame in which the object is at rest. Also, this contraction only affects the dimensions of the object which are parallel to the relative velocity between the object and observer. Thus, lengths perpendicular to the direction of motion are unaffected by length contraction.\n\n\n\nIn what follows, bold sans serif is used for 4-vectors while normal bold roman is used for ordinary 3-vectors.\n\n\nwhere formula_12 is known as the metric tensor. In special relativity, the metric tensor is the Minkowski metric:\n\n\nIn the above, \"ds\" is known as the spacetime interval. This inner product is invariant under the Lorentz transformation, that is,\n\nThe sign of the metric and the placement of the \"ct\", \"ct\"', \"cdt\", and \"cdtâ²\" time-based terms can vary depending on the author's choice. For instance, many times the time-based terms are placed first in the four-vectors, with the spatial terms following. Also, sometimes \"Î·\" is replaced with â\"Î·\", making the spatial terms produce negative contributions to the dot product or spacetime interval, while the time term makes a positive contribution. These differences can be used in any combination, so long as the choice of standards is followed completely throughout the computations performed.\n\nIt is possible to express the above coordinate transformation via a matrix. To simplify things, it can be best to replace \"t\", \"tâ²\", \"dt\", and \"dtâ²\" with \"ct\", \"ct\"', \"cdt\", and \"cdtâ²\", which has the dimensions of distance. So:\n\nthen in matrix form:\n\nThe vectors in the above transformation equation are known as four-vectors, in this case they are specifically the position four-vectors. In general, in special relativity, four-vectors can be transformed from one reference frame to another as follows:\n\nIn the above, formula_22 and formula_23 are the four-vector and the transformed four-vector, respectively, and Î is the transformation matrix, which, for a given transformation is the same for all four-vectors one might want to transform. So formula_22 can be a four-vector representing position, velocity, or momentum, and the same Î can be used when transforming between the same two frames. The most general Lorentz transformation includes boosts and rotations; the components are complicated and the transformation requires spinors.\n\nInvariance and unification of physical quantities both arise from four-vectors. The inner product of a 4-vector with itself is equal to a scalar (by definition of the inner product), and since the 4-vectors are physical quantities their magnitudes correspond to physical quantities also.\n\nGeneral doppler shift:\n\nDoppler shift for emitter and observer moving right towards each other (or directly away):\n\nDoppler shift for emitter and observer moving in a direction perpendicular to the line connecting them:\n\n"}
{"id": "9100987", "url": "https://en.wikipedia.org/wiki?curid=9100987", "title": "Lists of physics equations", "text": "Lists of physics equations\n\nIn physics, there are equations in every field to relate physical quantities to each other and perform calculations. Entire handbooks of equations can only summarize most of the full subject, else are highly specialized within a certain field. Physics is derived of formulae only.\n\n\n\n\n\n\n"}
{"id": "312541", "url": "https://en.wikipedia.org/wiki?curid=312541", "title": "Lists of rivers of England and Wales", "text": "Lists of rivers of England and Wales\n\nThe following articles contains lists of rivers of England and Wales:\n\n"}
{"id": "50149", "url": "https://en.wikipedia.org/wiki?curid=50149", "title": "Longitude rewards", "text": "Longitude rewards\n\nThe longitude rewards were the system of inducement prizes offered by the British government for a simple and practical method for the precise determination of a ship's longitude at sea. The rewards, established through an Act of Parliament (the Longitude Act) in 1714, were administered by the Board of Longitude.\n\nThis was by no means the first reward to be offered to solve this problem. Philip II of Spain offered one in 1567, Philip III in 1598 offered 6,000 ducats and a pension, whilst the States General of the Netherlands offered 10,000 florins shortly after. In 1675 Robert Hooke wanted to apply for a Â£1,000 reward in England for his invention of a spring-regulated watch. However, these large sums were never won, though several people were awarded smaller amounts for significant achievements.\n\nThe measurement of longitude was a problem that came into sharp focus as people began making transoceanic voyages.\nDetermining latitude was relatively easy in that it could be found from the altitude of the sun at noon with the aid of a table giving the sun's declination for the day. For longitude, early ocean navigators had to rely on dead reckoning, based on calculations of the vessel's heading and speed for a given time (much of which was based on intuition on the part of the master and/or navigator). This was inaccurate on long voyages out of sight of land, and these voyages sometimes ended in tragedy. An accurate determination of longitude was also necessary to determine the proper \"magnetic declination\", that is, the difference between indicated magnetic north and true north, which can differ by up to 10 degrees in the important trade latitudes of the Atlantic and Indian Oceans. Finding an adequate solution to determining longitude at sea was therefore of paramount importance.\n\nThe Longitude Act only addressed the determination of longitude at sea. Determining longitude reasonably accurately on land was, from the 17th century onwards, possible using the Galilean moons of Jupiter as an astronomical 'clock'. The moons were easily observable on land, but numerous attempts to reliably observe them from the deck of a ship resulted in failure. For details on other efforts towards determining the longitude, see History of longitude.\n\nThe need for better navigational accuracy for increasingly longer oceanic voyages had been an issue explored by many European nations for centuries before the passing of the Longitude Act in England in 1714. Monarchies in Portugal, Spain, and the Netherlands offered financial incentives for solutions to the problem of longitude as early as 1598.\n\nAddressing the problem of longitude fell, primarily, into three categories: terrestrial, celestial, and mechanical. This included detailed atlases, lunar charts, and timekeeping mechanisms at sea. It is postulated by scholars that the economic gains and political power to be had in oceanic exploration, and not scientific and technological curiosity, is what resulted in the swift passing of the Longitude Act of 1714 and the largest and most famous reward, the Longitude Prize being offered.\n\nIn the early 1700s, a series of very public and very tragic maritime disasters occurred, including the wrecking of a squadron of naval vessels on the Scilly Islands in 1707. Around the same time, mathematician Thomas Axe decreed in his will that a Â£1,000 prize be awarded for promising research into finding âtrue longitudeâ and that annual sums be paid to scholars involved in making corrected world maps.\n\nIn 1713, when the longitude proposal of William Whiston and Humphrey Ditton was presented at the opening of the session of Parliament, a general understanding of the longitude problem prompted the formation of a parliamentary committee and the swift passing of the Longitude Act on July 8, 1714. Within this act, is detailed three rewards based on levels of accuracy, which are the same accuracy requirements used for the Axe prize, set by Whiston and Ditton in their petition, and recommended by Sir Isaac Newton and Edmund Halley to the parliamentary committee.\n\nIn addition, rewards were on offer for those who could produce a method that worked within 80 geographical miles of the coast (where ships would be in most danger), and for those with promising ideas who needed financial help to bring them to trial.\n\nProposed methods would be tested by sailing through the ocean, from Britain to any port in West Indies (about six weeks) without losing its longitude beyond the limits listed above. Also, the contender would be required to demonstrate the accuracy of their method by determining the longitude of a specific land based feature whose longitude was already accurately known. The parliamentary committee also established the Board of Longitude. This panel of adjudicators would review proposed solutions and were also given authority to grant up to Â£2,000 in advances for promising projects that did not entirely fulfill the terms of the prize levels, but that were still found worthy of encouragement. The exact terms of the requirements for the prizes would later be contended by several recipients, including John Harrison. Ultimately, the Â£20,000 reward was not awarded to anyone in a lump sum, although John Harrison did receive a series of payments totaling Â£23,065. The Board of Longitude remained in existence for more than 100 years. When it was officially disbanded in 1828, an excess of Â£100,000 had been disbursed.\n\nThe Longitude Act offered a very large incentive for solutions to the longitude problem. Some later recipients of rewards, such as Euler and Mayer, made clear publicly that the money was not the incentive, but instead the important improvements to navigation and cartography. Other recipients, such as Kendall and Harrison had to appeal to the Board of Longitude and other governmental officials for adequate compensation for their work. Still others submitted radical and impractical theories, some of which can be seen in a collection at Harvardâs Houghton Library. Schemes and ideas for improvements to instruments and astronomy, both practical and impractical, can be seen among the digitised archives of the Board of Longitude.\n\nThough the Board of Longitude did not award Â£20,000 at one time, they did offer sums to various individuals in recognition of their work for improvements in instrumentation or in published atlases and star charts.\n\n\nA full list of rewards made by the Commissioners and Board of Longitude was drawn up by Derek Howse, in an Appendix to his article on the finances of the Board of Longitude.\n\nOnly two women are known to have submitted proposals to the Longitude Commissioners, Elizabeth Johnson and Jane Squire. Incoming submissions can be found among the correspondence of the digitised papers of the Board of Longitude.\n\nThe winner of the most reward money under the Longitude Act is John Harrison for sea timekeepers, including his H4 sea watch. Harrison was 21 years old when the Longitude Act was passed. He spent the next 45 years perfecting the design of his timekeepers. He first received a reward from the Commissioners of Longitude in 1737 and did not receive his final payment until he was 80.\n\nHarrison was first awarded Â£250 in 1737, in order to improve on his promising H1 sea clock, leading to the construction of H2. Â£2,000 was rewarded over the span of 1741â1755 for continued construction and completion of H2 and H3. From 1760 to 1765, Harrison received Â£2,865 for various expenses related to the construction, ocean trials, and eventual award for the performance of his sea watch H4. Despite\nthe performance of the H4 exceeding the accuracy requirement of the highest reward possible in the original Longitude Act, Harrison was rewarded Â£7,500 (that is, Â£10,000 minus payments he had received in 1762 and 1764) once he had revealed the method of making his device, and was told that he must show that his single machine could be replicated before the final Â£10,000 could be paid.\n\nHarrison made one rather than the requested two further copies of H4, and he and his family members eventually appealed to King George III after petitions for further rewards were not answered by the Board of Longitude. A reward of Â£8,750 was granted by Parliament in 1773 for a total payment of Â£23,065 spanning thirty-six years.\n\n\n\n\n"}
{"id": "28778141", "url": "https://en.wikipedia.org/wiki?curid=28778141", "title": "Louis-FranÃ§ois-Clement Breguet", "text": "Louis-FranÃ§ois-Clement Breguet\n\nLouis FranÃ§ois ClÃ©ment Breguet (22 December 1804 â 27 October 1883), was a French physicist and watchmaker, noted for his work in the early days of telegraphy.\n\nEducated in Switzerland, Breguet was the grandson of Abraham-Louis Breguet, founder of the watch manufacturing company Breguet. He became manager of \"Breguet et Fils\" watchmakers in 1833 after his father Louis Antoine Breguet retired.\n\nBetween 1835 and 1840 he standardized the company product line of watches, then making 350 watches per year, and diversified into scientific instruments, electrical devices, recording instruments, an electric thermometer, telegraph instruments and electrically synchronized clocks. With Alphonse Foy, in 1842 he developed an electrical needle telegraph to replace the optical telegraph system then in use. and a later step-by-step telegraph system (1847) was applied to French railways and exported to Japan. He observed in 1847 that small wires could be used to protect telegraph installations from lightning, the ancestor of the fuse.\n\nHe also manufactured the rotating mirror FizeauâFoucault apparatus, used by LÃ©on Foucault and Hippolyte Fizeau to measure the speed of light (1850). In 1856 he designed a public network of synchronized electric clocks for the center of Lyon. In 1866 he patented an electric clock controlled by a 100Â Hz tuning fork.\n\nIn 1870 he transferred the leadership of the company to Edward Brown. Breguet then focused entirely on the telegraph and the nascent field of telecommunications. He collaborated in the development of an induction coil, later improved by Heinrich Ruhmkorff.\n\nIn terms of honors, in 1843 he was appointed to the Bureau of Longitudes. In 1845 Breguet was awarded the Legion d'Honneur. He was made a member of the French Academy of Sciences in 1874, and was elevated to Officer of the Legion d'Honneur in 1877. He is one of the 72 French scientists whose names are written around the base of the Eiffel Tower.\n\nBreguet was married and had one son Antoine (1851â1882) who also joined the family electrical business. With his son, he met Alexander Graham Bell and obtained a license to manufacture Bell telephones for the French market. Grandfather of Louis Charles Breguet, aviation pioneer and aircraft manufacturer.\n"}
{"id": "14481970", "url": "https://en.wikipedia.org/wiki?curid=14481970", "title": "Monetae cudendae ratio", "text": "Monetae cudendae ratio\n\n\"Monetae cudendae ratio\" (also spelled \"MonetÃ¦ cudendÃ¦ ratio\"; English: \"On the Minting of Coin\" or \"On the Striking of Coin\"; sometimes, \"Treatise on Money\") is a paper on coinage by Nicolaus Copernicus (Polish: MikoÅaj Kopernik). It was written in 1526 at the request of Sigismund I the Old, King of Poland, and presented to the Prussian Diet.\n\nCopernicus' earliest draft of his essay in 1517 was entitled \"De aestimatione monetae\" (\"On the Value of Coin\"). He revised his original notes, while at Olsztyn (Allenstein) in 1519 (which he defended against the Teutonic Knights), as \"Tractatus de monetis\" (\"Treatise on Coin\") and \"Modus cudendi monetam\" (\"The Way to Strike Coin\"). He made these the basis of a report which he presented to the Prussian Diet at GrudziÄdz (Graudenz) in 1522; Copernicus' friend Tiedemann Giese accompanied him on the trip to Graudenz. For the 1528 Prussian Diet, Copernicus wrote an expanded version of this paper, \"Monetae cudendae ratio\", setting forth a general theory of money.\n\nIn the paper, Copernicus postulated the principle that \"bad money drives out good\", which later came to be referred to as Gresham's Law after a later describer, Sir Thomas Gresham. This phenomenon had been noted earlier by Nicole Oresme, but Copernicus rediscovered it independently. Gresham's Law is still known in Poland and Central and Eastern Europe as the Copernicus-Gresham Law.\n\nIn the same work, Copernicus also formulated an early version of the quantity theory of money, or the relation between a stock of money, its velocity, its price level, and the output of an economy. Like many later classical economists of the 18th and 19th centuries, he focused on the connection between increased money supply and inflation. \n\n\"Monetae cudendae ratio\" also draws a distinction between the use value and exchange value of commodities, anticipating by some 250 years the use of these concepts by Adam Smithâalthough it, too, had antecedents in earlier writers, including Aristotle.\n\nCopernicus' essay was republished in 1816 in the Polish capital, Warsaw, as \"Dissertatio de optima monetae cudendae ratione\" (\"Dissertation on the Optimal Minting of Coin\"), few copies of which survive.\n\n\n\n"}
{"id": "7282196", "url": "https://en.wikipedia.org/wiki?curid=7282196", "title": "Monteggia fracture", "text": "Monteggia fracture\n\nThe Monteggia fracture is a fracture of the proximal third of the ulna with dislocation of the proximal head of the radius. It is named after Giovanni Battista Monteggia.\n\nMechanisms include:\n\nThere are four types (depending upon displacement of the radial head):\n\nThese are known as the Bado types.\n\nMonteggia fractures may be managed conservatively in children with closed reduction (resetting and casting), but due to high risk of displacement causing malunion, open reduction internal fixation is typically performed.\n\nOsteosynthesis (open reduction and internal fixation) of the ulnar shaft is considered the standard of care in adults. It promotes stability of the radial head dislocation and allows very early mobilisation to prevent stiffness. The elbow joint is particularly susceptible to loss of motion.\n\nIn children, the results of early treatment are always good, typically normal or nearly so. If diagnosis is delayed, reconstructive surgery is needed and complications are much more common and results poorer. In adults, the healing is slower and results usually not as good. \n\nComplications of ORIF surgery for Monteggia fractures can include non-union, malunion, nerve palsy and damage, muscle damage, arthritis, tendonitis, infection, stiffness and loss of range of motion, compartment syndrome, audible popping or snapping, deformity, and chronic pain associated with surgical hardware such as pins, screws, and plates. Several surgeries may be needed to correct this type of fracture as it is almost always a very complex fracture that requires a skilled orthopedic surgeon, usually a 'specialist', familiar with this type of injury.\n\n\n"}
{"id": "51119464", "url": "https://en.wikipedia.org/wiki?curid=51119464", "title": "NGC 144", "text": "NGC 144\n\nNGC 144 is a spiral galaxy in the constellation Cetus (the Whale).\n\nThe galaxy was discovered in 1886 by Frank Muller.\n\n"}
{"id": "55911571", "url": "https://en.wikipedia.org/wiki?curid=55911571", "title": "National Centre for Social Research", "text": "National Centre for Social Research\n\nThe National Centre for Social Research is a registered charity trading as NatCen Social Research and is the largest independent social research institute in the UK. The research charity was founded in 1969 by Sir Roger Jowell and Gerald Hoinville with the aim of carrying out rigorous social policy research to improve society. \n\nNatCen is best known for its annual British Social Attitudes Survey, founded by the organisation in 1983. The British Social Attitudes survey is Britain's longest-running annual survey of public attitudes. It uses a random probability method and face to face interviews with more than 3,000 people to ensure that it achieves a sample that is representative of Britain. NatCen's sister organisation, the Scottish Centre for Social Research (ScotCen), carries out an equivalent of the survey in Scotland, called the Scottish Social Attitudes survey.\n\nIn addition to the British Social Attitudes survey, NatCen collects a number of statistics on behalf of the UK government and government bodies. These include the Health Survey for England, the English Housing Survey, The National Diet and Nutrition Survey and the Study of Early Education and Development (SEED). In 2015 NatCen also launched a new panel survey called the NatCen Panel, which was the first panel survey in the UK to use a probability methodology.\nThe National Centre for Social Research is not just a survey organisation, however, it is also well regarded for its research using qualitative and evaluation methods. NatCen researchers authored one of the key textbooks for qualitative researchers \"Qualitative Research Practice\" published by Sage Publishing.\n"}
{"id": "40739183", "url": "https://en.wikipedia.org/wiki?curid=40739183", "title": "Neochlamydia hartmannellae", "text": "Neochlamydia hartmannellae\n\nNeochlamydia hartmannellae is a species of bacteria, the type species of its genus. It is a bacterial endocytobionts of \"Hartmannella vermiformis\", hence its name.\n\n\n"}
{"id": "59174808", "url": "https://en.wikipedia.org/wiki?curid=59174808", "title": "Pauline Atherton Cochrane", "text": "Pauline Atherton Cochrane\n\nPauline Atherton Cochrane (born 1929) is an American librarian and one of the most highly-cited authors in the field of library and information sciences. She is considered a leading researcher in the campaign to redesign catalogues and indexes to provide improved online subject access in library and information services as well as \"a leading teacher and theorist in cataloging, indexing, and information access.\"\n\nCochrane has a B.A. in social science in 1951 from Illinois College. Her first professional job was as an indexer at the Corn Products Refining Company. She went on to receive an M.A. in library science from Rosary College (now Dominican University). She worked as a reference librarian at the Chicago Public Library and Chicago Teacherâs College before going on to pursue a Ph.D. from the University of Chicago. Focusing on classification research she saw her mission \"to make Ranganathanâs writings more accessible to North American LIS researchers, educators, and students.\" She was a co-founder of the Classification Research Study Group in the late 1950's, a group devoted to \"the intellectual/theoretical development of knowledge organization\" based on Ranganathan's Library Research Circle in India and the Classification Research Group in England.\n\nIn 1960 she was made Associate Director of the Documentation Research Project at the American Institute of Physics where she worked on \"A Project for the Development of a Reference Retrieval System for Physicists\" for the next four years. She differentiated how information retrieval would be performed by physicists as researchers versus physicists as authors using four facets which were in use at the American Institute of Physics until 2009. These facets were:\nUsing bibliometrics, Cochrane was able to improve coverage of physics journals in \"Physics Abstracts\" using automated techniques.\n\nIn 1971 Cochrane became president of ASIS&T. During her tenure AIS&T began a continuing education program and prepared an international information science directory. She continued in her work to help librarians learn how to use \"newer\" technology to help patrons find information and created a six part continuing education series for the American Library Association's magazine \"American Libraries\". Entitled \"Modern Subject Access in the Online Age\" Cochrane's lessons, co-written with various LIS colleagues, tackled topics such as creating a professional theory of information seeking behavior in users as well as an early awareness of information overload.\n\n"}
{"id": "3460422", "url": "https://en.wikipedia.org/wiki?curid=3460422", "title": "Psycho-Cybernetics", "text": "Psycho-Cybernetics\n\nPsycho-Cybernetics is a self-help book written by Maxwell Maltz in 1960. Motivational and self-help experts in personal development, including Zig Ziglar, Tony Robbins, Brian Tracy have based their techniques on Maxwell Maltz. Many of the psychological methods of training elite athletes are based on the concepts in Psycho-Cybernetics as well. The book combines the cognitive behavioral technique of teaching an individual how to regulate self-concept, using theories developed by Prescott Lecky, with the cybernetics of Norbert Wiener and John von Neumann. The book defines the mind-body connection as the core in succeeding in attaining personal goals.\n\nMaltz found that his plastic surgery patients often had expectations that were not satisfied by the surgery, so he pursued a means of helping them set the goal of a positive outcome through visualization of that positive outcome. Maltz became interested in why setting goals works. He learned that the power of self-affirmation and mental visualization techniques used the connection between the mind and the body. He specified techniques to develop a positive inner goal as a means of developing a positive outer goal. This concentration on inner attitudes is essential to his approach, as a person's outer success can never rise above the one visualized internally.\n\nThe book rapidly attained best-seller status and has remained in print continuously since its publication in 1960.\n\nSeveral later editions have been produced since his death in 1975.\n"}
{"id": "30138821", "url": "https://en.wikipedia.org/wiki?curid=30138821", "title": "Quantum cognition", "text": "Quantum cognition\n\nQuantum cognition is an emerging field which applies the mathematical formalism of quantum theory to model cognitive phenomena such as information processing by the human brain, language, decision making, human memory, concepts and conceptual reasoning, human judgment, and perception. The field clearly distinguishes itself from the quantum mind as it is not reliant on the hypothesis that there is something micro-physical quantum mechanical about the brain. Quantum cognition is based on the quantum-like paradigm or generalized quantum paradigm or quantum structure paradigm that information processing by complex systems such as the brain, taking into account contextual dependence of information and probabilistic reasoning, can be mathematically described in the framework of quantum information and quantum probability theory.\n\nQuantum cognition uses the mathematical formalism of quantum theory to inspire and formalize models of cognition that aim to be an advance over models based on traditional classical probability theory. The field focuses on modeling phenomena in cognitive science that have resisted traditional techniques or where traditional models seem to have reached a barrier (e.g., human memory), and modeling preferences in decision theory that seem paradoxical from a traditional rational point of view (e.g., preference reversals). Since the use of a quantum-theoretic framework is for modeling purposes, the identification of quantum structures in cognitive phenomena does not presuppose the existence of microscopic quantum processes in the human brain.\n\nThe brain is definitely a macroscopic physical system operating on the scales (of time, space, temperature) which differ crucially from the corresponding quantum scales. (The macroscopic quantum physical phenomena such as e.g. the Bose-Einstein condensate are also characterized by the special conditions which are definitely not fulfilled in the brain.) In particular, the brain is simply too hot to be able perform the real quantum information processing, i.e., to use the quantum carriers of information such as photons, ions, electrons. As is commonly accepted in brain science, the basic unit of information processing is a neuron. It is clear that a neuron cannot be in the superposition of two states: firing and non-firing. Hence, it cannot produce superposition playing the basic role in the quantum information processing. Superpositions of mental states are created by complex networks of neurons (and these are classical neural networks). Quantum cognition community states that the activity of such neural networks can produce effects which are formally described as interference (of probabilities) and entanglement. In principle, the community does not try to create the concrete models of quantum (-like) representation of information in the brain.\n\nThe quantum cognition project is based on the observation that various cognitive phenomena are more adequately described by quantum information theory and quantum probability than by the corresponding classical theories, see examples below. Thus the quantum formalism is considered as an operational formalism describing nonclassical processing of probabilistic data. Recent derivations of the complete quantum formalism from simple operational principles for representation of information supports the foundations of quantum cognition. The subjective probability viewpoint on quantum probability which was developed by C. Fuchs and collaborators also supports the quantum cognition approach, especially using of quantum probabilities to describe the process of decision making.\n\nAlthough at the moment we cannot present the concrete neurophysiological mechanisms of creation of the quantum-like representation of information in the brain, we can present general informational considerations supporting the idea that information processing in the brain matches with quantum information and probability. Here, contextuality is the key word, see the monograph of Khrennikov for detailed representation of this viewpoint. Quantum mechanics is fundamentally contextual. Quantum systems do not have objective properties which can be defined independently of measurement context. (As was pointed by N. Bohr, the whole experimental arrangement must be taken into account.) Contextuality implies existence of incompatible mental variables, violation of the classical law of total probability and (constructive and destructive) interference effects. Thus the quantum cognition approach can be considered as an attempt to formalize contextuality of mental processes by using the mathematical apparatus of quantum mechanics.\n\nSuppose a person is given an opportunity to play two rounds of the following gamble: a coin toss will determine whether the subject wins $200 or loses $100. Suppose the subject has decided to play the first round, and does so. Some subjects are then given the result (win or lose) of the first round, while other subjects are not yet given any information about the results. The experimenter then asks whether the subject wishes to play the second round. Performing this experiment with real subjects gives the following results:\n\n\nGiven these two separate choices, according to the \"sure thing\" principle of rational decision theory, they should also play the second round even if they donât know or think about the outcome of the first round. \nBut, experimentally, when subjects are not told the results of the first round, the majority of them decline to play a second round.\nThis finding violates the law of total probability, yet it can be explained as a quantum interference effect in a manner similar to the explanation for the results from double-slit experiment in quantum physics. Similar violations of the sure-thing principle are seen in empirical studies of the Prisoner's Dilemma and have likewise been modeled in terms of quantum interference.\n\nThe above deviations from classical rational expectations in agentsâ decisions under uncertainty produce well known paradoxes in behavioral economics, that is, the Allais, Ellsberg and Machina paradoxes. These deviations can be explained if one assumes that the overall conceptual landscape influences the subjectâs choice in a neither predictable nor controllable way. A decision process is thus an intrinsically contextual process, hence it cannot be modeled in a single Kolmogorovian probability space, which justifies the employment of quantum probability models in decision theory. More explicitly, the paradoxical situations above can be represented in a unified Hilbert space formalism where human behavior under uncertainty is explained in terms of genuine quantum aspects, namely, superposition, interference, contextuality and incompatibility.\n\nConsidering automated decision making, quantum decision trees have different structure compared to classical decision trees. Data can be analyzed to see if a quantum decision tree model fits the data better.\n\nQuantum probability provides a new way to explain human probability judgment errors including the conjunction and disjunction errors. A conjunction error occurs when a person judges the probability of a likely event L \"and\" an unlikely event U to be greater than the unlikely event U; a disjunction error occurs when a person judges the probability of a likely event L to be greater than the probability of the likely event L \"or\" an unlikely event U. Quantum probability theory is a generalization of Bayesian probability theory because it is based on a set of von Neumann axioms that relax some of the classic Kolmogorov axioms. The quantum model introduces a new fundamental concept to cognitionâthe compatibility versus incompatibility of questions and the effect this can have on the sequential order of judgments. Quantum probability provides a simple account of conjunction and disjunction errors as well as many other findings such as order effects on probability judgments.\n\nThe liar paradox - The contextual influence of a human subject on the truth behavior of a cognitive entity is explicitly exhibited by the so-called liar paradox, that is, the truth value of a sentence like \"this sentence is false\". One can show that the true-false state of this paradox is represented in a complex Hilbert space, while the typical oscillations between true and false are dynamically described by the SchrÃ¶dinger equation.\n\nConcepts are basic cognitive phenomena, which provide the content for inference, explanation, and language understanding. Cognitive psychology has researched different approaches for understanding concepts including exemplars, prototypes, and neural networks, and different fundamental problems have been identified, such as the experimentally tested non classical behavior for the conjunction and disjunction of concepts, more specifically the Pet-Fish problem or guppy effect, and the overextension and underextension of typicality and membership weight for conjunction and disjunction. By and large, quantum cognition has drawn on quantum theory in three ways to model concepts.\n\nThe large amount of data collected by Hampton on the combination of two concepts can be modeled in a specific quantum-theoretic framework in Fock space where the observed deviations from classical set (fuzzy set) theory, the above-mentioned over- and under- extension of membership weights, are explained in terms of contextual interactions, superposition, interference, entanglement and emergence. And, more, a cognitive test on a specific concept combination has been performed which directly reveals, through the violation of Bellâs inequalities, quantum entanglement between the component concepts.\n\nThe hypothesis that there may be something quantum-like about the human mental function was put forward with the quantum entanglement formula which attempted to model the effect that when a wordâs associative network is activated during study in memory experiment, it behaves like a quantum-entangled system. Models of cognitive agents and memory based on quantum collectives have been proposed by Subhash Kak. But he also points to specific problems of limits on observation and control of these memories due to fundamental logical reasons.\n\nThe research in (iv) had a deep impact on the understanding and initial development of a formalism to obtain semantic information when dealing with concepts, their combinations and variable contexts in a corpus of unstructured documents. This conundrum of natural language processing (NLP) and information retrieval (IR) on the web â and data bases in general â can be addressed using the mathematical formalism of quantum theory. As basic steps, (a) the seminal book \"The Geometry of Information Retrieval\" by K. Van Rijsbergen introduced a quantum structure approach to IR, (b) Widdows and Peters utilised a quantum logical negation for a concrete search system, and Aerts and Czachor identified quantum structure in semantic space theories, such as latent semantic analysis. Since then, the employment of techniques and procedures induced from the mathematical formalisms of quantum theory â Hilbert space, quantum logic and probability, non-commutative algebras, etc. â in fields such as IR and NLP, has produced significant results.\n\nBi-stable perceptual phenomena is a fascinating topic in the area of perception. If a stimulus has an ambiguous interpretation, such as a Necker cube, the interpretation tends to oscillate across time. Quantum models have been developed to predict the time period between oscillations and how these periods change with frequency of measurement. Quantum theory and an appropriate model have been developed by Elio Conte to account for interference effects obtained with measurements of ambiguous figures.\n\nThere are apparent similarities between Gestalt perception and quantum theory. In an article discussing the application of Gestalt to chemistry, Anton Amann writes: \"Quantum mechanics does \"not\" explain Gestalt perception, of course, but in quantum mechanics and Gestalt psychology there exist almost isomorphic conceptions and problems:\n\nAmann comments: \"The structural similarities between Gestalt perception and quantum mechanics are on a level of a parable, but even parables can teach us something, for example, that quantum mechanics is more than just production of numerical results or that the Gestalt concept is more than just a silly idea, incompatible with atomistic conceptions.\"\n\nThe assumption that information processing by the agents of the market follows the laws of quantum information theory and quantum probability was actively explored by many authors, e.g., E. Haven, O. Choustova, A. Khrennikov, see the book of E. Haven and A. Khrennikov, for detailed bibliography. We can mention, e.g., the Bohmian model of dynamics of prices of shares in which the quantum(-like) potential is generated by expectations of agents of the financial market and, hence, it has the mental nature. This approach can be used to model real financial data, see the book of E. Haven and A. Khrennikov (2012).\n\nAn isolated quantum system is an idealized theoretical entity. In reality interactions with environment have to be taken into account. This is the subject of theory of open quantum systems. Cognition is also fundamentally contextual. The brain is a kind of (self-)observer which makes context dependent decisions. Mental environment plays a crucial role in information processing. Therefore, it is natural to apply theory of open quantum systems to describe the process of decision making as the result of quantum-like dynamics of the mental state of a system interacting with an environment. The description of the process of decision making is mathematically equivalent to the description of the process of decoherence. This idea was explored in a series of works of the multidisciplinary group of researchers at Tokyo University of Science.\n\nSince in the quantum-like approach the formalism of quantum mechanics is considered as a purely operational formalism, it can be applied to the description of information processing by any biological system, i.e., not only by human beings.\n\nOperationally it is very convenient to consider e.g. a cell as a kind of decision maker processing information in the quantum information framework. This idea was explored in a series of papers of the Swedish-Japanese research group using the methods of theory of open quantum systems: genes expressions were modeled as decision making in the process of interaction with environment.\n\nHere is a short history of applying the formalisms of quantum theory to topics in psychology. Ideas for applying quantum formalisms to cognition first appeared in the 1990s by Diederik Aerts and his collaborators Jan Broekaert, Sonja Smets and Liane Gabora, by Harald Atmanspacher, Robert Bordley, and Andrei Khrennikov. A special issue on \"Quantum Cognition and Decision\" appeared in the Journal of Mathematical Psychology (2009, vol 53.), which planted a flag for the field. A few books related to quantum cognition have been published including those by Khrennikov (2004, 2010), Ivancivic and Ivancivic (2010), Busemeyer and Bruza (2012), E. Conte (2012). The first Quantum Interaction workshop was held at Stanford in 2007 organized by Peter Bruza, William Lawless, C. J. van Rijsbergen, and Don Sofge as part of the 2007 AAAI Spring Symposium Series. This was followed by workshops at Oxford in 2008, SaarbrÃ¼cken in 2009, at the 2010 AAAI Fall Symposium Series held in Washington, D.C., 2011 in Aberdeen, 2012 in Paris, and 2013 in Leicester. Tutorials also were presented annually beginning in 2007 until 2013 at the annual meeting of the Cognitive Science Society. A \"Special Issue on Quantum models of Cognition\" appeared in 2013 \"Topics in Cognitive Science.\"\n\nIt was suggested by theoretical physicists David Bohm and Basil Hiley that mind and matter both emerge from an \"implicate order\". Bohm and Hiley's approach to mind and matter is supported by philosopher Paavo PylkkÃ¤nen. PylkkÃ¤nen underlines \"unpredictable, uncontrollable, indivisible and non-logical\" features of conscious thought and draws parallels to a philosophical movement some call \"post-phenomenology\", in particular to Pauli PylkkÃ¶'s notion of the \"aconceptual experience\", an unstructured, unarticulated and pre-logical experience.\n\nThe mathematical techniques of both Conte's group and Hiley's group involve the use of Clifford algebras. These algebras account for \"non-commutativity\" of thought processes (for an example, \"see:\" noncommutative operations in everyday life).\n\nHowever, an area that needs to be investigated is the concept lateralised brain functioning. Some studies in marketing have related lateral influences on cognition and emotion in processing of attachment related stimuli.\n\n\n\n"}
{"id": "32463401", "url": "https://en.wikipedia.org/wiki?curid=32463401", "title": "Synthetic ecosystems", "text": "Synthetic ecosystems\n\nSynthetic ecosystems are on-chip integrated devices where cellular cultures (individuals) and ecosystem services - such as the renewal of growth, delivery of regulatory signals as well as removal of waste - are patterned into an integrated fluidic device using principles of landscape ecology, physiology and cell signaling.\n\nKlitgord N, SegrÃ¨ D (2010) Environments that Induce Synthetic Microbial Ecosystems. PLoS Comput Biol 6(11): e1001002. doi:10.1371/journal.pcbi.1001002\n"}
{"id": "59118860", "url": "https://en.wikipedia.org/wiki?curid=59118860", "title": "Tepidisphaera", "text": "Tepidisphaera\n\nTepidisphaera is a genus of bacteria from the family of Planctomycetaceae with one known species (\"Tepidisphaera mucosa\"). \"Tepidisphaera mucosa\" has been isolated from a hot spring from the Lake Baikal in Kamchatka in Russia.\n"}
{"id": "4768509", "url": "https://en.wikipedia.org/wiki?curid=4768509", "title": "The Second Stage", "text": "The Second Stage\n\nThe Second Stage is a 1981 book by American feminist, activist and writer Betty Friedan, best known for her earlier book \"The Feminine Mystique\".\n\nFriedan contends that \"first stage\" of feminism, a movement intended to liberate women from their traditional role as only mothers and house-wives, was coming to an end with the deadline for the ratification of The Equal Rights Amendment, and that it was time to take feminism to a new stage, which could better deal with the issues of a new generation of women.\n\nIssues discussed include: the double enslavement of women at work and at home, the social evolution of masculinity, political backlash to feminist lobbying, developments in the field of management and leadership, and the need to recognize the social and economic value of traditional female occupations.\n"}
{"id": "39717446", "url": "https://en.wikipedia.org/wiki?curid=39717446", "title": "Tietomaa", "text": "Tietomaa\n\nTietomaa is a science center in Oulu, Finland. It is located in Myllytulli neighbourhood near the city centre. It is the first science centre in Finland and it was opened in June 1988.\n\nTietomaa is located in old buildings of the Veljekset ÃstrÃ¶m Oy, a former leather processing factory. The main exhibitions are located in the former power station. Tietomaa's observation tower with a glass elevator outside the tower is a former water tower of the factory.\n\n"}
{"id": "51873625", "url": "https://en.wikipedia.org/wiki?curid=51873625", "title": "Welcome to the Universe", "text": "Welcome to the Universe\n\nWelcome to the Universe: An Astrophysical Tour is a popular science book by Neil deGrasse Tyson, Michael A. Strauss, and J. Richard Gott, based on an introductory astrophysics course they co-taught at Princeton University. The book was published by the Princeton University Press on September 20, 2016.\n\n\"Welcome to the Universe: An Astrophysical Tour\" has been praised by literary critics. \"Kirkus Reviews\" described the book as \"an accessible and comprehensive overview of our universe by three eminent astrophysicists\" and \"an entertaining introduction to astronomy.\" John Timpane of \"The Philadelphia Inquirer\" similarly called it \"a well-illustrated tour that includes Pluto, questions of intelligent life, and whether the universe is infinite.\" \"Publishers Weekly\" wrote:\n"}
{"id": "13989702", "url": "https://en.wikipedia.org/wiki?curid=13989702", "title": "Wolfram Demonstrations Project", "text": "Wolfram Demonstrations Project\n\nThe Wolfram Demonstrations Project is an organized, open-source collection of small (or medium-size) interactive programs called Demonstrations, which are meant to visually and interactively represent ideas from a range of fields. It is hosted by Wolfram Research, whose stated goal is to bring computational exploration to the widest possible audience. At its launch, it contained 1300 demonstrations but has grown to over 10,000. The site won a Parents' Choice Award in 2008.\n\nThe Demonstrations run in \"Mathematica\" 6 or above and in \"Wolfram CDF Player\" which is a free modified version of Wolfram's \"Mathematica\" and available for Windows, Linux and Macintosh and can operate as a web browser plugin.\n\nThey typically consist of a very direct user interface to a graphic or visualization, which dynamically recomputes in response to user actions such as moving a slider, clicking a button, or dragging a piece of graphics. Each Demonstration also has a brief description of the concept.\n\nDemonstrations are now easily embeddable into any website or blog. Each Demonstration page includes a snippet of JavaScript code in the Share section of the sidebar.\n\nThe website is organized by topic: for example, science, mathematics, computer science, art, biology, and finance. They cover a variety of levels, from elementary school mathematics to much more advanced topics such as quantum mechanics and models of biological organisms. The site is aimed at both educators and students, as well as researchers who wish to present their ideas to the broadest possible audience.\n\nWolfram Research's staff organizes and edits the Demonstrations, which may be created by any user of \"Mathematica\", then freely published and freely downloaded. The Demonstrations are open-source, which means that they not only demonstrate the concept itself but also show how to implement it.\n\nThe use of the web to transmit small interactive programs is reminiscent of Sun's Java applets, Adobe's Flash, and the open-source processing. However, those creating Demonstrations have access to the algorithmic and visualization capabilities of \"Mathematica\" making it more suitable for technical demonstrations. \n\nThe Demonstrations Project also has similarities to user-generated content websites like Wikipedia and Flickr. Its business model is similar to Adobe's Acrobat and Flash strategy of charging for development tools but providing a free reader.\n\n"}
{"id": "28637211", "url": "https://en.wikipedia.org/wiki?curid=28637211", "title": "WÃ¼stenhaus SchÃ¶nbrunn", "text": "WÃ¼stenhaus SchÃ¶nbrunn\n\nThe WÃ¼stenhaus SchÃ¶nbrunn (\"SchÃ¶nbrunn Desert House\") is a desert botanical exhibit in Vienna, Austria. It is located in the Sonnenuhrhaus (\"Sundial House\"), which was built in 1904 as the newest of the four botanical houses in SchÃ¶nbrunn Palace Park. The desert exhibit opened in 2004 as a counterpart to the \"Rainforest House\" that opened in 2002 in the nearby Zoo Vienna.\n\nThe Sundial House stands opposite the SchÃ¶nbrunn Palm House (\"Palmenhaus\"; another botanical exhibit), directly between the Hietzing Gate and the Zoo. The unprepossessing building owes its name to the sundial (\"Sonnenuhr\") located in the gardens to the south.\n\nIt was built with the encouragement of Charles von HÃ¼gel â diplomat, explorer and founder of the Vienna Horticultural Society â to replace an earlier greenhouse which could no longer meet its plants' needs. At first it housed the plants of the extensive \"New Holland Collection\" which HÃ¼gel had assembled, and which had been acquired by the Imperial Court in 1848, and later expanded with plants from southern Africa and the Americas that required similar conditions. The architect of the 1904 building was Alphons Custodis.\n\nA bomb attack in February 1945, which almost totally destroyed the windows of the nearby Palm House, left the bulk of the Sundial House's glazing intact, probably because the Palm House stood between it and the bombed area, and because the Sundial House had windows which were roughly parallel to the spreading blast waves (unlike the Palm House). A number of plants from the Palm House were therefore brought here for safekeeping, where space allowed. The Sundial House again served as a refuge between 1986 and 1990, while the Palm House was renovated.\n\nIn April 1990 the first butterfly zoo in Austria was established in the Sundial House, but it was transferred to the greenhouse in the Burggarten in 1998.\n\nRust on the steel framework caused the building to be closed in 1998 and renovated from 2000 to 2003. The Desert House was finally established here by a joint project between the Zoo and the Austrian Federal Gardens (\"BundesgÃ¤rten\"), which have managed the building since 1918 as successor to the Imperial and Royal Court Gardens. The exhibition includes succulent plants from the Federal Gardens, and small animals under the care of the Zoo, such as desert jerboas, reptiles and birds.\n\n\n300Â ft. long, 45Â ft. wide and 50Â ft. high, the building is fully glazed on the roof and the south face, while the north face is walled up. With a total floor space of 14,000 sq. ft., the interior is divided lengthwise into three sections; there are two annexes to the central section, namely a plant-rich east wing which serves as the entrance hall, and a west wing used as a coldhouse.\n\n\n"}
{"id": "11801199", "url": "https://en.wikipedia.org/wiki?curid=11801199", "title": "Zero field NMR", "text": "Zero field NMR\n\nZero field NMR is the acquisition of nuclear magnetic resonance spectra in an environment carefully screened from magnetic fields (Including from the Earth's field). It is useful for studying chemicals with magnetically active nuclei (spins 1/2 and greater), and for studying molecular dynamics.\n\nThe development of very sensitive magnetic sensors such as SQUID, GMR, and atomic magnetometers in the 2000s has made it possible to detect NMR signals directly in zero-field environments. Previous zero-field NMR experiments relied on indirect detection where the sample had to be shuttled from the shielded zero-field environment into a high magnetic field for detection with a conventional inductive pick-up coil.\n\nOne successful implementation was using atomic magnetometers at zero magnetic field working with rubidium vapor cells to detect zero-field NMR.\n\nIt is sometimes but inaccurately referred to as nuclear quadrupole resonance (NQR).\n\n\n\n"}
