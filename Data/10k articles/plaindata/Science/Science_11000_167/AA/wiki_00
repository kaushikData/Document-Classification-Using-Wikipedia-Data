{"id": "5950276", "url": "https://en.wikipedia.org/wiki?curid=5950276", "title": "Aerostatics", "text": "Aerostatics\n\nA subfield of fluid statics, aerostatics is the study of gases that are not in motion with respect to the coordinate system in which they are considered. The corresponding study of gases in motion is called aerodynamics.\n\nAerostatics studies density allocation, especially in air. One of the applications of this is the barometric formula.\n\nAn aerostat is a lighter than air craft, such as an airship or balloon, which uses the principles of aerostatics to float.\n\nTreatment of the equations of gaseous behaviour at rest is generally taken, as in hydrostatics, to begin with a consideration of the general equations of momentum for fluid flow, which can be expressed as:\n\nformula_1,\n\nwhere formula_2 is the mass density of the fluid, formula_3 is the instantaneous velocity, formula_4 is fluid pressure, formula_5 are the external body forces acting on the fluid, and formula_6 is the momentum transport coefficient. As the fluid's static nature mandates that formula_7, and that formula_8, the following set of partial differential equations representing the basic equations of aerostatics is found.\n\nformula_9\n\nHowever, the presence of a non-constant density as is found in gaseous fluid systems (due to the compressibility of gases) requires the inclusion of the ideal gas law:\n\nformula_10,\n\nwhere formula_11 denotes the universal gas constant, and formula_12 the temperature of the gas, in order to render the valid aerostatic partial differential equations:\n\nformula_13,\n\nwhich can be employed to compute the pressure distribution in gases whose thermodynamic states are given by the equation of state for ideal gases.\n\n\n"}
{"id": "4292269", "url": "https://en.wikipedia.org/wiki?curid=4292269", "title": "Alphabet (formal languages)", "text": "Alphabet (formal languages)\n\nIn formal language theory, a string is defined as a finite sequence of members of an underlying base set; this set is called the alphabet of a string or collection of strings. The members of the set are called \"symbols\", and are typically thought of as representing letters, characters, or digits. For example, a common alphabet is {0,1}, the binary alphabet, and a binary string is a string drawn from the alphabet {0,1}. An infinite sequence of letters may be constructed from elements of an alphabet as well.\n\nIf \"L\" is a formal language, i.e. a (possibly infinite) set of finite-length strings, the alphabet of \"L\" is the set of all symbols that may occur in any string in \"L\".\nFor example, if \"L\" is the set of all variable identifiers in the programming language C, \"L\"’s alphabet is the set { a, b, c, ..., x, y, z, A, B, C, ..., X, Y, Z, 0, 1, 2, ..., 7, 8, 9, _ }.\n\nGiven an alphabet formula_1, the set of all strings of length formula_2 over the alphabet formula_1 is indicated by formula_4. The set formula_5 of all finite strings (regardless of their length) is indicated by the Kleene star operator as formula_6, and is also called the Kleene closure of formula_1. The notation formula_8 indicates the set of all infinite sequences over the alphabet formula_1, and formula_10 indicates the set formula_11 of all finite or infinite sequences.\n\nFor example, using the binary alphabet {0,1}, the strings ε, 0, 1, 00, 01, 10, 11, 000, etc. are all in the Kleene closure of the alphabet (where ε represents the empty string).\n\nAlphabets are important in the use of formal languages, automata and semiautomata. In most cases, for defining instances of automata, such as deterministic finite automata (DFAs), it is required to specify an alphabet from which the input strings for the automaton are built. In these applications, an alphabet is usually required to be a finite set, but is not otherwise restricted.\n\nWhen using automata, regular expressions, or formal grammars as part of string-processing algorithms, the alphabet may be assumed to be the character set of the text to be processed by these algorithms, or a subset of allowable characters from the character set.\n\n\n"}
{"id": "14617046", "url": "https://en.wikipedia.org/wiki?curid=14617046", "title": "Boethius (Mercurian crater)", "text": "Boethius (Mercurian crater)\n\nBoethius is a crater on Mercury, named after Boethius, the Roman philosopher.\n"}
{"id": "39452320", "url": "https://en.wikipedia.org/wiki?curid=39452320", "title": "Bottlebrush (cave formation)", "text": "Bottlebrush (cave formation)\n\nA Bottlebrush is a cave formation which results when a stalactite is immersed in rising water which is supersaturated with calcium carbonate. The stalactite becomes coated with pool spar.\n"}
{"id": "40939955", "url": "https://en.wikipedia.org/wiki?curid=40939955", "title": "Chatt Lecture", "text": "Chatt Lecture\n\nThe Chatt Lecture, named after Joseph Chatt is a lectureship of the John Innes Centre\n\n"}
{"id": "5250810", "url": "https://en.wikipedia.org/wiki?curid=5250810", "title": "Chemistry set", "text": "Chemistry set\n\nA chemistry set is an educational toy allowing the user (typically a teenager) to perform simple chemistry experiments.\n\nThe earliest forerunners of the chemistry set are 17th century books on \"natural magick\", \"which all excellent wise men do admit and embrace, and worship with great applause; neither is there any thing more highly esteemed, or better thought of, by men of learning.\" Authors such as Giambattista della Porta included chemical magic tricks and scientific puzzles along with more serious topics.\n\nThe earliest chemistry sets were developed in the 18th century in England and Germany, with the purpose of teaching chemistry to adults. In 1791, \"Description of a portable chest of chemistry : or, Complete collection of chemical tests for the use of chemists, physicians, mineralogists, metallurgists, scientific artists, manufacturers, farmers, and the cultivators of natural philosophy\" by Johann Friedrich August Göttling, translated from German, was published in English. Friedrich Accum of London, England also sold portable chemistry sets and materials to refill them. Used primarily for training druggists and medical students, they could also be carried and used in the field.\n\nScientific kits also attracted well-educated members of the upper class who enjoyed experimenting and demonstrating their results. James Woodhouse of Philadelphia presented a \"Young Chemist's Pocket Companion\" (1797) with an accompanying portable laboratory, specifically targeted ladies and gentlemen. Jane Marcet's books on chemistry helped to popularize chemistry as a well-to-do pastime for both men and women.\n\nBeginning in the late 1850s John J. Griffin & Sons sold a line of \"chemical cabinets\", eventually offering 11 categories. These were marketed primarily to adults including elementary school teachers as well as students at the Royal Naval College, the Royal Agricultural Society, and the universities of Oxford and Cambridge.\n\nIn the mid to late 1800s England, magic and illusion toys enabled children to make their own fireworks, create disappearing inks and cause changes in color, tricks which were mostly chemically based. The \"Columbian Cyclopedia\" of 1897 defines \"CHEMISTRY TOYS\" as \"mostly pyrotechnic; recommended as illustrating to the young the rudiments of chemistry, but probably more dangerous than efficient for such use\", listing a variety of hazardous examples.\n\nBeginning in the early 1900s, modern chemistry sets targeted younger people with the intention to popularize chemistry. In the United States, Porter Chemical Company and the A. C. Gilbert Company produced the best known sets. Although Porter and Gilbert were the largest American producers of chemistry sets, other manufacturers such as the Skilcraft corporation were also active.\n\nJohn J. Porter and his brother Harold Mitchell Porter began The Porter Chemical Company in 1914. Their initial purpose was to sell packaged chemicals, but they soon introduced kits. John researched the experiments, while Harold wrote the instruction manuals. Their earliest toys, under the \"Chemcraft\" trademark, were \"chemical magic\" sets, selling for less than one dollar (or about $25 in 2017). By the 1920s, they sold six different sets, the largest of which sold for $25 (or about $320 in 2017). Their range of toys broadened throughout the 1930s. In the 1950s it was possible to buy toys featuring radioactive ores, such as the \"Gilbert U-238 Atomic Energy Laboratory,\" which included a Geiger counter and cloud chamber.\n\nAlfred Carlton Gilbert earned money by performing magic tricks while a medical student at Yale. He and John Petrie formed the Mysto Manufacturing Company (later the A. C. Gilbert Company) in 1909, and began selling boxed magic sets. By 1917, they sold chemistry sets, which they produced through World War II, in spite of restrictions on materials. Robert Treat Johnson, noting the number of chemistry students at Yale whose interest in the science began with a chemistry set, argued the production of chemistry sets was a \"patriotic duty.\"\n\nToy companies promoted chemistry sets through advertising campaigns, the \"Chemcraft Chemist Club\" and its accompanying \"Chemcraft Science Magazine\", comic books, and essay contests such as Porter's \"Why I want to be a scientist\". The goal of attracting students to a potential career in chemistry was often explicit in the sets' naming and promotion. Chemistry sets may have been the first American toys marketed toward parents with the goal of \"improving\" children for success in later life.\n\nThe target market for chemistry sets was almost exclusively at boys, whom they deemed \"young men of science.\" However, during the 1950s, Gilbert managed to introduce a set targeting girls. They sold the set in an attractive pink box, but the set identified girls as \"laboratory assistants\" or \"lab technicians,\" not scientists.\n\nWell-known chemistry sets from the United Kingdom include the 1960s and 1970s sets by Thomas Salter Science (produced in Scotland) and later Salter Science, then the \"MERIT\" sets through the 1970s and 1980s. Dekkertoys created a range of sets which were similar, complete with glass test tubes of dry chemicals.\n\nAround the 1960s, increasing social distrust of chemistry, safety concerns, and government regulation began to limit the range of materials and experiments available in chemistry sets. In the United States, the Federal Hazardous Substances Labeling Act of 1960, the Toy Safety Act of 1969 and the Consumer Product Safety Commission, established in 1972, and the Toxic Substances Control Act of 1976 all introduced new levels of regulation, which led to the decline of chemistry sets' popularity during the 1970s and 1980s. The A. C. Gilbert Company went out of business in 1967, and the Porter Chemical Company went out of business in 1984.\n\nModern chemistry sets, with a few exceptions, tend to include a more restricted range of chemicals and simplified instructions. Many chemistry kits are single use, containing only the types and amounts of chemicals for a specific application. Several authors note from the 1980s on, concerns about illegal drug production, terrorism, and legal liability have led to chemistry sets becoming increasingly bland and unexciting.\n\nNonetheless, a GCSE equipment set was produced, offering students better equipment, and there is a more up-market range of sets available from Thames & Kosmos such as the C3000 Kit.\n\nTypical contents found in chemistry sets, including equipment and chemicals, might include:\n\n\nThe experiments described in the instruction manual typically require a number of chemicals not shipped with the chemistry set, because they are common household chemicals:\nOther chemicals, including strong acids, bases and oxidizers cannot be safely shipped with the set and others having a limited shelf life have to be purchased separately from a drug store:\n\n\n"}
{"id": "28362507", "url": "https://en.wikipedia.org/wiki?curid=28362507", "title": "DENIS J081730.0-615520", "text": "DENIS J081730.0-615520\n\nDENIS J081730.0-615520 (also known as 2MASS 08173001-6155158) is a T brown dwarf approximately away in the constellation Carina. It was discovered by Etienne Artigau and his colleagues in April 2010. The star belongs to the T6 spectral class implying a photosphere temperature of about 950 K. It has a mass of about 15 M (Jupiter masses) or about 1.5% the mass of the Sun.\n\nDENIS J081730.0-615520 is the second-nearest isolated T dwarf to the Sun (after UGPS J0722-05) and the fifth-nearest (also after ε Indi Bab and SCR 1845-6357B) if one takes into account T dwarfs in multiple star systems. It is also the brightest T dwarf in the sky (in the J-band); it had been missed before due to its proximity to the galactic plane.\n\n"}
{"id": "9819646", "url": "https://en.wikipedia.org/wiki?curid=9819646", "title": "DEN 1048−3956", "text": "DEN 1048−3956\n\nDEN 1048−3956 is a brown dwarf about 13.1 light years from Earth in the southern constellation of Antlia, among the closest interstellar objects to Earth. This substellar object is very dim with an apparent magnitude of about 17, and will require a telescope with a camera to be seen. It was discovered in 2000 by Xavier Delfosse (Institute of Astrophysics of the Canary Islands, now Observatoire de Grenoble) and Thierry Forveille (Canada–France–Hawaii Telescope Corporation), with the assistance of nine other astronomers.\n\nIn 2005 a powerful flare from this object was detected by radio astronomy.\n\n\n\n"}
{"id": "15083895", "url": "https://en.wikipedia.org/wiki?curid=15083895", "title": "DMRG of the Heisenberg model", "text": "DMRG of the Heisenberg model\n\nWithin the study of the quantum many-body problem in physics, the DMRG analysis of the Heisenberg model is an important theoretical example applying techniques of the density matrix renormalization group (DMRG) to the Heisenberg model of a chain of spins. This article presents the \"infinite\" DMRG algorithm for the formula_1 antiferromagnetic Heisenberg chain, but the recipe can be applied for every translationally invariant one-dimensional lattice.\n\nDMRG is a renormalization-group technique because it offers an efficient truncation of the Hilbert space of one-dimensional quantum systems.\n\nTo simulate an infinite chain, starting with four sites. The first is the \"block site\", the last the \"universe-block site\" and the remaining are the \"added sites\", the right one is added to the universe-block site and the other to the block site.\n\nThe Hilbert space for the single site is formula_2 with the base formula_3. With this base the spin operators are formula_4, formula_5 and formula_6 for the single site. For every block, the two blocks and the two sites, there is its own Hilbert space formula_7, its base formula_8 (formula_9)and its own operators formula_10:\nAt the starting point all four Hilbert spaces are equivalent to formula_2, all spin operators are equivalent to formula_4, formula_5 and formula_6 and formula_37. This is always (at every iterations) true only for left and right sites.\n\nThe ingredients are the four block operators and the four universe-block operators, which at the first iteration are formula_38 matrices, the three left-site spin operators and the three right-site spin operators, which are always formula_38 matrices. The Hamiltonian matrix of the \"superblock\" (the chain), which at the first iteration has only four sites, is formed by these operators. In the Heisenberg antiferromagnetic S=1 model the Hamiltonian is:\n\nformula_40\n\nThese operators live in the superblock state space: formula_41, the base is formula_42. For example: (convention):\n\nformula_43\n\nformula_44\n\nThe Hamiltonian in the \"DMRG form\" is (we set formula_45):\n\nformula_46\n\nThe operators are formula_47 matrices, formula_48, for example:\n\nformula_49\n\nformula_50\n\nAt this point you must choose the eigenstate of the Hamiltonian for which some observables is calculated, this is the \"target state\" . At the beginning you can choose the ground state and use some advanced algorithm to find it, one of these is described in:\nThis step is the most time-consuming part of the algorithm.\n\nIf formula_51 is the target state, expectation value of various operators can be measured at this point using formula_52.\n\nForm the reduced density matrix formula_53 for the first two block system, the block and the left-site. By definition it is the formula_54 matrix:\nformula_55\nDiagonalize formula_53 and form the formula_57 matrix formula_58, which rows are the formula_59 eigenvectors associated with the formula_59 largest eigenvalues formula_61 of formula_53. So formula_58 is formed by the most significant eigenstates of the reduced density matrix. You choose formula_59 looking to the parameter formula_65: formula_66.\n\nForm the formula_54 matrix representation of operators for the system composite of the block and left-site, and for the system composite of right-site and universe-block, for example:\n\nformula_68\n\nformula_69\n\nformula_70\n\nformula_71\n\nNow, form the formula_72 matrix representations of the new block and universe-block operators, form a new block by changing basis with the transformation formula_58, for example:\nformula_74\nAt this point the iteration is ended and the algorithm goes back to step 1.\nThe algorithm stops successfully when the observable converges to some value.\n\n\n"}
{"id": "1708940", "url": "https://en.wikipedia.org/wiki?curid=1708940", "title": "Denaturation (fissile materials)", "text": "Denaturation (fissile materials)\n\nDenaturation of fissile materials suitable for nuclear weapons is the process of transforming them into a form that is not suitable for weapons use and can not easily be reversely transformed. For Uranium 235 this is straightforward, by mixing it with Uranium 238, but for plutonium it is more difficult and/or less effective, because other plutonium isotopes are either also suitable for weapons or not available and not practical to produce, while mixing with another element allows chemical separation.\n\nThe situation with Uranium-233 is more drastic. Decay of the attached Uranium-232 produces Thorium-228 with a radioactive half-life of 1.9 years and several short-lived daughter nuclides; these daughters include some very hard gamma-ray emitters like Thallium-208 and Lead-212. After approximately one single year the alpha activity of these decay products is several hundred curies per kilogram of U-233, and the gamma penetration radiation is a thousand times larger to some than from the plutonium. Aged U-233 is self-protected radiologically from diversion.\n"}
{"id": "20710642", "url": "https://en.wikipedia.org/wiki?curid=20710642", "title": "Destarch", "text": "Destarch\n\nDestarching occurs when part of a plant is \"deprived of starch, as by translocation\".\n\nIt is also the process of eliminating starch reserves in a plant for experiments concerning photosynthesis. This is done by leaving the plant(s) in a dark place for a long period of time. Due to the lack of photosynthesis in this place, stored starch is used up, thus the plant is destarched.\n"}
{"id": "42892336", "url": "https://en.wikipedia.org/wiki?curid=42892336", "title": "Disinhibited social engagement disorder", "text": "Disinhibited social engagement disorder\n\nDisinhibited Social Engagement Disorder (DSED) or Disinhibited Attachment Disorder of Childhood is an attachment disorder that consists of \"a pattern of behavior in which a child actively approaches and interacts with unfamiliar adults.\" and which \"...significantly impairs young children’s abilities to relate interpersonally to adults and peers.\" For example, sitting on the lap of a stranger or peer, or leaving with a stranger. DSED is exclusively a childhood disorder and is not diagnosed before the age of nine months or if symptoms did not appear until after the age of five. Infants and very young children are at risk if they receive inconsistent or insufficient care from a primary caregiver.\n\nThe most obvious symptom is \"...the absence of normal fear or discretion when approaching strangers. The child is unusually comfortable talking to, touching, and leaving a location with an adult stranger.\" DSED has some similar symptoms of Attention Deficit Hyperactivity Disorder (ADHD).\nCognitive delay\n\nLanguage delay\n\nSpeech delay\n\nDSED is a result of inconsistent or absent connection to a primary caregiver in the first years of life. Children who are institutionalized may receive inconsistent care, isolation during hospitalization, and parental problems such as mental health problems which interfere with attachment such as depression or a personality disorder, absence, poverty, teen parenting, or substance abuse. DSED \"...may have a biological cause in some cases (e.g., Williams syndrome).\"\n\nThe criteria for Disinhibited Social Engagement Disorder in the DSM-5:\n\nA. A pattern of behavior in which a child actively approaches and interacts with unfamiliar adults and exhibits at least two of the following:\n\n\nB. The behaviors in Criterion A are not limited to impulsivity (as in Attention-Deficit/Hyperactivity Disorder) but include socially disinhibited behavior.\n\nC. The child has exhibited a pattern of extremes of insufficient care as evidenced by at least one of the following:\n\n\nD. The care in Criterion C is presumed to be responsible for the disturbed behavior in Criterion A (e.g., the disturbances in Criterion A began following the pathogenic care in Criterion C).\n\nE. The child has a developmental age of at least nine months.\"\n\nSpecifiers\n\nIt is considered persistent if the duration is more than 12 months.\n\nIt is considered severe if all the symptoms are present.\n\nThe ICD-10 definition is: \"A particular pattern of abnormal social functioning that arises during the first five years of life and that tends to persist despite marked changes in environmental circumstances, e.g. diffuse, nonselectively focused attachment behaviour, attention-seeking and indiscriminately friendly behaviour, poorly modulated peer interactions; depending on circumstances there may also be associated emotional or behavioural disturbance.\"\nAttention Deficit Hyperactivity Disorder\n\nTwo effective treatment approaches are play therapy and expressive therapy help form attachment through multi-sensory means and some therapy can be non-verbal. \n\nThe exact prevalence is unknown. In high risk individuals, the prevalence rate is 20%.\nDisinhibited Social Engagement Disorder (DSM-5 313.89 (F94.2)) is the 2013 Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5) name formerly listed as a subtype of reactive attachment disorder (RAD) called \"disinhibited attachment disorder\" (DAD). The corresponding disorder in ICD-10 is Disinhibited Attachment Disorder of Childhood.\n\nThe American Psychiatric Association considers \"...disinhibited social engagement disorder more closely resembles ADHD; it may occur in children who do not necessarily lack attachments and may have established or even secure attachments. The two disorders differ in other important ways, including correlates, course, and response to intervention, and for these reasons are considered separate disorders.\"\n"}
{"id": "51424743", "url": "https://en.wikipedia.org/wiki?curid=51424743", "title": "Dragonfly 44", "text": "Dragonfly 44\n\nDragonfly 44 is an ultra diffuse galaxy in the Coma Cluster. Observations of the velocity dispersion suggest a mass of about one trillion solar masses, about the same as the mass of the Milky Way; the galaxy shows no evidence of rotation. This is also consistent with about 90 globular clusters observed around Dragonfly 44. However, the galaxy emits only 1% of the light emitted by the Milky Way. The galaxy was discovered with the Dragonfly Telephoto Array.\n\nTo determine the amount of dark matter in this galaxy, they used the DEIMOS instrument installed on Keck II to measure the velocities of stars for 33.5 hours over a period of six nights so they could determine the galaxy’s mass.\nThe scientists then used the Gemini Multi-Object Spectrograph on the 8-m Gemini North telescope to reveal a halo of spherical clusters of stars around the galaxy’s core.\n\nIn August 2016, astronomers reported that this galaxy might be made almost entirely of dark matter.\n\n"}
{"id": "52303418", "url": "https://en.wikipedia.org/wiki?curid=52303418", "title": "Dynamic scaling", "text": "Dynamic scaling\n\nDynamic scaling (sometimes known as Family-Vicsek scaling) is the litmus test of showing that an evolving system exhibits self-similarity. In general a function is said to exhibit dynamic scaling if it satisfies:\n\nHere the exponent formula_2 is fixed by the dimensional requirement formula_3. \nNow, the numerical value of formula_4 should remain invariant despite the unit of measurement of formula_5 is changed by some factor since formula_6 is a dimensionless quantity. However, Tamás Vicsek and Fereydoon Family first proposed the idea of dynamic scaling in the context of diffusion-limited aggregation DLA of clusters in two dimensions. The form of their proposal for dynamic scaling was:\n\nMany phenomena which physicists often investigate are not static but rather evolve probabilistically with time. The universe is perhaps one of the best examples which is expanding ever since the Big Bang. Similarly, growth of networks like the Internet are also ever growing systems. Another example is polymer degradation where degradation does not occur in a blink of an eye but rather over quite a long time. Spread of biological and computer viruses too does not happen over night. Many of these evolves in a self-similar fashion in the sense that data obtained from the snapshot at any fixed time is similar to the respective data taken from the snapshot of any earlier or later time. That is, the system is similar to itself at different times. The litmus test of such self-similarity is provided by the dynamic scaling.\n\nIn such system we find certain stochastic variable formula_8 which assume values that depend on time. In such cases, we are often interested to know the distribution of formula_8 at various instants of time i.e. formula_10. Now the numerical value of formula_11 and the typical or mean value of formula_8 may well be very different at every different instant measurement. The question is: What happens to the corresponding dimensionless variables? If the numerical values of the dimensional quantities are different, however, corresponding dimensionless quantities remain invariant then we can argue that the snapshot of the system at different times are similar. When this happens we conclude that the system is self-similar. \n\nOne way of verifying the dynamic scaling is to plot dimensionless variables formula_13 as a function of formula_14 of the data extracted at various different time. Then if all the plots of formula_11 vs formula_8 obtained at different times collapse onto a single universal curve then it is said that the systems at different time are similar and it obeys dynamic scaling. The idea of data collapse is deeply rooted to the Buckingham formula_17 theorem. Essentially such systems can be termed as temporal self-similarity since the same system is similar at different times.\n\nThere have many seemingly disparate systems which are found to exhibit dynamic scaling e.g., kinetics of aggregation described by Smoluchowski coagulation equation, complex network described by Barabasi–Albert model, kinetic and stochastic Cantor set. The growth model within the Kardar–Parisi–Zhang (KPZ) class, one find that the width of the surface formula_18 exhibits dynamic scaling. The area size distribution of the blocks of weighted planar stochastic lattice (WPSL) too exhibits dynamic scaling.\n"}
{"id": "12438341", "url": "https://en.wikipedia.org/wiki?curid=12438341", "title": "EDGE of Existence programme", "text": "EDGE of Existence programme\n\nThe EDGE of Existence programme is a research and conservation initiative that focuses on species deemed to be the world’s most \"Evolutionarily Distinct and Globally Endangered\" (EDGE). Developed by the Zoological Society of London (ZSL), the programme aims to raise awareness of the world’s EDGE species, implement targeted research and conservation actions to halt their decline, and to train in-country conservationists (called EDGE Fellows) to protect them now and in the future.\n\nThe EDGE programme seeks to:\n\nThe EDGE of Existence programme is centred on an interactive website that features information on the top 100 EDGE mammals, reptiles, birds, amphibians and top 25 EDGE corals, detailing their specific conservation requirements. Each of the top 100 species is given an EDGE-ometer rating according to the degree of conservation attention they are currently receiving, as well as its perceived rarity in its natural environment. Recent research suggests that 70% of the world’s top 100 EDGE mammals are currently receiving little or no conservation attention.\n\nEDGE research and conservation is carried out by ZSL researchers, a large network of partner organizations and in-country scientists. An integral part of the EDGE programme is the EDGE Fellowship scheme, which provides funding and support to in-country scientists for field research on the conservation status and threats facing a particular EDGE species. EDGE Fellows participate in all phases of a research project, from study design to data collection, analysis and interpretation and receive guidance and training in monitoring techniques, community outreach and education. Each project is focused on delivering a conservation action plan.\n\nOnce the action plan is completed, a meeting with local stakeholders is held to make additions and corrections to the document and to agree on a timeline and institutional responsibilities.\n\n"}
{"id": "1728145", "url": "https://en.wikipedia.org/wiki?curid=1728145", "title": "Effective half-life", "text": "Effective half-life\n\nIn pharmacokinetics, effective half-life is the rate of accumulation or elimination of a biochemical or pharmacological substance in an organism; the analogue of biological half-life when the kinetics are governed by multiple independent mechanisms. This is seen when there are multiple mechanisms of elimination, or when a drug occupies multiple pharmacological compartments. It reflects the cumulative effect of the individual half-lives, as observed by the changes in the actual serum concentration of a drug under a given dosing regimen. The complexity of biological systems means that most pharmacological substances do not have a single mechanism of elimination, and hence the observed or effective half-life does not reflect that of a single process, but rather the summation of multiple independent processes.\n\nWhen radionuclides are used pharmacologically, for example in radiation therapy, they are eliminated through a combination of radioactive decay and biological excretion. An effective half-life of the drug will involve a decay constant that represents the sum of the biological and physical decay constants, as in the formula:\n\nWith the decay constant it is possible to calculate the effective half-life using the formula:\n\nThe biological decay constant is often approximated as it is more difficult to accurately determine than the physical decay constant.\n\nAlternatively, since the radioactive decay contributes to the \"\"physical\" (i.e. \"radioactive\")\" half-life, while the metabolic elimination processes determines the \"biological\" half-life of the radionuclide, the two act as parallel paths for elimination of the radioactivity, the effective half-life could also be represented by the formula:\n\n"}
{"id": "22222481", "url": "https://en.wikipedia.org/wiki?curid=22222481", "title": "Euler's laws of motion", "text": "Euler's laws of motion\n\nIn classical mechanics, Euler's laws of motion are equations of motion which extend Newton's laws of motion for point particle to rigid body motion. They were formulated by Leonhard Euler about 50 years after Isaac Newton formulated his laws.\n\nEuler's first law states that the linear momentum of a body, (also denoted ) is equal to the product of the mass of the body and the velocity of its center of mass :\n\nInternal forces between the particles that make up a body do not contribute to changing the total momentum of the body as there is an equal and opposite force resulting in no net effect. The law is also stated as:\n\nwhere is the acceleration of the centre of mass and is the total applied force on the body. This is just the time derivative of the previous equation ( is a constant).\n\nEuler's second law states that the rate of change of angular momentum (sometimes denoted ) about a point that is fixed in an inertial reference frame (often the mass center of the body), is equal to the sum of the external moments of force (torques) acting on that body (also denoted or ) about that point: \n\nNote that the above formula holds only if both and are computed with respect to a fixed inertial frame or a frame parallel to the inertial frame but fixed on the center of mass. \nFor rigid bodies translating and rotating in only 2D, this can be expressed as:\nwhere is the position vector of the center of mass with respect to the point about which moments are summed, is the angular acceleration of the body about its center of mass, and is the moment of inertia of the body about its center of mass.\nSee also Euler's equations (rigid body dynamics).\n\nThe distribution of internal forces in a deformable body are not necessarily equal throughout, i.e. the stresses vary from one point to the next. This variation of internal forces throughout the body is governed by Newton's second law of motion of conservation of linear momentum and angular momentum, which for their simplest use are applied to a mass particle but are extended in continuum mechanics to a body of continuously distributed mass. For continuous bodies these laws are called Euler’s laws of motion. If a body is represented as an assemblage of discrete particles, each governed by Newton’s laws of motion, then Euler’s equations can be derived from Newton’s laws. Euler’s equations can, however, be taken as axioms describing the laws of motion for extended bodies, independently of any particle distribution.\n\nThe total body force applied to a continuous body with mass , mass density , and volume , is the volume integral integrated over the volume of the body:\n\nwhere is the force acting on the body per unit mass (dimensions of acceleration, misleadingly called the \"body force\"), and is an infinitesimal mass element of the body.\n\nBody forces and contact forces acting on the body lead to corresponding moments (torques) of those forces relative to a given point. Thus, the total applied torque about the origin is given by\n\nwhere and respectively indicate the moments caused by the body and contact forces.\n\nThus, the sum of all applied forces and torques (with respect to the origin of the coordinate system) acting on the body can be given as the sum of a volume and surface integral:\n\nwhere is called the surface traction, integrated over the surface of the body, in turn denotes a unit vector normal and directed outwards to the surface .\n\nLet the coordinate system be an inertial frame of reference, be the position vector of a point particle in the continuous body with respect to the origin of the coordinate system, and be the velocity vector of that point.\n\nEuler’s first axiom or law (law of balance of linear momentum or balance of forces) states that in an inertial frame the time rate of change of linear momentum of an arbitrary portion of a continuous body is equal to the total applied force acting on that portion, and it is expressed as\n\nEuler’s second axiom or law (law of balance of angular momentum or balance of torques) states that in an inertial frame the time rate of change of angular momentum of an arbitrary portion of a continuous body is equal to the total applied torque acting on that portion, and it is expressed as\n\nWhere formula_11 is the velocity, formula_12 the volume, and the derivatives of and are material derivatives.\n\n"}
{"id": "9917", "url": "https://en.wikipedia.org/wiki?curid=9917", "title": "Explorers Program", "text": "Explorers Program\n\nThe Explorers Program is a United States space exploration program that provides flight opportunities for physics, geophysics, heliophysics, and astrophysics investigations from space. Launched in 1958, Explorer 1 was USA's first spacecraft to achieve orbit. Over\n90 space missions have been launched since. Starting with Explorer 6, it has been operated by NASA, with regular collaboration with a variety of other institutions, including many international partners.\n\nLaunchers for the Explorer program have included Jupiter C (Juno I), Juno II, various Thor, Scout, Delta and Pegasus rockets, and Falcon 9.\n\nCurrently, the program is divided into \"MIDEX\", \"SMEX\", and \"UNEX\", with select \"Missions of Opportunity\" operated with other agencies.\n\nThe Explorers Program was the United States's first successful attempt to launch an artificial satellite. It began as a U.S. Army proposal (Project Orbiter) to place a, \"civilian\", scientific satellite into orbit during the International Geophysical Year; however, that proposal was rejected in favor of the U.S. Navy's Project Vanguard, which included the first sub-orbital flight Vanguard TV0. in December 1956. The Explorers Program was later reestablished to catch up with the Soviet Union its launch of Sputnik 1 on October 4, 1957 sparked the Sputnik crisis. Explorer 1 was launched on January 31, 1958, becoming the first U.S. satellite, as well as discovering the Van Allen radiation belt.\nAfter NASA was established in 1958, the Explorers Program was transferred from the US Army. NASA continued to use the name for an ongoing series of relatively small space missions, typically an artificial satellite with a specific science focus. Explorer 6 in 1959 was the first scientific satellite under the project direction of NASA's Goddard Space Flight Center. Over the following two decades, NASA has launched over 50 explorer missions, some in conjunction to military programs, usually of an exploratory or survey nature or had specific objectives not requiring the capabilities of a major observatory. Explorers satellites have made many important discoveries on: Earth's magnetosphere and the shape of its gravity field; the solar wind; properties of micrometeoroids raining down on the Earth; ultraviolet, cosmic, and X-rays from the Solar System and universe beyond; ionospheric physics; Solar plasma; solar energetic particles; and atmospheric physics. These missions have also investigated air density, radio astronomy, geodesy, and gamma ray astronomy. \n\nWith drops in NASA's budget, explorer missions became infrequent in early 1980s. In 1988, the Small Explorer (SMEX) program was established with a focus on frequent flight opportunities for highly focused and relatively inexpensive space science missions in the disciplines of astrophysics and space physics. The first three SMEX missions were chosen in April 1989 out of 51 candidates, and launched in 1992, 1996 and 1998. The second set of two missions were announced in September 1994 and launched in 1998 and 1999.\n\nBy mid 1990s, NASA initiated the Medium-class Explorer (MIDEX) program to enable more frequent flights. These were larger than SMEX missions but smaller and less expensive than \"Delta-class missions\", and were to be launched aboard a new Med-Lite class launch vehicle. This new launch vehicle was not developed and instead, these missions were flown on a modified Delta II rocket. The first announcement opportunity for MIDEX was issued in March 1995, and the first launch under this new program was FUSE in 1999.\n\nIn May 1994 NASA also started a new, Student Explorer Demonstration Initiative (STEDI) pilot program, to demonstrate that high-quality space science can be carried out with small, low-cost missions. Of the three selected missions, only, SNOE was launched in 1998 and TERRIERS in 1999, but the latter failed after launch. The STEDI program was terminated in 2001. Later, NASA established the University-Class Explorers (UNEX) program for much cheaper missions, and is regarded as a successor to STEDI.\n\nThe Explorer missions were at first managed by the Small Explorer Project Office at NASA's Goddard Space Flight Center (GSFC). In early 1999, that office was closed and with the announcement of opportunity for the third set of SMEX missions NASA converted the SMEX program so that each mission was managed by its Principal Investigator, with oversight by the GSFC Explorers Project. The Explorers Program Office at Goddard Space Flight Center in Greenbelt, Maryland, provides management of the many operational scientific exploration missions that are characterized by relatively moderate costs and small to medium-sized missions that are capable of being built, tested, and launched in a short time interval compared to larger observatories like NASA's Great Observatories.\n\nExcluding the launches, the MIDEX program has a current mission cap cost of in 2018, with future MIDEX missions being capped at $350 million. The cost cap for SMEX missions in 2017 was $165 million. UNEX missions are capped at $15 million. A subprogram called Missions of Opportunity (MO) has funded science instruments or hardware components of onboard non-NASA space missions, and have a total NASA cost cap of $70 million.\n\nThe selection of the next MIDEX mission is scheduled for 2019 between: Arcus, SPHEREx, while the MO selection includes the Compton Spectrometer and Imager Explorer balloon, Transient Astrophysics Observer on the ISS, and Contribution to ARIEL Spectroscopy of Exoplanets (CASE) in conjunction with ESA.\n\nExplorers name numbers can be found in the NSSDC master catalog, typically assigned to each spacecraft in a mission. These numbers were not officially assigned until after 1975.\n\nMany missions are proposed, but not selected. For example, in 2011, the Explorers Program received 22 full missions solicitations, 20 Missions of Opportunity, and 8 USPI. Sometimes mission are only partially developed but must be stopped for financial, technological, or bureaucratic reasons. Some missions failed upon reaching orbit including WIRE and TERRIERS.\n\nExamples of missions that were not developed or cancelled were:\n\n\nRecent examples of conclusions of launched missions, cancelled due to budgetary constraints:\n\nApproximate number of launches per decade: \n\n\n"}
{"id": "3800394", "url": "https://en.wikipedia.org/wiki?curid=3800394", "title": "Gaea (crater)", "text": "Gaea (crater)\n\nGaea is an impact crater on Amalthea, one of the small moons of Jupiter. The crater is 75 km wide and at least 10–20 km deep. Its center coordinates are 50°S, 95°W. Gaea is one of two named craters on Amalthea, the other being Pan. It is named after the Greek goddess Gaia.\n\nThird of Gaea's interior is covered by a bright spot – the largest on Amalthea. Its brightness is at least 2.3 times greater than outside. This spot is about 25 km wide and appear to be extended outside the crater.\n\nGaea is located near the South pole of Amalthea, far south from the two bright areas, Lyctos Facula and Ida Facula, which are positioned on the slopes of a prominent mountain elongated along meridian.\n\n"}
{"id": "30858993", "url": "https://en.wikipedia.org/wiki?curid=30858993", "title": "Goldstino", "text": "Goldstino\n\nThe goldstino is the Nambu−Goldstone fermion emerging in the spontaneous breaking of supersymmetry. It is the close fermionic analog of the Nambu−Goldstone bosons controlling the spontaneous breakdown of ordinary bosonic symmetries. \n\nAs in the case of Goldstone bosons, it is massless, unless there is, in addition, a small explicit supersymmetry breakdown involved, on top of the basic spontaneous breakdown; in this case it develops a \"small\" mass, analogous to that of Pseudo-Goldstone bosons of chiral symmetry breaking.\n\nIn theories where supersymmetry is a global symmetry, the goldstino is an ordinary particle (possibly the lightest supersymmetric particle, responsible for dark matter). \n\nIn theories where supersymmetry is a local symmetry, the goldstino is absorbed by the gravitino, the gauge field it couples to, becoming its longitudinal component, and giving it nonvanishing mass. This mechanism \nis a close analog of the way the Higgs field gives nonzero mass to the W and Z bosons. \n\nVestigial bosonic superpartners of the goldstinos, called \"sgoldstinos\", might also appear, but need not, as supermultiplets have been reduced to arrays. In effect, SSB of supersymmetry, by definition, implies a nonlinear realization of the supersymmetry in the Nambu−Goldstone mode, in which the goldstino couples \"identically to all particles\" in these arrays, and is thus the superpartner of \"all of them\", equally.\n"}
{"id": "8841036", "url": "https://en.wikipedia.org/wiki?curid=8841036", "title": "High-definition metrology", "text": "High-definition metrology\n\nIn metrology, high-definition metrology is where measurements are made densely across the observable extent of that surface or object and displayed with high-definition. In that sense, \"high-definition\" is analogous to \"high definition television\".\n\nHigh-definition measurements are therefore contrasted with \"global\" or \"overall\" or \"statistical\" measurements which provide some single or coarsely sampled measurement values that do not define the surface or object attribute variations in detail. Whereas the latter measurements may provide an indication of some overall characteristic of the item being measured, high-definition metrology is used where it is desired to also know more precisely at what location certain attributes occur, or where their values are outside of some specified range of values. In precision manufacturing, for example, this knowledge may enable remedial action to be taken to correct or control process variables that affect the dimensions of a manufactured part or assembly.\n"}
{"id": "24041303", "url": "https://en.wikipedia.org/wiki?curid=24041303", "title": "Hour circle", "text": "Hour circle\n\nIn astronomy, the hour circles, which together with declination and distance (from the planet's centre of mass) locate any celestial object, is the great circle through the object and the two celestial poles. As such, it is a higher concept than the meridian as defined in astronomy, which takes account of the terrain and depth to the centre of Earth at a ground observer's location. The hour circles, specifically, are perfect circles perpendicular (at right angles) to the celestial equator. By contrast, the declination of an object viewed on the celestial sphere is the angle of that object to/from the celestial equator (thus ranging from +90° to -90°). \n\nThe location of stars, planets, and other similarly distant objects is usually expressed in the following parameters, one for each of the three spatial dimensions: their declination, right ascension (epoch-fixed hour angle), and distance. These are as located at the vernal equinox for the epoch (e.g. J2000) stated.\n\nA meridian on the celestial sphere matches an hour circle at any time. The hour circle is a subtype whereby it is expressed in hours as opposed to degrees, radians, or other units of angle. The hour circles make for easy prediction of the angle (and time due to Earth's fairly regular rotation, approximately equal to the time) between the observation of two objects at the same, or similar declination. The hour circles (meridians) are measured in hours (or hours, minutes, and seconds); one rotation (360°) is equivalent to 24 hours; 1 hour is equivalent to 15°. \n\nAn astronomical meridians follow the same concept and, almost precisely, orientation of a meridian (also known as longitude) on a globe.\n\n\n"}
{"id": "57258458", "url": "https://en.wikipedia.org/wiki?curid=57258458", "title": "International Journal of Qualitative Methods", "text": "International Journal of Qualitative Methods\n\nThe International Journal of Qualitative Methods is a quarterly peer-reviewed open access academic journal covering research methods with respect to qualitative and mixed methods research. It was established in 2002 and is published by SAGE Publications on behalf of the University of Alberta's International Institute for Qualitative Methodology, of which it is the official journal. The editor-in-chief is Linda Liebenberg (Dalhousie University). According to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 1.387, ranking it 35th out of 98 journals in the category \"Social Sciences, Interdisciplinary\".\n"}
{"id": "23705312", "url": "https://en.wikipedia.org/wiki?curid=23705312", "title": "International Liquid Crystal Society", "text": "International Liquid Crystal Society\n\nThe International Liquid Crystal Society (ILCS) was founded in 1990 and currently has over 900 members in 43 countries worldwide. The ILCS was conceived in 1987 by Lui Lam and took three years to be founded at University of British Columbia, Vancouver, Canada, during the 13th International Liquid Crystal Conference. Specifically, the ILCS was born on July 27, 1990, in Vancouver, with 22 Members. \n\nThe aim of the Society is to unite scientists, engineers and students working in the broad field of fundamental and applied aspects of different liquid crystal systems, including thermotropic, lyotropic, polymer and polymer-modified liquid crystals. ILCS publishes two online publications on liquid crystals, Liquid Crystals Today, appearing under the sponsorship of Taylor & Francis, and archive-style electronic liquid crystal communications platform, e-lc.\n\n"}
{"id": "9584270", "url": "https://en.wikipedia.org/wiki?curid=9584270", "title": "Jan Ambjørn", "text": "Jan Ambjørn\n\nJan Ambjørn is a Danish physicist regarded as the primary founder of Causal Dynamical Triangulation Theory (CDT).\n\nAmbjørn began in the early 1990s searching for a physics model that bonded quantum mechanics and relativistic gravity in a way that didn't require supersymmetry. By 1994, he argued a simpler way to represent quantum gravity was to use Lorentzian space-time geometry as a framework. His first major publication on the lattice model that would become CDT theory was released in 1998. It was then that he and Renate Loll made triangulation and the causal approach the cornerstones to the lattice gauge theory.\n\nThe theory of Causal Dynamical Triangulations faced harsh scrutiny for some years because most physicists preferred models that required more dimensions and a fixed background.\n\nBy 2004, Ambjørn's assertion of the benefits to a non-perturbative quantum gravity model regained attention. \n\nAs of 2007, Ambjørn continues to research lattice gauge, string, and quantum gravity theories; matrix models and their applications; and statistical theories of random surfaces and paths. He is currently sharing a position as a professor at the Niels Bohr Institute and the Radboud University Nijmegen.\n\nAmbjørn has worked with or been acknowledged as an influence by quantum physicists such as Renate Loll, Fotini Markopoulou, Jerzy Jurkiewicz, and Lee Smolin.\n\n\n"}
{"id": "8035886", "url": "https://en.wikipedia.org/wiki?curid=8035886", "title": "Jansha (impact crater)", "text": "Jansha (impact crater)\n\nJansha is an impact crater on the southern hemisphere of Saturn's moon Enceladus. Jansha was first observed in \"Cassini\" images during that mission's March 2005 flyby of Enceladus. It is located at 30.4° South Latitude, 156.9° West Longitude, and is 9.8 kilometers across. Several of the southwest-northeast trending fractures that are prevalent in the region cut across Jansha, forming canyons several hundred meters deep along Jansha's rim.\n\nJansha is named after the hero in \"The Tale of Jansha\" (or Janshah) from \"The Book of One Thousand and One Nights\".\n"}
{"id": "24529515", "url": "https://en.wikipedia.org/wiki?curid=24529515", "title": "List of DVD recordable manufacturers", "text": "List of DVD recordable manufacturers\n\nThis aims to be a complete list of DVD recordable manufacturers.\nThis list is not necessarily complete or up to date - if you see a manufacturer that should be here but isn't (or one that shouldn't be here but is), please update the page accordingly.\n\n\n\n\n\n\n\n"}
{"id": "590280", "url": "https://en.wikipedia.org/wiki?curid=590280", "title": "List of Hewlett-Packard products", "text": "List of Hewlett-Packard products\n\nThe following is a partial list of products manufactured under the Hewlett-Packard brand.\n\nHP Categories of Printers as of November 2014 are:\n\n\nHigh-volume black and white laser printers\n\nOffice black and white laser printers\n\nDiscontinued models\n\nCompact photo printers\n\nBusiness ink multifunction printers\n\nPage wide array printers\n\n6+3262\n\n36+41\n\nDiscontinued models\n\nDiscontinued models \n\nInkjet Digital Web Press\n\nCurrent Line: (June 2015)\n\nCurrent Line: (June 2015)\n\nCurrent Line: (November 2014)\n\nPrinter Notes: \nIn HP printers introduced since ca 2006, alpha codes indicate product groupings and optional features, thus for example:\n\n\"\"* Lead alpha codes:\"\n\n\"* Trailing alpha description codes:\"\n\n\n\n\n\n\"Source: HP Handheld/Pocket/Palmtop PCs\"\n\nHP 9800 series desktop computers as follows:\n\n\nSee \"HP X-Terminals\"\n\nSee \"HP Business Desktops\"\n\n\"See also HP Mobile Thin Clients\"\n\n\nHP Blackbird 002\n\nCompaq Evo line of business desktops and laptops was rebranded HP Compaq (see below for recent products).\n\n\"See the HP EliteBook article for more details.\"\n\nThe xx30 generation comprised the following notebooks:\nThe xx40 series comprised the following models:\nThe xx60 series, announced on February 23, 2011, comprised the following models:\nThe fourth generation, announced on May 9, 2012, comprised the following models:\n\nIn chronological order of release,\n\nThis series of notebooks was discontinued after HP's acquisition of Compaq.\n\nExclusive Compaq brand of notebooks\n\nA series of multimedia notebooks. Some models had the HP developed QuickPlay software which enabled booting to a linux based DVD/Music player held on a separate partition.\n\nDespite the ProLiant name on some of HP's entry level servers, they are based on former HP tc series (NetServer) servers, and as such do not come with Compaq's SmartStart or Insight Management Agents.\n\nThese are in a tower form factor.\n\nThese are in a rack mount form factor.\nThe ProLiant servers below are based on Compaq's ProLiants and do come with SmartStart and Compaq's Insight Management Agents:\n\nThese are in a tower form factor.\nNote that 'e' indicates 'essential' and 'p' indicates 'performance' variants.\n\nThese are in a rack mount form factor.\n\n\n\n\n\nThese are in a blade form factor.\n\n\n\n\n\n\n\nHP Integrity\n\n\nIntegrity BL blades\nCompaq ProLiant\n\n\n\n\nProCurve Networking by HP is the networking division of HP.\n\n\n\n"}
{"id": "54171959", "url": "https://en.wikipedia.org/wiki?curid=54171959", "title": "List of Sudanese scientists", "text": "List of Sudanese scientists\n\nThe following people are notable \"Sudanese scientists\"':\n\n"}
{"id": "2678380", "url": "https://en.wikipedia.org/wiki?curid=2678380", "title": "List of compounds with carbon number 17", "text": "List of compounds with carbon number 17\n\nThis is a partial list of molecules that contain 17 carbon atoms.\n\n"}
{"id": "19984408", "url": "https://en.wikipedia.org/wiki?curid=19984408", "title": "List of cultural icons of the Netherlands", "text": "List of cultural icons of the Netherlands\n\nThe List of cultural icons of the Netherlands is a list of links to potential cultural icons of the Netherlands .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "36971126", "url": "https://en.wikipedia.org/wiki?curid=36971126", "title": "List of human leukocyte antigen alleles associated with cutaneous conditions", "text": "List of human leukocyte antigen alleles associated with cutaneous conditions\n\nThere are many human leukocyte antigen (HLA) alleles associated with conditions of or affecting the human integumentary system \n\n\n"}
{"id": "28969317", "url": "https://en.wikipedia.org/wiki?curid=28969317", "title": "List of medical schools in Russia", "text": "List of medical schools in Russia\n\nThis is a list of medical universities located in Russia.\n\n\n\n"}
{"id": "4647220", "url": "https://en.wikipedia.org/wiki?curid=4647220", "title": "List of multiple discoveries", "text": "List of multiple discoveries\n\nHistorians and sociologists have remarked the occurrence, in science, of \"multiple independent discovery\". Robert K. Merton defined such \"multiples\" as instances in which similar discoveries are made by scientists working independently of each other. \"Sometimes,\" writes Merton, \"the discoveries are simultaneous or almost so; sometimes a scientist will make a new discovery which, unknown to him, somebody else has made years before.\"\n\nCommonly cited examples of multiple independent discovery are the 17th-century independent formulation of calculus by Isaac Newton, Gottfried Wilhelm Leibniz and others, described by A. Rupert Hall; the 18th-century discovery of oxygen by Carl Wilhelm Scheele, Joseph Priestley, Antoine Lavoisier and others; and the theory of the evolution of species, independently advanced in the 19th century by Charles Darwin and Alfred Russel Wallace.\n\nMultiple independent discovery, however, is not limited to only a few historic instances involving giants of scientific research. Merton believed that it is multiple discoveries, rather than unique ones, that represent the \"common\" pattern in science.\n\nMerton contrasted a \"multiple\" with a \"singleton\"—a discovery that has been made uniquely by a single scientist or group of scientists working together.\n\nA distinction is drawn between a discovery and an invention, as discussed for example by Bolesław Prus. However, since the same phenomenon of multiplicity occurs in relation to both discoveries and inventions, this article lists both multiple discoveries and multiple \"inventions\".\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "5868205", "url": "https://en.wikipedia.org/wiki?curid=5868205", "title": "Marcellin Boule", "text": "Marcellin Boule\n\nMarcellin Boule (1 January 1861 – 4 July 1942) was a French palaeontologist.\n\nHe studied and published the first analysis of a complete Neanderthal specimen. The fossil discovered in La Chapelle-aux-Saints was an old man, and Boule characterized it as brutish, bent kneed and not a fully erect biped. In an illustration he commissioned, the Neanderthal was characterized as a hairy gorilla-like figure with opposable toes, according to a skeleton which was already distorted with arthritis. As a result, Neanderthals were viewed in subsequent decades as being highly primitive creatures with no direct relation to anatomically modern humans. Later re-evaluations of the La Chapelle-aux-Saints skeleton have roundly discredited Boule's initial work on the specimen.\n\nHe was one of the first to argue that eoliths were not manmade.\n\nBoule also expressed some scepticism about the \"Piltdown man\" discovery — later revealed to be a hoax. As early as 1915, Boule recognized that the jaw belonged to an ape rather than an ancient human. However, the Piltdown forgery has been characterised as providing evidential support for Boule's \"branching evolution\" conclusions drawn from his Neanderthal research — research which is likewise said to have \"prepar[ed] the international community for the appearance of a non-Neanderthal fossil such as Piltdown Man.\"\n"}
{"id": "2054011", "url": "https://en.wikipedia.org/wiki?curid=2054011", "title": "Military Intelligence Hall of Fame", "text": "Military Intelligence Hall of Fame\n\nThe Military Intelligence Hall of Fame is a hall of fame established by the Military Intelligence Corps of the United States Army in 1988 to honor soldiers and civilians who have made exceptional contributions to military intelligence. The hall is administered by the United States Army Intelligence Center at Fort Huachuca, Arizona.\n\n\n"}
{"id": "18075484", "url": "https://en.wikipedia.org/wiki?curid=18075484", "title": "Modern Operating Systems", "text": "Modern Operating Systems\n\nModern Operating Systems is a book written by Andrew Tanenbaum, a version (which does not target implementation) of his book \"\". It is now in its 4th edition, published March 2014 ().\n\n\"Modern Operating Systems\" (mostly known as \"MOS\") is a popular book across the globe and includes the fundamentals of an operating system with small amounts of code written in autonomous C language. \"MOS\" describes many scheduling algorithms.\n"}
{"id": "56080225", "url": "https://en.wikipedia.org/wiki?curid=56080225", "title": "Mucrocetin", "text": "Mucrocetin\n\nMucrocetin is a snake venom platelet-agglutinating factor, that acts in a vWF-independent manner. Binds specifically to platelet GPIbalpha (GP1BA) to a distinct binding site from that of flavocetin-A.\n\nThe platelet-agglutinating factor isolated from the venom of Taiwan habu (Protobothrops mucrosquamatus).\n"}
{"id": "19607864", "url": "https://en.wikipedia.org/wiki?curid=19607864", "title": "Open-notebook science", "text": "Open-notebook science\n\nOpen-notebook science is the practice of making the entire primary record of a research project publicly available online as it is recorded. This involves placing the personal, or laboratory, notebook of the researcher online along with all raw and processed data, and any associated material, as this material is generated. The approach may be summed up by the slogan 'no insider information'. It is the logical extreme of transparent approaches to research and explicitly includes the making available of failed, less significant, and otherwise unpublished experiments; so called 'dark data'. The practice of open notebook science, although not the norm in the academic community, has gained significant recent attention in the research and general media as part of a general trend towards more open approaches in research practice and publishing. Open notebook science can therefore be described as part of a wider open science movement that includes the advocacy and adoption of open access publication, open data, crowdsourcing data, and citizen science. It is inspired in part by the success of open-source software and draws on many of its ideas.\n\nThe term \"open-notebook science\" was first used in 2006 in a blog post by Jean-Claude Bradley, an Associate Professor of Chemistry at Drexel University at the time. Bradley described open-notebook science as follows:\n\n\"A team of groundbreaking scientists at SGC are now sharing their lab notebooks online\".\n\n\n\n\n\nThese are initiatives more open than traditional laboratory notebooks but lacking a key component for full Open Notebook Science. Usually either the notebook is only partially shared or shared with significant delay.\n\nA public laboratory notebook makes it convenient to cite the exact instances of experiments used to support arguments in articles. For example, in a paper on the optimization of a Ugi reaction, three different batches of product are used in the characterization and each spectrum references the specific experiment where each batch was used: EXP099, EXP203 and EXP206. This work was subsequently published in the Journal of Visualized Experiments, demonstrating that the integrity data provenance can be maintained from lab notebook to final publication in a peer-reviewed journal.\n\nWithout further qualifications, Open Notebook Science implies that the research is being reported on an ongoing basis without unreasonable delay or filter. This enables others to understand exactly how research actually happens within a field or a specific research group. Such information could be of value to collaborators, prospective students or future employers. Providing access to selective notebook pages or inserting an embargo period would be inconsistent with the meaning of the term \"Open\" in this context. Unless error corrections, failed experiments and ambiguous results are reported, it will not be possible for an outside observer to understand exactly how science is being done. Terms such as Pseudo or Partial have been used as qualifiers for the sharing of laboratory notebook information in a selective way or with a significant delay.\n\nThe arguments against adopting Open Notebook Science fall mainly into three categories which have differing importance in different fields of science. The primary concern, expressed particularly by biological and medical scientists is that of 'data theft' or 'being scooped'. While the degree to which research groups steal or adapt the results of others remains a subject of debate it is certainly the case that the fear of not being first to publish drives much behaviour, particularly in some fields. This is related to the focus in these fields on the published peer reviewed paper as being the main metric of career success.\n\nThe second argument advanced against Open Notebook Science is that it constitutes prior publication, thus making it impossible to patent and difficult to publish the results in the traditional peer reviewed literature. With respect to patents, publication on the web is clearly classified as disclosure. Therefore, while there may be arguments over the value of patents, and approaches that get around this problem, it is clear that Open Notebook Science is not appropriate for research for which patent protection is an expected and desired outcome. With respect to publication in the peer reviewed literature the case is less clear cut. Most publishers of scientific journals accept material that has previously been presented at a conference or in the form of a preprint. Those publishers that accept material that has been previously published in these forms have generally indicated informally that web publication of data, including Open Notebook Science, falls into this category. Open notebook projects have been successfully published in high impact factor peer reviewed journals but this has not been tested with a wide range of publishers. It is to be expected that those publishers that explicitly exclude these forms of pre-publication will not accept material previously disclosed in an open notebook.\n\nThe final argument relates to the problem of the 'data deluge'. If the current volume of the peer reviewed literature is too large for any one person to manage, then how can anyone be expected to cope with the huge quantity of non peer reviewed material that could potentially be available, especially when some, perhaps most, would be of poor quality? A related argument is that 'my notebook is too specific' for it to be of interest to anyone else. The question of how to discover high quality and relevant material is a related issue. The issue of curation and validating data and methodological quality is a serious issue and one that arguably has relevance beyond Open Notebook Science but is a particular challenge here.\n\nThe Open Notebook Science Challenge, now directed towards reporting solubility measurements in non-aqueous solvent, has received sponsorship from Submeta, Nature and Sigma-Aldrich. The first of ten winners of the contest for December 2008 was Jenny Hale.\n\nLogos can be used on notebooks to indicate the conditions of sharing. Fully open notebooks are marked as \"All Content\" and \"Immediate\" access. Partially open notebooks can be marked as either \"Selected Content\" and/or \"Delayed\".\n\n"}
{"id": "29071142", "url": "https://en.wikipedia.org/wiki?curid=29071142", "title": "P ring", "text": "P ring\n\nThe P ring is the part of the bacterial flagellum, which is known to be embedded in the peptidoglycan cell wall.\n"}
{"id": "9208413", "url": "https://en.wikipedia.org/wiki?curid=9208413", "title": "QuarkNet", "text": "QuarkNet\n\nQuarkNet is a long-term, research-based teacher professional development program in the United States jointly funded by the National Science Foundation and the US Department of Energy. Since 1999, QuarkNet has established centers at universities and national laboratories conducting research in particle physics (also called high-energy physics) across the United States. Mentor physicists and physics teachers collaborate to bring cutting-edge physics to high school classrooms. QuarkNet offers research experiences for teachers and students, teacher workshops and sustained follow-on support. Through these activities, teachers enhance their knowledge and understanding of scientific research and transfer this experience to their classrooms, engaging students in both the substance and processes of contemporary physics research. Teachers may receive academic credit for their participation. QuarkNet programs are designed and conducted according to “best practices” described in the National Research Council National Science Education Standards report (1995) and support the Next Generation Science Standards (2103).\n\nOriginally, QuarkNet established centers led by physicists participating in the CDF and DØ experiments at Fermilab's Tevatron in Batavia, Illinois and the ATLAS and CMS experiments at the Large Hadron Collider (LHC) at CERN in Geneva, Switzerland. It has expanded to include centers with participation in other particle physics experiments that are broadly representative of the field.\n\nQuarkNet supports two classroom visions:\n\n• Teachers use particle physics examples when teaching subjects such as momentum and energy.\n\n• Teachers create scientific inquiry-based learning environments that provide students with opportunities for in-depth engagement in science. Teaching strategies emulate the way scientists build knowledge through inquiry.\n\nQuarkNet is organized much like a particle physics experiment with a central design and infrastructure and work distributed among research groups nationwide. The interactions among physicist, teacher and student primarily take place at the local centers. The program prospers because these centers prosper. The current principal investigators for the program are: Marge Bardeen (Fermilab), Dan Karmgard and Mitch Wayne (University of Notre Dame) and Anna Goussiou (University of Washington). Bardeen serves as the spokesperson for the program. A staff of four physics teachers provides resources and services to the centers and guidelines for center performance. Also, staff is responsible for activities at the national level. They lead workshops including Data Camp and masterclasses, develop instructional materials including cosmic ray muon detectors, gather data for evaluation and more. In order to enhance its capacity, staff also works with a group of teachers, called QuarkNet Fellows, who help deliver the professional development programs for other teachers.\n\nThe summer Boot Camp is a continuing national activity that provides an opportunity for teachers to visit Fermilab and see the accelerators and detectors for themselves. Teachers from existing QuarkNet centers attend Boot Camp and form research groups to explore experimental data. They apply the physics that they know (i.e., energy and momentum conservation) to unravel the physics encoded in the data. This helps the teachers construct new knowledge about what has transpired inside the detector in exactly the same way that experimenters do. Teachers work in spearate groups investigating triggers released by CMS in early 2011. The groups search the data for evidence of the J/Psi, Z and W bosons. They used Excel to reconstruct the invariant mass of a particle when given the four-vector of that particle's decay products. In addition, participants attend several talks and go on tours of technical areas.\n\nThe main QuarkNet student investigations supported at the national level are cosmic ray studies. Working with Fermilab technicians and research physicists, QuarkNet staff has developed a classroom cosmic ray muon detector that uses the same technologies as the largest detectors at Fermilab and CERN. To support interschool collaboration, QuarkNet collaborates with the Interactions in Understanding the Universe Project (I2U2) to develop and support the Cosmic Ray e-Lab. An e-Lab is a student-led, teacher-guided investigation using experimental data. Students have an opportunity to organize and conduct authentic research and experience the environment of a scientific collaboration. Participating schools set up a detector somewhere at the school. Students collect and upload the data to a central server located at Argonne National Laboratory. Students can access data from all of the detectors in the cluster and use these data for studies, such as determining the (mean) lifetime of muons, the overall flux of muons in cosmic rays, or a study of extended air showers.\n\nIn summer 2007, QuarkNet inaugurated the QuarkNet Fellows Program to develop the leadership potential of teachers who would work with staff to provide professional development activities and support for centers. Three groups of fellows in the areas of cosmic ray studies, LHC and teaching and learning share responsibilities for offering workshops and sessions, developing workshop materials, supporting e-Labs and masterclasses, giving presentations at AAPT and more. In 2009 a new group of fellows joined the program. Leadership fellows work with staff to support centers and gather data about center performance.\n\nIn 2007, QuarkNet first piloted U.S. Masterclasses, modeled on a program offered by EPPOG and studying Large Electron-Positron Collider-era CERN data. Today masterclasses study ALICE, ATLAS or CMS data. Masterclasses are one-day national events in which teams of students visit a nearby university or research center to gain insight into topics and methods of particle physics. During the day students:\n\n• Attend lectures on the Standard Model and learn how to analyze events.\n\n• Analyze 1,000 events from various CERN particle physics experiments.\n\n• Discuss results with three to six other student teams, in a videoconference moderated by physicists at CERN or Fermilab.\n\n2007 was an analysis of electron-positron interactions at the DELPHI and OPAL detectors from the LEP experiments.\n\n2009 was an analysis of simulated proton-proton collisions in the ATLAS detector. \"Hidden\" in the data was one single event which represented the expected signature from the Higgs Boson.\n\n2010 was a return to LEP data.\n\n2011 was the first year that LHC data were incorporated into the masterclass There were exercises for ATLAS (Z- and W-bosons measurements), ALICE (for stranger particles), and CMS (J/ψ meson).\n\n2012 saw the incorporation of expanded CMS data (W/Z measurement; J/psi measurement still available but not supported for International Masterclasses).\n\nBased on a model at the University of Notre Dame, QuarkNet has offered a summer student research program since 2004. Typically, teams of four high school students supervised by one teacher spend six weeks involved in various physics research projects. Some centers choose to modify this model, involving more students and/or less time. The research is associated with ATLAS and CMS, the International Linear Collider R&D, cosmic ray muon detectors, optical fiber R&D and more. Teams are supported at up to 25 centers each summer. Examples of recent research titles include: \"Search and Identification of Comparing the Amount of Muon Events to Daily Weather Changes, Cosmic Ray Signals in Radar Echo, Fibers for Forward Calorimeter, The Effects of Impurities on Radio Signal Detection in Ice, Quartz Plate Calorimetery, Galactic Asymmetry of the Milky Way\" and \"RF Magnet Design, and Weak Lensing Mass Estimates of the Elliot Arc Cluster.\"\n\n"}
{"id": "32723423", "url": "https://en.wikipedia.org/wiki?curid=32723423", "title": "Samuel C. Bradford", "text": "Samuel C. Bradford\n\nSamuel Clement Bradford (10 January 1878 in London – 13 November 1948) was a British mathematician, librarian and documentalist at the Science Museum in London. He developed \"Bradford's law\" (or the \"law of scattering\") regarding differences in demand for scientific journals. This work influences bibliometrics and citation analysis of scientific publications. Bradford founded the British Society for International Bibliography (BSIB) (est. 1927) and he was elected president of International Federation for Information and Documentation (FID) in 1945. Bradford was a strong proponent of the UDC and of establishing abstracts of the scientific literature.\n\n\n"}
{"id": "49752515", "url": "https://en.wikipedia.org/wiki?curid=49752515", "title": "Scanning electron cryomicroscopy", "text": "Scanning electron cryomicroscopy\n\nScanning electron cryomicroscopy (CryoSEM) is a form of electron microscopy where a hydrated but cryogenically fixed sample is imaged on a scanning electron microscope's cold stage in a cryogenic chamber. The cooling is usually achieved with liquid nitrogen. CryoSEM of biological samples with a high moisture content can be done faster with fewer sample preparation steps than conventional SEM. In addition, the dehydration processes needed to prepare a biological sample for a conventional SEM chamber create numerous distortions in the tissue leading to structural artifacts during imaging.\n"}
{"id": "34033422", "url": "https://en.wikipedia.org/wiki?curid=34033422", "title": "Scholar Indices and Impact", "text": "Scholar Indices and Impact\n\nScholar indices are used to measure the contributions of scholars to their fields of research. Since the 2005 paper of Jorge E. Hirsch, the use of scholar indices has increased.\n\nSometimes called bibliometrics, scholar indices are mathematical and statistical tools that measure the significance of the contributions made by an academic to their field of research. Scholar indices may incorporate other assessments such as citation tracking and journal ranking.\n\nAny aggregator of citations and references, could, given time, money and inclination, generate their own set of scholar indices. Publishers who are prominent in this field include Elsevier and Thomson Reuters.\n\nCommercial software which use parsers and web search engines to generate sets of scholar indices or individual results are now available. Examples are: \"Publish or Perish\"; 'ScholarIndex'; 'Scopus' and 'Google Scholar'.\n\nEach software vendor primarily uses its own data as well as journals, publications, authority files, indexes, and subject categories to produce sets of scholar indices.\n\nWhile some companies provide the data and the evaluated metrics as free downloads, others require subscriptions to cover costs of manufacture and upkeep of an efficient parser, search engine and document database.\n\nScholar indices allow choice of journal collections, application of research funds, ranking of journals and the determination of significant contributors in a subject area.\nAdvocates of scholar indices recommend their use in areas such as liaison services, references, instruction and collection management.\n\nCritics of the use of scholar indices cite their limitations due to issues of accuracy, validity and applicability and debate their application to hiring, tenure, funding, award giving and membership decisions.\n\nAlthough scholar indices may not completely describe the impact of an individual researcher's work, some academics will determine their own scholar indices to include in promotional material and curriculum vita for example. Others may study their scholar indices simply for their own sake.\n\nThose interested in the field of scholar indices may find the results, for example data visualisation projects, exciting.\n\nTo date, a number of scholar indices have been developed. One is the 'h-index' introduced by Jorge E. Hirsch in August 2005. Hirsch described the h-index as unbiased as it involved the relationship of an academic's volume of published papers and the number of citations for those papers creating less bias than either measure alone.\n\nAnother scholar index is the 'g-index' which measures citation counts of all well cited works over a period of time. The 'm-quotient' was developed to introduce a time limit to the h-index which was otherwise, an ever-increasing quantity.\n\nOther variants of the h-index such as hI-index, e-index and others are being developed and reviewed.\n\nThe Erdős number was developed to measure the publication chain started by Paul Erdős.\n\nAll such scholar indices quantify the contribution of a researcher based on citations of their work only. Ideally, an assessment of a researcher's contribution to their field would include both scholar indices and an analysis of the quality of the work itself.\n\nThe h-index index was suggested by Jorge E. Hirsch, a physicist at UCSD, in 2005.\n\nHenry Schaefer, of the University of Georgia, US, together with colleague Amy Peterson, created rankings according to the h-index, from the ISI Web of Science. Though web-based applications can calculate h-indices, Peterson had to check for misspelt or duplicated names.\n\nThe h-index is defined as follows:\n\nTo calculate the h-index, the papers written by an academic are arranged in decreasing order of number of citations. The h index is where the number of papers equals the number of citations (beginning with the paper with the highest number of citations).\n\nAlthough widely used, the h index does not take into account the quality of the papers; other papers which cite the academics publications; the number of co-authors and the position of the author in the author list. Also, all fields are given equal value.\n\nAnother limitation is that the h index does not vary over time. For example, Évariste Galois has an h index of 2 while Claude Shannon has an h index of 7 \n\nWhile the h-index is independent of the date of an academic's career, the m-quotient aims at weighing the period of academic endeavour so that even junior scientists attain the importance that they deserve.\n\nThus, if \"n\"=number of years since the first published paper of the scientist, the m-quotient=h-index/\"n\".\n\nHowever, the m-quotient may not stabilise until later in the scientist's career. for researchers in the early part of their career, who have low h indices, small changes in the h-index can lead to large changes in the m-quotient. Hirsch suggests the researcher's first published paper may not always be the appropriate starting point, especially if it was a minor contribution that was published well before the academic's period of sustained productivity.\n\nAlthough the m-quotient adds time as a weighting factor, it does not cater to the major disadvantages of the h-index including quality of publication and quality of citation.\n\ng-index is a variant of the h-index, which takes into account the citation evolution of the most cited papers over time.\n\nIn other words, the g-index g is the largest rank (where papers are arranged in decreasing order of the number of citations they received) such that the first g papers have (together) at least g^2 citations. \"\n\nIt can be proved that for any set of papers g-index always exists and is unique.\n\nformula_1\n\nwhere the Lotkaian exponent and where T denotes the total number of sources.\n\nSince formula_2, formula_3\n\nFor example, if 2 scientists have h-index 4, it may happen that one of them has published 4 papers which have 4 or more citations, while another scientist may have published 10 papers out of which 3 have more than 100 citations and the 4th paper has 4 citations, and the remaining have less than 4 citations.\n\nIn an attempt to offer a higher weighting to the second scientist who in aggregation has received greater than 304 citations for 10 papers, the g-index was proposed. Thus in our example, the first scientist has g-index=4, while the second scientist has g-index significantly higher.\n\nAn Erdős number measures the collaborative distance between a person and mathematician Paul Erdős, measured by authorship of mathematical papers.\n\nConsidering Paul Erdős to have an index=0, people who co-authored with him have an index=1, co authors of those co-authors have index=2, and so on. Thus, to calculate one’s Erdős number, add 1 to the Erdős number of any co-author with the lowest Erdős, number. The Erdős-Number project at Oakland University maintains a website tracking the Erdős numbers of scientists world-wide.\n\nOne caveat is that most Erdős numbers recorded so far range up to 13, but the average is less than 5, and almost everyone with a finite Erdős number has a number less than 8.\n\nEvaluation of the complete contribution of a scholar to their field of research can be assessed in two ways. One is by accounting for the number of citations received by the scholar. The other is by accounting for the quality of the references referred to by the scholar.\n\nWhile being strongly cited makes a scholar a strong authority in his field, having strong references makes a scholar a strong hub, who knows all the significant work in that field. Calculation of the hub and authority indices requires the knowledge of the relationships between scholars being cited or referred to.\n\nThe Hubs and Authorities algorithm can be used for computing these indices. The algorithm performs a link analysis on a given network and assigns two scores to each node: a hub and an authority.;\n\nA valuable and informative node in a network is usually pointed to by a large number of links, that is, it has a large indegree (see Fig. 1). Such a node is called an authority.\n\nA node that points to many authority nodes is itself a useful resource and is called a hub. A hub usually has a large outdegree. In the context of literature citation, a hub is a review paper which cites many original papers, while an authority is an original seminal paper which is cited by many papers.\n\nA network can be constructed of nodes representing authors and links indicating references to published papers. Outgoing links indicate who the author cited and incoming links indicate who cited the author.\n\nA researcher's hub score is the sum of authors' scores whose work is cited. A researcher's authority score is the sum of authors' hub scores who referenced the researcher's work.\n\nThe hub score increases if the author cites papers published by authors with high authority scores. The authority score increases when published papers are cited by authors with high hubs score.\n\nformula_4\n\nformula_5\n\nThe equations can be rewritten in a matrix-vector form. Let A be an adjacency matrix of the network and vectors h and a to contain all hubs and authorities scores, respectively. Then the scores can be calculated by the following formulas.\n\nformula_6\n\nformula_7\n\nHubs and authorities indices require the knowledge of the connectivity between scholars who refer and cite each other’s work. Since it is not always possible to accurately obtain these connectivity patterns, the adjacency matrix (A) regarding the scholar’s connections can be estimated.\n\nFor example, a scientist has an estimated local connectivity matrix. It is a combination of the work by which he is cited and works which he cites. Once the adjacency network is estimated, the hub and authority indices are determined by eigen-decomposition of \"(A.A’)\" and \"(A’.A)\" respectively. The steps followed for this specific implementation are as follows:\nWhere blocks of X and blocks of Y replace the ‘1’s. The connectivity follows the Fig 2.\n\n\n1 formula_8\n\n2 formula_9\n\n3 formula_10\n\n4 formula_11\n\nThe table utilizes Scopus as a search engine only and the adjacency matrix is an estimation, thus the results tabulated above are extremely aggregated versions, and they must not be confused with absolute indices. A better estimate of adjacency matrix may produce variations in the indices. Also, Scopus keeps track of articles after 1995 only, so that is an additional constraint. (All the indices have been evaluated as of December 12, 2011.)\n\nIn this table, it is evident that different search engines yield different h-indices. It is possible that a scientist with high h-index may be a strong authority but not necessarily a strong hub. The validity of web-search engines is assessed as documents prior to 1995 are inaccessible. The number of publications of a particular author in a particular data base is responsible for affecting the hub-authority indices. Interdisciplinary work may be well assessed by the hub-authority index as opposed to other indices.\n\nThis is an example to understand the interplay of the various scholar indices.\n\nA new scientist who began her academic career in 2009, has published 3 papers. Two papers have 2 citations each while the third paper has no citations. She has referenced 60 papers with 17 strong references among the 60. One of her co-authors has the lowest Erdős number 3. Her scholar indices as per December 2011 are:\n\nIn another year, she publishes another paper with 20 new references such that now she has a cumulative of 31 strong references, 4 papers with 2 citations for 2 papers and 0 citations for other 2 papers. Her hub-authority indices change:\n\nHUB index=12.668\n\nAUTHORITY index= 0.1061\n\nIn another year, her citations for the 3 papers increases to 10 and she continues to have 60 references with 17 strong references:\n\nHUB index=11.568\n\nAUTHORITY index=0.3241\n\nIn another year, her citations increase to 10 and she writes another paper such that the number of references goes up to 80 (with 31 strong ones):\n\nHUB index=12.694\n\nAUTHORITY index=0.3284\n\nThus to summarize, the following hub-authority indices are observed for this toy example.\n\nThe matlab code for the Example 1 and Toy example instances in Example 2 is attached as File 1.\nThese examples demonstrate the importance of the hub-authority indices in quantitative evaluation of the contributions of a scholar in a complete academic career.\n\nScholar indices have limitations including lack of accuracy, validity and applicability. While the accuracy of scholar indices is questionable owing to the difference in spellings, difference in parser, search engines and document data bases maintained by various online sources, it might be possible to solve the accuracy issues if each author could be assigned to a unique ID instead of relying on the names for searches. Also each time these indices are reported, the method and search engines used must be mentioned to avoid ambiguity as much as possible.\n\nThe validity of scholar indices is a limitation because they do not weigh fields of varying importance. For example, John Pople, a theoretical chemist who has received a Nobel Prize, fares poorly in sets of h-indices.\n\nApplicability of scholar indices has limitations when scholars emphasise practical advancement in an area of endeavour rather than the publishing of papers. It is also difficult to document works of an earlier decade as online documents, thereby decreasing their online impact factor. For example, the Scopus is an online database that calculates scholar indices for documents found after 1995 only. Any earlier works are not documented or evaluated.\n\nJorge E. Hirsch suggested the h-index should inform hiring, promotion, funding, tenure, award and societal committees to make judicious decisions. However, because of their limitations they are best viewed in a balanced way.\n"}
{"id": "26496950", "url": "https://en.wikipedia.org/wiki?curid=26496950", "title": "Space Shuttle Orbital Maneuvering System", "text": "Space Shuttle Orbital Maneuvering System\n\nThe Space Shuttle Orbital Maneuvering System (OMS), is a system of hypergolic liquid-propellant rocket engines used on the Space Shuttle. Designed and manufactured in the United States by Aerojet, the system allowed the orbiter to perform various orbital maneuvers according to requirements of each mission profile: orbital injection after main engine cutoff, orbital corrections during flight, and the final deorbit burn. The OMS consists of two pods mounted on the orbiter's aft fuselage, on either side of the vertical stabilizer. Each pod contains a single AJ10-190 engine, based on the Apollo Service Module's Service Propulsion System engine, which produces of thrust with a specific impulse (\"I\") of 316 seconds. Each engine could be reused for 100 missions and was capable of a total of 1,000 starts and 15 hours of burn time.\n\nThese pods also contained the Orbiter's aft set of reaction control system (RCS) engines, and so were referred to as OMS/RCS pods. The OM engine and RCS systems both burned monomethylhydrazine (MMH) as fuel, which was oxidized with dinitrogen tetroxide (NO), with the propellants being stored in tanks within the OMS/RCS pod, alongside other fuel and engine management systems. When full, the pods together carried around of MMH and of NO, allowing the OMS to produce a total of around of delta-v with a 65,000-pound (29,500 kg) payload.\n"}
{"id": "26259261", "url": "https://en.wikipedia.org/wiki?curid=26259261", "title": "Sven Ludvig Lovén", "text": "Sven Ludvig Lovén\n\nProf Sven Ludvig Lovén (6 January 1809 – 3 September 1895), was a Swedish marine zoologist and malacologist. The Sven Loven Centre for Marine Sciences at both and are named in his honour.\n\nHe was born in Stockholm on 6 January 1809. He was first Keeper then Director of the Swedish National Museum in Stockholm. He was also Professor of Natural History at Stockholm University. He was made an honorary Fellow of the Royal Society of Edinburgh in 1881 and a Foreign Fellow of the Royal Society of London in 1885.\n\nSeveral geographical locations at Svalbard are named after him. These include the headland Kapp Lovén at Nordaustlandet, the mountain Lovénberget in Ny-Friesland at Spitsbergen, the lake Lovénvatnet in Oscar II Land, and the glaciers Lovénbreane at Brøggerhalvøya.\n\nThe World Register of Marine Species (WoRMS) lists 174 marine species named by Lovén. Many of these have become synonyms. Two species he named, the hydrozoan \"Lovenella clausa\" (Lovenellidae) and the crustacean \"Lovenula falcifera\" (now \"Paradiaptomus\"), were in genera named after him by other zoologists.\n\n"}
{"id": "42881967", "url": "https://en.wikipedia.org/wiki?curid=42881967", "title": "The Desideratum; or, Electricity Made Plain and Useful", "text": "The Desideratum; or, Electricity Made Plain and Useful\n\nThe Desideratum; or, Electricity Made Plain and Useful - By a Lover of Mankind, and of Common Sense is a 1760 book by John Wesley advocating the use of electric shock therapy. Wesley collected the accounts of other researchers with \"electrifying machines,\" and to them added observations from his own experiments in public clinics.\n\nThe 72-page book has led Wesley to be mentioned alongside his contemporaries Richard Lovett and Jean Paul Marat as a pioneer advocate of the medical uses of electroconvulsive therapy, despite the fact that Wesley's tests and results are not considered scientific by modern standards.\n\n"}
{"id": "33748539", "url": "https://en.wikipedia.org/wiki?curid=33748539", "title": "The Different Forms of Flowers on Plants of the Same Species", "text": "The Different Forms of Flowers on Plants of the Same Species\n\nThe Different Forms of Flowers on Plants of the Same Species is a book by Charles Darwin first published in 1877. It is the fifth of his six books devoted solely to the study of plants (excluding \"The Variation of Animals and Plants under Domestication\").\n\nThese writings contributed to Darwin's pursuit of evidence that would support his theory of natural selection. There were only two more books to follow: \"The Power of Movement in Plants\" (1880) and \"The Formation of Vegetable Mould through the Action of Worms\" (1881). He conducted a wide range of experiments and observations and the results of these form the framework of the book. He was assisted in this work by his son, Francis Darwin who also wrote a preface for the second edition which was published two years after his father's death in 1882. The book was dedicated to his longtime friend and colleague, Harvard botany professor Asa Gray \"as a small tribute of respect and affection\".\n\nUsing the four classifications established by Carl Linnaeus (hermaphroditic, monoecious, dioecious, polygamous), Darwin concentrated on two divisions of the hermaphroditic class, namely the cleistogamic and heterostyled.\n\nDarwin’s reflections indicate the economy of nature through a process of gradual modification of plants, their structures being modified and degraded for the purpose of the large scale production of seed which is necessary and advantageous for survival. Darwin states (p. 227): \"Cleistogamic flowers ... are admirably fitted to yield a copious supply of seed at a wonderfully small cost to the plant.\"\n\nFrancis Darwin indicated that the work on heterostylism had given his father extreme pleasure, especially as it had been one of the most puzzling bits of work he ever carried out. Darwin thought that hardly anyone had seen the full importance of heterostylism.\n\nIn 1883, Alfred Russel Wallace wrote a tribute to Darwin (entitled 'The Debt of Science to Darwin’) who had died the year before. One such tribute appeared in 'The Century', an illustrated monthly magazine. As part of this article he included a summary of Darwin's work relating to this book (p. 428):\n\n\"The cowslip (Primula veris) has two kinds of flowers in nearly equal proportions: in the one the stamens are long and the style short, and in the other the reverse, so that in one the stamens are visible at the mouth of the tube of the flower, in the other the stigma occupies the same place, while the stamens are halfway down the tube. The fact had been known to botanists for 70 years, but had been classed as a case of mere variability, and therefore considered to be of no importance. In 1860 Darwin set to find out what it meant, since, according to his views, a definite variation like this must have a purpose. After a considerable amount of observation and experiment, he found that bees and moths visited the flowers, and that their proboscis become covered with pollen while sucking up the nectar, and further, the pollen of a long stamened plant would most surely be deposited on the stigma of the long styled plants, and vice versa. Now followed a long series of experiments, in which cowslips were fertilised with either pollen from the same kind or from a different kind of flower, and the invariable result was that the crosses between the two different types of flowers produced more good capsules, and more seed in each capsule; and as these crosses would be most frequently effected by insects, it was clear that this curious arrangement directly served to increase the fertility of this common plant. The same thing was found to occur in the primrose, as well as in flax (Linum perenne), lungworts (Pulmonaria), and a host of other plants, including the American partridge-berry (Mitchella repens). These are called dimorphic heterostyled plants.\n\nStill more extraordinary is the case of the common loosestrife (Lythrum salicaria), which has both stamens and styles of three distinct lengths, each flower having two sets of stamens and one style, all of different lengths, and arranged in three different ways:\n\n\nThese flowers can be fertilised in eighteen distinct ways, necessitating a vast number of experiments, the result being, as in the case of the cowslip, that flowers fertilised by the pollen from stamens of the same length as the styles, gave on the average a larger number of capsules and a very much larger number of seeds than in any other case. The exact correspondence in the length of the style of each form with that of one set of stamens in the other form insures that the pollen attached to any part of the body of an insect shall be applied to a style of the same length on another plant, and there is thus a triple chance of the maximum of fertility...There is thus the clearest proof that these complex arrangements have the important end of securing both a more abundant and more vigorous offspring.”\n\nObservations and experiments still continue today to further the understanding of this phenomenon instigated by Darwin in this original and seminal work.\n\n"}
{"id": "26889856", "url": "https://en.wikipedia.org/wiki?curid=26889856", "title": "The Mind's Eye (book)", "text": "The Mind's Eye (book)\n\nThe Mind's Eye is a 2010 book by neurologist Oliver Sacks. The book contains case studies of people whose ability to navigate the world visually and communicate with others have been compromised, including the author's own experience with cancer of the eye and his lifelong inability to recognise faces.\n\nOne of the case studies concerns Susan R. Barry, nicknamed \"Stereo Sue,\" whom Sacks wrote about in 2006. Due to strabismus, she lived without stereoscopic vision for 48 years, but became able to see stereoscopically through vision therapy.\n\nAnother case study is of the acclaimed concert pianist Lilian Kallir, who suffered from posterior cortical atrophy yet was surprisingly resilient despite the numerous deficits it caused; the effect on her musical abilities was particularly notable. While her memory and personality were intact, she had problems processing visual stimuli, and was no longer able to read words or music, yet for years lived an extremely active life, frequently performing entirely from memory, with no one but her husband knowing she had any problems.\n\nAnother case study was about a very vivacious, social woman named Pat who suffered a stroke that resulted in aphasia —a complete inability to speak or understand words. One chapter is devoted to the case of Howard Engel, author of a popular series of mystery novels. Due to a small stroke, he developed alexia sine agraphia—an inability to read, while retaining the ability to write.\n\nBryan Appleyard, reviewing the book for \"Literary Review\", wrote: \"Sacks the doctor once again dramatises the most strange and thrilling scientific and cultural issue of our time—the nature of the human mind—through the simple act of telling stories.\" \n\n"}
{"id": "53965823", "url": "https://en.wikipedia.org/wiki?curid=53965823", "title": "Vermund Larsen", "text": "Vermund Larsen\n\nVermund Larsen (February 27, 1909 – February 28, 1970) was a Danish furniture designer and manufacturer. Larsen became known for his work while living in Aalborg, an industrial city in northern Denmark. Larsen is best known for creating Europe's first glass–fiber chair in 1955.\n\nVermund Larsen was born in 1909 in Hellerup, a suburb of Copenhagen, Denmark. Larsen moved to Aalborg at age 14 when his father, Captain S. K. Larsen, served in the Danish military in Aalborg. After studying at Aalborg Cathedral School, Larsen became a trainee at M. Kragelund Factories. He then served in the military. Upon completing his service at age 26, he bought P.C. Petersen, an abandoned iron production company, in 1935. At P.C. Petersen, Larsen began working with steel and became interested in steel furniture manufacturing. In 1944, Larsen invented and patented a window lock. The lock was child-proof and had a feather mechanism that theoretically could not be drilled through, thereby making it resistant to break-ins.\n\nIn 1948, Larsen began to focus on the construction of office furniture. Larsen took an interest in office chairs after reading an article in the \"Journal of Danish Handcraft\" entitled \"The Seated Position - and Chairs\" by Dr. Snorason. The article discussed ergonomics and the importance of furniture that supports proper sitting posture.\n\nLarsen then began designing and manufacturing ergonomic desk chairs. In 1951, production consisted of the following categories: barber inventory, ship equipment and office furniture.\n\n\nVermund Larsen died in Aalborg in 1970. His twin sons, Stig and Gorm, ran the company under the name VELA.\n\n"}
