{"id": "805617", "url": "https://en.wikipedia.org/wiki?curid=805617", "title": "A-004", "text": "A-004\n\nA-004 was the sixth and final test of the Apollo launch escape vehicle and the first flight of a Block I production-type Apollo Command/Service Module.\n\nMission A-004 was unmanned and was conducted to demonstrate that \n\nThe launch vehicle was the fifth and final Little Joe II flown. The propulsion system consisted of four Algol and five Recruit rocket motors. The attitude control system was similar to the one used on mission A-003 except that the reaction control system was deleted and the vehicle was provided with the capability of responding to a radio-transmitted pitch up command. The \npitch up maneuver was required to help initiate tumbling of the launch vehicle. The spacecraft for this mission consisted of a modified Block I command and service module boilerplate, BP-2, and a modified Block I launch escape system (airframe 002). The center of gravity and thrust vector were changed to assure that power-on tumbling would be attained after abort initiation. The earth landing system was essentially the same as that used during Pad Abort Test 2.\n\nThe vehicle was launched on January 20, 1966, at 08:17:01 a.m. MST (15:17:01 UTC) after several postponements due to technical difficulties and adverse weather conditions. The pitch up maneuver was commanded from the ground when telemetry showed that the desired altitude and velocity conditions had been reached. The planned abort was automatically initiated 2.9 seconds later. The launch escape vehicle tumbled immediately after abort initiation. Pitch and yaw rates reached peak values of 160 degrees per second, and roll rates reached a peak of minus 70 degrees per second. The launch escape system canard surfaces deployed at the proper time and stabilized the command module with the aft heat shield forward after the escape vehicle had tumbled about four times. Tower jettison and operation of the earth landing systems were normal, and the command module landed about 113,620 feet (34.6 km) from the launch pad after having reached a maximum altitude of 78,180 feet (23.8 km) above mean sea level.\n\nAll systems performed satisfactorily, and the dynamic loads and structural response values were within the design limits and predicted values. Although a structural loading value of primary interest was not achieved (local differential pressure between the interior and exterior of the command module wall), all test objectives were satisfied.\n\nThe boilerplate spacecraft is currently on display at the Cradle of Aviation.\n\n"}
{"id": "44817234", "url": "https://en.wikipedia.org/wiki?curid=44817234", "title": "Andrea Marshall", "text": "Andrea Marshall\n\nDr Andrea Marshall is a marine biologist known for her research into manta rays, and a founder of the Marine Megafauna Foundation. She was the subject of the Natural World documentary \"Andrea: Queen of Mantas\".\n\nShe completed her PhD on manta rays in 2008 and now works at the Manta Ray & Whale Shark Research Center at Tofo Beach, Mozambique, where she also lives.\n\nShe is credited with the inclusion of manta rays in the Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITIES). She gave a TED talk in 2012 and was named a National Geographic \"Emerging Explorer\" in 2013.\n\nShe developed Manta Matcher, an online citizen science database for identifying individual manta rays from their unique markings.\n\n\n"}
{"id": "28913414", "url": "https://en.wikipedia.org/wiki?curid=28913414", "title": "Antevs Glacier", "text": "Antevs Glacier\n\nAntevs Glacier (), also known as North Heim Glacier, is a glacier on Arrowsmith Peninsula, Graham Land, flowing north between Seue Peaks and Boyle Mountains into Muller Ice Shelf, Lallemand Fjord. It was named by the United Kingdom Antarctic Place-Names Committee in 1960 after Ernst V. Antevs, American glacial geologist.\n\n"}
{"id": "2074838", "url": "https://en.wikipedia.org/wiki?curid=2074838", "title": "Association for Behavior Analysis International", "text": "Association for Behavior Analysis International\n\nThe Association for Behavior Analysis International (ABAI) is a nonprofit organization dedicated to contributing to the well being of society by developing, enhancing, and supporting the growth of the science of behavior analysis through research, education, and practice. ABAI has over 7,000 members. It is committed to promoting the experimental, theoretical, and applied analysis of behavior.\n\nThe Association for Behavior Analysis International (ABAI) was founded in 1974 as the MidWestern Association for Behavior Analysis (MABA) to serve as an interdisciplinary group of professionals, paraprofessionals, and students. MABA was organized to establish a separate identity for the behavior analysis group of the larger psychological community and to provide a forum for the discussion of issues and the dissemination of information pertinent to the interests of its membership. The first annual conference was a response by a group of behavior analysts who were having problems presenting their work at psychology conferences and other related events. Some of the members included Sidney Bijou, James Dinsmoor, Bill Hopkins, and Roger Ulrich. The first headquarters were located on the campus of Western Michigan University (WMU) in Kalamazoo, Michigan. The association changed its name to the Association for Behavior Analysis in 1979. In 2002, the headquarters were moved off WMU's campus. In 2008, the association relocated to nearby Portage, Michigan (where it is today), and added \"International\" to its name. Today ABAI has more than 7,000 active members worldwide. One can obtain an ABAI membership if they want to further education, practice, and research goals in behavior analysis.\n\nABAI offers different membership levels. A full membership is available for those who have a master's degree in experiential or applied behavior analysis or have contributed to the field of behavior analysis. An affiliate membership is feasible for those with undergraduate degrees, have an interest in behavior analysis, and do not meet the criteria for a full membership. A student membership is offered to anyone who is a full-time undergraduate or graduate student, resident, or intern.\n\nABAI is the leading advocate in the United States for advances in behavior analysis and behavioral psychology in applied settings. The association provides support to the Society for the Advancement of Behavior Analysis (SABA), a 501(c)(3) nonprofit organization that grants funding for continuing research. ABAI is a member of the Federation of Behavioral, Psychological, and Cognitive Sciences (FABBS)—a group of scientific organizations that are interested in advancing science in areas of the brain, mind, and behavior.\n\nABAI organizes an annual convention dedicated to the advancements in the science of behavior analysis. More than 5,100 behavior analysts come to participate in workshops, poster sessions, “round-table” discussions, symposia, and listen to speakers. In 2017, ABAI will be hosting their 43rd Annual Convention in Denver, Colorado. Past host cities for the convention have included Chicago, San Antonio, Seattle, and other. Every two years, ABAI hosts an international conference; the last one took place in 2017 in Paris, France. The ABAI international conference has also been held in Japan, Spain, China, Australia, as well as many other places. The association also holds an annual autism conference; the 11th Annual Autism Conference will take place in San Juan, Puerto Rico, in January 2017. Additionally, ABAI has hosted many single-track conferences on topics of special interest to behavior analysts, such as theory and philosophy, climate change, behavioral economics, and education.\n\nThrough the sister organization of SABA, several categories of awards are given to individuals, organizations, and ABA research during the ABAI annual convention:\n\n\nThe Association of Applied Behavior Analysis International publishes four journals about behavior analysis various subjects of behavior analysis:\n\n\n\n"}
{"id": "43602719", "url": "https://en.wikipedia.org/wiki?curid=43602719", "title": "Association of European Operational Research Societies", "text": "Association of European Operational Research Societies\n\nThe Association of European Operational Research Societies (EURO) is a regional grouping within the International Federation of Operational Research Societies (IFORS) whose aim is to promote Operational Research throughout Europe. It was established in 1975.\n\nEURO is a nonprofit organization domiciled in Switzerland. It aims at the advancement of knowledge, interest and education in Operational Research by appropriate means, particularly by the exchange of information, the holding of meetings and conferences, the publication of books, papers, and journals, the\nawarding of prizes, and the promotion of early stage talents. The members of EURO are national Operational Research Societies which are full members of International Federation of Operational Research Societies IFORS and originate from Europe, the Middle East and Africa. Its affairs are regulated by a Council consisting of one representatives of all its members and an executive committee which constitutes its board of directors.\n\nThe current EURO member societies are:\n\nEURO publishes scholarly journals and books about operational research, and organizes international conferences. It also bestows Awards, supports working groups, and organizes educational meetings.\n\nEURO publishes 4 scholarly journals:\nand the book series \"EURO Advanced Tutorials in Operational Research\".\n\nEURO bestows a number of prizes:\n\nEURO organizes a number of different conferences and events throughout each year:\n\nEURO Working Groups are the organizational framework provided by EURO to groups of researchers and practitioners interested in a specific operational research topic. Each EURO Working Group holds at least one meeting per year, organizes sessions at conferences, publishes special issues of OR journals, and organizes conferences or seminars. The European Working Group on Multiple Criteria Decision Aiding (EWG-MCDA), the European Chapter on Combinatorial Optimization (ECCO), the European Working Group on Vehicle Routing and Logistics (VeRoLog), the EURO Working Group on Locational Analysis (EWGLA), the Continuous Optimization Working Group (EUROPT), and the European Working Group on Metaheuristics (EU/ME) are among the most active groups.\n\nEURO organizes educational meetings throughout each year:\n\n"}
{"id": "45450164", "url": "https://en.wikipedia.org/wiki?curid=45450164", "title": "Ayurvedic acupressure", "text": "Ayurvedic acupressure\n\nAyurvedic acupressure (also known as Marma therapy) is a particular kind of massage or an alternative medical treatment, which integrates the knowledge of ancient Ayurveda and the principles of acupressure, allegedly to completely heal and cure physical, mental, emotional and spiritual illnesses.\n\nAccording to Charaka Samhitā, an early text on Ayurveda (Indian traditional medicine), the cosmos and its correspondence – the human body, are composed of both physical (Visible) and metaphysical (Invisible) forces.\n\nAyurvedic acupressure talks about 10 elements – 5 physical and 5 metaphysical.\n\n\nIn Ayurveda the pressure points or the meridians are called Marma points. These are considered as energy points, just as Chinese refer them as pressure points, and they help in stimulating not only circulation, but also improve mental and emotional outlook. The Marma points are located throughout the body.\n\nTreatment in Ayurvedic Acupressure can be easily done without needles by:\nThe Acupressure shodh prashikshan evam upchar blakpipol sukh sansthan Allahabad is a non government organization working for the last 20 years in popularizing the Ayurvedic Acupressure\n\nThere are many branches of acupressure/acupuncture as we know it today. They are \nThese are just some of the forms. The scope and theories of each are a bit different.\n"}
{"id": "28914317", "url": "https://en.wikipedia.org/wiki?curid=28914317", "title": "Bartrum Glacier", "text": "Bartrum Glacier\n\nBartrum Glacier () is a small steeply crevassed glacier in the Brown Hills, flowing west between Bowling Green Plateau and Blank Peaks. It was mapped by the Victoria University of Wellington Antarctic Expedition (1962–63), and named after J.A. Bartrum (1885–1949), Professor of Geology at the University of Auckland, New Zealand.\n\n"}
{"id": "439171", "url": "https://en.wikipedia.org/wiki?curid=439171", "title": "Bergmann's rule", "text": "Bergmann's rule\n\nBergmann's rule is an ecogeographical rule that states that within a broadly distributed taxonomic clade, populations and species of larger size are found in colder environments, and species of smaller size are found in warmer regions. Although originally formulated in terms of species within a genus, it has often been recast in terms of populations within a species. It is also often cast in terms of latitude. It is possible that the rule also applies to some plants, such as \"Rapicactus\".\n\nThe rule is named after nineteenth century German biologist Carl Bergmann, who described the pattern in 1847, although he was not the first to notice it. Bergmann's rule is most often applied to mammals and birds which are endotherms, but some researchers have also found evidence for the rule in studies of ectothermic species. such as the ant \"Leptothorax acervorum\". While Bergmann's rule appears to hold true for many mammals and birds, there are exceptions.\n\nLarger-bodied animals tend to conform more closely to Bergmann's rule than smaller-bodied animals, at least up to certain latitudes. This perhaps reflects a reduced ability to avoid stressful environments, such as by burrowing. In addition to being a general pattern across space, Bergmann's rule has been reported in populations over historical and evolutionary time when exposed to varying thermal regimes. In particular, reversible dwarfing of mammals has been noted during two relatively brief upward excursions in temperature during the Paleogene: the Paleocene-Eocene thermal maximum and the Eocene Thermal Maximum 2.\n\nHuman populations near the poles, including the Inuit, Aleut, and Sami people, are on average heavier than populations from mid-latitudes, consistent with Bergmann's rule. They also tend to have shorter limbs and broader trunks, consistent with Allen's rule. According to Marshall T. Newman in 1953, Native American populations are generally consistent with Bergmann's rule although the cold climate and small body size combination of the Eastern Eskimo, Canoe Indians, Yuki people, Andes natives and Harrison Lake Lillouet runs contrary to the expectations of Bergmann's rule. Newman contends that Bergmann's rule holds for the populations of Eurasia, but it does not hold for those of sub-Saharan Africa.\n\nThe earliest explanation, given by Bergmann when originally formulating the rule, is that larger animals have a lower surface area to volume ratio than smaller animals, so they radiate less body heat per unit of mass, and therefore stay warmer in cold climates. Warmer climates impose the opposite problem: body heat generated by metabolism needs to be dissipated quickly rather than stored within.\n\nThus, the higher surface area-to-volume ratio of smaller animals in hot and dry climates facilitates heat loss through the skin and helps cool the body. It is important to note that when analyzing Bergmann's Rule in the field that groups of populations being studied are of different thermal environments, and also have been separated long enough to genetically differentiate in response to these thermal conditions.\n\nIn marine crustaceans, it has been proposed that an increase in size with latitude is observed because decreasing temperature results in increased cell size and increased life span, both of which lead to an increase in maximum body size (continued growth throughout life is characteristic of crustaceans). The size trend has been observed in hyperiid and gammarid amphipods, copepods, stomatopods, mysids, and planktonic euphausiids, both in comparisons of related species as well as within widely distributed species. Deep-sea gigantism is observed in some of the same groups, probably for the same reasons.\n\nIn 1937 German zoologist and ecologist Richard Hesse proposed an extension of Bergmann's rule. Hesse's rule, also known as the heart–weight rule, states that species inhabiting colder climates have a larger heart in relation to body weight than closely related species inhabiting warmer climates.\n\nAccording to a study by Valerius Geist in 1986, Bergmann's rule is false: the correlation with temperature is spurious; instead, Geist found that body size is proportional to the duration of the annual productivity pulse, or food availability per animal during the growing season.\n\nBecause many factors can affect body size, there are many critics of Bergmann's Rule. Some believe that latitude itself is a poor predictor of body mass. Examples of other selective factors that may contribute to body mass changes are the size of food items available, effects of body size on success as a predator, effects of body size on vulnerability to predation, and resource availability. For example, if an organism is adapted to tolerate cold temperatures, it may also tolerate periods of food shortage, due to correlation between cold temperature and food scarcity. A larger organism can rely on its greater fat stores to provide the energy needed for survival as well being able to procreate for longer periods.\n\nResource availability is a major constraint on the overall success of many organisms. Resource scarcity can limit the total number of organisms in a habitat, and over time can also cause organisms to adapt by becoming smaller in body size. Resource availability thus becomes a modifying restraint on Bergmann’s Rule.\n\nBergmann's rule cannot generally be applied to plants, above all for latitude. Regarding Cactaceae, the case of \"Carnegiea gigantea\", described as \"a botanical Bergmann trend\" by Niering, Whittaker, & Lowe, has instead been shown to depend on rainfall, particularly winter precipitation, and not temperature, so Bergmann's rule is not applicable to \"Carnegiea\" populations. Members of the genus \"Rapicactus\" are larger in cooler environments, as their stem diameter increases with altitude and, above all, latitude. Since \"Rapicactus\" grow in a distributional area in which average precipitation tends to diminish at higher latitudes, and their body size is not conditioned by climatic variables, this could suggest a possible Bergmann trend.\n\n\n"}
{"id": "18848199", "url": "https://en.wikipedia.org/wiki?curid=18848199", "title": "BioPAX", "text": "BioPAX\n\nBioPAX (Biological Pathway Exchange) is a RDF/OWL-based\nstandard language to represent biological pathways\nat the molecular and cellular level. Its major use is to facilitate the exchange of pathway data.\nPathway data captures our understanding of biological processes, but\nits rapid growth necessitates development of databases and computational tools to aid\ninterpretation. However, the current fragmentation of pathway information across many\ndatabases with incompatible formats presents barriers to its effective use. BioPAX solves this\nproblem by making pathway data substantially easier to collect, index, interpret and share.\nBioPAX can represent metabolic and signaling pathways, molecular and genetic interactions and\ngene regulation networks. BioPAX was created through a community process. Through BioPAX,\nmillions of interactions organized into thousands of pathways across many organisms, from a\ngrowing number of sources, are available. Thus, large amounts of pathway data are available in a\ncomputable form to support visualization, analysis and biological discovery.\n\nIt is supported by a variety of online databases (e.g. Reactome) and tools. The latest released version is BioPAX Level 3. There is also an effort to create a version of BioPAX as part of OBO.\n\nThe next version of BioPAX, Level 4, is being developed by a community of researchers. Development is coordinated by board of editors and facilitated by various BioPAX work groups.\n\nSystems Biology Pathway Exchange (SBPAX) is an extension for Level 3 and proposal for Level 4 to add quantitative data and systems biology terms (such as Systems Biology Ontology). SBPAX export has been implemented by the pathway databases Signaling Gateway Molecule Pages and the SABIO-Reaction Kinetics Database. SBPAX import has been implemented by the cellular modeling framework Virtual Cell.\n\nOther proposals for Level 4 include improved support for Semantic Web, validation and visualization.\n\nOnline databases offering BioPAX export include:\n\nSoftware supporting BioPAX include:\n\n"}
{"id": "8458825", "url": "https://en.wikipedia.org/wiki?curid=8458825", "title": "Borlaug CAST Communication Award", "text": "Borlaug CAST Communication Award\n\nThe Borlaug CAST Communication Award, formerly the Charles A. Black Award, is presented annually by the Council for Agricultural Science and Technology (CAST) for outstanding achievement by a scientist, engineer, technologist, or other professional working in the agricultural, environmental, or food sectors for contributing to the advancement of science in the public policy arena. Primary consideration will be given to candidates who are actively engaged in promoting agriculture through research, teaching, extension, or mass communication; who have made significant contributions to their discipline or field; and who demonstrate a passionate interest in communicating the importance of agriculture to policymakers, the news media, and the public.\n\nNominees must have demonstrated their ability to communicate by written material; public presentations; and/or the use of television, radio, or other social media. They should be recognized by their peers as scientists who have made significant contributions in their professional fields.\n\nThe Award is dedicated to Dr. Norman E. Borlaug, \"The Man Who Fed the World\" and author of CAST publication #1; and to Dr. Charles A. Black, first CAST President, first Executive Vice President, and member of the founding committee of CAST.\n\n\n\n"}
{"id": "3005139", "url": "https://en.wikipedia.org/wiki?curid=3005139", "title": "Cauchy's equation", "text": "Cauchy's equation\n\nCauchy's equation is an empirical relationship between the refractive index and wavelength of light for a particular transparent material. It is named for the mathematician Augustin-Louis Cauchy, who defined it in 1836.\n\nThe most general form of Cauchy's equation is\n\nwhere \"n\" is the refractive index, λ is the wavelength, \"B\", \"C\", \"D\", etc., are coefficients that can be determined for a material by fitting the equation to measured refractive indices at known wavelengths. The coefficients are usually quoted for λ as the vacuum wavelength in micrometres.\n\nUsually, it is sufficient to use a two-term form of the equation:\n\nwhere the coefficients \"B\" and \"C\" are determined specifically for this form of the equation.\n\nA table of coefficients for common optical materials is shown below:\n\nThe theory of light-matter interaction on which Cauchy based this equation was later found to be incorrect. In particular, the equation is only valid for regions of normal dispersion in the visible wavelength region. In the infrared, the equation becomes inaccurate, and it cannot represent regions of anomalous dispersion. Despite this, its mathematical simplicity makes it useful in some applications.\n\nThe Sellmeier equation is a later development of Cauchy's work that handles anomalously dispersive regions, and more accurately models a material's refractive index across the ultraviolet, visible, and infrared spectrum.\n\n\n"}
{"id": "36116472", "url": "https://en.wikipedia.org/wiki?curid=36116472", "title": "Circular flow land use management", "text": "Circular flow land use management\n\nCircular flow land use management, or \"CircUse\", is a name for a particular process in which neglected land in urban areas is put to better uses. \"CircUse\" as a concept aims to be integrated with existing structures and uses, and is put into practice on a broad scale. The concept also looks to reduce the consumption of un-built land through prioritizing inner development over outer development. In Germany the approach of circular flow land use management has been developed and tested by the German Institute for Urban Affairs (Deutsches Institut für Urbanistik) on behalf of the German Federal Ministry of Transport, Building and Urban Affairs (BMVBS) and German Federal Office for Building and Regional Planning (BBR).\n\nThe concept of circular flow land use management can be described with the slogan “reduce - recycle – avoid”. To create sustainable land uses, actions have to be supported to find new innovative ways to “reduce” the consumption of land by new development “recycle” or put back into use abandoned and derelict sites, and “avoid” future land use decisions that are not sustainable. The concept puts a high value upon the development of inner districts through infill measures.\nThis approach takes into consideration issues related to urban sprawl, urban planning, brownfield land, and land use planning. The concept was developed by the German Institute for Urban Affairs (Difu) and the Federal Office for Building and Regional Planning (BBR) between 2003 and 2007 during the ExWoSt research field “circular land use management in cities and urban regions” (Kreislaufwirtschaft in der städtischen/stadtregionalen Flächennuntzung -. Fläche im Kreis)\n\nTo incorporate the circular flow land use management concept into the practice of city planning in Central Europe, the project “Circular Flow Land Use Management (CircUse)” was initiated by the European Union Central Europe organization as well as co-financed by the European Regional Development Fund. The CircUse project involves 12 partner organizations and 3 associated organizations in the six countries of Austria, Czech Republic, Germany, Italy, Poland and Slovakia. The lead organization is the Institute for the Ecology of Urban Areas (IETU), located in Katowice, Poland.\nThe main core outputs of the CircUse project will be:\n\nThe CircUse project was initiated in March 2010 and is scheduled to run until the end of February 2013.\n\nCircUse also builds on the experiences the German \"ExWoSt Research Field: Circular land use management in cities and urban regions” (ExWoSt Themafeld: Kreislaufwirtschaft in der städtischen/stadtregionalen Flächennutzung) and the German Federal Ministry of Education and Research (BMBF) Research Programme \"Research for the Reduction of Land Consumption and for Sustainable Land Management\" (Forschung für die Reduzierung der Flächeninanspruchnahme und ein nachhaltiges Flächenmanagement). Similar projects carried out by other organizations include the European Union PLUREL project.\n\n\n"}
{"id": "72710", "url": "https://en.wikipedia.org/wiki?curid=72710", "title": "Conceptual metaphor", "text": "Conceptual metaphor\n\nIn cognitive linguistics, conceptual metaphor, or cognitive metaphor, refers to the understanding of one idea, or conceptual domain, in terms of another. An example of this is the understanding of quantity in terms of directionality (e.g. \"the price of peace is \"rising\"\") or the understanding of time in terms of money (e.g. \"I \"spent\" time at work today\").\n\nA conceptual domain can be any coherent organization of human experience. The regularity with which different languages employ the same metaphors, which often appear to be perceptually based, has led to the hypothesis that the mapping between conceptual domains corresponds to neural mappings in the brain. This theory has gained wide attention, although some researchers question its empirical accuracy. \n\nThis idea, and a detailed examination of the underlying processes, was first extensively explored by George Lakoff and Mark Johnson in their work \"Metaphors We Live By\" in 1980. Since then, the field of metaphor studies within the larger discipline of Cognitive Linguistics has increasingly developed, with several, annual academic conferences, scholarly societies, and research labs contributing to the subject area. Some researchers, such as Gerard Steen, have worked to develop empirical investigative tools for metaphor research, including the Metaphor Identification Procedure, or MIP. In Psychology, Raymond W. Gibbs, Jr., has investigated conceptual metaphor and embodiment through a number of psychological experiments. Other cognitive scientists, for example Gilles Fauconnier, study subjects similar to conceptual metaphor under the labels \"analogy\", \"conceptual blending\" and \"ideasthesia\".\n\nConceptual metaphors are seen in language in our everyday lives. Conceptual metaphors shape not just our communication, but also shape the way we think and act. In George Lakoff and Mark Johnson's work, \"Metaphors We Live By\" (1980), we see how everyday language is filled with metaphors we may not always notice. An example of one of the commonly used conceptual metaphors is \"argument is war\". This metaphor shapes our language in the way we view argument as war or as a battle to be won. It is not uncommon to hear someone say \"He won that argument\" or \"I attacked every weak point in his argument\". The very way argument is thought of is shaped by this metaphor of arguments being war and battles that must be won. Argument can be seen in other ways than a battle, but we use this concept to shape the way we think of argument and the way we go about arguing.\n\nConceptual metaphors are used very often to understand theories and models. A conceptual metaphor uses one idea and links it to another to better understand something. For example, the conceptual metaphor of viewing communication as a conduit is one large theory explained with a metaphor. So not only is our everyday communication shaped by the language of conceptual metaphors, but so is the very way we understand scholarly theories. These metaphors are prevalent in communication and we do not just use them in language; we actually perceive and act in accordance with the metaphors.\n\nIn the Western philosophical tradition, Aristotle is often situated as the first commentator on the nature of metaphor, writing in the \"Poetics\", \"A 'metaphorical term' involves the transferred use of a term that properly belongs to something else,\" and elsewhere in the \"Rhetoric\" he says that metaphors make learning pleasant; \"To learn easily is naturally pleasant to all people, and words signify something, so whatever words create knowledge in us are the pleasantest.\" Aristotle's writings on metaphor constitute a \"substitution view\" of metaphor, wherein a metaphor is simply a decorative word or phrase substituted for a more ordinary one. This has been sometimes called the \"Traditional View of Metaphor\" and at other times the \"Classical Theory of Metaphor\". Later in the first century A.D., the Roman rhetorician Quintilian builds upon Aristotle's earlier work of metaphor by focusing more on the comparative function of metaphorical language. In his work \"Institutio Oratoria,\" Quintilian states,\" In totum autem metaphora brevior est similitudo\" or \"on the whole, metaphor is a shorter form of simile\". Modern interpretations of these early theories have also been intensely debated. Janet Soskice, Professor of Philosophical Theology at the University of Cambridge, writes in summary that \"it is certain that we shall taste the freshness of their insights only if we free them from the obligation to answer questions that were never theirs to ask\". George Lakoff and Mark Johnson, although originally taking a hard-line interpretation of these early authors later concede that Aristotle was working within a different philosophical framework from what we engage with today and that critical interpretations should take this in to account.\n\nThere are two main roles for the conceptual domains posited in conceptual metaphors:\n\nA mapping is the systematic set of correspondences that exist between constituent elements of the source and the target domain. Many elements of target concepts come from source domains and are not preexisting. To know a conceptual metaphor is to know the set of mappings that applies to a given source-target pairing. The same idea of mapping between source and target is used to describe analogical reasoning and inferences.\n\nA primary tenet of this theory is that metaphors are matter of thought and not merely of language: hence, the term \"conceptual metaphor\". The metaphor may seem to consist of words or other linguistic expressions that come from the terminology of the more concrete conceptual domain, but conceptual metaphors underlie a system of related metaphorical expressions that appear on the linguistic surface. Similarly, the mappings of a conceptual metaphor are themselves motivated by image schemas which are pre-linguistic schemas concerning space, time, moving, controlling, and other core elements of embodied human experience.\n\nConceptual metaphors typically employ a more abstract concept as target and a more concrete or physical concept as their source. For instance, metaphors such as 'the days [the more abstract or target concept] ahead' or 'giving my time' rely on more concrete concepts, thus expressing time as a path into physical space, or as a substance that can be handled and offered as a gift. Different conceptual metaphors tend to be invoked when the speaker is trying to make a case for a certain point of view or course of action. For instance, one might associate \"the days ahead\" with leadership, whereas the phrase \"giving my time\" carries stronger connotations of bargaining. Selection of such metaphors tends to be directed by a subconscious or implicit habit in the mind of the person employing them.\n\nThe principle of unidirectionality states that the metaphorical process typically goes from the more concrete to the more abstract, and not the other way around. Accordingly, abstract concepts are understood in terms of prototype concrete processes. The term \"concrete,\" in this theory, has been further specified by Lakoff and Johnson as more closely related to the developmental, physical neural, and interactive body (see embodied philosophy). One manifestation of this view is found in the cognitive science of mathematics, where it is proposed that mathematics itself, the most widely accepted means of abstraction in the human community, is largely metaphorically constructed, and thereby reflects a cognitive bias unique to humans that uses embodied prototypical processes (e.g. counting, moving along a path) that are understood by all human beings through their experiences.\n\nThe conduit metaphor is a dominant class of figurative expressions used when discussing communication itself (metalanguage). It operates whenever people speak or write as if they \"insert\" their mental contents (feelings, meanings, thoughts, concepts, etc.) into \"containers\" (words, phrases, sentences, etc.) whose contents are then \"extracted\" by listeners and readers. Thus, language is viewed as a \"conduit\" conveying mental content between people.\n\nDefined and described by linguist Michael J. Reddy, PhD, his proposal of this conceptual metaphor refocused debate within and outside the linguistic community on the importance of metaphorical language.\n\nIn their 1980 work, Lakoff and Johnson closely examined a collection of basic conceptual metaphors, including:\n\nThe latter half of each of these phrases invokes certain assumptions about concrete experience and requires the reader or listener to apply them to the preceding abstract concepts of love or organizing in order to understand the sentence in which the conceptual metaphor is used.\n\nThere are numerous ways in which conceptual metaphors shape human perception and communication, especially in mass media and in public policy.\n\nLakoff and Johnson focus on English, and cognitive scholars writing in English have tended not to investigate the discourse of foreign languages in any great detail to determine the creative ways in which individuals negotiate, resist, and consolidate conceptual metaphors. Andrew Goatly in his book \"Washing the Brain\" (2007) considers ideological conceptual metaphors as well as Chinese conceptual metaphors.\n\nJames W. Underhill, a modern Humboldtian scholar, attempts to reestablish Wilhelm von Humboldt's concern for the different ways languages frame reality, and the strategies individuals adopt in creatively resisting and modifying existing patterns of thought. Taking on board the Lakoff-Johnson paradigm of conceptual metaphor, he investigates the way in which Czech communists appropriated the concept of the people, the state and struggle, and the way German Communists harnessed concepts of eternity and purity. He also reminds us that, as Klemperer, the main critic of Hitlerdeutsch, demonstrates, resisting patterns of thought means engaging in conceptual metaphors and refusing the logic that ideologies impose upon them. In multilingual studies (based on Czech, German, French & English), Underhill considers how different cultures reformulate key concepts such as truth, love, hate and war.\n\nA less extreme, but similar, claim is made by George Lakoff in his book \"Moral Politics\" and his later book on framing, \"Don't Think of an Elephant!.\" Lakoff claims that the public political arena in America reflects a basic conceptual metaphor of 'the family.' Accordingly, people understand political leaders in terms of 'strict father' and 'nurturant mother' roles. Two basic views of political economy arise from this desire to see the nation-state act 'more like a father' or 'more like a mother.' He further amplified these views in his latest book, \"The Political Mind.\"\n\nUrban theorist and ethicist Jane Jacobs made this distinction in less gender-driven if not wholly desexualizing terms by differentiating between a 'Guardian Ethic' and a 'Trader Ethic'. She states that guarding and trading are two concrete activities that human beings must learn to apply metaphorically to all choices in later life. In a society where guarding children is the primary female duty and trading in a market economy is the primary male duty, Lakoff posits that children assign the 'guardian' and 'trader' roles to their mothers and fathers, respectively.\n\nBoth of these theories suggest that there may be a great deal of social conditioning and pressure to form specific cognitive bias. Anthropologists observe that all societies tend to have roles assigned by age and gender, which supports this view.\n\nLakoff and Jacobs both devote a significant amount of time to current events and political theory, suggesting that respected linguists and theorists of conceptual metaphor may tend to channel their theories into political activism.\n\nCritics of this ethics-driven approach to language tend to accept that idioms reflect underlying conceptual metaphors, but that actual grammar, and the more basic cross-cultural concepts of scientific method and mathematical practice tend to minimize the impact of metaphors. Such critics tend to see Lakoff and Jacobs as 'left-wing figures,' and would not accept their politics as any kind of crusade against an ontology embedded in language and culture, but rather, as an idiosyncratic pastime, not part of the science of linguistics nor of much use. And others further, such as Deleuze and Guattari, Michel Foucault and, more recently, Manuel de Landa would criticize both of these two positions for mutually constituting the same old ontological ideology that would try to separate two parts of a whole that is greater than the sum of its parts.\n\nLakoff's 1987 work, \"Women, Fire, and Dangerous Things,\" answered some of these criticisms before they were even made: he explores the effects of cognitive metaphors (both culturally specific and human-universal) on the grammar per se of several languages, and the evidence of the limitations of the classical logical-positivist or Anglo-American School philosophical concept of the category usually used to explain or describe the scientific method. Lakoff's reliance on empirical scientific evidence, \"i.e.\" specifically falsifiable predictions, in the 1987 work and in \"Philosophy in the Flesh\" (1999) suggests that the cognitive-metaphor position has no objections to the scientific method, but instead considers the scientific method a finely developed reasoning system used to discover phenomena which are subsequently understood in terms of new conceptual metaphors (such as the metaphor of fluid motion for conducted electricity, which is described in terms of \"current\" \"flowing\" against \"impedance,\" or the gravitational metaphor for static-electric phenomena, or the \"planetary orbit\" model of the atomic nucleus and electrons, as used by Niels Bohr).\n\nFurther, partly in response to such criticisms, Lakoff and Rafael E. Núñez, in 2000, proposed a cognitive science of mathematics that would explain mathematics as a consequence of, not an alternative to, the human reliance on conceptual metaphor to understand abstraction in terms of basic experiential concretes.\n\nThe Linguistic Society of America has argued that \"the most recent linguistic approach to literature is that of cognitive metaphor, which claims that metaphor is not a mode of language, but a mode of thought. Metaphors project structures from source domains of schematized bodily or enculturated experience into abstract target domains. We conceive the abstract idea of life in terms of our experiences of a journey, a year, or a day. We do not understand Robert Frost's 'Stopping by Woods on a Snowy Evening' to be about a horse-and-wagon journey but about life. We understand Emily Dickinson's 'Because I could not stop for Death' as a poem about the end of the human life span, not a trip in a carriage. This work is redefining the critical notion of imagery. Perhaps for this reason, cognitive metaphor has significant promise for some kind of rapprochement between linguistics and literary study.\"\n\nTeaching thinking by analogy (metaphor) is one of the main themes of The Private Eye Project.\n\nThe work of political scientist Rūta Kazlauskaitė examines metaphorical models in school-history knowledge of the controversial Polish-Lithuanian past. On the basis of Lakoff and Johnson's conceptual metaphor theory, she shows how the implicit metaphorical models of everyday experience, which inform the abstract conceptualization of the past, truth, objectivity, knowledge, and multiperspectivity in the school textbooks, obstruct an understanding of the divergent narratives of past experience.\n\nThere is some evidence that an understanding of underlying conceptual metaphors can aid the retention of vocabulary for people learning a foreign language. To improve learners' awareness of conceptual metaphor, one monolingual learner's dictionary, the Macmillan English Dictionary has introduced 50 or so 'metaphor boxes' covering the most salient Lakoffian metaphors in English. For example, the dictionary entry for \"conversation\" includes a box with the heading: 'A conversation is like a journey, with the speakers going from one place to another', followed by vocabulary items (words and phrases) which embody this metaphorical schema. Language teaching experts are beginning to explore the relevance of conceptual metaphor to how learners learn and what teachers do in the classroom.\n\nA current study showed a natural tendency to systematically map an abstract dimension, such as social status, in our closest and non-linguistic relatives, the chimpanzees. In detail, discrimination performances between familiar conspecific faces were systematically modulated by the spatial location and the social status of the presented individuals, leading to discrimination facilitation or deterioration. High-ranked individuals presented at spatially higher position and low-ranked individuals presented at lower position led to discrimination facilitation, while high-ranked individuals at lower positions and low-ranked individuals at higher position led to discrimination deterioration. This suggests that this tendency had already evolved in the common ancestors of humans and chimpanzees and is not uniquely human, but describes a conceptual metaphorical mapping that predates language.\n\n\n\n"}
{"id": "38261726", "url": "https://en.wikipedia.org/wiki?curid=38261726", "title": "Developmental Leadership Program", "text": "Developmental Leadership Program\n\nThe Developmental Leadership Program (DLP, or DLPROG) is an international research and policy initiative. The Program looks at the political processes that underpin development goals such as sustainable economic growth, political stability and inclusive social development. In particular, DLP explores the central role of leaders, elites and coalitions in developing countries and how they can help or hinder the positive reform of institutions and policies in the public, private and civil society sectors. DLP is funded primarily by the Australian Aid Program.\n\nThe core aim of DLP is to develop a clearer understanding of the political processes that support developmental change and to embed that understanding in the thinking, policies and practices of the wider development community. DLP does this by:\n\n\nDLP is run by a Program Management Team. While DLP's funding comes primarily from the Australian national government, the Program is independent and autonomous. \nAs of July 2014, DLP's management team consists of:\n\n\nDLP has its origins in the work of the Leaders, Elites and Coalitions Research Program (LECRP), which began in 2007. It evolved into the Leadership Program: Developmental Leaders, Elites and Coalitions (LPDLEC) in 2009 before adopting its current form.\n\n"}
{"id": "22946021", "url": "https://en.wikipedia.org/wiki?curid=22946021", "title": "Dilly (crater)", "text": "Dilly (crater)\n\nDilly Crater is a crater in the Elysium quadrangle of Mars, located at \n13.24° North and 202.9° West. It is only 1.3 km in diameter and was named after a town in Mali.\n\nImpact craters generally have a rim with ejecta around them, in contrast volcanic craters usually do not have a rim or ejecta deposits. As craters get larger (greater than 10 km in diameter) they usually have a central peak. The peak is caused by a rebound of the crater floor following the impact. \n"}
{"id": "1331441", "url": "https://en.wikipedia.org/wiki?curid=1331441", "title": "Document classification", "text": "Document classification\n\nDocument classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done \"manually\" (or \"intellectually\") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.\n\nThe documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.\n\nDocuments may be classified according to their subjects or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach.\n\nContent-based classification is classification in which the weight given to particular subjects in a document determines the class to which the document is assigned. It is, for example, a common rule for classification in libraries, that at least 20% of the content of a book should be about the class to which the book is assigned. In automatic classification it could be the number of times given words appears in a document.\n\nRequest-oriented classification (or -indexing) is classification in which the anticipated request from users is influencing how documents are being classified. The classifier asks themself: “Under which descriptors should this entity be found?” and “think of all the possible queries and decide for which ones the entity at hand is relevant” (Soergel, 1985, p. 230).\n\nRequest-oriented classification may be classification that is targeted towards a particular audience or user group. For example, a library or a database for feminist studies may classify/index documents differently when compared to a historical library. It is probably better, however, to understand request-oriented classification as \"policy-based classification\": The classification is done according to some ideals and reflects the purpose of the library or database doing the classification. In this way it is not necessarily a kind of classification or indexing based on user studies. Only if empirical data about use or users are applied should request-oriented classification be regarded as a user-based approach.\n\nSometimes a distinction is made between assigning documents to classes (\"classification\") versus assigning subjects to documents (\"subject indexing\") but as Frederick Wilfrid Lancaster has argued, this distinction is not fruitful. \"These terminological distinctions,” he writes, “are quite meaningless and only serve to cause confusion” (Lancaster, 2003, p. 21). The view that this distinction is purely superficial is also supported by the fact that a classification system may be transformed into a thesaurus and vice versa (cf., Aitchison, 1986, 2004; Broughton, 2008; Riesthuis & Bliedung, 1991). Therefore, is the act of labeling a document (say by assigning a term from a controlled vocabulary to a document) at the same time to assign that document to the class of documents indexed by that term (all documents indexed or classified as X belong to the same class of documents).\n\nAutomatic document classification tasks can be divided into three sorts: supervised document classification where some external mechanism (such as human feedback) provides information on the correct classification for documents, unsupervised document classification (also known as document clustering), where the classification must be done entirely without reference to external information, and semi-supervised document classification, where parts of the documents are labeled by the external mechanism. There are several software products under various license models available.\n\nAutomatic document classification techniques include:\n\nClassification techniques have been applied to\n\n\n"}
{"id": "27669989", "url": "https://en.wikipedia.org/wiki?curid=27669989", "title": "Ensemble averaging (machine learning)", "text": "Ensemble averaging (machine learning)\n\nIn machine learning, particularly in the creation of artificial neural networks, ensemble averaging is the process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models \"average out.\"\n\nEnsemble averaging is one of the simplest types of committee machines. Along with boosting, it is one of the two major types of static committee machines. In contrast to standard network design in which many networks are generated but only one is kept, ensemble averaging keeps the less satisfactory networks around, but with less weight. The theory of ensemble averaging relies on two properties of artificial neural networks:\n\n\nEnsemble averaging creates a group of networks, each with low bias and high variance, then combines them to a new network with (hopefully) low bias and low variance. It is thus a resolution of the bias-variance dilemma. The idea of combining experts has been traced back to Pierre-Simon Laplace.\n\nThe theory mentioned above gives an obvious strategy: create a set of experts with low bias and high variance, and then average them. Generally, what this means is to create a set of experts with varying parameters; frequently, these are the initial synaptic weights, although other factors (such as the learning rate, momentum etc.) may be varied as well. Some authors recommend against varying weight decay and early stopping. The steps are therefore:\nAlternatively, domain knowledge may be used to generate several \"classes\" of experts. An expert from each class is trained, and then combined.\n\nA more complex version of ensemble average views the final result not as a mere average of all the experts, but rather as a weighted sum. If each expert is formula_1, then the overall result formula_2 can be defined as:\nwhere formula_4 is a set of weights. The optimization problem of finding alpha is readily solved through neural networks, hence a \"meta-network\" where each \"neuron\" is in fact an entire neural network can be trained, and the synaptic weights of the final network is the weight applied to each expert. This is known as a \"linear combination of experts\".\n\nIt can be seen that most forms of neural networks are some subset of a linear combination: the standard neural net (where only one expert is used) is simply a linear combination with all formula_5 and one formula_6. A raw average is where all formula_7 are equal to some constant value, namely one over the total number of experts.\n\nA more recent ensemble averaging method is negative correlation learning, proposed by Y. Liu and X. Yao. Now this method has been widely used in evolutionary computing.\n\n\n\n"}
{"id": "20840626", "url": "https://en.wikipedia.org/wiki?curid=20840626", "title": "European Lunar Explorer", "text": "European Lunar Explorer\n\nThe European Lunar Explorer (ELE) or European Lunar Lander (ELL) was a planned Romanian Lunar lander. It was being developed by ARCA, as an entrant for the Google Lunar X Prize, until it was cancelled in 2014.\n\nIt was intended to have a mass of 400 kilograms when fully fueled, including two upper stages to propel it from low Earth orbit onto a trajectory towards the Moon. The lander itself had a monopropellant cold rocket engine, fuelled by hydrogen peroxide, which was to slow its descent towards the surface of the Moon. The target landing site was Montes Carpatus. The spacecraft was designed to travel 500 metres after landing, in order to explore its landing site.\n\nELL was the part of the spacecraft to land on the Moon, while ELE was the complete spacecraft, including the two stages intended to propel it from low Earth orbit to a trans-lunar trajectory.\n\n"}
{"id": "962213", "url": "https://en.wikipedia.org/wiki?curid=962213", "title": "Excavatability", "text": "Excavatability\n\nThe excavatability of an earth (rock and regolith) material is a measure of the material to be excavated (dug) with conventional excavation equipment such as a bulldozer with rippers, backhoe, scraper and other grading equipment. Materials that cannot be excavated with conventional excavation equipment are said to be non-rippable. Such material typically requires pre-blasting or use of percussion hammers or chisels to facilitate excavation. The excavatability or rippability of earth materials is evaluated typically by a geophysicist, engineering geologist, or geotechnical engineer.\n\n"}
{"id": "4427242", "url": "https://en.wikipedia.org/wiki?curid=4427242", "title": "Finger Prints (book)", "text": "Finger Prints (book)\n\nFinger Prints is a book published by Francis Galton through Macmillan in 1892. It was one of the first books to provide a scientific footing for matching fingerprints and for later acceptance in courts.\n\n"}
{"id": "34265825", "url": "https://en.wikipedia.org/wiki?curid=34265825", "title": "Geroch's splitting theorem", "text": "Geroch's splitting theorem\n\nIn the theory of causal structure on Lorentzian manifolds, Geroch's theorem or Geroch's splitting theorem (first proved by Robert Geroch) gives a topological characterization of globally hyperbolic spacetimes.\n\nLet formula_1 be a globally hyperbolic spacetime. Then formula_1 is strongly causal and there exists a global \"time function\" on the manifold, i.e. a continuous, surjective map formula_3 such that:\nMoreover, all Cauchy surfaces are homeomorphic, and formula_7 is homeomorphic to formula_8 where formula_9 is any Cauchy surface of formula_7.\n"}
{"id": "25367246", "url": "https://en.wikipedia.org/wiki?curid=25367246", "title": "Grazing fire", "text": "Grazing fire\n\nGrazing fire is a term used in military science and defined by NATO and the US Department Of Defense as “Fire approximately parallel to the ground where the center of the cone of fire does not rise above one meter from the ground.” Grazing fire is often performed by machine guns. It is tactically advantageous when attempting to cut off an infantry ground assault or counter-attack.\n\nGrazing fire is when the cone of fire does not exceed 1.8 meters (the average height of a man) above the ground. When each bullet is fired, it will leave the barrel of the weapon at the axis of bore set from the angle of sight desired to strike the target area. The trajectory of the round should be constant as well as maximum ordinate. Maximum ordinate is the highest point of the trajectory, which is usually 2/3 the distance to the target from the weapon. Depending on the caliber of the round being fired and the slope of the terrain, as well as the distance to the target, the bullet will maintain a semi-flat trajectory. With 7.62×51mm NATO ammunition, which is most commonly used with, for example, the M240 machine gun, the bullet will reach a distance of 600 meters traveling at 900 meters per second or 2,800 feet per second, before it begins a pronounced downward trajectory to the earth. Any distance beyond that range will be considered plunging fire due to the arc (axis of bore + angle of sight) needed to impact the target area.\n\nGrazing fire is so named because it cuts the vegetation low as though a herd of cattle had been grazing there. The optimal height of fire should be at knee height. In addition to keeping the enemy pinned down, anybody wounded or going to cover will fall into the line of fire, not out of it.\n"}
{"id": "27029830", "url": "https://en.wikipedia.org/wiki?curid=27029830", "title": "Gynophore", "text": "Gynophore\n\nA gynophore is the stalk of certain flowers which supports the gynoecium (the ovule-producing part of a flower), elevating it above the branching points of other floral parts.\n\nPlant genera that have flowers with gynophores include \"Telopea\", \"Peritoma arborea\" and \"Brachychiton\".\n"}
{"id": "56698888", "url": "https://en.wikipedia.org/wiki?curid=56698888", "title": "Habis", "text": "Habis\n\nHabis (from the Cynete language meaning fawn) is a legendary king of the Spanish region of Tartessos.\n\nAccording to legend, Habis's story began when his mother was stung by a bee. Her father, the king of Tartessos, hurried to heal her. As he knelt to soften her wound with his mouth, a passion came over both, thus procreating the baby. Due to the shame of having committed incest, the king ordered the child to be abandoned on the prairies outside the kingdom. Eventually the boy was raised by the deer in there, acquiring deer features. He was later found and recognized as the true heir to the kingdom, and later accordingly named.\n"}
{"id": "26091147", "url": "https://en.wikipedia.org/wiki?curid=26091147", "title": "Herpetosiphonales", "text": "Herpetosiphonales\n\nHerpetosiphonales is one of two orders of bacteria in the class Chloroflexi.\n\n"}
{"id": "2334800", "url": "https://en.wikipedia.org/wiki?curid=2334800", "title": "Highly optimized tolerance", "text": "Highly optimized tolerance\n\nIn applied mathematics, highly optimized tolerance (HOT) is a method of generating power law behavior in systems by including a global optimization principle. For some systems that display a characteristic scale, a global optimization term could potentially be added that would then yield power law behavior. It has been used to generate and describe internet-like graphs, forest fire models and may also apply to biological systems.\n\nThe following is taken from Sornette's book.\n\nConsider a random variable, formula_1, that takes on values formula_2 with probability formula_3. Furthmore, lets assume for another parameter formula_4\nfor some fixed formula_6. We then want to minimize\nsubject to the constraint\nUsing Lagrange multipliers, this gives\ngiving us a power law. The global optimization of minimizing the energy along with the power law dependence between formula_2 and formula_4 gives us a power law distribution in probability.\n\n\n"}
{"id": "53950535", "url": "https://en.wikipedia.org/wiki?curid=53950535", "title": "How to Build a Dinosaur", "text": "How to Build a Dinosaur\n\nHow to Build a Dinosaur: Extinction Doesn't Have to Be Forever is a 2009 book by paleontologist Jack Horner and James Gorman. The book outlines Horner's theory for being able to resurrect a maniraptoran dinosaur by altering the genes of a chicken embryo. In 2010, a paperback version was published under the title How to Build a Dinosaur: The New Science of Reverse Evolution.\n\nPaleontologist Jack Horner describes evolutionary developmental biology (evo-devo) and outlines his theory for being able to resurrect a maniraptoran dinosaur from a chicken embryo, by activating and deactivating certain genes to restore dormant dinosaur characteristics such as a tail, claws, teeth, and a snout. Horner also discusses paleontology in the book.\n\nHorner's idea for the \"Chickenosaurus\" project came from \"a pretty good script\" that was written for \"Jurassic Park IV\" early in its development. The film's story, at that time, was expected to involve genetic engineering of dinosaurs. Horner was planning the book with co-author James Gorman in spring 2005. Gorman was deputy science editor for \"The New York Times\", and had previously co-written Horner's 1990 book, \"Digging Dinosaurs\".\n\nHorner and his publisher planned for the book to come out at the same time as \"Jurassic Park IV\", to serve as a scientific companion volume; however, the film was delayed. Horner's book, in hardcover form, was ultimately published by Dutton Penguin on March 19, 2009, without the accompaniment of the film. The book was initially published with the title \"How to Build a Dinosaur: Extinction Doesn't Have to Be Forever\". A paperback version was published by Plume on February 23, 2010, with the title \"How to Build a Dinosaur: The New Science of Reverse Evolution\".\n\n\"Publishers Weekly\" called the book \"provocative but frustrating\", writing that aside from the main concept, \"Much of the rest of the book offers background, but often digresses, for example, into hunting for DNA from 68-million-year-old dinosaur bones or the surfing habit of the man who discovered the polymerase chain reaction or how genetically close humans and Neanderthals are—none of which advances the book's central argument.\"\n\nKirkus Reviews wrote that the book \"has a comfortable, intelligent flow,\" but noted that Horner first \"wants to tell a story—and it's a good one, though at times meandering—about paleontology […]. Horner digresses about skinheads, Ted Kaczynski and chicken carcasses, but his main idea is reverse evolutionary engineering.\" Gilbert Taylor of Booklist wrote, \"Straight from the scientific frontier, Horner's work should excite anyone who's dreamed of walking with dinosaurs.\"\n\nBrian Switek of \"Smithsonian\" wrote, \"The importance of \"How to Build a Dinosaur\" does not lie in Horner's wish to create a dinochicken. That makes up only a small part of the book. Instead the slim volume indicates how paleontology is becoming more of an interdisciplinary science where studies of development and genetics are just as important as fossilized bones.\" Jeff Hecht of \"New Scientist\" wrote that Horner \"is at his best\" in the book, which he called \"provocative yet firmly grounded in science.\"\n"}
{"id": "6060363", "url": "https://en.wikipedia.org/wiki?curid=6060363", "title": "Lands Beyond", "text": "Lands Beyond\n\nLands Beyond is a study of geographical myths by L. Sprague de Camp and Willy Ley, first published in hardcover by Rinehart in 1952, and reissued by Barnes & Noble in 1993. It has been translated into French, Spanish, Portuguese, and Italian. It was the winner of the 1953 International Fantasy Award for nonfiction.\n\n\n\"New York Times\" columnist Charles Poore placed \"Lands Beyond\" on his annual list of books recommended for Christmas giving. Kirkus Reviews recommended it as \"a zestful geographical round-up which combines fact, legend and literature in equally interested parts\".\n\nBoucher and McComas praised the book, saying it was \"written with scholarly authority, literary grace, and an amusedly tolerant exposition of error, to make one of the season's most enjoyable items.\" \"New Worlds\" reviewer Leslie Flood described it as \"fascinating\". \"Weird Tales\" commended \"Lands Beyond\" to its audience, saying de Camp and Ley \"ably treated\" their subjects \"for reader enjoyment\". George O. Smith wrote that it was \"a book good for the younger and more impressionable to read, because it reduces to the realm of practicality many of the fabulous mysteries of the past, thus stripping the glamorous Long-Ago of its false superiority\".\n\n"}
{"id": "28842451", "url": "https://en.wikipedia.org/wiki?curid=28842451", "title": "List of British scientists", "text": "List of British scientists\n\nThis is a list of British scientists.\n\n\n"}
{"id": "4286042", "url": "https://en.wikipedia.org/wiki?curid=4286042", "title": "List of FTP server return codes", "text": "List of FTP server return codes\n\nFTP server return codes always have three digits, and each digit has a special meaning. The first digit denotes whether the response is good, bad or incomplete:\n\nThe second digit is a grouping digit and encodes the following information:\n\nBelow is a list of all known return codes that may be issued by an FTP server.\n\n"}
{"id": "29682267", "url": "https://en.wikipedia.org/wiki?curid=29682267", "title": "List of Ivy League medical schools", "text": "List of Ivy League medical schools\n\nThis list of Ivy League medical schools outlines the seven universities of the Ivy League that host a medical school. Only one Ivy League university, Princeton University, does not have a medical school. All Ivy League medical schools are located in the Northeast region of the United States and are privately owned and controlled. Only two of the schools, at Dartmouth and Penn, are located adjacent to the undergraduate campuses. The schools at Brown, Columbia, Harvard, and Yale, are located on separate campuses in the same metropolitan area as the university. The school at Cornell, Weill Cornell Medicine, is located in New York City, far from the main Ithaca campus.\n\nIvy League medical schools have some of the best reputations among medical schools in the United States. They are some of the oldest institutions of medical education, and leaders among medical research. They are also associated with some of the best hospitals in the United States. Some publications' most recent rankings of medical schools at these institutions are shown below.\n"}
{"id": "11096816", "url": "https://en.wikipedia.org/wiki?curid=11096816", "title": "List of Superfund sites in Washington, D.C.", "text": "List of Superfund sites in Washington, D.C.\n\nThis is a list of Superfund sites in Washington, DC designated under the Comprehensive Environmental Response, Compensation, and Liability Act.\n\n\n"}
{"id": "58361969", "url": "https://en.wikipedia.org/wiki?curid=58361969", "title": "List of neuroscience journals", "text": "List of neuroscience journals\n\nThis page lists peer-reviewed scientific journals in the field of neuroscience.\n\n\n\n\n\n\n"}
{"id": "25169071", "url": "https://en.wikipedia.org/wiki?curid=25169071", "title": "List of things named after Fibonacci", "text": "List of things named after Fibonacci\n\nThe Fibonacci numbers are the best known concept named after Leonardo of Pisa, known as Fibonacci. Among others are the following.\n\n\n\n\n"}
{"id": "7120299", "url": "https://en.wikipedia.org/wiki?curid=7120299", "title": "List of volcanoes in the United Kingdom", "text": "List of volcanoes in the United Kingdom\n\nThere are no active volcanoes in the United Kingdom of Great Britain and Northern Ireland, but there are a few active volcanoes in some British Overseas Territories. Queen Mary's Peak in Tristan da Cunha erupted in 1961-2. The Soufrière Hills volcano on the Caribbean island of Montserrat erupted three times between 1995 and 2013. Mount Belinda in the British Overseas Territory of South Georgia and the South Sandwich Islands, erupted between 2001 and 2007. Mount Michael, also in South Georgia and the South Sandwich Islands, was active in 2005; its summit crater is thought to possibly contain an active lava lake, one of only a handful in the world.\n\nBelow is a list of extinct volcanoes in the United Kingdom.\n\n"}
{"id": "53949226", "url": "https://en.wikipedia.org/wiki?curid=53949226", "title": "Man and Matter", "text": "Man and Matter\n\nMan and Matter - Essays Scientific & Christian is a 1951 book written by a British chemist, museum curator and historian of science Frank Sherwood Taylor. The work presents a critical mind's account of the clash between religion and science. It provides insight into an intriguing perspective of a person, who has been received into the Catholic Church after forty years of struggling to find his way in a conflicted world of scientific and religious explanations.\n\nThe book consists of a preface, personal introduction, and twelve essays, read to the followers of the Catholic Church. The essays reflect on various, more or less controversial issues dividing religion and science, such as materialism, pain, and morality. They have been written at different times and therefore represent the shifting views of the author, as he searches for decisive arguments. All essays have been intended to fall in line with \"the Christian doctrine and common sense\".\n\nIn the preface, the author informs the reader of the position he has assumed when writing the essays - one of a formerly confused, but now reassured, believer.\n\nFirst chapter, adequately titled Personal Introduction, gives insightful information on why a scientifically inclined and critical person would choose to return to Church, after having been exposed to various religious and scientific influences throughout life. This chapter provides rich insights into author's early experiences, such as growing up in an Anglican family, receiving Christian education, praying, learning the Bible, and partaking in various religious customs and traditions, all very typical of the times, place, and author's social class. However, despite these influences, Sherwood Taylor had great difficulties accepting faith and religion as they were presented to him. Realization that religion might lack rational foundation has severely swayed his views, and initiated an over 40-year long journey in quest of an ultimate verdict between religion and materialism. Along the way, the author has encountered themes such as mind and body, physical concepts of extension, mass, and motion, perception, superstitions, consciousness, spiritualism, qualia and many more, all having great influence on the author and contributing to his understanding of the world, but still not decisive. Sherwood Taylor's problem with materialism lied in its inability to account for mental experiences, and for the sense of \"self\" as a thinking entity. Additionally, scientific praise of determinism was hardly in line with Sherwood Taylor's belief in will and choice. Despite his great respect for science, the author started to find it increasingly difficult to believe that science could ever explain his life, thoughts, and experiences, his poetic, and mystic side, desperately searching for God.\n\nA pivotal turn took place, when F. Sherwood Taylor accidentally received a letter meant for a member of the Rationalist Press Association, asking to give a lecture on Galileo. Despite a mistake, Sherwood Taylor offered his services, and soon found himself an expert on Galileo's case. While investigating the matter, he came to the conclusion, that Galileo's story was full of deliberate distortions implemented by anti-Catholic and \"rational\" writers. This made him realize that science is guilty of all the offences usually assigned to Church - it's ill-founded, wicked, deceitful, and superstitious. Following this and other events, including hearing a voice in his head say \"Why are you wasting your life?\", F. Sherwood Taylor started to see Christianity as the purest and most intelligible of religions, offering so long-sought solutions to many countless problems of life. Additionally, his career in chemistry begun to feel uncomfortable, as it was contributing to a materialist worldview. He joined the Roman Catholic Church, and, although not without doubts, has made up his mind.\n\nThe remaining of the book presents 12 essays, which are F. Sherwood Taylor's attempt at progressive, but not final, integration of the religious and scientific methodologies and ways of considering and understanding the world.\n\n\nThe book has been reviewed by Sister Francis Augustine Richey, who regards the author as \"a distinguished scientist, a convert moreover from the fringes of scientism to catholicism, a writer with a singularly gifted mind, sensitive, imaginative, intuitive and logical\". She states, that as a chemist and historian of science, Sherwood Taylor relies on experience, and writes from a position that emphasizes the appreciation of science. However, he does so in a thoughtful and critical manner, raising \"an inspiring call into the battle against materialism\". He engages in a careful analysis of scientific method and knowledge, and does so from an easily approachable objective perspective, which Sister Francis Augustine Richey calls \"impersonally personal\". She describes the book as valuable and illuminating for \"teacher and pupil whether of science, philosophy, or religion\".\n"}
{"id": "49067628", "url": "https://en.wikipedia.org/wiki?curid=49067628", "title": "Mécanique analytique", "text": "Mécanique analytique\n\nMécanique analytique (1788–89) is a two volume French treatise on analytical mechanics, written by Joseph-Louis Lagrange, and published 101 years following Isaac Newton's \"Philosophiæ Naturalis Principia Mathematica\". It consolidated into one unified and harmonious system, the scattered developments of contributors such as Alexis Clairaut, Jean le Rond d'Alembert, Leonhard Euler, and Johann and Jacob Bernoulli in the historical transition from geometrical methods, as presented in Newton's \"Principia\", to the methods of mathematical analysis. The treatise expounds a great labor-saving and thought-saving general analytical method by which every mechanical question may be stated in a single algebraic equation.\n\nLagrange wrote that this work was entirely new and that his intent was to reduce the theory and the art of solving mechanics problems to general formulae, providing all the equations necessary for the solution of each problem. He stated that...No diagrams will be found in this work. The methods that I explain require neither geometrical, nor mechanical, constructions or reasoning, but only algebraical operations in accordance with regular and uniform procedure. Those who love Analysis will see with pleasure that Mechanics has become a branch of it, and will be grateful to me for having thus extended its domain.\n\nErnst Mach describes the work as follows:\nAnalytic mechanics... was brought to the highest degree of perfection... Lagrange's aim is... to dispose, \"once and for all\", of the reasoning necessary to resolve mechanical problems, by embodying as much as possible of it in a single formula. This he did. Every case... can now be dealt with by a very simple... schema; and whatever reasoning is left is performed by purely mechanical methods. The mechanics of Lagrange is a stupendous contribution to the economy of thought.\n\n"}
{"id": "2129870", "url": "https://en.wikipedia.org/wiki?curid=2129870", "title": "Progressive contextualization", "text": "Progressive contextualization\n\nProgressive contextualization \"(PC)\" is a scientific method pioneered and developed by Andrew P. Vayda and research team between 1979 and 1984. The method was developed to help understand cause of damage and destruction of forest and land during the New Order Regime in Indonesia, as well as practical ethnography. Vayda proposed the Progressive contextualization method due to his dissatisfaction with several conventional anthropological methods to describe accurately and quickly cases of illegal logging, land destruction and the network of actor-investor protecting the actions, as well as various consequences detrimental to the environment and social life.\n\nThe essence of this method is to track and assess: \n\nIt rejects the assumption of ecological and socio-cultural homogeneity. Instead, it focuses on diversity and it looks at how different individuals and groups operate in and adapt to their total environments through a variety of behaviors, technologies, organizations, structures and beliefs.\n\nBased on such a premise and through the practical interpretation of facts, the approach will lead to 'concrete findings on who is doing what, why they are doing it, and with what effects.'\n"}
{"id": "81863", "url": "https://en.wikipedia.org/wiki?curid=81863", "title": "Proportionality (mathematics)", "text": "Proportionality (mathematics)\n\nIn mathematics, two variables are proportional if there is always a constant ratio between them. The constant is called the coefficient of proportionality or proportionality constant.\n\n\nThe statement \"\"y\" is directly proportional to \"x\"\" is written mathematically as \",\" or \",\" where \"c\" is the proportionality constant.\n\nThe statement \"\"y\" is inversely proportional to \"x\"\" is written mathematically as \".\" This is equivalent to \"\"y\" is directly proportional to .\"\n\nGiven two variables \"x\" and \"y\", \"y\" is directly proportional to \"x\" if there is a non-zero constant \"k\" such that\n\nThe relation is often denoted, using the ∝ or ~ symbol, as\n\nand the constant ratio\n\nis called the proportionality constant, constant of variation or constant of proportionality. This can also be viewed as a two-variable linear equation with a y-intercept of 0.\n\n\nSince\n\nis equivalent to\n\nit follows that if \"y\" is directly proportional to \"x\", with (nonzero) proportionality constant \"k\", then \"x\" is also directly proportional to \"y\", with proportionality constant .\n\nIf \"y\" is directly proportional to \"x\", then the graph of \"y\" as a function of \"x\" is a straight line passing through the origin with the slope of the line equal to the constant of proportionality: it corresponds to linear growth.\n\nThe concept of \"inverse proportionality\" can be contrasted with \"direct proportionality\". Consider two variables said to be \"inversely proportional\" to each other. If all other variables are held constant, the magnitude or absolute value of one inversely proportional variable decreases if the other variable increases, while their product (the constant of proportionality \"k\") is always the same. As an example, the time taken for a journey is inversely proportional to the speed of travel.\n\nFormally, two variables are inversely proportional (also called varying inversely, in inverse variation, in inverse proportion, in reciprocal proportion) if each of the variables is directly proportional to the multiplicative inverse (reciprocal) of the other, or equivalently if their product is a constant. It follows that the variable \"y\" is inversely proportional to the variable \"x\" if there exists a non-zero constant \"k\" such that\nor equivalently formula_7 Hence the constant is the product of \"x\" and \"y\".\n\nThe graph of two variables varying inversely on the Cartesian coordinate plane is a rectangular hyperbola. The product of the \"x\" and \"y\" values of each point on the curve equals the constant of proportionality (\"k\"). Since neither \"x\" nor \"y\" can equal zero (because \"k\" is non-zero), the graph never crosses either axis.\n\nThe concepts of \"direct\" and \"inverse\" proportion lead to the location of points in the Cartesian plane by hyperbolic coordinates; the two coordinates correspond to the constant of direct proportionality that specifies a point as being on a particular ray and the constant of inverse proportionality that specifies a point as being on a particular hyperbola.\n\n\n"}
{"id": "27943629", "url": "https://en.wikipedia.org/wiki?curid=27943629", "title": "Reagent bottle", "text": "Reagent bottle\n\nReagent bottles, also known as media bottles or graduated bottles, are containers made of glass, plastic, borosilicate or related substances, and topped by special caps or stoppers and are intended to contain chemicals in liquid or powder form for laboratories and stored in cabinets or on shelves. Some reagent bottles are tinted amber (actinic), brown or red to protect light-sensitive chemical compounds from visible light, ultraviolet and infrared radiation which may alter them; other bottles are tinted blue (cobalt glass) or uranium green for decorative purposes -mostly vintage apothecary sets, from centuries in which a doctor or apothecary was a prominent figure. The bottles are called \"graduated\" when they have marks on the sides indicating the approximate (often with a 10% error) amount of liquid at a given level within the container. A reagent bottle is a type of laboratory glassware. The term \"reagent\" refers to a substance that is part of a chemical reaction (or an ingredient of which), and \"media\" is the plural form of \"medium\" which refers to the liquid or gas which a reaction happens within, or is a processing chemical tool such as (for example) a flux.\n\nSeveral companies produce reagent bottles, including Wheaton, Kimble, Corning, Schott AG and trademark glass names include Pyrex, Kimax, Duran, Boro and Bomex.\n\nCommon bottle sizes include 100 ml, 250 ml, 500 ml, 1000 ml (1 liter) and 2000 ml (2 liter). Older bottles, especially for medical use and for expensive chemicals, can be found of capacities well under 100 ml. \n\nThe selection of caps and stoppers that reagent bottles are closed with are as important as the material the bottles are made of, and the decision as to which cap to use is dependent on the material stored in the container, and the amount of heat which the cap can be subject to. Common cap sizes include 33-430 (33mm), 38-430 (38mm), and GL 45 (45mm). Caps range in size from narrow mouthed to wide mouthed and often a glass or plastic funnel is needed to properly fill a reagent bottle from a larger or equal sized container's mouth. Reagent bottle caps are commonly said to be \"autoclavable\".\n\nAntique or vintage reagent bottles tend to resemble the classic apothecary bottle and have a glass stopper, very often not of standard size, so that very old bottles and samples should be stored with care, as replacing a missing glass stopper would require dedicated glassworking.\n\nReagent bottles are subject to OSHA regulations, and global scientific standards.\n\n\n"}
{"id": "3406245", "url": "https://en.wikipedia.org/wiki?curid=3406245", "title": "Routhian mechanics", "text": "Routhian mechanics\n\nIn analytical mechanics, a branch of theoretical physics, Routhian mechanics is a hybrid formulation of Lagrangian mechanics and Hamiltonian mechanics developed by Edward John Routh. Correspondingly, the Routhian is the function which replaces both the Lagrangian and Hamiltonian functions.\n\nThe Routhian, like the Hamiltonian, can be obtained from a Legendre transform of the Lagrangian, and has a similar mathematical form to the Hamiltonian, but is not exactly the same. The difference between the Lagrangian, Hamiltonian, and Routhian functions are their variables. For a given set of generalized coordinates representing the degrees of freedom in the system, the Lagrangian is a function of the coordinates and velocities, while the Hamiltonian is a function of the coordinates and momenta.\n\nThe Routhian differs from these functions in that some coordinates are chosen to have corresponding generalized velocities, the rest to have corresponding generalized momenta. This choice is arbitrary, and can be done to simplify the problem. It also has the consequence that the Routhian equations are exactly the Hamiltonian equations for some coordinates and corresponding momenta, and the Lagrangian equations for the rest of the coordinates and their velocities. In each case the Lagrangian and Hamiltonian functions are replaced by a single function, the Routhian. The full set thus has the advantages of both sets of equations, with the convenience of splitting one set of coordinates to the Hamilton equations, and the rest to the Lagrangian equations.\n\nOften the Routhian approach may offer no advantage, but one notable case where this is useful is when a system has cyclic coordinates (also called \"ignorable coordinates\"), by definition those coordinates which do not appear in the original Lagrangian. The Lagrangian equations are powerful results, used frequently in theory and practice, since the equations of motion in the coordinates are easy to set up. However, if cyclic coordinates occur there will still be equations to solve for all the coordinates, including the cyclic coordinates despite their absence in the Lagrangian. The Hamiltonian equations are useful theoretical results, but less useful in practice because coordinates and momenta are related together in the solutions - after solving the equations the coordinates and momenta must be eliminated from each other. Nevertheless, the Hamiltonian equations are perfectly suited to cyclic coordinates because the equations in the cyclic coordinates trivially vanish, leaving only the equations in the non cyclic coordinates.\n\nThe Routhian approach has the best of both approaches, because cyclic coordinates can be split off to the Hamiltonian equations and eliminated, leaving behind the non cyclic coordinates to be solved from the Lagrangian equations. Overall fewer equations need to be solved compared to the Lagrangian approach.\n\nAs with the rest of analytical mechanics, Routhian mechanics is completely equivalent to Newtonian mechanics, all other formulations of classical mechanics, and introduces no new physics. It offers an alternative way to solve mechanical problems.\n\nIn the case of Lagrangian mechanics, the generalized coordinates , ... and the corresponding velocities , and possibly time , enter the Lagrangian,\n\nwhere the overdots denote time derivatives.\n\nIn Hamiltonian mechanics, the generalized coordinates and the corresponding generalized momenta and possibly time, enter the Hamiltonian,\n\nwhere the second equation is the definition of the generalized momentum corresponding to the coordinate (partial derivatives are denoted using ). The velocities are expressed as functions of their corresponding momenta by inverting their defining relation. In this context, is said to be the momentum \"canonically conjugate\" to .\n\nThe Routhian is intermediate between and ; some coordinates are chosen to have corresponding generalized momenta , the rest of the coordinates to have generalized velocities , and time may appear explicitly;\n\nwhere again the generalized velocity is to be expressed as a function of generalized momentum via its defining relation. The choice of which coordinates are to have corresponding momenta, out of the coordinates, is arbitrary.\n\nThe above is used by Landau and Lifshitz, and Goldstein. Some authors may define the Routhian to be the negative of the above definition.\n\nGiven the length of the general definition, a more compact notation is to use boldface for tuples (or vectors) of the variables, thus , , , and , so that\n\nwhere · is the dot product defined on the tuples, for the specific example appearing here:\n\nFor reference, the Lagrangian equations for degrees of freedom are a set of coupled second order ordinary differential equations in the coordinates\n\nwhere , and the Hamiltonian equations for degrees of freedom are a set of coupled first order ordinary differential equations in the coordinates and momenta\n\nBelow, the Routhian equations of motion are obtained in two ways, in the process other useful derivatives are found that can be used elsewhere.\n\nConsider the case of a system with two degrees of freedom, and , with generalized velocities and , and the Lagrangian is time-dependent. (The generalization to any number of degrees of freedom follows exactly the same procedure as with two). The Lagrangian of the system will have the form\n\nThe differential of is\n\nNow change variables, from the set (, , , ) to (, , , ), simply switching the velocity to the momentum . This change of variables in the differentials is the Legendre transformation. The differential of the new function to replace will be a sum of differentials in , , , , and . Using the definition of generalized momentum and Lagrange's equation for the coordinate :\n\nwe have\n\nand to replace by , recall the product rule for differentials, and substitute\n\nto obtain the differential of a new function in terms of the new set of variables:\n\nIntroducing the Routhian\n\nwhere again the velocity is a function of the momentum , we have\n\nbut from the above definition, the differential of the Routhian is\n\nComparing the coefficients of the differentials , , , , and , the results are Hamilton's equations for the coordinate ,\n\nand Lagrange's equation for the coordinate \n\nwhich follow from\n\nand taking the total time derivative of the second equation and equating to the first. Notice the Routhian replaces the Hamiltonian and Lagrangian functions in all the equations of motion.\n\nThe remaining equation states the partial time derivatives of and are negatives\n\nFor coordinates as defined above, with Routhian\n\nthe equations of motion can be derived by a Legendre transformation of this Routhian as in the previous section, but another way is to simply take the partial derivatives of with respect to the coordinates and , momenta , and velocities , where , and . The derivatives are\n\nThe first two are identically the Hamiltonian equations. Equating the total time derivative of the fourth set of equations with the third (for each value of ) gives the Lagrangian equations. The fifth is just the same relation between time partial derivatives as before. To summarize\n\nThe total number of equations is , there are Hamiltonian equations plus Lagrange equations.\n\nSince the Lagrangian has the same units as energy, the units of the Routhian are also energy. In SI units this is the Joule.\n\nTaking the total time derivative of the Lagrangian leads to the general result\n\nIf the Lagrangian is independent of time, the partial time derivative of the Lagrangian is zero, , so the quantity under the total time derivative in brackets must be a constant, it is the total energy of the system\n\n(If there are external fields interacting with the constituents of the system, they can vary throughout space but not time). This expression requires the partial derivatives of with respect to \"all\" the velocities and . Under the same condition of being time independent, the energy in terms of the Routhian is a little simpler, substituting the definition of and the partial derivatives of with respect to the velocities ,\n\nNotice only the partial derivatives of with respect to the velocities are needed. In the case that and the Routhian is explicitly time-independent, then , that is, the Routhian equals the energy of the system. The same expression for in when is also the Hamiltonian, so in all .\n\nIf the Routhian has explicit time dependence, the total energy of the system is not constant. The general result is\n\nwhich can be derived from the total time derivative of in the same way as for .\n\nThe Routhian formulation is useful for systems with cyclic coordinates, because by definition those coordinates do not enter , and hence . The corresponding partial derivatives of and with respect to those coordinates are zero, which equates to the corresponding generalized momenta reducing to constants. To make this concrete, if the are all cyclic coordinates, and the are all non cyclic, then\n\nwhere the are constants. With these constants substituted into the Routhian, is a function of only the non cyclic coordinates and velocities (and in general time also)\n\nThe Hamiltonian equation in the cyclic coordinates automatically vanishes,\n\nand the Lagrangian equations are in the non cyclic coordinates\n\nThus the problem has been reduced to solving the Lagrangian equations in the non cyclic coordinates, with the advantage of the Hamiltonian equations cleanly removing the cyclic coordinates. Using those solutions, the equations for formula_34can be integrated to compute formula_35.\n\nIf we are interested in how the cyclic coordinates change with time, the equations for the generalized velocities corresponding to the cyclic coordinates can be integrated.\n\nThe Routhian method does not guarantee the equations of motion will be simple, however it will lead to fewer equations.\n\nOne general class of mechanical systems with cyclic coordinates are those with central potentials, because potentials of this form only have dependence on radial separations and no dependence on angles.\n\nConsider a particle of mass under the influence of a central potential in spherical polar coordinates \n\nNotice is cyclic, because it does not appear in the Lagrangian. The momentum conjugate to is the constant\n\nin which and can vary with time, but the angular momentum is constant. The Routhian can be taken to be\n\nWe can solve for and using Lagrange's equations, and do not need to solve for since it is eliminated by Hamiltonian's equations. The equation is\n\nand the equation is\n\nThe Routhian approach has obtained two coupled nonlinear equations. By contrast the Lagrangian approach leads to \"three\" nonlinear coupled equations, mixing in the first and second time derivatives of in all of them, despite its absence from the Lagrangian.\n\nThe equation is\n\nthe equation is\n\nthe equation is\n\nConsider the spherical pendulum, a mass (known as a \"pendulum bob\") attached to a rigid rod of length of negligible mass, subject to a local gravitational field . The system rotates with angular velocity which is \"not\" constant. The angle between the rod and vertical is and is \"not\" constant.\n\nThe Lagrangian is\n\nand is the cyclic coordinate for the system with constant momentum\n\nwhich again is physically the angular momentum of the system about the vertical. The angle and angular velocity vary with time, but the angular momentum is constant. The Routhian is\n\nThe equation is found from the Lagrangian equations\n\nor simplifying by introducing the constants\n\ngives\n\nThis equation resembles the simple nonlinear pendulum equation, because it can swing through the vertical axis, with an additional term to account for the rotation about the vertical axis (the constant is related to the angular momentum ).\n\nApplying the Lagrangian approach there are two nonlinear coupled equations to solve.\n\nThe equation is\n\nand the equation is\n\nThe heavy symmetrical top of mass has Lagrangian\n\nwhere are the Euler angles, is the angle between the vertical -axis and the top's -axis, is the rotation of the top about its own -axis, and the azimuthal of the top's -axis around the vertical -axis. The principal moments of inertia are about the top's own axis, about the top's own axes, and about the top's own -axis. Since the top is symmetric about its -axis, . Here the simple relation for local gravitational potential energy is used where is the acceleration due to gravity, and the centre of mass of the top is a distance from its tip along its -axis.\n\nThe angles are cyclic. The constant momenta are the angular momenta of the top about its axis and its precession about the vertical, respectively:\n\nFrom these, eliminating :\n\nwe have\n\nand to eliminate , substitute this result into and solve for to find\n\nThe Routhian can be taken to be\n\nand since\n\nwe have\n\nThe first term is constant, and can be ignored since only the derivatives of \"R\" will enter the equations of motion. The simplified Routhian, without loss of information, is thus\n\nThe equation of motion for is, by direct calculation,\n\nor by introducing the constants\n\na simpler form of the equation is obtained\n\nAlthough the equation is highly nonlinear, there is only one equation to solve for, it was obtained directly, and the cyclic coordinates are not involved.\n\nBy contrast, the Lagrangian approach leads to \"three\" nonlinear coupled equations to solve, despite the absence of the coordinates and in the Lagrangian.\n\nThe equation is\n\nthe equation is\n\nand the equation is\n\nConsider a classical charged particle of mass and electric charge in a static (time-independent) uniform (constant throughout space) magnetic field . The Lagrangian for a charged particle in a general electromagnetic field given by the magnetic potential and electric potential is\n\nIt is convenient to use cylindrical coordinates , so that\n\nIn this case the electric potential is zero, , and we can choose the axial gauge for the magnetic potential\n\nand the Lagrangian is\n\nNotice this potential has an effectively cylindrical symmetry (although it also has angular velocity dependence), since the only spatial dependence is on the radial length from an imaginary cylinder axis.\n\nThere are two cyclic coordinates, and . The canonical momenta conjugate to and are the constants\n\nso the velocities are\n\nThe angular momentum about the \"z\" axis is \"not\" , but the quantity , which is not conserved due to the contribution from the magnetic field. The canonical momentum is the conserved quantity. It is still the case that is the linear or translational momentum along the \"z\" axis, which is also conserved.\n\nThe radial component and angular velocity can vary with time, but is constant, and since is constant it follows is constant. The Routhian can take the form\n\nwhere in the last line, the term is a constant and can be ignored without loss of continuity. The Hamiltonian equations for and automatically vanish and do not need to be solved for. The Lagrangian equation in \n\nis by direct calculation\n\nwhich after collecting terms is\n\nand simplifying further by introducing the constants\n\nthe differential equation is\n\nTo see how changes with time, integrate the momenta expression for above\n\nwhere is an arbitrary constant, the initial value of to be specified in the initial conditions.\n\nThe motion of the particle in this system is helicoidal, with the axial motion uniform (constant) but the radial and angular components varying in a spiral according to the equation of motion derived above. The initial conditions on , , , , will determine if the trajectory of the particle has a constant or varying . If initially is nonzero but , while and are arbitrary, then the initial velocity of the particle has no radial component, is constant, so the motion will be in a perfect helix. If \"r\" is constant, the angular velocity is also constant according to the conserved .\n\nWith the Lagrangian approach, the equation for would include which has to be eliminated, and there would be equations for and to solve for.\n\nThe equation is\n\nthe equation is\n\nand the equation is\n\nThe equation is trivial to integrate, but the and equations are not, in any case the time derivatives are mixed in all the equations and must be eliminated.\n\n\n"}
{"id": "212147", "url": "https://en.wikipedia.org/wiki?curid=212147", "title": "Scintillation counter", "text": "Scintillation counter\n\nA scintillation counter is an instrument for detecting and measuring ionizing radiation by using the excitation effect of incident radiation on a scintillating material, and detecting the resultant light pulses. \n\nIt consists of a scintillator which generates photons in response to incident radiation, a sensitive photodetector (usually a photomultiplier tube (PMT), a charge-coupled device (CCD) camera, or a photodiode), which converts the light to an electrical signal and electronics to process this signal.\n\nScintillation counters are widely used in radiation protection, assay of radioactive materials and physics research because they can be made inexpensively yet with good quantum efficiency, and can measure both the intensity and the energy of incident radiation.\n\nThe modern electronic scintillation counter was invented in 1944 by Sir Samuel Curran whilst he was working on the Manhattan Project at the University of California at Berkeley. There was a requirement to measure the radiation from small quantities of uranium and his innovation was to use one of the newly-available highly sensitive photomultiplier tubes made by the Radio Corporation of America to accurately count the flashes of light from a scintillator subjected to radiation.\nThis built upon the work of earlier researchers such as Antoine Henri Becquerel, who discovered radioactivity whilst working on the phosphorescence of uranium salts in 1896. Previously scintillation events had to be laboriously detected by eye using a spinthariscope which was a simple microscope to observe light flashes in the scintillator.\n\nWhen an ionizing particle passes into the scintillator material, atoms are ionized along a track. For charged particles the track is the path of the particle itself. For gamma rays (uncharged), their energy is converted to an energetic electron via either the photoelectric effect, Compton scattering or pair production. The chemistry of atomic de-excitation in the scintillator produces a multitude of low-energy photons, typically near the blue end of the visible spectrum. The number of such photons is in proportion to the amount of energy deposited by the ionizing particle. Some portion of these low-energy photons arrive at the photocathode of an attached photomultiplier tube. The photocathode emits at most one electron for each arriving photon by the photoelectric effect. This group of primary electrons is electrostatically accelerated and focused by an electrical potential so that they strike the first dynode of the tube. The impact of a single electron on the dynode releases a number of secondary electrons which are in turn accelerated to strike the second dynode. Each subsequent dynode impact releases further electrons, and so there is a current amplifying effect at each dynode stage. Each stage is at a higher potential than the previous to provide the accelerating field. The resultant output signal at the anode is in the form of a measurable pulse for each group of photons that arrived at the photocathode, and is passed to the processing electronics. The pulse carries information about the energy of the original incident radiation on the scintillator. The number of such pulses per unit time gives information about the intensity of the radiation. In some applications individual pulses are not counted, but rather only the average current at the anode is used as a measure of radiation intensity.\n\nThe scintillator must be shielded from all ambient light so that external photons do not swamp the ionization events caused by incident radiation. To achieve this a thin opaque foil, such as aluminized mylar, is often used, though it must have a low enough mass to minimize undue attenuation of the incident radiation being measured.\n\nThe article on the photomultiplier tube carries a detailed description of the tube's operation.\n\nThe scintillator consists of a transparent crystal, usually a phosphor, plastic (usually containing anthracene) or organic liquid (see liquid scintillation counting) that fluoresces when struck by ionizing radiation. \n\nCesium iodide (CsI) in crystalline form is used as the scintillator for the detection of protons and alpha particles. Sodium iodide (NaI) containing a small amount of thallium is used as a scintillator for the detection of gamma waves and zinc sulfide (ZnS) is widely used as a detector of alpha particles. Zinc sulfide is the material Rutherford used to perform his scattering experiment. Lithium iodide (LiI) is used in neutron detectors.\n\nThe quantum efficiency of a gamma-ray detector (per unit volume) depends upon the density of electrons in the detector, and certain scintillating materials, such as sodium iodide and bismuth germanate, achieve high electron densities as a result of the high atomic numbers of some of the elements of which they are composed. However, detectors based on semiconductors, notably hyperpure germanium, have better intrinsic energy resolution than scintillators, and are preferred where feasible for gamma-ray spectrometry.\n\nIn the case of neutron detectors, high efficiency is gained through the use of scintillating materials rich in hydrogen that scatter neutrons efficiently. Liquid scintillation counters are an efficient and practical means of quantifying beta radiation.\n\nScintillation counters are used to measure radiation in a variety of applications including hand held radiation survey meters, personnel and environmental monitoring for radioactive contamination, medical imaging, radiometric assay, nuclear security and nuclear plant safety.\n\nSeveral products have been introduced in the market utilising scintillation counters for detection of potentially dangerous gamma-emitting materials during transport. These include scintillation counters designed for freight terminals, border security, ports, weigh bridge applications, scrap metal yards and contamination monitoring of nuclear waste. There are variants of scintillation counters mounted on pick-up trucks and helicopters for rapid response in case of a security situation due to dirty bombs or radioactive waste. Hand-held units are also commonly used.\n\nIn the United Kingdom, the Health and Safety Executive, or HSE, has issued a user guidance note on selecting the correct radiation measurement instrument for the application concerned. This covers all radiation instrument technologies, and is a useful comparative guide to the use of scintillation detectors.\n\nIndustrial radioactive contamination monitors, either hand-held for area or personal surveys or installed for personnel monitoring require a large detection area to ensure efficient and rapid coverage of monitored surfaces. For this the scintillation counter with a large area scintillator window and integrated photomultiplier tube is ideally suited and finds wide application in the field of radioactive contamination monitoring of personnel and the environment. Detectors are designed to have one or two scintillation materials, depending on the application. \"Single phosphor\" detectors are used for either alpha or beta, and \"Dual phosphor\" detectors are used to detect both. \n\nA scintillator such as zinc sulphide is used for alpha particle detection, whilst plastic scintillators are used for beta detection. The resultant scintillation energies can be discriminated so that alpha and beta counts can be measured separately with the same detector. This technique is used in both hand-held and fixed monitoring equipment, and such instruments are relatively inexpensive compared with the gas proportional detector.\n\nScintillation materials are used for ambient gamma dose measurement, though a different construction is used to detect contamination, as no thin window is required.\n\nScintillators often convert a single photon of high energy radiation into high number of lower-energy photons, where the number of photons per megaelectronvolt of input energy is fairly constant. By measuring the intensity of the flash (the number of the photons produced by the x-ray or gamma photon) it is therefore possible to discern the original photon's energy.\n\nThe spectrometer consists of a suitable scintillator crystal, a photomultiplier tube, and a circuit for measuring the height of the pulses produced by the photomultiplier. The pulses are counted and sorted by their height, producing a x-y plot of scintillator flash brightness vs number of the flashes, which approximates the energy spectrum of the incident radiation, with some additional artifacts. A monochromatic gamma radiation produces a photopeak at its energy. The detector also shows response at the lower energies, caused by Compton scattering, two smaller escape peaks at energies 0.511 and 1.022 MeV below the photopeak for the creation of electron-positron pairs when one or both annihilation photons escape, and a backscatter peak. Higher energies can be measured when two or more photons strike the detector almost simultaneously (pile-up, within the time resolution of the data acquisition chain), appearing as sum peaks with energies up to the value of two or more photopeaks added.\n\n"}
{"id": "427750", "url": "https://en.wikipedia.org/wiki?curid=427750", "title": "Solar symbol", "text": "Solar symbol\n\nA solar symbol is a symbol representing the Sun. \nCommon solar symbols include circles with or without rays, crosses or spirals.\nIn religious iconography, personifications of the Sun or solar attributes are indicated by means of a halo or a radiate crown.\n\nWhen the systematic study of comparative mythology first became popular in the 19th century, scholarly opinion tended to over-interpret historical myths and iconography in terms of \"solar symbolism\". \nThis was especially the case with Max Müller and his followers beginning in the 1860s in the context of Indo-European studies. Many \"solar symbols\" claimed in the 19th century, such as the swastika, triskele, Sun cross, etc. have tended to be interpreted more conservatively in scholarship since the later 20th century.\n\nThe basic element of most solar symbols is the circular solar disk.\nThe disk can be modified in various ways, notably by adding rays (found in the Bronze Age in Egyptian depictions of Aten) or a cross. In Ancient Near East, the solar disk could also be modified by addition of the Uraeus (rearing cobra), and in Ancient Mesopotamia it was shown as winged.\n\nEgyptian hieroglyphs have a large inventory of solar symbolism because of the central position of solar deities (Ra, Horus, Aten etc.) in Ancient Egyptian religion.\nThe \"Sun\" ideogram in early Chinese writing, beginning with the oracle bone script (c. 12th century BC) also shows the solar disk with a central dot (whence the modern character 日), analogous to the Egyptian heroglyph.\n\nThe modern astronomical symbol for the Sun (circled dot, Unicode U+2609 ☉; c.f. U+2299 ⊙ \"circled dot operator\") was first used in the Renaissance. A diagram in Johannes Kamateros' 12th century \"Compendium of Astrology\" shows the Sun represented by a circle with a ray. \"Bianchini's planisphere\", produced in the 2nd century,\nhas a circlet with rays radiating from it.\n\nA circular disk with alternating triangular and wavy rays emanating from it is a frequent symbol or artistic depiction of the sun.\n\nThe ancient Mesopotamian \"star of Shamash\" could be represented with either eight wavy rays, or with four wavy and four triangular rays.\n\nThe Vergina Sun (also known as the Star of Vergina, Macedonian Star, or Argead Star) is a rayed solar symbol appearing in ancient Greek art from the 6th to 2nd centuries BC. The Vergina Sun appears in art variously with sixteen, twelve, or eight triangular rays.\n\nThe iconographic tradition of depicting the Sun with rays and with a human face develops in Western tradition in the high medieval period and becomes widespread in the Renaissance, harking back to the Sun god (Sol/Helios) being depicted as wearing a radiate crown on his head in classical antiquity.\n\nThe Jesuit emblem, the flag of Uruguay, the flag of Kiribati, some versions of the flag of Argentina, the Irish Defence Forces cap badge, and the are official insignia which incorporate rayed solar symbols.\n\nThe depictions of the sun on the flag of the Republic of China (Taiwan), the flag of Kazakhstan, the flag of Kurdistan, and the flag of Nepal have only straight (triangular) rays, while that on the flag of Kyrgyzstan has only curvy rays. The flag of the Philippines has short diverging rays grouped into threes.\n\nAnother form of rayed depiction of the sun is with simple radial lines dividing the field into two colors, as in the military flags of Japan and the current Flag of the Republic of Macedonia, and in the top parts of the flag of Tibet and the flag of Arizona.\n\nThe flag of New Mexico is based on the Zia people's sun symbol which has four groups of four parallel rays emanating symmetrically from a central circle.\n\nThe modern pictogram representing the Sun as a circle with rays, often eight in number (indicated by either straight lines or triangles; Unicode Miscellaneous Symbols U+2600; U+263C) is used for weather forecasts, indicating \"clear weather\".\nUse of such pictograms originates in television weather forecasts in the 1970s.\nThe Unicode (version 6.0) Miscellaneous Symbols and Pictographs block introduced another set of weather pictograms, with a character \"WHITE SUN\" depicted without rays in the official chart at 1F323 . The same block has also a \"Sun with face\" character, at U+1F31E .\n\nThe \"sun with rays\" pictogram is also used to represent the \"high brightness\" setting in display devices, encoded separately by Unicode (version 6.0) at U+1F506 (Miscellaneous Symbols and Pictographs block).\n\nThe \"sun cross\" or \"solar wheel\" (🜨) is often considered to represent the four seasons and the tropical year, and therefore the Sun.\nIn the prehistoric religion of Bronze Age Europe, crosses in circles appear frequently on artifacts identified as cult items, for example the \"miniature standard\" with an amber inlay that shows a cross shape when held against the light, dating to the Nordic Bronze Age, held at the National Museum of Denmark, Copenhagen. The Bronze Age symbol has also been connected with the spoked chariot wheel, which at the time was four-spoked (compare the Linear B ideogram 243 \"wheel\" ). In the context of a culture that celebrated the Sun chariot, it may thus have had a \"solar\" connotation (c.f. the Trundholm sun chariot).\n\nThe \"Arevakhach\" (\"solar cross\") symbol often found in Armenian memorial stelae is claimed as an ancient Armenian solar symbol.\nis one of the ancient Armenian symbols of eternity and light.\n\nSome Sami shaman drums have the Beaivi Sami sun symbol that resembles a sun cross.\n\nThe swastika can be derived from the sun cross, and is another solar symbol in some contexts.\nIt is used among Buddhists (\"manji\"), Jains, and Hindus; and many other cultures, though not necessarily as a solar symbol. Also see Malkh-Festival.\n\nSome forms of the triple spiral or triskelion signs have also been claimed as solar symbols.\n\nThe \"Black Sun\" (German \"Schwarze Sonne\") is a symbol of esoteric and occult significance based on a sun wheel mosaic with twelve-fold rotational symmetry incorporated into a floor of Wewelsburg Castle during the Nazi era, which was itself loosely based on swastika-like designs in Migration-period \"Zierscheiben\". The Kolovrat, or in Polish \"Kołowrót\", represents the Sun in Slavic neopaganism. It is familiar to almost every ancient Slavic culture.\n\n\n"}
{"id": "4713880", "url": "https://en.wikipedia.org/wiki?curid=4713880", "title": "Solar telescope", "text": "Solar telescope\n\nA solar telescope is a special purpose telescope used to observe the Sun. Solar telescopes usually detect light with wavelengths in, or not far outside, the visible spectrum. Obsolete names for sun telescopes include heliograph and photoheliograph.\n\nSolar telescopes need optics large enough to achieve the best possible diffraction limit but less so for the associated light-collecting power of other astronomical telescopes. However, recently newer narrower filters and higher framerates have also driven solar telescopes towards photon-starved operations. Both the European Solar Telescope (EST) as well as the Advanced Technology Solar Telescope (ATST) have larger apertures not only to increase the resolution, but also to increase the light-collecting power.\n\nBecause solar telescopes operate during the day, seeing is generally worse than for night-time telescopes, because the ground around the telescope is heated which causes turbulence and degrades the resolution. To alleviate this, solar telescopes are usually built on towers and the structures are painted white. The Dutch Open Telescope is built on an open framework to allow the wind to pass through the complete structure and provide cooling around the telescope's main mirror.\n\nAnother solar telescope-specific problem is the heat generated by the tightly-focused sunlight. For this reason, a heat stop is an integral part of the design of solar telescopes. For the upcoming ATST, the heat load is 2.5 MW/m, with peak powers of 11.4 kW. The goal of such a heat stop is not only to survive this heat load, but also to remain cool enough not to induce any additional turbulence inside the telescope's dome.\n\nProfessional solar observatories may have main optical elements with very long focal lengths (although not always, Dutch Open Telescope) and light paths operating in a vacuum or helium to eliminate air motion due to convection inside the telescope. However, this is not possible for apertures over 1 meter, at which the pressure difference at the entrance window of the vacuum tube becomes too large. Therefore, the EST and ATST have active cooling of the dome to minimize the temperature difference between the air inside and outside the telescope.\n\nBecause the Sun travels on a narrow fixed path across the sky, some solar telescopes are fixed in position (and are sometimes buried underground), with the only moving part being a heliostat to track the Sun. One example of this is the McMath-Pierce Solar Telescope.\n\n\nMost solar observatories observe optically at visible, UV, and near infrared wavelengths, but other solar phenomena can be observed — albeit not from the Earth's surface due to the absorption of the atmosphere:\n\nIn the field of amateur astronomy there are many methods used to observe the sun. Amateurs use everything from simple systems to project the sun on a piece of white paper, light blocking filters, Herschel wedges which redirect 95% of the light away from the eyepiece, up to hydrogen-alpha filter systems and even home built spectrohelioscopes. In contrast to professional telescopes, amateur solar telescopes are usually much smaller.\n\n\n"}
{"id": "43817287", "url": "https://en.wikipedia.org/wiki?curid=43817287", "title": "Soyuz MS", "text": "Soyuz MS\n\nThe Soyuz-MS (, GRAU: 11F732A48) is the latest revision of the Soyuz spacecraft. It is an evolution of the Soyuz TMA-M spacecraft, with modernization mostly concentrated on the communications and navigation subsystems.\n\nIt is used by the Russian Federal Space Agency for human spaceflight. Soyuz-MS has minimal external changes with respect to the Soyuz TMA-M, mostly limited to antennas and sensors, as well as the thruster placement.\n\nThe first launch was Soyuz MS-01 on July 7, 2016 aboard a Soyuz-FG launch vehicle towards the ISS. The trip included a two-day checkout phase for the design before docking with the ISS on July 9.\n\nA Soyuz spacecraft consists of three parts (from front to back):\n\n\nThe first two portions are habitable living space. By moving as much as possible into the orbital module, which does not have to be shielded or decelerated during atmospheric re-entry, the Soyuz three-part craft is both larger and lighter than the two-part Apollo spacecraft's command module. The Apollo command module had six cubic meters of living space and a mass of 5000 kg; the three-part Soyuz provided the same crew with nine cubic meters of living space, an airlock, and a service module for the mass of the Apollo capsule alone. This does not consider orbital module that could be used instead of LEM in Apollo.\n\nSoyuz can carry up to three cosmonauts and provide life support for them for about 30 person-days. The life support system provides a nitrogen/oxygen atmosphere at sea level partial pressures. The atmosphere is regenerated through KO cylinders, which absorb most of the CO and water produced by the crew and regenerates the oxygen, and LiOH cylinders which absorb leftover CO.\n\nThe vehicle is protected during launch by a nose fairing, which is jettisoned after passing through the atmosphere. It has an automatic docking system. The ship can be operated automatically, or by a pilot independently of ground control.\n\nThe forepart of the spacecraft is the orbital module (: бытовой отсек (БО), \"Bitovoy otsek (BO)\") also known as Habitation section. It houses all the equipment that will not be needed for reentry, such as experiments, cameras or cargo. Commonly, it is used as both eating area and lavatory. At its far end, it also contains the docking port. This module also contains a toilet, docking avionics and communications gear. On the latest Soyuz versions, a small window was introduced, providing the crew with a forward view.\n\nA hatch between it and the descent module can be closed so as to isolate it to act as an airlock if needed, cosmonauts exiting through its side port (at the bottom of this picture, near the descent module); on the launch pad, they have entered the spacecraft through this port.\n\nThis separation also lets the orbital module be customized to the mission with less risk to the life-critical descent module. The convention of orientation in zero gravity differs from that of the descent module, as cosmonauts stand or sit with their heads to the docking port.\n\nThe reentry module (: спускаемый аппарат (СА), \"Spuskaemiy apparat (SA)\") is used for launch and the journey back to Earth. It is covered by a heat-resistant covering to protect it during re-entry. It is slowed initially by the atmosphere, then by a braking parachute, followed by the main parachute which slows the craft for landing. At one meter above the ground, solid-fuel braking engines mounted behind the heat shield are fired to give a soft landing. One of the design requirements for the reentry module was for it to have the highest possible volumetric efficiency (internal volume divided by hull area). The best shape for this is a sphere, but such a shape can provide no lift, which results in a purely ballistic reentry. Ballistic reentries are hard on the occupants due to high deceleration and can't be steered beyond their initial deorbit burn. That is why it was decided to go with the \"headlight\" shape that the Soyuz uses — a hemispherical forward area joined by a barely angled conical section (seven degrees) to a classic spherical section heat shield. This shape allows a small amount of lift to be generated due to the unequal weight distribution. The nickname was coined at a time when nearly every automobile headlight was a circular paraboloid.\n\nAt the back of the vehicle is the service module (: приборно-агрегатный отсек (ПАО), \"Priborno-Agregatniy Otsek (PAO)\"). It has an instrumentation compartment (: приборный отсек (ПО), \"Priborniy Otsek (PO)\"), a pressurized container shaped like a bulging can that contains systems for temperature control, electric power supply, long-range radio communications, radio telemetry, and instruments for orientation and control. The propulsion compartment (: агрегатный отсек (АО), \"Agregatniy Otsek (AO)\"), a non-pressurized part of the service module, contains the main engine and a spare: liquid-fuel propulsion systems for maneuvering in orbit and initiating the descent back to Earth. The ship also has a system of low-thrust engines for orientation, attached to the intermediate compartment (: переходной отсек (ПхО), \"Perekhodnoi Otsek (PkhO)\"). Outside the service module are the sensors for the orientation system and the solar array, which is oriented towards the sun by rotating the ship.\n\nBecause its modular construction differs from that of previous designs, the Soyuz has an unusual sequence of events prior to re-entry. The spacecraft is turned engine-forward and the main engine is fired for de-orbiting fully 180° ahead of its planned landing site. This requires the least propellant for re-entry, the spacecraft traveling on an elliptical Hohmann orbit to a point where it will be low enough in the atmosphere to re-enter.\n\nEarly Soyuz spacecraft would then have the service and orbital modules detach simultaneously. As they are connected by tubing and electrical cables to the descent module, this would aid in their separation and avoid having the descent module alter its orientation. Later Soyuz spacecraft detach the orbital module before firing the main engine, which saves even more propellant, enabling the descent module to return more payload. In no case can the orbital module remain in orbit as an addition to a space station, for the hatch enabling it to function as an airlock is part of the descent module.\n\nRe-entry firing is typically done on the \"dawn\" side of the Earth, so that the spacecraft can be seen by recovery helicopters as it descends in the evening twilight, illuminated by the sun when it is above the shadow of the Earth. Since the beginning of Soyuz missions to the ISS, only five have performed nighttime landings.\n\nThe Soyuz MS received the following upgrades with respect to the Soyuz TMA-M:\n\n\n"}
{"id": "48840652", "url": "https://en.wikipedia.org/wiki?curid=48840652", "title": "Streaming instability", "text": "Streaming instability\n\nIn planetary science a streaming instability is a hypothetical mechanism for the formation of planetesimals in which the drag felt by solid particles orbiting in a gas disk leads to their spontaneous concentration into clumps which can gravitationally collapse. Small initial clumps increase the orbital velocity of the gas, slowing radial drift locally, leading to their growth as they are joined by faster drifting isolated particles. Massive filaments form that reach densities sufficient for the gravitational collapse into planetesimals the size of large asteroids, bypassing a number of barriers to the traditional formation mechanisms. The formation of streaming instabilities requires solids that are moderately coupled to the gas and a local solid to gas ratio of one or greater. The growth of solids large enough to become moderately coupled to the gas is more likely outside the ice line and in regions with limited turbulence. An initial concentration of solids with respect to the gas is necessary to suppress turbulence sufficiently to allow the solid to gas ratio to reach greater than one at the mid-plane. A wide variety of mechanisms to selectively remove gas or to concentrate solids have been proposed. In the inner Solar System the formation of streaming instabilities requires a greater initial concentration of solids or the growth of solid beyond the size of chondrules.\n\nPlanetesimals and larger bodies are traditionally thought to have formed via a hierarchical accretion, the formation of large objects via the collision and mergers of small objects. This process begins with the collision of dust due to Brownian motion producing larger aggregates held together by van der Waals forces. The aggregates settle toward the mid-plane of the disk and collide due to gas turbulence forming pebbles and larger objects. Further collisions and mergers eventually yield planetesimals 1–10 km in diameter held together by self-gravity. The growth of the largest planetesimals then accelerates, as gravitational focusing increases their effective cross-section, resulting in runaway accretion forming the larger asteroids. Later, gravitational scattering by the larger objects excites relative motions, causing a transition to slower oligarchic accretion that ends with the formation of planetary embryos. In the outer Solar System the planetary embryos grow large enough to accrete gas, forming the giant planets. In the inner Solar System the orbits of the planetary embryos become unstable, leading to giant impacts and the formation of the terrestrial planets.\n\nA number of obstacles to this process have been identified: barriers to growth via collisions, the radial drift of larger solids, and the turbulent stirring of planetesimals. As a particle grows the time required for its motion to react to changes in the motion of the gas in turbulent eddies increases. The relative motions of particles, and collision velocities, therefore increases as with the mass of the particles. For silicates the increased collision velocities cause dust aggregates to compact into solid particles that bounce rather than stick, ending growth at the size of chondrules, roughly 1 mm in diameter. Icy solids may not be affected by the bouncing barrier but their growth can be halted at larger sizes due to fragmentation as collision velocities increase. Radial drift is the result of the pressure support of the gas, enabling it to orbit at a slower velocity than the solids. Solids orbiting through this gas lose angular momentum and spiral toward the central star at rates that increase as they grow. At 1 AU this produces a meter-sized barrier, with the rapid loss of large objects in as little as ~1000 orbits, ending with their vaporization as they approach too close to the star. At greater distances the growth of icy bodies can become drift limited at smaller sizes when their drift timescales become shorter than their growth timescales. Turbulence in the protoplanetary disk can create density fluctuations which exert torques on planetesimals exciting their relative velocities. Outside the dead zone the higher random velocities can result in the destruction of smaller planetesimals, and the delay of the onset of runaway growth until planetesimals reach radii of 100 km.\n\nSome evidence exists that planetesimal formation may have bypassed these barriers to incremental growth. In the inner asteroid belt all of the low albedo asteroids that have not been identified as part of a collisional family are larger than 35 km. A change in the slope of the size distribution of asteroids at roughly 100 km can be reproduced in models if the minimal diameter of the planetesimals was 100 km and the smaller asteroids are debris from collisions. A similar change in slope has been observed in the size distribution of the Kuiper belt objects. The low numbers of small craters on Pluto has also been cited as evidence the largest KBO's formed directly. Furthermore, if the cold classical KBO's formed in situ from a low mass disk, as suggested by the presence of loosely bound binaries, they are unlikely to have formed via the traditional mechanism. The dust activity of comets indicates a low tensile strength that would be the result of a gentle formation process with collisions at free-fall velocities.\n\nStreaming instabilities, first described by Andrew Youdin and Jeremy Goodman, are driven by differences in the motions of the gas and solid particles in the protoplanetary disk. The gas is hotter and denser closer to the star, creating a pressure gradient that partially offsets gravity from the star. The partial support of the pressure gradient allows the gas to orbit at roughly 50 m/s below the Keplerian velocity at its distance. The solid particles, however, are not supported by the pressure gradient and would orbit at Keplerian velocities in the absence of the gas. The difference in velocities results in a headwind that causes the solid particles to spiral toward the central star as they lose momentum to aerodynamic drag. The drag also produces a back reaction on the gas, increasing its velocity. When solid particles cluster in the gas, the reaction reduces the headwind locally, allowing the cluster to orbit faster and undergo less inward drift. The slower drifting clusters are overtaken and joined by isolated particles, increasing the local density and further reducing radial drift, fueling an exponential growth of the initial clusters. In simulations the clusters form massive filaments that can grow or dissipate, and that can collide and merge or split into multiple filaments. The separation of filaments averages 0.2 gas scale heights, roughly 0.02 AU at the distance of the asteroid belt. The densities of the filaments can exceed a thousand times the gas density, sufficient to trigger the gravitational collapse and fragmentation of the filaments into bound clusters.\n\nThe clusters shrink as energy is dissipated by gas drag and inelastic collisions, leading to the formation of planetesimals the size of large asteroids. Impact speeds are limited during the collapse of the smaller clusters that form 1–10 km asteroids, reducing the fragmentation of particles, leading to the formation of porous pebble pile planetesimals with low densities. Gas drag slows the fall of the smallest particles and less frequent collisions slows the fall of the largest particles during this process, resulting in the size sorting of particles with mid-sized particles forming a porous core and a mix of particle sizes forming denser outer layers. The impact speeds and the fragmentation of particles increase with the mass of the clusters, lowering the porosity and increasing the density of the larger objects such as 100 km asteroid that form from a mixture of pebbles and pebble fragments. Collapsing swarms with excess angular momentum can fragment, forming binary or in some cases trinary objects resembling those in the Kuiper belt. In simulations the initial mass distribution of the planetesimals formed via streaming instabilities fits a power law: dn/dM ~ M, that is slightly steeper than that of small asteroids, with an exponential cutoff at larger masses. Continued accretion of chondrules from the disk may shift the size distribution of the largest objects toward that of the current asteroid belt. In the outer Solar System the largest objects can continue to grow via pebble accretion, possibly forming the cores of giant planets.\n\nStreaming instabilities form only in the presence of rotation and the radial drift of solids. The initial linear phase of a streaming instability, begins with a transient region of high pressure within the protoplanetary disk. The elevated pressure alters the local pressure gradient supporting the gas, reducing the gradient on the region's inner edge and increasing the gradient on the region's outer edge. The gas therefore must orbit faster near the inner edge and is able to orbit slower near the outer edge. The Coriolis forces resulting from these relative motions support the elevated pressure, creating a geostropic balance. The motions of the solids near the high pressure regions are also affected: solids at its outer edge face a greater headwind and undergo faster radial drift, solids at its inner edge face a lesser headwind and undergo a slower radial drift. This differential radial drift produces a buildup of solids in higher pressure regions. The drag felt by the solids moving toward the region also creates a back reaction on the gas that reinforces the elevated pressure leading to a runaway process. As more solids are carried toward the region by radial drift this eventually yields a concentration of solids sufficient to drive the increase of the velocity of the gas and reduce the local radial drift of solids seen in streaming instabilities.\n\nStreaming instabilities form when the solid particles are moderately coupled to the gas, with Stokes numbers of 0.01 - 3; the local solid to gas ratio is near or larger than 1; and the vertically integrated solid to gas ratio is a few times Solar. The Stokes number is a measure of the relative influences of inertia and gas drag on a particle's motion. In this context it is the product of the timescale for the exponential decay of a particle's velocity due to drag and the angular frequency of its orbit. Small particles like dust are strongly coupled and move with the gas, large bodies such as planetesimals are weakly coupled and orbit largely unaffected by the gas. Moderately coupled solids, sometimes referred to as pebbles, range from roughly cm- to m-sized at asteroid belt distances and from mm- to dm-sized beyond 10 AU. These objects orbit through the gas like planetesimals but are slowed due to the headwind and undergo significant radial drift. The moderately coupled solids that participate in streaming instabilities are those dynamically affected by changes in the motions of gas on scales similar to those of the Coriolis effect, allowing them to be captured by regions of high pressure in a rotating disk. Moderately coupled solids also retain influence on the motion of the gas. If the local solid to gas ratio is near or above 1, this influence is strong enough to reinforce regions of high pressure and to increase the orbital velocity of the gas and slow radial drift. Reaching and maintaining this local solid to gas at the mid-plane requires an average solid to gas ratio in a vertical cross section of the disk that is a few times solar. When the average solid to gas ratio is 0.01, roughly that estimated from measurements of the current Solar System, turbulence at the mid-plane generates a wavelike pattern that puffs up the mid-plane layer of solids. This reduces the solid to gas ratio at the mid-plane to less than 1, suppressing the formation of dense clumps. At higher average solid to gas ratios the mass of solids dampens this turbulence allowing a thin mid-plane layer to form. Stars with higher metallicities are more likely to reach the minimum solid to gas ratio making them favorable locations for planetesimal and planet formation.\n\nA high average solid to gas ratio may be reached due to the loss of gas or by the concentration of solids. Gas may be selectively lost due to photoevaporation late in the gas disk epoch, causing solids to be concentrated in a ring at the edge of a cavity that forms in the gas disk, though the mass of planetesimals that forms may be too small to produce planets. The solid to gas ratio can also increase in the outer disk due to photoevaporation, but in the giant planet region the resulting planetesimal formation may be too late to produce giant planets. If the magnetic field of the disc is aligned with its angular momentum the Hall effect increases viscosity which can result in a faster depletion of the inner gas disk. A pile up of solids in the inner disk can occur due to slower rates of radial drift as Stoke's numbers decline with increasing gas densities. This radial pile up is reinforced as the velocity of the gas increases with the surface density of solids and could result in the formation of bands of planetesimals extending from sublimation lines to a sharp outer edges where solid to gas ratios first reach critical values. For some ranges of particle size and gas viscosity outward flow of the gas may occur, reducing its density and further increasing the solid to gas ratio. The radial pile ups may be limited due to a reduction in the gas density as the disk evolves however, and shorter growth timescales of solids closer to the star could instead result in the loss of solids from the inside out. Radial pile-ups also occur at locations where rapidly drifting large solids fragment into smaller slower drifting solids, for example, inside the ice line where silicate grains are released as icy bodies sublimate. This pile up can also increase the local velocity of the gas, extending the pile up to outside the ice line where it is enhanced by the outward diffusion and recondensation of water vapor. The pile-up could be muted, however, if the icy bodies are highly porous, which slows their radial drift. Icy solids can be concentrated outside the ice line due to the outward diffusion and recondensation of water vapor. Solids are also concentrated in radial pressure bumps, where the pressure reaches a local maximum. At these locations radial drift converges from both closer and farther from the star. Radial pressure bumps are present at the inner edge of the dead zone, and can form due to the magnetorotational instability. Pressure bumps may also be produced due to the back-reaction of dust on the gas creating self-induced dust traps. The ice line has also been proposed as the site of a pressure bump, however, this requires a steep viscosity transition. If the back-reaction from the concentration of solids flattens the pressure gradient, the planetesimals formed at a pressure bump may be smaller than predicted at other locations. If the pressure gradient is maintained streaming instabilities may form at the location of a pressure bump even in viscous disks with significant turbulence. Local pressure bumps also form in the spiral arms of a massive self-gravitating disk and in anti-cyclonic vortices. The break-up of vortices could also leave a ring of solids from which a streaming instability may form. Solids may also be concentrated locally if disk winds lower the surface density of the inner disc, slowing or reversing their inward drift, or due to thermal diffusion.\n\nStreaming instabilities are more likely to form in regions of the disk where: the growth of solids is favored, the pressure gradient is small, and turbulence is low. Inside the ice-line the bouncing barrier may prevent the growth of silicates large enough to take part in streaming instabilities. Beyond the ice line hydrogen bonding allows particles of water ice to stick at higher collision velocities, possibly enabling the growth of large highly porous icy bodies to Stokes numbers approaching 1 before their growth is slowed by erosion. The condensation of vapor diffusing outward from sublimating icy bodies may also drive the growth of compact dm-size icy bodies outside the ice line. A similar growth of bodies due to recondensation of water could occur over a broader region following an FU Orionis event. At greater distances the growth of solids could again be limited if they are coated with a layer of CO or other ices that reduce the collision velocities where sticking occurs. A small pressure gradient reduces the rate of radial drift, limiting the turbulence generated by streaming instabilities. A smaller average solid to gas ratio is then necessary to suppress turbulence at the mid-plane. The diminished turbulence also enables the growth of larger solids by lowering impact velocities. Hydrodynamic models indicate that the smallest pressure gradients occur near the ice-line and in the inner parts of the disk. The pressure gradient also decreases late in the disk's evolution as the accretion rate and the temperature decline. A major source of turbulence in the protoplanetary disk is the magnetorotational instability. The impacts of turbulence generated by this instability could limit streaming instabilities to the dead zone, estimated to form near the mid-plane at 1-20 AU, where the ionization rate is too low to sustain the magnetorotational instability.\n\nIn the inner Solar System the formation of streaming instabilities requires a larger enhancement of the solid to gas ratio than beyond the ice line. The growth of silicate particles is limited by the bouncing barrier to ~1 mm, roughly the size of the chondrules found in meteorites. In the inner Solar System particles this small have Stokes numbers of ~0.001. At these Stokes numbers a vertically integrated solid to gas ratio greater than 0.04, roughly four times that of the overall gas disk, is required to form streaming instabilities. The required concentration may be reduced by half if the particles are able to grow to roughly cm-size. This growth, possibly aided by dusty rims that absorb impacts, may occur over a period of 10^5 years if a fraction of collisions result in sticking due to a broad distribution of collision velocities. Or, if turbulence and the collision velocities are reduced inside initial weak clumps, a runaway process may occur in which clumping aids the growth of solids and their growth strengthens clumping. A radial pile-up of solids may also lead to conditions that support streaming instabilities in a narrow annulus at roughly 1 AU. This would requires a shallow initial disk profile and that the growth of solids be limited by fragmentation instead of bouncing allowing cm-sized solids to form, however. The growth of particles may be further limited at high temperatures, possibly leading to an inner boundary of planetesimal formation where temperatures reaches 1000K.\n\nInstead of actively driving their own concentration, as in streaming instabilities, solids may be passively concentrated to sufficient densities for planetesimals to form via gravitational instabilities. In an early proposal dust settled at the mid-plane until sufficient densities were reached for the disk to gravitationally fragment and collapse into planetesimals. The difference in orbital velocities of the dust and gas, however, produces turbulence which inhibits settling preventing sufficient densities from being reached. If the average dust to gas ratio is increased by an order of magnitude at a pressure bump or by the slower drift of small particles derived from fragmenting larger bodies, this turbulence may be suppressed allowing the formation of planetesimals.\n\nThe cold classical Kuiper belt objects may have formed in a low mass disk dominated by cm-sized or smaller objects. In this model the gas disk epoch ends with km-sized objects, possibly formed via gravitational instability, embedded in a disk of small objects. The disk remains dynamically cool due to inelastic collisions among the cm-sized objects. The slow encounter velocities result in efficient growth with a sizable fraction of the mass ending in the large objects. The dynamical friction from the small bodies would also aid in the formation of binaries.\n\nPlanetesimals may also be formed from the concentration of chondrules between eddies in a turbulent disk. In this model the particles are split unequally when large eddies fragment increasing the concentrations of some clumps. As this process cascades to smaller eddies a fraction of these clumps may reach densities sufficient to be gravitationally bound and slowly collapse into planetesimals. Recent research, however, indicates that larger objects such as conglomerates of chondrules may be necessary and that the concentrations produced from chondrules may instead act as the seeds of streaming instabilities.\n\nIcy particles are more likely to stick and to resist compression in collisions which may allow the growth of large porous bodies. If the growth of these bodies is fractal, with their porosity increasing as larger porous bodies collide, their radial drift timescales become long, allowing them to grow until they are compressed by gas drag and self-gravity forming small planetesimimals. Alternatively, if the local solid density of the disk is sufficient, they may settle into a thin disk that fragments due to a gravitational instability, forming planetesimals the size of large asteroids, once they grow large enough to become decoupled from the gas. A similar fractal growth of porous silicates may also be possible if they are made up of nanometer-sized grains formed from the evaporation and recondensation of dust. However, the fractal growth of highly porous solids may be limited by the infilling of their cores with small particles generated in collisions due to turbulence; by erosion as the impact velocity due to the relative rates of radial drift of large and small bodies increases; and by sintering as they approach ice lines, reducing their ability to absorb collisions, resulting in bouncing or fragmentation during collisions.\n\nCollisions at velocities that would result in the fragmentation of equal sized particles can instead result in growth via mass transfer from the small to the larger particle. This process requires an initial population of 'lucky' particles that have grown larger than the majority of particles. These particles may form if collision velocities have a wide distribution, with a small fraction occurring at velocities that allow objects beyond the bouncing barrier to stick. However, the growth via mass transfer is slow relative to radial drift timescales, although it may occur locally if radial drift is halted locally at a pressure bump allowing the formation of planetesimals in 10^5 yrs.\n\nPlanetesimal accretion could reproduce the size distribution of the asteroids if it began with 100 meter planetesimals. In this model collisional dampening and gas drag dynamically cool the disk and the bend in the size distribution is caused by a transition between growth regimes. This however require a low level of turbulence in the gas and some mechanism for the formation of 100 meter planetesimals. Size dependent clearing of planetesimals due to secular resonance sweeping could also remove small bodies creating a break in the size distribution of asteroids. Secular resonances sweeping inward through the asteroid belt as the gas disk dissipated would excite the eccentricities of the planetesimals. As their eccentricities were damped due to gas drag and tidal interaction with the disk the largest and smallest objects would be lost as their semi-major axes shrank leaving behind the intermediate sized planetesimals.\n\n"}
{"id": "7663689", "url": "https://en.wikipedia.org/wiki?curid=7663689", "title": "TANPAKU", "text": "TANPAKU\n\nTANPAKU was a distributed computing project aimed at researching the protein structure prediction problem. The project used the Berkeley Open Infrastructure for Network Computing (BOINC) platform, and was developed in collaboration with the Yamato Lab (Department of Biological Science and Technology) and Takeda Lab (Department of Information Sciences) groups at the Tokyo University of Science.\n\nThe project was suspended on August 8, 2008 due to server problems. Currently, researchers are releasing its results, and the project is not expected to become active again.\n\n\n"}
{"id": "3267736", "url": "https://en.wikipedia.org/wiki?curid=3267736", "title": "Transporter Classification Database", "text": "Transporter Classification Database\n\nThe Transporter Classification Database (or TCDB) is an International Union of Biochemistry and Molecular Biology (IUBMB)-approved classification system for membrane transport proteins, including ion channels.\n\nThe upper level of classification and a few examples of proteins with known 3D structure:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThey include a number of transmembrane cytochrome b-like proteins including coenzyme Q - cytochrome c reductase (cytochrome bc1 ); cytochrome b6f complex; formate dehydrogenase, respiratory nitrate reductase; succinate - coenzyme Q reductase (fumarate reductase); and succinate dehydrogenase. See electron transport chain.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "12039054", "url": "https://en.wikipedia.org/wiki?curid=12039054", "title": "WHO Model List of Essential Medicines", "text": "WHO Model List of Essential Medicines\n\nThe WHO Model List of Essential Medicines (EML), published by the World Health Organization (WHO), contains the medications considered to be most effective and safe to meet the most important needs in a health system. The list is frequently used by countries to help develop their own local lists of essential medicine. As of 2016, more than 155 countries have created national lists of essential medicines based on the World Health Organization's model list. This includes countries in both the developed and developing world.\nThe list is divided into core items and complementary items. The core items are deemed to be the most cost effective options for key health problems and are usable with little additional health care resources. The complementary items either require additional infrastructure such as specially trained health care providers or diagnostic equipment or have a lower cost-benefit ratio. About 25% of items are in the complementary list. Some medications are listed as both core and complementary. While most medications on the list are available as generic products, being under patent does not preclude inclusion. \n\nThe first list was published in 1977 and included 212 medications. The WHO updates the list every two years. The 14th list was published in 2005 and contained 306 medications. In 2015 the 19th edition of the list was published and contains around 410 medications. The 20th edition was published in 2017 and comprises 433 drugs. The national lists contain between 334 and 580 medications.\nA separate list for children up to 12 years of age, known as the WHO Model List of Essential Medicines for Children (EMLc), was created in 2007 and is in its 6th edition. It was created to make sure that the needs of children were systematically considered such as availability of proper formulations. Everything in the children's list is also included in the main list. The list and notes are based on the 19th and 20th edition of the main list. An α indicates a medicine is only on the complementary list.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNucleoside/Nucleotide reverse transcriptase inhibitors\nNucleotide polymerase inhibitors\n\nProtease inhibitors\n\nNS5A inhibitors\n\nNon-nucleoside polymerase inhibitors\n\nOther antivirals\n\nFixed-dose combinations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn α indicates the medicine is only on the complementary list. For these items specialized diagnostic or monitoring or specialist training are needed. An item may also be listed as complementary on the basis of higher costs or a less attractive cost-benefit ratio.\n"}
{"id": "28306535", "url": "https://en.wikipedia.org/wiki?curid=28306535", "title": "William and Katherine Estes Award", "text": "William and Katherine Estes Award\n\nThe William and Katherine Estes Award, previously known as the NAS Award for Behavioral Research Relevant to the Prevention of Nuclear War is awarded by the U.S. National Academy of Sciences \"to recognize basic research in any field of cognitive or behavioral science that has employed rigorous formal or empirical methods, optimally a combination of these, to advance our understanding of problems or issues relating to the risk of nuclear war\". It was first awarded in 1990. \n\nSource: NAS\n\nScott D. Sagan (2015) - \nFor his pioneering theoretical and empirical work addressing the risks of nuclear possession and deployment and the causes of nuclear proliferation.\n\nRobert Powell (2012) - \nFor sophisticated game theoretic models of conflict that illuminate the heart of the strategic dilemmas of nuclear deterrence, including the importance of private information.\n\nGraham Allison (2009) -\nFor illuminating alternative ways of thinking about political decision making with special relevance to crises, including nuclear crises, as demonstrated in his groundbreaking \"Essence of Decision\" and subsequent works.\n\nRobert Jervis (2006) - \nFor showing, scientifically and in policy terms, how cognitive psychology, politically contextualized, can illuminate strategies for the avoidance of nuclear war.\n\nWalter Enders and Todd Sandler (2003) - \nFor their joint work on transnational terrorism using game theory and time series analysis to document the cyclic and shifting nature of terrorist attacks in response to defensive counteractions. \n\nPhilip E. Tetlock (2000) - \nFor successfully developing a semantic measure of cognitive complexity predictive of foreign policy decisions and for applying psychological analysis and knowledge to nuclear policy problems.\n\nAlexander L. George (1997) - \nFor combining theory with history to elucidate the requirements of deterrence, the limits to coercive diplomacy, and the relationship between force and statecraft.\n\nThomas C. Schelling (1993) - \nFor his pioneering work on the logic of military strategy, nuclear war, and arms races, which has profoundly influenced our understanding of this crucial subject.\n\nRobert Axelrod (1990) - \nFor his imaginative use of game theory, experimentation, and computer simulation to define and test strategies for confrontation and cooperation and other models of social interaction.\n\n"}
