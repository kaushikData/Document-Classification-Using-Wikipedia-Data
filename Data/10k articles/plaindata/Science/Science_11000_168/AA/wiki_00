{"id": "472881", "url": "https://en.wikipedia.org/wiki?curid=472881", "title": "3C 279", "text": "3C 279\n\n3C 279 (also known as 4C–05.55, NRAO 413, and PKS 1253–05) is an optically violent variable quasar (OVV), which is known in the astronomical community for its variations in the visible, radio, and x-ray bands. The quasar was observed to have undergone a period of extreme activity from 1987 until 1991. The Rosemary Hill Observatory (RHO) started observing 3C 279 in 1971, the object was further observed by the Compton Gamma Ray Observatory in 1991, when it was unexpectedly discovered to be one of the brightest gamma ray objects in the sky. It is also one of the brightest and most variable sources in the gamma ray sky monitored by the Fermi Space Telescope.\n\nApparent superluminal motion was detected during observations first made in 1973 in a jet of material departing from the quasar, though it should be understood that this effect is an optical illusion caused by naive estimations of the speed, and no truly superluminal motion is occurring.\n"}
{"id": "53666751", "url": "https://en.wikipedia.org/wiki?curid=53666751", "title": "Abigail Harrison", "text": "Abigail Harrison\n\nAbigail \"Abby\" Harrison (born June 11, 1997) popularly known as Astronaut Abby, is an American advocate for STEAM education, particularly in the area of the United States space program. She is a scientist, internet personality, public speaker, writer, science communicator, and a student at Wellesley College, Massachusetts majoring in Astrobiology and Russian Area Studies. [Note: Astrobiology is a self-defined major at Wellesley College]. Harrison is best known as an aspiring astronaut with a goal to be the first person to set foot on Mars.\n\nHarrison is the founder and leader of The Mars Generation, a 501(c)(3) nonprofit with a mission to excite young people and adults about human space exploration and STEAM (Science, Technology, Engineering, Art, Math) education and foster an understanding of the importance of these two elements to the future of humankind on Earth. As of August 2017, Harrison and The Mars Generation have amassed over one million followers on their combined social media channels.\n\nAbigail Harrison was born June 11, 1997, in Saint Paul, Minnesota. Harrison grew up with one sister and her mother. For as long as Harrison can remember, she has dreamed of becoming the first person to set foot on Mars. Harrison attended Saint Paul and Minneapolis Public Schools for K-12 education. Growing up, Harrison valued education and her enthusiasm to learn led her to expand her education with science, technology, engineering, and mathematics (STEM) programs and learning opportunities before and after school as well as attending multiple STEM summer camps.\n\nHarrison attended Field Middle School where she was a member of her school's top-ranked Science Bowl team, created an award-winning State History Day Project titled Debate and Diplomacy: The History of the ISS and participated in the school's Girls In Engineering Math and Science Program (GEMS).\n\nHarrison graduated in 2015 from South High School with high honors. During her junior and senior year in high school Harrison attended the University of Minnesota as part of a highly competitive dual enrollment post secondary education program operated by the State of Minnesota. During high school Harrison spent much of her time taking advanced classes in math, engineering, and science. Harrison also participated in the state's Model UN program and Youth in Government Programs, served as a captain of her gymnastics team, and was a varsity member of the dive team and track and field team.\n\nHarrison is currently attending Wellesley College. She is pursuing astrobiology (self-made major) and Russian Area Studies degrees. She intends to pursue a graduate education and eventually receive her doctorate degree in astrobiology. She plans to become a PhD scientist and astronaut.\n\nIn 2015, Harrison founded the 501(c)(3) nonprofit called The Mars Generation and currently serves as the President of the Executive Board. The mission of The Mars Generation is to educate and inspire young people and adults about science, technology, engineering, and mathematics (STEM) and human space exploration.  The Mars Generation provides several programs including a Student Space Ambassador Program, Outreach Program, and offers a full paid Space Camp Scholarship Program. The Mars Generation is volunteer-driven with executive and advisory boards of leaders from the space industry including astronauts, engineers scientists as well as nonprofit, education, and business sectors.\n\nThe Mars Generation has been invited to partner with multiple space organizations and attend events around the world to share their work and excite people about human space exploration and STEM education as well as to inspire the next generation to dream big. Partnerships and events include the Project Space Forum in the United Arab Emirates hosted by the Mohammed bin Rashid Space Centre in Dubai , the Humans to Mars Summit hosted by Explore Mars in Washington D.C., Team America Rocketry Challenge (TARC) in Washington D.C., National Air and Space Museum of the Smithsonian Institution in Washington D.C., NASA's Digital Learning Network Online., South by Southwest (SXSW) Generation Z (GenZ) panel, National Geographic and National Geographic Kids, and the Kennedy Space Center Visitor Complex, the NASA visitor center at Kennedy Space Center in Florida.\n\nDuring summer of 2016, Harrison was accepted by Wellesley College to take part in a scientific research expedition to Siberia's Lake Baikal. Harrison was part of a 12 student cohort working with Wellesley Professor Marianne Moore, in collaboration with a team of 15 scientists from the United States and Russia, in a long term study exploring the effect of climate change on Lake Baikal. During the research project, Harrison used grazer exclusion pens to study the effect the vast grazer populations in Lake Baikal have on the growth of algae. On October 31, 2016, Harrison and her research partners, presented a paper on the topic of \"Stream Delivery of Nutrients into Lake Systems and Resulting in Eutrophication\" at the Wellesley College Tanner Conference.\n\nIn June 2017, Harrison began working as an intern in the Mars Lab of Dr. Andrew Schuerger of the University of Florida located in the Space Life Sciences Labs at Kennedy Space Center in Florida. Her research focuses on the search for life on Mars, through characterizing the limits of growth by terrestrial microorganisms under Martian conditions. Dr. Schruerger and his team have successfully grown 31 bacteria (but no fungi or archaea) under Martian conditions. Harrison's work will help contribute to several ongoing projects pertaining to the growth of S. liquefaciens under simulated Martian conditions.\n\nIn 2011, at the age of 13, Harrison began to speak publicly about her dream of being the first astronaut to walk on Mars. Her work over the years as an international STEAM (science, technology, engineering, arts and mathematics) and space ambassador has led to a following of over 1,000,000 people on social media.\n\nHarrison's online presence as Astronaut Abby was spurred from an 8th grade National History Day project titled 'Debate and Diplomacy: The History of the ISS\". Harrison setup a Twitter account under the name of AstronautAbby to connect with NASA employees to get a quote for her project. Harrison got the quote and also found support of the space community as a leader in her generation. Harrison continued to use social media to both network with space industry professionals, help engage her peers on STEM subjects and excite and educate people about the importance of human space exploration and STEM education to the future of humankind.\n\nOn May 28, 2013, Harrison attended the Soyuz TMA-09M launch of her mentor, Astronaut Luca Parmitano to the International Space Station. She shared the experience with her social media audience, marking the beginning of her work as Parmitano's Earth Liaison.As his Earth Liaison, she shared his 6-month experience of living in space with her social media community and through her worldwide outreach program.\n\n\n\n"}
{"id": "593365", "url": "https://en.wikipedia.org/wiki?curid=593365", "title": "Absorptive capacity", "text": "Absorptive capacity\n\nIn business administration, absorptive capacity has been defined as \"a firm's ability to recognize the value of new information, assimilate it, and apply it to commercial ends\". It is studied on individual, group, firm, and national levels. Antecedents are prior-based knowledge (knowledge stocks and knowledge flows) and communication. Studies involve a firm's innovation performance, aspiration level, and organizational learning. It has been said that in order to be innovative an organization should develop its absorptive capacity.\n\nThe concept of absorptive capacity was first defined as a firm's \"ability to recognize the value of new information, assimilate it, and apply it to commercial ends\" by Cohen and Levinthal. For them, absorptive capacity depends greatly on prior related knowledge and diversity of background. Therefore, they’ve put the investments a firm makes in its research and development efforts (\"R&D\") central to their model of development of absorptive capacity. The absorptive capacity is seen as cumulative, meaning that it is easier for a firm to invest on a constant basis in its absorptive capacity than investing punctually. Efforts put to develop absorptive capacity in one period will make it easier to accumulate it in the next one.\n\n“The cumulativeness of absorptive capacity and its effect on expectation formation suggest an extreme case of path dependence in which once a firm ceases investing in its absorptive capacity in a quickly moving field, it may never assimilate and exploit new information in that field, regardless of the value of that information.”\n\nAbsorptive capacity is also said to be a reason for companies to invest in R&D instead of simply purchasing the results \"post factum\" (e.g. patents). Internal R&D teams increase the absorptive capacity of a company. A firm’s investment in R&D then impacts directly its absorptive capacity. The more a firm invests in research and development activities, the more it will be able to fully appreciate the value of new external information.\n\nCohen and Levinthal also stressed that diversity allows individual to make “novel associations and linkages”. They therefore encourage the hiring of diverse teams in order to have a variety of individuals working together and exposing themselves to other ways of looking at things.\n\nCohen and Levinthal have focused a lot on investments in R&D to develop one’s absorptive capacity, but many other researchers showed later on that several other areas could be explored to develop an organization’s absorptive capacity. This led to a review of the concept by Shaker Zahra and Gerry George and a reformulation of the definition that expanded greatly the concept and further defined it as being made of two different absorptive capacities: potential absorptive capacity and realized absorptive capacity. Their new definition of absorptive capacity is: “a set of organizational routines and processes by which firms acquire, assimilate, transform and exploit knowledge to produce a dynamic organizational capability.”\n\nZahra and George presented the potential absorptive capacity is made of two elements. First there is knowledge acquisition which “refers to a firm’s capability to identify and acquire externally generated knowledge that is critical to its operations.” Second, there is assimilation capability which “refers to the firm’s routines and processes that allow it to analyze, process, interpret and understand the information obtained from external sources.” “Potential absorptive capacity makes the firm receptive to acquiring and assimilating external knowledge.”\n\nRealized absorptive capacity is made up of transformation capability on one hand that can be defined as “a firm’s capability to develop and refine the routines that facilitate combining existing knowledge and the newly acquired and assimilated knowledge.” On the other hand, realized absorptive capacity is also made of the exploitation capability of a firm which is basically the capacity of a firm to apply the newly acquired knowledge in product or services that it can get financial benefit from. “Realized absorptive capacity is a function of the transformation and exploitation capabilities.”\n\nZahra and George go on to suggest a series of indicators that can be used to evaluate each element of absorptive capacity.\n\nGeorge and his colleagues (Zou, Ertug, George, 2018) conduct a meta-analysis of absorptive capacity and they find that: (1)Absorptive capacity is a strong predictor of innovation and knowledge transfer, and its effects on financial performance are fully mediated by innovation and knowledge transfer; (2) The firm size-absorptive capacity relationship is positive for small firms but negative for larger firms. The firm age-absorptive capacity relationship is negative for mature firms and not significant for young firms; (3) Social integration mechanisms, knowledge infrastructure, management support, and relational capability all have a positive and significant impact on the absorptive capacity-innovation relationship (whereas they do not find the breadth of external search or competitive intensity to impact that relationship). Environmental dynamism has a marginally significant negative impact on the absorptive capacity-innovation relationship; and (4) They also find that the absorptive capacity-innovation relationship is stronger when absorptive capacity is measured by surveys rather than when absorptive capacity is measured by archival proxies.\n\nA more recent contribution proposed to (a) reintroduce the original first component in Cohen and Levinthal's model. The contribution noted (b) that transformation is not a step after assimilation, but represents an alternative process. Consequently, it suggested that (c) the neat distinction between potential absorptive capacity and realized absorptive capacity does not hold any more.\n\nad (a): Firms often fail to identify and absorb new external knowledge; recognizing the value of new external knowledge is often biased and needs to be fostered; it is not automatic. Managers have often problems in assessing the value of new external knowledge when it is not relevant for the current demands of key customers.\n\nad (b): Both assimilation and transformation involve some degree of change of the new knowledge and its combination with the existing knowledge. When the new knowledge fits existing cognitive schemas well, it is assimilated. When the new knowledge cannot be assimilated, the cognitive structures must be transformed. Firms transform their knowledge structures when knowledge cannot be assimilated. Transformation does not follow assimilation, it is an alternative to it.\n\nad (c): As transformation is an alternative to assimilation and not sequential to assimilation, it becomes part of potential absorptive capacity in Zahra and George's model; consequently, realized absorptive capacity simply relabels the component of exploitation. Further, without the effect of realized capacity, potential capacity cannot have any effect on a firm's competitive advantage; potential absorptive capacity cannot be meaningful separated from realized absorptive capacity in empirical studies on value creation. \n\nAccordingly, firms with high levels of absorptive capacity (1) recognize the value of new external knowledge, (2) acquire, (3) assimilate \"or\" transform, and (4) exploit new external knowledge.\n\nResearches on subjects related to the development of absorptive capacity have included studies focusing on research and development, knowledge management, organizational structures, human resources, external interactions, internal interactions, social capital, supplier integration, client integration and inter-organizational fit. All those researches provide a better picture of absorptive capacity that makes it possible for any firm to develop its absorptive capacity improving different areas of their organization.\n\nToday the theory involves organizational learning, industrial economics, the resource-based view of the firm and dynamic capabilities. This theory has undergone major refinement, and today a firm's absorptive capacity is mostly conceptualized as a dynamic capability.\n\nTwo concepts related to absorptive capacity are:\n\n\nIn regions in the Global South, regional innovation agencies can work as knowledge gatekeepers to stimulate new technological trajectories by facilitating the acquisition, diffusion and engaging in the ‘tropicalization’ of extra-regional knowledge to facilitate its absorption within the regional innovation system.\n\n"}
{"id": "24407772", "url": "https://en.wikipedia.org/wiki?curid=24407772", "title": "Actuel Marx", "text": "Actuel Marx\n\nActuel Marx is a book series of Marxist studies, edited by Jacques Bidet, Gérard Duménil, and Emmanuel Renault. The series is published by the Presses universitaires de France with support from the Paris West University Nanterre La Défense and the Centre national de la recherche scientifique.\n\n\"Actuel Marx\" organises the Congrès Marx international at the Paris West University every three years.\n\n"}
{"id": "49720714", "url": "https://en.wikipedia.org/wiki?curid=49720714", "title": "Aerospace Logistics Technology Engineering Company", "text": "Aerospace Logistics Technology Engineering Company\n\nThe Aerospace Logistics Technology Engineering Company (ALTEC) is an Italian aerospace company owned by the Italian Space Agency and Thales Alenia Space. It was founded in 2001 by Alenia Spazio and Consorzio Icarus, and is based in Turin. It will serve as the Control Centre for the two ExoMars missions to Mars.\n\n"}
{"id": "45466834", "url": "https://en.wikipedia.org/wiki?curid=45466834", "title": "Alexandre Constant", "text": "Alexandre Constant\n\nAlexandre Constant also known as Alfred Constant (14 September 1829, Autun – 13 May 1901, Golfe-Juan) was a French entomologist who specialised in Lepidoptera.\n\nConstant was a banker. He became a Member of the Société entomologique de France in 1854. He was principally interested in Microlepidoptera.\n\nPartial list\n\n"}
{"id": "402688", "url": "https://en.wikipedia.org/wiki?curid=402688", "title": "Algorithmic probability", "text": "Algorithmic probability\n\nIn algorithmic information theory, algorithmic probability, also known as Solomonoff probability, is a mathematical method of assigning a prior probability to a given observation. It was invented by Ray Solomonoff in the 1960s. \nIt is used in inductive inference theory and analyses of algorithms. In his general theory of inductive inference, Solomonoff uses the prior obtained by this formula, in Bayes' rule for prediction .\n\nIn the mathematical formalism used, the observations have the form of finite binary strings, and the universal prior is a probability distribution over the set of finite binary strings . The prior is universal in the\nTuring-computability sense, i.e. no string has zero probability. It is not computable, but it can be approximated. \n\nAlgorithmic probability deals with the following questions: Given a body of data about some phenomenon that we want to understand, how can we select the most probable hypothesis of how it was caused from among all possible hypotheses and how can we evaluate the different hypotheses? How can we predict future data and how can we measure the likelihood of that prediction being the right one?\n\nFour principal inspirations for Solomonoff's algorithmic probability were: Occam's razor, Epicurus' principle of multiple explanations, modern computing theory (e.g. use of a universal Turing machine) and Bayes’ rule for prediction.\n\nOccam's razor and Epicurus' principle are essentially two different non-mathematical approximations of the universal prior.\n\n\n\nAt the heart of the universal prior is an abstract model of a computer, such as a universal Turing machine. Any abstract computer will do, as long as it is Turing-complete, i.e. every finite binary string has at least one program that will compute it on the abstract computer.\n\nThe abstract computer is used to give precise meaning to the phrase \"simple explanation\" . In the formalism used, explanations, or theories of phenomena, are computer programs that generate observation strings when run on the abstract computer. A simple explanation is a short computer program. A complex explanation is a long computer program. Simple explanations are more likely, so a high-probability observation string is one generated\nby a short computer program, or perhaps by any of a large number of slightly longer computer programs. A low-probability observation string is one that can only be generated by a long computer program .\n\nThese ideas can be made specific and the probabilities used to construct a prior probability\ndistribution for the given observation. Solomonoff's main reason for inventing this prior is so that it can be used in Bayes' rule when the actual prior is unknown, enabling prediction under uncertainty. It predicts the most likely continuation of that observation, and provides a measure of how likely this continuation will be.\n\nAlthough the universal probability of an observation (and its extension) is incomputable, there is a computer algorithm, Levin Search, which, when run for longer and longer periods of time, will generate a sequence of approximations which converge to the universal probability distribution.\n\nSolomonoff proved this distribution to be \nmachine-invariant within a constant factor (called the invariance theorem).\n\nSolomonoff invented the concept of algorithmic probability with its associated invariance theorem around 1960, publishing a report on it: \"A Preliminary Report on a General Theory of Inductive Inference.\" He clarified these ideas more fully in 1964 with \"A Formal Theory of Inductive Inference,\" Part I and Part II.\n\nSolomonoff described a universal computer with a randomly generated input program. The program computes some possibly infinite output. The universal probability distribution is the probability distribution on all possible output strings with random input.\n\nThe algorithmic probability of any given finite output prefix \"q\" is the sum of the probabilities of the programs that compute something starting with \"q\". Certain long objects with short programs have high probability.\n\nAlgorithmic probability is the main ingredient of Solomonoff's theory of inductive inference, the theory of prediction based on observations; it was invented with the goal of using it for machine learning; given a sequence of symbols, which one will come next? Solomonoff's theory provides an answer that is optimal in a certain sense, although it is incomputable. Unlike, for example, Karl Popper's informal inductive inference theory, Solomonoff's is mathematically rigorous.\n\nAlgorithmic probability is closely related to the concept of Kolmogorov complexity. Kolmogorov's introduction of complexity was motivated by information theory and problems in randomness, while Solomonoff introduced algorithmic complexity for a different reason: inductive reasoning. A single universal prior probability that can be substituted for each actual prior probability in Bayes’s rule was invented by Solomonoff with Kolmogorov complexity as a side product.\n\nSolomonoff's enumerable measure is universal in a certain powerful sense, but the computation time can be infinite. One way of dealing with this issue is a variant of Leonid Levin's Search Algorithm, which limits the time spent computing the success of possible programs, with shorter programs given more time. Other methods of limiting the search space include training sequences.\n\n\n\n\n\n"}
{"id": "16709409", "url": "https://en.wikipedia.org/wiki?curid=16709409", "title": "Andrius Baltuška", "text": "Andrius Baltuška\n\nAndrius Baltuška (born 26 November 1971 in Leningrad) is a Lithuanian physicist. \n\nBaltuška studied physics at the University of Vilnius later at the University of Amsterdam and received his Ph.D. from the University of Groningen in 2000. After postdoctoral positions at the University of Tokyo, Technical University of Vienna and Max Planck Institute for Quantum Optics he became Professor at the Technical University of Vienna in 2006.\n\n"}
{"id": "10918960", "url": "https://en.wikipedia.org/wiki?curid=10918960", "title": "Applied academics", "text": "Applied academics\n\nApplied academics is an approach to learning and teaching that focuses on how academic subjects (communications, mathematics, science, and basic literacy) can apply to the real world. Further, applied academics can be viewed as theoretical knowledge supporting practical applications.\n\nApplied Academics is an approach to learning which focuses on motivating and challenging students to connect what they learn with the world they experience and with what interests them. The basic premise is that if academic content is made more relevant, participatory and concrete, students learn better, retain more and apply learning in their lives. Teaching in this model uses hands-on innovative teaching methods sometimes called contextual learning. Teachers help students understand the reasons for studying their subject matter and capitalize on students' natural learning inclinations and problem-solving approaches they can use well beyond the classroom throughout their lives.\n\nApplied Academics is an attempt to break from disconnected learning (where students go to different classes for different subjects for specified periods of time and don't gain a sense of the interconnectedness of learning) that has become a part of traditional approaches to education. This approach attempts to reintegrate learning by doing such things (for example) as teaching math, science, writing, or speech within other contexts such as a learning experience dealing with some form of technology training.\n\nResearch in cognitive science has shown that learners are not passive receptacles into which knowledge may be \"poured\". Learning occurs when the learner constructs, invents, and solves problems. Studies have shown that students taught theoretical principles, processes and skills in isolation without practice do not transfer these skills and knowledge as well to real life situations. Although \"learning to know,\" \"learning to do,\" and their \"application\" are often separated, there is no effective learning or understanding of one kind without the other two.\n\nIn the United States the Secretary's Commission on Achieving Necessary Skills (SCANS) states that \"good jobs require people who can put knowledge to work\", and listed the knowledge (academics) that it considered critical for all students to possess.\n\nThese competencies include:\n\nThe concept of action research is related to Applied Academics, as one tenet of action research is to discover theory through practice, and then implement using the foundation of theory.\n\n"}
{"id": "25702676", "url": "https://en.wikipedia.org/wiki?curid=25702676", "title": "Arp 187", "text": "Arp 187\n\nArp 187 is a radio galaxy and merger remnant located in the constellation Eridanus. It is an interacting galaxy pair (MCG-02-13-040). It is included in the Atlas of Peculiar Galaxies in the category galaxies with narrow filaments.\n\nThe galaxy has two prominent radio lobes, however the emission of its AGN in X-ray is low, and so is the thermal output of the AGN torus as observed in infrared, suggesting that it has been quenched. The detection of a narrow line region (with dimensions up to 10 kpc) with still active emission mean that the AGN was still active up to at least 10 years ago. In the nucleus of Arp 187 is predicted to lie a supermassive black hole with estimated mass around 6.7 × 10 .\n\n"}
{"id": "44438723", "url": "https://en.wikipedia.org/wiki?curid=44438723", "title": "Association of Law Teachers", "text": "Association of Law Teachers\n\nThe Association of Law Teachers, is a learned society in the United Kingdom dedicated to advancing legal education. It is a member of the Academy of Social Sciences. The association runs an academic journal: the \"\".\n\n"}
{"id": "5932221", "url": "https://en.wikipedia.org/wiki?curid=5932221", "title": "Atilio Lombardo", "text": "Atilio Lombardo\n\nAtilio Lombardo Nolle (1902 - 21 June 1984) was a Uruguayan professor, botanist and agrostologist.\n"}
{"id": "19468046", "url": "https://en.wikipedia.org/wiki?curid=19468046", "title": "Autoimmune disease", "text": "Autoimmune disease\n\nAn autoimmune disease is a condition arising from an abnormal immune response to a normal body part. There are at least 80 types of autoimmune diseases. Nearly any body part can be involved. Common symptoms include low grade fever and feeling tired. Often symptoms come and go.\nThe cause is generally unknown. Some autoimmune diseases such as lupus run in families, and certain cases may be triggered by infections or other environmental factors. Some common diseases that are generally considered autoimmune include celiac disease, diabetes mellitus type 1, Graves' disease, inflammatory bowel disease, multiple sclerosis, psoriasis, rheumatoid arthritis, and systemic lupus erythematosus. The diagnosis can be difficult to determine.\nTreatment depends on the type and severity of the condition. Nonsteroidal anti-inflammatory drugs (NSAIDs) and immunosuppressants are often used. Intravenous immunoglobulin may also occasionally be used. While treatment usually improves symptoms, they do not typically cure the disease.\nAbout 24 million (7%) people in the United States are affected by an autoimmune disease. Women are more commonly affected than men. Often they start during adulthood. The first autoimmune diseases were described in the early 1900s.\n\nFor a disease to be regarded as an autoimmune disease it needs to answer to \"Witebsky's postulates\" (first formulated by Ernest Witebsky and colleagues in 1957 and modified in 1994):\n\nAutoimmune diseases have a wide variety of different effects. They do tend to have one of three characteristic pathological effects which characterize them as autoimmune diseases:\nIt has been estimated that autoimmune diseases are among the leading causes of death among women in the United States in all age groups up to 65 years.\n\nA substantial minority of the population suffers from these diseases, which are often chronic, debilitating, and life-threatening.\n\nThere are more than 80 illnesses caused by autoimmunity.\n\nThe human immune system typically produces both T cells and B cells that are capable of being reactive with self-antigens, but these self-reactive cells are usually either killed prior to becoming active within the immune system, placed into a state of anergy (silently removed from their role within the immune system due to over-activation), or removed from their role within the immune system by regulatory cells. When any one of these mechanisms fail, it is possible to have a reservoir of self-reactive cells that become functional within the immune system. The mechanisms of preventing self-reactive T cells from being created takes place through negative selection process within the thymus as the T cell is developing into a mature immune cell.\n\nSome infections, such as \"Campylobacter jejuni\", have antigens that are similar (but not identical) to our own self-molecules. In this case, a normal immune response to \"C. jejuni\" can result in the production of antibodies that also react to a lesser degree with receptors on skeletal muscle (i.e., myasthenia gravis). A major understanding of the underlying pathophysiology of autoimmune diseases has been the application of genome wide association scans that have identified a degree of genetic sharing among the autoimmune diseases.\n\nAutoimmunity, on the other hand, is the presence of self-reactive immune response (e.g., auto-antibodies, self-reactive T cells), with or without damage or pathology resulting from it. This may be restricted to certain organs (e.g. in autoimmune thyroiditis) or involve a particular tissue in different places (e.g. Goodpasture's disease which may affect the basement membrane in both the lung and the kidney).\n\nThere are many theories as to how an autoimmune disease state arises. Some common ones are listed below.\n\nAlthough it is possible for a potential autoantigen to be spatially sequestered in an immune privileged site within the body (e.g. the eye), mechanisms exist to express even these antigens in a tolerogenic fashion to the immune system. However, it is impossible to induce tolerance (immune unresponsiveness) to all aspects of an autoantigen. This is because under normal physiologic conditions some regions of a self-antigen are not expressed at a sufficient level to induce tolerance. These poorly displayed areas of an antigen are called \"cryptic determinants.\" The immune system maintains a high-affinity repertoire to the cryptic self because the presentation of these determinants was insufficient to induce strong tolerance.\n\nThe concept of molecular mimicry describes a situation in which a foreign antigen can initiate an immune response in which a T or B cell component cross-recognizes self. The cross reactive immune response is responsible for the autoimmune disease state. Cross-reactive immune responses to self were first described for antibodies.\n\nAccording to this theory the effector function of the immune response is mediated by the glycans (polysaccharides) displayed by the cells and humoral components of the immune system. Individuals with autoimmunity have alterations in their glycosylation profile such that a proinflammatory immune response is favored. It is further hypothesized that individual autoimmune diseases will have unique glycan signatures.\n\nAccording to the hygiene hypothesis, high levels of cleanliness expose children to fewer antigens than in the past, causing their immune systems to become overactive and more likely to misidentify own tissues as foreign, resulting in autoimmune conditions such as asthma.\n\nThe first estimate of US prevalence for autoimmune diseases as a group was published in 1997 by Jacobson, et al. They reported US prevalence to be around 9 million, applying prevalence estimates for 24 diseases to a US population of 279 million. Jacobson's work was updated by Hayter & Cook in 2012. This study used Witebsky's postulates, as revised by Rose & Bona, to extend the list to 81 diseases and estimated overall cumulative US prevalence for the 81 autoimmune diseases at 5.0%, with 3.0% for males and 7.1% for females. The estimated community\nprevalence, which takes into account the observation that many people have more than one autoimmune disease, was 4.5% overall, with 2.7% for males and 6.4% for females.\n\nIn both autoimmune and inflammatory diseases, the condition arises through aberrant reactions of the human adaptive or innate immune systems. In autoimmunity, the patient's immune system is activated against the body's own proteins. In chronic inflammatory diseases, neutrophils and other leukocytes are constitutively recruited by cytokines and chemokines, leading to tissue damage.\n\nMitigation of inflammation by activation of anti-inflammatory genes and the suppression of inflammatory genes in immune cells is a promising therapeutic approach. There is a body of evidence that once the production of autoantibodies has been initialized, autoantibodies have the capacity to maintain their own production.\n\nStem cell transplantation is being studied and has shown promising results in certain cases.\n\nTraditionally it was believed that the immune system was unable to react against the body's own tissues, a concept described by the German immunologist Paul Ehrlich as \"horror autotoxicus\". In 1904 this theory was challenged by the discovery of a substance in the serum of patients with paroxysmal cold hemoglobinuria that reacted with red blood cells.\n\n"}
{"id": "32068953", "url": "https://en.wikipedia.org/wiki?curid=32068953", "title": "Blocking (linguistics)", "text": "Blocking (linguistics)\n\nIn linguistics, blocking refers to the morphological phenomenon in which a possible form for a word cannot surface because it is \"blocked\" by another form whose features are the most appropriate to the surface form's environment. More basically, it may also be construed as the \"non-occurrence of one form due to the simple existence of another.\" \n\nWord formation employs processes such as the plural marker in English \"s\" or \"es\" (e.g. \"dog\" and \"dogs\" or \"wish\" and \"wishes\"). This plural marker is not, however, acceptable on the word \"child\" (as in *\"childs\"), because it is \"blocked\" by the presence of the competing form \"children\", which in this case inherits features from an older morphological process.\n\nBlocking may also prevent the formation of words with existing synonyms, particularly if the blocked form is morphologically complex and the existing synonym is morphologically simple, e.g. *\"stealer\" which is blocked by the existing simple form \"thief\". \n\nOne possible approach to blocking effects is that of distributed morphology, which asserts that semantic and syntactic features create slots or cells in which items can appear. Blocking happens when one cell is engaged by one form as opposed to another. Blocking has been explained along two primary dimensions: the size of the blocking object, and the existence of ungrammatical forms. \n"}
{"id": "56056488", "url": "https://en.wikipedia.org/wiki?curid=56056488", "title": "Brevifollis", "text": "Brevifollis\n\nBrevifollis is a Gram-negative genus of bacteria from the family of Verrucomicrobiaceae with one known species (\"Brevifollis gellanilyticus\").\n\n \n"}
{"id": "29333761", "url": "https://en.wikipedia.org/wiki?curid=29333761", "title": "Browns Glacier", "text": "Browns Glacier\n\nBrowns Glacier () is a small glacier north of Chaos Glacier, flowing westward into the northern extremity of Ranvik Bay. The glacier was charted by Norwegian cartographers from air photographs taken by the Lars Christensen Expedition (1936–37), and was further identified in John H. Roscoe's 1952 study of this area from U.S. Navy Operation Highjump (1946–47) photography. It was named by Roscoe for Lieutenant (j.g.) Eduardo P. Brown, U.S. Navy, photographic officer with the western task group of Operation Highjump.\n\n"}
{"id": "37911337", "url": "https://en.wikipedia.org/wiki?curid=37911337", "title": "Central place foraging", "text": "Central place foraging\n\nCentral place foraging (CPF) theory is an evolutionary ecology model for analyzing how an organism can maximize foraging rates while traveling through a patch (a discreet resource concentration), but maintains the key distinction of a forager traveling from a home base to a distant foraging location rather than simply passing through an area or travelling at random. CPF was initially developed to explain how red-winged blackbirds might maximize energy returns when traveling to and from a nest. The model has been further refined and used by anthropologists studying human behavioral ecology and archaeology.\n\nOrians and Pearson (1979) found that red-winged blackbirds in eastern Washington State tend to capture a larger number of single species prey items per trip compared to the same species in Costa Rica, which brought back large, single insects. Foraging specialization by Costa Rican blackbirds was attributed to increased search and handling costs of nocturnal foraging, whereas birds in Eastern Washington forage diurnally for prey with lower search and handling costs. Studies with sea birds and seals have also found that load size tends to increase with foraging distance from the nest, as predicted by CPF. Other central place foragers, such as social insects, also show support for CPF theory. European honeybees increase their nectar load as travel time to nectar sites from a hive increases. Beavers have been found to preferentially collect larger diameter trees as distance from their lodge increases.\n\nTo apply the central place foraging model to ethnographic and experimental archaeological data driven by middle range theory, Bettinger et al. (1997) simplify the Barlow and Metcalf (1996) central place model to explore the archaeological implications of acorn (\"Quercus kelloggii\") and mussel (\"Mytilus californianus\") procurement and processing. This model assumes foragers are gathering resources at a distance from their central place with the goal of efficiently returning the resource home. Travel time is expected to determine the degree to which foragers will process a resource in order to increase its utility prior to returning from a foraging location to their central place.\nTransport capabilities in aboriginal California were established by measuring the volume of burden baskets and extrapolating the load weight based on ethnographic data on basket use.\n\nEthnographic and experimental data was used to estimate utility at each possible stage of processing.\nExamining ecology and procurement methods, the central place foraging model was used to predict the conditions in which field processing of the two species will occur.\n\nAn understanding of central place foraging has implications for studying archaeological site formation. Variability of remains at sites can tell us about mobility – whether or not groups are central place foragers, what resource they’re mapping on to, and their degree of mobility. Based on central place foraging application for the processing of mussels and acorns, Bettinger et al. (1997) make several predictions for archaeological expectations. The study shows that procurement with field processing is more costly compared to foraging and processing resources residentially. These results imply that highly mobile foragers will establish a home base in close proximity to staple resources, and all processing of those resources will be done residentially. Less residentially mobile populations would in turn only be mapped onto a few resources, and would be expected to field process non-local resources on logistical procurement forays at greater distances from their central place. Processing debris from archaeological sites should reflect changes in mobility.\n\n\nGlover (2009) used a CPF model to determine if late nineteenth century silver miners near Gothic, Colorado were choosing mine locations efficiently given the costs of transporting silver ore to the mill, the value of silver, and the amount of silver per kilogram of ore. Estimates of the costs associated with transport were obtained using research from physiology to determine the most energetically efficient load size. Newspaper articles were used to determine the hourly wage that a miner could be making if they worked in town instead. Newspapers were also used to estimate the value of silver at that time, and estimates of the amount of silver per kilogram of ore were obtained through records from area silver mills, as well as through newspapers. These differed, with the newspapers optimistically claiming that silver deposits were far more productive than the more accurate mill records demonstrated.\n\nThese estimates were used to determine the optimal placement of mines. A number of historic mining locations were recorded using GPS. These data were used to calculate least cost paths from the mines to Gothic, which provided the distances to the central place. The results were compared to two different CPF models based on newspaper propaganda and the more realistic mill records, respectively.\n\nMiners were choosing locations that were much further away than feasible given the value of silver and its actual abundance. However, the mines were within the distance predicted using the optimistic newspaper estimates. Glover suggested that miners, being new to the area, used social learning strategies and based their decisions on newspaper propaganda and rumors, rather than individual experience. Therefore, they chose locations that were too far away to be economically viable.\n\nShellfish exemplify the resources targeted by the CPF model – those with a heavy, bulky,\nlow utility component (e.g. shell) surrounding a smaller, lighter high utility component\n(e.g. meat). If foragers differentially field process and transport shellfish prey items,\nanalyses of midden composition may incorrectly estimate the importance of some species\nand their relative contribution to prehistoric diets. Using foraging data from the Meriam\nof Australia, Bird and Bliege Bird (1997) compare observed shellfish field acquisition to\nshell deposition at residential sites, and test the hypotheses of the CPF model.\n\nThe Meriam inhabit Torres Strait Islands of Australia, are of Melanesian descent, and\nhave strong cultural and historical ties to New Guinea. They continue to harvest marine\nresources such as sea turtles, fishes, squid, and shellfish. Bird and Bliege Bird conducted\n“focal individual foraging follows” of 33 children, 16 men and 42 women during\nintertidal foraging bouts on reef flats and rock shores. Foraging technology includes 10-\nliter plastic buckets, long-blade knives, and hammers. Foragers are constrained by time\n(2–4 hours at low tide) and load size (10-liter bucket).\n\nLarge clams (\"Hippopus hippopus\" and \"Tridacna spp.\") collected on the reef flat constitute\nover half of the edible weight collected, but since they are almost always field processed\ntheir shells only make up 10% of the residential site deposition. In contrast, sunset clams\n(\"Asaphis violascens\") and nerites (\"Nerita undata\") are usually processed residentially.\nLarge clams were, therefore, underrepresented while small clams and nerites were\noverrepresented in the reconstructed diet.\n\nSince reef flat and rocky shore foraging occurs at multiple sites at variable distances from the residential camp, the authors calculated the mean one-way travel distance processing threshold (formula_1, in meters) for each species. The CPF model accurately predicts\nfield processing for the majority of reef flat foraging events for bivalves. \"Hippopus\" and\n\"Tridacna\" have small processing threshold distances (formula_1 = 74.6 and 137 respectively),\nand no shell is returned to camp at distances beyond 150 meters. Women’s fit nears\n100%, but children and men made the optimal choice less frequently because they usually\nforage for shellfish opportunistically, and therefore do not always carry the appropriate\nprocessing technology.\n\nFor gastropods (\"Lambis lambis\", formula_1 = 278.7), the model accurately predicts processing\nonly 58-59% of the time. This could in part be due to a preference for cooking some species inside of their shells (i.e. the shell has some utility), or also because some prey items are prepared at “dinner-time camps” rather than the residential camp. \"A. violascens\" and \"N. undata\" are never field processed, consistent with their large processing threshold distances (2418.5 and 5355.7 respectively).\n\nOverall, prey types that were difficult or inefficient to process and/or were collected near the residential or temporary camp were not field processed. Species that required little processing time to increase returns and/or were collected far from camp were field\nprocessed. The field processing predictions of the CPF model might be incorrect where shellfish are transported whole in order to maintain freshness for later consumption or trade, or where the shell itself is valuable.\n\nOverall, prey types that were difficult or inefficient to process and/or were collected near the residential or temporary camp were not field processed. Species that required little processing time to increase returns and/or were collected far from camp were field processed. The field processing predictions of the CPF model might be incorrect where shellfish are transported whole in order to maintain freshness for later consumption or trade, or where the shell itself is valuable.\n\nBarlow and Metcalfe (1996) address the issues of field processing of plant materials. Decisions\nof central place foragers may confound archaeological interpretations about the contribution plant\nmaterial to the diet. Two interrelated issues are pertinent: the location of the central place, and field\nprocessing.\n\nBarlow and Metcalfe study archaeological materials from two sites, Danger Cave and Hogup Cave, in the\narea of the Great Salt Lake. These sites contain evidence for the use of piñon pine\n(\"Pinus monophylla\") and pickleweed (\"Allenrolfea occidentalis\").\n\nSamples were obtained for experimental processing from extant piñon groves and pickleweed patches\nin the vicinity as the cave sites. Piñon and pickleweed were harvested and processed in carefully timed\nand controlled stages. After each stage the useful, i.e. edible, portion of the remaining material was\nweighed and recorded before proceeding to the next stage. Stages consisted of: gathering, drying, and\na variety of processes (parching, hulling, winnowing, etc.) to remove inedible constituents. Caloric\nvalues of the samples were then determined via laboratory analysis. These values, as well as assumed\nload sizes from 3 to 15 kg (based on ethnographic burden basket sizes) were then used to generate field\nprocessing model predictions.\n\nAt a distance of 15 kilometers from the central place, the estimated net return rates for field processing loads of piñon and pickleweed are 3,000 and 190 calories per hour, respectively. Since piñon has higher overall return rates, field processing produces a higher rate of return. Because pickleweed has a lower rate of return, it is not worthwhile to spend the additional effort required for field processing.\nTherefore, the central place will be situated closer to pickleweed patches than to piñon in order to more effectively exploit the lower-ranked resource.\n\nThese results imply that the archaeological evidence for pickleweed at the cave may over estimate its actual contribution to the diet. If foragers choose to reside closer to pickleweed patches and bring back largely unprocessed plants, a high density of pickleweed macrofossils will be incorporated into site deposits. However, the opposite is true for piñon, which is largely processed in the field. Thus, most sites will contain little macrofossil evidence of the inedible portions of piñon that could later be recovered by archaeologists. As such, the relative abundance of macrofossils in most cases does not directly translate into the relative contribution of those resources to the diet of central place foragers.\n\nThe goal of the field processing model is for a forager to maximize its return rate per roundtrip from home base to patch. The model typically solves for some amount of travel time that makes it worthwhile to process a resource to a certain stage. To determine this, we need to relate the benefit of processing and the time spent processing to the travel time. We let\n\nformula_4 point on transport-time axis where field processing become profitable\n\nformula_5 time to procure unprocessed resources\n\nformula_6 time to procure and process a load of resources\n\nformula_7 utility of load without field processing\n\nformula_8 utility of load with field processing\n\nThe relationship is then specified by:\nformula_9\nWith values for the utility and time of processed formula_10 and unprocessed loads formula_11, we can solve for formula_12. The right hand side of the equation is the proportion of relative utility*time to utility. \n\nIf formula_13 then formula_14.\n\nIf formula_15, then formula_16.\n\nMany resources have multiple components that can be removed during processing to increase utility. Multistage field processing models provide a way to calculate travel thresholds for each stage when a resource has more than one component. As one increases the utility per load, the time needed to procure a complete load increases.\n\nThe benefit of each stage of processing is:\nformula_17\nwhere\n\nformula_18utility of resource component \"j\"\n\nformula_19proportion of package composed of resource component \"j\" prior to processing\n\nformula_20utility of load at field-processing stage \"j\"\n\nThe cost in terms of time for each stage of processing is:\nformula_21\nwhere\n\nformula_22time required to remove resource component \"j\"\n\nformula_23weight of optimal load size for transport\n\nformula_24weight of unmodified resource package\n\nformula_25time required to handle each resource package\n\nformula_26total handling and processing time required to reach each stage \"j\" of processing\n\nNow these values can be used to calculate formula_27, which is the travel threshold for processing to stage \"j\". In addition to a resource with multiple components, this same model generalizes to a resource with multiple stages, each of which is composed of multiple resources, each of which can be removed independently of each other (i.e., with no additional cost). This model can be further generalized to the case where multiple components with additional costs can be removed in multiple stages of processing through recursion.\n\nThis model rests on a number of assumptions. The most important are listed here. \n\nThere are three key predictions from the field processing model.\n\nTransport decay curves demonstrate the reduction in return rates (cal/hour) experienced by a central place forager as a function of round trip travel time.\n\n"}
{"id": "26120574", "url": "https://en.wikipedia.org/wiki?curid=26120574", "title": "CityEngine", "text": "CityEngine\n\nEsri CityEngine is a three-dimensional (3D) modeling software application developed by Esri R&D Center Zurich (formerly Procedural Inc.) and is specialized in the generation of 3D urban environments. With the procedural modeling approach, CityEngine supports the creation of detailed large-scale 3D city models. CityEngine works with architectural object placement and arrangement in the same manner that VUE manages terrain, ecosystems and atmosphere mapping. Unlike the traditional 3D modeling methodology which is using Computer-Aided Design (CAD) tools, CityEngine improves the shape generation via the rule-based system and data sets—similar as the Geographic Information System (GIS). Due to this dominant feature, CityEngine has been broadly used in academic research or building virtual environments, e.g., urban planning, architecture, visualization, game development, entertainment, GIS, archeology, and cultural heritage. After being integrated with the Building Information Model (BIM), CityEngine can visualize the data of buildings in a larger urban context, enhancing its working scenario toward real construction projects.\n\nIn 2007, Procedural Inc. was founded and separated from ETH Zurich, the top-ranking technology university in Switzerland. In the summer of 2011(), Procedural Inc. was acquired by Esri Inc and became Esri R&D Center Zurich, continually studying in the fields of computer graphics, computer vision, software engineering, finance, marketing, and business.\n\nCityEngine was developed at ETH Zurich by the original author Pascal Mueller, co-founder and CEO of Procedural Inc. During his PhD research at ETH Computer Vision Lab, Mueller invented a number of techniques for procedural modeling of 3D architectural content which make up the foundation of CityEngine. Since CityEngine's public debut in the 2001 Siggraph conference, additional research papers have contributed to featuring CityEngine. In 2008, the first commercial version of CityEngine was released by the Swiss company Procedural Inc and was used by professionals in urban planning, architecture, visualization, game development, entertainment, GIS, archeology, and cultural heritage.\n\nProcedural Modeling Core (CGA Shape Grammar Language): CGA (computer generated architecture) rules allow to control mass, geometry assets, proportions, or texturing of buildings or streets on a citywide scale. (\"More details can be seen in the \"Procedural Modeling\" section.\")\n\nGet Map Data: Users can create a 3D urban environment in few minutes via the download helper; Users can select a target location and import geo-referenced satellite imagery and 3D terrain of that place. If they are available in the OpenStreetMap (OSM), the data of street and building footprint can be easily retrieved to build 3D models via default CGA rules.\n\nGIS/CAD Data Support: Support for industry-standard formats such as Esri Shapefile, File Geodatabase and OpenStreetMap which allow to import/export any geo-spatial/vector data.\n\nParametric Modeling Interface: An interface to interactively control specific street or building parameters, such as the height or age (defined by the CGA rules)\n\nDynamic City Layouts: Interactive design, editing and modification of urban layouts consisting of (curved) streets, blocks and parcels.\n\nMap-Controlled City Modeling: Global control of buildings and street parameters through image maps (for example the building heights or the landuse-mix).\n\nStreet Networks Patterns: Street grow tools to design and construct urban layouts.\n\nIndustry-Standard 3D Formats: CityEngine supports Collada, Autodesk FBX, 3DS, Wavefront OBJ, RenderMan RIB, mental ray MI and e-on software's Vue.\n\nCustom Report Generation: Users can script and generate rule-based reports to show social-economic figures (e.g., Gross Floor Area (GFA), Floor Area Ratio (FAR)) and to analyze their urban design proposals.\n\n3D Web Scene Export: The model built in CityEngine can be directly exported and then used to create a WebGL scene in a browser. The 3D environment in the web scene can be rotated, explored, compared and commented online by multiple users.\n\n360 VR Experience: The scenarios of urban environments can be used to generate a series of panoramic photos for publishing them online. Users can look around by turning their heads in virtual reality (VR) headsets. (Currently, it only supports Samsung Oculus Gear)\n\nPython Scripting Interface: CityEngine provides ce.py as a built-in library.\n\nFacade Wizard: Rule creator and visual facade authoring tool.\n\n3D Format Support for Game Engines (VR/AR): Now the model built in CityEngine can be directly exported to Unreal Engine, with the loading capacity of tens of millions of polygons and tens of thousands of objects, as well as non-limited material textures. Meanwhile, exporting to Unity3D still requires users to use Autodesk Maya as a transfer station.\n\nAvailable for All Platforms: Available for Windows (64bit only), Mac OS X (64bit), and Linux (32/64bit).\n\nCityEngine uses a procedural modeling approach to automatically generate models through a predefined rule set. The rules are defined through a CGA shape grammar system enabling the creation of complex parametric models. Users can change or add the shape grammar as much as needed providing room for new designs.\n\nModeling an urban environment within CityEngine can start out with creating a street network either with the street drawing tool or with data imported from openstreetmap.org or from Esri data formats such as Shapefiles or File Geodatabase. The next step is to subdivide all the lots as many times as specified resulting in a map of lots and streets. By selecting all or some of the lots CityEngine can be instructed to start generating the buildings. Due to the procedural modeling technology, all buildings can be made to vary from one another to achieve an urban aesthetic. At this point the city model can be re-designed and adjusted by changing parameters or the shape grammar itself.\n\nCGA Shape Grammar system can read Esri-Oracle format datasets directly, and it operates as a top-bottom generation tree: it generates complex components from simple Shapefiles polygons/poly-lines/points whereas each branch and leaf of the generation tree cannot interact with others. It is different than main-stream shape grammars like Grasshopper in Rhinoceros 3D and Dynamo in Autodesk Revit.\n\nTraditionally, building a 3D urban environment is very time-consuming resulted from numerous buildings and details of a city. Designers used CAD software to create shapes one by one, and researchers analyzed cities by computing 2D information in GIS (GIS only supports limited 3D shape generation like extrusion.) CityEngine's Procedural Modeling system makes it possible to generate complex 3D models via information massively, bringing a large number of relevant applications. It not only enhances the workflow of urban design/study/planning and merges to a new field of study called Geodesign (means using geospatial information to design a city), but also lowers the threshold of making city environments in game and movie industry.\n\nDiscussions on geodesign often mention the use of Esri CityEngine, although it is not an analytical tool like GIS. As a crucial tool to enhance 3D shape generation in ArcGIS, Esri CityEngine is the critical product to improve the applicability of GeoDesign, using geospatial information to design or analyze a city.\n\nGarsdale Design were early pioneers of Esri CityEngine in the creation of city master plans in Iraq pre-2013. using it to not just model existing historic areas but also model future plans. Larger companies like Foster+Partners and HOK Architects have also used CityEngine in their sizable urban planning projects. Before using that, it took them numerous work hours on creating interactive visualizations of hundred thousands of buildings. With CityEngine, the designers and clients of projects can communicate via craft fluid, data-rich, and real-time rendered experiences.\n\nDue to its dominant feature in building informative city models, urban researchers are using CityEngine to compare land-use planning schemes, starting from the densest global cities such as Hong Kong and Seoul. When urban designers/planners enjoy the quantitive analyst, environmental scientists also like the instant 3D model generation in CityEngine, leading to more convenient informative research out of the time-consumption on creating a city from each building.\n\nTriple-A Games require detailed 3D environments to assign interactive scripts, causing CityEngine's participation in the creation of game scene. Currently, game scenes become larger than that of old video games ten years ago. Large sandbox or open-world games such as GTA series or Assassin Creeds series need millions of distinguished 3D buildings in their virtual world. Designing these games with instantly testing and editing can reduce workloads and increase the rationality of a game scene in the gameplay.\n\n\"Zootopia,\" which won the 2016 Academy Award for Best Animated Feature Film, used CityEngine to establish an impressive metropolis where humans don’t exist. From giraffes to shrews, animals own diverse scales in the system of transportation, houses, and amenities. To build up a multi-scaling city, the designers used CityEngine due to its rule-based system. Before Zootopia (also known as Zootroplis in countries outside the USA), CityEngine was also used to create the Japanese-style city—San Fransokyo—in \"Big Hero 6\" .\n\n\n\n"}
{"id": "8324086", "url": "https://en.wikipedia.org/wiki?curid=8324086", "title": "Computer: A History of the Information Machine", "text": "Computer: A History of the Information Machine\n\nComputer: A History of the Information Machine is a history of computing written by Martin Campbell-Kelly and William Aspray first published in 1996. It follows the history of \"information machines\" from Charles Babbage's difference engine through Herman Hollerith's tabulating machines to the invention of the modern electronic digital computer. A revised 2nd edition published in 2004 included new material on the Internet and World Wide Web, while the updated 3rd edition published in 2013 includes contributions from historians Nathan Ensmenger and Jeffrey Yost. The 3rd edition extends the story to include recent phenomena such as social networking and revises the discussion of early history to reflect new insights from the literature.\n\n\nThe revised second edition ends, somewhat ominously:\n\nAccording to Michael Mahoney's 1998 review in \"IEEE Annals of the History of Computing\", Campbell-Kelly and Aspray's account is \"a highly readable, broad-brush picture of the development of computing, or rather of the computer industry, from its beginning to the present\" which \"sets a new standard for the history of computing.\"\n\nIn his review in \"Technology and Culture\", Robert Seidel writes that \"\"Computer\" is a readable and comprehensive history intended to acquaint novices with a growing historical literature as well as to provide an overview of that history from Charles Babbage through Bill Gates. The authors are well-known contributors to that literature. They have gone beyond it, however, in their interpretation, adding insights that can arise only from a synthetic view of the origins, development, and use of the computer.\"\n\n"}
{"id": "48647585", "url": "https://en.wikipedia.org/wiki?curid=48647585", "title": "Cynthia Pine", "text": "Cynthia Pine\n\nProfessor Cynthia Pine CBE (born October, 1953) is a British Dentistry educator. In 2003, she became the first woman to lead a British school of dentistry. She is the first woman appointed to head a dentistry school in the UK.\n\nShe is one of a group of highly successful Guyanese people in Britain (Michael White of \"The Guardian\" refers to them as the \"Guyanese mafia\"), which includes Waheed Alli, Raj Persaud, Herman Ouseley and David Dabydeen, Keith Waithe and Rudolph Dunbar.\n\nPine was born in 1953 in Guyana. She graduated The University of Manchester with a Bachelor of Dental Surgery (BDS) in 1976 and a PhD from the same institution in 1982, and has an MBA from the University of Dundee.\n\nShe was appointed Dean of the University of Liverpool School of Dentistry in 2003. She was later appointed as Pro Vice Chancellor, International at Salford University.\n\nSince 2013 Pine has been a Professor of Dental Public Health at the Institute of Dentistry, Barts and The London School of Medicine and Dentistry, and Academic Lead and Head of the Unit of Dental Public Health since 2014.\n\nPine was made a Commander of the Most Excellent Order of the British Empire (CBE) in 2006. Pine has been included in the Powerlist of the UK’s 100 most influential people of African and Afro-Caribbean descent.\n"}
{"id": "21195116", "url": "https://en.wikipedia.org/wiki?curid=21195116", "title": "Darwin Core", "text": "Darwin Core\n\nDarwin Core (often abbreviated to DwC) is an extension of Dublin Core for biodiversity informatics. It is meant to provide a stable standard reference for sharing information on biological diversity. The terms described in this standard are a part of a larger set of vocabularies and technical specifications under development and maintained by Biodiversity Information Standards (TDWG) (formerly known as the Taxonomic Databases Working Group (TDWG)).\n\nThe Darwin Core is a body of standards. It includes a glossary of terms (in other contexts these might be called properties, elements, fields, columns, attributes, or concepts) intended to facilitate the sharing of information about biological diversity by providing reference definitions, examples, and commentaries. The Darwin Core is primarily based on taxa, their occurrence in nature as documented by observations, specimens, and samples, and related information. Included in the standard are documents describing how these terms are managed, how the set of terms can be extended for new purposes, and how the terms can be used. The Simple Darwin Core is a specification for one particular way to use the terms and to share data about taxa and their occurrences in a simply-structured way. It is likely what is meant if someone were to suggest \"formatting your data according to the Darwin Core\".\n\nEach term has a definition and commentaries that are meant to promote the consistent use of the terms across applications and disciplines. Evolving commentaries that discuss, refine, expand, or translate the definitions and examples are referred to through links in the Comments attribute of each term. This approach to documentation allows the standard to adapt to new purposes without disrupting existing applications. There is meant to be a clear separation between the terms defined in the standard and the applications that make use of them. For example, though the data types and constraints are not provided in the term definitions, recommendations are made about how to restrict the values where appropriate.\n\nIn practice, Darwin Core decouples the definition and semantics of individual terms from application of these terms in different technologies such as XML, RDF or simple CSV text files. Darwin Core provides separate guidelines on how to encode the terms as XML or text files.\n\nDarwin Core was originally created as a Z39.50 profile by the Z39.50 Biology Implementers Group (ZBIG), supported by funding from a USA National Science Foundation award. The name \"Darwin Core\" was first coined by Allen Allison at the first meeting of the ZBIG held at the University of Kansas in 1998 while commenting on the profile's conceptual similarity with Dublin Core. The Darwin Core profile was later expressed as an XML Schema document for use by the Distributed Generic Information Retrieval (DiGIR) protocol. A TDWG task group was created to revise the Darwin Core, and a ratified metadata standard was officially released on 9 October 2009.\n\nThough ratified as a TDWG/Biodiversity Information Standards standard since then, Darwin Core has had numerous previous versions in production usage. The published standard contains a history with details of the versions leading to the current standard.\n\n\n\n"}
{"id": "56773226", "url": "https://en.wikipedia.org/wiki?curid=56773226", "title": "Derek Lee (biologist)", "text": "Derek Lee (biologist)\n\nDerek Lee (also known as Derek E. Lee or Derek Edward Lee) is an American ecologist and wildlife biologist specializing in population biology and conservation biology. Lee was born in Lodi, California on March 15, 1971, and attended Tokay High School. Lee earned his bachelor's degree from the University of California at Santa Barbara, his master's degree from Humboldt State University, and his Ph.D. from Dartmouth College. For his MS degree he investigated the migratory behavior of black brant geese in Humboldt Bay using capture-recapture statistics to estimate stopover duration and space use. For his Ph.D., he studied the spatial demography of giraffes in the Tarangire ecosystem of Tanzania. His academic work on climate influences on marine bird demography, spotted owls and forest fire, and computer vision applications to wildlife biology are highly cited. His discovery of a white leucistic giraffe was widely reported in popular media. \n\nHe worked at Point Blue Conservation Science for 8 years on Southeast Farallon Island studying how ocean climate change affects the population dynamics of marine predators such as elephant seals, Cassin's auklets, and common murres. He is founder and CEO of Wild Nature Institute, a research organization. Since 2018 he has also worked at Pennsylvania State University as an Associate Research Professor. \n\nLee has published more than 35 scientific peer-reviewed papers in the field of ecology, mainly focused on demography and population biology of wild vertebrates.\n"}
{"id": "31149664", "url": "https://en.wikipedia.org/wiki?curid=31149664", "title": "Dufour effect", "text": "Dufour effect\n\nThe Dufour effect is the energy flux due to a mass concentration gradient occurring as a coupled effect of irreversible processes. It is the reciprocal phenomenon to the Soret effect. The concentration gradient results in a temperature change. For binary liquid mixtures, the Dufour effect is usually considered negligible, whereas in binary gas mixtures the effect can be significant.\n"}
{"id": "8366089", "url": "https://en.wikipedia.org/wiki?curid=8366089", "title": "Francis Arthur Bather", "text": "Francis Arthur Bather\n\nFrancis Arthur Bather FRS (17 February 1863, in Richmond upon Thames – 20 March 1934) was a British palaeontologist, geologist and malacologist. His mother, Lucy Elizabeth Blomfield, was a daughter of Charles Blomfield, Bishop of London. His father, Arthur Henry Bather, who was deaf, was a clerk in the office of the Accountant-General for the Navy.\n\nBather joined the Department of Geology at the Natural History Museum in 1887. He became Keeper in succession to Arthur Smith Woodward in 1924, retiring in 1928.\n\nBather was awarded the Lyell Medal of the Geological Society, of which he also served as President. He was an Honorary Member of the Royal Geological Society of Cornwall, and was elected a fellow of the Royal Society in 1909. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1928. In 1932 Bather was awarded the Mary Clark Thompson Medal from the National Academy of Sciences.\n\nHe married Stina Bergöö, daughter of Adolf Bergöö of Stockholm, and sister of the Swedish artist Karin Bergöö Larsson and they had a daughter and two sons.A photographic portrait is in the National Gallery. \n\n"}
{"id": "775263", "url": "https://en.wikipedia.org/wiki?curid=775263", "title": "Gordon J. Garradd", "text": "Gordon J. Garradd\n\nGordon John Garradd (born 1959) is an Australian amateur astronomer and photographer from Loomberah, New South Wales. He has discovered numerous asteroids and comets, including the hyperbolic comet C/2009 P1, and four novae in the Large Magellanic Cloud. The asteroid and Mars-crosser, 5066 Garradd, was named in his honor.\n\nHe has worked for a number of astronomical institutions in the US and Australia, most recently at Siding Spring Observatory on the Siding Spring Survey, part of the NASA-funded Catalina Sky Survey for near-Earth objects (2002–2011). , the Minor Planet Center credits him with the discovery of 31 minor planets \"(see table)\". There are 16 comets and an asteroid that bear his name. His cometary discoveries include 186P/Garradd (comet Garradd 1), a Jupiter-family comet, and 259P/Garradd (comet Garradd 4), an Encke-type comet.\n\nGarradd was born in Australia and lived his early life in Sydney, Canberra, Oberon, and Tamworth. Astronomy has been an interest since his childhood, and he has built many telescopes himself, starting with a 20 cm (8\") f/7 Newtonian while still in high school, graduating to making mirrors up to 46 cm (18″) diameter and mounts up to the fork mount for the 46 cm f/5.4 Newtonian, and German equatorial mounted 25 cm (10\") f/4.1 that he used for observing near-Earth asteroids and comets.\n\nHis initial profession was as an accountant, but he left that in 1984 to pursue astronomy and photography full-time. He lives with his wife Hether, off the power grid, using solar and wind power. He is a photographer, mountain bike rider, and solar- and wind-power enthusiast.\n\n"}
{"id": "41023487", "url": "https://en.wikipedia.org/wiki?curid=41023487", "title": "Hendrik C. Ferreira", "text": "Hendrik C. Ferreira\n\nHendrik Christoffel Ferreira is a professor in Digital Communications and Information Theory at the University of Johannesburg, Johannesburg, South Africa. He studied electrical engineering at the University of Pretoria, South Africa, where he obtained his Ph.D. in 1980. He worked as a visiting researcher at Linkabit in San Diego. He joined the Rand Afrikaans University in 1983, where, in 1989, he was appointed full professor. In recognition of his excellence in research and educating post-graduate students, he has been appointed as a research professor at the University of Johannesburg in 2007. He is a Fellow of the SAIEE, the South African Institute of Electrical Engineers.\n\nFerreira published close to 250 research papers on topics such as digital communications, power line communications, vehicular communication systems. With his work he introduced and developed a new theme in Information Theory, namely coding techniques for constructing combined channel codes, where error correction and channel properties are considered jointly.\n\nFerreira is a pioneering initiator and stimulator of the research fields of Information Theory and Power Line Communications in South Africa. He has also been an organizer for the IEEE Information Theory Society and Power Line Communications within South Africa and Africa. He is a member of the Technical Committee for Power Line Communications of the IEEE Communications Society, and he has served on the Technical Program Committee of several IEEE conferences, including the IEEE (ISIT) International Symposium on Information Theory, the IEEE (ISPLC) International Symposium on Power Line Communications, and the IEEE Africon and Chinacom conferences.\n"}
{"id": "21653817", "url": "https://en.wikipedia.org/wiki?curid=21653817", "title": "Hopkins Ultraviolet Telescope", "text": "Hopkins Ultraviolet Telescope\n\nThe Hopkins Ultraviolet Telescope (HUT) was a space telescope designed to make spectroscopic observations in the far-ultraviolet region of the electromagnetic spectrum. It was flown into orbit on the Space Shuttle and operated from the Shuttle's payload bay on two occasions: in December 1990, as part of Shuttle mission STS-35, and in March 1995, as part of mission STS-67.\n\nHUT was designed and built by a team based at Johns Hopkins University, led by Arthur Davidsen. The telescope consisted of a 90 cm main mirror used to focus ultraviolet light onto a spectrograph situated at the prime focus. This instrument had a spectroscopic range of 82.5 to 185 nms, and a spectral resolution of about 0.3 nm.\nIt weighed 789 kilograms (1736 pounds).\n\nHUT was used to observe a wide range of astrophysical sources, including supernova remnants, active galactic nuclei, cataclysmic variable stars, as well as various planets in the Solar System. During the 1990 flight, HUT was used to make 106 observations of 77 astronomical targets. During the 1995 flight, 385 observations were made of 265 targets.\n\nHUT was co-mounted with WUPPE, HIT, and BBXRT on the Astro-1 mission (1990) and with just WUPPE and HIT on Astro-2 (in 1995).\n\n"}
{"id": "118450", "url": "https://en.wikipedia.org/wiki?curid=118450", "title": "Innovation", "text": "Innovation\n\nInnovation can be simply defined as a \"new idea, creative thoughts, new imaginations in form of device or method\". However, innovation is often also viewed as the application of better solutions that meet new requirements, unarticulated needs, or existingmarket needs. Such innovation takes place through the provision of more-effective products, processes, services, technologies, or business models that are made available to markets, governments and society. The term \"innovation\" can be defined as something original and more effective and, as a consequence, new, that \"breaks into\" the market or society. Innovations tend to be produced by outsiders and founders in startups, rather than existing organizations. Innovation is related to, but not the same as, invention, as innovation is more apt to involve the practical implementation of an invention (i.e. new/improved ability) to make a meaningful impact in the market or society, and not all innovations require an invention. Innovation often manifests itself via the engineering process, when the problem being solved is of a technical or scientific nature. The opposite of innovation is exnovation.\n\nPronunciation of the word has long been argued about due to the unusual sounding of the o becoming an i. The phonetic sound would suggest a spelling of inivative if taken at the word of a true scholar. \n\nWhile a novel device is often described as an innovation, in economics, management science, and other fields of practice and analysis, innovation is generally considered to be the result of a process that brings together various novel ideas in such a way that they affect society. In industrial economics, innovations are created and found empirically from services to meet growing consumer demand.\n\nA 2014 survey of literature on innovation found over 40 definitions. In an industrial survey of how the software industry defined innovation, the following definition given by Crossan and Apaydin was considered to be the most complete, which builds on the Organisation for Economic Co-operation and Development (OECD) manual's definition: \n\nAccording to Kanter innovation includes original invention and creative use and defines innovation as a generation, admission and realization of new ideas, products, services and processes.\n\nTwo main dimensions of innovation were degree of novelty (patent) (i.e. whether an innovation is new to the firm, new to the market, new to the industry, or new to the world) and kind of innovation (i.e. whether it is processor product-service system innovation). In recent organizational scholarship, researchers of workplaces have also distinguished innovation to be separate from creativity, by providing an updated definition of these two related but distinct constructs:\n\nIn business and in economics, innovation can become a catalyst for growth. With rapid advancements in transportation and communications over the past few decades, the old-world concepts of factor endowments and comparative advantage which focused on an area's unique inputs are outmoded for today's global economy. Economist Joseph Schumpeter (1883–1950), who contributed greatly to the study of innovation economics, argued that industries must incessantly revolutionize the economic structure from within, that is innovate with better or more effective processes and products, as well as market distribution, such as the connection from the craft shop to factory. He famously asserted that \"creative destruction is the essential fact about capitalism\". Entrepreneurs continuously look for better ways to satisfy their consumer base with improved quality, durability, service and price which come to fruition in innovation with advanced technologies and organizational strategies.\n\nA prime example of innovation involved the explosive boom of Silicon Valley startups out of the Stanford Industrial Park. In 1957, dissatisfied employees of Shockley Semiconductor, the company of Nobel laureate and co-inventor of the transistor William Shockley, left to form an independent firm, Fairchild Semiconductor. After several years, Fairchild developed into a formidable presence in the sector. Eventually, these founders left to start their own companies based on their own, unique, latest ideas, and then leading employees started their own firms. Over the next 20 years, this snowball process launched the momentous startup-company explosion of information-technology firms. Essentially, Silicon Valley began as 65 new enterprises born out of Shockley's eight former employees. Since then, hubs of innovation have sprung up globally with similar metonyms, including Silicon Alley encompassing New York City.\n\nAnother example involves business incubators – a phenomenon nurtured by governments around the world, close to knowledge clusters (mostly research-based) like universities or other Government Excellence Centres – which aim primarily to channel generated knowledge to applied innovation outcomes in order to stimulate regional or national economic growth.\n\nIn the organizational context, innovation may be linked to positive changes in efficiency, productivity, quality, competitiveness, and market share. However, recent research findings highlight the complementary role of organizational culture in enabling organizations to translate innovative activity into tangible performance improvements. Organizations can also improve profits and performance by providing work groups opportunities and resources to innovate, in addition to employee's core job tasks. Peter Drucker wrote:\nAccording to Clayton Christensen, disruptive innovation is the key to future success in business. The organisation requires a proper structure in order to retain competitive advantage. It is necessary to create and nurture an environment of innovation. Executives and managers need to break away from traditional ways of thinking and use change to their advantage. It is a time of risk but even greater opportunity. The world of work is changing with the increase in the use of technology and both companies and businesses are becoming increasingly competitive. Companies will have to downsize and re-engineer their operations to remain competitive. This will affect employment as businesses will be forced to reduce the number of people employed while accomplishing the same amount of work if not more.\n\nWhile disruptive innovation will typically \"attack a traditional business model with a lower-cost solution and overtake incumbent firms quickly,\" foundational innovation is slower, and typically has the potential to create new foundations for global technology systems over the longer term. Foundational innovation tends to transform business operating models as entirely new business models emerge over many years, with gradual and steady adoption of the innovation leading to waves of technological and institutional change that gain momentum more slowly. The advent of the packet-switched communication protocol TCP/IP—originally introduced in 1972 to support a single use case for United States Department of Defense electronic communication (email), and which gained widespread adoption only in the mid-1990s with the advent of the World Wide Web—is a foundational technology.\n\nAll organizations can innovate, including for example hospitals, universities, and local governments. For instance, former Mayor Martin O’Malley pushed the City of Baltimore to use CitiStat, a performance-measurement data and management system that allows city officials to maintain statistics on several areas from crime trends to the conditions of potholes. This system aids in better evaluation of policies and procedures with accountability and efficiency in terms of time and money. In its first year, CitiStat saved the city $13.2 million. Even mass transit systems have innovated with hybrid bus fleets to real-time tracking at bus stands. In addition, the growing use of mobile data terminals in vehicles, that serve as communication hubs between vehicles and a control center, automatically send data on location, passenger counts, engine performance, mileage and other information. This tool helps to deliver and manage transportation systems.\n\nStill other innovative strategies include hospitals digitizing medical information in electronic medical records. For example, the U.S. Department of Housing and Urban Development's HOPE VI initiatives turned severely distressed public housing in urban areas into revitalized, mixed-income environments; the Harlem Children’s Zone used a community-based approach to educate local area children; and the Environmental Protection Agency's brownfield grants facilitates turning over brownfields for environmental protection, green spaces, community and commercial development.\n\nThere are several sources of innovation. It can occur as a result of a focus effort by a range of different agents, by chance, or as a result of a major system failure.\n\nAccording to Peter F. Drucker, the general sources of innovations are different changes in industry structure, in market structure, in local and global demographics, in human perception, mood and meaning, in the amount of already available scientific knowledge, etc.\nIn the simplest linear model of innovation the traditionally recognized source is \"manufacturer innovation\". This is where an agent (person or business) innovates in order to sell the innovation. Specifically, R&D measurement is the commonly used input for innovation, in particular in the business sector, named Business Expenditure on R&D (BERD) that grew over the years on the expenses of the declining R&D invested by the public sector.\n\nAnother source of innovation, only now becoming widely recognized, is \"end-user innovation\". This is where an agent (person or company) develops an innovation for their own (personal or in-house) use because existing products do not meet their needs. MIT economist Eric von Hippel has identified end-user innovation as, by far, the most important and critical in his classic book on the subject, \"The Sources of Innovation\".\n\nThe robotics engineer Joseph F. Engelberger asserts that innovations require only three things:\n\nHowever, innovation processes usually involve: identifying customer needs, macro and meso trends, developing competences, and finding financial support.\n\nThe Kline chain-linked model of innovation places emphasis on potential market needs as drivers of the innovation process, and describes the complex and often iterative feedback loops between marketing, design, manufacturing, and R&D.\n\nInnovation by businesses is achieved in many ways, with much attention now given to formal research and development (R&D) for \"breakthrough innovations\". R&D help spur on patents and other scientific innovations that leads to productive growth in such areas as industry, medicine, engineering, and government. Yet, innovations can be developed by less formal on-the-job modifications of practice, through exchange and combination of professional experience and by many other routes. Investigation of relationship between the concepts of innovation and technology transfer revealed overlap. The more radical and revolutionary innovations tend to emerge from R&D, while more incremental innovations may emerge from practice – but there are many exceptions to each of these trends.\n\nInformation technology and changing business processes and management style can produce a work climate favorable to innovation. For example, the software tool company Atlassian conducts quarterly \"ShipIt Days\" in which employees may work on anything related to the company's products. Google employees work on self-directed projects for 20% of their time (known as Innovation Time Off). Both companies cite these bottom-up processes as major sources for new products and features.\n\nAn important innovation factor includes customers buying products or using services. As a result, firms may incorporate users in focus groups (user centred approach), work closely with so called lead users (lead user approach) or users might adapt their products themselves. The lead user method focuses on idea generation based on leading users to develop breakthrough innovations. U-STIR, a project to innovate Europe’s surface transportation system, employs such workshops. Regarding this user innovation, a great deal of innovation is done by those actually implementing and using technologies and products as part of their normal activities. Sometimes user-innovators may become entrepreneurs, selling their product, they may choose to trade their innovation in exchange for other innovations, or they may be adopted by their suppliers. Nowadays, they may also choose to freely reveal their innovations, using methods like open source. In such networks of innovation the users or communities of users can further develop technologies and reinvent their social meaning.\n\nOne technique for innovating a solution to an identified problem is to actually attempt an experiment with many possible solutions. This technique was famously used by Thomas Edison's laboratory to find a version of the incandescent light bulb economically viable for home use, which involved searching through thousands of possible filament designs before settling on carbonized bamboo.\n\nThis technique is sometimes used in pharmaceutical drug discovery. Thousands of chemical compounds are subjected to high-throughput screening to see if they have any activity against a target molecule which has been identified as biologically significant to a disease. Promising compounds can then be studied; modified to improve efficacy, reduce side effects, and reduce cost of manufacture; and if successful turned into treatments.\n\nThe related technique of A/B testing is often used to help optimize the design of web sites and mobile apps. This is used by major sites such as amazon.com, Facebook, Google, and Netflix. Procter & Gamble uses computer-simulated products and onlinen user panels to conduct larger numbers of experiments to guide the design, packaging, and shelf placement of consumer products. Capital One uses this technique to drive credit card marketing offers.\n\nPrograms of organizational innovation are typically tightly linked to organizational goals and objectives, to the business plan, and to market competitive positioning. One driver for innovation programs in corporations is to achieve growth objectives. As Davila et al. (2006) notes, \"Companies cannot grow through cost reduction and reengineering alone... Innovation is the key element in providing aggressive top-line growth, and for increasing bottom-line results\".\n\nOne survey across a large number of manufacturing and services organizations found, ranked in decreasing order of popularity, that systematic programs of organizational innovation are most frequently driven by: improved quality, creation of new markets, extension of the product range, reduced labor costs, improved production processes, reduced materials, reduced environmental damage, replacement of products/services, reduced energy consumption, conformance to regulations.\n\nThese goals vary between improvements to products, processes and services and dispel a popular myth that innovation deals mainly with new product development. Most of the goals could apply to any organisation be it a manufacturing facility, marketing firm, hospital or local government. Whether innovation goals are successfully achieved or otherwise depends greatly on the environment prevailing in the firm.\n\nConversely, failure can develop in programs of innovations. The causes of failure have been widely researched and can vary considerably. Some causes will be external to the organization and outside its influence of control. Others will be internal and ultimately within the control of the organization. Internal causes of failure can be divided into causes associated with the cultural infrastructure and causes associated with the innovation process itself. Common causes of failure within the innovation process in most organizations can be distilled into five types: poor goal definition, poor alignment of actions to goals, poor participation in teams, poor monitoring of results, poor communication and access to information.\n\nDiffusion of innovation research was first started in 1903 by seminal researcher Gabriel Tarde, who first plotted the S-shaped diffusion curve. Tarde defined the innovation-decision process as a series of steps that includes:\n\nOnce innovation occurs, innovations may be spread from the innovator to other individuals and groups. This process has been proposed that the life cycle of innovations can be described using the 's-curve' or diffusion curve. The s-curve maps growth of revenue or productivity against time. In the early stage of a particular innovation, growth is relatively slow as the new product establishes itself. At some point, customers begin to demand and the product growth increases more rapidly. New incremental innovations or changes to the product allow growth to continue. Towards the end of its lifecycle, growth slows and may even begin to decline. In the later stages, no amount of new investment in that product will yield a normal rate of return\n\nThe s-curve derives from an assumption that new products are likely to have \"product life\" – i.e., a start-up phase, a rapid increase in revenue and eventual decline. In fact, the great majority of innovations never get off the bottom of the curve, and never produce normal returns.\n\nInnovative companies will typically be working on new innovations that will eventually replace older ones. Successive s-curves will come along to replace older ones and continue to drive growth upwards. In the figure above the first curve shows a current technology. The second shows an emerging technology that currently yields lower growth but will eventually overtake current technology and lead to even greater levels of growth. The length of life will depend on many factors.\n\nMeasuring innovation is inherently difficult as it implies commensurability so that comparisons can be made in quantitative terms. Innovation, however, is by definition novelty. Comparisons are thus often meaningless across products or service. Nevertheless, Edison et al. in their review of literature on innovation management found 232 innovation metrics. They categorized these measures along five dimensions i.e. inputs to the innovation process, output from the innovation process, effect of the innovation output, measures to access the activities in an innovation process and availability of factors that facilitate such a process.\n\nThere are two different types of measures for innovation: the organizational level and the political level.\n\nThe measure of innovation at the organizational level relates to individuals, team-level assessments, and private companies from the smallest to the largest company. Measure of innovation for organizations can be conducted by surveys, workshops, consultants, or internal benchmarking. There is today no established general way to measure organizational innovation. Corporate measurements are generally structured around balanced scorecards which cover several aspects of innovation such as business measures related to finances, innovation process efficiency, employees' contribution and motivation, as well benefits for customers. Measured values will vary widely between businesses, covering for example new product revenue, spending in R&D, time to market, customer and employee perception & satisfaction, number of patents, additional sales resulting from past innovations.\n\nFor the political level, measures of innovation are more focused on a country or region competitive advantage through innovation. In this context, organizational capabilities can be evaluated through various evaluation frameworks, such as those of the European Foundation for Quality Management. The OECD Oslo Manual (1995) suggests standard guidelines on measuring technological product and process innovation. Some people consider the Oslo Manual complementary to the Frascati Manual from 1963. The new Oslo manual from 2005 takes a wider perspective to innovation, and includes marketing and organizational innovation. These standards are used for example in the European Community Innovation Surveys.\n\nOther ways of measuring innovation have traditionally been expenditure, for example, investment in R&D (Research and Development) as percentage of GNP (Gross National Product). Whether this is a good measurement of innovation has been widely discussed and the Oslo Manual has incorporated some of the critique against earlier methods of measuring. The traditional methods of measuring still inform many policy decisions. The EU Lisbon Strategy has set as a goal that their average expenditure on R&D should be 3% of GDP.\n\nInnovation is starting to decline due to the newly introduced immigration policies. The new immigration policies have effects on the economy because 25% of the companies were created by foreign entrepreneurs. \n\nMany scholars claim that there is a great bias towards the \"science and technology mode\" (S&T-mode or STI-mode), while the \"learning by doing, using and interacting mode\" (DUI-mode) is ignored and measurements and research about it rarely done. For example, an institution may be high tech with the latest equipment, but lacks crucial doing, using and interacting tasks important for innovation.\n\nA common industry view (unsupported by empirical evidence) is that comparative cost-effectiveness research is a form of price control which reduces returns to industry, and thus limits R&D expenditure, stifles future innovation and compromises new products access to markets. \nSome academics claim cost-effectiveness research is a valuable value-based measure of innovation which accords \"truly significant\" therapeutic advances (i.e. providing \"health gain\") higher prices than free market mechanisms. Such value-based pricing has been viewed as a means of indicating to industry the type of innovation that should be rewarded from the public purse.\n\nAn Australian academic developed the case that national comparative cost-effectiveness analysis systems should be viewed as measuring \"health innovation\" as an evidence-based policy concept for valuing innovation distinct from valuing through competitive markets, a method which requires strong anti-trust laws to be effective, on the basis that both methods of assessing pharmaceutical innovations are mentioned in annex 2C.1 of the Australia-United States Free Trade Agreement.\n\nSeveral indices attempt to measure innovation and rank entities based on these measures, such as:\n\nMany research studies try to rank countries based on measures of innovation. Common areas of focus include: high-tech companies, manufacturing, patents, post secondary education, research and development, and research personnel. The left ranking of the top 10 countries below is based on the 2016 Bloomberg Innovation Index. However, studies may vary widely; for example the Global Innovation Index 2016 ranks Switzerland as number one wherein countries like South Korea and Japan do not even make the top ten.\n\nIn 2005 Jonathan Huebner, a physicist working at the Pentagon's Naval Air Warfare Center, argued on the basis of both U.S. patents and world technological breakthroughs, per capita, that the rate of human technological innovation peaked in 1873 and has been slowing ever since. In his article, he asked \"Will the level of technology reach a maximum and then decline as in the Dark Ages?\" In later comments to \"New Scientist\" magazine, Huebner clarified that while he believed that we will reach a rate of innovation in 2024 equivalent to that of the Dark Ages, he was not predicting the reoccurrence of the Dark Ages themselves.\n\nJohn Smart criticized the claim and asserted that technological singularity researcher Ray Kurzweil and others showed a \"clear trend of acceleration, not deceleration\" when it came to innovations. The foundation replied to Huebner the journal his article was published in, citing Second Life and eHarmony as proof of accelerating innovation; to which Huebner replied. \nHowever, Huebner's findings were confirmed in 2010 with U.S. Patent Office data. and in a 2012 paper.\n\nThe theme of innovation as a tool to disrupting patterns of poverty has gained momentum since the mid-2000s among major international development actors such as DFID, Gates Foundation's use of the Grand Challenge funding model, and USAID's Global Development Lab. Networks have been established to support innovation in development, such as D-Lab at MIT. Investment funds have been established to identify and catalyze innovations in developing countries, such as DFID's Global Innovation Fund, Human Development Innovation Fund, and (in partnership with USAID) the Global Development Innovation Ventures.\n\nGiven the noticeable effects on efficiency, quality of life, and productive growth, innovation is a key factor in society and economy. Consequently, policymakers have long worked to develop environments that will foster innovation and its resulting positive benefits, from funding Research and Development to supporting regulatory change, funding the development of innovation clusters, and using public purchasing and standardisation to 'pull' innovation through.\n\nFor instance, experts are advocating that the U.S. federal government launch a National Infrastructure Foundation, a nimble, collaborative strategic intervention organization that will house innovations programs from fragmented silos under one entity, inform federal officials on innovation performance metrics, strengthen industry-university partnerships, and support innovation economic development initiatives, especially to strengthen regional clusters. Because clusters are the geographic incubators of innovative products and processes, a cluster development grant program would also be targeted for implementation. By focusing on innovating in such areas as precision manufacturing, information technology, and clean energy, other areas of national concern would be tackled including government debt, carbon footprint, and oil dependence. The U.S. Economic Development Administration understand this reality in their continued Regional Innovation Clusters initiative. In addition, federal grants in R&D, a crucial driver of innovation and productive growth, should be expanded to levels similar to Japan, Finland, South Korea, and Switzerland in order to stay globally competitive. Also, such grants should be better procured to metropolitan areas, the essential engines of the American economy.\n\nMany countries recognize the importance of research and development as well as innovation including Japan's Ministry of Education, Culture, Sports, Science and Technology (MEXT); Germany's Federal Ministry of Education and Research; and the Ministry of Science and Technology in the People's Republic of China. Furthermore, Russia's innovation programme is the Medvedev modernisation programme which aims at creating a diversified economy based on high technology and innovation. Also, the Government of Western Australia has established a number of innovation incentives for government departments. Landgate was the first Western Australian government agency to establish its Innovation Program.\n\nRegions have taken a more proactive role in supporting innovation. Many regional governments are setting up regional innovation agency to strengthen regional innovation capabilities. In Medellin, Colombia, the municipality of Medellin created in 2009 Ruta N to transform the city into a knowledge city.\n"}
{"id": "1065472", "url": "https://en.wikipedia.org/wiki?curid=1065472", "title": "Kosmos 213", "text": "Kosmos 213\n\nKosmos 213 ( meaning \"Cosmos 213\") was one of a series of Soviet Soyuz programme test spacecraft whose purpose was to further test and develop the passenger version. Scientific data and measurements were relayed to earth by multichannel telemetry systems equipped with space-borne memory units. Kosmos 212 and Kosmos 213 automatically docked in orbit on April 15, 1968. Both spacecraft landed on Soviet territory.\n\n\nText comes from NASA NSSDC Master Catalog\n"}
{"id": "22803034", "url": "https://en.wikipedia.org/wiki?curid=22803034", "title": "List of instruction sets", "text": "List of instruction sets\n\nA list of computer central processor instruction sets:<br>\n\n\n\n\n\n\n\n\nFor StarCore DSP architecture, refer to Motorola section.\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduced in the textbook of Prof. Donald Knuth\n\nDSP Group and Parthus Technologies plc were merged into CEVA, Inc. in 2002.\n\n\n\n\n\n\n\nThe company was established as a subsidiary of General Instrument in 1987, then became an independent company as Microchip Technology in 1989.\n\n\n\n\n\nThese are instruction sets introduced by Honeywell; for the instruction sets from General Electric, refer to the General Electric section.\n\n\n\n\nby Prof. David May\n\n\n\n\n\n\n\n\n\n\nFor the Power Architecture, refer to IBM section.\n\n\n\n</ref> 8-bit MCU\n</ref> 16/8-bit MCU\n</ref> 16/8-bit MCU\n\n\n\n\n\nThe semiconductor operations of Hitachi and Mitsubishi Electric were transferred to Renesas Technology Corporation on April 1, 2003. In addition, NEC Electronics Corporation, a subsidiary of NEC Corporation, and Renesas Technology were merged into Renesas Electronics Corporation on April 1, 2010.\n\n</ref> 16/32-bit CISC MCU, 2 banks of 4 × 16-bit data and 2 × 24-bit address registers\n\n\nUbicom was acquired by Qualcomm in 2012.\n\n\n\nFor SPC5 Power Architecture Book E product line, refer to IBM section.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "399520", "url": "https://en.wikipedia.org/wiki?curid=399520", "title": "List of interface bit rates", "text": "List of interface bit rates\n\nThis is a list of interface bit rates, is a measure of information transfer rates, or digital bandwidth capacity, at which digital interfaces in a computer or network can communicate over various kinds of buses and channels. The distinction can be arbitrary between a \"computer bus\", often closer in space, and larger telecommunications networks. Many device interfaces or protocols (e.g., SATA, USB, SAS, PCIe) are used both inside many-device boxes, such as a PC, and one-device-boxes, such as a hard drive enclosure. Accordingly, this page lists both the internal ribbon and external communications cable standards together in one sortable table.\n\nMost of the listed rates are theoretical maximum throughput measures; in practice, the actual effective throughput is almost inevitably lower in proportion to the load from other devices (network/bus contention), physical or temporal distances, and other overhead in data link layer protocols etc. The maximum goodput (for example, the file transfer rate) may be even lower due to higher layer protocol overhead and data packet retransmissions caused by line noise or interference such as crosstalk, or lost packets in congested intermediate network nodes. All protocols lose something, and the more robust ones that deal resiliently with very many failure situations tend to lose more maximum throughput to get higher total long term rates.\n\nDevice interfaces where one bus transfers data via another will be limited to the throughput of the slowest interface, at best. For instance, SATA revision 3.0 (6 Gbit/s) controllers on one PCI Express 2.0 (5 Gbit/s) channel will be limited to the 5 Gbit/s rate and have to employ more channels to get around this problem. Early implementations of new protocols very often have this kind of problem. The physical phenomena on which the device relies (such as spinning platters in a hard drive) will also impose limits; for instance, no spinning platter shipping in 2009 saturates SATA revision 2.0 (3 Gbit/s), so moving from this 3 Gbit/s interface to USB 3.0 at 4.8 Gbit/s for one spinning drive will result in no increase in realized transfer rate.\n\nContention in a wireless or noisy spectrum, where the physical medium is entirely out of the control of those who specify the protocol, requires measures that also use up throughput. Wireless devices, BPL, and modems may produce a higher line rate or gross bit rate, due to error-correcting codes and other physical layer overhead. It is extremely common for throughput to be far less than half of theoretical maximum, though the more recent technologies (notably BPL) employ preemptive spectrum analysis to avoid this and so have much more potential to reach actual gigabit rates in practice than prior modems.\n\nAnother factor reducing throughput is deliberate policy decisions made by Internet service providers that are made for contractual, risk management, aggregation saturation, or marketing reasons. Examples are rate limiting, bandwidth throttling, and the assignment of IP addresses to groups. These practices tend to minimize the throughput available to every user, but maximize the number of users that can be supported on one backbone.\n\nFurthermore, chips are often not available in order to implement the fastest rates. AMD, for instance, does not support the 32-bit HyperTransport interface on any CPU it has shipped as of the end of 2009. Additionally, WiMAX service providers in the US typically support only up to 4 Mbit/s as of the end of 2009.\n\nChoosing service providers or interfaces based on theoretical maxima is unwise, especially for commercial needs. A good example is large scale data centers, which should be more concerned with price per port to support the interface, wattage and heat considerations, and total cost of the solution. Because some protocols such as SCSI and Ethernet now operate many orders of magnitude faster than when originally deployed, scalability of the interface is one major factor, as it prevents costly shifts to technologies that are not backward compatible. Underscoring this is the fact that these shifts often happen involuntarily or by surprise, especially when a vendor abandons support for a proprietary system.\n\nBy convention, bus and network data rates are denoted either in bits per second (bit/s) or bytes per second (B/s). In general, parallel interfaces are quoted in B/s and serial in bit/s. The more commonly used is shown below in bold type.\n\nOn devices like modems, bytes may be more than 8 bits long because they may be individually padded out with additional start and stop bits; the figures below will reflect this. Where channels use line codes (such as Ethernet, Serial ATA and PCI Express), quoted rates are for the decoded signal.\n\nThe figures below are simplex data rates, which may conflict with the duplex rates vendors sometimes use in promotional materials. Where two values are listed, the first value is the downstream rate and the second value is the upstream rate.\n\nAll quoted figures are in metric decimal units. Note that these aren't the traditional binary prefixes for memory size. These decimal prefixes have long been established in data communications. This occurred before 1998 when IEC and other organizations introduced new binary prefixes and attempted to standardize their use across all computing applications.\n\nThe figures below are grouped by network or bus type, then sorted within each group from lowest to highest bandwidth; gray shading indicates a lack of known implementations.\n\n802.11 networks in infrastructure mode are half-duplex; all stations share the medium. In infrastructure or access point mode, all traffic has to pass through an Access Point (AP). Thus, two stations on the same access point that are communicating with each other must have each and every frame transmitted twice: from the sender to the access point, then from the access point to the receiver. This approximately halves the effective bandwidth.\n\n802.11 networks in ad hoc mode are still half-duplex, but devices communicate directly rather than through an access point. In this mode all devices must be able to \"see\" each other, instead of only having to be able to \"see\" the access point.\n\n LPC protocol includes high overhead. While the gross data rate equals 33.3 million 4-bit-transfers per second (or ), the fastest transfer, firmware read, results in . The next fastest bus cycle, 32-bit ISA-style DMA write, yields only . Other transfers may be as low as .\n\n Uses 8b/10b encoding\n\nThe table below shows values for PC memory module types.\nThese modules usually combine multiple chips on one circuit board.\nSIMM modules connect to the computer via an 8-bit- or 32-bit-wide interface.\nDIMM modules connect to the computer via a 64-bit-wide interface.\nSome other computer architectures use different modules with a different bus width.\n\nIn a single-channel configuration, only one module at a time can transfer information to the CPU.\nIn multi-channel configurations, multiple modules can transfer information to the CPU at the same time, in parallel.\nFPM, EDO, SDR, and RDRAM memory was not commonly installed in a dual-channel configuration. DDR and DDR2 memory is usually installed in single- or dual-channel configuration. DDR3 memory is installed in single-, dual-, tri-, and quad-channel configurations.\nBit rates of multi-channel configurations are the product of the module bit-rate (given below) and the number of channels.\n\nRAM memory modules are also utilised by graphics processing units; however, memory modules for those differ somewhat from standard computer memory, particularly with lower power requirements, and are specialised to serve GPUs: for example, GDDR3 was fundamentally based on DDR2. Every graphics memory chip is directly connected to the GPU (point-to-point). The total GPU memory bus width varies with the number of memory chips and the number of lanes per chip. For example, GDDR5 specifies either 16 or 32 lanes per \"device\" (chip), while GDDR5X specifies 64 lanes per chip. Over the years, bus widths rose from 64-bit to 512-bit and beyond: e.g. HBM is 1024 bits wide.\nBecause of this variability, graphics memory speeds are sometimes compared per pin. For direct comparison to the values for 64-bit modules shown above, video RAM is compared here in 64-lane lots, corresponding to two chips for those devices with 32-bit widths.\nIn 2012, high-end GPUs used 8 or even 12 chips with 32 lanes each, for a total memory bus width of 256 or 384 bits. Combined with a transfer rate per pin of 5 GT/s or more, such cards could reach 240 GB/s or more.\n\nRAM frequencies used for a given chip technology vary greatly. Where single values are given below, they are examples from high-end cards. Since many cards have more than one pair of chips, the total bandwidth is correspondingly higher. For example, high-end cards often have eight chips, each 32 bits wide, so the total bandwidth for such cards is four times the value given below.\n\nData rates given are from the video source (e.g., video card) to receiving device (e.g., monitor) only. Out of band and reverse signaling channels are not included.\n\n\n"}
{"id": "13103839", "url": "https://en.wikipedia.org/wiki?curid=13103839", "title": "Magnetic water treatment", "text": "Magnetic water treatment\n\nMagnetic water treatment (also known as anti-scale magnetic treatment or AMT) is a method of supposedly reducing the effects of hard water by passing it through a magnetic field as a non-chemical alternative to water softening. Magnetic water treatment is regarded as unproven and unscientific.\n\nThere is a lack of peer-reviewed laboratory data, mechanistic explanations, and documented field studies to support its effectiveness. Erroneous conclusions about their efficacy are based on applications with uncontrolled variables. There are, however, some studies which have claimed significant effects and proposed possible mechanisms for the observed decrease in water scale.\n\nVendors of magnetic water treatment devices frequently use pictures and testimonials to support their claims, but omit quantitative detail and well-controlled studies. Advertisements and promotions generally omit system variables, such as corrosion or system mass balance analyticals, as well as measurements of post-treatment water such as concentration of hardness ions or the distribution, structure, and morphology of suspended particles.\n\nDuration of exposure and field strength, gradient, rate of change, and orientation along or perpendicular to flow are variously cited as important to the results. Magnetic water treatment proponent Klaus Kronenberg proposed that the shapes of solute lime molecules are modified by strong magnetic fields, leading them to precipitate as spherical or round crystals rather than deposit as sheets or platelets of hard crystals. Simon Parsons of the School of Water Sciences at Cranfield University proposed that the magnetic field reduces the surface charge on small particles, increasing the tendency to coagulate as large particles that stay with the flow rather than depositing as scale. However, an internal study in 1996 at Lawrence Livermore National Laboratory found no difference in preferred crystal structure of scale deposited in magnetic water treatment systems. \nLiu \"et al.\" and Coey and Cass published research in 2010 and 2000 reporting that magnetic treatment causes water containing minerals to favor formation of a more soluble form of calcium carbonate (aragonite rather than calcite).\n\nThe effect of magnetic treatment depends on properties of the pipe. The magnititude of the effect depends on pipe conductivity and surface roughness.\n\nThere are related non-chemical devices based on a variety of physical phenomena which have been marketed for over 50 years with similar claims of scale inhibition. Whilst some are effective, such as electrolytic devices, most do not work.\nOther uses of magnetic devices:\n\n"}
{"id": "50803130", "url": "https://en.wikipedia.org/wiki?curid=50803130", "title": "Marie-Therese Mackowsky", "text": "Marie-Therese Mackowsky\n\nMarie-Therese Mackowsky (December 7, 1913 - August 4, 1986) was a German mineralogist known for her research in coal geology and her prolific writing.\n\nMackowsky was born in Koblenz and was educated at the University of Freiburg, the University of Königsberg, and the University of Bonn. She earned her doctorate in 1938 with a dissertation on the properties of garnet, and began working at the Syndicate for Mining Interests in 1940. She was promoted to its director of the section of mineralogy and petrology and retired at the end of 1978, having spent her entire career at the Syndicate. She began her teaching career as a visiting professor in 1957, supervising doctoral students and teaching coal petrography and technical mineralogy at the University of Münster. Throughout her career, she published 107 scientific pieces and translated extensively.\n\n"}
{"id": "10611092", "url": "https://en.wikipedia.org/wiki?curid=10611092", "title": "Nonextensive entropy", "text": "Nonextensive entropy\n\nEntropy is considered to be an extensive property, i.e., that its value depends on the amount of material present. Constantino Tsallis has proposed a nonextensive entropy (Tsallis entropy), which is a generalization of the traditional Boltzmann–Gibbs entropy.\n\nThe rationale behind the theory is that Gibbs-Boltzmann entropy leads to systems that have a strong dependence on initial conditions. In reality most materials behave quite independently of initial conditions.\n\nNonextensive entropy leads to nonextensive statistical mechanics, whose typical functions are power laws, instead of the traditional exponentials.\n\n"}
{"id": "3583044", "url": "https://en.wikipedia.org/wiki?curid=3583044", "title": "OTS 44", "text": "OTS 44\n\nOTS 44 is a free-floating planetary-mass object or brown dwarf located at in the constellation Chamaeleon. It is among the lowest-mass free-floating substellar objects, with approximately 11.5 times the mass of Jupiter, or approximately 1.1% that of the Sun.\nIts radius is not very well known and is estimated to be 23–57% that of the Sun.\n\nOTS 44 was discovered in 1998 by Oasa, Tamura, and Sugitani as a member of the star-forming region Chamaeleon I. Based upon infrared observations with the Spitzer Space Telescope and the Herschel Space Observatory, OTS 44 emits an excess of infrared radiation for an object of its type, suggesting it has a circumstellar disk of dust and particles of rock and ice. This disk has a mass of at least 10 Earth masses.\nObservations with the SINFONI spectrograph at the Very Large Telescope show that the disk\nis accreting matter at the rate of approximately 10 of the mass of the Sun per year. It could eventually develop into a planetary system.\n\n\n"}
{"id": "43072081", "url": "https://en.wikipedia.org/wiki?curid=43072081", "title": "Opitutus terrae", "text": "Opitutus terrae\n\nOpitutus terrae is an obligately anaerobic (can survive in absence of air) bacteria first isolated from rice paddy soil, hence its epithet. It is coccus-shaped and is motile by means of a flagellum. Its type strain is PB90-1 (= DSM 11246). Its genome has been sequenced.\n\n\n"}
{"id": "36435581", "url": "https://en.wikipedia.org/wiki?curid=36435581", "title": "Organization of Behavior", "text": "Organization of Behavior\n\nOrganization of Behavior is a 1949 book by the psychologist Donald O. Hebb, in which the author introduces his theory about the neural bases of learning, which is now commonly known as \"Hebb's postulate\".\n\nHebb proposes that whenever conditioned reflexes are established in an organism through learning, a new anatomical substratum is established in the brain through a physiological process in which weak or non-existent synapses are strengthened by biochemical modification or by permanent changes in their electrical properties. Learning, according to Hebb's hypothesis, is not simply something impressive upon a passive brain, but a process in which the cellular structure of the brain is permanently modified.\n\n\"Organization of Behavior\" is the most influential outline of Hebb's hypothesis. Richard Webster comments in \"Why Freud Was Wrong\" (1995) that the hypothesis has classic status within science and is supported by recent research.\n\n"}
{"id": "57012211", "url": "https://en.wikipedia.org/wiki?curid=57012211", "title": "Ornatilinea", "text": "Ornatilinea\n\nOrnatilinea is a bacteria genus from the family of Anaerolineaceae with one known species (\"Ornatilinea apprima\"). \"Ornatilinea apprima\" has been isolated from microbial mat from a anaerobic sludge blanket reactor from the Tomsk Region in Russia.\n"}
{"id": "1607154", "url": "https://en.wikipedia.org/wiki?curid=1607154", "title": "Proton conductor", "text": "Proton conductor\n\nA proton conductor is an electrolyte, typically a solid electrolyte, in which H are the primary charge carriers.\n\nAcid solutions exhibit proton-conductivity, but for practical applications, proton conductors are usually dry solids. Typical materials are polymers or ceramic. Typically, the pores in practical materials are small such that protons dominate direct current and transport of cations or bulk solvent is prevented. Water ice is a common example of a proton conductor, albeit a relatively poor one.\n\nSolid-phase proton conduction was first suggested by Alfred Rene Jean Paul Ubbelohde and S. E. Rogers. in 1950., although electrolyte proton currents have been recognized since 1806.\n\nProton conduction has also been observed in the new type of proton conductors for fuel cells - protic organic ionic plastic crystals (POIPCs), such as 1,2,4-triazolium perfluorobutanesulfonate and imidazolium methanesulfonate. In particular, a high ionic conductivity of 10 mS/cm is reached at 185 °C in the plastic phase of imidazolium methanesulfonate.\n\nWhen in the form of thin membranes, proton conductors are an essential part of small, inexpensive fuel cells. The polymer nafion is a typical proton conductor in fuel cells. A jelly-like substance similar to nafion residing in the ampullae of Lorenzini of sharks has proton conductivity only slightly lower than nafion.\n\nHigh proton conductivity has been reported among alkaline-earth cerates and zirconate based perovskite materials such as acceptor doped SrCeO, BaCeO and BaZrO.\nRelatively high proton conductivity has also been found in rare-earth ortho-niobates and ortho-tantalates as well as rare-earth tungstates.\n"}
{"id": "14516153", "url": "https://en.wikipedia.org/wiki?curid=14516153", "title": "Scottish government economy directorates", "text": "Scottish government economy directorates\n\nThe Scottish government economy directorates were a set of directorates within the Scottish government, the executive arm of the devolved government of Scotland. The directorates were headed by Dr Andrew Goudie, who also acted as the chief economic adviser to the Scottish government (Goudie retired in 2011 and joined Strathclyde University). The directorates were responsible for transport, sustainable development and planning. In 2010, the transport directorate merged with Transport Scotland, an Executive Agency of the Scottish Government and accountable to Scottish Ministers.\n"}
{"id": "2230989", "url": "https://en.wikipedia.org/wiki?curid=2230989", "title": "Space Shuttle thermal protection system", "text": "Space Shuttle thermal protection system\n\nThe Space Shuttle thermal protection system (TPS) is the barrier that protected the Space Shuttle Orbiter during the searing heat of atmospheric reentry. A secondary goal was to protect from the heat and cold of space while in orbit.\n\nThe TPS covered essentially the entire orbiter surface, and consisted of seven different materials in varying locations based on amount of required heat protection:\n\n\nEach type of TPS had specific heat protection, impact resistance, and weight characteristics, which determined the locations where it was used and the amount used.\n\nThe shuttle TPS has three key characteristics that distinguish it from the TPS used on previous spacecraft:\n\n\nThe orbiter's aluminum structure could not withstand temperatures over without structural failure.\nAerodynamic heating during reentry would push the temperature well above this level in areas, so an effective insulator was needed.\n\nReentry heating differs from the normal atmospheric heating associated with jet aircraft, and this governed TPS design and characteristics. The skin of high-speed jet aircraft can also become hot, but this is from frictional heating due to atmospheric friction, similar to warming one's hands by rubbing them together. The orbiter reentered the atmosphere as a blunt body by having a very high (40-degree) angle of attack, with its broad lower surface facing the direction of flight. Over 80% of the heating the orbiter experiences during reentry is caused by compression of the air ahead of the hypersonic vehicle, in accordance with the basic thermodynamic relation between pressure and temperature. A hot shock wave was created in front of the vehicle, which deflected most of the heat and prevented the orbiter's surface from directly contacting the peak heat. Therefore reentry heating was largely convective heat transfer between the shock wave and the orbiter's skin through superheated plasma. The key to a reusable shield against this type of heating is very low-density material, similar to how a thermos bottle inhibits convective heat transfer.\n\nSome high temperature metal alloys can withstand reentry heat; they simply get hot and re-radiate the absorbed heat. This technique, called \"heat sink\" thermal protection, was planned for the X-20 Dyna-Soar winged space vehicle. However, the amount of high-temperature metal required to protect a large vehicle like the Space Shuttle Orbiter would have been very heavy and entailed a severe penalty to the vehicle's performance. Similarly, ablative TPS would be heavy, possibly disturb vehicle aerodynamics as it burned off during reentry, and require significant maintenance to reapply after each mission. (Unfortunately, TPS tile, which was originally specified never to take debris strikes during launch, in practice also needed to be closely inspected and repaired after each landing, due to damage invariably incurred during ascent, even before new on-orbit inspection policies were established following the loss of Space Shuttle \"Columbia\".)\n\nThe TPS was a system of different protection types, not just silica tiles. They are in two basic categories: tile TPS and non-tile TPS. The main selection criteria used the lightest weight protection capable of handling the heat in a given area. However in some cases a heavier type was used if additional impact resistance was needed. The FIB blankets were primarily adopted for reduced maintenance, not for thermal or weight reasons.\n\nMuch of the shuttle was covered with LI-900 silica tiles, made from essentially very pure quartz sand. The insulation prevented heat transfer to the underlying orbiter aluminum skin and structure. These tiles were such poor heat conductors that one could hold one by the edges while it was still red hot.\nThere were about 24,300 unique tiles individually fitted on the vehicle, for which the orbiter has been called \"the flying brickyard\". Researchers at University of Minnesota and Pennsylvania State University are performing the atomistic simulations to obtain accurate description of interactions between atomic and molecular oxygen with silica surfaces to develop better high-temperature oxidation-protection systems for leading edges on hypersonic vehicles.\n\nThe tiles were not mechanically fastened to the vehicle, but glued. Since the brittle tiles could not flex with the underlying vehicle skin, they were glued to Nomex felt Strain Isolation Pads (SIPs) with RTV silicone adhesive, which were in turn glued to the orbiter skin. These isolated the tiles from the orbiter's structural deflections and expansions.\n\nHRSI tiles (black in color) provided protection against temperatures up to . There were 20,548 HRSI tiles which covered the landing gear doors, external tank umbilical connection doors, and the rest of the orbiter's under surfaces. They were also used in areas on the upper forward fuselage, parts of the orbital maneuvering system pods, vertical stabilizer leading edge, elevon trailing edges, and upper body flap surface. They varied in thickness from , depending upon the heat load encountered during reentry. Except for closeout areas, these tiles were normally square. The HRSI tile was composed of high purity silica fibers. Ninety percent of the volume of the tile was empty space, giving it a very low density () making it light enough for spaceflight. The uncoated tiles were bright white in appearance and looked more like a solid ceramic than the foam-like material that they were.\n\nThe black coating on the tiles was Reaction Cured Glass (RCG) of which tetrasilicide and borosilicate glass were some of several ingredients. RCG was applied to all but one side of the tile to protect the porous silica and to increase the heat sink properties. The coating was absent from a small margin of the sides adjacent to the uncoated (bottom) side. To waterproof the tile, dimethylethoxysilane was injected into the tiles by syringe. Densifying the tile with tetraethyl orthosilicate (TEOS) also helped to protect the silica and added additional waterproofing.\nAn uncoated HRSI tile held in the hand feels like a very light foam, less dense than styrofoam, and the delicate, friable material must be handled with extreme care to prevent damage. The coating feels like a thin, hard shell and encapsulates the white insulating ceramic to resolve its friability, except on the uncoated side. Even a coated tile feels very light, lighter than a same-sized block of styrofoam. As expected for silica, they are odorless and inert.\n\nHRSI was primarily designed to withstand transition from areas of extremely low temperature (the void of space, about ) to the high temperatures of re-entry (caused by interaction, mostly compression at the hypersonic shock, between the gases of the upper atmosphere & the hull of the Space Shuttle, typically around ).\n\nThe black FRCI tiles provided improved durability, resistance to coating cracking and weight reduction. Some HRSI tiles were replaced by this type.\n\nA stronger, tougher tile which came into use in 1996. TUFI tiles came in high temperature black versions for use in the orbiter's underside, and lower temperature white versions for use on the upper body. While more impact resistant than other tiles, white versions conducted more heat which limited their use to the orbiter's upper body flap and main engine area. Black versions had sufficient heat insulation for the orbiter underside but had greater weight. These factors restricted their use to specific areas.\n\nWhite in color, these covered the upper wing near the leading edge. They were also used in selected areas of the forward, mid, and aft fuselage, vertical tail, and the OMS/RCS pods. These tiles protected areas where reentry temperatures are below . The LRSI tiles were manufactured in the same manner as the HRSI tiles, except that the tiles were square and had a white RCG coating made of silica compounds with shiny aluminum oxide. The white color was by design and helped to manage heat on orbit when the orbiter was exposed to direct sunlight.\n\nThese tiles were reusable for up to 100 missions with refurbishment (100 missions was also the design lifetime of each orbiter). They were carefully inspected in the Orbiter Processing Facility after each mission, and damaged or worn tiles were immediately replaced before the next mission. Fabric sheets known as gap fillers were also inserted between tiles where necessary. These allowed for a snug fit between tiles, preventing excess plasma from penetrating between them, yet allowing for thermal expansion and flexing of the underlying vehicle skin.\n\nPrior to the introduction of FIB blankets, LRSI tiles occupied all of the areas now covered by the blankets, including the upper fuselage and the whole surface of the OMS pods. This TPS configuration was only used on \"Columbia\" and \"Challenger\".\n\nDeveloped after the initial delivery of \"Columbia\" and first used on the OMS pods of \"Challenger\". This white low-density fibrous silica batting material had a quilt-like appearance, and replaced the vast majority of the LRSI tiles. They required much less maintenance than LRSI tiles yet had about the same thermal properties. After their limited use on \"Challenger\", they were used much more extensively beginning with \"Discovery\" and replaced many of the LRSI tiles on \"Columbia\" after the loss of \"Challenger\".\n\nThe light gray material which withstood reentry temperatures up to protected the wing leading edges and nose cap. Each of the orbiters' wings had 22 RCC panels about thick. T-seals between each panel allowed for thermal expansion and lateral movement between these panels and the wing.\n\nRCC was a laminated composite material made from carbon fibres impregnated with a phenolic resin. After curing at high temperature in an autoclave, the laminate was pyrolized to convert the resin to pure carbon. This was then impregnated with furfural alcohol in a vacuum chamber, then cured and pyrolized again to convert the furfural alcohol to carbon. This process was repeated three times until the desired carbon-carbon properties were achieved.\n\nTo provide oxidation resistance for reuse capability, the outer layers of the RCC were coated with silicon carbide. The silicon-carbide coating protected the carbon-carbon from oxidation. The RCC was highly resistant to fatigue loading that was experienced during ascent and entry. It was stronger than the tiles and was also used around the socket of the forward attach point of the orbiter to the External Tank to accommodate the shock loads of the explosive bolt detonation. RCC was the only TPS material that also served as structural support for part of the orbiter's aerodynamic shape: the wing leading edges and the nose cap. All other TPS components (tiles and blankets) were mounted onto structural materials that supported them, mainly the aluminum frame and skin of the orbiter.\n\nThis white, flexible fabric offered protection at up to . FRSI covered the orbiter's upper wing surfaces, upper payload bay doors, portions of the OMS/RCS pods, and aft fuselage.\n\nGap fillers were placed at doors and moving surfaces to minimize heating by preventing the formation of vortices. Doors and moving surfaces created open gaps in the heat protection system that had to be protected from heat. Some of these gaps were safe, but there were some areas on the heat shield where surface pressure gradients caused a crossflow of boundary layer air in those gaps.\n\nThe filler materials were made of either white AB312 fibers or black AB312 cloth covers (which contain alumina fibers). These materials were used around the leading edge of the nose cap, windshields, side hatch, wing, trailing edge of elevons, vertical stabilizer, the rudder/speed brake, body flap, and heat shield of the shuttle's main engines.\n\nOn STS-114, some of this material was dislodged and determined to pose a potential safety risk. It was possible that the gap filler could cause turbulent airflow further down the fuselage, which would result in much higher heating, potentially damaging the orbiter. The cloth was removed during a spacewalk during the mission.\n\nWhile RCC had the best heat protection characteristics, it was also much heavier than the silica tiles and FIB blankets, so it was limited to relatively small areas. In general the goal was to use the lightest weight insulation consistent with the required thermal protection. Weight per unit volume of each TPS type:\n\n\nTotal area and weight of each TPS type (used on Orbiter 102) (pre-1996):\n\nTiles often fell off and caused much of the delay in the launch of STS-1, the first shuttle mission, which was originally scheduled for 1979 but did not occur until April 1981. NASA was unused to lengthy delays in its programs, and was under great pressure from the government and military to launch soon. In March 1979 it moved the incomplete \"Columbia\", with 7,800 of the 31,000 tiles missing, from the Rockwell International plant in Palmdale, California to Kennedy Space Center in Florida. Beyond creating the appearance of progress in the program, NASA hoped that the tiling could be finished while the rest of the orbiter was prepared. This was a mistake; some of the Rockwell tilers disliked Florida and soon returned to California, and the Orbiter Processing Facility was not designed for manufacturing and was too small for its 400 workers.\n\nEach tile used cement that required 16 hours to cure. After the tile was affixed to the cement, a jack held it in place for another 16 hours. In March 1979 it took each worker 40 hours to install one tile; by using young, efficient college students during the summer the pace sped up to 1.8 tiles per worker per week. Thousands of tiles failed stress tests and had to be replaced. By fall NASA realized that the speed of tiling would determine the launch date. The tiles were so problematic that officials would have switched to any other thermal protection method, but none other existed.\n\nBecause it had to be ferried without all tiles the gaps were filled with material to maintain the Shuttle's aerodynamics while in transit.\n\nThe tile TPS was an area of concern during shuttle development, mainly concerning adhesion reliability. Some engineers thought a failure mode could exist whereby one tile could detach, and resulting aerodynamic pressure would create a \"zipper effect\" stripping off other tiles. Whether during ascent or reentry, the result would be disastrous.\n\nAnother problem was ice or other debris impacting the tiles during ascent. This had never been fully and thoroughly solved, as the debris had never been eliminated, and the tiles remained susceptible to damage from it. NASA's final strategy for mitigating this problem was to aggressively inspect for, assess, and address any damage that may occur, while on orbit and before reentry, in addition to on the ground between flights.\n\nThese concerns were sufficiently great that NASA did significant work developing an emergency-use tile repair kit which the STS-1 crew could use before deorbiting. By December 1979, prototypes and early procedures were completed, most of which involved equipping the astronauts with a special in-space repair kit and a jet pack called the Manned Maneuvering Unit, or MMU, developed by Martin Marietta.\n\nAnother element was a maneuverable work platform which would secure an MMU-propelled spacewalking astronaut to the fragile tiles beneath the orbiter. The concept used electrically-controlled adhesive cups which would lock the work platform into position on the featureless tile surface. About one year before the 1981 STS-1 launch, NASA decided the repair capability was not worth the additional risk and training, so discontinued development. There were unresolved problems with the repair tools and techniques; also further tests indicated the tiles were unlikely to come off. The first shuttle mission did suffer several tile losses, but they were in non-critical areas, and no \"zipper effect\" occurred.\n\nOn February 1, 2003, the Space Shuttle \"Columbia\" was destroyed on reentry due to a failure of the TPS. The investigation team found and reported that the probable cause of the accident was that during launch, a piece of foam debris punctured an RCC panel on the left wing's leading edge and allowed hot gases from the reentry to enter the wing and disintegrate the wing from within, leading to eventual loss of control and breakup of the shuttle.\n\nThe Space Shuttle's thermal protection system received a number of controls and modifications after the disaster. They were applied to the three remaining shuttles, \"Discovery\", \"Atlantis\" and \"Endeavour\" in preparation for subsequent mission launches into space.\n\nOn 2005's STS-114 mission, in which \"Discovery\" made the first flight to follow the \"Columbia\" accident, NASA took a number of steps to verify that the TPS was undamaged. The Orbiter Boom Sensor System, a new extension to the Remote Manipulator System, was used to perform laser imaging of the TPS to inspect for damage. Prior to docking with the International Space Station, \"Discovery\" performed a Rendezvous Pitch Maneuver, simply a 360° backflip rotation, allowing all areas of the vehicle to be photographed from ISS. Two gap fillers were protruding from the orbiter's underside more than the nominally allowed distance, and the agency cautiously decided it would be best to attempt to remove the fillers or cut them flush rather than risk the increased heating they would cause. Even though each one protruded less than , it was believed that leaving them could cause heating increases of 25% upon reentry.\n\nBecause the orbiter did not have any handholds on its underside (as they would cause much more trouble with reentry heating than the protruding gap fillers of concern), astronaut Stephen K. Robinson worked from the ISS's robotic arm, Canadarm2. Because the TPS tiles were quite fragile, there had been concern that anyone working under the vehicle could cause more damage to the vehicle than was already there, but NASA officials felt that leaving the gap fillers alone was a greater risk. In the event, Robinson was able to pull the gap fillers free by hand, and caused no damage to the TPS on \"Discovery\".\n\n, with the impending Space Shuttle retirement, NASA is donating TPS tiles to schools, universities, and museums for the cost of shipping; each. About 7000 tiles were available on a first-come, first-served basis, but limited to one each per institution.\n\n\n\n"}
{"id": "179132", "url": "https://en.wikipedia.org/wiki?curid=179132", "title": "Spacelab", "text": "Spacelab\n\nSpacelab was a reusable laboratory used on certain spaceflights flown by the Space Shuttle. The laboratory comprised multiple components, including a pressurized module, an unpressurized carrier and other related hardware housed in the Shuttle's cargo bay. The components were arranged in various configurations to meet the needs of each spaceflight.\n\nSpacelab components flew on a total of 32 Shuttle missions. Spacelab allowed scientists to perform experiments in microgravity in earth orbit. There was a variety of Spacelab-associated hardware, so a distinction can be made between the major Spacelab program missions with European scientists running missions in the Spacelab habitable module, missions running other Spacelab hardware experiments, and other STS missions that used some component of Spacelab hardware. There is some variation in counts of Spacelab missions, in part because there were different types of Spacelab missions with a large range in the amount of Spacelab hardware flown and the nature of each mission. There were at least 22 major Spacelab missions between 1983 and 1998, and Spacelab hardware was used on a number other missions, with some of the Spacelab pallets being flown as late as 2008.\n\nIn August 1973, NASA and ESRO (now European Space Agency or ESA) signed a Memorandum of Understanding to build a science laboratory for use on Space Shuttle flights. Construction of Spacelab was started in 1974 by the ERNO (subsidiary of VFW-Fokker GmbH, after merger with MBB named MBB/ERNO, and part of EADS SPACE Transportation since 2003). The first lab module, \"LM1\", was donated to NASA in exchange for flight opportunities for European astronauts. A second module, \"LM2\", was bought by NASA for its own use from ERNO.\n\nConstruction on the Spacelab modules began in 1974 by what then the company ERNO-VFW-Fokker.\n\nIn the early 1970s NASA shifted its focus from the Lunar missions to the Space Shuttle, and also space research. The NASA Administrator at the time moved the focus from a new space station to space laboratory for planned Space Shuttle. This would allow technologies for future space stations to be researched and harness the capabilites of the Space Shuttle for research.\n\nSpacelab was produced by a consortium of ten European countries including:\n\nIn addition to the laboratory module, the complete set also included five external pallets for experiments in vacuum built by British Aerospace (BAe) and a pressurized Igloo containing the subsystems needed for the pallet-only flight configuration operation. Eight flight configurations were qualified, though more could be assembled if needed.\n\nSpacelab consisted of a variety of interchangeable components, with the major one being manned laboratory that could be flown in Space Shuttle orbiter's bay and returned to Earth. However, the habitable module did not have to be flown to conduct a Spacelab-type mission and there was a variety of pallets and other hardware supporting space research. The Habitable module expanded the volume for astronauts to work in a shirt sleeve environment and had space for equipment racks and related support equipment. When the habitable module was not used, some of the support equipment for the pallets could be housed in the smaller \"Igloo\", a pressurized cylinder connected to the Space Shuttle orbiter crew area.\n\nSpacelab mission typically supported multiple experiments, and the Spacelab-1 mission had experiments in the fields of space plasma physics, solar physics, atmosphere physics, astronomy, and Earth observation. The selection of appropriate modules was part of mission planning for Spacelab Shuttle missions, and for example, a mission might need less habitable space and more pallets, or vice versa.\n\n\n\n"}
{"id": "24750056", "url": "https://en.wikipedia.org/wiki?curid=24750056", "title": "SuperFreakonomics", "text": "SuperFreakonomics\n\nSuperFreakonomics: Global Cooling, Patriotic Prostitutes, and Why Suicide Bombers Should Buy Life Insurance is the second non-fiction book by University of Chicago economist Steven Levitt and \"The New York Times\" journalist Stephen J. Dubner, released in early October 2009 in Europe and on October 20, 2009 in the United States. It is a sequel to \"\".\n\nThe explanatory note states that the theme of the book explores the concept that we all work for a particular reward. The introduction states we should look at problems economically. The examples given include the preference for sons in India and the hardships Indian women face, as well as the horse manure issue at the turn of the 20th century.\n\nThe first chapter explores prostitution and pimps in South Chicago, one high class escort, and real estate brokers. The pimps and brokers are compared based on the idea that they are helping to sell one's services to the larger market. Inequalities in pay grades for men and women are also covered in the chapter.\n\nThe second chapter is about patterns and details. Patterns in the ages of soccer players, health issues of children in the womb during Ramadan, and the upbringings of terrorists are observed. Next, the book discusses the skills of hospital doctors and how Azyxxi was created, and draws parallels to how terrorists in the UK were tracked down by banks.\n\nAltruism is discussed in the third chapter, and uses examples of the murder of Kitty Genovese, crime rates as affected by television, and economic experimental games such as prisoner's dilemma, ultimatum, and the work of John A. List.\n\nThe fourth chapter is about unintended consequences and simple fixes. It goes into detail about Ignaz Semmelweis' work in hospitals, use of seatbelts and child seats, and the possibility of reducing hurricanes.\n\nThe fifth chapter discusses externalities and global warming. It discusses how economics does not necessarily take environmental issues into account. Further, the authors posit an alternative way of solving global warming by adding sulfur dioxide to the atmosphere.\n\nThe epilogue is about microeconomics, and discusses a study as to whether monkeys can be trained to use money.\n\n\"SuperFreakonomics\" has been praised for its entertainment value, but has drawn criticism for taking unconventional approaches to its subject matter, particularly global warming.\n\nIn the \"Financial Times\", Tim Harford, author of \"The Undercover Economist\", said that \"SuperFreakonomics\" \"is a lot like \"Freakonomics\", but better.\" In the \"New York Post\", critic and novelist Kyle Smith described the book as \"brave, bracing and beautifully contrarian\". \"BusinessWeek\" gave the book three and a half stars out of five, saying that the book is \"[a]n inventive and even useful application of economics to unusual subjects\".\nIn the book's fifth chapter, the author proposes that the climate system can be intentionally regulated by construction of a stratoshield.\n\nThe chapter has been criticized by economists and climate science experts who say it contains numerous misleading statements and discredited arguments, including this presentation of geoengineering as a replacement for emissions reduction. Among the critics are Paul Krugman, Brad DeLong, Raymond Pierrehumbert, \"The Guardian\", \"Bloomberg News\", and \"The Economist\". Elizabeth Kolbert, a science writer for \"The New Yorker\" who has written extensively on global warming, contends that \"just about everything they [Levitt and Dubner] have to say on the topic is, factually speaking, wrong.\"\n\nJoseph Romm said that \"SuperFreakonomics\" had seriously misrepresented the position of climate scientist Ken Caldeira; the book says among other things, that Caldeira's research tells him is \"not the right villain\", an assertion Caldeira strongly disputes. \n\nCaldeira has commented \nDubner responded that with hindsight the line describing Ken Caldeira overstated his position, but noted that Caldeira had been sent a preview of the text and had approved it.\n\nCaldeira has acknowledged that he did receive the preview, but disagreed that the errors were his responsibility: \"I feel no need to read, fact check, or make detailed comments on documents that arrive in my in-box. I have lots of other things to do, like trying to get my science out the door.\"\nAs it transpired, Dubner had been apprised of Caldeira's objection to the \"right villain\" assertion, but did not delete the line as Caldeira expected, although Caldeira believes this was due to a good faith misunderstanding.\n\nRegarding their own views on global warming, Levitt and Dubner have stated on their \"Freakonomics\" blog that global warming is \"a man-made phenomenon\" and \"an important issue to solve\". They go on to explain that where they differ with the establishment view is in what the most effective solutions to that problem are.\nFor an establishment review of how Levitt and Dubner use arithmetic in their dismissal of the solar photovoltaic option to mitigate global warming, see Raymond Pierrehumbert's open letter to Stephen Levitt at Realclimate.org.\n\n\n"}
{"id": "32821707", "url": "https://en.wikipedia.org/wiki?curid=32821707", "title": "Telmatobacter", "text": "Telmatobacter\n\nTelmatobacter is a genus of bacteria in the family Acidobacteriaceae.\n\n\"Telmatobacter\" was first described in 2011. The name derives from the Greek noun telma–atos, meaning swamp or bog, and the noun bacter meaning short rod.\n\n\"Telmatobacter\" is a genus of Gram-negative bacteria. They appear as motile, single rods. \"Telmatobacter\" reproduce by normal cell division and do not form spores. They are facultative anaerobes as well as chemo-organotrophs. They prefer to grow with sugars and pectin as growth substrates, although they are capable of fermenting sugars and several polysaccharides, including crystalline cellulose. They prefer acidic conditions and moderate temperatures, and they grow better on liquid medium than solid agar medium. Unlike some other bacteria, \"Telmatobacter\" do not produce hydrogen sulfide gas from thiosulfate, nor indole from tryptophan. Salt inhibits growth at concentrations above 0.1% (w/v). The DNA G+C content is 57.6%. \"Telmatobacter\" are found in acidic wetlands, specifically Sphagnum peat bogs. The only and type species is \"Telmatobacter bradus\".\n"}
{"id": "36069912", "url": "https://en.wikipedia.org/wiki?curid=36069912", "title": "The Mobile Wave", "text": "The Mobile Wave\n\nThe Mobile Wave: How Mobile Intelligence Will Change Everything is a 2012 nonfiction book by the technologist Michael J. Saylor, founder, chairman, and CEO of MicroStrategy, Inc. \"The Mobile Wave\" provides an analysis of then-current trends in mobile technology from the point of view of a scholar of the history of science. The book argues that mobile devices will become essential tools for life in the modern day, changing how businesses operate and how industries and economies are powered.\n\nIn \"The Mobile Wave: How Mobile Intelligence Will Change Everything\", Saylor argues that mobile technology will change how people live worldwide, affecting businesses, entire industries, and the very economies they power.\n\nSaylor has also been featured in Forbes and CNN for the ideas presented in his book.\n\nChapter 1 - The Wave: The Shape of the Wave lays the foundation for some of the themes that occur throughout the book. These include the universal nature of mobile devices, and how mobile and cloud computing technologies will continue to disrupt established industries and the old ways of doing business. This chapter also introduces the ongoing Information Revolution as the third great revolution to transform society, after the Agricultural and Industrial Revolutions.\n\nChapter 2 - Computers: The Evolution to Mobile Computing discusses the four great waves of computing described by technologists as leading to the fifth mobile wave: the mainframe, the minicomputer, the personal computer and the Internet PC. Saylor writes that a critical turning point unleashing the mobile wave was triggered by Apple, with the introduction of the affordable iPhone and its App Store, multi-touch capabilities and built-in GPS.\n\nChapter 3 - Paper: The Demise of Paper covers a short history of media delivery systems, ranging from the first clay tablets around 3000 B.C., up to the highly disruptive electronic content transforming the print industry. The chapter also discusses the benefits of electronic publishing, from distribution efficiencies that save money and protect the environment, to display format flexibility making the content more appealing.\n\nChapter 4 – Entertainment: The New Universal Screen describes how mobile technology has evolved to display movies, TV programming and video games. The impact of content mobility on advertisers is also discussed.\n\nChapter 5 – Wallet: A Smarter Wallet and Intelligent Money covers a short history of money and payment systems, and describes the effect the mobile wave is having on banking systems, including offering greater security.\n\nChapter 6 – Social Networks: A Mobile Social World discusses how evolving mobile/social applications are changing society, including their impact on events ranging from 2010’s unrest in the Middle East, to the social activities of today’s teenagers.\n\nChapter 7 – Medicine: The New Landscape of Global Healthcare describes the impact of mobile technology on the medical industry. Topics include the wide benefits of Telemedicine, improvements to medical record keeping, and technical advances to assist sight and hearing impaired people.\n\nChapter 8 – Education: Remaking Education for Everyone covers the positive disruption mobile technology brings to the education system. Expanded access to affordable education along with improved learning and retention using mobile content are shown as two ways the mobile wave is improving education.\n\nChapter 9 – Developing World: Bootstrapping the Developing World highlights the benefits of mobile technology on Third World countries. By facilitating an improved communications infrastructure, launching more secure, less corrupt financial transaction systems and enabling businesses to more easily sell their products where there’s higher demand, the developing world is already enjoying the benefits of the Mobile Wave.\n\nChapter 10 – New World: Human Energy Unleashed ties the book’s themes together again, including reiterating the transformative effects on business and society of mobile technology. The privacy issue is touched upon, along with a suggestion that perhaps an electronic bill of rights type document could be useful to help navigate these issues.\n\n\"The Mobile Wave\" has been featured on several book bestseller lists:\n\nIn \"USA Today\", reviewer Jeanne Destro writes, \n\"The Washingtonian\" review of the book cited Saylor's vision for the future:\n"}
{"id": "5501977", "url": "https://en.wikipedia.org/wiki?curid=5501977", "title": "Zero-order hold", "text": "Zero-order hold\n\nThe zero-order hold (ZOH) is a mathematical model of the practical signal reconstruction done by a conventional digital-to-analog converter (DAC). That is, it describes the effect of converting a discrete-time signal to a continuous-time signal by holding each sample value for one sample interval. It has several applications in electrical communication.\n\nA zero-order hold reconstructs the following continuous-time waveform from a sample sequence \"x\"[\"n\"], assuming one sample per time interval \"T\":\n\nThe function formula_3 is depicted in Figure 1, and formula_4 is the piecewise-constant signal depicted in Figure 2.\n\nThe equation above for the output of the ZOH can also be modeled as the output of a linear time-invariant filter with impulse response equal to a rect function, and with input being a sequence of dirac impulses scaled to the sample values. The filter can then be analyzed in the frequency domain, for comparison with other reconstruction methods such as the Whittaker–Shannon interpolation formula suggested by the Nyquist–Shannon sampling theorem, or such as the first-order hold or linear interpolation between sample values.\n\nIn this method, a sequence of dirac impulses, \"x\"(\"t\"), representing the discrete samples, \"x\"[\"n\"], is low-pass filtered to recover a continuous-time signal, \"x\"(\"t\").\n\nEven though this is \"not\" what a DAC does in reality, the DAC output can be modeled by applying the hypothetical sequence of dirac impulses, \"x\"(\"t\"), to a linear, time-invariant filter with such characteristics (which, for an LTI system, are fully described by the impulse response) so that each input impulse results in the correct constant pulse in the output.\n\nBegin by defining a continuous-time signal from the sample values, as above but using delta functions instead of rect functions:\n\nThe scaling by formula_6, which arises naturally by time-scaling the delta function, has the result that the mean value of \"x\"(\"t\") is equal to the mean value of the samples, so that the lowpass filter needed will have a DC gain of 1. Some authors use this scaling, while many others omit the time-scaling and the \"T\", resulting in a low-pass filter model with a DC gain of \"T\", and hence dependent on the units of measurement of time.\n\nThe zero-order hold is the hypothetical filter or LTI system that converts the sequence of modulated Dirac impulses \"x\"(\"t\")to the piecewise-constant signal (shown in Figure 2):\n\nresulting in an effective impulse response (shown in Figure 4) of:\n\nThe effective frequency response is the continuous Fourier transform of the impulse response.\n\nThe Laplace transform transfer function of the ZOH is found by substituting \"s\" = \"i\" 2 π \"f\":\n\nThe fact that practical digital-to-analog converters (DAC) do not output a sequence of dirac impulses, \"x\"(\"t\") (that, if ideally low-pass filtered, would result in the unique underlying bandlimited signal before sampling), but instead output a sequence of rectangular pulses, \"x\"(\"t\") (a piecewise constant function), means that there is an inherent effect of the ZOH on the effective frequency response of the DAC, resulting in a mild roll-off of gain at the higher frequencies (a 3.9224 dB loss at the Nyquist frequency, corresponding to a gain of sinc(1/2) = 2/π). This drop is a consequence of the \"hold\" property of a conventional DAC, and is \"not\" due to the sample and hold that might precede a conventional analog-to-digital converter (ADC).\n\n"}
{"id": "41302762", "url": "https://en.wikipedia.org/wiki?curid=41302762", "title": "Zoi Lygerou", "text": "Zoi Lygerou\n\nZoi Lygerou is a Greek associate professor of biology at the Medical School University of Patras whose works have been published in such journals as the European Journal of Biochemistry, Journal of Cell Science, the Molecular and Cellular Biology journal, Journal of Biological Chemistry, and both Science and Nature journals among others. In 1991 she got her first degree at the University of Athens followed by a Ph.D. from the University of Heidelberg four years later. She also used to be a postdoc at the Imperial Cancer Research Fund of London under guidance from Paul Nurse. On April 12, 1996 she and her colleagues (along with David Tollervey) have discovered that the minimum amount of enzymes are required for the eukaryotes' ribosomal RNA.\n"}
{"id": "47644551", "url": "https://en.wikipedia.org/wiki?curid=47644551", "title": "Zygomaticomaxillary complex fracture", "text": "Zygomaticomaxillary complex fracture\n\nThe zygomaticomaxillary complex fracture, also known as a quadripod fracture, quadramalar fracture, and formerly referred to as a tripod fracture or trimalar fracture, has four components: the lateral orbital wall (at either the zygomaticofrontal suture superiorly along the wall or zygomaticosphenoid suture inferiorly), separation of the maxilla and zygoma along the anterior maxilla (near the zygomaticomaxillary suture), the zygomatic arch, and the orbital floor near the infraorbital canal.\n\nOn physical exam, the fracture appears as a loss of cheek projection with increased width of the face. In most cases, there is loss of sensation in the cheek and upper lip due to infraorbital nerve injury. Facial bruising, periorbital ecchymosis, soft tissue gas, swelling, trismus, altered mastication, diplopia, and ophthalmoplegia are other indirect features of the injury. The zygomatic arch usually fractures at its weakest point, 1.5 cm behind the zygomaticotemporal suture.\n\nThe cause is usually a direct blow to the malar eminence of the cheek during assault. The paired zygomas each have two attachments to the cranium, and two attachments to the maxilla, making up the orbital floors and lateral walls. These complexes are referred to as the zygomaticomaxillary complex. The upper and transverse maxillary bone has the zygomaticomaxillary and zygomaticotemporal sutures, while the lateral and vertical maxillary bone has the zygomaticomaxillary and frontozygomatic sutures.\n\nThe formerly used 'tripod fracture' refers to these buttresses, but did not also incorporate the posterior relationship of the zygoma to the sphenoid bone at the zygomaticosphenoid suture.\n\nThere is an association of ZMC fractures with naso-orbito-ethmoidal fractures (NOE) on the same side as the injury. Concomitant NOE fractures predict a higher incidence of post operative deformity.\n\nNon-displaced or minimally displaced fractures may be treated conservatively. Open reduction and internal fixation is reserved for cases that are severely angulated or comminuted. The purpose of fixation is to restore the normal appearance of the face. Specific attention is given to the position of the malar eminence and reduction of orbital volume by realigning the zygoma and sphenoid. Failure to correct can result in rotational deformity and increase the volume of the orbit, causing the eye to sink inwards.\n\nFractures with displacement require surgery consisting of fracture reduction with miniplates, microplates and screws. Gillie's approach is used for depressed zygomatic fractures. The prognosis of tripod fractures is generally good. In some cases there may be persistent post-surgical facial asymmetry, which can require further treatment.\n"}
{"id": "28360272", "url": "https://en.wikipedia.org/wiki?curid=28360272", "title": "École de physique des Houches", "text": "École de physique des Houches\n\nL’École de Physique des Houches (the Physics School of Les Houches) was founded in 1951 by a young French scientist, Cécile DeWitt-Morette.\n\nHistorically the first lessons were given in 1951 by Léon van Hove on quantum mechanics. The conditions were very spartan with the lessons lasting eight weeks in alpine chalets devoid of all comforts, a few kilometers from the village of Les Houches.\n\nSoon, the school rapidly attracted the greatest names of modern physics, such as Enrico Fermi, Wolfgang Pauli, Murray Gell-Mann and John Bardeen amongst others. The young students, then unknown, included such future scientists as Pierre-Gilles de Gennes, Georges Charpak, and Claude Cohen-Tannoudji, all future winners of the Nobel prize for Physics, as well as mathematician Alain Connes, future winner of the Fields medal.\n\n\n\n\n\n\n\n"}
