{"id": "22685246", "url": "https://en.wikipedia.org/wiki?curid=22685246", "title": "ASC Purple", "text": "ASC Purple\n\nASC Purple was a supercomputer installed at the Lawrence Livermore National Laboratory in Livermore, CA. The computer was a collaboration between IBM Corporation and Lawrence Livermore Lab. Announced November 19th, 2002, it was installed in July 2005 and decommissioned on November 10th, 2010. The contract for this computer along with the Blue Gene/L supercomputer was worth US $290 million. As of November 2009, the computer ranked 66th on the TOP500 supercomputer list.\n\nIt was a redundant ring of POWER5 SMP servers. 196 of these machines were connected together. The system contained 12,544 POWER5 microprocessors in total with 50 terabytes of total memory and 2 petabytes of total disk storage. The system ran IBM's AIX 5L operating system. The computer consumed 7.5 MW of electricity, including cooling. It has a theoretical processing speed of 100 teraflops. \n\nIt was built as stage five of the Advanced Simulation and Computing Program (ASC) started by the U.S. Department of Energy and the National Nuclear Security Administration to build a simulator to replace live WMD testing following the moratorium on testing started by President George H. W. Bush in 1992 and extended by Bill Clinton in 1993.\n\n"}
{"id": "30408954", "url": "https://en.wikipedia.org/wiki?curid=30408954", "title": "Amit Goyal", "text": "Amit Goyal\n\nAmit Goyal is the Director of the multidisciplinary and interdisciplinary RENEW (Research & Education in Energy, Environment & Water) Institute at SUNY-Buffalo in Buffalo, New York. He is also Empire Innovation Professor at SUNY-Buffalo.\n\nPreviously he was a UT-Battelle Corporate Fellow, a Battelle Distinguished Inventor and an ORNL Distinguished Scientist at Oak Ridge National Laboratories in Tennessee. He was also the Chair of the UT-Battelle-ORNL Corporate Fellow Council.\n\nGoyal is one of the leading scientists worldwide in the field of advanced electronic and energy materials including High Temperature Superconductors. He has over 85 issued patents. He also has over 350 publications. In 2009, an analysis of citations and papers published worldwide in the last decade in the field of high-temperature superconductivity, between 1999–2009, conducted by Thomson Reuters Essential Science Indicators (ESI), ranked him as the most cited author worldwide during those years.\n\nHe is a member of the National Academy of Engineering and the National Academy of Inventors.\n\nGoyal did his schooling at Mayo College, in Ajmer, Rajasthan, India. He received the degree of Bachelor of Technology (B. Tech) Honors degree in Metallurgical Engineering from Indian Institute of Technology, Kharagpur in 1986. He completed his M.S. in Mechanical and Aerospace Engineering from the University of Rochester in 1988. He completed his Ph.D. in Material Science and Engineering from the same institute in 1991. He has an Executive MBA from the Krannert Business School at Purdue University, an executive International MBA from Tilburg University, The Netherlands and Executive business training from the MIT Sloan School of Management.\n\nThe University of Rochester, New York, awarded him a \"Distinguished Scholar Medal\" in 2006 and the Indian Institute of Technology awarded him the \"Distinguished Alumnus Award\" in 2009.\n\nGoyal joined SUNY-Buffalo as Director of the RENEW Institute and as Empire Innovation Professor in January 2015. Previously, Goyal was at the Oak Ridge National Laboratory from 1991 to December 2014. He works with the development of clean energy technologies, and in the field of electronic devices such as superconductors and photovoltaics. His research has contributed to the development of single crystal like behaviour in long lengths of superconducting materials, and of wires that allow high-temperature superconductors to allow very high performance to be obtained in a cost-effective manner. He has also made significant contributions to the fields of texture and grain boundary network control and to other electronic materials such as photovoltaics.\n\nIn 2011, Goyal received the inaugural E. O. Lawrence Award for Energy Science and Innovation. The E. O. Lawrence Award is given by the United States Energy Secretary on behalf of the President of the United States, for meritorious contributions to the development, use or control of atomic energy, and is officially awarded by the United States Secretary of Energy. It consists of a certificate, a gold medal and a cash prize of $20,000. The award was officially awarded in a ceremony on May 21, 2012. Goyal's award cites his \"pioneering research and transformative contributions to the field of applied high temperature superconductivity, including fundamental materials science advances and technical innovations enabling large-scale applications of these novel materials\". Goyal was invited by the Energy Secretary to give a special lecture associated with this award at USDOE. The lecture was live streamed and has been archived on Science Cinema and on YouTube.\n\nIn 2010, he received the 2010 R&D Magazine's Innovator of the Year Award.\n\nThe R&D magazine also awards the R&D100 awards to the 100 most innovative products introduced each year in any field. Goyal has received ten R&D 100 awards.\n\nThe NRI today listed him in the top 10 list of \"movers and shakers\" in 2010. He was also listed in top 50 Coolest Desis of 2010 by DesiClub.com.\n\nHe is the Founder, President & CEO of TapeSolar Inc, which develops solar cells. He is also the Founder, President & CEO of TexMat LLC, a Delaware-based intellectual property holding and consulting company.\n\n- Elected to National Academy of Engineering (2018) \n- Presidential Level, DOE's E. O. Lawrence Award in \"Energy Science & Innovation\" (2011) \n- Elected to National Academy of Inventors (2014) \n- R&D Magazine's Innovator-of-the-Year Award (2010) \n- World Technology Award in Advanced Materials (2012) \n- Ten R&D 100 Awards \n- DOE's Energy 100 Award \n- Lockheed-Martin's NOVA Award \n- Distinguished Scholar Award from the University of Rochester \n- Distinguished Alumnus Award from the Indian Institute of Technology \n- MIT's Global Indus Technovator Award (2005) \n- MIT's Technology Review Magazine's TR-100 Award (1999) \n- Fellow of NAI, MRS, APS, ACERS, ASM, IOP, WTN, WIF \n\nHe currently serves or has served on the Advisory Boards of NanoTech Briefs, the Journal of the Korean Institute of Applied Superconductivity, Recent Patents on Materials Science and Superconductor Science & Technology. He also serves on the Editorial Boards of Nature Magazine's Scientific Reports, the Journal of Materials Research and the Journal of the American Ceramic Society, and has served as Guest Editor for the TMS publication, Journal of Minerals, Metals and Materials (JOM). He has served as Chair of the Electronics Division of the American Ceramic Society.\n\nHe is a fellow of eight, prestigious, professional societies including the Materials Research Society, American Physical Society, the American Association for the Advancement of Science, the ASM International, the Institute of Physics, the American Ceramic Society, the World Innovation Foundation and the World Technology Network.\n\nHe is a member of the National Academy of Engineering (NAE) and the National Academy of Inventors (NAI).\n\n"}
{"id": "2033132", "url": "https://en.wikipedia.org/wiki?curid=2033132", "title": "Amodal perception", "text": "Amodal perception\n\nAmodal perception is the perception of the whole of a physical structure when only parts of it affect the sensory receptors. For example, a table will be perceived as a complete volumetric structure even if only part of it—the facing surface—projects to the retina; it is perceived as possessing internal volume and hidden rear surfaces despite the fact that only the near surfaces are exposed to view. Similarly, the world around us is perceived as a surrounding plenum, even though only part of it is in view at any time. Another much quoted example is that of the \"dog behind a picket fence\" in which a long narrow object (the dog) is partially occluded by fence-posts in front of it, but is nevertheless perceived as a single continuous object. Albert Bregman noted an auditory analogue of this phenomenon: when a melody is interrupted by bursts of white noise, it is nonetheless heard as a single melody continuing \"behind\" the bursts of noise.\n\nFormulation of the theory is credited to the Belgian psychologist Albert Michotte and Fabio Metelli, an Italian psychologist, with their work developed in recent years by E.S. Reed and the Gestaltists.\n\nModal completion is a similar phenomenon in which a shape is perceived to be occluding other shapes even when the shape itself is not drawn. Examples include the triangle that appears to be occluding three disks and an outlined triangle in the Kanizsa triangle and the circles and squares that appear in different versions of the Koffka cross.\n\n\n"}
{"id": "8087746", "url": "https://en.wikipedia.org/wiki?curid=8087746", "title": "Attribution (psychology)", "text": "Attribution (psychology)\n\nHumans are motivated to assign causes to their actions and behaviors. In social psychology, attribution is the process by which individuals explain the causes of behavior and events. Models to explain this process are called attribution theory. Psychological research into attribution began with the work of Fritz Heider in the early 20th century, and the theory was further advanced by Harold Kelley and Bernard Weiner.\n\nGestalt psychologist Fritz Heider is often described as the early-20th-century \"father of attribution theory\".\n\nIn his 1920s dissertation, Heider addressed the problem of phenomenology: why do perceivers attribute the properties such as color to perceived objects, when those properties are mental constructs? Heider's answer that perceivers attribute that which they \"directly\" sense – vibrations in the air for instance – to an object they construe as causing those sense data. \"Perceivers faced with sensory data thus see the perceptual object as 'out there', because they attribute the sensory data to their underlying causes in the world.\"\n\nHeider extended this idea to attributions about people: \"motives, intentions, sentiments ... the core processes which manifest themselves in overt behavior\".\n\nExternal attribution, also called situational attribution, refers to interpreting someone's behavior as being caused by the situation that the individual is in. For example, if Jacob's car tire is punctured he may attribute that to a hole in the road; by making attributions to the poor condition of the highway, he can make sense of the event without any discomfort that it may in reality have been the result of his bad driving.\n\nThe process of assigning the cause of behavior to some internal characteristic, rather than to outside forces.\nThis concept has overlap with the Locus of control, in which individuals feel they are personally responsible for everything that happens to them.\n\nFrom the book \"The Psychology of Interpersonal Relations\" (1958), Fritz Heider tried to explore the nature of interpersonal relationship, and espoused the concept of what he called \"common sense\" or \"naïve psychology\". In his theory, he believed that people observe, analyze, and explain behaviors with explanations. Although people have different kinds of explanations for the events of human behaviors, Heider found it is very useful to group explanation into two categories; Internal (personal) and external (situational) attributions. When an internal attribution is made, the cause of the given behavior is assigned to the individual's characteristics such as ability, personality, mood, efforts, attitudes, or disposition. When an external attribution is made, the cause of the given behavior is assigned to the situation in which the behavior was seen such as the task, other people, or luck (that the individual producing the behavior did so because of the surrounding environment or the social situation). These two types lead to very different perceptions of the individual engaging in a behavior.\n\nCorrespondent inferences state that people make inferences about a person when their actions are freely chosen, are unexpected, and result in a small number of desirable effects. According to Edward E. Jones and Keith Davis' correspondent inference theory, people make correspondent inferences by reviewing the context of behavior. It describes how people try to find out individual's personal characteristics from the behavioral evidence. People make inferences on the basis of three factors; degree of choice, expectedness of behavior, and effects of someone's behaviors. For example, we believe we can make stronger assumptions about a man who gives half of his money to charity, than we can about one who gives $5 to charity. An average person would not want to donate as much as the first man because they would lose a lot of money. By donating half of his money, it is easier for someone to figure out what the first man's personality is like. The second factor, that affects correspondence of action and inferred characteristic, is the number of differences between the choices made and the previous alternatives. If there aren't many differences, the assumption made will match the action because it is easy to guess the important aspect between each choice.\n\nThe covariation model states that people attribute behavior to the factors that are present when a behavior occurs and absent when it does not. Thus, the theory assumes that people make causal attributions in a rational, logical fashion, and that they assign the cause of an action to the factor that co-varies most closely with that action. Harold Kelley's covariation model of attribution looks to three main types of information from which to make an attribution decision about an individual's behavior. The first is \"consensus information\", or information on how other people in the same situation and with the same stimulus behave. The second is \"distinctive information\", or how the individual responds to different stimuli. The third is \"consistency information\", or how frequent the individual's behavior can be observed with similar stimulus but varied situations. From these three sources of information observers make attribution decisions on the individual's behavior as either internal or external. There have been claims that people under-utilise consensus information, although there has been some dispute over this.\n\nThere are several levels in the covariation model: high and low. Each of these levels influences the three covariation model criteria. High consensus is when many people can agree on an event or area of interest. Low consensus is when very few people can agree. High distinctiveness is when the event or area of interest is very unusual, whereas low distinctness is when the event or area of interest is fairly common. High consistency is when the event or area of interest continues for a length of time and low consistency is when the event or area of interest goes away quickly.\n\nBernard Weiner proposed that individuals have initial affective responses to the potential consequences of the intrinsic or extrinsic motives of the actor, which in turn influence future behavior. That is, a person's own perceptions or attributions as to why they succeeded or failed at an activity determine the amount of effort the person will engage in activities in the future. Weiner suggests that individuals exert their attribution search and cognitively evaluate casual properties on the behaviors they experience. When attributions lead to positive affect and high expectancy of future success, such attributions should result in greater willingness to approach to similar achievement tasks in the future than those attributions that produce negative affect and low expectancy of future success. Eventually, such affective and cognitive assessment influences future behavior when individuals encounter similar situations.\n\nWeiner's achievement attribution has three categories:\n\nStability influences individuals' expectancy about their future; control is related with individuals' persistence on mission; causality influences emotional responses to the outcome of task.\n\nWhile people strive to find reasons for behaviors, they fall into many traps of biases and errors. As Fritz Heider says, \"our perceptions of causality are often distorted by our needs and certain cognitive biases\". The following are examples of attributional biases.\n\nThe fundamental attribution error describes the habit to misunderstand dispositional or personality-based explanations for behavior, rather than considering external factors. The fundamental attribution error is most visible when people explain and assume the behavior of others. For example, if a person is overweight, a person's first assumption might be that they have a problem with overeating or are lazy and not that they might have a medical reason for being heavier set. When evaluating others' behaviors, the situational context is often ignored in favor of the disposition of the actor to be the cause of an observed behavior. This is because when a behavior occurs attention is most often focused on the person performing the behavior. Thus, the individual is more salient than the environment and dispositional attributions are made more often than situational attributions to explain the behavior of others. However, when evaluating one's own behavior, the situational factors are often exaggerated when there is a negative outcome while dispositional factors are exaggerated when there is a positive outcome.\n\nThe core process assumptions of attitude construction models are mainstays of social cognition research and are not controversial—as long as we talk about \"judgment\". Once the particular judgment made can be thought of as a person's \"attitude\", however, construal assumptions elicit discomfort, presumably because they dispense with the intuitively appealing attitude concept.\n\nCulture bias is when someone makes an assumption about the behavior of a person based on their cultural practices and beliefs. People in individualist cultures, generally Anglo-America and Anglo-Saxon European societies, value individuals, personal goals, and independence. People in collectivist cultures see individuals as members of groups such as families, tribes, work units, and nations, and tend to value conformity and interdependence. In other words, working together and being involved as a group is more common in certain cultures that views each person as a part of the community. This cultural trait is common in Asia, traditional Native American societies, and Africa. Research shows that culture, either individualist or collectivist, affects how people make attributions.\n\nPeople from individualist cultures are more inclined to make fundamental-attribution error than people from collectivist cultures. Individualist cultures tend to attribute a person's behavior due to their internal factors whereas collectivist cultures tend to attribute a person's behavior to his external factors.\n\nResearch suggests that individualist cultures engage in self-serving bias more than do collectivist cultures, i.e. individualist cultures tend to attribute success to internal factors and to attribute failure to external factors. In contrast, collectivist cultures engage in the opposite of self-serving bias i.e. self-effacing bias, which is: attributing success to external factors and blaming failure on internal factors (the individual).\n\nPeople tend to attribute other people's behaviors to their dispositional factors while attributing own actions to situational factors. In the same situation, people's attribution can differ depending on their role as actor or observer. For example, when a person scores a low grade on a test, they find situational factors to justify the negative event such as saying that the teacher asked a question that he/she never went over in class. However, if another person scores poorly on a test, the person will attribute the results to internal factors such as laziness and inattentiveness in classes. The theory of the actor-observer bias was first developed by E. Jones and R. Nisbett in 1971, whose explanation for the effect was that when we observe other people, we tend to focus on the person, whereas when we are actors, our attention is focused towards situational factors. The actor/observer bias is used less frequently with people one knows well such as friends and family since one knows how his/her close friends and family will behave in certain situation, leading him/her to think more about the external factors rather than internal factors.\n\nDispositional attribution is a tendency to attribute people's behaviors to their dispositions; that is, to their personality, character, and ability.\nFor example, when a normally pleasant waiter is being rude to his/her customer, the customer may assume he/she has a bad temper. The customer, just by looking at the attitude that the waiter is giving him/her, instantly decides that the waiter is a bad person. The customer oversimplifies the situation by not taking into account all the unfortunate events that might have happened to the waiter which made him/her become rude at that moment. Therefore, the customer made dispositional attribution by attributing the waiter's behavior directly to his/her personality rather than considering situational factors that might have caused the whole \"rudeness\".\n\nSelf-serving bias is attributing dispositional and internal factors for success, while external and uncontrollable factors are used to explain the reason for failure. For example, if a person gets promoted, it is because of his/her ability and competence whereas if he/she does not get promoted, it is because his/her manager does not like him/her (external, uncontrollable factor). Originally, researchers assumed that self-serving bias is strongly related to the fact that people want to protect their self-esteem. However, an alternative information processing explanation is that when the outcomes match people's expectations, they make attributions to internal factors. For example, if you pass a test you believe it was because of your intelligence; when the outcome does not match their expectations, they make external attributions or excuses. Whereas if you fail a test, you would give an excuse saying that you did not have enough time to study. People also use defensive attribution to avoid feelings of vulnerability and to differentiate themselves from a victim of a tragic accident. An alternative version of the theory of self-serving bias states that the bias does not arise because people wish to protect their private self-esteem, but to protect their self-image (a self-presentational bias). This version of the theory would predict that people attribute their successes to situational factors, for fear that others will disapprove of them looking overly vain if they should attribute successes to themselves.\n\nFor example, it is suggested that coming to believe that \"good things happen to good people and bad things happen to bad people\" will reduce feelings of vulnerability . This belief would have side-effects of blaming the victim even in tragic situations. When a mudslide destroys several houses in a rural neighborhood, a person living in a more urban setting might blame the victims for choosing to live in a certain area or not building a safer, stronger house. Another example of attributional bias is optimism bias in which most people believe positive events happen to them more often than to others and that negative events happen to them less often than to others. For example, smokers on average believe they are less likely to get lung cancer than other smokers.\n\nThe defensive attribution hypothesis is a social psychological term referring to a set of beliefs held by an individual with the function of defending themselves from concern that they will be the cause or victim of a mishap. Commonly, defensive attributions are made when individuals witness or learn of a mishap happening to another person. In these situations, attributions of responsibility to the victim or harm-doer for the mishap will depend upon the severity of the outcomes of the mishap and the level of personal and situational similarity between the individual and victim. More responsibility will be attributed to the harm-doer as the outcome becomes more severe, and as personal or situational similarity decreases.\n\nAn example of defensive attribution is the just-world hypothesis, which is where \"good things happen to good people and bad things happen to bad people\". People believe in this in order to avoid feeling vulnerable to situations that they have no control over. However, this also leads to blaming the victim even in a tragic situation. When people hear someone died from a car accident, they decide that the driver was drunk at the time of the accident, and so they reassure themselves that an accident will never happen to them. Despite the fact there was no other information provided, people will automatically attribute that the accident was the driver's fault due to an internal factor (in this case, deciding to drive while drunk), and thus they would not allow it to happen to themselves.\n\nAnother example of defensive attribution is optimism bias, in which people believe positive events happen to them more often than to others and that negative events happen to them less often than to others. Too much optimism leads people to ignore some warnings and precautions given to them. For example, smokers believe that they are less likely to get lung cancer than other smokers.\n\nAttribution theory can be applied to juror decision making. Jurors use attributions to explain the cause of the defendant's intent and actions related to the criminal behavior. The attribution made (situational or dispositional) might affect a juror's punitiveness towards the defendant. When jurors attribute a defendant's behavior to dispositional attributions they tend to be more punitive and are more likely find a defendant guilty and to recommend a death sentence compared to a life sentence.\n\nAttribution theory has had a big application in clinical psychology. Abramson, Seligman, and Teasdale developed a theory of the depressive attributional style, claiming that individuals who tend to attribute their failures to internal, stable and global factors are more vulnerable to clinical depression. The Attributional Style Questionnaire (ASQ) has been developed to assess whether individuals have the depressogenic attributional style. However, the ASQ has been criticized, with some researchers preferring to use a technique called Content Analysis of Verbatim Explanation (CAVE) in which an individual's ordinary writings are analysed to assess whether s/he is vulnerable to the depressive attributional style.\n\nThe concept of learned helplessness emerged from animal research in which psychologists Martin Seligman and Steven F. Maier discovered that dogs classically conditioned to an electrical shock which they could not escape, subsequently failed to attempt to escape an avoidable shock in a similar situation. They argued that learned helplessness applied to human psychopathology. In particular, individuals who attribute negative outcomes to internal, stable and global factors reflect a view in which they have no control over their situation. It is suggested that this aspect of not attempting to better a situation exacerbates negative mood, and may lead to clinical depression and related mental illnesses.\n\nWhen people try to make attributions about another's behavior, their information focuses on the individual. Their perception of that individual is lacking most of the external factors which might affect the individual. The gaps tend to be skipped over and the attribution is made based on the perception information most salient. The most salient perceptual information dominates a person's perception of the situation.\n\nFor individuals making behavioral attributions about themselves, the situation and external environment are entirely salient, but their own body and behavior are less so. This leads to the tendency to make an external attribution in regard to their own behavior.\n\nAttribution theory has been criticised as being mechanistic and reductionist for assuming that people are rational, logical, and systematic thinkers. The fundamental attribution error, however, demonstrates that they are cognitive misers and motivated tactician. It also fails to address the social, cultural, and historical factors that shape attributions of cause. This has been addressed extensively by discourse analysis, a branch of psychology that prefers to use qualitative methods including the use of language to understand psychological phenomena. The linguistic categorization theory for example demonstrates how language influences our attribution style.\n\n"}
{"id": "27138453", "url": "https://en.wikipedia.org/wiki?curid=27138453", "title": "Biopôle", "text": "Biopôle\n\nBiopôle is a science park of 80,000 square metres destined to host companies whose main activity is in the life sciences field. While it is open to all therapeutic areas, the main focus is on developing innovative solutions in the fields of oncology, immunology and personalised medicine. Biopôle offers space for industrial research and development or administrative use.\n\nThe Biopôle is owned at 98% by the Canton of Vaud. It is located in Épalinges, to the north of Lausanne and is the first of its kind in French-speaking part of Switzerland. The Biopôle site is located at the intersection of the large Lausanne campus, complementing the University Hospital of Lausanne (CHUV), the Swiss Federal Institute of Technology in Lausanne (EPFL), the University of Lausanne and the major industrial concerns.\n\n"}
{"id": "8964665", "url": "https://en.wikipedia.org/wiki?curid=8964665", "title": "Category utility", "text": "Category utility\n\nCategory utility is a measure of \"category goodness\" defined in and . It attempts to maximize both the probability that two objects in the same category have attribute values in common, and the probability that objects from different categories have different attribute values. It was intended to supersede more limited measures of category goodness such as \"cue validity\" (; ) and \"collocation index\" . It provides a normative information-theoretic measure of the \"predictive advantage\" gained by the observer who possesses knowledge of the given category structure (i.e., the class labels of instances) over the observer who does \"not\" possess knowledge of the category structure. In this sense the motivation for the \"category utility\" measure is similar to the information gain metric used in decision tree learning. In certain presentations, it is also formally equivalent to the mutual information, as discussed below. A review of \"category utility\" in its probabilistic incarnation, with applications to machine learning, is provided in .\n\nThe probability-theoretic definition of \"category utility\" given in and is as follows:\n\nwhere formula_2 is a size-formula_3 set of formula_4-ary features, and formula_5 is a set of formula_6 categories. The term formula_7 designates the marginal probability that feature formula_8 takes on value formula_9, and the term formula_10 designates the category-conditional probability that feature formula_8 takes on value formula_9 \"given\" that the object in question belongs to category formula_13.\n\nThe motivation and development of this expression for \"category utility\", and the role of the multiplicand formula_14 as a crude overfitting control, is given in the above sources. Loosely , the term formula_15 is the expected number of attribute values that can be correctly guessed by an observer using a probability-matching strategy together with knowledge of the category labels, while formula_16 is the expected number of attribute values that can be correctly guessed by an observer the same strategy but without any knowledge of the category labels. Their difference therefore reflects the relative advantage accruing to the observer by having knowledge of the category structure.\n\nThe information-theoretic definition of \"category utility\" for a set of entities with size-formula_3 binary feature set formula_2, and a binary category formula_19 is given in as follows:\n\nwhere formula_21 is the prior probability of an entity belonging to the positive category formula_22 (in the absence of any feature information), formula_23 is the conditional probability of an entity having feature formula_8 given that the entity belongs to category formula_22, formula_26 is likewise the conditional probability of an entity having feature formula_8 given that the entity belongs to category formula_28, and formula_29 is the prior probability of an entity possessing feature formula_8 (in the absence of any category information).\n\nThe intuition behind the above expression is as follows: The term formula_31 represents the cost (in bits) of optimally encoding (or transmitting) feature information when it known that the objects to be described belong to category formula_22. Similarly, the term formula_33 represents the cost (in bits) of optimally encoding (or transmitting) feature information when it known that the objects to be described belong to category formula_28. The sum of these two terms in the brackets is therefore the weighted average of these two costs. The final term, formula_35, represents the cost (in bits) of optimally encoding (or transmitting) feature information when no category information is available. The value of the \"category utility\" will, in the above formulation, be negative (???).\n\nIt is mentioned in and that the category utility is equivalent to the mutual information. Here we provide a simple demonstration of the nature of this equivalence. Let us assume a set of entities each having the same formula_36 features, i.e., feature set formula_2, with each feature variable having cardinality formula_38. That is, each feature has the capacity to adopt any of formula_38 distinct values (which need \"not\" be ordered; all variables can be nominal); for the special case formula_40 these features would be considered \"binary\", but more generally, for any formula_38, the features are simply \"m-ary\". For our purposes, without loss of generality, we can replace feature set formula_42 with a single aggregate variable formula_43 that has cardinality formula_44, and adopts a unique value formula_45 corresponding to each feature combination in the Cartesian product formula_46. (Ordinality does \"not\" matter, because the mutual information is not sensitive to ordinality.) In what follows, a term such as formula_47 or simply formula_48 refers to the probability with which formula_43 adopts the particular value formula_50. (Using the aggregate feature variable formula_43 replaces multiple summations, and simplifies the presentation to follow.)\n\nWe assume also a single category variable formula_52, which has cardinality formula_53. This is equivalent to a classification system in which there are formula_53 non-intersecting categories. In the special case of formula_55 we have the two-category case discussed above. From the definition of mutual information for discrete variables, the mutual information formula_56 between the aggregate feature variable formula_43 and the category variable formula_52 is given by:\n\nwhere formula_48 is the prior probability of feature variable formula_43 adopting value formula_50, formula_63 is the marginal probability of category variable formula_52 adopting value formula_65, and formula_66 is the joint probability of variables formula_43 and formula_52 simultaneously adopting those respective values. In terms of the conditional probabilities this can be re-written (or defined) as\n\nIf we will rewrite the original definition of the category utility from above, with formula_19, we have\n\nThis equation clearly has the same form as the (blue) equation expressing the mutual information between the feature set and the category variable; the difference is that the sum formula_72 in the \"category utility\" equation runs over independent binary variables formula_2, whereas the sum formula_74 in the mutual information runs over \"values\" of the single formula_44-ary variable formula_43. The two measures are actually equivalent then \"only\" when the features formula_77, are \"independent\" (and assuming that terms in the sum corresponding to formula_78 are also added).\n\nLike the mutual information, the \"category utility\" is not sensitive to any \"ordering\" in the feature or category variable values. That is, as far as the \"category utility\" is concerned, the category set codice_1 is not qualitatively different from the category set codice_2 since the formulation of the \"category utility\" does not account for any ordering of the class variable. Similarly, a feature variable adopting values codice_3 is not qualitatively different from a feature variable adopting values codice_4. As far as the \"category utility\" or \"mutual information\" are concerned, \"all\" category and feature variables are \"nominal variables.\" For this reason, \"category utility\" does not reflect any \"gestalt\" aspects of \"category goodness\" that might be based on such ordering effects. One possible adjustment for this insensitivity to ordinality is given by the weighting scheme described in the article for mutual information.\n\nThis section provides some background on the origins of, and need for, formal measures of \"category goodness\" such as the \"category utility\", and some of the history that lead to the development of this particular metric.\n\nAt least since the time of Aristotle there has been a tremendous fascination in philosophy with the nature of concepts and universals. What kind of \"entity\" is a concept such as \"horse\"? Such abstractions do not designate any particular individual in the world, and yet we can scarcely imagine being able to comprehend the world without their use. Does the concept \"horse\" therefore have an independent existence outside of the mind? If it does, then what is the locus of this independent existence? The question of locus was an important issue on which the classical schools of Plato and Aristotle famously differed. However, they remained in agreement that universals \"did\" indeed have a mind-independent existence. There was, therefore, always a \"fact to the matter\" about which concepts and universals exist in the world.\n\nIn the late Middle Ages (perhaps beginning with Occam, although Porphyry also makes a much earlier remark indicating a certain discomfort with the status quo), however, the certainty that existed on this issue began to erode, and it became acceptable among the so-called nominalists and empiricists to consider concepts and universals as strictly mental entities or conventions of language. On this view of concepts—that they are purely representational constructs—a new question then comes to the fore: \"Why do we possess one set of concepts rather than another?\" What makes one set of concepts \"good\" and another set of concepts \"bad\"? This is a question that modern philosophers, and subsequently machine learning theorists and cognitive scientists, have struggled with for many decades.\n\nOne approach to answering such questions is to investigate the \"role\" or \"purpose\" of concepts in cognition. Thus, we ask: \"What are concepts good for in the first place?\" The answer provided by and many others is that classification (conception) is a precursor to \"induction\": By imposing a particular categorization on the universe, an organism gains the ability to deal with physically non-identical objects or situations in an identical fashion, thereby gaining substantial predictive leverage (;). As J.S. Mill puts it ,\n\nFrom this base, Mill reaches the following conclusion, which foreshadows much subsequent thinking about category goodness, including the notion of \"category utility\":\n\nOne may compare this to the \"category utility hypothesis\" proposed by : \"A category is useful to the extent that it can be expected to improve the ability of a person to accurately predict the features of instances of that category.\" Mill here seems to be suggesting that the best category structure is one in which object features (properties) are maximally informative about the object's class, and, simultaneously, the object class is maximally informative about the object's features. In other words, a useful classification scheme is one in which we can use category knowledge to accurately infer object properties, and we can use property knowledge to accurately infer object classes. One may also compare this idea to Aristotle's criterion of \"counter-predication\" for definitional predicates, as well as to the notion of concepts described in formal concept analysis.\n\nA variety of different measures have been suggested with an aim of formally capturing this notion of \"category goodness,\" the best known of which is probably the \"cue validity\". Cue validity of a feature formula_8 with respect to category formula_13 is defined as the conditional probability of the category given the feature (;;), formula_81, or as the deviation of the conditional probability from the category base rate (;), formula_82. Clearly, these measures quantify only inference from feature to category (i.e., \"cue validity\"), but not from category to feature, i.e., the \"category validity\" formula_83. Also, while the cue validity was originally intended to account for the demonstrable appearance of \"basic categories\" in human cognition—categories of a particular level of generality that are evidently preferred by human learners—a number of major flaws in the cue validity quickly emerged in this regard (;;, and others).\n\nOne attempt to address both problems by simultaneously maximizing both feature validity and category validity was made by in defining the \"collocation index\" as the product formula_84, but this construction was fairly \"ad hoc\" (see ). The \"category utility\" was introduced as a more sophisticated refinement of the cue validity, which attempts to more rigorously quantify the full inferential power of a class structure. As shown above, on a certain view the category utility is equivalent to the mutual information between the feature variable and the category variable. It has been suggested that categories having the greatest overall \"category utility\" are those that are not only those \"best\" in a normative sense, but also those human learners prefer to use, e.g., \"basic\" categories . Other related measures of category goodness are \"cohesion\" (;) and \"salience\" .\n\n\n\n"}
{"id": "147027", "url": "https://en.wikipedia.org/wiki?curid=147027", "title": "Chemical property", "text": "Chemical property\n\nA chemical property is any of a material's properties that becomes evident during, or after, a chemical reaction; that is, any quality that can be established only by changing a substance's chemical identity. Simply speaking, chemical properties cannot be determined just by viewing or touching the substance; the substance's internal structure must be affected greatly for its chemical properties to be investigated. When a substance goes under a chemical reaction, the properties will change drastically, resulting in chemical change. However, a catalytic property would also be a chemical property.\n\nChemical properties can be contrasted with physical properties, which can be discerned without changing the substance's structure. However, for many properties within the scope of physical chemistry, and other disciplines at the boundary between chemistry and physics, the distinction may be a matter of researcher's perspective. Material properties, both physical and chemical, can be viewed as supervenient; i.e., secondary to the underlying reality. Several layers of superveniency are possible.\n\nChemical properties can be used for building chemical classifications. They can also be useful to identify an unknown substance or to separate or purify it from other substances. Materials science will normally consider the chemical properties of a substance to guide its applications.\n\n\n"}
{"id": "11530507", "url": "https://en.wikipedia.org/wiki?curid=11530507", "title": "Convective temperature", "text": "Convective temperature\n\nThe convective temperature (CT or T) is the approximate temperature that air near the surface must reach for cloud formation without mechanical lift. In such case, cloud base begins at the convective condensation level (CCL), whilst with mechanical lifting, condensation begins at the lifted condensation level (LCL). Convective temperature is important to forecasting thunderstorm development.\n\n"}
{"id": "14656069", "url": "https://en.wikipedia.org/wiki?curid=14656069", "title": "Crown sprouting", "text": "Crown sprouting\n\nCrown sprouting is the ability of a plant to regenerate its shoot system after destruction (usually by fire) by activating dormant vegetative structures to produce regrowth from the root crown (the junction between the root and shoot portions of a plant). These dormant structures take the form of lignotubers or basal epicormic buds. Plant species that can accomplish crown sprouting are called crown resprouters (distinguishing them from stem or trunk resprouters) and, like them, are characteristic of fire-prone habitats such as chaparral.\n\nIn contrast to plant fire survival strategies that decrease the flammability of the plant, or by requiring heat to germinate, crown sprouting allows for the total destruction of the above ground growth. Crown sprouting plants typically have extensive root systems in which they store nutrients allowing them to survive during fires and sprout afterwards. Early researchers suggested that crown sprouting species might lack species genetic diversity; however, research on Gondwanan shrubland suggests that crown sprouting species have similar genetic diversity to seed sprouters. Some genera, such as \"Arctostaphylos\" and \"Ceanothus\", have species that are both resprouters and not, both adapted to fire.\n\nCalifornia Buckeye, \"Aesculus californica\", is an example of a western United States tree which can regenerate from its root crown after a fire event, but can also regenerate by seed.\n\n\n"}
{"id": "230885", "url": "https://en.wikipedia.org/wiki?curid=230885", "title": "DARPA LifeLog", "text": "DARPA LifeLog\n\nLifeLog was a project of the Information Processing Techniques Office of the Defense Advanced Research Projects Agency (DARPA) of the U.S. Department of Defense (DOD). According to its bid solicitation pamphlet, it was to be \"an ontology-based (sub)system that captures, stores, and makes accessible the flow of one person's experience in and interactions with the world in order to support a broad spectrum of associates/assistants and other system capabilities\". The objective of the LifeLog concept was \"to be able to trace the 'threads' of an individual's life in terms of events, states, and relationships\", and it has the ability to \"take in all of a subject's experience, from phone numbers dialed and e-mail messages viewed to every breath taken, step made and place gone\".\n\nLifeLog aimed to compile a massive electronic database of every activity and relationship a person engages in. This was to include credit card purchases, web sites visited, the content of telephone calls and e-mails sent and received, scans of faxes and postal mail sent and received, instant messages sent and received, books and magazines read, television and radio selections, physical location recorded via wearable GPS sensors, biomedical data captured through wearable sensors. The high level goal of this data logging was to identify \"preferences, plans, goals, and other markers of intentionality\".\n\nThe DARPA program was canceled in 2004 after criticism from civil libertarians concerning the privacy implications of the system. \n\nGenerically, the term \"lifelog\" or \"flog\" is used to describe a storage system that can automatically and persistently record and archive some informational dimension of an object's (object lifelog) or user's (user lifelog) life experience in a particular data category.\n\nNews reports in the media described LifeLog as the \"diary to end all diaries—a multimedia, digital record of everywhere you go and everything you see, hear, read, say and touch\". \n\nAccording to U.S. government officials, LifeLog is not connected with Total Information Awareness.\n\n\n"}
{"id": "9541", "url": "https://en.wikipedia.org/wiki?curid=9541", "title": "Design of experiments", "text": "Design of experiments\n\nThe design of experiments (DOE, DOX, or experimental design) is the design of any task that aims to describe or explain the variation of information under conditions that are hypothesized to reflect the variation. The term is generally associated with experiments in which the design introduces conditions that directly affect the variation, but may also refer to the design of quasi-experiments, in which natural conditions that influence the variation are selected for observation.\n\nIn its simplest form, an experiment aims at predicting the outcome by introducing a change of the preconditions, which is represented by one or more independent variables, also referred to as \"input variables\" or \"predictor variables.\" The change in one or more independent variables is generally hypothesized to result in a change in one or more dependent variables, also referred to as \"output variables\" or \"response variables.\" The experimental design may also identify control variables that must be held constant to prevent external factors from affecting the results. Experimental design involves not only the selection of suitable independent, dependent, and control variables, but planning the delivery of the experiment under statistically optimal conditions given the constraints of available resources. There are multiple approaches for determining the set of design points (unique combinations of the settings of the independent variables) to be used in the experiment.\n\nMain concerns in experimental design include the establishment of validity, reliability, and replicability. For example, these concerns can be partially addressed by carefully choosing the independent variable, reducing the risk of measurement error, and ensuring that the documentation of the method is sufficiently detailed. Related concerns include achieving appropriate levels of statistical power and sensitivity.\n\nCorrectly designed experiments advance knowledge in the natural and social sciences and engineering. Other applications include marketing and policy making.\n\nIn 1747, while serving as surgeon on HMS \"Salisbury\", James Lind carried out a systematic clinical trial to compare remedies for scurvy. This systematic clinical trial constitutes a type of DOE.\n\nLind selected 12 men from the ship, all suffering from scurvy. Lind limited his subjects to men who \"were as similar as I could have them,\" that is, he provided strict entry requirements to reduce extraneous variation. He divided them into six pairs, giving each pair different supplements to their basic diet for two weeks. The treatments were all remedies that had been proposed:\n\nThe citrus treatment stopped after six days when they ran out of fruit, but by that time one sailor was fit for duty while the other had almost recovered. Apart from that, only group one (cider) showed some effect of its treatment. The remainder of the crew presumably served as a control, but Lind did not report results from any control (untreated) group.\n\nA theory of statistical inference was developed by Charles S. Peirce in \"Illustrations of the Logic of Science\" (1877–1878) and \"A Theory of Probable Inference\" (1883), two publications that emphasized the importance of randomization-based inference in statistics.\n\nCharles S. Peirce randomly assigned volunteers to a blinded, repeated-measures design to evaluate their ability to discriminate weights.\nPeirce's experiment inspired other researchers in psychology and education, which developed a research tradition of randomized experiments in laboratories and specialized textbooks in the 1800s.\n\nCharles S. Peirce also contributed the first English-language publication on an optimal design for regression models in 1876. A pioneering optimal design for polynomial regression was suggested by Gergonne in 1815. In 1918, Kirstine Smith published optimal designs for polynomials of degree six (and less).\n\nThe use of a sequence of experiments, where the design of each may depend on the results of previous experiments, including the possible decision to stop experimenting, is within the scope of Sequential analysis, a field that was pioneered by Abraham Wald in the context of sequential tests of statistical hypotheses. Herman Chernoff wrote an overview of optimal sequential designs, while adaptive designs have been surveyed by S. Zacks. One specific type of sequential design is the \"two-armed bandit\", generalized to the multi-armed bandit, on which early work was done by Herbert Robbins in 1952.\n\nA methodology for designing experiments was proposed by Ronald Fisher, in his innovative books: \"The Arrangement of Field Experiments\" (1926) and \"The Design of Experiments\" (1935). Much of his pioneering work dealt with agricultural applications of statistical methods. As a mundane example, he described how to test the lady tasting tea hypothesis, that a certain lady could distinguish by flavour alone whether the milk or the tea was first placed in the cup. These methods have been broadly adapted in the physical and social sciences, are still used in agricultural engineering and differ from the design and analysis of computer experiments.\n\n\n\n\n\n\n\nThis example is attributed to Harold Hotelling. It conveys some of the flavor of those aspects of the subject that involve combinatorial designs.\n\nWeights of eight objects are measured using a pan balance and set of standard weights. Each weighing measures the weight difference between objects in the left pan vs. any objects in the right pan by adding calibrated weights to the lighter pan until the balance is in equilibrium. Each measurement has a random error. The average error is zero; the standard deviations of the probability distribution of the errors is the same number σ on different weighings; errors on different weighings are independent. Denote the true weights by\n\nWe consider two different experiments:\n\n\nThe question of design of experiments is: which experiment is better?\n\nThe variance of the estimate \"X\" of θ is σ if we use the first experiment. But if we use the second experiment, the variance of the estimate given above is σ/8. Thus the second experiment gives us 8 times as much precision for the estimate of a single item, and estimates all items simultaneously, with the same precision. What the second experiment achieves with eight would require 64 weighings if the items are weighed separately. However, note that the estimates for the items obtained in the second experiment have errors that correlate with each other.\n\nMany problems of the design of experiments involve combinatorial designs, as in this example and others.\n\nFalse positive conclusions, often resulting from the pressure to publish or the author's own confirmation bias, are an inherent hazard in many fields. A good way to prevent biases potentially leading to false positives in the data collection phase is to use a double-blind design. When a double-blind design is used, participants are randomly assigned to experimental groups but the researcher is unaware of what participants belong to which group. Therefore, the researcher can not affect the participants' response to the intervention. \nExperimental designs with undisclosed degrees of freedom are a problem. This can lead to conscious or unconscious \"p-hacking\": trying multiple things until you get the desired result. It typically involves the manipulation - perhaps unconsciously - of the process of statistical analysis and the degrees of freedom until they return a figure below the p<.05 level of statistical significance. So the design of the experiment should include a clear statement proposing the analyses to be undertaken. P-hacking can be prevented by preregistering researches, in which researchers have to send their data analysis plan to the journal they wish to publish their paper in before they even start their data collection, so no data manipulation is possible (https://osf.io). Another way to prevent this is taking the double-blind design to the data-analysis phase, where the data are sent to a data-analyst unrelated to the research who scrambles up the data so there is no way to know which participants belong to before they are potentially taken away as outliers.\n\nClear and complete documentation of the experimental methodology is also important in order to support replication of results.\n\nAn experimental design or randomized clinical trial requires careful consideration of several factors before actually doing the experiment. An experimental design is the laying out of a detailed experimental plan in advance of doing the experiment. Some of the following topics have already been discussed in the principles of experimental design section:\n\n\nThe independent variable of a study often has many levels or different groups. In a true experiment, researchers can have an experimental group, which is where their intervention testing the hypothesis is implemented, and a control group, which has all the same element as the experimental group, without the interventional element. Thus, when everything else except for one intervention is held constant, researchers can certify with some certainty that this one element is what caused the observed change. In some instances, having a control group is not ethical. This is sometimes solved using two different experimental groups. In some cases, independent variables cannot be manipulated, for example when testing the difference between two groups who have a different disease, or testing the difference between genders (obviously variables that would be hard or unethical to assign participants to). In these cases, a quasi-experimental design may be used.\n\nIn the pure experimental design, the independent (predictor) variable is manipulated by the researcher - that is - every participant of the research is chosen randomly from the population, and each participant chosen is assigned randomly to conditions of the independent variable. Only when this is done is it possible to certify with high probability that the reason for the differences in the outcome variables are caused by the different conditions. Therefore, researchers should choose the experimental design over other design types whenever possible. However, the nature of the independent variable does not always allow for manipulation. In those cases, researchers must be aware of not certifying about causal attribution when their design doesn't allow for it. For example, in observational designs, participants are not assigned randomly to conditions, and so if there are differences found in outcome variables between conditions, it is likely that there is something other than the differences between the conditions that causes the differences in outcomes, that is - a third variable. The same goes for studies with correlational design. (Adér & Mellenbergh, 2008).\n\nIt is best that a process be in reasonable statistical control prior to conducting designed experiments. When this is not possible, proper blocking, replication, and randomization allow for the careful conduct of designed experiments.\nTo control for nuisance variables, researchers institute control checks as additional measures. Investigators should ensure that uncontrolled influences (e.g., source credibility perception) do not skew the findings of the study. A manipulation check is one example of a control check. Manipulation checks allow investigators to isolate the chief variables to strengthen support that these variables are operating as planned.\n\nOne of the most important requirements of experimental research designs is the necessity of eliminating the effects of spurious, intervening, and antecedent variables. In the most basic model, cause (X) leads to effect (Y). But there could be a third variable (Z) that influences (Y), and X might not be the true cause at all. Z is said to be a spurious variable and must be controlled for. The same is true for intervening variables (a variable in between the supposed cause (X) and the effect (Y)), and anteceding variables (a variable prior to the supposed cause (X) that is the true cause). When a third variable is involved and has not been controlled for, the relation is said to be a zero order relationship. In most practical applications of experimental research designs there are several causes (X1, X2, X3). In most designs, only one of these causes is manipulated at a time.\n\nSome efficient designs for estimating several main effects were found independently and in near succession by Raj Chandra Bose and K. Kishen in 1940 at the Indian Statistical Institute, but remained little known until the Plackett–Burman designs were published in \"Biometrika\" in 1946. About the same time, C. R. Rao introduced the concepts of orthogonal arrays as experimental designs. This concept played a central role in the development of Taguchi methods by Genichi Taguchi, which took place during his visit to Indian Statistical Institute in early 1950s. His methods were successfully applied and adopted by Japanese and Indian industries and subsequently were also embraced by US industry albeit with some reservations.\n\nIn 1950, Gertrude Mary Cox and William Gemmell Cochran published the book \"Experimental Designs,\" which became the major reference work on the design of experiments for statisticians for years afterwards.\n\nDevelopments of the theory of linear models have encompassed and surpassed the cases that concerned early writers. Today, the theory rests on advanced topics in linear algebra, algebra and combinatorics.\n\nAs with other branches of statistics, experimental design is pursued using both frequentist and Bayesian approaches: In evaluating statistical procedures like experimental designs, frequentist statistics studies the sampling distribution while Bayesian statistics updates a probability distribution on the parameter space.\n\nSome important contributors to the field of experimental designs are C. S. Peirce, R. A. Fisher, F. Yates, C. R. Rao, R. C. Bose, J. N. Srivastava, Shrikhande S. S., D. Raghavarao, W. G. Cochran, O. Kempthorne, W. T. Federer, V. V. Fedorov, A. S. Hedayat, J. A. Nelder, R. A. Bailey, J. Kiefer, W. J. Studden, A. Pázman, F. Pukelsheim, D. R. Cox, H. P. Wynn, A. C. Atkinson, G. E. P. Box and G. Taguchi. The textbooks of D. Montgomery, R. Myers, and G. Box/W. Hunter/J.S. Hunter have reached generations of students and practitioners.\n\nSome discussion of experimental design in the context of system identification (model building for static or dynamic models) is given in and.\n\nLaws and ethical considerations preclude some carefully designed \nexperiments with human subjects. Legal constraints are dependent on \njurisdiction. Constraints may involve \ninstitutional review boards, informed consent \nand confidentiality affecting both clinical (medical) trials and \nbehavioral and social science experiments.\nIn the field of toxicology, for example, experimentation is performed \non laboratory \"animals\" with the goal of defining safe exposure limits \nfor \"humans\". Balancing\nthe constraints are views from the medical field. Regarding the randomization of patients, \n\"... if no one knows which therapy is better, there is no ethical \nimperative to use one therapy or another.\" (p 380) Regarding \nexperimental design, \"...it is clearly not ethical to place subjects \nat risk to collect data in a poorly designed study when this situation \ncan be easily avoided...\". (p 393)\n\n"}
{"id": "38635705", "url": "https://en.wikipedia.org/wiki?curid=38635705", "title": "Dijet event", "text": "Dijet event\n\nIn particle physics, a dijet event is a collision between subatomic particles that produces two particle jets.\n\nDijet events are measured at LHC to constrain QCD models, in particular the parton evolution equations and parton distribution functions. This is accomplished by measuring the azimuthal correlations between the two jets.\n"}
{"id": "18836338", "url": "https://en.wikipedia.org/wiki?curid=18836338", "title": "Electric Time Company", "text": "Electric Time Company\n\nElectric Time Company, founded in 1928, is a manufacturer of tower and street clocks marketed worldwide.\n\nElectric Time Company was founded in 1928 and incorporated in the state of Massachusetts in 1932. It was founded by a salesman for the Self Winding Clock Company and Telechron of Ashland, Massachusetts. Originally part of Telechron it progressed from an office in Boston, Massachusetts to manufacturing facilities in Wellesley, Massachusetts; South Natick, Massachusetts; and, currently, a manufacturing facility in Medfield, Massachusetts. \n\nElectric Time produces approximately 1000 clocks annually and is the largest maker of custom outdoor clocks in the United States. Electric Time produces tower clocks, post clocks, and bracket clocks. Electric Time has many clock installations throughout the world, ranging from Brazil to Turkey to Europe. Some notable installations are Wrigley Field, Tiffany & Co, Disneyworld, Disneyland and the Great American Ball Park.\n\nElectric Time has been featured on the documentary series \"How It's Made\" and the local Boston television news magazine \"Chronicle\". Electric Time was the basis for an article in \"The Wall Street Journal\" on Street Clocks.\n\n"}
{"id": "9804", "url": "https://en.wikipedia.org/wiki?curid=9804", "title": "Electric charge", "text": "Electric charge\n\nElectric charge is the physical property of matter that causes it to experience a force when placed in an electromagnetic field. There are two types of electric charges; \"positive\" and \"negative\" (commonly carried by protons and electrons respectively). Like charges repel and unlike attract. An object with an absence of net charge is referred to as \"\". Early knowledge of how charged substances interact is now called classical electrodynamics, and is still accurate for problems that do not require consideration of quantum effects. \n\nElectric charge is a conserved property; the net charge of an isolated system, the amount of positive charge minus the amount of negative charge, cannot change. Electric charge is carried by subatomic particles. In ordinary matter, negative charge is carried by electrons, and positive charge is carried by the protons in the nuclei of atoms. If there are more electrons than protons in a piece of matter, it will have a negative charge, if there are fewer it will have a positive charge, and if there are equal numbers it will be neutral. Charge is \"quantized\"; it comes in integer multiples of individual small units called the elementary charge, \"e\", about , which is the smallest charge which can exist free (particles called quarks have smaller charges, multiples of \"e\", but they are only found in combination, and always combine to form particles with integer charge). The proton has a charge of +\"e\", and the electron has a charge of −\"e\".\n\nElectric charges create an electric field, if they are moving they also generate a magnetic field. The combination of the electric and magnetic field is called the electromagnetic field, and its interaction with charges is the source of the electromagnetic force, which is one of the four fundamental forces in physics. The study of charged particles, and how their interactions are mediated by photons, is called quantum electrodynamics.\n\nThe SI derived unit of electric charge is the coulomb (C) named after French physicist Charles-Augustin de Coulomb. In electrical engineering, it is also common to use the ampere-hour (Ah); in physics and chemistry, it is common to use the elementary charge (\"e\" as a unit). Chemistry also uses the Faraday constant as the charge on a mole of electrons. The symbol \"Q\" often denotes charge.\n\nCharge is the fundamental property of forms of matter that exhibit electrostatic attraction or repulsion in the presence of other matter. Electric charge is a characteristic property of many subatomic particles. The charges of free-standing particles are integer multiples of the elementary charge \"e\"; we say that electric charge is \"quantized\". Michael Faraday, in his electrolysis experiments, was the first to note the discrete nature of electric charge. Robert Millikan's oil drop experiment demonstrated this fact directly, and measured the elementary charge. It has been discovered that one type of particle, quarks, have fractional charges of either − or +, but it is believed they always occur in multiples of integral charge; free-standing quarks have never been observed.\n\nBy convention, the charge of an electron is negative, \"−e\", while that of a proton is positive, \"+e\". Charged particles whose charges have the same sign repel one another, and particles whose charges have different signs attract. Coulomb's law quantifies the electrostatic force between two particles by asserting that the force is proportional to the product of their charges, and inversely proportional to the square of the distance between them. The charge of an antiparticle equals that of the corresponding particle, but with opposite sign.\n\nThe electric charge of a macroscopic object is the sum of the electric charges of the particles that make it up. This charge is often small, because matter is made of atoms, and atoms typically have equal numbers of protons and electrons, in which case their charges cancel out, yielding a net charge of zero, thus making the atom neutral.\n\nAn \"ion\" is an atom (or group of atoms) that has lost one or more electrons, giving it a net positive charge (cation), or that has gained one or more electrons, giving it a net negative charge (anion). \"Monatomic ions\" are formed from single atoms, while \"polyatomic ions\" are formed from two or more atoms that have been bonded together, in each case yielding an ion with a positive or negative net charge.\nDuring formation of macroscopic objects, constituent atoms and ions usually combine to form structures composed of neutral \"ionic compounds\" electrically bound to neutral atoms. Thus macroscopic objects tend toward being neutral overall, but macroscopic objects are rarely perfectly net neutral.\n\nSometimes macroscopic objects contain ions distributed throughout the material, rigidly bound in place, giving an overall net positive or negative charge to the object. Also, macroscopic objects made of conductive elements, can more or less easily (depending on the element) take on or give off electrons, and then maintain a net negative or positive charge indefinitely. When the net electric charge of an object is non-zero and motionless, the phenomenon is known as static electricity. This can easily be produced by rubbing two dissimilar materials together, such as rubbing amber with fur or glass with silk. In this way non-conductive materials can be charged to a significant degree, either positively or negatively. Charge taken from one material is moved to the other material, leaving an opposite charge of the same magnitude behind. The law of \"conservation of charge\" always applies, giving the object from which a negative charge is taken a positive charge of the same magnitude, and vice versa.\n\nEven when an object's net charge is zero, charge can be distributed non-uniformly in the object (e.g., due to an external electromagnetic field, or bound polar molecules). In such cases the object is said to be polarized. The charge due to polarization is known as bound charge, while charge on an object produced by electrons gained or lost from outside the object is called \"free charge\". The motion of electrons in conductive metals in a specific direction is known as electric current.\n\nThe SI derived unit of quantity of electric charge is the coulomb (symbol: C). The coulomb is defined as the quantity of charge that passes through the cross section of an electrical conductor carrying one ampere for one second. This unit was proposed in 1946 and ratified in 1948. In modern practice, the phrase \"amount of charge\" is used instead of \"quantity of charge\". The amount of charge in 1 electron (elementary charge) is approximately , and 1 coulomb corresponds to the amount of charge for about . The symbol \"Q\" is often used to denote a quantity of electricity or charge. The quantity of electric charge can be directly measured with an electrometer, or indirectly measured with a ballistic galvanometer.\n\nAfter finding the quantized character of charge, in 1891 George Stoney proposed the unit 'electron' for this fundamental unit of electrical charge. This was before the discovery of the particle by J. J. Thomson in 1897. The unit is today treated as nameless, referred to as , , or simply as . A measure of charge should be a multiple of the elementary charge \"e\", even if at large scales charge seems to behave as a real quantity. In some contexts it is meaningful to speak of fractions of a charge; for example in the charging of a capacitor, or in the fractional quantum Hall effect.\n\nThe unit faraday is sometimes used in electrochemistry. One faraday of charge is the magnitude of the charge of one mole of electrons, i.e. 96485.33289(59) C.\n\nIn systems of units other than SI such as cgs, electric charge is expressed as combination of only three fundamental quantities (length, mass, and time), and not four, as in SI, where electric charge is a combination of length, mass, time, and electric current.\n\nFrom ancient times, persons were familiar with four types of phenomena that today would all be explained using the concept of electric charge: (a) lightning, (b) the torpedo fish (or electric ray), (c) St Elmo's Fire, and (d) that amber rubbed with fur would attract small, light objects. The first account of the is often attributed to the ancient Greek mathematician Thales of Miletus, who lived from c. 624 – c. 546 BC, but there are doubts about whether Thales left any writings; his account about amber is known from an account from early 200s. This account can be taken as evidence that the phenomenon was known since at least c. 600 BC, but Thales explained this phenomenon as evidence for inanimate objects having a soul. In other words, there was no indication of any conception of electric charge. More generally, the ancient Greeks did not understand the connections among these four kinds of phenomena. The Greeks observed that the charged amber buttons could attract light objects such as hair. They also found that if they rubbed the amber for long enough, they could even get an electric spark to jump, but there is also a claim that no mention of electric sparks appeared until late 17th century. This property derives from the triboelectric effect.\nIn late 1100s, the substance jet, a compacted form of coal, was noted to have an amber effect, and in the middle of the 1500s, Girolamo Fracastoro, discovered that diamond also showed this effect. Some efforts were made by Fracastoro and others, especially Gerolamo Cardano to develop explanations for this phenomenon.\n\nIn contrast to astronomy, mechanics, and optics, which had been studied quantitatively since antiquity, the start of ongoing qualitative and quantitative research into electrical phenomena can be marked with the publication of \"De Magnete\" by the English scientist William Gilbert in 1600. In this book, there was a small section where Gilbert returned to the amber effect (as he called it) in addressing many of the earlier theories, and coined the New Latin word \"electrica\" (from (ēlektron), the Greek word for \"amber\"). The Latin word was translated into English as . Gilbert is also credited with the term \"electrical\", while the term \"electricity\" came later, first attributed to Sir Thomas Browne in his Pseudodoxia Epidemica from 1646. (For more linguistic details see Etymology of electricity.) Gilbert was followed in 1660 by Otto von Guericke, who invented what was probably the first electrostatic generator. Other European pioneers were Robert Boyle, who in 1675 stated that electric attraction and repulsion can act across a vacuum; Stephen Gray, who in 1729 classified materials as conductors and insulators. In 1733 Charles François de Cisternay du Fay, inspired by Gray's work, made a series of experiments (reported in \"Mémoires de l'Académie Royale des Sciences\"), showing that more or less all substances could be 'electrified' by rubbing, except for metals and fluids and proposed that electricity comes in two varieties that cancel each other, which he expressed in terms of a two-fluid theory. When glass was rubbed with silk, du Fay said that the glass was charged with \"vitreous electricity\", and, when amber was rubbed with fur, the amber was charged with \"resinous electricity\". Another important two-fluid theory from this time was proposed by Jean-Antoine Nollet (1745). In 1839, Michael Faraday showed that the apparent division between static electricity, current electricity, and bioelectricity was incorrect, and all were a consequence of the behavior of a single kind of electricity appearing in opposite polarities. It is arbitrary which polarity is called positive and which is called negative. Positive charge can be defined as the charge left on a glass rod after being rubbed with silk.\n\nOne of the foremost experts on electricity in the 18th century was Benjamin Franklin, who argued in favour of a one-fluid theory of electricity. Franklin imagined electricity as being a type of invisible fluid present in all matter; for example, he believed that it was the glass in a Leyden jar that held the accumulated charge. He posited that rubbing insulating surfaces together caused this fluid to change location, and that a flow of this fluid constitutes an electric current. He also posited that when matter contained too little of the fluid it was charged, and when it had an excess it was charged. For a reason that was not recorded, he identified the term with vitreous electricity and with resinous electricity. William Watson independently arrived at the same explanation at about the same time (1746).\n\nIt is now known that the Franklin–Watson model was fundamentally correct. There is only one kind of electrical charge, and only one variable is required to keep track of the amount of charge. On the other hand, just knowing the charge is not a complete description of the situation. Matter is composed of several kinds of electrically charged particles, and these particles have many properties, not just charge.\n\nAll bodies are electrified, but may appear not electrified because of the relatively similar charge of neighboring objects in the environment. An object further electrified + or – creates an equivalent or opposite charge by default in neighboring objects, until those charges can equalize. The effects of attraction can be observed in high-voltage experiments, while lower voltage effects are merely weaker and therefore less obvious. Coulomb's law has a corollary for acceleration in a gravitational field. See also Casimir effect.\n\nStatic electricity refers to the electric charge of an object and the related electrostatic discharge when two objects are brought together that are not at equilibrium. An electrostatic discharge creates a change in the charge of each of the two objects.\n\nWhen a piece of glass and a piece of resin—neither of which exhibit any electrical properties—are rubbed together and left with the rubbed surfaces in contact, they still exhibit no electrical properties. When separated, they attract each other.\n\nA second piece of glass rubbed with a second piece of resin, then separated and suspended near the former pieces of glass and resin causes these phenomena:\n\nThis attraction and repulsion is an \"electrical phenomenon\", and the bodies that exhibit them are said to be \"electrified\", or \"electrically charged\". Bodies may be electrified in many other ways, as well as by friction. The electrical properties of the two pieces of glass are similar to each other but opposite to those of the two pieces of resin: The glass attracts what the resin repels and repels what the resin attracts.\n\nIf a body electrified in any manner whatsoever behaves as the glass does, that is, if it repels the glass and attracts the resin, the body is said to be \"vitreously\" electrified, and if it attracts the glass and repels the resin it is said to be \"resinously\" electrified. All electrified bodies are either vitreously or resinously electrified.\n\nAn established convention in the scientific community defines vitreous electrification as positive, and resinous electrification as negative. The exactly opposite properties of the two kinds of electrification justify our indicating them by opposite signs, but the application of the positive sign to one rather than to the other kind must be considered as a matter of arbitrary convention—just as it is a matter of convention in mathematical diagram to reckon positive distances towards the right hand.\n\nNo force, either of attraction or of repulsion, can be observed between an electrified body and a body not electrified.\n\nElectric current is the flow of electric charge through an object, which produces no net loss or gain of electric charge. The most common charge carriers are the positively charged proton and the negatively charged electron. The movement of any of these charged particles constitutes an electric current. In many situations, it suffices to speak of the \"conventional current\" without regard to whether it is carried by positive charges moving in the direction of the conventional current or by negative charges moving in the opposite direction. This macroscopic viewpoint is an approximation that simplifies electromagnetic concepts and calculations.\n\nAt the opposite extreme, if one looks at the microscopic situation, one sees there are many ways of carrying an electric current, including: a flow of electrons; a flow of electron holes that act like positive particles; and both negative and positive particles (ions or other charged particles) flowing in opposite directions in an electrolytic solution or a plasma.\n\nBeware that, in the common and important case of metallic wires, the direction of the conventional current is opposite to the drift velocity of the actual charge carriers; i.e., the electrons. This is a source of confusion for beginners.\n\nThe total electric charge of an isolated system remains constant regardless of changes within the system itself. This law is inherent to all processes known to physics and can be derived in a local form from gauge invariance of the wave function. The conservation of charge results in the charge-current continuity equation. More generally, the rate of change in charge density \"ρ\" within a volume of integration \"V\" is equal to the area integral over the current density J through the closed surface \"S\" = ∂\"V\", which is in turn equal to the net current \"I\":\n\nThus, the conservation of electric charge, as expressed by the continuity equation, gives the result:\n\nThe charge transferred between times formula_2 and formula_3 is obtained by integrating both sides:\nwhere \"I\" is the net outward current through a closed surface and \"Q\" is the electric charge contained within the volume defined by the surface.\n\nAside from the properties described in articles about electromagnetism, charge is a relativistic invariant. This means that any particle that has charge \"Q\", no matter how fast it goes, always has charge \"Q\". This property has been experimentally verified by showing that the charge of \"one\" helium nucleus (two protons and two neutrons bound together in a nucleus and moving around at high speeds) is the same as \"two\" deuterium nuclei (one proton and one neutron bound together, but moving much more slowly than they would if they were in a helium nucleus).\n\n\n"}
{"id": "22533219", "url": "https://en.wikipedia.org/wiki?curid=22533219", "title": "Ernst August Girschner", "text": "Ernst August Girschner\n\nErnst August Girschner, usually just Ernst Girschner (29 October 1860 – 28 April 1914) was a German entomologist who specialised in Diptera.\n\nGirschner was born (and died) in Torgau, Province of Saxony. He taught at the Gymnasium in Torgau. Girschner described many new species of Diptera but made much more important contributions notably formalising the use of chaetotaxy in Calyptratae \"it was the merit of Mr. E. Girschner to give to Chaetotaxy a much greater development and application than it had had before, and to treat it as a sine qua non of descriptive dipterology. His enviable talent for drawing enabled him to illustrate his papers by diagrams more eloquent than any descriptions\" \n\nHe was a friend of entomologist Josef Mik.\n\nPartial list\n\nPart of his collection was purchased by Colbran J. Wainwright in 1909 (now in the Natural History Museum London).\n\n"}
{"id": "1025538", "url": "https://en.wikipedia.org/wiki?curid=1025538", "title": "Findability", "text": "Findability\n\nFindability is the ease with which information contained on a website can be found, both from outside the website (using search engines and the like) and by users already on the website. Although findability has relevance outside the World Wide Web, the term is usually used in that context. Most relevant websites do not come up in the top results because designers and engineers do not cater to the way ranking algorithms work currently. Its importance can be determined from the first law of e-commerce, which states \"If the user can’t find the product, the user can’t buy the product.\" As of December 2014, out of 10.3 billion monthly Google searches by Internet users in the United States, an estimated 78% are made to research products and services online.\n\nFindability encompasses aspects of information architecture, user interface design, accessibility and search engine optimization (SEO), among others.\n\nFindability is similar to discoverability, which is defined as the ability of something, especially a piece of content or information, to be found. It is different from web search in that the word \"find\" refers to locating something in a known space while 'search' is in an unknown space or not in an expected location.\n\nMark Baker, the author of \"Every Page is Page One\", mentions that findability \"is a content problem, not a search problem\". Even when the right content is present, users often find themselves deep within the content of a website but not in the right place. He further adds that findability is intractable, perfect findability is unattainable, but we need to focus on reducing the effort for finding that a user would have to do for themselves.\n\nFindability can be divided into external findability and on-site findability, based on where the customers need to find the information.\n\nHeather Lutze is thought to have created the term in the early 2000s. The popularization of the term \"findability\" for the Web is usually credited to Peter Morville. In 2005 he defined it as: \"the ability of users to identify an appropriate Web site and navigate the pages of the site to discover and retrieve relevant information resources\", though it appears to have been first coined in a public context referring to the web and information retrieval by Alkis Papadopoullos in a 2005 article entitled \"Findability\".\n\nExternal findability is the domain of Internet marketing and search engine optimization (SEO) tactics. External findability can be very influential for businesses. Smaller companies may have trouble influencing external findability, due to being less aware to consumers. Other means are taken to make sure that they are found in search results.\n\nSeveral factors affect external findability:\n\nOn-site findability is concerned with the ability of a potential customer to find what they are looking for within a specific site. More than 90 percent of customers use internal searches in a website compared to browsing. Of those, only 50 percent find what they are looking for. Improving the quality of on-site searches highly improves the business of the website. Several factors affect findability on a website:\n\nBaseline findability is the existing findability before changes are made in order to improve it. This is measured by participants who represent the customer base of the website, who try to locate a sample set of items using the existing navigation of the website.\n\nIn order to evaluate how easily information can be found by searching a site using a search engine or information retrieval system, retrievability measures were developed, and similarly, navigability measures now measure ease of information access through browsing a site (e.g. PageRank, MNav, InfoScent (see Information foraging), etc.).\n\nFindability also can be evaluated via the following techniques:\n\nFindability Sciences defines a findability index in terms of each user's influence, context, and sentiments. For seamless search, current websites focus on a combination of structured hypertext-based information architectures and rich Internet application-enabled visualization techniques.\n\n\n\n"}
{"id": "435023", "url": "https://en.wikipedia.org/wiki?curid=435023", "title": "Global Consciousness Project", "text": "Global Consciousness Project\n\nThe Global Consciousness Project (GCP, also called the EGG Project) is a parapsychology experiment begun in 1998 as an attempt to detect possible interactions of \"global consciousness\" with physical systems. The project monitors a geographically distributed network of hardware random number generators in a bid to identify anomalous outputs that correlate with widespread emotional responses to sets of world events, or periods of focused attention by large numbers of people. The GCP is privately funded through the Institute of Noetic Sciences and describes itself as an international collaboration of about 100 research scientists and engineers.\n\nSkeptics such as Robert T. Carroll, Claus Larsen, and others have questioned the methodology of the Global Consciousness Project, particularly how the data are selected and interpreted, saying the data anomalies reported by the project are the result of \"pattern matching\" and selection bias which ultimately fail to support a belief in psi or global consciousness. Other critics have stated that the open access to the test data \"is a testimony to the integrity and curiosity of those involved\". But in analyzing the data for 11 September 2001, May et al. concluded that the statistically significant result given by the published GCP hypothesis was fortuitous, and found that an alternative method of analysis gave only chance deviations throughout.\n\nRoger D. Nelson developed the project as an extrapolation of two decades of experiments from the controversial Princeton Engineering Anomalies Research Lab (PEAR).\n\nIn an extension of the laboratory research utilizing hardware random number generators called FieldREG, investigators examined the outputs of REGs in the field before, during and after highly focused or coherent group events. The group events studied included psychotherapy sessions, theater presentations, religious rituals, sports competitions such as the Football World Cup, and television broadcasts such as the Academy Awards.\n\nFieldREG was extended to global dimensions in studies looking at data from 12 independent REGs in the US and Europe during a web-promoted \"Gaiamind Meditation\" in January 1997, and then again in September 1997 after the death of Diana, Princess of Wales. The project claimed the results suggested it would be worthwhile to build a permanent network of continuously-running REGs. This became the EGG project or Global Consciousness Project.\n\nComparing the GCP to PEAR, Nelson, referring to the \"field\" studies with REGs done by PEAR, said the GCP used \"exactly the same procedure... applied on a broader scale.\"\n\nThe GCP's methodology is based on the hypothesis that events which elicit widespread emotion or draw the simultaneous attention of large numbers of people may affect the output of hardware random number generators in a statistically significant way. The GCP maintains a network of hardware random number generators which are interfaced to computers at 70 locations around the world. Custom software reads the output of the random number generators and records a trial (sum of 200 bits) once every second. The data are sent to a server in Princeton, creating a database of synchronized parallel sequences of random numbers. The GCP is run as a replication experiment, essentially combining the results of many distinct tests of the hypothesis. The hypothesis is tested by calculating the extent of data fluctuations at the time of events. The procedure is specified by a three-step experimental protocol. In the first step, the event duration and the calculation algorithm are pre-specified and entered into a formal registry. In the second step, the event data are extracted from the database and a Z score, which indicates the degree of deviation from the null hypothesis, is calculated from the pre-specified algorithm. In the third step, the event Z-score is combined with the Z-scores from previous events to yield an overall result for the experiment.\n\nThe remote devices have been dubbed \"Princeton Eggs\", a reference to the coinage \"electrogaiagram\", a portmanteau of electroencephalogram and Gaia. Supporters and skeptics have referred to the aim of the GCP as being analogous to detecting \"a great disturbance in the Force.\"\n\nThe GCP has suggested changes in the level of randomness may have occurred during the September 11, 2001 attacks at the times of the plane impacts and the building collapses, and over the two days following the attacks.\n\nIndependent scientists Edwin May and James Spottiswoode conducted an analysis of the data around the 11 September 2001 events and concluded there was no statistically significant change in the randomness of the GCP data during the attacks and the apparent significant deviation reported by Nelson and Radin existed only in their chosen time window. Spikes and fluctuations are to be expected in any random distribution of data, and there is no set time frame for how close a spike has to be to a given event for the GCP to say they have found a correlation. \nWolcotte Smith said \"A couple of additional statistical adjustments would have to be made to determine if there really was a spike in the numbers,\" referencing the data related to September 11, 2001. Similarly, Jeffrey D. Scargle believes unless both Bayesian and classical p-value analysis agree and both show the same anomalous effects, the kind of result GCP proposes will not be generally accepted.\n\nIn 2003, a New York Times article concluded \"All things considered at this point, the stock market seems a more reliable gauge of the national—if not the global—emotional resonance.\"\n\nAccording to \"The Age\", Nelson concedes \"the data, so far, is not solid enough for global consciousness to be said to exist at all. It is not possible, for example, to look at the data and predict with any accuracy what (if anything) the eggs may be responding to.\"\n\nRobert Matthews said that while it was \"the most sophisticated attempt yet\" to prove psychokinesis existed, the unreliability of significant events to cause statistically significant spikes meant that \"the only conclusion to emerge from the Global Consciousness Project so far is that data without a theory is as meaningless as words without a narrative\".\n\n"}
{"id": "31597935", "url": "https://en.wikipedia.org/wiki?curid=31597935", "title": "Glossary of quantum philosophy", "text": "Glossary of quantum philosophy\n\nThis is a glossary for the terminology applied in the foundations of quantum mechanics and quantum metaphysics, collectively called \"quantum philosophy\", a subfield of philosophy of physics.\n\nNote that this is a highly debated field, hence different researchers may have different definitions on the terms.\n\n\nSee also: entangled\n\n\n\n\n\n\nList of interpretations:\n\n\n\nEarly researchers (before the 1950s)\n\n1950s–2010s\n\n\n\n"}
{"id": "25249497", "url": "https://en.wikipedia.org/wiki?curid=25249497", "title": "Goniophotometry", "text": "Goniophotometry\n\nGoniophotometry or goniometric optical scatter measurement is the technique of measuring the angular distribution of light, possibly wavelength-dependent, scattered from a surface.\n\nGoniophotometry is used to measure intensity distributions from lamps and luminaries and to evaluate the gloss of paints and other surface finishes.\n\n"}
{"id": "24041315", "url": "https://en.wikipedia.org/wiki?curid=24041315", "title": "Handbook of Porphyrin Science", "text": "Handbook of Porphyrin Science\n\nPublished by World Scientific, the Handbook of Porphyrin Science: With Applications to Chemistry, Physics, Materials Science, Engineering, Biology and Medicine is a multi-volume reference set edited by scientists Karl Kadish, Kevin Smith and Roger Guilard. The first ten volumes were published in 2010 and the next ten are expected to be published in 2011.\n\nTopics covered include:\n\nThe current work stems from World Scientific's Journal of Porphyrins and Phthalocyanines (JPP) and from the research interests of the three editors and hundreds of authors who have presented the results of their research in this society-run journal since its founding in 1997.\n\n"}
{"id": "31081397", "url": "https://en.wikipedia.org/wiki?curid=31081397", "title": "Indochinite", "text": "Indochinite\n\nAn Indochinite is a type of tektite. Tektites were ejected into the Earth's upper atmosphere by a meteorite impact and subsequently cooled to form the distinctive glass-like structure. Indochinites are distinctly dark black in contrast to the green of European moldavite tektites. It is estimated that these bodies of solidified magma are 700,000 years old. Indochinite tektites, as the name suggests, are found in the Indochinese peninsula, from Australia and the Pacific islands of Micronesia in the east and south, to China and Indonesia in the north and west. The largest indochinite is a Muong-Nong type tektite (which are layered tektites), which had a mass of 29.0 kg.\n\n"}
{"id": "38407748", "url": "https://en.wikipedia.org/wiki?curid=38407748", "title": "International Association for Official Statistics", "text": "International Association for Official Statistics\n\nThe International Association for Official Statistics (IAOS) is an association founded in 1985. It is an international non-governmental organization (NGO), which was created and developed as a specialized section of the International Statistical Institute (ISI). \n\nIt is thus an Association of physical and legal persons who have scientific or professional interest in the field of official statistics. IAOS brings together producers and users of official statistics. \nIAOS aims to promote the understanding and advancement of official statistics and related subjects and to foster the development of effective and efficient official statistical services through international contacts among individuals and organizations, including users of official statistics as well as research institutes.\nIts next biennial conferences will be from 8 to 10 October 2014 in Da Nang, Vietnam, to discuss how best to meet the statistical needs of a changing world. \nThe Association publishes a quarterly statistical journal, the Statistical Journal of the IAOS, which includes peer reviewed articles relevant to the field of official statistics.\nAn annual Young Statisticians Prize is awarded to the best paper in the field of official statistics written by a young statistician.\n\n"}
{"id": "39241111", "url": "https://en.wikipedia.org/wiki?curid=39241111", "title": "International Society for Cultural and Activity Research", "text": "International Society for Cultural and Activity Research\n\nThe International Society for Cultural and Activity Research (ISCAR) was founded in 2002 by the merging of the International Society for Cultural Research and Activity Theory and the Conference for Sociocultural Research. It is focused on sociocultural theory, and its application to practice.\n\n"}
{"id": "40835006", "url": "https://en.wikipedia.org/wiki?curid=40835006", "title": "Invalid science", "text": "Invalid science\n\nInvalid science consists of scientific claims based on experiments that cannot be reproduced or that are contradicted by experiments that can be reproduced. Recent analyses indicate that the proportion of invalid claims in the scientific literature is steadily increasing. The number of retractions has grown tenfold over the past decade, but they still make up approximately 0.2% of the 1.4m papers published annually in scholarly journals.\n\nThe U.S. Office of Research Integrity (ORI), investigates scientific misconduct.\n\nScience magazine ranked first for the number of articles retracted at 70, just edging out PNAS, which retracted 69. Thirty-two of Science's retractions were due to fraud or suspected fraud, and 37 to error. A subsequent \"retraction index\" indicated that journals with relatively high impact factors, such as Science, Nature and Cell, had a higher rate of retractions. Under 0.1% of papers in PubMed had were retracted of more than 25 million papers going back to the 1940s.\n\nThe fraction of retracted papers due to scientific misconduct was estimated at two-thirds, according to studies of 2047 papers published since 1977. Misconducted included fraud and plagiarism. Another one-fifth were retracted because of mistakes, and the rest were pulled for unknown or other reasons.\n\nA separate study analyzed 432 claims of genetic links for various health risks that vary between men and women. Only one of these claims proved to be consistently reproducible. Another meta review, found that of the 49 most-cited clinical research studies published between 1990 and 2003, more than 40 percent of them were later shown to be either totally wrong or significantly incorrect.\n\nIn 2012 biotech firm Amgen was able to reproduce just six of 53 important studies in cancer research. Earlier, a group at Bayer, a drug company, successfully repeated only one fourth of 67 important papers. In 2000-10 roughly 80,000 patients took part in clinical trials based on research that was later retracted because of mistakes or improprieties.\n\nNathan Mhyrvold failed repeatedly to replicate the findings of several papers on dinosaur growth. Dinosaurs added a layer to their bones each year. Tyrannosaurus rex was thought to have increased in size by more than 700 kg a year, until Mhyrvold showed that this was a factor of 2 too large. In 4 of 12 papers he examined, the original data had been lost. In three, the statistics were correct, while three had serious errors that invalidated their conclusions. Two papers mistakenly relied on data from these three. He discovered that some of the paper's graphs did not reflect the data. In one case, he found that only four of nine points on the graph came from data cited in the paper.\n\nTorcetrapib was originally hyped as a drug that could block a protein that converts HDL cholesterol into LDL with the potential to \"redefine cardiovascular treatment\". One clinical trial showed that the drug could increase HDL and decrease LDL. Two days after Pfizer announced its plans for the drug, it ended the Phase III clinical trial due to higher rates of chest pain and heart failure and a 60 percent increase in overall mortality. Pfizer had invested more than $1 billion in developing the drug.\n\nAn in-depth review of the most highly cited biomarkers (whose presence are used to infer illness and measure treatment effects) claimed that 83 percent of supposed correlations became significantly weaker in subsequent studies. Homocysteine is an amino acid whose levels correlated with heart disease. However, a 2010 study showed that lowering homocysteine by nearly 30 percent had no effect on heart attack or stroke.\n\n\"Priming\" studies claim that decisions can be influenced by apparently irrelevant events that a subject witnesses just before making a choice. Nobel Prize-winner Daniel Kahneman allege that much of it is poorly founded. Researchers have been unable to replicate some of the more widely cited examples. A paper in PLoS ONE reported that nine separate could not reproduce a study purporting to show that thinking about a professor before taking an intelligence test leads to a higher score than imagining a football hooligan.\n\nIn the 1950s, when academic research accelerated during the cold war, the total number of scientists was a few hundred thousand. In the new century 6m-7m researchers are active. The number of research jobs has not matched this increase. Every year six new PhDs compete for every academic post. Replicating other researcher’s results is not perceived to be valuable. The struggle to compete encourages exaggeration of findings and biased data selection. A recent survey found that one in three researchers knows of a colleague who has at least somewhat distorted their results.\n\nMajor journals reject in excess of 90% of submitted manuscripts and tend to favor the most dramatic claims. The statistical measures that researchers use to test their claims allow a fraction of false claims to appear valid. Invalid claims are more likely to be dramatic (because they are false.) Without replication, such errors are less likely to be caught.\n\nConversely, failures to prove a hypothesis are rarely even offered for publication. “Negative results” now account for only 14% of published papers, down from 30% in 1990. Knowledge of what is not true is as important as of what is true.\n\nPeer review is the primary validation technique employed by scientific publications. However, a prominent medical journal tested the system and found major failings. It supplied research with induced errors and found that most reviewers failed to spot the mistakes, even after being told of the tests.\n\nA pseudonymous fabricated paper on the effects of a chemical derived from lichen on cancer cells was submitted to 304 journals for peer review. The paper was filled with errors of study design, analysis and interpretation. 157 lower-rated journals accepted it. Another study sent an article containing eight deliberate mistakes in study design, analysis and interpretation to more than 200 of the British Medical Journal’s regular reviewers. On average, they reported fewer than two of the problems.\n\nPeer reviewers typically do not re-analyse data from scratch, checking only that the authors’ analysis is properly conceived.\n\nScientists divide errors into type I, incorrectly asserting the truth of a hypothesis (false positive) and type II, rejecting a correct hypothesis (false negative). Statistical checks assess the probability that data which seem to support a hypothesis come about simply by chance. If the probability is less than 5%, the evidence is rated “statistically significant”. One definitiomal consequence is a type one error rate of one in 20.\n\nIn 2005 Stanford epidemiologist John Ioannidis showed that the idea that only one paper in 20 gives a false-positive result was incorrect. He claimed, “most published research findings are probably false.” He found three categories of problems: insufficient “statistical power” (avoiding type II errors); the unlikeliness of the hypothesis; and publication bias favoring novel claims.\n\nA statistically powerful study identifies factors with only small effects on data. In general studies with more repetitions that run the experiment more times on more subjects have greater power. A power of 0.8 means that of ten true hypotheses tested, the effects of two are missed. Ioannidis found that in neuroscience the typical statistical power is 0.21; another study found that psychology studies average 0.35.\n\nUnlikeliness is a measure of the degree of surprise in a result. Scientists prefer surprising results, leading them to test hypotheses that are unlikely to very unlikely. Ioannidis claimed that in epidemiology, some one in ten hypotheses should be true. In exploratory disciplines like genomics, which rely on examining voluminous data about genes and proteins, only one in a thousand should prove correct.\n\nIn a discipline in which 100 out of 1,000 hypotheses are true, studies with a power of 0.8 will find 80 and miss 20. Of the 900 incorrect hypotheses, 5% or 45 will be accepted because of type I errors. Adding the 45 false positives to the 80 true positives gives 125 positive results, or 36% specious. Dropping statistical power to 0.4, optimistic for many fields, would still produce 45 false positives but only 40 true positives, less than half.\n\nNegative results are more reliable. Statistical power of 0.8 produces 875 negative results of which only 20 are false, giving an accuracy of over 97%. Negative results however account for a minority of published results, varying by discipline. A study of 4,600 papers found that the proportion of published negative results dropped from 30% to 14% between 1990 and 2007.\n\nSubatomic physics sets an acceptable false-positive rate of one in 3.5m (known as the five-sigma standard). However, even this does not provide perfect protection. The problem invalidates some 3/4s of machine learning studies according to one review.\n\nStatistical significance is a measure for testing statistical correlation. It was invented by English mathematician Ronald Fisher in the 1920s. It defines a “significant” result as any data point that would be produced by chance less than 5 (or more stringently, 1) percent of the time. A significant result is widely seen as an important indicator that the correlation is not random.\n\nWhile correlations track the relationship between truly independent measurements, such as smoking and cancer, they are much less effective when variables cannot be isolated, a common circumstance in biological systems. For example, statistics found a high correlation between lower back pain and abnormalities in spinal discs, although it was later discovered that serious abnormalities were present in two-thirds of pain-free patients.\n\nJournals such as PLoS One use a “minimal-threshold” standard, seeking to publish as much science as possible, rather than to pick out the best work. Their peer reviewers assess only whether a paper is methodologically sound. Almost half of their submissions are still rejected on that basis.\n\nOnly 22% of the clinical trials financed by the National Institutes of Health (NIH) released summary results within one year of completion, even though the NIH requires it. Fewer than half published within 30 months; a third remained unpublished after 51 months. When other scientists rely on invalid research, they may waste time on lines of research that are themselves invalid. The failure to report failures means that researchers waste money and effort exploring blind alleys already investigated by other scientists.\n\nIn 21 surveys of academics (mostly in the biomedical sciences but also in civil engineering, chemistry and economics) carried out between 1987 and 2008, 2% admitted fabricating data, but 28% claimed to know of colleagues who engaged in questionable research practices.\n\nClinical trials are generally too costly to rerun. Access to trial data is the only practical approach to reassessment. A campaign to persuade pharmaceutical firms to make all trial data available won its first convert in February 2013 when GlaxoSmithKline became the first to agree.\n\nSoftware used in a trial is generally considered to be proprietary intellectual property and is not available to replicators, further complicating matters. Journals that insist on data-sharing tend not to do the same for software.\n\nEven well-written papers may not include sufficient detail and/or tacit knowledge (subtle skills and extemporisations not considered notable) for the replication to succeed. One cause of replication failure is insufficient control of the protocol, which can cause disputes between the original and replicating researchers.\n\nGeneticists have begun more careful reviews, particularly of the use of statistical techniques. The effect was to stop a flood of specious results from genome sequencing.\n\nRegistering research protocols in advance and monitoring them over the course of a study can prevent researchers from modifying the protocol midstream to highlight preferred results. Providing raw data for other researchers to inspect and test can also better hold researchers to account.\n\nReplacing peer review with post-publication evaluations can encourage researchers to think more about the long-term consequences of excessive or unsubstantiated claims. That system was adopted in physics and mathematics with good results.\n\nFew researchers, especially junior workers, seek opportunities to replicate others' work, partly to protect relationships with senior researchers.\n\nReproduction benefits from access to the original study's methods and data. More than half of 238 biomedical papers published in 84 journals failed to identify all the resources (such as chemical reagents) necessary to reproduce the results. In 2008 some 60% of researchers said they would share raw data; in 2013 just 45% do. Journals have begun to demand that at least some raw data be made available, although only 143 of 351 randomly selected papers covered by some data-sharing policy actually complied.\n\nThe Reproducibility Initiative is a service allowing life scientists to pay to have their work validated by an independent lab. In October 2013 the initiative received funding to review 50 of the highest-impact cancer findings published between 2010 and 2012. \"Blog Syn\" is a website run by graduate students that is dedicated to reproducing chemical reactions reported in papers.\n\nIn 2013 replication efforts received greater attention. \"Nature\" and related publications introduced an 18-point checklist for life science authors in May, in its effort to ensure that its published research can be reproduced. Expanded \"methods\" sections and all data were to be available online. The Centre for Open Science opened as an independent laboratory focused on replication. The journal Perspectives on Psychological Science announced a section devoted to replications. Another project announced plans to replicate 100 studies published in the first three months of 2008 in three leading psychology journals.\n\nMajor funders, including the European Research Council, the US National Science Foundation and Research Councils UK have not changed their preference for new work over replications.\n\n"}
{"id": "1825489", "url": "https://en.wikipedia.org/wiki?curid=1825489", "title": "Isaac Asimov's Guide to Earth and Space", "text": "Isaac Asimov's Guide to Earth and Space\n\nGuide to Earth and Space () is a non-fiction work by the well-known science fiction writer Isaac Asimov. The book differs somewhat in structure from typical literature by presenting its information in the form of answers to a series of questions, presumably posed by the reader. Like many of Asimov's non-fiction pieces, this \"Guide\" starts with the basics, answering relatively simple (to the modern reader) questions about the Earth - is it flat, does it spin, is it the center of the universe, etc... \n\nFrom there the questions progress roughly through the evolution of astronomy and discovery to introduce more complex topics, from the orbits of the planets to the formation of stars and the characteristics of quasars and black holes.\n\nMany of the concepts discussed in the latter sections of the books can be compared with those presented in Asimov's 1966 work \"The Universe: From Flat Earth to Quasar\"; furthermore, they serve in several cases to update the state of the art from the intervening 25 years between publications.\n"}
{"id": "6196538", "url": "https://en.wikipedia.org/wiki?curid=6196538", "title": "John Morris (geologist)", "text": "John Morris (geologist)\n\nJohn Morris (19 February 1810 – 7 January 1886) was an English geologist.\n\nHe was born in 1810 at Homerton, London, and educated at private schools. \nHe was engaged for some years as a pharmaceutical chemist at Kensington, but soon became interested in geology and other branches of science, and ultimately retired from business. \nHis published papers speedily attracted notice, and his \"Catalogue of British Fossils,\" published in 1845, a work involving much critical research, added greatly to his reputation.\n\nMorris was professor of geology at University College, London from 1854 to 1877. He was elected F.G.S. in 1845. Along with Bowerbank and five others, he was a founding member of the London Clay Club. Morris was president of the Geologists' Association from 1868–1871 and from 1877–1879. He was awarded the Lyell Medal in 1876. Morris's best original work was done on Eocene and Jurassic rocks. His \"Catalogue of British Fossils\" was an important pioneering effort in palaeontology.\n\nHe died on 7 January 1886, and was buried at Kensal Green. One daughter survived him.\n"}
{"id": "72023", "url": "https://en.wikipedia.org/wiki?curid=72023", "title": "John White (colonist and artist)", "text": "John White (colonist and artist)\n\nJohn White (c. 1540 – c. 1593) was a settler among those who sailed with Richard Grenville to present-day North Carolina in 1585, acting as artist and mapmaker to the expedition.\n\nDuring his time at Roanoke Island he made a number of watercolor sketches of the surrounding landscape and the native Algonkin peoples. These works are significant as they are the most informative illustrations of a Native American society of the Eastern seaboard; the surviving original watercolors are now preserved in the print room of the British Museum.\n\nIn 1587, White became governor of Sir Walter Raleigh's failed attempt at a permanent settlement on Roanoke Island, known to history as the \"Lost Colony\". This was the earliest effort to establish a permanent English colony in the New World. White's granddaughter Virginia Dare was the first English child born in the Americas.\n\nAfter the failure of the colony, White retired to Raleigh's estates in Ireland, reflecting upon the \"evils and unfortunate events\" which had ruined his hopes in America, though never giving up hope that his daughter and granddaughter were still alive.\n\nJohn White's exact date of birth is unknown but it seems likely he was born some time between 1540 and 1550. There is a record dated 22 February 1539, of a christening in the Church of St Augustine, London, of a \"John White\" on that same day; but there is no proof this is the same person. White is known to have attended church in the parish of St. Martin Ludgate in London. In 1566 he married Tomasyn Cooper; with whom he had a son, Tom, who died young, and a daughter Eleanor. Little is known of White's training as an artist but it is possible that he apprenticed as an illustrator under a London master.\n\nIn the late sixteenth century efforts to establish an English colony in the New World began to gain momentum, and White soon became an enthusiastic supporter. In 1585 White accompanied the expedition led by Sir Ralph Lane to attempt to found the first English colony in North America. White was sent by Sir Walter Raleigh as Sir Richard Grenville's artist-illustrator on his first voyage to the New World; he served as mapmaker and artist to the expedition, which encountered considerable difficulties and returned to England in 1586 .\n\nIn 1585 White had been commissioned to \"draw to life\" the inhabitants of the New World and their surroundings. During White's time at Roanoke Island, he completed numerous watercolor drawings of the surrounding landscape and native peoples. These works are significant as they are the most informative illustrations of a Native American society of the Eastern seaboard, and predate the first body of \"discovery voyage art\" created in the late 18th century by the artists who sailed with Captain James Cook. They represent the sole-surviving visual record of the native inhabitants of America encountered by England's first settlers.\n\nWhite's enthusiasm for watercolor was unusual – most contemporary painters preferred to use oil-based paints. White's watercolors would soon become a sensation in Europe; it was not long before the watercolors were engraved by the Flemish master engraver Theodore de Bry. Through the medium of print, the illustrations became widely known and distributed; they were published in 1590 under the title \"America\".\n\nAfter Lane's colonists returned to England in 1586, Sir Walter Raleigh, who held the land patent for the proposed English colony of Virginia, tasked White with the job of organising a new settlement in the Chesapeake Bay area, one which would be self-sustaining and which would include women and children. During 1586 White was able to persuade 113 prospective colonists to join Raleigh's expedition, including his daughter Eleanor and his son-in-law Ananias Dare, recently married at St Bride's Church in Fleet Street. His efforts did not go unrewarded; on 7 January 1587, Raleigh named \"John White of London Gentleman, to be the chief Governor\" of the new colony. White, with thirteen others, were incorporated under the name of \"The Governor and Assistants of the Cities of Raleigh of Virginia\".\n\nIn May 1587 White's colonists sailed for Virginia in the \"Lion\". They were guided by the Portuguese navigator Simon Fernandez, the same pilot who had led the 1585 expedition and who was given by his fellow sailors the unhappy nickname of \"the swine.\" The settlers' chosen destination was not Roanoke but the Chesapeake Bay. But, upon reaching Roanoke in late July, and allowing the colonists to disembark, Fernandez refused to let White's men re-board ship.\n\nAccording to White's journal, Fernandez's deputy \"called to the sailors in the pinesse, charging them not to bring any of the planters [settlers] back againe, but leave them on the island.\" Faced with what amounted to a mutiny by his navigator, White appears to have backed down and acquiesced in this sudden change of plan. Despite the governor's protests, Fernandez held that \"summer was farre spent [summer was almost over], wherefore hee would land all the planters in no other place.\"\n\nThis second colony at Roanoke set about repairing the structures left behind in 1585. They also searched for the fifteen men left behind by the previous expedition, but found only bones. From an early stage there were tensions with the local Algonkin Indians, though initially things went well. White quickly made contact with friendly natives led by Chief Manteo, who explained to him that the lost fifteen had been killed by hostile Secotan, Aquascogoc, and Dasamongueponke warriors, choosing a time and place of attack \"of great advantage to the savages.\"\nOn 8 August 1587, White led a dawn attack on the Dasamongueponkes that went disastrously wrong. White and his soldiers entered the Dasamongueponke village in the morning \"so early that it was yet dark,\" but mistakenly attacked a group of hitherto friendly Indians, killing one and wounding many. \"We were deceaved,\" wrote White in his journal, \"for the savages were our friendes.\" Henceforth, relations with the local tribes would steadily deteriorate.\n\nOn 18 August 1587, there was happier news – White became a grandfather. \"Elenora, daughter to the governour and wife to Ananias Dare, one of the assistants, was delivered of a daughter in Roanoke.\" The child was healthy and \"was christened there the Sunday following, and because this child was the first Christian born in Virginia, she was named Virginia.\"\n\nHowever, the colonists' food supplies soon began to grow short, and in late 1587 the settlers pressed White to return to England \"for the better and sooner obtaining of supplies, and other necessaries.\" Because the colony had been deposited in Roanoke rather than the Chesapeake area, supply ships from England ignorant of Fernandez's change of plan would most likely not land in Roanoke at all, and the settlement might not survive the coming winter. White was reluctant to abandon his colony, anxious that his enemies in England \"would not spare to slander [him] falsely\" should he leave, and worried that his \"stuff and goods might be spoiled and most of it pilfered away.\" Eventually the colonists agreed to stand surety for White's belongings and he was prevailed upon to sail, \"much against his will,\" to seek help.\n\nMisfortune struck White's return to England from the beginning. The anchor of the flyboat on which White was quartered could not be raised, and many crew members were severely injured during the attempt. Worse, their journey home was delayed by \"scarce and variable winds\" followed by \"a storm at the north-east,\" and many sailors starved or died of scurvy. On 16 October 1587 the desperate crew at last landed in Smewicke, in the west of Ireland, and White was finally able to make his way back to Southampton.\n\nFurther bad news awaited White on his return to England. Just two weeks previously Queen Elizabeth I had issued a general \"stay of shipping,\" preventing any ships from leaving English shores. The reason was the \"invincible fleetes made by the King of Spain, joyned with the power of the Pope, for the invading of England\" – the Spanish Armada. White's patron Sir Walter Raleigh attempted to provide ships to rescue the colony but he was over-ruled by the Queen.\n\nIn early 1588 White was able to scrape together a pair of small pinnaces, the \"Brave\" and the \"Roe\", which were unsuitable for military service and could be spared for the expedition to Roanoke. Unluckily for White, they were barely suited for the Atlantic crossing and the governor endured further bad luck as the ships were intercepted by French pirates, who \"playd extreemely upon us with their shot,\" hitting White (to his great embarrassment) \"in the side of the buttoke.\" White and his crew escaped to England with their lives, but \"they robbed us of all our victuals, powder, weapons and provision,\" and the journey to Virginia had to be abandoned. By this stage White appears to have formed the view that he was born under \"an unlucky star.\"\n\nFinally, in March 1590, with the immediate threat of a Spanish invasion by now abated, Raleigh was able to equip White's rescue expedition. Two ships, the \"Hopewell\" and the \"Moonlight\" set sail for Roanoke. The return journey was prolonged by extensive privateering and a number of sea battles, and White's eventual landing at the Outer Banks was further imperiled by poor weather. The landing was hazardous and was beset by bad conditions and adverse currents. During the landing on Roanoke, of the mariners who accompanied White, \"seven of the chiefest were drowned.\"\n\nGovernor White finally reached Roanoke Island on 18 August 1590, his granddaughter's third birthday, but he found his colony had been long deserted. The buildings had collapsed and \"the houses [were] taken downe.\" The few clues about the colonists' whereabouts included the letters \"CRO\" carved into a tree, and the word \"CROATOAN\" carved on a post of the fort. Croatoan was the name of a nearby island (likely modern-day Hatteras Island) and of a local tribe of Native Americans. Roanoke Island was not the original planned location for the colony and the idea of moving elsewhere had been discussed. Before the Governor's departure, he and the colonists had agreed that a message would be carved into a tree if they had moved and would include an image of a Maltese Cross if the decision was made by force. White found no such cross and was hopeful that his family were still alive.\n\nTrue to their word, the colonists had looked after White's belongings, which had been carefully buried and hidden. However, local Indians had evidently looted the hiding place, and White found \"about the place many of my things spoyled and broken, and my books torne from the covers, the frames of some of my pictures and mappes rotten and spoyled with rayne, and my armour almost eaten through with rust.\"\n\nDue to weather which \"grew to be fouler and fouler,\" White had to abandon the search of adjacent islands for the colonists. The ship's captain had already lost three anchors and could not afford the loss of another. White returned to Plymouth, England, on 24 October 1590.\n\nThe loss of the colony was a personal tragedy for White, from which he never fully recovered. He would never return to the New World, and in a letter to Richard Hakluyt he wrote that he must hand over the fate of the colonists and his family \"to the merciful help of the Almighty, whom I most humbly beseech to helpe and comfort them.\"\n\nLittle is known of White's life after the failure of the Roanoke Colony. He lived in Plymouth, and also owned a house at Newtown, Kylmore (Kilmore, County Cork), Ireland. He appears to have been in Ireland living on the estates of Sir Walter Raleigh, making maps of land for Raleigh's tenants, and reflecting upon the \"evils and unfortunate events\" which had ruined his hopes in the New World, though never giving up hope that his daughter and granddaughter were still alive.\n\nThe last surviving document related to White is a letter he wrote from Ireland in 1593 to the publisher of the prints of his Roanoke drawings. However, a record from May 1606 that a Bridget White was appointed estate administrator for her brother \"John White\" may refer to him. \n\nWhite is chiefly remembered today for his watercolors, which represent a unique record of 16th-century Algonquian society. All of White's surviving works are now in the print room of the British Museum.\n\nIn 2007, the British Museum placed the entire group of John White's watercolors on public display under the collection, \"\"A New World: England's First View of America\".\" There are more than seventy watercolors in the travelling exhibit. There were plans to show the collection at the North Carolina Museum of History.\n\n\n\n\n"}
{"id": "5347179", "url": "https://en.wikipedia.org/wiki?curid=5347179", "title": "Linear–quadratic–Gaussian control", "text": "Linear–quadratic–Gaussian control\n\nIn control theory, the linear–quadratic–Gaussian (LQG) control problem is one of the most fundamental optimal control problems. It concerns linear systems driven by additive white Gaussian noise. The problem is to determine an output feedback law that is optimal in the sense of minimizing the expected value of a quadratic cost criterion. Output measurements are assumed to be corrupted by Gaussian noise and the initial state, likewise, is assumed to be a Gaussian random vector.\n\nUnder these assumptions an optimal control scheme \"within the class of linear control laws\" can be derived by a completion-of-squares argument. This control law which is known as the LQG controller, is \"unique\" and it is simply a combination of a Kalman filter, i.e. a linear–quadratic state estimator (LQE), together with a linear–quadratic regulator (LQR). The separation principle states that the state estimator and the state feedback can be designed independently. LQG control applies to both linear time-invariant systems as well as linear time-varying systems, and constitutes a linear dynamic feedback control law that is easily computed and implemented. I.e., the LQG controller itself is a dynamic system like the system it controls. Both systems have the same state dimension.\n\nA deeper statement of the separation principle is that the LQG controller is still optimal in a wider class of possibly nonlinear controllers. That is, \"utilizing a nonlinear control scheme will not improve the expected value of the cost functional\". This version of the separation principle is a special case of the separation principle of stochastic control which states that even when the process and output noise sources are possibly non-Gaussian martingales, as long as the system dynamics are linear, the optimal control separates into an optimal state estimator (which may no longer be a Kalman filter) and an LQR regulator.\n\nIn the classical LQG setting, implementation of the LQG controller may be problematic when the dimension of the system state is large. The reduced-order LQG problem (fixed-order LQG problem) overcomes this by fixing \"a priori\" the number of states of the LQG controller. This problem is more difficult to solve because it is no longer separable. Also the solution is no longer unique. Despite these facts numerical algorithms are available to solve the associated optimal projection equations which constitute necessary and sufficient conditions for a locally optimal reduced-order LQG controller.\n\nLQG optimality does not automatically ensure good robustness properties. The robust stability of the closed loop system must be checked separately after the LQG controller has been designed. To promote robustness some of the system parameters may be assumed stochastic instead of deterministic. The associated more difficult control problem leads to a similar optimal controller of which only the controller parameters are different.\n\nFinally the LQG controller is also used to control perturbed non-linear systems.\n\nConsider the continuous-time linear dynamic system\n\nwhere formula_3 represents the vector of state variables of the system, formula_4 the vector of control inputs and formula_5 the vector of measured outputs available for feedback. Both additive white Gaussian system noise formula_6 and additive white Gaussian measurement noise formula_7 affect the system. Given this system the objective is to find the control input history formula_8 which at every time formula_9 may depend linearly only on the past measurements formula_10 such that the following cost function is minimized:\n\nwhere formula_13 denotes the expected value. The final time (horizon) formula_14 may be either finite or infinite. If the horizon tends to infinity the first term formula_15 of the cost function becomes negligible and irrelevant to the problem. Also to keep the costs finite the cost function has to be taken to be formula_16.\n\nThe LQG controller that solves the LQG control problem is specified by the following equations:\n\nThe matrix formula_19 is called the Kalman gain of the associated Kalman filter represented by the first equation. At each time formula_9 this filter generates estimates formula_21 of the state formula_22 using the past measurements and inputs. The Kalman gain formula_19 is computed from the matrices formula_24, the two intensity matrices formula_25 associated to the white Gaussian noises formula_6 and formula_7 and finally formula_28. These five matrices determine the Kalman gain through the following associated matrix Riccati differential equation:\n\nGiven the solution formula_31 the Kalman gain equals\n\nThe matrix formula_33 is called the feedback gain matrix. This matrix is determined by the matrices formula_34 and formula_35 through the following associated matrix Riccati differential equation:\n\nGiven the solution formula_38 the feedback gain equals\n\nObserve the similarity of the two matrix Riccati differential equations, the first one running forward in time, the second one running backward in time. This similarity is called duality. The first matrix Riccati differential equation solves the linear–quadratic estimation problem (LQE). The second matrix Riccati differential equation solves the linear–quadratic regulator problem (LQR). These problems are dual and together they solve the linear–quadratic–Gaussian control problem (LQG). So the LQG problem separates into the LQE and LQR problem that can be solved independently. Therefore, the LQG problem is called separable.\n\nWhen formula_40 and the noise intensity matrices formula_41, formula_42 do not depend on formula_9 and when formula_14 tends to infinity the LQG controller becomes a time-invariant dynamic system. In that case both matrix Riccati differential equations may be replaced by the two associated algebraic Riccati equations.\n\nSince the discrete-time LQG control problem is similar to the one in continuous-time, the description below focuses on the mathematical equations.\n\nThe discrete-time linear system equations are\n\nHere formula_47 represents the discrete time index and formula_48 represent discrete-time Gaussian white noise processes with covariance matrices formula_49 respectively.\n\nThe quadratic cost function to be minimized is\n\nThe discrete-time LQG controller is\n\nThe Kalman gain equals\n\nwhere formula_55 is determined by the following matrix Riccati difference equation that runs forward in time:\n\nThe feedback gain matrix equals\n\nwhere formula_58 is determined by the following matrix Riccati difference equation that runs backward in time:\n\nIf all the matrices in the problem formulation are time-invariant and if the horizon formula_60 tends to infinity the discrete-time LQG controller becomes time-invariant. In that case the matrix Riccati difference equations may be replaced by their associated discrete-time algebraic Riccati equations. These determine the time-invariant linear–quadratic estimator and the time-invariant linear–quadratic regulator in discrete-time. To keep the costs finite instead of formula_61 one has to consider formula_62 in this case.\n\n"}
{"id": "11136994", "url": "https://en.wikipedia.org/wiki?curid=11136994", "title": "List of Superfund sites in New York", "text": "List of Superfund sites in New York\n\nSuperfund sites in New York are designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA). CERCLA, a federal law passed in 1980, authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). The NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of October, 2013, there were 87 Superfund sites on the NPL in New York. Two new sites have been proposed for addition to the list, and 26 sites have been deleted from the list following cleanup.\n\n\n"}
{"id": "415874", "url": "https://en.wikipedia.org/wiki?curid=415874", "title": "List of astronomical observatories", "text": "List of astronomical observatories\n\nThis is a list of astronomical observatories ordered by name, along with initial dates of operation (where an accurate date is available) and location. The list also includes a final year of operation for many observatories that are no longer in operation. While other sciences, such as volcanology and meteorology, also use facilities called observatories for research and observations, this list is limited to observatories that are used to observe celestial objects.\n\nAstronomical observatories are mainly divided into four categories: space-based, airborne, ground-based, and underground-based.\n\nMany modern telescopes and observatories are located in space to observe astronomical objects in wavelengths of the electromagnetic spectrum that cannot penetrate the Earth's atmosphere (such as ultraviolet radiation, X-rays, and gamma rays) and are thus impossible to observe using ground-based telescopes.\nBeing above the atmosphere, these space observatories can also avoid the effects of atmospheric turbulence that plague ground based telescopes, although new generations of adaptive optics telescopes have since then dramatically improved the situation on the ground. The space high vacuum environment also frees the detectors from the ancestral diurnal cycle due to the atmospheric blue light background of the sky, thereby increasing significantly the observation time.\n\nAn intermediate variant is the airborne observatory, specialised in the infrared wavelengths of the EM spectrum, that conduct observations above the part of the atmosphere containing water vapor that absorbs them, in the stratosphere.\n\nHistorically, astronomical observatories consisted generally in a building or group of buildings where observations of astronomical objects such as sunspots, planets, asteroids, comets, stars, nebulae, and galaxies in the visible wavelengths of the electromagnetic spectrum were conducted. At first, for millennia, astronomical observations have been made with naked eyes. Then with the discovery of optics, with the help of different types of refractor telescopes and later with reflector telescopes. Their use allowed to dramatically increase both the collecting power and limit of resolution, thus the brightness, level of detail and apparent angular size of distant celestial objects allowing them to be better studied and understood. Following the development of modern physics, new ground based facilities have been constructed to conduct research in the radio and microwave wavelengths of the electromagnetic spectrum, with radio telescopes and dedicated microwave telescopes.\n\nModern astrophysics has extended the field of study of celestial bodies to non electromagnetic vectors, such as neutrinos, neutrons and cosmic-rays or gravitational waves. Thus new types of observatories have been developed. Interferometers are at the core of gravitational wave detectors. In order to limit the natural or artificial background noise, most particle detector based observatories are built deep underground.\n\n"}
{"id": "22965771", "url": "https://en.wikipedia.org/wiki?curid=22965771", "title": "List of burial mounds in the United States", "text": "List of burial mounds in the United States\n\nThis is a list of notable burial mounds in the United States built by Native Americans. Burial mounds were built by many different cultural groups over a span of many thousands of years, beginning in the Late Archaic period and continuing through the Woodland period up to the time of European contact.\n\n\n"}
{"id": "11485625", "url": "https://en.wikipedia.org/wiki?curid=11485625", "title": "List of towns and cities with 100,000 or more inhabitants/country: T-U-V-W", "text": "List of towns and cities with 100,000 or more inhabitants/country: T-U-V-W\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "25445212", "url": "https://en.wikipedia.org/wiki?curid=25445212", "title": "Magnetic proton recoil neutron spectrometer", "text": "Magnetic proton recoil neutron spectrometer\n\nMagnetic Proton Recoil neutron spectrometer is a large high-resolution neutron spectrometer installed at JET.\n\nThe Magnetic Proton Recoil (MPR) neutron spectrometer is a thin-foil spectrometer which was installed at JET in 1996 and upgraded (MPRu) 2001-2005. \n\nIn the MPR the fusion neutrons are collimated into a neutron beam. The neutron beam is directed onto a thin plastic film (Polyethylene) where the neutrons scatter elastically on the protons of the foil. The recoil protons emitted in the forward direction enter a magnetic part of the spectrometer where they are momentum analyzed and focused onto the focal plane. An array of plastic scintillators coupled to photomultiplier tubes (PMTs) register the spatial distribution of the protons. This proton distribution is then related to the neutron energy spectrum.\n\nThe MPR has a semi-tangential line of sight through the plasma. The MPR has a 700 mm long cylindrical steel neutron collimator with a 10-cm2 bore. At a distance 170 mm behind the end of the collimator is placed a 10 cm2 polythene conversion foil, defining the active area of the spectrometer. The collimator-foil arrangement defines the spectrometers field-of-view into the plasma.\n\nThe MPR determines the neutron spectrum from which important plasma parameters can be determined, such as the ion temperature, the collective motion of the main plasma, the fuel ion densities and their velocity distributions.\n\n\nThe thin-foil magnetic proton recoil neutron spectrometer MPRu at JET \n"}
{"id": "21561", "url": "https://en.wikipedia.org/wiki?curid=21561", "title": "Nanoengineering", "text": "Nanoengineering\n\nNanoengineering is the practice of engineering on the nanoscale. It derives its name from the nanometre, a unit of measurement equalling one billionth of a meter.\n\nNanoengineering is largely a synonym for nanotechnology, but emphasizes the engineering rather than the pure science aspects of the field.\n\nThe first nanoengineering program was started at the University of Toronto within the Engineering Science program as one of the options of study in the final years. In 2003, the Lund Institute of Technology started a program in Nanoengineering. In 2004, the College of Nanoscale Science and Engineering at SUNY Polytechnic Institute was established on the campus of the University at Albany. In 2005, the University of Waterloo established a unique program which offers a full degree in Nanotechnology Engineering. Louisiana Tech University started the first program in the U.S. in 2005. In 2006 the University of Duisburg-Essen started a Bachelor and a Master program NanoEngineering. Unlike early NanoEngineering programs, the first Nanoengineering Department in the world, offering both undergraduate and graduate degrees, was established by the University of California, San Diego in 2007.\nIn 2009, the University of Toronto began offering all Options of study in Engineering Science as degrees, bringing the second nanoengineering degree to Canada. Rice University established in 2016 a Department of Materials Science and NanoEngineering (MSNE).\nDTU Nanotech - the Department of Micro- and Nanotechnology - is a department at the Technical University of Denmark established in 1990.\n\nIn 2013, Wayne State University began offering a Nanoengineering Undergraduate Certificate Program, which is funded by a Nanoengineering Undergraduate Education (NUE) grant from the National Science Foundation. The primary goal is to offer specialized undergraduate training in nanotechnology. Other goals are: 1) to teach emerging technologies at the undergraduate level, 2) to train a new adaptive workforce, and 3) to retrain working engineers and professionals.\n\n\n\n \n"}
{"id": "15324726", "url": "https://en.wikipedia.org/wiki?curid=15324726", "title": "Odyssey Moon", "text": "Odyssey Moon\n\nOn 6 December 2007, Odyssey Moon was the first team to register for the Google Lunar X Prize competition, an event that hopes to rekindle the efforts of humans to return to the moon. The competition is referred to as \"Moon 2.0\" and is composed of other private organizations like Odyssey Moon Limited, the commercial lunar enterprise that makes up this team. Each team will be competing for a $20 million first prize, a $5 million second prize, and additional $5 million in (potential) bonuses.\nOdyssey Moon Limited is based on The Isle of Man, and is the design of Robert D. Richards. His goals include developing the first commercial enterprise that utilizes the energy and resources on the moon. To achieve this end, the team enlisted the part-time consultant services of Alan Stern, NASA's former top-rank planetary scientist. On 22 September 2008, another veteran of NASA joined Odyssey Moon. Jay F. Honeycutt was named president and will be responsible for all programs and commercial launch operations. He brings a great deal of expertise in managing large scale engineering operations. His experience at NASA was diverse. He was director of the Kennedy Space Center for several years and was director of Shuttle Management and Operations for more than five years. Outside NASA, another part of his forty years of professional experience was as president of Lockheed Martin Space Operations from 1997-2004.\n\nThe team's goals are to build and deploy a robotic lander that will deliver exploration as well as scientific payloads to the moon. The new lander/spacecraft has been dubbed \"MoonOne (M-1)\". These efforts have been contracted to MacDonald Dettwiler, a Canadian corporation with a successful history of providing technical space solutions for several NASA projects including the Space Shuttle and the International Space Station.\n\nThe Planetary Society, an international space interest group co-founded by Carl Sagan, joined Odyssey Moon's efforts in 2007, specifically with public outreach and coordination between public and private organizations.\n\nColin Pillinger, a scientist with a background in studying meteorites, led the European Space Agency's failed Beagle 2 Mars lander project in 2003. In 2009 he was in discussion with Odyssey Moon regarding the use of an identical version of Beagle's most powerful instrument on their lander.\n\n\n\n"}
{"id": "1869246", "url": "https://en.wikipedia.org/wiki?curid=1869246", "title": "Operation Crossbow", "text": "Operation Crossbow\n\nCrossbow was the code name of the World War II campaign of Anglo-American \"operations against all phases of the German long-range weapons programme. It included operations against research and development of the weapons, their manufacture, transportation and their launching sites, and against missiles in flight\".\n\nThe original 1943 code name \"Bodyline\" was replaced with \"Crossbow\" on November 15, 1943. Post-war, Crossbow operations became known as Operation \"Crossbow\" as early as 1962, particularly following the 1965 film of the same name.\n\nIn May 1943 Allied surveillance observed the construction of the first of 11 large sites in northern France for secret German weapons, including six for the V-2 rocket. In November it discovered the first of 96 \"ski sites\" for the V-1 flying bomb.\n\nOfficials debated the extent of the German weapons' danger; some viewed the sites as decoys to divert Allied bombers, while others feared chemical or biological warheads. When reconnaissance and intelligence information regarding the V-2 became convincing, the War Cabinet Defence Committee (Operations) directed the campaign's first planned raid (the Operation Hydra attack of Peenemünde in August 1943).\n\nFollowing Operation Hydra, a few \"Crossbow\" attacks were conducted on the \"Heavy Crossbow\" bunkers of Watten (V-2) and Mimoyecques (V-3) through November. \"Crossbow Operations Against Ski Sites\" began on December 5 with the \"Noball\" code name used for the targets (e.g., 'Noball 27' was the site, \"Noball No. 93\" was in the Cherbourg area, \"Noball No. 107\" was at Grand Parc, and \"Noball V1 site No.147\" was at Ligescourt).\n\nThe US formed its own Crossbow Committee under General Stephen Henry (New Developments Division) on December 29, 1943, and the US subsequently developed bombing techniques for ski sites in February/March 1944 at the Air Corps Proving Ground (a June plan to attack V-1 launch sites from aircraft carriers with USMC fighters was disapproved). V-2 facilities were also bombed in 1944, including smaller facilities such as V-2 storage depots and liquid oxygen plants, such as the Mery-sur-Oise V-2 storage depot on August 4, 1944 and, by the Eighth Air Force, which bombed five cryogenic LOX plants in Belgium on August 25, 1944 and aborted the next day \"to hit liquid oxygen plants at La Louviere, Torte and Willebroeck, Belgium ... due to clouds.\"\n\nAt the request of the British War Cabinet, on April 19, 1944, Dwight Eisenhower directed \"Crossbow\" attacks to have absolute priority over all other air operations, including \"wearing down German industry\" and morale \"for the time being\", which he confirmed after the V-1 assault began on the night of June 12/13, 1944: \"with respect to \"Crossbow\" targets, these targets are to take first priority over everything except the urgent requirements of the Overlord battle; this priority to obtain until we can be certain that we have definitely gotten the upper hand of this particular business\" (Eisenhower to Arthur Tedder, June 16). The launches surprised the Allies, who had believed that the earlier attacks on the sites had eliminated the danger. The British, who had not expected German bombing of Britain to resume so late in the war, were especially upset. Some suggested using gas on the launch sites, or even executing German civilians as punishment.\n\nCarl Spaatz, commander of U.S. Strategic Air Forces in Europe (USTTAF), responded on June 28 to \"complain that \"Crossbow\" was a 'diversion' from the main task of wearing down the Luftwaffe and bombing German industry\" for the Combined Bomber Offensive, and to recommend instead that \"Crossbow\" be a secondary priority since \"days of bad weather over Germany's industrial targets would still allow enough weight of attack for the rocket sites and the lesser tactical crises.\" By July 10, Tedder had published a list of Crossbow targets which assigned 30 to RAF Bomber Command, six to Tedder's tactical forces, and 68 to Spaatz' USSTAF; after which Spaatz again complained, so Eisenhower allowed \"spare\" bombing of non-Crossbow targets: \"Instructions for continuing to make \"Crossbow\" targets our first priority must stand, but ... when ... the entire strategic forces cannot be used against \"Crossbow\", we should attack—(a) Aircraft industry, (b) Oil, (c) ball bearing (German): Kugellagerwerke, (d) Vehicular production\" (Eisenhower, July 18).\n\nNonetheless, over 25 percent of the Combined Bomber Offensive's tonnage of bombs were used against V-weapon sites in July and August; many of the attacks were ineffective, as they were against unused sites rather than the launchers themselves. Spaatz unsuccessfully proposed that attacks concentrate on the Calais electrical grid, and on gyrocompass factories in Germany and V-weapon storage depots in France. The gyrocompass attacks, along with targeting liquid oxygen tanks (which the Allies knew the V-2 needed), might have been very effective against the missiles. On August 25, 1944, the Joint Crossbow Target Priorities Committee (established July 21) prepared the \"Plan for Attack on the German Rocket Organization When Rocket Attacks Commence\"—in addition to bombing of storage, liquid-oxygen, and launch sites; the plan included aerial reconnaissance operations.\nFollowing the last V-1 launch from France on September 1, 1944, and since the expected V-2 attacks had not begun, \"Crossbow\" bombing was suspended on September 3 and the campaign against German oil facilities became the highest priority.\n\nThe V-1 threat from occupied France ended on September 5, 1944, when elements of the 7th Canadian Reconnaissance Regiment and the 3rd Canadian Infantry Division contained the German military units of the Nord-Pas de Calais area with their surrender following on September 30.\n\n\"Crossbow\" bombing resumed after the first V-2 attack and included a large September 17 raid on Dutch targets suspected as bases for Heinkel He 111s, which were air-launching V-1s. Modified V-1s (865 total) were \"air-launched\" from September 16, 1944 to January 14, 1945. The British had initially considered that an earlier July 18–21, 1944 effort of 50 air-launched V-1s had been \"ground-launched\" from the Low Countries, particularly near Ostend. In addition to air-launched V-1s, launches were from ramps built in the province of South Holland, the Netherlands in 1945.\n\nAllied reconnaissance detected two sites at Vlaardingen and Ypenburg, and along with a third at Delft, they launched 274 V-1s at London from March 3–29. Only 125 reached the British defences, and only thirteen of those reached the target area. Three additional sites directed their fire on Antwerp. After using medium bombers against V-2 launch site in the Haagse Bos on March 3, the RAF attacked the Holland V-1 sites with two squadrons. An RAF Fighter Command unit used Spitfires against Ypenburg on March 20 and 23, while a 2nd Tactical Air Force unit used Typhoons against Vlaardingen on March 23. Counterattacks on Holland's V-1 and V-2 sites ended on April 3, and all \"Crossbow\" countermeasures ended on May 2 with the end of World War II in Europe.\n\nOn January 2, 1944, Roderic Hill submitted his plan to deploy 1,332 guns for the air defence of London, Bristol and the Solent against the V-1 \"Robot Blitz\" (the \"Diver Operations Room\" was at RAF Biggin Hill). V-1s that had not run out of fuel or veered off course were attacked by select units of Fighter Command (No. 150 Wing RAF) operating high speed fighters, the anti-aircraft guns of Anti-Aircraft Command, and approximately 1,750 barrage balloons of Balloon Command around London.\"\n\n\"Flabby\" was the code name for medium weather conditions when fighters were allowed to chase flying bombs over the gun-belt to the balloon line, and during Operation \"Totter\", the Royal Observer Corps fired \"Snowflake\" illuminating rocket flares from the ground to identify V-1 flying bombs to RAF fighters. After the Robot Blitz began on the night of June 12/13, an RAF fighter first intercepted a V-1 on June 14/15. Moreover, anti-aircraft guns increased the rate of downed V-1s to 1 per 77 rounds fired after \"the first few weeks of proximity fuse operation\" (Reginald Victor Jones). By June 27, \"over 100,000 houses had been damaged or destroyed by the V-1 ... and shattered sewage systems threatened serious epidemics unless fixed by winter.\"\n\nOf the 638 air-launched V-1s that had been observed (e.g., by the Royal Observer Corps), guns and fighters downed 403 and the remainder fell in the London Civil Defence Region (66), at Manchester (1), or elsewhere (168, including Southampton on July 7). Additionally, the gunners on W/Cdr. S.G. Birch's Lancaster claimed they downed a V-1 over the target area on a March 3, 1945, raid on the Ladbergen aqueduct.\n\nThe Bodyline Scientific Committee (19 members, including Duncan Sandys, Edward Victor Appleton, John Cockcroft, Robert Watson-Watt) was formed in September 1943 regarding the suspected V-2 rocket, and after the 1944 crash of a test V-2 in Sweden, \"transmitters to jam the guidance system of the rocket\" were prepared. A British sound-ranging system provided \"trajectory [data] from which the general launching area could be determined\", and the microphone(s) in East Kent reported the times of the first V-2 strikes on September 8, 1944: 18:40:52 and 18:41:08.\n\nOn March 21, 1945, the plan for the \"Engagement of Long Range Rockets with AA Gunfire\" which called for anti-aircraft units to fire into a radar-predicted airspace to intercept the V-2 was ready, but the plan was not used due to the danger of shells falling on Greater London. Happenstance instances of Allied aircraft engaging launched V-2 rockets include the following:\n\nAfter the last combat V-2 launch on March 27, 1945, the British discontinued their use of radar in the defence region to detect V-2 launches on April 13.\n\n\n"}
{"id": "23872172", "url": "https://en.wikipedia.org/wiki?curid=23872172", "title": "POSC Caesar", "text": "POSC Caesar\n\nPOSC Caesar Association (PCA) is an international, open, not-for-profit, member organization that promotes the development of open specifications to be used as standards for enabling the interoperability of data, software and related matters.\n\nPCA is the initiator of ISO 15926 \"Integration of life-cycle data for process plants including oil and gas production facilities\" and is committed to its maintenance and enhancement.\n\nNils Sandsmark has been the General Manager of POSC Caesar Association since 1999 and Thore Langeland, Norwegian Oil Industry Association (, OLF), is the Chairman of the Board.\n\nThe first predecessor of POSC Caesar Association, the Caesar Offshore program, started in 1993.\nThe original focus was on standardizing technical data definitions for capital intensive projects at the handover from the EPC contractor to the owner/operators of onshore and offshore oil and gas production facilities. The program was sponsored by The Research Council of Norway, two EPC contractors (Aker Maritime and Kværner), three owners/operators (Norsk Hydro, Saga Petroleum and Statoil) and DNV as service provider and project owner.\n\nDuring the period 1994-96, Caesar Offshore Program was defined as a project of Petrotechnical Open Software Corporation (POSC) (now Energistics), and changed its name to the POSC Caesar Project.\n\nIn 1995 the project was joined by BP, Brown and Root and Elf Aquitaine and in 1997 by Intergraph, IBM, Oracle, Lloyd's, Shell, ABB and UMOE Technologies.\n\nDuring that time, POSC Caesar also became a member of European Process Industries STEP Technical Liaison Executive (EPISTLE) where it collaborates with PISTEP (UK), and USPI-NL (The Netherlands) on the development of ISO 10303, also known as \"Standard for the Exchange of Product model data (STEP).\"\n\nIn 1997, POSC Caesar Association was founded as an independent, global, non-profit, member organization. POSC Caesar Association serves an international membership and collaborates with other international organizations. It has its main office in Norway.\n\nAlbeit the name of POSC Caesar Association still hints to its past as a project within the Petrotechnical Open Software Corporation (POSC) (now Energistics), from 1997 onwards, the organization has been independent. Energistics and POSC Caesar Association do collaborate, and are formally member in each other's organization.\n\nPOSC Caesar Association has with its current 36 members from around the world established an international footprint (with a strong membership in Norway) that includes a wide range from academia, solution providers to engineering contractors and owners/operators. The members are (subdivided by organization type):\n\nIn general, the organization holds three membership meetings a year; one in January / February in North-America (typically USA), one in April / May in Europe (typically Norway) and one in October in Asia (typically Malaysia).\n\nIn consultation with the other EPISTLE members and the International Organization for Standardization (ISO), it was decided in 2003 (some say already in 1997) that for modeling-technical reasons it was better to discontinue the development of ISO 10303 and to initiate the development of ISO 15926 \"Integration of life-cycle data for process plants including oil and gas production facilities.\"\n\nOver the years, the scope of the standard has increased from the initial capital-intensive projects in the upstream oil and gas industry, to include also relevant terminology for downstream oil and gas industry applications and to deal with real-time data related to the actual oil and gas production.\n\nISO 15926 has also over the years evolved from a dictionary (a list of terms with definitions), over a taxonomy (added hierarchy) to an ontology (a formal representation of a set of concepts within a domain and the relationships between those concepts). ISO 15926 is therefore sometimes nicknamed the \"Oil and Gas Ontology.\", for some considered to be an essential prerequisite together with Semantic Web technologies\nto get to better interoperability, an optimal use of all available data across boundaries and an increase in efficiency. This is what some call the next generation of Integrated Operations.\n\nPlaceholders:\n\nPlaceholders:\n\nThere are a number of projects (co-)organized by POSC Caesar Association working on the extension of the ISO 15926 standard in different application areas.\n\nThe following projects are running at the moment (August 2009):\n\n\nThe following projects are currently running (August 2009):\n\n\nFinalised projects include:\n\n\nPOSC Caesar is collaborating with a number of standardization bodies, including:\n\n\n"}
{"id": "25959502", "url": "https://en.wikipedia.org/wiki?curid=25959502", "title": "Planning theory in ancient China", "text": "Planning theory in ancient China\n\nAncient large-scale cities in China were always governmental center of the royal blood and planned to be symbols of this kind of government. The design ideas and city layout were affected by consciousness of concentrated royal power. By means of social research methods, the article confirms the close relationship between city planning ideas and political system in order to establish a sort of research base for the modern planning system.\n\nIn Chinese traditional philosophy, it focuses on the practice. Specifically, architecture comes with transport, apparel and tool. Architecture is the crucial factor, not because of its value of art. It can be seen just as a symbol of class.\n\nThe “Thought of Etiquette” decides the expression of planning and architecture in ancient China.In China’s traditional urban planning, the ruling class use physical boundaries to protect themselves. “The Great Wall” is one of the most famous physical boundaries in China. The theory of urban planning in ancient China reflects the Thought of Etiquette’s impact on the urban construction. 《周礼.考工记》 is one chapter of the \"Thought of Etiquette\". In this chapter, it talks about the regulation of urban planning in ancient China.\n\nThe concept of “urban” in ancient China is separated into two parts: one is “city”, the place that the ruling class and residents live. The other is “market”, the place that residents can have commercial activities.\n\nThe history of urban development in China begins in Zhou dynasty (1066BC). Politic is the most important factor of the urban existence in ancient China. Therefore, the original and development reflect Deep-rooted authoritarian political regimes. The ruling class not only decides urban construction, but also has some special ideas of the urban scale, layout, functional division and style of architecture during the process of urban construction.\n\nThe influence of the ruler’s thoughts is obvious. The checkerboard pattern appeared. The layout of most cities highlights the political center. Emphasized on the axis and layout palace in front, markets in back. Laid the basic pattern for the next 2000 years development.\n\nHe, Congrong (2007, December 18). \"Chapter 2 Architecture of Xia, Shang, Zhou Dynasties and Spring and Autumn Period\". CORE OCW. https://web.archive.org/web/20081110104828/http://202.205.161.91/CORE/architecture/the-history-of-ancient-chinese-architecture/chater-2-architecture-of-xia-shang-zhou-dynasties-and-spring-and-autumn-period/. Retrieved February 8, 2009.\n\nFrieldmann, J. 1995. Where We Stand: Decade of World City Research, in Knox P and Taylor P J (eds). World Cities in a World System. Cambridge: Cambridge UP 21–47(p. 15).\n"}
{"id": "3027901", "url": "https://en.wikipedia.org/wiki?curid=3027901", "title": "Plutonism", "text": "Plutonism\n\nPlutonism (or volcanism) is the geologic theory that the igneous rocks forming the Earth originated from intrusive magmatic activity, with a continuing gradual process of weathering and erosion wearing away rocks, which were then deposited on the sea bed, re-formed into layers of sedimentary rock by heat and pressure, and raised again. It proposes that basalt is solidified molten magma. The name \"plutonism\" references Pluto, the classical ruler of the underworld, while \"volcanism\" echoes the name of Vulcan, the ancient Roman god of fire and volcanoes. The \"Oxford English Dictionary\" traces use of the word \"plutonists\" to 1799, and the appearance of the word \"plutonism\" to 1842.\n\nAbbé Anton Moro, who had studied volcanic islands, first proposed the theory before 1750, and James Hutton subsequently developed it as part of his \"Theory of the Earth\",\npublished in 1788. The idea contested Abraham Werner's neptunist theory which proposed that the Earth had formed from a mass of water and suspended material which had formed rocks as layers of deposited sediment which became the continents when the water retreated, further layers being deposited by floods and some volcanic activity.\n\nPlutonists strongly disputed the neptunist view that rocks had formed by processes that no longer operated, instead supporting Hutton's uniformitarianism. A key issue of the debate revolved around the neptunist belief that basalt was sedimentary, and some fossils had been found in it. Against this, Hutton's friend John Playfair (1748-1819) argued that this rock contained no fossils as it had formed from molten magma, and it had been found cutting through other rocks in volcanic dykes. The arguments continued into the early 19th century, and eventually the plutonist views on the origin of rocks prevailed in the wake of the work of Charles Lyell in the 1830s. However, geologists regard sedimentary rocks such as limestone as having resulted from processes like those described by the neptunists, and so modern petrological theory can be seen as a synthesis of the two approaches.\n\n"}
{"id": "49803411", "url": "https://en.wikipedia.org/wiki?curid=49803411", "title": "Psychology &amp; Developing Societies", "text": "Psychology &amp; Developing Societies\n\nPsychology and Developing Societies (PDS) is a peer reviewed journal. This is a forum for discussion for psychologists from different parts of the world concerned with the problems of developing societies. PDS provides information in different areas of psychology. \nIt is published twice a year by SAGE Publications\n\n\"Psychology and Developing Societies\" is abstracted and indexed in:\n\n"}
{"id": "25677465", "url": "https://en.wikipedia.org/wiki?curid=25677465", "title": "RNAi Global Initiative", "text": "RNAi Global Initiative\n\nThe RNAi Global Initiative is an alliance of international biomedical researchers that has been established to increase and accelerate the utility of genome-wide RNAi libraries.\n\nGenome-wide RNAi screening has the potential to fundamentally change biological research by increasing scientists' ability to understand disease mechanisms and facilitating faster drug discovery and development. The RNAi Global Initiative provides a forum for member institutions to share research protocols, establish experimental standards, and develop mechanisms for exchanging and comparing screening data.\n\nThis ongoing interaction between the RNAi Global Initiative members is expected to help researchers optimize high-throughput human-genome-wide RNAi screening and accelerate drug discovery. Membership is open to non-profit biomedical research institutions across the globe.\n\nThe RNAi Global Initiative was established and is being coordinated under the auspices of the Dharmacon Product line of GE Healthcare, whose Research and Development scientists actively contribute to the Initiative.\n\nThrough collaboration and the meaningful exchange of information and data, the RNAi Global Initiative intends to draw a comprehensive roadmap of human gene function and use this as a foundation to revolutionize the way medicine and healthcare are delivered.\n\nTo this end, members of the RNAi Global Initiative are actively engaged in promoting the concept and implementation of minimum information standards to facilitate data sharing within the extended RNAi community. Building on established standards such as MIAME (Minimum Information About a Microarray Experiment), the RNAi Global Initiative has contributed work towards a community-wide effort known as the Minimum Information About an RNAi Experiment (MIARE). These reporting guidelines were developed in part by a large inter-laboratory benchmarking study and in part by workshops and discussions amongst the RNAi Global Initiative members.\n\nAs of January 2010, there were over 50 member groups in 15\ndifferent countries.\n"}
{"id": "34612110", "url": "https://en.wikipedia.org/wiki?curid=34612110", "title": "Robert C. Thorne", "text": "Robert C. Thorne\n\nRobert Coin Thorne (25 November 1898 – 27 May 1960) was an American paleontologist.\n\nThorne was born in Ashley, Utah.\n\nHe participated at the \"2nd Captain Marshall Field Paleontological Expedition\" in 1926. Other participants were Elmer S. Riggs (Leader and Photographer), Rudolf Stahlecker (Collector) and Felipe Mendez. The expedition started in April 1926 and finished in November 1926. The purpose was geology fossil collecting in Puerta Corral Quemado, Catamarca, Argentina, South America. The expedition was successful, and even new species like Stahleckeria have been found during this collaboration.\n\nHe was a veteran of World War I, an experienced outdoors man, mule driver and fossil collector. He was married to Constance and had with her a son, R. Neil Thorne. His letters about the expedition to his wife have 70 years later been published by their son at his own expense.\n\nHe died in Vernal, Utah.\n\n"}
{"id": "3711690", "url": "https://en.wikipedia.org/wiki?curid=3711690", "title": "Shuttle Landing Facility", "text": "Shuttle Landing Facility\n\nThe Shuttle Landing Facility (SLF) is an airport located on Merritt Island in Brevard County, Florida, USA. It is a part of the John F. Kennedy Space Center (KSC), and was used by NASA's Space Shuttle for landing until the program's end in 2011. It was also used for takeoffs and landings for NASA training jets such as the Shuttle Carrier Aircraft and for civilian aircraft.\n\nStarting in 2015, Space Florida manages and operates the facility under a 30-year lease from NASA. Private companies have been utilizing the SLF for its unique properties since 2011 and will continue to do so via Space Florida.\n\nThe Shuttle Landing Facility covers and has a single runway, 15/33. It is one of the longest runways in the world, at , and is wide. (Despite its length, astronaut Jack R. Lousma stated that he would have preferred the runway to be \"half as wide and twice as long\".) Additionally, the SLF has of paved overruns at each end. The Mate-Demate Device (MDD), for use when the shuttle was transported by the Shuttle Carrier Aircraft, was located just off the runway.\n\nThe runway is designated runway 15, or 33, depending on the direction of use. The runway surface consists of an extremely high-friction concrete strip designed to maximize the braking ability of the Space Shuttle at its high landing speed, with a paving thickness of at the center. It uses a grooved design to provide drainage and further increase the coefficient of friction. The original groove design was found to actually provide too much friction for the rubber used in the Shuttle's tires, causing failures during several landings. This issue was resolved by grinding down the pavement, reducing the depth of the grooves significantly.\n\nA local nickname for the runway is the \"gator tanning facility\", as some of the 4,000 alligators living at Kennedy Space Center regularly bask in the sun on the runway.\n\nThe landing facility is managed by contractor EG&G, which provides air traffic control services, as well as managing potential hazards to landing aircraft, such as bird life. The Bird Team kept the facility clear of both local and migratory birds during shuttle landings using pyrotechnics, blank rounds fired from shotguns and a series of 25 propane cannons arranged around the facility.\n\n\"Columbia\" was the first shuttle to arrive at the SLF via the Shuttle Carrier Aircraft on March 24, 1979.\n\nThe runway was first used by a space shuttle on 11 February 1984, when the STS-41-B mission returned to Earth. This also marked the first-ever landing of a spacecraft at its launch site. Prior to this, all shuttle landings were performed at Edwards Air Force Base in California (with the exception of STS-3, which landed at White Sands Space Harbor) while the landing facility continued testing and shuttle crews developed landing skills at White Sands and Edwards, where the margin for error is much greater than SLF and its water hazards. On September 22, 1993, \"Discovery\" was the first space shuttle to land at night at the SLF. A total of 78 shuttle missions landed at the SLF.\n\nThe final landing of a space shuttle occurred on July 21, 2011 by \"Atlantis\" for STS-135. \"Discovery\" and \"Endeavour\" took off from the SLF on top of the Shuttle Carrier Aircraft for museums in Washington, D.C. and Los Angeles.\n\nIn 2012, NASA's Johnson Space Center's Project Morpheus's first vehicle arrived at KSC. Prior to arrival at KSC and throughout the project, Morpheus vehicle tests were performed at other NASA centers; KSC was the site for advanced testing. Multiple tests, including free flight, were performed at the SLF in 2013-2014. Multiple vehicles and iterations of the vehicles were tested, due to upgrades and damages during this experimental test program. During the August 9, 2012 test at the SLF, a vehicle exploded; no one was injured.\n\nThe SLF has also been used by commercial users. Zero Gravity Corporation, which offers flights where passengers experience brief periods of microgravity, has operated from the SLF, as have record-setting attempts by the Virgin Atlantic GlobalFlyer.\nNASCAR teams have also used the facility for vehicle testing.\n\nIn 2012, Performance Power's Johnny Bohmer did set the Guinness World Record for the Fastest Standing Mile-Street Car when his Ford GT broke the barrier, setting the record at . \n\nIn 2014, the Hennessey Venom GT recorded a top speed of .\n\n\n"}
{"id": "28189", "url": "https://en.wikipedia.org/wiki?curid=28189", "title": "Space Shuttle", "text": "Space Shuttle\n\nThe Space Shuttle was a partially reusable low Earth orbital spacecraft system operated by the U.S. National Aeronautics and Space Administration (NASA) as part of the Space Shuttle program. Its official program name was \"Space Transportation System (STS)\", taken from a 1969 plan for a system of reusable spacecraft of which it was the only item funded for development. The first of four orbital test flights occurred in 1981, leading to operational flights beginning in 1982. In addition to the prototype whose completion was cancelled, five complete Shuttle systems were built and used on a total of 135 missions from 1981 to 2011, launched from the Kennedy Space Center (KSC) in Florida. Operational missions launched numerous satellites, interplanetary probes, and the Hubble Space Telescope (HST); conducted science experiments in orbit; and participated in construction and servicing of the International Space Station. The Shuttle fleet's total mission time was 1322 days, 19 hours, 21 minutes and 23 seconds.\n\nShuttle components included the Orbiter Vehicle (OV) with three clustered Rocketdyne RS-25 main engines, a pair of recoverable solid rocket boosters (SRBs), and the expendable external tank (ET) containing liquid hydrogen and liquid oxygen. The Space Shuttle was launched vertically, like a conventional rocket, with the two SRBs operating in parallel with the OV's three main engines, which were fueled from the ET. The SRBs were jettisoned before the vehicle reached orbit, and the ET was jettisoned just before orbit insertion, which used the orbiter's two Orbital Maneuvering System (OMS) engines. At the conclusion of the mission, the orbiter fired its OMS to de-orbit and re-enter the atmosphere. The orbiter then glided as a spaceplane to a runway landing, usually to the Shuttle Landing Facility at Kennedy Space Center, Florida or Rogers Dry Lake in Edwards Air Force Base, California. After landing at Edwards, the orbiter was flown back to the KSC on the Shuttle Carrier Aircraft, a specially modified Boeing 747.\n\nThe first orbiter, \"Enterprise\", was built in 1976, used in Approach and Landing Tests and had no orbital capability. Four fully operational orbiters were initially built: \"Columbia\", \"Challenger\", \"Discovery\", and \"Atlantis\". Of these, two were lost in mission accidents: \"Challenger\" in 1986 and \"Columbia\" in 2003, with a total of fourteen astronauts killed. A fifth operational (and sixth in total) orbiter, \"Endeavour\", was built in 1991 to replace \"Challenger\". The Space Shuttle was retired from service upon the conclusion of \"Atlantis\"s final flight on July 21, 2011. The U.S. has since relied primarily on the Russian Soyuz spacecraft to transport supplies and astronauts to the International Space Station.\n\nThe Space Shuttle was a partially reusable human spaceflight vehicle capable of reaching low Earth orbit, commissioned and operated by the U.S. National Aeronautics and Space Administration (NASA) from 1981 to 2011. It resulted from shuttle design studies conducted by NASA and the U.S. Air Force in the 1960s and was first proposed for development as part of an ambitious second-generation Space Transportation System (STS) of space vehicles to follow the Apollo program in a September 1969 report of a Space Task Group headed by Vice President Spiro Agnew to President Richard Nixon. Nixon's post-Apollo NASA budgeting withdrew support of all system components except the Shuttle, to which NASA applied the STS name.\n\nThe vehicle consisted of a spaceplane for orbit and re-entry, fueled from an expendable External Tank containing liquid hydrogen and liquid oxygen, with two reusable strap-on solid rocket boosters. The first of four orbital test flights occurred in 1981, leading to operational flights beginning in 1982, all launched from the Kennedy Space Center, Florida. The system was retired from service in 2011 after 135 missions, with \"Atlantis\" making the final launch of the three-decade Shuttle program on July 8, 2011. The program ended after \"Atlantis\" landed at the Kennedy Space Center on July 21, 2011. Major missions included launching numerous satellites and interplanetary probes, conducting space science experiments, and servicing and construction of space stations. The first orbiter vehicle, named \"Enterprise\", was used in the initial Approach and Landing Tests phase but installation of engines, heat shielding, and other equipment necessary for orbital flight was cancelled. A total of five operational orbiters were built, and of these, two were destroyed in accidents.\n\nIt was used for orbital space missions by NASA, the U.S. Department of Defense, the European Space Agency, Japan, and Germany. The United States funded Shuttle development and operations except for the Spacelab modules used on D1 and D2sponsored by Germany. SL-J was partially funded by Japan.\n\nAt launch, it consisted of the \"stack\", including the dark orange external tank (ET) (for the first two launches the tank was painted white); two white, slender solid rocket boosters (SRBs); and the Orbiter Vehicle, which contained the crew and payload. Some payloads were launched into higher orbits with either of two different upper stages developed for the STS (single-stage Payload Assist Module or two-stage Inertial Upper Stage). The Space Shuttle was stacked in the Vehicle Assembly Building, and the stack mounted on a mobile launch platform held down by four frangible nuts on each SRB, which were detonated at launch.\n\nThe Shuttle stack launched vertically like a conventional rocket. It lifted off under the power of its two SRBs and three main engines, which were fueled by liquid hydrogen and liquid oxygen from the ET. The Space Shuttle had a two-stage ascent. The SRBs provided additional thrust during liftoff and first-stage flight. About two minutes after liftoff, frangible nuts were fired, releasing the SRBs, which then parachuted into the ocean, to be retrieved by NASA recovery ships for refurbishment and reuse. The orbiter and ET continued to ascend on an increasingly horizontal flight path under power from its main engines. Upon reaching 17,500 mph (7.8 km/s), necessary for low Earth orbit, the main engines were shut down. The ET, attached by two frangible nuts was then jettisoned to burn up in the atmosphere. After jettisoning the external tank, the orbital maneuvering system (OMS) engines were used to adjust the orbit.\nThe orbiter carried astronauts and payloads such as satellites or space station parts into low Earth orbit, the Earth's upper atmosphere or thermosphere. Usually, five to seven crew members rode in the orbiter. Two crew members, the commander and pilot, were sufficient for a minimal flight, as in the first four \"test\" flights, STS-1 through STS-4. The typical payload capacity was about but could be increased depending on the choice of launch configuration. The orbiter carried its payload in a large cargo bay with doors that opened along the length of its top, a feature which made the Space Shuttle unique among spacecraft. This feature made possible the deployment of large satellites such as the Hubble Space Telescope and also the capture and return of large payloads back to Earth.\n\nWhen the orbiter's space mission was complete, it fired its OMS thrusters to drop out of orbit and re-enter the lower atmosphere. During descent, the orbiter passed through different layers of the atmosphere and decelerated from hypersonic speed primarily by aerobraking. In the lower atmosphere and landing phase, it was more like a glider but with reaction control system (RCS) thrusters and fly-by-wire-controlled hydraulically actuated flight surfaces controlling its descent. It landed on a long runway as a conventional aircraft. The aerodynamic shape was a compromise between the demands of radically different speeds and air pressures during re-entry, hypersonic flight, and subsonic atmospheric flight. As a result, the orbiter had a relatively high sink rate at low altitudes, and it transitioned during re-entry from using RCS thrusters at very high altitudes to flight surfaces in the lower atmosphere.\n\nThe formal design of what became the Space Shuttle began with the \"Phase A\" contract design studies issued in the late 1960s. Conceptualization had begun two decades earlier, before the Apollo program of the 1960s. One of the places the concept of a spacecraft returning from space to a horizontal landing originated was within NACA, in 1954, in the form of an aeronautics research experiment later named the X-15. The NACA proposal was submitted by Walter Dornberger.\n\nIn 1958, the X-15 concept further developed into a proposal to launch an X-15 into space, and another X-series spaceplane proposal, named X-20 Dyna-Soar, as well as variety of aerospace plane concepts and studies. Neil Armstrong was selected to pilot both the X-15 and the X-20. Though the X-20 was not built, another spaceplane similar to the X-20 was built several years later and delivered to NASA in January 1966 called the HL-10 (\"HL\" indicated \"horizontal landing\").\n\nIn the mid-1960s, the U.S. Air Force conducted classified studies on next-generation space transportation systems and concluded that semi-reusable designs were the cheapest choice. It proposed a development program with an immediate start on a \"Class I\" vehicle with expendable boosters, followed by slower development of a \"Class II\" semi-reusable design and possible \"Class III\" fully reusable design later. In 1967, George Mueller held a one-day symposium at NASA headquarters to study the options. Eighty people attended and presented a wide variety of designs, including earlier U.S. Air Force designs such as the X-20 Dyna-Soar.\n\nIn 1968, NASA officially began work on what was then known as the Integrated Launch and Re-entry Vehicle (ILRV). At the same time, NASA held a separate Space Shuttle Main Engine (SSME) competition. NASA offices in Houston and Huntsville jointly issued a Request for Proposal (RFP) for ILRV studies to design a spacecraft that could deliver a payload to orbit but also re-enter the atmosphere and fly back to Earth. For example, one of the responses was for a two-stage design, featuring a large booster and a small orbiter, called the DC-3, one of several Phase A Shuttle designs. After the aforementioned \"Phase A\" studies, B, C, and D phases progressively evaluated in-depth designs up to 1972. In the final design, the bottom stage consisted of recoverable solid rocket boosters, and the top stage used an expendable external tank.\n\nIn 1969, President Richard Nixon decided to support proceeding with Space Shuttle development. A series of development programs and analysis refined the basic design, prior to full development and testing. In August 1973, the X-24B proved that an unpowered spaceplane could re-enter Earth's atmosphere for a horizontal landing.\n\nAcross the Atlantic, European ministers met in Belgium in 1973 to authorize Western Europe's manned orbital project and its main contribution to Space Shuttlethe Spacelab program. Spacelab would provide a multidisciplinary orbital space laboratory and additional space equipment for the Shuttle.\n\nThe Space Shuttle was the first operational orbital spacecraft designed for reuse. It carried different payloads to low Earth orbit, provided crew rotation and supplies for the International Space Station (ISS), and performed satellite servicing and repair. The orbiter could also recover satellites and other payloads from orbit and return them to Earth. Each Shuttle was designed for a projected lifespan of 100 launches or ten years of operational life, although this was later extended. The person in charge of designing the STS was Maxime Faget, who had also overseen the Mercury, Gemini, and Apollo spacecraft designs. The crucial factor in the size and shape of the Shuttle orbiter was the requirement that it be able to accommodate the largest planned commercial and military satellites, and have over 1,000 mile cross-range recovery range to meet the requirement for classified USAF missions for a once-around abort from a launch to a polar orbit. The militarily specified cross range requirement was one of the primary reasons for the Shuttle's large wings, compared to modern commercial designs with very minimal control surfaces and glide capability. Factors involved in opting for solid rockets and an expendable fuel tank included the desire of the Pentagon to obtain a high-capacity payload vehicle for satellite deployment, and the desire of the Nixon administration to reduce the costs of space exploration by developing a spacecraft with reusable components.\n\nEach Space Shuttle was a reusable launch system composed of three main assemblies: the reusable OV, the expendable ET, and the two reusable SRBs. Only the OV entered orbit shortly after the tank and boosters are jettisoned. The vehicle was launched vertically like a conventional rocket, and the orbiter glided to a horizontal landing like an airplane, after which it was refurbished for reuse. The SRBs parachuted to splashdown in the ocean where they were towed back to shore and refurbished for later Shuttle missions.\n\nFive operational OVs were built: \"Columbia\" (OV-102), \"Challenger\" (OV-099), \"Discovery\" (OV-103), \"Atlantis\" (OV-104), and \"Endeavour\" (OV-105). A mock-up, \"Inspiration\", currently stands at the entrance to the Astronaut Hall of Fame. An additional craft, \"Enterprise\" (OV-101), was built for atmospheric testing gliding and landing; it was originally intended to be outfitted for orbital operations after the test program, but it was found more economical to upgrade the structural test article STA-099 into orbiter \"Challenger\" (OV-099). \"Challenger\" disintegrated 73 seconds after launch in 1986, and \"Endeavour\" was built as a replacement from structural spare components. Building \"Endeavour\" cost about US$1.7 billion. \"Columbia\" broke apart over Texas during re-entry in 2003.\n\nA Space Shuttle launch cost around $450 million. Roger A. Pielke, Jr. has estimated that the Space Shuttle program cost about US$170 billion (2008 dollars) through early 2008; the average cost per flight was about US$1.5 billion. Two missions were paid for by Germany, Spacelab D1 and D2 (D for \"Deutschland\") with a payload control center in Oberpfaffenhofen. D1 was the first time that control of a manned STS mission payload was not in U.S. hands.\n\nAt times, the orbiter itself was referred to as the Space Shuttle. This was not technically correct as the \"Space Shuttle\" was the combination of the orbiter, the external tank, and the two solid rocket boosters. These components, once assembled in the Vehicle Assembly Building originally built to assemble the Apollo Saturn V rocket, were commonly referred to as the \"stack\".\n\nResponsibility for the Shuttle components was spread among multiple NASA field centers. The Kennedy Space Center was responsible for launch, landing and turnaround operations for equatorial orbits (the only orbit profile actually used in the program), the U.S. Air Force at the Vandenberg Air Force Base was responsible for launch, landing and turnaround operations for polar orbits (though this was never used), the Johnson Space Center served as the central point for all Shuttle operations, the Marshall Space Flight Center was responsible for the main engines, external tank, and solid rocket boosters, the John C. Stennis Space Center handled main engine testing, and the Goddard Space Flight Center managed the global tracking network.\n\nThe orbiter resembled a conventional aircraft, with double-delta wings swept 81° at the inner leading edge and 45° at the outer leading edge. Its vertical stabilizer's leading edge was swept back at a 50° angle. The four elevons, mounted at the trailing edge of the wings, and the rudder/speed brake, attached at the trailing edge of the stabilizer, with the body flap, controlled the orbiter during descent and landing.\n\nThe orbiter's -long payload bay, comprising most of the fuselage, could accommodate cylindrical payloads up to in diameter. Information declassified in 2011 showed that these measurements were chosen specifically to accommodate the KH-9 HEXAGON spy satellite operated by the National Reconnaissance Office. Two mostly-symmetrical lengthwise payload bay doors hinged on either side of the bay comprised its entire top. Payloads were generally loaded horizontally into the bay while the orbiter was standing upright on the launch pad and unloaded vertically in the near-weightless orbital environment by the orbiter's robotic remote manipulator arm (under astronaut control), EVA astronauts, or under the payloads' own power (as for satellites attached to a rocket \"upper stage\" for deployment.)\n\nThree Space Shuttle Main Engines (SSMEs) were mounted on the orbiter's aft fuselage in a triangular pattern. The engine nozzles could gimbal 10.5 degrees up and down, and 8.5 degrees from side to side during ascent to change the direction of their thrust to steer the Shuttle. The orbiter structure was made primarily from aluminum alloy, although the engine structure was made primarily from titanium alloy.\n\nThe operational orbiters built were OV-102 \"Columbia\", OV-099 \"Challenger\", OV-103 \"Discovery\", OV-104 \"Atlantis\", and OV-105 \"Endeavour\".\n\nThe main function of the Space Shuttle external tank was to supply the liquid oxygen and hydrogen fuel to the main engines. It was also the backbone of the launch vehicle, providing attachment points for the two solid rocket boosters and the orbiter. The external tank was the only part of the Shuttle system that was not reused. Although the external tanks were always discarded, it would have been possible to take them into orbit and re-use them (such as a wet workshop for incorporation into a space station).\n\nTwo solid rocket boosters (SRBs) each provided of thrust at liftoff, which was 83% of the total thrust at liftoff. The SRBs were jettisoned two minutes after launch at a height of about , and then deployed parachutes and landed in the ocean to be recovered. The SRB cases were made of steel about ½ inch (13 mm) thick. The solid rocket boosters were re-used many times; the casing used in Ares I engine testing in 2009 consisted of motor cases that had been flown, collectively, on 48 Shuttle missions, including STS-1.\n\nAstronauts who have flown on multiple spacecraft report that Shuttle delivers a rougher ride than Apollo or Soyuz. The additional vibration is caused by the solid rocket boosters, as solid fuel does not burn as evenly as liquid fuel. The vibration dampens down after the solid rocket boosters have been jettisoned.\n\nThe orbiter could be used in conjunction with a variety of add-ons depending on the mission. This included orbital laboratories (Spacelab, Spacehab), boosters for launching payloads farther into space (Inertial Upper Stage, Payload Assist Module), and other functions, such as provided by Extended Duration Orbiter, Multi-Purpose Logistics Modules, or Canadarm (RMS). An upper stage called Transfer Orbit Stage (Orbital Science Corp. TOS-21) was also used once with the orbiter. Other types of systems and racks were part of the modular Spacelab system pallets, igloo, IPS, etc., which also supported special missions such as SRTM.\n\nA major component of the Space Shuttle Program was Spacelab, primarily contributed by a consortium of European countries, and operated in conjunction with the United States and international partners. Supported by a modular system of pressurized modules, pallets, and systems, Spacelab missions executed on multidisciplinary science, orbital logistics, and international cooperation. Over 29 missions flew on subjects ranging from astronomy, microgravity, radar, and life sciences, to name a few. Spacelab hardware also supported missions such as Hubble (HST) servicing and space station resupply. STS-2 and STS-3 provided testing, and the first full mission was Spacelab-1 (STS-9) launched on November 28, 1983.\n\nSpacelab formally began in 1973, after a meeting in Brussels, Belgium, by European heads of state. Within the decade, Spacelab went into orbit and provided Europe and the United States with an orbital workshop and hardware system. International cooperation, science, and exploration were realized on Spacelab.\n\nThe Shuttle was one of the earliest craft to use a computerized fly-by-wire digital flight control system. This means no mechanical or hydraulic linkages connected the pilot's control stick to the control surfaces or reaction control system thrusters. The control algorithm, which used a classical Proportional Integral Derivative (PID) approach, was developed and maintained by Honeywell. The Shuttle's fly-by-wire digital flight control system was composed of 4 control systems each addressing a different mission phase: Ascent, Descent, On-Orbit and Aborts. Honeywell is also credited with the design and implementation of the Shuttle's Nose Wheel Steering Control Algorithm that allowed the Orbiter to safely land at Kennedy Space Center's Shuttle Runway.\n\nA concern with using digital fly-by-wire systems on the Shuttle was reliability. Considerable research went into the Shuttle computer system. The Shuttle used five identical redundant IBM 32-bit general purpose computers (GPCs), model AP-101, constituting a type of embedded system. Four computers ran specialized software called the Primary Avionics Software System (PASS). A fifth backup computer ran separate software called the Backup Flight System (BFS). Collectively they were called the Data Processing System (DPS).\nThe design goal of the Shuttle's DPS was fail-operational/fail-safe reliability. After a single failure, the Shuttle could still continue the mission. After two failures, it could still land safely.\n\nThe four general-purpose computers operated essentially in lockstep, checking each other. If one computer provided a different result than the other three (i.e. the one computer failed), the three functioning computers \"voted\" it out of the system. This isolated it from vehicle control. If a second computer of the three remaining failed, the two functioning computers voted it out. A very unlikely failure mode would have been where two of the computers produced result A, and two produced result B (a two-two split). In this unlikely case, one group of two was to be picked at random.\n\nThe Backup Flight System (BFS) was separately developed software running on the fifth computer, used only if the entire four-computer primary system failed. The BFS was created because although the four primary computers were hardware redundant, they all ran the same software, so a generic software problem could crash all of them. Embedded system avionic software was developed under totally different conditions from public commercial software: the number of code lines was tiny compared to a public commercial software product, changes were only made infrequently and with extensive testing, and many programming and test personnel worked on the small amount of computer code. However, in theory it could have still failed, and the BFS existed for that contingency. While the BFS could run in parallel with PASS, the BFS never engaged to take over control from PASS during any Shuttle mission.\n\nThe software for the Shuttle computers was written in a high-level language called HAL/S, somewhat similar to PL/I. It is specifically designed for a real time embedded system environment.\n\nThe IBM AP-101 computers originally had about 424 kilobytes of magnetic core memory each. The CPU could process about 400,000 instructions per second. They had no hard disk drive, and loaded software from magnetic tape cartridges.\n\nIn 1990, the original computers were replaced with an upgraded model AP-101S, which had about 2.5 times the memory capacity (about 1 megabyte) and three times the processor speed (about 1.2 million instructions per second). The memory was changed from magnetic core to semiconductor with battery backup.\n\nEarly Shuttle missions, starting in November 1983, took along the Grid Compass, arguably one of the first laptop computers. The GRiD was given the name SPOC, for Shuttle Portable Onboard Computer. Use on the Shuttle required both hardware and software modifications which were incorporated into later versions of the commercial product. It was used to monitor and display the Shuttle's ground position, path of the next two orbits, show where the Shuttle had line of sight communications with ground stations, and determine points for location-specific observations of the Earth. The Compass sold poorly, as it cost at least US$8000, but it offered unmatched performance for its weight and size. NASA was one of its main customers.\n\nDuring its service life, the Shuttle's Control System never experienced a failure. Many of the lessons learned have been used to design today's high speed control algorithms.\n\nThe prototype orbiter \"Enterprise\" originally had a flag of the United States on the upper surface of the left wing and the letters \"USA\" in black on the right wing. The name \"Enterprise\" was painted in black on the payload bay doors just above the hinge and behind the crew module; on the aft end of the payload bay doors was the NASA \"worm\" logotype in gray. Underneath the rear of the payload bay doors on the side of the fuselage just above the wing is the text \"United States\" in black with a flag of the United States ahead of it.\n\nThe first operational orbiter, \"Columbia\", originally had the same markings as \"Enterprise\", although the letters \"USA\" on the right wing were slightly larger and spaced farther apart. \"Columbia\" also had black markings which \"Enterprise\" lacked on its forward RCS module, around the cockpit windows, and on its vertical stabilizer, and had distinctive black \"chines\" on the forward part of its upper wing surfaces, which none of the other orbiters had.\n\n\"Challenger\" established a modified marking scheme for the shuttle fleet that was matched by \"Discovery\", \"Atlantis\" and \"Endeavour\". The letters \"USA\" in black above an American flag were displayed on the left wing, with the NASA \"worm\" logotype in gray centered above the name of the orbiter in black on the right wing. The name of the orbiter was inscribed not on the payload bay doors, but on the forward fuselage just below and behind the cockpit windows. This would make the name visible when the shuttle was photographed in orbit with the doors open.\n\nIn 1983, \"Enterprise\" had its wing markings changed to match \"Challenger\", and the NASA \"worm\" logotype on the aft end of the payload bay doors was changed from gray to black. Some black markings were added to the nose, cockpit windows and vertical tail to more closely resemble the flight vehicles, but the name \"Enterprise\" remained on the payload bay doors as there was never any need to open them. \"Columbia\" had its name moved to the forward fuselage to match the other flight vehicles after STS-61-C, during the 1986–88 hiatus when the shuttle fleet was grounded following the loss of \"Challenger\", but retained its original wing markings until its last overhaul (after STS-93), and its unique black wing \"chines\" for the remainder of its operational life.\n\nBeginning in 1998, the flight vehicles' markings were modified to incorporate the NASA \"meatball\" insignia. The \"worm\" logotype, which the agency had phased out, was removed from the payload bay doors and the \"meatball\" insignia was added aft of the \"United States\" text on the lower aft fuselage. The \"meatball\" insignia was also displayed on the left wing, with the American flag above the orbiter's name, left-justified rather than centered, on the right wing. The three surviving flight vehicles, \"Discovery\", \"Atlantis\" and \"Endeavour\", still bear these markings as museum displays. \"Enterprise\" became the property of the Smithsonian Institution in 1985 and was no longer under NASA's control when these changes were made, hence the prototype orbiter still has its 1983 markings and still has its name on the payload bay doors.\n\nThe Space Shuttle was initially developed in the 1970s, but received many upgrades and modifications afterward to improve performance, reliability and safety. Internally, the Shuttle remained largely similar to the original design, with the exception of the improved avionics computers. In addition to the computer upgrades, the original analog primary flight instruments were replaced with modern full-color, flat-panel display screens, called a glass cockpit, which is similar to those of contemporary airliners. To facilitate construction of ISS, the internal airlocks of each orbiter except \"Columbia\" were replaced with external docking systems to allow for a greater amount of cargo to be stored on the Shuttle's mid-deck during station resupply missions.\n\nThe Space Shuttle Main Engines (SSMEs) had several improvements to enhance reliability and power. This explains phrases such as \"Main engines throttling up to 104 percent.\" This did not mean the engines were being run over a safe limit. The 100 percent figure was the original specified power level. During the lengthy development program, Rocketdyne determined the engine was capable of safe reliable operation at 104 percent of the originally specified thrust. NASA could have rescaled the output number, saying in essence 104 percent is now 100 percent. To clarify this would have required revising much previous documentation and software, so the 104 percent number was retained. SSME upgrades were denoted as \"block numbers\", such as block I, block II, and block IIA. The upgrades improved engine reliability, maintainability and performance. The 109% thrust level was finally reached in flight hardware with the Block II engines in 2001. The normal maximum throttle was 104 percent, with 106 percent or 109 percent used for mission aborts.\n\nFor the first two missions, STS-1 and STS-2, the external tank was painted white to protect the insulation that covers much of the tank, but improvements and testing showed that it was not required. The weight saved by not painting the tank resulted in an increase in payload capability to orbit. Additional weight was saved by removing some of the internal \"stringers\" in the hydrogen tank that proved unnecessary. The resulting \"light-weight external tank\" was first flown on STS-6 and used on the majority of Shuttle missions. STS-91 saw the first flight of the \"super light-weight external tank\". This version of the tank was made of the 2195 aluminum-lithium alloy. It weighed 3.4 metric tons (7,500 lb) less than the last run of lightweight tanks, allowing the Shuttle to deliver heavy elements to ISS's high inclination orbit. As the Shuttle was always operated with a crew, each of these improvements was first flown on operational mission flights.\n\nThe solid rocket boosters underwent improvements as well. Design engineers added a third O-ring seal to the joints between the segments after the 1986 Space Shuttle \"Challenger\" disaster.\nSeveral other SRB improvements were planned to improve performance and safety, but never came to be. These culminated in the considerably simpler, lower cost, probably safer and better-performing Advanced Solid Rocket Booster. These rockets entered production in the early to mid-1990s to support the Space Station, but were later canceled to save money after the expenditure of $2.2 billion. The loss of the ASRB program resulted in the development of the Super LightWeight external Tank (SLWT), which provided some of the increased payload capability, while not providing any of the safety improvements. In addition, the U.S. Air Force developed their own much lighter single-piece SRB design using a filament-wound system, but this too was canceled.\n\nSTS-70 was delayed in 1995, when woodpeckers bored holes in the foam insulation of \"Discovery\"'s external tank. Since then, NASA has installed commercial plastic owl decoys and inflatable owl balloons which had to be removed prior to launch. The delicate nature of the foam insulation had been the cause of damage to the Thermal Protection System, the tile heat shield and heat wrap of the orbiter. NASA remained confident that this damage, while it was the primary cause of the Space Shuttle \"Columbia\" disaster on February 1, 2003, would not jeopardize the completion of the International Space Station (ISS) in the projected time allotted.\n\nA cargo-only, unmanned variant of the Shuttle was variously proposed and rejected since the 1980s. It was called the Shuttle-C, and would have traded re-usability for cargo capability, with large potential savings from reusing technology developed for the Space Shuttle. Another proposal was to convert the payload bay into a passenger area, with versions ranging from 30 to 74 seats, three days in orbit, and cost US$1.5 million per seat.\n\nOn the first four Shuttle missions, astronauts wore modified U.S. Air Force high-altitude full-pressure suits, which included a full-pressure helmet during ascent and descent. From the fifth flight, STS-5, until the loss of \"Challenger\", one-piece light blue nomex flight suits and partial-pressure helmets were worn. A less-bulky, partial-pressure version of the high-altitude pressure suits with a helmet was reinstated when Shuttle flights resumed in 1988. The Launch-Entry Suit ended its service life in late 1995, and was replaced by the full-pressure Advanced Crew Escape Suit (ACES), which resembled the Gemini space suit in design, but retained the orange color of the Launch-Entry Suit.\n\nTo extend the duration that orbiters could stay docked at the ISS, the Station-to-Shuttle Power Transfer System (SSPTS) was installed. The SSPTS allowed these orbiters to use power provided by the ISS to preserve their consumables. The SSPTS was first used successfully on STS-118.\n\nOrbiter (for \"Endeavour\", OV-105)\n\nExternal tank (for SLWT)\n\nSolid Rocket Boosters\n\nSystem Stack\n\nAll Space Shuttle missions were launched from Kennedy Space Center (KSC). The weather criteria used for launch included, but were not limited to: precipitation, temperatures, cloud cover, lightning forecast, wind, and humidity. The Shuttle was not launched under conditions where it could have been struck by lightning. Aircraft are often struck by lightning with no adverse effects because the electricity of the strike is dissipated through its conductive structure and the aircraft is not electrically grounded. Like most jet airliners, the Shuttle was mainly constructed of conductive aluminum, which would normally shield and protect the internal systems. However, upon liftoff the Shuttle sent out a long exhaust plume as it ascended, and this plume could have triggered lightning by providing a current path to ground. The NASA Anvil Rule for a Shuttle launch stated that an anvil cloud could not appear within a distance of 10 nautical miles. The Shuttle Launch Weather Officer monitored conditions until the final decision to scrub a launch was announced. In addition, the weather conditions had to be acceptable at one of the Transatlantic Abort Landing sites (one of several Space Shuttle abort modes) to launch as well as the solid rocket booster recovery area. While the Shuttle might have safely endured a lightning strike, a similar strike caused problems on Apollo 12, so for safety NASA chose not to launch the Shuttle if lightning was possible (NPR8715.5).\n\nHistorically, the Shuttle was not launched if its flight would run from December to January (a year-end rollover or YERO). Its flight software, designed in the 1970s, was not designed for this, and would require the orbiter's computers be reset through a change of year, which could cause a glitch while in orbit. In 2007, NASA engineers devised a solution so Shuttle flights could cross the year-end boundary.\n\nAfter the final hold in the countdown at T-minus 9 minutes, the Shuttle went through its final preparations for launch, and the countdown was automatically controlled by the Ground Launch Sequencer (GLS), software at the Launch Control Center, which stopped the count if it sensed a critical problem with any of the Shuttle's onboard systems. The GLS handed off the count to the Shuttle's on-board computers at T minus 31 seconds, in a process called auto sequence start.\n\nAt T-minus 16 seconds, the massive sound suppression system (SPS) began to drench the Mobile Launcher Platform (MLP) and SRB trenches with of water to protect the Orbiter from damage by acoustical energy and rocket exhaust reflected from the flame trench and MLP during lift off.\n\nAt T-minus 10 seconds, hydrogen igniters were activated under each engine bell to quell the stagnant gas inside the cones before ignition. Failure to burn these gases could trip the onboard sensors and create the possibility of an overpressure and explosion of the vehicle during the firing phase. The main engine turbopumps also began charging the combustion chambers with liquid hydrogen and liquid oxygen at this time. The computers reciprocated this action by allowing the redundant computer systems to begin the firing phase.\nThe three main engines (SSMEs) started at \"T\"-6.6 seconds. The main engines ignited sequentially via the Shuttle's general purpose computers (GPCs) at 120 millisecond intervals. All three SSMEs were required to reach 90% rated thrust within three seconds, otherwise the onboard computers would initiate an RSLS abort. If all three engines indicated nominal performance by \"T\"-3 seconds, they were commanded to gimbal to liftoff configuration and the command would be issued to arm the SRBs for ignition at \"T\"-0. Between \"T\"-6.6 seconds and \"T\"-3 seconds, while the SSMEs were firing but the SRBs were still bolted to the pad, the offset thrust caused the entire launch stack (boosters, tank and orbiter) to pitch down measured at the tip of the external tank. The three second delay after confirmation of SSME operation was to allow the stack to return to nearly vertical. At \"T\"-0 seconds, the 8 frangible nuts holding the SRBs to the pad were detonated, the SSMEs were commanded to 100% throttle, and the SRBs were ignited. By \"T\"+0.23 seconds, the SRBs built up enough thrust for liftoff to commence, and reached maximum chamber pressure by \"T\"+0.6 seconds. The Johnson Space Center's Mission Control Center assumed control of the flight once the SRBs had cleared the launch tower.\n\nShortly after liftoff, the Shuttle's main engines were throttled up to 104.5% and the vehicle began a combined roll, pitch and yaw maneuver that placed it onto the correct heading (azimuth) for the planned orbital inclination and in a heads down attitude with wings level. The Shuttle flew upside down during the ascent phase. This orientation allowed a trim angle of attack that was favorable for aerodynamic loads during the region of high dynamic pressure, resulting in a net positive load factor, as well as providing the flight crew with a view of the horizon as a visual reference. The vehicle climbed in a progressively flattening arc, accelerating as the mass of the SRBs and main tank decreased. To achieve low orbit requires much more horizontal than vertical acceleration. This was not visually obvious, since the vehicle rose vertically and was out of sight for most of the horizontal acceleration. The near circular orbital velocity at the altitude of the International Space Station is , roughly equivalent to Mach 23 at sea level. As the International Space Station orbits at an inclination of 51.6 degrees, missions going there must set orbital inclination to the same value in order to rendezvous with the station.\n\nAround 30 seconds into ascent, the SSMEs were throttled down—usually to 72%, though this varied—to reduce the maximum aerodynamic forces acting on the Shuttle at a point called Max Q. Additionally, the propellant grain design of the SRBs caused their thrust to drop by about 30% by 50 seconds into ascent. Once the Orbiter's guidance verified that Max Q would be within Shuttle structural limits, the main engines were throttled back up to 104.5%; this throttling down and back up was called the \"thrust bucket\". To maximize performance, the throttle level and timing of the thrust bucket was shaped to bring the Shuttle as close to aerodynamic limits as possible.\nAt around \"T\"+126 seconds, pyrotechnic fasteners released the SRBs and small separation rockets pushed them laterally away from the vehicle. The SRBs parachuted back to the ocean to be reused. The Shuttle then began accelerating to orbit on the main engines. Acceleration at this point would typically fall to .9 \"g\", and the vehicle would take on a somewhat nose-up angle to the horizonit used the main engines to gain and then maintain altitude while it accelerated horizontally towards orbit. At about five and three-quarter minutes into ascent, the orbiter's direct communication links with the ground began to fade, at which point it rolled heads up to reroute its communication links to the Tracking and Data Relay Satellite system.\n\nAt about seven and a half minutes into ascent, the mass of the vehicle was low enough that the engines had to be throttled back to limit vehicle acceleration to 3 \"g\" (29.4 m/s² or 96.5 ft/s², equivalent to accelerating from zero to in a second). The Shuttle would maintain this acceleration for the next minute, and main engine cut-off (MECO) occurred at about eight and a half minutes after launch. The main engines were shut down before complete depletion of propellant, as running dry would have destroyed the engines. The oxygen supply was terminated before the hydrogen supply, as the SSMEs reacted unfavorably to other shutdown modes. (Liquid oxygen has a tendency to react violently, and supports combustion when it encounters hot engine metal.) A few seconds after MECO, the external tank was released by firing pyrotechnic fasteners.\n\nAt this point the Shuttle and external tank were on a slightly suborbital trajectory, coasting up towards apogee. Once at apogee, about half an hour after MECO, the Shuttle's Orbital Maneuvering System (OMS) engines were fired to raise its perigee and achieve orbit, while the external tank fell back into the atmosphere and burned up over the Indian Ocean or the Pacific Ocean depending on launch profile. The sealing action of the tank plumbing and lack of pressure relief systems on the external tank helped it break up in the lower atmosphere. After the foam burned away during re-entry, the heat caused a pressure buildup in the remaining liquid oxygen and hydrogen until the tank exploded. This ensured that any pieces that fell back to Earth were small.\n\n\nThe Shuttle was monitored throughout its ascent for short range tracking (10 seconds before liftoff through 57 seconds after), medium range (7 seconds before liftoff through 110 seconds after) and long range (7 seconds before liftoff through 165 seconds after). Short range cameras included 22 16mm cameras on the Mobile Launch Platform and 8 16mm on the Fixed Service Structure, 4 high speed fixed cameras located on the perimeter of the launch complex plus an additional 42 fixed cameras with 16mm motion picture film. Medium range cameras included remotely operated tracking cameras at the launch complex plus 6 sites along the immediate coast north and south of the launch pad, each with 800mm lens and high speed cameras running 100 frames per second. These cameras ran for only 4–10 seconds due to limitations in the amount of film available. Long range cameras included those mounted on the external tank, SRBs and orbiter itself which streamed live video back to the ground providing valuable information about any debris falling during ascent. Long range tracking cameras with 400-inch film and 200-inch video lenses were operated by a photographer at Playalinda Beach as well as 9 other sites from 38 miles north at the Ponce Inlet to 23 miles south to Patrick Air Force Base (PAFB) and additional mobile optical tracking camera was stationed on Merritt Island during launches. A total of 10 HD cameras were used both for ascent information for engineers and broadcast feeds to networks such as NASA TV and HDNet. The number of cameras significantly increased and numerous existing cameras were upgraded at the recommendation of the Columbia Accident Investigation Board to provide better information about the debris during launch. Debris was also tracked using a pair of Weibel Continuous Pulse Doppler X-band radars, one on board the SRB recovery ship MV \"Liberty Star\" positioned north east of the launch pad and on a ship positioned south of the launch pad. Additionally, during the first 2 flights following the loss of \"Columbia\" and her crew, a pair of NASA WB-57 reconnaissance aircraft equipped with HD Video and Infrared flew at to provide additional views of the launch ascent. Kennedy Space Center also invested nearly $3 million in improvements to the digital video analysis systems in support of debris tracking.\n\nOnce in orbit, the Shuttle usually flew at an altitude of , although the STS-82 mission reached . In the 1980s and 1990s, many flights involved space science missions on the NASA/ESA Spacelab, or launching various types of satellites and science probes. By the 1990s and 2000s the focus shifted more to servicing the space station, with fewer satellite launches. Most missions involved staying in orbit several days to two weeks, although longer missions were possible with the Extended Duration Orbiter add-on or when attached to a space station. STS-80 was the longest at almost 17 days and 16 hours.\n\nAlmost the entire Space Shuttle re-entry procedure, except for lowering the landing gear and deploying the air data probes, was normally performed under computer control. However, the re-entry could be flown entirely manually if an emergency arose. The approach and landing phase could be controlled by the autopilot, but was usually hand flown.\nThe vehicle began re-entry by firing the Orbital maneuvering system engines, while flying upside down, backside first, in the opposite direction to orbital motion for approximately three minutes, which reduced the Shuttle's velocity by about . The resultant slowing of the Shuttle lowered its orbital perigee down into the upper atmosphere. The Shuttle then flipped over, by pushing its nose down (which was actually \"up\" relative to the Earth, because it was flying upside down). This OMS firing was done roughly halfway around the globe from the landing site.\n\nThe vehicle started encountering more significant air density in the lower thermosphere at about , at around Mach 25, . The vehicle was controlled by a combination of RCS thrusters and control surfaces, to fly at a 40-degree nose-up attitude, producing high drag, not only to slow it down to landing speed, but also to reduce reentry heating. As the vehicle encountered progressively denser air, it began a gradual transition from spacecraft to aircraft. In a straight line, its 40-degree nose-up attitude would cause the descent angle to flatten-out, or even rise. The vehicle therefore performed a series of four steep S-shaped banking turns, each lasting several minutes, at up to 70 degrees of bank, while still maintaining the 40-degree angle of attack. In this way it dissipated speed sideways rather than upwards. This occurred during the 'hottest' phase of re-entry, when the heat-shield glowed red and the G-forces were at their highest. By the end of the last turn, the transition to aircraft was almost complete. The vehicle leveled its wings, lowered its nose into a shallow dive and began its approach to the landing site.\n\nThe orbiter's maximum glide ratio/lift-to-drag ratio varies considerably with speed, ranging from 1:1 at hypersonic speeds, 2:1 at supersonic speeds and reaching 4.5:1 at subsonic speeds during approach and landing.\n\nIn the lower atmosphere, the orbiter flies much like a conventional glider, except for a much higher descent rate, over or 9,800 fpm. At approximately Mach 3, two air data probes, located on the left and right sides of the orbiter's forward lower fuselage, are deployed to sense air pressure related to the vehicle's movement in the atmosphere.\n\nWhen the approach and landing phase began, the orbiter was at a altitude, from the runway. The pilots applied aerodynamic braking to help slow down the vehicle. The orbiter's speed was reduced from , approximately, at touch-down (compared to for a jet airliner). The landing gear was deployed while the Orbiter was flying at . To assist the speed brakes, a drag chute was deployed either after main gear or nose gear touchdown (depending on selected chute deploy mode) at about . The chute was jettisoned once the orbiter slowed to .\n\nAfter landing, the vehicle stayed on the runway for several hours for the orbiter to cool. Teams at the front and rear of the orbiter tested for presence of hydrogen, hydrazine, monomethylhydrazine, nitrogen tetroxide and ammonia (fuels and by-products of the reaction control system and the orbiter's three APUs). If hydrogen was detected, an emergency would be declared, the orbiter powered down and teams would evacuate the area. A convoy of 25 specially designed vehicles and 150 trained engineers and technicians approached the orbiter. Purge and vent lines were attached to remove toxic gases from fuel lines and the cargo bay about 45–60 minutes after landing. A flight surgeon boarded the orbiter for initial medical checks of the crew before disembarking. Once the crew left the orbiter, responsibility for the vehicle was handed from the Johnson Space Center back to the Kennedy Space Center.\n\nIf the mission ended at Edwards Air Force Base in California, White Sands Space Harbor in New Mexico, or any of the runways the orbiter might use in an emergency, the orbiter was loaded atop the Shuttle Carrier Aircraft, a modified 747, for transport back to the Kennedy Space Center, landing at the Shuttle Landing Facility. Once at the Shuttle Landing Facility, the orbiter was then towed along a tow-way and access roads normally used by tour buses and KSC employees to the Orbiter Processing Facility where it began a months-long preparation process for the next mission.\n\nNASA preferred Space Shuttle landings to be at Kennedy Space Center. If weather conditions made landing there unfavorable, the Shuttle could delay its landing until conditions are favorable, touch down at Edwards Air Force Base, California, or use one of the multiple alternate landing sites around the world. A landing at any site other than Kennedy Space Center meant that after touchdown the Shuttle must be mated to the Shuttle Carrier Aircraft and returned to Cape Canaveral. Space Shuttle \"Columbia\" (STS-3) once landed at the White Sands Space Harbor, New Mexico; this was viewed as a last resort as NASA scientists believed that the sand could potentially damage the Shuttle's exterior.\n\nThere were many alternative landing sites that were never used.\n\nAn example of technical risk analysis for a STS mission is SPRA iteration 3.1 top risk contributors for STS-133:\n\nAn internal NASA risk assessment study (conducted by the Shuttle Program Safety and Mission Assurance Office at Johnson Space Center) released in late 2010 or early 2011 concluded that the agency had seriously underestimated the level of risk involved in operating the Shuttle. The report assessed that there was a 1 in 9 chance of a catastrophic disaster during the first nine flights of the Shuttle but that safety improvements had later improved the risk ratio to 1 in 90.\n\nBelow is a list of major events in the Space Shuttle orbiter fleet.\nSources: NASA launch manifest, NASA Space Shuttle archive\n\nOn January 28, 1986, \"Challenger\" disintegrated 73 seconds after launch due to the failure of the right SRB, killing all seven astronauts on board. The disaster was caused by low-temperature impairment of an O-ring, a mission critical seal used between segments of the SRB casing. Failure of the O-ring allowed hot combustion gases to escape from between the booster sections and burn through the adjacent external tank, causing it to explode. Repeated warnings from design engineers voicing concerns about the lack of evidence of the O-rings' safety when the temperature was below 53 °F (12 °C) had been ignored by NASA managers.\n\nOn February 1, 2003, \"Columbia\" disintegrated during re-entry, killing its crew of seven, because of damage to the carbon-carbon leading edge of the wing caused during launch. Ground control engineers had made three separate requests for high-resolution images taken by the Department of Defense that would have provided an understanding of the extent of the damage, while NASA's chief thermal protection system (TPS) engineer requested that astronauts on board \"Columbia\" be allowed to leave the vehicle to inspect the damage. NASA managers intervened to stop the Department of Defense's assistance and refused the request for the spacewalk, and thus the feasibility of scenarios for astronaut repair or rescue by \"Atlantis\" were not considered by NASA management at the time.\n\nNASA retired the Space Shuttle in 2011, after 30 years of service. The Shuttle was originally conceived of and presented to the public as a \"Space Truck\", which would, among other things, be used to build a United States space station in low earth orbit in the early 1990s. When the U.S. space station evolved into the International Space Station project, which suffered from long delays and design changes before it could be completed, the retirement of the Space Shuttle was delayed several times until 2011, serving at least 15 years longer than originally planned. \"Discovery\" was the first of NASA's three remaining operational Space Shuttles to be retired.\n\nThe final Space Shuttle mission was originally scheduled for late 2010, but the program was later extended to July 2011 when Michael Suffredini of the ISS program said that one additional trip was needed in 2011 to deliver parts to the International Space Station. The Shuttle's final mission consisted of just four astronauts—Christopher Ferguson (Commander), Douglas Hurley (Pilot), Sandra Magnus (Mission Specialist 1), and Rex Walheim (Mission Specialist 2); they conducted the 135th and last space Shuttle mission on board \"Atlantis\", which launched on July 8, 2011, and landed safely at the Kennedy Space Center on July 21, 2011, at 5:57 AM EDT (09:57 UTC).\n\nNASA announced it would transfer orbiters to education institutions or museums at the conclusion of the Space Shuttle program. Each museum or institution is responsible for covering the cost of preparing and transporting each vehicle for display. Twenty museums from across the country submitted proposals for receiving one of the retired orbiters. NASA also made Space Shuttle thermal protection system tiles available to schools and universities for less than US$25 each. About 7,000 tiles were available on a first-come, first-served basis, limited to one per institution.\n\nOn April 12, 2011, NASA announced selection of locations for the remaining Shuttle orbiters:\n\n\nIn August 2011, the NASA Office of Inspector General (OIG) published a \"Review of NASA's Selection of Display Locations for the Space Shuttle Orbiters\"; the review had four main findings: \nThe NASA OIG had three recommendations, saying NASA should:\n\nIn September 2011, the CEO and two board members of Seattle's Museum of Flight met with NASA Administrator Charles Bolden, pointing out \"significant errors in deciding where to put its four retiring Space Shuttles\"; the errors alleged include inaccurate information on Museum of Flight's attendance and international visitor statistics, as well as the readiness of the Intrepid Sea-Air-Space Museum's exhibit site.\n\n\nFlight and mid-deck training hardware will be taken from the Johnson Space Center and will go to the National Air and Space Museum and the National Museum of the U.S. Air Force. The full fuselage mockup, which includes the payload bay and aft section but no wings, is to go to the Museum of Flight in Seattle. Mission Simulation and Training Facility's fixed simulator will go to the Adler Planetarium in Chicago, and the motion simulator will go to the Texas A&M Aerospace Engineering Department in College Station, Texas. Other simulators used in Shuttle astronaut training will go to the Wings of Dreams Aviation Museum in Starke, Florida and the Virginia Air and Space Center in Hampton, Virginia.\n\nUntil another U.S. manned spacecraft is ready, crews will travel to and from the International Space Station (ISS) exclusively aboard the Russian Soyuz spacecraft.\n\nA planned successor to STS was the \"Shuttle II\", during the 1980s and 1990s, and later the Constellation program during the 2004–2010 period. CSTS was a proposal to continue to operate STS commercially, after NASA. In September 2011, NASA announced the selection of the design for the new Space Launch System that is planned to launch the Orion spacecraft and other hardware to missions beyond low earth-orbit.\n\nThe Commercial Orbital Transportation Services program began in 2006 with the purpose of creating commercially operated unmanned cargo vehicles to service the ISS. The Commercial Crew Development (CCDev) program was started in 2010 to create commercially operated manned spacecraft capable of delivering at least four crew members to the ISS, to stay docked for 180 days, and then return them back to Earth. These spacecraft were to become operational in the 2010s.\n\nSpace Shuttles have been features of fiction and nonfiction, from children's movies to documentaries. Early examples include the 1979 \"James Bond\" film, \"Moonraker\", the 1982 Activision videogame \"Space Shuttle: A Journey into Space\" (1982) and G. Harry Stine's 1981 novel \"Shuttle Down\". In the 1986 film \"SpaceCamp\", \"Atlantis\" accidentally launches into space with a group of U.S. Space Camp participants as its crew. A space shuttle named \"Intrepid\" is featured in the 1989 film \"Moontrap\".\n\nThe 1998 film \"Armageddon\" portrays a combined crew of offshore oil rig workers and U.S. military staff who pilot two modified Shuttles to avert the destruction of Earth by an asteroid. Retired American test pilots visit a Russian satellite in the 2000 Clint Eastwood adventure film \"Space Cowboys\". In the 2003 film \"The Core\", the \"Endeavour\"s landing is disrupted by the Earth's magnetic core, and its crew is selected to pilot a vehicle designed to restart the core. The 2004 Bollywood movie \"Swades\", where a Space Shuttle is used to launch a special rainfall monitoring satellite, was filmed at Kennedy Space Center in the year after the \"Columbia\" disaster that had taken the life of Indian-American astronaut KC Chawla.\n\nOn television, the 1996 drama \"The Cape\" portrays the lives of a group of NASA astronauts as they prepare for and fly Shuttle missions. \"Odyssey 5\" was a short-lived sci-fi series that features the crew of a Space Shuttle as the last survivors of a disaster that destroys Earth. The 1997–2007 sci-fi series \"Stargate SG-1\" has a shuttle rescue written into an episode.\n\nThe 2013 film \"Gravity\" features the fictional Space Shuttle \"Explorer\" during STS-157, whose crew are killed or left stranded after it is destroyed by a shower of high speed orbital debris. The 2017 Lego film \"The Lego Batman Movie\" features a hybrid between the Batmobile and a Space Shuttle, named \"the Bat Space Shuttle\" by Dick Grayson. It's clearly based on the Lego City set 3367 (\"Space Shuttle\"), but is black and weapon-equipped.\nThe Space Shuttle has also been the subject of toys and models; for example, a large Lego Space Shuttle model was constructed by visitors at Kennedy Space Center, and smaller models have been sold commercially as a standard \"LegoLand\" set. A 1980 pinball machine \"Space Shuttle\" was produced by Zaccaria and a 1984 pinball machine \"Space Shuttle: Pinball Adventure\" was produced by Williams and features a plastic Space Shuttle model among other artwork of astronauts on the play field. The Space Shuttle also appears in a number of flight simulator and space flight simulator games such as \"Microsoft Space Simulator\", \"Orbiter\", \"FlightGear\", \"X-Plane\" and Space Shuttle Mission 2007. Several Transformers toys were modeled after the Space Shuttle.\n\nThe U.S. Postal Service has released several postage issues that depict the Space Shuttle. The first such stamps were issued in 1981, and are on display at the National Postal Museum.\n\n\n\n\n\n"}
{"id": "12711731", "url": "https://en.wikipedia.org/wiki?curid=12711731", "title": "Stanford University Mathematics Camp", "text": "Stanford University Mathematics Camp\n\nStanford University Mathematics Camp, or SUMaC, is a competitive summer mathematics program for rising high school juniors and seniors around the world. The camp lasts for 4 weeks, usually from mid-July to mid-August. It is based on the campus of Stanford University.\n\nLike the Ross Program at Ohio State and the PROMYS program at Boston University, SUMaC does not put emphasis on competition-math preparations but focuses instead on advanced undergraduate math topics.\n\nSUMaC was founded in 1995 by Professors Rafe Mazzeo and Ralph Cohen of the Stanford Mathematics Department and has been directed from the beginning by Prof. Mazzeo, and Dr. Rick Sommer. Dr. Sommer was an Assistant Professor in the Stanford Mathematics Department and is currently a Deputy Director of the Education Program for Gifted Youth (EPGY), at Stanford. He designed the Program I course and has been teaching versions of it since the first SUMaC in 1995. The Program II course was designed and has been taught by Prof. Rafe Mazzeo. (In recent years, the course was cotaught by Dr. Pierre Albin, a former Stanford graduate student who currently teaches at MIT, and is currently taught by Dr. Simon Rubinstein-Salzedo, a postdoctoral fellow in statistics at Stanford.)\n\n\nDuring the camp, there are frequent guest lectures given by internationally renowned mathematicians. These talks are in the areas of current mathematical research. Ravi Vakil, a current Stanford mathematics professor and a 4-time Putnam Fellow, talked to the students in 2007.\nAlso in 2007, Tyson Mao, one of the best cube solvers in the world, taught SUMaC students how to solve the Rubik's Cube. Other speakers in 2007 included Drs. Kay Kirkpatrick (MIT), Ted Shifrin (University of Georgia), and Pete Storm (Stanford).\nIn 2014, John Edmark, Professor of Art and Art History at Stanford, spoke about his current work in mathematics-inspired sculptures, and Brian Conrey, executive director of the American Institute of Mathematics, gave a lecture on the Twin Primes Conjecture and the Riemann Hypothesis.\nStudents at SUMaC also engage in a variety of sports activities during their free time.\nSuch sports include basketball, tennis, badminton, table tennis, and ultimate.\n\nSUMaC was the backdrop for Justina Chen Headley's book \"Nothing But the Truth (and a Few White Lies)\", a teen novel about a half-Taiwanese girl who finally finds her identity at the math camp.\n\nMost of the SUMaC residential counselors and teaching assistants are Stanford mathematics graduate students and undergraduate math majors. SUMaC usually has a 1-to-4 ratio of staff to students, with most of the teaching assistants serving in the role of live-in counselors. Many of the SUMaC teaching assistants and counselors return from previous years, and some attended SUMaC in high school.\n\nSUMaC students have traditionally been housed in Synergy , a small Stanford-student residence that is a famously vegetarian co-op during the academic year, though there have been years when students lived in Κappa Αlpha (KA). Each student has one,two or three roommates, and the floors are divided by sex. Dining takes place at a separate dining hall shared by other summer youth programs at Stanford.\n\n"}
{"id": "2191056", "url": "https://en.wikipedia.org/wiki?curid=2191056", "title": "Sylvatic", "text": "Sylvatic\n\nSylvatic is a scientific term referring to wild animals, often in context of diseases or pathogens that only affect them (sylvan means \"forest-dwelling\"). In the context of animal research, its opposite is domestic, which refers to pets, farm animals or other animals which do not dwell in the wild.\n\n\nThe word \"sylvatic\" is also simply a synonym for \"sylvan\" (or \"silvan\") = \"of the forest\".\n\nThe Latin words \"silvatica\", \"silvaticus\" of this root are commonly used in biological taxonomy: \"Rana sylvatica\" (Wood Frog), \"agaricus silvaticus\" (Scaly Wood Mushroom), etc.\n\n"}
{"id": "44120932", "url": "https://en.wikipedia.org/wiki?curid=44120932", "title": "Sznajd model", "text": "Sznajd model\n\nThe Sznajd model or United we stand, divided we fall (USDF) model is an econophysics model suggested in 2000 introduced to gain fundamental understanding about opinion dynamics using methods from statistical physics. The Sznajd model implements a phenomenon called social validation and thus extends the Ising spin model. In simple words, the model states:\nFor simplicity, one assumes that each individual formula_1 has\nan opinion S which might be Boolean (formula_2 for \"no\", formula_3 for \"yes\") in its simplest formulation, which means that each individual either agrees or disagrees to a given question.\n\nIn the original 1D-formulation, each individual has exactly two neighbors just like beads on a bracelet. At each time step a pair of individual formula_4 and formula_5 is chosen at random to change their nearest neighbors' opinion (or: Ising spins) formula_6 and formula_7 according to two dynamical rules:\n\nIn a closed (1 dimensional) community, two steady states are always reached, namely complete consensus (which is called \"ferromagnetic state\" in physics) or stalemate (the \"antiferromagnetic state\").\nFurthermore, Monte Carlo simulations showed that these simple rules lead to complicated dynamics, in particular to a power law in the decision time distribution with an exponent of -1.5.\n\nThe final (antiferromagnetic) state of alternating all-on and all-off is unrealistic to represent the behavior of a community. It would mean that the complete population uniformly changes their opinion from one time step to the next. For this reason an alternative dynamical rule was proposed. One possibility is that two spins formula_4 and formula_5 change their nearest neighbors according to the two following rules: \n\nIn recent years, statistical physics has been accepted as modeling framework for phenomena outside the traditional physics. Fields as econophysics or sociophysics formed, and many quantitative analysts in finance are physicists. The Ising model in statistical physics has been a very important step in the history of studying collective (critical) phenomena. The Sznajd model is a simple but yet important variation of prototypical Ising system.\n\nIn 2007, Katarzyna Sznajd-Weron has been recognized by the Young Scientist Award for Socio- and Econophysics of the Deutsche Physikalische Gesellschaft (German Physical Society) for an outstanding original contribution using physical methods to develop a better understanding of socio-economic problems.\n\nThe Sznajd model belongs to the class of binary-state dynamics on a networks also referred to as Boolean networks. This class of systems includes the Ising model, the voter model and the q-voter model, the Bass diffusion model, threshold models and others.\nThe Sznajd model can be applied to various fields:\n\n"}
{"id": "31508724", "url": "https://en.wikipedia.org/wiki?curid=31508724", "title": "Theory of indispensable attributes", "text": "Theory of indispensable attributes\n\nThe theory of indispensable attributes (TIA) is a theory in the context of perceptual organisation which asks for the functional units and elementary features that are relevant for a perceptual system in the constitution of perceptual objects. Earlier versions of the theory emerged in the context of an application of research on vision to audition, and analogies between vision and audition were emphasised,\nwhereas in more recent writings the necessity of a modality-general theory of perceptual organisation and objecthood is stressed.\n\nThe subject of perceptual organisation, and with it TIA, constitute a prime example of how theories of Gestalt psychology have been taken up and kept alive in cognitive psychology.\n\nTIA has been drawn on in the context of music research, in the areas of music philosophy,\nand systematic music theory.\n\nSince the perception of objects implies a segregation of some parts of the environment (\"figure\") from other parts of the environment (\"ground\"), a perceptual system will have to rely on certain features in the environment for the aggregation of what goes together. This aggregation is termed \"perceptual grouping\" (PG), and the aim of TIA is the identification of conditions for the occurrence of PG.\n\nPG is considered as a transformation happening between some input and some output. The input formula_1 is considered a set of discrete elements which are distributed over some medium formula_2. Media are also termed \"indispensable attributes\" (IA). The output PP is termed a \"phenomenal partition\" of formula_1 into subsets, or \"blocks\", E, E, ..., E.\n\nThe grouping into some block E occurs in reference to at least one feature F from a set of features formula_4. Kubovy and Van Valkenburg (2003) recommend the following expression for the description of a PP: \"... the elements of formula_1 spread over formula_2, are grouped by formula_4.\"\n\n"}
{"id": "14765461", "url": "https://en.wikipedia.org/wiki?curid=14765461", "title": "Vanguardia de la Ciencia", "text": "Vanguardia de la Ciencia\n\nVanguardia de la Ciencia was a Spanish science podcast and radio program which was broadcast on the shortwave band by Radio Exterior de España, one of the stations of Radio Nacional de España (RNE). The program aired weekly, without interruption, from April 1995 until June 2007. It was available for download as mp3-files from the web pages of RNE from September 2003. The program was created by Ángel Rodríguez Lozano. A total of 98 programs are still available online. In addition to \"Vanguardia de la Ciencia\", Lozano also hosted another popular science radio program and podcast, \"El Sueño de Arquímedes\".\n\n\"Vanguardia de la Ciencia\" had a large audience worldwide. Although exact numbers of downloads and listeners are unavailable, Radio exterior de España has 80 million listeners, only surpassed by the BBC and Radio Vaticana. In addition, \"Vanguardia de la Ciencia\" was retransmitted by several radio stations in Latin America and Spain. The program was popular throughout the Spanish-speaking world\nThe program aimed at being accessible to everyone with an interest for science. Some basic knowledge of science was required to get the most it, as it often went into some technical detail, especially in the interviews. The audio production was of top quality.\n\n\nIn June 2007, \"Vanguardia de la Ciencia\" and \"El sueño de Arquímedes\" were abruptly terminated. In the correspondence section of one of the last programs, Lozano explained, in response to a letter from an outraged listener, that the decision to terminate the program was made due to a re-structuration of RNE, and that he was but one of 4,150 employees who had to leave.\nIn the previously referenced interview, he explained that everyone older than 52 years had to retire early, and that he was 54 years old at the time.\nThe decision to terminate the programs was widely criticized in Spanish-speaking blogs.\n\nAt the web-site of RNE, 36 programs are still available. In addition, 62 previous programs are available at an external site (eSnips).\n\"Vanguardia de la Ciencia's\" sister program, \"El Sueño de Arquímedes\", still has 35 programs for download on RNE's web site.\n"}
{"id": "16971323", "url": "https://en.wikipedia.org/wiki?curid=16971323", "title": "Álvaro Penteado Crósta", "text": "Álvaro Penteado Crósta\n\nÁlvaro Penteado Crósta is a Brazilian geologist, specialized in remote sensing and mineral exploration. He is an authority on the impact structures of Brazil and known for the \"Crosta method/technique\".\n\nCrósta got a Bachelor's degree in geology from the University of São Paulo in 1977, a Master's degree from Brazil's National Institute for Space Research (INPE) in 1982, and a Ph.D. degree from the Imperial College of University of London in 1990. In 1995—1996 he was a visiting scholar at the Desert Research Institute of the University of Nevada at Reno.\n\nAs of 2005, he is a Full Professor at the Geosciences Institute of the State University of Campinas (UNICAMP). He was the Institute's Director from 2005 to 2010 and UNICAMP´s Vice-Rector from 2013 to 2017.\n\n"}
