{"id": "39375034", "url": "https://en.wikipedia.org/wiki?curid=39375034", "title": "Abell S1077", "text": "Abell S1077\n\nAbell S1077 is a galaxy cluster located in the constellation Piscis Austrinus.\n"}
{"id": "35965855", "url": "https://en.wikipedia.org/wiki?curid=35965855", "title": "Actinobiology", "text": "Actinobiology\n\nActinobiology is the study of the effects of radiation upon living organisms. Actinobiology has become particularly relevant in space travel due to the abundance of cosmic radiation, which may negatively impact unprotected living organisms in space. \n\nRadiations are the most important physical mutagens. H.J. Muller who used X-rays for the first time to increase the rate of mutation in \"Drosophila\", opened an entirely new field in inducing mutations. So Muller is considered as the 'Father of Actinobiology'.\n\nIn 2002, Russian scientists found melanin-rich fungi growing near ground zero at Chernobyl. It was soon discovered that these fungi were using radiation as an energy source in much the same way that plants use the sun for photosynthesis. Radiotrophic bacterium have also been discovered deep underground, feeding on uranium emissions.\n\n"}
{"id": "7876216", "url": "https://en.wikipedia.org/wiki?curid=7876216", "title": "All About Radiation", "text": "All About Radiation\n\nAll About Radiation is one of the books by L. Ron Hubbard that form the canonical texts of Scientology, although it is no longer promoted by the Church of Scientology nor included in their \"Basics\" book canon. Its first printing was from HASI (Hubbard Association of Scientologists International) by way of the Speedwell Printing Company, Kent, England, 1957. Later editions were published by the Church of Scientology's in-house publisher Bridge Publications. It is controversial for its claims, amongst other things, that radiation poisoning and even cancer can be cured by courses of vitamins. There is no known cure for radiation poisoning and current medical practice is to provide palliative care until the symptoms subside or the patient dies.\n\nEarly printings of the book were credited on the cover as simply \"By a nuclear physicist and a medical doctor\", while subsequent ones credited L. Ron Hubbard as being the nuclear physicist and \"Medicus\" as being the doctor.\n\nBy the 1979 edition, the \"medical doctor\" was credited as being Richard Farley.\n\nIn the book's most recent edition, the book's authorship is attributed to Hubbard and Gene Denk and Farley R. Spink.\n\nThe book has gone through a number of printings since its initial run, and has undergone a few modifications over the years, mostly to remove controversial assertions made in the original lectures. As it is a fundamental Scientology tenet that Hubbard's works are considered immutable Standard Tech, not to be altered in any way, except by Hubbard himself, these changes are considered evidence to Freezone practitioners that the current Church alters the text. It is to be noted that the first part of the book is not by Hubbard and that the second part was not written by Hubbard but edited from four of his lectures given in April 1957 in London. These lectures are available since March 2005 with a transcription which makes it possible to see how the book text was edited from the lectures. The book was not reissued in June 2007 as part of the Golden Age of Knowledge program. Among the text removed from the book in later editions:\n\n\nDespite calling himself a nuclear physicist (some editions of the book even call him \"one of America's first nuclear physicists\" on the dustjacket), Hubbard was not a qualified physicist. His degree was from the unaccredited Sequoia University, a diploma mill. The one course in nuclear physics Hubbard took was in 1931 at George Washington University, whose records indicate that he scored an F in the course. Hubbard dropped out of school shortly thereafter, with a 2.28 grade point average.\n\nHubbard referred to himself as a nuclear physicist on many occasions in the 1950s, such as in the tape-recorded 1956 lecture \"A Postulate Out of a Golden Age\", where he not only claimed to be a nuclear physicist, but that he was offered (and turned down) a U.S. Government post as one. This comment has been edited out of the CD version of the lecture currently offered by the L. Ron Hubbard Classic Lectures series.\n\nIn February 1966, Hubbard defended his mail-order degree: \"I was a Ph.D., Sequoia's [sic] University and therefore a perfectly valid doctor under the laws of the State of California\". But only a month later, he announced: \"having reviewed the damage being done in our society with nuclear physics and psychiatry by persons calling themselves \"Doctor\" [I] do hereby resign in protest my university degree as a Doctor of philosophy (Ph. D.)\" \n\nThe final results of the Anderson Report in 1965 declared:\n\n\"The Board heard evidence from a highly qualified radiologist who has made a special study of radiation and its effects. He said that Hubbard's knowledge of radiation, as displayed by his writings in \"All About Radiation\", was the 'sort of knowledge that perhaps a boy who has read Intermediate Physics might, with a lot of misapprehensions and lack of understanding, demonstrate'... From this witness's evidence it is apparent that Hubbard is completely incompetent to deal with the subject of radiation and that his knowledge of nuclear physics is distorted, inaccurate, mistaken and negligible. No evidence was called which disputed in any way these conclusions.\" \n\n\n"}
{"id": "34191472", "url": "https://en.wikipedia.org/wiki?curid=34191472", "title": "Allan Hay", "text": "Allan Hay\n\nAllan Stuart Hay FRS (July 23, 1929 in Edmonton, Alberta, Canada – August 14, 2017) was a Canadian chemist, and Tomlinson Emeritus Professor of Chemistry at McGill University. He is best known for his synthesization of Polyphenylene Oxide, leading to the development of Noryl and various other plastics.\n\nHay graduated from the University of Alberta with a B.Sc. in 1950 and an M.Sc. in 1952, and from the University of Illinois at Chicago with a Ph.D. in 1955.\n\nHe was a research chemist, and manager at General Electric, from 1955 to 1988. In 1975, he became adjunct faculty at the University of Massachusetts Amherst.\n\nIn 1987, after retiring from GE, he became a research professor of polymer chemistry at McGill University in Montreal, Quebec, Canada. Hay held the GE/NSERC Chair of Polymer Chemistry from 1987 to 1995, and the Tomlinson Chair in Chemistry from 1997 to 2014. He retired from McGill in 2014, returning to Niskayuna, New York.\n\nIn 1981, Hay was named a fellow of the Royal Society of London. In 1984 he received the IRI Achievement Award from the Industrial Research Institute in recognition for his contributions to science and technology, and society generally, for discoveries in polymerization by oxidative coupling. In 1985\nhe received the Chemical Pioneer Award from the American Institute of Chemists.\n\n"}
{"id": "23211150", "url": "https://en.wikipedia.org/wiki?curid=23211150", "title": "Amaral (crater)", "text": "Amaral (crater)\n\nAmaral is a crater on the planet Mercury. With its smooth floor, surrounding ejecta, and small secondary craters, it appears noticeably younger than the heavily cratered surface around it. Along with a smooth crater floor, Amaral also has a central peak. Bright material on this peak is of particular interest as it appears to have an unusual color. In color-enhanced images, the central peak of Amaral appears as a bright blue color in striking contrast to the otherwise orange tones of surface material nearby. The different color of the central peak likely indicates rocks with different chemical composition from those on the neighboring surface.\n"}
{"id": "3636956", "url": "https://en.wikipedia.org/wiki?curid=3636956", "title": "Ancestry-informative marker", "text": "Ancestry-informative marker\n\nIn population genetics, an ancestry-informative marker (AIM) is a single-nucleotide polymorphism that exhibits substantially different frequencies between different populations. A set of many AIMs can be used to estimate the proportion of ancestry of an individual derived from each population. \n\nA single-nucleotide polymorphism is a modification of a single nucleotide base within a DNA sequence. There are an estimated 15 million SNP (Single-nucleotide polymorphism) sites (out of roughly 3 billion base pairs, or about 0.4%) from among which AIMs may potentially be selected. The SNPs that relate to ancestry are often traced to the Y chromosome and mitochondrial DNA because both of these areas are inherited from one parent, eradicating complexities that come with parental gene recombination. SNP mutations are rare, so sequences with SNPs tend to be passed down through generations rather than altered each generation. However, because any given SNP is relatively common in a population, analysts must examine groups of SNPs (otherwise known as AIMS) to determine someone’s ancestry. Using statistical methods such as apparent error rate and Improved Bayesian Estimate, the set of SNPs with the highest accuracy for predicting a specific ancestry can be found. \n\nExamining a suite of these markers more or less evenly spaced across the genome is also a cost-effective way to discover novel genes underlying complex diseases in a technique called admixture mapping or mapping by admixture linkage disequilibrium.\n\nAs one example, the Duffy Null allele (FY*0) has a frequency of almost 100% of Sub-Saharan Africans, but occurs very infrequently in populations outside of this region. A person having this allele is thus more likely to have Sub-Saharan African ancestors. North and South Han Chinese ancestry can be distinguished unambiguously using a set of 140 AIMS.\n\nCollections of AIMs have been developed that can estimate the geographical origins of ancestors from within Europe.\n\nThe discovery of ancestry-informative markers was made possible by the development of next generation sequencing, or NGS. NGS enables the study of genetic markers by isolating specific gene sequences. One such method for sequence extraction is the use restriction enzymes, specifically endonuclease, which modifies the DNA sequence. This enzyme can be used with DNA ligase (connecting two different DNA), modifying  DNA by inserting DNA from other organism. Another method, cDNA sequencing, or RNA-seq, can also help to acquire information of the transcriptomes in a broad range of organisms and find SNPs (single nucleotide polymorphisms), within a DNA sequence. \n\nAncestry informative markers have a number of applications in genetic research, forensics, and private industry. AIMs that indicate a predisposition for diseases such as type 2 diabetes mellitus and renal disease have been shown to reduce the effects of genetic admixture in ancestral mapping when using admixture mapping software. The differential ability of ancestry-informative markers allows scientists and researchers to narrow geographical populations of concern; for example, illegal organ trafficking can be traced to certain areas by comparing the samples taken from organ recipients and deciphering the foreign marker in their body. An array of private companies, such as 23andMe and AncestryDNA, provide cost-effective direct-to-consumers (DTC) genetic testing by analyzing ancestry informative markers to determine geographic origins. These private companies collect massive quantities of data such as biological samples and self-reported information from consumers, a practice known as biobanking, enabling their researchers to discover more insights on AIMs.\n\nThough AIM panels can be useful for disease screening, the Genetic Information Nondiscrimination Act (GINA) prevents the use of genetic information for insurance and workplace discrimination.\nDifferent ancestral traits and their affiliation to diseases can help scientists determine appropriate approaches of treatment for a specific population. Medical researchers have revealed the link between ancestry traits and some common diseases; for example, individuals of African descent have been found to be at higher risk of asthma than those of European ancestry.\n\nAIM panels can be used for detecting disease risk factors. One such panel was created for African American ancestry based on subsets of commercially available SNP arrays. These types of arrays can help reduce the cost of identifying risk factors, since they allow researchers to screen for ancestry markers instead of the entire genome. This is due to the fact that these SNP arrays narrow the scope of the necessary screening from hundreds of thousands of SNP markers to a panel of a few thousands of AIMs.\n\nWhile some believe that structured populations should be used in studies to better ascertain genetic associations to diseases, the social implications of the potential racial stigma that may result from such studies is a major concern. However, the study done by Yang et al. (2005) suggests that the technology to conduct deeper research into and identify ancestry-associated variations in human disease does already exist.\n\n\n"}
{"id": "19229690", "url": "https://en.wikipedia.org/wiki?curid=19229690", "title": "Arbejdsmarkedets Tillægspension", "text": "Arbejdsmarkedets Tillægspension\n\nArbejdsmarkedets Tillægspension (ATP) is a supplementary (income-related) pension in Denmark, and is Denmark's largest lifelong pension plan. Citizens of Denmark become eligible for ATP payments as soon as they turn 65 years old. Arbejdsmarkedets Tillægspension was amended into law on March 7, 1964.\n\n"}
{"id": "58855314", "url": "https://en.wikipedia.org/wiki?curid=58855314", "title": "Avi Melamed", "text": "Avi Melamed\n\nAvi Melamed is an Israeli author, educator, and independent Middle East strategic intelligence analyst. Melamed is an author of two books entitled: \"Inside the Middle East: Making Sense of the Most Dangerous and Complicated Region on Earth\" (2016) and co-author of \"Separate and Unequal\" (1999).\n\nMelamed is a former senior official on Arab affairs and an Israeli intelligence official. He is an expert on current affairs in the Muslim and Arab society and their effect on the Middle East and Israel. He also serves as the Fellow of Intelligence and Middle East Affairs for the Eisenhower Institute.\n\nHe has been involved in an array of intelligence field positions on behalf of Israeli government agencies and Israeli Defense Forces.\nMelamed is also known as the founder of Feenjan – Israel speaks Arabic, an online medium for Arabs and Israelis to engage in various issues and discussions. The topics and discussions are centered on the latest trends of the Israeli culture and the Arab world in Arabic.\n\n"}
{"id": "346896", "url": "https://en.wikipedia.org/wiki?curid=346896", "title": "Berkeley Open Infrastructure for Network Computing", "text": "Berkeley Open Infrastructure for Network Computing\n\nThe Berkeley Open Infrastructure for Network Computing (BOINC, pronounced – rhymes with \"oink\"), an open-source middleware system, supports volunteer and grid computing. Originally developed to support the SETI@home project, it became generalized as a platform for other distributed applications in areas as diverse as mathematics, linguistics, medicine, molecular biology, climatology, environmental science, and astrophysics, among others. BOINC aims to enable researchers to tap into the enormous processing resources of multiple personal computers around the world.\n\nBOINC development originated with a team based at the Space Sciences Laboratory (SSL) at the University of California, Berkeley and led by David Anderson, who also leads SETI@home. As a high-performance distributed computing platform, BOINC brings together about 311,742 active participants and 834,343 active computers (hosts) worldwide processing on average 26.431 PetaFLOPS . (it would be the fourth largest processing capability in the world compared with an individual supercomputer Supercomputer TOP500 list) The National Science Foundation (NSF) funds BOINC through awards SCI/0221529, SCI/0438443 and SCI/0721124. \"Guinness World Records\" ranks BOINC as the largest computing grid in the world.\n\nBOINC code runs on various operating systems, including Microsoft Windows, macOS, Android, Linux and FreeBSD. BOINC is free software released under the terms of the GNU Lesser General Public License (LGPL).\n\nBOINC was originally developed to manage the SETI@home project.\n\nThe original SETI client was a non-BOINC software exclusively for SETI@home. As one of the first volunteer grid computing projects, it was not designed with a high level of security. As a result, some participants in the project attempted to cheat the project to gain \"credits,\" while some others submitted entirely falsified work. BOINC was designed, in part, to combat these security breaches.\n\nThe BOINC project started in February 2002, and the first version was released on April 10, 2002. The first BOINC-based project was Predictor@home launched on June 9, 2004. In 2009, AQUA@home deployed multi-threaded CPU applications for the first time, followed by the first OpenCL application in 2010.\n\n, 37 BOINC projects are active.\n\nIn essence, BOINC is software that can use the unused CPU and GPU cycles on a computer to do scientific computing—what one individual does not use of his/her computer, BOINC uses. In late 2008, BOINC's official website announced that Nvidia had developed a system called CUDA that uses GPUs for scientific computing. With NVIDIA's assistance, some BOINC-based projects (e.g., SETI@home, MilkyWay@home) now have applications that run on NVIDIA GPUs using CUDA. Beginning in October 2009, BOINC added support for the ATI/AMD family of GPUs also. These applications run from 2 to 10 times faster than the former CPU-only versions. In 7.x preview versions, GPU support (via OpenCL) was added for computers using Mac OS X with AMD Radeon graphic cards.\n\nBOINC consists of a server system and client software that communicate with each other to distribute and process work units and return the results.\n\nBOINC can be controlled remotely by remote procedure calls (RPC), from the command line, and from the BOINC Account Manager.\n\nBOINC Manager currently has two \"views\": the \"Advanced View\" and the \"Simplified GUI\". The \"Grid View\" was removed in the 6.6.x clients as it was redundant.\n\nThe appearance (skin) of the Simplified GUI is user-customizable, in that users can create their own designs.\n\nA BOINC app also exists for Android, allowing every person owning an Android device – smartphone, tablet and Kindle – to share their unused computing power. The user is allowed to select the research projects they want to support, if it is in the app's available project list.\n\nBy default, the application will allow computing only when the device is connected to a WiFi network, is being charged, and the battery has a charge of at least 90%. Only some of the BOINC projects are available, including Asteroids@home, Collatz Conjecture, Einstein@home, Enigma@home, LHC@home, Moo! Wrapper, Quake Catcher Network, Rosetta@home, SETI@home, theSkyNet POGS, Universe@Home, World Community Grid and Yoyo@home.\n\nA BOINC Account Manager is an application that manages multiple BOINC project accounts across multiple computers (CPUs) and operating systems. Account managers were designed for people who are new to BOINC or have several computers participating in several projects. The account manager concept was conceived and developed jointly by GridRepublic and BOINC. Current account managers include:\n\nThe BOINC Credit System is designed to avoid cheating by validating results before granting credit.\n\nThere are about 35 projects currently listed, of which about half yield published reports. The licensing of the projects varies.\n\nSince 2013, the cryptocurrency Gridcoin has been associated with BOINC as a remunerative coin. Gridcoin uses a modified proof-of-stake timestamping system called proof-of-research to reward participants for computational work completed on BOINC. The proof-of-research system was implemented on October 11, 2014. The system takes into account a parameter supplied with the limited number of white-listed projects called RAC (Recent Average Credit), and distributes the coin according to the proportion of RAC acquired in the project to the people who are computing in it. Each whitelisted project gets the same amount of GRC to distribute among its contributors.\n\n\n"}
{"id": "7135901", "url": "https://en.wikipedia.org/wiki?curid=7135901", "title": "Carpathite", "text": "Carpathite\n\nCarpathite (also \"pendletonite\" and \"karpatite\") is a very rare hydrocarbon mineral. It is the mineral form of the polycyclic aromatic hydrocarbon coronene with formula: CH. \n\nIt was first described in 1955 for an occurrence in Transcarpathian Oblast, Ukraine. It was named for the Carpathian Mountains. It has also been reported from the Presov Region of the Slovak Republic, the Kamchatka Oblast in Russia and from San Benito County, California.\n\nIt occurs at the contact zone of a diorite intrusive into argillite within cavities in the Ukraine. In the California occurrence it appears as a low temperature hydrothermal phase. It is associated with idrialite, amorphous organic material, calcite, barite, quartz, cinnabar, and metacinnabar. \n"}
{"id": "2537182", "url": "https://en.wikipedia.org/wiki?curid=2537182", "title": "Categories for the Working Mathematician", "text": "Categories for the Working Mathematician\n\nCategories for the Working Mathematician (CWM) is a textbook in category theory written by American mathematician Saunders Mac Lane, who cofounded the subject together with Samuel Eilenberg. It was first published in 1971, and is based on his lectures on the subject given at the University of Chicago, the Australian National University, Bowdoin College, and Tulane University. It is widely regarded as the premier introduction to the subject.\n\nThe book has twelve chapters, which are:\n\nChapters XI and XII were added in the 1998 second edition, the first in view of its importance in string theory and quantum field theory, and the second to address higher-dimensional categories that have come into prominence.\n\nAlthough it is the classic reference for category theory, some of the terminology is not standard. In particular, Mac Lane attempted to settle an ambiguity in usage for the terms epimorphism and monomorphism by introducing the terms \"epic\" and \"monic,\" but the distinction is not in common use.\n"}
{"id": "6397814", "url": "https://en.wikipedia.org/wiki?curid=6397814", "title": "Cecil Edgar Tilley", "text": "Cecil Edgar Tilley\n\nCecil Edgar Tilley FRS (14 May 1894 – 24 January 1973) was an Australian-British petrologist and geologist.\n\nHe was born in Unley, Adelaide, the youngest child of John Thomas Edward Tilley, a civil engineer from London, and his wife South Australia-born wife Catherine Jane (née Nicholas). He was educated at Adelaide High School, then studied under William Rowan Browne at the University of Adelaide, and the University of Sydney. He went to England to work in the munitions industry during World War I but returned after the Armistice.\n\nHe won an Exhibition of 1851 scholarship to the University of Cambridge in 1919 where he studied under Alfred Harker. Most of the remainder of his career was spent in England, though spent 1938-9 in Australia and visited regularly after the War.\n\nIn 1929, while investigating a volcanic plug at Scawt Hill, near Larne, Northern Ireland for the Mineralogical Magazine he identified and named the new minerals larnite and scawtite.\n\nIn 1931 he was appointed professor of Mineralogy and Petrology at Trinity Hall, Cambridge, a newly created department.\n\n"}
{"id": "13902952", "url": "https://en.wikipedia.org/wiki?curid=13902952", "title": "College &amp; Research Libraries", "text": "College &amp; Research Libraries\n\nCollege & Research Libraries is a bimonthly peer-reviewed academic journal published by the Association of College and Research Libraries.\n\nIt was established in December 1939 and was published quarterly for its first 18 years, then bimonthly since 1956. It publishes articles that are intended to help academic librarians build an intellectual framework to serve the needs of collegiate users. The editor-in-chief is Wendi Arant Kaspar (Texas A&M University Policy Sciences and Economics Library). The journal is open access since 2011.\n\nThe journal is abstracted and indexed in Scopus, Social Science Citation Index, , Academic Search Premier, FRANCIS, PASCAL, EBSCO Education Source, Educational research abstracts (ERA), Information Science and Technology Abstracts, Library and Information Science Abstracts, Library Literature and Information Science, and MLA - Modern Language Association Database.\n\nAccording to \"Journal Citation Reports\", the journal has a 2017 impact factor of 1.626, ranking it 36th out of 88 journals in the category \"Information Science & Library Science\".\n"}
{"id": "7174287", "url": "https://en.wikipedia.org/wiki?curid=7174287", "title": "Contact number", "text": "Contact number\n\nIn chemistry, a contact number (CN) is a simple solvent exposure measure that measures residue burial in proteins. The definition of CN varies between authors, but is generally defined as the number of either Cformula_1 or Cformula_2 atoms within a sphere around the Cformula_1 or Cformula_2 atom of the residue. The radius of the sphere is typically chosen to be between 8 and 14Å.\n\n\n9179333174\n"}
{"id": "56499662", "url": "https://en.wikipedia.org/wiki?curid=56499662", "title": "Data science competition platform", "text": "Data science competition platform\n\nA data science competition platform is used by businesses to host data science challenges that are hard to solve for one group. Historically, crowdsourcing challenges have been known to solve very complex problems. The Netflix Prize is one such competition. Since then there have been several platforms developed on the idea of data science competitions. Research has been completed on how competition can improve research performance. \n\nExamples of data science competition platforms include Kaggle, InnoCentive, and Alibaba Tianchi. Alibaba's competition platform which was used in KDD 2017.\n"}
{"id": "164430", "url": "https://en.wikipedia.org/wiki?curid=164430", "title": "Documentation", "text": "Documentation\n\nDocumentation is a set of documents provided on paper, or online, or on digital or analog media, such as audio tape or CDs. Examples are user guides, white papers, on-line help, quick-reference guides. It is becoming less common to see paper (hard-copy) documentation. Documentation is distributed via websites, software products, and other on-line applications.\n\nProfessionals educated in this field are termed documentalists. This field changed its name to information science in 1968, but some uses of the term documentation still exists and there have been efforts to reintroduce the term documentation as a field of study.\n\nWhile associated ISO standards are not easily available publicly, a guide from other sources for this topic may serve the purpose. . David Berger has provided several principles of document writing, regarding the terms used, procedure numbering and even lengths of sentences, etc.\n\nThe following is a list of guides dealing with each specific field and type:\n\nThe procedures of documentation vary from one sector, or one type, to another. In general, these may involve document drafting, formatting, submitting, reviewing, approving, distributing, reposting and tracking, etc., and are convened by associated SOPs in a regulatory industry. It could also involve creating content from scratch. Documentation should be easy to read and understand. If it's too long and too wordy, it may be misunderstood or ignored. Clear, Short, Familiar words should be used to a maximum of 15 words to a sentence. Only gender hyper neutral word should be used and cultural biases should be avoided. Procedures should be numbered when they are to be performed. .\n\nTechnical writers and corporate communicators are professionals whose field and work is documentation. Ideally, technical writers have a background in both the subject matter and also in writing and managing content (information architecture). Technical writers more commonly collaborate with subject matter experts (SMEs), such as engineers, technical experts, medical professionals, or other types of clients to define and then create content (documentation) that meets the user's needs. Corporate communications includes other types of written documentation that is required for most companies.\n\n\n\nThe following are typical software documentation types\n\nThe following are typical hardware and service documentation types\n\nDocumentation include such as feasibility report, technical documentation, operational documentation, log book, etc.\n\nThere are many types of software and applications used to create documentation.\n\nSOFTWARE DOCUMENTATION FOLDER (SDF)\n\nA common type of software document written by software engineers in the simulation industry is the SDF. When developing software for a simulator, which can range from embedded avionics devices to 3D terrain databases by way of full motion control systems, the engineer keeps a notebook detailing the development \"the build\" of the project or module. The document can be a wiki page, MS word document or other environment. They should contain a \"requirements\" section, an \"interface\" section to detail the communication interface of the software. Often a \"notes\" section is used to detail the proof of concept, and then track errors and enhancements. Finally, a \"testing\" section to document how the software was tested. This documents conformance to the client's requirements. The result is a detailed description of how the software is designed, how to build and install the software on the target device, and any known defects and work-arounds. This build document enables future developers and maintainers to come up to speed on the software in a timely manner, and also provides a roadmap to modifying code or searching for bugs.\n\nSOFTWARE FOR NETWORK INVENTORY AND CONFIGURATION (CMDB)\n\nThese software tools can automatically collect data of your network equipment. The data could be for inventory and for configuration information. The ITIL Library requests to create such a database as a basis for all information for the IT responsible. It's also the basis for IT documentation. Examples include XIA Configuration.\n\n\"Documentation\" is the preferred term for the process of populating criminal databases. Examples include the National Counter-terrorism Center's Terrorist Identities Datamart Environment (\"TIDE\"), sex offender registries, and gang databases.\n\nDocumentation, as it pertains to the Early Childhood Education field, is \"when we notice and value children's ideas, thinking, questions, and theories about the world and then collect traces of their work (drawings, photographs of the children in action, and transcripts of their words) to share with a wider community\" \n\nThusly, documentation is a process, used to link the educator's knowledge and learning of the child/children with the families, other collaborators, and even to the children themselves. \n\nDocumentation is an integral part of the cycle of inquiry - observing, reflecting, documenting, sharing and responding. \n\nPedagogical documentation, in terms of the teacher documentation, is the \"teacher's story of the movement in children's understanding\". According to Stephanie Cox Suarez in 'Documentation - Transforming our Perspectives', \"teachers are considered researchers, and documentation is a research tool to support knowledge building among children and adults\" \n\nDocumentation can take many different styles in the classroom. The following exemplifies ways in which documentation can make the 'research', or learning, visible:\n\n\nDocumentation is certainly a process in and of itself, and it is also a process within the educator. The following is the development of documentation as it progresses for and in the educator themselves:\n\n\n\n"}
{"id": "1206859", "url": "https://en.wikipedia.org/wiki?curid=1206859", "title": "Extended X-ray absorption fine structure", "text": "Extended X-ray absorption fine structure\n\nformula_4\n\nThe absorption coefficient is obtained by taking the log ratio of the incident x-ray intensity to the transmitted x-ray intensity. \n\nformula_5\n\nWhen the incident x-ray energy matches the binding energy of an electron of an atom within the sample, the number of x-rays absorbed by the sample increases dramatically, causing a drop in the transmitted x-ray intensity. This results in an absorption edge. Each element on the periodic table has a set of unique absorption edges corresponding to different binding energies of its electrons, giving XAS element selectivity. XAS spectra are most often collected at synchrotrons. Because X-rays are highly penetrating, XAS samples can be gases, solids or liquids. And because of the brilliance of synchrotron X-ray sources the concentration of the absorbing element can be as low as a few ppm. \n\nEXAFS spectra are displayed as graphs of the absorption coefficient of a given material versus energy, typically in a 500 – 1000 eV range beginning before an absorption edge of an element in the sample. The x-ray absorption coefficient is usually normalized to unit step height. This is done by regressing a line to the region before and after the absorption edge, subtracting the pre-edge line from the entire data set and dividing by the absorption step height, which is determined by the difference between the pre-edge and post-edge lines at the value of E0 (on the absorption edge).\n\nThe normalized absorption spectra are often called XANES spectra. These spectra can be used to determine the average oxidation state of the element in the sample. The XANES spectra are also sensitive to the coordination environment of the absorbing atom in the sample. Finger printing methods have been used to match the XANES spectra of an unknown sample to those of known \"standards\". Linear combination fitting of several different standard spectra can give an estimate to the amount of each of the known standard spectra within an unknown sample.\n\nX-ray absorption spectra are produced over the range of 200 – 35,000 eV. The dominant physical process is one where the absorbed photon ejects a core photoelectron from the absorbing atom, leaving behind a core hole. The atom with the core hole is now excited. The ejected photoelectron’s energy will be equal to that of the absorbed photon minus the binding energy of the initial core state. The ejected photoelectron interacts with electrons in the surrounding non-excited atoms.\n\nIf the ejected photoelectron is taken to have a wave-like nature and the surrounding atoms are described as point scatterers, it is possible to imagine the backscattered electron waves interfering with the forward-propagating waves. The resulting interference pattern shows up as a modulation of the measured absorption coefficient, thereby causing the oscillation in the EXAFS spectra. A simplified plane-wave single-scattering theory has been used for interpretation of EXAFS spectra for many years, although modern methods (like FEFF, GNXAS) have shown that curved-wave corrections and multiple-scattering effects can not be neglected. The photelectron scattering amplitude in the low energy range (5-200 eV) of the photoelectron kinetic energy become much larger so that multiple scattering events become dominant in the XANES (or NEXAFS) spectra.\n\nThe wavelength of the photoelectron is dependent on the energy and phase of the backscattered wave which exists at the central atom. The wavelength changes as a function of the energy of the incoming photon. The phase and amplitude of the backscattered wave are dependent on the type of atom doing the backscattering and the distance of the backscattering atom from the central atom. The dependence of the scattering on atomic species makes it possible to obtain information pertaining to the chemical coordination environment of the original absorbing (centrally excited) atom by analyzing these EXAFS data.\n\nSince EXAFS requires a tunable x-ray source, data are always collected at synchrotrons, often at beamlines which are especially optimized for the purpose. The utility of a particular synchrotron to study a particular solid depends on the brightness of the x-ray flux at the absorption edges of the relevant elements.\n\nXAS is an interdisciplinary technique and its unique properties, as compared to x-ray diffraction, have been exploited for\nunderstanding the details of local structure in:\n\n\nEXAFS is, like XANES, a highly sensitive technique with elemental specificity. As such, EXAFS is an extremely useful way to determine the chemical state of practically important species which occur in very low abundance or concentration. Frequent use of EXAFS occurs in environmental chemistry, where scientists try to understand the propagation of pollutants through an ecosystem. EXAFS can be used along with accelerator mass spectrometry in forensic examinations, particularly in nuclear non-proliferation applications.\n\nFor an example of an EXAFS study of uranium chemistry in glass see , and for a general study of trivalent lanthanides and actinides in chloride containing aqueous media can be read at \n\n\nA very detailed, balanced and informative account about the history of EXAFS (originally called Kossel's structures) is given in the paper \"A History of the X-ray Absorption Fine Structure\" by R. Stumm von Bordwehr, Ann. Phys. Fr. vol. 14, 377-466 (1989).\nA more modern and accurate account of the history of XAFS (EXAFS and XANES) is given by the leader of the group that developed the modern version of XAFS in the award lecture \"Musings about the development of the XAFS\" by Edward A. Stern, J. Synchrotron Rad. (2001). 8, 49-54.\n\n\n\nBook Chapters\n\n"}
{"id": "49641387", "url": "https://en.wikipedia.org/wiki?curid=49641387", "title": "Feodosiyite", "text": "Feodosiyite\n\nFeodosiyite is a very rare chloride mineral, just recently approved, with the formula CuMgCl(OH)•16HO. Its structure is unique. Feodosiyite comes from the Tolbachik volcano, famous for many rare fumarolic minerals. Chemically similar minerals, chlorides containing both copper and magnesium, include haydeeite, paratacamite-(Mg) and tondiite.\n"}
{"id": "1517562", "url": "https://en.wikipedia.org/wiki?curid=1517562", "title": "Gillet de Laumont", "text": "Gillet de Laumont\n\nFrançois Pierre Nicholas Gillet de Laumont (28 May 1747 – 1 June 1834) was a French mineralogist.\n\nHe was born in Paris, educated at a military school and served in the army from 1772 to 1784, when he was appointed inspector of mines. His attention in his leisure time was wholly given to mineralogy, and he assisted in organizing the new École des Mines in Paris. \n\nHe was author of numerous mineralogical papers in the \"Journal et Annales des Mines\". The mineral laumontite, which Laumont discovered in the mines of Huelgoat, was named after him by René Just Haüy. After the death of Jean-Baptiste L. Romé de l'Isle in 1790, Laumont purchased his large collection of minerals and crystals.\n\nDuring his career, he was awarded with the Ordre de la Réunion (1813), the Légion d'honneur (1815) and the Ordre de Saint-Michel (1819). He died in Paris in 1834.\n\n"}
{"id": "28765301", "url": "https://en.wikipedia.org/wiki?curid=28765301", "title": "Groves classification system", "text": "Groves classification system\n\nThe Groves Classification is a numbering system to enable the shape of any academic gown or hood to be easily described and identified. It was devised by Nicholas Groves to establish a common terminology for hoods and gown to remedy the situation of individual universities using differing terms to describe the same item. As such it is used in same manner as an heraldic blazon whereby a textual description enables a coat of arms to be drawn. The system was first described in the Burgon Society's annual in 2001 and adopted as standard by robe makers and scholars of academic dress.\n\nThe original Groves Classification included a standardization for shapes and patterns of hoods and gowns worn by graduates and undergraduates. Further information was given regarding the use of different fabrics and standardization of colors, but the focus was placed on gowns and hoods which are explained further below.\n\nThe Classification undergoes periodic revision as new hood and gown patterns emerge.\n\nHoods in the Classification are divided into three different types as summarised in the table below. Unlike the gowns and robes, these are based on the shape of the hood rather than the degrees for which they are worn.\n\n\nGowns in the Groves system are divided into three classes. These generally follow the shapes associated with each different academic degree in the British educational system. \n\nThe Groves classification system was first published in 2001. Since that time, it has been adapted and changed to include newly devised academic dress and revisions of existing schemes. The Burgon Society maintains a comprehensive listing of system shapes on its website. In addition to those included above, the current list classifies undergraduate gowns in use throughout the United Kingdom and academic headwear.\n"}
{"id": "42848605", "url": "https://en.wikipedia.org/wiki?curid=42848605", "title": "Habitable Planets for Man", "text": "Habitable Planets for Man\n\nHabitable Planets For Man is a work by Stephen Dole, first edition published by Blaisdell Publishing Company, A division of Ginn and Company, copyright 1964 by The RAND Corporation. Originally 158 pages, it was republished in a posthumous second edition in 2007, as \"Planets for Man\". The revised edition, 174-page book, contains a detailed scientific study on the nature of worlds that may support life in the universe, the probability of their existence, and ways of finding them,<ref name=\"http://blogs.scientificamerican.com\"></ref> including assessments of 14 stars within 22 light years with a relatively high probability of having habitable planets (a collective probability of 43%). Writing in a \"Scientific American\" blog in 2011, Caleb Scharf called it \"extraordinarily detailed and prescient\".\n\n\n"}
{"id": "48621332", "url": "https://en.wikipedia.org/wiki?curid=48621332", "title": "Human Universe (book)", "text": "Human Universe (book)\n\nHuman Universe is a 2014 book by the theoretical physicists Brian Cox and Andrew Cohen. The book aims to explore Human life as well as understand what it is, and is explained in a way that is accessible to a general reader. The book is based on a series with the same name \"Human Universe\".\n"}
{"id": "2769656", "url": "https://en.wikipedia.org/wiki?curid=2769656", "title": "International Botanical Congress", "text": "International Botanical Congress\n\nInternational Botanical Congress (IBC) is an international meeting of botanists in all scientific fields, authorized by the International Association of Botanical and Mycological Societies (IABMS) and held every six years, with the location rotating between different continents. The current numbering system for the congresses starts from the year 1900; the XVIII IBC was held in Melbourne, Australia, 24–30 July 2011, and the XIX IBC was held in Shenzhen, China, 23–29 July 2017.\n\nThe IBC has the power to alter the ICN (International Code of Nomenclature for algae, fungi, and plants), which was renamed from the International Code of Botanical Nomenclature (ICBN) at the XVIII IBC. Formally the power resides with the Plenary Session; in practice this approves the decisions of the Nomenclature Section. The Nomenclature Section meets before the actual Congress and deals with all proposals to modify the Code: this includes ratifying recommendations from sub-committees on conservation. To reduce the risk of a hasty decision the Nomenclature Section adopts a 60% majority requirement for any change not already recommended by a committee.\n\nPrior to the first International Botanical Congress, local congresses concerned with natural sciences generally had grown to be very large, and a more specialized but also international meeting was considered desirable. The first annual IBC was held in 1864 in Brussels, in conjunction with an international horticultural exhibit. At the second annual congress (held in Amsterdam), Karl Koch made a proposal to standardize botanical nomenclature, and the third congress (held in London) resolved that this matter would be dealt with by the next congress.\n\nThe fourth congress, which had as one of its principal purposes to establish laws of botanical nomenclature, was organized by la Société botanique de France, and took place in Paris in August 1867. The laws adopted were based on those prepared by Alphonse de Candolle. Regular international botanical and/or horticultural congresses were held but made no further changes to nomenclature until the 1892 meeting in Genoa, which made some small changes to the laws of nomenclature. Subsequent meetings are as follows in the table below. The \"Code\" column shows whether a code of nomenclature was adopted.\n\n"}
{"id": "57315272", "url": "https://en.wikipedia.org/wiki?curid=57315272", "title": "List of German physicists", "text": "List of German physicists\n\nThis is a list of German physicists.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1525551", "url": "https://en.wikipedia.org/wiki?curid=1525551", "title": "List of astronomical interferometers at visible and infrared wavelengths", "text": "List of astronomical interferometers at visible and infrared wavelengths\n\nHere is a list of currently existing astronomical optical interferometers (i.e. operating from visible to mid-infrared wavelengths), and some parameters describing their performance.\n\nColumns 2-5 determine the range of targets that can be observed and the range of science which can be done. Higher limiting magnitude means that the array can observe fainter sources. The limiting magnitude is determined by the atmospheric seeing, the diameters of the telescopes and the light lost in the system. A larger range of baselines means that a wider variety of science can be done and on a wider range of sources.\n\nColumns 6-10 indicate the approximate quality and total amount of science data the array is expected to obtain. This is per year, to account for the average number of cloud-free nights on which each array is operated.\n\n"}
{"id": "3258074", "url": "https://en.wikipedia.org/wiki?curid=3258074", "title": "List of common resolutions", "text": "List of common resolutions\n\nThis article lists computer monitor screen resolutions that are defined by standards or in common use. Most of them use certain preferred numbers.\n\n\nFor television, the display aspect ratio (DAR) is shown, not the storage aspect ratio (SAR); analog television does not have well-defined pixels, while several digital television standards have non-square pixels.\n\nThe below distinguish SAR (aspect ratio of pixel dimensions), DAR (aspect ratio of displayed image dimensions), and the corresponding PAR (aspect ratio of individual pixels), though it currently contains some errors (inconsistencies), as flagged.\n\n\n\n"}
{"id": "48628750", "url": "https://en.wikipedia.org/wiki?curid=48628750", "title": "List of log-structured file systems", "text": "List of log-structured file systems\n\nList of log-structured file system implementations.\n\n\nSome kinds of storage media, such as flash memory and CD-RW, slowly degrade as they are written to and have a limited number of erase/write cycles at any one location. Log-structured file systems are sometimes used on these media because they make fewer in-place writes and thus prolong the life of the device by wear leveling. The more common such file systems include:\n\n\n"}
{"id": "3648693", "url": "https://en.wikipedia.org/wiki?curid=3648693", "title": "List of physics journals", "text": "List of physics journals\n\nThis is a list of physics journals with existing articles on Wikipedia. The list is organized by subfields of physics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "8635249", "url": "https://en.wikipedia.org/wiki?curid=8635249", "title": "List of solar cycles", "text": "List of solar cycles\n\nThe following is a list of solar cycles (sometimes called sunspot cycles), tracked since 1755 following the original numbering proposed by Rudolf Wolf in the mid-19th century The source data are the revised International Sunspot Numbers (ISN v2.0), as available at SILSO.\nSunspot number counts exist since 1610 but the cycle numbering is not well defined during the Maunder minimum. It was proposed that one cycle might have been lost in the late 18th century, but this still remains not fully confirmed.\n\nThe smoothing was done using the traditional SIDC smoothing formula. Other smoothing formulas exist, and they usually give slightly different values for the amplitude and timings of the solar cycles. An example is the Meeus smoothing formula, with related solar cycles characteristics available in this STCE news item.\n\nIn the table below, the number of spotless days is the number between the maximum of the previous solar cycle and the maximum of the new solar cycle. As an example, there were 817 spotless days during the transit from solar cycle 23 to solar cycle 24.\n\n"}
{"id": "3817699", "url": "https://en.wikipedia.org/wiki?curid=3817699", "title": "Malthusian trap", "text": "Malthusian trap\n\nThe Malthusian trap or population trap is a condition whereby excess population would stop growing due to shortage of food supply leading to starvation. It is named for Thomas Robert Malthus, who suggested that while technological advances could increase a society's supply of resources, such as food, and thereby improve the standard of living, the resource abundance would enable population growth, which would eventually bring the per capita supply of resources back to its original level. Some economists contend that since the industrial revolution, mankind has broken out of the trap. Others argue that the continuation of extreme poverty indicates that the Malthusian trap continues to operate. Others further argue that due to lack of food availability coupled with excessive pollution, developing countries show more evidence of the trap.\n\nMalthus argued that society has a natural propensity to increase its population, a propensity that causes population growth to be the best measure of the happiness of a people: \"The happiness of a country does not depend, absolutely, upon its poverty, or its riches, upon its youth, or its age, upon its being thinly, or fully inhabited, but upon the rapidity with which it is increasing, upon the degree in which the yearly increase of food approaches to the yearly increase of an unrestricted population.\"\n\nHowever, the propensity for population increase also leads to a natural cycle of abundance and shortages:\nMalthus faced opposition from economists both during his life and since. A vocal critic several decades later was Friedrich Engels.\n\nResearch indicates that technological superiority and higher land productivity had significant positive effects on population density but insignificant effects on the standard of living during the time period 1–1500 AD. In addition, scholars have reported on the lack of a significant trend of wages in various places over the world for very long stretches of time. In Babylonia during the period 1800 to 1600 BC, for example, the daily wage for a common laborer was enough to buy about 15 pounds of wheat. In Classical Athens in about 328 BC, the corresponding wage could buy about 24 pounds of wheat. In England in 1800 AD the wage was about 13 pounds of wheat. In spite of the technological developments across these societies, the daily wage hardly varied. In Britain between 1200 and 1800, only relatively minor fluctuations from the mean (less than a factor of two) in real wages occurred. Following depopulation by the Black Death and other epidemics, real income in Britain peaked around 1450–1500 and began declining until the British Agricultural Revolution. Historian Walter Scheidel posits that waves of plague following the initial outbreak of the Black Death throughout Europe had a leveling effect that changed the ratio of land to labor, reducing the value of the former while boosting that of the latter, which lowered economic inequality by making employers and landowners less well off while improving the economic prospects and living standards of workers. He says that \"the observed improvement in living standards of the laboring population was rooted in the suffering and premature death of tens of millions over the course of several generations.\" This leveling effect was reversed by a \"demographic recovery that resulted in renewed population pressure.\"\n\nRobert Fogel published a study of lifespans and nutrition from about a century before Malthus to the 19th century that examined European birth and death records, military and other records of height and weight that found significant stunted height and low body weight indicative of chronic hunger and malnutrition. He also found short lifespans that he attributed to chronic malnourishment which left people susceptible to disease. Lifespans, height and weight began to steadily increase in the UK and France after 1750. Fogel's findings are consistent with estimates of available food supply.\n\nThe rapid increase in the global population since 1900 exemplifies Malthus's predicted population patterns, whereby expansion of food supply has encouraged population growth. \"Neo-Malthusianism\" may be used as a label for those who are concerned that human overpopulation may increase resource depletion or environmental degradation to a degree that is not sustainable. Many in environmental movements express concern over the potential dangers of population growth. In 1968, ecologist Garrett Hardin published an influential essay in \"Science\" that drew heavily from Malthusian theory. His essay, \"The Tragedy of the Commons,\" argued that \"a finite world can support only a finite population\" and that \"freedom to breed will bring ruin to all.\" The Club of Rome published a famous book entitled \"The Limits to Growth\" in 1972. Paul R. Ehrlich is a prominent neo-Malthusian who first raised concerns in 1968 with the publication of \"The Population Bomb\".\n\nOn the other hand, in 2011 Andrey Korotayev and his colleagues suggested that the emergence of major sociopolitical upheavals at the escape from the Malthusian trap is not an abnormal, but a regular phenomenon.\n\nThe view that a \"breakout\" from the Malthusian trap has led to an era of sustained economic growth is explored by \"unified growth theory\". One branch of unified growth theory is devoted to the interaction between human evolution and economic development. Some argue that natural selection during the Malthusian epoch selected beneficial traits to the growth process and brought about the Industrial Revolution.\n\nSome researchers contend that a British breakout occurred due to technological improvements and structural change away from agricultural production, while coal, capital, and trade played a minor role. Economic historian Gregory Clark has argued, in his book \"A Farewell to Alms\", that a British breakout may have been caused by differences in reproduction rates among the rich and the poor (the rich were more likely to marry, tended to have more children, and, in a society where disease was rampant and childhood mortality at times approached 50%, upper-class children were more likely to survive to adulthood than poor children.) This in turn led to sustained \"downward mobility\": the descendants of the rich becoming more populous in British society and spreading middle-class values such as hard work and literacy.\n\n"}
{"id": "5734615", "url": "https://en.wikipedia.org/wiki?curid=5734615", "title": "Mosaic virus", "text": "Mosaic virus\n\nMosaic viruses are plant viruses that cause the leaves to have a speckled appearance. Mosaic virus is not a taxon.\n\nSpecies include:\n\n"}
{"id": "1323590", "url": "https://en.wikipedia.org/wiki?curid=1323590", "title": "National Medal of Technology and Innovation", "text": "National Medal of Technology and Innovation\n\nThe National Medal of Technology and Innovation (formerly the National Medal of Technology) is an honor granted by the President of the United States to American inventors and innovators who have made significant contributions to the development of new and important technology. The award may be granted to a specific person, to a group of people or to an entire organization or corporation. It is the highest honor the United States can confer to a US citizen for achievements related to technological progress.\n\nThe National Medal of Technology was created in 1980 by the United States Congress under the Stevenson-Wydler Technology Innovation Act. It was a bipartisan effort to foster technological innovation and the technological competitiveness of the United States in the international arena. The first National Medals of Technology were issued in 1985 by then-U.S. President Ronald Reagan to 12 individuals and one company. Among the first recipients were Steve Jobs and Stephen Wozniak, founders of Apple Computer. The medal has since been awarded annually.\n\nOn August 9, 2007, President George Bush signed the America COMPETES (Creating Opportunities to Meaningfully Promote Excellence in Technology, Education, and Science) Act of 2007. The Act amended Section 16 of the Stevenson-Wydler Technology Innovation Act of 1980, changing the name of the Medal to the \"National Medal of Technology and Innovation\".\n\nEach year the Technology Administration under the U.S. Department of Commerce calls for the nomination of new candidates for the National Medal of Technology. Candidates are nominated by their peers who have direct, first-hand knowledge of the candidates achievements. Candidates may be individuals, teams of individuals (up to 4), organizations or corporations. Individuals and all members of teams nominated must be U.S. citizens and organizations and corporations must be U.S.-owned (i.e. 50% of their assets or shares must be currently held by U.S. citizens).\n\nAll nominations are referred to the National Medal of Technology Evaluation Committee which issues recommendations to the U.S. Secretary of Commerce. All nominees selected as finalists through the merit review process will be subject to an FBI security check. Information collected through the security check may be considered in the final selection of winners. The Secretary of Commerce is then able to advise the President of the United States as to which candidates ought to receive the National Medal of Technology. The new National Medal of Technology laureates are then announced by the U.S. President once the final selections have been made.\n\n, there have been more than 135 individuals and 12 companies recognized. Summarized here is a list of notable laureates and a summary of their accomplishments.\n\n\n"}
{"id": "2219887", "url": "https://en.wikipedia.org/wiki?curid=2219887", "title": "Negative refraction", "text": "Negative refraction\n\nNegative refraction is the name for an electromagnetic phenomenon where light rays are refracted at an interface in the reverse sense to that normally expected. Such an effect can be obtained using a metamaterial which has been designed to achieve a negative value for both (electric) permittivity ε and (magnetic) permeability μ, as in such cases the material can be assigned a negative refractive index. Such materials are sometimes called \"double negative\" materials.\n\nNegative refraction occurs at interfaces between materials at which one has an ordinary positive phase velocity (i.e. a positive refractive index), and the other has the more exotic negative phase velocity (a negative refractive index).\n\nNegative phase velocity (NPV) is a property of light propagation in a medium. There are different definitions of NPV, the most common being Veselago's original proposal of opposition of wavevector and (Abraham) Poynting vector, i.e. E×H; other common choices are opposition of wavevector to group velocity, or to energy velocity. The use of \"phase velocity\" in the naming convention, as opposed to the perhaps more appropriate \"wave vector\", follows since phase velocity has the same sign as the wavevector.\n\nA typical criterion used to determine Veselago NPV is that the dot product of the Poynting vector and wavevector is negative (i.e. that formula_1); however this definition is not covariant. Whilst this restriction is rarely of practical significance, the criterion has nevertheless been generalized into a covariant form. For plane waves propagating in a Veselago NPV medium, the electric field, magnetic field and wave vector follow a left-hand rule, rather than the usual right-hand rule. This gives rise to the name \"left-handed (meta)materials\". However, the terms left-handed and right-handed can also arise in the study of chiral media, so this terminology is best avoided.\n\nWe can choose to avoid directly considering the Poynting vector and wavevector or a propagating light field, and consider instead the response of the materials directly: that is, we consider what values of permittivity ε and permeability µ result in negative phase velocity (NPV). Since both ε and µ are in general complex, their imaginary parts do not have to be negative for a passive (i.e. lossy) material to display negative refraction. The most general Veselago criterion applying to ε and µ is that of Depine and Lakhtakia, although other less general forms exist. The Depine-Lakhtakia criterion for negative phase velocity is\n\nwhere formula_3 are the real valued parts of ε and µ, respectively. However, negative refraction (negative refractive index) and negative phase velocity can be distinct from each other, even in passive materials, but also in active materials.\n\nTypically, the refractive index \"n\" is determined using formula_4, where by convention the positive square root is chosen for \"n\". However, in NPV materials, we reverse that convention and pick the negative sign to mimic the fact that the wavevector (and hence phase velocity) are likewise reversed. Strictly speaking, the refractive index is a derived quantity telling us how the wavevector is related to the optical frequency and propagation direction of the light, thus the sign of \"n\" must be chosen to match the physical situation.\n\nThe principal symptom of negative refraction is just that – light rays are \nrefracted on the \"same\" side of the normal on entering the material, as indicated in the diagram, and by a suitably general form of Snell's law.\n\n\n"}
{"id": "34543558", "url": "https://en.wikipedia.org/wiki?curid=34543558", "title": "Nucleus incertus", "text": "Nucleus incertus\n\nThe nucleus incertus is a region of the rodent pontine brainstem just ventral to the 4th ventricle.\n\nThe nucleus incertus has a bilateral structure which sits near the brainstem, and is often also called 'nucleus O'. It consists of mostly ascending GABAergic projection neurons and glutamatergic neurons which innervate a broad range of forebrain regions involved in behavioural activation and the response to stress.\n\nThis tegmental nucleus is part of the theta network. The nucleus incertus is a relay from the reticularis pontis oralis nucleus to the septo-hippocampal system. The stimulation of the nucleus incertus activates the hippocampal theta rhythm and either its lesion or inhibition suppress the theta oscillation induced by brainstem stimulation. The nucleus incertus itself presents theta oscillations coupled to the hippocampal theta rhythm.\n"}
{"id": "910662", "url": "https://en.wikipedia.org/wiki?curid=910662", "title": "Object relations theory", "text": "Object relations theory\n\nObject relations theory in psychoanalytic psychology is the process of developing a psyche in relation to others in the environment during childhood. Based on psychodynamic theory, the object relations theory suggests that the way people relate to others and situations in their adult lives is shaped by family experiences during infancy. For example, an adult who experienced neglect or abuse in infancy would expect similar behavior from others who remind them of the neglectful or abusive parent from their past. These images of people and events turn into \"objects\" in the unconscious that the \"self\" carries into adulthood, and they are used by the unconscious to predict people's behavior in their social relationships and interactions.\n\nThe first \"object\" in someone is usually an internalized image of one's mother. Internal objects are formed by the patterns in one's experience of being taken care of as a baby, which may or may not be accurate representations of the actual, external caretakers. Objects are usually internalized images of one's mother, father, or primary caregiver, although they could also consist of parts of a person such as an infant relating to the breast or things in one's inner world (one's internalized image of others).\nLater experiences can reshape these early patterns, but objects often continue to exert a strong influence throughout life. Objects are initially comprehended in the infant mind by their functions and are termed \"part objects\". The breast that feeds the hungry infant is the \"good breast\", while a hungry infant that finds no breast is in relation to the \"bad breast\". With a \"good enough\" facilitating environment, part object functions eventually transform into a comprehension of whole objects. This corresponds with the ability to tolerate ambiguity, to see that both the \"good\" and the \"bad\" breast are a part of the same mother figure.\n\nThe initial line of thought emerged in 1917 with Ferenczi and, early in the 1930s, Sullivan, coiner of the term \"interpersonal,\". British psychologists Melanie Klein, Donald Winnicott, Harry Guntrip, Scott Stuart, and others extended object relations theory during the 1940s and 1950s. Ronald Fairbairn in 1952 independently formulated his theory of object relations.\n\nWhile Fairbairn popularized the term \"object relations\", Melanie Klein's work tends to be most commonly identified with the terms \"object relations theory\" and \"British object relations\", at least in contemporary North America, though the influence of 'what is known as the \"British independent perspective\", which argued that the primary motivation of the child is object seeking rather than drive gratification', is becoming increasingly recognized. Klein felt that the psychodynamic battleground that Freud proposed occurs very early in life, during infancy. Furthermore, its origins are different from those that Freud proposed. The interactions between infant and mother are so deep and intense that they form the focus of the infant's structure of drives. Some of these interactions provoke anger and frustration; others provoke strong emotions of dependence as the child begins to recognize the mother is more than a breast from which to feed. These reactions threaten to overwhelm the individuality of the infant. The way in which the infant resolves the conflict, Klein believed, is reflected in the adult's personality.\n\nFreud originally identified people in a subject's environment with the term \"object\" to identify people as the object of drives. Fairbairn took a radical departure from Freud by positing that humans were not seeking satisfaction of the drive, but actually seek the satisfaction that comes in being in relation to real others. Klein and Fairbairn were working along similar lines, but unlike Fairbairn, Klein always held that she was not departing from Freudian theory, but simply elaborating early developmental phenomena consistent with Freudian theory.\n\nWithin the London psychoanalytic community, a conflict of loyalties took place between Klein and object relations theory (sometimes referred to as \"id psychology\"),\nand Anna Freud and ego psychology. In America, Anna Freud heavily influenced American psychoanalysis in the 1940s, 1950s, and 1960s. American ego psychology was furthered in the works of Hartmann, Kris, Loewenstein, Rapaport, Erikson, Jacobson, and Mahler. In London, those who refused to choose sides were termed the \"middle school,\" whose members included Michael Balint and D.W. Winnicott. A certain division developed in England between the school of Anna Freud and that of Melanie Klein, which later influenced psychoanalytic politics worldwide. Klein was popularized in South America while A. Freud garnered an American allegiance.\n\nFairbairn revised much of Freud's model of the mind. He identified how people who were abused as children internalize that experience. Fairbairn's \"moral defense\" is the tendency seen in survivors of abuse to take all the bad upon themselves, each believing he is morally bad so his caretaker object can be regarded as good. This is a use of splitting as a defense to maintain an attachment relationship in an unsafe world. Fairbairn introduced a four-year-old girl with a broken arm to a doctor friend of his. He told the little girl that they were going to find her a new mommy. \"Oh no!\" the girl cried. \"I want my real mommy.\" \"You mean the mommy that broke your arm?\" Fairbairn asked. \"I was bad,\" the girl replied. She needed to believe that her love object (mother) was all good, so that she could believe she would one day receive the love and nurturing she needed. If she accepted her mother was bad, then she would be bereft and alone in the world, an intolerable state. She used the Moral Defense to make herself bad, but preserve her mother's goodness.\n\nKlein termed the psychological aspect of instinct unconscious \"phantasy\" (deliberately spelled with 'ph' to distinguish it from the word 'fantasy'). Phantasy is a given of psychic life which moves outward towards the world. These image-potentials are given a priority with the drives and eventually allow the development of more complex states of mental life. Unconscious phantasy in the infant’s emerging mental life is modified by the environment as the infant has contact with reality.\nFrom the moment the infant starts interacting with the outer world, he is engaged in testing his phantasies in a reality setting. I want to suggest that the origin of thought lies in this process of testing phantasy against reality; that is, that thought is not only contrasted with phantasy, but based on it and derived from it.\nThe role of unconscious phantasy is essential in the development of a capacity for thinking. In Bion's terms, the phantasy image is a preconception that will not be a thought until experience combines with a realization in the world of experience. The preconception and realization combine to take form as a concept that can be thought. The classic example of this is the infant’s observed rooting for the nipple in the first hours of life. The instinctual rooting is the preconception. The provision of the nipple provides the realization in the world of experience, and through time, with repeated experience, the preconception and realization combined to create the concept. Mental capacity builds upon previous experience as the environment and infant interact.\n\nThe first bodily experiences begin to build up the first memories, and external realities are progressively woven into the texture of phantasy. Before long, the child's phantasies are able to draw upon plastic images as well as sensations—visual, auditory, kinæsthetic, touch, taste, smell images, etc. And these plastic images and dramatic representations of phantasy are progressively elaborated along with articulated perceptions of the external world.\n\nWith adequate care, the infant is able to tolerate increasing awareness of experience which is underlain by unconscious phantasy and leads to attainment of consecutive developmental achievements, \"the positions\" in Kleinian theory.\n\nAs a specific term, projective identification is introduced by Klein in “Notes on some schizoid mechanisms.”\n\n[Projection] helps the ego to overcome anxiety by ridding it of danger and badness. Introjection of the good object is also used by the ego as a defense against anxiety. . . .The processes of splitting off parts of the self and projecting them into objects are thus of vital importance for normal development as well as for abnormal object-relation. The effect of introjection on object relations is equally important. The introjection of the good object, first of all the mother’s breast, is a precondition for normal development . . . It comes to form a focal point in the ego and makes for cohesiveness of the ego. . . . I suggest for these processes the term ‘projective identification’.\n\nKlein imagined this function as a defense which contributes to the normal development of the infant, including ego structure and the development of object relations. The introjection of the good breast provides a location where one can hide from persecution, an early step in developing a capacity to self-soothe.\n\nOgden identifies four functions that projective identification may serve. As in the traditional Kleinian model, it serves as a defense. Projective identification serves as a mode of communication. It is a form of object relations, and “a pathway for psychological change.” As a form of object relationship, projective identification is a way of relating with others who are not seen as entirely separate from the individual. Instead, this relating takes place “between the stage of the subjective object and that of true object relatedness”.\n\nThe positions of Kleinian theory, underlain by unconscious phantasy, are stages in the normal development of ego and object relationships, each with its own characteristic defenses and organizational structure. The paranoid-schizoid and depressive positions occur in the pre-oedipal, oral phase of development.\n\nIn contrast to Fairbairn and later Guntrip, Klein believed that both good and bad objects are introjected by the infant, the internalization of good object being essential to the development of healthy ego function. Klein conceptualized the depressive position as “the most mature form of psychological organization”, which continues to develop throughout the life span.\n\nThe depressive position occurs during the second quarter of the first year. Prior to that the infant is in the paranoid-schizoid position, which is characterized by persecutory anxieties and the mechanisms of splitting, projection, introjection, and omnipotence—which includes idealizing and denial—to defend against these anxieties. Depressive and paranoid-schizoid modes of experience continue to intermingle throughout the first few years of childhood.\n\nThe paranoid-schizoid position is characterized by part object relationships. Part objects are a function of splitting, which takes place in phantasy. At this developmental stage, experience can only be perceived as all good or all bad. As part objects, it is the function that is identified by the experiencing self, rather than whole and autonomous others. The hungry infant desires the good breast who feeds it. Should that breast appear, it is the good breast. If the breast does not appear, the hungry and now frustrated infant in its distress, has destructive phantasies dominated by oral aggression towards the bad, hallucinated breast.\n\nKlein notes that in splitting the object, the ego is also split. The infant who phantasies destruction of the bad breast is not the same infant that takes in the good breast, at least not until obtaining the depressive position, at which point good and bad can be tolerated simultaneously in the same person and the capacity for remorse and reparation ensue.\n\nThe anxieties of the paranoid schizoid position are of a persecutory nature, fear of the ego’s annihilation. Splitting allows good to stay separate from bad. Projection is an attempt to eject the bad in order to control through omnipotent mastery. Splitting is never fully effective, according to Klein, as the ego tends towards integration.\n\nKlein saw the depressive position as an important developmental milestone that continues to mature throughout the life span. The splitting and part object relations that characterize the earlier phase are succeeded by the capacity to perceive that the other who frustrates is also the one who gratifies. Schizoid defenses are still in evidence, but feelings of guilt, grief, and the desire for reparation gain dominance in the developing mind.\n\nIn the depressive position, the infant is able to experience others as whole, which radically alters object relationships from the earlier phase. “Before the depressive position, a good object is not in any way the same thing as a bad object. It is only in the depressive position that polar qualities can be seen as different aspects of the same object.” Increasing nearness of good and bad brings a corresponding integration of ego.\n\nIn a development which Grotstein terms the \"primal split\", the infant becomes aware of separateness from the mother. This awareness allows guilt to arise in response to the infant’s previous aggressive phantasies when bad was split from good. The mother’s temporary absences allow for continuous restoration of her “as an image of representation” in the infant mind. Symbolic thought may now arise, and can only emerge once access to the depressive position has been obtained. With the awareness of the primal split, a space is created in which the symbol, the symbolized, and the experiencing subject coexist. History, subjectivity, interiority, and empathy all become possible.\n\nThe anxieties characteristic of the depressive position shift from a fear of being destroyed to a fear of destroying others. In fact or phantasy, one now realizes the capacity to harm or drive away a person who one ambivalently loves. The defenses characteristic of the depressive position include the manic defenses, repression and reparation. The manic defenses are the same defenses evidenced in the paranoid-schizoid position, but now mobilized to protect the mind from depressive anxiety. As the depressive position brings about an increasing integration in the ego, earlier defenses change in character, becoming less intense and allow increasing awareness of psychic reality.\n\nIn working through depressive anxiety, projections are withdrawn, allowing the other more autonomy, reality, and a separate existence. The infant, whose destructive phantasies were directed towards the bad mother who frustrated, now begins to realize that bad and good, frustrating and satiating, it is always the same mother. Unconscious guilt for destructive phantasies arises in response to the continuing love and attention provided by caretakers.\n\n[As] fears of losing the loved one become active, a very important step is made in the development. These feelings of guilt and distress now enter as a new element into the emotion of love. They become an inherent part of love, and influence it profoundly both in quality and quantity.\n\nFrom this developmental milestone come a capacity for sympathy, responsibility to and concern for others, and an ability to identify with the subjective experience of people one cares about. With the withdrawal of the destructive projections, repression of the aggressive impulses takes place. The child allows caretakers a more separate existence, which facilitates increasing differentiation of inner and outer reality. Omnipotence is lessened, which corresponds to a decrease in guilt and the fear of loss.\n\nWhen all goes well, the developing child is able to comprehend that external others are autonomous people with their own needs and subjectivity.\n\nPreviously, extended absences of the object (the good breast, the mother) was experienced as persecutory, and, according to the theory of unconscious phantasy, the persecuted infant phantisizes destruction of the bad object. The good object who then arrives is not the object which did not arrive. Likewise, the infant who destroyed the bad object is not the infant who loves the good object.\n\nIn phantasy, the good internal mother can be psychically destroyed by the aggressive impulses. It is crucial that the real parental figures are around to demonstrate the continuity of their love. In this way, the child perceives that what happens to good objects in phantasy does not happen to them in reality. Psychic reality is allowed to evolve as a place separate from the literalness of the physical world.\n\nThrough repeated experience with good enough parenting, the internal image that the child has of external others, that is the child's internal object, is modified by experience and the image transforms, merging experiences of good and bad which becomes more similar to the real object (e.g. the mother, who can be both good and bad). In Freudian terms, the pleasure principle is modified by the reality principle.\n\nMelanie Klein saw this surfacing from the depressive position as a prerequisite for social life. Moreover, she viewed the establishment of an inside and an outside world as the start of interpersonal relationships.\n\nKlein argued that people who never succeed in working through the depressive position in their childhood will, as a result, continue to struggle with this problem in adult life. For example: the cause that a person may maintain suffering from intense guilt feelings over the death of a loved one, may be found in the unworked- through depressive position. The guilt is there because of a lack of differentiation between phantasy and reality. It also functions as a defense mechanism to defend the self against unbearable feelings of sadness and sorrow, and the internal object of the loved one against the unbearable rage of the self, which, it is feared, could destroy the internal object forever.\n\nWilfred Bion articulates the dynamic nature of the positions, a point emphasised by Thomas Ogden, and expanded by John Steiner in terms of '\"The equilibrium between the paranoid-schizoid and the depressive positions\"'. Ogden and James Grotstein have continued to explore early infantile states of mind, and incorporating the work of Donald Meltzer, Ester Bick and others, postulate a position preceding the paranoid-schizoid. Grotstein, following Bion, also hypothesizes a transcendent position which emerges following attainment of the depressive position. This aspect of both Ogden and Grotstein's work remains controversial for many within classical object relations theory.\n\nSigmund Freud developed the concept object relation to describe or emphasize that bodily drives satisfy their need through a medium, an object, on a specific focus. The central thesis in Melanie Klein's object relations theory was that objects play a decisive role in the development of a subject and can be either part-objects or whole-objects, i.e. a single organ (a mother's breast) or a whole person (a mother). Consequently, both a mother or just the mother's breast can be the focus of satisfaction for a drive. Furthermore, according to traditional psychoanalysis, there are at least two types of drives, the libido (mythical counterpart: Eros), and the death drive, mortido (mythical counterpart: Thanatos). Thus, the objects can be receivers of both love and hate, the affective effects of the libido and the death drive.\n\nFairbairn posited six ego positions or inner voices, or 3 pairs:\n\nThe Fairbairnian Object Relations therapist imagines that all interactions between the client and the therapist are occurring in the client's inner object relations world, in one of the three dyads. If the client thinks the therapist is wise and compassionate the therapist sees this as an interaction between the client's Libidinal Ego and Exciting Object. If the client is angry at the therapist for not meeting the client's needs, the therapist might see it as an interaction between the client's Antilibidinal Ego and the Bad Object. The therapist might ask the client if this particular interaction reminds the client of something from childhood.\n\nThe Fairbairnian Object Relations therapist also uses his/her own emotional reactions as therapeutic cues. If the therapist is feeling irritated at the client, or bored, he/she might interpret that as a re-enactment of the Antilibidinal Ego and the Bad Object, with the therapist cast in the role of Bad Object. If the therapist can patiently be an empathic therapist through the client's re-enactment, then the client has a new experience to incorporate into their inner object world, hopefully expanding their inner picture of their Good Object. Cure is seen as the client being able to receive from their inner Good Object often enough to have a more stable peaceful life.\n\nThe Fairbairnian Object Relations Therapist also uses their mistakes in the therapy. If the therapist has absent mindedly made a mistake that hurts the client, the therapist admits the mistake, and empathizes with the client's pain, but instead of apologizing, the therapist asks: How did this mistake in therapy re-enact a childhood scene?\n\nThe Personality Studies Institute, guided by Otto F. Kernberg, in New York City receives referrals from surrounding therapists who have become frustrated with the most severe personality disordered clients. Therapists at the Institute wean their clients off all anti-depressants and anti-anxiety medications, and then work with the clients using Transference Focused Psychotherapy, a form of Object Relations Theory. A typical successful therapy lasts 150 sessions.\n\nNumerous research studies have found that most all models of psychotherapy are equally helpful, the difference mainly being the quality of the individual therapist, not the theory the therapist subscribes to. Object Relations Theory attempts to explain this phenomenon via the theory of the Good Object. If a therapist can be patient and empathic, most clients improve their functioning in their world. The client carries with them a picture of the empathic therapist that helps them cope with the stressors of daily life, regardless of what theory of psychology they subscribe to.\n\nAttachment theory, researched by John Bowlby and others, has continued to deepen our understanding of early object relationships. While a different strain of psychoanalytic theory and research, the findings in attachment studies have continued to support the validity of the developmental progressions described in object relations. Recent decades in developmental psychological research, for example on the onset of a \"theory of mind\" in children, has suggested that the formation of the mental world is enabled by the infant-parent interpersonal interaction which was the main thesis of British object-relations tradition (e.g. Fairbairn, 1952).\n\nWhile object relations theory grew out of psychoanalysis, it has been applied to the general fields of psychiatry and psychotherapy by such authors as N. Gregory Hamilton and Glen O. Gabbard. In making object relations theory more useful as a general psychology N. Gregory Hamilton added the specific ego functions to Otto F. Kernberg's concept of object relations units.\n\nIndividuals:\n\n\n"}
{"id": "10061393", "url": "https://en.wikipedia.org/wiki?curid=10061393", "title": "Peter Wellnhofer", "text": "Peter Wellnhofer\n\nPeter Wellnhofer (born Munich, 1936) is a German paleontologist at the \"Bayerische Staatssammlung fur Paläontologie\" in Munich. He is best known for his work on the various fossil specimens of \"Archaeopteryx\" or \"Urvogel\", the first known bird. Wellnhofer's other work includes \"The Illustrated Encyclopedia of Pterosaurs\".\n\n\"Wellnhoferia\", a bird closely related to \"Archaeopteryx\", or a species of the Urvogel itself, was named in his honor.\n\nIn 2007 a special meeting of pterosaur experts in Munich was dedicated to Wellnhofer, describing him as \"the foremost authority on pterosaurs for the last four decades.\" The meeting produced a festschrift in his honor titled \"Flugsaurier: pterosaur papers in honour of Peter Wellnhofer\".\n"}
{"id": "868221", "url": "https://en.wikipedia.org/wiki?curid=868221", "title": "Richard Bryan", "text": "Richard Bryan\n\nRichard Hudson Bryan (born July 16, 1937) is an American attorney and politician who served as the 25th Governor of the U.S. state of Nevada from 1983–89, and as a United States Senator from Nevada from 1989 to 2001. He is a member of the Democratic Party.\n\nBryan was born in Washington, D.C. and graduated from the University of Nevada at Reno in 1959 where he was a member of Alpha Tau Omega and the president of ASUN. He earned his law degree from the University of California, Hastings College of Law. In 1963 he was admitted to the Nevada Bar.\n\nBryan served as a member of the Nevada Senate from 1972 to 1978. In 1979, Bryan became the Nevada Attorney General, and served in the position until 1983. Bryan was the Governor of Nevada from 1983 to 1989.\n\nAfter that, he represented Nevada in the U.S. Senate from 1989 until 2001. Bryan served on the following Senate Committees: Finance, Banking, Vice Chairman-U.S. Senate Select Committee on Intelligence and Commerce. He chose not to run for re-election in 2000.\n\nBryan was an opponent of SETI and introduced an amendment to the 1994 budget that secured the cancellation of the High Resolution Microwave Survey and terminated NASA's SETI efforts.\n\nNASA criticized Bryan for his opposition to its SETI (Search for Extraterrestrial Intelligence) program, especially because Bryan ignored the meeting requests from NASA staff.\n\nAnother focus was on preventing Yucca Mountain, Nevada, being used as a nuclear waste long-term storage site. Though the repository would be built during Bryan's time in the senate, his opposition, along with numerous others', delayed any actual storage occurring. This opposition would continue long after Bryan had retired before plans for storage were discontinued by President Obama.\n\n"}
{"id": "7702855", "url": "https://en.wikipedia.org/wiki?curid=7702855", "title": "Saturn-Shuttle", "text": "Saturn-Shuttle\n\nThe Saturn-Shuttle was a preliminary concept of launching the Space Shuttle orbiter using the Saturn V rocket. It was studied and considered in 1971-1972.\n\nAn interstage would be fitted on top of the S-IC stage to support the external tank in the space occupied by the S-II stage in the Saturn V. It was an alternative to the SRBs.\n\nThe addition of wings (and some form of landing gear) on the S-IC stage would allow the booster to fly back to the Kennedy Space Center, where technicians would then refurbish the booster (by replacing only the five F-1 engines and reusing the tanks and other hardware for later flights).\n\nThe Shuttle would handle space station logistics, while Saturn V would launch components. \nThis would have allowed the International Space Station, using a Skylab or Mir configuration with both U.S. and Russian docking ports, to have been lifted with just a handful of launches. The Saturn-Shuttle concept also would have eliminated the Space Shuttle Solid Rocket Boosters that ultimately precipitated the Space Shuttle \"Challenger\" accident in 1986.\n\nBecause the shuttle orbiter would be riding piggyback on the external tank, and the need to prevent damage to the delicate thermal protection tiles, the five-engine variant of the Saturn-Shuttle would require the center engine to be shut down 45 to 50 seconds after launch, while two of the outboard engines would have to be shut down prior to staging. Once the S-IC was jettisoned, the three onboard high-energy Space Shuttle main engines would then propel the orbiter into LEO, shutting down 6.5 minutes after ignition. The external tank would then be jettisoned, as on the actually flown shuttle configuration, and the orbiter would then perform its mission.\n\nBut because of the need to keep costs down and to allow President Richard Nixon to approve the shuttle program in 1972, NASA decided to utilize segmented solid rocket boosters similar to those used on the Titan III rocket instead of the S-IC, thus ending the Saturn program after the initial Saturn V order was completed.\n\n\n\n"}
{"id": "40327872", "url": "https://en.wikipedia.org/wiki?curid=40327872", "title": "Scientriffic", "text": "Scientriffic\n\nScientriffic was a bi-monthly kids science magazine published by CSIRO Publishing. It was established in 1999 as a sister publication to \"The Helix\", CSIRO Publishing's magazine for teens. \"Scientriffic\" targeted kids aged 7 and older.\n\nThe magazine was usually 40 pages long and trimmed to quarto paper size. It typically contained feature articles about science and mathematics of interest to kids.\n\nThe magazine was relaunched in July 2015 as \"Double Helix\", combining both \"Scientriffic\" and \"The Helix\" into one magazine, starting from Issue 1, with 8 issues per year.\n\nThe final editor-in-chief was Sarah Kellett. Previous editors included Bianca Nogrady, Heather Catchpole, Cristy Burne, Tanya Patrick and Jasmine Fellows.\n"}
{"id": "59041864", "url": "https://en.wikipedia.org/wiki?curid=59041864", "title": "Singulisphaera rosea", "text": "Singulisphaera rosea\n\nSingulisphaera rosea is a moderately acidophilic, mesophilic, aerobic and non-motile bacterium from the genus of \"Singulisphaera\" which has been isolated from Sphagnum peat from the Tver Region in Russia.\n"}
{"id": "9667001", "url": "https://en.wikipedia.org/wiki?curid=9667001", "title": "Social development theory", "text": "Social development theory\n\nSocial development theory attempts to explain qualitative changes in the structure and framework of society, that help the society to better realize aims and objectives. Development can be defined in a manner applicable to all societies at all historical periods as an upward ascending movement featuring greater levels of energy, efficiency, quality, productivity, complexity, comprehension, creativity, mastery, enjoyment and accomplishment. Development is a process of social change, not merely a set of policies and programs instituted for some specific results. During the last five centuries this process has picked up in speed and intensity, and during the last five decades has witnessed a marked surge in acceleration.\n\nThe basic mechanism driving social change is increasing awareness leading to better organization. When society senses new and better opportunities for progress it develops new forms of organization to exploit these new openings successfully. The new forms of organization are better able to harness the available social energies and skills and resources to use the opportunities to get the intended results.\n\nDevelopment is governed by many factors that influence the results of developmental efforts. There must be a motive that drives the social change and essential preconditions for that change to occur. The motive must be powerful enough to overcome obstructions that impede that change from occurring. Development also requires resources such as capital, technology, and supporting infrastructure.\n\nDevelopment is the result of society's capacity to organize resources to meet challenges and opportunities. Society passes through well-defined stages in the course of its development. They are nomadic hunting and gathering, rural agrarian, urban, commercial, industrial, and post-industrial societies. Pioneers introduce new ideas, practices, and habits that conservative elements initially resist. At a later stage, innovations are accepted, imitated, organized, and used by other members of the community. Organizational improvements introduced to support the innovations can take place simultaneously at four different levels—physical, social, mental, and psychological. Moreover four different types of resources are involved in promoting development. Of these four, physical resources are most visible, but least capable of expansion. Productivity of resources increases enormously as the quality of organization and level of knowledge inputs rise.\n\nDevelopment pace and scope varies according to the stage society is in. The three main stages are physical, vital (\"vital\" refers to the dynamic and nervous social energies of humanity that propel individuals to accomplish), and mental.\n\n\"'Though the term \"development\" usually refers to economic progress, it can apply to political, social, and technological progress as well. These various sectors of society are so intertwined that it is difficult to neatly separate them. Development in all these sectors is governed by the same principles and laws, and therefore the term applies uniformly.\n\nEconomic development and human development need not mean the same thing. Strategies and policies aimed at greater growth may produce greater income in a country without improving the average living standard. This happened in oil-producing Middle Eastern countries—a surge in oil prices boosted their national income without much benefit to poorer citizens. Conversely, people-oriented programs and policies can improve health, education, living standards, and other quality-of-life measures with no special emphasis on monetary growth. This occurred in the 30 years of socialist and communist rule in Kerala in India.\n\nFour related but distinct terms and phenomena form successive steps in a graded series: survival, growth, development, and evolution. Survival refers to a subsistence lifestyle with no marked qualitative changes in living standards. Growth refers to horizontal expansion in the existing plane characterized by quantitative expansion—such as a farmer increasing the area under cultivation, or a retailer opening more stores. Development refers to a vertical shift in the level of operations that causes qualitative changes, such as a retailer turning into a manufacturer or an elementary school turning into a high school.\n\nDevelopment is a human process, in the sense that human beings, not material factors, drive development. The energy and aspiration of people who seek development form the motive force that drives development. People's awareness may decide the direction of development. Their efficiency, productivity, creativity, and organizational capacities determine the level of people’s accomplishment and enjoyment. Development is the outer realization of latent inner potentials. The level of people's education, intensity of their aspiration and energies, quality of their attitudes and values, skills and information all affect the extent and pace of development. These factors come into play whether it is the development of the individual, family, community, nation, or the whole world.\n\nHuman development normally proceeds from experience to comprehension. As society develops over centuries, it accumulates the experience of countless pioneers. The essence of that experience becomes the formula for accomplishment and success. The fact that experience precedes knowledge can be taken to mean that development is an unconscious process that gets carried out first, while knowledge becomes conscious later on only. \"Unconscious\" refers to activities that people carry out without knowing what the end results will be, or where their actions will lead. They carry out the acts without knowing the conditions required for success.\n\nThe gathering conscious knowledge of society matures and breaks out on the surface in the form of new ideas—espoused by pioneers who also take new initiatives to give expression to those ideas. Those initiatives may call for new strategies and new organizations, which conservative elements may resist. If the pioneer's initiatives succeed, it encourages imitation and slow propagation in the rest of the community. Later, growing success leads to society assimilating the new practice, and it becomes regularized and institutionalized. This can be viewed in three distinct phases of social preparedness, initiative of pioneers, and assimilation by the society.\n\nThe pioneer as such plays an important role in the development process—since through that person, unconscious knowledge becomes conscious. The awakening comes to the lone receptive individual first, and that person spreads the awakening to the rest of the society. Though pioneers appear as lone individuals, they act as conscious representatives of society as a whole, and their role should be viewed in that light.\n\nThough a pioneer comes up with innovative ideas very often the initial response to a pioneer is one of indifference, ridicule or even one of outright hostility. If the pioneer persists and succeeds in an initiative, that person's efforts may eventually get the endorsement of the public. That endorsement tempts others to imitate the pioneer. If they also succeed, news spreads and brings wider acceptance. Conscious efforts to lend organizational support to the new initiative helps institutionalize the new innovation.\n\nOrganization is the human capacity to harness all available information, knowledge, resources, technology, infrastructure, and human skills to exploit new opportunities—and the face challenges and hurdles that block progress. Development comes through improvements in the human capacity for organization. In other words, development comes through emergence of better organizations that enhance society's capacity to make use of opportunities and face challenges.\n\nThe development of organizations may come through formulation of new laws and regulations, or through new systems. Each new step of progress brings a corresponding new organization. Increasing European international trade in the 16th and 17th centuries demanded corresponding development in the banking industry, as well as new commercial laws and civil arbitration facilities. New types of business ventures formed to attract the capital needed to finance expanding trade. As a result, a new business entity appeared—the joint-stock company, which limited the investors' liability to the extent of their personal investment without endangering other properties.\n\nEach new developmental advance is accompanied by new or more suitable organizations that facilitate that advance. Often, existing inadequate organizations must change to accommodate new advances.\n\nMany countries have introduced scores of new reforms and procedures—such as the release of business directories, franchising, lease purchase,service, credit rating, collection agencies, industrial estates, free trade zones, and credit cards. Additionally, a diverse range of Internet services have formed. Each new facility improves effective use of available social energies for productive purposes. The importance of these facilities for speeding development is apparent when they are absent. When Eastern European countries wanted to transition to market-type economies, they were seriously hampered in their efforts due to the absence of supportive systems and facilities.organisation is not only development that we are also have oppourtunities for doing new activities in the society by knowing new sourses we can achieve some what what we have seen in that new innovations & developments\n\nAt a particular stage, organizations mature into institutions that become part of society. Beyond this point, an organization does not need laws or agencies to foster growth or ensure a continued presence. The transformation of an organization into an institution signifies society's total acceptance of that new organization.\n\nThe income tax office is an example of an organization that is actively maintained by the enactment of laws and the formation of an office for procuring taxes. Without active governmental support, this organization would disappear, as it does not enjoy universal public support. On the other hand, the institution of marriage is universally accepted, and would persist even if governments withdrew regulations that demand registration of marriage and impose age restrictions. The institution of marriage is sustained by the weight of tradition, not by government agencies and legal enactments.\n\nFamilies play a major role in the propagation of new activities once they win the support of the society. A family is a miniature version of the larger society—acceptance by the larger entity is reflected in the smaller entity. The family educates the younger generation and transmits social values like self-restraint, responsibility, skills, and occupational training. Though children do not follow their parents' footsteps as much as they once did, parents still mold their children's attitudes and thoughts regarding careers and future occupations. When families propagate a new activity, it signals that the new activity has become an integral part of the society.\n\nOne of the most powerful means of propagating and sustaining new developments is the educational system in a society. Education transmits society's collective knowledge from one generation to the next. It equips each new generation to face future opportunities and challenges with knowledge gathered from the past. It shows the young generation the opportunities ahead for them, and thereby raises their aspiration to achieve more. Information imparted by education raises the level of expectations of youth, as well as aspirations for higher income. It also equips youth with the mental capacity to devise ways and means to improve productivity and enhance living standards.\n\nSociety can be conceived as a complex fabric that consists of interrelated activities, systems, and organizations. Development occurs when this complex fabric improves its own organization. That organizational improvement can take place simultaneously in several dimensions.\n\nSuch organizational innovations occur all the time, as a continuous process. New organizations emerge whenever a new developmental stage is reached, and old organizations are modified to suit new developmental requirements. The impact of these new organizations may be powerful enough to make people believe they are powerful in their own right—but it is society that creates the new organizations required to achieve its objectives.\n\nThe direction that the developmental process takes is influenced by the population's awareness of opportunities. Increasing awareness leads to greater aspiration, which releases greater energy that helps bring about greater accomplishment.\n\nSince the time of the English economist Thomas Malthus, some have thought that capacity for development is limited by availability of natural resources. Resources can be divided into four major categories: physical, social, mental, and human. Land, water, mineral and oil, etc. constitute physical resources. Social resources consist of society's capacity to manage and direct complex systems and activities. Knowledge, information and technology are mental resources. The energy, skill and capacities of people constitute human resources.\n\nThe science of economics is much concerned with scarcity of resources. Though physical resources are limited, social, mental, and human resources are not subject to inherent limits. Even if these appear limited, there is no fixity about the limitation, and these resources continue to expand over time. That expansion can be accelerated by the use of appropriate strategies. In recent decades the rate of growth of these three resources has accelerated dramatically.\n\nThe role of physical resources tends to diminish as society moves to higher developmental levels. Correspondingly, the role of non-material resources increases as development advances. One of the most important non-material resources is information, which has become a key input. Information is a non-material resource that is not exhausted by distribution or sharing. Greater access to information helps increase the pace of its development. Ready access to information about economic factors helps investors transfer capital to sectors and areas where it fetches a higher return. Greater input of non-material resources helps explain the rising productivity of societies in spite of a limited physical resource base.\n\nApplication of higher non-material inputs also raises the productivity of physical inputs. Modern technology has helped increase the proven sources of oil by 50% in recent years—and at the same time, reduced the cost of search operations by 75%. Moreover, technology shows it is possible to reduce the amount of physical inputs in a wide range of activities. Scientific agricultural methods demonstrated that soil productivity could be raised through synthetic fertilizers. Dutch farm scientists have demonstrated that a minimal water consumption of 1.4 liters is enough to raise a kilogram of vegetables, compared to the thousand liters that traditional irrigation methods normally require.\n\nHenry Ford's assembly line techniques reduced the man-hours of labor required to deliver a car from 783 minutes to 93 minutes. These examples show that the greater input of higher non-material resources can raise the productivity of physical resources and thereby extend their limits.\n\nWhen the mind engages in pure creative thinking, it comes up with new thoughts and ideas. When it applies itself to society it can come up with new organizations. When it turns to the study of nature, it discovers nature's laws and mechanisms. When it applies itself to technology, it makes new discoveries and practical inventions that boost productivity. Technical creativity has had an erratic course through history, with some intense periods of creative output followed by some dull and inactive periods. However, the period since 1700 has been marked by an intense burst of technological creativity that is multiplying human capacities exponentially.\n\nThough many reasons can be cited for the accelerating pace of technological inventions, a major cause is the role played by mental creativity in an increasing atmosphere of freedom. Political freedom and liberation from religious dogma had a powerful impact on creative thinking during the Age of Enlightenment. Dogmas and superstitions greatly restricted mental creativity. For example, when the astronomer Copernicus proposed a heliocentric view of the world, the church rejected it because it did not conform to established religious doctrine. When Galileo used a telescope to view the planets, the church condemned the device as an instrument of the devil, as it seemed so unusual. The Enlightenment shattered such obscurantist fetters on freedom of thought. From then on, the spirit of experimentation thrived.\n\nThough technological inventions have increased the pace of development, the tendency to view developmental accomplishments as mainly powered by technology misses the bigger picture. Technological innovation was spurred by general advances in the social organization of knowledge. In the Middle Ages, efforts at scientific progress were few, mainly because there was no effective system to preserve and disseminate knowledge. Since there was no organized protection for patent rights, scientists and inventors were secretive about observations and discoveries. Establishment of scientific associations and scientific journals spurred the exchange of knowledge and created a written record for posterity.\n\nTechnological development depends on social organizations. Nobel laureate economist Arthur Lewis observed that the mechanization of factory production in England—the Industrial Revolution—was a direct result of the reorganization of English agriculture. Enclosure of common lands in England generated surplus income for farmers. That extra income generated additional raw materials for industrial processing, and produced greater demand for industrial products that traditional manufacturing processes could not meet.\n\nThe opening of sea trade further boosted demand for industrial production for export. Factory production increased many times when production was reorganized to use steam energy, combined with moving assembly lines, specialization, and division of labor. Thus, technological development was both a result of and a contributing factor to the overall development of society.\n\nIndividual scientific inventions do not spring out of the blue. They build on past accomplishments in an incremental manner, and give a conscious form to the unconscious knowledge that society gathers over time. As pioneers are more conscious than the surrounding community, their inventions normally meet with initial resistance, which recedes over time as their inventions gain wider acceptance. If opposition is stronger than the pioneer, then the introduction of an invention gets delayed.\n\nIn medieval times, when guilds tightly controlled their members, medical progress was slow mainly because physicians were secretive about their remedies. When Denis Papin demonstrated his steam engine, German naval authorities refused to accept it, fearing it would lead to increased unemployment. John Kay, who developed a flying shuttle textile loom, was physically threatened by English weavers who feared the loss of their jobs. He fled to France where his invention was more favorably received.\n\nThe widespread use of computers and application of biotechnology raises similar resistance among the public today. Whether the public receives an invention readily or resists depends on their awareness and willingness to entertain rapid change. Regardless of the response, technological inventions occurs as part of overall social development, not as an isolated field of activity.\n\nThe concept of inherent limits to development arose mainly because past development was determined largely by availability of physical resources. Humanity relied more on muscle-power than thought-power to accomplish work. That is no longer the case. Today, mental resources are the primary determinant of development. Where people drove a simple bullock cart, they now design ships and aircraft that carry huge loads across immense distances. Humanity has tamed rivers, cleared jungles and even turned arid desert lands into cultivable lands through irrigation.\n\nBy using intelligence, society has turned sand into powerful silicon chips that carry huge amounts of information and form the basis of computers. Since there is no inherent limit to the expansion of society's mental resources, the notion of limits to growth cannot be ultimately binding.\n\nSociety's developmental journey is marked by three stages: physical, vital, and mental. These are not clear-cut stages, but overlap. All three are present in any society at time. One of them is predominant while the other two play subordinate roles. The term 'vital' denotes the emotional and nervous energies that empower society's drive towards accomplishment and express most directly in the interactions between human beings. Before the full development of mind, it is these vital energies that predominate in human personality and gradually yield the ground as the mental element becomes stronger. The speed and circumstances of social transition from one stage to another varies.\n\nThe physical stage is characterized by the domination of the physical element of the human personality. During this phase, society is preoccupied with bare survival and subsistence. People follow tradition strictly and there is little innovation and change. Land is the main asset and productive resource during the physical stage and wealth is measured by the size of land holdings. This is the agrarian and feudal phase of society. Inherited wealth and position rule the roost and there is very little upward mobility. Feudal lords and military chiefs function as the leaders of the society. Commerce and money play a relatively minor role. As innovative thinking and experimental approaches are discouraged, people follow tradition unwaveringly and show little inclination to think outside of established guidelines. Occupational skills are passed down from parent to child by a long process of apprenticeship.\n\nGuilds restrict the dissemination of trade secrets and technical knowledge. The Church controls the spread of new knowledge and tries to smother new ideas that does not agree with established dogmas. The physical stage comes to an end when the reorganization of agriculture gives scope for commerce and industry to expand. This happened in Europe during the 18th century when political revolutions abolished feudalism and the Industrial Revolution gave a boost to factory production. The shift to the vital and mental stages helps to break the bonds of tradition and inject new dynamism in social life.\n\nThe vital stage of society is infused with dynamism and change. The vital activities of society expand markedly. Society becomes curious, innovative and adventurous. During the vital stage emphasis shifts from interactions with the physical environment to social interactions between people. Trade supplants agriculture as the principal source of wealth.\n\nThe dawning of this phase in Europe led to exploratory voyages across the seas leading to the discovery of new lands and an expansion of sea trade. Equally important, society at this time began to more effectively harness the power of money. Commerce took over from agriculture, and money replaced land as the most productive resource. The center of life shifted from the countryside to the towns where opportunities for trade and business were in greater abundance.\n\nThe center of power shifted from the aristocracy to the business class, which employed the growing power of money to gain political influence. During the vital stage, the rule of law becomes more formal and binding, providing a secure and safe environment for business to flourish. Banks, shipping companies and joint-stock companies increase in numbers to make use of the opportunities. Fresh innovative thinking leads to new ways of life that people accept as they prove beneficial. Science and experimental approaches begin to make a headway as the hold of tradition and dogma weaken. Demand for education rises.\n\nAs the vital stage matures through the expansion of the commercial and industrial complex, surplus income arises, which prompts people to spend more on items so far considered out of reach. People begin to aspire for luxury and leisure that was not possible when life was at a subsistence level.\n\nThis stage has three essential characteristics: practical, social, and political application of mind. The practical application of mind generates many inventions. The social application of mind leads to new and more effective types of social organization. The political application leads to changes in the political systems that empower the populace to exercise political and human rights in a free and democratic manner. These changes began in the Renaissance and Enlightenment, and gained momentum in the Reformation, which proclaimed the right of individuals to relate directly to God without the mediation of priests. The political application of mind led to the American and French Revolutions, which produced writing that first recognized the rights of the common man and gradually led to the actual enjoyment of these rights.\n\nOrganization is a mental invention. Therefore it is not surprising that the mental stage of development is responsible for the formulation of a great number of organizational innovations. Huge business corporations have emerged that make more money than even the total earnings of some small countries. Global networks for transportation and communication now connect the nations of the world within a common unified social fabric for sea and air travel, telecommunications, weather reporting and information exchange.\n\nIn addition to spurring technological and organizational innovation, the mental phase is also marked by the increasing power of ideas to change social life. Ethical ideals have been with humanity since the dawn of civilization. But their practical application in daily social life had to wait for the mental stage of development to emerge. The proclamation of human rights and the recognition of the value of the individual have become effective only after the development of mind and spread of education. The 20th century truly emerged as the century of the common man. Political, social, economic and many other rights were extended to more and more sections of humanity with each succeeding decade.\n\nThe relative duration of these three stages and the speed of transition from one to another varies from one society to another. However broadly speaking, the essential features of the physical, vital and mental stages of development are strikingly similar and therefore quite recognizable even in societies separated by great distance and having little direct contact with one another.\n\nMoreover, societies also learn from those who have gone through these transitions before and, therefore, may be able to make the transitions faster and better. When the Netherlands introduced primary education in 1618, it was a pioneering initiative. When Japan did the same thing late in the 19th century, it had the advantage of the experience of the USA and other countries. When many Asian countries initiated primary education in the 1950s after winning independence, they could draw on the vast experience of more developed nations. This is a major reason for the quickening pace of progress.\n\nNatural development is distinct from development by government initiatives and planning. Natural development is the spontaneous and unconscious process of development that normally occurs. Planned development is the result of deliberate conscious initiatives by the government to speed development through special programs and policies. Natural development is an unconscious process, since it results from the behavior of countless individuals acting on their own—rather than conscious intention of the community. It is also unconscious in the sense that society achieves the results without being fully conscious of how it did so.\n\nThe natural development of democracy in Europe over the past few centuries can be contrasted with the conscious effort to introduce democratic forms of government in former colonial nations after World War II. Planned development is also largely unconscious: the goals may be conscious, but the most effective means for achieving them may remain poorly understood. Planned development can become fully conscious only when the process of development itself is fully understood. While in planned development the government is the initiator in the natural version it is private individuals or groups that are responsible for the initiative. Whoever initiates, the principles and policies are the same and success is assured only when the conditions and right principles are followed.\n\nThe Green Revolution in India is a good example of a planned development initiative that brings out all the essential features of the development process. Until 1960 agriculture in India did not differ markedly from what it had been during the colonial period beginning 200 years ago. The Green Revolution is usually described as the introduction of hybrid varieties of wheat and rice, but the adoption of hybrids alone is not sufficient to explain the phenomenal achievements of the Green Revolution. Success was made possible by a comprehensive and well-coordinated program involving multiple changes in the way society managed the production of food.\n\nPrior to the Green Revolution, Indian agriculture was largely based on subsistence-level farming, which did not generate sufficient production to meet the country's food requirements. In the past, this led to periodic food shortages and famines, which were managed by huge imports from abroad. The Green Revolution was an attempt to break out of this condition and increase food production to make the country self-sufficient.\n\nThe Indian government realized that it needed to do many things to win the cooperation of Indian farmers to make the green revolution successful. First, the government had to convince farmers to accept the hybrid varieties that would lead to increased yields. Then, they had to assure farmers that increased production would not drive down prices as bumper harvests had done in the past. The government had to ensure a supply of quality seeds, fertilizers, and provide adequate storage space. It also had to train a network of extension agents to train farmers to cultivate their fields with new methods.\n\nThe government accomplished all this by setting up many new organizations. It set up the Food Corporation to buy food grains from surplus production areas and distribute it in areas with shortages. It constituted an Agricultural Pricing Commission to ensure a minimum floor price to farmers so that there was no disincentive for increased production. Seed and fertilizer corporations were formed to ensure supply of good quality seeds and timely supply of fertilizers, etc. Agricultural scientists were motivated to do their work better by the offer of better pay scales and greater infrastructural facilities.\n\nOn top of all this the government established 100,000 demonstration plots across the country to prove to the farmers that the hybrid varieties were indeed more productive.\n\nThe Green Revolution succeeded not only because it was a planned initiative, but also because it was a conscious and well-conceived program. It adopted the right approaches and was alive to the needs and aspirations of the farmers. Therefore it was well received. The planning and awareness exhibited in the project helped create a higher level organization that could harness the enthusiasm and energies of the farmers more effectively.\n\nPlanned development differs from natural development, in the sense that it is a program sponsored by the government to accelerate the development process. The success of a planned initiative depends on its ability to ensure the terms and conditions that help the natural process succeed. Many planned government initiatives fail because they begin without proper understanding of the conditions necessary for their fulfillment. During the 1960s only the Government of India had the resources necessary to launch a massive program of such dimensions. But today, India's private sector is perhaps even better equipped than government to bring about rapid development as illustrated by the dramatic expansion of the country's IT industry.\n\n\n"}
{"id": "22614887", "url": "https://en.wikipedia.org/wiki?curid=22614887", "title": "Surviving Disaster", "text": "Surviving Disaster\n\nSurviving Disaster is a 2006 BBC, Discovery Channel, and ProSieben co-production documentary series about disasters in the 20th century, starring people who survived them. It was produced in association with France 5.\n\nIt is narrated by Bernard Hill.\n"}
{"id": "55800745", "url": "https://en.wikipedia.org/wiki?curid=55800745", "title": "The Science of Desire", "text": "The Science of Desire\n\nThe Science of Desire: The Search for the Gay Gene and the Biology of Behavior is a 1994 book by the geneticist Dean Hamer and the journalist Peter Copeland, in which the authors discuss Hamer's research into the genetics of homosexuality. The book received both positive and mixed reviews. It was praised as a well-written discussion of science that properly acknowledged the limitations of genetic research on homosexuality, usefully explored its ethical implications, and drew on a wide range of sources and publications. However, reviewers were unconvinced by Hamer's suggestions about the possible evolutionary basis of homosexuality, and argued that some of his claims were incorrect, that more work needed to be done to confirm or refute his genetic findings, and that his use of the term \"gay gene\" was misleading. Hamer's treatment of psychology and psychiatry was also criticized as biased.\n\nHamer argues that human sexuality can and should be studied scientifically and discusses his findings about Xq28, a region of the X chromosome. According to Hamer, he shifted his area of research from metallothionein to the genetics of homosexuality after reading the naturalist Charles Darwin's \"The Descent of Man, and Selection in Relation to Sex\" (1871) and the evolutionary geneticist Richard Lewontin, the neurobiologist Steven Rose and the psychologist Leon Kamin's \"Not in Our Genes\" (1984). Hamer writes that Darwin's book surprised him because of the amount of space it devotes to discussing sexual selection. He describes Lewontin \"et al.\"′s work as \"a political rather than a scientific book\" and expresses his disagreement with its politics. However, he comments that it taught him that the genetics of behavior is an emotionally and politically charged topic, especially where it concerns sexuality. He criticizes Alfred Kinsey, as well as Sigmund Freud and his views on homosexuality.\n\n\"The Science of Desire\" was first published in 1994 by Simon & Schuster.\n\n\"The Science of Desire\" received positive reviews from Genevieve Stuttaford in \"Publishers Weekly\", Constance Rinaldo in \"Library Journal\", the science journalist Natalie Angier in \"The New York Times Book Review\", Martin Johnson in \"New Scientist\", the science writer Jonathan Weiner in \"The New Republic\", and the biologist Paul R. Gross in \"National Review\", and mixed reviews from W. Lener in \"Choice\" and Richard Horton in \"The New York Review of Books\". The book was also reviewed by Deborah Franklin in \"The Washington Post\" and the historian of science Daniel Kevles in \"The New Yorker\".\n\nStuttaford described the book as \"admirably lucid and surprisingly lively\". She credited Hamer with carefully pointing out the limitations of genetic research on sexual orientation, simplifying complex ideas for general readers, and exploring the ethical implications of a \"gay gene\" with \"laudable compassion and common sense.\" Rinaldo described the book as an engaging and personalized account of scientific investigation, and recommended it for both lay readers and specialists. Angier described the book as \"a surprising delight to read\", writing that Hamer did not overstate his case for the innateness of sexual orientation and \"has the welcome habit of sticking to the facts.\" However, she wrote that the last sections, in which Hamer discussed \"still-floundering searches for the genes behind alcoholism, manic depression, shyness, aggression and the like\" were less satisfactory.\n\nJohnson believed that Hamer's work made clear the difficulties involved in studying sexuality. He considered its title misleading, as no \"gay gene\" has been \"proven or identified\", and criticized Hamer for confusing \"sexuality and sex\" and for implying that \"AIDS (rather than HIV) is sexually transmitted\". He also questioned why Hamer needed a co-author, and why a scientist as busy as Hamer claimed to be would find writing a book \"describing the background to a single, as yet unconfirmed, unextended and far from concluded, study\" a high priority. However, the book convinced him that \"the more cynical answers to this question\" were mistaken. He found the book well-written, writing that it \"conveys something of the serendipity of scientific advance\", gave insightful descriptions of scientists, and outlined \"the problems of determining whether there are genetic bases for complex behaviours\" and suggested a way of overcoming them that made clear that this methodology relies on \"redefining a complex characteristic in very limited terms\". He concluded that it was \"probably the best attempt at a genetic analysis of human sexuality that we have so far, which indicates just how far we have to go\" and predicted that it would \"fuel the moral debate about what any genetic component to sexuality might mean for ethics and the law.\" He wrote that Hamer should be \"congratulated and encouraged\" for contributing to the debate.\n\nWeiner described the book as a calm and modestly written account of Hamer's research. He credited Hamer with avoiding over-stating the importance of his work or sensationalizing his results, writing that he had produced a \"clear and capable book about a difficult subject.\" Gross wrote that the book deserved to be widely read. Lener wrote that parts of the book were \"diary-like\", and gave Hamer's \"descriptions of methods used\" and \"life histories of gay men who participated in the study\" as examples. He also wrote that some of Hamer's statements, such as that nobody has studied the number of gay men in the United States since Kinsey's research, were incorrect, but nevertheless considered the book generally \"very accurate\" and credited Hamer with drawing on \"recent, important publications.\" He concluded that, \"General readers as well as professionals and practitioners in sex education/therapy will find much of interest and value.\"\n\nHorton described the book as a \"popular account\" of Hamer's research and credited Hamer, along with other researchers, with helping to make a \"forceful but by no means definitive case for the view that biological and genetic influences have an important--perhaps even decisive--part in determining sexual preference among males.\" Though noting that Hamer acknowledged the limitations of his research, he criticized Hamer for having an unsubtle view of the meaning of \"biologial influence\" on sexual orientation that ignores the question of how genes produce an \"unpredictable interplay of behavioral impulses\", and engaging in \"overstretched speculations\" about \"why a gene for homosexuality still exists when it apparently has little apparent survival value in evolutionary terms.\" He concluded that while Hamer's work \"presents technical and conceptual difficulties\" and his \"preliminary findings obviously need replication or refutation\" it \"represents a genuine epistemological break away from the past's rigid and withered conceptions of sexual preference.\"\n\n\"The Science of Desire\" was reviewed by the journalist Steven Petrow in \"The Advocate\". The book also received reviews from \"New York Native\" and \"Lambda Book Report\".\n\n\"The Science of Desire\" received a positive review from the psychiatrist Susan Bradley in \"The New England Journal of Medicine\" and a mixed review from the psychologist John C. Gonsiorek in the \"Journal of Sex Research\". The book was also reviewed by Nancy Ordover in \"Socialist Review\" and discussed in \"The Lancet\".\n\nBradley described the book as a well-written account of the process of science. She commented that more work needed to be done to replicate Hamer's research and identify the relevant gene, but credited Hamer with being \"cautious in extrapolating his findings to larger issues\" and acknowledging the limitations of the research. However, she wrote that despite his caution, Hamer used the term \"gay gene\" in a misleading fashion, since \"the term implies more than Hamer's work establishes.\" She also commented that, \"the least well thought out and presented part of this book is the section on psychological theories\", which she considered understandable given that Hamer is \"not trained in psychology or psychiatry\". In her view, \"even its subtitle, “Sissies, Freud, and Sex Acts,” suggests a hasty dismissal of the importance of psychological theories.\" She concluded that the book, \"makes enjoyable reading, particularly for anyone interested in the making of science as it relates to human behavior or sexuality.\"\n\nGonsiorek considered the book a competent and engaging discussion of Hamer's scientific work, but one that showed the \"tension and unease\" inherent in popularizing science. He questioned Hamer's description of how he shifted \"from an obscure to a high-profile area of genetics\", writing that it strained credibility. He called Hamer's discussion of the development of his research protocol \"gossipy\" and wrote that it contained \"many tidbits, some less than kind, and many less than relevant\" about the people involved. He praised Hamer's \"ability to synthesize information from diverse sources and apply it creatively\", his discussions of possible biological mechanisms for the heritability of male homosexuality and the public policy implications of his scientific research, and his criticism of social constructionism. However, he found Hamer's attempt to synthesize his ideas with \"psychological concepts\" unsuccessful and his evolutionary ideas intriguing but insufficiently developed. He wrote that Hamer tried to connect his ideas to \"issues from handedness to Alzheimer's, usually coming off sounding like he has not sufficiently mastered the specific areas\", and failed in his discussion of the \"behavioral and social science literature\", for example by mistakenly identifying social constructionism as a form of behaviorism. He also considered the book as a whole \"marred\" by Hamer's persistent hostility towards psychiatry.\n\n\"The Lancet\" endorsed Hamer's proposal for \"the creation of a US centre--a National Institute for Sexual Health--to coordinate federal funding for research into sexuality\", agreeing with Hamer that \"the place of sexuality in human life\" is \"sufficiently important to merit specific and rigorous scientific study.\"\n\nThe philosopher Timothy F. Murphy noted in \"Gay Science\" (1997) that while Hamer's research, if valid, establishes \"evidence that sexual orientation in some people is subject to genetic influences\", the nature of that influence remains to be specified and there is \"still conflict about the meaning of Hamer's results.\" He noted that Hamer's research, like similar studies, has been criticized on the grounds of its small sample size, as well as on other grounds specific to it, and that other researchers have reported difficulty in replicating Hamer's findings. He concluded that much more work had to be done before it could be accepted that genes at Xq28 determine the sexual orientation of some males, despite Hamer's conducting a follow-up study. Murphy argued that while Hamer believed that the study of animal sexual behavior will help make society more tolerant of homosexuality, it was uncertain that it would and if it did, this \"should not be because descriptions of the behavior of animals have normative force in regard to the behavior of human beings.\"\n\nThe gay activist Dennis Altman described Hamer's suggestion that homosexuality has a genetic basis as dubious in \"The End of the Homosexual?\" (2013). He noted that it conflicted with \"the theories and discoveries of both Freud and Kinsey\".\n\n\n\n\n"}
{"id": "6330972", "url": "https://en.wikipedia.org/wiki?curid=6330972", "title": "Therapeutic gene modulation", "text": "Therapeutic gene modulation\n\nTherapeutic gene modulation refers to the practice of altering the expression of a gene at one of various stages, with a view to alleviate some form of ailment. It differs from gene therapy in that gene modulation seeks to alter the expression of an endogenous gene (perhaps through the introduction of a gene encoding a novel modulatory protein) whereas gene therapy concerns the introduction of a gene whose product aids the recipient directly.\n\nModulation of gene expression can be mediated at the level of transcription by DNA-binding agents (which may be artificial transcription factors), small molecules, or synthetic oligonucleotides. It may also be mediated post-transcriptionally through RNA interference.\n\nAn approach to therapeutic modulation utilizes agents that modulate endogenous transcription by specifically targeting those genes at the gDNA level. The advantage to this approach over modulation at the mRNA or protein level is that every cell contains only a single gDNA copy. Thus the target copy number is significantly lower allowing the drugs to theoretically be administered at much lower doses.\n\nThis approach also offers several advantages over traditional gene therapy. Directly targeting endogenous transcription should yield correct relative expression of splice variants. In contrast, traditional gene therapy typically introduces a gene which can express only one transcript, rather than a set of stoichiometrically-expressed spliced transcript variants. Additionally, virally-introduced genes can be targeted for gene silencing by methylation which can counteract the effect of traditional gene therapy. This is not anticipated to be a problem for transcriptional modulation as it acts on endogenous DNA.\n\nThere are three major categories of agents that act as transcriptional gene modulators: triplex-forming oligonucleotides (TFOs), synthetic polyamides (SPAs), and DNA binding proteins.\n\nTriplex-forming oligonucleotides (TFO) are one potential method to achieve therapeutic gene modulation. TFOs are approximately 10-40 base pairs long and can bind in the major groove in duplex DNA which creates a third strand or a triple helix. The binding occurs at polypurine or polypyrimidine regions via Hoogsteen hydrogen bonds to the purine (A / G) bases on the double stranded DNA that is already in the form of the Watson-Crick helix.\n\nTFOs can be either polypurine or polypyrimidine molecules and bind to one of the two strands in the double helix in either parallel or antiparallel orientation to target polypurine or polypyrimidine regions. Since the DNA-recognition codes are different for the parallel and the anti-parallel fashion of TFO binding, TFOs composed of pyrimidines (C / T) bind to the purine-rich strand of the target double helix via Hoogsteen hydrogen bonds in a parallel fashion. TFOs composed of purines (A / G), or mixed purine and pyrimidine bind to the same purine-rich strand via reverse Hoogsteen bonds in an anti-parallel fashion. TFO's can recognize purine-rich target strands for duplex DNA.\nIn order for TFO motifs to bind in a parallel fashion and create hydrogen bonds, the nitrogen atom at position 3 on the cytosine residue needs to be protonated, but at physiological pH levels it is not, which could prevent parallel binding.\n\nAnother limitation is that TFOs can only bind to purine-rich target strands and this would limit the choice of endogenous gene target sites to polypurine-polypyrimidine stretches in duplex DNA. If a method to also allow TFOs to bind to pyrimidine bases was generated, this would enable TFOs to target any part of the genome. Also the human genome is rich in polypurine and polypyrimidine sequences which could affect the specificity of TFO to bind to a target DNA region. An approach to overcome this limitation is to develop TFOs with modified nucleotides that act as locked nucleic acids to increase the affinity of the TFO for specific target sequences.\n\nOther limitations include concerns regarding binding affinity and specificity, in vivo stability, and uptake into cells. Researchers are attempting to overcome these limitations by improving TFO characteristics through chemical modifications, such as modifying the TFO backbone to reduce electrostatic repulsions between the TFO and the DNA duplex. Also due to their high molecular weight, uptake into cells is limited and some strategies to overcome this include DNA condensing agents, coupling of the TFO to hydrophobic residues like cholesterol, or cell permeabilization agents.\n\nScientists are still refining the technology to turn TFOs into a therapeutic product and much of this revolves around their potential applications in antigene therapy. In particular they have been used as inducers of site-specific mutations, reagents that selectively and specifically cleave target DNA, and as modulators of gene expression. One such gene sequence modification method is through the targeting DNA with TFOs to active a target gene. If a target sequence is located between two inactive copies of a gene, DNA ligands, such as TFOs, can bind to the target site and would be recognized as DNA lesions. To fix these lesions, DNA repair complexes are assembled on the targeted sequence, the DNA is repaired. Damage of the intramolecular recombination substrate can then be repaired and detected if resection goes far enough to produce compatible ends on both sides of the cleavage site and then 3' overhangs are ligated leading to the formation of a single active copy of the gene and the loss of all the sequences between the two copies of the gene.\n\nIn model systems TFOs can inhibit gene expression at the DNA level as well as induce targeted mutagenesis in the model. TFO-induced inhibition of transcription elongation on endogenous targets have been tested on cell cultures with success. However, despite much in vitro success, there has been limited achievement in cellular applications potentially due to target accessibility.\n\nTFOs have the potential to silence silence gene by targeting transcription initiation or elongation, arresting at the triplex binding sites, or introducing permanent changes in a target sequence via stimulating a cell's inherent repair pathways. These applications can be relevant in creating cancer therapies that inhibit gene expression at the DNA level. Since aberrant gene expression is a hallmark of cancer, modulating these endogenous genes' expression levels could potentially act as a therapy for multiple cancer types.\n\nSynthetic polyamides are a set of small molecules that form specific hydrogen bonds to the minor groove of DNA. They can exert an effect either directly, by binding a regulatory region or transcribed region of a gene to modify transcription, or indirectly, by designed conjugation with another agent that makes alterations around the DNA target site.\n\nSpecific bases in the minor groove of DNA can be recognized and bound by small synthetic polyamides (SPAs). DNA-binding SPAs have been engineered to contain three polyamide amino acid components: hydroxypyrrole (Hp), imidazole (Im), and pyrrole (Py). Chains of these amino acids loop back on themselves in a hairpin structure. The amino acids on either side of the hairpin form a pair which can specifically recognize both sides of a Watson-Crick base pair. This occurs through hydrogen bonding within the minor groove of DNA. The amide pairs Py/Im, Py/Hp, Hp/Py, and Im/Py recognize the Watson-Crick base pairs C-G, A-T, T-A, and G-C, respectively (Table 1). See figure for a graphical representation of 5'-GTAC-3' recognition by a SPA. SPAs have low toxicity, but have not yet been used in human gene modulation.\n\nThe major structural drawback to unmodified SPAs as gene modulators is that their recognition sequence cannot be extended beyond 5 Watson-Crick base pairings. The natural curvature of the DNA minor groove is too tight a turn for the hairpin structure to match. There are several groups with proposed workarounds to this problem. SPAs can be made to better follow the curvature of the minor groove by inserting beta-alanine which relaxes the structure. Another approach to extending the recognition length is to use several short hairpins in succession. This approach has increased the recognition length to up to eleven Watson-Crick base pairs.\n\nSPAs may inhibit transcription through binding within a transcribed region of a target gene. This inhibition occurs through blocking of elongation by an RNA polymerase.\nSPAs may also modulate transcription by targeting a transcription regulator binding site. If the regulator is an activator of transcription, this will decrease transcriptional levels. As an example, SPA targeting to the binding site for the activating transcription factor TFIIIA has been demonstrated to inhibit transcription of the downstream 5S RNA. In contrast, if the regulator is a repressor, this will increase transcriptional levels. As an example, SPA targeting to the host factor LSF, which represses expression of the human immunodeficiency virus (HIV) type 1 long terminal repeat (LTR), blocks binding of LSF and consequently de-represses expression of LTR\n\nSPAs have not been shown to directly modify DNA or have activity other than direct blocking of other factors or processes. However, modifying agents can be bound to the tail ends of the hairpin structure. The specific binding of the SPA to DNA allows for site-specific targeting of the conjugated modifying agent.\nSPAs have been paired with the DNA-alkylating moieties cyclopropylpyrroloindole and chlorambucil that were able to damage and crosslink SV40 DNA. This effect inhibited cell cycling and growth. Chlorambucil, a chemotherapeutic agent, was more effective when conjugated to an SPA than without.\n\nIn 2012, SPAs were conjugated to SAHA, a potent histone deacetylase (HDAC) inhibitor. SPAs with conjugated SAHA were targeted to Oct-3/4 and Nanog which induced epigenetic remodeling and consequently increased expression of multiple pluripotency related genes in mouse embryonic fibroblasts.\n\nDesigner zinc-finger proteins are engineered proteins used to target specific areas of DNA. These proteins capitalize on the DNA-binding capacity of natural zinc-finger domains to modulate specific target areas of the genome. In both designer and natural zinc-finger motifs, the protein consists of two β-sheets and one α-helix. Two histidine residues on the α-helix and two cysteine residues on the β-sheets are bonded to a zinc atom, which serves to stabilize the protein domain as a whole. This stabilization particularly benefits the α-helix in its function as the DNA-recognition and -binding domain. Transcription factor TFIIIA is an example of a naturally-occurring protein with zinc-finger motifs.\n\nZinc-finger motifs bind into the major groove of helical DNA, where the amino acid residue sequence on the α-helix gives the motif its target sequence specificity. The domain binds to a seven-nucleotide sequence of DNA (positions 1 through 6 on the primary strand of DNA, plus positions 0 and 3 on the complementary strand), thereby ensuring that the protein motif is highly selective of its target. In engineering a designer zinc-finger protein, researchers can utilize techniques such as site-directed mutagenesis followed by randomized trials for binding capacity, or the in vitro recombination of motifs with known target specificity to produce a library of sequence-specific final proteins.\n\nDesigner zinc-finger proteins can modulate genome expression in a number of ways. Ultimately, two factors are primarily responsible for the end result on expression: whether the targeted sequence is a regulatory region or a coding region of DNA, and whether and what types of effector domains are bound to the zinc-finger domain. If the target sequence for an engineered designer protein is a regulatory domain - e.g., a promoter or a repressor of replication - the binding site for naturally-occurring transcription factors will be obscured, leading to a corresponding decrease or increase, respectively, in transcription for the associated gene. Similarly, if the target sequence is an exon, the designer zinc-finger will obscure the sequence from RNA polymerase transcription complexes, resulting in a truncated or otherwise nonfunctional gene product.\n\nEffector domains bound to the zinc-finger can also have comparable effects. It is the function of these effector domains which are arguably the most important with respect to the use of designer zinc-finger proteins for therapeutic gene modulation. If a methylase domain is bound to the designer zinc-finger protein, when the zinc-finger protein binds to the target DNA sequence an increase in methylation state of DNA in that region will subsequently result. Transcription rates of genes so-affected will be reduced. Many of the effector domains function to modulate either the DNA directly - e.g. via methylation, cleaving, or recombination of the target DNA sequence - or by modulating its transcription rate - e.g. inhibiting transcription via repressor domains that block transcriptional machinery, promoting transcription with activation domains that recruit transcriptional machinery to the site, or histone- or other epigenetic-modification domains that affect chromatin state and the ability of transcriptional machinery to access the affected genes. Epigenetic modification is a major theme in determining varying expression levels for genes, as explained by the idea that how tightly-wound the DNA strand is - from histones at the local level up to chromatin at the chromosomal level - can influence the accessibility of sequences of DNA to transcription machinery, thereby influencing the rate at which it can be transcribed. If, instead of impacting the DNA strand directly, as described above, a designer zinc-finger protein instead affects epigenetic modification state for a target DNA region, modulation of gene expression could similarly be accomplished.\nIn the first case to successfully demonstrate the use of designer zinc-finger proteins to modulate gene expression in vivo, Choo \"et al\" designed a protein consisting of three zinc-finger domains that targeted a specific sequence on a BCR-ABL fusion oncogene. This specific oncogene is implicated in acute lymphoblastic leukemia. The oncogene typically enables leukemia cells to proliferate in the absence of specific growth factors, a hallmark of cancer. By including a nuclear localization signal with the tri-domain zinc-finger protein in order to facilitate binding of the protein to genomic DNA in the nucleus, Choo \"et al\" were able to demonstrate that their engineered protein could block transcription of the oncogene in vivo. Leukemia cells became dependent on regular growth factors, bringing the cell cycle back under the control of normal regulation.\n\nThe major approach to post-transcriptional gene modulation is via RNA interference (RNAi). The primary problem with using RNAi in gene modulation is drug delivery to target cells. RNAi gene modulation has been successfully applied to mice toward the treatment of a mouse model for inflammatory bowel disease. This treatment utilized liposome-based beta-7 integrin-targeted, stabilized nanoparticles entrapping short interfering RNAs (siRNAs). There are several other forms of RNAi delivery, including: polyplex delivery, ligand-siRNA conjugates, naked delivery, inorganic particle deliver using gold nanoparticles, and site specific local delivery.\n\nDesigner zinc-finger proteins, on the other hand, have undergone some trials in the clinical arena. The efficacy and safety of EW-A-401, an engineered zinc-finger transcription factor, as a pharmacologic agent for treating claudication, a cardiovascular ailment, has been investigated in clinical trials. The protein consists of an engineered plasmid DNA that prompts the patient to produce an engineered transcription factor, the target of which is the vascular endothelial growth factor-A (VEGF-A) gene, which positively influences blood vessel development. Although not yet approved by the U.S. Food and Drug Administration (FDA), two Phase I clinical studies have been completed which identify this zinc-finger protein as a promising and safe potential therapeutic agent for treatment of peripheral arterial disease in humans.\n\n"}
{"id": "454078", "url": "https://en.wikipedia.org/wiki?curid=454078", "title": "Tornado records", "text": "Tornado records\n\nThis article lists various tornado records. The most \"extreme\" tornado in recorded history was the Tri-State Tornado, which spread through parts of Missouri, Illinois, and Indiana on March 18, 1925. It is considered an F5, even though tornadoes were not ranked on any scale at the time. It holds records for longest path length at , longest duration at about 3½ hours, and fastest forward speed for a significant tornado at anywhere on Earth. In addition, it is the deadliest single tornado in United States history with 695 fatalities. It was also the third-costliest tornado in history at the time, but has been surpassed by several others non-normalized. When costs are normalized for wealth and inflation, it still ranks third today.\n\nThe deadliest tornado in world history was the Daulatpur–Saturia tornado in Bangladesh on April 26, 1989, which killed approximately 1,300 people. In its history, Bangladesh has had at least 19 tornadoes kill more than 100 people, almost half of the total for the rest of the world.\n\nFor 37 years, the most extensive tornado outbreak on record, in almost every category, was the 1974 Super Outbreak, which affected a large area of the central United States and extreme southern Ontario in Canada on April 3 and April 4, 1974. Not only did this outbreak feature 148 tornadoes in only 18 hours, but an unprecedented number of them were violent; 7 were of F5 intensity and 23 were F4. During the peak of this outbreak there were 16 tornadoes on the ground at the same time. More than 300 people, possibly as many as 330, were killed by tornadoes during this outbreak. However, this record was later broken during the 2011 Super Outbreak, which resulted in 360 tornadoes and 324 tornadic fatalities.\n\nThe 2011 Super Outbreak was the most prolific tornado outbreak in USA history. It produced 360 tornadoes, with 216 of those in a single 24-hour period on April 27, including 11 EF4 and 4 EF5 tornadoes. 348 deaths occurred in that outbreak, of which 324 were tornado related. The outbreak helped smash the record for most tornadoes in the month of April with 770 tornadoes, almost triple the prior record (267 in April 1974). The overall record for a single month was 542 in May 2003, which was also broken.\n\nThe infamous 1974 Super Outbreak of April 3–4, 1974, which spawned 148 confirmed tornadoes across eastern North America, held the record for the most prolific tornado outbreak for many years. Not only did it produce an exceptional number of tornadoes, but it was also an inordinately intense outbreak producing dozens of large, long-track tornadoes, including 7 F5 and 23 F4 tornadoes. More significant tornadoes occurred within 24 hours than any other week in the tornado record. Due to a secular trend in tornado reporting, the 2011 and 1974 tornado counts are not directly comparable.\n\nMost tornado outbreaks in North America occur in the spring, but there is a secondary peak of tornado activity in the fall which is less consistent but can include exceptionally large and/or intense outbreaks. In 1992, an estimated 95 tornadoes broke out in a record 41 hours of continuous tornado activity from November 21 to 23. This is also among the largest-known outbreaks in areal expanse. Many other very large outbreaks have occurred in autumn, especially in October and November.\n\nThe greatest number of tornadoes spawned from a hurricane is 118 from Hurricane Ivan in 2004. Caution is advised comparing the raw number of counted tornadoes from recent decades to decades prior to the 1990s since more tornadoes that occur are now recorded than in the past.\n\nOn April 26, 1989 in Bangladesh a large tornado took at least 1,300 lives.\n\nThe Tri-State Tornado of March 18, 1925 killed 695 people in Missouri (11), Illinois (613), and Indiana (71). The outbreak it occurred with was also the deadliest known tornado outbreak, with a combined death toll of 747 across the Mississippi River Valley.\n\nSimilar to fatalities, damage (and observations) of a tornado are a coincidence of what character of tornado interacts with certain characteristics of built up areas. That is, destructive tornadoes are in a sense \"accidents\" of a large tornado striking a large population. In addition to population and changes thereof, comparing damage historically is subject to changes in wealth and inflation. The 1896 St. Louis–East St. Louis tornado on May 27, incurred the most damages adjusted for wealth and inflation, at an estimated $2.9 billion (1997 USD). In raw numbers, the Joplin tornado of May 22, 2011 is considered the costliest tornado in recent history, with damage totals near $2.8 billion (2011 USD). Until 2011, the \"Bridge Creek-Moore tornado\" of May 3, 1999 was the most damaging.\n\nDuring the F5 1999 Bridge Creek–Moore tornado on May 3, 1999 in the southern Oklahoma City metro area, a Doppler on Wheels situated near the tornado measured winds of momentarily in a small area inside the funnel approximately above ground level. These are also the highest wind speeds observed on Earth.\n\nOn May 31, 2013, a tornado hit rural areas near El Reno, Oklahoma. The tornado was originally rated as an EF3 based on damage; however, after mobile radar data analysis was conducted, it was concluded to have been an EF5 due to a measured wind speed of greater than , second only to the Bridge Creek - Moore tornado. Revised RaXPol analysis found winds of well above ground level and ≥ below with some subvortices moving at . These winds may possibly be as high or higher than the winds recorded on May 3, 1999. Despite the recorded windspeed, the El Reno tornado was later downgraded back to EF3 due to the fact that no EF5 damage was found, likely due to the lack of sufficient damage indicators in the largely rural area west of Oklahoma City.\n\nWinds were measured at using portable Doppler radar in the Red Rock, Oklahoma tornado during the April 26, 1991 tornado outbreak in north-central Oklahoma. Though these winds are possibly indicative of an F5 strength tornado, this particular tornado's path never encountered any significant structures and caused minimal damage. Thus it was rated an F4.\n\nThe longest-known track for a single tornado is the Tri-State Tornado with a path length of . For years there was debate whether the originally recognized path length of over 3.5 hours was from one tornado or a series. Some very long track (VLT) tornadoes were later determined to be successive tornadoes spawned by the same supercell thunderstorm, which are known as a tornado family. The Tri-State Tornado, however, appeared to have no gaps in the damage. A six-year reanalysis study by a team of severe convective storm meteorologists found insufficient evidence to make firm conclusions but does conclude that it is likely that the beginning and ending of the path was resultant of separate tornadoes comprising a tornado family. It also found that the tornado began to the west and ended farther east than previously known, bringing the total path to . The segment from central Madison County, Missouri to Pike County, Indiana is likely one continuous tornado and the segment from central Bollinger County, Missouri to western Pike County, Indiana is very likely a single continuous tornado. Another significant tornado was found about east-northeast of the end of aforementioned segment(s) of the Tri-State Tornado Family and is likely another member of the family. Its path length of over about 20 minutes makes the known tornado family path length total to over about 5½ hours. Grazulis in 2001 wrote that the first of the (originally recognized) track is probably the result of two or more tornadoes and that a path length of was seemingly continuous.\n\nWhat at one time was thought to be the record holder for the longest tornado path is now thought to be the longest tornado family, with a track of at least on May 26, 1917 from the Missouri border across Illinois into Indiana. It caused severe damage and mass casualties in Charleston and Mattoon, Illinois.\n\nWhat was probably the longest track supercell thunderstorm tracked across 6 states in 17.5 hours on March 12, 2006 as part of the March 2006 tornado outbreak sequence. It began in Noble County, Oklahoma and ended in Jackson County, Michigan, producing many tornadoes in Missouri and Illinois.\n\nOfficially, the widest tornado on record is the El Reno, Oklahoma tornado of May 31, 2013 with a width of at its peak. This is the width found by the National Weather Service based on preliminary data from University of Oklahoma RaxPol mobile radar that also sampled winds of which was used to upgrade the tornado to EF5. However, it was revealed that these winds did not impact any structures, and as a result the tornado was downgraded to EF3 based on damage. However, another possible contender for the widest tornado as measured by radar was the F4 Mulhall tornado in north-central Oklahoma which occurred during the 1999 Oklahoma tornado outbreak. The diameter of the maximum winds (over ) was over as measured by a DOW radar. Although the tornado passed largely over rural terrain, the width of the wind swath capable of producing damage was as wide as .\n\nThe F4 Hallam, Nebraska tornado during the outbreak of May 22, 2004 was the previous official record holder for the widest tornado, surveyed at wide. A similar size tornado struck Edmonson, Texas on May 31, 1968, when a damage path width between was recorded from an F3 tornado.\n\nThe highest forward speed of a tornado on record was 73 miles per hour (117 km/h) from the 1925 Tri-State Tornado (other weak tornadoes have approached or exceeded this speed, but this is the fastest forward movement observed in a major tornado).\n\nA pressure deficit of was observed when a violent tornado near Manchester, South Dakota on June 24, 2003 passed directly over an in-situ probe that storm chasing researcher Tim Samaras deployed. In less than a minute, the pressure dropped to , which are the greatest pressure decline and the lowest pressure ever recorded at the Earth's surface when adjusted to sea level.\n\nOn April 21, 2007, a pressure deficit was reported when a tornado struck a storm chasing vehicle in Tulia, Texas. The tornado caused EF2 damage as it passed through Tulia. The reported pressure drop far exceeds that which would be expected based on theoretical calculations.\n\nThere is a questionable and unofficial citizen's barometer measurement of a drop around Minneapolis in 1904.\n\n\n\nBefore the Greensburg EF5 tornado on May 4, 2007, it had been 8 years and one day since the US had a confirmed F5 or EF5 tornado. The last confirmed F5 or EF5 had hit southern Oklahoma City metro area and surrounding communities during the May 3, 1999 event. This is the longest interval without an F5 or EF5 tornado since official records began in 1950.\n\nMatt Suter of Fordland, Missouri holds the record for the longest-known distance traveled by anyone picked up by a tornado and survive. On March 12, 2006 he was carried , shy of , according to National Weather Service measurements.\n\nThe small town of Codell, Kansas, was hit by a tornado on the same date (May 20) three consecutive years: 1916, 1917, and 1918. The United States has about 100,000 thunderstorms per year; less than 1% produce a tornado. The odds of this coincidence occurring again are extremely small.\n\nTanner, a small town in northern Alabama, was hit by an F5 tornado on April 3, 1974 and was struck again 45 minutes later by a second F5 (however, the rating is disputed and it may have been high-end F4), demolishing what remained of the town. Thirty-seven years later, on April 27, 2011 (the largest and deadliest outbreak since 1974), Tanner was hit yet again by the EF5 2011 Hackleburg–Phil Campbell, Alabama tornado, which produced high-end EF4 damage in the southern portion of town. The suburban community of Harvest, Alabama, just to the northeast, also sustained major impacts from all three Tanner tornadoes, and was also hit by destructive tornadoes in 1995 and 2012.\n\nThe south Oklahoma City suburb of Moore, Oklahoma was hit by strong to violent tornadoes in 1973, 1998, 1999, 2003, 2010, 2013, and 2015, five of which were of F4/EF4 strength or greater. The 1999 and 2013 events were rated F5 and EF5, respectively. In total, about 23 tornadoes have struck within the immediate vicinity of Moore since 1890, the most recent of which was an EF2 on March 25, 2015.\n\nThe city of Jackson, Tennessee has been hit by an F4/EF4 tornado three separate times, in 1999, 2003, and 2008. All three of these tornadoes occurred after dark and were preceded or followed by a separate F3/EF3 tornado that caused additional destruction in the Jackson area.\n\nThe Grand Island, Nebraska area was hit by seven tornadoes in 4 hours.\n\n\n"}
{"id": "3676802", "url": "https://en.wikipedia.org/wiki?curid=3676802", "title": "W70", "text": "W70\n\nW70 is the designation for a tactical nuclear warhead developed by the United States in the early 1970s. The Lawrence Livermore National Laboratory designed W70 was used on the MGM-52 Lance. About 1250 were built in total. The warhead had a variable yield of between 1 and 100 kilotons, selectable by the user. The design dates from 1973.\n\nThe W70-3 was a modified version of the W70 and one of the first warheads to be battlefield-ready with an \"enhanced radiation\" (i.e. neutron bomb) feature. It had an explosive yield of about 1 kt., was manufactured during 1981-83, and was retired by 1992; 380 were built. Note that using the explosive yield of a neutron weapon to measure its destructive power can be deceptive, especially given the W70s variable yield - so-called \"neutron bombs\" only display their unique qualities at lower yields; at higher yields, the radius of intense heat and blast effects far outstrips the range that the prompt radiation can reach through an atmosphere. This effect is exaggerated against tank crews, who are extremely well protected against heat and blast, but whose vehicle's armour is readily penetrated by energetic neutron radiation.\n\nThe inventor of the neutron bomb, Samuel Cohen, has criticized the description of the W70 as a \"neutron bomb\":\nthe W-70 ... is not even remotely a \"neutron bomb.\" Instead of being the type of weapon that, in the popular mind, \"kills people and spares buildings\" it is one that both kills and physically destroys on a massive scale. The W-70 is not a discriminate weapon, like the neutron bomb—which, incidentally, should be considered a weapon that \"kills enemy personnel while sparing the physical fabric of the attacked populace, and even the populace too.\"\n\n\n"}
{"id": "33168709", "url": "https://en.wikipedia.org/wiki?curid=33168709", "title": "WISEPA J031325.96+780744.2", "text": "WISEPA J031325.96+780744.2\n\nWISEPA J031325.96+780744.2 (designation abbreviated to WISE 0313+7807, or WISE J0313+7807) is a brown dwarf of spectral class T8.5, located in constellation Cepheus at approximately 21 light-years from Earth.\n\nWISE 0313+7807 was discovered in 2011 by J. Davy Kirkpatrick et al. from data, collected by Wide-field Infrared Survey Explorer (WISE) Earth-orbiting satellite — NASA infrared-wavelength 40 cm (16 in) space telescope, which mission lasted from December 2009 to February 2011. In 2011 Kirkpatrick et al. published a paper in The Astrophysical Journal Supplement, where they presented discovery of 98 new found by WISE brown dwarf systems with components of spectral types M, L, T and Y, among which also was WISE 0313+7807.\n\nTrigonometric parallax of WISE 0313+7807 is 153 ± 15 mas, corresponding to distance of 6.5 pc or 21.3 ly.\n"}
{"id": "30472789", "url": "https://en.wikipedia.org/wiki?curid=30472789", "title": "Waga sculpture", "text": "Waga sculpture\n\nA waga, also known as a waka or waaka, is a type of memorial statue carved from wood in southern Ethiopia. Modern wagas or mini-wagas may also be carved specifically for sale to tourists, in which case the rod-shaped \"kallaccas\" on their heads are often exaggerated into phallic shapes.\n"}
{"id": "24329959", "url": "https://en.wikipedia.org/wiki?curid=24329959", "title": "Weiss magneton", "text": "Weiss magneton\n\nThe Weiss magneton was an experimentally derived unit of magnetic moment equal to joules per tesla, which is about 20% of the Bohr magneton. It was suggested in 1911 by Pierre Weiss.\n\nThe idea of elementary magnets originated from the Swiss physicist Walther Ritz, who tried to explain atomic spectra. In 1907 he suggested that atoms might contain chains of magnetized and neutral rods, which were the cause of magnetic properties of materials. Just like elementary charges, this was supposed to give rise to discrete values of the total magnetic moment per atom. In 1909, Weiss performed measurements of the saturation magnetization at the temperature of liquid hydrogen in the laboratory of Heike Kamerlingh Onnes in Leiden. In 1911, Weiss announced that the molar moments of nickel and iron had the ratio of 3:11, from which he derived the value of a magneton.\n\nWeiss gave an address about the magneton at a conference in Karlsruhe in September 1911. Several theorists commented that the magneton should involve Planck's constant \"h\". By postulating that the ratio of electron kinetic energy to orbital frequency should be equal to \"h\", Richard Gans computed a value that was almost an order of magnitude larger than the value obtained by Weiss. At the First Solvay Conference in November that year, Paul Langevin obtained a submultiple which gave better agreement. But once the old quantum theory was a bit better understood, no theoretical argument could be found to justify Weiss's value. In 1920, Wolfgang Pauli wrote an article where he called the magneton of the experimentalists the Weiss magneton, and the theoretical value the Bohr magneton.\n\nDespite theoretical problems, Weiss and other experimentalists like Blas Cabrera continued to analyze data in terms of the Weiss magneton until the 1930s.\n"}
{"id": "4163705", "url": "https://en.wikipedia.org/wiki?curid=4163705", "title": "Yhprum's law", "text": "Yhprum's law\n\nYhprum's law is the opposite of Murphy's law. \n\nThe simple formula of Yhprum's law is: \"Everything that can work, will work.\" \n\nAnother formulation of the law due to Richard Zeckhauser, a professor for political economy at Harvard University states: \"Sometimes systems that should not work, work nevertheless.\" \n\nResnick \"et al\". (2006) used this law to describe how intensive and seemingly altruistic participation by giving ranking is observed in the eBay system.\n"}
