{"id": "19997209", "url": "https://en.wikipedia.org/wiki?curid=19997209", "title": "Acarodomatia", "text": "Acarodomatia\n\nAcarodomatia (singular \"Acarodomatium\") (Latin: \"Acari\" - mites, \"domus\" - dwelling), are tussocks of hairs or nonglandular trichomes located in pits situated in major leaf vein axes of many plant species, occupied and caused by predatory and mycophagous mites.\n\n"}
{"id": "690512", "url": "https://en.wikipedia.org/wiki?curid=690512", "title": "Akaike information criterion", "text": "Akaike information criterion\n\nThe Akaike information criterion (AIC) is an estimator of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Thus, AIC provides a means for model selection.\n\nAIC is founded on information theory. When a statistical model is used to represent the process that generated the data, the model will almost never be exact; so some information will be lost by using the model to represent the process. AIC estimates the relative information lost by a given model: the less information a model loses, the higher the quality of that model. (In making an estimate of the information lost, AIC deals with the trade-off between the goodness of fit of the model and the simplicity of the model.)\n\nThe Akaike information criterion is named after the statistician Hirotugu Akaike, who formulated it. It now forms the basis of a paradigm for the foundations of statistics; as well, it is widely used for statistical inference.\n\nSuppose that we have a statistical model of some data. Let be the number of estimated parameters in the model. Let formula_1 be the maximum value of the likelihood function for the model. Then the AIC value of the model is the following.\n\nGiven a set of candidate models for the data, the preferred model is the one with the minimum AIC value. Thus, AIC rewards goodness of fit (as assessed by the likelihood function), but it also includes a penalty that is an increasing function of the number of estimated parameters. The penalty discourages overfitting, because increasing the number of parameters in the model almost always improves the goodness of the fit.\n\nAIC is founded in information theory. Suppose that the data is generated by some unknown process \"f\". We consider two candidate models to represent \"f\": \"g\" and \"g\". If we knew \"f\", then we could find the information lost from using \"g\" to represent \"f\" by calculating the Kullback–Leibler divergence, ; similarly, the information lost from using \"g\" to represent \"f\" could be found by calculating . We would then choose the candidate model that minimized the information loss.\n\nWe cannot choose with certainty, because we do not know \"f\". showed, however, that we can estimate, via AIC, how much more (or less) information is lost by \"g\" than by \"g\". The estimate, though, is only valid asymptotically; if the number of data points is small, then some correction is often necessary (see AICc, below).\n\nNote that AIC does not provide a test of a model in the sense of testing a null hypothesis. It tells nothing about the absolute quality of a model, only the quality relative to other models. Thus, if all the candidate models fit poorly, AIC will not give any warning of that.\n\nTo apply AIC in practice, we start with a set of candidate models, and then find the models' corresponding AIC values. There will almost always be information lost due to using a candidate model to represent the \"true model\" (i.e. the process that generated the data). We wish to select, from among the candidate models, the model that minimizes the information loss. We cannot choose with certainty, but we can minimize the estimated information loss.\n\nSuppose that there are \"R\" candidate models. Denote the AIC values of those models by AIC, AIC, AIC, …, AIC. Let AIC be the minimum of those values. Then the quantity exp((AIC − AIC)/2) can be interpreted as being proportional to the probability that the \"i\"th model minimizes the (estimated) information loss.\n\nAs an example, suppose that there are three candidate models, whose AIC values are 100, 102, and 110. Then the second model is exp((100 − 102)/2) = 0.368 times as probable as the first model to minimize the information loss. Similarly, the third model is exp((100 − 110)/2) = 0.007 times as probable as the first model to minimize the information loss.\n\nIn this example, we would omit the third model from further consideration. We then have three options: (1) gather more data, in the hope that this will allow clearly distinguishing between the first two models; (2) simply conclude that the data is insufficient to support selecting one model from among the first two; (3) take a weighted average of the first two models, with weights proportional to 1 and 0.368, respectively, and then do statistical inference based on the weighted multimodel.\n\nThe quantity exp((AIC − AIC)/2) is known as the \"relative likelihood\" of model \"i\". It is closely related to the likelihood ratio used in the likelihood-ratio test. Indeed, if all the models in the candidate set have the same number of parameters, then using AIC might at first appear to be very similar to using the likelihood-ratio test. There are, however, important distinctions. In particular, the likelihood-ratio test is valid only for nested models, whereas AIC (and AICc) has no such restriction.\n\nWhen the sample size is small, there is a substantial probability that AIC will select models that have too many parameters, i.e. that AIC will overfit. To address such potential overfitting, AICc was developed: AICc is AIC with a correction for small sample sizes.\n\nThe formula for AICc depends upon the statistical model. Assuming that the model is univariate, is linear in its parameters, and has normally-distributed residuals (conditional upon regressors), then the formula for AICc is as follows.\n\n—where denotes the sample size and denotes the number of parameters. Thus, AICc is essentially AIC with an extra penalty term for the number of parameters. Note that as , the extra penalty term converges to 0, and thus AICc converges to AIC.\n\nIf the assumption that the model is univariate and linear with normal residuals does not hold, then the formula for AICc will generally be different from the formula above. For some models, the formula can be difficult to determine. For every model that has AICc available, though, the formula for AICc is given by AIC plus terms that includes both and . In comparison, the formula for AIC includes but not . In other words, AIC is a first-order estimate (of the information loss), whereas AICc is a second-order estimate.\n\nFurther discussion of the formula, with examples of other assumptions, is given by and by . In particular, with other assumptions, bootstrap estimation of the formula is often feasible.\n\nTo summarize, AICc has the advantage of tending to be more accurate than AIC (especially for small samples), but AICc also has the disadvantage of sometimes being much more difficult to compute than AIC. Note that if all the candidate models have the same and the same formula for AICc, then AICc and AIC will give identical (relative) valuations; hence, there will be no disadvantage in using AIC, instead of AICc. Furthermore, if is many times larger than , then the extra penalty term will be negligible; hence, the disadvantage in using AIC, instead of AICc, will be negligible.\n\nThe Akaike information criterion was formulated by the statistician Hirotugu Akaike; it was originally named \"an information criterion\". It was first announced by Akaike at a 1971 symposium, the proceedings of which were published in 1973. The 1973 publication, though, was only an informal presentation of the concepts. The first formal publication was a 1974 paper by Akaike. , the 1974 paper had received more than 14000 citations in the Web of Science: making it the 73rd most-cited research paper of all time.\n\nNowadays, AIC has become common enough that it is often used without citing Akaike's 1974 paper. Indeed, there are over 150,000 scholarly articles/books that use AIC (as assessed by Google Scholar).\n\nThe initial derivation of AIC relied upon some strong assumptions. showed that the assumptions could be made much weaker. Takeuchi's work, however, was in Japanese and was not widely known outside Japan for many years.\n\nAICc was originally proposed for linear regression (only) by . That instigated the work of , and several further papers by the same authors, which extended the situations in which AICc could be applied.\n\nThe first general exposition of the information-theoretic approach was the volume by . It includes an English presentation of the work of Takeuchi. The volume led to far greater use of AIC, and it now has more than 42000 citations on Google Scholar.\n\nAkaike called his approach an \"entropy maximization principle\", because the approach is founded on the concept of entropy in information theory. Indeed, minimizing AIC in a statistical model is effectively equivalent to maximizing entropy in a thermodynamic system; in other words, the information-theoretic approach in statistics is essentially applying the Second Law of Thermodynamics. As such, AIC has roots in the work of Ludwig Boltzmann on entropy. For more on these issues, see and .\n\nA statistical model must fit all the data points. Thus, a straight line, on its own, is not a model of the data, unless all the data points lie exactly on the line. We can, however, choose a model that is \"a straight line plus noise\"; such a model might be formally described thus:\n\"y\" = \"b\" + \"b\"\"x\" + ε. Here, the ε are the residuals from the straight line fit. If the ε are assumed to be i.i.d. Gaussian (with zero mean), then the model has three parameters:\n\"b\", \"b\", and the variance of the Gaussian distributions.\nThus, when calculating the AIC value of this model, we should use \"k\"=3. More generally, for any least squares model with i.i.d. Gaussian residuals, the variance of the residuals' distributions should be counted as one of the parameters.\n\nAs another example, consider a first-order autoregressive model, defined by\n\"x\" = \"c\" + \"φx\" + ε, with the ε being i.i.d. Gaussian (with zero mean). For this model, there are three parameters: \"c\", \"φ\", and the variance of the ε. More generally, a \"p\"th-order autoregressive model has \"p\" + 2 parameters. (If, however, \"c\" is not estimated from the data, but instead given in advance, then there are only \"p\" + 1 parameters.)\n\nThe AIC values of the candidate models must all be computed with the same data set. Sometimes, though, we might want to compare a model of the response variable, , with a model of the logarithm of the response variable, . More generally, we might want to compare a model of the data with a model of transformed data. Following is an illustration of how to deal with data transforms (adapted from : \"Investigators should be sure that all hypotheses are modeled using the same response variable\").\n\nSuppose that we want to compare two models: one with a normal distribution of and one with a normal distribution of . We should \"not\" directly compare the AIC values of the two models. Instead, we should transform the normal cumulative distribution function to first take the logarithm of . To do that, we need to perform the relevant integration by substitution: thus, we need to multiply by the derivative of the (natural) logarithm function, which is . Hence, the transformed distribution has the following probability density function:\n—which is the probability density function for the log-normal distribution. We then compare the AIC value of the normal model against the AIC value of the log-normal model.\n\nSome statistical software will report the value of AIC or the maximum value of the log-likelihood function, but the reported values are not always correct.\nTypically, any incorrectness is due to a constant in the log-likelihood function being omitted. For example,\nthe log-likelihood function for independent identical normal distributions is\n—this is the function that is maximized, when obtaining the value of AIC. Some software, however, omits the constant term , and so reports erroneous values for the log-likelihood maximum—and thus for AIC. Such errors do not matter for AIC-based comparisons, \"if\" all the models have their residuals as normally-distributed: because then the errors cancel out. In general, however, the constant term needs to be included in the log-likelihood function. Hence, before using software to calculate AIC, it is generally good practice to run some simple tests on the software, to ensure that the function values are correct.\n\nThe formula for the Bayesian information criterion (BIC) is similar to the formula for AIC, but with a different penalty for the number of parameters. With AIC the penalty is , whereas with BIC the penalty is .\n\nA comparison of AIC/AICc and BIC is given by , with follow-up remarks by . The authors show that AIC/AICc can be derived in the same Bayesian framework as BIC, just by using different prior probabilities. In the Bayesian derivation of BIC, though, each candidate model has a prior probability of 1/\"R\" (where \"R\" is the number of candidate models); such a derivation is \"not sensible\", because the prior should be a decreasing function of . Additionally, the authors present a few simulation studies that suggest AICc tends to have practical/performance advantages over BIC.\n\nA point made by several researchers is that AIC and BIC are appropriate for different tasks. In particular, BIC is argued to be appropriate for selecting the \"true model\" (i.e. the process that generated the data) from the set of candidate models, whereas AIC is not appropriate. To be specific, if the \"true model\" is in the set of candidates, then BIC will select the \"true model\" with probability 1, as ; in contrast, when selection is done via AIC, the probability can be less than 1. Proponents of AIC argue that this issue is negligible, because the \"true model\" is virtually never in the candidate set. Indeed, it is a common aphorism in statistics that \"all models are wrong\"; hence the \"true model\" (i.e. reality) cannot be in the candidate set.\n\nAnother comparison of AIC and BIC is given by . Vrieze presents a simulation study—which allows the \"true model\" to be in the candidate set (unlike with virtually all real data). The simulation study demonstrates, in particular, that AIC sometimes selects a much better model than BIC even when the \"true model\" is in the candidate set. The reason is that, for finite , BIC can have a substantial risk of selecting a very bad model from the candidate set. This reason can arise even when is much larger than . With AIC, the risk of selecting a very bad model is minimized.\n\nIf the \"true model\" is not in the candidate set, then the most that we can hope to do is select the model that best approximates the \"true model\". AIC is appropriate for finding the best approximating model, under certain assumptions. (Those assumptions include, in particular, that the approximating is done with regard to information loss.)\n\nComparison of AIC and BIC in the context of regression is given by . In regression, AIC is asymptotically optimal for selecting the model with the least mean squared error, under the assumption that the \"true model\" is not in the candidate set. BIC is not asymptotically optimal under the assumption. Yang additionally shows that the rate at which AIC converges to the optimum is, in a certain sense, the best possible.\n\nLeave-one-out cross-validation is asymptotically equivalent to AIC, for ordinary linear regression models. Asymptotic equivalence to AIC also holds for mixed-effects models.\n\nSometimes, each candidate model assumes that the residuals are distributed according to independent identical normal distributions (with zero mean). That gives rise to least squares model fitting.\n\nWith least squares fitting, the maximum likelihood estimate for the variance of a model's residuals distributions is formula_6, where formula_7 is the residual sum of squares: formula_8. Then, the maximum value of a model's log-likelihood function is \n—where is a constant independent of the model, and dependent only on the particular data points, i.e. it does not change if the data does not change.\n\nThat gives AIC = . Because only differences in AIC are meaningful, the constant can be ignored, which allows us to conveniently take AIC = for model comparisons. Note that if all the models have the same , then selecting the model with minimum AIC is equivalent to selecting the model with minimum —which is the usual objective of model selection based on least squares.\n\nMallows's \"C\" is equivalent to AIC in the case of (Gaussian) linear regression.\n\n\n"}
{"id": "49994218", "url": "https://en.wikipedia.org/wiki?curid=49994218", "title": "Aline Elizabeth Black", "text": "Aline Elizabeth Black\n\nAline Elizabeth Black, also known under her married name of Aline Elizabeth Black Hicks, (born 1906, died 1974) was an African-American educator, known for taking part in a civil rights court case that centered on unequal pay.\n\nIn 2008 the Library of Virginia honored Black as part of their Notable African Americans in Virginia History project. Black also received the Education Association of Norfolk's Backbone Award in 1971, in recognition for her role in establishing educational and professional equality.\n\nBlack was born in Norfolk, Virginia on March 23, 1906 to Charles and Ida Black. She was educated locally and attended Booker T. Washington High School. Black went on to receive a degree from the Virginia Normal and Industrial Institute and received a Master of Science at the University of Pennsylvania in 1935. While in school Black began working as a teacher at Booker T. Washington High School until she lost her job in 1939 as retaliation for a legal case over salary discrimination. After losing her job Black began working on a doctorate of chemistry at New York University, but did not complete the degree. Black was re-hired as an employee of the Norfolk School Board in 1941 and resumed her former job as a chemistry teacher. She remained in this job until 1970, after which point she began working at Jacox Junior High School as an instructional development specialist until her retirement in 1973.\n\nBlack married Frank A. Hicks at some point during World War II and had one daughter. She died in Norfolk on August 22, 1974.\n\nWhile she was attending college, Black began working in Norfolk as a science instructor in the public school system. Black received only two-thirds of what a white teacher received for the same job, a common occurrence for African-American educators. This fact was the focus of ire from the Norfolk Teachers Association and the Virginia State Teachers Association, who felt that this racial inequality was a violation of the Fourteenth Amendment to the United States Constitution. In October 1938 Black petitioned the Norfolk School Board to base their educators' salaries on experience and qualifications rather than race or color. The school board denied her petition, stating that Black had waived any right to contest the pay or seek redress when she signed her yearly contract. With the backing of the teachers' associations and the cooperation of the NAACP, Black then filed a suit against the Norfolk school board in March 1939, marking her as the first teacher to file a salary discrimination suit in the state of Virginia. She was accompanied by a team of attorneys, one of which was the future Associate Justice of the Supreme Court of the United States Thurgood Marshall. Black's suit was dismissed and her attorneys appealed to the Virginia Supreme Court of Appeals. In the meantime the Norfolk School Board retaliated against Black for her lawsuit by not renewing her contract in June 1939. As she was no longer an employee and did not have the standing to sue, the court system denied Black's appeal. The school board's actions were highly criticized and the focus of a protest on June 24 of the same year.\n\nUndeterred, the teachers associations produced another plaintiff, Melvin O. Alston, who filed another suit against the school board, \"Alston v. School Board of City of Norfolk\". This lawsuit succeeded in going to the United States Supreme Court, who upheld a ruling that teacher salaries fell under the Fourteenth Amendment.\n"}
{"id": "36082139", "url": "https://en.wikipedia.org/wiki?curid=36082139", "title": "Apollo spacecraft feasibility study", "text": "Apollo spacecraft feasibility study\n\nThe Apollo spacecraft feasibility study was conducted by NASA from July 1960 through May 1961 to investigate preliminary designs for a post-Project Mercury multi-manned spacecraft to be used for possible space station, circum-lunar, lunar orbital, or manned lunar landing missions. Six-month, $250,000 study contracts were awarded to General Dynamics/Convair, General Electric, and the Glenn L. Martin Company. Meanwhile, NASA conducted its own inhouse design study led by Maxime Faget, intended as a gauge of the competitors' entries. The three companies spent varying amounts of their own money in excess of the $250,000 to produce designs which included a \"re-entry module\" separate from the \"mission module\" cabin, and a \"propulsion and equipment module\".\n\nOne week after the presentation of the contractors' designs, President John F. Kennedy committed NASA to a manned lunar landing, giving the Apollo program an immediate, critical focus. NASA decided to discard the study designs and the mission module cabin, and based the lunar landing mission design on Faget's inhouse design, with a cone-shaped \"command module\", supported by a cylindrical \"service module\" containing return propulsion and supporting equipment. This would be carried to the lunar surface by a still-to-be-defined landing propulsion module. NASA then launched another competition for the command/service module procurement contract.\n\nIn December 1961, GE publicly presented their feasibility study design to the American Astronautical Society . Similarities in the basic mission-command-propulsion module design have been noted to the Soviet Union's Soyuz spacecraft designed by Sergei Korolev and Vasily Mishin. It has been speculated that Korolev and Mishin could have incorporated GE design elements in the existing OKB-1 Sever designs (1959-1962) that eventually became the cancelled Soyuz-A (7K) (1963) and approved Soyuz 7K-OK (1965-1967).\n\nIn July and August 1960, NASA's Space Task Group (STG) hosted a series of NASA-industry conferences to discuss post-Project Mercury manned spacecraft plans. Deputy Administrator Hugh L. Dryden announced at the conference opening that \"the next spacecraft beyond Mercury will be called Apollo.\" \n\nOn August 30, NASA presented plans to award three feasibility study contracts for the Apollo spacecraft, conceived as a three-man Earth orbital and circumlunar craft, with growth potential for manned lunar landings. A Request For Proposal was issued on September 12, and fourteen bids were received by October 9. On October 25, NASA awarded the $250,000, six-month contracts to General Dynamics/Convair, General Electric, and the Glenn L. Martin Company. Meanwhile, members of the Space Task Group performed their own spacecraft design studies, to serve as a gauge to judge and monitor the three industry designs.\n\nAll three competitors supplemented the $250,000 contracts with their own money: Convair spent $1 million, GE $2 million, and Martin $3 million. The Manager of GE Space Vehicle Systems (Philadelphia), George Arthur, led the GE proposal team that included Harold Bloom, Charles Bixler, Jacob Abel, and Arnold Cohen. On May 15 to 17, 1961, the contractors presented their study results to NASA. All three designs employed a \"mission module\" cabin separate from the \"command module\" (piloting and re-entry cabin), and a \"propulsion and equipment module\". Martin studied three different reentry module shapes, including a conical capsule vehicle similar to the STG configuration. GE also studied several reentry module shapes. GD/Convair's proposal employed a lifting body shape.\n\nConvair/Astronautics' entry was designed primarily for lunar orbit, with flexibility and growth potential built in to accommodate lunar landing. The company estimated a total program cost of $1.25 billion over about six years.\n\nConvair selected a lifting body for the return vehicle (command module), similar to one conceived several years earlier by Alfred J. Eggers of NASA-Ames. This had an abort tower attached through launch, and nestled inside a large mission module. Convair/Astronautics envisioned a progressive flight development plan, with many Earth-orbital missions before attempting circumlunar, and then lunar-orbital missions. Earth landings would be by glidesail parachute near San Antonio, Texas. The development flights would experiment with space rendezvous, docking, artificial gravity, and maneuverable landing, leading to an eventual lunar landing. The study cost the contractor about $1 million.\n\nGE's design capitalized upon hardware almost ready to fly: a bullet-shaped descent module, carried between a conical mission module cabin containing life support and avionics, and the cylindrical propulsion module. The entire craft was long, with one innovation: a cocoonlike wrapping for secondary pressure protection in case of cabin leaks or meteoroid puncture. Had this configuration been selected, the payload sent to the Moon would have resembled the nose cone flown on the early Saturn I rockets. Although GE did not estimate the final costs in its summary, the company was confident of achieving circumlunar flight by the end of 1966 and lunar-orbital flight shortly thereafter.\n\nSeeking professional recognition for their design work on the GE proposal, George Arthur and Jacob Abel publicly presented their papers documenting the GE D-2 design in December 1961 at a special symposium of the American Astronautical Society in Denver, Colorado.\n\nThe Martin Company spent about $3 million, employing almost 300 persons for the better part of the six-month term, to produce the most elaborate study of the three, not only following all the Space Task Group guidelines, but also going far beyond in systems analysis. The complete proposal consisted of 9,000 pages. Focusing on versatility, flexibility, safety margins, and growth, this was the only study that detailed the progression of steps from lunar orbiting to lunar landing. Martin's spacecraft would have been similar to the Apollo spacecraft that ultimately emerged. When Martin later entered the Apollo hardware procurement contract competition, NASA scored them highest of all the entrants on configuration design.\n\nMartin recommended a five-part spacecraft. The command module was a flat-bottomed cone with a rounded apex and a tower for a tractor-rocket launch escape system. Behind the flat aft bulkhead were propulsion, equipment, and mission modules. Tradeoffs between weight and propulsion requirements led to the selection of a pressurized shell of semimonocoque aluminum alloy coated with a composite heatshield of superalloy with a charring ablator. Two crewmen would sit abreast, with the third behind, in couches that could rotate for reentry g-load protection and for getting in and out of the spacecraft. Flaps for limited maneuverability on reentry, a parachute landing system, and a jettisonable mission module that could also serve as a solar storm cellar, a laboratory, or even the descent stage for a lunar lander, were also featured.\n\nNASA did not get a chance to deliberate long on the study results, due to the pressure placed on America's space program by the Soviet's launching of the first man in space, Yuri Gagarin, on April 12, 1961. On May 25, one week after presentation of the study results, President John F. Kennedy proposed the Moon landing objective to the US Congress, giving the Apollo program a clear focus and sense of urgency. NASA turned its focus to what relevant data could be mined from the proposals (abandoning the mission module), and launched another competition for the hardware procurement phase, fixing the reentry module configuration to the conical shape designed by Maxime Faget.\n\nNASA awarded the contract for the Apollo Command/Service Module (CSM) to North American Aviation on November 28, 1961, when it was still assumed the lunar landing would be achieved by direct descent or Earth orbit rendezvous rather than by lunar orbit rendezvous. Therefore, design proceeded without a means of docking the Command Module to a lunar lander spacecraft. \nIn the summer of 1962, the selection of the LOR proposal from NASA's Langley Research Center, plus several technical obstacles encountered in some subsystems (such as environmental control), soon made it clear that substantial redesign would be required. By 1963, NASA decided the most efficient way to keep the Apollo program on track and address technical obstacles encountered in some subsystems such as environmental control, was to proceed with the development of two CSM versions: the preliminary Block I, and the advanced Block II.\n\nSimilarities have been noted between the GE D2 design and the Russian Soyuz spacecraft, which was designed and built after George Arthur and Jacob Abel's AAS presentation (Denver, CO) in December 1961. In particular, Soyuz uses an orbital module located in front of the descent module, which uses a similar sphere-cone-sphere shape.\n\nVictor Minenko, one of the OKB-1 designers with Korolev in 1950s and 1960s, who was active with RSC Energia in 1993, noted that in 1961 there were 40 people in several departments working on early designs and versions of the eventual Soyuz. \"We use to read carefully the U.S. literature by the leading astrodynamicists - Ferri, Chapman, Van Driest, Lees, and the top Russians - Sibulkin, Koropkin\". Vassily Mishin, chief Soyuz designer after Korolev's death, noted that a logical comparison of the \"Soyuz\" was to the US Apollo command/service module, since both were designed for lunar transport.\n\nIn 1983, Phillip S. Clark and Ralph F. Gibbons discussed the Russian Soyuz program development (1963-1967) and adaptation of design elements from other programs and studies (Soviet and foreign).\n\nA similar modular design was used in the Russian Progress spacecraft (essentially the unmanned version of Soyuz), the Chinese Shenzhou spacecraft, and the planned Indian ISRO Orbital Vehicle.\n\n\n\n\n"}
{"id": "22640378", "url": "https://en.wikipedia.org/wiki?curid=22640378", "title": "Ascalaph Designer", "text": "Ascalaph Designer\n\nAscalaph Designer is a computer program for general purpose molecular modelling for molecular design and simulations. It provides a graphical environment for the common programs of quantum and classical molecular modelling ORCA, NWChem, Firefly, CP2K and MDynaMix \n. The molecular mechanics calculations cover model building, energy optimizations and molecular dynamics. Firefly (formerly named PC GAMESS) covers a wide range of quantum chemistry methods. Ascalaph Designer is free and open-source software, released under the GNU General Public License, version 2 (GPLv2).\n\n\n"}
{"id": "47723216", "url": "https://en.wikipedia.org/wiki?curid=47723216", "title": "Barry Bozeman", "text": "Barry Bozeman\n\nBarry Bozeman is a professor at Arizona State University where he is Director, Center for Organization Research and Design, and Arizona Centennial Professor of Technology Policy and Public Management. He specializes in two disparate fields, organization theory and science and technology policy.\n\nBozeman was born in Birmingham, Alabama on January 18, 1947 to Glenn Bozeman and Audrey J. Bozeman (née Martin). His mother was a full-time homemaker and this father a construction worker. His early life was characterized by much family relocation, resulting in his attending 21 different schools before the 7th grade. In 1960, the family settled down in West Palm Beach Florida and Bozeman graduated from Palm Beach High School in 1964. He attended Palm Beach High Junior College, in Lake Worth Florida, (now Palm Beach State College), graduating in 1966. Bozeman played on the varsity baseball team at Palm Beach Junior College where he had the distinction of scoring the first run in the new team’s history.\n\nIn 1970, Bozeman entered the doctoral program in political science at Ohio State University, focusing on public policy studies. He graduated in 1973 and, that same year, began as an Assistant Professor of Political Science at Georgia Institute of Technology. During that period, he began specializing in Science and Technology Policy and in 1974, as part of the Intergovernmental Personnel Act, took a position as an analyst in the National Science Foundation’s Division of Information Science and Technology. After returning briefly to Georgia Tech, Bozeman took a job at the University of Missouri-Columbia, where he was appointed in the Department of Political Science, as well as the new Department of Public Administration. In 1977, Bozeman began a long stay (1977-1993) at Syracuse University’s Maxwell School of Citizenship and Public Affairs. While at Syracuse, Bozeman was the Director of the Doctoral Program (1979-1986) and was the founding director of the Center for Technology and Information Policy. He moved in 1993 to be the first full-time director of the new School of Public Policy at Georgia Tech and was later appointed as Regent’s Professor, the first social scientist to become a Regent’s Professor at Georgia Tech. In 2006, Bozeman moved to University of Georgia where he became the first holder of the Department of Public Administration and Policy’s Ander Crenshaw Chair in Public Policy. In 2013, Bozeman moved to Arizona State University where he is Arizona Centennial Professor of Technology Policy and Public Management and Director of the Center for Organization Research and Design.\n\n\nBozeman’s chief contributions to organization theory and public administration include:\n\n\nBozeman’s chief contributions to science and technology policy include:\n\nBarry Bozeman and his wife Monica Gaughan live in Tempe, Arizona. Dr. Gaughan, a sociologist, is a faculty member at Arizona State University. Bozeman has two children from an earlier marriage, a son John and a daughter Brandyn.\n\n\n\"This list only contains articles from 2000 onward.\"\n\n"}
{"id": "5158653", "url": "https://en.wikipedia.org/wiki?curid=5158653", "title": "Breakthrough of the Year", "text": "Breakthrough of the Year\n\nThe Breakthrough of the Year is an annual award made by the AAAS journal, \"Science\", for the most significant development in scientific research. Originating in 1989 as the \"Molecule of the Year\", and inspired by \"Time\" Man of the Year, it was renamed the Breakthrough of the Year in 1996. The Breakthrough of the Year is widely recognized as one of the highest distinctions in science.\n\n\nTop 10 scientific breakthroughs and the winners of each year.\n\nOther Top 10 Breakthroughs of the Year:\n"}
{"id": "9223758", "url": "https://en.wikipedia.org/wiki?curid=9223758", "title": "Curt Eisner", "text": "Curt Eisner\n\nCurt Eisner (born April 28, 1890 in Zabrze - died December 30, 1981 in The Hague) was a German entomologist who specialised in snow butterflies or Parnassinae. His collections of Parnassinae are in Naturalis, in Leiden, and his Ornithoptera and Morphidae are in the Museum für Naturkunde in Berlin.\n\n\n"}
{"id": "14101612", "url": "https://en.wikipedia.org/wiki?curid=14101612", "title": "DARPA Grand Challenge (2005)", "text": "DARPA Grand Challenge (2005)\n\nThe second driverless car competition of the DARPA Grand Challenge was a off-road course that began at 6:40am on October 8, 2005, near the California/Nevada state line. All but one of the 23 finalists in the 2005 race surpassed the distance completed by the best vehicle in the 2004 race. Five vehicles successfully completed the course:\n\nVehicles in the 2005 race passed through three narrow tunnels and navigated more than 100 sharp left and right turns. The race concluded through Beer Bottle Pass, a winding mountain pass with sheer drop-offs on both sides. Although the 2004 course had required more elevation gain and some very sharp switchbacks (Daggett Ridge) had been required near the beginning of the route, it had had far fewer curves and generally wider roads than the 2005 course.\n\nThe natural rivalry between the teams from Stanford and Carnegie Mellon (Sebastian Thrun, head of the Stanford team was previously a faculty member at Carnegie Mellon and colleague of Red Whittaker, head of the CMU team) was played out during the race. Mechanical problems plagued H1ghlander before it was passed by Stanley. Gray Team’s entry was a miracle in itself, as the team from the suburbs of New Orleans was caught in Hurricane Katrina a few short months before the race. The fifth finisher, Terramax, a 30,000 pound entry from Oshkosh Truck, finished on the second day. The huge truck spent the night idling on the course, and was particularly nimble in carefully picking its way down the narrow roads of Beer Bottle Pass.\n\nOf the original 195 applicants, initially 40 teams were selected to participate in the National Qualification Event (NQE). Three teams were added on Aug. 23, 2005 to the semi-finalist 40 teams who were selected from site visits, sending 43 teams to the NQE.\n\nThe National Qualification Event was held at California Speedway in Fontana from September 27 through October 5. The results of the NQE were used to cut the 43 teams down to 23 for the race on October 8.\n\nVehicle performances at the NQE were judged by (1) elapsed time to complete the course; (2) number of obstacles successfully passed without contact; (3) number of gates successfully passed. DARPA did not reveal the relative importance of these three factors. DARPA's final ranking of the vehicles, for purposes of pole position in the Grand Challenge Event (GCE), may have been partly subjective.\n\nThe results of the 2005 DARPA NQEs are shown below sorted top to bottom by runs completed and gates passed, compiled from DARPA's published NQE results. This ordering does not correspond to DARPA's ranking of team performance. (For example, this ordering does not reflect speed as an element of performance.) Teams highlighted in green were the teams that DARPA selected to participate in the Grand Challenge desert race.\n\nThere were four NQE runs. The above four major columns are sorted from worst to best runs of each team. \"Time\" signifies the time in minutes for a completed run and \"x\" indicates an incomplete run. \"Gates\" indicates the number of gates along the track that were passed (there were 50 total per run). \"Obstacles\" indicates the number of obstacles on the track that were passed (there were 5 total per run). \"Finishes\" indicates the total number of runs that the team successfully completed (there were 4 runs total). \"Total Gates\" indicates the total number of gates that the team successfully passed. The teams are sorted from top to bottom according to runs completed and then by total gates passed. Teams indicated as a \"desert race participant\" are those teams invited by DARPA to participate in the Grand Challenge desert race. Source data\n\nFor the first and second run, only 4 obstacles were present whereas for the third and fourth runs 5 obstacles were present. The NQE results presented in the table above show each team's runs sorted left to right from their worst run to their best run in order to better illustrate relative ranking (i.e. not in order of the actual run sequence).\n\nOn October 6, the selected teams transported their robots to the starting location. On October 7 the teams had a day to fix any portions of the robots broken in the course of transportation prior to the actual race.\n\n\"A Google Talk video about the DARPA 2005 race is available at YouTube\"\n\nThe route to be followed by the robots was supplied to the teams two hours before the start as a computer file with GPS coordinates, one every 72 m (237 feet) of the route, with more frequent waypoints in difficult patches. Some teams used topographic maps and aerial imagery to manually map out and program precise path and speed settings. Once the race had started, the robots were not allowed to contact humans in any way.\n\nEach robot started at a different time and was \"paused\" for different amounts of time during the race; DARPA compensated for the staggered start times and subtracted the pause time from each robot's total to derive its final official time. The $2 million prize was awarded on Sunday, October 9, 2005.\n\nThe 2005 competitors were much more successful than those of 2004; only one failed to pass the mark set by the best-performing 2004 entry, Sandstorm. By the end, 18 robots had been disabled and five robots finished the course. On the first day, Stanley from Stanford University, and H1ghlander and Sandstorm from Carnegie Mellon University, finished within minutes of each other, with Stanley crossing the finish line first. Kat-5 from Gray Team started much later, but finished in a comparable time. The race paused overnight with one competitor, TerraMax, left on the course at mile 83; TerraMax had the stage to itself on Sunday as it belatedly rumbled home.\nThe winner of the 2005 DARPA Grand Challenge was Stanley, with a course time of 6 hours 53 minutes and 8 seconds (6:53:08) with average speed of 30.7 km/h (19.1 mph). CMU's Sandstorm followed with 7:04:50 at 29.9 km/h (18.6 mph) and H1ghlander at 7:14:00 at 29.3 km/h (18.2 mph). Gray Team's Kat-5 came through at 7:30:16 with average speed of 28.2 km/h (17.5 mph). Oshkosh Truck's Terramax finished at 12:51 and would not have been eligible for the prize because it exceeded the ten-hour limit.\n\nThe Official Website (requires Flash player plugin) contained a map and positions of the competitors, while TG Daily posted a running summary of the day's events.\n\nDNF = Did Not Finish\n\n\n\n\n"}
{"id": "18265957", "url": "https://en.wikipedia.org/wiki?curid=18265957", "title": "Documentary collection", "text": "Documentary collection\n\nA documentary collection is a process in which a seller instructs their bank to forward documents related to the export of goods to a buyer's bank with a request to present these documents to the buyer for payment, indicating when and on what conditions these documents can be released to the buyer.\n\nThe buyer may obtain possession of goods and clear them through customs, if the buyer has the shipping documents (original bill of lading, certificate of origin, etc.). The documents, however, are only released to the buyer after payment has been made (\"Documents against Payment\") or payment undertaking has been given - the buyer has accepted a bill of exchange issued by the seller and payable at a certain date in the future (maturity date) (\"Documents against Acceptance\").\n\nDocumentary Collections facilitate import/export operations. They do not provide the same level of security as Letters of Credit, but, as a result, the costs are lower. Unlike the Letters of Credit, for a Documentary Collection, the bank acts as a channel for the documents but does not issue any payment covenants (does not guarantee payment). The bank that has received a documentary collection may debit the buyer's account and make payment only if authorised by the buyer.\n\nPossibilities and advantages:\n\nMake international trade operations more flexible,\nUse Documentary Collection in cases when the seller does not want to deliver goods to the buyer on \"open account\" basis, but due to a long-term stable business relationship between the parties there is no need for security provided by a Letter of Credit or payment guarantee,\nDocumentary collection is suitable to the seller:\nif the seller has no doubts about the buyer's ability to meet its payment obligations,\nif the political and economic situation in the buyer's country is stable,\nif there are no foreign exchange restrictions in the seller's country;\nDocumentary collection is convenient for the buyer because:\nthere is no need for an advance payment; payment for goods can be made when shipping documents have been received,\nin cases of documents released against acceptance the buyer has the possibility to sell the goods first and afterwards make payment to the seller.\nDocumentary Collection assures the seller that the shipping documents will be released to the buyer only upon payment or acceptance of a Bill of Exchange.\n"}
{"id": "37125569", "url": "https://en.wikipedia.org/wiki?curid=37125569", "title": "Estimated maximum possible concentration", "text": "Estimated maximum possible concentration\n\nEstimated maximum possible concentration (EMPC) is a term used in dioxin concentration determination for a concentration between limit of quantification and limit of detection.\n"}
{"id": "166404", "url": "https://en.wikipedia.org/wiki?curid=166404", "title": "First law of thermodynamics", "text": "First law of thermodynamics\n\nThe first law of thermodynamics is a version of the law of conservation of energy, adapted for thermodynamic systems. The law of conservation of energy states that the total energy of an isolated system is constant; energy can be transformed from one form to another, but can be neither created nor destroyed. The first law is often formulated\n\nIt states that the change in the internal energy of a closed system is equal to the amount of heat supplied \"to\" the system, minus the amount of work done \"by\" the system on its surroundings. An equivalent statement is that perpetual motion machines of the first kind are impossible.\n\nInvestigations into the nature of heat and work and their relationship began with the invention of the first engines used to extract water from mines. Improvements to such engines so as to increase their efficiency and power output came first from mechanics that worked with such machines but only slowly advanced the art. Deeper investigations that placed those on a mathematical and physics basis came later.\n\nThe first law of thermodynamics was developed empirically over about half a century. The first full statements of the law came in 1850 from Rudolf Clausius and from William Rankine; Rankine's statement is considered less distinct relative to Clausius'. A main aspect of the struggle was to deal with the previously proposed caloric theory of heat.\n\nIn 1840, Germain Hess stated a conservation law for the so-called 'heat of reaction' for chemical reactions. His law was later recognized as a consequence of the first law of thermodynamics, but Hess's statement was not explicitly concerned with the relation between energy exchanges by heat and work.\n\nAccording to Truesdell (1980), Julius Robert von Mayer in 1841 made a statement that meant that \"in a process at constant pressure, the heat used to produce expansion is universally interconvertible with work\", but this is not a general statement of the first law.\n\nThe original nineteenth century statements of the first law of thermodynamics appeared in a conceptual framework in which transfer of energy as heat was taken as a primitive notion, not defined or constructed by the theoretical development of the framework, but rather presupposed as prior to it and already accepted. The primitive notion of heat was taken as empirically established, especially through calorimetry regarded as a subject in its own right, prior to thermodynamics. Jointly primitive with this notion of heat were the notions of empirical temperature and thermal equilibrium. This framework also took as primitive the notion of transfer of energy as work. This framework did not presume a concept of energy in general, but regarded it as derived or synthesized from the prior notions of heat and work. By one author, this framework has been called the \"thermodynamic\" approach.\n\nThe first explicit statement of the first law of thermodynamics, by Rudolf Clausius in 1850, referred to cyclic thermodynamic processes.\n\nClausius also stated the law in another form, referring to the existence of a function of state of the system, the internal energy, and expressed it in terms of a differential equation for the increments of a thermodynamic process. This equation may be described as follows:\n\nBecause of its definition in terms of increments, the value of the internal energy of a system is not uniquely defined. It is defined only up to an arbitrary additive constant of integration, which can be adjusted to give arbitrary reference zero levels. This non-uniqueness is in keeping with the abstract mathematical nature of the internal energy. The internal energy is customarily stated relative to a conventionally chosen standard reference state of the system.\n\nThe concept of internal energy is considered by Bailyn to be of \"enormous interest\". Its quantity cannot be immediately measured, but can only be inferred, by differencing actual immediate measurements. Bailyn likens it to the energy states of an atom, that were revealed by Bohr's energy relation \"hν\" = \"E\" − \"E\". In each case, an unmeasurable quantity (the internal energy, the atomic energy level) is revealed by considering the difference of measured quantities (increments of internal energy, quantities of emitted or absorbed radiative energy).\n\nIn 1907, George H. Bryan wrote about systems between which there is no transfer of matter (closed systems): \"Definition. When energy flows from one system or part of a system to another otherwise than by the performance of mechanical work, the energy so transferred is called \"heat\".\" This definition may be regarded as expressing a conceptual revision, as follows. This was systematically expounded in 1909 by Constantin Carathéodory, whose attention had been drawn to it by Max Born. Largely through Born's influence, this revised conceptual approach to the definition of heat came to be preferred by many twentieth-century writers. It might be called the \"mechanical approach\".\n\nEnergy can also be transferred from one thermodynamic system to another in association with transfer of matter. Born points out that in general such energy transfer is not resolvable uniquely into work and heat moieties. In general, when there is transfer of energy associated with matter transfer, work and heat transfers can be distinguished only when they pass through walls physically separate from those for matter transfer.\n\nThe \"mechanical\" approach postulates the law of conservation of energy. It also postulates that energy can be transferred from one thermodynamic system to another adiabatically as work, and that energy can be held as the internal energy of a thermodynamic system. It also postulates that energy can be transferred from one thermodynamic system to another by a path that is non-adiabatic, and is unaccompanied by matter transfer. Initially, it \"cleverly\" (according to Bailyn) refrains from labelling as 'heat' such non-adiabatic, unaccompanied transfer of energy. It rests on the primitive notion of \"walls\", especially adiabatic walls and non-adiabatic walls, defined as follows. Temporarily, only for purpose of this definition, one can prohibit transfer of energy as work across a wall of interest. Then walls of interest fall into two classes, (a) those such that arbitrary systems separated by them remain independently in their own previously established respective states of internal thermodynamic equilibrium; they are defined as adiabatic; and (b) those without such independence; they are defined as non-adiabatic.\n\nThis approach derives the notions of transfer of energy as heat, and of temperature, as theoretical developments, not taking them as primitives. It regards calorimetry as a derived theory. It has an early origin in the nineteenth century, for example in the work of Helmholtz, but also in the work of many others.\n\nThe revised statement of the first law postulates that a change in the internal energy of a system due to any arbitrary process, that takes the system from a given initial thermodynamic state to a given final equilibrium thermodynamic state, can be determined through the physical existence, for those given states, of a reference process that occurs purely through stages of adiabatic work.\n\nThe revised statement is then\n\nThis statement is much less close to the empirical basis than are the original statements, but is often regarded as conceptually parsimonious in that it rests only on the concepts of adiabatic work and of non-adiabatic processes, not on the concepts of transfer of energy as heat and of empirical temperature that are presupposed by the original statements. Largely through the influence of Max Born, it is often regarded as theoretically preferable because of this conceptual parsimony. Born particularly observes that the revised approach avoids thinking in terms of what he calls the \"imported engineering\" concept of heat engines.\n\nBasing his thinking on the mechanical approach, Born in 1921, and again in 1949, proposed to revise the definition of heat. In particular, he referred to the work of Constantin Carathéodory, who had in 1909 stated the first law without defining quantity of heat. Born's definition was specifically for transfers of energy without transfer of matter, and it has been widely followed in textbooks (examples:). Born observes that a transfer of matter between two systems is accompanied by a transfer of internal energy that cannot be resolved into heat and work components. There can be pathways to other systems, spatially separate from that of the matter transfer, that allow heat and work transfer independent of and simultaneous with the matter transfer. Energy is conserved in such transfers.\n\nThe first law of thermodynamics for a closed system was expressed in two ways by Clausius. One way referred to cyclic processes and the inputs and outputs of the system, but did not refer to increments in the internal state of the system. The other way referred to an incremental change in the internal state of the system, and did not expect the process to be cyclic.\n\nA cyclic process is one that can be repeated indefinitely often, returning the system to its initial state. Of particular interest for single cycle of a cyclic process are the net work done, and the net heat taken in (or 'consumed', in Clausius' statement), by the system.\n\nIn a cyclic process in which the system does net work on its surroundings, it is observed to be physically necessary not only that heat be taken into the system, but also, importantly, that some heat leave the system. The difference is the heat converted by the cycle into work. In each repetition of a cyclic process, the net work done by the system, measured in mechanical units, is proportional to the heat consumed, measured in calorimetric units.\n\nThe constant of proportionality is universal and independent of the system and in 1845 and 1847 was measured by James Joule, who described it as the \"mechanical equivalent of heat\".\n\nIn a non-cyclic process, the change in the internal energy of a system is equal to net energy added as heat to the system minus the net work done by the system, both being measured in mechanical units. Taking as a change in internal energy, one writes\n\nwhere denotes the net quantity of heat supplied to the system by its surroundings and denotes the net work done by the system. This sign convention is implicit in Clausius' statement of the law given above. It originated with the study of heat engines that produce useful work by consumption of heat.\n\nOften nowadays, however, writers use the IUPAC convention by which the first law is formulated with work done on the system by its surroundings having a positive sign. With this now often used sign convention for work, the first law for a closed system may be written:\n\nThis convention follows physicists such as Max Planck, and considers all net energy transfers to the system as positive and all net energy transfers from the system as negative, irrespective of any use for the system as an engine or other device.\n\nWhen a system expands in a fictive quasistatic process, the work done by the system on the environment is the product, ,  of pressure, , and volume change, , whereas the work done \"on\" the system is .  Using either sign convention for work, the change in internal energy of the system is:\n\nwhere denotes the infinitesimal amount of heat supplied to the system from its surroundings.\n\nWork and heat are expressions of actual physical processes of supply or removal of energy, while the internal energy is a mathematical abstraction that keeps account of the exchanges of energy that befall the system. Thus the term heat for means \"that amount of energy added or removed by conduction of heat or by thermal radiation\", rather than referring to a form of energy within the system. Likewise, the term work energy for means \"that amount of energy gained or lost as the result of work\". Internal energy is a property of the system whereas work done and heat supplied are not. A significant result of this distinction is that a given internal energy change can be achieved by, in principle, many combinations of heat and work.\n\nThe law is of great importance and generality and is consequently thought of from several points of view. Most careful textbook statements of the law express it for closed systems. It is stated in several ways, sometimes even by the same author.\n\nFor the thermodynamics of closed systems, the distinction between transfers of energy as work and as heat is central and is within the scope of the present article. For the thermodynamics of open systems, such a distinction is beyond the scope of the present article, but some limited comments are made on it in the section below headed 'First law of thermodynamics for open systems'.\n\nThere are two main ways of stating a law of thermodynamics, physically or mathematically. They should be logically coherent and consistent with one another.\n\nAn example of a physical statement is that of Planck (1897/1903):\n\nThis physical statement is restricted neither to closed systems nor to systems with states that are strictly defined only for thermodynamic equilibrium; it has meaning also for open systems and for systems with states that are not in thermodynamic equilibrium.\n\nAn example of a mathematical statement is that of Crawford (1963):\n\nThis statement by Crawford, for , uses the sign convention of IUPAC, not that of Clausius. Though it does not explicitly say so, this statement refers to closed systems, and to internal energy defined for bodies in states of thermodynamic equilibrium, which possess well-defined temperatures.\n\nThe history of statements of the law for closed systems has two main periods, before and after the work of Bryan (1907), of Carathéodory (1909), and the approval of Carathéodory's work given by Born (1921). The earlier traditional versions of the law for closed systems are nowadays often considered to be out of date.\n\nCarathéodory's celebrated presentation of equilibrium thermodynamics refers to closed systems, which are allowed to contain several phases connected by internal walls of various kinds of impermeability and permeability (explicitly including walls that are permeable only to heat). Carathéodory's 1909 version of the first law of thermodynamics was stated in an axiom which refrained from defining or mentioning temperature or quantity of heat transferred. That axiom stated that the internal energy of a phase in equilibrium is a function of state, that the sum of the internal energies of the phases is the total internal energy of the system, and that the value of the total internal energy of the system is changed by the amount of work done adiabatically on it, considering work as a form of energy. That article considered this statement to be an expression of the law of conservation of energy for such systems. This version is nowadays widely accepted as authoritative, but is stated in slightly varied ways by different authors.\n\nSuch statements of the first law for closed systems assert the existence of internal energy as a function of state defined in terms of adiabatic work. Thus heat is not defined calorimetrically or as due to temperature difference. It is defined as a residual difference between change of internal energy and work done on the system, when that work does not account for the whole of the change of internal energy and the system is not adiabatically isolated.\n\nThe 1909 Carathéodory statement of the law in axiomatic form does not mention heat or temperature, but the equilibrium states to which it refers are explicitly defined by variable sets that necessarily include \"non-deformation variables\", such as pressures, which, within reasonable restrictions, can be rightly interpreted as empirical temperatures, and the walls connecting the phases of the system are explicitly defined as possibly impermeable to heat or permeable only to heat.\n\nAccording to Münster (1970), \"A somewhat unsatisfactory aspect of Carathéodory's theory is that a consequence of the Second Law must be considered at this point [in the statement of the first law], i.e. that it is not always possible to reach any state 2 from any other state 1 by means of an adiabatic process.\" Münster instances that no adiabatic process can reduce the internal energy of a system at constant volume. Carathéodory's paper asserts that its statement of the first law corresponds exactly to Joule's experimental arrangement, regarded as an instance of adiabatic work. It does not point out that Joule's experimental arrangement performed essentially irreversible work, through friction of paddles in a liquid, or passage of electric current through a resistance inside the system, driven by motion of a coil and inductive heating, or by an external current source, which can access the system only by the passage of electrons, and so is not strictly adiabatic, because electrons are a form of matter, which cannot penetrate adiabatic walls. The paper goes on to base its main argument on the possibility of quasi-static adiabatic work, which is essentially reversible. The paper asserts that it will avoid reference to Carnot cycles, and then proceeds to base its argument on cycles of forward and backward quasi-static adiabatic stages, with isothermal stages of zero magnitude.\n\nSometimes the concept of internal energy is not made explicit in the statement.\n\nSometimes the existence of the internal energy is made explicit but work is not explicitly mentioned in the statement of the first postulate of thermodynamics. Heat supplied is then defined as the residual change in internal energy after work has been taken into account, in a non-adiabatic process.\n\nA respected modern author states the first law of thermodynamics as \"Heat is a form of energy\", which explicitly mentions neither internal energy nor adiabatic work. Heat is defined as energy transferred by thermal contact with a reservoir, which has a temperature, and is generally so large that addition and removal of heat do not alter its temperature. A current student text on chemistry defines heat thus: \"\"heat\" is the exchange of thermal energy between a system and its surroundings caused by a temperature difference.\" The author then explains how heat is defined or measured by calorimetry, in terms of heat capacity, specific heat capacity, molar heat capacity, and temperature.\n\nA respected text disregards the Carathéodory's exclusion of mention of heat from the statement of the first law for closed systems, and admits heat calorimetrically defined along with work and internal energy. Another respected text defines heat exchange as determined by temperature difference, but also mentions that the Born (1921) version is \"completely rigorous\". These versions follow the traditional approach that is now considered out of date, exemplified by that of Planck (1897/1903).\n\nThe first law of thermodynamics for closed systems was originally induced from empirically observed evidence, including calorimetric evidence. It is nowadays, however, taken to provide the definition of heat via the law of conservation of energy and the definition of work in terms of changes in the external parameters of a system. The original discovery of the law was gradual over a period of perhaps half a century or more, and some early studies were in terms of cyclic processes.\n\nThe following is an account in terms of changes of state of a closed system through compound processes that are not necessarily cyclic. This account first considers processes for which the first law is easily verified because of their simplicity, namely adiabatic processes (in which there is no transfer as heat) and adynamic processes (in which there is no transfer as work).\n\nIn an adiabatic process, there is transfer of energy as work but not as heat. For all adiabatic process that takes a system from a given initial state to a given final state, irrespective of how the work is done, the respective eventual total quantities of energy transferred as work are one and the same, determined just by the given initial and final states. The work done on the system is defined and measured by changes in mechanical or quasi-mechanical variables external to the system. Physically, adiabatic transfer of energy as work requires the existence of adiabatic enclosures.\n\nFor instance, in Joule's experiment, the initial system is a tank of water with a paddle wheel inside. If we isolate the tank thermally, and move the paddle wheel with a pulley and a weight, we can relate the increase in temperature with the distance descended by the mass. Next, the system is returned to its initial state, isolated again, and the same amount of work is done on the tank using different devices (an electric motor, a chemical battery, a spring...). In every case, the amount of work can be measured independently. The return to the initial state is not conducted by doing adiabatic work on the system. The evidence shows that the final state of the water (in particular, its temperature and volume) is the same in every case. It is irrelevant if the work is electrical, mechanical, chemical... or if done suddenly or slowly, as long as it is performed in an adiabatic way, that is to say, without heat transfer into or out of the system.\n\nEvidence of this kind shows that to increase the temperature of the water in the tank, the qualitative kind of adiabatically performed work does not matter. No qualitative kind of adiabatic work has ever been observed to decrease the temperature of the water in the tank.\n\nA change from one state to another, for example an increase of both temperature and volume, may be conducted in several stages, for example by externally supplied electrical work on a resistor in the body, and adiabatic expansion allowing the body to do work on the surroundings. It needs to be shown that the time order of the stages, and their relative magnitudes, does not affect the amount of adiabatic work that needs to be done for the change of state. According to one respected scholar: \"Unfortunately, it does not seem that experiments of this kind have ever been carried out carefully. ... We must therefore admit that the statement which we have enunciated here, and which is equivalent to the first law of thermodynamics, is not well founded on direct experimental evidence.\" Another expression of this view is \"... no systematic precise experiments to verify this generalization directly have ever been attempted.\"\n\nThis kind of evidence, of independence of sequence of stages, combined with the above-mentioned evidence, of independence of qualitative kind of work, would show the existence of an important state variable that corresponds with adiabatic work, but not that such a state variable represented a conserved quantity. For the latter, another step of evidence is needed, which may be related to the concept of reversibility, as mentioned below.\n\nThat important state variable was first recognized and denoted formula_8 by Clausius in 1850, but he did not then name it, and he defined it in terms not only of work but also of heat transfer in the same process. It was also independently recognized in 1850 by Rankine, who also denoted it formula_8 ; and in 1851 by Kelvin who then called it \"mechanical energy\", and later \"intrinsic energy\". In 1865, after some hestitation, Clausius began calling his state function formula_8 \"energy\". In 1882 it was named as the \"internal energy\" by Helmholtz. If only adiabatic processes were of interest, and heat could be ignored, the concept of internal energy would hardly arise or be needed. The relevant physics would be largely covered by the concept of potential energy, as was intended in the 1847 paper of Helmholtz on the principle of conservation of energy, though that did not deal with forces that cannot be described by a potential, and thus did not fully justify the principle. Moreover, that paper was critical of the early work of Joule that had by then been performed. A great merit of the internal energy concept is that it frees thermodynamics from a restriction to cyclic processes, and allows a treatment in terms of thermodynamic states.\n\nIn an adiabatic process, adiabatic work takes the system either from a reference state formula_11 with internal energy formula_12 to an arbitrary one formula_13 with internal energy formula_14, or from the state formula_13 to the state formula_11:\n\nExcept under the special, and strictly speaking, fictional, condition of reversibility, only one of the processes  formula_18   or  formula_19  is empirically feasible by a simple application of externally supplied work. The reason for this is given as the second law of thermodynamics and is not considered in the present article.\n\nThe fact of such irreversibility may be dealt with in two main ways, according to different points of view:\n\n\n\nThe formula (1) above allows that to go by processes of quasi-static adiabatic work from the state formula_13 to the state formula_22 we can take a path that goes through the reference state formula_11, since the quasi-static adiabatic work is independent of the path\n\nThis kind of empirical evidence, coupled with theory of this kind, largely justifies the following statement:\n\nA complementary observable aspect of the first law is about heat transfer. Adynamic transfer of energy as heat can be measured empirically by changes in the surroundings of the system of interest by calorimetry. This again requires the existence of adiabatic enclosure of the entire process, system and surroundings, though the separating wall between the surroundings and the system is thermally conductive or radiatively permeable, not adiabatic. A calorimeter can rely on measurement of sensible heat, which requires the existence of thermometers and measurement of temperature change in bodies of known sensible heat capacity under specified conditions; or it can rely on the measurement of latent heat, through measurement of masses of material that change phase, at temperatures fixed by the occurrence of phase changes under specified conditions in bodies of known latent heat of phase change. The calorimeter can be calibrated by adiabatically doing externally determined work on it. The most accurate method is by passing an electric current from outside through a resistance inside the calorimeter. The calibration allows comparison of calorimetric measurement of quantity of heat transferred with quantity of energy transferred as work. According to one textbook, \"The most common device for measuring formula_26 is an adiabatic bomb calorimeter.\" According to another textbook, \"Calorimetry is widely used in present day laboratories.\" According to one opinion, \"Most thermodynamic data come from calorimetry...\" According to another opinion, \"The most common method of measuring \"heat\" is with a calorimeter.\"\n\nWhen the system evolves with transfer of energy as heat, without energy being transferred as work, in an adynamic process, the heat transferred to the system is equal to the increase in its internal energy:\n\nHeat transfer is practically reversible when it is driven by practically negligibly small temperature gradients. Work transfer is practically reversible when it occurs so slowly that there are no frictional effects within the system; frictional effects outside the system should also be zero if the process is to be globally reversible. For a particular reversible process in general, the work done reversibly on the system, formula_28, and the heat transferred reversibly to the system, formula_29 are not required to occur respectively adiabatically or adynamically, but they must belong to the same particular process defined by its particular reversible path, formula_30, through the space of thermodynamic states. Then the work and heat transfers can occur and be calculated simultaneously.\n\nPutting the two complementary aspects together, the first law for a particular reversible process can be written\n\nThis combined statement is the expression the first law of thermodynamics for reversible processes for closed systems.\n\nIn particular, if no work is done on a thermally isolated closed system we have\n\nThis is one aspect of the law of conservation of energy and can be stated:\n\nIf, in a process of change of state of a closed system, the energy transfer is not under a practically zero temperature gradient and practically frictionless, then the process is irreversible. Then the heat and work transfers may be difficult to calculate, and irreversible thermodynamics is called for. Nevertheless, the first law still holds and provides a check on the measurements and calculations of the work done irreversibly on the system, formula_33, and the heat transferred irreversibly to the system, formula_34, which belong to the same particular process defined by its particular irreversible path, formula_35, through the space of thermodynamic states.\n\nThis means that the internal energy formula_8 is a function of state and that the internal energy change formula_26 between two states is a function only of the two states.\n\nThe first law of thermodynamics is so general that its predictions cannot all be directly tested. In many properly conducted experiments it has been precisely supported, and never violated. Indeed, within its scope of applicability, the law is so reliably established, that, nowadays, rather than experiment being considered as testing the accuracy of the law, it is more practical and realistic to think of the law as testing the accuracy of experiment. An experimental result that seems to violate the law may be assumed to be inaccurate or wrongly conceived, for example due to failure to account for an important physical factor. Thus, some may regard it as a principle more abstract than a law.\n\nWhen the heat and work transfers in the equations above are infinitesimal in magnitude, they are often denoted by , rather than exact differentials denoted by , as a reminder that heat and work do not describe the \"state\" of any system. The integral of an inexact differential depends upon the particular path taken through the space of thermodynamic parameters while the integral of an exact differential depends only upon the initial and final states. If the initial and final states are the same, then the integral of an inexact differential may or may not be zero, but the integral of an exact differential is always zero. The path taken by a thermodynamic system through a chemical or physical change is known as a thermodynamic process.\n\nThe first law for a closed homogeneous system may be stated in terms that include concepts that are established in the second law. The internal energy may then be expressed as a function of the system's defining state variables , entropy, and , volume: . In these terms, , the system's temperature, and , its pressure, are partial derivatives of with respect to and . These variables are important throughout thermodynamics, though not necessary for the statement of the first law. Rigorously, they are defined only when the system is in its own state of internal thermodynamic equilibrium. For some purposes, the concepts provide good approximations for scenarios sufficiently near to the system's internal thermodynamic equilibrium.\n\nThe first law requires that:\n\nThen, for the fictive case of a reversible process, can be written in terms of exact differentials. One may imagine reversible changes, such that there is at each instant negligible departure from thermodynamic equilibrium within the system. This excludes isochoric work. Then, mechanical work is given by and the quantity of heat added can be expressed as . For these conditions\n\nWhile this has been shown here for reversible changes, it is valid in general, as can be considered as a thermodynamic state function of the defining state variables and :\n\nEquation (2) is known as the fundamental thermodynamic relation for a closed system in the energy representation, for which the defining state variables are and , with respect to which and are partial derivatives of . It is only in the fictive reversible case, when isochoric work is excluded, that the work done and heat transferred are given by and .\n\nIn the case of a closed system in which the particles of the system are of different types and, because chemical reactions may occur, their respective numbers are not necessarily constant, the fundamental thermodynamic relation for d\"U\" becomes:\n\nwhere d\"N\" is the (small) increase in amount of type-i particles in the reaction, and \"μ\" is known as the chemical potential of the type-i particles in the system. If d\"N\" is expressed in mol then \"μ\" is expressed in J/mol. If the system has more external mechanical variables than just the volume that can change, the fundamental thermodynamic relation further generalizes to:\n\nHere the \"X\" are the generalized forces corresponding to the external variables \"x\". The parameters \"X\" are independent of the size of the system and are called intensive parameters and the \"x\" are proportional to the size and called extensive parameters.\n\nFor an open system, there can be transfers of particles as well as energy into or out of the system during a process. For this case, the first law of thermodynamics still holds, in the form that the internal energy is a function of state and the change of internal energy in a process is a function only of its initial and final states, as noted in the section below headed First law of thermodynamics for open systems.\n\nA useful idea from mechanics is that the energy gained by a particle is equal to the force applied to the particle multiplied by the displacement of the particle while that force is applied. Now consider the first law without the heating term: d\"U\" = -\"P\"d\"V\". The pressure \"P\" can be viewed as a force (and in fact has units of force per unit area) while d\"V\"is the displacement (with units of distance times area). We may say, with respect to this work term, that a pressure difference forces a transfer of volume, and that the product of the two (work) is the amount of energy transferred out of the system as a result of the process. If one were to make this term negative then this would be the work done on the system.\n\nIt is useful to view the \"T\"d\"S\" term in the same light: here the temperature is known as a \"generalized\" force (rather than an actual mechanical force) and the entropy is a generalized displacement.\n\nSimilarly, a difference in chemical potential between groups of particles in the system drives a chemical reaction that changes the numbers of particles, and the corresponding product is the amount of chemical potential energy transformed in process. For example, consider a system consisting of two phases: liquid water and water vapor. There is a generalized \"force\" of evaporation that drives water molecules out of the liquid. There is a generalized \"force\" of condensation that drives vapor molecules out of the vapor. Only when these two \"forces\" (or chemical potentials) are equal is there equilibrium, and the net rate of transfer zero.\n\nThe two thermodynamic parameters that form a generalized force-displacement pair are called \"conjugate variables\". The two most familiar pairs are, of course, pressure-volume, and temperature-entropy.\n\nClassical thermodynamics is initially focused on closed homogeneous systems (e.g. Planck 1897/1903), which might be regarded as 'zero-dimensional' in the sense that they have no spatial variation. But it is desired to study also systems with distinct internal motion and spatial inhomogeneity. For such systems, the principle of conservation of energy is expressed in terms not only of internal energy as defined for homogeneous systems, but also in terms of kinetic energy and potential energies of parts of the inhomogeneous system with respect to each other and with respect to long-range external forces. How the total energy of a system is allocated between these three more specific kinds of energy varies according to the purposes of different writers; this is because these components of energy are to some extent mathematical artefacts rather than actually measured physical quantities. For any closed homogeneous component of an inhomogeneous closed system, if formula_44 denotes the total energy of that component system, one may write\n\nwhere formula_46 and formula_47 denote respectively the total kinetic energy and the total potential energy of the component closed homogeneous system, and formula_8 denotes its internal energy.\n\nPotential energy can be exchanged with the surroundings of the system when the surroundings impose a force field, such as gravitational or electromagnetic, on the system.\n\nA compound system consisting of two interacting closed homogeneous component subsystems has a potential energy of interaction formula_49 between the subsystems. Thus, in an obvious notation, one may write\n\nThe quantity formula_49 in general lacks an assignment to either subsystem in a way that is not arbitrary, and this stands in the way of a general non-arbitrary definition of transfer of energy as work. On occasions, authors make their various respective arbitrary assignments.\n\nThe distinction between internal and kinetic energy is hard to make in the presence of turbulent motion within the system, as friction gradually dissipates macroscopic kinetic energy of localised bulk flow into molecular random motion of molecules that is classified as internal energy. The rate of dissipation by friction of kinetic energy of localised bulk flow into internal energy, whether in turbulent or in streamlined flow, is an important quantity in non-equilibrium thermodynamics. This is a serious difficulty for attempts to define entropy for time-varying spatially inhomogeneous systems.\n\nFor the first law of thermodynamics, there is no trivial passage of physical conception from the closed system view to an open system view. For closed systems, the concepts of an adiabatic enclosure and of an adiabatic wall are fundamental. Matter and internal energy cannot permeate or penetrate such a wall. For an open system, there is a wall that allows penetration by matter. In general, matter in diffusive motion carries with it some internal energy, and some microscopic potential energy changes accompany the motion. An open system is not adiabatically enclosed.\n\nThere are some cases in which a process for an open system can, for particular purposes, be considered as if it were for a closed system. In an open system, by definition hypothetically or potentially, matter can pass between the system and its surroundings. But when, in a particular case, the process of interest involves only hypothetical or potential but no actual passage of matter, the process can be considered as if it were for a closed system.\n\nSince the revised and more rigorous definition of the internal energy of a closed system rests upon the possibility of processes by which adiabatic work takes the system from one state to another, this leaves a problem for the definition of internal energy for an open system, for which adiabatic work is not in general possible. According to Max Born, the transfer of matter and energy across an open connection \"cannot be reduced to mechanics\". In contrast to the case of closed systems, for open systems, in the presence of diffusion, there is no unconstrained and unconditional physical distinction between convective transfer of internal energy by bulk flow of matter, the transfer of internal energy without transfer of matter (usually called heat conduction and work transfer), and change of various potential energies. The older traditional way and the conceptually revised (Carathéodory) way agree that there is no physically unique definition of heat and work transfer processes between open systems.\n\nIn particular, between two otherwise isolated open systems an adiabatic wall is by definition impossible. This problem is solved by recourse to the principle of conservation of energy. This principle allows a composite isolated system to be derived from two other component non-interacting isolated systems, in such a way that the total energy of the composite isolated system is equal to the sum of the total energies of the two component isolated systems. Two previously isolated systems can be subjected to the thermodynamic operation of placement between them of a wall permeable to matter and energy, followed by a time for establishment of a new thermodynamic state of internal equilibrium in the new single unpartitioned system. The internal energies of the initial two systems and of the final new system, considered respectively as closed systems as above, can be measured. Then the law of conservation of energy requires that\n\nwhere and denote the changes in internal energy of the system and of its surroundings respectively. This is a statement of the first law of thermodynamics for a transfer between two otherwise isolated open systems, that fits well with the conceptually revised and rigorous statement of the law stated above.\n\nFor the thermodynamic operation of adding two systems with internal energies and , to produce a new system with internal energy , one may write ; the reference states for , and should be specified accordingly, maintaining also that the internal energy of a system be proportional to its mass, so that the internal energies are extensive variables.\n\nThere is a sense in which this kind of additivity expresses a fundamental postulate that goes beyond the simplest ideas of classical closed system thermodynamics; the extensivity of some variables is not obvious, and needs explicit expression; indeed one author goes so far as to say that it could be recognized as a fourth law of thermodynamics, though this is not repeated by other authors.\n\nAlso of course\n\nwhere and denote the changes in mole number of a component substance of the system and of its surroundings respectively. This is a statement of the law of conservation of mass.\n\nA system connected to its surroundings only through contact by a single permeable wall, but otherwise isolated, is an open system. If it is initially in a state of contact equilibrium with a surrounding subsystem, a thermodynamic process of transfer of matter can be made to occur between them if the surrounding subsystem is subjected to some thermodynamic operation, for example, removal of a partition between it and some further surrounding subsystem. The removal of the partition in the surroundings initiates a process of exchange between the system and its contiguous surrounding subsystem.\n\nAn example is evaporation. One may consider an open system consisting of a collection of liquid, enclosed except where it is allowed to evaporate into or to receive condensate from its vapor above it, which may be considered as its contiguous surrounding subsystem, and subject to control of its volume and temperature.\n\nA thermodynamic process might be initiated by a thermodynamic operation in the surroundings, that mechanically increases in the controlled volume of the vapor. Some mechanical work will be done within the surroundings by the vapor, but also some of the parent liquid will evaporate and enter the vapor collection which is the contiguous surrounding subsystem. Some internal energy will accompany the vapor that leaves the system, but it will not make sense to try to uniquely identify part of that internal energy as heat and part of it as work. Consequently, the energy transfer that accompanies the transfer of matter between the system and its surrounding subsystem cannot be uniquely split into heat and work transfers to or from the open system. The component of total energy transfer that accompanies the transfer of vapor into the surrounding subsystem is customarily called 'latent heat of evaporation', but this use of the word heat is a quirk of customary historical language, not in strict compliance with the thermodynamic definition of transfer of energy as heat. In this example, kinetic energy of bulk flow and potential energy with respect to long-range external forces such as gravity are both considered to be zero. The first law of thermodynamics refers to the change of internal energy of the open system, between its initial and final states of internal equilibrium.\n\nAn open system can be in contact equilibrium with several other systems at once.\n\nThis includes cases in which there is contact equilibrium between the system, and several subsystems in its surroundings, including separate connections with subsystems through walls that are permeable to the transfer of matter and internal energy as heat and allowing friction of passage of the transferred matter, but immovable, and separate connections through adiabatic walls with others, and separate connections through diathermic walls impermeable to matter with yet others. Because there are physically separate connections that are permeable to energy but impermeable to matter, between the system and its surroundings, energy transfers between them can occur with definite heat and work characters. Conceptually essential here is that the internal energy transferred with the transfer of matter is measured by a variable that is mathematically independent of the variables that measure heat and work.\n\nWith such independence of variables, the total increase of internal energy in the process is then determined as the sum of the internal energy transferred from the surroundings with the transfer of matter through the walls that are permeable to it, and of the internal energy transferred to the system as heat through the diathermic walls, and of the energy transferred to the system as work through the adiabatic walls, including the energy transferred to the system by long-range forces. These simultaneously transferred quantities of energy are defined by events in the surroundings of the system. Because the internal energy transferred with matter is not in general uniquely resolvable into heat and work components, the total energy transfer cannot in general be uniquely resolved into heat and work components. Under these conditions, the following formula can describe the process in terms of externally defined thermodynamic variables, as a statement of the first law of thermodynamics:\n\nwhere Δ\"U\" denotes the change of internal energy of the system, and denotes the change of internal energy of the of the surrounding subsystems that are in open contact with the system, due to transfer between the system and that surrounding subsystem, and denotes the internal energy transferred as heat from the heat reservoir of the surroundings to the system, and denotes the energy transferred from the system to the surrounding subsystems that are in adiabatic connection with it. The case of a wall that is permeable to matter and can move so as to allow transfer of energy as work is not considered here.\n\nIf the system is described by the energetic fundamental equation, \"U\" = \"U\"(\"S\", \"V\", \"N\"), and if the process can be described in the quasi-static formalism, in terms of the internal state variables of the system, then the process can also be described by a combination of the first and second laws of thermodynamics, by the formula\n\nwhere there are \"n\" chemical constituents of the system and permeably connected surrounding subsystems, and where \"T\", \"S\", \"P\", \"V\", \"N\", and \"μ\", are defined as above.\n\nFor a general natural process, there is no immediate term-wise correspondence between equations (3) and (4), because they describe the process in different conceptual frames.\n\nNevertheless, a conditional correspondence exists. There are three relevant kinds of wall here: purely diathermal, adiabatic, and permeable to matter. If two of those kinds of wall are sealed off, leaving only one that permits transfers of energy, as work, as heat, or with matter, then the remaining permitted terms correspond precisely. If two of the kinds of wall are left unsealed, then energy transfer can be shared between them, so that the two remaining permitted terms do not correspond precisely.\n\nFor the special fictive case of quasi-static transfers, there is a simple correspondence. For this, it is supposed that the system has multiple areas of contact with its surroundings. There are pistons that allow adiabatic work, purely diathermal walls, and open connections with surrounding subsystems of completely controllable chemical potential (or equivalent controls for charged species). Then, for a suitable fictive quasi-static transfer, one can write\n\nFor fictive quasi-static transfers for which the chemical potentials in the connected surrounding subsystems are suitably controlled, these can be put into equation (4) to yield\n\nThe reference does not actually write equation (5), but what it does write is fully compatible with it. Another helpful account is given by Tschoegl.\n\nThere are several other accounts of this, in apparent mutual conflict.\n\nThe transfer of energy between an open system and a single contiguous subsystem of its surroundings is considered also in non-equilibrium thermodynamics. The problem of definition arises also in this case. It may be allowed that the wall between the system and the subsystem is not only permeable to matter and to internal energy, but also may be movable so as to allow work to be done when the two systems have different pressures. In this case, the transfer of energy as heat is not defined.\n\nMethods for study of non-equilibrium processes mostly deal with spatially continuous flow systems. In this case, the open connection between system and surroundings is usually taken to fully surround the system, so that there are no separate connections impermeable to matter but permeable to heat. Except for the special case mentioned above when there is no actual transfer of matter, which can be treated as if for a closed system, in strictly defined thermodynamic terms, it follows that transfer of energy as heat is not defined. In this sense, there is no such thing as 'heat flow' for a continuous-flow open system. Properly, for closed systems, one speaks of transfer of internal energy as heat, but in general, for open systems, one can speak safely only of transfer of internal energy. A factor here is that there are often cross-effects between distinct transfers, for example that transfer of one substance may cause transfer of another even when the latter has zero chemical potential gradient.\n\nUsually transfer between a system and its surroundings applies to transfer of a state variable, and obeys a balance law, that the amount lost by the donor system is equal to the amount gained by the receptor system. Heat is not a state variable. For his 1947 definition of \"heat transfer\" for discrete open systems, the author Prigogine carefully explains at some length that his definition of it does not obey a balance law. He describes this as paradoxical.\n\nThe situation is clarified by Gyarmati, who shows that his definition of \"heat transfer\", for continuous-flow systems, really refers not specifically to heat, but rather to transfer of internal energy, as follows. He considers a conceptual small cell in a situation of continuous-flow as a system defined in the so-called Lagrangian way, moving with the local center of mass. The flow of matter across the boundary is zero when considered as a flow of total mass. Nevertheless, if the material constitution is of several chemically distinct components that can diffuse with respect to one another, the system is considered to be open, the diffusive flows of the components being defined with respect to the center of mass of the system, and balancing one another as to mass transfer. Still there can be a distinction between bulk flow of internal energy and diffusive flow of internal energy in this case, because the internal energy density does not have to be constant per unit mass of material, and allowing for non-conservation of internal energy because of local conversion of kinetic energy of bulk flow to internal energy by viscosity.\n\nGyarmati shows that his definition of \"the heat flow vector\" is strictly speaking a definition of flow of internal energy, not specifically of heat, and so it turns out that his use here of the word heat is contrary to the strict thermodynamic definition of heat, though it is more or less compatible with historical custom, that often enough did not clearly distinguish between heat and internal energy; he writes \"that this relation must be considered to be the exact definition of the concept of heat flow, fairly loosely used in experimental physics and heat technics.\" Apparently in a different frame of thinking from that of the above-mentioned paradoxical usage in the earlier sections of the historic 1947 work by Prigogine, about discrete systems, this usage of Gyarmati is consistent with the later sections of the same 1947 work by Prigogine, about continuous-flow systems, which use the term \"heat flux\" in just this way. This usage is also followed by Glansdorff and Prigogine in their 1971 text about continuous-flow systems. They write: \"Again the flow of internal energy may be split into a convection flow and a conduction flow. This conduction flow is by definition the heat flow . Therefore: where denotes the [internal] energy per unit mass. [These authors actually use the symbols and to denote internal energy but their notation has been changed here to accord with the notation of the present article. These authors actually use the symbol to refer to total energy, including kinetic energy of bulk flow.]\" This usage is followed also by other writers on non-equilibrium thermodynamics such as Lebon, Jou, and Casas-Vásquez, and de Groot and Mazur. This usage is described by Bailyn as stating the non-convective flow of internal energy, and is listed as his definition number 1, according to the first law of thermodynamics. This usage is also followed by workers in the kinetic theory of gases. This is not the \"ad hoc\" definition of \"reduced heat flux\" of Haase.\n\nIn the case of a flowing system of only one chemical constituent, in the Lagrangian representation, there is no distinction between bulk flow and diffusion of matter. Moreover, the flow of matter is zero into or out of the cell that moves with the local center of mass. In effect, in this description, one is dealing with a system effectively closed to the transfer of matter. But still one can validly talk of a distinction between bulk flow and diffusive flow of internal energy, the latter driven by a temperature gradient within the flowing material, and being defined with respect to the local center of mass of the bulk flow. In this case of a virtually closed system, because of the zero matter transfer, as noted above, one can safely distinguish between transfer of energy as work, and transfer of internal energy as heat.\n\n\n\n\n"}
{"id": "7513712", "url": "https://en.wikipedia.org/wiki?curid=7513712", "title": "Flyback chronograph", "text": "Flyback chronograph\n\nA flyback chronograph is a complication watch, in which you can use the reset function without the need to first stop the chronograph. In regular chronographs you need to stop, reset, and restart the chronograph if you want to time an event after you have started the chronograph.\n\nThe flyback function is also known by some other names:\n\nThe flyback function is a complication inspired by everyday life, as are some other complications: world time, universal time, power reserve.\n\nMost flyback chronographs are constructed with the usual crown at 3 o'clock and 2 pushpieces at 2 and 4 o'clock. Usually the flyback function is controlled by the button at 4 o'clock whereas the one at 2 o'clock is used to stop the chronograph.\n\nMany chronographs are equipped with the flyback function, especially watches meant for pilots. It is much easier from the point of view of time saving, to instantly restart the chronograph with one push of the button, instead of three.\n\nThe appearance of the flyback function was catalyzed by the first watch with a separate chronograph button (the stop and reset functions were previously controlled by the winding-crown). This watch appeared in 1923 and was developed by Breitling. \n\nLater, the chronographs gained their current form. In 1934 the Breitling chronographs received the second chronograph button with the function of returning the seconds hand to zero. Thus it became possible to measure short intervals of time using the add function. However the patent of the flyback chronograph belongs to Longines with its first flyback chronograph dating back to 1936.\n\n"}
{"id": "25946936", "url": "https://en.wikipedia.org/wiki?curid=25946936", "title": "Foundered strata", "text": "Foundered strata\n\nFoundered strata is a term used by the British Geological Survey in its maps and geological memoirs, and indeed by other authors, to describe rock strata which have collapsed due, for example, to the dissolution of underlying strata. Foundered strata may retain original bedding more or less intact or they may assume a chaotic structure in the process of collapse or some intermediate state of disruption.\n\nExamples of foundered strata are found along the northern margin of the outcrop of the Twrch Sandstone (the former Basal Grit of the Millstone Grit) in the Brecon Beacons National Park of South Wales. The immediately underlying Carboniferous Limestone has dissolved away in numerous locations such as at Pwll Byfre and resulted in the wholesale collapse of the sandstone beds above it. It is thought that the sandstone has been let down several hundred feet in places due to the removal by solution of limestone strata.\n\n"}
{"id": "33488832", "url": "https://en.wikipedia.org/wiki?curid=33488832", "title": "Frederick Wilfrid Lancaster", "text": "Frederick Wilfrid Lancaster\n\nFrederick Wilfrid (\"Wilf\") Lancaster (September 4, 1933 – August 25, 2013) was a British-American information scientist. He immigrated to the USA in 1959 and worked as information specialist for the National Library of Medicine in Bethesda, Maryland from 1965–68. He was a professor at the University of Illinois, Urbana from 1972-92 and professor emeritus from 1992-2013. He continued as an honored scholar after retirement speaking on the evolution of librarianship in the 20th and 21st century. Lancaster made notable achievements with early online retrieval systems, including evaluation studies of MEDLARS. He published broadly in Library and Information Science over a period of four decades and continuously emerged as a visionary leader in the field, where research, writing, and teaching earned him the highest honors in the profession. Lancaster excelled at many fronts: as scholar, educator, mentor, and writer.\n\n\n\n\n"}
{"id": "6760551", "url": "https://en.wikipedia.org/wiki?curid=6760551", "title": "Fundamentals of Stack Gas Dispersion", "text": "Fundamentals of Stack Gas Dispersion\n\nFundamentals of Stack Gas Dispersion is a book devoted to the fundamentals of air pollution dispersion modeling of continuous, buoyant pollution plumes from stationary point sources. The first edition was published in 1979. The current fourth edition was published in 2005.\n\nThe subjects covered in the book include atmospheric turbulence and stability classes, buoyant plume rise, Gaussian dispersion calculations and modeling, time-averaged concentrations, wind velocity profiles, fumigations, trapped plumes and gas flare stack plumes. The constraints and assumptions involved in the basic equations are fully explained.\n\nThe book has received favorable reviews, including a description of its \"simple straightforward explanations\" for a \"full course in single-source dispersive modeling\".\n\nThe book has been purchased in 84 countries and as of 2015 is available in 233 libraries worldwide. It has been referenced or cited as an educational resource more than 880 times in the technical literature and on the Internet, including 34 regulatory publications of state or national governmental agencies worldwide. It has also been used as recommended reading or a textbook in 61 university courses. \n\nThe book is now available only as a downloadable version in PDF format.\n\n\n\n\n"}
{"id": "57003665", "url": "https://en.wikipedia.org/wiki?curid=57003665", "title": "GPIb-IX-V", "text": "GPIb-IX-V\n\nThis transmembrane glycoprotein complex is composed of four subunits: GPIbα, GPIbβ, GPV and GPIX. Each of them has a variable number of leucine-rich repeats. GPIbα and GPIbβ are linked by disulfide bridges, while the GPV and GPIX associate non-covalently with the complex. GPIbα subunit bears the binding site for von Willebrand factor (vWF), α-thrombin, leukocyte integrin αMβ2 and P-selectin. The binding between GPIbα and vWF mediates the capture of platelets to the injured vascular wall. The deficiency in glycoprotein Ib-IX-V complex synthesis leads to Bernard-Soulier syndrome.\n"}
{"id": "42963936", "url": "https://en.wikipedia.org/wiki?curid=42963936", "title": "Grapevine yellow speckle viroid", "text": "Grapevine yellow speckle viroid\n\nThe Grapevine yellow speckle viroid is a type of grapevine viroid.\n\n"}
{"id": "14472512", "url": "https://en.wikipedia.org/wiki?curid=14472512", "title": "Heng-O Corona", "text": "Heng-O Corona\n\nHeng-O Corona is a corona in Guinevere Planitia on the planet Venus at Latitude 2° North, Longitude 355° East. It has a diameter of , and is the 2nd largest corona on Venus.\n\nIt is named for Heng O (also known as Chang'e), the Chinese goddess of the Moon. \n"}
{"id": "14229", "url": "https://en.wikipedia.org/wiki?curid=14229", "title": "Homeopathy", "text": "Homeopathy\n\nHomeopathy or homœopathy is a system of alternative medicine created in 1796 by Samuel Hahnemann, based on his doctrine of \"like cures like\" (\"similia similibus curentur\"), a claim that a substance that causes the symptoms of a disease in healthy people would cure similar symptoms in sick people. Homeopathy is a pseudoscience – a belief that is incorrectly presented as scientific. Homeopathic preparations are not effective for treating any condition; large-scale studies have found homeopathy to be no more effective than a placebo, indicating that any positive effects that follow treatment are due to factors such as normal recovery from illness, or regression toward the mean.\n\nHahnemann believed the underlying causes of disease were phenomena that he termed \"miasms\", and that homeopathic preparations addressed these. The preparations are manufactured using a process of homeopathic dilution, in which a chosen substance is repeatedly diluted in alcohol or distilled water, each time with the containing vessel being struck against an elastic material, commonly a leather-bound book. Dilution typically continues well past the point where no molecules of the original substance remain. Homeopaths select homeopathics by consulting reference books known as \"repertories\", and by considering the totality of the patient's symptoms, personal traits, physical and psychological state, and life history.\n\nHomeopathy is not a plausible system of treatment, as its dogmas about how drugs, illness, the human body, liquids and solutions operate are contradicted by a wide range of discoveries across biology, psychology, physics and chemistry made in the two centuries since its invention. Although some clinical trials produce positive results, multiple systematic reviews have shown that this is because of chance, flawed research methods, and reporting bias. Homeopathic practice has been criticized as unethical because it discourages the use of effective treatments, with the World Health Organization warning against using homeopathy to try to treat severe diseases such as HIV and malaria. The continued practice of homeopathy, despite a lack of evidence of efficacy, has led to it being characterized within the scientific and medical communities as nonsense, quackery, and a sham.\n\nThere have been four large scale assessments of homeopathy by national or international bodies: the Australian National Health and Medical Research Council; the United Kingdom's House of Commons Science and Technology Committee; the European Academies' Science Advisory Council; and the Swiss Federal Health Office. Each concluded that homeopathy is ineffective, and recommended against the practice receiving any further funding. The National Health Service in England has announced a policy of not funding homeopathic medicine because it is \"a misuse of resources\". They called on the UK Department of Health to add homeopathic remedies to the blacklist of forbidden prescription items, and the NHS ceased funding homeopathic remedies in November 2017.\n\nThe concept of \"like cures like\" may have been suggested by Hippocrates around 400 BC, when he prescribed a small dose of mandrake root to treat mania, knowing it produces mania in much larger doses. Similarly, in the 16th century, Paracelsus wrote \"similia similibus curantur\" (similar to the subjunctive form later used by Hahnemann), often translated as \"what makes a man ill also cures him\".\n\nIn the late 18th and 19th centuries, mainstream medicine used methods like bloodletting and purging, and administered complex mixtures, such as Venice treacle, which was made from 64 substances including opium, myrrh, and viper's flesh. These treatments often worsened symptoms and sometimes proved fatal. Hahnemann rejected these practices – which had been extolled for centuries – as irrational and inadvisable;\ninstead, he advocated the use of single drugs at lower doses and promoted an immaterial, vitalistic view of how living organisms function, believing that diseases have spiritual, as well as physical causes.\n\nThe term \"homeopathy\" was coined by Hahnemann and first appeared in print in 1807.\n\nHahnemann conceived of homeopathy while translating a medical treatise by the Scottish physician and chemist William Cullen into German. Being sceptical of Cullen's theory concerning cinchona's use for curing malaria, Hahnemann ingested some bark specifically to investigate what would happen. He experienced fever, shivering and joint pain: symptoms similar to those of malaria itself. From this, Hahnemann came to believe that all effective drugs produce symptoms in healthy individuals similar to those of the diseases that they treat, in accord with the \"law of similars\" that had been proposed by ancient physicians. An account of the effects of eating cinchona bark noted by Oliver Wendell Holmes, and published in 1861, failed to reproduce the symptoms Hahnemann reported. Hahnemann's law of similars is a postulate rather than a scientific law. This led to the name \"homeopathy\", which comes from the \"hómoios\", \"-like\" and \"páthos\", \"suffering\".\n\nSubsequent scientific work showed that cinchona cures malaria because it contains quinine, which kills the \"Plasmodium falciparum\" parasite that causes the disease; the mechanism of action is unrelated to Hahnemann's ideas.\n\nHahnemann began to test what effects substances produced in humans, a procedure that would later become known as \"homeopathic proving\". These tests required subjects to test the effects of ingesting substances by clearly recording all of their symptoms as well as the ancillary conditions under which they appeared. He published a collection of provings in 1805, and a second collection of 65 preparations appeared in his book, \"Materia Medica Pura\", in 1810.\n\nBecause Hahnemann believed that large doses of drugs that caused similar symptoms would only aggravate illness, he advocated extreme dilutions of the substances; he devised a technique for making dilutions that he believed would preserve a substance's therapeutic properties while removing its harmful effects. Hahnemann believed that this process aroused and enhanced \"the spirit-like medicinal powers of the crude substances\".\nHe gathered and published a complete overview of his new medical system in his 1810 book, \"The Organon of the Healing Art\", whose 6th edition, published in 1921, is still used by homeopaths today.\n\nIn the \"Organon\", Hahnemann introduced the concept of \"miasms\" as \"infectious principles\" underlying chronic disease. Hahnemann associated each miasm with specific diseases, and thought that initial exposure to miasms causes local symptoms, such as skin or venereal diseases. If, however, these symptoms were suppressed by medication, the cause went deeper and began to manifest itself as diseases of the internal organs. Homeopathy maintains that treating diseases by directly alleviating their symptoms, as is sometimes done in conventional medicine, is ineffective because all \"disease can generally be traced to some latent, deep-seated, underlying chronic, or inherited tendency\". The underlying imputed miasm still remains, and deep-seated ailments can be corrected only by removing the deeper disturbance of the vital force.\n\nHahnemann's hypotheses for the direct or remote cause of all chronic diseases (miasms) originally presented only three, psora (the itch), syphilis (venereal disease) or sycosis (fig-wart disease). Of these three the most important was \"psora\" (Greek for \"itch\"), described as being related to any itching diseases of the skin, supposed to be derived from suppressed scabies, and claimed to be the foundation of many further disease conditions. Hahnemann believed psora to be the cause of such diseases as epilepsy, cancer, jaundice, deafness, and cataracts.\nSince Hahnemann's time, other miasms have been proposed, some replacing one or more of psora's proposed functions, including tuberculosis and cancer miasms.\n\nThe law of susceptibility implies that a negative state of mind can attract hypothetical disease entities called \"miasms\" to invade the body and produce symptoms of diseases. Hahnemann rejected the notion of a disease as a separate thing or invading entity, and insisted it was always part of the \"living whole\". Hahnemann coined the expression \"allopathic medicine\", which was used to pejoratively refer to traditional Western medicine.\n\nHahnemann's miasm theory remains disputed and controversial within homeopathy even in modern times. The theory of miasms has been criticized as an explanation developed by Hahnemann to preserve the system of homeopathy in the face of treatment failures, and for being inadequate to cover the many hundreds of sorts of diseases, as well as for failing to explain disease predispositions, as well as genetics, environmental factors, and the unique disease history of each patient.\n\nHomeopathy achieved its greatest popularity in the 19th century. It was introduced to the United States in 1825 by Hans Birch Gram, a student of Hahnemann. The first homeopathic school in the US opened in 1835, and in 1844, the first US national medical association, the American Institute of Homeopathy, was established. Throughout the 19th century, dozens of homeopathic institutions appeared in Europe and the United States, and by 1900, there were 22 homeopathic colleges and 15,000 practitioners in the United States. Because medical practice of the time relied on ineffective and often dangerous treatments, patients of homeopaths often had better outcomes than those of the doctors of the time. Homeopathic preparations, even if ineffective, would almost surely cause no harm, making the users of homeopathic preparations less likely to be killed by the treatment that was supposed to be helping them. The relative success of homeopathy in the 19th century may have led to the abandonment of the ineffective and harmful treatments of bloodletting and purging and to have begun the move towards more effective, science-based medicine.\nOne reason for the growing popularity of homeopathy was its apparent success in treating people suffering from infectious disease epidemics.\nDuring 19th-century epidemics of diseases such as cholera, death rates in homeopathic hospitals were often lower than in conventional hospitals, where the treatments used at the time were often harmful and did little or nothing to combat the diseases.\n\nFrom its inception, however, homeopathy was criticized by mainstream science. Sir John Forbes, physician to Queen Victoria, said in 1843 that the extremely small doses of homeopathy were regularly derided as useless, \"an outrage to human reason\". James Young Simpson said in 1853 of the highly diluted drugs: \"No poison, however strong or powerful, the billionth or decillionth of which would in the least degree affect a man or harm a fly.\"\n19th-century American physician and author Oliver Wendell Holmes, Sr. was also a vocal critic of homeopathy and published an essay in 1842 entitled \"Homœopathy and Its Kindred Delusions\". The members of the French Homeopathic Society observed in 1867 that some leading homeopathists of Europe not only were abandoning the practice of administering infinitesimal doses but were also no longer defending it. The last school in the US exclusively teaching homeopathy closed in 1920.\n\nAccording to , the Nazi regime in Germany was fascinated by homeopathy, and spent large sums of money on researching its mechanisms, but without gaining a positive result. Unschuld further argues that homeopathy never subsequently took root in the United States, but remained more deeply established in European thinking.\nIn the United States, the \"Food, Drug, and Cosmetic Act\" of 1938 (sponsored by Royal Copeland, a Senator from New York and homeopathic physician) recognized homeopathic preparations as drugs. In the 1950s, there were only 75 pure homeopaths practising in the U.S. However, by the mid to late 1970s, homeopathy made a significant comeback and sales of some homeopathic companies increased tenfold. Some homeopaths give credit for the revival to Greek homeopath George Vithoulkas, who performed a \"great deal of research to update the scenarios and refine the theories and practice of homeopathy\", beginning in the 1970s, but Ernst and Singh consider it to be linked to the rise of the New Age movement. Whichever is correct, mainstream pharmacy chains recognized the business potential of selling homeopathic preparations. The Food and Drug Administration held a hearing April 20 and 21, 2015, requesting public comment on regulation of homeopathic drugs. The FDA cited the growth of sales of over-the-counter homeopathic medicines, which was $2.7 billion for 2007.\n\nBruce Hood has argued that the increased popularity of homeopathy in recent times may be due to the comparatively long consultations practitioners are willing to give their patients, and to an irrational preference for \"natural\" products, which people think are the basis of homeopathic preparations.\n\nSince the beginning of the 21st century a series of meta analysis has further shown that therapeutic claims of Homeopathy lack scientific justification. In a 2010 report, the Science and Technology Committee of the United Kingdom House of Commons recommended that homeopathy should no longer be a beneficiary of NHS funding due its lack of scientific credibility; funding ceased in 2017. In March 2015, the National Health and Medical Research Council of Australia published an information paper on Homeopathy. The main findings of the report were 'there are no health conditions for which there is reliable evidence that homeopathy is effective\". Reactions to the report sparked world headlines which suggested that the NHMRC had found that homeopathy is not effective for all conditions.\n\nIn 2018, Australian pharmacies ignored recommendations for a homeopathic ban in the broader scope of the federal government accepting only three of the 45 recommendations made by the review of Pharmacy Remuneration and Regulation (which were delivered in September 2017 to Health Minister Greg Hunt).\n\nHomeopathic preparations are referred to as \"homeopathics\" or \"remedies\". Practitioners rely on two types of reference when prescribing: \"Materia Medica\" and repertories. A homeopathic \"materia medica\" is a collection of \"drug pictures\", organized alphabetically. These entries describe the symptom patterns associated with individual preparations. A homeopathic repertory is an index of disease symptoms that lists preparations associated with specific symptoms. In both cases different compilers may dispute particular inclusions. The first symptomatic homeopathic \"materia medica\" was arranged by Hahnemann. The first homeopathic repertory was Georg Jahr's \"Symptomenkodex\", published in German in 1835, and translated into English as the \"Repertory to the more Characteristic Symptoms of Materia Medica\" by Constantine Hering in 1838.\nThis version was less focused on disease categories and was the forerunner to later works by James Tyler Kent. Repertories, in particular, may be very large.\n\nHomeopathy uses animal, plant, mineral, and synthetic substances in its preparations, generally referring to them using Latin or faux-Latin names. Examples include \"arsenicum album\" (arsenic oxide), \"natrum muriaticum\" (sodium chloride or table salt), \"Lachesis muta\" (the venom of the bushmaster snake), \"opium\", and \"thyroidinum\" (thyroid hormone).\n\nSome homeopaths use so-called \"nosodes\" (from the Greek \"nosos\", disease) made from diseased or pathological products such as fecal, urinary, and respiratory discharges, blood, and tissue. Conversely, preparations made from \"healthy\" specimens are called \"sarcodes\".\n\nSome modern homeopaths use preparations they call \"imponderables\" because they do not originate from a substance but some other phenomenon presumed to have been \"captured\" by alcohol or lactose. Examples include X-rays\nand sunlight.\n\nOther minority practices include paper preparations, where the substance and dilution are written on pieces of paper and either pinned to the patients' clothing, put in their pockets, or placed under glasses of water that are then given to the patients, and the use of radionics to manufacture preparations. Such practices have been strongly criticized by classical homeopaths as unfounded, speculative, and verging upon magic and superstition.\n\nHahnemann found that undiluted doses caused reactions, sometimes dangerous ones, so specified that preparations be given at the lowest possible dose. He found that this reduced potency as well as side-effects, but formed the view that vigorous shaking and striking on an elastic surface – a process he termed \"Schütteln\", translated as \"succussion\" – nullified this. A common explanation for his settling on this process is said to be that he found preparations subjected to agitation in transit, such as in saddle bags or in a carriage, were more \"potent\". Hahnemann had a saddle-maker construct a special wooden striking board covered in leather on one side and stuffed with horsehair. Insoluble solids, such as granite, diamond, and platinum, are diluted by grinding them with lactose (\"trituration\").\n\nThe process of dilution and succussion is termed \"dynamization\" or \"potentization\" by homeopaths. In industrial manufacture this may be done by machine.\n\nSerial dilution is achieved by taking an amount of the mixture and adding solvent, but the \"Korsakovian\" method may also be used, whereby the vessel in which the preparations are manufactured is emptied, refilled with solvent, and the volume of fluid adhering to the walls of the vessel is deemed sufficient for the new batch. The Korsakovian method is sometimes referred to as K on the label of a homeopathic preparation, e.g. 200CK is a 200C preparation made using the Korsakovian method.\n\nFluxion and radionics methods of preparation do not require succussion. There are differences of opinion on the number and force of strikes, and some practitioners dispute the need for succussion at all while others reject the Korsakovian and other non-classical preparations. There are no laboratory assays and the importance and techniques for succussion cannot be determined with any certainty from the literature.\n\nThree main logarithmic potency scales are in regular use in homeopathy. Hahnemann created the \"centesimal\" or \"C scale\", diluting a substance by a factor of 100 at each stage. The centesimal scale was favoured by Hahnemann for most of his life.\n\nA 2C dilution requires a substance to be diluted to one part in 100, and then some of that diluted solution diluted by a further factor of 100.\n\nThis works out to one part of the original substance in 10,000 parts of the solution. A 6C dilution repeats this process six times, ending up with the original substance diluted by a factor of 100=10 (one part in one trillion or 1/1,000,000,000,000). Higher dilutions follow the same pattern.\n\nIn homeopathy, a solution that is more dilute is described as having a higher \"potency\", and more dilute substances are considered by homeopaths to be stronger and deeper-acting. The end product is often so diluted as to be indistinguishable from the diluent (pure water, sugar or alcohol). There is also a decimal potency scale (notated as \"X\" or \"D\") in which the preparation is diluted by a factor of 10 at each stage.\n\nHahnemann advocated 30C dilutions for most purposes (that is, dilution by a factor of 10). Hahnemann regularly used potencies up to 300C but opined that \"there must be a limit to the matter, it cannot go on indefinitely\".\n\nIn Hahnemann's time, it was reasonable to assume the preparations could be diluted indefinitely, as the concept of the atom or molecule as the smallest possible unit of a chemical substance was just beginning to be recognized.\n\nThe greatest dilution reasonably likely to contain even one molecule of the original substance is 12C.\nCritics and advocates of homeopathy alike commonly attempt to illustrate the dilutions involved in homeopathy with analogies.\nHahnemann is reported to have joked that a suitable procedure to deal with an epidemic would be to empty a bottle of poison into Lake Geneva, if it could be succussed 60 times.\nAnother example given by a critic of homeopathy states that a 12C solution is equivalent to a \"pinch of salt in both the North and South Atlantic Oceans\", which is approximately correct.\nOne-third of a drop of some original substance diluted into all the water on earth would produce a preparation with a concentration of about 13C. A popular homeopathic treatment for the flu is a 200C dilution of duck liver, marketed under the name Oscillococcinum. As there are only about 10 atoms in the entire observable universe, a dilution of one molecule in the observable universe would be about 40C. Oscillococcinum would thus require 10 more universes to simply have one molecule in the final substance.\nThe high dilutions characteristically used are often considered to be the most controversial and implausible aspect of homeopathy.\n\nNot all homeopaths advocate high dilutions. Preparations at concentrations below 4X are considered an important part of homeopathic heritage. Many of the early homeopaths were originally doctors and generally used lower dilutions such as \"3X\" or \"6X\", rarely going beyond \"12X\".\nThe split between lower and higher dilutions followed ideological lines.\nThose favouring low dilutions stressed pathology and a stronger link to conventional medicine, while those favouring high dilutions emphasized vital force, miasms and a spiritual interpretation of disease.\nSome products with such relatively lower dilutions continue to be sold, but like their counterparts, they have not been conclusively demonstrated to have any effect beyond that of a placebo.\n\nA homeopathic \"proving\" is the method by which the profile of a homeopathic preparation is determined.\n\nAt first Hahnemann used undiluted doses for provings, but he later advocated provings with preparations at a 30C dilution, and most modern provings are carried out using ultra-dilute preparations in which it is highly unlikely that any of the original molecules remain. During the proving process, Hahnemann administered preparations to healthy volunteers, and the resulting symptoms were compiled by observers into a \"drug picture\".\n\nThe volunteers were observed for months at a time and made to keep extensive journals detailing all of their symptoms at specific times throughout the day. They were forbidden from consuming coffee, tea, spices, or wine for the duration of the experiment; playing chess was also prohibited because Hahnemann considered it to be \"too exciting\", though they were allowed to drink beer and encouraged to exercise in moderation.\n\nAfter the experiments were over, Hahnemann made the volunteers take an oath swearing that what they reported in their journals was the truth, at which time he would interrogate them extensively concerning their symptoms.\n\nProvings are claimed to have been important in the development of the clinical trial, due to their early use of simple control groups, systematic and quantitative procedures, and some of the first application of statistics in medicine. The lengthy records of self-experimentation by homeopaths have occasionally proven useful in the development of modern drugs: For example, evidence that nitroglycerin might be useful as a treatment for angina was discovered by looking through homeopathic provings, though homeopaths themselves never used it for that purpose at that time.\nThe first recorded provings were published by Hahnemann in his 1796 \"Essay on a New Principle\".\nHis \"Fragmenta de Viribus\" (1805) contained the results of 27 provings, and his 1810 \"Materia Medica Pura\" contained 65.\nFor James Tyler Kent's 1905 \"Lectures on Homoeopathic Materia Medica\", 217 preparations underwent provings and newer substances are continually added to contemporary versions.\n\nThough the proving process has superficial similarities with clinical trials, it is fundamentally different in that the process is subjective, not blinded, and modern provings are unlikely to use pharmacologically active levels of the substance under proving. As early as 1842, Holmes noted the provings were impossibly vague, and the purported effect was not repeatable among different subjects.\n\nHomeopaths generally begin with detailed examinations of their patients' histories, including questions regarding their physical, mental and emotional states, their life circumstances and any physical or emotional illnesses. The homeopath then attempts to translate this information into a complex formula of mental and physical symptoms, including likes, dislikes, innate predispositions and even body type.\n\nFrom these symptoms, the homeopath chooses how to treat the patient using \"materia medica\" and repertories. In classical homeopathy, the practitioner attempts to match a single preparation to the totality of symptoms (the \"simlilum\"), while \"clinical homeopathy\" involves combinations of preparations based on the various symptoms of an illness.\n\nHomeopathic pills are made from an inert substance (often sugars, typically lactose), upon which a drop of liquid homeopathic preparation is placed and allowed to evaporate.\n\nThe process of homeopathic dilution results in no objectively detectable active ingredient in most cases, but some preparations (e.g. calendula and arnica creams) do contain pharmacologically active doses. One product, Zicam Cold Remedy, which was marketed as an \"unapproved homeopathic\" product, contains two ingredients that are only \"slightly\" diluted: zinc acetate (2X = 1/100 dilution) and zinc gluconate (1X = 1/10 dilution), which means both are present in a biologically active concentration strong enough to have caused some people to lose their sense of smell, a condition termed anosmia. Zicam also listed several normal homeopathic potencies as \"inactive ingredients\", including \"galphimia glauca\", histamine dihydrochloride (homeopathic name, \"histaminum hydrochloricum\"), \"luffa operculata\", and sulfur.\n\nIsopathy is a therapy derived from homeopathy, invented by Johann Joseph Wilhelm Lux in the 1830s. Isopathy differs from homeopathy in general in that the preparations, known as \"nosodes\", are made up either from things that cause the disease or from products of the disease, such as pus. Many so-called \"homeopathic vaccines\" are a form of isopathy. Tautopathy is a form of isopathy where the preparations are composed of drugs or vaccines that a person has consumed in the past, in the belief that this can reverse lingering damage caused by the initial use. There is no convincing scientific evidence for isopathy as an effective method of treatment.\n\nFlower preparations can be produced by placing flowers in water and exposing them to sunlight. The most famous of these are the Bach flower remedies, which were developed by the physician and homeopath Edward Bach. Although the proponents of these preparations share homeopathy's vitalist world-view and the preparations are claimed to act through the same hypothetical \"vital force\" as homeopathy, the method of preparation is different. Bach flower preparations are manufactured in allegedly \"gentler\" ways such as placing flowers in bowls of sunlit water, and the preparations are not succussed. There is no convincing scientific or clinical evidence for flower preparations being effective.\n\nThe idea of using homeopathy as a treatment for other animals termed \"veterinary homeopathy\", dates back to the inception of homeopathy; Hahnemann himself wrote and spoke of the use of homeopathy in animals other than humans. The FDA has not approved homeopathic products as veterinary medicine in the U.S. In the UK, veterinary surgeons who use homeopathy may belong to the Faculty of Homeopathy and/or to the British Association of Homeopathic Veterinary Surgeons. Animals may be treated only by qualified veterinary surgeons in the UK and some other countries. Internationally, the body that supports and represents homeopathic veterinarians is the International Association for Veterinary Homeopathy.\n\nThe use of homeopathy in veterinary medicine is controversial; the little existing research on the subject is not of a high enough scientific standard to provide reliable data on efficacy. Given that homeopathy's effects in humans appear to be mainly due to the placebo effect and the counseling aspects of the consultation, it is unlikely that homeopathic treatments would be effective in animals. Other studies have also found that giving animals placebos can play active roles in influencing pet owners to believe in the effectiveness of the treatment when none exists. The British Veterinary Association's position statement on alternative medicines says that it \"cannot endorse\" homeopathy, and the Australian Veterinary Association includes it on its list of \"ineffective therapies\". A 2016 review of peer-reviewed articles from 1981 to 2014 by scientists from the University of Kassel, Germany, concluded that there was insufficient evidence to support the use of homeopathy in livestock as a way to prevent or treat infectious diseases.\n\nThe UK's Department for Environment, Food and Rural Affairs (Defra) has adopted a robust position against use of \"alternative\" pet preparations including homeopathy.\n\nPopular in the late nineteenth century, electrohomeopathy has been described as \"utter idiocy\".\n\nElectrohomeopathy is somewhat associated with an Spagyric medicine in that the disease is usually multi-organic in cause or effect and calls for holistic treatment that is both complex and natural.\n\nThe Allahabad High Court in Kanpur handed down a decree in 2012 which stated that electrohomeopathy was an unrecognized system of medicine which was quackery.\n\nThe use of homeopathy as a preventive for serious infectious diseases is especially controversial, in the context of ill-founded public alarm over the safety of vaccines stoked by the anti-vaccination movement. Promotion of homeopathic alternatives to vaccines has been characterized as dangerous, inappropriate and irresponsible. In December 2014, Australian homeopathy supplier Homeopathy Plus! were found to have acted deceptively in promoting homeopathic alternatives to vaccines.\n\nThe low concentration of homeopathic preparations, which often lack even a single molecule of the diluted substance, has been the basis of questions about the effects of the preparations since the 19th century. Modern advocates of homeopathy have proposed a concept of \"water memory\", according to which water \"remembers\" the substances mixed in it, and transmits the effect of those substances when consumed. This concept is inconsistent with the current understanding of matter, and water memory has never been demonstrated to have any detectable effect, biological or otherwise. Pharmacological research has found instead that stronger effects of an active ingredient come from higher, not lower doses.\n\nJames Randi and the groups have highlighted the lack of active ingredients in most homeopathic products by taking large 'overdoses'. None of the hundreds of demonstrators in the UK, Australia, New Zealand, Canada and the US were injured and \"no one was cured of anything, either\".\n\nOutside of the alternative medicine community, scientists have long considered homeopathy a sham or a pseudoscience, and the mainstream medical community regards it as quackery. There is an overall absence of sound statistical evidence of therapeutic efficacy, which is consistent with the lack of any biologically plausible pharmacological agent or mechanism.\n\nAbstract concepts within theoretical physics have been invoked to suggest explanations of how or why preparations might work, including quantum entanglement, quantum nonlocality, the theory of relativity and chaos theory. Contrariwise, quantum superposition has been invoked to explain why homeopathy does \"not\" work in double-blind trials. However, the explanations are offered by nonspecialists within the field, and often include speculations that are incorrect in their application of the concepts and not supported by actual experiments. Several of the key concepts of homeopathy conflict with fundamental concepts of physics and chemistry. The use of quantum entanglement to explain homeopathy's purported effects is \"patent nonsense\", as entanglement is a delicate state that rarely lasts longer than a fraction of a second. While entanglement may result in certain aspects of individual subatomic particles acquiring linked quantum states, this does not mean the particles will mirror or duplicate each other, nor cause health-improving transformations.\n\nThe proposed mechanisms for homeopathy are precluded from having any effect by the laws of physics and physical chemistry. The extreme dilutions used in homeopathic preparations usually leave not one molecule of the original substance in the final product.\n\nA number of speculative mechanisms have been advanced to counter this, the most widely discussed being water memory, though this is now considered erroneous since short-range order in water only persists for about 1 picosecond. No evidence of stable clusters of water molecules was found when homeopathic preparations were studied using nuclear magnetic resonance, and many other physical experiments in homeopathy have been found to be of low methodological quality, which precludes any meaningful conclusion. Existence of a pharmacological effect in the absence of any true active ingredient is inconsistent with the law of mass action and the observed dose-response relationships characteristic of therapeutic drugs (whereas placebo effects are non-specific and unrelated to pharmacological activity).\n\nHomeopaths contend that their methods produce a therapeutically active preparation, selectively including only the intended substance, though critics note that any water will have been in contact with millions of different substances throughout its history, and homeopaths have not been able to account for a reason why only the selected homeopathic substance would be a special case in their process. For comparison, ISO 3696:1987 defines a standard for water used in laboratory analysis; this allows for a contaminant level of ten parts per billion, 4C in homeopathic notation. This water may not be kept in glass as contaminants will leach out into the water.\n\nPractitioners of homeopathy hold that higher dilutions―described as being of higher \"potency\"―produce stronger medicinal effects. This idea is also inconsistent with observed dose-response relationships, where effects are dependent on the concentration of the active ingredient in the body. This dose-response relationship has been confirmed in myriad experiments on organisms as diverse as nematodes, rats, and humans. Some homeopaths contend that the phenomenon of hormesis may support the idea of dilution increasing potency, but the dose-response relationship outside the zone of hormesis declines with dilution as normal, and nonlinear pharmacological effects do not provide any credible support for homeopathy.\n\nPhysicist Robert L. Park, former executive director of the American Physical Society, is quoted as saying:\n\n\"since the least amount of a substance in a solution is one molecule, a 30C solution would have to have at least one molecule of the original substance dissolved in a minimum of 1,000,000,000,000,000,000,000,000,000,000,<wbr>000,000,000,000,000,000,000,000,000,000 [or 10] molecules of water. This would require a container more than 30,000,000,000 times the size of the Earth.\"\nPark is also quoted as saying that, \"to expect to get even one molecule of the 'medicinal' substance allegedly present in 30X pills, it would be necessary to take some two billion of them, which would total about a thousand tons of lactose plus whatever impurities the lactose contained\".\n\nThe laws of chemistry state that there is a limit to the dilution that can be made without losing the original substance altogether. This limit, which is related to Avogadro's number, is roughly equal to homeopathic dilutions of 12C or 24X (1 part in 10).\n\nScientific tests run by both the BBC's \"Horizon\" and ABC's \"20/20\" programmes were unable to differentiate homeopathic dilutions from water, even when using tests suggested by homeopaths themselves.\n\nIn May 2018, the German skeptical organization GWUP issued an invitation to individuals and groups to respond to its challenge \"to identify homeopathic preparations in high potency and to give a detailed description on how this can be achieved reproducibly.\" The first participant to correctly identify selected homeopathic preparations under an agreed-upon protocol will receive €50,000.\n\nNo individual homeopathic preparation has been unambiguously shown by research to be different from placebo. The methodological quality of the primary research was generally low, with such problems as weaknesses in study design and reporting, small sample size, and selection bias. Since better quality trials have become available, the evidence for efficacy of homeopathy preparations has diminished; the highest-quality trials indicate that the preparations themselves exert no intrinsic effect. A review conducted in 2010 of all the pertinent studies of \"best evidence\" produced by the Cochrane Collaboration concluded that \"the most reliable evidence – that produced by Cochrane reviews – fails to demonstrate that homeopathic medicines have effects beyond placebo.\"\n\nGovernment-level reviews have been conducted in recent years by Switzerland (2005), the United Kingdom (2009), Australia (2015) and the European Academies' Science Advisory Council (2017).\n\nThe Swiss \"programme for the evaluation of complementary medicine\" (PEK) resulted in the peer-reviewed Shang publication (see \"Systematic reviews and meta-analyses of efficacy\") and a controversial competing analysis by homeopaths and advocates led by Gudrun Bornhöft and Peter Matthiessen, which has misleadingly been presented as a Swiss government report by homeopathy proponents, a claim that has been repudiated by the Swiss Federal Office of Public Health. The Swiss Government terminated reimbursement, though it was subsequently reinstated after a political campaign and referendum for a further six-year trial period.\n\nThe United Kingdom's House of Commons Science and Technology Committee sought written evidence and submissions from concerned parties and, following a review of all submissions, concluded that there was no compelling evidence of effect other than placebo and recommended that the Medicines and Healthcare products Regulatory Agency (MHRA) should not allow homeopathic product labels to make medical claims, that homeopathic products should no longer be licensed by the MHRA, as they are not medicines, and that further clinical trials of homeopathy could not be justified. They recommended that funding of homeopathic hospitals should not continue, and NHS doctors should not refer patients to homeopaths. The Secretary of State for Health deferred to local NHS on funding homeopathy, in the name of patient choice. By February 2011 only one-third of primary care trusts still funded homeopathy. By 2012, no British universities offered homeopathy courses. In July 2017, as part of a plan to save £200m a year by preventing the \"misuse of scarce\" funding, the NHS announced that it would no longer provide homeopathic medicines. A legal appeal by the British Homeopathic Association against the decision was rejected in June 2018.\n\nThe Australian National Health and Medical Research Council completed a comprehensive review of the effectiveness of homeopathic preparations in 2015, in which it concluded that \"there were no health conditions for which there was reliable evidence that homeopathy was effective. No good-quality, well-designed studies with enough participants for a meaningful result reported either that homeopathy caused greater health improvements than placebo, or caused health improvements equal to those of another treatment.\"\n\nOn September 20, 2017, the European Academies' Science Advisory Council (EASAC) published its official analysis and conclusion on the use of homeopathic products, finding a lack of evidence that homeopathic products are effective, and raising concerns about quality control.\n\nThe fact that individual randomized controlled trials have given positive results is not in contradiction with an overall lack of statistical evidence of efficacy. A small proportion of randomized controlled trials inevitably provide false-positive outcomes due to the play of chance: a \"statistically significant\" positive outcome is commonly adjudicated when the probability of it being due to chance rather than a real effect is no more than 5%―a level at which about 1 in 20 tests can be expected to show a positive result in the absence of any therapeutic effect. Furthermore, trials of low methodological quality (i.e. ones that have been inappropriately designed, conducted or reported) are prone to give misleading results. In a systematic review of the methodological quality of randomized trials in three branches of alternative medicine, Linde \"et al.\" highlighted major weaknesses in the homeopathy sector, including poor randomization. A separate 2001 systematic review that assessed the quality of clinical trials of homeopathy found that such trials were generally of lower quality than trials of conventional medicine.\n\nA related issue is publication bias: researchers are more likely to submit trials that report a positive finding for publication, and journals prefer to publish positive results. Publication bias has been particularly marked in alternative medicine journals, where few of the published articles (just 5% during the year 2000) tend to report null results. Regarding the way in which homeopathy is represented in the medical literature, a systematic review found signs of bias in the publications of clinical trials (towards negative representation in mainstream medical journals, and \"vice versa\" in alternative medicine journals), but not in reviews.\n\nPositive results are much more likely to be false if the prior probability of the claim under test is low.\n\nBoth meta-analyses, which statistically combine the results of several randomized controlled trials, and other systematic reviews of the literature are essential tools to summarize evidence of therapeutic efficacy. Early systematic reviews and meta-analyses of trials evaluating the efficacy of homeopathic preparations in comparison with placebo more often tended to generate positive results, but appeared unconvincing overall. In particular, reports of three large meta-analyses warned readers that firm conclusions could not be reached, largely due to methodological flaws in the primary studies and the difficulty in controlling for publication bias. The positive finding of one of the most prominent of the early meta-analyses, published in \"The Lancet\" in 1997 by Linde et al., was later reframed by the same research team, who wrote:\n\nThe evidence of bias [in the primary studies] weakens the findings of our original meta-analysis. Since we completed our literature search in 1995, a considerable number of new homeopathy trials have been published. The fact that a number of the new high-quality trials ... have negative results, and a recent update of our review for the most \"original\" subtype of homeopathy (classical or individualized homeopathy), seem to confirm the finding that more rigorous trials have less-promising results. It seems, therefore, likely that our meta-analysis at least overestimated the effects of homeopathic treatments.\n\nSubsequent work by John Ioannidis and others has shown that for treatments with no prior plausibility, the chances of a positive result being a false positive are much higher, and that any result not consistent with the null hypothesis should be assumed to be a false positive.\n\nA systematic review of the available systematic reviews confirmed in 2002 that higher-quality trials tended to have less positive results, and found no convincing evidence that any homeopathic preparation exerts clinical effects different from placebo.\n\nIn 2005, \"The Lancet\" medical journal published a meta-analysis of 110 placebo-controlled homeopathy trials and 110 matched medical trials based upon the Swiss government's Programme for Evaluating Complementary Medicine, or PEK. The study concluded that its findings were \"compatible with the notion that the clinical effects of homeopathy are placebo effects\". This was accompanied by an editorial pronouncing \"The end of homoeopathy\", which was denounced by the homeopath Peter Fisher.\n\nOther meta-analyses include homeopathic treatments to reduce cancer therapy side-effects following radiotherapy and chemotherapy, allergic rhinitis, attention-deficit hyperactivity disorder and childhood diarrhoea, adenoid vegetation, asthma, upper respiratory tract infection in children, insomnia, fibromyalgia, psychiatric conditions and Cochrane Library systematic reviews of homeopathic treatments for asthma, dementia, attention-deficit hyperactivity disorder, induction of labour, upper respiratory tract infections in children, and irritable bowel syndrome. Other reviews covered osteoarthritis, migraines, postoperative ecchymosis and edema, delayed-onset muscle soreness, preventing postpartum haemorrhage, or eczema and other dermatological conditions.\n\nA 2017 systematic review and meta-analysis found that the most reliable evidence did not support the effectiveness of non-individualized homeopathy. The authors noted that \"the quality of the body of evidence is low.\"\n\nThe results of these reviews are generally negative or only weakly positive, and reviewers consistently report the poor quality of trials. The finding of Linde \"et. al.\" that more rigorous studies produce less positive results is supported in several and contradicted by none.\n\nSome clinical trials have tested individualized homeopathy, and there have been reviews of this, specifically. A 1998 review found 32 trials that met their inclusion criteria, 19 of which were placebo-controlled and provided enough data for meta-analysis. These 19 studies showed a pooled odds ratio of 1.17 to 2.23 in favour of individualized homeopathy over the placebo, but no difference was seen when the analysis was restricted to the methodologically best trials. The authors concluded that \"the results of the available randomized trials suggest that individualized homeopathy has an effect over placebo. The evidence, however, is not convincing because of methodological shortcomings and inconsistencies.\" Jay Shelton, author of a book on homeopathy, has stated that the claim assumes without evidence that classical, individualized homeopathy works better than nonclassical variations. A 2014 systematic review and meta-analysis found that individualized homeopathic remedies may be slightly more effective than placebos, though the authors noted that their findings were based on low- or unclear-quality evidence. The same research team later reported that taking into account model validity did not significantly affect this conclusion.\n\nHealth organizations such as the UK's National Health Service, the American Medical Association, the FASEB, and the National Health and Medical Research Council of Australia, have issued statements of their conclusion that there is \"no good-quality evidence that homeopathy is effective as a treatment for any health condition\". In 2009, World Health Organization official Mario Raviglione criticized the use of homeopathy to treat tuberculosis; similarly, another WHO spokesperson argued there was no evidence homeopathy would be an effective treatment for diarrhoea.\n\nThe American College of Medical Toxicology and the American Academy of Clinical Toxicology recommend that no one use homeopathic treatment for disease or as a preventive health measure. These organizations report that no evidence exists that homeopathic treatment is effective, but that there is evidence that using these treatments produces harm and can bring indirect health risks by delaying conventional treatment.\n\nScience offers a variety of explanations for how homeopathy may appear to cure diseases or alleviate symptoms even though the preparations themselves are inert:\n\nWhile some articles have suggested that homeopathic solutions of high dilution can have statistically significant effects on organic processes including the growth of grain, histamine release by leukocytes, and enzyme reactions, such evidence is disputed since attempts to replicate them have failed. A 2007 systematic review of high-dilution experiments found that none of the experiments with positive results could be reproduced by all investigators.\n\nIn 1987, French immunologist Jacques Benveniste submitted a paper to the journal \"Nature\" while working at INSERM. The paper purported to have discovered that basophils, a type of white blood cell, released histamine when exposed to a homeopathic dilution of anti-immunoglobulin E antibody. The journal editors, sceptical of the results, requested that the study be replicated in a separate laboratory. Upon replication in four separate laboratories the study was published. Still sceptical of the findings, \"Nature\" assembled an independent investigative team to determine the accuracy of the research, consisting of \"Nature\" editor and physicist Sir John Maddox, American scientific fraud investigator and chemist Walter Stewart, and sceptic James Randi. After investigating the findings and methodology of the experiment, the team found that the experiments were \"statistically ill-controlled\", \"interpretation has been clouded by the exclusion of measurements in conflict with the claim\", and concluded, \"We believe that experimental data have been uncritically assessed and their imperfections inadequately reported.\" James Randi stated that he doubted that there had been any conscious fraud, but that the researchers had allowed \"wishful thinking\" to influence their interpretation of the data.\n\nIn 2001 and 2004, Madeleine Ennis published a number of studies that reported that homeopathic dilutions of histamine exerted an effect on the activity of basophils. In response to the first of these studies, \"Horizon\" aired a programme in which British scientists attempted to replicate Ennis' results; they were unable to do so.\n\nThe provision of homeopathic preparations has been described as unethical. Michael Baum, Professor Emeritus of Surgery and visiting Professor of Medical Humanities at University College London (UCL), has described homoeopathy as a \"cruel deception\".\n\nEdzard Ernst, the first \"Professor of Complementary Medicine\" in the United Kingdom and a former homeopathic practitioner, has expressed his concerns about pharmacists who violate their ethical code by failing to provide customers with \"necessary and relevant information\" about the true nature of the homeopathic products they advertise and sell:\n\nPatients who choose to use homeopathy rather than evidence-based medicine risk missing timely diagnosis and effective treatment of serious conditions such as cancer.\n\nIn 2013 the UK Advertising Standards Authority concluded that the Society of Homeopaths were targeting vulnerable ill people and discouraging the use of essential medical treatment while making misleading claims of efficacy for homeopathic products.\n\nIn 2015 the Federal Court of Australia imposed penalties on a homeopathic company, Homeopathy Plus! Pty Ltd and its director, for making false or misleading statements about the efficacy of the whooping cough vaccine and homeopathic remedies as an alternative to the whooping cough vaccine, in breach of the Australian Consumer Law.\n\nSome homeopathic preparations involve poisons such as Belladonna, arsenic, and poison ivy, which are highly diluted in the homeopathic preparation. In rare cases, the original ingredients are present at detectable levels. This may be due to improper preparation or intentional low dilution. Serious adverse effects such as seizures and death have been reported or associated with some homeopathic preparations.\n\nOn September 30, 2016 the FDA issued a safety alert to consumers warning against the use of homeopathic teething gels and tablets following reports of adverse events after their use. The agency recommended that parents discard these products and \"seek advice from their health care professional for safe alternatives\" to homeopathy for teething. The pharmacy CVS announced, also on September 30, that it was voluntarily withdrawing the products from sale and on October 11 Hyland's (the manufacturer) announced that it was discontinuing their teething medicine in the United States though the products remain on sale in Canada. On October 12, Buzzfeed reported that the regulator had \"examined more than 400 reports of seizures, fever and vomiting, as well as 10 deaths\" over a six-year period. The investigation (including analyses of the products) is still ongoing and the FDA does not know yet if the deaths and illnesses were caused by the products. However a previous FDA investigation in 2010, following adverse effects reported then, found that these same products were improperly diluted and contained \"unsafe levels of belladonna, also known as deadly nightshade\" and that the reports of serious adverse events in children using this product were \"consistent with belladonna toxicity\".\n\nInstances of arsenic poisoning have occurred after use of arsenic-containing homeopathic preparations. Zicam Cold remedy Nasal Gel, which contains 2X (1:100) zinc gluconate, reportedly caused a small percentage of users to lose their sense of smell; 340 cases were settled out of court in 2006 for . In 2009, the FDA advised consumers to stop using three discontinued cold remedy Zicam products because it could cause permanent damage to users' sense of smell. Zicam was launched without a New Drug Application (NDA) under a provision in the FDA's Compliance Policy Guide called \"Conditions under which homeopathic drugs may be marketed\" (CPG 7132.15), but the FDA warned Matrixx Initiatives, its manufacturer, via a Warning Letter that this policy does not apply when there is a health risk to consumers.\n\nA 2000 review by homeopaths reported that homeopathic preparations are \"unlikely to provoke severe adverse reactions\". In 2012, a systematic review evaluating evidence of homeopathy's possible adverse effects concluded that \"homeopathy has the potential to harm patients and consumers in both direct and indirect ways\". One of the reviewers, Edzard Ernst, supplemented the article on his blog, writing: \"I have said it often and I say it again: if used as an alternative to an effective cure, even the most 'harmless' treatment can become life-threatening.\" A 2016 systematic review and meta-analysis found that, in homeopathic clinical trials, adverse effects were reported among the patients who received homeopathy about as often as they were reported among patients who received placebo or conventional medicine.\n\nThe lack of convincing scientific evidence supporting its efficacy and its use of preparations without active ingredients have led to characterizations as pseudoscience and quackery, or, in the words of a 1998 medical review, \"placebo therapy at best and quackery at worst\". The Russian Academy of Sciences considers homeopathy a \"dangerous 'pseudoscience' that does not work\", and \"urges people to treat homeopathy 'on a par with magic. The Chief Medical Officer for England, Dame Sally Davies, has stated that homeopathic preparations are \"rubbish\" and do not serve as anything more than placebos. Jack Killen, acting deputy director of the National Center for Complementary and Alternative Medicine, says homeopathy \"goes beyond current understanding of chemistry and physics\". He adds: \"There is, to my knowledge, no condition for which homeopathy has been proven to be an effective treatment.\" Ben Goldacre says that homeopaths who misrepresent scientific evidence to a scientifically illiterate public, have \"... walled themselves off from academic medicine, and critique has been all too often met with avoidance rather than argument\". Homeopaths often prefer to ignore meta-analyses in favour of cherry picked positive results, such as by promoting a particular observational study (one which Goldacre describes as \"little more than a customer-satisfaction survey\") as if it were more informative than a series of randomized controlled trials.\n\nReferring specifically to homeopathy, the British House of Commons Science and Technology Committee has stated:\n\nThe National Center for Complementary and Alternative Medicine of the United States' National Institutes of Health states:\n\nBen Goldacre noted that in the early days of homeopathy, when medicine was dogmatic and frequently worse than doing nothing, homeopathy at least failed to make matters worse:\n\nOn clinical grounds, patients who choose to use homeopathy in preference to normal medicine risk missing timely diagnosis and effective treatment, thereby worsening the outcomes of serious conditions. Critics of homeopathy have cited individual cases of patients of homeopathy failing to receive proper treatment for diseases that could have been easily diagnosed and managed with conventional medicine and who have died as a result, and the \"marketing practice\" of criticizing and downplaying the effectiveness of mainstream medicine. Homeopaths claim that use of conventional medicines will \"push the disease deeper\" and cause more serious conditions, a process referred to as \"suppression\". Some homeopaths (particularly those who are non-physicians) advise their patients against immunization. Some homeopaths suggest that vaccines be replaced with homeopathic \"nosodes\", created from biological materials such as pus, diseased tissue, bacilli from sputum or (in the case of \"bowel nosodes\") faeces. While Hahnemann was opposed to such preparations, modern homeopaths often use them although there is no evidence to indicate they have any beneficial effects. Cases of homeopaths advising against the use of anti-malarial drugs have been identified. This puts visitors to the tropics who take this advice in severe danger, since homeopathic preparations are completely ineffective against the malaria parasite. Also, in one case in 2004, a homeopath instructed one of her patients to stop taking conventional medication for a heart condition, advising her on June 22, 2004 to \"Stop ALL medications including homeopathic\", advising her on or around August 20 that she no longer needed to take her heart medication, and adding on August 23, \"She just cannot take ANY drugs – I have suggested some homeopathic remedies ... I feel confident that if she follows the advice she will regain her health.\" The patient was admitted to hospital the next day, and died eight days later, the final diagnosis being \"acute heart failure due to treatment discontinuation\".\n\nIn 1978, Anthony Campbell, then a consultant physician at the Royal London Homeopathic Hospital, criticized statements by George Vithoulkas claiming that syphilis, when treated with antibiotics, would develop into secondary and tertiary syphilis with involvement of the central nervous system, saying that \"The unfortunate layman might well be misled by Vithoulkas' rhetoric into refusing orthodox treatment\".\nVithoulkas' claims echo the idea that treating a disease with external medication used to treat the symptoms would only drive it deeper into the body and conflict with scientific studies, which indicate that penicillin treatment produces a complete cure of syphilis in more than 90% of cases.\n\nA 2006 review by W. Steven Pray of the College of Pharmacy at Southwestern Oklahoma State University recommends that pharmacy colleges include a required course in unproven medications and therapies, that ethical dilemmas inherent in recommending products lacking proven safety and efficacy data be discussed, and that students should be taught where unproven systems such as homeopathy depart from evidence-based medicine.\n\nIn an article entitled \"Should We Maintain an Open Mind about Homeopathy?\" published in the \"American Journal of Medicine\", Michael Baum and Edzard Ernstwriting to other physicianswrote that \"Homeopathy is among the worst examples of faith-based medicine... These axioms [of homeopathy] are not only out of line with scientific facts but also directly opposed to them. If homeopathy is correct, much of physics, chemistry, and pharmacology must be incorrect...\".\n\nIn 2013, Mark Walport, the UK Government Chief Scientific Adviser and head of the Government Office for Science, had this to say: \"My view scientifically is absolutely clear: homoeopathy is nonsense, it is non-science. My advice to ministers is clear: that there is no science in homoeopathy. The most it can have is a placebo effect – it is then a political decision whether they spend money on it or not.\" His predecessor, John Beddington, referring to his views on homeopathy being \"fundamentally ignored\" by the Government, said: \"The only one [view being ignored] I could think of was homoeopathy, which is mad. It has no underpinning of scientific basis. In fact, all the science points to the fact that it is not at all sensible. The clear evidence is saying this is wrong, but homoeopathy is still used on the NHS.\"\n\nHomeopathy is fairly common in some countries while being uncommon in others; is highly regulated in some countries and mostly unregulated in others. It is practised worldwide and professional qualifications and licences are needed in most countries. In some countries, there are no specific legal regulations concerning the use of homeopathy, while in others, licences or degrees in conventional medicine from accredited universities are required. In Germany, to become a homeopathic physician, one must attend a three-year training programme, while France, Austria and Denmark mandate licences to diagnose any illness or dispense of any product whose purpose is to treat any illness.\n\nSome homeopathic treatment is covered by the public health service of several European countries, including France, Scotland, Luxembourg and England (though the latter will cease in February 2019). In other countries, such as Belgium, homeopathy is not covered. In Austria, the public health service requires scientific proof of effectiveness in order to reimburse medical treatments and homeopathy is listed as not reimbursable, but exceptions can be made; private health insurance policies sometimes include homeopathic treatment. The Swiss government, after a 5-year trial, withdrew coverage of homeopathy and four other complementary treatments in 2005, stating that they did not meet efficacy and cost-effectiveness criteria, but following a referendum in 2009 the five therapies have been reinstated for a further 6-year trial period from 2012.\nThe Indian government recognizes homeopathy as one of its national systems of medicine; it has established AYUSH or the Department of Ayurveda, Yoga and Naturopathy, Unani, Siddha and Homoeopathy under the Ministry of Health & Family Welfare. The south Indian state of Kerala also has a cabinet-level AYUSH department. The Central Council of Homoeopathy was established in 1973 to monitor higher education in homeopathy, and National Institute of Homoeopathy in 1975. A minimum of a recognized diploma in homeopathy and registration on a state register or the Central Register of Homoeopathy is required to practise homeopathy in India.\n\nOn September 28, 2016 the UK's Committee of Advertising Practice (CAP) Compliance team wrote to homeopaths in the UK to \"remind them of the rules that govern what they can and can't say in their marketing materials\". The letter highlights that \"homeopaths may not currently make either direct or implied claims to treat medical conditions\" and asks them to review their marketing communications \"including websites and social media pages\" to ensure compliance by November 3, 2016. The letter also includes information on sanctions in the event of non-compliance including, ultimately, \"referral by the ASA to Trading Standards under the Consumer Protection from Unfair Trading Regulations 2008\".\n\nIn February 2017, Russian Academy of Sciences declared homeopathy to be \"dangerous pseudoscience\" and \"on a par with magic\".\n\nIn the April 1997 edition of FDA Consumer, William T. Jarvis, the President of the National Council Against Health Fraud, said \"Homeopathy is a fraud perpetrated on the public with the government's blessing, thanks to the abuse of political power of Sen. Royal S. Copeland [chief sponsor of the 1938 Food, Drug, and Cosmetic Act].\"\n\nMock \"overdosing\" on homeopathic preparations by individuals or groups in \"mass suicides\" have become more popular since James Randi began taking entire bottles of homeopathic sleeping pills before giving lectures. In 2010 The Merseyside Skeptics Society from the United Kingdom launched the , encouraging groups to publicly overdose as groups. In 2011 the 10:23 campaign expanded and saw sixty-nine groups participate; fifty-four submitted videos. In April 2012, at the Berkeley SkeptiCal conference, over 100 people participated in a mass overdose, taking \"coffea cruda\", which is supposed to treat sleeplessness.\n\nIn 2011, the non-profit, educational organizations Center for Inquiry (CFI) and the associated Committee for Skeptical Inquiry (CSI) have petitioned the U.S. Food and Drug Administration (FDA) to initiate \"rulemaking that would require all over-the-counter homeopathic drugs to meet the same standards of effectiveness as non-homeopathic drugs\" and \"to place warning labels on homeopathic drugs until such time as they are shown to be effective\". In a separate petition, CFI and CSI request FDA to issue warning letters to Boiron, maker of Oscillococcinum, regarding their marketing tactic and criticize Boiron for misleading labelling and advertising of Oscillococcinum. In 2015, CFI filed comments urging the Federal Trade Commission to end the false advertising practice of homeopathy. On November 15, 2016, FTC declared that homeopathic products cannot include claims of effectiveness without \"competent and reliable scientific evidence\". If no such evidence exists, they must state this fact clearly on their labeling, and state that the product's claims are based only on 18th-century theories that have been discarded by modern science. Failure to do so will be considered a violation of the FTC Act.\nCFI in Canada is calling for persons that feel they were harmed by homeopathic products to contact them.\n\nIn August 2011, a class action lawsuit was filed against Boiron on behalf of \"all California residents who purchased Oscillo at any time within the past four years\". The lawsuit charged that it \"is nothing more than a sugar pill\", \"despite falsely advertising that it contains an active ingredient known to treat flu symptoms\". In March 2012, Boiron agreed to spend up to $12 million to settle the claims of falsely advertising the benefits of its homeopathic preparations.\n\nIn July 2012, CBC News reporter Erica Johnson for \"Marketplace\" conducted an investigation on the homeopathy industry in Canada; her findings were that it is \"based on flawed science and some loopy thinking\". Center for Inquiry (CFI) Vancouver skeptics participated in a mass overdose outside an emergency room in Vancouver, B.C., taking entire bottles of \"medications\" that should have made them sleepy, nauseous or dead; after 45 minutes of observation no ill effects were felt. Johnson asked homeopaths and company representatives about cures for cancer and vaccine claims. All reported positive results but none could offer any science backing up their statements, only that \"it works\". Johnson was unable to find any evidence that homeopathic preparations contain any active ingredient. Analysis performed at the University of Toronto's chemistry department found that the active ingredient is so small \"it is equivalent to 5 billion times less than the amount of aspirin ... in a single pellet\". Belladonna and ipecac \"would be indistinguishable from each other in a blind test\".\n\nHomeopathic services offered at Bristol Homeopathic Hospital in the UK ceased in October 2015, partly in response to increased public awareness as a result of the and a campaign led by the Good Thinking Society, University Hospitals Bristol confirmed that it would cease to offer homeopathic therapies from October 2015, at which point homeopathic therapies would no longer be included in the contract. Homeopathic services in the Bristol area were relocated to \"a new independent social enterprise\" at which Bristol Clinical Commissioning Group revealed \"there are currently no (NHS) contracts for homeopathy in place.\" Following a threat of legal action by the Good Thinking Society campaign group, the British government has stated that the Department of Health will hold a consultation in 2016 regarding whether homeopathic treatments should be added to the NHS treatments blacklist (officially, Schedule 1 of the National Health Service (General Medical Services Contracts) (Prescription of Drugs etc.) Regulations 2004), that specifies a blacklist of medicines not to be prescribed under the NHS.\n\nIn March 2016, the University of Barcelona cancelled its master's degree in Homeopathy citing \"lack of scientific basis\", after advice from the Spanish Ministry of Health stated that \"Homeopathy has not definitely proved its efficacy under any indication or concrete clinical situation\". Shortly afterwards, in April 2016, the University of Valencia announced the elimination of its Masters in Homeopathy for 2017.\n\nIn June 2016, blogger and sceptic Jithin Mohandas launched a petition through Change.org asking the government of Kerala, India, to stop admitting students to homeopathy medical colleges. Mohandas said that government approval of these colleges makes them appear legitimate, leading thousands of talented students to join them and end up with invalid degrees. The petition asks that homeopathy colleges be converted to regular medical colleges and that people with homeopathy degrees be provided with training in scientific medicine.\n\nIn Germany, physician and critic of alternative medicine Irmgard Oepen was a relentless critic of homeopathy.\n\nOn April 20–21, 2015, the FDA held a hearing on homeopathic product regulation. Invitees representing the scientific and medical community, and various pro-homeopathy stakeholders, gave testimonials on homeopathic products and the regulatory role played by the FDA.\nMichael de Dora, a representative from the Center for Inquiry (CFI), on behalf of the organization and dozens of doctors and scientists associated with CFI and the Committee for Skeptical Inquiry (CSI) gave a testimonial which summarized the basis of the organization's objection to homeopathic products, the harm that is done to the general public and proposed regulatory actions:\n\nThe CFI testimonial stated that the principle of homeopathy is at complete odds with the basic principles of modern biology, chemistry and physics and that decades of scientific examination of homeopathic products shows that there is no evidence that it is effective in treating illnesses other than acting as a placebo. Further, it noted a 2012 report by the American Association of Poison Control Centers which listed 10,311 reported cases of poison exposure related to homeopathic agents, among which 8,788 cases were attributed to young children five years of age or younger, as well as examples of harm – including deaths – caused to patients who relied on homeopathics instead of proven medical treatment.\n\nThe CFI urged the FDA to announce and implement strict guidelines that \"require all homeopathic products meet the same standards as non-homeopathic drugs\", arguing that the consumers can only have true freedom of choice (an often used argument from the homeopathy proponents) if they are fully informed of the choices. CFI proposed that the FDA take these three steps:\n\nIn December 2017, the FDA announced it would strengthen regulation of homeopathic products focusing on \"situations where homeopathic treatments are being marketed for serious diseases or conditions but have not been shown to offer clinical benefits\" and where \"products labeled as homeopathic contain potentially harmful ingredients or do not meet current good manufacturing practices.\"\n\nIn March 2015, the National Health and Medical Research Council of Australia issued the following conclusions and recommendations:\n\nIn November 2016, The United States FTC issued an \"Enforcement Policy Statement Regarding Marketing Claims for Over-the-Counter Homeopathic Drugs\" which specified that the FTC will hold efficacy and safety claims for OTC homeopathic drugs to the same standard as other products making similar claims. A November 15, 2016, FTC press release summarized the policy as follows:\n\nIn conjunction with the 2016 FTC Enforcement Policy Statement, the FTC also released its \"Homeopathic Medicine & Advertising Workshop Report\", which summarizes the panel presentations and related public comments in addition to describing consumer research commissioned by the FTC. The report concluded:\n\n\n"}
{"id": "16924116", "url": "https://en.wikipedia.org/wiki?curid=16924116", "title": "Introduction to M-theory", "text": "Introduction to M-theory\n\nIn non-technical terms, M-theory presents an idea about the basic substance of the universe. So far there is no experimental evidence that M-theory is a description of the real world. Although a complete mathematical formulation of M-theory is not known, the general approach is the leading contender for a universal \"Theory of Everything\" that unifies gravity with other forces such as electromagnetism. The goal of M-theory is to unify quantum mechanics with general relativity's gravitational force in a mathematically consistent way. In comparison, other theories such as loop quantum gravity are considered less elegant because they posit gravity to be completely different from forces such as the electromagnetic force.\n\nIn the early years of the 20th century the atom – long believed to be the smallest building-block of matter – was proven to consist of even smaller components called protons, neutrons and electrons, which are known as subatomic particles. Starting in the 1960's, other subatomic particles were discovered. In the 1970s, it was discovered that protons and neutrons (and other hadrons) are themselves made up of smaller particles called quarks. The Standard Model is the set of rules that describes the interactions of these particles.\n\nIn the 1980s, a new mathematical model of theoretical physics, called string theory, emerged. It showed how all the different subatomic particles known to science could be constructed by hypothetical one-dimensional \"strings\", infinitesimal building-blocks that have only the dimension of length, but not height or width.\n\nHowever, for string theory to be mathematically consistent, the strings must be in a universe of ten dimensions. This contradicts the experience that our real universe has four dimensions: three space dimensions (height, width, and length) and one time dimension. To \"save\" their theory, string theorists therefore added the explanation that the additional six dimensions exist but cannot be detected directly; this was explained by sophisticated mathematical objects called Calabi–Yau manifolds. The number of dimensions was later increased to 11 based on various interpretations of the 10-dimensional theory that led to five partial theories, as described below. Supergravity theory also played a significant part in establishing the necessity of the 11th dimension.\n\nThese \"strings\" vibrate in multiple dimensions and, depending on how they vibrate, they might be seen in three-dimensional space as matter, light or gravity. It is the vibration of the string that determines whether it appears to be matter or energy, and every form of matter or energy is the result of the vibration of strings.\n\nString theory as described above ran into a problem: another version of the equations was discovered, then another, and then another. Eventually, five major string theories were developed. The main differences between the theories were principally the number of dimensions in which the strings developed, and their characteristics (some were open loops, some were closed loops, etc.). Furthermore, all these theories appeared to be workable. Scientists were not comfortable with five seemingly contradictory sets of equations to describe the same thing.\n\nSpeaking at the string theory conference at the University of Southern California in 1995, Edward Witten of the Institute for Advanced Study suggested that the five different versions of string theory might be describing the same thing seen from different perspectives. He proposed a unifying theory called \"M-theory\", in which the \"M\" is not specifically defined but is generally understood to stand for \"membrane\". The words \"matrix\", \"master\", \"mother\", \"monster\", \"mystery\" and \"magic\" have also been claimed. M-theory brought all of the string theories together. It did this by asserting that strings are really one-dimensional slices of a two-dimensional membrane vibrating in 11-dimensional spacetime. Vibrations of higher-dimensional objects (as in three-dimensional vibrating blob or sphere or even more possible dimensions) are certainly a part of M-theory, but the basic theory of branes is still in progress. Higher-dimensional objects are much harder to mathematically calculate than a point in classical physics or a one-dimension string in string theory or two-dimensional membranes in M-theory.\n\nM-theory is not complete, but the mathematics of the approach has been explored in great detail. However, so far no experimental support of the M-theory exists. Some physicists are skeptical that this approach will ever lead to a physical theory describing our real world, due to fundamental issues.\n\nNevertheless, some cosmologists are drawn to M-theory because of its mathematical elegance and relative simplicity, triggering the hope that the simplicity is a reason why it may describe our world. Stephen Hawking originally believed that M-theory might be the ultimate theory but later suggested that the search for understanding of mathematics and physics will never be complete.\n\nOne feature of M-theory that has drawn great interest is that naturally predicts the existence of the graviton, a spin-2 particle hypothesized to mediate the gravitational force; furthermore, M-theory naturally predicts a phenomenon that resembles black hole evaporation. Competing unification theories such as asymptotically safe gravity, E8 theory, noncommutative geometry, and causal fermion systems have not demonstrated any level of mathematical consistency. M-theory's chief rival is loop quantum gravity, a non-unifying theory; many physicists consider loop quantum gravity to be less elegant than M-theory because it posits gravity to be completely different from the other fundamental forces.\n\n\n\n"}
{"id": "4340403", "url": "https://en.wikipedia.org/wiki?curid=4340403", "title": "List of adiabatic concepts", "text": "List of adiabatic concepts\n\nAdiabatic (from \"Gr.\" ἀ \"negative\" + διάβασις \"passage; transference\") refers to any process that occurs without heat transfer. This concept is used in many areas of physics and engineering. Notable examples are listed below.\n\n\n\n\n\n\n\n"}
{"id": "38388322", "url": "https://en.wikipedia.org/wiki?curid=38388322", "title": "List of anthropology journals", "text": "List of anthropology journals\n\nAcademic anthropological knowledge is the product of lengthy research, and is published in recognized peer-review periodicals. As part of this peer review, theories and reports are rigorously and comparatively tested before publication. The following publications are generally recognized as the major sources of anthropological knowledge.\n\nThese journals publish articles in the four fields of anthropology: archaeology, biological, cultural, and linguistic.\n\n\n\n\n\n"}
{"id": "398810", "url": "https://en.wikipedia.org/wiki?curid=398810", "title": "List of graphical methods", "text": "List of graphical methods\n\nThis is a list of graphical methods with a mathematical basis.\nIncluded are diagram techniques, chart techniques, plot techniques, and other forms of visualization.\n\nThere is also a list of computer graphics and descriptive geometry topics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1196020", "url": "https://en.wikipedia.org/wiki?curid=1196020", "title": "List of interactive geometry software", "text": "List of interactive geometry software\n\nInteractive geometry software (IGS) or dynamic geometry environments (DGEs) are computer programs which allow one to create and then manipulate geometric constructions, primarily in plane geometry. In most IGS, one starts construction by putting a few points and using them to define new objects such as lines, circles or other points. After some construction is done, one can move the points one started with and see how the construction changes.\n\nThe earliest IGS was the Geometric Supposer, which was developed in the early 1980s. This was soon followed by Cabri in 1986 and The Geometer's Sketchpad.\n\nThere are three main types of computer environments for studying school geometry: supposers, dynamic geometry environments (DGEs) and Logo-based programs. Most are DGEs: software that allows the user to manipulate (\"drag\") the geometric object into different shapes or positions. The main example of a supposer is the Geometric Supposer, which does not have draggable objects, but allows students to study pre-defined shapes. Nearly all of the following programs are DGEs. For a related, comparative physical example of these algorithms, see Lenart Sphere.\n\nThe following table provides a first comparison of the different software according to their licence and platform.\n\nThe following table provides a more detailed comparison :\n\nFeatures related to macro constructions: (TODO)\n\nLoci features related to IGS: (TODO)\n\nWe detail here the proof related features. (TODO)\n\nMeasurement and calculation features related to IGS: (TODO)\n\nC.a.R. is a free GPL analog of The Geometer's Sketchpad (GSP), written in Java.\n\nCaRMetal is a free GPL software written in Java. Derived from C.a.R., it provides a different user interface.\n\nCinderella, written in Java, is very different from The Geometer's Sketchpad. The later version Cinderella.2 also includes a physics simulation engine and a scripting language. Also, it now supports macros, line segments, calculations, arbitrary functions, plots, etc. Full documentation is available online.\n\nDr Genius was an attempt to merge Dr. Geo and the Genius calculator.\n\nDr. Geo is a GPL interactive software intended for younger students (7-15). The later version, Dr. Geo II, is a complete rewrite of Dr. Geo, for the Squeak/Smalltalk environment.\n\nGCLC is a dynamic geometry tool for visualizing and teaching geometry, and for producing mathematical illustrations. In GCLC, figures are described rather than drawn. This approach stresses the fact that geometrical constructions are abstract, formal procedures and not figures. A concrete figure can be generated on the basis of the abstract description. There are several output formats, including LaTeX, LaTeX/PStricks, LaTeX/Tikz, SVG and PostScript. There is a built-in geometry theorem prover (based on the area method). GCLC is available for Windows and Linux. WinGCLC is a Windows version of GCLC with a graphical interface that provides a range of additional functionalities.\n\nGeoGebra is software that combines geometry, algebra and calculus for mathematics education in schools and universities. It is available free of charge for non-commercial users. \n\n\nGeoKone.NET is an interactive recursive natural geometry (or \"sacred geometry\") generator that runs in a web browser. GeoKone allows the user to create geometric figures using naturalistic rules of recursive copying, such as the Golden ratio.\n\nGeolog is a logic programming language for finitary geometric logic.\n\nGeometry Expressions Does symbolic geometry. It uses real symbolic inputs and returns real and symbolic outputs. It emphasises use with a Computer Algebra System (CAS), as well as exporting and sharing via interactive HTML5, Lua, and OS X dashboard widget apps.\n\nThe Geometer's Sketchpad (GSP)\n\n\nThe Geometric Supposer\n\nGeoProof is a free GPL dynamic geometry software, written in OCaml.\n\nGEUP is a more calculus-oriented analog of The Geometer's Sketchpad.\n\n\nGRACE (The Graphical Ruler And Compass Editor) is an analog of The Geometer's Sketchpad (GSP), written in Java.\n\niGeom is freeware interactive geometry software hosted on the Internet for learning and teaching geometry (an analog of GSP and Cabri), written in Java.\n\nIsard is an interactive geometry software originally written in Smalltalk. The latest version only works under VisualWorks 7.\n\nJeometry is a dynamic geometry applet.\n\nKig is a free (GPL) analog of The Geometer's Sketchpad (GSP) for KDE, but more calculus-oriented. It is a part of the KDE Edutainment Project.\n\nKgeo was a free (GPL) analog of The Geometer's Sketchpad (GSP) for KDE, but more calculus-oriented, with an interface similar to Kig's. Development has stopped, and the project was replaced and superseded by Kig.\n\nKmPlot is a mathematical function plotter released under the free GPL license. Includes a powerful parser and precision printing in correct scale. Simultaneously plot multiple functions and combine function terms to build new functions. Supports functions with parameters and functions in polar coordinates. Several grid modes are available. Features include:\n\nKSEG is a free (GPL) analog of The Geometer's Sketchpad (GSP) with some unique features. This software can handle heavy, complex constructions in Euclidean geometry.\n\n\nLive Geometry is a free CodePlex project that lets you create interactive ruler and compass constructions and experiment with them. It is written in Silverlight 4 and C# 4.0 (Visual Studio 2010). The core engine is a flexible and extensible framework that allows easy addition of new figure types and features. The project has two front-ends: WPF and Silverlight, which both share the common \"DynamicGeometry\" library.\n\nNon-Euclid is a very basic Java-IGS used only for hyperbolic geometry in the Poincaré disk and the upper half-plane models.\n\nOpenEuclide is a GPL 2D geometry software.\n\nSarit2d is a library for Javascript created for drawing and solving 2d-geometric problems. The library contains many functions for drawing the main geometric shapes: segments, arcs, points, texts, etc. But the very core of the library are the functions for solving the most common geometry problems: intersections between shapes, areas calculation, geometric formulas, etc.With Sarit2d library it's possible solving hard problems through few code rows.\n\nSphaerica is an open source geometry software for spherical geometry. It supports orthographic, stereographic and gnomonic projections and various tools for constructions on the sphere.\n\nTabula is a commercial dynamic geometry program created by Numeracy Works. Tabula supports hands-on learning and can be used to construct, cut, tape, fold, measure, and transform geometric figures. Built using Silverlight, it is both Mac OS and Windows compatible.\n\nTabulae is a dynamic geometry software written in Java. It is under development by the Federal University of Rio de Janeiro. It is available in Brazilian and Portuguese.\n\nTracenPoche is a completely Adobe Flash program. It is available in English, Spanish, and French.\n\nWingeom is a program for high-precision geometric constructions in both two and three dimensions.\n\nArchimedes Geo3D\n\nGanja.js implements 2D and 3D projective and conformal Geometric Algebra. It features animation, interactivity and live\nediting without registration or download. \n\nEuler (software)\n\nEuler 3D is a program that allows you to create and manipulate your own polyhedrons. It has a number of facilities: transformations, animations, creating duals, import/export VRML, etc.\n\nFree registration required.\n\nGeomview\n\nAll these programs can be divided into two category: deterministic and continuous.\nGeoGebra can be deterministic or continuous (one can change it in preferences).\n\nAll constructions in the deterministic programs (GSP, Cabri, Kseg and most of others) are completely determined by the given points but the result of some constructions can jump or behave unexpectedly when a given point is moved.\n\nOn the contrary, some constructions in continuous programs (so far only Cinderella and GeoGebra), depend on the number of hidden parameters and in such a way that moving a given point produces a continuous motion of the construction, as a result, if the point is moved back to the original position the result of construction might be different.\n\nHere is a test to check whether a particular program is continuous:\n\nConstruct the orthocenter of triangle and three midpoints (say \"A', B' C' \" ) between vertices and orthocenter.\n\nConstruct a circumcircle of \"A'B'C' \".\n\nThis is the nine-point circle, it intersects each side of the original triangle at two points: the base of altitude and midpoint. Construct an intersection of one side with the circle at midpoint now move opposite vertex of the original triangle, if the constructed point does not move when base of altitude moves through it that probably means that your program is continuous.\n\nAlthough it is possible to make a deterministic program which behaves continuously in this and similar simple examples, in general it can be proved that no program can be continuous and deterministic at the same time.\n\n\n"}
{"id": "19699786", "url": "https://en.wikipedia.org/wiki?curid=19699786", "title": "List of medical schools in Syria", "text": "List of medical schools in Syria\n\nThis is a list of medical schools located in Syria.\n"}
{"id": "44120229", "url": "https://en.wikipedia.org/wiki?curid=44120229", "title": "List of original programs distributed by Sony Crackle", "text": "List of original programs distributed by Sony Crackle\n\nSony Crackle is an on-demand internet streaming media provider owned by Sony, that distributes a number of Crackle-exclusive programs, including original series like \"Chosen\".\n"}
{"id": "5109980", "url": "https://en.wikipedia.org/wiki?curid=5109980", "title": "Lockin effect", "text": "Lockin effect\n\nIn superconductivity, the Lockin Effect refers to the preference of vortex phases to be positioned at certain points within cells of a crystal lattice of an organic superconductor.\n\nStudies of the Vortex Phases in an Organic Superconductor\n\n\"Lockin Effect, Irreversibility Field, and Josephson Vortex Penetration Depth in κ-(ET)2Cu[N(CN)2]Br \", R. Giannetta N.H. Tea, F.A.B. Chaves, S. Rao, M.B. Salamon, A.M. Kini, H.H. Wang, U. Geiser, J.M. Schlueter, M.W. Trawick, J.C. Garland, Physica C 321, 207 (1999)\n"}
{"id": "45684490", "url": "https://en.wikipedia.org/wiki?curid=45684490", "title": "Ministry of Research, Technology and Higher Education (Indonesia)", "text": "Ministry of Research, Technology and Higher Education (Indonesia)\n\nMinistry of Research, Technology and Higher Education of the Republic of Indonesia is a government ministry that has the task of conducting affairs in the field of research, science and technology in the government to assist the President of Indonesia in carrying out state. The ministry was formerly known as the National Research Affairs Ministry of the Republic of Indonesia.\n\nFounded in 1962 under the name National Research Affairs Ministry of the Republic of Indonesia, and in 1973 changed its name to the Ministry of Research. Year 1986-2001 as Minister of State for Research and Technology, and in 2002 according Circular Minister of State for Administrative Reform concerning Naming Government Agencies, Office of the Secretary of State referred to the Ministry of Research and Technology. In 2005 pursuant to Presidential Decree No. 9 In 2005, this institution called the Ministry of Research and Technology (KNRT) or as the State Ministry of Research and Technology. In 2009 pursuant to Presidential Decree 47 of 2009 referred to the Ministry of Research and Technology.\n\n\nIn formulating the main directions and priorities of development of science and technology as well as the preparation of a strategic policy of national science and technology development, the Ministry of Research and Technology supported by the National Research Council (DRN).\n\n\nBased on Presidential Decree No. 4 of 2003 on the co-ordination of formulation, Strategic Policy Development and Implementation of National Science and Technology, Ministry of Research, and Technology co-ordinate government agencies non-ministry as follows;\n\nMinistry of Research and Technology also co-ordinate, and manage institutions as follows:\n"}
{"id": "3165257", "url": "https://en.wikipedia.org/wiki?curid=3165257", "title": "Nanotechnology in fiction", "text": "Nanotechnology in fiction\n\nThe use of nanotechnology in fiction has attracted scholarly attention. The first use of the distinguishing concepts of nanotechnology was \"There's Plenty of Room at the Bottom\", a talk given by physicist Richard Feynman in 1959. K. Eric Drexler's 1986 book \"Engines of Creation\" introduced the general public to the concept of nanotechnology. Since then, nanotechnology has been used frequently in a diverse range of fiction, often as a justification for unusual or far-fetched occurrences featured in speculative fiction.\n\nIn 1881 the Russian writer Nikolai Leskov wrote \"The Tale of Cross-eyed Lefty from Tula and the Steel Flea\", which included the concept of text that can be seen only through a microscope at 5,000,000 times magnification.\n\nIn 1931, Boris Zhitkov wrote a short story called \"Microhands\" (), where the narrator builds for himself a pair of microscopic remote manipulators, and uses them for fine tasks like eye surgery. When he attempts to build even smaller manipulators to be manipulated by the first pair, the story goes into detail about the problem of regular materials behaving differently on a microscopic scale.\n\nIn his 1956 short story \"The Next Tenants\", Arthur C. Clarke describes tiny machines that operate at the micrometre scale – although not strictly nanoscale (billionth of a meter), they are the first fictional example of the concepts now associated with nanotechnology.\n\nStanislaw Lem's 1964 novel \"The Invincible\" involves the discovery of an artificial ecosystem of minuscule robots, although like in Clarke's story they are larger than what is strictly meant by the term 'nanotechnology'.\n\nRobert Silverberg's 1969 short story \"How It Was when the Past Went Away\" describes nanotechnology being used in the construction of stereo loudspeakers, with a thousand speakers per inch.\n\nThe 1984 novel \"Peace on Earth\" by Stanislaw Lem tells about small bacteria-sized nanorobots looking as normal dust (developed by artificial intelligence placed by humans on the Moon in the era of cold warfare) that has later came to Earth and are replicating, destroying all weapons, modern technology and software, leaving living organisms (as there were no living organisms on the Moon) intact.\n\nThe 1985 novel \"Blood Music\" by Greg Bear (originally a 1983 short story) features genetically engineered white blood cells that eventually learn to manipulate matter on an atomic scale.\n\nThe 1991 novelization of \"\", authored by Randall Frakes, expands the origin story of the T-1000 Terminator through the inclusion of a prologue set in the future. It is explained that the T-1000 is a 'Nanomorph', that was created by Skynet, through the use of programmable Nanotechnology. This was only implied in the film itself.\n\nThe 1992 novel, \"Assemblers of Infinity\" is a science-fiction novel authored by Kevin J. Anderson and Doug Beason. The plot line makes specific mention of nano-assembly and nano-disassembly robots, along with admonitions regarding the dangers that these bacteria-sized machines might pose. \n\nNeal Stephenson's 1995 novel \"The Diamond Age\" is set in a world where nanotechnology is commonplace. Nanoscale warfare, fabrication at the molecular scale, and self-assembling islands all exist.\n\nThe \"Trinity Blood\" series features an alien nanomachine found on Mars which is present in the body of the protagonist, Abel Nighroad. These nanomachines are known as Krusnik nanomachines, and feed on the cells of vampires.\n\nNanobots (called Nanoes) are central to Stel Pavlou's novel \"Decipher\" (2001).\n\nMichael Crichton's novel \"Prey\" (2002) is a cautionary tale about the possible risks of developing nanotechnology. In \"Prey\", a swarm of molecule-sized nanorobots develop intelligence and become a large scale threat.\n\nRobert Ludlum's 2005 novel \"The Lazarus Vendetta\" also focuses around nanotechnology, focusing mainly on its ability to cure cancer.\n\nA nanomorph, term first coined by Science Fiction writer David Pulver in 1986's \"GURPS Robots\", is a fictional robot entirely made of nanomachines. Its brain is distributed throughout its whole body, which also acts as an all-around sensor, hence making it impossible to surprise as long as the target is on line of sight. A nanomorph is arguably the robotic ultimate in versatility, maybe even in power. Further uses of the concept could include using parts of its body as a tracking device, splitting the body for doing several tasks, or merging two nanomorphs in a greater one, or else gliding/flying in an ornithopter-like way (by molding itself like a giant, articulated kite). A common but facultative (without this feature, it would still qualify as a nanomorph) improvement is the ability to cover itself with specific colors and textures in a realistic looking manner (the ultimate being to look like a human, à la doppelgänger).\n\nOne of the first mentions on a television show was an announcement to students over the school loudspeakers in the 1987 \"Max Headroom\" episode, \"Academy\" that, \"Nanotechnology pod test results are posted in the Submicron Lab for your viewing.\"\n\nThe anime series \"\" employs a plotline heavily involved in the use of \"micromachines\" as a form of treatment against complex diseases after a subject undergoing cyberisation.\n\nIn the \"Star Trek\" universe, from \"\" onward, the Borg use nanomachines, referred to as \"nanoprobes\", to assimilate individuals into their collective.\n\nOn the television show \"Red Dwarf\", nanobots played a notable role in series VII to IX. Nanobots are nanotechnology created to be a self-repair system for androids like Kryten as they can also change anything into anything else. Kryten's nanobots grow bored of their duties and take over the ship \"Red Dwarf\", leaving the crew to try and recapture it aboard the smaller \"Starbug\". In the end the ship they are chasing is actually a smaller \"Red Dwarf\" built by the nanobots (which evaded their scanners in the end by coming aboard \"Starbug\"), with the rest being changed into a planet. Once the crew discover this and find the nanobots, they force them to rebuild \"Red Dwarf\" (as well as Dave Lister's then-missing arm). In the end the nanobots build an enhanced \"Red Dwarf\" based on the original design plans. They also resurrect the original full crew killed in the first episode.\n\nThe episode The New Breed of the show \"Outer Limits\" featured nanobots.\n\nNanobots were also featured during the Sci-Fi Channel era of \"Mystery Science Theater 3000\", where they were known as \"nanites\". They were depicted on the show as microscopic, bug-like, freestanding robots with distinct personalities.\n\nNanotechnology appeared several times in the TV series \"Stargate SG-1\" and \"Stargate Atlantis\", in the form of the replicators and the Asurans, respectively. A \"nanovirus\" is also seen in \"Stargate Atlantis\".\n\nIn \"\" (2001), a criminal blows up a tanker trunk containing a nanobot virus that instantly kills thousands.\n\nIn the 2003 film \"Agent Cody Banks\", a scientist creates nanobots programmed to clean up oil spills.\n\nIn the 2004 film \"I, Robot\", nanites are used to wipe out artificial intelligence in the event of a malfunction and are depicted as a liquid containing tiny silver objects.\n\nIn the 2005 \"Doctor Who\" television episode \"The Empty Child/The Doctor Dances\" a metal cylinder falling from space and lands in the time of World War II-era London, breaks releasing nanobots which transform every human it comes into contact with into a gas mask-wearing zombie, like its first contact, a child.\n\nIn the 2008 film \"The Day the Earth Stood Still\", the alien robot \"GORT\" disintegrates into a swarm of self-replicating nanobots shaped like bugs that cover Earth and destroy all humans and artificial structures by seemingly devouring them within seconds.\n\nThe revamped \"Knight Rider\" television series and TV movie incorporate nanotechnology into the Knight Industries Three Thousand (KITT), allowing it to change color and shape, as well as providing abilities such as self-regeneration.\n\nIn the 2009 film \"\", the main plot is to save the world from a warhead containing deadly nanobots called the \"Nanomites\", which if detonated over a city could destroy it in hours.\n\nThe popular NBC science fiction show, \"Revolution\", is based on a worldwide blackout due to the manipulation of nanotechnology.\n\nIn 2010 \"Generator Rex\" was shown on Cartoon Network. It was based on a laboratory experiment going wrong and infecting the world with bad \"Nanites\" which turned people into monsters known as E.V.Os.\n\nIn the \"Ben 10\" series, there is a nanotechnology-based alien called Nanomech, who first appeared in the live-action movie .\n\nNanotechnology is featured heavily within the \"Terminator\" film series. The 1991 film \"\" and 2015 film \"\" feature the T-1000 terminator. The T-1000 is composed of Mimetic Polyalloy, a liquid metal that utilizes nanites for shapeshifting abilities; Giving the T-1000 the ability to mimic anyone it samples through physical contact. It can also form its arms into blades and stabbing weapons and instantly recover from any damage. In the 2003 film \"\" a new terminator, the \"T-X\", also utilities Mimetic Polyalloy for shapeshifting abilities; like the T-1000 it can mimic anyone it touches. The T-X is also equipped with nanotechnological transjectors, and can infect and control other Terminators using nanites.\n\nIn \"Terminator Genisys\", human resistance leader \"John Connor\" is infected with \"machine phase matter\" by a T-5000 terminator, transforming John into a \"T-3000\". Constructed entirely of nanocytes, the T-3000, like the T-1000 and T-X terminators, has shapeshifting and replication abilities. This unit's deadly nanocyte structure gives the T-3000 the unique ability to instantly scatter into particles and then quickly reform to avoid harmful impact as well as instantly recovering from damage.\n\nIn the 2014 film \"Transcendence\", the uploaded consciousness of Will Caster (Johnny Depp) uses nanotechnology to turn himself, and the local townsfolk, into a self-healing defense force with superhuman strength.\n\nIn Venture Brothers Season 6 Episode 3 \"Faking Miracles\" a laboratory accident leads nanobots to enter Dean Venture's body. Billy Quizboy and Peter White take remote control of the nanobots, inadvertently torturing Dean to showcase the power of the nanobots to Dr. Venture. Eventually they are used, unbeknownst to Dean, to improve his intelligence so that he can pass an entrance examination for college. In the post-credit scene Dean painfully urinates them out like a set of kidney stones.\n\nIn the 2018 Marvel Cinematic Universe film \"Black Panther\", the title character's suit of armor is made of nanites that are stored in a necklace when not in use, and also have the ability to absorb and redirect kinetic energy as needed. In the 2018 MCU film \"\", Iron Man's armor is made up of nanites that are stored in the arc reactor in Tony Stark's chest. The nanites are able to self repair damage sustained to it. Spider-Man's Iron Spider suit, also created by Stark, uses nanotechnology as well.\n\nIn \"Total Annihilation\" nanobots are used to build structures.\n\nIn some games of the \"Mortal Kombat\" series, the character Smoke is a cloud of nanobots.\n\nIn \"System Shock 2\" (1999), \"nanites\" are used as currency as well as a type of weapon ammo.\n\nIn \"Deus Ex\" (2000), nanotechnology is an important part of both the plot and game mechanics. A very dangerous technology in the wrong hands, it provides a number of abilities to the protagonist.\n\nThe MMORPG \"Anarchy Online\" (launched 2001) is set on a planet with well-developed nanotechnology, which generally is used as magic in fantasy-themed games.\n\nThe protagonist of \"\" (2001) had artificial blood infused with nanomachine that served functions such as healing. \"\" (2008) featured a great deal of nanotechnology, such as the Sons of the Patriots, an artificial intelligence/nanomachine network that regulated and enhanced the actions of every lawful combatant in the world. In \"Metal Gear Rising Revengeance\" the main antagonist Senator Armstrong also augments himself with nanotechnology.\n\nIn \"Red Faction\" (2001), nanotechnology is used on Mars to control miners, and \"Red Faction Guerilla\" (2009) features nanotechnology, in particular a device called the Nano Forge, as a major plot point.\n\nThe computer game Hostile Waters features a narrative involving nanotech assemblers.\n\nIn the \"Ratchet & Clank\" series, the health system involves nanotechnology. The nanotech can be upgraded by purchase in the first game, or by defeating enemies in other games of the series.\n\nNanotechnology is also found in \"Crysis\" (2007), \"Crysis 2\" (2011), and Crysis 3 (2013). The protagonists of these games are equipped with a \"Nano Suit\", which enables them to become stronger, invisible, heavily armored, etc.\n\nIn \"\" (2009), Reed Richards creates nanites that are meant to control the minds of supervillains. However, the nanites evolve into a group mind called the Fold which serves as the primary antagonist for the game.\n\nIn \"SpaceChem\" the player has to build molecular assembler/disassemblers using nanomachines called \"Waldos\" controlled by a visual programming language.\n\nIn the manga series \"\", nanotechnology is referenced numerously and its use is heavily restricted, owing to the loss of Mercury as a potential planetary colony due to a grey goo catastrophe. Its danger and control has become one of the main driving narratives in the story.\n\nIn Dx13: Nano A Mano - a manga series by Kirupagaren Kanni - the protagonist uses nanobots to create a giant mecha, which is remotely controlled by custom-built equipment such as electronic glove, microphones, cameras, etc.\n\nNanomites appear in the \"G.I. Joe Reinstated\" series published by Devil's Due.\n\nIn the anime and manga series \"Black Cat\", Eve has the ability to manipulate nanomachines. Nanobots are later used for a variety of purposes, from turning victims into berserk warriors to granting Creed Diskenth immortality.\n\nIn the anime and manga series \"To Love-Ru\", the Transformation Weapons Golden Darkness and Mea Kurosaki have nanomachines within them, in the same manner as Eve from Black Cat.\n\nIn the anime and manga series \"Project ARMS\", the ARMS are weapons made from many nanomachines imbued into compatible biological beings, granting them a great variety of combative abilities and regeneration. The four protagonists each have an ARMS that have artificial intelligence, but the Keith series and the modulated ARMS do not.\n\n"}
{"id": "5753978", "url": "https://en.wikipedia.org/wiki?curid=5753978", "title": "Ohnesorge number", "text": "Ohnesorge number\n\nThe Ohnesorge number (Oh) is a dimensionless number that relates the viscous forces to inertial and surface tension forces. The number was defined by Wolfgang von Ohnesorge in his 1936 doctoral thesis.\n\nIt is defined as:\n\nWhere\n\n\nThe Ohnesorge number for a 3 mm diameter rain drop is typically ~0.002. Larger Ohnesorge numbers indicate a greater influence of the viscosity.\n\nThis is often used to relate to free surface fluid dynamics such as dispersion of liquids in gases and in spray technology.\n\nIn inkjet printing, liquids whose Ohnesorge number is less than 1 and greater than 0.1 are jettable (1<Z<10 where Z is the reciprocal of the Ohnersorge number).\n\n"}
{"id": "11102618", "url": "https://en.wikipedia.org/wiki?curid=11102618", "title": "Peggotty Bluff", "text": "Peggotty Bluff\n\nPeggotty Bluff or Peggotty Camp, is a bluff on the north side and near the head of King Haakon Bay, South Georgia. \n\nIn 1916, Ernest Shackleton's Imperial Trans-Antarctic Expedition party from Elephant Island established a camp, using the upturned \"James Caird\" near the head of King Haakon Bay which they called Peggotty Camp, after the Peggotty family in Charles Dickens' \"David Copperfield\" who lived in a home made from a beached boat. While they were waiting in this location, Henry McNish took screws from the boat and put them in the crew's shoes in order that they could walk across ice more easily.\n\nDuring the South Georgia Survey, 1955-56, King Haakon Bay was surveyed and the approximate position of the camp deduced. The name Peggotty Bluff was given to the feature now described, which is close to the ITAE campsite.\n\n"}
{"id": "2388190", "url": "https://en.wikipedia.org/wiki?curid=2388190", "title": "Peter C. Aichelburg", "text": "Peter C. Aichelburg\n\nPeter C. Aichelburg (born 9 November 1941) is an Austrian physicist well known for his contributions to general relativity, particularly for his joint work with Roman Sexl on the Aichelburg–Sexl ultraboost of the Schwarzschild vacuum.\n\nPeter Aichelburg is the second child of Ludwig Aichelburg (born 1917) and Martha Michalek (born 1920), a descendant of the Bohemian line of the House of Aichelburg from Carinthia.\n\nLectures by Walter Thirring influenced him to pursue theoretical problems. He was a postdoc at the International Centre for Theoretical Physics (ICTP), Trieste, Italy from 1968 to 1969. In an interview he recounts that his father was surprised that after such difficult studies he did not immediately obtain a tenured position.\n\nBefore his retirement in November 2007 Aichelburg taught at the University of Vienna, where he held a position in the Institute of Theoretical Physics.\n\n"}
{"id": "31037021", "url": "https://en.wikipedia.org/wiki?curid=31037021", "title": "Postcognitive psychology", "text": "Postcognitive psychology\n\nPostcognitive psychology is the postmodern condition of a psychology yet to come as proposed by theorist Matthew Giobbi. The term postcognitive was first used in Giobbi's book \"A Postcognitive Negation: The Sadomasochistic Dialectic of American Psychology\". Psychologists and theorists have discussed the post-cognitive which Giobbi differentiates by exclusion of the hyphen. Giobbi's postcognitive is a folding upon itself in a non-linear fashion which transcends the narrative function of the hyphen, thus leaving the field on a plateau of new ways of doing psychology.\n"}
{"id": "7922175", "url": "https://en.wikipedia.org/wiki?curid=7922175", "title": "Radmilovac", "text": "Radmilovac\n\nRadmilovac (Serbian Cyrillic: Радмиловац) is a suburban settlement of Belgrade, the capital of Serbia, and an experimental farm of the University of Belgrade's Faculty of Agriculture. It is located in the Belgrade municipality of Grocka.\n\nRadmilovac is actually a westernmost extension of the Belgrade's suburb of Vinča (to which it makes no urban connections). It is located north of the road of \"Smederevski put\" which connects Belgrade and the town of Smederevo. It is located 14 kilometers north-east of downtown Belgrade, between Vinča and Kaluđerica with Leštane being located right across the \"Smederevski put\". Right behind the settlement is the Vinča Nuclear Institute.\n\nThe experimental agricultural farm of Radmilovac, a section of the Faculty of Agriculture in Belgrade is the original core of the neighborhood. Farm originated from the lands bequested to the Faculty by the industrialist, deputy and judge Milan Vukićević in 1941, when he died. His wife Radmila Vukićević was the first manager of the farm from 1941 to 1945 when she died, too.In 1947 the farm was named Radmilovac in her honor (Serbian for “Radmila’s place”). After World War II the land was nationalized, returned to the Faculty in the 1960s, taken by the state again and given to the PKB company, main agricultural supplier of the Belgrade market. In the late 1980s the farm was finally returned to the Faculty again. The reconstruction and expansion of the farm began in 2006, with new small fishponds and projected halls and covered areas.\n\nToday the farm, mainly known for its orchards, covers an area of 86 hectares. It is the largest agricultural gene bank in eastern and southeastern Europe.\n\nOrchards cover 15 hectares and are populated by apples, plums, peaches and pears, including one peach cultivar created here. Trees are planted differently from the usual way, with lesser space between them, only one meter apart (up to 3.000 seedlings per hectare). Anti-hail net is placed between the trees and a drip irrigation system is introduced. One section is reserved for the old local and new worldwide cultivars, including hundreds of unique fruit brands. Internationally known type of brandies are being produced here.\n\nVineyards spread over 13 hectares. Farm developed 23 new grape cultivars, 15 table and 8 wine varieties. Yield varies from 35 to 60 tons per year, with 20 tons internationally recognized brands of wines made of it.\n\nArable land covers 10 hecatares and includes several large greenhouses.\n\nThe farm also contains bees gene bank and 40 beehives. They produce several different types of honey: black locust, floral, sunflower, multifloral (“meadow”). Curiosity is honey produced from sophora (“Japanese acacia”).\n\nCenter for fishery and applied hydrobiology occupies 5 hectares. Formerly, a stream flowed through the farm. It received wastewater from the neighboring settlements and was so polluted that it was named \"Šugavac\" (\"Scabies stream\"). It was conducted underground into the sewage system and instead an artificial short clean stream, named \"Little Danube\". It is 1.5 kilometers long and is a miniature representation of the entire Danube's flow, from the Black Forest to the Black Sea, including islands, peninsulas, hills, mountains and plains.Little Danube is populated with 40 fish species and plants were planted along its banks, both fishes and plants being characteristic for the \"Big\" Danube. A series of fish ponds were created. Main species include common carp and trout. Inside the Center, there are 40 aquariums with numerous types of fish: common barbel, huchen, brook trout, common minnow, eel, goldfish, European mudminnow, common roach, common bream, Wels catfish, zander and Northern pike, but also the genetically mixed fish population. The fishing of carp and catfish is allowed. One section is turned into the botanical water garden with 40 species of aquatic plants, with 15 species of ducks, 5 species of geese, swans and peacocks roaming between the ponds. In one pond a small artificial island is constructed and a fisherman’s house built on it that can be reached by walking over the short hanging bridge. In the house, the old tools used by the fishermen are exhibited, so as the model of \"tikvara\", old type of the fishing boat.\n\nProducts of Radmilovac can be bought on the farm. They include wines, brandies, seedlings, but also some unusual products like a tomato jam.\n\nRadmilovac is a small, exclusively residential settlement of few dozen houses located around the hotel \"Radmilovac\", major such facility in the area between Belgrade and Smederevo. It developed on the hill above the farm, beginning in the late 1970s, and today has an estimated population of 500.\n"}
{"id": "40941387", "url": "https://en.wikipedia.org/wiki?curid=40941387", "title": "Social heuristics", "text": "Social heuristics\n\nSocial heuristics as a tool of bounded rationality are thought to guide behavior and decisions in the social environment. Social environments tend to be characterised by complexity and uncertainty, and agents with limited informational or cognitive resources may rely on simple rules of thumb to make decisions. The class of phenomena described by social heuristics overlap with those typically investigated by social psychology and game theory. At the intersection of these fields, social heuristics have been applied to explain cooperation in the prisoner's dilemma, based on the argument that cooperation is typically advantageous in daily life, and therefore people develop a cooperation heuristic that gets applied even to one-shot anonymous interactions (the so-called \"social heuristics hypothesis\" of human cooperation).\nWithin social psychology, some researchers have viewed heuristics as closely linked to cognitive biases. Others have argued that these biases result from the application of social heuristics depending on the structure of the environment that they operate in. Researchers in the latter approach treat the study of social heuristics as closely linked to social rationality, a field of research that applies the ideas of bounded rationality and heuristics to the realm of social environments. According to them, social heuristics include those that may use social information, operate in social contexts, or both. For instance, the follow-the-majority heuristic uses social information as inputs but is not necessarily applied in a social context, while the equity-heuristic uses non-social information but in a social context such as the allocation of parental resources amongst offspring. Such heuristics may be used by humans and other animals, but may also be potentially applied to artificial intelligent systems.\n\nTypical social heuristics as laid out in a seminal paper are:\n\n\nIn the dominant dual-systems approach in social psychology, heuristics are believed to be automatically and unconsciously applied. The study of social heuristics as a tool of bounded rationality asserts that heuristics may be used consciously or unconsciously.\n"}
{"id": "17737922", "url": "https://en.wikipedia.org/wiki?curid=17737922", "title": "Society for Scientific Exploration", "text": "Society for Scientific Exploration\n\nThe Society for Scientific Exploration, or SSE, is a group committed to studying fringe science. The opinions of the organization in regard to what are the proper limits of scientific exploration are often at odds with those of mainstream science. Critics argue that the SSE is devoted to disreputable ideas far outside the scientific mainstream.\n\nThe Society was founded in 1982. Its first meeting took place at the University of Maryland, College Park in 1982.\n\nOf the SSE and its journal, journalist Michael D.. Lemonick writes, \"Pretty much anything that might have shown up on The X-Files or in the National Enquirer shows up first here. But what also shows up is a surprising attitude of skepticism.\"\n\nThe society's magazine, the \"Journal of Scientific Exploration\", was established to provide a scientific forum for ufology, parapsychology and cryptozoology, having published research articles, essays, book reviews and letters on those and many other topics that are largely ignored in mainstream journals.\n\nThe journal is currently edited by parapsychologist and philosopher Stephen E. Braude.\n\nThe Spirituality and Psychiatry Special Interest Group of the Royal College of Psychiatrists says that the journal has reports about anomalies in science, particularly in the parapsychological and extraterrestrial fields. Some academics have noted that the journal publishes on anomalous issues, topics often on the fringe of science. The journal is indexed in EBSCO Information Services, which provides a range of library database services.\n\nKendrick Frazier, editor of \"Skeptical Inquirer\" and Committee for Skeptical Inquiry fellow, has suggested that:The JSE, while presented as neutral and objective, appears to hold a hidden agenda. They seem to be interested in promoting fringe topics as real mysteries and they tend to ignore most evidence to the contrary. They publish \"scholarly\" articles promoting the reality of dowsing, neo-astrology, ESP, and psychokinesis. Most of the prominent and active members are strong believers in the reality of such phenomena.Clinical community psychologist and professor of social psychology at the University of Connecticut Seth Kalichman regards the journal as a publisher of pseudoscience, with the journal serving as a \"major outlet for UFOology, paranormal activity, extrasensory powers, alien abductions etc\".\n\nPhilosopher of science Noretta Koertge described the journal as an \"attempt to institutionalize pseudoscience\".\n\nSkeptic Robert Sheaffer writes that the SSE journal has published articles implying that certain topics, like paranormal activities, dowsing and reincarnation, are true and have been verified scientifically. The articles, often written in impressive jargon by scientists with impressive academic credentials, try to convince other scientists that further research into those topics is warranted; but, Sheaffer argues, those articles failed to convince the mainstream scientific community.\n\nThe SSE holds an annual meeting in the US every spring and periodic meetings in Europe. In the US meeting, around a hundred of researchers who came to hear talks on, as journalist Michael Lemonick writes, \"among other things, consciousness physics, astrology and parapsychology ... [M]any of the scientists here are on the faculty at major universities, and were doing fine at conventional research. But sometimes that gets boring.\"\n\nAccording to experimental psychologist Roger D. Nelson, head of the Global Consciousness Project, the SSE aims to \"give everyone a respectful hearing. If we think a speaker is doing bad science, we consider it our duty to criticize it. We get our share of lunatics, but they don't hang around long.\"\n\nOn June 19, 1998 it was reported that \"an international panel of scientists\" was convened to conduct \"the first independent review of UFO phenomena since 1966\", according to the wording used by Associated Press. The \"Skeptical Inquirer\" published an article by Robert Sheaffer who wrote that the SSE was a non-mainstream organization that was biased towards uncritically believing UFO phenomena, that the panel included many scientists that were UFO advocates but no scientists that were skeptics of UFO claims, and that all the uphold cases were old cases that had failed to convince any skeptic of its accuracy or veracity. These included the Cash-Landrum incident, the Trans-en-Provence Case and the Aurora, Texas UFO Incident.\n\nAs of 2005:\n\nAs of 2008, the Leaders Emeritus were Peter A. Sturrock, from the Department of Physics & Department of Applied Physics of Stanford University and Larry Frederick and Charles Tolbert from the Department of Astronomy of University of Virginia.\n\n"}
{"id": "25306680", "url": "https://en.wikipedia.org/wiki?curid=25306680", "title": "Structuralism (psychology)", "text": "Structuralism (psychology)\n\nStructuralism in psychology (also structural psychology) is a theory of consciousness developed by Wilhelm Wundt and his student Edward Bradford Titchener. This theory was challenged in the 20th century. It is debated who deserves the credit for finding this field of psychology, but it is widely accepted that Wundt created the foundation on which Titchener expanded. Structuralism as a school of psychology seeks to analyze the adult mind (the total sum of experience from birth to the present) in terms of the simplest definable components and then to find how these components fit together to form more complex experiences as well as how they correlated to physical events. To do this, psychologists employ introspection, self-reports of sensations, views, feelings, emotions, etc.\n\nEdward B. Titchener, along with Wilhelm Wundt, is credited for the theory of structuralism. It is considered to be the first \"school\" of psychology. Because he was a student of Wilhelm Wundt at the University of Leipzig, Titchener's ideas on how the mind worked were heavily influenced by Wundt's theory of voluntarism and his ideas of association and apperception (the passive and active combinations of elements of consciousness respectively). Titchener attempted to classify the structures of the mind, like chemists classify the elements of nature, into the nature.\n\nTitchener said that only observable events constituted that science and that any speculation concerning unobservable events have no place in society (this view was similar to the one expressed by Ernst Mach). In his book, \"Systematic Psychology\", Titchener wrote:\nTitchener believed the mind was the accumulated experience of a lifetime. He believed that he could understand reasoning and the structure of the mind if he could define and categorize the basic components of mind and the rules by which the components interacted.\n\nThe main tool Titchener used to try to determine the different components of consciousness was introspection. Titchener writes in his \"Systematic Psychology\":\nThe state of consciousness which is to be the matter of psychology ... can become an object of immediate knowledge only by way of introspection or self-awareness.\nand in his book \"An Outline of Psychology\":\n...within the sphere of psychology, introspection is the final and only court of appeal, that psychological evidence cannot be other than introspective evidence.\n\nUnlike Wundt's method of introspection, Titchener had very strict guidelines for the reporting of an introspective analysis. The subject would be presented with an object, such as a pencil. The subject would then report the characteristics of that pencil (color, length, etc.). The subject would be instructed not to report the name of the object (pencil) because that did not describe the raw data of what the subject was experiencing. Titchener referred to this as stimulus error.\n\nIn his translation of Wundt's work, Titchener illustrates Wundt as a supporter of introspection as a method through which to observe consciousness. However, introspection only fits Wundt's theories if the term is taken to refer to psychophysical methods.\n\nIntrospection literally means 'looking within', to try to describe a person's memory, perceptions, cognitive processes, and/or motivations. \n\nTitchener's theory began with the question of what each element of the mind is. He concluded from his research that there were three types of mental elements constituting conscious experience: Sensations (elements of perceptions), Images (elements of ideas), and affections (elements of emotions).These elements could be broken down into their respective properties, which he determined were quality, intensity, duration, clearness, and extensity. Both sensations and images contained all of these qualities; however, affections were lacking in both clearness and extensity. And images and affections could be broken down further into just clusters of sensations. Therefore, by following this train of thinking all thoughts were images, which being constructed from elementary sensations meant that all complex reasoning and thought could eventually be broken down into just the sensations which he could get at through introspection.\n\nThe second issue in Titchener's theory of structuralism was the question of how the mental elements combined and interacted with each other to form conscious experience. His conclusions were largely based on ideas of associationism. In particular, Titchener focuses on the law of contiguity, which is the idea that the thought of something will tend to cause thoughts of things that are usually experienced along with it.\n\nTitchener rejected Wundt's notions of apperception and creative synthesis (voluntary action), which were the basis of Wundt's voluntarism. Titchener argued that attention was simply a manifestation of the \"clearness\" property within sensation.\n\nOnce Titchener identified the elements of mind and their interaction, his theory then asked the question of why the elements interact in the way they do. In particular, Titchener was interested in the relationship between the conscious experience and the physical processes. Titchener believed that physiological processes provide a continuous substratum that give psychological processes a continuity they otherwise would not have. Therefore, the nervous system does not cause conscious experience, but can be used to explain some characteristics of mental events..\n\nWilhelm Wundt instructed Titchener, the founder of structuralism, at the University of Leipzig. The 'science of immediate experience' was stated by him. This simply means that the complex perceptions can be raised through basic sensory information. Wundt is often associated in past literature with structuralism and the use of similar introspective methods. Wundt makes a clear distinction between pure introspection, which is the relatively unstructured self-observation used by earlier philosophers, and experimental introspection. Wundt believes this type of introspection to be acceptable since it uses laboratory instruments to vary conditions and make results of internal perceptions more precise.\n\nThe reason for this confusion lies in the translation of Wundt's writings. When Titchener brought his theory to America, he also brought with him Wundt's work. Titchener translated these works for the American audience, and in so doing misinterpreted Wundt's meaning. He then used this translation to show that Wundt supported Titchener's own theories. In fact, Wundt's main theory was that of psychological voluntarism (\"psychologische Voluntarismus\"), the doctrine that the power of the will organizes the mind’s content into higher-level thought processes.\n\nStructuralism has faced a large amount of criticism, particularly from the school of psychology, functionalism which later evolved into the psychology of pragmatism (reconvening introspection into acceptable practices of observation). The main critique of structuralism was its focus on introspection as the method by which to gain an understanding of conscious experience. Critics argue that self-analysis was not feasible, since introspective students cannot appreciate the processes or mechanisms of their own mental processes. Introspection, therefore, yielded different results depending on who was using it and what they were seeking. Some critics also pointed out that introspective techniques actually resulted in retrospection – the memory of a sensation rather than the sensation itself.\n\nBehaviorists, specifically methodological behaviorists, fully rejected even the idea of the conscious experience as a worthy topic in psychology, since they believed that the subject matter of scientific psychology should be strictly operationalized in an objective and measurable way. Because the notion of a mind could not be objectively measured, it was not worth further inquiry. However, radical behaviorism includes thinking, feeling, and private events in its theory and analysis of psychology. Structuralism also believes that the mind could be dissected into its individual parts, which then formed conscious experience. This also received criticism from the Gestalt school of psychology, which argues that the mind cannot be broken down into individual elements.\n\nBesides theoretical attacks, structuralism was criticized for excluding and ignoring important developments happening outside of structuralism. For instance, structuralism did not concern itself with the study of animal behavior, and personality.\n\nTitchener himself was criticized for not using his psychology to help answer practical problems. Instead, Titchener was interested in seeking pure knowledge that to him was more important than commonplace issues.\n\nOne alternative theory to structuralism, to which Titchener took offense, was functionalism (functional psychology). Functionalism was developed by William James in contrast to structuralism. It stressed the importance of empirical, rational thought over an experimental, trial-and-error philosophy. James in his theory included introspection (i.e., the psychologist's study of his own states of mind), but also included things like analysis (i.e., the logical criticism of precursor and contemporary views of the mind), experiment (e.g., in hypnosis or neurology), and comparison (i.e., the use of statistical means to distinguish norms from anomalies) which gave it somewhat of an edge. Functionalism also differed in that it focused on the how useful certain processes were in the brain to the environment you were in and not the processes and other detail like in structuralism.\n\nResearchers are still working to offer objective experimental approaches to measuring conscious experience, in particular within the field of cognitive psychology and is in some ways carrying on the torch of Titchener's ideas. It is working on the same type of issues such as sensations and perceptions. Today, any introspective methodologies are done under highly controlled situations and are understood to be subjective and retrospective. Proponents argue that psychology can still gain useful information from using introspection in this case.\n\n\n"}
{"id": "35076445", "url": "https://en.wikipedia.org/wiki?curid=35076445", "title": "Suicide of Amina Filali", "text": "Suicide of Amina Filali\n\nAmina Filali was a 16-year-old girl from Larache, Morocco, who committed suicide by taking rat poison in 2012 after she was forced to marry her rapist. According to Article 475 of Moroccan law, the rapist was allowed to avoid prosecution by marrying his victim. This incident drew much attention to Moroccan law, and many people expressed a desire to have the law changed. Local human rights groups also called for the repeal of Article 475 of the Moroccan penal code, which de-criminalises a rape if the rapist later marries their victim.\nTwo years after the suicide, the parliament decided to repeal Article 475; it was repealed in 2014.\n\nA documentary about Amina Filali was released in 2013, which found that there had been four similar incidents in the town's history.\n\n\n"}
{"id": "24952021", "url": "https://en.wikipedia.org/wiki?curid=24952021", "title": "Tensor product model transformation", "text": "Tensor product model transformation\n\nIn mathematics, the tensor product (TP) model transformation was proposed by Baranyi and Yam\n\nA free MATLAB implementation of the TP model transformation can be downloaded at or an old version of the toolbox is aviable at MATLAB Central . A key underpinning of the transformation is the higher-order singular value decomposition.\n\nBesides being a transformation of functions, the TP model transformation is also a new concept in qLPV based control which plays a central role in the providing a valuable means of bridging between identification and polytopic systems theories. The TP model transformation is uniquely effective in manipulating the convex hull of polytopic forms, and, as a result has revealed and proved the fact that convex hull manipulation is a necessary and crucial step in achieving optimal solutions and decreasing conservativeness in modern LMI based control theory. Thus, although it is a transformation in a mathematical sense, it has established a conceptually new direction in control theory and has laid the ground for further new approaches towards optimality. Further details on the control theoretical aspects of the TP model transformation can be found here: TP model transformation in control theory.\n\nThe TP model transformation motivated the definition of the \"HOSVD canonical form of TP functions\", on which further information can be found here. It has been proved that the TP model transformation is capable of numerically reconstructing this HOSVD based canonical form. Thus, the TP model transformation can be viewed as a numerical method to compute the HOSVD of functions, which provides exact results if the given function has a TP function structure and approximative results otherwise.\n\nThe TP model transformation has recently been extended in order to derive various types of convex TP functions and to manipulate them. This feature has led to new optimization approaches in qLPV system analysis and design, as described here: TP model transformation in control theory.\n\n\nthat is, using compact tensor notation (using the tensor product operation formula_4 of ):\n\nwhere core tensor formula_6 is constructed from formula_7, and row vector formula_8 contains continuous univariate weighting functions formula_9. The function formula_10 is the formula_11-th weighting function defined on the formula_12-th dimension, and formula_13 is the formula_12-the element of vector formula_15. Finite element means that formula_16 is bounded for all formula_17. For qLPV modelling and control applications a higher structure of TP functions are referred to as TP model.\n\n\nHere formula_19 is a tensor as formula_20, thus the size of the core tensor is formula_21. The product operator formula_22 has the same role as formula_23, but expresses the fact that the tensor product is applied on the formula_24 sized tensor elements of the core tensor formula_25. Vector formula_26 is an element of the closed hypercube formula_27.\n\n\nThis means that formula_30 is inside the convex hull defined by the core tensor for all formula_31.\n\n\nnamely it generates the core tensor formula_35 and the weighting functions formula_36 for all formula_37. Its free MATLAB implementation is downloadable at or at MATLAB Central .\n\nIf the given formula_38 does not have TP structure (i.e. it is not in the class of TP models), then the TP model transformation determines its approximation:\n\nwhere trade-off is offered by the TP model transformation between complexity (number of components in the core tensor or the number of weighting functions) and the approximation accuracy. The TP model can be generated according to various constrains. Typical TP models generated by the TP model transformation are:\n\n\n\n"}
{"id": "8909780", "url": "https://en.wikipedia.org/wiki?curid=8909780", "title": "The Lure of the Local: Sense of Place in a Multicentered Society", "text": "The Lure of the Local: Sense of Place in a Multicentered Society\n\nThe Lure of the Local: Sense of Place in a Multicentered Society is a 1997 study of the sense of place, by American author and 1968 Guggenheim Fellowship winner Lucy Lippard. The phrase, coined by Lippard, in this study refers to a sense of place that an individual can have about where she lives, or where he lived in his childhood.\n"}
{"id": "11451083", "url": "https://en.wikipedia.org/wiki?curid=11451083", "title": "The Man Who Invented the Twentieth Century", "text": "The Man Who Invented the Twentieth Century\n\nThe Man Who Invented the Twentieth Century: Nikola Tesla, forgotten genius of electricity ( : OCLC 40839685) is a book by Robert Lomas detailing the life of Nikola Tesla. Lomas covers the times of the electric engineer in the United States and the inventors' work.\n"}
{"id": "8665223", "url": "https://en.wikipedia.org/wiki?curid=8665223", "title": "Timeline of particle physics", "text": "Timeline of particle physics\n\nThe timeline of particle physics lists the sequence of particle physics theories and discoveries in chronological order. The most modern developments follow the scientific development of the discipline of particle physics.\n\n\n\n"}
{"id": "36950844", "url": "https://en.wikipedia.org/wiki?curid=36950844", "title": "University of Belgrade Faculty of Political Sciences", "text": "University of Belgrade Faculty of Political Sciences\n\nThe Faculty of Political Sciences (, abbreviated FPN) is a constituent institution of the University of Belgrade which focuses on education and research in the fields of political science, international relations, journalism and communication studies, and social policy and social work. It was established in 1968, as a first faculty of this type in former Yugoslavia.\n\nThe building, built in the Brutalist style, is located in the urban neighborhood of Voždovac, close to the Faculty of Organizational Sciences. The Faculty of Political Sciences offers BA, MA and PhD programmes, as well as advanced professional development programmes.\n\nThe Faculty of Political Sciences was established in 1968, after the National Assembly of Serbia brought an establishment act. It was the first institution of its kind in former Yugoslavia. Since its founding to 2013, it has educated more than 8,100 students, with 975 of them having completed MA programmes, and 464 having defended their PhD theses. Many of the alumni have become recognized experts and scholars, professors and high ranking government officials.\n\nMany of the faculties of political sciences, established subsequently in other parts of the former Yugoslavia were formed from the University of Belgrade Faculty of Political Sciences as a core, and a large number of professors at the various faculties of political sciences in all the countries of the former Yugoslavia, had obtained their academic titles at the University of Belgrade Faculty of Political Sciences.\n\nCelebration of the faculty's 40th anniversary was held in the hall of the National Assembly of Serbia in November 2008. It was sponsored by the President of Serbia. An international conference \"State and Democracy\" was held as a part of anniversary.\n\nThe University of Belgrade Faculty of Political Sciences has among its academic staff some of the most eminent experts in the field of political sciences, as well as in a range of others (economy, sociology, philosophy, law, history, etc.) Amongst them, there are many widely recognized within the domestic and international academic community, as well as former or current high government officials – Ministers, Ambassadors, etc.\n\nNotable members of the academic staff include:\n\nStudent life at the Faculty includes a diverse array of student activities. Student organisations at the Faculty of Political Sciences include a Debate Club, European Students Forum, Youth Team Initiative TIM, Serbian Political Forum SPF, United Nations Club, Diplomatic Forum, Action for Political Emancipation of Youth APEM and many others. Students also elect their representatives (Student Vice-Dean and the Student Parliament) to represent their interests in the Faculty bodies, participate in the evaluation of studies and the faculty, and secure the involvement and participation of students in all matters and issues of interest to them.\n\nStudents periodically publish their own newspaper – \"Politikolog\" (The Political Scientist), and take part in various competitions through their sports club. Also, students of journalism prepare radio and TV shows which are broadcast on radios and televisions with national frequencies.\n\nStudents have the opportunity to participate in numerous events and gatherings (national and international). Student Conference at Kopaonik which is held every year provides them with the chance to discuss the most recent developments in both domestic and international politics with prominent experts, professors and diplomats. Study trips that are organized every year include visits to Serbia's diplomatic missions across Europe, European Parliament, European Court of Human Rights, United Nations, International Committee of the Red Cross and many other institutions. Also, a regional gathering of political sciences students called \"Politijada\" is being held every year with students from Serbia, Montenegro, Bosnia and Herzegovina, Republic of Macedonia and Slovenia taking part.\n\n"}
{"id": "55487961", "url": "https://en.wikipedia.org/wiki?curid=55487961", "title": "W. A. Gayle Planetarium", "text": "W. A. Gayle Planetarium\n\nThe W.A. Gayle Planetarium is operated by Troy University for the city of Montgomery, Alabama. It provides public presentations and exhibits on astronomy, planetary science, and space exploration. \n\nThe W.A. Gayle Planetarium is a planetarium located in Oak Park in the city of Montgomery, Alabama. Dedicated on September 25, 1968, the planetarium is named after William Armistead Gayle, mayor of Montgomery from 1952 to 1959. It is currently operated by Troy University for the City of Montgomery.\n\nDaily public shows typically offer visitors a full-dome movie about a current topic in astronomy and planetary science. Interactive presentations provide a tour of the night sky over Montgomery, pointing out what planets, stars, and constellations can be seen overhead. The 159-seat theater uses a Super MediaGlobe II digital projection system (installed in 2014) to simulate the night sky. Other exhibits include a scale model of the Hubble Space Telescope, images of celestial objects, and a black lit hallway depicting the history of astronomy. The planetarium is currently closed on Saturdays.\n"}
{"id": "28838375", "url": "https://en.wikipedia.org/wiki?curid=28838375", "title": "Wendy Schmidt Oil Cleanup X Challenge", "text": "Wendy Schmidt Oil Cleanup X Challenge\n\nThe Wendy Schmidt Oil Cleanup X CHALLENGE was a challenge award offered by the X PRIZE Foundation for efficient capturing of crude oil from ocean water. Inspired by the ongoing \"Deepwater Horizon\" oil spill disaster, the award was announced on July 29, 2010, and the official one-year competition began on August 1, 2010. The first three teams were to be awarded US$1 million, US$300,000 and US$100,000 respectively.\n\nOf the over 350 teams that preregistered to participate in the challenge by January 2011, 37 officially registered the following month. By the time of the official testing in May 2011, the field of challengers had been narrowed to ten teams.\n\nThe winners for the challenge were announced on October 11, 2011. The first-place finisher and winner of the $1 million prize was the team of Elastec/American Marine using a design similar to a Tesla turbine. The second-place finisher and winner of $300,000 was the team NOFI. No other team met all of the challenge's goals, so the planned third place prize was not awarded.\n\nThe US$1.4 million prize purse was offered by Wendy Schmidt from The Schmidt Family Foundation.\n\n"}
{"id": "8467910", "url": "https://en.wikipedia.org/wiki?curid=8467910", "title": "Wilhelm Lilljeborg", "text": "Wilhelm Lilljeborg\n\nWilhelm Lilljeborg (6 October 1816 – 24 July 1908) was a Swedish zoologist. He is particularly known for his work on the Cladocera of Sweden, and on the Balaenoptera. Lilljeborg was a member of the Royal Swedish Academy of Sciences from 1861.\n\n"}
{"id": "11001950", "url": "https://en.wikipedia.org/wiki?curid=11001950", "title": "Łukasiewicz logic", "text": "Łukasiewicz logic\n\nIn mathematics, Łukasiewicz logic (; ) is a non-classical, many-valued logic. It was originally defined in the early 20th century by Jan Łukasiewicz as a three-valued logic; it was later generalized to \"n\"-valued (for all finite \"n\") as well as infinitely-many-valued (ℵ-valued) variants, both propositional and first-order. The ℵ-valued version was published in 1930 by Łukasiewicz and Alfred Tarski; consequently it is sometimes called the Łukasiewicz-Tarski logic. It belongs to the classes of t-norm fuzzy logics and substructural logics.\n\nThis article presents the Łukasiewicz[-Tarski] logic in its full generality, i.e. as an infinite-valued logic. For an elementary introduction to the three-valued instantiation Ł, see three-valued logic.\n\nThe propositional connectives of Łukasiewicz logic are\n\"implication\" formula_1,\n\"negation\" formula_2,\n\"equivalence\" formula_3,\n\"weak conjunction\" formula_4,\n\"strong conjunction\" formula_5,\n\"weak disjunction\" formula_6,\n\"strong disjunction\" formula_7,\nand propositional constants formula_8 and formula_9.\nThe presence of conjunction and disjunction is a common feature of substructural logics without the rule of contraction, to which Łukasiewicz logic belongs.\n\nThe original system of axioms for propositional infinite-valued Łukasiewicz logic used implication and negation as the primitive connectives:\n\nPropositional infinite-valued Łukasiewicz logic can also be axiomatized by adding the following axioms to the axiomatic system of monoidal t-norm logic:\n\nThat is, infinite-valued Łukasiewicz logic arises by adding the axiom of double negation to basic t-norm logic BL, or by adding the axiom of divisibility to the logic IMTL.\n\nFinite-valued Łukasiewicz logics require additional axioms.\n\nInfinite-valued Łukasiewicz logic is a real-valued logic in which sentences from sentential calculus may be assigned a truth value of not only zero or one but also any real number in between (e.g. 0.25). Valuations have a recursive definition where:\n\nand where the definitions of the operations hold as follows:\n\nThe truth function formula_25 of strong conjunction is the Łukasiewicz t-norm and the truth function formula_26 of strong disjunction is its dual t-conorm. The truth function formula_27 is the residuum of the Łukasiewicz t-norm. All truth functions of the basic connectives are continuous.\n\nBy definition, a formula is a tautology of infinite-valued Łukasiewicz logic if it evaluates to 1 under any valuation of propositional variables by real numbers in the interval [0, 1].\n\nUsing exactly the same valuation formulas as for real-valued semantics Łukasiewicz (1922) also defined (up to isomorphism) semantics over \n\nThe standard real-valued semantics determined by the Łukasiewicz t-norm is not the only possible semantics of Łukasiewicz logic. General algebraic semantics of propositional infinite-valued Łukasiewicz logic is formed by the class of all MV-algebras. The standard real-valued semantics is a special MV-algebra, called the \"standard MV-algebra\".\n\nLike other t-norm fuzzy logics, propositional infinite-valued Łukasiewicz logic enjoys completeness with respect to the class of all algebras for which the logic is sound (that is, MV-algebras) as well as with respect to only linear ones. This is expressed by the general, linear, and standard completeness theorems:\n\nFont, Rodriguez and Torrens introduced in 1984 the Wajsberg algebra as an alternative model for the infinite-valued Łukasiewicz logic.\n\nA 1940s attempt by Grigore Moisil to provide algebraic semantics for the \"n\"-valued Łukasiewicz logic by means of his Łukasiewicz–Moisil (LM) algebra (which Moisil called \"Łukasiewicz algebras\") turned out to be an incorrect model for \"n\" ≥ 5. This issue was made public by Alan Rose in 1956. C. C. Chang's MV-algebra, which is a model for the ℵ-valued (infinitely-many-valued) Łukasiewicz-Tarski logic, was published in 1958. For the axiomatically more complicated (finite) \"n\"-valued Łukasiewicz logics, suitable algebras were published in 1977 by Revaz Grigolia and called MV-algebras. MV-algebras are a subclass of LM-algebras, and the inclusion is strict for \"n\" ≥ 5. In 1982 Roberto Cignoli published some additional constraints that added to LM-algebras produce proper models for \"n\"-valued Łukasiewicz logic; Cignoli called his discovery \"proper Łukasiewicz algebras\".\n\n"}
