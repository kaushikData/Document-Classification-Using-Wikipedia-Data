{"id": "34698401", "url": "https://en.wikipedia.org/wiki?curid=34698401", "title": "A Universe from Nothing", "text": "A Universe from Nothing\n\nA Universe from Nothing: Why There Is Something Rather than Nothing is a non-fiction book by the physicist Lawrence M. Krauss, initially published on January 10, 2012 by Free Press. It discusses modern cosmogony and its implications for the debate about the existence of God. The main theme of the book is how \"we have discovered that all signs suggest a universe that could and plausibly did arise from a deeper nothing—involving the absence of space itself and— which may one day return to nothing via processes that may not only be comprehensible but also processes that do not require any external control or direction.\"\n\nThe book ends with an afterword by Richard Dawkins in which he compares the potential impact of the book to that of \"The Origin of Species\" — a comparison that Krauss himself called \"pretentious\". Christopher Hitchens had agreed to write a foreword for the book prior to his death but was too ill to complete it. To write the book, Krauss expanded material from a lecture on the cosmological implications of a flat expanding universe he gave to the Richard Dawkins Foundation at the 2009 Atheist Alliance International conference. The book appeared on \"The New York Times\" bestseller list on January 29, 2012.\n\nIn the \"New York Times\", philosopher of science and physicist David Albert said the book failed to live up to its title; he claimed Krauss dismissed concerns about what Albert calls his misuse of the term \"nothing\".\n\nCaleb Scharf, writing in \"Nature\", said that \"it would be easy for this remarkable story to revel in self-congratulation, but Krauss steers it soberly and with grace\".\n\nRay Jayawardhana, Canada Research Chair in observational astrophysics at the University of Toronto, wrote for \"The Globe and Mail\" that Kraus \"delivers a spirited, fast-paced romp through modern cosmology and its strong underpinnings in astronomical observations and particle physics theory\" and that he \"makes a persuasive case that the ultimate question of cosmic origin – how something, namely the universe, could arise from nothing – belongs in the realm of science rather than theology or philosophy\".\n\nIn \"New Scientist\", Michael Brooks wrote, \"Krauss will be preaching only to the converted. That said, we should be happy to be preached to so intelligently. The same can't be said about the Dawkins afterword, which is both superfluous and silly.\"\n\nCommenting on the philosophical debate sparked by the book, the physicist Sean M. Carroll asked, \"Do advances in modern physics and cosmology help us address these underlying questions, of why there is something called the universe at all, and why there are things called 'the laws of physics,' and why those laws seem to take the form of quantum mechanics, and why some particular wave function and Hamiltonian? In a word: no. I don't see how they could.\"\n\n"}
{"id": "57560460", "url": "https://en.wikipedia.org/wiki?curid=57560460", "title": "Affect as information hypothesis", "text": "Affect as information hypothesis\n\nIn cognitive psychology the affect-as-information hypothesis, or ‘approach’ is a model of evaluative processing, postulating that affective feelings provide a source of information about objects, tasks, and decision alternatives. A goal of this approach is to understand the extent of influence that affect has on cognitive functioning. It has been proposed that affect has two major dimensions, namely affective valence and affective arousal, and in this way is an embodied source of information. Affect is thought to impact three main cognitive functions: judgement, thought processing and memory. In a variety of scenarios, the influence of affect on these processes is thought to be mediated by its effects on attention. The approach is thought to account for a wide variety of behavioural phenomena in psychology.\n\nThe affect as information hypothesis emphasises significance of the information that affect communicates, rather than the affective feelings themselves. Affective reactions or ‘responses’ provide an embodied source of information about ‘value’ or valence, as well as affective arousal provides an embodied source of information about importance. More specifically, affective reactions are pleasant or unpleasant reactions that provide information on positive (‘good’) and negative (‘bad’) value, respectively. Objects, task situations, and other stimuli or targets- if experienced as the source of an affective reaction- take on the positive or negative affect value. Affective arousal involves implicit (stress hormonal response such as from the adrenergic system) or explicit responses (subjective experience) that provide information on relevance, urgency, or importance. Both dimensions are thought to effect cognitive functioning through influencing judgements and decision making, processing, and memory, and in a variety of situations this is thought to be mediated through its influences on attention.\n\nA finding by Schwarz and Clore (1988) has been noted in literature (for example,), wherein individuals are thought to review and attend to their affective reactions when making evaluative judgements and decisions. They typically ask themselves, “How do I feel about it”. A broad example of this (which may be applied to a variety of situations) is when considering how much we like something. Our judgements are based on how we feel about a particular stimulus as opposed to the attributes or characteristics of the said stimulus. Essentially, if the object, task situation, or other stimulus is experienced as the source of an affective reaction and takes on the affect value, then a judgement or decision may be made about the stimulus based on these attributed feelings. Positive and negative affect typically lead to positive and negative judgements, respectively.\n\nAffective arousal is thought to intensify positive and negative affective reactions and, as a result, judgemental evaluations- as if relying on affective arousal to indicate how strongly one feels about something. An empirical example of this is findings of intensified evaluations under high affective arousal when watching advertisements. Advertisements with positive and negative affective tone were evaluated as more positive and negative respectively, under conditions of high affective arousal relative to under conditions of low affective arousal.\n\nAffect may be elicited by the stimulus or situation of relevance, which is referred to as integral affect. Conversely, affect may be momentary and coincide in time with the presence of the relevant stimulus, but be unrelated to the stimulus. This is referred to as incidental affect. Integral affect provides meaningful, relevant information about a target stimulus on which a judgement or decision can be made. Incidental affect may lead to affective feelings being subsequently mis-attributed to the stimulus at hand and thus provide mistaken information about the stimulus, from which a mislead judgement or decision is formed.\n\nThere is a variety of research investigating the influence of affect on judgement, and the misattribution of affect. One study in the area investigated affective valence. They induced positive and negative affective states based on a life event (happy versus sad), and the weather (sunny versus rainy). Induced positive affective states (good experiences, sunny weather) lead to more positive evaluations of ratings of general well-being, relative to in an induced negative affective state (sad experiences, rainy weather), which lead to more negative life evaluations. They also found that negative life evaluations from an induced negative affective state was removed when the state was induced to be attributed to an irrelevant source (not to life satisfaction). This misattribution did not occur when in an induced positive affective state. Their conclusion was that the results indicate that affective state indirectly influenced judgements of life satisfaction through the type of information conveyed by affective feelings associated with life satisfaction.\n\nAnother study in the area illustrates misattribution, and an effect of affective arousal on judgement. They induced high affective arousal in the experimental condition where participants had to cross a high bridge, and a low affective arousal state in the control condition where participants had to cross a low bridge. All participants were male, and met an attractive female demonstrator on the other side of the bridge. They found that participants in the induced high affective arousal condition contacted the attractive demonstrator after the experiment significantly more than those in the induced low affective arousal condition. Males with a high affective arousal state had amplified attraction to the woman, as they misattributed their feelings of arousal caused by the bridge, to feelings of affective arousal caused by the attractive woman.\n\nAffect has been indicated to provide information about how benign versus problematic a situation or task is, which is dependent on the type of affective reaction or arousal. Consequently, this influences the type of information processing style that is used to interpret the situation. \nIf the affect is attributed as a response to a situation or stimulus then positive affect signals a benign situation that promotes relational processing which involves primarily top down processing of information. Conversely, negative affect signals a problematic situation, inhibiting processing associated with positive affect and promoting referential processing which is associated primarily with bottom up processing.\n\nAn example of the influence of affect on processing style is that of its influence on the use of a global versus a local focus. Positive affect is thought to induce a more global processing style (processing of wholes). Conversely, negative affect reduces global processing and enhances a more local processing style (processing of parts or details).\n\nOther research in this area has illustrated that the informative influence of affective valence is dependent on the accessibility of these global and local processing styles. In situations when either a global or local style was more accessible, positive affect lead to a greater increase in global or local processing respectively, compared to negative affect.\n\nAffective valence has also been indicated to have informative effects on stereotypical thought processing. Positive affect is thought to lead to increased stereotypical thought processing, illustrated in research where participants in an induced positive mood were more likely than those in an induced neutral mood or negative mood to make a stereotypical judgement. This was suggested to be due to stereotypical thought elicited from the positive affect. Additionally, negative affect has been demonstrated to lead to decreased stereotypical thought processing.\n\nAs in the effect of arousal in judgement, affective arousal is proposed to intensify cognitive processing that has been induced as a result of information provided by affective valence. An illustration is that empirical research on false memories showed with high affective arousal, more false memories were produced within different valence conditions (positive, negative, neutral) than with low affective arousal.\n\nAffective arousal has predominantly been discussed with regard to the informative influence of affect on memory. However, the influence of affective valence on memory has been demonstrated in research, such as at memory retrieval. It was illustrated that after exposure to pleasant or unpleasant odours to induce positive or negative affective valence respectively, those with induced positive affect provided significantly more positive ratings (rated as happy memory) of a retrieved memory than those with induced negative affect. Affective valence has also been implicated at memory encoding.\n\nAffective arousal has been indicated to influence memory as it redirects or narrows attention towards potentially important or salient factors such as environmental stimuli, thus influencing what information gets encoded in to memory. As mentioned, this effect is mediated by implicit and explicit affective arousal responses, indicative of the level of importance in a situation. Affective arousal has also been implicated in consolidation of long-term memory through the mediating effects of stress hormones of the adrenergic system on the amygdala- whose functions include memory consolidation.\n\nEmpirical research investigating both affective valence and arousal found effects on different aspects of memory. In their research, they investigated the effect of induced positive versus negative affective valence, and induced high and low arousal on memory encoding using picture slides, in a delayed (to assess long-term memory) and immediate free-recall task. In both free recall conditions, high arousal at encoding lead to better memory performance than low arousal at encoding. A similar finding occurred for affective valence where positive affect at encoding lead to better memory performance, however only for the immediate free-recall condition. They concluded that their results indicate the influence of both dimensions of affect at encoding, and involvement of affective arousal in long-term memory.\n\nA researcher in the field of cognitive psychology by the name of Norbert Schwarz was interested in the informative function of feelings. He commenced research alongside Robert Wyer and Gerald Clore. Building off the ideas of earlier works, one he summarised was that current feelings elicited increase the availability of information congruent with that feeling. He recognised that these ideas were developed upon by the earlier researchers; Isen, Shalker, Clark, and Karp (1978), and Bower (1981), who were researching affect and cognitive functioning. Additionally, the influential idea in this field stemming from the work of Wyer and Carlston (1979) was that a function of affect is to provide information about a target. Their investigation into the functions of feelings gave further insight into the informative functions of feelings, from which they derived core postulates of their theory. Development of the theory and postulates derived are detailed and summarised in Schwarz’s (2010) ‘Feelings as information theory’.\n"}
{"id": "22696309", "url": "https://en.wikipedia.org/wiki?curid=22696309", "title": "Alternative employment arrangements", "text": "Alternative employment arrangements\n\nIn economics, alternative employment arrangements are categorized in four types of alternative employment arrangements: independent contractors, on-call workers, temporary help agency workers, and workers provided by contract firms. \n\n\n"}
{"id": "24884394", "url": "https://en.wikipedia.org/wiki?curid=24884394", "title": "Auxesis (biology)", "text": "Auxesis (biology)\n\nAuxesis (from the Greek word meaning increase; grow) refers to growth from an increase in cell size rather than an increase in the number of cells. Auxetic growth occurs in certain tissues, such as muscle, of the higher animals as well as in some organisms, such as nematodes, tunicates, and rotifers. \n\nIn plant physiology, an auxetic substance will tend to increase cell growth without any cell division. Auxins are auxetic plant hormones.\n\n"}
{"id": "2924276", "url": "https://en.wikipedia.org/wiki?curid=2924276", "title": "Bathvillite", "text": "Bathvillite\n\nBathvillite is a naturally occurring organic substance. It is an amorphous, opaque, and very friable material of fawn-brown color, filling cavities in the torbanite or Boghead coal of Bathville, Lothian, Scotland. It has a specific gravity of 1.01, and is insoluble in benzene. It may resemble wood in its final stage of decay.\n"}
{"id": "17541682", "url": "https://en.wikipedia.org/wiki?curid=17541682", "title": "Bert Main", "text": "Bert Main\n\nProfessor Albert (Bert) Russell Main CBE FAA FANZAAS (6 March 1919 – 3 December 2009) was an Australian zoologist.\n\nBorn in Perth, Western Australia, he studied zoology at The University of Western Australia. He served in the Australian Imperial Force and the Royal Australian Air Force during World War II, but later returned to zoology, qualifying as a Doctor of Philosophy in 1956, and becoming a Professof of Zoology in 1967.\n\nHe received many honours for his contribution to zoology including the Mueller Medal, the Gold Medal of the Australian Ecological Society, a CBE, the Centenary Medal, and a Royal Society of Western Australia Medal. He was elected a Fellow of the Australian Academy of Science in 1969.\n\nHe was married to arachnologist Barbara York Main.\n\n"}
{"id": "87713", "url": "https://en.wikipedia.org/wiki?curid=87713", "title": "Cargo cult science", "text": "Cargo cult science\n\nCargo cult science is a phrase describing practices that have the semblance of being scientific, but do not in fact follow the scientific method. The term was first used by physicist Richard Feynman during his 1974 commencement address at the California Institute of Technology.\n\nCargo cults are religious practices that have appeared in many traditional tribal societies in the wake of interaction with technologically advanced cultures. They focus on obtaining the material wealth (the \"cargo\") of the advanced culture by imitating the actions they believe cause the appearance of cargo: by building landing strips, mock aircraft, mock radios, and the like. Similarly, although cargo cult sciences employ the trappings of the scientific method, like an airplane with no motor, they fail to deliver anything of value.\n\nThe speech is widely posted, and Feynman adapted it in his book \"Surely You're Joking, Mr. Feynman!\". Feynman based the phrase on a concept in anthropology, the cargo cult, which describes how some pre-industrialized cultures interpreted technologically sophisticated visitors as religious or supernatural figures who brought boons of cargo. Later, in an effort to call for a second visit the natives would develop and engage in complex religious rituals, mirroring the previously observed behavior of the visitors manipulating their machines but without understanding the true nature of those tasks. Just as cargo cultists create mock airports that fail to produce airplanes, cargo cult scientists conduct flawed research that superficially resembles the scientific method, but which fails to produce scientifically useful results.\n\nThe following is an excerpt from a speech (taken from the book).\nIn the South Seas there is a cargo cult of people. During the war they saw airplanes land with lots of good materials, and they want the same thing to happen now. So they've arranged to imitate things like runways, to put fires along the sides of the runways, to make a wooden hut for a man to sit in, with two wooden pieces on his head like headphones and bars of bamboo sticking out like antennas—he's the controller—and they wait for the airplanes to land. They're doing everything right. The form is perfect. It looks exactly the way it looked before. But it doesn't work. No airplanes land. So I call these things cargo cult science, because they follow all the apparent precepts and forms of scientific investigation, but they're missing something essential, because the planes don't land.\nFeynman cautioned that to avoid becoming cargo cult scientists, researchers must avoid fooling themselves, be willing to question and doubt their own theories and their own results, and investigate possible flaws in a theory or an experiment. He recommended that researchers adopt an unusually high level of honesty which is rarely encountered in everyday life, and gave examples from advertising, politics, and psychology to illustrate the everyday dishonesty which should be unacceptable in science. Feynman cautioned,\n\nWe've learned from experience that the truth will come out. Other experimenters will repeat your experiment and find out whether you were wrong or right. Nature's phenomena will agree or they'll disagree with your theory. And, although you may gain some temporary fame and excitement, you will not gain a good reputation as a scientist if you haven't tried to be very careful in this kind of work. And it's this type of integrity, this kind of care not to fool yourself, that is missing to a large extent in much of the research in cargo cult science.\nAn example of cargo cult science is an experiment that uses another researcher's results in lieu of an experimental control. Since the other researcher's conditions might differ from those of the present experiment in unknown ways, differences in the outcome might have no relation to the independent variable under consideration. Other examples, given by Feynman, are from educational research, psychology (particularly parapsychology), and physics. He also mentions other kinds of dishonesty, for example, falsely promoting one's research to secure funding. Feynman believed a scientist of integrity must attempt to give out as much information as possible about their experiments so others could accurately appraise their contribution.\n\n\nIn his commencement address, Richard Feynman stated his belief that the antidote to both cargo cult science and pseudoscience is scientific integrity, which he describes as, \"a kind of leaning over backwards\" to make sure scientists do not fool themselves or others. According to Feynman an ethical scientist must make the extra effort to ensure that their methods and results are transparent, allowing other people to accurately appraise and understand the scientist's research. Feynman uses the case of a Wesson cooking oil advertisement as an example of an unethical and deceptive use of science that delivers nothing of value. The advertisement made the claim that the oil would not soak through food. In reality no oil will soak through food if it is cold enough, and all oil will soak through food if hot enough. Since these facts would not advance Wesson's agenda, these facts were not made readily available for consumers.\n\nIn the eighth chapter of his book \"Interpreting Biomedical Science,\" Ülo Maiväli presents some possible solutions to the perceived prevalence of cargo cult science. These proposed solutions are as follows: the integration, or the making whole or of science, a more philosophical approach to science, training that instills values of common ownership, egalitarianism and disinterestedness, and a system that distances scientists from economical temptation and scarcity. This system would reduce the amount of graduate studies and graduate students to reduce funding pressures. Ideally this would give researchers the financial support to be honest and innovative. Maiväli postulates that honesty is more important than methodology, and is the most effective antidote to cargo cult science.\n\n\n"}
{"id": "1455034", "url": "https://en.wikipedia.org/wiki?curid=1455034", "title": "Carlo Beenakker", "text": "Carlo Beenakker\n\nCarlo Willem Joannes Beenakker (born June 9, 1960) is a professor at Leiden University and leader of the university's mesoscopic physics group, established in 1992.\n\nBorn in Leiden as the son of physicists and Elena Manaresi, Beenakker graduated from Leiden University in 1982 and obtained his doctorate two years later.\n\nAfter the awarding of his doctorate, he then spent one year working in the United States of America as a fellow of the Niels Stensen Foundation before returning to the Netherlands as a member of the scientific staff of the Philips Research Laboratories in Eindhoven. He was made External Professor of Theoretical Physics at Leiden in 1991.\n\nHis work in mesoscopic physics addresses fundamental physical problems that occur when a macroscopic object is miniaturized.\n\nIn 1993, he shared the Royal/Shell prize for \"the discovery and explanation of quantum effects in the electrical conduction in mesoscopic systems\". He was elected a member of the Royal Holland Society of Sciences and Humanities in 2001, and the Royal Netherlands Academy of Arts and Sciences in 2002. He was awarded one of the Netherlands' most prestigious science awards, the Spinozapremie, in 1999. In 2006 he was honored with the AkzoNobel Science Award \"for his pioneering work in the field of nanoscience\".\n\nIn a 1997 study by the Institute for Scientific Information, Beenakker rated in the top 300 most cited physicists of the previous 16 years.\n\nIn 2008, Beenakker attended the 24th Solvay Conference on Physics.\n\n"}
{"id": "4480666", "url": "https://en.wikipedia.org/wiki?curid=4480666", "title": "Ceramic engineering", "text": "Ceramic engineering\n\nCeramic engineering is the science and technology of creating objects from inorganic, non-metallic materials. This is done either by the action of heat, or at lower temperatures using precipitation reactions from high-purity chemical solutions. The term includes the purification of raw materials, the study and production of the chemical compounds concerned, their formation into components and the study of their structure, composition and properties.\n\nCeramic materials may have a crystalline or partly crystalline structure, with long-range order on atomic scale. Glass ceramics may have an amorphous or glassy structure, with limited or short-range atomic order. They are either formed from a molten mass that solidifies on cooling, formed and matured by the action of heat, or chemically synthesized at low temperatures using, for example, hydrothermal or sol-gel synthesis.\n\nThe special character of ceramic materials gives rise to many applications in materials engineering, electrical engineering, chemical engineering and mechanical engineering. As ceramics are heat resistant, they can be used for many tasks for which materials like metal and polymers are unsuitable. Ceramic materials are used in a wide range of industries, including mining, aerospace, medicine, refinery, food and chemical industries, packaging science, electronics, industrial and transmission electricity, and guided lightwave transmission.\n\nThe word \"ceramic\" is derived from the Greek word κεραμικός (\"keramikos\") meaning pottery. It is related to the older Indo-European language root \"to burn\",\n\n\"Ceramic\" may be used as a noun in the singular to refer to a ceramic material or the product of ceramic manufacture, or as an adjective. The plural \"ceramics\" may be used to refer the making of things out of ceramic materials. Ceramic engineering, like many sciences, evolved from a different discipline by today's standards. Materials science engineering is grouped with ceramics engineering to this day.\n\nAbraham Darby first used coke in 1709 in Shropshire, England, to improve the yield of a smelting process. Coke is now widely used to produce carbide ceramics. Potter Josiah Wedgwood opened the first modern ceramics factory in Stoke-on-Trent, England, in 1759. Austrian chemist Carl Josef Bayer, working for the textile industry in Russia, developed a process to separate alumina from bauxite ore in 1888. The Bayer process is still used to purify alumina for the ceramic and aluminium industries. Brothers Pierre and Jacques Curie discovered piezoelectricity in Rochelle salt circa 1880. Piezoelectricity is one of the key properties of electroceramics.\n\nE.G. Acheson heated a mixture of coke and clay in 1893, and invented carborundum, or synthetic silicon carbide. Henri Moissan also synthesized SiC and tungsten carbide in his electric arc furnace in Paris about the same time as Acheson. Karl Schröter used liquid-phase sintering to bond or \"cement\" Moissan's tungsten carbide particles with cobalt in 1923 in Germany. Cemented (metal-bonded) carbide edges greatly increase the durability of hardened steel cutting tools. W.H. Nernst developed cubic-stabilized zirconia in the 1920s in Berlin. This material is used as an oxygen sensor in exhaust systems. The main limitation on the use of ceramics in engineering is brittleness.\n\nThe military requirements of World War II encouraged developments, which created a need for high-performance materials and helped speed the development of ceramic science and engineering. Throughout the 1960s and 1970s, new types of ceramics were developed in response to advances in atomic energy, electronics, communications, and space travel. The discovery of ceramic superconductors in 1986 has spurred intense research to develop superconducting ceramic parts for electronic devices, electric motors, and transportation equipment.\n\nThere is an increasing need in the military sector for high-strength, robust materials which have the capability to transmit light around the visible (0.4–0.7 micrometers) and mid-infrared (1–5 micrometers) regions of the spectrum. These materials are needed for applications requiring transparent armour. Transparent armor is a material or system of materials designed to be optically transparent, yet protect from fragmentation or ballistic impacts. The primary requirement for a transparent armour system is to not only defeat the designated threat but also provide a multi-hit capability with minimized distortion of surrounding areas. Transparent armour windows must also be compatible with night vision equipment. New materials that are thinner, lightweight, and offer better ballistic performance are being sought.\nSuch solid-state components have found widespread use for various applications in the electro-optical field including: optical fibres for guided lightwave transmission, optical switches, laser amplifiers and lenses, hosts for solid-state lasers and optical window materials for gas lasers, and infrared (IR) heat seeking devices for missile guidance systems and IR night vision.\n\nNow a multibillion-dollar a year industry, ceramic engineering and research has established itself as an important field of science. Applications continue to expand as researchers develop new kinds of ceramics to serve different purposes.\n\n\n\nGlass-ceramic materials share many properties with both glasses and ceramics. Glass-ceramics have an amorphous phase and one or more crystalline phases and are produced by a so-called \"controlled crystallization\", which is typically avoided in glass manufacturing. Glass-ceramics often contain a crystalline phase which constitutes anywhere from 30% [m/m] to 90% [m/m] of its composition by volume, yielding an array of materials with interesting thermomechanical properties.\n\nIn the processing of glass-ceramics, molten glass is cooled down gradually before reheating and annealing. In this heat treatment the glass partly crystallizes. In many cases, so-called 'nucleation agents' are added in order to regulate and control the crystallization process. Because there is usually no pressing and sintering, glass-ceramics do not contain the volume fraction of porosity typically present in sintered ceramics.\n\nThe term mainly refers to a mix of lithium and aluminosilicates which yields an array of materials with interesting thermomechanical properties. The most commercially important of these have the distinction of being impervious to thermal shock. Thus, glass-ceramics have become extremely useful for countertop cooking. The negative thermal expansion coefficient (TEC) of the crystalline ceramic phase can be balanced with the positive TEC of the glassy phase. At a certain point (~70% crystalline) the glass-ceramic has a net TEC near zero. This type of glass-ceramic exhibits excellent mechanical properties and can sustain repeated and quick temperature changes up to 1000 °C.\n\nThe traditional ceramic process generally follows this sequence: Milling → Batching → Mixing → Forming → Drying → Firing → Assembly.\n\n\nCeramic forming techniques include throwing, slipcasting, tape casting, freeze-casting, injection moulding, dry pressing, isostatic pressing, hot isostatic pressing (HIP) and others. Methods for forming ceramic powders into complex shapes are desirable in many areas of technology. Such methods are required for producing advanced, high-temperature structural parts such as heat engine components and turbines. Materials other than ceramics which are used in these processes may include: wood, metal, water, plaster and epoxy—most of which will be eliminated upon firing.\n\nThese forming techniques are well known for providing tools and other components with dimensional stability, surface quality, high (near theoretical) density and microstructural uniformity. The increasing use and diversity of speciality forms of ceramics adds to the diversity of process technologies to be used.\n\nThus, reinforcing fibres and filaments are mainly made by polymer, sol-gel, or CVD processes, but melt processing also has applicability. The most widely used speciality form is layered structures, with tape casting for electronic substrates and packages being pre-eminent. Photo-lithography is of increasing interest for precise patterning of conductors and other components for such packaging. Tape casting or forming processes are also of increasing interest for other applications, ranging from open structures such as fuel cells to ceramic composites.\n\nThe other major layer structure is coating, where melt spraying is very important, but chemical and physical vapour deposition and chemical (e.g., sol-gel and polymer pyrolysis) methods are all seeing increased use. Besides open structures from formed tape, extruded structures, such as honeycomb catalyst supports, and highly porous structures, including various foams, for example, reticulated foam, are of increasing use.\n\nDensification of consolidated powder bodies continues to be achieved predominantly by (pressureless) sintering. However, the use of pressure sintering by hot pressing is increasing, especially for non-oxides and parts of simple shapes where higher quality (mainly microstructural homogeneity) is needed, and larger size or multiple parts per pressing can be an advantage.\n\nThe principles of sintering-based methods are simple (\"sinter\" has roots in the English \"cinder\"). The firing is done at a temperature below the melting point of the ceramic. Once a roughly-held-together object called a \"green body\" is made, it is baked in a kiln, where atomic and molecular diffusion processes give rise to significant changes in the primary microstructural features. This includes the gradual elimination of porosity, which is typically accompanied by a net shrinkage and overall densification of the component. Thus, the pores in the object may close up, resulting in a denser product of significantly greater strength and fracture toughness.\n\nAnother major change in the body during the firing or sintering process will be the establishment of the polycrystalline nature of the solid. This change will introduce some form of grain size distribution, which will have a significant impact on the ultimate physical properties of the material. The grain sizes will either be associated with the initial particle size, or possibly the sizes of aggregates or particle clusters which arise during the initial stages of processing.\n\nThe ultimate microstructure (and thus the physical properties) of the final product will be limited by and subject to the form of the structural template or precursor which is created in the initial stages of chemical synthesis and physical forming. Hence the importance of chemical powder and polymer processing as it pertains to the synthesis of industrial ceramics, glasses and glass-ceramics.\n\nThere are numerous possible refinements of the sintering process. Some of the most common involve pressing the green body to give the densification a head start and reduce the sintering time needed. Sometimes organic binders such as polyvinyl alcohol are added to hold the green body together; these burn out during the firing (at 200–350 °C). Sometimes organic lubricants are added during pressing to increase densification. It is common to combine these, and add binders and lubricants to a powder, then press. (The formulation of these organic chemical additives is an art in itself. This is particularly important in the manufacture of high performance ceramics such as those used by the billions for electronics, in capacitors, inductors, sensors, etc.)\n\nA slurry can be used in place of a powder, and then cast into a desired shape, dried and then sintered. Indeed, traditional pottery is done with this type of method, using a plastic mixture worked with the hands. If a mixture of different materials is used together in a ceramic, the sintering temperature is sometimes above the melting point of one minor component – a \"liquid phase\" sintering. This results in shorter sintering times compared to solid state sintering.\n\nA material's strength is dependent on its microstructure. The engineering processes to which a material is subjected can alter its microstructure. The variety of strengthening mechanisms that alter the strength of a material include the mechanism of grain boundary strengthening. Thus, although yield strength is maximized with decreasing grain size, ultimately, very small grain sizes make the material brittle. Considered in tandem with the fact that the yield strength is the parameter that predicts plastic deformation in the material, one can make informed decisions on how to increase the strength of a material depending on its microstructural properties and the desired end effect.\n\nThe relation between yield stress and grain size is described mathematically by the Hall-Petch equation which is\nwhere \"k\" is the strengthening coefficient (a constant unique to each material), \"σ\" is a materials constant for the starting stress for dislocation movement (or the resistance of the lattice to dislocation motion), \"d\" is the grain diameter, and \"σ\" is the yield stress.\n\nTheoretically, a material could be made infinitely strong if the grains are made infinitely small. This is, unfortunately, impossible because the lower limit of grain size is a single unit cell of the material. Even then, if the grains of a material are the size of a single unit cell, then the material is in fact amorphous, not crystalline, since there is no long range order, and dislocations can not be defined in an amorphous material. It has been observed experimentally that the microstructure with the highest yield strength is a grain size of about 10 nanometres, because grains smaller than this undergo another yielding mechanism, grain boundary sliding. Producing engineering materials with this ideal grain size is difficult because of the limitations of initial particle sizes inherent to nanomaterials and nanotechnology.\n\nIn the processing of fine ceramics, the irregular particle sizes and shapes in a typical powder often lead to non-uniform packing morphologies that result in packing density variations in the powder compact. Uncontrolled agglomeration of powders due to attractive van der Waals forces can also give rise to in microstructural inhomogeneities.\n\nDifferential stresses that develop as a result of non-uniform drying shrinkage are directly related to the rate at which the solvent can be removed, and thus highly dependent upon the distribution of porosity. Such stresses have been associated with a plastic-to-brittle transition in consolidated bodies,\nand can yield to crack propagation in the unfired body if not relieved.\n\nIn addition, any fluctuations in packing density in the compact as it is prepared for the kiln are often amplified during the sintering process, yielding inhomogeneous densification.\nSome pores and other structural defects associated with density variations have been shown to play a detrimental role in the sintering process by growing and thus limiting end-point densities.\nDifferential stresses arising from inhomogeneous densification have also been shown to result in the propagation of internal cracks, thus becoming the strength-controlling flaws.\n\nIt would therefore appear desirable to process a material in such a way that it is physically uniform with regard to the distribution of components and porosity, rather than using particle size distributions which will maximize the green density. The containment of a uniformly dispersed assembly of strongly interacting particles in suspension requires total control over particle-particle interactions. Monodisperse colloids provide this potential.\n\nMonodisperse powders of colloidal silica, for example, may therefore be stabilized sufficiently to ensure a high degree of order in the colloidal crystal or polycrystalline colloidal solid which results from aggregation. The degree of order appears to be limited by the time and space allowed for longer-range correlations to be established.\n\nSuch defective polycrystalline colloidal structures would appear to be the basic elements of submicrometer colloidal materials science, and, therefore, provide the first step in developing a more rigorous understanding of the mechanisms involved in microstructural evolution in inorganic systems such as polycrystalline ceramics.\n\nSelf-assembly is the most common term in use in the modern scientific community to describe the spontaneous aggregation of particles (atoms, molecules, colloids, micelles, etc.) without the influence of any external forces. Large groups of such particles are known to assemble themselves into thermodynamically stable, structurally well-defined arrays, quite reminiscent of one of the 7 crystal systems found in metallurgy and mineralogy (e.g. face-centred cubic, body-centred cubic, etc.). The fundamental difference in equilibrium structure is in the spatial scale of the unit cell (or lattice parameter) in each particular case.\n\nThus, self-assembly is emerging as a new strategy in chemical synthesis and nanotechnology. Molecular self-assembly has been observed in various biological systems and underlies the formation of a wide variety of complex biological structures. Molecular crystals, liquid crystals, colloids, micelles, emulsions, phase-separated polymers, thin films and self-assembled monolayers all represent examples of the types of highly ordered structures which are obtained using these techniques. The distinguishing feature of these methods is self-organization in the absence of any external forces.\n\nIn addition, the principal mechanical characteristics and structures of biological ceramics, polymer composites, elastomers, and cellular materials are being re-evaluated, with an emphasis on bioinspired materials and structures. Traditional approaches focus on design methods of biological materials using conventional synthetic materials. This includes an emerging class of mechanically superior biomaterials based on microstructural features and designs found in nature. The new horizons have been identified in the synthesis of bioinspired materials through processes that are characteristic of biological systems in nature. This includes the nanoscale self-assembly of the components and the development of hierarchical structures.\n\nSubstantial interest has arisen in recent years in fabricating ceramic composites. While there is considerable interest in composites with one or more non-ceramic constituents, the greatest attention is on composites in which all constituents are ceramic. These typically comprise two ceramic constituents: a continuous matrix, and a dispersed phase of ceramic particles, whiskers, or short (chopped) or continuous ceramic fibres. The challenge, as in wet chemical processing, is to obtain a uniform or homogeneous distribution of the dispersed particle or fibre phase.\nConsider first the processing of particulate composites. The particulate phase of greatest interest is tetragonal zirconia because of the toughening that can be achieved from the phase transformation from the metastable tetragonal to the monoclinic crystalline phase, aka transformation toughening. There is also substantial interest in dispersion of hard, non-oxide phases such as SiC, TiB, TiC, boron, carbon and especially oxide matrices like alumina and mullite. There is also interest too incorporating other ceramic particulates, especially those of highly anisotropic thermal expansion. Examples include AlO, TiO, graphite, and boron nitride.\n\nIn processing particulate composites, the issue is not only homogeneity of the size and spatial distribution of the dispersed and matrix phases, but also control of the matrix grain size. However, there is some built-in self-control due to inhibition of matrix grain growth by the dispersed phase. Particulate composites, though generally offer increased resistance to damage, failure, or both, are still quite sensitive to inhomogeneities of composition as well as other processing defects such as pores. Thus they need good processing to be effective.\n\nParticulate composites have been made on a commercial basis by simply mixing powders of the two constituents. Although this approach is inherently limited in the homogeneity that can be achieved, it is the most readily adaptable for existing ceramic production technology. However, other approaches are of interest.\n\nFrom the technological standpoint, a particularly desirable approach to fabricating particulate composites is to coat the matrix or its precursor onto fine particles of the dispersed phase with good control of the starting dispersed particle size and the resultant matrix coating thickness. One should in principle be able to achieve the ultimate in homogeneity of distribution and thereby optimize composite performance. This can also have other ramifications, such as allowing more useful composite performance to be achieved in a body having porosity, which might be desired for other factors, such as limiting thermal conductivity.\n\nThere are also some opportunities to utilize melt processing for fabrication of ceramic, particulate, whisker and short-fibre, and continuous-fibre composites. Clearly, both particulate and whisker composites are conceivable by solid-state precipitation after solidification of the melt. This can also be obtained in some cases by sintering, as for precipitation-toughened, partially stabilized zirconia. Similarly, it is known that one can directionally solidify ceramic eutectic mixtures and hence obtain uniaxially aligned fibre composites. Such composite processing has typically been limited to very simple shapes and thus suffers from serious economic problems due to high machining costs.\n\nClearly, there are possibilities of using melt casting for many of these approaches. Potentially even more desirable is using melt-derived particles. In this method, quenching is done in a solid solution or in a fine eutectic structure, in which the particles are then processed by more typical ceramic powder processing methods into a useful body. There have also been preliminary attempts to use melt spraying as a means of forming composites by introducing the dispersed particulate, whisker, or fibre phase in conjunction with the melt spraying process.\n\nOther methods besides melt infiltration to manufacture ceramic composites with long fibre reinforcement are chemical vapour infiltration and the infiltration of fibre preforms with organic precursor, which after pyrolysis yield an amorphous ceramic matrix, initially with a low density. With repeated cycles of infiltration and pyrolysis one of those types of ceramic matrix composites is produced. Chemical vapour infiltration is used to manufacture carbon/carbon and silicon carbide reinforced with carbon or silicon carbide fibres.\n\nBesides many process improvements, the first of two major needs for fibre composites is lower fibre costs. The second major need is fibre compositions or coatings, or composite processing, to reduce degradation that results from high-temperature composite exposure under oxidizing conditions.\n\nThe products of technical ceramics include tiles used in the Space Shuttle program, gas burner nozzles, ballistic protection, nuclear fuel uranium oxide pellets, bio-medical implants, jet engine turbine blades, and missile nose cones.\n\nIts products are often made from materials other than clay, chosen for their particular physical properties. These may be classified as follows:\n\n\nCeramics can be used in many technological industries. One application is the ceramic tiles on NASA's Space Shuttle, used to protect it and the future supersonic space planes from the searing heat of re-entry into the Earth's atmosphere. They are also used widely in electronics and optics. In addition to the applications listed here, ceramics are also used as a coating in various engineering cases. An example would be a ceramic bearing coating over a titanium frame used for an aircraft. Recently the field has come to include the studies of single crystals or glass fibres, in addition to traditional polycrystalline materials, and the applications of these have been overlapping and changing rapidly.\n\n\n\n\n\n\nSilicification is quite common in the biological world and occurs in bacteria, single-celled organisms, plants, and animals (invertebrates and vertebrates). Crystalline minerals formed in such environment often show exceptional physical properties (e.g. strength, hardness, fracture toughness) and tend to form hierarchical structures that exhibit microstructural order over a range of length or spatial scales. The minerals are crystallized from an environment that is undersaturated with respect to silicon, and under conditions of neutral pH and low temperature (0–40 °C). Formation of the mineral may occur either within or outside of the cell wall of an organism, and specific biochemical reactions for mineral deposition exist that include lipids, proteins and carbohydrates.\n\nMost natural (or biological) materials are complex composites whose mechanical properties are often outstanding, considering the weak constituents from which they are assembled. These complex structures, which have risen from hundreds of million years of evolution, are inspiring the design of novel materials with exceptional physical properties for high performance in adverse conditions. Their defining characteristics such as hierarchy, multifunctionality, and the capacity for self-healing, are currently being investigated.\n\nThe basic building blocks begin with the 20 amino acids and proceed to polypeptides, polysaccharides, and polypeptides–saccharides. These, in turn, compose the basic proteins, which are the primary constituents of the ‘soft tissues’ common to most biominerals. With well over 1000 proteins possible, current research emphasizes the use of collagen, chitin, keratin, and elastin. The ‘hard’ phases are often strengthened by crystalline minerals, which nucleate and grow in a biomediated environment that determines the size, shape and distribution of individual crystals. The most important mineral phases have been identified as hydroxyapatite, silica, and aragonite. Using the classification of Wegst and Ashby, the principal mechanical characteristics and structures of biological ceramics, polymer composites, elastomers, and cellular materials have been presented. Selected systems in each class are being investigated with emphasis on the relationship between their microstructure over a range of length scales and their mechanical response.\n\nThus, the crystallization of inorganic materials in nature generally occurs at ambient temperature and pressure. Yet the vital organisms through which these minerals form are capable of consistently producing extremely precise and complex structures. Understanding the processes in which living organisms control the growth of crystalline minerals such as silica could lead to significant advances in the field of materials science, and open the door to novel synthesis techniques for nanoscale composite materials, or nanocomposites.\nHigh-resolution SEM observations were performed of the microstructure of the mother-of-pearl (or nacre) portion of the abalone shell. Those shells exhibit the highest mechanical strength and fracture toughness of any non-metallic substance known. The nacre from the shell of the abalone has become one of the more intensively studied biological structures in materials science. Clearly visible in these images are the neatly stacked (or ordered) mineral tiles separated by thin organic sheets along with a macrostructure of larger periodic growth bands which collectively form what scientists are currently referring to as a hierarchical composite structure. (The term hierarchy simply implies that there are a range of structural features which exist over a wide range of length scales).\n\nFuture developments reside in the synthesis of bio-inspired materials through processing methods and strategies that are characteristic of biological systems. These involve nanoscale self-assembly of the components and the development of hierarchical structures.\n\nhttp://standardceramics.org\n\n "}
{"id": "3234363", "url": "https://en.wikipedia.org/wiki?curid=3234363", "title": "Conglomerate merger", "text": "Conglomerate merger\n\nA conglomerate merger is \"any merger that is not horizontal or vertical; in general, it is the combination of firms in different industries or firms operating in different geographic areas\". Conglomerate mergers can serve various purposes, including extending corporate territories and extending a product range. One example of a conglomerate merger was the merger between the Walt Disney Company and the American Broadcasting Company.\n\nBecause a conglomerate merger is one between two strategically unrelated firms, it is unlikely that the economic benefits will be generated for the target or the bidder. As such, conglomerate mergers seldom occur today. However, conglomerate mergers were popular in the U.S. in the 1960s and 1970s. Many conglomerate mergers are divested shortly after they are completed.\n"}
{"id": "165264", "url": "https://en.wikipedia.org/wiki?curid=165264", "title": "Culture theory", "text": "Culture theory\n\nCulture theory is the branch of comparative anthropology and semiotics (not to be confused with cultural sociology or cultural studies) that seeks to define the heuristic concept of culture in operational and/or scientific terms.\n\nIn the 19th century, \"culture\" was used by some to refer to a wide array of human activities, and by some others as a synonym for \"civilization\". In the 20th century, anthropologists began theorizing about culture as an object of scientific analysis. Some used it to distinguish human adaptive strategies from the largely instinctive adaptive strategies of animals, including the adaptive strategies of other primates and non-human hominids, whereas others used it to refer to symbolic representations and expressions of human experience, with no direct adaptive value. Both groups understood culture as being definitive of human nature.\n\nAccording to many theories that have gained wide acceptance among anthropologists, culture exhibits the way that humans interpret their biology and their environment. According to this point of view, culture becomes such an integral part of human existence that it \"is\" the human environment, and most cultural change can be attributed to human adaptation to historical events. Moreover, given that culture is seen as the primary adaptive mechanism of humans and takes place much faster than human biological evolution, most cultural change can be viewed as culture adapting to itself.\n\nAlthough most anthropologists try to define culture in such a way that it separates human beings from other animals, many human traits are similar to those of other animals, particularly the traits of other primates. For example, chimpanzees have big brains, but human brains are bigger. Similarly, bonobos exhibit complex sexual behaviour, but human beings exhibit much more complex sexual behaviours. As such, anthropologists often debate whether human behaviour is different from animal behaviour in degree rather than in kind; they must also find ways to distinguish cultural behaviour from sociological behaviour and psychological behavior.\n\nAcceleration and amplification of these various aspects of culture change have been explored by complexity economist, W. Brian Arthur. In his book, \"The Nature of Technology\", Arthur attempts to articulate a theory of change that considers that existing technologies (or material culture) are combined in unique ways that lead to novel new technologies. Behind that novel combination is a purposeful effort arising in human motivation. This articulation would suggest that we are just beginning to understand what might be required for a more robust theory of culture and culture change, one that brings coherence across many disciplines and reflects an integrating elegance.\n\n\n"}
{"id": "57830094", "url": "https://en.wikipedia.org/wiki?curid=57830094", "title": "Danielle Posthuma", "text": "Danielle Posthuma\n\nDanielle Posthuma (born 1972) is a Dutch neuroscientist who specializes in statistical genetics. She is a University Research Chair professor at VU University Amsterdam, where she is also head of the Department of Complex Trait Genetics. She has been a member of the Young Academy of the Royal Dutch Academy of Sciences since 2005. She is known for studying the genetics of intelligence, which she first became interested in researching in the 1990s.\n\n"}
{"id": "17635701", "url": "https://en.wikipedia.org/wiki?curid=17635701", "title": "DeSmogBlog", "text": "DeSmogBlog\n\nThe DeSmogBlog, founded in January 2006, is a blog that focuses on topics related to global warming. DeSmogBlog opposes what it describes as \"a well-funded and highly organized public relations campaign\" that it says is \"poisoning\" the climate change debate. The site was co-founded by James Hoggan, president of a public relations firm based in Vancouver, British Columbia, Canada.\n\nThe blog was co-founded in January 2006 by James Hoggan, president of the public relations firm Hoggan and Associates. In a February 2007 interview with the \"Vancouver Sun\", Hoggan conveys his anger at industry interests who he believes mislead the public about the scientific understanding of global warming. He referred to this alleged misrepresentation of the facts as, \"public relations at its sleaziest\". Hoggan used his public relations skills to start a blog that would \"clear the PR pollution that clouds the science of climate change\" and expose organizations and individuals which he considered to be unethical. DeSmogBlog says it reports on the credibility of experts who appear to misrepresent the science of global warming in the media by investigating their scientific background, funding sources, and industry interests. The site originally targeted a Canadian audience but is now involved in global climate change coverage.\n\nContributors to the site assist in researching organizations that the site's staff believe are phony, grassroots organizations, or astroturf groups sponsored directly or indirectly by industries seeking to thwart climate change-related legislation. Organizations alleged by the blog to be astroturfs include Friends of Science, Natural Resources Stewardship Project, Global Climate Coalition, and International Climate Science Coalition. Individuals that the site has identified as pushing an anti-climate change point of view are listed in the site's \"Denial Database\", with accompanying information about their industry affiliations and professional biographies. In a \"Financial Post\" column, Canadian environmentalist Lawrence Solomon stated that the organization was, in Solomon's words, \"specifically created for the purpose of discrediting skeptics.\" \n\nIn a 2007 report in \"The Globe and Mail\", Hoggan stated that the most frequent visitors to the site came from Calgary, Ottawa, and Washington D.C.\n\nIn one instance, the site responded to a 2006 open letter opposing the Canadian Government's climate-change plans, claimed to be signed by \"accredited experts in climate and related scientific disciplines\", by analyzing the list of the signatories. The site concluded that those checked had few peer-reviewed publications on the topic and/or had fossil-fuel industry connections.\n\nDeSmogBlog has criticized \"Financial Post\" editor and columnist Terence Corcoran, claiming he impedes progress on climate change and environmental protection legislation in Canada. In turn, Corcoran has criticized Hoggan and his website, accusing both of serving the interests of large corporations hoping to make money on emissions trading.\n\nThe blog has been referenced in \"The Guardian\" by George Monbiot, who most recently cited a study by the website showing that in 2008 \"the number of internet pages proposing that man-made global warming is a hoax or a lie more than doubled\". In another column, Monbiot noted that DeSmogBlog posted a video critical of Anthony Watts's blog Watts Up With That that Watts had deleted from YouTube for copyright reasons. Monbiot has also mentioned DeSmogBlog's efforts to expose efforts by oil, coal, and electricity companies to manipulate media views on climate change.\n\nIn February 2012, DeSmogBlog posted a number of internal documents purportedly from The Heartland Institute, a libertarian think tank. According to a statement posted on the Heartland Institute website, \"Some of these documents were stolen from Heartland, at least one is a fake, and some may have been altered ... the authenticity of those documents has not been confirmed.\"\n\nDays after the document posting, blogger and journalist Megan McArdle wrote on \"The Atlantic\" website of a comment to a blog post that suggested that one of the documents, a memo titled \"2012 Heartland Climate Strategy,\" was likely a fake based on the document being a scan which included metadata with a US west coast time zone. DeSmogBog responded that they had \"no evidence supporting Heartland's claim that the Strategic document is fake\" and then included a number of references to McArdle's first piece on the topic. McArdle then said of the DeSmogBlog response that \"The first two links are to my post, and they are an egregious misrepresentation of what I said,\" and goes on to note that \"the stubborn willingness to ignore obvious problems \"becomes the story\".\"\n\nOn February 20, 2012, Peter Gleick issued a statement in the \"Huffington Post\" explaining that he had received an anonymous document in the mail that seemed to contain details on the climate program strategy of The Heartland Institute. He admitted to soliciting and receiving additional material from the Institute \"under someone else's name,\" calling his actions \"a serious lapse of my own and professional judgment and ethics.\"\n\nThe site's co-founder, James Hoggan, is President of the Vancouver-based public relations firm James Hoggan & Associates, chair of the David Suzuki Foundation, a trustee of the Dalai Lama Center for Peace and Education, and an executive member of the Urban Development Institute. He is the author (with Richard Littlemore) of the 2009 book \"Climate Cover-Up: The Crusade to Deny Global Warming\" (), which criticizes global warming denial and conspiracy theories. The sources do not identify the site's other co-founder.\n\nThe website names John Lefebvre as a benefactor. Frequent contributors to the blog include Ross Gelbspan and Richard Littlemore. Littlemore is a science writer who formerly worked for the \"Vancouver Sun\". The site's project manager was Kevin Grandia, who left to become the Director of Online Strategy at Greenpeace . As of 2018 the site is now managed by Brendan DeMelle.\n\nThe site was recognized in December 2007 by three British Columbia chapters of the Canadian Public Relations Society, the Vancouver, Victoria (CPRS-vi) and \"Northern Lights\" in Prince George, with an award for demonstrating \"The highest ethical and professional standards while performing outstanding work\". In a CPRS press release which accompanied the award, Hoggan stated that the site had been viewed by 520,000 people over its history, had been cited as a source by 24 media outlets, and mentioned in more than 4,500 other blogs. According to the press release, the blog was selected for the award by a panel of journalists and public relations professionals in Victoria, Vancouver, and Prince George.\n\nDeSmogBlog was also listed by Time magazine as one of the \"best blogs of 2011\" in June, 2011.\n\n"}
{"id": "14333272", "url": "https://en.wikipedia.org/wiki?curid=14333272", "title": "Dominance-based rough set approach", "text": "Dominance-based rough set approach\n\nThe dominance-based rough set approach (DRSA) is an extension of rough set theory for multi-criteria decision analysis (MCDA), introduced by Greco, Matarazzo and Słowiński. The main change compared to the classical rough sets is the substitution for the indiscernibility relation by a dominance relation, which permits one to deal with inconsistencies typical to consideration of criteria and preference-ordered decision classes.\n\nMulticriteria classification (sorting) is one of the problems considered within MCDA and can be stated as follows: given a set of objects evaluated by a set of criteria (attributes with preference-order domains), assign these objects to some pre-defined and preference-ordered decision classes, such that each object is assigned to exactly one class. Due to the preference ordering, improvement of evaluations of an object on the criteria should not worsen its class assignment. The sorting problem is very similar to the problem of classification, however, in the latter, the objects are evaluated by regular attributes and the decision classes are not necessarily preference ordered. The problem of multicriteria classification is also referred to as ordinal classification problem with monotonicity constraints and often appears in real-life application when ordinal and monotone properties follow from the domain knowledge about the problem.\n\nAs an illustrative example, consider the problem of evaluation in a high school. The director of the school wants to assign students (\"objects\") to three classes: \"bad\", \"medium\" and \"good\" (notice that class \"good\" is preferred to \"medium\" and \"medium\" is preferred to \"bad\"). Each student is described by three criteria: level in Physics, Mathematics and Literature, each taking one of three possible values \"bad\", \"medium\" and \"good\". Criteria are preference-ordered and improving the level from one of the subjects should not result in worse global evaluation (class).\n\nAs a more serious example, consider classification of bank clients, from the viewpoint of bankruptcy risk, into classes \"safe\" and \"risky\". This may involve such characteristics as \"return on equity (ROE)\", \"return on investment (ROI)\" and \"return on sales (ROS)\". The domains of these attributes are not simply ordered but involve a preference order since, from the viewpoint of bank managers, greater values of ROE, ROI or ROS are better for clients being analysed for bankruptcy risk . Thus, these attributes are criteria. Neglecting this information in knowledge discovery may lead to wrong conclusions.\n\nIn DRSA, data are often presented using a particular form of decision table. Formally, a DRSA decision table is a 4-tuple formula_1, where formula_2 is a finite set of objects, formula_3 is a finite set of criteria, formula_4 where formula_5 is the domain of the criterion formula_6 and formula_7 is an \"information function\" such that formula_8 for every formula_9. The set formula_3 is divided into \"condition criteria\" (set formula_11) and the \"decision criterion\" (\"class\") formula_12. Notice, that formula_13 is an evaluation of object formula_14 on criterion formula_15, while formula_16 is the class assignment (decision value) of the object. An example of decision table is shown in Table 1 below.\n\nIt is assumed that the domain of a criterion formula_17 is completely preordered by an outranking relation formula_18; formula_19 means that formula_14 is at least as good as (outranks) formula_21 with respect to the criterion formula_6. Without loss of generality, we assume that the domain of formula_6 is a subset of reals, formula_24, and that the outranking relation is a simple order between real numbers formula_25 such that the following relation holds: formula_26. This relation is straightforward for gain-type (\"the more, the better\") criterion, e.g. \"company profit\". For cost-type (\"the less, the better\") criterion, e.g. \"product price\", this relation can be satisfied by negating the values from formula_5.\n\nLet formula_28. The domain of decision criterion, formula_29 consist of formula_30 elements (without loss of generality we assume formula_31) and induces a partition of formula_2 into formula_30 classes formula_34, where formula_35. Each object formula_36 is assigned to one and only one class formula_37. The classes are preference-ordered according to an increasing order of class indices, i.e. for all formula_38 such that formula_39, the objects from formula_40 are strictly preferred to the objects from formula_41. For this reason, we can consider the upward and downward unions of classes, defined respectively, as:\n\nWe say that formula_14 dominates formula_21 with respect to formula_45, denoted by formula_46, if formula_14 is better than formula_21 on every criterion from formula_49, formula_50. For each formula_45, the dominance relation formula_52 is reflexive and transitive, i.e. it is a partial pre-order. Given formula_45 and formula_36, let\n\nrepresent P\"-dominating set and P\"-dominated set with respect to formula_36, respectively.\n\nThe key idea of the rough set philosophy is approximation of one knowledge by another knowledge. In DRSA, the knowledge being approximated is a collection of upward and downward unions of decision classes and the \"granules of knowledge\" used for approximation are \"P\"-dominating and \"P\"-dominated sets.\n\nThe P\"-lower and the P\"-upper approximation of formula_58 with respect to formula_45, denoted as formula_60 and formula_61, respectively, are defined as:\n\nAnalogously, the \"P\"-lower and the \"P\"-upper approximation of formula_64 with respect to formula_45, denoted as formula_66 and formula_67, respectively, are defined as:\n\nLower approximations group the objects which \"certainly\" belong to class union formula_70 (respectively formula_71). This certainty comes from the fact, that object formula_36 belongs to the lower approximation formula_73 (respectively formula_74), if no other object in formula_2 contradicts this claim, i.e. every object formula_76 which \"P\"-dominates formula_14, also belong to the class union formula_70 (respectively formula_71). Upper approximations group the objects which \"could belong\" to formula_70 (respectively formula_71), since object formula_36 belongs to the upper approximation formula_83 (respectively formula_84), if there exist another object formula_76 \"P\"-dominated by formula_14 from class union formula_70 (respectively formula_71).\n\nThe \"P\"-lower and \"P\"-upper approximations defined as above satisfy the following properties for all formula_89 and for any formula_45:\n\nThe \"P\"-boundaries (\"P-doubtful regions\") of formula_93 and formula_94 are defined as:\n\nThe ratio\n\ndefines the quality of approximation of the partition formula_98 into classes by means of the set of criteria formula_49. This ratio express the relation between all the \"P\"-correctly classified objects and all the objects in the table.\n\nEvery minimal subset formula_45 such that formula_101 is called a reduct of formula_102 and is denoted by formula_103. A decision table may have more than one reduct. The intersection of all reducts is known as the \"core\".\n\nOn the basis of the approximations obtained by means of the dominance relations, it is possible to induce a generalized description of the preferential information contained in the decision table, in terms of decision rules. The decision rules are expressions of the form \"if\" [condition] \"then\" [consequent], that represent a form of dependency between condition criteria and decision criteria. Procedures for generating decision rules from a decision table use an inducive learning principle. We can distinguish three types of rules: certain, possible and approximate. Certain rules are generated from lower approximations of unions of classes; possible rules are generated from upper approximations of unions of classes and approximate rules are generated from boundary regions.\n\nCertain rules has the following form:\nif formula_104 and formula_105 and formula_106 then formula_107\nif formula_108 and formula_109 and formula_110 then formula_111\n\nPossible rules has a similar syntax, however the \"consequent\" part of the rule has the form: formula_14 \"could belong to\" formula_93 or the form: formula_14 \"could belong to\" formula_94.\n\nFinally, approximate rules has the syntax:\nif formula_104 and formula_105 and formula_118 and formula_119 and formula_120 and formula_110\nthen formula_122\n\nThe certain, possible and approximate rules represent certain, possible and ambiguous knowledge extracted from the decision table.\n\nEach decision rule should be minimal. Since a decision rule is an implication, by a minimal decision rule we understand such an implication that there is no other implication with an antecedent of at least the same weakness (in other words, rule using a subset of elementary conditions or/and weaker elementary conditions) and a consequent of at least the same strength (in other words, rule assigning objects to the same union or sub-union of classes).\n\nA set of decision rules is \"complete\" if it is able to cover all objects from the decision table in such a way that consistent objects are re-classified to their original classes and inconsistent objects are classified to clusters of classes referring to this inconsistency. We call \"minimal\" each set of decision rules that is complete and non-redundant, i.e. exclusion of any rule from this set makes it non-complete.\nOne of three induction strategies can be adopted to obtain a set of decision rules:\n\n\nThe most popular rule induction algorithm for dominance-based rough set approach is DOMLEM, which generates minimal set of rules.\n\nConsider the following problem of high school students’ evaluations:\n\nEach object (student) is described by three criteria formula_123, related to the levels in Mathematics, Physics and Literature, respectively. According to the decision attribute, the students are divided into three preference-ordered classes: formula_124, formula_125 and formula_126. Thus, the following unions of classes were approximated:\n\n\nNotice that evaluations of objects formula_131 and formula_132 are inconsistent, because formula_131 has better evaluations on all three criteria than formula_132 but worse global score.\n\nTherefore, lower approximations of class unions consist of the following objects:\n\nThus, only classes formula_127 and formula_129 cannot be approximated precisely. Their upper approximations are as follows:\n\nwhile their boundary regions are:\n\nOf course, since formula_128 and formula_130 are approximated precisely, we have formula_146, formula_147 and formula_148\n\nThe following minimal set of 10 rules can be induced from the decision table:\n\n\nThe last rule is approximate, while the rest are certain.\n\nThe other two problems considered within multi-criteria decision analysis, multicriteria choice and ranking problems, can also be solved using dominance-based rough set approach. This is done by converting the decision table into pairwise comparison table (PCT).\n\nThe definitions of rough approximations are based on a strict application of the dominance principle. However, when defining non-ambiguous objects, it is reasonable to accept a limited proportion of negative examples, particularly for large decision tables. Such extended version of DRSA is called Variable-Consistency DRSA model (VC-DRSA)\n\nIn real-life data, particularly for large datasets, the notions of rough approximations were found to be excessively restrictive. Therefore, an extension of DRSA, based on stochastic model (Stochastic DRSA), which allows inconsistencies to some degree, has been introduced. Having stated the probabilistic model for ordinal classification problems with monotonicity constraints, the concepts of lower approximations are extended to the\nstochastic case. The method is based on estimating the conditional probabilities using the nonparametric maximum likelihood method which leads\nto the problem of isotonic regression.\n\nStochastic dominance-based rough sets can also be regarded as a sort of variable-consistency model.\n\n4eMka2 is a decision support system for multiple criteria classification problems based on dominance-based rough sets (DRSA). JAMM is a much more advanced successor of 4eMka2. Both systems are freely available for non-profit purposes on the Laboratory of Intelligent Decision Support Systems (IDSS) website.\n\n\n\n"}
{"id": "255217", "url": "https://en.wikipedia.org/wiki?curid=255217", "title": "Dynamo theory", "text": "Dynamo theory\n\nIn physics, the dynamo theory proposes a mechanism by which a celestial body such as Earth or a star generates a magnetic field. The dynamo theory describes the process through which a rotating, convecting, and electrically conducting fluid can maintain a magnetic field over astronomical time scales. A dynamo is thought to be the source of the Earth's magnetic field and the magnetic fields of other planets.\n\nWhen William Gilbert published \"de Magnete\" in 1600, he concluded that the Earth is magnetic and proposed the first hypothesis for the origin of this magnetism: permanent magnetism such as that found in lodestone. In 1919, Joseph Larmor proposed that a dynamo might be generating the field. However, even after he advanced his hypothesis, some prominent scientists advanced alternative explanations. Einstein believed that there might be an asymmetry between the charges of the electron and proton so that the Earth's magnetic field would be produced by the entire Earth. The Nobel Prize winner Patrick Blackett did a series of experiments looking for a fundamental relation between angular momentum and magnetic moment, but found none.\n\nWalter M. Elsasser, considered a \"father\" of the presently accepted dynamo theory as an explanation of the Earth's magnetism, proposed that this magnetic field resulted from electric currents induced in the fluid outer core of the Earth. He revealed the history of the Earth's magnetic field through pioneering the study of the magnetic orientation of minerals in rocks.\n\nIn order to maintain the magnetic field against ohmic decay (which would occur for the dipole field in 20,000 years), the outer core must be convecting. The convection is likely some combination of thermal and compositional convection. The mantle controls the rate at which heat is extracted from the core. Heat sources include gravitational energy released by the compression of the core, gravitational energy released by the rejection of light elements (probably sulfur, oxygen, or silicon) at the inner core boundary as it grows, latent heat of crystallization at the inner core boundary, and radioactivity of potassium, uranium and thorium.\n\nAt the dawn of the 21st century, numerical modeling of the Earth's magnetic field has not been successfully demonstrated, but appears to be in reach. Initial models are focused on field generation by convection in the planet's fluid outer core. It was possible to show the generation of a strong, Earth-like field when the model assumed a uniform core-surface temperature and exceptionally high viscosities for the core fluid. Computations which incorporated more realistic parameter values yielded magnetic fields that were less Earth-like, but also point the way to model refinements which may ultimately lead to an accurate analytic model. Slight variations in the core-surface temperature, in the range of a few millikelvins, result in significant increases in convective flow and produce more realistic magnetic fields.\n\nDynamo theory describes the process through which a rotating, convecting, and electrically conducting fluid acts to maintain a magnetic field. This theory is used to explain the presence of anomalously long-lived magnetic fields in astrophysical bodies. The conductive fluid in the geodynamo is liquid iron in the outer core, and in the solar dynamo is ionized gas at the tachocline. Dynamo theory of astrophysical bodies uses magnetohydrodynamic equations to investigate how the fluid can continuously regenerate the magnetic field.\n\nIt was once believed that the dipole, which comprises much of the Earth's magnetic field and is misaligned along the rotation axis by 11.3 degrees, was caused by permanent magnetization of the materials in the earth. This means that dynamo theory was originally used to explain the Sun's magnetic field in its relationship with that of the Earth. However, this hypothesis, which was initially proposed by Joseph Larmor in 1919, has been modified due to extensive studies of magnetic secular variation, paleomagnetism (including polarity reversals), seismology, and the solar system's abundance of elements. Also, the application of the theories of Carl Friedrich Gauss to magnetic observations showed that Earth's magnetic field had an internal, rather than external, origin.\n\nThere are three requisites for a dynamo to operate:\n\n\nIn the case of the Earth, the magnetic field is induced and constantly maintained by the convection of liquid iron in the outer core. A requirement for the induction of field is a rotating fluid. Rotation in the outer core is supplied by the Coriolis effect caused by the rotation of the Earth. The Coriolis force tends to organize fluid motions and electric currents into columns (also see Taylor columns) aligned with the rotation axis. Induction or creation of magnetic field is described by the induction equation:\n\nwhere u is velocity, B is magnetic field, \"t\" is time, and formula_2 is the magnetic diffusivity with formula_3 electrical conductivity and formula_4 permeability. The ratio of the second term on the right hand side to the first term gives the Magnetic Reynolds number, a dimensionless ratio of advection of magnetic field to diffusion.\n\nTidal forces between celestial orbiting bodies cause friction that heats up their interiors. This is known as tidal heating, and it helps keep the interior in a liquid state. A liquid interior that can conduct electricity is required to produce a dynamo. Saturn's Enceladus and Jupiter's Io have enough tidal heating to liquify their inner cores, but they may not create a dynamo because they cannot conduct electricity. Mercury, despite its small size, has a magnetic field, because it has a conductive liquid core created by its iron composition and friction resulting from its highly elliptical orbit. It is theorized that the Moon once had a magnetic field, based on evidence from magnetized lunar rocks, due to its short-lived closer distance to Earth creating tidal heating. An orbit and rotation of a planet helps provide a liquid core, and supplements kinetic energy that supports a dynamo action.\n\nIn kinematic dynamo theory the velocity field is prescribed, instead of being a dynamic variable. This method cannot provide the time variable behavior of a fully nonlinear chaotic dynamo but is useful in studying how magnetic field strength varies with the flow structure and speed.\n\nUsing Maxwell's equations simultaneously with the curl of Ohm's law, one can derive what is basically the linear eigenvalue equation for magnetic fields (B) which can be done when assuming that the magnetic field is independent from the velocity field. One arrives at a critical \"magnetic Reynolds number\" above which the flow strength is sufficient to amplify the imposed magnetic field, and below which it decays.\n\nThe most functional feature of kinematic dynamo theory is that it can be used to test whether a velocity field is or is not capable of dynamo action. By applying a certain velocity field to a small magnetic field, it can be determined through observation whether the magnetic field tends to grow or not in reaction to the applied flow. If the magnetic field does grow, then the system is either capable of dynamo action or is a dynamo, but if the magnetic field does not grow, then it is simply referred to as non-dynamo.\n\nThe membrane paradigm is a way of looking at black holes that allows for the material near their surfaces to be expressed in the language of dynamo theory.\n\nKinematic dynamo can be also viewed as the phenomenon of the spontaneous breakdown of the topological supersymmetry of the associated stochastic differential equation related to the flow of the background matter. Within supersymmetric theory of stochastics, this supersymmetry is an intrinsic property of all stochastic differential equations, its meaning is the preservation of the continuity of the phase space of the model by continuous time flows, and its spontaneous breakdown is the stochastic generalization of the concept of deterministic chaos. In other words, kinematic dynamo is a manifestation of the chaoticity of the underlying flow of the background matter.\n\nThe kinematic approximation becomes invalid when the magnetic field becomes strong enough to affect the fluid motions. In that case the velocity field becomes affected by the Lorentz force, and so the induction equation is no longer linear in the magnetic field. In most cases this leads to a quenching of the amplitude of the dynamo. Such dynamos are sometimes also referred to as \"hydromagnetic dynamos\".\nVirtually all dynamos in astrophysics and geophysics are hydromagnetic dynamos.\n\nThe main idea of the theory is that any small magnetic field existing in the outer core, creates currents in the moving fluid there due to Lorenz force. These currents create further magnetic field due to Ampere's law. With the fluid motion, the currents are carried in a way that the magnetic field gets stronger (as long as formula_5 is negative). Thus a \"seed\" magnetic field can get stronger and stronger until it reaches some value that is related to existing non-magnetic forces.\n\nNumerical models are used to simulate fully nonlinear dynamos. The following equations are used:\n\n\n\n\n\nThese equations are then non-dimensionalized, introducing the non-dimensional parameters,\n\nwhere \"Ra\" is the Rayleigh number, \"E\" the Ekman number, \"Pr\" and \"Pm\" the Prandtl and magnetic Prandtl number. Magnetic field scaling is often in Elsasser number units formula_23.\n\nThe scalar product of the above form of Navier-Stokes equation with formula_24 gives the rate of increase of kinetic energy density, formula_25, on the left-hand side. The last term on the right-hand side is then formula_5, the local contribution to the kinetic energy due to Lorentz force.\n\nThe scalar product of the induction equation with formula_27 gives the rate of increase of the magnetic energy density, formula_28, on the left-hand side. The last term on the right-hand side is then formula_29. Since the equation is volume-integrated, this term is equivalent up to a boundary term (and with the double use of the scalar triple product identity) to formula_30 (where one of Maxwell's equations was used). This is the local contribution to the magnetic energy due to fluid motion.\n\nThus the term formula_31 is the rate of transformation of kinetic energy to magnetic energy. This has to be non-negative at least in part of the volume, for the dynamo to produce magnetic field.\n\nThe above formula for the rate of conversion of kinetic energy to magnetic energy, is equivalent to a rate of work done by a force of formula_32 on the outer core matter, whose velocity is formula_33. This work is the result of non-magnetic forces acting on the fluid.\n\nOf those, the gravitational force and the centrifugal force are conservative and therefore have no overall contribution to fluid moving in closed loops. Ekman number (defined above), which is the ratio between the two remaining forces, namely the viscosity and Coriolis force, is very low inside Earth's outer core, because its viscosity is low (1.2-1.5 x10 pascal-second ) due to its liquidity.\n\nThus the main time-averaged contribution to the work is from Coriolis force, whose size is formula_34, though this quantity and formula_32 are related only indirectly and are not in general equal locally (thus they affect each other but not in the same place and time).\n\nThe current density \"J\" is itself the result of the magnetic field according to Ohm's law. Again, due to matter motion and current flow, this is not necessarily the field at the same place and time. However these relations can still be used to deduce orders of magnitude of the quantities in question.\n\nIn terms of order of magnitude, formula_36 and formula_37, giving formula_38, or:\n\nThe exact ratio between both sides is the square root of Elsasser number.\n\nNote that the magnetic field direction cannot be inferred from this approximation (at least not its sign) as it appears squared, and is, indeed, sometimes reversed, though in general it lies on a similar axis to that of formula_40.\n\nFor earth outer core, \"ρ\" is approximately 10 kg/m, \"Ω\"=2π/day = 7.3x10 seconds and \"σ\" is approximately 10Ωm.\nThis gives 2.7x10 Tesla.\n\nThe magnetic field of a magnetic dipole has an inverse cubic dependence in distance, so its order of magnitude at the earth surface can be approximated by multiplying the above result with (\"R\"/\"R\") = (2890/6370) = 0.093, giving 2.5x10 Tesla, not far from to the measured value of 3x10 Tesla at the equator.\n\nThe equations for the geodynamo are enormously difficult to solve, and the realism of the solutions is limited mainly by computer power. For decades, theorists were confined to \"kinematic dynamo\" models described above, in which the fluid motion is chosen in advance and the effect on the magnetic field calculated. Kinematic dynamo theory was mainly a matter of trying different flow geometries and seeing whether they could sustain a dynamo.\n\nThe first \"self-consistent\" dynamo models, ones that determine both the fluid motions and the magnetic field, were developed by two groups in 1995, one in Japan and one in the United States. The latter received significant attention because it successfully reproduced some of the characteristics of the Earth's field, including geomagnetic reversals.\n\n\n"}
{"id": "2606750", "url": "https://en.wikipedia.org/wiki?curid=2606750", "title": "Economic Cooperation Administration", "text": "Economic Cooperation Administration\n\nThe Economic Cooperation Administration (ECA) was a U.S. government agency set up in 1948 to administer the Marshall Plan. It reported to both the State Department and the Department of Commerce. The agency's first head was Paul G. Hoffman, a former leader of car manufacturer Studebaker; he was succeeded by William Chapman Foster in 1950. The rest of the organization was also headed by major business figures such as Arthur A. Kimball (who was a key contributor to the ECA's founding) as well as David K.E. Bruce (who worked at the Office of Strategic Services in Europe during World War II).\n\nThe ECA had an office in the capital of each of the 16 countries participating in the Marshall Plan. In theory the ECA served as joint administrator of the Marshall Plan development projects in each European country. In practice, local officials knew far more about what was needed than ECA representatives, who developed a management strategy of listening to local officials and allowed them to set priorities for reconstruction assistance.\n\nIt was succeeded by the Mutual Security Agency in 1951, one of the predecessor to the United States Agency for International Development.\n\n"}
{"id": "7818361", "url": "https://en.wikipedia.org/wiki?curid=7818361", "title": "Eldridge M. Moores", "text": "Eldridge M. Moores\n\nEldridge Moores (October 13, 1938 – October 28, 2018) was an American geologist. He specialized in the understanding of ophiolites (fragments of oceanic crust and mantle that have been emplaced onto the continental crust) and the geology of the continental crust of the Western United States and Tethyan belt, the geology of Greece, Cyprus, and Pakistan, and the tectonic development of the Sierra Nevada and the Alpine - Himalayan systems.\n\nMoores was Distinguished Professor Emeritus of Geology at the University of California, Davis.\n\nIn 1996, Moores was President of the Geological Society of America (GSA) and editor of the society's journal \"Geology\" from 1981 to 1989. He is the recipient of the GSA's Distinguished Service Award and the Geological Association of Canada Medal.\n\nTogether with geologist Robert J. Twiss, Moores co-authored two textbooks: \"Tectonics\" and \"Structural Geology\"\n\nMoores is the main subject of the John McPhee book on California geology, \"Assembling California\" (1993), as well as McPhee's \"Annals of the Former World\" (1998).\n\nIn 2013, Eldridge Moores was awarded the title of UC Davis distinguished professor emeritus. This title is awarded annually by the UC Davis Emeriti Association on the basis of outstanding contributions following retirement in the traditional areas of teaching, research and service.\n\n\n"}
{"id": "7949248", "url": "https://en.wikipedia.org/wiki?curid=7949248", "title": "Expanded bed adsorption", "text": "Expanded bed adsorption\n\nExpanded bed adsorption (EBA) is a preparative chromatographic technique which makes processing of viscous and particulate liquids possible.\n\nThe protein binding principles in EBA are the same as in classical column chromatography and the common ion-exchange, hydrophobic interaction and affinity chromatography ligands can be used. After the adsorption step is complete, the fluidized bed is washed to flush out any remaining particulates. Elution of the adsorbed proteins was commonly performed with the eluent flow in the reverse direction; that is, as a conventional packed bed, in order to recover the adsorbed solutes in a smaller volume of eluent. However, a new generation of EBA columns has been developed, which maintain the bed in the expanded state during this phase, producing high-purity, high yields of e.g. MAbs [monoclonal antibodies] in even smaller volumes of eluent. Process duration at manufacturing scale has also been cut considerably (under 7 hours in some cases).\n\nEBA may be considered to combine both the \"Removal of Insolubles\" and the \"Isolation\" steps of the 4-step downstream processing heuristic. The major limitations associated with EBA technology is biomass interactions and aggregations onto adsorbent during processing.\n\nWhere classical column chromatography uses a solid phase made by a packed bed, EBA uses particles in a fluidized state, ideally expanded by a factor of 2. Expanded bed adsorption is, however, different from fluidised bed chromatography in essentially two ways: one, the EBA resin contains particles of varying size and density which results in a gradient of particle size when expanded; and two, when the bed is in its expanded state, local loops are formed. Particles such as whole cells or cell debris, which would clog a packed bed column, readily pass through a fluidized bed. EBA can therefore be used on crude culture broths or slurries of broken cells, thereby bypassing initial clearing steps such as centrifugation and filtration, which is mandatory when packed beds are used. In older EBA column designs, the feed flow rate is kept low enough that the solid packing remains stratified and does not fluidize completely. Hence EBA can be modelled as frontal adsorption in a packed bed, rather than as a well-mixed, continuous-flow adsorber.\n\n"}
{"id": "11270152", "url": "https://en.wikipedia.org/wiki?curid=11270152", "title": "General Architecture for Text Engineering", "text": "General Architecture for Text Engineering\n\nGeneral Architecture for Text Engineering or GATE is a Java suite of tools originally developed at the University of Sheffield beginning in 1995 and now used worldwide by a wide community of scientists, companies, teachers and students for many natural language processing tasks, including information extraction in many languages.\n\nGATE has been compared to NLTK, R and RapidMiner. As well as being widely used in its own right, it forms the basis of the KIM semantic platform.\n\nGATE community and research has been involved in several European research projects including TAO, SEKT, NeOn, Media-Campaign, Musing, Service-Finder, LIRICS and KnowledgeWeb, as well as many other projects.\n\nAs of May 28, 2011, 881 people are on the gate-users mailing list at SourceForge.net, and 111,932 downloads from SourceForge are recorded since the project moved to SourceForge in 2005. The paper \"GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications\" has received over 800 citations in the seven years since publication (according to Google Scholar). Books covering the use of GATE, in addition to the GATE User Guide, include \"Building Search Applications: Lucene, LingPipe, and Gate\", by Manu Konchady, and \"Introduction to Linguistic Annotation and Text Analytics\", by Graham Wilcock.\n\nGATE includes an information extraction system called ANNIE (A Nearly-New Information Extraction System) which is a set of modules comprising a tokenizer, a gazetteer, a sentence splitter, a part of speech tagger, a named entities transducer and a coreference tagger. ANNIE can be used as-is to provide basic information extraction functionality, or provide a starting point for more specific tasks.\n\nLanguages currently handled in GATE include English, Chinese, Arabic, Bulgarian, French, German, Hindi, Italian, Cebuano, Romanian, Russian, Danish.\n\nPlugins are included for machine learning with Weka, RASP, MAXENT, SVM Light, as well as a LIBSVM integration and an in-house perceptron implementation, for managing ontologies like WordNet, for querying search engines like Google or Yahoo, for part of speech tagging with Brill or TreeTagger, and many more. Many external plugins are also available, for handling e.g. tweets.\n\nGATE accepts input in various formats, such as TXT, HTML, XML, Doc, PDF documents, and Java Serial, PostgreSQL, Lucene, Oracle Databases with help of RDBMS storage over JDBC.\n\nJAPE transducers are used within GATE to manipulate annotations on text. Documentation is provided in the GATE User Guide. A tutorial has also been written by Press Association Images.\n\nThe screenshot shows the document viewer used to display a document and its annotations. In pink are hyperlink annotations from an HTML file. The right list is the annotation sets list, and the bottom table is the annotation list. In the center is the annotation editor window.\n\nGATE generates vast quantities of information including; natural language text, semantic annotations, and ontological information. Sometimes the data itself is the end product of an application but often the information would be more useful if it could be efficiently searched. GATE Mimir provides support for indexing and searching the linguistic and semantic information generated by such applications and allows for querying the information using arbitrary combinations of text, structural information, and SPARQL.\n\n"}
{"id": "23831747", "url": "https://en.wikipedia.org/wiki?curid=23831747", "title": "Gustav Flor", "text": "Gustav Flor\n\nGustav August Adam Flor (13 August 1829, Vecsalaca (), present-day Latvia – 13 May 1883 Tartu (), present-day Estonia) was a Baltic German zoologist from Livonia.\n\nFlor studied medicine and natural science at the University of Dorpat, later becoming a professor of zoology. In 1860, he became director of the zoological cabinet, which he remained until his death in 1883 (the zoological cabinet was the predecessor of the Zoology Museum at the University of Tartu, where his 11,815-specimen entomological collection still resides).\n\nHe is best known for his studies of Hemiptera, especially the two-volume work \"Die Rhynchoten Livlands in systematischer Folge beschrieben\" (\"The Hemiptera of Livonia, Described In Systematic Order\").\n\n\n"}
{"id": "11459267", "url": "https://en.wikipedia.org/wiki?curid=11459267", "title": "Holographic grating", "text": "Holographic grating\n\nA holographic grating is a type of diffraction grating formed by an interference-fringe field of two laser beams whose standing-wave pattern is exposed to a polished substrate coated with photoresist. Processing of the exposed medium results in a pattern of straight lines with a sinusoidal cross section.\n\nHolographic gratings may exhibit less scattered light than ruled gratings. Due to their sinusoidal groove profile, holographic gratings cannot be easily blazed and their efficiency is usually considerably less than a comparable ruled grating. However, an exception exists when the ratio of the period to the wavelength is near one; in this case, a holographic grating has virtually the same efficiency as the ruled version.\n\nHolographic master gratings are replicated by a process identical to that used for ruled gratings.\n\n"}
{"id": "56612502", "url": "https://en.wikipedia.org/wiki?curid=56612502", "title": "Hongkongers in the Netherlands", "text": "Hongkongers in the Netherlands\n\nHongkonger in the Netherlands are people in the Netherlands originated from Hong Kong or having at least once such parent. \n\nAccording to OECD figures there are 9,935 people in the Netherlands migrated from Hong Kong, a figure which would exclude people who declared other sources of origin and people born in the Netherlands to Hong Kong parents. \n\n, figures from the Netherlands' Centraal Bureau voor de Statistiek showed that 9,757 Hong Kong-born persons (4,808 men, 4,949 women), and 8,440 persons with at least one parent born in Hong Kong (4,300 men, 4,140 women). \n\nThe number of persons of Hong Kong background has shown only mild growth, entirely due to natural increase rather than additional migration; in fact the stock of Hong Kong migrants fell by 5.6% during the same period. \n\nAccording to F. N. Pieke, Hong Kong was a significant source of Chinese migrants to the Netherlands in the late 1970s and early 1980s, with about 600 to 800 per year, falling off to around 300 to 400 per year by the late 1980s.\n\nWhile most Hongkongers are Cantonese by descent, there are Hongkongers who are of Teochew, Hakka, Shanghainese, Hokkien or South Asian descent, and people from overseas Chinese communities in Southeast Asia. Owing to Hong Kong's previous status as a British crown colony, the British nationality of the inhabitants and the existence of people with non-Chinese ancestry, Hongkongers overseas may or may not identify with the Chinese diaspora in the same country. \n\nOne of the sources of Hongkongers in the Netherlands are Chinese Indonesians who first migrated to Hong Kong and moved onwards to the Netherlands with their children.\n\n\n"}
{"id": "37522119", "url": "https://en.wikipedia.org/wiki?curid=37522119", "title": "Illinois lunar sample displays", "text": "Illinois lunar sample displays\n\nThe Illinois lunar sample displays are two commemorative plaques consisting of small fragments of moon specimen brought back with the Apollo 11 and Apollo 17 lunar missions and given in the 1970s to the people of Illinois by United States President Richard Nixon as goodwill gifts.\n\nThe Illinois Apollo 11 lunar sample display commemorative plaque consists of four \"moon rock\" rice-size particle specimens that were collected by Apollo 11 astronauts Neil Armstrong and Buzz Aldrin in 1969 and a small Illinois state flag that was taken to the moon and back on Apollo 11.\n\nThe four charcoal-gray \"moon rocks\" range in size from 1.5 to 3 millimeters (1/16 to 1/8 inch) and weigh about 50 mg total. They are encased in a clear plastic button the size of a coin which is mounted to a wooden board approximately one foot square on a small podium pedestal display. The small podium plaque display also has mounted on it a small Illinois flag that had been taken to the moon and back, which lies directly below the \"moon rocks\", and a formal message from President Richard Nixon stating that it is a gift to the people of the state of Illinois. Similar lunar sample displays were also distributed to all the other states of the United States and all the countries of the world at the time.\n\nThe Illinois Apollo 17 lunar sample display commemorative style plaque, measuring 10 by 14 inches, consists of one \"moon rock\" particle specimen that was cut from lunar basalt 70017 and an Illinois state flag. The basalt 70017 was collected by Apollo 17 astronaut Harrison Schmitt on the moon in 1972. Once lunar basalt 70017 was brought back to earth from the moon, the basalt moon rock was cut up into small fragments of approximately 1 gram. The specimen was encased in a plastic ball and mounted on the wooden plaque along with the Illinois state flag which had been taken to the moon and back by the crew of Apollo 17. The plaque was then distributed in the 1970s by President Richard Nixon to the people of Illinois as he did for the Apollo 11 plaque display gifts. This was done as a goodwill gesture to promote peace and harmony.\n\nThe Illinois Apollo 11 \"goodwill moon rocks\" plaque display was received by Illinois Governor Richard Ogilvie on behalf of the people of the state of Illinois. Ogilvie, in turn, presented the Illinois Apollo 11 lunar samples display to Milton D. Thompson, Director of the Illinois State Museum. Richard Leary, curator of geology, ultimately handled the display. The display is now on permanent exhibit in the Illinois State Museum near the Hall of Geology.\n\nWhile other \"goodwill moon rocks\" commemorative displays were reported lost or missing by many recipient states, both the Illinois Apollo 11 and Apollo 17 \"goodwill moon rocks\" commemorative plaque displays were kept at the Illinois State Museum. Both were on public display until about 1990. They were then removed and put away in a cabinet. For about 10 years the public did not see the displays, but during that time they were brought for viewing to a Research and Collections Center in Springfield and to various gem, mineral, and rock shows. In 2010 a student from the University of Phoenix investigated the whereabouts of the \"goodwill moon rocks\" as an assignment in investigative techniques. Because of her tenacity, the Illinois State Museum put the displays back on public exhibition that year.\n\n"}
{"id": "14914", "url": "https://en.wikipedia.org/wiki?curid=14914", "title": "Industrial Revolution", "text": "Industrial Revolution\n\nThe Industrial Revolution was the transition to new manufacturing processes in the period from about 1760 to sometime between 1820 and 1840. This transition included going from hand production methods to machines, new chemical manufacturing and iron production processes, the increasing use of steam power, the development of machine tools and the rise of the factory system.\n\nTextiles were the dominant industry of the Industrial Revolution in terms of employment, value of output and capital invested. The textile industry was also the first to use modern production methods.\n\nThe Industrial Revolution began in Great Britain, and many of the technological innovations were of British origin. By the mid-18th century Britain was the world's leading commercial nation, controlling a global trading empire with colonies in North America and the Caribbean, and with some political influence on the Indian subcontinent, through the activities of the East India Company. The development of trade and the rise of business were major causes of the Industrial Revolution.\n\nThe Industrial Revolution marks a major turning point in history; almost every aspect of daily life was influenced in some way. In particular, average income and population began to exhibit unprecedented sustained growth. Some economists say that the major impact of the Industrial Revolution was that the standard of living for the general population began to increase consistently for the first time in history, although others have said that it did not begin to meaningfully improve until the late 19th and 20th centuries.\n\nGDP per capita was broadly stable before the Industrial Revolution and the emergence of the modern capitalist economy, while the Industrial Revolution began an era of per-capita economic growth in capitalist economies. Economic historians are in agreement that the onset of the Industrial Revolution is the most important event in the history of humanity since the domestication of animals and plants.\n\nAlthough the structural change from agriculture to industry is widely associated with Industrial Revolution, in United Kingdom it was already almost complete by 1760.\n\nThe precise start and end of the Industrial Revolution is still debated among historians, as is the pace of economic and social changes. Eric Hobsbawm held that the Industrial Revolution began in Britain in the 1780s and was not fully felt until the 1830s or 1840s, while T. S. Ashton held that it occurred roughly between 1760 and 1830. Rapid industrialization first began in Britain, starting with mechanized spinning in the 1780s, with high rates of growth in steam power and iron production occurring after 1800. Mechanized textile production spread from Great Britain to continental Europe and the United States in the early 19th century, with important centres of textiles, iron and coal emerging in Belgium and the United States and later textiles in France.\n\nAn economic recession occurred from the late 1830s to the early 1840s when the adoption of the original innovations of the Industrial Revolution, such as mechanized spinning and weaving, slowed and their markets matured. Innovations developed late in the period, such as the increasing adoption of locomotives, steamboats and steamships, hot blast iron smelting and new technologies, such as the electrical telegraph, widely introduced in the 1840s and 1850s, were not powerful enough to drive high rates of growth. Rapid economic growth began to occur after 1870, springing from a new group of innovations in what has been called the Second Industrial Revolution. These new innovations included new steel making processes, the large-scale manufacture of machine tools and the use of increasingly advanced machinery in steam-powered factories.\n\nThe earliest recorded use of the term \"Industrial Revolution\" seems to have been in a letter from 6 July 1799 written by French envoy Louis-Guillaume Otto, announcing that France had entered the race to industrialise. In his 1976 book \"\", Raymond Williams states in the entry for \"Industry\": \"The idea of a new social order based on major industrial change was clear in Southey and Owen, between 1811 and 1818, and was implicit as early as Blake in the early 1790s and Wordsworth at the turn of the [19th] century.\" The term \"Industrial Revolution\" applied to technological change was becoming more common by the late 1830s, as in Jérôme-Adolphe Blanqui's description in 1837 of \"la révolution industrielle\". Friedrich Engels in \"The Condition of the Working Class in England\" in 1844 spoke of \"an industrial revolution, a revolution which at the same time changed the whole of civil society\". However, although Engels wrote in the 1840s, his book was not translated into English until the late 1800s, and his expression did not enter everyday language until then. Credit for popularising the term may be given to Arnold Toynbee, whose 1881 lectures gave a detailed account of the term.\n\nSome historians, such as John Clapham and Nicholas Crafts, have argued that the economic and social changes occurred gradually and the term \"revolution\" is a misnomer. This is still a subject of debate among some historians.\n\nThe commencement of the Industrial Revolution is closely linked to a small number of innovations, beginning in the second half of the 18th century. By the 1830s the following gains had been made in important technologies:\n\nIn 1750 Britain imported 2.5 million pounds of raw cotton, most of which was spun and woven by cottage industry in Lancashire. The work was done by hand in workers' homes or occasionally in shops of master weavers. In 1787 raw cotton consumption was 22 million pounds, most of which was cleaned, carded and spun on machines. The British textile industry used 52 million pounds of cotton in 1800, which increased to 588 million pounds in 1850.\n\nThe share of value added by the cotton textile industry in Britain was 2.6% in 1760, 17% in 1801 and 22.4% in 1831. Value added by the British woollen industry was 14.1% in 1801. Cotton factories in Britain numbered approximately 900 in 1797. In 1760 approximately one-third of cotton cloth manufactured in Britain was exported, rising to two-thirds by 1800. In 1781 cotton spun amounted to 5.1 million pounds, which increased to 56 million pounds by 1800. In 1800 less than 0.1% of world cotton cloth was produced on machinery invented in Britain. In 1788 there were 50,000 spindles in Britain, rising to 7 million over the next 30 years.\n\nWages in Lancashire, a core region for cottage industry and later factory spinning and weaving, were about six times those in India in 1770, when overall productivity in Britain was about three times higher than in India.\n\nParts of India, China, Central America, South America and the Middle-East have a long history of hand manufacturing cotton textiles, which became a major industry sometime after 1000 AD. In tropical and subtropical regions where it was grown, most was grown by small farmers alongside their food crops and was spun and woven in households, largely for domestic consumption. In the 15th century China began to require households to pay part of their taxes in cotton cloth. By the 17th century almost all Chinese wore cotton clothing. Almost everywhere cotton cloth could be used as a medium of exchange. In India a significant amount of cotton textiles were manufactured for distant markets, often produced by professional weavers. Some merchants also owned small weaving workshops. India produced a variety of cotton cloth, some of exceptionally fine quality.\n\nCotton was a difficult raw material for Europe to obtain before it was grown on colonial plantations in the Americas. The early Spanish explorers found Native Americans growing unknown species of excellent quality cotton: sea island cotton (\"Gossypium barbadense\") and upland green seeded cotton \"Gossypium hirsutum\". Sea island cotton grew in tropical areas and on barrier islands of Georgia and South Carolina, but did poorly inland. Sea island cotton began being exported from Barbados in the 1650s. Upland green seeded cotton grew well on inland areas of the southern U.S., but was not economical because of the difficulty of removing seed, a problem solved by the cotton gin. A strain of cotton seed brought from Mexico to Natchez, Mississippi, USA in 1806 became the parent genetic material for over 90% of world cotton production today; it produced bolls that were three to four times faster to pick.\n\nThe Age of Discovery was followed by a period of colonialism beginning around the 16th century. Following the discovery of a trade route to India around southern Africa by the Portuguese, the Dutch established the Verenigde Oostindische Compagnie (abbr. VOC) or Dutch East India Company and the British founded the East India Company, along with smaller companies of different nationalities which established trading posts and employed agents to engage in trade throughout the Indian Ocean region and between the Indian Ocean region and North Atlantic Europe. One of the largest segments of this trade was in cotton textiles, which were purchased in India and sold in Southeast Asia, including the Indonesian archipelago, where spices were purchased for sale to Southeast Asia and Europe. By the mid-1760s cloth was over three-quarters of the East India Company's exports. Indian textiles were in demand in North Atlantic region of Europe where previously only wool and linen were available; however, the amount of cotton goods consumed in Western Europe was minor until the early 19th century.\n\nBy 1600 Flemish refugees began weaving cotton cloth in English towns where cottage spinning and weaving of wool and linen was well established; however, they were left alone by the guilds who did not consider cotton a threat. Earlier European attempts at cotton spinning and weaving were in 12th century Italy and 15th century southern Germany, but these industries eventually ended when the supply of cotton was cut off. The Moors in Spain grew, spun and wove cotton beginning around the 10th century.\n\nBritish cloth could not compete with Indian cloth because India's labor cost was approximately one-fifth to one-sixth that of Britain's. In 1700 and 1721 the British government passed Calico Acts in order to protect the domestic woollen and linen industries from the increasing amounts of cotton fabric imported from India.\n\nThe demand for heavier fabric was met by a domestic industry based around Lancashire that produced fustian, a cloth with flax warp and cotton weft. Flax was used for the warp because wheel-spun cotton did not have sufficient strength, but the resulting blend was not as soft as 100% cotton and was more difficult to sew.\n\nOn the eve of the Industrial Revolution, spinning and weaving were done in households, for domestic consumption and as a cottage industry under the putting-out system. Occasionally the work was done in the workshop of a master weaver. Under the putting-out system, home-based workers produced under contract to merchant sellers, who often supplied the raw materials. In the off season the women, typically farmers' wives, did the spinning and the men did the weaving. Using the spinning wheel, it took anywhere from four to eight spinners to supply one hand loom weaver.\n\nThe flying shuttle, patented in 1733 by John Kay, with a number of subsequent improvements including an important one in 1747, doubled the output of a weaver, worsening the imbalance between spinning and weaving. It became widely used around Lancashire after 1760 when John's son, Robert, invented the drop box, which facilitated changing thread colors.\n\nLewis Paul patented the roller spinning frame and the flyer-and-bobbin system for drawing wool to a more even thickness. The technology was developed with the help of John Wyatt of Birmingham. Paul and Wyatt opened a mill in Birmingham which used their new rolling machine powered by a donkey. In 1743 a factory opened in Northampton with 50 spindles on each of five of Paul and Wyatt's machines. This operated until about 1764. A similar mill was built by Daniel Bourn in Leominster, but this burnt down. Both Lewis Paul and Daniel Bourn patented carding machines in 1748. Based on two sets of rollers that travelled at different speeds, it was later used in the first cotton spinning mill. Lewis's invention was later developed and improved by Richard Arkwright in his water frame and Samuel Crompton in his spinning mule.\n\nIn 1764 in the village of Stanhill, Lancashire, James Hargreaves invented the spinning jenny, which he patented in 1770. It was the first practical spinning frame with multiple spindles. The jenny worked in a similar manner to the spinning wheel, by first clamping down on the fibres, then by drawing them out, followed by twisting. It was a simple, wooden framed machine that only cost about £6 for a 40-spindle model in 1792, and was used mainly by home spinners. The jenny produced a lightly twisted yarn only suitable for weft, not warp.\n\nThe spinning frame or water frame was developed by Richard Arkwright who, along with two partners, patented it in 1769. The design was partly based on a spinning machine built for Thomas High by clockmaker John Kay, who was hired by Arkwright. For each spindle the water frame used a series of four pairs of rollers, each operating at a successively higher rotating speed, to draw out the fibre, which was then twisted by the spindle. The roller spacing was slightly longer than the fibre length. Too close a spacing caused the fibres to break while too distant a spacing caused uneven thread. The top rollers were leather-covered and loading on the rollers was applied by a weight. The weights kept the twist from backing up before the rollers. The bottom rollers were wood and metal, with fluting along the length. The water frame was able to produce a hard, medium count thread suitable for warp, finally allowing 100% cotton cloth to be made in Britain. A horse powered the first factory to use the spinning frame. Arkwright and his partners used water power at a factory in Cromford, Derbyshire in 1771, giving the invention its name.\n\nSamuel Crompton's Spinning Mule was introduced in 1779. Mule implies a hybrid because it was a combination of the spinning jenny and the water frame, in which the spindles were placed on a carriage, which went through an operational sequence during which the rollers stopped while the carriage moved away from the drawing roller to finish drawing out the fibres as the spindles started rotating. Crompton's mule was able to produce finer thread than hand spinning and at a lower cost. Mule spun thread was of suitable strength to be used as warp, and finally allowed Britain to produce highly competitive yarn in large quantities.\n\nRealising that the expiration of the Arkwright patent would greatly increase the supply of spun cotton and lead to a shortage of weavers, Edmund Cartwright developed a vertical power loom which he patented in 1785. In 1776 he patented a two-man operated loom which was more conventional. Cartwright built two factories; the first burned down and the second was sabotaged by his workers. Cartwright's loom design had several flaws, the most serious being thread breakage. Samuel Horrocks patented a fairly successful loom in 1813. Horock's loom was improved by Richard Roberts in 1822 and these were produced in large numbers by Roberts, Hill & Co.\n\nThe demand for cotton presented an opportunity to planters in the Southern United States, who thought upland cotton would be a profitable crop if a better way could be found to remove the seed. Eli Whitney responded to the challenge by inventing the inexpensive cotton gin. A man using a cotton gin could remove seed from as much upland cotton in one day as would previously, working at the rate of one pound of cotton per day, have taken a woman two months to process.\n\nThese advances were capitalised on by entrepreneurs, of whom the best known is Richard Arkwright. He is credited with a list of inventions, but these were actually developed by such people as Thomas Highs and John Kay; Arkwright nurtured the inventors, patented the ideas, financed the initiatives, and protected the machines. He created the cotton mill which brought the production processes together in a factory, and he developed the use of power – first horse power and then water powerwhich made cotton manufacture a mechanised industry. Other inventors increased the efficiency of the individual steps of spinning (carding, twisting and spinning, and rolling) so that the supply of yarn increased greatly. Before long steam power was applied to drive textile machinery. Manchester acquired the nickname Cottonopolis during the early 19th century owing to its sprawl of textile factories.\n\nAlthough mechanization dramatically decreased the cost of cotton cloth, by the mid-19th century machine-woven cloth still could not equal the quality of hand-woven Indian cloth, in part due to the fineness of thread made possible by the type of cotton used in India, which allowed high thread counts. However, the high productivity of British textile manufacturing allowed coarser grades of British cloth to undersell hand-spun and woven fabric in low-wage India, eventually destroying the industry.\n\nThe earliest European attempts at mechanized spinning were with wool; however, wool spinning proved more difficult to mechanize than cotton. Productivity improvement in wool spinning during the Industrial Revolution was significant but was far less than that of cotton.\n\nArguably the first highly mechanised factory was John Lombe's water-powered silk mill at Derby, operational by 1721. Lombe learned silk thread manufacturing by taking a job in Italy and acting as an industrial spy; however, because the Italian silk industry guarded its secrets closely, the state of the industry at that time is unknown. Although Lombe's factory was technically successful, the supply of raw silk from Italy was cut off to eliminate competition. In order to promote manufacturing the Crown paid for models of Lombe's machinery which were exhibited in the Tower of London.\n\nBar iron was the commodity form of iron used as the raw material for making hardware goods such as nails, wire, hinges, horse shoes, wagon tires, chains, etc. and for structural shapes. A small amount of bar iron was converted into steel. Cast iron was used for pots, stoves and other items where its brittleness was tolerable. Most cast iron was refined and converted to bar iron, with substantial losses. Bar iron was also made by the bloomery process, which was the predominant iron smelting process until the late 18th century.\n\nIn the UK in 1720 there were 20,500 tons of cast iron produced with charcoal and 400 tons with coke. In 1750 charcoal iron production was 24,500 and coke iron was 2,500 tons. In 1788 the production of charcoal cast iron was 14,000 tons while coke iron production was 54,000 tons. In 1806 charcoal cast iron production was 7,800 tons and coke cast iron was 250,000 tons.\n\nIn 1750 the UK imported 31,200 tons of bar iron and either refined from cast iron or directly produced 18,800 tons of bar iron using charcoal and 100 tons using coke. In 1796 the UK was making 125,000 tons of bar iron with coke and 6,400 tons with charcoal; imports were 38,000 tons and exports were 24,600 tons. In 1806 the UK did not import bar iron but exported 31,500 tons.\n\nA major change in the iron industries during the era of the Industrial Revolution was the replacement of wood and other bio-fuels with coal. For a given amount of heat, coal required much less labour to mine than cutting wood and converting it to charcoal, and coal was much more abundant than wood, supplies of which were becoming scarce before the enormous increase in iron production that took place in the late 18th century. By 1750 coke had generally replaced charcoal in smelting of copper and lead and was in widespread use in making glass. In the smelting and refining of iron, coal and coke produced inferior iron to that made with charcoal because of the coal's sulfur content. Low sulfur coals were known, but they still contained harmful amounts. Conversion of coal to coke only slightly reduces the sulfur content. A minority of coals are coking.\n\nAnother factor limiting the iron industry before the Industrial Revolution was the scarcity of water power to power blast bellows. This limitation was overcome by the steam engine.\n\nUse of coal in iron smelting started somewhat before the Industrial Revolution, based on innovations by Sir Clement Clerke and others from 1678, using coal reverberatory furnaces known as cupolas. These were operated by the flames playing on the ore and charcoal or coke mixture, reducing the oxide to metal. This has the advantage that impurities (such as sulphur ash) in the coal do not migrate into the metal. This technology was applied to lead from 1678 and to copper from 1687. It was also applied to iron foundry work in the 1690s, but in this case the reverberatory furnace was known as an air furnace. (The foundry cupola is a different, and later, innovation.)\n\nBy 1709 Abraham Darby made progress using coke to fuel his blast furnaces at Coalbrookdale. However, the coke pig iron he made was not suitable for making wrought iron and was used mostly for the production of cast iron goods, such as pots and kettles. He had the advantage over his rivals in that his pots, cast by his patented process, were thinner and cheaper than theirs.\n\nCoke pig iron was hardly used to produce wrought iron until 1755-56, when Darby's son Abraham Darby II built furnaces at Horsehay and Ketley where low sulfur coal was available (and not far from Coalbrookdale). These new furnaces were equipped with water-powered bellows, the water being pumped by Newcomen steam engines. The Newcomen engines were not attached directly to the blowing cylinders because the engines alone could not produce a steady air blast. Abraham Darby III installed similar steam-pumped, water-powered blowing cylinders at the Dale Company when he took control in 1768. The Dale Company used several Newcomen engines to drain its mines and made parts for engines which it sold throughout the country.\n\nSteam engines made the use of higher-pressure and volume blast practical; however, the leather used in bellows was expensive to replace. In 1757, iron master John Wilkinson patented a hydraulic powered blowing engine for blast furnaces. The blowing cylinder for blast furnaces was introduced in 1760 and the first blowing cylinder made of cast iron is believed to be the one used at Carrington in 1768 that was designed by John Smeaton. Cast iron cylinders for use with a piston were difficult to manufacture; the cylinders had to be free of holes and had to be machined smooth and straight to remove any warping. James Watt had great difficulty trying to have a cylinder made for his first steam engine. In 1774 John Wilkinson, who built a cast iron blowing cylinder for his iron works, invented a precision boring machine for boring cylinders. After Wilkinson bored the first successful cylinder for a Boulton and Watt steam engine in 1776, he was given an exclusive contract for providing cylinders. After Watt developed a rotary steam engine in 1782, they were widely applied to blowing, hammering, rolling and slitting.\n\nThe solutions to the sulfur problem were the addition of sufficient limestone to the furnace to force sulfur into the slag and the use of low sulfur coal. Use of lime or limestone required higher furnace temperatures to form a free-flowing slag. The increased furnace temperature made possible by improved blowing also increased the capacity of blast furnaces and allowed for increased furnace height. In addition to lower cost and greater availability, coke had other important advantages over charcoal in that it was harder and made the column of materials (iron ore, fuel, slag) flowing down the blast furnace more porous and did not crush in the much taller furnaces of the late 19th century.\n\nAs cast iron became cheaper and widely available, it began being a structural material for bridges and buildings. A famous early example was the Iron Bridge built in 1778 with cast iron produced by Abraham Darby III. However, most cast iron was converted to wrought iron.\n\nEurope relied on the bloomery for most of its wrought iron until the large scale production of cast iron. Conversion of cast iron was done in a finery forge, as it long had been. An improved refining process known as potting and stamping was developed, but this was superseded by Henry Cort's puddling process. Cort developed two significant iron manufacturing processes: rolling in 1783 and puddling in 1784. Puddling produced a structural grade iron at a relatively low cost.\n\nPuddling was a means of decarburizing molten pig iron by slow oxidation in a reverberatory furnace by manually stirring it with a long rod. The decarburized iron, having a higher melting point than cast iron, was raked into globs by the puddler. When the glob was large enough, the puddler would remove it. Puddling was backbreaking and extremely hot work. Few puddlers lived to be 40. Because puddling was done in a reverberatory furnace, coal or coke could be used as fuel. The puddling process continued to be used until the late 19th century when iron was being displaced by steel. Because puddling required human skill in sensing the iron globs, it was never successfully mechanised. Rolling was an important part of the puddling process because the grooved rollers expelled most of the molten slag and consolidated the mass of hot wrought iron. Rolling was 15 times faster at this than a trip hammer. A different use of rolling, which was done at lower temperatures than that for expelling slag, was in the production of iron sheets, and later structural shapes such as beams, angles and rails.\nThe puddling process was improved in 1818 by Baldwyn Rogers, who replaced some of the sand lining on the reverberatory furnace bottom with iron oxide. In 1838 John Hall patented the use of roasted tap cinder (iron silicate) for the furnace bottom, greatly reducing the loss of iron through increased slag caused by a sand lined bottom. The tap cinder also tied up some phosphorus, but this was not understood at the time. Hall's process also used iron scale or rust, which reacted with carbon in the molten iron. Hall's process, called \"wet puddling\", reduced losses of iron with the slag from almost 50% to around 8%.\n\nPuddling became widely used after 1800. Up to that time British iron manufacturers had used considerable amounts of iron imported from Sweden and Russia to supplement domestic supplies. Because of the increased British production, imports began to decline in 1785 and by the 1790s Britain eliminated imports and became a net exporter of bar iron.\n\nHot blast, patented by James Beaumont Neilson in 1828, was the most important development of the 19th century for saving energy in making pig iron. By using preheated combustion air, the amount of fuel to make a unit of pig iron was reduced at first by between one-third using coke or two-thirds using coal; however, the efficiency gains continued as the technology improved. Hot blast also raised the operating temperature of furnaces, increasing their capacity. Using less coal or coke meant introducing fewer impurities into the pig iron. This meant that lower quality coal or anthracite could be used in areas where coking coal was unavailable or too expensive; however, by the end of the 19th century transportation costs fell considerably.\n\nShortly before the Industrial Revolution an improvement was made in the production of steel, which was an expensive commodity and used only where iron would not do, such as for cutting edge tools and for springs. Benjamin Huntsman developed his crucible steel technique in the 1740s. The raw material for this was blister steel, made by the cementation process.\n\nThe supply of cheaper iron and steel aided a number of industries, such as those making nails, hinges, wire and other hardware items. The development of machine tools allowed better working of iron, causing it to be increasingly used in the rapidly growing machinery and engine industries.\n\nThe development of the stationary steam engine was an important element of the Industrial Revolution; however, during the early period of the Industrial Revolution, most industrial power was supplied by water and wind. In Britain by 1800 an estimated 10,000 horsepower was being supplied by steam. By 1815 steam power had grown to 210,000 hp.\n\nThe first commercially successful industrial use of steam power was due to Thomas Savery in 1698. He constructed and patented in London a low-lift combined vacuum and pressure water pump, that generated about one horsepower (hp) and was used in numerous water works and in a few mines (hence its \"brand name\", \"The Miner's Friend\"). Savery's pump was economical in small horsepower ranges, but was prone to boiler explosions in larger sizes. Savery pumps continued to be produced until the late 18th century.\n\nThe first successful piston steam engine was introduced by Thomas Newcomen before 1712. A number of Newcomen engines were installed in Britain for draining hitherto unworkable deep mines, with the engine on the surface; these were large machines, requiring a significant amount of capital to build, and produced upwards of . They were also used to power municipal water supply pumps. They were extremely inefficient by modern standards, but when located where coal was cheap at pit heads, opened up a great expansion in coal mining by allowing mines to go deeper. Despite their disadvantages, Newcomen engines were reliable and easy to maintain and continued to be used in the coalfields until the early decades of the 19th century. By 1729, when Newcomen died, his engines had spread (first) to Hungary in 1722, Germany, Austria, and Sweden. A total of 110 are known to have been built by 1733 when the joint patent expired, of which 14 were abroad. In the 1770s the engineer John Smeaton built some very large examples and introduced a number of improvements. A total of 1,454 engines had been built by 1800.\n\nA fundamental change in working principles was brought about by Scotsman James Watt. With financial support from his business partner Englishman Matthew Boulton, he had succeeded by 1778 in perfecting his steam engine, which incorporated a series of radical improvements, notably the closing off of the upper part of the cylinder, thereby making the low-pressure steam drive the top of the piston instead of the atmosphere, use of a steam jacket and the celebrated separate steam condenser chamber. The separate condenser did away with the cooling water that had been injected directly into the cylinder, which cooled the cylinder and wasted steam. Likewise, the steam jacket kept steam from condensing in the cylinder, also improving efficiency. These improvements increased engine efficiency so that Boulton & Watts engines used only 20–25% as much coal per horsepower-hour as Newcomen's. Boulton and Watt opened the Soho Foundry for the manufacture of such engines in 1795.\n\nBy 1783 the Watt steam engine had been fully developed into a double-acting rotative type, which meant that it could be used to directly drive the rotary machinery of a factory or mill. Both of Watt's basic engine types were commercially very successful, and by 1800, the firm Boulton & Watt had constructed 496 engines, with 164 driving reciprocating pumps, 24 serving blast furnaces, and 308 powering mill machinery; most of the engines generated from .\n\nUntil about 1800 the most common pattern of steam engine was the beam engine, built as an integral part of a stone or brick engine-house, but soon various patterns of self-contained rotative engines (readily removable, but not on wheels) were developed, such as the table engine. Around the start of the 19th century, at which time the Boulton and Watt patent expired, the Cornish engineer Richard Trevithick and the American Oliver Evans began to construct higher-pressure non-condensing steam engines, exhausting against the atmosphere. High pressure yielded an engine and boiler compact enough to be used on mobile road and rail locomotives and steam boats.\n\nThe development of machine tools, such as the engine lathe, planing, milling and shaping machines powered by these engines, enabled all the metal parts of the engines to be easily and accurately cut and in turn made it possible to build larger and more powerful engines.\n\nSmall industrial power requirements continued to be provided by animal and human muscle until widespread electrification in the early 20th century. These included crank-powered, treadle-powered and horse-powered workshop and light industrial machinery.\n\nPre-industrial machinery was built by various craftsmen – millwrights built water and windmills, carpenters made wooden framing, and smiths and turners made metal parts. Wooden components had the disadvantage of changing dimensions with temperature and humidity, and the various joints tended to rack (work loose) over time. As the Industrial Revolution progressed, machines with metal parts and frames became more common. Other important uses of metal parts were in firearms and threaded fasteners, such as machine screws, bolts and nuts. There was also the need for precision in making parts. Precision would allow better working machinery, interchangeability of parts and standardization of threaded fasteners.\n\nThe demand for metal parts led to the development of several machine tools. They have their origins in the tools developed in the 18th century by makers of clocks and watches and scientific instrument makers to enable them to batch-produce small mechanisms.\n\nBefore the advent of machine tools, metal was worked manually using the basic hand tools of hammers, files, scrapers, saws and chisels. Consequently, the use of metal machine parts was kept to a minimum. Hand methods of production were very laborious and costly and precision was difficult to achieve.\n\nThe first large precision machine tool was the cylinder boring machine invented by John Wilkinson in 1774. It used for boring the large-diameter cylinders on early steam engines. Wilkinson's boring machine differed from earlier cantilevered machines used for boring cannon in that the cutting tool was mounted on a beam that ran through the cylinder being bored and was supported outside on both ends.\n\nThe planing machine, the milling machine and the shaping machine were developed in the early decades of the 19th century. Although the milling machine was invented at this time, it was not developed as a serious workshop tool until somewhat later in the 19th century.\n\nHenry Maudslay, who trained a school of machine tool makers early in the 19th century, was a mechanic with superior ability who had been employed at the Royal Arsenal, Woolwich. He worked as an apprentice in the Royal Gun Foundry of Jan Verbruggen. In 1774 Jan Verbruggen had installed a horizontal boring machine in Woolwich which was the first industrial size Lathe in the UK. Maudslay was hired away by Joseph Bramah for the production of high-security metal locks that required precision craftsmanship. Bramah patented a lathe that had similarities to the slide rest lathe. Maudslay perfected the slide rest lathe, which could cut machine screws of different thread pitches by using changeable gears between the spindle and the lead screw. Before its invention screws could not be cut to any precision using various earlier lathe designs, some of which copied from a template. The slide rest lathe was called one of history's most important inventions. Although it was not entirely Maudslay's idea, he was the first person to build a functional lathe using a combination of known innovations of the lead screw, slide rest and change gears.\n\nMaudslay left Bramah's employment and set up his own shop. He was engaged to build the machinery for making ships' pulley blocks for the Royal Navy in the Portsmouth Block Mills. These machines were all-metal and were the first machines for mass production and making components with a degree of interchangeability. The lessons Maudslay learned about the need for stability and precision he adapted to the development of machine tools, and in his workshops he trained a generation of men to build on his work, such as Richard Roberts, Joseph Clement and Joseph Whitworth.\n\nJames Fox of Derby had a healthy export trade in machine tools for the first third of the century, as did Matthew Murray of Leeds. Roberts was a maker of high-quality machine tools and a pioneer of the use of jigs and gauges for precision workshop measurement.\n\nThe impact of machine tools during the Industrial Revolution was not that great because other than firearms, threaded fasteners and a few other industries there were few mass-produced metal parts. The techniques to make mass-produced metal parts made with sufficient precision to be interchangeable is largely attributed to a program of the U.S. Department of War which perfected interchangeable parts for firearms in the early 19th century.\n\nIn the half century following the invention of the fundamental machine tools the machine industry became the largest industrial sector of the U.S. economy, by value added.\n\nThe large-scale production of chemicals was an important development during the Industrial Revolution. The first of these was the production of sulphuric acid by the lead chamber process invented by the Englishman John Roebuck (James Watt's first partner) in 1746. He was able to greatly increase the scale of the manufacture by replacing the relatively expensive glass vessels formerly used with larger, less expensive chambers made of riveted sheets of lead. Instead of making a small amount each time, he was able to make around in each of the chambers, at least a tenfold increase.\n\nThe production of an alkali on a large scale became an important goal as well, and Nicolas Leblanc succeeded in 1791 in introducing a method for the production of sodium carbonate. The Leblanc process was a reaction of sulphuric acid with sodium chloride to give sodium sulphate and hydrochloric acid. The sodium sulphate was heated with limestone (calcium carbonate) and coal to give a mixture of sodium carbonate and calcium sulphide. Adding water separated the soluble sodium carbonate from the calcium sulphide. The process produced a large amount of pollution (the hydrochloric acid was initially vented to the air, and calcium sulphide was a useless waste product). Nonetheless, this synthetic soda ash proved economical compared to that from burning specific plants (barilla) or from kelp, which were the previously dominant sources of soda ash, and also to potash (potassium carbonate) produced from hardwood ashes.\n\nThese two chemicals were very important because they enabled the introduction of a host of other inventions, replacing many small-scale operations with more cost-effective and controllable processes. Sodium carbonate had many uses in the glass, textile, soap, and paper industries. Early uses for sulphuric acid included pickling (removing rust) iron and steel, and for bleaching cloth.\n\nThe development of bleaching powder (calcium hypochlorite) by Scottish chemist Charles Tennant in about 1800, based on the discoveries of French chemist Claude Louis Berthollet, revolutionised the bleaching processes in the textile industry by dramatically reducing the time required (from months to days) for the traditional process then in use, which required repeated exposure to the sun in bleach fields after soaking the textiles with alkali or sour milk. Tennant's factory at St Rollox, North Glasgow, became the largest chemical plant in the world.\n\nAfter 1860 the focus on chemical innovation was in dyestuffs, and Germany took world leadership, building a strong chemical industry. Aspiring chemists flocked to German universities in the 1860–1914 era to learn the latest techniques. British scientists by contrast, lacked research universities and did not train advanced students; instead, the practice was to hire German-trained chemists.\n\nIn 1824 Joseph Aspdin, a British bricklayer turned builder, patented a chemical process for making portland cement which was an important advance in the building trades. This process involves sintering a mixture of clay and limestone to about , then grinding it into a fine powder which is then mixed with water, sand and gravel to produce concrete. Portland cement was used by the famous English engineer Marc Isambard Brunel several years later when constructing the Thames Tunnel. Cement was used on a large scale in the construction of the London sewerage system a generation later.\n\nAnother major industry of the later Industrial Revolution was gas lighting. Though others made a similar innovation elsewhere, the large-scale introduction of this was the work of William Murdoch, an employee of Boulton & Watt, the Birmingham steam engine pioneers. The process consisted of the large-scale gasification of coal in furnaces, the purification of the gas (removal of sulphur, ammonia, and heavy hydrocarbons), and its storage and distribution. The first gas lighting utilities were established in London between 1812 and 1820. They soon became one of the major consumers of coal in the UK. Gas lighting affected social and industrial organisation because it allowed factories and stores to remain open longer than with tallow candles or oil. Its introduction allowed nightlife to flourish in cities and towns as interiors and streets could be lighted on a larger scale than before.\n\nA new method of producing glass, known as the cylinder process, was developed in Europe during the early 19th century. In 1832 this process was used by the Chance Brothers to create sheet glass. They became the leading producers of window and plate glass. This advancement allowed for larger panes of glass to be created without interruption, thus freeing up the space planning in interiors as well as the fenestration of buildings. The Crystal Palace is the supreme example of the use of sheet glass in a new and innovative structure.\n\nA machine for making a continuous sheet of paper on a loop of wire fabric was patented in 1798 by Nicholas Louis Robert who worked for Saint-Léger Didot family in France. The paper machine is known as a Fourdrinier after the financiers, brothers Sealy and Henry Fourdrinier, who were stationers in London. Although greatly improved and with many variations, the Fourdriner machine is the predominant means of paper production today.\n\nThe method of continuous production demonstrated by the paper machine influenced the development of continuous rolling of iron and later steel and other continuous production processes.\n\nThe British Agricultural Revolution is considered one of the causes of the Industrial Revolution because improved agricultural productivity freed up workers to work in other sectors of the economy. However, per-capita food supply in Europe was stagnant or declining and did not improve in some parts of Europe until the late 18th century.\n\nIndustrial technologies that affected farming included the seed drill, the Dutch plough, which contained iron parts, and the threshing machine.\n\nJethro Tull invented an improved seed drill in 1701. It was a mechanical seeder which distributed seeds evenly across a plot of land and planted them at the correct depth. This was important because the yield of seeds harvested to seeds planted at that time was around four or five. Tull's seed drill was very expensive and not very reliable and therefore did not have much of an impact. Good quality seed drills were not produced until the mid 18th century.\n\nJoseph Foljambe's \"Rotherham plough\" of 1730 was the first commercially successful iron plough. The threshing machine, invented by Andrew Meikle in 1784, displaced hand threshing with a flail, a laborious job that took about one-quarter of agricultural labour. It took several decades to diffuse and was the final straw for many farm labourers, who faced near starvation, leading to the 1830 agricultural rebellion of the Swing Riots.\n\nMachine tools and metalworking techniques developed during the Industrial Revolution eventually resulted in precision manufacturing techniques in the late 19th century for mass-producing agricultural equipment, such as reapers, binders and combine harvesters.\n\nCoal mining in Britain, particularly in South Wales, started early. Before the steam engine, pits were often shallow bell pits following a seam of coal along the surface, which were abandoned as the coal was extracted. In other cases, if the geology was favourable, the coal was mined by means of an adit or drift mine driven into the side of a hill. Shaft mining was done in some areas, but the limiting factor was the problem of removing water. It could be done by hauling buckets of water up the shaft or to a sough (a tunnel driven into a hill to drain a mine). In either case, the water had to be discharged into a stream or ditch at a level where it could flow away by gravity. The introduction of the steam pump by Thomas Savery in 1698 and the Newcomen steam engine in 1712 greatly facilitated the removal of water and enabled shafts to be made deeper, enabling more coal to be extracted. These were developments that had begun before the Industrial Revolution, but the adoption of John Smeaton's improvements to the Newcomen engine followed by James Watt's more efficient steam engines from the 1770s reduced the fuel costs of engines, making mines more profitable. The Cornish engine, developed in the 1810s, was much more efficient than the Watt steam engine.\n\nCoal mining was very dangerous owing to the presence of firedamp in many coal seams. Some degree of safety was provided by the safety lamp which was invented in 1816 by Sir Humphry Davy and independently by George Stephenson. However, the lamps proved a false dawn because they became unsafe very quickly and provided a weak light. Firedamp explosions continued, often setting off coal dust explosions, so casualties grew during the entire 19th century. Conditions of work were very poor, with a high casualty rate from rock falls.\n\nAt the beginning of the Industrial Revolution, inland transport was by navigable rivers and roads, with coastal vessels employed to move heavy goods by sea. Wagonways were used for conveying coal to rivers for further shipment, but canals had not yet been widely constructed. Animals supplied all of the motive power on land, with sails providing the motive power on the sea. The first horse railways were introduced toward the end of the 18th century, with steam locomotives being introduced in the early decades of the 19th century. Improving sailing technologies boosted average sailing speed 50% between 1750 and 1830.\n\nThe Industrial Revolution improved Britain's transport infrastructure with a turnpike road network, a canal and waterway network, and a railway network. Raw materials and finished products could be moved more quickly and cheaply than before. Improved transportation also allowed new ideas to spread quickly.\n\nBefore and during the Industrial Revolution navigation on several British rivers was improved by removing obstructions, straightening curves, widening and deepening and building navigation locks. Britain had over 1000 miles of navigable rivers and streams by 1750.\n\nCanals and waterways allowed bulk materials to be economically transported long distances inland. This was because a horse could pull a barge with a load dozens of times larger than the load that could be drawn in a cart.\n\nBuilding of canals dates to ancient times. The Grand Canal in China, \"the world's largest artificial waterway and oldest canal still in existence,\" parts of which were started between the 6th and 4th centuries BC, is long and links Hangzhou with Beijing.\n\nIn the UK, canals began to be built in the late 18th century to link the major manufacturing centres across the country. Known for its huge commercial success, the Bridgewater Canal in North West England, which opened in 1761 and was mostly funded by The 3rd Duke of Bridgewater. From Worsley to the rapidly growing town of Manchester its construction cost £168,000 (£ ), but its advantages over land and river transport meant that within a year of its opening in 1761, the price of coal in Manchester fell by about half. This success helped inspire a period of intense canal building, known as Canal Mania. New canals were hastily built in the aim of replicating the commercial success of the Bridgewater Canal, the most notable being the Leeds and Liverpool Canal and the Thames and Severn Canal which opened in 1774 and 1789 respectively.\n\nBy the 1820s a national network was in existence. Canal construction served as a model for the organisation and methods later used to construct the railways. They were eventually largely superseded as profitable commercial enterprises by the spread of the railways from the 1840s on. The last major canal to be built in the United Kingdom was the Manchester Ship Canal, which upon opening in 1894 was the largest ship canal in the world, and opened Manchester as a port. However it never achieved the commercial success its sponsors had hoped for and signalled canals as a dying mode of transport in an age dominated by railways, which were quicker and often cheaper.\n\nBritain's canal network, together with its surviving mill buildings, is one of the most enduring features of the early Industrial Revolution to be seen in Britain.\n\nFrance was known for having an excellent system of roads at the time of the Industrial Revolution; however, most of the roads on the European Continent and in the U.K. were in bad condition and dangerously rutted.\n\nMuch of the original British road system was poorly maintained by thousands of local parishes, but from the 1720s (and occasionally earlier) turnpike trusts were set up to charge tolls and maintain some roads. Increasing numbers of main roads were turnpiked from the 1750s to the extent that almost every main road in England and Wales was the responsibility of a turnpike trust. New engineered roads were built by John Metcalf, Thomas Telford and most notably John McAdam, with the first 'macadamised' stretch of road being Marsh Road at Ashton Gate, Bristol in 1816. The major turnpikes radiated from London and were the means by which the Royal Mail was able to reach the rest of the country. Heavy goods transport on these roads was by means of slow, broad wheeled, carts hauled by teams of horses. Lighter goods were conveyed by smaller carts or by teams of pack horse. Stagecoaches carried the rich, and the less wealthy could pay to ride on carriers carts.\n\nReducing friction was one of the major reasons for the success of railroads compared to wagons. This was demonstrated on an iron plate covered wooden tramway in 1805 at Croydon, England.\n“A good horse on an ordinary turnpike road can draw two thousand pounds, or one ton. A party of gentlemen were invited to witness the experiment, that the superiority of the new road might be established by ocular demonstration. Twelve wagons were loaded with stones, till each wagon weighed three tons, and the wagons were fastened together. A horse was then attached, which drew the wagons with ease, six miles in two hours, having stopped four times, in order to show he had the power of starting, as well as drawing his great load.”\n\nRailways were made practical by the widespread introduction of inexpensive puddled iron after 1800, the rolling mill for making rails, and the development of the high-pressure steam engine also around 1800.\n\nWagonways for moving coal in the mining areas had started in the 17th century and were often associated with canal or river systems for the further movement of coal. These were all horse drawn or relied on gravity, with a stationary steam engine to haul the wagons back to the top of the incline. The first applications of the steam locomotive were on wagon or plate ways (as they were then often called from the cast-iron plates used). Horse-drawn public railways did not begin until the early years of the 19th century when improvements to pig and wrought iron production were lowering costs. See: Metallurgy\n\nSteam locomotives began being built after the introduction of high-pressure steam engines after the expiration of the Boulton and Watt patent in 1800. High-pressure engines exhausted used steam to the atmosphere, doing away with the condenser and cooling water. They were also much lighter weight and smaller in size for a given horsepower than the stationary condensing engines. A few of these early locomotives were used in mines. Steam-hauled public railways began with the Stockton and Darlington Railway in 1825.\n\nThe rapid introduction of railways followed the 1829 Rainhill Trials, which demonstrated Robert Stephenson's successful locomotive design and the 1828 development of Hot blast, which dramatically reduced the fuel consumption of making iron and increased the capacity the blast furnace.\n\nOn 15 September 1830, the Liverpool and Manchester Railway was opened, the first inter-city railway in the world and was attended by Prime Minister, the Duke of Wellington. The railway was engineered by Joseph Locke and George Stephenson, linked the rapidly expanding industrial town of Manchester with the port town of Liverpool. The opening was marred by problems, due to the primitive nature of the technology being employed, however problems were gradually ironed out and the railway became highly successful, transporting passengers and freight. The success of the inter-city railway, particularly in the transport of freight and commodities, led to Railway Mania.\n\nConstruction of major railways connecting the larger cities and towns began in the 1830s but only gained momentum at the very end of the first Industrial Revolution. After many of the workers had completed the railways, they did not return to their rural lifestyles but instead remained in the cities, providing additional workers for the factories.\n\nOther developments included more efficient water wheels, based on experiments conducted by the British engineer John Smeaton the beginnings of a machine industry and the rediscovery of concrete (based on hydraulic lime mortar) by John Smeaton, which had been lost for 1300 years.\n\nPrior to the Industrial Revolution, most of the workforce was employed in agriculture, either as self-employed farmers as landowners or tenants, or as landless agricultural labourers. It was common for families in various parts of the world to spin yarn, weave cloth and make their own clothing. Households also spun and wove for market production. At the beginning of the Industrial Revolution India, China and regions of Iraq and elsewhere in Asia and the Middle East produced most of the world's cotton cloth while Europeans produced wool and linen goods.\n\nIn Britain by the 16th century the putting-out system, by which farmers and townspeople produced goods for market in their homes, often described as \"cottage industry\", was being practiced. Typical putting out system goods included spinning and weaving. Merchant capitalist typically provided the raw materials, paid workers by the piece, and were responsible for the sale of the goods. Embezzlement of supplies by workers and poor quality were common problems. The logistical effort in procuring and distributing raw materials and picking up finished goods were also limitations of the putting out system.\n\nSome early spinning and weaving machinery, such as a 40 spindle jenny for about six pounds in 1792, was affordable for cottagers. Later machinery such as spinning frames, spinning mules and power looms were expensive (especially if water powered), giving rise to capitalist ownership of factories.\n\nThe majority of textile factory workers during the Industrial Revolution were unmarried women and children, including many orphans. They typically worked for 12 to 14 hours per day with only Sundays off. It was common for women take factory jobs seasonally during slack periods of farm work. Lack of adequate transportation, long hours and poor pay made it difficult to recruit and maintain workers. Many workers, such as displaced farmers and agricultural workers, who had nothing but their labour to sell, became factory workers out of necessity. (See: British Agricultural Revolution, Threshing machine)\n\nThe change in the social relationship of the factory worker compared to farmers and cottagers was viewed unfavourably by Karl Marx, however, he recognized the increase in productivity made possible by technology.\n\nSome economists, such as Robert E. Lucas, Jr., say that the real impact of the Industrial Revolution was that \"for the first time in history, the living standards of the masses of ordinary people have begun to undergo sustained growth ... Nothing remotely like this economic behaviour is mentioned by the classical economists, even as a theoretical possibility.\" Others, however, argue that while growth of the economy's overall productive powers was unprecedented during the Industrial Revolution, living standards for the majority of the population did not grow meaningfully until the late 19th and 20th centuries, and that in many ways workers' living standards declined under early capitalism: for instance, studies have shown that real wages in Britain only increased 15% between the 1780s and 1850s, and that life expectancy in Britain did not begin to dramatically increase until the 1870s. Similarly, the average height of the population declined during the Industrial Revolution, implying that their nutritional status was also decreasing. Real wages were not keeping up with the price of food.\n\nDuring the Industrial Revolution, the life expectancy of children increased dramatically. The percentage of the children born in London who died before the age of five decreased from 74.5% in 1730–1749 to 31.8% in 1810–1829.\n\nThe effects on living conditions the industrial revolution have been very controversial, and were hotly debated by economic and social historians from the 1950s to the 1980s. A series of 1950s essays by Henry Phelps Brown and Sheila V. Hopkins later set the academic consensus that the bulk of the population, that was at the bottom of the social ladder, suffered severe reductions in their living standards. During 1813–1913, there was a significant increase in worker wages.\n\nChronic hunger and malnutrition were the norm for the majority of the population of the world including Britain and France, until the late 19th century. Until about 1750, in large part due to malnutrition, life expectancy in France was about 35 years and about 40 years in Britain. The United States population of the time was adequately fed, much taller on average and had life expectancy of 45–50 years although U.S. life expectancy declined by a few years by the mid 19th century. Food consumption per capita also declined during an episode known as the Antebellum Puzzle.\n\nFood supply in Great Britain was adversely affected by the Corn Laws (1815-1846). The Corn Laws, which imposed tariffs on imported grain, were enacted to keep prices high in order to benefit domestic producers. The Corn Laws were repealed in the early years of the Great Irish Famine.\n\nThe initial technologies of the Industrial Revolution, such as mechanized textiles, iron and coal, did little, if anything, to lower food prices. In Britain and the Netherlands, food supply increased before the Industrial Revolution due to better agricultural practices; however, population grew too, as noted by Thomas Malthus. This condition is called the Malthusian trap, and it finally started to overcome by transportation improvements, such as canals, improved roads and steamships. Railroads and steamships were introduced near the end of the Industrial Revolution.\n\nThe very rapid growth in population in the 19th century in the cities included the new industrial and manufacturing cities, as well as service centers such as Edinburgh and London. The critical factor was financing, which was handled by building societies that dealt directly with large contracting firms. Private renting from housing landlords was the dominant tenure. P. Kemp says this was usually of advantage to tenants. People moved in so rapidly that there was not enough capital to build adequate housing for everyone, so low-income newcomers squeezed into increasingly overcrowded slums. Clean water, sanitation, and public health facilities were inadequate; the death rate was high, especially infant mortality, and tuberculosis among young adults. Cholera from polluted water and typhoid were endemic. Unlike rural areas, there were no famines such as devastated Ireland in the 1840s.\n\nA large exposé literature grew up condemning the unhealthy conditions. By far the most famous publication was by one of the founders of the Socialist movement, \"The Condition of the Working Class in England\" in 1844 Friedrich Engels described backstreet sections of Manchester and other mill towns, where people lived in crude shanties and shacks, some not completely enclosed, some with dirt floors. These shanty towns had narrow walkways between irregularly shaped lots and dwellings. There were no sanitary facilities. Population density was extremely high. Not everyone lived in such poor conditions. The Industrial Revolution also created a middle class of businessmen, clerks, foremen and engineers who lived in much better conditions.\n\nConditions improved over the course of the 19th century due to new public health acts regulating things such as sewage, hygiene and home construction. In the introduction of his 1892 edition, Engels notes that most of the conditions he wrote about in 1844 had been greatly improved. For example, the Public Health Act 1875 led to the more sanitary byelaw terraced house.\n\nIn \"The Condition of the Working Class in England\" in 1844 Friedrich Engels described how untreated sewage created awful odors and turned the rivers green in industrial cities.\n\nIn 1854 John Snow traced a cholera outbreak in Soho to fecal contamination of a public water well by a home cesspit. Snow's findings that cholera could be spread by contaminated water took some years to be accepted, but his work led to fundamental changes in the design of public water and waste systems.\n\nPre-industrial water supply relied on gravity systems and pumping of water was done by water wheels. Pipes were typically made of wood. Steam powered pumps and iron pipes allowed the widespread piping of water to horse watering troughs and households.\n\nThe invention of the paper machine and the application of steam power to the industrial processes of printing supported a massive expansion of newspaper and popular book publishing, which contributed to rising literacy and demands for mass political participation.\n\nConsumers benefited from falling prices for clothing and household articles such as cast iron cooking utensils, and in the following decades, stoves for cooking and space heating. Coffee, tea, sugar, tobacco and chocolate became affordable to many in Europe. Watches and household clocks became popular consumer items.\n\nMeeting the demands of the consumer revolution and growth in wealth of the middle classes in Britain, potter and entrepreneur Josiah Wedgwood, founder of Wedgwood fine china and porcelain, created goods such as tableware, which was starting to become a common feature on dining tables.\n\nThe Industrial Revolution was the first period in history during which there was a simultaneous increase in both population and per capita income.\n\nAccording to Robert Hughes in \"The Fatal Shore\", the population of England and Wales, which had remained steady at six million from 1700 to 1740, rose dramatically after 1740. The population of England had more than doubled from 8.3 million in 1801 to 16.8 million in 1850 and, by 1901, had nearly doubled again to 30.5 million. Improved conditions led to the population of Britain increasing from 10 million to 40 million in the 1800s. Europe's population increased from about 100 million in 1700 to 400 million by 1900.\n\nThe growth of modern industry since the late 18th century led to massive urbanisation and the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas. In 1800, only 3% of the world's population lived in cities, compared to nearly 50% today (the beginning of the 21st century). Manchester had a population of 10,000 in 1717, but by 1911 it had burgeoned to 2.3 million.\n\nWomen's historians have debated the effect of the Industrial Revolution and capitalism generally on the status of women. Taking a pessimistic side, Alice Clark argued that when capitalism arrived in 17th century England, it lowered the status of women as they lost much of their economic importance. Clark argues that in 16th-century England, women were engaged in many aspects of industry and agriculture. The home was a central unit of production and women played a vital role in running farms, and in some trades and landed estates. Their useful economic roles gave them a sort of equality with their husbands. However, Clark argues, as capitalism expanded in the 17th century, there was more and more division of labour with the husband taking paid labour jobs outside the home, and the wife reduced to unpaid household work. Middle- and upper-class women were confined to an idle domestic existence, supervising servants; lower-class women were forced to take poorly paid jobs. Capitalism, therefore, had a negative effect on powerful women.\n\nIn a more positive interpretation, Ivy Pinchbeck argues that capitalism created the conditions for women's emancipation. Tilly and Scott have emphasised the continuity in the status of women, finding three stages in English history. In the pre-industrial era, production was mostly for home use and women produce much of the needs of the households. The second stage was the \"family wage economy\" of early industrialisation; the entire family depended on the collective wages of its members, including husband, wife and older children. The third or modern stage is the \"family consumer economy,\" in which the family is the site of consumption, and women are employed in large numbers in retail and clerical jobs to support rising standards of consumption.\n\nIdeas of thrift and hard work characterized middle-class families as the Industrial Revolution swept Europe. These values were displayed in Samuel Smiles' book \"Self-Help\", in which he states that the misery of the poorer classes was \"voluntary and self-imposed - the results of idleness, thriftlessness, intemperance, and misconduct.\"\n\nIn terms of social structure, the Industrial Revolution witnessed the triumph of a middle class of industrialists and businessmen over a landed class of nobility and gentry. Ordinary working people found increased opportunities for employment in the new mills and factories, but these were often under strict working conditions with long hours of labour dominated by a pace set by machines. As late as the year 1900, most industrial workers in the United States still worked a 10-hour day (12 hours in the steel industry), yet earned from 20% to 40% less than the minimum deemed necessary for a decent life; however, most workers in textiles, which was by far the leading industry in terms of employment, were women and children. For workers of the laboring classes, industrial life \"was a stony desert, which they had to make habitable by their own efforts.\" Also, harsh working conditions were prevalent long before the Industrial Revolution took place. Pre-industrial society was very static and often cruel – child labour, dirty living conditions, and long working hours were just as prevalent before the Industrial Revolution.\n\nIndustrialisation led to the creation of the factory. The factory system contributed to the growth of urban areas, as large numbers of workers migrated into the cities in search of work in the factories. Nowhere was this better illustrated than the mills and associated industries of Manchester, nicknamed \"Cottonopolis\", and the world's first industrial city. Manchester experienced a six-times increase in its population between 1771 and 1831. Bradford grew by 50% every ten years between 1811 and 1851 and by 1851 only 50% of the population of Bradford was actually born there.\n\nIn addition, between 1815 and 1939, 20 percent of Europe's population left home, pushed by poverty, a rapidly growing population, and the displacement of peasant farming and artisan manufacturing. They were pulled abroad by the enormous demand for labor overseas, the ready availability of land, and cheap transportation. Still, many did not find a satisfactory life in their new homes, leading 7 million of them to return to Europe. This mass migration had large demographic impacts: in 1800, less than one percent of the world population consisted of overseas Europeans and their descendants; by 1930, they represented 11 percent. The Americas felt the brunt of this huge emigration, largely concentrated in the United States.\n\nFor much of the 19th century, production was done in small mills, which were typically water-powered and built to serve local needs. Later, each factory would have its own steam engine and a chimney to give an efficient draft through its boiler.\n\nIn other industries, the transition to factory production was not so divisive. Some industrialists themselves tried to improve factory and living conditions for their workers. One of the earliest such reformers was Robert Owen, known for his pioneering efforts in improving conditions for workers at the New Lanark mills, and often regarded as one of the key thinkers of the early socialist movement.\n\nBy 1746 an integrated brass mill was working at Warmley near Bristol. Raw material went in at one end, was smelted into brass and was turned into pans, pins, wire, and other goods. Housing was provided for workers on site. Josiah Wedgwood and Matthew Boulton (whose Soho Manufactory was completed in 1766) were other prominent early industrialists, who employed the factory system.\n\nThe Industrial Revolution led to a population increase but the chances of surviving childhood did not improve throughout the Industrial Revolution, although \"infant\" mortality rates were reduced markedly. There was still limited opportunity for education and children were expected to work. Employers could pay a child less than an adult even though their productivity was comparable; there was no need for strength to operate an industrial machine, and since the industrial system was completely new, there were no experienced adult labourers. This made child labour the labour of choice for manufacturing in the early phases of the Industrial Revolution between the 18th and 19th centuries. In England and Scotland in 1788, two-thirds of the workers in 143 water-powered cotton mills were described as children.\n\nChild labour existed before the Industrial Revolution but with the increase in population and education it became more visible. Many children were forced to work in relatively bad conditions for much lower pay than their elders, 10–20% of an adult male's wage. Children as young as four were employed. Beatings and long hours were common, with some child coal miners and hurriers working from 4 am until 5 pm. Conditions were dangerous, with some children killed when they dozed off and fell into the path of the carts, while others died from gas explosions. Many children developed lung cancer and other diseases and died before the age of 25. Workhouses would sell orphans and abandoned children as \"pauper apprentices\", working without wages for board and lodging. Those who ran away would be whipped and returned to their masters, with some masters shackling them to prevent escape. Children employed as mule scavengers by cotton mills would crawl under machinery to pick up cotton, working 14 hours a day, six days a week. Some lost hands or limbs, others were crushed under the machines, and some were decapitated. Young girls worked at match factories, where phosphorus fumes would cause many to develop phossy jaw. Children employed at glassworks were regularly burned and blinded, and those working at potteries were vulnerable to poisonous clay dust.\n\nReports were written detailing some of the abuses, particularly in the coal mines and textile factories, and these helped to popularise the children's plight. The public outcry, especially among the upper and middle classes, helped stir change in the young workers' welfare.\n\nPoliticians and the government tried to limit child labour by law but factory owners resisted; some felt that they were aiding the poor by giving their children money to buy food to avoid starvation, and others simply welcomed the cheap labour. In 1833 and 1844, the first general laws against child labour, the Factory Acts, were passed in Britain: Children younger than nine were not allowed to work, children were not permitted to work at night, and the work day of youth under the age of 18 was limited to twelve hours. Factory inspectors supervised the execution of the law, however, their scarcity made enforcement difficult. About ten years later, the employment of children and women in mining was forbidden. Although laws such as these decreased the number of child labourers, child labour remained significantly present in Europe and the United States until the 20th century.\n\nThe Industrial Revolution concentrated labour into mills, factories and mines, thus facilitating the organisation of \"combinations\" or trade unions to help advance the interests of working people. The power of a union could demand better terms by withdrawing all labour and causing a consequent cessation of production. Employers had to decide between giving in to the union demands at a cost to themselves or suffering the cost of the lost production. Skilled workers were hard to replace, and these were the first groups to successfully advance their conditions through this kind of bargaining.\n\nThe main method the unions used to effect change was strike action. Many strikes were painful events for both sides, the unions and the management. In Britain, the Combination Act 1799 forbade workers to form any kind of trade union until its repeal in 1824. Even after this, unions were still severely restricted. One British newspaper in 1834 described unions as \"the most dangerous institutions that were ever permitted to take root, under shelter of law, in any country...\"\n\nIn 1832, the Reform Act extended the vote in Britain but did not grant universal suffrage. That year six men from Tolpuddle in Dorset founded the Friendly Society of Agricultural Labourers to protest against the gradual lowering of wages in the 1830s. They refused to work for less than ten shillings a week, although by this time wages had been reduced to seven shillings a week and were due to be further reduced to six. In 1834 James Frampton, a local landowner, wrote to the Prime Minister, Lord Melbourne, to complain about the union, invoking an obscure law from 1797 prohibiting people from swearing oaths to each other, which the members of the Friendly Society had done. James Brine, James Hammett, George Loveless, George's brother James Loveless, George's brother in-law Thomas Standfield, and Thomas's son John Standfield were arrested, found guilty, and transported to Australia. They became known as the Tolpuddle Martyrs. In the 1830s and 1840s, the Chartist movement was the first large-scale organised working class political movement which campaigned for political equality and social justice. Its \"Charter\" of reforms received over three million signatures but was rejected by Parliament without consideration.\n\nWorking people also formed friendly societies and co-operative societies as mutual support groups against times of economic hardship. Enlightened industrialists, such as Robert Owen also supported these organisations to improve the conditions of the working class.\n\nUnions slowly overcame the legal restrictions on the right to strike. In 1842, a general strike involving cotton workers and colliers was organised through the Chartist movement which stopped production across Great Britain.\n\nEventually, effective political organisation for working people was achieved through the trades unions who, after the extensions of the franchise in 1867 and 1885, began to support socialist political parties that later merged to become the British Labour Party.\n\nThe rapid industrialisation of the English economy cost many craft workers their jobs. The movement started first with lace and hosiery workers near Nottingham and spread to other areas of the textile industry owing to early industrialisation. Many weavers also found themselves suddenly unemployed since they could no longer compete with machines which only required relatively limited (and unskilled) labour to produce more cloth than a single weaver. Many such unemployed workers, weavers, and others, turned their animosity towards the machines that had taken their jobs and began destroying factories and machinery. These attackers became known as Luddites, supposedly followers of Ned Ludd, a folklore figure. The first attacks of the Luddite movement began in 1811. The Luddites rapidly gained popularity, and the British government took drastic measures, using the militia or army to protect industry. Those rioters who were caught were tried and hanged, or transported for life.\n\nUnrest continued in other sectors as they industrialised, such as with agricultural labourers in the 1830s when large parts of southern Britain were affected by the Captain Swing disturbances. Threshing machines were a particular target, and hayrick burning was a popular activity. However, the riots led to the first formation of trade unions, and further pressure for reform.\n\nThe traditional centers of hand textile production such as India, parts of the Middle East and later China could not withstand the competition from machine-made textiles, which over a period of decades destroyed the hand made textile industries and left millions of people without work, many of whom starved.\n\nThe Industrial Revolution also generated an enormous and unprecedented economic division in the world, as measured by the share of manufacturing output.\nCheap cotton textiles increased the demand for raw cotton; previously, it had primarily been consumed in subtropical regions where it was grown, with little raw cotton available for export. Consequently, prices of raw cotton rose. Some cotton had been grown in the West Indies, particularly in Hispaniola, but Haitian cotton production was halted by the Haitian Revolution in 1791. The invention of the cotton gin in 1792 allowed Georgia green seeded cotton to be profitable, leading to the widespread growth of cotton plantations in the United States and Brazil. In 1791 world cotton production was estimated to be 490,000,000 pounds with U.S. production accounting to 2,000,000 pounds. By 1800 U.S. production was 35,000,000 pounds, of which 17,790,000 were exported. In 1945 the U.S. produced seven-eights of the 1,169,600,000 pounds of world production.\n\nThe Americas, particularly the U.S., had labor shortages and high priced labor, which made slavery attractive. America's cotton plantations were highly efficient and profitable, and able to keep up with demand. The U.S. Civil war created a \"cotton famine\" that lead to increased production in other areas of the world, including new colonies in Africa.\n\nThe origins of the environmental movement lay in the response to increasing levels of smoke pollution in the atmosphere during the Industrial Revolution. The emergence of great factories and the concomitant immense growth in coal consumption gave rise to an unprecedented level of air pollution in industrial centers; after 1900 the large volume of industrial chemical discharges added to the growing load of untreated human waste. The first large-scale, modern environmental laws came in the form of Britain's Alkali Acts, passed in 1863, to regulate the deleterious air pollution (gaseous hydrochloric acid) given off by the Leblanc process, used to produce soda ash. An Alkali inspector and four sub-inspectors were appointed to curb this pollution. The responsibilities of the inspectorate were gradually expanded, culminating in the Alkali Order 1958 which placed all major heavy industries that emitted smoke, grit, dust and fumes under supervision.\n\nThe manufactured gas industry began in British cities in 1812–1820. The technique used produced highly toxic effluent that was dumped into sewers and rivers. The gas companies were repeatedly sued in nuisance lawsuits. They usually lost and modified the worst practices. The City of London repeatedly indicted gas companies in the 1820s for polluting the Thames and poisoning its fish. Finally, Parliament wrote company charters to regulate toxicity. The industry reached the US around 1850 causing pollution and lawsuits.\n\nIn industrial cities local experts and reformers, especially after 1890, took the lead in identifying environmental degradation and pollution, and initiating grass-roots movements to demand and achieve reforms. Typically the highest priority went to water and air pollution. The Coal Smoke Abatement Society was formed in Britain in 1898 making it one of the oldest environmental NGOs. It was founded by artist Sir William Blake Richmond, frustrated with the pall cast by coal smoke. Although there were earlier pieces of legislation, the Public Health Act 1875 required all furnaces and fireplaces to consume their own smoke. It also provided for sanctions against factories that emitted large amounts of black smoke. The provisions of this law were extended in 1926 with the Smoke Abatement Act to include other emissions, such as soot, ash, and gritty particles and to empower local authorities to impose their own regulations.\n\nThe Industrial Revolution on Continental Europe came a little later than in Great Britain. In many industries, this involved the application of technology developed in Britain in new places. Often the technology was purchased from Britain or British engineers and entrepreneurs moved abroad in search of new opportunities. By 1809, part of the Ruhr Valley in Westphalia was called 'Miniature England' because of its similarities to the industrial areas of England. The German, Russian and Belgian governments all provided state funding to the new industries. In some cases (such as iron), the different availability of resources locally meant that only some aspects of the British technology were adopted.\n\nBelgium was the second country, after Britain, in which the Industrial Revolution took place and the first in continental Europe: Wallonia (French speaking southern Belgium) was the first region to follow the British model successfully. Starting in the middle of the 1820s, and especially after Belgium became an independent nation in 1830, numerous works comprising coke blast furnaces as well as puddling and rolling mills were built in the coal mining areas around Liège and Charleroi. The leader was a transplanted Englishman John Cockerill. His factories at Seraing integrated all stages of production, from engineering to the supply of raw materials, as early as 1825.\n\nWallonia exemplified the radical evolution of industrial expansion. Thanks to coal (the French word \"houille\" was coined in Wallonia), the region geared up to become the 2nd industrial power in the world after Britain. But it is also pointed out by many researchers, with its \"Sillon industriel\", 'Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, [...] there was a huge industrial development based on coal-mining and iron-making...'. Philippe Raxhon wrote about the period after 1830: \"It was not propaganda but a reality the Walloon regions were becoming the second industrial power all over the world after Britain.\" \"The sole industrial centre outside the collieries and blast furnaces of Walloon was the old cloth making town of Ghent.\" Michel De Coster, Professor at the Université de Liège wrote also: \"The historians and the economists say that Belgium was the second industrial power of the world, in proportion to its population and its territory [...] But this rank is the one of Wallonia where the coal-mines, the blast furnaces, the iron and zinc factories, the wool industry, the glass industry, the weapons industry... were concentrated.\" \n\nWallonia was also the birthplace of a strong Socialist party and strong trade-unions in a particular sociological landscape. At the left, the \"Sillon industriel\", which runs from Mons in the west, to Verviers in the east (except part of North Flanders, in another period of the industrial revolution, after 1920). Even if Belgium is the second industrial country after Britain, the effect of the industrial revolution there was very different. In 'Breaking stereotypes', Muriel Neven and Isabelle Devious say:\n\nThe industrial revolution changed a mainly rural society into an urban one, but with a strong contrast between northern and southern Belgium. During the Middle Ages and the Early Modern Period, Flanders was characterised by the presence of large urban centres [...] at the beginning of the nineteenth century this region (Flanders), with an urbanisation degree of more than 30 per cent, remained one of the most urbanised in the world. By comparison, this proportion reached only 17 per cent in Wallonia, barely 10 per cent in most West European countries, 16 per cent in France and 25 per cent in Britain. Nineteenth century industrialisation did not affect the traditional urban infrastructure, except in Ghent [...] Also, in Wallonia the traditional urban network was largely unaffected by the industrialisation process, even though the proportion of city-dwellers rose from 17 to 45 per cent between 1831 and 1910. Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, where there was a huge industrial development based on coal-mining and iron-making, urbanisation was fast. During these eighty years the number of municipalities with more than 5,000 inhabitants increased from only 21 to more than one hundred, concentrating nearly half of the Walloon population in this region. Nevertheless, industrialisation remained quite traditional in the sense that it did not lead to the growth of modern and large urban centres, but to a conurbation of industrial villages and towns developed around a coal-mine or a factory. Communication routes between these small centres only became populated later and created a much less dense urban morphology than, for instance, the area around Liège where the old town was there to direct migratory flows.\n\nThe industrial revolution in France followed a particular course as it did not correspond to the main model followed by other countries. Notably, most French historians argue France did not go through a clear \"take-off\". Instead, France's economic growth and industrialisation process was slow and steady through the 18th and 19th centuries. However, some stages were identified by Maurice Lévy-Leboyer:\n\nBased on its leadership in chemical research in the universities and industrial laboratories, Germany, which was unified in 1871, became dominant in the world's chemical industry in the late 19th century. At first the production of dyes based on aniline was critical.\n\nGermany's political disunity – with three dozen states – and a pervasive conservatism made it difficult to build railways in the 1830s. However, by the 1840s, trunk lines linked the major cities; each German state was responsible for the lines within its own borders. Lacking a technological base at first, the Germans imported their engineering and hardware from Britain, but quickly learned the skills needed to operate and expand the railways. In many cities, the new railway shops were the centres of technological awareness and training, so that by 1850, Germany was self-sufficient in meeting the demands of railroad construction, and the railways were a major impetus for the growth of the new steel industry. Observers found that even as late as 1890, their engineering was inferior to Britain's. However, German unification in 1870 stimulated consolidation, nationalisation into state-owned companies, and further rapid growth. Unlike the situation in France, the goal was support of industrialisation, and so heavy lines crisscrossed the Ruhr and other industrial districts, and provided good connections to the major ports of Hamburg and Bremen. By 1880, Germany had 9,400 locomotives pulling 43,000 passengers and 30,000 tons of freight, and pulled ahead of France\n\nDuring the period 1790–1815 Sweden experienced two parallel economic movements: an \"agricultural revolution\" with larger agricultural estates, new crops and farming tools and a commercialisation of farming, and a \"protoindustrialisation\", with small industries being established in the countryside and with workers switching between agricultural work in summer and industrial production in winter. This led to economic growth benefiting large sections of the population and leading up to a \"consumption revolution\" starting in the 1820s.\n\nDuring 1815–1850 the protoindustries developed into more specialised and larger industries. This period witnessed increasing regional specialisation with mining in Bergslagen, textile mills in Sjuhäradsbygden and forestry in Norrland. Several important institutional changes took place in this period, such as free and mandatory schooling introduced 1842 (as first country in the world), the abolition of the national monopoly on trade in handicrafts in 1846, and a stock company law in 1848.\n\nDuring 1850–1890, Sweden experienced a veritable explosion in export, dominated by crops, wood and steel. Sweden abolished most tariffs and other barriers to free trade in the 1850s and joined the gold standard in 1873.\n\nDuring 1890–1930, Sweden experienced the second industrial revolution. New industries developed with their focus on the domestic market: mechanical engineering, power utilities, papermaking and textile.\n\nThe industrial revolution began about 1870 as Meiji period leaders decided to catch up with the West. The government built railroads, improved roads, and inaugurated a land reform programme to prepare the country for further development. It inaugurated a new Western-based education system for all young people, sent thousands of students to the United States and Europe, and hired more than 3,000 Westerners to teach modern science, mathematics, technology, and foreign languages in Japan (Foreign government advisors in Meiji Japan).\n\nIn 1871, a group of Japanese politicians known as the Iwakura Mission toured Europe and the United States to learn western ways. The result was a deliberate state-led industrialisation policy to enable Japan to quickly catch up. The Bank of Japan, founded in 1882, used taxes to fund model steel and textile factories. Education was expanded and Japanese students were sent to study in the west.\n\nModern industry first appeared in textiles, including cotton and especially silk, which was based in home workshops in rural areas.\n\nDuring the late 18th an early 19th centuries when the UK and parts of Western Europe began to industrialise, the US was primarily an agricultural and natural resource producing and processing economy. The building of roads and canals, the introduction of steamboats and the building of railroads were important for handling agricultural and natural resource products in the large and sparsely populated country of the period.\n\nImportant American technological contributions during the period of the Industrial Revolution were the cotton gin and the development of a system for making interchangeable parts, the latter aided by the development of the milling machine in the US. The development of machine tools and the system of interchangeable parts were the basis for the rise of the US as the world's leading industrial nation in the late 19th century.\n\nOliver Evans invented an automated flour mill in the mid-1780s that used control mechanisms and conveyors so that no labour was needed from the time grain was loaded into the elevator buckets until flour was discharged into a wagon. This is considered to be the first modern materials handling system an important advance in the progress toward mass production.\n\nThe United States originally used horse-powered machinery for small scale applications such as grain milling, but eventually switched to water power after textile factories began being built in the 1790s. As a result, industrialisation was concentrated in New England and the Northeastern United States, which has fast-moving rivers. The newer water-powered production lines proved more economical than horse-drawn production. In the late 19th century steam-powered manufacturing overtook water-powered manufacturing, allowing the industry to spread to the Midwest.\n\nThomas Somers and the Cabot Brothers founded the Beverly Cotton Manufactory in 1787, the first cotton mill in America, the largest cotton mill of its era, and a significant milestone in the research and development of cotton mills in the future. This mill was designed to use horse power, but the operators quickly learned that the horse-drawn platform was economically unstable, and had economic losses for years. Despite the losses, the Manufactory served as a playground of innovation, both in turning a large amount of cotton, but also developing the water-powered milling structure used in Slater's Mill.\n\nIn 1793, Samuel Slater (1768–1835) founded the Slater Mill at Pawtucket, Rhode Island. He had learned of the new textile technologies as a boy apprentice in Derbyshire, England, and defied laws against the emigration of skilled workers by leaving for New York in 1789, hoping to make money with his knowledge. After founding Slater's Mill, he went on to own 13 textile mills. Daniel Day established a wool carding mill in the Blackstone Valley at Uxbridge, Massachusetts in 1809, the third woollen mill established in the US (The first was in Hartford, Connecticut, and the second at Watertown, Massachusetts.) The John H. Chafee Blackstone River Valley National Heritage Corridor retraces the history of \"America's Hardest-Working River', the Blackstone. The Blackstone River and its tributaries, which cover more than from Worcester, Massachusetts to Providence, Rhode Island, was the birthplace of America's Industrial Revolution. At its peak over 1100 mills operated in this valley, including Slater's mill, and with it the earliest beginnings of America's Industrial and Technological Development.\n\nMerchant Francis Cabot Lowell from Newburyport, Massachusetts memorised the design of textile machines on his tour of British factories in 1810. Realising that the War of 1812 had ruined his import business but that a demand for domestic finished cloth was emerging in America, on his return to the United States, he set up the Boston Manufacturing Company. Lowell and his partners built America's second cotton-to-cloth textile mill at Waltham, Massachusetts, second to the Beverly Cotton Manufactory. After his death in 1817, his associates built America's first planned factory town, which they named after him. This enterprise was capitalised in a public stock offering, one of the first uses of it in the United States. Lowell, Massachusetts, using of canals and 10,000 horsepower delivered by the Merrimack River, is considered by some as a major contributor to the success of the American Industrial Revolution. The short-lived utopia-like Waltham-Lowell system was formed, as a direct response to the poor working conditions in Britain. However, by 1850, especially following the Irish Potato Famine, the system had been replaced by poor immigrant labour.\n\nA major U.S. contribution to industrialization was the development of techniques to make interchangeable parts from metal. Precision metal machining techniques were developed by the U.S. Department of War to make interchangeable parts for small firearms. The development work took place at the Federal Arsenals at Springfield Armory and Harpers Ferry Armory. Techniques for precision machining using machine tools included using fixtures to hold the parts in proper position, jigs to guide the cutting tools and precision blocks and gauges to measure the accuracy. The milling machine, a fundamental machine tool, is believed to have been invented by Eli Whitney, who was a government contractor who built firearms as part of this program. Another important invention was the Blanchard lathe, invented by Thomas Blanchard. The Blanchard lathe, or pattern tracing lathe, was actually a shaper that could produce copies of wooden gun stocks. The use of machinery and the techniques for producing standardized and interchangeable parts became known as the American system of manufacturing.\n\nPrecision manufacturing techniques made it possible to build machines that mechanized the shoe industry. and the watch industry. The industrialisation of the watch industry started 1854 also in Waltham, Massachusetts, at the Waltham Watch Company, with the development of machine tools, gauges and assembling methods adapted to the micro precision required for watches.\n\n \nSteel is often cited as the first of several new areas for industrial mass-production, which are said to characterise a \"Second Industrial Revolution\", beginning around 1850, although a method for mass manufacture of steel was not invented until the 1860s, when Sir Henry Bessemer invented a new furnace which could convert molten pig iron into steel in large quantities. However, it only became widely available in the 1870s after the process was modified to produce more uniform quality. Bessemer steel was being displaced by the open hearth furnace near the end of the 19th century.\n\nThis Second Industrial Revolution gradually grew to include chemicals, mainly the chemical industries, petroleum (refining and distribution), and, in the 20th century, the automotive industry, and was marked by a transition of technological leadership from Britain to the United States and Germany.\n\nThe increasing availability of economical petroleum products also reduced the importance of coal and further widened the potential for industrialisation.\n\nA new revolution began with electricity and electrification in the electrical industries. The introduction of hydroelectric power generation in the Alps enabled the rapid industrialisation of coal-deprived northern Italy, beginning in the 1890s.\n\nBy the 1890s, industrialisation in these areas had created the first giant industrial corporations with burgeoning global interests, as companies like U.S. Steel, General Electric, Standard Oil and Bayer AG joined the railroad and ship companies on the world's stock markets.\n\nThe causes of the Industrial Revolution were complicated and remain a topic for debate, with some historians believing the Industrial Revolution was an outgrowth of social and institutional changes brought by the end of feudalism in Britain after the English Civil War in the 17th century. The Enclosure movement and the British Agricultural Revolution made food production more efficient and less labour-intensive, forcing the farmers who could no longer be self-sufficient in agriculture into cottage industry, for example weaving, and in the longer term into the cities and the newly developed factories. The colonial expansion of the 17th century with the accompanying development of international trade, creation of financial markets and accumulation of capital are also cited as factors, as is the scientific revolution of the 17th century. A change in marrying patterns to getting married later made people able to accumulate more human capital during their youth, thereby encouraging economic development.\n\nUntil the 1980s, it was universally believed by academic historians that technological innovation was the heart of the Industrial Revolution and the key enabling technology was the invention and improvement of the steam engine. However, recent research into the Marketing Era has challenged the traditional, supply-oriented interpretation of the Industrial Revolution.\n\nLewis Mumford has proposed that the Industrial Revolution had its origins in the Early Middle Ages, much earlier than most estimates. He explains that the model for standardised mass production was the printing press and that \"the archetypal model for the industrial era was the clock\". He also cites the monastic emphasis on order and time-keeping, as well as the fact that medieval cities had at their centre a church with bell ringing at regular intervals as being necessary precursors to a greater synchronisation necessary for later, more physical, manifestations such as the steam engine.\n\nThe presence of a large domestic market should also be considered an important driver of the Industrial Revolution, particularly explaining why it occurred in Britain. In other nations, such as France, markets were split up by local regions, which often imposed tolls and tariffs on goods traded among them. Internal tariffs were abolished by Henry VIII of England, they survived in Russia until 1753, 1789 in France and 1839 in Spain.\n\nGovernments' grant of limited monopolies to inventors under a developing patent system (the Statute of Monopolies in 1623) is considered an influential factor. The effects of patents, both good and ill, on the development of industrialisation are clearly illustrated in the history of the steam engine, the key enabling technology. In return for publicly revealing the workings of an invention the patent system rewarded inventors such as James Watt by allowing them to monopolise the production of the first steam engines, thereby rewarding inventors and increasing the pace of technological development. However, monopolies bring with them their own inefficiencies which may counterbalance, or even overbalance, the beneficial effects of publicising ingenuity and rewarding inventors. Watt's monopoly prevented other inventors, such as Richard Trevithick, William Murdoch, or Jonathan Hornblower, whom Boulton and Watt sued, from introducing improved steam engines, thereby retarding the spread of steam power.\n\nOne question of active interest to historians is why the Industrial Revolution occurred in Europe and not in other parts of the world in the 18th century, particularly China, India, and the Middle East (which pioneered in shipbuilding, textile production, water mills, and much more in the period between 750 and 1100), or at other times like in Classical Antiquity or the Middle Ages. A recent account argued that Europeans have been characterized for thousands of years by a freedom-loving culture originating from the aristocratic societies of early Indo-European invaders. Many historians, however, have challenged this explanation as being not only Eurocentric, but also ignoring historical context. In fact, before the Industrial Revolution, \"there existed something of a global economic parity between the most advanced regions in the world economy.\" These historians have suggested a number of other factors, including education, technological changes (see Scientific Revolution in Europe), \"modern\" government, \"modern\" work attitudes, ecology, and culture.\n\nChina was the world's most technologically advanced country for many centuries; however, China stagnated economically and technologically and was surpassed by Western Europe before the Age of Exploration, by which time China banned imports and denied entry to foreigners. China was also a totalitarian society. Modern estimates of per capita income in Western Europe in the late 18th century are of roughly 1,500 dollars in purchasing power parity (and Britain had a per capita income of nearly 2,000 dollars) whereas China, by comparison, had only 450 dollars. India was essentially feudal, politically fragmented and not as economically advanced as Western Europe.\n\nHistorians such as David Landes and Max Weber credit the different belief systems in Asia and Europe with dictating where the revolution occurred. The religion and beliefs of Europe were largely products of Judaeo-Christianity and Greek thought. Conversely, Chinese society was founded on men like Confucius, Mencius, Han Feizi (Legalism), Lao Tzu (Taoism), and Buddha (Buddhism), resulting in very different worldviews. Other factors include the considerable distance of China's coal deposits, though large, from its cities as well as the then unnavigable Yellow River that connects these deposits to the sea.\n\nRegarding India, the Marxist historian Rajani Palme Dutt said: \"The capital to finance the Industrial Revolution in India instead went into financing the Industrial Revolution in Britain.\" In contrast to China, India was split up into many competing kingdoms after the decline of the Mughal Empire, with the major ones in its aftermath including the Marathas, Sikhs, Bengal Subah, and Kingdom of Mysore. In addition, the economy was highly dependent on two sectors – agriculture of subsistence and cotton, and there appears to have been little technical innovation. It is believed that the vast amounts of wealth were largely stored away in palace treasuries by totalitarian monarchs prior to the British take over.\n\nEconomic historian Joel Mokyr has argued that political fragmentation (the presence of a large number of European states) made it possible for heterodox ideas to thrive, as entrepreneurs, innovators, ideologues and heretics could easily flee to a neighboring state in the event that the one state would try to suppress their ideas and activities. This is what set Europe apart from the technologically advanced, large unitary empires such as China and India by providing \"an insurance against economic and technological stagnation\". China had both a printing press and movable type, and India had similar levels scientific and technological achievement as Europe in 1700, yet the Industrial Revolution would occur in Europe, not China or India. In Europe, political fragmentation was coupled with an \"integrated market for ideas\" where Europe's intellectuals used the lingua franca of Latin, had a shared intellectual basis in Europe's classical heritage and the pan-European institution of the Republic of Letters.\n\nIn addition, Europe's monarchs desperately needed revenue, pushing them into alliances with their merchant classes. Small groups of merchants were granted monopolies and tax-collecting responsibilities in exchange for payments to the state. Located in a region \"at the hub of the largest and most varied network of exchange in history,\" Europe advanced as the leader of the Industrial Revolution. In the Americas, Europeans found a windfall of silver, timber, fish, and maize, leading historian Peter Stearns to conclude that \"Europe's Industrial Revolution stemmed in great part from Europe's ability to draw disproportionately on world resources.\"\n\nGreat Britain provided the legal and cultural foundations that enabled entrepreneurs to pioneer the Industrial Revolution. Key factors fostering this environment were: (1) The period of peace and stability which followed the unification of England and Scotland; (2) no trade barriers between England and Scotland; (3) the rule of law (enforcing property rights and respecting the sanctity of contracts); (4) a straightforward legal system that allowed the formation of joint-stock companies (corporations); (5) absence of tolls, which had largely disappeared from Britain by the 15th century, but were an extreme burden on goods elsewhere in the world, and (6) a free market (capitalism).\n\nGeographical and natural resource advantages of Great Britain were the fact that it had extensive coastlines and many navigable rivers in an age where water was the easiest means of transportation and having the highest quality coal in Europe.\n\nThere were two main values that really drove the Industrial Revolution in Britain. These values were self-interest and an entrepreneurial spirit. Because of these interests, many industrial advances were made that resulted in a huge increase in personal wealth and a consumer revolution. These advancements also greatly benefitted the British society as a whole. Countries around the world started to recognise the changes and advancements in Britain and use them as an example to begin their own Industrial Revolutions.\n\nThe debate about the start of the Industrial Revolution also concerns the massive lead that Great Britain had over other countries. Some have stressed the importance of natural or financial resources that Britain received from its many overseas colonies or that profits from the British slave trade between Africa and the Caribbean helped fuel industrial investment. However, it has been pointed out that slave trade and West Indian plantations provided only 5% of the British national income during the years of the Industrial Revolution. Even though slavery accounted for so little, Caribbean-based demand accounted for 12% of Britain's industrial output.\nInstead, the greater liberalisation of trade from a large merchant base may have allowed Britain to produce and use emerging scientific and technological developments more effectively than countries with stronger monarchies, particularly China and Russia. Britain emerged from the Napoleonic Wars as the only European nation not ravaged by financial plunder and economic collapse, and having the only merchant fleet of any useful size (European merchant fleets were destroyed during the war by the Royal Navy). Britain's extensive exporting cottage industries also ensured markets were already available for many early forms of manufactured goods. The conflict resulted in most British warfare being conducted overseas, reducing the devastating effects of territorial conquest that affected much of Europe. This was further aided by Britain's geographical position – an island separated from the rest of mainland Europe.\nAnother theory is that Britain was able to succeed in the Industrial Revolution due to the availability of key resources it possessed. It had a dense population for its small geographical size. Enclosure of common land and the related agricultural revolution made a supply of this labour readily available. There was also a local coincidence of natural resources in the North of England, the English Midlands, South Wales and the Scottish Lowlands. Local supplies of coal, iron, lead, copper, tin, limestone and water power, resulted in excellent conditions for the development and expansion of industry. Also, the damp, mild weather conditions of the North West of England provided ideal conditions for the spinning of cotton, providing a natural starting point for the birth of the textiles industry.\n\nThe stable political situation in Britain from around 1688 following the Glorious Revolution, and British society's greater receptiveness to change (compared with other European countries) can also be said to be factors favouring the Industrial Revolution. Peasant resistance to industrialisation was largely eliminated by the Enclosure movement, and the landed upper classes developed commercial interests that made them pioneers in removing obstacles to the growth of capitalism. (This point is also made in Hilaire Belloc's \"The Servile State\".)\n\nThe French philosopher Voltaire wrote about capitalism and religious tolerance in his book on English society, \"Letters on the English\" (1733), noting why England at that time was more prosperous in comparison to the country's less religiously tolerant European neighbours. \"Take a view of the Royal Exchange in London, a place more venerable than many courts of justice, where the representatives of all nations meet for the benefit of mankind. There the Jew, the Mahometan [Muslim], and the Christian transact together, as though they all professed the same religion, and give the name of infidel to none but bankrupts. There the Presbyterian confides in the Anabaptist, and the Churchman depends on the Quaker's word. If one religion only were allowed in England, the Government would very possibly become arbitrary; if there were but two, the people would cut one another's throats; but as there are such a multitude, they all live happy and in peace.\"\n\nBritain's population grew 280% 1550–1820, while the rest of Western Europe grew 50–80%. Seventy percent of European urbanisation happened in Britain 1750–1800. By 1800, only the Netherlands was more urbanised than Britain. This was only possible because coal, coke, imported cotton, brick and slate had replaced wood, charcoal, flax, peat and thatch. The latter compete with land grown to feed people while mined materials do not. Yet more land would be freed when chemical fertilisers replaced manure and horse's work was mechanised. A workhorse needs for fodder while even early steam engines produced four times more mechanical energy.\n\nIn 1700, 5/6 of coal mined worldwide was in Britain, while the Netherlands had none; so despite having Europe's best transport, most urbanised, well paid, literate people and lowest taxes, it failed to industrialise. In the 18th century, it was the only European country whose cities and population shrank. Without coal, Britain would have run out of suitable river sites for mills by the 1830s.\n\nEconomic historian Robert Allen has argued that high wages, cheap capital and very cheap energy in Britain made it the ideal place for the industrial revolution to occur. These factors made it vastly more profitable to invest in research and development, and to put technology to use in Britain than other societies. However, two 2018 studies in \"The Economic History Review\" showed that wages were not particularly high in the British spinning sector or the construction sector, casting doubt on Allen's explanation.\n\nKnowledge of innovation was spread by several means. Workers who were trained in the technique might move to another employer or might be poached. A common method was for someone to make a study tour, gathering information where he could. During the whole of the Industrial Revolution and for the century before, all European countries and America engaged in study-touring; some nations, like Sweden and France, even trained civil servants or technicians to undertake it as a matter of state policy. In other countries, notably Britain and America, this practice was carried out by individual manufacturers eager to improve their own methods. Study tours were common then, as now, as was the keeping of travel diaries. Records made by industrialists and technicians of the period are an incomparable source of information about their methods.\n\nAnother means for the spread of innovation was by the network of informal philosophical societies, like the Lunar Society of Birmingham, in which members met to discuss 'natural philosophy' (\"i.e.\" science) and often its application to manufacturing. The Lunar Society flourished from 1765 to 1809, and it has been said of them, \"They were, if you like, the revolutionary committee of that most far reaching of all the eighteenth century revolutions, the Industrial Revolution\". Other such societies published volumes of proceedings and transactions. For example, the London-based Royal Society of Arts published an illustrated volume of new inventions, as well as papers about them in its annual \"Transactions\".\n\nThere were publications describing technology. Encyclopaedias such as Harris's \"Lexicon Technicum\" (1704) and Abraham Rees's \"Cyclopaedia\" (1802–1819) contain much of value. \"Cyclopaedia\" contains an enormous amount of information about the science and technology of the first half of the Industrial Revolution, very well illustrated by fine engravings. Foreign printed sources such as the \"Descriptions des Arts et Métiers\" and Diderot's \"Encyclopédie\" explained foreign methods with fine engraved plates.\n\nPeriodical publications about manufacturing and technology began to appear in the last decade of the 18th century, and many regularly included notice of the latest patents. Foreign periodicals, such as the \"Annales des Mines\", published accounts of travels made by French engineers who observed British methods on study tours.\n\nAnother theory is that the British advance was due to the presence of an entrepreneurial class which believed in progress, technology and hard work. The existence of this class is often linked to the Protestant work ethic (see Max Weber) and the particular status of the Baptists and the dissenting Protestant sects, such as the Quakers and Presbyterians that had flourished with the English Civil War. Reinforcement of confidence in the rule of law, which followed establishment of the prototype of constitutional monarchy in Britain in the Glorious Revolution of 1688, and the emergence of a stable financial market there based on the management of the national debt by the Bank of England, contributed to the capacity for, and interest in, private financial investment in industrial ventures.\n\nDissenters found themselves barred or discouraged from almost all public offices, as well as education at England's only two universities at the time (although dissenters were still free to study at Scotland's four universities). When the restoration of the monarchy took place and membership in the official Anglican Church became mandatory due to the Test Act, they thereupon became active in banking, manufacturing and education. The Unitarians, in particular, were very involved in education, by running Dissenting Academies, where, in contrast to the universities of Oxford and Cambridge and schools such as Eton and Harrow, much attention was given to mathematics and the sciences – areas of scholarship vital to the development of manufacturing technologies.\n\nHistorians sometimes consider this social factor to be extremely important, along with the nature of the national economies involved. While members of these sects were excluded from certain circles of the government, they were considered fellow Protestants, to a limited extent, by many in the middle class, such as traditional financiers or other businessmen. Given this relative tolerance and the supply of capital, the natural outlet for the more enterprising members of these sects would be to seek new opportunities in the technologies created in the wake of the scientific revolution of the 17th century.\n\nDuring the Industrial Revolution an intellectual and artistic hostility towards the new industrialisation developed, associated with the Romantic movement. Romanticism revered the traditionalism of rural life and recoiled against the upheavals caused by industrialization, urbanization and the wretchedness of the working classes. Its major exponents in English included the artist and poet William Blake and poets William Wordsworth, Samuel Taylor Coleridge, John Keats, Lord Byron and Percy Bysshe Shelley. The movement stressed the importance of \"nature\" in art and language, in contrast to \"monstrous\" machines and factories; the \"Dark satanic mills\" of Blake's poem \"And did those feet in ancient time\". Mary Shelley's novel \"Frankenstein\" reflected concerns that scientific progress might be two-edged. French Romanticism likewise was highly critical of industry.\n\n\n\n"}
{"id": "42570315", "url": "https://en.wikipedia.org/wiki?curid=42570315", "title": "International Conference of Young Scientists", "text": "International Conference of Young Scientists\n\nInternational Conference of Young Scientist or ICYS is the unofficial world championship of research papers in several scientific disciplines for secondary school students. It has gained notable participation across three continents (Europe, Asia, and America). It is organised by a host institution and vary for each year. ICYS is a member of World Federation of Physics Competitions.\n\nIn 1993 the representatives of Eotvos Lorand University, Budapest and the State University of Belarus, Minsk decided to organize together a conference for 14- to 18-year-old secondary school students. The aim of the organizers with organizing such a new type of a competition was to acquaint secondary school students with the methods of scientific research. This includes different phases of research work from the very beginning the pointing out the topic to the last step, summing up the results of the research in a foreign language-lecture. The conference gives the challenging opportunity to the young scientists to get some feedback of the work with which they are just trying to deal, and to measure their strength in an international field.\nDuring the years students from the following countries have participated in the Conference: Belarus, Czech Republic, Finland, Georgia, Germany, Greece, Hungary, India, Macedonia, Poland, Romania, Russia, Singapore, Slovakia, The Netherlands, Ukraine, United States of America, Yugoslavia.\nThe conference in Hungary (Visegrad, 1994, 1996, 1998), in Belarus (Baranovichi, 1995, 1997, 1999), in 2000 in The Netherlands, in Nijmegen, in 2001 in Poland, in Katowice, in 2002 in Georgia, in Kutaisi was organized respectively, in 2003 will be held in Czech Republic, in Prague.\nThe first ICYS was in Visegrad in 1994. Then 70 lectures were presented by students from 5 countries. Hungarian organizers always come back to Visegrad, the competition become widely known as \"Visegrad Conference\".\nIn 1996 the number of participants increased, 86 lecturers from 9 countries gave their lectures. The Conference every year becomes richer, new countries join to the competition. The Conference has a good reputation not only in the Middle European countries, but outside Europe too. This year we have an observer from Singapore, a country where the education receives from the government the highest financial support all over the world. This year we have more than 80 secondary school participants, and more than 70 lectures with wide variety of topics. We are convinced, that nowadays, during the permanently decreasing popularity of sciences a scientific event which attracts the young generation has special significance. Perhaps most of the participating students will later become students at various universities and will be researchers reporting at scientific conferences. This first appearance may be a decisive factor in their future scientific career.\nIt celebrated its 20th year in Bali, Indonesia. The 2014 edition were supposed to be held in Ukraine but due to the Ukraine crisis, it were transferred to Serbia.\n\n\n\n\"Only Belarus, Hungary and Ukraine have made it to all Conferences. Netherlands and Russia, each missed a Conference.\"\n\n\n"}
{"id": "3728135", "url": "https://en.wikipedia.org/wiki?curid=3728135", "title": "James Ayscough", "text": "James Ayscough\n\nJames Ayscough (died 1759) was an English optician and designer and maker of scientific instruments. He was apprenticed to an optician named James Mann from 1743 to 1747. James Ayscough became known for his microscopes. His shop was in London between 1740 and 1759. Around the year 1752, James Ayscough introduced spectacles with double-hinged side pieces. Although he made clear lenses, he recommended lenses tinted blue or green to treat some vision problems. These spectacles with tinted lenses are believed to be the precursors to sunglasses.\n"}
{"id": "4229230", "url": "https://en.wikipedia.org/wiki?curid=4229230", "title": "John A. Powers", "text": "John A. Powers\n\nJohn Anthony Powers (August 22, 1922 – December 31, 1979), better known as Shorty Powers, was an American public affairs officer for NASA from 1959 to 1963 during Project Mercury. A U.S. Air Force lieutenant colonel and war veteran, he was known as the \"voice of the astronauts,\" the \"voice of Mercury Control,\" and the \"eighth astronaut.\" He received his nickname for his 5-foot, 6-inch (1.68 m) height.\n\nPowers was born August 22, 1922, to first generation Welsh immigrant parents in Toledo, Ohio. Powers father's last name was actually Power, however, upon signing the immigration documents, Power became Powers. When Powers was an infant his family moved to Downers Grove, Illinois where he was a cheerleader at Downers Grove North High School, from which he graduated in 1941. After graduation, he enlisted in the U.S. Army Air Corps in 1942 and became a C-46 and C-47 pilot with the 349th Troop Carrier Group. He was one of six pilots who volunteered to learn the technique of snatching fully loaded troop gliders off the ground, and spent the end of World War II ferrying gasoline in cargo planes to Gen. George Patton's command in Germany.\n\nPowers left the service in January 1947; but was recalled to active duty in December 1948 and flew as part of the Berlin Airlift, making 185 round-trip flights. He later volunteered for the Korean War. He flew 55 night missions in B-26 bombers with the 13th Bombardment Squadron and received the Bronze Star Medal, the Air Medal, the Distinguished Flying Cross and a combat promotion to Major.\n\nFollowing Korea, Powers bounced around the Air Force, helping establish the first Community Relations Program in 1955. After being assigned to the personal staff of Maj. Gen. Bernard Schriever with the Air Research Development Command in Los Angeles, he handled the public dissemination of information related to the Air Force's ballistic missile program.\n\nPowers' experience with public affairs caught the attention of the newly formed NASA, and he was detailed to NASA's Space Task Group in April 1959 as its public affairs officer at the request of T. Keith Glennan, NASA's first administrator. Very early on April 12, 1961, John G. Warner, a UPI rewrite-man in Washington, D.C., roused Powers from sleep at Langley Research Center in Hampton, Virginia seeking comment on the flight of Soviet cosmonaut Yuri Gagarin, the first person in space. Powers replied, in part, \"We're all asleep down here,\" which made headlines.\n\nHe served as mission commentator for the six manned Mercury flights, introducing \"A-OK\" into the American vocabulary to signify procedures during the missions had proceeded as planned. He claimed astronaut Alan Shepard first used the expression during his \"Freedom 7\" flight, but communication transcripts later showed he had not. In \"The Right Stuff\", Tom Wolfe wrote that Powers had borrowed it \"from NASA engineers who used it during radio transmission tests because the sharper sound of \"A\" cut through the static better than \"O\"\".\n\nPowers enjoyed the limelight, and was accused of scheduling news conferences so he could appear live on national television and occasionally twisting the facts. For example, he told reporters the day before Gus Grissom's flight that the astronaut had gone fishing that day and had cooked and eaten his catch, which would have violated his pre-flight diet.\n\nManned Spacecraft Center Director Robert Gilruth announced Powers' reassignment on July 26, 1963, reportedly following a dispute with NASA Headquarters over handling publicity for the final Mercury flight. Powers objected to the HQ decision to release the mission's 22-orbit flight plan in advance. He was succeeded by Paul Haney on September 1, and Powers soon resigned.\n\nPowers retired from the Air Force in 1964 and opened a public relations firm in Houston. He became part owner of KMSC-FM in Clear Lake, Texas (the call letters standing for the Manned Spacecraft Center), where he anchored live coverage of Gemini and Apollo flights, distributed to radio stations across the country. He also served a spokesman for products including the 1965 Oldsmobile Delta 88 (touting its \"Super Rocket V-8\" engine), Carrier air conditioners, Triptone motion sickness pills, and Tareyton cigarettes (which claimed to use the same charcoal-activated filter used for the astronauts' oxygen supply). He lectured extensively about the space program, and served as emcee at the dedication of the Clear Lake Theatre Time Capsule on April 20, 1966. In 1967, he authored a newspaper column syndicated nationally by Field Enterprises called \"Space Talk\", answering readers' questions.\n\nPowers was married three times and was the father of three children. He married Sara Kay McSherry, women's editor of the \"Indianapolis News\", on August 7, 1965.\n\nPowers moved to Phoenix, Arizona in 1978, and died there at his home on December 31, 1979 at age 57 from a gastrointestinal hemorrhage related to chronic alcoholism.\n\nPowers appeared as himself in the 1963 episode entitled \"Junior Astronaut\" of CBS's sitcom, \"Dennis the Menace\", starring Jay North in the title role.\n\nHe was the narrator for the 1966 Jerry Lewis space comedy, \"Way...Way Out\".\n\nHe is referenced in the 1988 cult film, \"Miracle Mile\", by actor Kurt Fuller when, as Soviet warheads appear over Los Angeles, he states, \"Talk me down, Shorty Powers\".\n\n\n"}
{"id": "13026324", "url": "https://en.wikipedia.org/wiki?curid=13026324", "title": "Joseph Franz von Jacquin", "text": "Joseph Franz von Jacquin\n\nJoseph \"Krystel\" Franz Freiherr von Jacquin or Baron Joseph von Jacquin (7 February 1766 in Schemnitz (now Banská Štiavnica) – 26 October 1839 in Vienna) was an Austrian scientist who studied medicine, chemistry, zoology and botany.\n\nThe son of Nikolaus von Jacquin, he graduated from the University of Vienna as a doctor of medicine in 1788.\n\nBetween 1788 and 1791 Jacquin was sent on a scientific journey to Germany, France and England by Emperor Francis II.\n\nHe inherited his father's position as professor of botany and chemistry at the University of Vienna, which he held from 1797 until his retirement in 1838. In 1821, he was elected a foreign member of the Royal Swedish Academy of Sciences.\n\nJacquin, J. F. \"Beyträge zur Geschichte der Vögel\". C.F. Wappler, Wien 1784.\n\nJacquin, J.F. \"Lehrbuch der allgemeinen und medicinischen Chymie zum Gebrauche seiner Vorlesungen\". C.F. Wappler, Wien 1798.\n\nJacquin, J.F., E. Fenzl & I. Schreibers. \"Eclogae plantarum rariorum aut minus cognitarum : quas ad vivum descripsit et iconibus coloratis illustravit\". A. Strauss, Wien, 1811–1844.\n\nJacquin, J.F., E. Fenzl & I. Schreibers. \"Eclogae graminum rariorum aut minus cognitarum : quae ad vivum descripsit et iconibus coloratis illustravit\". A. Strauss et Sommer, Wien, 1813–1844.\n\nJacquin, J. F. \"Ueber den Ginkgo\", Carl Gerold, Wien, 1819.\n\n"}
{"id": "7044567", "url": "https://en.wikipedia.org/wiki?curid=7044567", "title": "Kari Enqvist", "text": "Kari Enqvist\n\nKari-Pekka Enqvist (born February 16, 1954 in Lahti, Finland) is a professor of cosmology in the Department of Physical Sciences at the University of Helsinki. Enqvist was awarded his PhD in theoretical physics in 1983.\n\nEnqvist is the chairman of the scientific advisory board of Skepsis ry (a Finnish sceptics' society) and has written many books that popularize physics.\n\nIn 1997 Enqvist was granted the Magnus Ehrnrooth Foundation Physics Award for his efforts in particle physics and cosmology.\n\nIn 1999, he was awarded the Tieto-Finlandia award, Finland's most significant award for non-fiction, for his book \"Olemisen porteilla\".\n\n"}
{"id": "49239358", "url": "https://en.wikipedia.org/wiki?curid=49239358", "title": "Kirklington Hall Research Station", "text": "Kirklington Hall Research Station\n\nKirklington Hall Research Station was a geophysical research institute of BP in Kirklington, Nottinghamshire.\n\nAs part of the East Midlands Oil Province, oil was found in eastern Nottinghamshire. It was also known as the BP Research Centre or the Geophysical Centre, part of BP's Exploration Division. The research centre was established in 1950. Its first employee was Jack Birks, later Managing Director of BP. From 1950 it was the main research site of BP, until BP sold the site in 1957 for £12,000. Research moved to Sunbury-on-Thames, in Surrey, in 1957. Sunbury had been built around the same time as the Kirklington site, in the early 1950s.\n\nKirklington Hall today is a private school.\n\nThe former site is situated north of the A617.\n\nIt conducted geophysical research for exploration for BP. This part of BP is now known as BP Exploration. Work would be conducted on core samples and with seismic methods.\n\n\n\n"}
{"id": "478311", "url": "https://en.wikipedia.org/wiki?curid=478311", "title": "Laser star model of quasars", "text": "Laser star model of quasars\n\nThe laser star hypothesis is a hypothesis put forward by the physicist Y. P. Varshni in 1973 in response to several developments in astrophysical and earth-based research, and his belief that the existing body of work on quasars, or \"quasi stellar objects,\" suffered fundamental difficulties. It states that quasars are hot stars with strong stellar winds which are producing natural lasing. This model is contradicted by the current astrophysical science community understanding that quasars are distant active galactic nuclei.\n\n\n"}
{"id": "48534508", "url": "https://en.wikipedia.org/wiki?curid=48534508", "title": "List of Solar System objects most distant from the Sun in 2015", "text": "List of Solar System objects most distant from the Sun in 2015\n\nThese Solar System objects were the furthest away from the Sun as of December 2015. The objects have been categorized by their approximate heliocentric distance from the Sun, not by the greatest calculated aphelion in their orbit. \n\nThis list does not mention objects that were discovered, but not announced, by 2015. An example would be 2015 TH367, which was discovered in October 2015 but not announced until 2018.\n\nThe list changes over time because the objects are moving. Some objects are inbound and some are outbound. It would be difficult to detect long-distance comets if it weren't for their comas, which become visible when heated by the Sun.\n\nDistances are measured in astronomical units (AU, Sun–Earth distances). The distances are not the minimum (perihelion) or the maximum (aphelion) that may be achieved by these objects in the future.\n\nThe discovery of an object known as V774104 was announced in November 2015 and was heralded by many news outlets as \"the most distant Solar System object\", surpassing Eris by close to 7 AU (not counting space probes and long-period comets). There is some confusion regarding its distance as of 2018, as V774104 might now be suspected of being closer than Eris, but this is due to a misunderstanding (which arose because Scott Sheppard was referring to an unannounced Sednoid).\n\nAnother very distant body is Sedna, which was discovered in November 2003. Although it takes over 10,000 years to orbit, during the next 60 years it will slowly move closer to the Sun as it comes to perihelion.\n\n\n\n"}
{"id": "24404684", "url": "https://en.wikipedia.org/wiki?curid=24404684", "title": "List of dinosaurs of the Morrison Formation", "text": "List of dinosaurs of the Morrison Formation\n\nThe Morrison Formation is a distinctive sequence of Late Jurassic sedimentary rock that is found in the western United States, which has been the most fertile source of dinosaur fossils in North America. It is composed of mudstone, sandstone, siltstone and limestone and is light grey, greenish gray, or red. Most of the fossils occur in the green siltstone beds and lower sandstones, relics of the rivers and floodplains of the Jurassic period.\n\nThe fauna of Morrison Formation is similar to one in the coeval rocks of Tendaguru Beds (in Tanzania) and Lourinhã Formation in Portugal, mostly with the second. Some genera are shared in Morrison and Lourinhã, such as \"Torvosaurus\", \"Ceratosaurus\", \"Stegosaurus\", \"Dryosaurus\", and \"Allosaurus\".\nIn sum, Morrison Fm has 37 valid genera of dinosaurs.\n\nThe herbivorous ornithischian dinosaurs were diverse but not as common as sauropods in the Morrison. \"Fruitadens\", previously known as the \"Fruita \"Echinodon\"\", was found to be a heterodontosaurid. Plate-backed stegosaurids included \"Hesperosaurus mjosi\", \"Stegosaurus armatus\", \"S. ungulatus\", \"S. stenops\", and \"Alcovasaurus longispinus\". Armored dinosaurs that weren't stegosaurs were unknown in the formation until the 1990s. Two have been named: \"Gargoyleosaurus parkpinorum\" and \"Mymoorapelta maysi\". Ornithopods, bipedal herbivores, came in several types. Small \"hypsilophodonts\" included \"Drinker nisti\", \"Laosaurus celer\", \"L.\" \"gracilis\", \"Nanosaurus agilis\", \"Othnielia rex\", and \"Othnielosaurus consors\". Larger but similar-looking dryosaurids were represented by \"Dryosaurus altus\" and the camptosaurid \"Uteodon aphanoecetes\", which is currently known only from Dinosaur National Monument. Still larger was the more common \"Camptosaurus dispar\". Dryosaurids and camptosaurids were early iguanodonts, a group that would later spawn the duck-billed dinosaurs.\n\nSauropods, the giant long-necked long-tailed four-legged herbivorous dinosaurs, are among the most common and famous Morrison fossils. A few have uncertain relationships, like \"Apatosaurus\" \"minimus\" (possibly a basal titanosauriform) and \"Haplocanthosaurus\". Sauropods including \"Haplocanthosaurus priscus\", \"H. delfsi\", and the diplodocid \"Eobrontosaurus\" appeared in the early stages of the Morrison. The middle stages were dominated by familiar forms such as the Giraffe-like \"Brachiosaurus altithorax\", which were uncommon, but related camarasaurids, like \"Camarasaurus supremus\", \"C. grandis\", \"C. lentus\", and \"Cathetosaurus\", were very common. Also common were long, low diplodocids, like \"Apatosaurus ajax\", \"A. louisae\", \"Brontosaurus excelsus\", \"B. parvus\", \"Barosaurus lentus\", \"Diplodocus longus\", \"D. carnegii\", \"Galeamopus\" and \"Dyslocosaurus polyonychius\".\n\nBy the late Morrison, gigantic diplodocids (or likely diplodocids) had appeared, including \"Diplodocus hallorum\" (formerly \"Seismosaurus\"), \"Supersaurus vivianae\", \"Amphicoelias altus\", and \"M. fragilimus\". Smaller sauropods, such as \"Suuwassea emiliae\" from Montana, tend to be found in the northern reaches of the Morrison, near the shores of the ancient Sundance Sea, suggesting ecological niches favoring smaller body size there compared with the giants found further south.\n\nTheropod dinosaurs, the carnivorous dinosaurs, came in several different types. The less derived types, the ceratosaurs and megalosaurids, included \"Ceratosaurus nasicornis\", \"C. dentisulcatus\", \"C. magnicornis\", \"Elaphrosaurus\" sp., and the megalosaur \"Torvosaurus tanneri\" (including \"Edmarka rex\"). Allosaurids included the common \"Allosaurus fragilis\" (including \"Epanterias amplexus\"), \"Allosaurus\" new species, \"A. lucasi\", and giant \"Saurophaganax maximus\" (potentially included in \"Allosaurus\"?). Also a recently discovered dinosaur has turned out to be a new species, not Allosaurus.\n\nIndeterminate theropod remains have been recovered in Utah. Indeterminate theropod tracks have been recovered from both Utah and Arizona.\n\nCoelurosaurs, more derived types more closely related to birds, included \"Coelurus fragilis\", \"Ornitholestes hermanni\", \"Tanycolagreus topwilsoni\", the possible troodontid \"Koparion douglassi\", and the definite troodontid WDC DML 001. There was also the possible early tyrannosaur relative \"Stokesosaurus clevelandi\".\n\nDinosaur eggs have been found in Utah.\n\nMorrison ornithopod trace fossils are represented by three toed tracks which are generally small. The toes of Morrison ornithopod tracks are usually more widely splayed than the theropod tracks preserved in the formation.\n\nStegosaur tracks were first recognized in 1996 from a hindprint-only trackway discovered at the Clevland-Lloyd quarry, which is located near Price, Utah. Two years later, a new ichnogenus called \"Stegopodus\" was erected for another set of stegosaur tracks which were found near Arches National Park, also in Utah. Unlike the first, this trackway preserved traces of the forefeet. Fossil remains indicate that stegosaurs have five digits on the forefeet and three weight-bearing digits on the hind feet. From this, scientists were able to successfully predict the appearance of stegosaur tracks in 1990, six years in advance of the first actual discovery of Morrison stegosaur tracks. Since the erection of \"Stegopodus\", more trackways have been found, however none have preserved traces of the front feet, and stegosaur traces remain rare.\n\nIndeterminate theropod tracks have been recovered from both Utah and Arizona.\n\n"}
{"id": "466401", "url": "https://en.wikipedia.org/wiki?curid=466401", "title": "List of human blood components", "text": "List of human blood components\n\nIn blood banking, the fractions of Whole Blood used for transfusion are also called components.\n\n"}
{"id": "1276699", "url": "https://en.wikipedia.org/wiki?curid=1276699", "title": "List of network protocols (OSI model)", "text": "List of network protocols (OSI model)\n\nThis article lists protocols, categorized by their nearest Open Systems Interconnection (OSI) model layers. This list is not exclusive to only the OSI protocol family. Many of these protocols are originally based on the Internet Protocol Suite (TCP/IP) and other models and they often do not fit neatly into OSI layers.\n\n\n\n\n\n\n\n\nThis layer, presentation Layer and application layer are combined in TCP/IP model.\n\n\n\n\n\n\n"}
{"id": "44069998", "url": "https://en.wikipedia.org/wiki?curid=44069998", "title": "List of organizations opposing mainstream science", "text": "List of organizations opposing mainstream science\n\nThis is a list of organizations opposing mainstream science by frequently challenging the facts and conclusions recognized by the mainstream scientific community. By claiming to employ the scientific method in order to advance certain fringe ideas and theories, they are often charged with promotion of various forms of pseudoscience.\n\n\nQuackery is the promotion of ineffective or fraudulent medical treatments.\n\n"}
{"id": "8938470", "url": "https://en.wikipedia.org/wiki?curid=8938470", "title": "List of volcanoes in New Zealand", "text": "List of volcanoes in New Zealand\n\nThis is a list of active, dormant, and extinct volcanoes in New Zealand.\n\nNew Zealand also has \"de facto\" administration over Ross Dependency in Antarctica, which contains the following volcanoes:\n\n"}
{"id": "45561261", "url": "https://en.wikipedia.org/wiki?curid=45561261", "title": "Making Contact (book)", "text": "Making Contact (book)\n\nMaking Contact: A Serious Handbook for Locating and Communicating With Extraterrestrials is a book published in 1998.\n\n\"Making Contact\" discusses all aspects of close encounters with extraterrestrials. Editor Bill Fawcett said their existence is \"unquestionable\". The book includes explanations of signs and symptoms of an alien abduction, such as burns, tinnitus, a metallic taste in the mouth, and double vision. Particular locations, such as Area 51 and Mexico City, are described as giving the best chance of an encounter.\n"}
{"id": "39130312", "url": "https://en.wikipedia.org/wiki?curid=39130312", "title": "Mart Bax", "text": "Mart Bax\n\nMarten Meile Gerrit \"Mart\" Bax (born 13 April 1937, Zutphen) is a Dutch emeritus (retired in 2002) endowed professor in political anthropology at the Vrije Universiteit (VU University), Amsterdam, the Netherlands. After his retirement he came into prominence to a wider public in the Netherlands in 2012 because of serious suspicions of scientific misconduct. In September 2013 these suspicions were confirmed in an official report.\n\nHe wrote his dissertation (\"cum laude\") for the University of Amsterdam in 1973 about the anonymized Irish town \"Patricksville\".\n\nHe further wrote about an anonymized pilgrimage center, called \"Neerdonk\" in the Dutch province of North Brabant.\n\nHe also wrote extensively about the pilgrimage center Medjugorje in the former Yugoslavia.\n\nThe presentation by Bax of the town of \"Patricksville\" as having extensive corruption, bribery, and clientelism is considered controversial among experts.\n\nHe claimed in his scientific publications there had been an estimated 140 killings, 60 people missing and 600 refugees from the pilgrimage village Medjugorje, in Bosnia during the Bosnian War (1992–1995). Bax wrote that he had based his observations on extensive local field research. He called these killing the \"small war\". The reason for the killings in 1991/1992 were according to Bax not ethnic conflict but a vendetta between clans. Apart from very local writings near Medjugorje these claims were first criticized in journalistic writings in 2008. In April 2013 at latest both the existence of the mass killing, missing people, and refugees turned to out to be false beyond reasonable doubt.\n\nIn October 2012 the Dutch book \"Ontspoorde Wetenschap\" (Engl.: \"Derailed science\") by the science journalist Frank van Kolfschooten was published. In the book Van Kolfschooten pointed to the lack of confirmation for the vendetta. Bax had published anonymized results about field research that he stated to have done in a monastery in the province of North Brabant, the Netherlands. Van Kolfschooten had doubts about the existence of the monastery, because the existence of the monastery could not be confirmed by experts and because Bax refused to tell anyone in confidence the name and the location of the monastery.The writings by Van Kolfschooten were largely based on the unpublished work of Dr. Peter Jan Margry of the Meertens institute. The Free University, Amsterdam, reacted by announcing an investigation to Bax's works.\n\nThe Dutch Volkskrant newspaper wrote in April 2013 that Bax's scientific writings about Medjugorje contained many important incorrect citations to local writings or to non-existing local writings. Bax also incorrectly stated that the resident register was destroyed during the Bosnian war.\n\nThe Volkskrant also wrote in the same lengthy article that approximately one third of the scientific publications that Bax had submitted in the internal database, called \"Metis\", of the university does not exist. The Volkskrant accuses him of fraud.\nA commission to investigate this possible scientific misconduct was chaired by Michiel Baud and published its findings in a report dated September 9, 2013, and made public September 23, 2013. The commission confirmed serious misconduct:\n\nThe VU announced that it will warn academic publishers against Bax.The VU stated that it will not take legal steps against Bax because he is old and because the crime of written misrepresentation has passed the statute of limitations.\n\nAccording to his own words, starting in the mid 1990s Margry could not make sense of Bax's works regarding the Neerdonk case, in particular Bax's inaugural speech from 1989. Margry contacted Bax and received replies from Bax that did not remove his doubts.\n\nIn 2008 the Frankfurter Rundschau did not receive a reply from Bax about the fact that he could not have observed in Medjugorje what he claimed to have observed.\n\nIn 2012 Van Kolfschooten did not receive a reply to the letters that he sent to Bax. \n\nIn April 2013 Bax wrote to the Volkskrant the following.\n\nIn 2013 the Baud committee spoke three times with Bax. In addition to the explanations to the Volkskrant, Bax said that he had written down improbable alleged historical events with certainty to make his articles accessible to readers. He stated that he had followed in this respect the example of Norbert Elias.\n\nIn September 2013 Bax did not reply to the report of the Baud committee.\n\n\n\n\n"}
{"id": "27287720", "url": "https://en.wikipedia.org/wiki?curid=27287720", "title": "Miniature hydraulics", "text": "Miniature hydraulics\n\nMiniature Hydraulics are copies or models that represent and reproduce regular or standard sized hydraulic systems and components, but in a reduced state, on a small scale, or in a greatly reduced size. True working and functional miniature hydraulics follow the same operating principles and behavioral properties as their standard or regular size hydraulic prototypes, but are done so primarily at reduced sizes and pressures. Although uncommon, miniature hydraulics do exist, and are obtainable through a variety of sources. Miniature hydraulics, mini hydraulics, and micro hydraulics are abbreviated as or otherwise known as M-H.\n\n"}
{"id": "198603", "url": "https://en.wikipedia.org/wiki?curid=198603", "title": "Nikolay Rukavishnikov", "text": "Nikolay Rukavishnikov\n\nNikolay Nikolayevich Rukavishnikov (; September 18, 1932 – October 19, 2002) was a Soviet cosmonaut who flew three space missions of the Soyuz programme: Soyuz 10, Soyuz 16, and Soyuz 33. Two of these missions, Soyuz 10 and Soyuz 33 were intended to dock with Salyut space stations, but failed to do so.\n\nRukavishnikov studied at the Moscow Engineering and Physics Institute and after graduation worked for Sergey Korolev's design bureau. He was selected for cosmonaut training in 1967.\n\nRukavishnikov became the 50th human to fly in space on April 23, 1971, the launch date of Soyuz 10. He subsequently resigned from the space programme in 1987 and returned to work for the same bureau he started with, by then known as Energia.\n\nHe died of a heart attack on October 18, 2002.\n\nHe was awarded:\n"}
{"id": "17377082", "url": "https://en.wikipedia.org/wiki?curid=17377082", "title": "Nobleite", "text": "Nobleite\n\nNobleite is a rare borate mineral with the chemical formula CaBO(OH)·3HO. It was discovered in 1961, in Death Valley, California, and is named for Levi F. Noble, a USGS geologist, in honor of his contributions to the geology of the Death Valley region.\n\nNobleite has also been identified at two localities in Chile and Argentina.\n\n"}
{"id": "14958673", "url": "https://en.wikipedia.org/wiki?curid=14958673", "title": "Optogenetics", "text": "Optogenetics\n\nOptogenetics () is a biological technique that involves the use of light to control cells in living tissue, typically neurons, that have been genetically modified to express light-sensitive ion channels. It is a neuromodulation method that uses a combination of techniques from optics and genetics to control and monitor the activities of individual neurons in living tissue—even within freely-moving animals—and to precisely measure these manipulation effects in real-time. The key reagents used in optogenetics are light-sensitive proteins. Neuronal control is achieved using optogenetic actuators like channelrhodopsin, halorhodopsin, and archaerhodopsin, while optical recording of neuronal activities can be made with the help of optogenetic sensors for calcium (GCaMP), vesicular release (synapto-pHluorin), neurotransmitter (GluSnFRs), or membrane voltage (arc lightning, ASAP1). Control (or recording) of activity is restricted to genetically defined neurons and performed in a spatiotemporal-specific manner by light.\n\nIn 2010, optogenetics was chosen as the \"Method of the Year\" across all fields of science and engineering by the interdisciplinary research journal \"Nature Methods\". At the same time, optogenetics was highlighted in the article on \"Breakthroughs of the Decade\" in the academic research journal \"Science\". These journals also referenced recent public-access general-interest video Method of the year video and textual SciAm summaries of optogenetics.\n\nThe \"far-fetched\" possibility of using light for selectively controlling precise neural activity (action potential) patterns within subtypes of cells in the brain was thought of by Francis Crick in his Kuffler Lectures at the University of California in San Diego in 1999. An earlier use of light to activate neurons was carried out by Richard Fork, who demonstrated laser activation of neurons within intact tissue, although not in a genetically-targeted manner. The earliest genetically targeted method that used light to control rhodopsin-sensitized neurons was reported in January 2002, by Boris Zemelman and Gero Miesenböck, who employed \"Drosophila\" rhodopsin cultured mammalian neurons. In 2003, Zemelman and Miesenböck developed a second method for light-dependent activation of neurons in which single inotropic channels TRPV1, TRPM8 and P2X2 were gated by photocaged ligands in response to light. Beginning in 2004, the Kramer and Isacoff groups developed organic photoswitches or \"reversibly caged\" compounds in collaboration with the Trauner group that could interact with genetically introduced ion channels. TRPV1 methodology, albeit without the illumination trigger, was subsequently used by several laboratories to alter feeding, locomotion and behavioral resilience in laboratory animals. However, light-based approaches for altering neuronal activity were not applied outside the original laboratories, likely because the easier to employ channelrhodopsin was cloned soon thereafter.\n\nPeter Hegemann, studying the light response of green algae at the University of Regensburg, had discovered photocurrents that were too fast to be explained by the classic g-protein-coupled animal rhodopsins. Teaming up with the electrophysiologist Georg Nagel at the Max Planck Institute in Frankfurt, they could demonstrate that a single gene from the alga \"Chlamydomonas\" produced large photocurents when expressed in the oocyte of a frog. To identify expressing cells, they replaced the cytoplasmic tail of the algal protein with the fluorescent protein YFP, generating the first generally applicable optogenetic tool. \n\nZhuo-Hua Pan of Wayne State University, researching on restore sight to blindness, thought about using channelrhodopsin when it came out in late 2003. By February 2004, he was trying channelrhodopsin out in ganglion cells—the neurons in our eyes that connect directly to the brain—that he had cultured in a dish. Indeed, the transfected neurons became electrically active in response to light. In 2005, Zhuo-Hua Pan reported successful in-vivo transfection of channelrhodopsin in retinal ganglion cells of mice, and electrical responses to photostimulation in retinal slice culture \n\nIn April 2005, Susana Lima and Miesenböck reported the first use of genetically-targeted P2X2 photostimulation to control the behaviour of an animal. They showed that photostimulation of genetically circumscribed groups of neurons, such as those of the dopaminergic system, elicited characteristic behavioural changes in fruit flies. \n\nIn August 2005, Karl Deisseroth's laboratory in the Bioengineering Department at Stanford including graduate students Ed Boyden and Feng Zhang published the first demonstration of a single-component optogenetic system in cultured mammalian neurons, using the channelrhodopsin-2(H134R)-eYFP construct from Nagel and Hegemann. \n\nThe groups of Gottschalk and Nagel were first to use channelrhodopsin-2 for controlling neuronal activity in an intact animal, showing that motor patterns in the roundworm \"Caenorhabditis elegans\" could be evoked by light stimulation of genetically selected neural circuits (published in December 2005). In mice, controlled expression of optogenetic tools is often achieved with cell-type-specific Cre/loxP methods developed for neuroscience by Joe Z. Tsien back in the 1990s to activate or inhibit specific brain regions and cell-types in vivo.\n\nThe primary tools for optogenetic recordings have been genetically encoded calcium indicators (GECIs). The first GECI to be used to image activity in an animal was cameleon, designed by Atsushi Miyawaki, Roger Tsien and coworkers. Cameleon was first used successfully in an animal by Rex Kerr, William Schafer and coworkers to record from neurons and muscle cells of the nematode \"C. elegans\". Cameleon was subsequently used to record neural activity in flies and zebrafish. In mammals, the first GECI to be used in vivo was GCaMP, first developed by Nakai and coworkers. GCaMP has undergone numerous improvements, and GCaMP6 in particular has become widely used throughout neuroscience.\n\nIn 2010, Karl Deisseroth at Stanford University was awarded the inaugural HFSP Nakasone Award \"for his pioneering work on the development of optogenetic methods for studying the function of neuronal networks underlying behavior\". In 2012, Gero Miesenböck was awarded the InBev-Baillet Latour International Health Prize for \"pioneering optogenetic approaches to manipulate neuronal activity and to control animal behaviour.\" In 2013, Ernst Bamberg, Ed Boyden, Karl Deisseroth, Peter Hegemann, Gero Miesenböck and Georg Nagel were awarded The Brain Prize for \"their invention and refinement of optogenetics.\" Karl Deisseroth was awarded the Else Kröner Fresenius Research Prize 2017 (4 million euro) for his \"contributions to the understanding of the biological basis of psychiatric disorders\".\n\nOptogenetics provides millisecond-scale temporal precision which allows the experimenter to keep pace with fast biological information processing (for example, in probing the causal role of specific action potential patterns in defined neurons). Indeed, to probe the neural code, optogenetics by definition must operate on the millisecond timescale to allow addition or deletion of precise activity patterns within specific cells in the brains of intact animals, including mammals (see Figure 1). By comparison, the temporal precision of traditional genetic manipulations (employed to probe the causal role of specific genes within cells, via \"loss-of-function\" or \"gain of function\" changes in these genes) is rather slow, from hours or days to months. It is important to also have fast readouts in optogenetics that can keep pace with the optical control. This can be done with electrical recordings (\"optrodes\") or with reporter proteins that are biosensors, where scientists have fused fluorescent proteins to detector proteins. An example of this is voltage-sensitive fluorescent protein (VSFP2). Additionally, beyond its scientific impact optogenetics represents an important case study in the value of both ecological conservation (as many of the key tools of optogenetics arise from microbial organisms occupying specialized environmental niches), and in the importance of pure basic science as these opsins were studied over decades for their own sake by biophysicists and microbiologists, without involving consideration of their potential value in delivering insights into neuroscience and neuropsychiatric disease.\n\nLight-activated proteins: channels, pumps and enzymes\n\nThe hallmark of optogenetics therefore is introduction of fast light-activated channels, pumps, and enzymes that allow temporally precise manipulation of electrical and biochemical events while maintaining cell-type resolution through the use of specific targeting mechanisms. Among the microbial opsins which can be used to investigate the function of neural systems are the channelrhodopsins (ChR2, ChR1, VChR1, and SFOs) to excite neurons and anion-conducting channelrhodopsins for light-induced inhibition. Indirectly light-controlled potassium channels have recently been engineered to prevent action potential generation in neurons during blue light illumination. Light-driven ion pumps are also used to inhibit neuronal activity, e.g. halorhodopsin (NpHR), enhanced halorhodopsins (eNpHR2.0 and eNpHR3.0, see Figure 2), archaerhodopsin (Arch), fungal opsins (Mac) and enhanced bacteriorhodopsin (eBR).\n\nOptogenetic control of well-defined biochemical events within behaving mammals is now also possible. Building on prior work fusing vertebrate opsins to specific G-protein coupled receptors a family of chimeric single-component optogenetic tools was created that allowed researchers to manipulate within behaving mammals the concentration of defined intracellular messengers such as cAMP and IP3 in targeted cells. Other biochemical approaches to optogenetics (crucially, with tools that displayed low activity in the dark) followed soon thereafter, when optical control over small GTPases and adenylyl cyclases was achieved in cultured cells using novel strategies from several different laboratories. This emerging repertoire of optogenetic probes now allows cell-type-specific and temporally precise control of multiple axes of cellular function within intact animals.\n\nHardware for light application\n\nAnother necessary factor is hardware (e.g. integrated fiberoptic and solid-state light sources) to allow specific cell types, even deep within the brain, to be controlled in freely behaving animals. Most commonly, the latter is now achieved using the fiberoptic-coupled diode technology introduced in 2007, though to avoid use of implanted electrodes, researchers have engineered ways to inscribe a \"window\" made of zirconia that has been modified to be transparent and implanted in mice skulls, to allow optical waves to penetrate more deeply to stimulate or inhibit individual neurons. To stimulate superficial brain areas such as the cerebral cortex, optical fibers or LEDs can be directly mounted to the skull of the animal. More deeply implanted optical fibers have been used to deliver light to deeper brain areas. Complementary to fiber-tethered approaches, completely wireless techniques have been developed utilizing wirelessly delivered power to headborne LEDs for unhindered study of complex behaviors in freely behaving organisms.\n\nExpression of optogenetic actuators\n\nOptogenetics also necessarily includes the development of genetic targeting strategies such as cell-specific promoters or other customized conditionally-active viruses, to deliver the light-sensitive probes to specific populations of neurons in the brain of living animals (e.g. worms, fruit flies, mice, rats, and monkeys). In invertebrates such as worms and fruit flies some amount of all-trans-retinal (ATR) is supplemented with food. A key advantage of microbial opsins as noted above is that they are fully functional without the addition of exogenous co-factors in vertebrates.\n\nThe technique of using optogenetics is flexible and adaptable to the experimenter's needs. For starters, experimenters genetically engineer a microbial opsin based on the gating properties (rate of excitability, refractory period, etc..) required for the experiment.\n\nThere is a challenge in introducing the microbial opsin, an optogenetic actuator, into a specific region of the organism in question. A rudimentary approach is to introduce an engineered viral vector that contains the optogenetic actuator gene attached to a recognizable promoter such as CAMKIIα. This allows for some level of specificity as cells that already contain and can translate the given promoter will be infected with the viral vector and hopefully express the optogenetic actuator gene.\n\nAnother approach is the creation of transgenic mice where the optogenetic actuator gene is introduced into mice zygotes with a given promoter, most commonly Thy1. Introduction of the optogenetic actuator at an early stage allows for a larger genetic code to be incorporated and as a result, increases the specificity of cells to be infected.\n\nA third and rather novel approach that has been developed is creating transgenic mice with Cre recombinase, an enzyme that catalyzes recombination between two lox-P sites. Then by introducing an engineered viral vector containing the optogenetic actuator gene in between two lox-P sites, only the cells containing the Cre recombinase will express the microbial opsin. This last technique has allowed for multiple modified optogenetic actuators to be used without the need to create a whole line of transgenic animals every time a new microbial opsin is needed.\n\nAfter the introduction and expression of the microbial opsin, depending on the type of analysis being performed, application of light can be placed at the terminal ends or the main region where the infected cells are situated. Light stimulation can be performed with a vast array of instruments from light emitting diodes (LEDs) or diode-pumped solid state (DPSS). These light sources are most commonly connected to a computer through a fiber optic cable. Recent advances include the advent of wireless head-mounted devices that also apply LED to targeted areas and as a result give the animal more freedom of mobility to reproduce \"in vivo\" results.\n\nAlthough already a powerful scientific tool, optogenetics, according to Doug Tischer & Orion D. Weiner of the University of California San Francisco, should be regarded as a \"first-generation GFP\" because of its immense potential for both utilization and optimization. With that being said, the current approach to optogenetics is limited primarily by its versatility. Even within the field of Neuroscience where it is most potent, the technique is less robust on a subcellular level.\n\nOne of the main problems of optogenetics is that not all the cells in question may express the microbial opsin gene at the same level. Thus, even illumination with a defined light intensity will have variable effects on individual cells. Optogenetic stimulation of neurons in the brain is even less controlled as the light intensity drops exponentially from the light source (e.g. implanted optical fiber).\n\nMoreover, mathematical modelling shows that selective expression of opsin in specific cell types can dramatically alter the dynamical behavior of the neural circuitry. In particular, optogenetic stimulation that preferentially targets inhibitory cells can transform the excitability of the neural tissue from Type 1 — where neurons operate as integrators — to Type 2 where neurons operate as resonators.\nType 1 excitable media sustain propagating waves of activity whereas Type 2 excitable media do not. The transformation from one to the other explains how constant optical stimulation of primate motor cortex elicits gamma-band (40–80 Hz) oscillations in the manner of a Type 2 excitable medium. Yet those same oscillations propagate far into the surrounding tissue in the manner of a Type 1 excitable medium.\n\nNonetheless, it remains difficult to target opsin to defined subcellular compartments, e.g. the plasma membrane, synaptic vesicles, or mitochondria. Restricting the opsin to specific regions of the plasma membrane such as dendrites, somata or axon terminals would provide a more robust understanding of neuronal circuitry.\n\nAn issue with channelrhodopsin-2 is that its gating properties don't mimic \"in vivo\" cation channels of cortical neurons. A solution to this issue with a protein's kinetic property is introduction of variants of channelrhodopsin-2 with more favorable kinetics.\n\nAnother one of the technique's limitations is that light stimulation produces a synchronous activation of infected cells and this removes any individual cell properties of activation among the population affected. Therefore, it is difficult to understand how the cells in the population affected communicate with one another or how their phasic properties of activation may relate to the circuitry being observed.\n\nOptogenetic activation has been combined with functional magnetic resonance imaging (ofMRI) to elucidate the connectome, a thorough map of the brain’s neural connections. The results, however, are limited by the general properties of fMRI. The readouts from this neuroimaging procedure lack the spatial and temporal resolution appropriate for studying the densely packed and rapid-firing neuronal circuits.\n\nThe opsin proteins currently in use have absorption peaks across the visual spectrum, but remain considerable sensitivity to blue light. This spectral overlap makes it very difficult to combine opsin activation with genenetically encoded indictors (GEVIs, GECIs, GluSnFR, synapto-pHluorin), most of which need blue light excitation. Opsins with infrared activation would, at a standard irradiance value, increase light penetration and augment resolution through reduction of light scattering.\n\nThe field of optogenetics has furthered the fundamental scientific understanding of how specific cell types contribute to the function of biological tissues such as neural circuits \"in vivo\" (see references from the scientific literature below). Moreover, on the clinical side, optogenetics-driven research has led to insights into Parkinson's disease and other neurological and psychiatric disorders. Indeed, optogenetics papers in 2009 have also provided insight into neural codes relevant to autism, Schizophrenia, drug abuse, anxiety, and depression.\n\nOptogenetic approaches have been used to map neural circuits in the amygdala that contribute to fear conditioning. One such example of a neural circuit is the connection made from the basolateral amygdala to the dorsal-medial prefrontal cortex where neuronal oscillations of 4 Hz have been observed in correlation to fear induced freezing behaviors in mice. Transgenic mice were introduced with channelrhodoposin-2 attached with a parvalbumin-Cre promoter that selectively infected interneurons located both in the basolateral amygdala and the dorsal-medial prefrontal cortex responsible for the 4 Hz oscillations. The interneurons were optically stimulated generating a freezing behavior and as a result provided evidence that these 4 Hz oscillations may be responsible for the basic fear response produced by the neuronal populations along the dorsal-medial prefrontal cortex and basolateral amygdala.\n\nOptogenetic activation of olfactory sensory neurons was critical for demonstrating timing in odor processing and for mechanism of neuromodulatory mediated olfactory guided behaviors (e.g. aggression, mating) In addition, with the aid of optogenetics, evidence has been reproduced to show that the \"afterimage\" of odors is concentrated more centrally around the olfactory bulb rather than on the periphery where the olfactory receptor neurons would be located. Transgenic mice infected with channel-rhodopsin Thy1-ChR2, were stimulated with a 473 nm laser transcranially positioned over the dorsal section of the olfactory bulb. Longer photostimulation of mitral cells in the olfactory bulb led to observations of longer lasting neuronal activity in the region after the photostimulation had ceased, meaning the olfactory sensory system is able to undergo long term changes and recognize differences between old and new odors.\n\nOptogenetics, freely moving mammalian behavior, \"in vivo\" electrophysiology, and slice physiology have been integrated to probe the cholinergic interneurons of the nucleus accumbens by direct excitation or inhibition. Despite representing less than 1% of the total population of accumbal neurons, these cholinergic cells are able to control the activity of the dopaminergic terminals that innervate medium spiny neurons (MSNs) in the nucleus accumbens. These accumbal MSNs are known to be involved in the neural pathway through which cocaine exerts its effects, because decreasing cocaine-induced changes in the activity of these neurons has been shown to inhibit cocaine conditioning. The few cholinergic neurons present in the nucleus accumbens may prove viable targets for pharmacotherapy in the treatment of cocaine dependence\n\n\"In vivo\" and \"in vitro\" recordings (by the Cooper laboratory) of individual CAMKII AAV-ChR2 expressing pyramidal neurons within the prefrontal cortex demonstrated high fidelity action potential output with short pulses of blue light at 20 Hz (Figure 1). The same group recorded complete green light-induced silencing of spontaneous activity in the same prefrontal cortical neuronal population expressing an AAV-NpHR vector (Figure 2).\n\nOptogenetics was applied on atrial cardiomyocytes to end spiral wave arrhythmias, found to occur in atrial fibrillation, with light. This method is still in the development stage. A recent study explored the possibilities of optogenetics as a method to correct for arrythmias and resynchronize cardiac pacing. The study introduced channelrhodopsin-2 into cardiomyocytes in ventricular areas of hearts of transgenic mice and performed \"in vitro\" studies of photostimulation on both open-cavity and closed-cavity mice. Photostimulation led to increased activation of cells and thus increased ventricular contractions resulting in increasing heart rates. In addition, this approach has been applied in cardiac resynchronization therapy (CRT) as a new biological pacemaker as a substitute for electrode based-CRT. Lately, optogenetics has been used in the heart to defibrillate ventricular arrhythmias with local epicardial illumination, a generalized whole heart illumination or with customized stimulation patterns based on arrhythmogenic mechanisms in order to lower defibrillation energy.\n\nOptogenetic stimulation of the spiral ganglion in deaf mice restored auditory activity. Optogenetic application onto the cochlear region allows for the stimulation or inhibition of the spiral ganglion cells (SGN). In addition, due to the characteristics of the resting potentials of SGN's, different variants of the protein channelrhodopsin-2 have been employed such as Chronos and CatCh. Chronos and CatCh variants are particularly useful in that they have less time spent in their deactivated states, which allow for more activity with less bursts of blue light emitted. The result being that the LED producing the light would require less energy and the idea of cochlear prosthetics in association with photo-stimulation, would be more feasible.\n\nOptogenetic stimulation of a modified red-light excitable channelrhodopsin (ReaChR) expressed in the facial motor nucleus enabled minimally invasive activation of motoneurons effective in driving whisker movements in mice. One novel study employed optogenetics on the Dorsal Ralphe Nucleus to both activate and inhibit dopaminergic release onto the ventral tegmental area. To produce activation transgenic mice were infected with channelrhodopsin-2 with a TH-Cre promoter and to produce inhibition the hyperpolarizing opsin NpHR was added onto the TH-Cre promoter. Results showed that optically activating dopaminergic neurons led to an increase in social interactions, and their inhibition decreased the need to socialize only after a period of isolation.\n\nThe currently available optogenetic actuators allow for the accurate temporal control of the required intervention (i.e. inhibition or excitation of the target neurons) with precision routinely going down to the millisecond level. Therefore, experiments can now be devised where the light used for the intervention is triggered by a particular element of behavior (to inhibit the behavior), a particular unconditioned stimulus (to associate something to that stimulus) or a particular oscillatory event in the brain (to inhibit the event). This kind of approach has already been used in several brain regions:\n\nSharp waves and ripple complexes (SWRs) are distinct high frequency oscillatory events in the hippocampus thought to play a role in memory formation and consolidation. These events can be readily detected by following the oscillatory cycles of the on-line recorded local field potential. In this way the onset of the event can be used as a trigger signal for a light flash that is guided back into the hippocampus to inhibit neurons specifically during the SWRs and also to optogenetically inhibit the oscillation itself These kinds of \"closed-loop\" experiments are useful to study SWR complexes and their role in memory.\n\nThe optogenetic toolkit has proven pivotal for the field of neuroscience as it allows precise manipulation of neuronal excitability. Moreover, this technique has been shown to extend outside neurons to an increasing number of proteins and cellular functions. Cellular scale modifications including manipulation of contractile forces relevant to cell migration, cell division and wound healing have been optogenetically manipulated. The field has not developed to the point where processes crucial to cellular and developmental biology and cell signaling including protein localization, post-translational modification and GTP loading can be consistently controlled via optogenetics.\n\nWhile this extension of optogenetics remains to be further investigated, there are various conceptual methodologies that may prove to immediately robust. There is a considerable body of literature outlining photosensitive proteins that have been utilized in cell signaling pathways. CRY2, LOV, DRONPA and PHYB are photosynthetic proteins involved in inducible protein association whereby activation via light can induce/turn off a signaling cascade via recruitment of a signaling domain to its respective substrate. LOV and PHYB are photosensitive proteins that engage in homodimerization and/or heterodimerization to recruit some DNA-modifying protein, translocate to the site of DNA and alter gene expression levels. CRY2, a protein that inherently clusters when active, has been fused with signaling domains and subsequently photoactivated allowing for clustering-based activation. Proteins LOV and Dronpa have also been adapted to cell signaling manipulation; exposure to light induces conformational changes in the photosensitive protein which can subsequently reveal a previously obscured signaling domain and/or activate a protein that was otherwise allosterically inhibited. LOV has been fused to caspase 3 to produce a construct capable of inducing apoptosis upon light stimulation.\n\nA different set of signaling cascades respond to stimulus timing duration and dynamics. Adaptive signaling pathways, for instance, adjust in accordance to the current level of the projected stimulus and display activity only when these levels change as opposed to responding to absolute levels of the input. Stimulus dynamics also can trigger activity; treating PC12 cells with epidermal growth factor (inducing a transient profile of ERK activity) leads to cellular proliferation whereas introduction of nerve growth factor (inducing a sustained profile of ERK activity) is associated with a different cellular decision whereby the PC12 cells differentiate into neuron-like cells. This discovery was guided pharmacologically but the finding was replicated utilizing optogenetic inputs instead. This ability to optogenetically control signals for various time durations is being explored to elucidate various cell signaling pathways where there is not a strong enough understanding to utilize either drug/genetic manipulation.\n\n\n"}
{"id": "19280607", "url": "https://en.wikipedia.org/wiki?curid=19280607", "title": "Outline of economics", "text": "Outline of economics\n\nThe following outline is provided as an overview of and topical guide to economics:\n\nEconomics – analyzes the production, distribution, and consumption of goods and services. It aims to explain how economies work and how economic agents interact.\n\nEconomics can be described as all of the following:\n\n\n\n\n\n\nEconomy – system of human activities related to the production, distribution, exchange, and consumption of goods and services of a country or other area.\n\n\n\n\n\n\n\n\n\n\n\nEconomic policy\n\nInfrastructure\n\nMarket\n\n\n\nMarket form\n\n\nMoney\n\nResource management\n\nFactors of production\n\nLand\n\nCapital\n\n\n\nHistory of economic thought\n\nEconomic history\n\n\n\n\n\n\n\n"}
{"id": "32007766", "url": "https://en.wikipedia.org/wiki?curid=32007766", "title": "PQube", "text": "PQube\n\nPQube is a registered trademark of Power Standards Lab for an electronic measuring instrument that records power quality and electric energy on the electric power grid.\n\nPQube instruments are widely used to gather data for academic research, and at United States Department of Energy National Laboratories and State energy regulators. U.S. federal government agencies use PQubes to detect power quality issues – for example, the Federal Aviation Administration tracks disturbances at radar control centers.\n\nEach PQube instrument is traceable to the National Institute of Standards and Technology, so often these instruments are used in international academic and research environments.\n\nPQubes are a key element in many Smart Grid projects, recording power disturbance and power flow data to examine efficiency and reliability effects.\n\nApproximately 50 PQubes, located in approximately 40 countries, have been designated by their owners as free public sources of information at codice_1 . The site is updated approximately every 2 minutes with worldwide power quality and energy recordings. \n\nData from these PQubes can be used, for example, for developing and testing power quality algorithms.\n\n"}
{"id": "51909382", "url": "https://en.wikipedia.org/wiki?curid=51909382", "title": "Pauline Dy Phon", "text": "Pauline Dy Phon\n\nPauline Dy Phon (ប៉ូលីន ឌី ផុន) (1933-21 May 2010) was a Cambodian botanist who specialized in the flora of South East Asia.\n\nComing to study in France, she obtained her license in 1959 at the Faculty of Sciences in Paris. She became teacher and researcher at the University of Phnom Penh, though in 1975 she was forced to stop work because the Khmer Rouge came to power. In 1980, she managed to flee to France and work in the Botanical Laboratory of the National Museum of Natural History. In the same institution she contributed significantly to identifying and classifying plants of Cambodia and Indochina, which remain relatively unknown. In 1980 she was awarded the Prix de Coincy by the Académie des Sciences. She published a 915-page directory of the \"Dictionary of plants used in Cambodia\" in 2000.\n"}
{"id": "7503662", "url": "https://en.wikipedia.org/wiki?curid=7503662", "title": "Somnath Bharadwaj", "text": "Somnath Bharadwaj\n\nSomnath Bharadwaj (born 28 October 1964) is an Indian theoretical physicist who works on Theoretical Astrophysics and Cosmology.\n\nBharadwaj was born in India, studied at the Indian Institute of Technology in Kharagpur, and later received his Ph.D. from the Indian Institute of Science. After having worked at the Harish-Chandra Research Institute, he is now a professor at IIT Kharagpur. He has made significant contributions to the dynamics of large-scale structure formation.\n\nIn 2003, he was selected to be one of the professors from IIT whose class room lectures would be broadcast in the Eklavya Technology Channel.\n\nBharadwaj was an invited speakers on Galaxy Formation at the prestigious Indo-US Frontier of Science symposium which was organized by the U.S. National Academy of Sciences in 2005.\nHe is currently in the Editorial Board of the \"Journal of Astrophysics & Astronomy\" published by the Indian Academy of Sciences.\n\n"}
{"id": "44838163", "url": "https://en.wikipedia.org/wiki?curid=44838163", "title": "Susan Cachel", "text": "Susan Cachel\n\nSusan Cachel is an American anthropologist, paleontologist and researcher who specializes in primate evolution, including humans. In 2009 she was named a Fellow of the American Association for the Advancement of Science in honor of her work in primate evolution. She is the author of \"Primate and Human Evolution\", published in 2006 by the Cambridge University Press.\n\n"}
{"id": "32954448", "url": "https://en.wikipedia.org/wiki?curid=32954448", "title": "The 50 Most Extreme Places in Our Solar System", "text": "The 50 Most Extreme Places in Our Solar System\n\nThe 50 Most Extreme Places in Our Solar System is the title of a science book by David Baker, professor of physics at Austin College and Todd Ratcliff, a planetary geophysicist at the Jet Propulsion Laboratory.\n\nThe book is divided into nine sections with a few chapters in each section. The divisions are roughly phenomenological.\n\n\n\n\n\n\n\n\n\n\nThe book is currently available in two translations with a third in progress.\n\nReviews have been positive including being awarded Honorable Mention in Cosmology and Astronomy in the 2010 PROSE Awards and named an Outstanding University Press Book for Public and Secondary School Libraries by the Association of American University Presses.\n\n\n"}
{"id": "58878706", "url": "https://en.wikipedia.org/wiki?curid=58878706", "title": "Tomáš Čajka", "text": "Tomáš Čajka\n\nTomáš Čajka (Ph.D. in 2009), is a Czech chemist, who is active in the field of analytical chemistry; he is an associate professor of the Department of metabolomics, Institute of physiology CAS (Prague).\n\n\n\n"}
