{"id": "12008404", "url": "https://en.wikipedia.org/wiki?curid=12008404", "title": "A632 road", "text": "A632 road\n\nThe A632 is a major road in Derbyshire and Nottinghamshire, England. It starts in Matlock and joins the town with Chesterfield. From there, it goes through Bolsover and then onto the A616 at the village of Cuckney \n"}
{"id": "786580", "url": "https://en.wikipedia.org/wiki?curid=786580", "title": "Agracetus", "text": "Agracetus\n\nThe Agracetus Campus of Monsanto Company is a soybean transformation laboratory, the world's greatest. It has over 21,700 employees worldwide, and an annual revenue of US$11.365 billion reported for 2008.\n\nThe first successful genetically engineered crop ever produced for the commercial market was the \"Roundup Ready soybean\", produced at Agracetus in 1991, and was one of fourteen successful transformation events. Scientists there used gold bead gene transfer technology coupled with the β-Glucuronidase reporter gene to produce the plant. The actual gun that shot the gold beads and produced the genetic modifications is now owned by the Smithsonian museum in Washington, DC. \n\nEvery \"Roundup Ready soybean\" in the world has a relative which was genetically transformed at Agracetus. 80% of the world's soybeans are \"Roundup Ready\".\n\nAgracetus was founded in 1981 as Cetus Corporation. Acquired by Monsanto in 1996, the research and development facility is located 8 miles (13 km) west of Madison in the city of Middleton, Wisconsin, on . The site has of research space, of greenhouse space, about 75 employees, and ten laboratories. Output of genetically modified soy plants is many thousands of transformation events per year.\n\nGenetically modified cotton and genetically modified rice is also an important effort at Agracetus.\n"}
{"id": "40421442", "url": "https://en.wikipedia.org/wiki?curid=40421442", "title": "Akkermansia muciniphila", "text": "Akkermansia muciniphila\n\nAkkermansia muciniphila is a species of human intestinal mucin-degrading bacterium, the type species for a new genus, \"Akkermansia\", proposed in 2004 by Muriel Derrien and others. Extensive research is being undertaken to understand its association with obesity, diabetes, and inflammation.\n\n\"A. muciniphila\" is a Gram-negative, strictly anaerobic, non-motile, non-spore-forming, oval-shaped bacterium. Its type strain is Muc (=ATCC BAA-835 =CIP 107961). \"A. muciniphila\" is able to use mucin as its sole source of carbon and nitrogen, is culturable under anaerobic conditions on medium containing gastric mucin, and is able to colonize the gastrointestinal tracts of a number of animal species.\n\nRecently, \"A. muciniphila\" strain Urmite became the first (evidently) unculturable bacterial strain to be sequenced in its entirety entirely from a human stool sample.\n\n\"A. muciniphila\" is believed to have anti-inflammatory effects in humans, and studies have shown inverse relationships between \"A. muciniphila\" colonization and inflammatory conditions such as appendicitis or inflammatory bowel disease (IBD). In one study, reduced levels of \"A. muciniphila\" correlated with increased severity of appendicitis. In a separate study, IBD patients were found to have lower levels \"A. muciniphila\" in their intestinal tract than individuals without IBD.\n\nResearchers have discovered that \"A. muciniphila\" may be able to be used to combat obesity and type 2 diabetes. The study was carried out with mice, overfed to contain three times more fat than its lean cousin. The obese mice were then fed the bacteria, which were shown to reduce the fat burden of the mice by half without any change to the mice's diet. A study published in June 2015 showed an association between \"A. muciniphila\" abundance, insulin sensitivity and healthier metabolic status in overweight/obese adults. The healthier subjects were those with high \"A. muciniphila\" abundance and gut microbial richness. In addition, this study showed that having higher abundance of \"A. muciniphila\" at baseline was associated with greater clinical benefits after weight loss. The bacterium is naturally present in the human digestive tract at 3-5%, but has been seen to fall with obesity. It is thought that eating the bacterium increases the gut wall thickness, with the addition of mucin, which will block food from being absorbed by the body.\n\nIn August 2015, additional research demonstrated that dietary fats influence the growth of \"Akkermansia muciniphilia\" relative to other bacterium in the dietary tract. Researchers conducted a study in which mice were fed diets which varied in fat composition but were otherwise identical; one group received lard while the other received fish oil. After 11 weeks, the group receiving a fish oil diet had increased levels of \"A. muciniphila\" and bacterium of genus \"Lactobacillus\", while the group receiving a lard diet had decreased levels of \"A. muciniphila\" and \"Lactobacillus\". Additional testing was performed by conducting fecal transplants from mice on the fish oil diet or the lard based diet into a new group of mice which had their native gut flora eradicated with antibiotics. All of these mice were then fed a lard based diet. Despite receiving the same lard-based diet for 3 weeks, recipients of transplants from lard-fed donor mice showed increased levels of \"Lactobacillus\" and increased levels of inflammation, while recipients of transplants from fish oil-fed donors showed increased levels of \"A. muciniphila\" and decreased levels of inflammation. Researchers concluded that the increase in \"A. muciniphila\" corresponded to a reduction in inflammation, indicating a link between dietary fats, gut flora composition, and inflammation levels.\n\nOne study looked at 249 patients with lung or kidney cancer, \"A. muciniphila\" was in 69% of patients that did respond compared with just a third of those who did not. Boosting levels of \"A. muciniphila\" in mice seemed to also boost their response to immunotherapy.\n\n"}
{"id": "52044774", "url": "https://en.wikipedia.org/wiki?curid=52044774", "title": "Attitude-behavior consistency", "text": "Attitude-behavior consistency\n\nAttitude-behavior consistency is when a person's attitude is consistent with their behavior. This is not true in many cases. The fact that people often express attitudes that are inconsistent with how they act may surprise those unfamiliar with social and behavioral science, but it is an important fact to understand because facts are often reported as if they are about people's actions when they may only be known to be true about their words. It is often much easier to conduct interviews or surveys than to obtain records of how people behave in situations. Sometimes attitudes, such as voting, are measurably consistent with behavior. In such cases it may be possible to obtain accurate estimates of behavior. However, there is no general method for correcting for attitude-behavior inconsistency.\n\nAttitude-behavior consistency is an important concept for social science research because claims are often made about behavior based on evidence which is really about attitudes. The attitudinal fallacy is committed when verbal data are used to support claims not about what people believe or say, but what they do. Data collection methods based on self-report like surveys, and interviews are vulnerable to the attitudinal fallacy if they attempt to measure behavior and reported attitudes are inconsistent with the behavior. \n\nResearch methods that directly observe behaviors avoid the attitudinal fallacy as a matter of course. However many kinds of behavior are not easily observed. Ethnography can make rich observations and descriptions of behavior and allow for comparison between behavior and attitude. Unfortunately, in general ethnographic data cannot be used to draw statistically generalizable conclusions about behavior in a population.Ethnographers can still commit the attitudinal fallacy if they rely on quotations as evidence for behaviors. Experiments in laboratories make it possible to observe behavior, with the limitations that come from the laboratory situation. Internet research makes it possible to study a wide array of behaviors that leave traces online. Data from the Internet of things and sensors that record behavior like from location tracking may make it possible to measure more kinds of behavior that avoids the attitudinal fallacy. It may not be possible to study some kinds of behavior other than through interviews or surveys. While the knowledge produced in such cases may be useful. The possibility of inconsistency between behavior and reported attitudes is always a concern. \n\nMethods that are limited by their inability to measure behavior can still contribute to important understandings. These include how meaning is created, the significance of events to individuals, emotion, semiotics, representation and opinions.\n\n\nReports of attitudes and behaviors may be subject to social desirability bias. Even in cases where respondents are anonymous, people may be less likely to acknowledge behaviors that they see as undesirable. Conversely they may be more likely to report behaviors that are seen as more desirable. Social desirability bias results from inconsistency between attitudes and behaviors. This is because although people may have positive attitudes toward behaviors they see as desirable, they do not actually perform the behaviors as often as they say they do. Studies making claims about behaviors based on reports when behaviors may be seen as desirable may be particularly sensitive to the attitudinal fallacy. Although indirect questioning has been a common practice among survey researchers, it is not generally effective at mitigating social desirability bias. \n\nPeople do not necessarily agree on which attitudes are socially desirable. Moreover, these attitudes may be situational (see below) and vary from setting to setting. Therefore, the ways in which attitudes are biased by social desirability may be interesting in its own right. Therefore, social desirability may not invalidate measures of internal factors such as personality in the same way as it invalidates studies of behavior.\n\nA person's attitude and behavior both vary from situation to situation. Behaviors may vary as when a person who rarely edits Wikipedia does so for a class assignment. Attitudes also vary from situation to situation. A college freshman may disapprove of binge drinking, only to subsequently become socialized to practice and celebrate doing so in the course of tailgating. Sometimes attitudes and behavior may be linked to reduce cognitive dissonance. But this need not be the case.\n\n"}
{"id": "4711669", "url": "https://en.wikipedia.org/wiki?curid=4711669", "title": "Az-Zubair Prize for Innovation and Scientific Excellence", "text": "Az-Zubair Prize for Innovation and Scientific Excellence\n\nAz-Zubair Prize for Innovation and Scientific Excellence is an annual Sudanese scientific prize awarded by the President of Sudan for scientific innovation and creativity in applied and technological fields.\n\nOn November 7, 1998, President Omer Al-Bashir passed a bill to establish an annual scientific award for innovation and scientific excellence that commemorate General Az-Zubair Mohammed Salih, former Sudan's vice president who died in an aeroplane crash. The award is governed by the Association for the Promotion of Scientific Innovation (APSI), a specialized organization that was established by General Az-Zubair for sponsoring technological advances and scientific research.\n\nThe prize is divided onto two major levels:\n\n\nThe general fields of the Az-Zubair Prize are:\n\nThe prize award is composed of:\n\n"}
{"id": "6055455", "url": "https://en.wikipedia.org/wiki?curid=6055455", "title": "Bandwidth extension", "text": "Bandwidth extension\n\nBandwidth extension of signal is defined as the deliberate process of expanding the frequency range (bandwidth) of a signal in which it contains an appreciable and useful content, and/or the frequency range in which its effects are such. Its significant advancement in recent years has led to the technology being adopted commercially in several areas including psychacoustic bass enhancement of small loudspeakers and the high frequency enhancement of coded speech and audio. \n\nBandwidth extension has been used in both speech and audio compression applications. The algorithms used in G.729.1 and Spectral Band Replication (SBR) are two of many examples of bandwidth extension algorithms currently in use. In these methods, the low band of the spectrum is encoded using an existing codec, whereas the high band is coarsely parameterized using fewer parameters. Many of these bandwidth extension algorithms make use of the correlation between the low band and the high band in order to predict the wider band signal from extracted lower-band features. Others encode the high band using very few bits. This is often sufficient since the ear is less sensitive to distortions in the high band compared to the low band. \n\nMost often small loudspeakers are physically incapable of reproducing low frequency material. Using a psycho-acoustical phenomenon like the missing fundamental, perception of low frequencies can be greatly increased. By generating harmonics of lower frequencies and removing the lower frequencies themselves the suggestion is created that these frequencies are still remaining in the signal. This process is usually applied through external equipment or embedded in the speaker system using a digital signal processor. \n\nHigh frequency response can also be enhanced through generation of harmonics. Instead of mapping frequencies inside the reproducible region of the speaker, the speaker itself is used to generate frequencies outside the normal reproducible region. By boosting high frequencies and overdriving the speaker or amplifier slightly, higher harmonics can be generated.\n\nTelephone speech signals are usually very degraded in quality. Part of this degradation is due to the limited bandwidth used in the telephone systems. In most systems frequencies lower than 250 Hz are cut and bandwidth only extends to frequencies of 4 or 8 kHz. Using filtering and waveshaping low and high frequency response can be extended.\n\nBy low pass filtering the lowest octave and half-wave rectifying a waveform is created with a fundamental half of the original frequency. Due to the discontinuity in the waveform low pass filtering is needed to filter all harmonics. Using such a subharmonic synthesizer the essential frequency band between 125 - 250 Hz is recreated, adding weight to the signal.\n\nTo extend the high frequency bandwidth, we can isolate the top octave using high pass filtering and then generating harmonics of this. The generation of harmonics can be done through a simple full wave rectification, which is computationally cheap and not amplitude-dependent. As an alternative single-sideband modulation can be used, giving precise control over the number and amplitude of the harmonics. In theory envelope estimation can be used to extract the original high frequency envelope and regenerating high frequencies using a noise source. The sparse information available in the small bandwidth will probably be too limited to extract a proper envelope.\n\nSpectral band replication (SBR) is a new technique that has gained popularity as an “add-on” to popular perceptual audio codecs such as MP-3 and the Advanced Audio Coding (AAC). New audio coders consisting of a marriage between SBR and the conventional audio coders have been formed, namely the MP3Pro and AAC+. In these algorithms, the lower spectrum is encoded using either MP-3 or AAC, whereas the high band is encoded using SBR. The key to the SBR algorithm is the information used to describe the high frequency portion of the signal. The primary design goal of this algorithm is to reconstruct the high band spectrum without introducing any aliasing artifacts and to provide good spectral and time resolution. A 64-band complex-valued polyphase filterbank is used at the analysis portion. At the encoder, the filterbank is used to obtain energy samples of the original input signal's high band. These energy samples are then used as reference values for the envelope adjustment scheme used at the decoder.\n\n"}
{"id": "17211617", "url": "https://en.wikipedia.org/wiki?curid=17211617", "title": "Beverly Halstead", "text": "Beverly Halstead\n\nLambert Beverly Halstead (13 June 1933 – 30 April 1991), who also went by Lambert Beverly Halstead Tarlo, was a British paleontologist & professor of Geology & Zoology and popularizer of science. He was noted for his candid theories of dinosaur sexual habits, and also for a prolonged assault on phylogenetic systematics (or \"cladism\", as he referred to it), in a series of letters and editorials to the journal Nature in the late 1970s and early 1980s.\n\n"}
{"id": "2749049", "url": "https://en.wikipedia.org/wiki?curid=2749049", "title": "Brazilian Women's Articulation", "text": "Brazilian Women's Articulation\n\nThe Brazilian Women's Articulation (in Portuguese Articulação das Mulheres Brasileiras) is a Brazilian feminist organization that links women's organizations in Brazil's 26 states and its federal district. It was created to organize the Brazilian feminist movement's preparation and follow-up to the 1995 Beijing Women's Conference. In 2008, the AMB organized video conferences in order to publicise and promote awareness of the Lei Maria da Penha, a Brazilian anti-domestic violence law.\n\n"}
{"id": "36199570", "url": "https://en.wikipedia.org/wiki?curid=36199570", "title": "Child displacement", "text": "Child displacement\n\nChild displacement is the removal or separation of children from their parents and immediate family or settings in which they have initially been reared. Displaced children includes varying categories of children who experience separation from their families and social settings due to several varied reasons. These populations include children separated from their parents, refugees, children sent to boarding schools, Internally displaced persons or IDPs, and asylum seekers. Thus child displacement refers to a broad range of factors due to which children are removed from their parents and social setting. This include persecution, war, armed conflict and disruption and separation for varied reasons.\n\nAccording to UNHCR (United Nations High Commissioner for Refugees) as of (2002) there were approximately 22 million displaced children in the world. Several of whom are displaced for a very long time spanning years. Children in worst affected areas in armed conflict or disruption face an average of 6 to 7 years of displacement.\n\nThe internationally accepted and acknowledged definition of a \"child\" is anyone who is under the age of 18 regardless of any context.\n\n\nPsychologists and social-behavioral scientists agree that children thrive better both psychologically and developmentally in two rather than one parent families. Bowlby (1969) stated that there is a critical period that was sensitive to the development of attachment, during which attachment is easily formed between the child and its parents. Early research on adoptions gives support to this view, though scholars state that the sensitive period is actually quite extensive. This might imply that children need longer period of interaction with their parents than previously thought.\n\nThe development of attachment occurs as a result of the process of reciprocal interaction between parent and the child. This reciprocal interaction helps the child to discriminate its parents from others and helps develop emotional relationships with its parents. Infant-parent attachment helps to develop psychological security, self-confidence and enables the development of trust in other Humans. The amount of time spend together is not the only factor that influences the development of attachments. Some threshold level of interaction is also needed for attachment to develop. Opportunities for regular interaction is important for the development of attachment. Children in both single-parent and two-parent families seem to be better adjusted when they enjoy warm and affirmative interactions with two parents who are actively involved. Empirical literature shows that children need regular interaction with attachment figures in order to maintain relationships. Extended separation of the child from either of the parent is detrimental as it hinders the development of attachment and relationship between the child and the parent. It is because of this reason that it becomes extremely difficult to re-establish relationship between child and parents once it is disrupted. For this reason it is best to avoid such disruptions. A child's relationship with its parents has significant influence on the nature of social, psychological and emotional development of the child. Empirical research also shows that disruption in relationship between child and its parents has adverse effects on a child's development. Those children who are hindered from having stable and regular interaction and meaningful relationship with either of the parent are at a higher potential of psychological risk. Thus children are more likely and able to attain their psychological potential when they are able to have healthy and regular relationship with both their parents.\n\nThe ideal situation of relationship between children and parents is one in which there is everyday interaction between the child and its parents. This includes interaction in various family and social contexts like play, basic care, limit setting, putting to bed etc. Everyday activities promotes and maintains the development of trust and helps to strengthen and deepen parent–child relationships.\n\nIn terms of divorce and separation adverse effects have been noted due to severed father-child relationship. Therefore, in terms of separation due to divorce it is important to maintain regular interaction between child and both parents. It is therefore unfortunate that in contemporary practice relationship is not fostered between the child and the non custodial parent in divorce proceedings. So when it is beneficial, children face potential risk due to removal from or disappearance of their non custodial fathers. This risk is added on with the other baggage of problems that occurs due to divorce including financial burden and less social support. For Children separation from parents is stressful and painful.\n\nThe NICHD study of Early child care was designed to assess the long-term outcomes of non parental care giving. Non Parental care giving involved both relatives (kinship care) and non relatives (Day care). The NICHD study was based on the ecological theory of Uri Bronfenbrenner (1979). Analysis of the effects of family and child care revealed that the characteristics of the family and the nature and quality of the mothers relationship with the child was a significantly better predictor of children's outcome.\nProlonged separations from parents have profound disruptive influence on children's development. Such prolonged separations include separations due to death, institutionalization which entails sending a child to live in an institution or setting without parents, divorce and desertion of the child by parent and hospitalization or prolonged absence due to illness. Prolonged exposure to poor institutional care could lead to despair, apathy and deficit in social responsiveness. According to Rutter (1979). it is the failure to develop secure attachment with parents that leads to problems in social responsiveness. Institutionalization is extreme form of separation of the child from its parents.\n\nConsiderable negative impact occurs to a child due to neglect. Outright rejection, intimidation, or isolation of the child are forms of emotional child abuse.\n\nWith the view that children develop through adolescence by building on prior periods and that some effects of early experiences during child rearing might manifest later on in what is called as \"Sleeper effect\". It may be seen that effects of child rearing may be seen even later.\n\n"}
{"id": "39834718", "url": "https://en.wikipedia.org/wiki?curid=39834718", "title": "Dennis Wojtkiewicz", "text": "Dennis Wojtkiewicz\n\nDennis Wojtkiewicz (born 1956, Chicago, Illinois) is an American Hyperrealist painter and draughtsman.\n\nWojtkiewicz graduated from Southern Illinois University and is artist associated with the Hyperrealist movement. He is best known for his large scale renderings of sliced fruit and flowers. In order to achieve his desired effect, technically, Wojtkiewicz relies on traditional oil paint and brushes with a classical application, he also uses pastel for his drawings.\n\nDennis Wojtkiewicz has exhibited his work in leading fine art galleries in the United States and abroad. His work is owned by many leading private, corporate and public collections including the Evanston Museum of Art, Fidelity Investments in Boston and the University of South Dakota.\n\n\n"}
{"id": "1444970", "url": "https://en.wikipedia.org/wiki?curid=1444970", "title": "Directional statistics", "text": "Directional statistics\n\nDirectional statistics (also circular statistics or spherical statistics) is the subdiscipline of statistics that deals with directions (unit vectors in R), axes (lines through the origin in R) or rotations in R. More generally, directional statistics deals with observations on compact Riemannian manifolds.\n\nThe fact that 0 degrees and 360 degrees are identical angles, so that for example 180 degrees is not a sensible mean of 2 degrees and 358 degrees, provides one illustration that special statistical methods are required for the analysis of some types of data (in this case, angular data). Other examples of data that may be regarded as directional include statistics involving temporal periods (e.g. time of day, week, month, year, etc.), compass directions, dihedral angles in molecules, orientations, rotations and so on.\n\nAny probability density function (pdf) formula_1 on the line can be \"wrapped\" around the circumference of a circle of unit radius. That is, the pdf of the wrapped variable \nis\n\nThis concept can be extended to the multivariate context by an extension of the simple sum to a number of formula_4 sums that cover all dimensions in the feature space:\nwhere formula_6 is the formula_7th Euclidean basis vector.\n\nThe following sections show some relevant circular distributions.\n\nThe von Mises distribution is a circular distribution which, like any other circular distribution, may be thought of as a wrapping of a certain linear probability distribution around the circle. The underlying linear probability distribution for the von Mises distribution is mathematically intractable; however, for statistical purposes, there is no need to deal with the underlying linear distribution. The usefulness of the von Mises distribution is twofold: it is the most mathematically tractable of all circular distributions, allowing simpler statistical analysis, and it is a close approximation to the wrapped normal distribution, which, analogously to the linear normal distribution, is important because it is the limiting case for the sum of a large number of small angular deviations. In fact, the von Mises distribution is often known as the \"circular normal\" distribution because of its ease of use and its close relationship to the wrapped normal distribution (Fisher, 1993).\n\nThe pdf of the von Mises distribution is:\n\nwhere formula_9 is the modified Bessel function of order 0.\nThe probability density function (pdf) of the circular uniform distribution is given by\n\nIt can also be thought of as formula_11 of the von Mises above.\n\nThe pdf of the wrapped normal distribution (WN) is:\n\nThe pdf of the wrapped Cauchy distribution (WC) is:\n\nThe pdf of the Wrapped Lévy distribution (WL) is:\n\nwhere the value of the summand is taken to be zero when formula_21, formula_22 is the scale factor and formula_23 is the location parameter.\n\nThere also exist distributions on the two-dimensional sphere (such as the Kent distribution), the \"N\"-dimensional sphere (the von Mises–Fisher distribution) or the torus (the bivariate von Mises distribution).\n\nThe matrix von Mises–Fisher distribution is a distribution on the Stiefel manifold, and can be used to construct probability distributions over rotation matrices.\n\nThe Bingham distribution is a distribution over axes in \"N\" dimensions, or equivalently, over points on the (\"N\" − 1)-dimensional sphere with the antipodes identified. For example, if \"N\" = 2, the axes are undirected lines through the origin in the plane. In this case, each axis cuts the unit circle in the plane (which is the one-dimensional sphere) at two points that are each other's antipodes. For \"N\" = 4, the Bingham distribution is a distribution over the space of unit quaternions. Since a unit quaternion corresponds to a rotation matrix, the Bingham distribution for \"N\" = 4 can be used to construct probability distributions over the space of rotations, just like the Matrix-von Mises–Fisher distribution.\n\nThese distributions are for example used in geology, crystallography and bioinformatics.\n\nThe raw vector (or trigonometric) moments of a circular distribution are defined as\n\nwhere formula_25 is any interval of length formula_26, formula_27 is the PDF of the circular distribution, and formula_28. Since the integral formula_27 is unity, and the integration interval is finite, it follows that the moments of any circular distribution are always finite and well defined.\n\nSample moments are analogously defined:\n\nThe population resultant vector, length, and mean angle are defined in analogy with the corresponding sample parameters.\n\nIn addition, the lengths of the higher moments are defined as:\n\nwhile the angular parts of the higher moments are just formula_35. The lengths of the higher moments will all lie between 0 and 1.\n\nVarious measures of location and spread may be defined for both the population and a sample drawn from that population. The most common measure of location is the circular mean. The population circular mean is simply the first moment of the distribution while the sample mean is the first moment of the sample. The sample mean will serve as an unbiased estimator of the population mean.\n\nWhen data is concentrated, the median and mode may be defined by analogy to the linear case, but for more dispersed or multi-modal data, these concepts are not useful.\n\nThe most common measures of circular spread are:\n\n\n\n\nGiven a set of \"N\" measurements formula_44 the mean value of \"z\" is defined as:\n\nwhich may be expressed as\n\nwhere\n\nor, alternatively as:\n\nwhere\n\nThe distribution of the mean (formula_50) for a circular pdf \"P\"(\"θ\") will be given by:\n\nwhere formula_25 is over any interval of length formula_26 and the integral is subject to the constraint that formula_54 and formula_55 are constant, or, alternatively, that formula_56 and formula_50 are constant.\n\nThe calculation of the distribution of the mean for most circular distributions is not analytically possible, and in order to carry out an analysis of variance, numerical or mathematical approximations are needed.\n\nThe central limit theorem may be applied to the distribution of the sample means. (main article: Central limit theorem for directional statistics). It can be shown that the distribution of formula_58 approaches a bivariate normal distribution in the limit of large sample size.\n\nFor cyclic data - (e.g., is it uniformly distributed) :\n\n\n"}
{"id": "7466964", "url": "https://en.wikipedia.org/wiki?curid=7466964", "title": "Enterprise modelling", "text": "Enterprise modelling\n\nEnterprise modelling is the abstract representation, description and definition of the structure, processes, information and resources of an identifiable business, government body, or other large organization.\n\nIt deals with the process of understanding an organization and improving its performance through creation and analysis of enterprise models. This includes the modelling of the relevant business domain (usually relatively stable), business processes (usually more volatile), and uses of information technology within the business domain and its processes.\n\nEnterprise modelling is the process of building models of whole or part of an enterprise with process models, data models, resource models and or new ontologies etc. It is based on knowledge about the enterprise, previous models and/or reference models as well as domain ontologies using model representation languages. An enterprise in general is a unit of economic organization or activity. These activities are required to develop and deliver products and/or services to a customer. An enterprise includes a number of functions and operations such as purchasing, manufacturing, marketing, finance, engineering, and research and development. The enterprise of interest are those corporate functions and operations necessary to manufacture current and potential future variants of a product.\n\nThe term \"enterprise model\" is used in industry to represent differing enterprise representations, with no real standardized definition. Due to the complexity of enterprise organizations, a vast number of differing enterprise modelling approaches have been pursued across industry and academia. Enterprise modelling constructs can focus upon manufacturing operations and/or business operations; however, a common thread in enterprise modelling is an inclusion of assessment of information technology. For example, the use of networked computers to trigger and receive replacement orders along a material supply chain is an example of how information technology is used to coordinate manufacturing operations within an enterprise.\n\nThe basic idea of enterprise modelling according to Ulrich Frank is \"to offer different views on an enterprise, thereby providing a medium to foster dialogues between various stakeholders - both in academia and in practice. For this purpose they include abstractions suitable for strategic planning, organisational (re-) design and software engineering. The views should complement each other and thereby foster a better understanding of complex systems by systematic abstractions. The views should be generic in the sense that they can be applied to any enterprise. At the same time they should offer abstractions that help with designing information systems which are well integrated with a company's long term strategy and its organisation. Hence, enterprise models can be regarded as the conceptual infrastructure that support a high level of integration.\"\n\nEnterprise modelling has its roots in systems modelling and especially information systems modelling. One of the earliest pioneering works in modelling information systems was done by Young and Kent (1958), who argued for \"a precise and abstract way of specifying the informational and time characteristics of a data processing problem\". They wanted to create \"a notation that should enable the analyst to organize the problem around any piece of hardware\". Their work was a first effort to create an abstract specification and invariant basis for designing different alternative implementations using different hardware components. A next step in IS modelling was taken by CODASYL, an IT industry consortium formed in 1959, who essentially aimed at the same thing as Young and Kent: the development of \"a proper structure for machine independent problem definition language, at the system level of data processing\". This led to the development of a specific IS information algebra.\n\nThe first methods dealing with enterprise modelling emerged in the 1970s. They were the entity-relationship approach of Peter Chen (1976) and SADT of Douglas T. Ross (1977), the one concentrate on the information view and the other on the function view of business entities. These first methods have been followed end 1970s by numerous methods for software engineering, such as SSADM, Structured Design, Structured Analysis and others. Specific methods for enterprise modelling in the context of Computer Integrated Manufacturing appeared in the early 1980s. They include the IDEF family of methods (ICAM, 1981) and the GRAI method by Guy Doumeingts in 1984 followed by GRAI/GIM by Doumeingts and others in 1992.\n\nThese second generation of methods were activity-based methods which have been surpassed on the one hand by process-centred modelling methods developed in the 1990s such as Architecture of Integrated Information Systems (ARIS), CIMOSA and Integrated Enterprise Modeling (IEM). And on the other hand by object-oriented methods, such as Object-oriented analysis (OOA) and Object-modelling technique (OMT).\n\nAn enterprise model is a representation of the structure, activities, processes, information, resources, people, behavior, goals, and constraints of a business, government, or other enterprises. Thomas Naylor (1970) defined a (simulation) model as \"an attempt to describe the interrelationships among a corporation's financial, marketing, and production activities in terms of a set of mathematical and logical relationships which are programmed into the computer.\" These interrelationships should according to Gershefski (1971) represent in detail all aspects of the firm including \"the physical operations of the company, the accounting and financial practices followed, and the response to investment in key areas\" Programming the modelled relationships into the computer is not always necessary: enterprise models, under different names, have existed for centuries and were described, for example, by Adam Smith, Walter Bagehot, and many others.\n\nAccording to Fox and Gruninger (1998) from \"a design perspective, an enterprise model should provide the language used to explicitly define an enterprise... From an operations perspective, the enterprise model must be able to represent what is planned, what might happen, and what has happened. It must supply the information and knowledge necessary to support the operations of the enterprise, whether they be performed by hand or machine.\"\n\nIn a two-volume set entitled \"The Managerial Cybernetics of Organization\" Stafford Beer introduced a model of the enterprise, the Viable System Model (VSM). Volume 2, \"The Heart of Enterprise,\" analyzed the VSM as a recursive organization of five systems: System One (S1) through System Five (S5). Beer's model differs from others in that the VSM is recursive, not hierarchical: \"In a recursive organizational structure, any viable system contains, and is contained in, a viable system.\"\n\nFunction modelling in systems engineering is a structured representation of the functions, activities or processes within the modelled system or subject area.\n\nA function model, also called an activity model or process model, is a graphical representation of an enterprise's function within a defined scope. The purpose of the function model are to describe the functions and processes, assist with discovery of information needs, help identify opportunities, and establish a basis for determining product and service costs. A function model is created with a functional modelling perspective. A functional perspectives is one or more perspectives possible in process modelling. Other perspectives possible are for example behavioural, organisational or informational.\n\nA functional modelling perspective concentrates on describing the dynamic process. The main concept in this modelling perspective is the process, this could be a function, transformation, activity, action, task etc. A well-known example of a modelling language employing this perspective is data flow diagrams. The perspective uses four symbols to describe a process, these being:\nNow, with these symbols, a process can be represented as a network of these symbols. This decomposed process is a DFD, data flow diagram. In Dynamic Enterprise Modeling, for example, a division is made in the Control model, Function Model, Process model and Organizational model.\n\nData modelling is the process of creating a data model by applying formal data model descriptions using data modelling techniques. Data modelling is a technique for defining business requirements for a database. It is sometimes called \"database modelling\" because a data model is eventually implemented in a database.\n\nThe figure illustrates the way data models are developed and used today. A conceptual data model is developed based on the data requirements for the application that is being developed, perhaps in the context of an activity model. The data model will normally consist of entity types, attributes, relationships, integrity rules, and the definitions of those objects. This is then used as the start point for interface or database design.\n\nBusiness process modelling, not to be confused with the wider Business Process Management (BPM) discipline, is the activity of representing processes of an enterprise, so that the current (\"as is\") process may be analyzed and improved in future (\"to be\"). Business process modelling is typically performed by business analysts and managers who are seeking to improve process efficiency and quality. The process improvements identified by business process modelling may or may not require Information Technology involvement, although that is a common driver for the need to model a business process, by creating a process master.\n\nChange management programs are typically involved to put the improved business processes into practice. With advances in technology from large platform vendors, the vision of business process modelling models becoming fully executable (and capable of simulations and round-trip engineering) is coming closer to reality every day.\n\nThe RM-ODP reference model identifies enterprise modelling as providing one of the five viewpoints of an open distributed system. Note that such a system need not be a modern-day IT system: a banking clearing house in the 19th century may be used as an example ().\n\nThere are several techniques for modelling the enterprise such as \n\nMore enterprise modelling techniques are developed into Enterprise Architecture framework such as:\nAnd metamodelling frameworks such as:\n\nEnterprise engineering is the discipline concerning the design and the engineering of enterprises, regarding both their business and organization. In theory and practice two types of enterprise engineering has emerged. A more general connected to engineering and the management of enterprises, and a more specific related to software engineering, enterprise modelling and enterprise architecture.\n\nIn the field of engineering a more general enterprise engineering emerged, defined as the application of engineering principals to the management of enterprises. It encompasses the application of knowledge, principles, and disciplines related to the analysis, design, implementation and operation of all elements associated with an enterprise. In essence this is an interdisciplinary field which combines systems engineering and strategic management as it seeks to engineer the entire enterprise in terms of the products, processes and business operations. The view is one of continuous improvement and continued adaptation as firms, processes and markets develop along their life cycles. This total systems approach encompasses the traditional areas of research and development, product design, operations and manufacturing as well as information systems and strategic management. This fields is related to engineering management, operations management, service management and systems engineering.\n\nIn the context of software development a specific field of enterprise engineering has emerged, which deals with the modelling and integration of various organizational and technical parts of business processes. In the context of information systems development it has been the area of activity in the organization of the systems analysis, and an extension of the scope of Information Modelling. It can also be viewed as the extension and generalization of the systems analysis and systems design phases of the software development process. Here Enterprise modelling can be part of the early, middle and late information system development life cycle. Explicit representation of the organizational and technical system infrastructure is being created in order to understand the orderly transformations of existing work practices. This field is also called Enterprise architecture, or defined with Enterprise Ontology as being two major parts of Enterprise architecture.\n\nBusiness reference modelling is the development of reference models concentrating on the functional and organizational aspects of the core business of an enterprise, service organization or government agency. In enterprise engineering a business reference model is part of an enterprise architecture framework. This framework defines in a series of reference models, how to organize the structure and views associated with an Enterprise Architecture.\n\nA reference model in general is a model of something that embodies the basic goal or idea of something and can then be looked at as a reference for various purposes. A business reference model is a means to describe the business operations of an organization, independent of the organizational structure that perform them. Other types of business reference model can also depict the relationship between the business processes, business functions, and the business area’s business reference model. These reference model can be constructed in layers, and offer a foundation for the analysis of service components, technology, data, and performance.\n\nEconomic modelling is the theoretical representation of economic processes by a set of variables and a set of logical and/or quantitative relationships between them. The economic model is a simplified framework designed to illustrate complex processes, often but not always using mathematical techniques. Frequently, economic models use structural parameters. Structural parameters are underlying parameters in a model or class of models. A model may have various parameters and those parameters may change to create various properties.\n\nIn general terms, economic models have two functions: first as a simplification of and abstraction from observed data, and second as a means of selection of data based on a paradigm of econometric study. The simplification is particularly important for economics given the enormous complexity of economic processes. This complexity can be attributed to the diversity of factors that determine economic activity; these factors include: individual and cooperative decision processes, resource limitations, environmental and geographical constraints, institutional and legal requirements and purely random fluctuations. Economists therefore must make a reasoned choice of which variables and which relationships between these variables are relevant and which ways of analyzing and presenting this information are useful.\n\nOntology engineering or \"ontology building\" is a subfield of knowledge engineering that studies the methods and methodologies for building ontologies. In the domain of enterprise architecture, an ontology is an outline or a schema used to structure objects, their attributes and relationships in a consistent manner. As in enterprise modelling, an ontology can be composed of other ontologies. The purpose of ontologies in enterprise modelling is to formalize and establish the sharability, re-usability, assimilation and dissemination of information across all organizations and departments within an enterprise. Thus, an ontology enables integration of the various functions and processes which take place in an enterprise.\n\nOne common language with well articulated structure and vocabulary would enable the company to be more efficient in its operations. A common ontology will allow for effective communication, understanding and thus coordination among the various divisions of an enterprise. There are various kinds of ontologies used in numerous environments. While the language example given earlier dealt with the area of information systems and design, other ontologies may be defined for processes, methods, activities, etc., within an enterprise.\n\nUsing ontologies in enterprise modelling offers several advantages. Ontologies ensure clarity, consistency, and structure to a model. They promote efficient model definition and analysis. Generic enterprise ontologies allow for reusability of and automation of components. Because ontologies are schemata or outlines, the use of ontologies does not ensure proper enterprise model definition, analysis, or clarity. Ontologies are limited by how they are defined and implemented. An ontology may or may not include the potential or capability to capture all of the aspects of what is being modelled.\n\nThe modelling of the enterprise and its environment could facilitate the creation of enhanced understanding of the business domain and processes of the extended enterprise, and especially of the relations—both those that \"hold the enterprise together\" and those that extend across the boundaries of the enterprise. Since enterprise is a system, concepts used in system thinking can be successfully reused in modelling enterprises.\n\nThis way a fast understanding can be achieved throughout the enterprise about how business functions are working and how they depend upon other functions in the organization.\n\n\n\n"}
{"id": "194690", "url": "https://en.wikipedia.org/wiki?curid=194690", "title": "Enzyme Commission number", "text": "Enzyme Commission number\n\nThe Enzyme Commission number (EC number) is a numerical classification scheme for enzymes, based on the chemical reactions they catalyze.\nAs a system of enzyme nomenclature, every EC number is associated with a recommended name for the respective enzyme.\n\nStrictly speaking, EC numbers do not specify enzymes, but enzyme-catalyzed reactions. If different enzymes (for instance from different organisms) catalyze the same reaction, then they receive the same EC number. Furthermore, through convergent evolution, completely different protein folds can catalyze an identical reaction and therefore would be assigned an identical EC number (these are called non-homologous isofunctional enzymes, or NISE). By contrast, UniProt identifiers uniquely specify a protein by its amino acid sequence.\n\nEvery enzyme code consists of the letters \"EC\" followed by four numbers separated by periods. Those numbers represent a progressively finer classification of the enzyme. Preliminary EC numbers exist and have an 'n' as part of the fourth (serial) digit (e.g. EC 3.5.1.n3).\n\nFor example, the tripeptide aminopeptidases have the code \"EC 3.4.11.4\", whose components indicate the following groups of enzymes:\n\n\nSimilarity between enzymatic reactions (EC) can be calculated by using bond changes, reaction centres or substructure metrics (EC-BLAST).\n\nThe enzyme nomenclature scheme was developed starting in 1955, when the International Congress of Biochemistry in Brussels set up an Enzyme Commission. The first version was published in 1961. The current sixth edition, published by the International Union of Biochemistry and Molecular Biology in 1992, contains 3196 different enzymes. Supplements 1-4 were published 1993-1999. Subsequent supplements have been published electronically, at the website of the Nomenclature Committee of the International Union of Biochemistry and Molecular Biology. In August 2018, the Enzyme Commission reorganized the nomenclature by adding the top-level EC 7 category describing translocases.\n\n\n"}
{"id": "1630130", "url": "https://en.wikipedia.org/wiki?curid=1630130", "title": "Floyd's Bluff", "text": "Floyd's Bluff\n\nFloyd's Bluff is a hill in southern Sioux City, Iowa that is named for Sergeant Charles Floyd.\n\nFloyd, who was the quartermaster for the Lewis and Clark Expedition's Corps of Discovery, was the only fatality during the expedition. The bluff was Floyd's original burial site in 1804, and is now the location of a National Historic Landmark in his honor. The Floyd Monument is located above the east bank of the Missouri River, just downstream from the mouth of the Floyd River. The bluff itself is part of the Loess Hills formation.\n\nThe Floyd's Bluff area was settled in 1848 by William Thompson, a recent veteran of the Mexican–American War. He established a trading post, and registered the name \"Floyd's Bluff\" as the name for his city, which never materialized. Instead, settlers upriver between the mouths of the Floyd and Big Sioux rivers were successful in establishing and incorporating Sioux City in the 1850s. Floyd's Bluff later became part of Sioux City with the development of the suburb of Morningside in the 1870s.\n\n\n"}
{"id": "10761673", "url": "https://en.wikipedia.org/wiki?curid=10761673", "title": "Fragmentation (sociology)", "text": "Fragmentation (sociology)\n\nIn urban sociology, fragmentation refers to the absence or the underdevelopment of connections between the society and the groupings of some members of that society on the lines of a common culture, nationality, race, language, occupation, religion, income level, or other common interests. This gap between the concerned group and the rest might be social, indicating poor interrelationships among each other; economical based on structural inequalities; institutional in terms of formal and specific political, occupational, educative or associative organizations and/or geographic implying regional or residential concentration. bell hooks coined the term when addressing the problem of 'hierarchy of oppression' within the feminist movement; where some felt experiencing more types of oppression gave greater validity to one's opinion and, therefore undermined group strength and solidarity within the movement as much as non-intersectional identity did in the 1970s [where female identity was seen predominantly through the lens of white, middle-class women and didn't take into consideration that identity could be made up of many more cultural influences such as race, gender, sexuality, spirituality etc. all intersecting across points of privilege and oppression]. hooks argued for greater inclusivity, mutual support and an understanding of various types of feminism within the movement; each sharing the same equity goals, yet having different ideas on the methods to achieve such goals.\n"}
{"id": "47470974", "url": "https://en.wikipedia.org/wiki?curid=47470974", "title": "Glacial refugium", "text": "Glacial refugium\n\nA glacial refugium (\"plural refugia\") is a geographic region which made possible the survival of flora and fauna in times of ice ages and allowed a post-glacial re-colonization. Different types of glacial refugia can be distinguished, namely nunatak, peripheral and lowland refugia. Glacial refugia have been suggested as a major cause of the distributions of flora and fauna in both temperate and tropical latitudes. However, in spite of the continuing use of historical refugia to explain modern-day species distributions, especially in birds, doubt has been cast on the validity of such inferences, as much of the differentiation between populations observed today may have occurred before or after their restriction to refugia.\n\n"}
{"id": "4864036", "url": "https://en.wikipedia.org/wiki?curid=4864036", "title": "Grand supercycle", "text": "Grand supercycle\n\nA grand supercycle is the longest period, or \"wave\", in the growth of a financial market as described by the Elliott wave principle, originally discovered and formulated by Ralph Nelson Elliott. Elliott speculated that a grand supercycle advance had started in the United States stock market in 1857 and ran to the year 1928, but acknowledged another interpretation that it may have been the third or even the fifth grand supercycle wave. However, these assignments have been reevaluated and clarified using larger historical financial data sets in the works of A. J. Frost and R.R. Prechter, and the start is now considered to be 1789, when stock market data began to be recorded.\n\nLike all Elliott waves, grand supercycle waves are subdivided into smaller generations of waves. The next smaller generation of waves are those of Supercycle degree. Modern applications of the Wave Principle also describe waves of larger degree spanning millennial periods of time.\n\nModern application of Elliott wave theory posits that a grand supercycle wave five is completing in the 21st century and should be followed by a corrective price pattern of decline that will represent the largest economic recession since the 1700s.\n\nIn technical analysis, grand supercycles and supercycles are often compared to the Kondratiev wave, which is a cycle of 50 to 60 years, but these are in detail distinct concepts.\n\nSome Elliott wave analysts believe that a grand supercycle bear market in US and European stocks started in 1987. When that was proven incorrect it was later revised to be 2000 and then 2006.\n\nDuring 2006–2007 the Dow Jones Industrial Average reached a new all-time high, which has been interpreted by some Elliott Wave analysts as indicating that 2000–2002 was not the beginning of a Grand Supercycle bear market. However, as this new high was merely a nominal new high in US dollars, and not a new high when measured in ounces of gold other Elliott Wave analysts believe this new high to be 'phony'.\n\nA controversial issue is whether the severe economic recession accompanying the termination of the current grand supercycle will take the form of either a deflationary depression or a hyperinflationary period. Robert Prechter has repeatedly stated that the collapse will take the form of a deflationary depression probably followed by hyperinflation. This is made clear in the following quotes from October 2006:\n\nJIM: If you were to make your case for deflation right now, what would be the key factors supporting that view?\n\nBOB: The credit bubble: the fact that we do not have currency inflation as much as we have credit inflation. And credit bubbles have always imploded. The amount of dollars out there that are greenbacks – actual cash – is compared to the dollar value of credit instruments. So in my view the Fed is utterly powerless to prevent the ultimate deflation of the credit bubble. And some people say, \"Well, they can print money.\" Fine, that would just make the credit bubble collapse faster as soon as bond holders realize that's what they were doing. There's no way out of it. So that's the argument.\n\nBOB: Well, the hyperinflation part is a pure guess based on politics. It has nothing to do with reading markets. I think the markets are telegraphing deflation, and I'm very confident about that. Hyperinflation to me is going to be the natural political response. I mean these people in Congress are so irresponsible – except to themselves and their families, of course. They always get reelected so they're doing that correctly. I mean, it's working for them as individuals but it's not working for the country. Anyway, to save their own skins I think the most likely thing is that they will turn to the Treasury, whether they keep the Federal Reserve System or not, and say, \"Let's print, let's get the machines going and print those greenbacks and spread them around.\"\nMany controversies surround the concept of the grand supercycle:\n\n\n\n"}
{"id": "66267", "url": "https://en.wikipedia.org/wiki?curid=66267", "title": "Homesteading the Noosphere", "text": "Homesteading the Noosphere\n\n\"Homesteading the Noosphere\" (abbreviated HtN) is an essay written by Eric S. Raymond about the social workings of open-source software development. It follows his previous piece \"The Cathedral and the Bazaar\" (1997).\n\nThe essay examines issues of project ownership and transfer, as well as investigating possible anthropological roots of the gift culture in open source as contrasted with the exchange culture of closed source software. Raymond also investigates the nature of the spread of open source into the untamed frontier of ideas he terms the noosphere, postulating that projects that range too far ahead of their time fail because they are too far out in the wilderness, and that successful projects tend to relate to existing projects.\n\nRaymond delves into the contrast between the stated aims of open source and observed behaviors, and also explores the underlying motivations of people involved in the open source movement. He seems to settle on the idea that open-source practitioners find striving for a great reputation within the \"tribe\" a key motivational feature.\n\n\"Homesteading the Noosphere\" has been referenced in various papers, including:\n\n\n\n\n"}
{"id": "13126988", "url": "https://en.wikipedia.org/wiki?curid=13126988", "title": "Job plot", "text": "Job plot\n\nA Job plot, otherwise known as the method of continuous variation or Job's method, is a method used in analytical chemistry to determine the stoichiometry of a binding event. The method is named after Paul Job and is also used in instrumental analysis and advanced chemical equilibrium texts and research articles. Job first published his method in 1928, while studying the associations of ions in solution. By plotting the UV absorbance of a solution of against the mole fraction of , he produced a graph which provided information about the equilibrium complexes present in solution.\n\nIn solutions where two species are present (i.e. species A and species B), one species (A) may bind to the other species (B). In some cases, more than one A will bind with a single B. One way to determine the amount of A binding to B is by using a Job plot.\n\nIn this method, the total molar concentration of the two binding partners (e.g. a protein and ligand or a metal and a ligand) are held constant, but their mole fractions are varied. An observable that is proportional to complex formation (such as absorption signal or enzymatic activity) is plotted against the mole fractions of these two components.\n\nχ is the mole fraction of compound A and P is the physical property being measured to understand complex formation. This property is most oftentimes UV absorbance.\n\nThe maximum (or minimum) on the plot corresponds to the stoichiometry of the two species if sufficiently high concentrations are used. The plot also provides insight to understand the equilibrium constant (K) of complex formation. A greater curvature leads to a more evenly distributed equilibrium, while a more triangle-shaped plot signifies a large K. Further, after determining the equilibrium constant, we can determine what complexes (ratio of A and B) are present in solution. In addition, the peak of the Job Plot corresponds to the mole fraction of ligands bound to a molecule, which is important for studying ligand field theory. An early work of I. Ostromisslensky describes essentially this approach.\n\nThere are several conditions that must be met in order for Job's method to be applicable. Firstly, the property being studied must vary in direct proportion to the concentration of the species. In the case of UV-visible spectroscopy, for example, this means that the system must conform to the Beer-Lambert law. In addition, the total concentration of the two binding partners, the pH and ionic strength of the solution must all be maintained at fixed values throughout the experiment.\n\nFinally, there must only be only one complex in solution which predominates over all others under the conditions of the experiment. This requirement means that only systems with high association constants, or systems in which only one stoichiometry can form, are suitable for analysis by Job plot. As such, the use of the Job plot in supramolecular chemistry has been advised against.\n"}
{"id": "3676111", "url": "https://en.wikipedia.org/wiki?curid=3676111", "title": "List of Lepidoptera that feed on cotton plants", "text": "List of Lepidoptera that feed on cotton plants\n\nCotton plants (\"Gossypium\" species) are used as food plants by the larvae of a number of Lepidoptera species including:\n\n\n\n\n\n"}
{"id": "36741888", "url": "https://en.wikipedia.org/wiki?curid=36741888", "title": "List of Shuttle Carrier Aircraft flights", "text": "List of Shuttle Carrier Aircraft flights\n\nShuttle Carrier Aircraft ferry flights generally originated at the Edwards Air Force Base in California or on rare occasions White Sands Missile Range in New Mexico following missions which land there, especially in the early days of the Space Shuttle program or when weather at the Shuttle Landing Facility (SLF) at the Kennedy Space Center prevents ending missions there. Flights generally ended at the SLF. A number of flights began at the Dryden Flight Research Center following delivery of the orbiter from Rockwell International to NASA from the nearby facilities in Palmdale, California.\n\n\n\nSpace shuttle orbiters were constructed in Palmdale, California and transported overland to the Dryden Flight Research Center (DFRC), a distance of 36 miles. The shuttle carrier aircraft was not used for this initial leg of the journey but was used to transport the orbiters for launch at the Kennedy Space Center in Florida. Additionally the orbiters are routinely transported from the Shuttle Landing Facility to the Orbiter Processing Facility after landing at the Kennedy Space Center either after missions or after removal from the SCA. \n\n"}
{"id": "57478411", "url": "https://en.wikipedia.org/wiki?curid=57478411", "title": "List of U.S. Virgin Islands territorial symbols", "text": "List of U.S. Virgin Islands territorial symbols\n\nThis is a list of U.S. Virgin Islands territorial symbols:\n"}
{"id": "4650823", "url": "https://en.wikipedia.org/wiki?curid=4650823", "title": "List of computer systems from Serbia", "text": "List of computer systems from Serbia\n\nThis is the list of computer systems from Serbia. See History of computer hardware in Serbia for more information.\n\n"}
{"id": "54899959", "url": "https://en.wikipedia.org/wiki?curid=54899959", "title": "List of craters in the Solar System", "text": "List of craters in the Solar System\n\nThis is a list of named craters in the Solar System as named by IAU's Working Group for Planetary System Nomenclature. As of 2017, there is a total of 5,223 craters on 40 astronomical bodies, which includes minor planets (asteroids and dwarf planets), planets, and natural satellites. All geological features of a body (including craters) are typically named after a specific theme. For completeness, the list also refers to the craters on , which naming process is not overseen by IAU's WGPSN.\n\n"}
{"id": "13856857", "url": "https://en.wikipedia.org/wiki?curid=13856857", "title": "List of engineering branches", "text": "List of engineering branches\n\nEngineering is the discipline and profession that applies scientific theories, mathematical methods, and empirical evidence to design, create, and analyze technological solutions cognizant of safety, human factors, physical laws, regulations, practicality, and cost. In the contemporary era, engineering is generally considered to consist of the major primary branches of chemical engineering, civil engineering, electrical engineering, and mechanical engineering. There are numerous other engineering subdisciplines and interdisciplinary subjects that may or may not be part of these major engineering branches.\n\nChemical engineering is the application of chemical, physical and biological sciences to the process of converting raw materials or chemicals into more useful or valuable forms.\n\nCivil engineering comprises the design, construction and maintenance of the physical and natural built environments.\n\nElectrical engineering comprises the study and application of electricity, electronics and electromagnetism.\nMechanical engineering comprises the design and analysis of heat and mechanical power for the operation of machines and mechanical systems.\n"}
{"id": "318374", "url": "https://en.wikipedia.org/wiki?curid=318374", "title": "List of experiments", "text": "List of experiments\n\nThe following is a list of historically important scientific experiments and observations demonstrating something of great scientific interest, typically in an elegant or clever manner.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "39563465", "url": "https://en.wikipedia.org/wiki?curid=39563465", "title": "List of open-source software for mathematics", "text": "List of open-source software for mathematics\n\nThis is a list of open-source software to be used for high-order mathematical calculations. This software has played an important role in the field of mathematics. Open-source software in mathematics has become pivotal in education because of the high cost of textbooks.\n\nA computer algebra system is a type of software set that is used in manipulation of mathematical formulae. The principal objective of a computer algebra system is to systematize monotonous and sometimes problematic algebraic manipulation tasks. The primary difference between a computer algebra system and a traditional calculator is the ability to deal with equations symbolically rather than numerically. The precise uses and capabilities of these systems differ greatly from one system to another, yet their purpose remains the same: manipulation of symbolic equations. Computer algebra system often include facilities for graphing equations and provide a programming language for the users' own procedures.\n\nAxiom is a general-purpose computer algebra system. It has been in development since 1971 by IBM, originally named \"scratchpad\". Richard Jenks originally headed it but over the years Barry Trager who then shaped the direction of the scratchpad project took the project. Project scratchpad was eventually sold to a numerical group called Numerical Algorithms Group (NAG) and was renamed Axiom. After a failure to launch as a product, NAG decided to release it as a free software in 2001 with more than 300 man-years worth of research involved. Axiom is licensed under a Modified BSD license.\n\nThis free software had an earlier incarnation, Macsyma. Developed by Massachusetts Institute of Technology in the 1960s, it was maintained by William Schelter from 1982 to 2001. In 1998, Schelter obtained the permission to release MAXIMA as an open-source software under the GNU General Public license. Subsequently, he released the source code to the world later that year. Since his passing in 2001, a group of MAXIMA enthusiasts have continued to provide technical support.\n\nGAP was initiated by RWTH Aachen university in 1986. This was the case until in 1997 when they decided to co-develop GAP further with CIRCA (Centre for Research in Computational Algebra). Unlike MAXIMA and Axiom, GAP is a system for computational discrete algebra with particular emphasis on computational group theory. In March 2005 the GAP Council and the GAP developers have agreed that status and responsibilities of \"GAP Headquarters\" should be passed to an equal collaboration of a number of \"GAP Centres\", where there is permanent staff involvement and an element of collective or organizational commitment, while fully recognizing the vital contributions of many individuals outside those centers.\n\nA Computer Algebra System designed for the solution of problems in field theory. An unpublished computational program written in pascal called Abra inspired this open-source software. \"Abra\" was originally designed for physicists to compute problems present in quantum mechanics. Kespers Peeters then decided to write a similar program in C computing language rather in pascal, which he renamed Cadabra. However, Cadabra has been expanded for a wider range of uses, it is no longer restricted to physicists.\n\nCoCoA (COmputations in COmmutative Algebra) is open-source software used for computing multivariate polynomials and initiated in 1987. Originally written in Pascal, CoCoA was later translated into C language.\n\nXcas/Giac is an open-source project developed at the Joseph Fourier University of Grenoble since 2000. Written in C++ language, maintained by Bernard Parisse's et al.and available for Windows, Mac, GNU/Linux and many others platforms. It has a compatibility mode with Maple, Derive and MuPAD software and TI-89, TI-92 and Voyage 200 calculators. The system was chosen by Hewlett-Packard as the CAS for their HP Prime calculator, which utilizes the Giac/Xcas 1.1.2 engine under a dual-license scheme.\n\nPARI/GP is a computer algebra system that facilitates number-theory computation. Besides support of factoring, algebraic number theory, and analysis of elliptic curves, it works with mathematical objects like matrices, polynomials, power series, algebraic numbers, and transcendental functions. Originally developed by Henri Cohen et al at Université Bordeaux I, France, it now is GPL software. The gp interactive shell allows GP-language scripting; the gp2c compiler compiles GP scripts into C; and the PARI C library allows C programs to use PARI/GP functions.\n\nSympy is a computer algebra system written in Python.\n\nNumerical analysis is an area of mathematics that creates and analyzes algorithms for obtaining numerical approximations to problems involving continuous variables. When an arbitrary function does not have a closed form as its solution, there would not be any analytical tools present to evaluate the desired solutions, hence an approximation method is employed instead.\n\nModelica is an object-oriented, declarative, multi-domain modeling language for component-oriented modeling of complex systems including algebraic and differential equations. OpenModelica and Jmodelica are some of the opensource implementations of the language.\n\nOctave (aka GNU Octave) is an alternative to MATLAB. Originally conceived in 1988 by John W. Eaton as a companion software for an undergraduate textbook, Eaton later opted to modify it into a more flexible tool. Development begun in 1992 and the alpha version was released in 1993. Subsequently, version 1.0 was released a year after that in 1994. Octave is a high level language with the primary intention in numerical computation.\n\nInspired by MATLAB, Scilab was initiated in the mid-1980s at the INRIA (French national Institute for computer science and control). François Delebecque and Serge Steer developed it and it was released by INRIA in 1994 as an open-source software. Since 2008, Scilab has been distributed under the CeCILL license, which is GPL compatible. In 2010, Scilab Enterprise was founded to provide even more support to the software.\n\nFreeMat is an alternative to MATLAB.\n\nSciPy is a python programming language library to take advantage of Python's ability to handle large data sets.\n\nGnuplot in an open-source graphing program and has extensive graphing features, but it also has least squares fitting capabilities for a broad range of user-defined functions in two and three dimensions. http://www.gnuplot.info/\n\nGraph is an open-source 2D graphing program with some simple numerical analysis features including least squares fitting for a broad range of functions, computing derivatives, and computing integrals. https://www.padowan.dk/\n\nZeGrapher is an open-source 2D plotting software. It can plot functions (also their derivatives and primitives), parametric equations (can be animated), sequences and 2D experimental data (to fill in by hand or from a CSV. Data can be fitted with polynomials).\nhttp://en.zegrapher.com/\n\nAMoreAccurateFourierTransform is an open-source program for computing Fourier transforms. It computes a discrete Fourier transform from a two column input file over a specified range of input and output values and writes the results to an output file. https://sourceforge.net/projects/amoreaccuratefouriertransform/ \n\nStatistics is the study of how to collate and interpret numerical information from data. It is the science of learning from data and communicating uncertainty. There are two branches in statistics: ‘Descriptive statistics’’ and ‘’ Inferential statistics\n\nDescriptive statistics involves methods of organizing, picturing and summarizing information from data. Inferential statistics involves methods of using information from a sample to draw conclusions about the Population.\n\nPSPP and JASP are open source software competitors to SPSS, widely used for statistical analysis of sampled data. PSPP is maintained by the GNU project. \n\nR is both a language and software used for statistical computing and graphing. R was originally developed by Bell Laboratories (Currently known as Lucent Technologies) by John Chambers. Since R is largely written in C language, users can use C or C++ commands to manipulate R-objects directly. Also, R runs on most UNIX platforms. R is currently part of the \"Free Software Foundation\" GNU project.\n\nDemetra is a program for seasonal adjustments that was developed and published by Eurostat – European Commission under the EUPL license.\n\nSuch software were created with the original intent of providing a math platform that can be compared to proprietary software such as \"MATLAB\" and \"MATHEMATICA\". They contain multiple other free software and hence have more features than the rest of the software mentioned.\n\nSageMath is designed partially as a free alternative to the general-purpose mathematics products Maple and MATLAB. It can be downloaded or used through a web site. SageMath comprises a variety of other free packages, with a common interface and language.\n\nSageMath was initiated by William Stein, of Harvard University in 2005 for his personal project in Number Theory. It was originally known as ‘’HECKE and Manin’’. After a short while it was renamed SAGE, which stands for ‘’Software of Algebra and Geometry Experimentation’’. Sage 0.1 was released in 2005 and almost a year later Sage 1.0 was released. It already consisted of Pari, GAP, Singular and MAXIMA with an interface that rivals that of Mathematica.\n\nDataMelt program was initially written by S. Chekanov in 2005 for High-energy physics. \nThe original name was JHepWork, but later it was renamed to SCaViS and then to DataMelt.\nThe main idea was to build a multi-purpose computational environment that is fully multiplatform and combine the best open-source Java libraries.\nDataMelt uses high-level programming languages, such as Jython (Python implemented in Java), Groovy, JRuby, but Java coding can also be used to call numerical and graphical libraries.\nThe main source of the documentation are the books and but the online manual is also available.\n\nMathBuntu is a script for installing a large collection of mathematics-related software and textbooks for Ubuntu or Kubuntu operating systems.\n"}
{"id": "7425653", "url": "https://en.wikipedia.org/wiki?curid=7425653", "title": "MARIACHI", "text": "MARIACHI\n\nMARIACHI, the Mixed Apparatus for Radar Investigation of Cosmic-rays of High Ionization, is an apparatus for the detection of ultra-high-energy cosmic rays (UHECR) via bi-static radar interferometry using VHF transmitters.\nMARIACHI is also the name of the research project created and directed by Brookhaven National Laboratory (BNL) on Long Island, New York, initially intended to verify the concept that VHF signals can be reflected off the ionization patch produced by a cosmic ray shower. Project emphasis subsequently shifted to the attempted detection of radio wave reflections from a high energy ionization beam apparatus located at BNL's NASA Space Radiation Laboratory.\n\nIts inventors hope the MARIACHI apparatus will detect UHECR over much larger areas than previously possible, and that it will also detect ultra-high-energy neutrino flux. The ground array detectors are scintillator arrays that are built and operated by high school students and teachers.\n\n\n"}
{"id": "27372985", "url": "https://en.wikipedia.org/wiki?curid=27372985", "title": "Mass action law (electronics)", "text": "Mass action law (electronics)\n\nUnder thermal equilibrium the product of the free electron concentration formula_1 and the free hole concentration formula_2 is equal to a constant square of intrinsic carrier concentration formula_3. The intrinsic carrier concentration is a function of temperature.\n\nThe equation for the mass action law for semiconductors is:\n\nIn semiconductors, free electrons and holes are the carriers that provide conduction. For cases where the number of carriers are much less than the number of band states, the carrier concentrations can be approximated by using Boltzmann statistics, giving the results below.\n\nThe free electron concentration \"n\" can be approximated by\nwhere \n\nThe free hole concentration \"p\" is given by a similar formula\nwhere \n\nUsing the carrier concentration equations given above, the mass action law can be stated as\nwhere \"E\" is the bandgap energy given by \"E\" = \"E\" − \"E\"\n\n\n"}
{"id": "36405951", "url": "https://en.wikipedia.org/wiki?curid=36405951", "title": "Medic to Medic", "text": "Medic to Medic\n\nMedic to Medic (also known as M2M, Medic 2 Medic) is a programme of the International Medical Education Trust 2000 (IMET 2000), a UK charity that provides medical education worldwide.\n\nFounded by Dr. Kate Mandeville, it became affiliated with IMET 2000 to address the shortage of in-country health care workers, as well as a way of helping students who risked dropping out of medical school because of extreme poverty. Since its inception in 2007, it has projects in Malawi and in Uganda.\n\nMedic to Medic aims to alleviate students from financial hardship so as to allow them to complete their studies. Given the number of professionals who migrate in search of better career prospects, Medic to Medic also aims to boost numbers of in-country health workers. The programme links students to private donors, akin to other well-known child sponsor models. E-mentoring from UK professionals is also part of the programme. \n\nIn Malawi, the programme supports students training to be doctors and allied health professionals at Malawi College of Medicine and Malawi College of Health Sciences. In Uganda, it works with Makerere University and Gulu University to support the most needy students, especially those from Northern Uganda.\n\nMedical students at some UK universities are involved in fundraising activities, sponsorship and mentoring support in the field over the summer.\n\n"}
{"id": "6321284", "url": "https://en.wikipedia.org/wiki?curid=6321284", "title": "Molecularium Project", "text": "Molecularium Project\n\nThe Molecularium Project is an informal science education project of Rensselaer Polytechnic Institute. The Molecularium Project introduces young audiences to the world of atoms and molecules using character driven stories, immersive animation, interactive games and activities, and state of the art molecular visualizations. Rensselaer's three principal Scientist / Educators behind the project are Dr. Linda S. Schadler, Dr. Richard W. Siegel, and Dr. Shekhar Garde. The Molecularium Project began as an outreach project of Rensselaer's Nanoscale Science and Engineering Center. To realize the productions, the scientists employed the creative team Nanotoon Entertainment, led by writer/director V. Owen Bush, and writer/producer Kurt Przybilla. The Molecularium Project is funded by Rensselaer, the National Science Foundation, and New York State.\n\nIn 2002, Dr. Schadler and Dr. Garde produced a seven-minute pilot show for the local planetarium called “Molecularium” for the Digistar II Planetarium system. It introduces children to the concepts of atoms and molecules from small molecules like H2O to larger molecules like polymers.\n\nIn early 2004, Schadler, Garde, and Siegel were awarded a U.S. National Science Foundation grant to make a new Molecularium show exclusively for the fulldome medum. They recruited the filmmaker and experience designer V. Owen Bush to bring the idea to life. Bush founded the production company Nanotoon Entertainment with writer/producer Kurt Przybilla to realize the new project. Bush and Przybilla proposed an adventure story of personified atoms flying a ship called the Molecularium through nanoscale materials including a snowflake, a penny, a stick of gum and the human body.\n\nIn February 2005, the team debuted \"Molecularium - Riding Snowflakes\" a 23-minute digital planetarium show at the Children's Museum of Science and Technology. In 2005, \"Molecularium - Riding Snowflakes\" won the Domie at Domefest in Albuquerque New Mexico. \"Molecularium - Riding Snowflakes\" has shown at Chabot Space and Science Center in Oakland Ca., the Newark Museum Planetarium in Newark, NJ, Dubai Children’s City, UAE, and Thinktank, Birmingham, UK, among many other digital planetariums. It has been translated and versioned in Arabic, Korean and Turkish. It is Distributed by E&S, Spitz, Sky-Skan, and e-Planetarium.\n\nIn 2010, the American Library Association (ALA) selected the Molecularium Kid's Site for inclusion to its Great Websites for Kids.\n\nMolecules to the MAX! is a 41-minute fully animated 3D IMAX film for the Giant Screen. The film re-imagines the characters and story developed for \"Molecularium- Riding Snowflakes\" for an older audience and a different medium. The film's simulations and rendering were partially computed at the Computational Center for Nanotechnology Innovations. The film was produced by Nanotoon Entertainment and Developed at Rensselaer, with a gift from Curtis R. Priem, co-founder of Nvidia corporation.\n\nThe digital version of the film premiered at EMPAC, in Troy, NY on Feb. 27, 2009. The IMAX version premiered at the Giant Screen Cinema Association International Conference and Trade Show in Indianapolis, Indiana, on September 22, 2009. The IMAX 3D Premiere was at the GSCA Film Expo in Los Angeles on Feb. 24, 2010. It is available in 2D & 3D for 15/70 and 8/70 large format film and in digital 3D. The film has been composed with Omnimax / IMAX Domes in mind. Molecules to the MAX! was nominated for Best Film Produced for the Giant Screen, Best Film for Lifelong Learning and Best Sound Design at the 2010 GSCA’s Achievement Awards. Molecules to the MAX! has shown at the National Museum of Natural Science (Taichung, Taiwan) Maloka Interactive Museum (Bogata, Columbia), The Scientific Center (Salmiya, Kuwait), McWane Science Center (Birmingham, Alabama) Proctor's Theatre (Schenectady, New York) among others. It has been translated and versioned in Spanish, Chinese, Japanese and Arabic. It is distributed to Giant Screen theaters by SK Films.\n\nIn the spring of 2012, the Molecularium Project launched NanoSpace, an online molecular theme park. Visitors to NanoSpace learn scientific concepts with games, activities and movies. Areas within Nanospace include the Hall of Atoms and Molecules, H2O park (the water cycle), Sizes in the Universe (scale and scientific notation), Material Boulevard (Materials Science), and DNA Land (Molecular Biology).\n"}
{"id": "30429825", "url": "https://en.wikipedia.org/wiki?curid=30429825", "title": "Montage (image software)", "text": "Montage (image software)\n\nMontage is a software toolkit used in astrophotography to assemble astronomical images in Flexible Image Transport System (FITS) format into composite images, called mosaics, that preserve the calibration and positional fidelity of the original input images. It won a NASA Space Act Award in 2006.\n\nMontage was developed to support scientific research. It enables astronomers to create images of regions of the sky that are too large to be produced by astronomical cameras. It also creates composite images of a region of the sky that has been measured with different wavelengths and with different instruments; the composite appears as if the area was measured with the same instrument on the same telescope. There is also associated software developed by the Montage user community including Bash and C shell scripts to make mosaics, and the montage-wrapper Python application programming interface, a part of the Astropy project.\n\nMontage uses a BSD 3-Clause License, which permits unlimited redistribution of Montage code for any purpose as long as its copyright notices and the license's disclaimers of warranty are included.\n\n"}
{"id": "49127301", "url": "https://en.wikipedia.org/wiki?curid=49127301", "title": "MuSIASEM", "text": "MuSIASEM\n\nMuSIASEM or Multi-Scale Integrated Analysis of Societal and Ecosystem Metabolism, is a method of accounting used to analyse socio-ecosystems and to simulate possible patterns of development. It is based on maintaining coherence across scales and different dimensions (e.g. economic, demographic, energetic) of quantitative assessments generated using different metrics. It is designed to detect and analyze patterns in the societal use of resources and the impacts they create on the environment. The approach was created around 1997 by Mario Giampietro and Kozo Mayumi, and has been developed since then by the members of the IASTE (Integrated Assessment: Sociology, Technology and the Environment) group at the Institute of Environmental Science and Technology of the \"Universitat Autònoma de Barcelona\" and its external collaborators. MuSIASEM strives to characterize metabolic patterns of Socio-Ecological Systems (how and why humans use resources and how this use depends on and affects the stability of the ecosystems embedding the society). This integrated approach allows for a quantitative implementation of the DPSIR framework (Drivers, Pressures, States, Impacts and Responses) and application as a decision support tool. Different alternatives of the option space can be checked in terms of feasibility (compatibility with processes outside human control), viability (compatibility with processes under human control) and desirability (compatibility with normative values and institutions). The ability to integrate quantitative assessments across dimensions and scales makes MuSIASEM particularly suited for different types of sustainability analysis: (i) the nexus between food, energy, water and land uses; (ii) urban metabolism; (iii) waste metabolism; (iv) tourism metabolism; (v) rural development.\n\nMuSIASEM accounting has been used for the integrated assessment of agricultural systems, biofuels \n, nuclear power\n, energetics, sustainability of water use, mining, urban waste management systems, and urban metabolism in developing countries. Moreover, the methodology has been applied to assess societal metabolism at the municipal, regional (rural Laos; Catalonia; China; Galapagos Islands; national,\n, and supernational scale. An application of MuSIASEM to the nexus between natural resources is in the book ‘Resource Accounting for Sustainability: The Nexus between Energy, Food, Water and Land Use’ This work has been tested in collaboration with FAO. The Ecuadorian National Secretariat for Development and Planning (SENPLADES) has included the MuSIASEM approach in the training of its personnel. Finally, several master courses about the application to the approach to energy system in various Southern African Universities have been elaborated under the Participia project. MuSIASEM has been applied to the analysis of Shanghai's urban metabolism.\n\n\n"}
{"id": "2285713", "url": "https://en.wikipedia.org/wiki?curid=2285713", "title": "Nie Haisheng", "text": "Nie Haisheng\n\nNie Haisheng (; born 13 October 1964) is a Chinese military pilot and CNSA astronaut.\n\nNie was born on 13 October 1964 in Yangdang town of Zaoyang, Hubei Province. After graduating from high school he joined the People's Liberation Army Air Force and became a fighter pilot. He trained at the PLAAF's No. 7 Flying School and graduated in 1987.\n\nOn 12 June 1989 while flying at 13,000 feet (4000 m) his plane suffered an explosion and he lost his engine. The plane began to spin to the ground and the cabin began to heat up. Trying to regain control, he waited until the plane was 1300 to 1700 feet (400 to 500 meters) before choosing to eject. For his handling of the situation he was honored with third-class merit.\n\nPrior to Shenzhou 10, Nie had attained the rank of Major General.\n\nIn 1998, he was selected for the Chinese spaceflight program and was one of three candidates who were part of the final group to train for the \"Shenzhou 5\" flight, China's first manned spaceflight. Yang Liwei was picked for the flight, with Zhai Zhigang ranked second ahead of Niè Hǎishèng.\n\nNie went into orbit, along with Fei Junlong (commander), as flight engineer of the \"Shenzhou 6\" flight on 12 October 2005. The mission lasted just under five days.\n\nHe was selected to be the commander of the backup crew for the Shenzhou 9 mission. In 2013, Nie was selected to command the Shenzhou 10 second manned space mission to the first Chinese space station Tiangong 1. He became the first officer hold general rank at the time of their launch in the Chinese program with the Shenzhou 10 mission.\n\nHe is married to Niè Jiélín (聂捷琳) and they have a daughter named Nie Tianxiang (聂天翔). During the \"Shenzhou 6\" mission he celebrated his 41st birthday in space.\n\nThe asteroid 9517 Niehaisheng was named after him.\n\n\n"}
{"id": "19555053", "url": "https://en.wikipedia.org/wiki?curid=19555053", "title": "Peter F. Stevens", "text": "Peter F. Stevens\n\nPeter Francis Stevens is a British botanist born in 1944.\n\nHe is a researcher at the Missouri Botanical Garden and professor of Biology of the University of Missouri–St. Louis. He is a member of the Angiosperm Phylogeny Group which created the APG, APG II, APG III, and APG IV systems.\n\nHe maintains a web site, APweb, hosted by the Missouri Botanical Garden, which has been regularly updated since 2001, and is a useful source for the latest research in angiosperm phylogeny which follows the APG approach.\n\nThe standard author abbreviation P.F.Stevens is used to indicate P. F. Stevens as the author when citing a botanical name. He has named dozens of species, mostly in the families Clusiaceae and Ericaceae, and he also described the genus \"Romnalda\" (Asparagaceae).\n\n"}
{"id": "15210243", "url": "https://en.wikipedia.org/wiki?curid=15210243", "title": "Peter Jacob Hjelm", "text": "Peter Jacob Hjelm\n\nPeter (Petter) Jacob Hjelm (2 October 1746 – 7 October 1813) was a Swedish chemist and the first person to isolate the element molybdenum in 1781, four years after its discovery.\nAfter his studies at the University of Uppsala he received his Ph.D. He became professor at the Mining academy and in 1782 he became head of the Royal Mint. From 1784 on he was a member of the Royal Swedish Academy of Sciences. His last position was at the laboratory of the Ministry of Mining.\n"}
{"id": "40624593", "url": "https://en.wikipedia.org/wiki?curid=40624593", "title": "Phil S. Baran", "text": "Phil S. Baran\n\nPhil S. Baran (born 1977) is a Professor in the Department of Chemistry at the Scripps Research Institute and Member of the Skaggs Institute for Chemical Biology. He received his B.S. in chemistry from New York University in 1997 and his Ph.D. from The Scripps Research Institute in 2001, under the supervision of K.C. Nicolaou. He did his postdoctoral fellowship in the laboratory of Nobel Laureate E. J. Corey at Harvard University.\n\nBaran has authored over 130 published scientific articles. He has several patents.\n\nHis work is focused on synthesizing complex organic compounds, the development of new reactions, and the development of new reagents.\n\n\n"}
{"id": "8138285", "url": "https://en.wikipedia.org/wiki?curid=8138285", "title": "Political Instability Task Force", "text": "Political Instability Task Force\n\nThe Political Instability Task Force (PITF), formerly known as State Failure Task Force, is a U.S. government-sponsored research project to build a database on major domestic political conflicts leading to state failures. The study analyzed factors to denote the effectiveness of state institutions, population well-being, and found that partial democracies with low involvement in international trade and with high infant mortality are most prone to revolutions. One of the members of the task force resigned on January 20, 2017 in protest of the Trump administration.\n\nThe project began as an unclassified study that was commissioned to a group of academics (particularly active was the Center for Global Policy at George Mason University) by the Central Intelligence Agency's Directorate of Intelligence in response to a request from senior U.S. policy makers. The State Failure Problem Set dataset and spreadsheets were originally prepared in 1994 by researchers at the Center for International Development and Conflict Management (CIDCM) at the University of Maryland under the direction of Ted Robert Gurr and subject to the review of the State Failure Task Force. The Problem Set was subsequently reviewed, revised, and updated on an annual basis through 1999 under the direction of Ted Gurr and, beginning in 1999, Monty G. Marshall at CIDCM. In January 2001, a major review and revision of the Problem Set coding guidelines and dataset, under the direction of Monty G. Marshall, was concluded that substantially altered the case identifications and case parameters recorded in the Problem Set.\n\nThe PITF first identified over 100 \"problem cases\" in the world from 1955 to 2011. Four distinct types of state failure events are included in the dataset: revolutionary wars, ethnic wars, adverse regime changes, and genocides and politicides. The Problem Set data includes the following information on each case: country, month and year of onset, month and year of ending (unless ongoing as of December 31 of the current update year), type of case, and annual codes on magnitude variables. The basic structure of the data is the \"case-year,\" that is, there is a separate case-entry for each additional year of a multi-year episode. Only the first annual record for each event contains a brief narrative description of the event.\n\nThe goal was to find factors associated with major political conflicts. Over 400 cases were analyzed, both for global and regional data sets.\n\nThe common variables listed in each data version are as follows: \n\nRevolutionary wars are episodes of violent conflict between governments and politically organized groups (political challengers) that seek to overthrow the central government, to replace its leaders, or to seize power in one region. Conflicts must include substantial use of violence by one or both parties to qualify as \"wars.\"\n\nEthnic wars are episodes of violent conflict between governments and national, ethnic, religious, or other communal minorities (ethnic challengers) in which the challengers seek major changes in their status. Most ethnic wars since 1955 have been guerrilla or civil wars in which the challengers have sought independence or regional autonomy. A few, like the events in South Africa's black townships in 1976-77, involve large-scale demonstrations and riots aimed at sweeping political reform that were violently suppressed by police and military. Rioting and warfare between rival communal groups is not coded as ethnic warfare unless it involves conflict over political power or government policy.\n\nAdditional variables specific to the Ethnic and Revolutionary War episodes are as follows:\n\nAdverse Regime Changes are defined by the State Failure Task Force as major, adverse shifts in patterns of governance, including\nAbrupt transitions from more authoritarian rule to more open, institutionalized governance systems, defined by the State Failure Task Force as \"democratic transitions,\" are not considered state failures in this sense and, thus, are not included.\n\nAdditional variables specific to the Adverse Regime Change episodes are as follows: \n\nGenocide and politicide events involve the promotion, execution, and/or implied consent of sustained policies by governing elites or their agents or in the case of civil war, either of the contending authorities that result in the deaths of a substantial portion of a communal group or politicized non communal group. In genocides the victimized groups are defined primarily in terms of their communal (ethnolinguistic, religious) characteristics. In politicides, by contrast, groups are defined primarily in terms of their political opposition to the regime and dominant groups.\n\nGenocide and politicide are distinguished from state repression and terror. In cases of state terror authorities arrest, persecute or execute a few members of a group in ways designed to terrorize the majority of the group into passivity or acquiescence. In the case of genocide and politicide authorities physically exterminate enough (not necessarily all) members of a target group so that it can no longer pose any conceivable threat to their rule or interests.\n\nAdditional variables specific to the Genocide/Politicide episodes are as follows:\n\nFairly consistent findings were produced, suggesting that there are three statistically significant variables most often associated with political upheavals:\nThus partial democracies with low involvement in international trade and with high infant mortality are most prone to revolutions.\n\nQuantitative models developed during the study would have accurately predicted over 85% of major state crises events occurring in 1990–1997. However while the models can predict state crises, they cannot predict their magnitude and eventual outcome.\n\nThere are four versions of the Political Instability (State Failure) Problem Set data; each are downloadable from the PITF Problem Set page in Microsoft Excel format. This format was chosen because it is readily importable to most spreadsheet and statistical software applications. The four versions are as follows: \n\n80 cases; 16 ongoing\n\n65 cases; 4 ongoing\n\n115 cases; 2 ongoing\n\n41 cases; 1 ongoing\n\nAverage magnitude over all years: 2.4\n\nGenocides/politicides occurred most frequently in 1975 and 1978. Frequency of occurrence increased beginning in 1962 until 1980, when frequency dropped slightly. The frequency of genocides/politicides plateaued from 1980 to 1987. In 1988 the world saw a slight increase of genocides/politicides, which then quickly turned into a steady decrease in events through 2011.\n\nAn example of a typical event: Uganda, 1980-1986. Average magnitude: 3.3.\n\n\n"}
{"id": "4536630", "url": "https://en.wikipedia.org/wiki?curid=4536630", "title": "Ptolemy Project", "text": "Ptolemy Project\n\nThe Ptolemy Project is an ongoing project aimed at modeling, simulating, and designing concurrent, real-time, embedded systems. The focus of the Ptolemy Project is on assembling concurrent components. The principal product of the project is the Ptolemy II model based design and simulation tool. The Ptolemy Project is conducted in the Industrial Cyber-Physical Systems Center (iCyPhy) in the Department of Electrical Engineering and Computer Sciences of the University of California at Berkeley, and is directed by Prof. Edward A. Lee.\n\nThe key underlying principle in the project is the use of well-defined models of computation that govern the interaction between components. A major problem area being addressed is the use of heterogeneous mixtures of models of computation.\n\nThe project is named after Claudius Ptolemaeus, the 2nd century Greek astronomer, mathematician, and geographer.\n\nThe Kepler Project, a community-driven collaboration among researchers at three other University of California campuses has created the Kepler scientific workflow system which is based on Ptolemy II.\n\n"}
{"id": "5235231", "url": "https://en.wikipedia.org/wiki?curid=5235231", "title": "Railway engineering", "text": "Railway engineering\n\nRailway engineering is a multi-faceted engineering discipline dealing with the design, construction and operation of all types of rail transport systems. It encompasses a wide range of engineering disciplines, including civil engineering, computer engineering, electrical engineering, mechanical engineering, industrial engineering and production engineering. A great many other engineering sub-disciplines are also called upon.\n\nWith the advent of the railways in the early nineteenth century, a need arose for a specialized group of engineers capable of dealing with the unique problems associated with railway engineering. As the railways expanded and became a major economic force, a great many engineers became involved in the field, probably the most notable in Britain being Richard Trevithick, George Stephenson and Isambard Kingdom shembud. Today, railway systems engineering continues to be a vibrant field of engineering. E.Shendya a civil engineer(now retired) introduced the Metro Railway System in India. He is also known as 'the Siddharth kashikar of India\n\n\n\"In the UK:\" The Railway Division of the Institution of Mechanical Engineers (IMechE).\n\n\"In the US\" The American Railway Engineering and Maintenance-of-Way Association (AREMA)\n\n\"Worldwide\" The Institute of Railway Signal Engineers (IRSE)\n\n"}
{"id": "3843998", "url": "https://en.wikipedia.org/wiki?curid=3843998", "title": "Scalar–tensor–vector gravity", "text": "Scalar–tensor–vector gravity\n\nScalar–tensor–vector gravity (STVG) is a modified theory of gravity developed by John Moffat, a researcher at the Perimeter Institute for Theoretical Physics in Waterloo, Ontario. The theory is also often referred to by the acronym MOG (\"MO\"dified \"G\"ravity).\n\nScalar–tensor–vector gravity theory, also known as MOdified Gravity (MOG), is based on an action principle and postulates the existence of a vector field, while elevating the three constants of the theory to scalar fields. In the weak-field approximation, STVG produces a Yukawa-like modification of the gravitational force due to a point source. Intuitively, this result can be described as follows: far from a source gravity is stronger than the Newtonian prediction, but at shorter distances, it is counteracted by a repulsive fifth force due to the vector field.\n\nSTVG has been used successfully to explain galaxy rotation curves, the mass profiles of galaxy clusters, gravitational lensing in the Bullet Cluster, and cosmological observations without the need for dark matter. On a smaller scale, in the Solar System, STVG predicts no observable deviation from general relativity. The theory may also offer an explanation for the origin of inertia.\n\nSTVG is formulated using the action principle. In the following discussion, a metric signature of formula_1 will be used; the speed of light is set to formula_2, and we are using the following definition for the Ricci tensor:\n\nWe begin with the Einstein-Hilbert Lagrangian:\n\nwhere formula_5 is the trace of the Ricci tensor, formula_6 is the gravitational constant, formula_7 is the determinant of the metric tensor formula_8, while formula_9 is the cosmological constant.\n\nWe introduce the Maxwell-Proca Lagrangian for the STVG vector field formula_10:\n\nwhere formula_12 is the mass of the vector field, formula_13 characterizes the strength of the coupling between the fifth force and matter, and formula_14 is a self-interaction potential.\n\nThe three constants of the theory, formula_15 and formula_16 are promoted to scalar fields by introducing associated kinetic and potential terms in the Lagrangian density:\n\nwhere formula_18 denotes covariant differentiation with respect to the metric formula_19 while formula_20 and formula_21 are the self-interaction potentials associated with the scalar fields.\n\nThe STVG action integral takes the form\n\nwhere formula_23 is the ordinary matter Lagrangian density.\n\nThe field equations of STVG can be developed from the action integral using the variational principle. First a test particle Lagrangian is postulated in the form\n\nwhere formula_25 is the test particle mass, formula_26 is a factor representing the nonlinearity of the theory, formula_27 is the test particle's fifth-force charge, and formula_28 is its four-velocity. Assuming that the fifth-force charge is proportional to mass, i.e., formula_29 the value of formula_30 is determined and the following equation of motion is obtained in the spherically symmetric, static gravitational field of a point mass of mass formula_31:\n\nwhere formula_33 is Newton's constant of gravitation. Further study of the field equations allows a determination of formula_26 and formula_35 for a point gravitational source of mass formula_31 in the form\n\nwhere formula_39 is determined from cosmological observations, while for the constants formula_40 and formula_41 galaxy rotation curves yield the following values:\n\nwhere formula_44 is the mass of the Sun. These results form the basis of a series of calculations that are used to confront the theory with observation.\n\nSTVG/MOG has been applied successfully to a range of astronomical, astrophysical, and cosmological phenomena.\n\nOn the scale of the Solar System, the theory predicts no deviation from the results of Newton and Einstein. This is also true for star clusters containing no more than a maximum of a few million solar masses.\n\nThe theory accounts for the rotation curves of spiral galaxies, correctly reproducing the Tully-Fisher law.\n\nSTVG is in good agreement with the mass profiles of galaxy clusters.\n\nSTVG can also account for key cosmological observations, including:\n\nThe weak field limit of this theory predicts an enhanced gravitational attraction on the boundaries of galaxies, where phenomena related to dark matter use to happen and agrees with General Relativity inward. The behavior of this weak field limit served, for instance, to correctly describe galaxy rotation curves, galactic light bending, and the Bullet Cluster phenomena, without requiring the existence of dark matter. The transition of a standard to enhanced gravitational attraction comes from the interplay between tensor, vector and scalar physical fields. However, this mechanism seems to work solely where gravity is weak. Close to black holes or other compact objects like neutron stars, the gravitational field is very strong and Moffat’s mechanism to retrieve General Relativity breaks.\n\n"}
{"id": "495795", "url": "https://en.wikipedia.org/wiki?curid=495795", "title": "Scientific Integrity in Policymaking", "text": "Scientific Integrity in Policymaking\n\n\"Scientific Integrity in Policymaking: An Investigation into the Bush Administration's Misuse of Science\" is the title of a report published by the Union of Concerned Scientists in February, 2004. The report was the culmination of an investigation of the Bush administration's objectivity in science, and ultimately a criticism thereof. (After it was published, the report's existence was fairly well-publicized by the United States' mass media.)\n\nA central thesis of the report, according to the Executive Summary (on page 2 of the text), was that the Bush administration had behaved in ways considered to be consistent with the following three situations.\n\n\nIn \"Part III\", the text of the report posits that the aforementioned activities are unprecedented in the history of the United States. The report lists the following persons and organization who had supposedly acted or made statements to support this claim.\n\n\"This list is sorted first by category, then by the order in which the persons or organizations are mentioned in the report.\"\n\n\nPage 29 of the report states: \"This behavior by the administration violates the central premise of the scientific method, and is therefore of particularly grave concern to the scientific community.\" It then goes on, in a short section titled \"Conclusions and Recommendations: What's at Stake\" at the end of the report, to provide recommendations for \"restoring scientific integrity to federal policymaking\" (page 30). These recommendations (on pages 30–31) include a suggestion for the President of the United States to issue executive orders, and other actions, that would prevent further \"abuse\"; for the United States Congress to hold appropriate hearings, consider the consequences of statutory law under its influence, increase the amount of publicly available scientific information, and establish an organization to guide Congress in its deliberations in technical matters; for scientists to raise awareness of the aforementioned issues and provide public policy recommendations; for the public to exercise its political influence in a constructive manner.\n\nOn April 2, 2004, the White House Office of Science and Technology Policy issued a statement by Dr. John Marburger, the director of OSTP, that claims the descriptions of the incidents in the UCS report are all \"false,\" \"wrong,\" or \"a distortion.\" He said he was disappointed with the report and dismissed it as \"biased.\".\n\nThe following is a duplication of the report's table of contents.\n\n\nAt the time of issue of this report, the UCS released a statement supporting the criticisms detailed in the above report. This statement was originally signed by the 62 prominent scientists listed below. Since that time it has gathered support from more than 12,000 scientists.\n\nSignatories of the original statement include:\n\n"}
{"id": "33838184", "url": "https://en.wikipedia.org/wiki?curid=33838184", "title": "SeaOrbiter", "text": "SeaOrbiter\n\nThe SeaOrbiter, also known as Sea Orbiter (two words), is an oceangoing research vessel. Construction was to begin in late 2014. The SeaOrbiter is planned to allow scientists and others a residential yet mobile research station positioned under the oceans' surface. The station will have laboratories, workshops, living quarters and a pressurized deck to support divers and submarines.\n\nSeaOrbiter is a project of the \"Floating oceanographic laboratory\" organisation. It is headed by French architect Jacques Rougerie, oceanographer Jacques Piccard and astronaut Jean-Loup Chretien. The cost is expected to be around $52.7 million.\n\nThe laboratory is semi-submersible oceangoing craft and weighs 1000 tonnes. It has a total height of 51 meters with 31 meters below sea level.\n\nIt is designed to float vertically and drift with the ocean currents but has two small propellers allowing it to modify its trajectory and maneuver in confined waters. Underwater robots can be sent from the laboratory to explore the seabed. The hull is made of an alloy of aluminum and magnesium, and is five times thicker than that of a conventional vessel.\n\nIts vertical alignment in the sea will leave a small part visible above the surface with much larger accommodation and laboratories below the sea's surface. Some levels will have a cabin pressure equal to the external water pressure allowing divers to live for extended periods at depth and make frequent excursions.\n\n\n\nConstruction was due to start in 2014; however, as of May 2015, only the Eye of \"SeaOrbiter\" has been completed.\n\n\n"}
{"id": "40372241", "url": "https://en.wikipedia.org/wiki?curid=40372241", "title": "Subbasin", "text": "Subbasin\n\nA subbasin or sub-basin is a structural geologic feature where a larger basin is divided into a series of smaller basins with intervening intrabasinal highs. Some subbasins are referred to as \"sags\", although that term has a broader definition than a subbasin does.\n\nThe term subbasin has common use in geologic literature, but has yet to be included in the API Glossary of Geology.\n"}
{"id": "41137550", "url": "https://en.wikipedia.org/wiki?curid=41137550", "title": "Suzanne M. Bianchi", "text": "Suzanne M. Bianchi\n\nSuzanne M. Bianchi (April 15, 1952, Fort Dodge, Iowa – November 4, 2013, Santa Monica, California) was an American sociologist.\n\nSuzanne M. Bianchi was born in Fort Dodge, Iowa to Rita and Pesho Bianchi. Her mother was a housewife and her father was a meat packing plant employee. Bianchi is the oldest of six children. She and her husband had three children.\n\nAfter graduating valedictorian from her high school, Bianchi was the first in her family to go to college and earned her B.A. in Sociology from Creighton University, her M.A. from University of Notre Dame, and her Ph.D from the University of Michigan, Ann Arbor.\n\nShe began her career as a demographer for the U.S. Census Bureau, where she remained until 1994, then she joined the faculty at the University of Maryland, where she eventually chaired the university’s sociology department and directed the Maryland Population Research Center. In 2000 she served as President of the Population Association of America.\n\nIn 2009 she moved to UCLA, where she was Dorothy L. Meier Chair in Social Equities. Among her main fields of study she focused on working mothers, researching and analyzing changes in American family life during the last decades.\n\nShe researched women's employment, how husbands and wives divide housework and time with children, and how women take care of their children and parents.\n\nBianchi died from pancreatic cancer on November 4, 2013, aged 61. Her daughter Jennifer said of her mother, \"She was very much aware of the constraints of juggling career and motherhood. She lived as well as researched it.\"\n\nBianchi made many major contributions with her use of \"time diaries\". She was also the co-author of many books. One of her co-authors, Judith Selzer, said of her, \"She always identified puzzles in the social world and tried to solve them by rigorous empirical studies.\"\n\n"}
{"id": "20683004", "url": "https://en.wikipedia.org/wiki?curid=20683004", "title": "The Great Betrayal: Fraud in Science", "text": "The Great Betrayal: Fraud in Science\n\nThe Great Betrayal: Fraud in Science is a 2004 book by Horace Freeland Judson. The book explains that science as a discipline is not immune to fraud, and the book surveys many cases where scientific misconduct by aberrant scientists has threatened the reliability and foundations of the scientific process.\n\n"}
{"id": "54711454", "url": "https://en.wikipedia.org/wiki?curid=54711454", "title": "The Greatest Story Ever Told—So Far", "text": "The Greatest Story Ever Told—So Far\n\nThe Greatest Story Ever Told—So Far: Why Are We Here? is the tenth full-length non-fiction book by the American theoretical physicist Lawrence M. Krauss. The book was initially published on March 21, 2017 by Atria Books.\n\nThe book deals with the current scientific understanding of the creation of the Universe and gives a history of how scientists have formulated the Standard Model of Particle Physics.\n\n"}
{"id": "30964384", "url": "https://en.wikipedia.org/wiki?curid=30964384", "title": "USA-226", "text": "USA-226\n\nUSA-226 is the first flight of the second Boeing X-37B, the Orbital Test Vehicle 2 (X-37B OTV-2), an American unmanned robotic vertical-takeoff, horizontal-landing spaceplane. It was launched aboard an Atlas V rocket from Cape Canaveral on 5 March 2011, and landed at Vandenberg Air Force Base on 16 June 2012. It operated in low Earth orbit. Its mission designation is part of the USA series.\n\nThe spaceplane was operated by the United States Air Force, which has not revealed the specific identity of the payload for the first flight. The Air Force stated only that the spacecraft would \"demonstrate various experiments and allow satellite sensors, subsystems, components, and associated technology to be transported into space and back.\"\n\nOTV-2 was launched aboard an Atlas V rocket, tail number AV-026, on 5 March 2011 from Space Launch Complex 41 at Cape Canaveral Air Force Station in Florida. It was scheduled to launch on the previous day, 4 March, but weather prevented the launch on that day, forcing the reschedule to 5 March.\n\nThe launch was conducted by United Launch Alliance.\n\nThe X-37B spacecraft was originally intended to be deployed from the payload bay of a NASA Space Shuttle, but following the \"Columbia\" accident, it was transferred to a Delta II 7920, then subsequently transferred to the Atlas V following concerns over the X-37B's aerodynamic properties during launch.\n\nPrior to the installation of the spacecraft, the Atlas rocket was moved to the launch pad and performed a wet dress rehearsal on 4 February 2011. It was returned to the Vertical Integration Facility the following day for final assembly.\n\nMost of the mission parameters for the first OTV-2 flight have not been disclosed. The Air Force stated the mission time would depend on progress of the craft's experiments during orbit. On 29 November 2011 a spokesperson for the Secretary of the Air Force announced the mission was extended beyond its original life expectancy, citing ongoing experimentation.\n\nIn addition to its unspecified payload, OTV-2 carried a folded solar panel in its cargo bay to power the spacecraft during its year and a half long mission.\n\nAfter completing its mission, OTV-2 deorbited, entered the atmosphere, and landed at Vandenberg Air Force Base on 16 June 2012 at 05:48 PT (12:48 GMT). OTV-2 is the third reusable spaceplane to perform an automated landing after returning from orbit, the first being the Soviet Buran spacecraft in 1988 and the second, its sister craft, the OTV-1.\n\n\n"}
{"id": "574666", "url": "https://en.wikipedia.org/wiki?curid=574666", "title": "University of San Diego", "text": "University of San Diego\n\nThe University of San Diego (USD) is a private Roman Catholic research university located in the Linda Vista neighborhood of San Diego, California, United States. Founded in 1949 as the San Diego College for Women and San Diego University, the academic institutions merged into the University of San Diego in 1972. Since then, the university has grown to comprise nine undergraduate and graduate schools, to include the Shiley-Marcos School of Engineering, Joan B. Kroc School of Peace Studies, and School of Law. USD offers 79 undergraduate and graduate programs, and enrolls approximately 9,073 undergraduate, paralegal, graduate and law students.\n\nChartered in 1949, the university opened its doors to its first class of students in 1952 as the San Diego College for Women. Reverend Charles F. Buddy, D.D., then bishop of the Diocese of San Diego and Reverend Mother Rosalie Hill, RSCJ, a Superior Vicaress of the Society of the Sacred Heart of Jesus, chartered the institution from resources drawn from their respective organizations on a stretch of land known as \"Alcalá Park,\" named for San Diego de Alcalá. In September 1954, the San Diego College for Men and the School of Law opened. These two schools originally occupied Bogue Hall on the same site of University High School, which would later become the home of the University of San Diego High School. Starting in 1954, Alcalá Park also served as the diocesan chancery office and housed the episcopal offices, until the diocese moved to a vacated Benedictine convent that was converted to a pastoral center. In 1957, Immaculate Heart Major Seminary and St. Francis Minor Seminary were moved into their newly completed facility, now known as Maher Hall. The Immaculata Chapel, now no longer affiliated with USD, also opened that year as part of the seminary facilities. For nearly two decades, these schools co-existed on Alcalá Park. Immaculate Heart closed at the end of 1968, when its building was renamed De Sales Hall; St. Francis remained open until 1970, when it was transferred to another location on campus, leaving all of the newly named Bishop Leo T. Maher Hall to the newly merged co-educational University of San Diego in 1972. Since then, the university has grown quickly and has been able to increase its assets and academic programs. The student body, the local community, patrons, alumni, and many organizations have been integral to the university's development.\nSignificant periods of expansion of the university, since the 1972 merger, occurred in the mid-1980s, as well as in 1998, when Joan B. Kroc, philanthropist and wife of McDonald's financier Ray Kroc, endowed USD with a gift of $25 million for the construction of the Institute for Peace & Justice. Other significant donations to the college came in the form of multimillion-dollar gifts from weight-loss tycoon Jenny Craig, inventor Donald Shiley, investment banker and alumnus Bert Degheri, and an additional gift of $50 million Mrs. Kroc left the School of Peace Studies upon her death. These gifts helped make possible, respectively, the Jenny Craig Pavilion (an athletic arena), the Donald P. Shiley Center for Science and Technology, the Joan B. Kroc School of Peace Studies, and the Degheri Alumni Center. As a result, USD has been able to host the West Coast Conference (WCC) basketball tournament in 2002, 2003 and 2008, and hosted international functions such as the Kyoto Laureate Symposium at the Joan B. Kroc Institute for Peace & Justice and at USD's Shiley Theatre. Shiley's gift has provided the university with some additional, and more advanced, teaching laboratories than it had previously. In 2005, the university expanded the Colachis Plaza from the Immaculata along Marian Way to the east end of Hall, which effectively closed the east end of the campus to vehicular traffic. That same year, the student body approved plans for a renovation and expansion of the Hahn University Center which began at the end of 2007. The new Student Life Pavilion (SLP) opened in 2009 and hosts the university's new student dining area(s), offices for student organizations and event spaces. The Hahn University Center is now home to administrative offices, meeting and event spaces, and a restaurant and wine bar, La Gran Terazza.\n\nIn the fall of 2018, USD's total enrollment was 8,905 undergraduate, graduate, and law students.\n\nAlcalá Park sits atop the edge of a mesa overlooking Mission Bay and other parts of San Diego. The philosophy of USD's founder and her fellow religious relied on the belief that studying in beautiful surroundings could improve the educational experience of students. Thus, the university's buildings are designed in a 16th-century Spanish Renaissance architectural style, paying homage to both San Diego's Catholic heritage and the Universidad de Alcalá in Spain. In September 2011, \"Travel+Leisure\" named it as one of the most beautiful college campuses in the United States.\n\nThe campus is located approximately two miles north of downtown San Diego, on the north crest of Mission Valley in the community of Linda Vista. From the westernmost edges of Alcalá Park the communities of Mission Hills, Old Town, Point Loma, Ocean Beach, Bay Park, Mission Beach and Pacific Beach can be seen. Also, the Pacific Ocean, San Diego Harbor, the Coronado Islands and La Jolla are visible from the campus.\n\nThough a Catholic university, the school is no longer governed directly by the Diocese of San Diego. Today, a lay board of trustees governs the university's operations. However, the Bishop of San Diego, Robert W. McElroy, retains a seat as a permanent member and retains control of the school's designation of \"Catholic.\"\n\nUSD offers more than 79 degrees at the bachelor's, master's, and doctoral levels. USD is divided into six schools and colleges. The College of Arts and Sciences and the School of Law are the oldest academic divisions at USD; the Joan B. Kroc School of Peace Studies is the university's newest school. USD offers an honors program at the undergraduate level, with approximately 300 students enrolled annually.\n\nUSD's undergraduate programs have been recognized by multiple publications including \"PayScale\", \"U.S. News & World Report\", \"Princeton Review\", \"Bloomberg Businessweek\", and \"Forbes\". In 2017, USD ranked 86th among national universities by \"U.S. News & World Report\" and 188th by \"'Forbes' and \"Washington Monthly\".\n\nIn 2012, \"Princeton Review\" included USD in its annual guidebook of the 376 best universities. The \"Princeton Review\" ranked the school 2nd for Best Campus Environment and 39th for Most Beautiful Campus. \"Travel & Leisure\" and \"Newsweek\" have also \n\nrecognized USD's campus as one of the most beautiful in the United States.\n\nQS Global 200 Business Schools Report ranked USD's MBA program 59th in North America. The MBA program is also ranked 39th in the world for social responsibility in the Beyond Grey Pinstripes Global 100 list, and is the highest ranking program on that measure in Southern California.\n\nIn 2014, University of San Diego was ranked the 482nd top college in the United States by Payscale and CollegeNet's Social Mobility Index college rankings.\n\nAccording to the Institute of International Education, USD ranked first in undergraduate participation.\n\nIn 2016, the MBA program in the University of San Diego School of Business was ranked 28th in the United States (33rd in 2015) and 59th in the world (66th in 2015) in the 2016 \"Financial Times\" Top 100 MBA rankings. In July 2015, Financial Times ranked the University of San Diego’s School of Business MBA third in the world for entrepreneurship. In August 2016, \"CEOWORLD Magazine\" Global Business Schools rankings for executives and entrepreneurs ranked San Diego’s School of Business 66th in the world.\n\n\n\n"}
{"id": "322493", "url": "https://en.wikipedia.org/wiki?curid=322493", "title": "Wardian case", "text": "Wardian case\n\nThe Wardian case was an early type of sealed protective container for plants, an early version of the terrarium. It found great use in the 19th century in protecting foreign plants imported to Europe from overseas, the great majority of which had previously died from exposure during long sea journeys, frustrating the many scientific and amateur botanists of the time. The Wardian case was the direct forerunner of the modern terrarium, Vivarium (and the inspiration for the glass aquarium), and was invented by Dr. Nathaniel Bagshaw Ward (1791–1868), of London, in about 1829 after an accidental discovery inspired him. He published a book titled \"On the Growth of Plants in Closely Glazed Cases\" in 1842. A Scottish botanist named A.A. Maconochie had created a similar terrarium almost a decade earlier, but his failure to publish meant that Ward received credit as the sole inventor.\nDr. Ward was a physician with a passion for botany. His personally collected herbarium amounted to 25,000 specimens. The ferns in his London garden in Wellclose Square, however, were being poisoned by London's air pollution, which consisted heavily of coal smoke and sulphuric acid.\n\nDr. Ward also kept cocoons of moths and the like in sealed glass bottles, and in one, he found that a fern spore and a species of grass had germinated and were growing in a bit of soil. Interested but not yet seeing the opportunities, he left the seal intact for about four years, noting that the grass actually bloomed once. After that time however, the seal had rusted, and the plants soon died from the bad air. Understanding the possibilities, he had a carpenter build him a closely fitted glazed wooden case and found that ferns grown in it thrived.\n\nDr. Ward published his experiment and followed it up with a book in 1842, \"On the Growth of Plants in Closely Glazed Cases\".\n\nEnglish botanists and commercial nurserymen had been passionately prospecting the world for new plants since the end of the 16th century, but these had to travel as seeds or corms, or as dry rhizomes and roots, as salty air, lack of light, lack of fresh water and lack of sufficient care often destroyed all or almost all plants even in large shipments. With the new Wardian cases, tender young plants could be set on deck to benefit from daylight and the condensed moisture within the case that kept them watered, but protected from salt spray. The first test of the glazed cases was made in July 1833, when Dr. Ward shipped two specially constructed glazed cases filled with British ferns and grasses all the way to Sydney, Australia, a voyage of several months that found the protected plants still in good condition upon arrival. Somewhat more interesting plants made the return trip: a number of Australian native species that had never survived the transportation previously. The plants arrived in good shape, after a stormy voyage around Cape Horn.\n\nOne of Dr. Ward's correspondents was William Jackson Hooker, later director of the Royal Botanic Gardens, Kew. Hooker's son Joseph Dalton Hooker was one of the first plant explorers to use the new Wardian cases, when he shipped live plants back to England from New Zealand in 1841, during the pioneering voyage of HMS \"Erebus\" that circumnavigated Antarctica.\n\nWardian cases soon became features of stylish drawing rooms in Western Europe and the United States. In the polluted air of Victorian cities, the fern craze and the craze for growing orchids that followed owed much of their impetus to the new Wardian cases.\n\nMore importantly, the Wardian case unleashed a revolution in the mobility of commercially important plants. In Wardian cases, Robert Fortune shipped to British India 20,000 tea plants smuggled out of Shanghai, China, to begin the tea plantations of Assam. After germination of imported seeds in the heated glasshouses of Kew, seedlings of the rubber tree of Brazil were shipped successfully in Wardian cases to Ceylon (Sri Lanka) and the new British territories in Malaya to start the rubber plantations. Wardian cases have thus been credited for helping break geographic monopolies in the production of important agricultural goods.\n\nDr. Ward was always active in the Society of Apothecaries of London, of which he became Master in 1854. Until very recently, the Society managed the Chelsea Physic Garden, London, the oldest botanical garden in the UK. Ward was a founding member of both the Botanical Society of Edinburgh and the Royal Microscopical Society, a Fellow of the Linnean Society and a Fellow of the Royal Society.\n\n\n\n"}
