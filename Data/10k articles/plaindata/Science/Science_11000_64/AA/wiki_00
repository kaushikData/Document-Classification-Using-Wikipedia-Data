{"id": "473803", "url": "https://en.wikipedia.org/wiki?curid=473803", "title": "A19 road", "text": "A19 road\n\nThe A19 is a major road in England running approximately parallel to and east of the A1 road, although the two roads meet at the northern end of the A19, the two roads originally met at the southern end of the A19 in Doncaster but the old route of the A1 was changed to the A638. From Sunderland northwards, the route was formerly the A108. In the past the route was known as the East of Snaith-York-Thirsk-Stockton-on-Tees-Sunderland Trunk Road. Most traffic joins the A19, heading for Teesside, from the A168 at Dishforth Interchange.\n\nThe southern end of the A19 starts at the \"St Mary's Roundabout\" with the A630 \"Church Way\" and A638 just to the north of Doncaster itself near to the parish church; this junction has been improved in recent years. It leaves the A638 at the next roundabout as \"Bentley Road\", and then winds its way over the East Coast Main Line, which it follows through Selby and York, through the suburb of Bentley passing the Shell \"Bentley Service Station\", St Peter's church and the \"Druid's Arms\" and out into the countryside to the north of the urban area. It then passes the Pavilion exhibition centre.\n\nMuch of the course of the southern section of the A19 runs through the old Yorkshire coalfield, with evidence of old slag-heaps and colliery buildings. It passes through Toll Bar and the primary school. It passes through Askern, a former mining village. It meets the B1220 for Carcroft and goes through Owston, passing the \"Owston Park Lodge\". Here it passes the \"Askern Hotel\", \"Red Lion Hotel\" and \"Askern Service Station\" and goes over a level crossing. There is also a boating lake, St Peter's church and a greyhound stadium. There is a left turn for Norton. There are some long straights north of here, and the surrounds are mostly flat as the road heads towards the M62. It enters North Yorkshire and the district of Selby where it crosses the River Went near Walden Stubbs. There are some crossroads at Balne Moor, and it passes through Whitley Thorpe and Whitley and the \"George & Dragon\". It meets the M62 at junction 34.\nFrom the M62, the village of Eggborough has been bypassed in recent years, with the new road travelling from this roundabout to near the power station to the right (there are three power stations in a row at this point, running west-east: Ferrybridge, Eggborough, and Drax, with its enormous chimney, to the east). Close by is Whitley Bridge and the A19 then meets the A645 at a roundabout and its previous alignment to the north of the village, before travelling through Chapel Haddlesey where it crosses the River Aire and the small village of Burn, west of the former RAF Burn, where it crosses the Selby Canal, then before Brayton, it joins the A63. The £44m six mile A63 Selby bypass, to the south of the town opened on 11 June 2004. Before this happened, all the traffic, headed straight towards the centre of Selby, over a level-crossing and on to a busy traffic-light junction with the A63 from Leeds. The A19 took the major of the concurrency through the town centre, whilst crossing the old toll bridge and heading on north towards York. The road is still the A19 through Selby, but the bypass is the A63. However, north-bound traffic follows the A63.\n\nThe £5m Riccall and Barlby bypass opened in October 1987. This provided better junctions with the A63 (Howden) and A163 (Holme-on-Spalding-Moor). The A63 and A19 meet at a roundabout near a large pickle factory. It heads towards Riccall where the road is much straighter after the bypass; it is following what was the East Coast Main Line before the Selby Diversion was built. Where the road leaves the old railway, the Trans Pennine Trail follows along the old track. At Escrick, it enters the Vale of York, and passes the BP \"York Road Garage\", the Parsonage Hotel and the church of St Helen. Next is Deighton, passing the \"White Swan Inn\", then it heads towards Crockey Hill. It meets the A64 near the headquarters of Persimmon plc.\nThe York Northern By-Pass as the A1237 is a substitute for the A19 through York – this road is poorly engineered and has frequent roundabouts. The A19 still goes through York, beginning with the \"Fulford Interchange\" with the A64 close to a shopping centre, then Fulford, meeting the B1222 and passing St Oswald's church. It crosses the East Coast Main Line and passes through Clifton and Rawcliffe. North of York, the road passes the \"Riverside Farm\" pub, then goes through Skelton as \"Shipton Road\" passing the \"Blacksmith's Arms\" and \"Ramada York Hotel\". It re-enters North Yorkshire and the district of Hambleton and goes straight through the middle of Shipton by Beningbrough as \"Main Street\", to the annoyance of many residents. It passes the \"Sidings Hotel\", \"Dawnay Arms\" and the Holy Evangelists church. Leaving the village it passes a garage on the left; on 25 July 2004 Mark Hobson was caught by the police here.\nThere is a left turn for Tollerton and goes through Tollerton Forest. Heading northward the section between York and Thirsk was not helped much by the opening of the £5m Easingwold Bypass in November 1994, as the road remained single carriageway, starting at a roundabout. There is a left turn for Raskelf. The residents of Thormanby look forward to their village being bypassed. Here it passes the \"Black Bull\" pub. There is the small dwelling of Birdforth with a roadside cafe and crossroads for Hutton Sessay and Carlton Husthwaite. It crosses the Thirkleby Beck near Great Thirkleby and goes across Pudding Pie Hill. It meets the A168 from the south, and the old route through Thirsk is now the A170 then the A61. The bypass meets the A61 and A168 (for Northallerton) at a junction near South Kilvington.\n\nNorth of Thirsk, the A19 takes over from the A168 as the link from the A1 to Teesside and becomes a fast dual carriageway with mostly grade separated interchanges. The five-mile £4.4 million Thirsk By-pass was opened on 5 September 1972 by Robin Turton, Baron Tranmire, the local MP (from 1929), with a flypast by four Royal Air Force Vickers Varsity aircraft - RAF Topcliffe is to the south-west of Thirsk. It passes North Kilvington, and the £0.3 million South of Knayton (at Swan Lane) to north of Thirsk Bypass section opened in the early 1970s. It climbs slightly past the junction at Knayton near Borrowby and skirting the western edge of the North York Moors, meeting the A684 (for Northallerton) a Clack Lane End after passing through Leake and by the \"Haynes Arms\". The Borrowby Diversion opened in the late 1960s. The £1.1 million South of Clack Lane End to north end of Borrowby Diversion opened in the early 1970s. The Cleveland Tontine to Clack Lane End improvement opened in the early 1970s. It drops towards the Cleveland Tontine at the junction with the A172 (for Stokesley and Guisborough). later, it passes the BP \"Exelby Services\" on both sides of the road. Eventually after passing the Crathorne/Yarm exit the road passes over the Leven Viaduct towards Teesside. From the Crathorne bypass, the road leaves the old route to the east, with the old route now being the A67 then the A135 through Stockton. About from the Parkway Turn (A174) in Middlesbrough the road is raised slightly, overlooking Thornaby Industrial Estate and one of Europe's biggest housing estates known as Ingleby Barwick, giving clues that Teesside is imminent.\nAt the Parkway the lighting columns appear then the road widens to three lanes, then at Acklam at the A1130 interchange it becomes four before two peel off for the A66 for Stockton-on-Tees and Middlesbrough. The Tees Bridge opened in 1975. Either side of the River Tees crossing, the Tees Viaduct, is a retail park - Teesside Park with a Morrisons to the south of the river and Portrack with an Asda on the Stockton side, with a mass of industry in the vicinity of the A66/A19 interchange. This interchange is one of the few 4-way free-flow interchanges in Britain not found on the motorway network, and is similar to a four-level stack interchange, but with a single loop ramp covering the A19 south to A66 west movement. This road was improved in 1998 by widening from 2 to 3 & 4 lanes each way the section between the Parkway and Norton. Even in rush hour the road still flows quite well. The grade-separated £19m Billingham Diversion was officially opened in February 1983, which diverted the traffic through a sub-standard section with roundabouts (Wolviston By-pass) built in the late 1960s.\n\nPast Teesside the A19 enters rural landscape, meeting the former route through Billingham, where it enters the borough of Hartlepool. There is a right turn for Dalton Piercy at the Windmill Motel, and two link roads into Elwick, to the east. At Sheraton with Hulam, there is an intersection for the B1280 (for Wingate to the west), and the A179 (for Hartlepool, to the east). At this intersection the road enters County Durham. The route over Sedgewick Hill has been improved to the east. There is staggered crossroads, for Hutton Henry, to the left. There used to be a right turn for Castle Eden, now accessible only from the southbound carriageway. North of here, the Castle Eden Diversion opened in the early 1970s. It crosses a former railway (now NCN 1 and 14), and meets the A181 (for Wheatley Hill and Durham), and the B1281 (for Hesleden) at an intersection, and passes west of Shotton, where it joins the former route. There is a large intersection at \"Burnhope Way Roundabout\" for Shotton Colliery and a large industrial estate, to the west, and the B1320 for the new town of Peterlee, to the east.\nA flyover was constructed in the early 1990s to replace the previous roundabout, known locally as the Turnpike.\nThe 3.5-mile Easington and Cold Hesledon Diversion opened in the early 1970s, initially designated as the A19(M). There is an access road to the south from Easington and the A1086 (for Peterlee and Hartlepool) has limited access to the northbound and from the southbound routes. There is an intersection for the A182 (for Hetton-le-Hole), and limited access from the B1283 (for Easington Village), with no access from the southbound route. The former route north of Easington is the B1432 (to the east). At Cold Hesledon, there is an intersection for the A182 (to Seaham, to the east) and the B1285 (for Murton, to the west). The three-mile New Seaham and Seaton Diversion opened in the early 1970s, with the former route now the B1285 through Dalton-le-Dale. The eight miles of sections from Easington to Seaham were built by A. R. Carmichael in late 1971, and made the A19 from Thirsk to Sunderland completely dual-carriageway, with the contract awarded in October 1969. At Seaton with Slingley, there is a limited-access (to and from the south) intersection for the A1018, for Sunderland and Ryhope. At the same point there is a limited-access intersection (to and from the north) for the B1404 for Seaton and Houghton-le-Spring. The former route through the south of Sunderland is now the B1522. At the point where a former railway crosses (now NCN Route 1) the road enters the City of Sunderland.\nAt this point, the A19 makes a large deviation from its former route, by bypassing Sunderland from the west. Its former route went near the coast. The 8.75-mile Sunderland Bypass opened as the A108, and was built by W.C. French, with fourteen bridges and five underpasses, with the contract awarded in February 1970. The A108 was also previously the number of an A road in north London, for a re-routed A10 to Hoddesdon. At Herrington the A19 meets the A690 (for Houghton-le-Spring) and the B1286 at an interchange. It is crossed by the B1286. At Offerton and Hastings Hill there is an interchange with the A183 road (for Penshaw and Pennywell). The road crosses the River Wear on the Hylton Bridge, which was built as the A108 in 1975 by W.C. French (Construction) Ltd. At North Hylton, there is an interchange with the A1231 (for Washington and Castletown). It passes the Sunderland Nissan plant on the left, formerly the site of the Battle of Britain airfield, RAF Usworth. It meets the A1290, for Washington, at an interchange, where the road enters the borough of South Tyneside and is crossed by the Great North Forest Trail.\nAt Testo's Roundabout with the A184 (for Gateshead and The Boldons), the A19 originally ended as the A1 took over to run through the Tyne Tunnel, before that classification became assigned to the Newcastle Western Bypass from the Angel of the North to Kingston Park. To the east the A19 now approaches the Tyne Tunnel, where a second tunnel has recently been completed to relieve traffic congestion. There is a limited access junction (from the north) for Hedworth, and the road is crossed by the Green Line of the Tyne and Wear Metro. It meets the A194 (for South Shields) at an interchange. At the Jarrow Interchange, there is a roundabout for the A185 (for Hebburn) and the B1297 at the start of the single-carriageway £13m Tyne Tunnel, opened in October 1967 as the A108. The former route north of Sunderland is now the A1018.\n\nThe A19 continues in a north-westerly direction through North Tyneside past Killingworth and Cramlington, rejoining the current A1, just north of Newcastle at Seaton Burn.\n\nBetween Testo's Roundabout and Seaton Burn, the A19 was designated as part of the A1 until the opening of the Newcastle Western Bypass.\n\nIn November 1986 a tanker loaded with toluene overturned and caught fire near Brookfield. The driver and the occupants of three cars were injured. The fire burned for eight hours and led to residents being warned by Cleveland Police of potentially toxic fumes. The fire service later criticised the police response as a \"massive overreaction\".\n\nIn June 2008 a fuel tanker began leaking oil from its engine covering a mile-long stetch, including bend, before stopping near Hartlepool. A small fire broke out and cars began sliding, although none crashed. The fire service shut down the road to clean it.\nThe road also inspired the song \"A19\" by the North East band Maximo Park.\n\n"}
{"id": "3031629", "url": "https://en.wikipedia.org/wiki?curid=3031629", "title": "Anatoly Babko", "text": "Anatoly Babko\n\nAnatoly Babko (15 October 1905 in Sudzhenskoye, Tomsk Governorate – 7 January 1968) was a famous Ukrainian chemist, specializing in analytical chemistry and in the chemistry of complex compounds. \n\nBabko was a student of Professor N. Tananaev, a Member of the Academy of Sciences of the Ukrainian Soviet Republic (since 1957), and an Honoured Science Worker of the Ukrainian SSR (after 1966). In 1939, he organized the research department at the Institute of General and Inorganic Chemistry of the Ukrainian SSR, and managed it until the end of his life. In 1943 he was appointed to a professorship, and in 1944 became the Head of the Department of Analytical Chemistry at the University of Kiev. \n\nBabko's main works are devoted to the physical chemistry of complex compounds and their use in analytical chemistry as well as photometric and fluorescence methods of analysis.\n\nHe published more than 450 scientific works and 9 books that have been translated into several languages.\n"}
{"id": "24448768", "url": "https://en.wikipedia.org/wiki?curid=24448768", "title": "Annual pharmaceutical drug sales", "text": "Annual pharmaceutical drug sales\n"}
{"id": "1070769", "url": "https://en.wikipedia.org/wiki?curid=1070769", "title": "Asimov on Science Fiction", "text": "Asimov on Science Fiction\n\nAsimov on Science Fiction () is a 1981\nnon-fiction work by Isaac Asimov. It is a collection of short essays\ndealing with various aspects of science fiction. Many of the essays are\n(slightly edited versions of) editorials from \"Isaac Asimov's Science Fiction Magazine\". Asimov wrote forewords to them that bind the collection together and grouped them in the following sections:\n"}
{"id": "3270024", "url": "https://en.wikipedia.org/wiki?curid=3270024", "title": "Bibliographic coupling", "text": "Bibliographic coupling\n\nBibliographic coupling, like Co-citation, is a similarity measure that uses citation analysis to establish a similarity relationship between documents. Bibliographic coupling occurs when two works reference a common third work in their bibliographies. It is an indication that a probability exists that the two works treat a related subject matter.\n\nTwo documents are \"bibliographically coupled\" if they both \"cite\" one or more documents in common. The \"coupling strength\" of two given documents is higher the more citations to other documents they share. The figure to the right illustrates the concept of bibliographic coupling. In the figure, documents A and B both cite documents C, D and E. Thus, documents A and B have a bibliographic coupling strength of 3 - the number of elements in the intersection of their two reference lists.\n\nSimilarly, two authors are \"bibliographically coupled\" if the cumulative reference lists of their respective oeuvres each contain a reference to a common document, and their coupling strength also increases with the citations to other documents that their share. If the cumulative reference list of an author's oeuvre is determined as the multiset union of the documents that the author has co-authored, then the \"author bibliographic coupling strength\" of two authors (or more precisely, of their oeuvres) is defined as the size of the multiset intersection of their cumulative reference lists, however.<ref name=\"DOI: 10.1002/asi.20910\"></ref>\n\nBibliographic coupling can be useful in a wide variety of fields, since it helps researchers find related research done in the past. On the other hand, two documents are co-cited if they are both independently \"cited by\" one or more documents.\n\nThe concept of bibliographic coupling was introduced by M. M. Kessler of MIT in a paper published in 1963, and has been embraced in the work of the information scientist Eugene Garfield. It is one of the earliest citation analysis methods for document similarity computation and some have questioned its usefulness, pointing out that two works may reference completely unrelated subject matter in the third. Furthermore, bibliographic coupling is a retrospective similarity measure, meaning the information used to establish the similarity relationship between documents lies in the past and is static, i.e. bibliographic coupling strength cannot change over time, since outgoing citation counts are fixed.\n\nThe co-citation analysis approach introduced by Henry Small and published in 1973 addressed this shortcoming of bibliographic coupling by considering a document's incoming citations to assess similarity, a measure that can change over time. Additionally, the co-citation measure reflects the opinion of many authors and thus represents a better indicator of subject similarity.\n\nIn 1972 Robert Amsler published a paper describing a measure for determining subject similarity between two documents by fusing bibliographic coupling and co-citation analysis.\n\nIn 1981 Howard White and Belver Griffith introduced author co-citation analysis (ACA). Not until 2008 did Dangzhi Zhao and Andreas Strotmann combine their work and that of M. M. Kessler to define author bibliographic coupling analysis (ABCA), noting that as long as authors are active this metric is not static and that it is particularly useful when combined with ACA.\n\nMore recently, in 2009, Gipp and Beel introduced a new approach termed Co-citation Proximity Analysis (CPA). CPA is based on the concept of co-citation, but represents a refinement to Small's measure in that CPA additionally considers the placement and proximity of citations within a document's full-text. The assumption is that citations in closer proximity are more likely to exhibit a stronger similarity relationship.\n\nIn summary, a chronological overview of citation analysis methods includes: \n\nOnline sites that make use of bibliographic coupling include\nThe Collection of Computer Science Bibliographies and CiteSeer.IST\n\nFor an interesting summary of the progression of the study of citations see. The paper is more a memoir than a research paper, filled with decisions, research expectations, interests and motivations—including the story of how Henry Small approached Belver Griffith with the idea of co-citation and they became collaborators, mapping science as a whole.\n\n\n\n\n\n\n\nJeppe Nicolaisen, Bibliographic coupling in Birger Hjørland, ed., Core Concepts in Library and Information Science\n"}
{"id": "3784223", "url": "https://en.wikipedia.org/wiki?curid=3784223", "title": "Capital strike", "text": "Capital strike\n\nCapital strike refers to the withholding of new investment in an economy. It arises from the determination that return on investment may be low or nonexistent. \n\nA capital strike can occur when banks decide to raise lending standards or minimum loan requirements to individuals and business entities, and decide to sit on cash reserves, rather than take many loan risks, until a later point in time. Capital strikes may sometimes result when governments pursue policies that investors consider \"unfriendly\" or \"inflexible,\" such as rent control or nationalization. The term can refer to a capital strike by a single investor or a large group such as those who refused to invest in the United States in 1937. A capital strike is the premise of Ayn Rand's novel \"Atlas Shrugged\".\n\nIn response, governments sometimes act to appease investors; however, governments committed to endogenous development (recently, in Latin America) occasionally refuse to capitulate and instead pursue economic development plans which utilize other resources.\n\n"}
{"id": "4870777", "url": "https://en.wikipedia.org/wiki?curid=4870777", "title": "Champagne flow model", "text": "Champagne flow model\n\nA champagne flow is an astrophysical event whereby an HII region created inside a molecular cloud from ionization due to a recently formed star (usually an O-star) expands outward until it reaches the interstellar medium, at which point the ionized hydrogen gas bursts outward like an uncorked champagne bottle. This event is also sometimes called a Blister.\n\nThe champagne model is perhaps one of the first numerical calculations of the propagation of ionisation fronts and of the expansion of HII regions that \ndid not assumed a constant density medium around the massive exciting star. The model assumes that \nstar formation takes place in a dense cloud, surrounded and in pressure equilibrium with a low density inter-cloud gas.\nThe ample supply of UV photons generated by the star rapidly establishes an HII region and the expansion of this, sooner or later \nallows also for the ionisation of the inter cloud gas. Ionisation disrupts then the former pressure balance between the \ncloud and the inter-cloud gas as under the stellar radiation field all photo-ionised gas acquires a temperature of the order of 10000 K.\nIn this way, the ionised cloud material acquires an excess pressure, a pressure larger than the ionised low density inter cloud gas and this \nprovoques the supersonic expansion of the ionised cloud matter into the surrounding gas (the champagne flow). The streaming of matter out of the cloud allows \nfor the ionisation of a larger portion of the original cloud susteining in this way the pressure imbalance which eventually leads to the complete disruption\nof the parent cloud. The terms champagne model and champagne flow were coined by Mexican astrophysicist Guillermo Tenorio-Tagle in a paper in 1979 (Astronomy and Astrophysics 1979A&A...71...59T).\nThe model focus on the size, velocity field and the large density variations observed in HII regions. This article was followed by further hydrodynamical \ncalculations in one and two dimensions, in collaboration with Drs. Peter Bodenheimer, Harold W. Yorke and Piet Bedijn see:1979ApJ...233…85B.1983A&A...127..313Y, 1979A&A...80..110T, 1982ASSL...93….1T, 1984A&A...138..325Y, 1981A&A...98…85B\n\n"}
{"id": "28692427", "url": "https://en.wikipedia.org/wiki?curid=28692427", "title": "Christine Davies", "text": "Christine Davies\n\nChristine Tullis Hunter Davies (born 1959) is a Professor of Physics at the University of Glasgow.\n\nDavies was educated Colchester County High School for Girls and the University of Cambridge where she was an undergraduate student of Churchill College, Cambridge. She was awarded a Bachelor of Arts (BA) degree in 1981 in physics with theoretical physics followed by a PhD in 1984 for research on quantum chromodynamics (QCD) and the Drell–Yan process while working in the Cavendish Laboratory in Cambridge.\n\nDavies research investigates the strong interaction and the solution of quantum chromodynamics using a numerical method known as Lattice QCD.\n\nDuring her career she has held academic appointments at the University of Glasgow, CERN, Cornell University, Ohio State University and the University of California at Santa Barbara. Her research has been funded by the Science and Technology Facilities Council (STFC), the Particle Physics and Astronomy Research Council (PPARC), the Leverhulme Trust, Royal Society and the Fulbright Program.\n\nShe chairs the project management board for the Distributed Research utilising Advanced Computing (DiRAC) High Performance Computing (HPC) facility, is a member of the STFC particle physics advisory panel and serves as an external examiner for the School of Physics and Astronomy at the University of Manchester.\n\nShe was appointed Order of the British Empire (OBE) in the 2006 Birthday Honours for services to science, elected a Fellow of the Royal Society of Edinburgh (FRSE) in 2001 and has been a Fellow of the Institute of Physics (FInstP) since 1988. She was awarded the Rosalind Franklin Award in 2005 and a Royal Society Wolfson Research Merit Award in 2012.\n"}
{"id": "11763375", "url": "https://en.wikipedia.org/wiki?curid=11763375", "title": "Concatenated error correction code", "text": "Concatenated error correction code\n\nIn coding theory, concatenated codes form a class of error-correcting codes that are derived by combining an inner code and an outer code. They were conceived in 1966 by Dave Forney as a solution to the problem of finding a code that has both exponentially decreasing error probability with increasing block length and polynomial-time decoding complexity.\nConcatenated codes became widely used in space communications in the 1970s.\n\nThe field of channel coding is concerned with sending a stream of data at the highest possible rate over a given communications channel, and then decoding the original data reliably at the receiver, using encoding and decoding algorithms that are feasible to implement in a given technology.\n\nShannon's channel coding theorem shows that over many common channels there exist channel coding schemes that are able to transmit data reliably at all rates formula_1 less than a certain threshold formula_2, called the channel capacity of the given channel. In fact, the probability of decoding error can be made to decrease exponentially as the block length formula_3 of the coding scheme goes to infinity. However, the complexity of a naive optimum decoding scheme that simply computes the likelihood of every possible transmitted codeword increases exponentially with formula_3, so such an optimum decoder rapidly becomes infeasible.\n\nIn his doctoral thesis, Dave Forney showed that concatenated codes could be used to achieve exponentially decreasing error probabilities at all data rates less than capacity, with decoding complexity that increases only polynomially with the code block length.\n\nLet \"C\" be a [\"n\", \"k\", \"d\"] code, that is, a block code of length \"n\", dimension \"k\", minimum Hamming distance \"d\", and rate \"r\" = \"k\"/\"n\", over an alphabet \"A\":\nLet \"C\" be a [\"N\", \"K\", \"D\"] code over an alphabet \"B\" with |\"B\"| = |\"A\"| symbols:\nThe inner code \"C\" takes one of |\"A\"| = |\"B\"| possible inputs, encodes into an \"n\"-tuple over \"A\", transmits, and decodes into one of |\"B\"| possible outputs. We regard this as a (super) channel which can transmit one symbol from the alphabet \"B\". We use this channel \"N\" times to transmit each of the \"N\" symbols in a codeword of \"C\". The \"concatenation\" of \"C\" (as outer code) with \"C\" (as inner code), denoted \"C\"∘\"C\", is thus a code of length \"Nn\" over the alphabet \"A\":\nIt maps each input message \"m\" = (\"m\", \"m\", ..., \"m\") to a codeword (\"C\"(\"m\"<nowiki>'</nowiki>), \"C\"(\"m\"<nowiki>'</nowiki>), ..., \"C\"(\"m\"<nowiki>'</nowiki>)),\nwhere (\"m\"<nowiki>'</nowiki>, \"m\"<nowiki>'</nowiki>, ..., \"m\"<nowiki>'</nowiki>) = \"C\"(\"m\", \"m\", ..., \"m\").\n\nThe \"key insight\" in this approach is that if \"C\" is decoded using a maximum-likelihood approach (thus showing an exponentially decreasing error probability with increasing length), and \"C\" is a code with length \"N\" = 2 that can be decoded in polynomial time of \"N\", then the concatenated code can be decoded in polynomial time of its combined length \"n\"2 = \"O\"(\"N\"⋅log(\"N\")) and shows an exponentially decreasing error probability, even if \"C\" has exponential decoding complexity. This is discussed in more detail in section Decoding concatenated codes.\n\nIn a generalization of above concatenation, there are \"N\" possible inner codes \"C\" and the \"i\"-th symbol in a codeword of \"C\" is transmitted across the inner channel using the \"i\"-th inner code. The Justesen codes are examples of generalized concatenated codes, where the outer code is a Reed–Solomon code.\n\n1. The distance of the concatenated code \"C\"∘\"C\" is at least \"dD\", that is, it is a [\"nN\", \"kK\", \"D\"<nowiki>'</nowiki>] code with \"D\"<nowiki>'</nowiki> ≥ \"dD\".\n\n\"Proof:\"\nConsider two different messages \"m\" ≠ \"m\" ∈ \"B\". Let Δ denote the distance between two codewords. Then\n\nThus, there are at least \"D\" positions in which the sequence of \"N\" symbols of the codewords \"C\"(\"m\") and \"C\"(\"m\") differ. For these positions, denoted \"i\", we have\n\nConsequently, there are at least \"d\"⋅\"D\" positions in the sequence of \"n\"⋅\"N\" symbols taken from the alphabet \"A\" in which the two codewords differ, and hence\n\n2. If \"C\" and \"C\" are linear block codes, then \"C\"∘\"C\" is also a linear block code.\n\nThis property can be easily shown based on the idea of defining a generator matrix for the concatenated code in terms of the generator matrices of \"C\" and \"C\".\n\nA natural concept for a decoding algorithm for concatenated codes is to first decode the inner code and then the outer code. For the algorithm to be practical it must be polynomial-time in the final block length. Consider that there is a polynomial-time unique decoding algorithm for the outer code. Now we have to find a polynomial-time decoding algorithm for the inner code. It is understood that polynomial running time here means that running time is polynomial in the final block length. The main idea is that if the inner block length is selected to be logarithmic in the size of the outer code then the decoding algorithm for the inner code may run in exponential time of the inner block length, and we can thus use an exponential-time but optimal maximum likelihood decoder (MLD) for the inner code.\n\nIn detail, let the input to the decoder be the vector \"y\" = (\"y\", ..., \"y\") ∈ (\"A\"). Then the decoding algorithm is a two-step process:\n\nNow, the time complexity of the first step is \"O\"(\"N\"⋅exp(\"n\")), where \"n\" = \"O\"(log(\"N\")) is the inner block length. In other words, it is \"N\" (i.e., polynomial-time) in terms of the outer block length \"N\". As the outer decoding algorithm in step two is assumed to run in polynomial time the complexity of the overall decoding algorithm is polynomial-time as well.\n\nThe decoding algorithm described above can be used to correct all errors up to less than \"dD\"/4 in number. Using minimum distance decoding, the outer decoder can correct all inputs \"y\"<nowiki>'</nowiki> with less than \"D\"/2 symbols \"y\"<nowiki>'</nowiki> in error. Similarly, the inner code can reliably correct an input \"y\" if less than \"d\"/2 inner symbols are erroneous. Thus, for an outer symbol \"y\"<nowiki>'</nowiki> to be incorrect after inner decoding at least \"d\"/2 inner symbols must have been in error, and for the outer code to fail this must have happened for at least \"D\"/2 outer symbols. Consequently, the total number of inner symbols that must be received incorrectly for the concatenated code to fail must be at least \"d\"/2⋅\"D\"/2 = \"dD\"/4.\n\nThe algorithm also works if the inner codes are different, e.g., for Justesen codes. The generalized minimum distance algorithm, developed by Forney, can be used to correct up to \"dD\"/2 errors.\nIt uses erasure information from the inner code to improve performance of the outer code, and was the first example of an algorithm using soft-decision decoding.\n\nAlthough a simple concatenation scheme was implemented already for the 1971 Mariner Mars orbiter mission, concatenated codes were starting to be regularly used for deep space communication with the Voyager program, which launched two space probes in 1977. Since then, concatenated codes became the workhorse for efficient error correction coding, and stayed so at least until the invention of turbo codes and LDPC codes.\n\nTypically, the inner code is not a block code but a soft-decision convolutional Viterbi-decoded code with a short constraint length.\nFor the outer code, a longer hard-decision block code, frequently a Reed-Solomon code with eight-bit symbols, is used.\nThe larger symbol size makes the outer code more robust to error bursts that can occur due to channel impairments, and also because erroneous output of the convolutional code itself is bursty. An interleaving layer is usually added between the two codes to spread error bursts across a wider range.\n\nThe combination of an inner Viterbi convolutional code with an outer Reed–Solomon code (known as an RSV code) was first used in \"Voyager 2\", and it became a popular construction both within and outside of the space sector. It is still notably used today for satellite communications, such as the DVB-S digital television broadcast standard.\n\nIn a looser sense, any (serial) combination of two or more codes may be referred to as a concatenated code. For example, within the DVB-S2 standard, a highly efficient LDPC code is combined with an algebraic outer code in order to remove any resilient errors left over from the inner LDPC code due to its inherent error floor.\n\nA simple concatenation scheme is also used on the compact disc (CD), where an interleaving layer between two Reed–Solomon codes of different sizes spreads errors across various blocks.\n\nThe description above is given for what is now called a serially concatenated code. Turbo codes, as described first in 1993, implemented a parallel concatenation of two convolutional codes, with an interleaver between the two codes and an iterative decoder that passes information forth and back between the codes. This design has a better performance than any previously conceived concatenated codes.\n\nHowever, a key aspect of turbo codes is their iterated decoding approach. Iterated decoding is now also applied to serial concatenations in order to achieve higher coding gains, such as within serially concatenated convolutional codes (SCCCs). An early form of iterated decoding was implemented with two to five iterations in the \"Galileo code\" of the Galileo space probe.\n\n\n\n"}
{"id": "14826184", "url": "https://en.wikipedia.org/wiki?curid=14826184", "title": "Cool It: The Skeptical Environmentalist's Guide to Global Warming", "text": "Cool It: The Skeptical Environmentalist's Guide to Global Warming\n\nCool It: The Skeptical Environmentalist's Guide to Global Warming is a book by the Danish statistician and political scientist Bjørn Lomborg. The book is a sequel to \"The Skeptical Environmentalist\" (first published in Danish in 1998), which in English translation brought the author to world attention. Lomborg argues that many of the elaborate and expensive actions being considered to stop global warming will cost hundreds of billions of dollars without the same return on investment, often are based on emotional rather than strictly scientific assumptions, and may have very little impact on the world's temperature for centuries. Lomborg concludes that a limited carbon tax is needed in the First World as well as subsidies from the First World to the Third World to help fight ongoing humanitarian crises.\n\nIn a review in \"The New York Times\", Andrew Revkin says that Lomborg uses the book to reprise \"his earlier argument with a tighter focus. He tries to puncture more of what he says are environmental myths, like the imminent demise of polar bears.\"\n\nEconomist Frank Ackerman of Tufts University and the Stockholm Environment Institute, wrote a review of Lomborg's book. In it, Ackerman criticised Lomborg for his views on the economics of climate change, including the costs of the Kyoto Protocol and the use of cost-benefit analysis.\n\nIPCC lead author Brian O'Neill wrote a mixed review of \"Cool It\", concluding:\n\n[...] Bjorn Lomborg is like the Oliver Stone of climate change. He has written a book that sets out to support a certain point of view, and, unless you are an expert, you will never know which facts are correct and appropriately used and which are not. You might not be aware that large (and crucial) chunks of the story are skipped altogether. But like a Stone movie, it is a well-told tale and raises some questions that are worth thinking about. So if you are going to read only one book on climate, don’t read this one. But if you are going to read ten, reading Lomborg may be worthwhile.\n\nIn 2010, Howard Friel wrote \"The Lomborg Deception\", a book-length critique of \"Cool It\", which traces Lomborg’s many references and tests their authority and substance. Friel has said he found \"misrepresentation of academic research, misquotation of data, reliance on studies irrelevant to the author’s claims and citation of sources that seem not to exist\".\n\nAccording to Lomborg, Friel's book appears to be aimed primarily at the popular version of \"Cool It\" as opposed to the longer more thoroughly cited edition.\n\nOn 12 November 2010, Lomborg released a feature-length documentary film \"Cool It\" in the United States.\n\n\"The Atlantic\" says \"Cool It\" is \"an urgent, intelligent, and entertaining account of the climate policy debate, with a strong focus on cost-effective solutions\".\n\n\n\n"}
{"id": "1356824", "url": "https://en.wikipedia.org/wiki?curid=1356824", "title": "Disclosed fees", "text": "Disclosed fees\n\nIn business disclosed fees is debt and equity underwriting and advisory revenue reported by investment banks.\n\n"}
{"id": "1633075", "url": "https://en.wikipedia.org/wiki?curid=1633075", "title": "Electron microprobe", "text": "Electron microprobe\n\nAn electron microprobe (EMP), also known as an electron probe microanalyzer (EPMA) or electron micro probe analyzer (EMPA), is an analytical tool used to non-destructively determine the chemical composition of small volumes of solid materials. It works similarly to a scanning electron microscope: the sample is bombarded with an electron beam, emitting x-rays at wavelengths characteristic to the elements being analyzed. This enables the abundances of elements present within small sample volumes (typically 10-30 cubic micrometers or less) to be determined. The concentrations of elements from beryllium to plutonium can be measured at levels as low as 100 parts per million (ppm). Recent models of EPMAs can accurately measure elemental concentrations of approximately 10 ppm.\n\nIn 1944, MIT built an electron microprobe, combining an electron microscope and an energy loss spectrometer. Electron energy loss spectroscopy is very good for light element analysis and they obtained spectra of C-Kα, N-Kα and O-Kα radiation. In 1947, Hiller patented the idea of using an electron beam to produce analytical X-rays, but never constructed a working model. His design proposed using Bragg diffraction from a flat crystal to select specific X-ray wavelengths and a photographic plate as a detector.\n\nIn 1948-1950, , supervised by André Guinier, built the first electron “microsonde électronique” (electron microprobe) at ONERA. This microprobe produced an electron beam diameter of 1-3 μm with a beam current of ~10 nanoamperes (nA) and used a Geiger counter to detect the X-rays produced from the sample. However, the Geiger counter could not distinguish X-rays produced from specific elements and in 1950, Castaing added a quartz crystal between the sample and the detector to permit wavelength discrimination. He also added an optical microscope to view the point of beam impact. The resulting microprobe was described in Castaing's 1951 PhD Thesis, in which he laid the foundations of the theory and application of quantitative analysis by electron microprobe, establishing the theoretical framework for the matrix corrections of absorption and fluorescence effects. Castaing (1921-1999) is considered the \"father\" of electron microprobe analysis.\n\nCAMECA (France) produced the first commercial microprobe, the “MS85,” in 1956. It was soon followed by many microprobes from other companies; however, all companies except CAMECA , JEOL and Shimadzu Corporation are now out of business. In addition, many researchers build electron microprobes in their labs. Significant subsequent improvements and modifications to microprobes included scanning the electron beam to make X-ray maps (1960), the addition of solid state EDS detectors (1968) and the development of synthetic multilayer diffracting crystals for analysis of light elements (1984). Later, CAMECA became also the pioneer on manufacturing a shielded version of the electron microprobe for nuclear applications. Several new advances in CAMECA instruments in the last decades allowed them to expand their range of applications on metallurgy, electronics, geology, mineralogy, nuclear plants, trace elements, dentistry, etc.\n\nA beam of electrons is fired at a sample. The beam causes each element in the sample to emit X-rays at a characteristic frequency; the X-rays can then be detected by the electron microprobe. The size and current density of the electron beam determines the trade-off between resolution and scan time and/or analysis time.\n\nLow-energy electrons are produced from a tungsten filament, a lanthanum hexaboride crystal cathode or a field emission electron source and accelerated by a positively biased anode plate to 3 to 30 thousand electron volts (keV). The anode plate has central aperture and electrons that pass through it are collimated and focused by a series of magnetic lenses and apertures. The resulting electron beam (approximately 5 nm to 10 μm diameter) may be rastered across the sample or used in spot mode to produce excitation of various effects in the sample. Among these effects are: phonon excitation (heat), cathodoluminescence (visible light fluorescence), continuum X-ray radiation (bremsstrahlung), characteristic X-ray radiation, secondary electrons (plasmon production), backscattered electron production, and Auger electron production.\n\nWhen the beam electrons (and scattered electrons from the sample) interact with bound electrons in the innermost electron shells of the atoms of the various elements in the sample, they can scatter the bound electrons from the electron shell producing a vacancy in that shell (ionization of the atom). This vacancy is unstable and must be filled by an electron from either a higher energy bound shell in the atom (producing another vacancy which is in turn filled by electrons from yet higher energy bound shells) or by unbound electrons of low energy. The difference in binding energy between the electron shell in which the vacancy was produced and the shell from which the electron comes to fill the vacancy is emitted as a photon. The energy of the photon is in the X-ray region of the electromagnetic spectrum. As the electron structure of each element is unique, the series X-ray line energies produced by vacancies in the innermost shells is characteristic of that element, although lines from different elements may overlap. As the innermost shells are involved, the X-ray line energies are generally not affected by chemical effects produced by bonding between elements in compounds except in low atomic number (Z) elements ( B, C, N, O and F for K and Al to Cl for K) where line energies may be shifted as a result of the involvement of the electron shell from which vacancies are filled in chemical bonding.\n\nThe characteristic X-rays are used for chemical analysis. Specific X-ray wavelengths or energies are selected and counted, either by wavelength dispersive X-ray spectroscopy (WDS) or energy dispersive X-ray spectroscopy (EDS). WDS utilizes Bragg diffraction from crystals to select X-ray wavelengths of interest and direct them to gas-flow or sealed proportional detectors. In contrast, EDS uses a solid state semiconductor detector to accumulate X-rays of all wavelengths produced from the sample. While EDS yields more information and typically requires a much shorter counting time, WDS is generally a more precise technique with lower limits of detection because its superior X-ray peak resolution and greater peak to background ratio.\n\nChemical composition is determined by comparing the intensities of characteristic X-rays from the sample material with intensities from known composition (standards). Counts from the sample must be corrected for matrix effects (depth of production of the X-rays, absorption and secondary fluorescence) to yield quantitative chemical compositions. The resulting chemical information is gathered in textural context. Variations in chemical composition within a material (zoning), such as a mineral grain or metal, can be readily determined.\n\nVolume from which chemical information is gathered (volume of X-rays generation) is 0.3 – 3 cubic micrometers.\n\n\nThe technique is commonly used for analyzing the chemical composition of metals, alloys, ceramics, and glasses. It is particularly useful for assessing the composition of individual particles or grains and chemical changes on the scale of a few micrometres to millimeters. The electron microprobe is widely used for research, quality control, and failure analysis.\n\nThis technique is most commonly used by mineralogists and petrologists. Most rocks are aggregates of small mineral grains. These grains may preserve chemical information adopted during their formation and subsequent alteration. This information may illuminate geologic processes, such as crystallization, lithification, volcanism, metamorphism, orogenic events (mountain building), plate tectonics. This technique is also used for the study of extraterrestrial rocks (i.e. meteorites), and provides chemical data which is vital to understanding the evolution of the planets, asteroids, and comets.\n\nThe change in elemental composition from the center (also known as core) to the edge (or rim) of a mineral can yield information about the history of the crystal's formation, including the temperature, pressure, and chemistry of the surrounding medium. Quartz crystals, for example, incorporate a small, but measurable amount of titanium into their structure as a function of temperature, pressure, and the amount of titanium available in their environment. Changes in these parameters are recorded by titanium as the crystal grows.\n\nIn exceptionally preserved fossils, such as those of the Burgess shale, soft parts of organisms may be preserved. Since these fossils are often compressed into a 2D film, it can be difficult to deduce what features were what: a famous example is that of triangular extensions in \"Opabinia\", which were interpreted as either legs or extensions of the gut. Elemental mapping showed that they had a similar composition to the gut, favouring the second interpretation. Because of the thin nature of the carbon films, only low voltages (5-15 kV) can be used in such specimens.\n\nThe chemical composition of meteorites can be analysed quite accurately using EPMA technique. This can reveal a lot of information about the conditions that existed in our Solar System many years ago.\n\n\n"}
{"id": "706128", "url": "https://en.wikipedia.org/wiki?curid=706128", "title": "Electroscope", "text": "Electroscope\n\nAn electroscope is a scientific instrument used to detect the presence and magnitude of electric charge on a body. It was the first electrical measuring instrument. The first electroscope, a pivoted needle called the \"versorium\", was invented by British physician William Gilbert around 1600. The pith-ball electroscope and the gold-leaf electroscope are two classical types of electroscope that are still used in physics education to demonstrate the principles of electrostatics. A type of electroscope is also used in the quartz fiber radiation dosimeter. Electroscopes were used by the Austrian scientist Victor Hess in the discovery of cosmic rays.\n\nElectroscopes detect electric charge by the motion of a test object due to the Coulomb electrostatic force. Since the electric potential or voltage of an object with respect to ground equals its charge divided by its capacitance to ground, an electroscope can be regarded as a crude voltmeter. However, the accumulation of enough charge to detect with an electroscope requires hundreds or thousands of volts, so electroscopes are only used with high-voltage sources such as static electricity and electrostatic machines. Electroscopes generally give only a rough, qualitative indication of the magnitude of the charge. An instrument that measures charge quantitatively is called an electrometer.\nThe pith-ball electroscope, invented by British schoolmaster and physicist John Canton in 1754, consists of one or two small balls of a lightweight nonconductive substance, a spongy plant material called pith, suspended by linen threads.\nModern electroscopes frequently use plastic balls and the ball is suspended by a silk thread from the hook of an insulated stand. In order to test the presence of a charge on an object, the object is brought near to the uncharged pith ball. If the object is charged, the ball will be attracted to it and move toward it.\n\nThe attraction occurs because of induced polarization of the atoms inside the pith ball. All matter consists of electrically charged particles located close together; each atom consists of a positively charged nucleus with a cloud of negatively charged electrons surrounding it. The pith is a nonconductor, so the electrons in the ball are bound to atoms of the pith and are not free to leave the atoms and move about in the ball, but they can move a little within the atoms. See diagram at right. If, for example, a positively charged object \"(B)\" is brought near the pith ball \"(A)\", the negative electrons \"(blue minus signs)\" in each atom \"(yellow ovals)\" will be attracted and move slightly toward the side of the atom nearer the object. The positively charged nuclei \"(red plus signs)\" will be repelled and will move slightly away. Since the negative charges in the pith ball are now nearer the object than the positive charges \"(C)\", their attraction is greater than the repulsion of the positive charges, resulting in a net attractive force. This separation of charge is microscopic, but since there are so many atoms, the tiny forces add up to a large enough force to move a light pith ball.\n\nThe pith ball can be charged by touching it to a charged object, so some of the charges on the surface of the charged object move to the surface of the ball. Then the ball can be used to distinguish the polarity of charge on other objects because it will be repelled by objects charged with the same polarity or sign it has, but attracted to charges of the opposite polarity.\n\nOften the electroscope will have a pair of suspended pith balls. This allows one to tell at a glance whether the pith balls are charged. If one of the pith balls is touched to a charged object, charging it, the second one will be attracted and touch it, communicating some of the charge to the surface of the second ball. Now both balls have the same polarity charge, so they repel each other. They hang in an inverted 'V' shape with the balls spread apart. The distance between the balls will give a rough idea of the magnitude of the charge.\n\nThe gold-leaf electroscope was developed in 1787 by British clergyman and physicist Abraham Bennet, as a more sensitive instrument than pith ball or straw blade electroscopes then in use. It consists of a vertical metal rod, usually brass, from the end of which hang two parallel strips of thin flexible gold leaf. A disk or ball terminal is attached to the top of the rod, where the charge to be tested is applied. To protect the gold leaves from drafts of air they are enclosed in a glass bottle, usually open at the bottom and mounted over a conductive base. Often there are grounded metal plates or foil strips in the bottle flanking the gold leaves on either side. These are a safety measure; if an excessive charge is applied to the delicate gold leaves, they will touch the grounding plates and discharge before tearing. They also capture charge leaking through the air that accumulate on the glass walls, and that increase the sensitivity of the instrument. In the precision instruments the inside of the bottle was occasionally evacuated, to prevent the charge on the terminal from leaking off through the ionization of the air.\n\nWhen the metal terminal is touched with a charged object, the gold leaves spread apart in a 'V'. This is because some of the charge on the object is conducted through the terminal and metal rod to the leaves. Since they receive the same sign charge they repel each other and thus diverge. If the terminal is grounded by touching it with a finger, the charge is transferred through the human body into the earth and the gold leaves close together.\n\nThe electroscope can also be charged without touching it to a charged object, by electrostatic induction. If a charged object is brought near the electroscope terminal, the leaves also diverge, because the electric field of the object causes charges in the electroscope rod to separate the leaves. Charges of the opposite polarity to the charged object are attracted to the terminal, while charges with the same polarity are repelled to the leaves, causing them to spread. If the electroscope terminal is grounded while the charged object is nearby, by touching it momentarily with a finger, the same polarity charges in the leaves drain away to ground, leaving the electroscope with a net charge of opposite polarity to the object. The leaves close because the charge is all concentrated at the terminal end. When the charged object is moved away, the charge at the terminal spreads into the leaves, causing them to spread apart again.\n\n\n"}
{"id": "34608709", "url": "https://en.wikipedia.org/wiki?curid=34608709", "title": "Eliza Amy Hodgson", "text": "Eliza Amy Hodgson\n\nEliza Amy Hodgson (10 October 1888 – 7 January 1983) was a New Zealand botanist who specialized in liverworts. \n\nHodgson was born in Havelock North and attended Pukahu primary school and Napier Girls' High School. She was self-educated in botany as her father refused to allow her to attend university.\n\nHodgson published her first scientific paper at the age of 42 and went on to publish more than 30 papers thereafter. She described two new species of liverworts and nine new genera. The liverwort \"Lejeunea hodgsoniana\" was named in her honour as was the species \"Lepidolaena hodgsoniae\".\n\nShe became a Fellow of the Linnean Society of London and the Royal Society of New Zealand. Hodgson was also an honorary member of the British Bryological Society.\n\nHodgson was awarded an honorary doctorate by Massey University in 1976. \n"}
{"id": "3537242", "url": "https://en.wikipedia.org/wiki?curid=3537242", "title": "Frank M. Carpenter", "text": "Frank M. Carpenter\n\nFrank Morton Carpenter (September 6, 1902 – January 18, 1994) received his PhD from Harvard University, and was curator of fossil insects at the Harvard Museum of Comparative Zoology for 60 years. He studied the Permian fossil insects of Elmo, Kansas, and compared the North American fossil insect fauna with Paleozoic taxa known from elsewhere in the world. A careful and methodical worker, he used venation and mouthparts to determine the relationships of fossil taxa, and was author of the \"Treatise\" volume on Insects. He reduced the number of extinct insect orders then described from about fifty to nine.\n\nEntomologists David Grimaldi and Michael S. Engel consider him \"the most influential paleoentomologist of his generation\" (Grimaldi and Engel 2005 p. 143). He has been memorialized frequently with patronyms, including the hanging fly \"Bittacus carpenteri\" Cheng, 1957, the fossil parasitic wasp \"Carpenteriana tumida\" Yoshimoto, 1975, the fossil snakefly \"Fibla carpenteri\" Engel, 1995, the fossil ant \"Protrechina carpenteri\" Wilson, 1985, and the caddisfly \"Rhyacophila carpenteri\" Milne, 1936.\n\n\n"}
{"id": "50899446", "url": "https://en.wikipedia.org/wiki?curid=50899446", "title": "Girolamo della Volpaia", "text": "Girolamo della Volpaia\n\nGirolamo della Volpaia (ca. 1530-1614) was an Italian maker of clocks and scientific instruments from Volpaia.\n\nGirolamo continued the business of his father Camillo della Volpaia (1484-1560) and his uncles Benvenuto della Volpaia (1486-1532) and Eufrosino della Volpaia (late 15th century - 16th century), who were an important family of craftsmen in Tuscany. In 1554, he made an armillary sphere, now preserved in the Science Museum in London. In 1560, he succeeded his father as superintendent of the large clock in the Palazzo Vecchio. He also asked to be assigned the maintenance of his grandfather Lorenzo's (1446-1512) \"Orologio dei Pianeti\" [Planetary Clock], which he restored himself. In 1564, he designed a clock for the Piazza San Marco in Venice. In 1590, he built his last clock, preserved at the Museo Galileo of Florence (inv. 2460).\n"}
{"id": "1248949", "url": "https://en.wikipedia.org/wiki?curid=1248949", "title": "Hapaxanth", "text": "Hapaxanth\n\nA hapaxanth is a plant species whose individuals flower only once in their lifetimes and die subsequently. Other terms with the same meaning are \"semelparous\" and \"monocarpic\". The term was first used by Alexander Braun.\n\nThis term is used for plants that only have one flowering event and then die. It is used most often in conjunction with describing some of the taxa of Arecaceae (palms) and some species of bamboo, but rarely used otherwise. Many annuals flower over a period of time and can have flowers and seeds on the same plant at the same time, thus they are not hapaxanthic, because flowering does not lead to death. A related term is pleonanthic, again used with Arecaceae (palms) and some bamboos for stems that flower more than once. Its antonym is often given as \"pollakanth\", although \"pleonanth\" is also seen.\n"}
{"id": "876358", "url": "https://en.wikipedia.org/wiki?curid=876358", "title": "International Commission on Stratigraphy", "text": "International Commission on Stratigraphy\n\nThe International Commission on Stratigraphy (ICS), sometimes referred to by the unofficial name \"International Stratigraphic Commission\" is a daughter or major subcommittee grade scientific daughter organization that concerns itself with stratigraphy, geological, and geochronological matters on a global scale. \n\nIt is a subordinate body of the International Union of Geological Sciences—of which it is the largest body within the organisation—and of which it is essentially a permanent working subcommittee that meets far more regularly than the quadrennial meetings scheduled by the IUGS, when it meets as a congress or membership of the whole.\n\nOne of its main aims, a project begun in 1974, is to establish a multidisciplinary standard and global geologic time scale that will ease paleontological and geobiological comparisons region to region by benchmarks with stringent and rigorous strata criteria called Global Boundary Stratotype Section and Points (GSSPs) within the fossil record. (i.e. section of the rock record as of a core sample section or accessible exposed strata, which when a core sample are usually \"trayed\" in long pieces, also called \"sections\" about a meter in length.)\n\nAdditionally the ICS defines an alternative type of benchmark and criteria called Global Standard Stratigraphic Ages (GSSAs) where the characteristics and dating criteria set solely by physical sciences methods (such as magnetic alignment sequences, radiological criteria, etcetera.) as well as encouraging an international and open debate amongst Earth scientists in the paleontology, geology, geobiology and chronostratigraphy fields, among others. \n\nThe International Commission on Stratigraphy has spawned numerous subcommittee level organizations organized and mobilized on a local country-wide or regional basis that are the true working committees of the IUGS, and these do the field work, basis comparisons in conference or co-ordination research committee meetings of local or wide-scale scope.\n\nThe ICS publishes various reports and findings as well as revised references periodically, summarized in the International Stratigraphic Chart, a combined \"working proposal\" and \"guideline-to-date\" released after the last ICS deliberations prior to the upcoming (next) meeting of the IUGS. Until the IUGS accepts the recommendations, they are unofficial since the IUGS parent approves or dismisses the individual deliberation reports of the ICS, which are presented as recommendations, and span dating and strata selection criteria, and related issues including nomenclatures. In \"de facto\" everyday matters, the deliberative results reported out of any meetings of the ICS are widely accepted and immediately enter everyday use, except in the rare cases where they result in a strong body of dissenting opinion, which matters are resolved before the full IUGS.\n\nOne such controversy arose in 2009 when the ICS deliberated and decided that the Pliocene Series of the current but unofficially named Quaternary Period should be shifted into the Neogene System and Neogene Period. The term Quaternary has yet to be officially adopted by the IUGS, but has widespread support as acceptable nomenclature for the current geologic period beginning at the GSSP accepted at 5,332,000 years ago at the transition between the Messinian Age to the Zanclean Age (3.6 mya). The ICS voted, perhaps because the time units span human paleo-archaeological strata, to begin the Quaternary at the end GSSP of the Piacenzian Age (2.588 mya) or possibly the end of the Gelasian (1.806 mya), any of which are in a different epoch.\n\nThe logo of International Commission on Stratigraphy was designed after the Chinese character of \"mountain\".\n\n"}
{"id": "48979827", "url": "https://en.wikipedia.org/wiki?curid=48979827", "title": "John Joseph Lalor", "text": "John Joseph Lalor\n\nJohn Joseph Lalor ( to 9 June 1899) political scientist. Translator of work by Ludwig Nohl and Wilhelm Roscher and best known for \"Cyclopaedia of Political Science, Political Economy, and the Political History of the United States\" (1895).\n\nIn 1885, Lalor taught at East Side High School, Milwaukee. Lalor worked as a translator in the Director of Mint, U. S. Treasury Department. Lalor collaborated with Louis Wolowski, Ludwig Nohl, and Paul Shorey. He translated works by Rudolf von Jhering and Wilhelm Roscher. He translated from German two works by Ludwig Nohl, a biography of Ludwig Beethoven in \"Life of Beethoven\" (1881) and Wolfgang Mozart in \"Life of Mozart\" (1880).\n\nIn 1899, Lalor died from injuries due to a fall.\n\n\n"}
{"id": "53880785", "url": "https://en.wikipedia.org/wiki?curid=53880785", "title": "Jonathan Abbatt", "text": "Jonathan Abbatt\n\nJonathan Abbatt is a Canadian chemist currently at the University of Toronto and an Elected Fellow of the American Geophysical Union. His work mainly focuses on chemical processes in the atmosphere.\n"}
{"id": "37258226", "url": "https://en.wikipedia.org/wiki?curid=37258226", "title": "Joseph Gutheinz", "text": "Joseph Gutheinz\n\nJoseph Richard Gutheinz (born August 13, 1955) is an American attorney, college instructor, commissioner, writer, and former Army intelligence officer, Army aviator, and Federal law enforcement officer. He is known as the founder of the \"Moon Rock Project\" which aims to track down missing Apollo moon rock samples.\n\nJoseph Gutheinz's father was a lieutenant colonel in the US Marines and a veteran of WW-ll, China, the Korean War and the Vietnam War, and his mother, Rita O’Leary Gutheinz, was a Marine Corps enlisted woman; his grandfather was an Army veteran of WW-l and was wounded by mustard gas in that war; his great grandfather was an Army veteran of the American Civil War.\nHe holds six college degrees from Monterey Peninsula College, California State University, Sacramento, the University of Southern California and South Texas College of Law, and eight teaching credentials and ten law licenses. He is an attorney at law (1996 to present) He has taught for Central Texas College, Alvin Community College, Thurgood Marshall Law School, and for the University of Phoenix from 2002 to present. He is a former Commissioner on the Texas Commission on Fire Protection (2013–), having been appointed by Texas Governor Rick Perry. He has served as a Member of the Texas Council on Sex Offender Treatment (2009 to 2012). He has served as a Member of the Texas Criminal Justice Advisory Committee on Medical and Mental Impairments (2004 to 2008). He is a former advisor to the Coalition for an Airline Passenger Bill of Rights.\n\nGutheinz is also the founder of the law practice Gutheinz Law Firm, LLP. Two of his sons are partners in the firm, both of whom are also Army veterans as he is.\n\nGutheinz led the Omniplan task force investigation, which determined that Omniplan, a NASA contractor, was submitting false claims to NASA. The company had claimed it was leasing through three companies that were in fact shell companies controlled by Omniplan's owner Ralph Montijo. Gutheinz created the task force with 25 agents, inspectors, auditors and a financial analyst from eight agencies. The investigation led to the closure of 7 companies, making it one of the highest profile in NASA history at that time. Gutheinz subsequently gave a speech to the International Business Forum about the Omniplan case, a speech attended by one of the principal defense attorneys in that case.\n\nIn the 1990s Gutheinz also served as the case agent in a joint investigation with the U.S. Department of Energy Office of Inspector General investigating the Arkansas Aerospace Education Center. Gutheinz led the Rockwell, United Space Alliance and Boeing North America task force investigation that resulted in a Federal civil lawsuit and an out of court settlement. In the civil suit the government alleged that Rockwell Space Operation's Company knew of Omniplan's fraudulent leases and failed to properly disclose that information to NASA or law enforcement. Gutheinz also investigated and arrested Jerry Alan Whittredge, an astronaut impersonator. Gutheinz also investigated the Russian space program and a fire and collision on the \"Mir\" space station.\n\nGutheinz led and went undercover in Operation Lunar Eclipse, a sting operation to recover the Honduras Apollo 17 Goodwill Moon Rock. The Honduras Apollo 17 Goodwill Moon Rock had been offered to Gutheinz for 5 million dollars, and Texas billionaire H. Ross Perot put up the money which facilitated the moon rock's recovery. The three agencies in Operation Lunar Eclipse were NASA Office of Inspector General (SSA Joseph Gutheinz aka Tony Coriasso), the United States Postal Inspection Service (Inspector Bob Cregger aka John Marta) and United States Customs (SA David Attwood and SA Dwight Weikel). Operation Lunar Eclipse was designed to catch con-artists selling terrestrial rocks and dust and offering them as Apollo era moon rocks and lunar dust. Upon determining that the seller of the Honduras Apollo 17 Goodwill Moon Rock was selling a genuine Apollo era moon rock, Gutheinz investigated further and determined that there was a lack of accountability on all the Apollo 11 and 17 Moon Rocks and lunar dust that the Nixon, Ford and Carter Administrations gifted away to the states and nations of the world. Gutheinz had previously requested, in 1998-1999, that NASA OIG attempt to account for the gifted moon rocks and lunar dust. In December 2011 NASA OIG revealed the finding of an audit they conducted on loaned, not gifted, Apollo era lunar samples, which revealed a lack of accountability by both NASA and the recipient individuals and entities.\n\nGutheinz has received awards from six Federal agencies and one state for his governmental service. These awards include the NASA Exceptional Service Medal; the President's Council on Integrity and Efficiency Career Achievement Award; the NASA Superior Accomplishment Award (Special Act or Service) for his investigation of the Russian Mir Space Station fire and collision; a Special Commendation from the United States Attorney's Office for the Southern District of Texas as well as a Letter of Commendation from the Director of the FBI, for his leadership of the Omniplan task force investigation; a Certificate of Appreciation from the United States Attorney's Office for the Southern District of Texas for his leadership in the Lockheed-Martin investigation; a United States Department of Transportation Certificate of Achievement for Superior Performance for his investigation of the Denver Airport and pilots covered by the FBI/FAA pilot match program; and the Honor Graduate Pin from the Federal Law Enforcement Training Center as well as a Letter of Commendation from the Department of Treasury, for graduating first in his class in the Criminal Investigators Course at the Federal Law Enforcement Training Center in Glynco, Georgia.\n\nGutheinz has been on American news shows and been quoted in print publications about aviation safety and security. His activism over aviation safety and security began when he was a student aviator and saw a friend die in a helicopter crash at the U.S. Army flight school and continued thereafter with his past assignments as a Special Agent with The FAA Civil Aviation Security; a Special Agent for U.S. Department of Transportation Office of Inspector General; a Senior Special Agent with NASA Office of Inspector General and as an advisor and spokesperson for the Coalition for an Airline Passenger Bill of Rights. Gutheinz is an advocate for greater transparency in NASA. Since retiring from NASA, he has been critical of NASA's handling of the Columbia disaster and post Columbia decisions. He has also been critical of the lack of policing for American air balloon pilots.\n\nGutheinz created and led The Moon Rock Project at the University of Phoenix, a project where criminal justice graduate students investigated and tracked down missing Apollo 11 lunar samples and Apollo 17 Goodwill Moon Rocks.\n\nGutheinz and his students have assisted in successfully tracking down 79 missing Apollo 11 lunar samples and Apollo 17 Goodwill Moon Rocks and plaques, including three that were retained by Governor John Vanderhoof of Colorado, Governor Arch Moore, Jr. of West Virginia, and Governor Kit Bond of Missouri. On March 16, 2013, the Oprah Winfrey Network's television show \"Lost and Found\" aired a show titled \"Unbelievable Mysteries Solved\", which included Gutheinz, his former graduate student Sandra Shelton, and the recovery of the West Virginia Apollo 17 Goodwill Moon Rock.\n\nIn 2009 Associated Press reporter Toby Sterling took interest in Gutheinz's Moon Rock Project and then enlisted the efforts of other AP reporters. With Gutheinz's assistance they tracked down additional moon rocks. Gutheinz determined that both the Cyprus Apollo 11 lunar samples and Apollo 17 Goodwill Moon Rock, had both been taken or destroyed during Cyprus civil disturbances of 1973 and 1974, but later discovered that the Cyprus Apollo 17 Goodwill Moon Rock was in the possession of a deceased American diplomat's child whose father worked at the American Embassy in Cyprus. Following this he made public requests for the rock to be returned. On April 16, 2010 the NASA Office of Inspector General at Johnson Space Center took custody of the rock.\n\nIn 2002, Gutheinz first assigned his University of Phoenix students to track down Canada's Apollo 17 Moon Rock. Their efforts discerned that the moon rock and plaque were unaccounted for and believed stolen decades earlier. Gutheinz's students tracked down the moon rock to a storage facility at the Canadian Museum of Nature. In 2009 Gutheinz and his students pressured Canada and the Museum to place the Canadian Apollo 17 Goodwill Moon Rock on display for the 40th anniversary Apollo 11. The moon rock was transferred to the Canadian Science and Technology Museum and placed on display in 2009.\n\nIn 2012 Gutheinz traveled to Buffalo, Texas, to look at an alleged Apollo era moon rock being sold on EBay for $300,000. Gutheinz has also been critical of NASA's handling of moon rocks to include loaned moon rocks and the lack of security some temporary recipients have provided to America's Apollo era lunar samples.\n\nBecause of Gutheinz's efforts to find missing and stolen moon rocks for over a decade he has acquired a nickname: \"The Moon Rock Hunter\". This nickname was originally given to Gutheinz by his graduate students, as a joking takeoff on \"The Crocodile Hunter\". Two documentaries have been made about his investigations, \"Moon for Sale\" (2007) and \"Lunarcy!\" (2012).\n\nIn 2013 he and his student were on one episode of \"Unbelievable Mysteries\" on the OWN Network. In 2014 Joe Gutheinz appeared on \"Lost History with Brad Meltzer\" on the History Channel. In 2015 a documentary about Joe Gutheinz and his students, \"Missing Moon Rocks\", won an Emmy Award for Best Historical Documentary. In 2016 Gutheinz appeared in eleven episodes of \"NASA’s Unexplained Files\" on the Science Channel. Joe Gutheinz is also acting in a film entitled \"Operation Lunar Eclipse\" about his 1998 undercover sting operation of the same name. Filming locations for the movie have included Kennedy Space Center, Marshall Space Flight Center, Washington D.C., Miami, and Honduras. A release date is set to come some time in 2017.\n\n"}
{"id": "31419238", "url": "https://en.wikipedia.org/wiki?curid=31419238", "title": "Knowledge assessment methodology", "text": "Knowledge assessment methodology\n\nThe knowledge assessment methodology (KAM) is \"an interactive benchmarking tool created by the World Bank's Knowledge for Development Program to help countries identify the challenges and opportunities they face in making the transition to the knowledge-based economy.\"\n\nKAM does so by providing information on knowledge economy indicators for 146 countries. Its products include the Knowledge Economy Index and the Knowledge Index.\n"}
{"id": "30492995", "url": "https://en.wikipedia.org/wiki?curid=30492995", "title": "Knut Olai Bjørlykke", "text": "Knut Olai Bjørlykke\n\nKnut Olai Knutsen Bjørlykke (11 February 1860 – 26 February 1946) was a Norwegian geologist and sedimentologist.\nHe was born in Sandøy in Møre og Romsdal, Norway. He was the father of Harald Bjørlykke and through him a grandfather of Arne Bjørlykke.\n\nHe took the dr.philos. degree in 1907, and his best known academic work was \"Det centrale Norges fjeldbygning\" (1905). He worked for the Norwegian Geological Survey from 1889 to 1905 and the Norwegian College of Agriculture from 1898 to 1931. He was awarded his PhD. in 1907 and appointed professor of geology. \n\nIn the Norwegian parliamentary election, 1915 he stood as a candidate for the Liberal Party in the constituency \"Bærum og Follo\", but lost to Christian Fredrik Michelet with only 864 out of 9,133 votes.\n"}
{"id": "307258", "url": "https://en.wikipedia.org/wiki?curid=307258", "title": "Le Sage's theory of gravitation", "text": "Le Sage's theory of gravitation\n\nLe Sage's theory of gravitation is a kinetic theory of gravity originally proposed by Nicolas Fatio de Duillier in 1690 and later by Georges-Louis Le Sage in 1748. The theory proposed a mechanical explanation for Newton's gravitational force in terms of streams of tiny unseen particles (which Le Sage called ultra-mundane corpuscles) impacting all material objects from all directions. According to this model, any two material bodies partially shield each other from the impinging corpuscles, resulting in a net imbalance in the pressure exerted by the impact of corpuscles on the bodies, tending to drive the bodies together. This mechanical explanation for gravity never gained widespread acceptance, although it continued to be studied occasionally by physicists until the beginning of the 20th century, by which time it was generally considered to be conclusively discredited.\n\nThe theory posits that the force of gravity is the result of tiny particles (corpuscles) moving at high speed in all directions, throughout the universe. The intensity of the flux of particles is assumed to be the same in all directions, so an isolated object A is struck equally from all sides, resulting in only an inward-directed pressure but no net directional force (P1).\n\nWith a second object \"B\" present, however, a fraction of the particles that would otherwise have struck A from the direction of B is intercepted, so B works as a shield, i.e. from the direction of B, A will be struck by fewer particles than from the opposite direction. Likewise B will be struck by fewer particles from the direction of A than from the opposite direction. One can say that A and B are \"shadowing\" each other, and the two bodies are pushed toward each other by the resulting imbalance of forces (P2). Thus the apparent attraction between bodies is, according to this theory, actually a diminished push from the direction of other bodies, so the theory is sometimes called \"push gravity\" or \"shadow gravity\", although it is more widely referred to as \"Lesage gravity\".\n\n\nIf the collisions of body A and the gravific particles are fully elastic, the intensity of the reflected particles would be as strong as of the incoming ones, so no net directional force would arise. The same is true if a second body B is introduced, where B acts as a shield against gravific particles in the direction of A. The gravific particle C which ordinarily would strike on A is blocked by B, but another particle D which ordinarily would not have struck A, is re-directed by the reflection on B, and therefore replaces C. Thus if the collisions are fully elastic, the reflected particles between A and B would fully compensate any shadowing effect. In order to account for a net gravitational force, it must be assumed that the collisions are not fully elastic, or at least that the reflected particles are slowed, so that their momentum is reduced after the impact. This would result in streams with diminished momentum departing from A, and streams with undiminished momentum arriving at A, so a net directional momentum toward the center of A would arise (P3). Under this assumption, the reflected particles in the two-body case will not fully compensate the shadowing effect, because the reflected flux is weaker than the incident flux.\n\n\nSince it is assumed that some or all of the gravific particles converging on an object are either absorbed or slowed by the object, it follows that the intensity of the flux of gravific particles emanating from the direction of a massive object is less than the flux converging on the object. We can imagine this imbalance of momentum flow - and therefore of the force exerted on any other body in the vicinity - distributed over a spherical surface centered on the object (P4). The imbalance of momentum flow over an entire spherical surface enclosing the object is independent of the size of the enclosing sphere, whereas the surface area of the sphere increases in proportion to the square of the radius. Therefore, the momentum imbalance per unit area decreases inversely as the square of the distance.\n\nFrom the premises outlined so far, there arises only a force which is proportional to the surface of the bodies. But gravity is proportional to the masses. To satisfy the need for mass proportionality, the theory posits that a) the basic elements of matter are very small so that gross matter consists mostly of empty space, and b) that the particles are so small, that only a small fraction of them would be intercepted by gross matter. The result is, that the \"shadow\" of each body is proportional to the surface of every single element of matter. If it is then assumed that the elementary opaque elements of all matter are identical (i.e., having the same ratio of density to area), it will follow that the shadow effect is, at least approximately, proportional to the mass (P5).\n\nNicolas Fatio presented the first formulation of his thoughts on gravitation in a letter to Christiaan Huygens in the spring of 1690. Two days later Fatio read the content of the letter before the Royal Society in London. In the following years Fatio composed several draft manuscripts of his major work \"De la Cause de la Pesanteur\", but none of this material was published in his lifetime. In 1731 Fatio also sent his theory as a Latin poem, in the style of Lucretius, to the Paris Academy of Science, but it was dismissed. Some fragments of these manuscripts and copies of the poem were later acquired by Le Sage who failed to find a publisher for Fatio's papers. So it lasted until 1929, when the only complete copy of Fatio's manuscript was published by Karl Bopp, and in 1949 Gagnebin used the collected fragments in possession of Le Sage to reconstruct the paper. The Gagnebin edition includes revisions made by Fatio as late as 1743, forty years after he composed the draft on which the Bopp edition was based. However, the second half of the Bopp edition contains the mathematically most advanced parts of Fatio's theory, and were not included by Gagnebin in his edition. For a detailed analysis of Fatio's work, and a comparison between the Bopp and the Gagnebin editions, see Zehe The following description is mainly based on the Bopp edition.\n\n\nFatio assumed that the universe is filled with minute particles, which are moving indiscriminately with very high speed and rectilinearly in all directions. To illustrate his thoughts he used the following example: Suppose an object \"C\", on which an infinite small plane zz and a sphere centered about \"zz\" is drawn. Into this sphere Fatio placed the pyramid \"PzzQ\", in which some particles are streaming in the direction of \"zz\" and also some particles, which were already reflected by \"C\" and therefore depart from \"zz\". Fatio proposed that the mean velocity of the reflected particles is lower and therefore their momentum is weaker than that of the incident particles. The result is \"one stream\", which pushes all bodies in the direction of \"zz\". So on one hand the speed of the stream remains constant, but on the other hand at larger proximity to \"zz\" the density of the stream increases and therefore its intensity is proportional to 1/r. And because one can draw an infinite number of such pyramids around \"C\", the proportionality applies to the entire range around \"C\".\n\nIn order to justify the assumption, that the particles are traveling after their reflection with diminished velocities, Fatio stated the following assumptions:\nThese passages are the most incomprehensible parts of Fatio's theory, because he never clearly decided which sort of collision he actually preferred. However, in the last version of his theory in 1742 he shortened the related passages and ascribed \"perfect elasticity or spring force\" to the particles and on the other hand \"imperfect elasticity\" to gross matter, therefore the particles would be reflected with diminished velocities. Additionally, Fatio faced another problem: What is happening if the particles collide with each other? Inelastic collisions would lead to a steady decrease of the particle speed and therefore a decrease of the gravitational force. To avoid this problem, Fatio supposed that the diameter of the particles is very small compared to their mutual distance, so their interactions are very rare.\n\nFatio thought for a long time that, since corpuscles approach material bodies at a higher speed than they recede from them (after reflection), there would be a progressive accumulation of corpuscles near material bodies (an effect which he called \"condensation\"). However, he later realized that although the incoming corpuscles are quicker, they are spaced further apart than are the reflected corpuscles, so the inward and outward flow rates are the same. Hence there is no secular accumulation of corpuscles, i.e., the density of the reflected corpuscles remains constant (assuming that they are small enough that no noticeably greater rate of self-collision occurs near the massive body). More importantly, Fatio noted that, by increasing both the velocity and the elasticity of the corpuscles, the difference between the speeds of the incoming and reflected corpuscles (and hence the difference in densities) can be made arbitrarily small while still maintaining the same effective gravitational force.\n\n\nIn order to ensure mass proportionality, Fatio assumed that gross matter is extremely permeable to the flux of corpuscles. He sketched 3 models to justify this assumption:\n\nAlready in 1690 Fatio assumed, that the \"push force\" exerted by the particles on a plain surface is the sixth part of the force, which would be produced if all particles are lined up normal to the surface. Fatio now gave a proof of this proposal by determination of the force, which is exerted by the particles on a certain point zz. He derived the formula \"p=ρvzz/6\". This solution is very similar to the formula known in the kinetic theory of gases \"p=ρv/3\", which was found by Daniel Bernoulli in 1738. This was the first time that a solution analogous to the similar result in kinetic theory was pointed out - long \"before\" the basic concept of the latter theory was developed. However, Bernoulli's value is twice as large as Fatio's one, because according to Zehe, Fatio only calculated the value \"mv\" for the change of impulse after the collision, but not \"2mv\" and therefore got the wrong result. (His result is only correct in the case of totally inelastic collisions.) Fatio tried to use his solution not only for explaining gravitation, but for explaining the behaviour of gases as well. He tried to construct a thermometer, which should indicate the \"state of motion\" of the air molecules and therefore estimate the temperature. But Fatio (unlike Bernoulli) did not \"identify\" heat and the movements of the air particles - he used another fluid, which should be responsible for this effect. It is also unknown, whether Bernoulli was influenced by Fatio or not.\n\nIn this chapter Fatio examines the connections between the term \"infinity\" and its relations to his theory. Fatio often justified his considerations with the fact that different phenomena are \"infinitely smaller or larger\" than others and so many problems can be reduced to an undetectable value. For example, the diameter of the bars is infinitely smaller than their distance to each other; or the speed of the particles is infinitely larger than those of gross matter; or the speed difference between reflected and non-reflected particles is infinitely small.\n\nThis is the mathematically most complex part of Fatio's theory. There he tried to estimate the resistance of the particle streams for moving bodies. Supposing \"u\" is the velocity of gross matter, \"v\" is the velocity of the gravific particles and \"ρ\" the density of the medium. In the case \"v « u\" and \"ρ = const.\" Fatio stated that the resistance is \"ρu\". In the case \"v » u\" and \"ρ = const.\" the resistance is \"4/3ρuv\". Now, Newton stated that the lack of resistance to the orbital motion requires an extreme sparseness of any medium in space. So Fatio decreased the density of the medium and stated, that to maintain sufficient gravitational force this reduction must be compensated by changing v \"inverse proportional to the square root of the density\". This follows from Fatio's particle pressure, which is proportional to \"ρv\". According to Zehe, Fatio's attempt to increase v to a very high value would actually leave the resistance very small compared with gravity, because the resistance in Fatio's model is proportional to \"ρuv\" but gravity (i.e. the particle pressure) is proportional to \"ρv\".\n\nFatio was in communication with some of the most famous scientists of his time.\nThere was a strong personal relationship between Isaac Newton and Fatio in the years 1690 to 1693. Newton's statements on Fatio's theory differed widely. For example, after describing the necessary conditions for a mechanical explanation of gravity, he wrote in an (unpublished) note in his own printed copy of the \"Principia\" in 1692:\"The unique hypothesis by which gravity can be explained is however of this kind, and was first devised by the most ingenious geometer Mr. N. Fatio.\" On the other hand, Fatio himself stated that although Newton had commented privately that Fatio's theory was the best possible \"mechanical\" explanation of gravity, he also acknowledged that Newton tended to believe that the true explanation of gravitation was not mechanical. Also, Gregory noted in his \"Memoranda\": \"Mr. Newton and Mr. Halley laugh at Mr. Fatio’s manner of explaining gravity.\" This was allegedly noted by him on December 28, 1691. However, the real date is unknown, because both ink and feather which were used, differ from the rest of the page. After 1694, the relationship between the two men cooled down.\n\nChristiaan Huygens was the first person informed by Fatio of his theory, but never accepted it. Fatio believed he had convinced Huygens of the consistency of his theory, but Huygens denied this in a letter to Gottfried Leibniz. There was also a short correspondence between Fatio and Leibniz on the theory. Leibniz criticized Fatio's theory for demanding empty space between the particles, which was rejected by him (Leibniz) on philosophical grounds. Jakob Bernoulli expressed an interest in Fatio's Theory, and urged Fatio to write his thoughts on gravitation in a complete manuscript, which was actually done by Fatio. Bernoulli then copied the manuscript, which now resides in the university library of Basel, and was the base of the Bopp edition.\n\nNevertheless, Fatio's theory remained largely unknown with a few exceptions like Cramer and Le Sage, because he never was able to formally publish his works and he fell under the influence of a group of religious fanatics called the \"French prophets\" (which belonged to the camisards) and therefore his public reputation was ruined.\n\nIn 1731 the Swiss mathematician Gabriel Cramer published a dissertation, at the end of which appeared a sketch of a theory very similar to Fatio's - including net structure of matter, analogy to light, shading - but without mentioning Fatio's name. It was known to Fatio that Cramer had access to a copy of his main paper, so he accused Cramer of only repeating his theory without understanding it. It was also Cramer who informed Le Sage about Fatio's theory in 1749. In 1736 the German physician Franz Albert Redeker also published a similar theory. Any connection between Redeker and Fatio is unknown.\n\nThe first exposition of his theory, \"Essai sur l'origine des forces mortes\", was sent by Le Sage to the Academy of Sciences at Paris in 1748, but it was never published. According to Le Sage, \"after\" creating and sending his essay he was informed on the theories of Fatio, Cramer and Redeker. In 1756 for the first time one of his expositions of the theory was published, and in 1758 he sent a more detailed exposition, \"Essai de Chymie Méchanique\", to a competition to the Academy of Sciences in Rouen. In this paper he tried to explain both the nature of gravitation and chemical affinities. The exposition of the theory which became accessible to a broader public, \"Lucrèce Newtonien\" (1784), in which the correspondence with Lucretius’ concepts was fully developed. Another exposition of the theory was published from Le Sage's notes posthumously by Pierre Prévost in 1818.\n\nLe Sage discussed the theory in great detail and he proposed quantitative estimates for some of the theory's parameters.\nLe Sage said that he was the first one, who drew all consequences from the theory and also Prévost said that Le Sage's theory was more developed than Fatio's theory. However, by comparing the two theories and after a detailed analysis of Fatio's papers (which also were in possession of Le Sage) Zehe judged that Le Sage contributed nothing essentially new and he often did not reach Fatio's level.\n\nLe Sage’s ideas were not well-received during his day, except for some of his friends and associates like Pierre Prévost, Charles Bonnet, Jean-André Deluc, Charles Mahon, 3rd Earl Stanhope and Simon Lhuilier. They mentioned and described Le Sage's theory in their books and papers, which were used by their contemporaries as a secondary source for Le Sage's theory (because of the lack of published papers by Le Sage himself) .\n\nLeonhard Euler once remarked that Le Sage's model was \"infinitely better\" than that of all other authors, and that all objections are balanced out in this model, but later he said the analogy to light had no weight for him, because he believed in the wave nature of light. After further consideration, Euler came to disapprove of the model, and he wrote to Le Sage:\n\nDaniel Bernoulli was pleased by the similarity of Le Sage's model and his own thoughts on the nature of gases. However, Bernoulli himself was the opinion that his own kinetic theory of gases was only a speculation, and likewise he regarded Le Sage's theory as highly speculative.\n\nRoger Joseph Boscovich pointed out, that Le Sage's theory is the first one, which actually can explain gravity by mechanical means. However, he rejected the model because of the enormous and unused quantity of ultramundane matter. John Playfair described Boscovich's arguments by saying:\n\nA very similar argument was later given by Maxwell (see the sections below). Additionally, Boscovich denied the existence of all contact and immediate impulse at all, but proposed repulsive and attractive actions at a distance.\n\nGeorg Christoph Lichtenberg's knowledge of Le Sage's theory was based on \"Lucrece Newtonien\" and a summary by Prévost. Lichtenberg originally believed (like Descartes) that every explanation of natural phenomena must be based on rectilinear motion and impulsion, and Le Sage's theory fulfilled these conditions. In 1790 he expressed in one of his papers his enthusiasm for the theory, believing that Le Sage's theory embraces all of our knowledge and makes any further dreaming on that topic useless. He went on by saying: \"If it is a dream, it is the greatest and the most magnificent which was ever dreamed...\" and that we can fill with it a gap in our books, which can only be filled by a dream.\n\nHe often referred to Le Sage's theory in his lectures on physics at the University of Göttingen. However, around 1796 Lichtenberg changed his views after being persuaded by the arguments of Immanuel Kant, who criticized any kind of theory that attempted to replace attraction with impulsion. Kant pointed out that the very existence of spatially extended configurations of matter, such as particles of non-zero radius, implies the existence of some sort of binding force to hold the extended parts of the particle together. Now, that force cannot be explained by the push from the gravitational particles, because those particles too must hold together in the same way. To avoid this circular reasoning, Kant asserted that there must exist a fundamental attractive force. This was precisely the same objection that had always been raised against the impulse doctrine of Descartes in the previous century, and had led even the followers of Descartes to abandon that aspect of his philosophy.\n\nAnother German philosopher, Friedrich Wilhelm Joseph Schelling, rejected Le Sage's model because its mechanistic materialism was incompatible with Schelling's very idealistic and anti-materialistic philosophy.\n\nPartly in consideration of Le Sage's theory, Pierre-Simon Laplace undertook to determine the necessary speed of gravity in order to be consistent with astronomical observations. He calculated that the speed must be “at least a hundred millions of times greater than that of light”, in order to avoid unacceptably large inequalities due to aberration effects in the lunar motion. This was taken by most researchers, including Laplace, as support for the Newtonian concept of instantaneous action at a distance, and to indicate the implausibility of any model such as Le Sage's. Laplace also argued that to maintain mass-proportionality the upper limit for earth's molecular surface area is at the most the ten-millionth of earth surface. To Le Sage's disappointment, Laplace never directly mentioned Le Sage's theory in his works.\n\nBecause the theories of Fatio, Cramer and Redeker were not widely known, Le Sage's exposition of the theory enjoyed a resurgence of interest in the latter half of the 19th century, coinciding with the development of the kinetic theory.\n\nSince Le Sage's particles must lose speed when colliding with ordinary matter (in order to produce a net gravitational force), a huge amount of energy must be converted to internal energy modes. If those particles have no internal energy modes, the excess energy can only be absorbed by ordinary matter. Addressing this problem, Armand Jean Leray proposed a particle model (perfectly similar to Le Sage's) in which he asserted that the absorbed energy is used by the bodies to produce magnetism and heat. He suggested, that this might be an answer for the question of where the energy output of the stars comes from.\n\n\nLe Sage's \"own\" theory became a subject of re-newed interest in the latter part of the 19th century following a paper published by Kelvin in 1873. Unlike Leray, who treated the heat problem imprecisely, Kelvin stated that the absorbed energy represents a very high heat, sufficient to vaporize any object in a fraction of a second. So Kelvin re-iterated an idea that Fatio had originally proposed in the 1690s for attempting to deal with the thermodynamic problem inherent in Le Sage's theory. He proposed that the excess heat might be absorbed by internal energy modes of the particles themselves, based on his proposal of the vortex-nature of matter. In other words, the original translational kinetic energy of the particles is transferred to internal energy modes, chiefly vibrational or rotational, of the particles. Appealing to Clausius's proposition that the energy in any particular mode of a gas molecule tends toward a fixed ratio of the total energy, Kelvin went on to suggest that the \"energized\" but slower moving particles would subsequently be restored to their original condition due to collisions (on the cosmological scale) with other particles. Kelvin also asserted that it would be possible to extract limitless amounts of free energy from the ultramundane flux, and described a perpetual motion machine to accomplish this.\n\nSubsequently, Peter Guthrie Tait called the Le Sage theory the only plausible explanation of gravitation which has been propounded at that time. He went on by saying:\n\nKelvin himself, however, was not optimistic that Le Sage's theory could ultimately give a satisfactory account of phenomena. After his brief paper in 1873 noted above, he never returned to the subject, except to make the following comment:\n\nSamuel Tolver Preston illustrated that many of the postulates introduced by Le Sage concerning the gravitational particles, such as rectilinear motion, rare interactions, \"etc.\"., could be collected under the single notion that they behaved (on the cosmological scale) as the particles of a gas with an extremely long mean free path. Preston also accepted Kelvin's proposal of internal energy modes of the particles. He illustrated Kelvin's model by comparing it with the collision of a steel ring and an anvil - the anvil would not be shaken very much, but the steel ring would be in a state of vibration and therefore departs with diminished velocity. He also argued, that the mean free path of the particles is at least the distance between the planets - on longer distances the particles regain their translational energy due collisions with each other, so he concluded that on longer distances there would be no attraction between the bodies, \"independent of their size\". Paul Drude suggested that this could possibly be a connection with some theories of Carl Gottfried Neumann and Hugo von Seeliger, who proposed some sort of absorption of gravity in open space.\n\n\nA review of the Kelvin-Le Sage theory was published by James Clerk Maxwell in the Ninth Edition of the Encyclopædia Britannica under the title \"Atom\" in 1875. After describing the basic concept of the theory he wrote (with sarcasm according to Aronson):\n\nMaxwell commented on Kelvin’s suggestion of different energy modes of the particles that this implies the gravitational particles are not simple primitive entities, but rather systems, with their own internal energy modes, which must be held together by (unexplained) forces of attraction. He argues that the temperature of bodies must tend to approach that at which the average kinetic energy of a molecule of the body would be equal to the average kinetic energy of an ultra-mundane particle and he states that the latter quantity must be much greater than the former and concludes that ordinary matter should be incinerated within seconds under the Le Sage bombardment. He wrote:\n\nMaxwell also argued that the theory requires \"an enormous expenditure of external power\" and therefore violating the conservation of energy as the fundamental principle of nature. Preston responded to Maxwell's criticism by arguing that the kinetic energy of each individual simple particle could be made arbitrarily low by positing a sufficiently low mass (and higher number density) for the particles. But this issue later was discussed in a more detailed way by Poincaré, who showed that the thermodynamic problem within Le Sage models remained unresolved.\n\nCaspar Isenkrahe presented his model in a variety of publications between 1879-1915.\n\nHis basic assumptions were very similar to those of Le Sage and Preston, but he gave a more detailed application of the kinetic theory. However, by asserting that the velocity of the corpuscles after collision was reduced without any corresponding increase in the energy of any other object, his model violated the conservation of energy. He noted that there is a connection between the weight of a body and its density (because any decrease in the density of an object reduces the internal shielding) so he went on to assert that warm bodies should be heavier than colder ones (related to the effect of thermal expansion).\n\nIn another model Adalbert Ryšánek in 1887\n\nalso gave a careful analysis, including an application of Maxwell's law of the particle velocities in a gas. He distinguished between a gravitational and a luminiferous aether. This separation of those two mediums was necessary, because according to his calculations the absence of any drag effect in the orbit of Neptune implies a lower limit for the particle velocity of 5 · 10 cm/s. He (like Leray) argued that the absorbed energy is converted into heat, which might be transferred into the luminiferous aether and/or is used by the stars to maintain their energy output. However, these qualitative suggestions were unsupported by any quantitative evaluation of the amount of heat actually produced.\n\nIn 1888 Paul du Bois-Reymond argued against Le Sage's model, partly because the predicted force of gravity in Le Sage's theory is not strictly proportional to mass. In order to achieve exact mass proportionality as in Newton's theory (which implies no shielding or saturation effects and an infinitely porous structure of matter), the ultramundane flux must be infinitely intense. Du Bois-Reymond rejected this as absurd. In addition, du Bois-Reymond like Kant observed that Le Sage's theory cannot meet its goal, because it invokes concepts like \"elasticity\" and \"absolute hardness\" etc., which (in his opinion) can only be explained by means of attractive forces. The same problem arises for the cohesive forces in molecules. As a result, the basic intent of such models, which is to dispense with elementary forces of attraction, is impossible.\n\nIn 1863, François Antoine Edouard and Em. Keller presented a theory by using a Le Sage type mechanism in combination with longitudinal waves of the aether. They supposed that those waves are propagating in every direction and losing some of their momentum after the impact on bodies, so between two bodies the pressure exerted by the waves is weaker than the pressure around them. In 1869, Paul-Emile Lecoq de Boisbaudran presented the same model as Leray (including absorption and the production of heat etc.), but like Keller and Keller, he replaced the particles with longitudinal waves of the aether.\n\n\nAfter these attempts, other authors in the early 20th century substituted electromagnetic radiation for Le Sage’s particles. This was in connection with Lorentz ether theory and the electron theory of that time, in which the electrical constitution of matter was assumed.\n\nIn 1900 Hendrik Lorentz wrote that Le Sage's particle model is not consistent with the electron theory of his time. But the realization that trains of electromagnetic waves could produce some pressure, in combination with the penetrating power of Röntgen rays (now called x-rays), led him to conclude that nothing argues against the possible existence of even more penetrating radiation than x-rays, which could replace Le Sage's particles. Lorentz showed that an attractive force between charged particles (which might be taken to model the elementary subunits of matter) would indeed arise, but only if the incident energy were entirely absorbed. This was the same fundamental problem which had afflicted the particle models. So Lorentz wrote:\n\nIn 1922 Lorentz first examined Martin Knudsen's investigation on rarefied gases and in connection with that he discussed Le Sage's particle model, followed by a summary of his own electromagnetic Le Sage model - but he repeated his conclusion from 1900: Without absorption no gravitational effect.\n\nIn 1913 David Hilbert referred to Lorentz's theory and criticised it by arguing that no force in the form 1/r can arise, if the mutual distance of the atoms is large enough when compared with their wavelength.\n\nIn 1904 J. J. Thomson considered a Le Sage-type model in which the primary ultramundane flux consisted of a hypothetical form of radiation much more penetrating even than x-rays. He argued that Maxwell's heat problem might be avoided by assuming that the absorbed energy is not converted into heat, but \"re-radiated\" in a still more penetrating form. He noted that this process possibly can explain where the energy of radioactive substances comes from - however, he stated that an internal cause of radioactivity is more probable. In 1911 Thomson went back to this subject in his article \"Matter\" in the Encyclopædia Britannica Eleventh Edition. There he stated, that this form of secondary radiation is somewhat analogous to how the passage of electrified particles through matter causes the radiation of the even more penetrating x-rays. He remarked:\n\n\nUnlike Lorentz and Thomson, Thomas Tommasina between 1903 and 1928 suggested long wavelength radiation to explain gravity, and short wavelength radiation for explaining the cohesive forces of matter. Charles F. Brush in 1911 also proposed long wavelength radiation. But he later revised his view and changed to extremely short wavelengths.\n\nIn 1905, George Darwin subsequently calculated the gravitational force between two bodies at extremely close range to determine if geometrical effects would lead to a deviation from Newton’s law. Here Darwin replaced Le Sage's cage-like units of ordinary matter with microscopic hard spheres of uniform size. He concluded that only in the instance of perfectly inelastic collisions (zero reflection) would Newton’s law stand up, thus reinforcing the thermodynamic problem of Le Sage's theory. Also, such a theory is only valid if the normal \"and\" the tangential components of impact are totally inelastic (contrary to Le Sage's scattering mechanism), and the elementary particles are exactly of the same size. He went on to say that the emission of light is the exact converse of the absorption of Le Sage's particles. A body with different surface temperatures will move in the direction of the colder part. In a later review of gravitational theories, Darwin briefly described Le Sage's theory and said he gave the theory serious consideration, but then wrote:\n\n\nPartially based on the calculations of Darwin, an important criticism was given by Henri Poincaré in 1908. He concluded that the attraction is proportional to formula_1, where \"S\" is earth's molecular surface area, \"v\" is the velocity of the particles, and ρ is the density of the medium. Following Laplace, he argued that to maintain mass-proportionality the upper limit for \"S\" is at the most a ten-millionth of the Earth's surface. Now, drag (i.e. the resistance of the medium) is proportional to \"Sρv\" and therefore the ratio of drag to attraction is inversely proportional to \"Sv\". To reduce drag, Poincaré calculated a lower limit for \"v\" = 24 · 10 times the speed of light. So there are lower limits for \"Sv\" and v, and an upper limit for \"S\" and with those values one can calculate the produced heat, which is proportional to \"Sρv\". The calculation shows that earth's temperature would rise by 10 degrees per second. Poincaré noticed, \"that the earth could not long stand such a regime.\" Poincaré also analyzed some wave models (Tommasina and Lorentz), remarking that they suffered the same problems as the particle models. To reduce drag, superluminal wave velocities were necessary, and they would still be subject to the heating problem. After describing a similar re-radiation model like Thomson, he concluded: \"Such are the complicated hypotheses to which we are led when we seek to make Le Sage's theory tenable\".\n\nHe also stated that if in Lorentz' model the absorbed energy is fully converted into heat, that would raise earth's temperature by 10 degrees per second. Poincaré then went on to consider Le Sage's theory in the context of the \"new dynamics\" that had been developed at the end of the 19th and the beginning of the 20th centuries, specifically recognizing the relativity principle. For a particle theory, he remarked that \"it is difficult to imagine a law of collision compatible with the principle of relativity\", and the problems of drag and heating remain.\n\nA basic prediction of the theory is the extreme porosity of matter. As supposed by Fatio and Le Sage in 1690/1758 (and before them, Huygens) matter must consist mostly of empty space so that the very small particles can penetrate the bodies nearly undisturbed and therefore every single part of matter can take part in the gravitational interaction. This prediction has been (in some respects) confirmed over the course of the time. Indeed, matter consists mostly of empty space and certain particles like neutrinos can pass through matter nearly unhindered. However, the image of elementary particles as classical entities who interact directly, determined by their shapes and sizes (in the sense of the net structure proposed by Fatio/Le Sage and the equisized spheres of Isenkrahe/Darwin), is not consistent with current understanding of elementary particles. The Lorentz/Thomson proposal of electrical charged particles as the \"basic constituents\" of matter is inconsistent with current physics as well.\n\nEvery Le Sage-type model assumes the existence of a space-filling isotropic flux or radiation of enormous intensity and penetrating capability. This has some similarity to the cosmic microwave background radiation (CMBR) discovered in the 20th century. CMBR is indeed a space-filling and fairly isotropic flux, but its intensity is extremely small, as is its penetrating capability. The flux of neutrinos, emanating from (for example) the sun, possesses the penetrating properties envisaged by Le Sage for his ultramundane corpuscles, but this flux is not isotropic (since individual stars are the main sources of neutrinos) and the intensity is even less than that of the CMBR. Of course, neither the CMBR nor neutrinos propagate at superluminal speeds, which is another necessary attribute of Le Sage’s particles. From a more modern point of view, discarding the simple “push” concept of Le Sage, the suggestion that the neutrino (or some other particle similar to the neutrino) might be the mediating particle in a quantum field theory of gravitation was considered and disproved by Feynman.\n\nAlthough matter is postulated to be very sparse in the Fatio-Le Sage theory, it cannot be perfectly transparent, because in that case no gravitational force would exist. However, the lack of perfect transparency leads to problems: with sufficient mass the amount of shading produced by two pieces of matter becomes less than the sum of the shading that each of them would produce separately, due to the overlap of their shadows (P10, above). This hypothetical effect, called gravitational shielding, implies that addition of matter does not result in a direct proportional increase in the gravitational mass. Therefore, in order to be viable, Fatio and Le Sage postulated that the shielding effect is so small as to be undetectable, which requires that the interaction cross-section of matter must be extremely small (P10, below). This places an extremely high lower-bound on the intensity of the flux required to produce the observed force of gravity. Any form of gravitational shielding would represent a violation of the equivalence principle, and would be inconsistent with the extremely precise null result observed in the Eötvös experiment and its successors — all of which have instead confirmed the precise equivalence of active and passive gravitational mass with inertial mass that was predicted by general relativity. For more historical information on the connection between gravitational shielding and Le Sage gravity, see Martins, and Borzeszkowski et al.\n\nSince Isenkrahe's proposal on the connection between density, temperature and weight was based purely on the anticipated effects of changes in material \"density\", and since temperature at a given density can be increased or decreased, Isenkrahe's comments do not imply any fundamental relation between temperature and gravitation. (There actually \"is\" a relation between temperature and gravitation, as well as between binding energy and gravitation, but these actual effects have nothing to do with Isenkrahe's proposal. See the section below on \"Coupling to Energy\".) Regarding the prediction of a relation between gravitation and density, all experimental evidence indicates that there is no such relation.\n\nAccording to Le Sage's theory, an isolated body is subjected to drag if it is in motion relative to the unique isotropic frame of the ultramundane flux (i.e., the frame in which the speed of the ultramundane corpuscles is the same in all directions). This is due to the fact that, if a body is in motion, the particles striking the body from the front have a higher speed (relative to the body) than those striking the body from behind - this effect will act to \"decrease\" the distance between the sun and the earth. The magnitude of this drag is proportional to vu, where v is the speed of the particles and u is the speed of the body, whereas the characteristic force of gravity is proportional to v, so the ratio of drag to gravitational force is proportional to u/v. Thus for a given characteristic strength of gravity, the amount of drag for a given speed u can be made arbitrarily small by increasing the speed v of the ultramundane corpuscles. However, in order to reduce the drag to an acceptable level (i.e., consistent with observation) in terms of classical mechanics, the speed v must be many orders of magnitude greater than the speed of light. This makes Le Sage theory fundamentally incompatible with the modern science of mechanics based on special relativity, according to which no particle (or wave) can exceed the speed of light. In addition, even if superluminal particles were possible, the effective temperature of such a flux would be sufficient to incinerate all ordinary matter in a fraction of a second.\n\nAs shown by Laplace, another possible Le Sage effect is orbital aberration due to finite speed of gravity. Unless the Le Sage particles are moving at speeds much greater than the speed of light, as Le Sage and Kelvin supposed, there is a time delay in the interactions between bodies (the transit time). In the case of orbital motion this results in each body reacting to a retarded position of the other, which creates a leading force component. Contrary to the drag effect, this component will act to accelerate both objects away from each other. In order to maintain stable orbits, the effect of gravity must either propagate much faster than the speed of light or must not be a purely central force. This has been suggested by many as a conclusive disproof of any Le Sage type of theory. In contrast, general relativity is consistent with the lack of appreciable aberration identified by Laplace, because even though gravity propagates at the speed of light in general relativity, the expected aberration is almost exactly cancelled by velocity-dependent terms in the interaction.\n\nIn many particle models, such as Kelvin's, the range of gravity is limited due to the nature of particle interactions amongst themselves. The range is effectively determined by the rate that the proposed \"internal modes\" of the particles can eliminate the momentum defects (\"shadows\") that are created by passing through matter. Such predictions as to the effective range of gravity will vary and are dependent upon the specific aspects and assumptions as to the modes of interactions that are available during particle interactions. However, for this class of models the observed large-scale structure of the cosmos constrains such dispersion to those that will allow for the aggregation of such immense gravitational structures.\n\nAs noted in the historical section, a major problem for every Le Sage model is the energy and heat issue. As Maxwell and Poincaré showed, inelastic collisions lead to a vaporization of matter within fractions of a second and the suggested solutions were not convincing. For example, Aronson gave a simple proof of Maxwell's assertion:\n\nLikewise Isenkrahe's violation of the energy conservation law is unacceptable, and Kelvin's application of Clausius' theorem leads (as noted by Kelvin himself) to some sort of perpetual motion mechanism. The suggestion of a secondary re-radiation mechanism for wave models attracted the interest of JJ Thomson, but was not taken very seriously by either Maxwell or Poincaré, because it entails a gross violation of the second law of thermodynamics (huge amounts of energy spontaneously being converted from a colder to a hotter form), which is one of the most solidly established of all physical laws.\n\nThe energy problem has also been considered in relation to the idea of mass accretion in connection with the Expanding Earth theory. Among the early theorists to link mass increase in some sort of push gravity model to Earth expansion were Yarkovsky and Hilgenberg. The idea of mass accretion and the expanding earth theory are not currently considered to be viable by mainstream scientists. This is because, among other reasons, according to the principle of mass-energy equivalence, if the Earth was absorbing the energy of the ultramundane flux at the rate necessary to produce the observed force of gravity (i.e. by using the values calculated by Poincaré), its mass would be doubling in each fraction of a second.\n\nBased on observational evidence, it is now known that gravity interacts with all forms of energy, and not just with mass. The electrostatic binding energy of the nucleus, the energy of weak interactions in the nucleus, and the kinetic energy of electrons in atoms, all contribute to the gravitational mass of an atom, as has been confirmed to high precision in Eötvös type experiments.\nThis means, for example, that when the atoms of a quantity of gas are moving more rapidly, the gravitation of that gas increases.\nMoreover, Lunar Laser Ranging experiments have shown that even gravitational binding energy itself also gravitates, with a strength consistent with the equivalence principle to high precision\nwhich furthermore demonstrates that any successful theory of gravitation must be nonlinear and self-coupling.\n\nLe Sage's theory does not predict any of these aforementioned effects, nor do any of the known variants of Le Sage's theory.\n\nLyman Spitzer in 1941 calculated, that absorption of radiation between two dust particles lead to a net attractive force which varies proportional to 1/r (evidently he was unaware of Le Sage's shadow mechanism and especially Lorentz's considerations on radiation pressure and gravity). George Gamow, who called this effect \"mock gravity\", proposed in 1949 that after the big bang the temperature of the electrons has dropped faster than the temperature of the background radiation. Absorption of the radiation lead to a Lesage mechanism between the electrons, which might have had an important role in the process of galaxy formation shortly after the big bang. However, this proposal was disproved by Field in 1971, who showed that this effect was much too small, because electrons and the radiation were nearly in thermal equilibrium. Hogan and White proposed in 1986 that mock gravity might have influenced the galaxy formation by absorption of pregalactic starlight. But it was shown by Wang and Field that any form of mock gravity is incapable of producing enough force to influence galaxy formation.\n\nThe Le Sage mechanism also has been identified as a significant factor in the behavior of dusty plasma. A.M. Ignatov has shown that an attractive force arises between two dust grains suspended in an isotropic collisionless plasma due to inelastic collisions between ions of the plasma and the grains of dust. This attractive force is inversely proportional to the square of the distance between dust grains, and can counterbalance the Coulomb repulsion between dust grains.\n\nIn quantum field theory the existence of virtual particles is proposed, which lead to the so-called Casimir effect. Casimir calculated that between two plates only particles with specific wavelengths should be counted when calculating the vacuum energy. Therefore, the energy density between the plates is less if the plates are close together, leading to a net attractive force between the plates. However, the conceptual framework of this effect is very different from the theory of Fatio and Le Sage.\n\nThe re-examination of Le Sage's theory in the 19th century identified several closely interconnected problems with the theory. These relate to excessive heating, frictional drag, shielding, and gravitational aberration. The recognition of these problems, in conjunction with a general shift away from mechanical based theories, resulted in a progressive loss of interest in Le Sage’s theory. Ultimately in the 20th century Le Sage’s theory was eclipsed by Einstein’s theory of general relativity.\n\nIn 1965 Richard Feynman examined the Fatio/Lesage mechanism, primarily as an example of an attempt to explain a \"complicated\" physical law (in this case, Newton's inverse-square law of gravity) in terms of simpler primitive operations without the use of complex mathematics, and also as an example of a failed theory. He notes that the mechanism of \"bouncing particles\" reproduces the inverse-square force law and that \"the strangeness of the mathematical relation will be very much reduced\", but then remarks that the scheme \"does not work\", because of the drag it predicts would be experienced by moving bodies, \"so that is the end of that theory\".\n\nAlthough it is not regarded as a viable theory within the mainstream scientific community, there are occasional attempts to re-habilitate the theory outside the mainstream, including those of Radzievskii and Kagalnikova (1960), Shneiderov (1961), Buonomano and Engels (1976), Adamut (1982), Popescu (1982), Jaakkola (1996), Tom Van Flandern (1999), Edwards (2007) and Eid (2016).\n\nA variety of Le Sage models and related topics are discussed in Edwards, et al.\n"}
{"id": "14733509", "url": "https://en.wikipedia.org/wiki?curid=14733509", "title": "List of California state symbols", "text": "List of California state symbols\n\nThis is a list of the officially designated symbols of the U.S. state of California. Most such designations are found in sections 420-429.8 of the California Government Code.\n\n\n"}
{"id": "17885119", "url": "https://en.wikipedia.org/wiki?curid=17885119", "title": "List of Internet pioneers", "text": "List of Internet pioneers\n\nInstead of a single \"inventor\", the Internet was developed by many people over many years. The following are some Internet pioneers who contributed to its early development. These include early theoretical foundations, specifying original protocols, and expansion beyond a research tool to wide deployment.\n\nClaude Shannon (1916–2001) called the \"father of modern information theory\", published \"A Mathematical Theory of Communication\" in 1948. His paper gave a formal way of studying communication channels. It established fundamental limits on the efficiency of communication over noisy channels, and presented the challenge of finding families of codes to achieve capacity.\n\nVannevar Bush (1890–1974) helped to establish a partnership between U.S. military, university research, and independent think tanks. He was appointed Chairman of the National Defense Research Committee in 1940 by President Franklin D. Roosevelt, appointed Director of the Office of Scientific Research and Development in 1941, and from 1946 to 1947, he served as chairman of the Joint Research and Development Board. Out of this would come DARPA, which in turn would lead to the ARPANET Project. His July 1945 \"Atlantic Monthly\" article \"As We May Think\" proposed Memex, a theoretical proto-hypertext computer system in which an individual compresses and stores all of their books, records, and communications, which is then mechanized so that it may be consulted with exceeding speed and flexibility.\n\nPaul Baran (1926–2011) developed the field of redundant distributed networks while conducting research at RAND Corporation starting in 1959 when Baran began investigating the development of survivable communication networks. This led to a series of papers titled \"On Distributed communications\" that in 1964 described a detailed architecture for a distributed survivable packet switched communications network. In 2012, Baran was inducted into the Internet Hall of Fame by the Internet Society.\n\nJoseph Carl Robnett Licklider (1915–1990) was a faculty member of Massachusetts Institute of Technology, and researcher at Bolt, Beranek and Newman. He developed the idea of a universal network at the Information Processing Techniques Office (IPTO) of the United States Department of Defense Advanced Research Projects Agency (ARPA). He headed IPTO from 1962 to 1963, and again from 1974 to 1975. His 1960 paper envisions that mutually-interdependent, \"living together\", tightly-coupled human brains and computing machines would prove to complement each other's strengths.\n\nDonald Davies (1924–2000) independently invented and named the concept of packet switching in 1965 at the United Kingdom's National Physical Laboratory (NPL). In the same year, he proposed a national data network based on packet switching in the UK. After the proposal was not taken up nationally, during 1966 he headed a team which produced a design for a local area network to serve the needs of NPL and prove the feasibility of packet switching. In 1967, a written version of the proposal entitled \"NPL Data Network\" was presented by a member of his team (Roger Scantlebury) at the ACM conference in Gatlinburg. The paper described how equipment (\"nodes\") used to transmit signals (\"packets\") would be connected by electrical links to re-transmit the signals between and to the nodes, and interface computers would be used to link node networks to so-called time-sharing computers and other users. The interface computers would transmit multiplex signals between networks, and nodes would switch transmissions while connected to electrical circuitry functioning at a rate of processing amounting to mega-bits. Scantlebury suggested packet switching for use in the ARPANET; Larry Roberts incorporated it into the design and sought input from Paul Baran as well. Davies gave the first public demonstration of packet switching in 1968 and built the local area NPL network in England, influencing other research in the UK and Europe. The NPL network followed by ARPANET were the first two networks in the world to use packet switching. In 2012, Davies was inducted into the Internet Hall of Fame by the Internet Society.\n\nCharles M. Herzfeld (1925-2017) was an American scientist and scientific manager, best known for his time as Director of DARPA, during which, among other things, he personally took the decision to authorize the creation of the ARPANET, the predecessor of the Internet.\n\nIn 2012, Herzfeld was inducted into the Internet Hall of Fame by the Internet Society.\n\nRobert W. Taylor (February 10, 1932 – April 13, 2017) was director of ARPA's Information Processing Techniques Office from 1965 through 1969, where he convinced ARPA to fund a computer network. From 1970 to 1983, he managed the Computer Science Laboratory of the Xerox Palo Alto Research Center (PARC), where technologies such as Ethernet and the Xerox Alto were developed. He was the founder and manager of Digital Equipment Corporation's Systems Research Center until 1996. The 1968 paper, \"The Computer as a Communication Device\", that he wrote together with J.C.R. Licklider starts out: \"In a few years, men will be able to communicate more effectively through a machine than face to face.\" And while their vision would take more than \"a few years\", the paper lays out the future of what the Internet would eventually become.\n\nLawrence G. \"Larry\" Roberts (born 1937) is an American computer scientist. After earning his PhD in electrical engineering from MIT in 1963, Roberts continued to work at MIT's Lincoln Laboratory where in 1965 he connected Lincoln Lab's TX-2 computer to the SDC Q-32 computer in Santa Monica. In 1966, he became a program manager in the ARPA Information Processing Techniques Office (IPTO), where he led the development of the ARPANET, the first wide area packet switching network. Roberts applied Donald Davies' concepts of packet switching for the ARPANET, and also sought input from Paul Baran and Leonard Kleinrock. After Robert Taylor left ARPA in 1969, Roberts became director of the IPTO. In 1973, he left ARPA to commercialize the nascent technology in the form of Telenet, the first data network utility, and served as its CEO from 1973 to 1980. In 2012, Roberts was inducted into the Internet Hall of Fame by the Internet Society.\n\nLeonard Kleinrock (born 1934) published his first paper on queueing theory in digital networks, \"Information Flow in Large Communication Nets\", in 1961. After completing his Ph.D. thesis in 1962, which provided a fundamental theory of digital message switching, he moved to UCLA. In the late 1960s, building on his earlier work on queueing theory, Kleinrock carried out theoretical work to model the performance of packet-switched networks, which underpinned the development of the ARPANET. In 1969, a team at UCLA connected a computer to an Interface Message Processor, becoming the first node on ARPANET. His theoretical work on hierarchical routing in the late 1970s with student Farouk Kamoun remains critical to the operation of the Internet today. In 2012, Kleinrock was inducted into the Internet Hall of Fame by the Internet Society.\n\nDouglas Engelbart (1925-2013) was an early researcher at the Stanford Research Institute. His Augmentation Research Center laboratory became the second node on the ARPANET in October 1969, and SRI became the early Network Information Center, which evolved into the domain name registry.\n\nEngelbart was a committed, vocal proponent of the development and use of computers and computer networks to help cope with the world’s increasingly urgent and complex problems. He is best known for his work on the challenges of human–computer interaction, resulting in the invention of the computer mouse, and the development of hypertext, networked computers, and precursors to graphical user interfaces.\n\nElizabeth J. \"Jake\" Feinler (born 1931) was a staff member of Doug Engelbart's Augmentation Research Center at SRI and PI for the Network Information Center (NIC) for the ARPANET and the Defense Data Network (DDN) from 1972 until 1989. In 2012, Feinler was inducted into the Internet Hall of Fame by the Internet Society.\n\nLouis Pouzin (born 1931) is a French computer scientist. He invented the datagram and designed an early packet communications network, CYCLADES. Concepts from his work were used by Robert Kahn, Vinton Cerf, and others in the development of TCP/IP. In 1997, Pouzin received the ACM SIGCOMM Award for \"pioneering work on connectionless packet communication\". Louis Pouzin was named a Chevalier of the Legion of Honor by the French government on March 19, 2003. In 2012, Pouzin was inducted into the Internet Hall of Fame by the Internet Society.\n\nGlenda Schroeder is an American software engineer who described the first e-mail implementation in 1964-65 with Pat Crisman and Louis Pouzin by outlining a system of notifying users about backups of files. She is also noted for implementing the first command line user interface shell. This creation for the Multics operating system at MIT was the predecessor to the Unix shell implemented at Bell Labs and still in use today.\n\nJohn Klensin's involvement with Internet began in 1969, when he worked on the File Transfer Protocol. \nKlensin was involved in the early procedural and definitional work for DNS administration and top-level domain definitions and was part of the committee that worked out the transition of DNS-related responsibilities between USC-ISI and what became ICANN.\n\nHis career includes 30 years as a principal research scientist at MIT, a stint as INFOODS Project Coordinator for the United Nations University, Distinguished Engineering Fellow at MCI WorldCom, and Internet Architecture Vice President at AT&T; he is now an independent consultant. In 1992 Randy Bush and John Klensin created the \"Network Startup Resource Center\", helping dozens of countries to establish connections with FidoNet, UseNet, and when possible the Internet.\n\nIn 2003, he received an International Committee for Information Technology Standards Merit Award.\nIn 2007, he was inducted as a Fellow of the Association for Computing Machinery for contributions to networking standards and Internet applications. In 2012, Klensin was inducted into the Internet Hall of Fame by the Internet Society.\n\nRobert E. \"Bob\" Kahn (born 1938) is an American engineer and computer scientist, who in 1974, along with Vint Cerf, invented the TCP/IP protocols. After earning a Ph.D. degree from Princeton University in 1964, he worked for AT&T Bell Laboratories, as an assistant professor at MIT, and at Bolt, Beranek and Newman (BBN), where he helped develop the ARPANET IMP. In 1972, he began work at the Information Processing Techniques Office (IPTO) within ARPA. In 1986 he left ARPA to found the Corporation for National Research Initiatives (CNRI), a nonprofit organization providing leadership and funding for research and development of the National Information Infrastructure\n\nVinton G. \"Vint\" Cerf (born 1943) is an American computer scientist. He is recognized as one of \"the fathers of the Internet\", sharing this title with Bob Kahn.\n\nHe earned his Ph.D. from UCLA in 1972. At UCLA he worked in Professor Leonard Kleinrock's networking group that connected the first two nodes of the ARPANET and contributed to the ARPANET host-to-host protocol. Cerf was an assistant professor at Stanford University from 1972–1976, where he conducted research on packet network interconnection protocols and co-designed the DoD TCP/IP protocol suite with Bob Kahn. He was a program manager for the Advanced Research Projects Agency (ARPA) from 1976 to 1982. Cerf was instrumental in the formation of both the Internet Society and Internet Corporation for Assigned Names and Numbers (ICANN), serving as founding president of the Internet Society from 1992–1995 and in 1999 as Chairman of the Board and as ICANN Chairman from 2000 to 2007. His many awards include the National Medal of Technology, the Turing Award, the Presidential Medal of Freedom, and membership in the National Academy of Engineering and the Internet Society's Internet Hall of Fame.\n\nPeter T. Kirstein (born 1933) is a British computer scientist and a leader in the international development of the Internet. In 1973, he established one of the first two international nodes of the ARPANET. In 1978 he co-authored \"Issues in packet-network interconnection\" with Vint Cerf, one of the early technical papers on the internet concept. His research group at University College London adopted TCP/IP in 1982, a year ahead of ARPANET, and played a significant role in the very earliest experimental Internet work. Starting in 1983 he chaired the International Collaboration Board, which involved six NATO countries, served on the Networking Panel of the NATO Science Committee (serving as chair in 2001), and on Advisory Committees for the Australian Research Council, the Canadian Department of Communications, the German GMD, and the Indian Education and Research Network (ERNET) Project. He leads the Silk Project, which provides satellite-based Internet access to the Newly Independent States in the Southern Caucasus and Central Asia. In 2012, Kirstein was inducted into the Internet Hall of Fame by the Internet Society.\n\nSteve Crocker (born 1944 in Pasadena, California) has worked in the ARPANET and Internet communities since their inception. As a UCLA graduate student in the 1960s, he helped create the ARPANET protocols which were the foundation for today's Internet. He created the Request for Comments series, authoring the very first RFC and many more. He was instrumental in creating the ARPA \"Network Working Group\", the forerunner of the modern Internet Engineering Task Force.\n\nCrocker has been a program manager at the Advanced Research Projects Agency (ARPA), a senior researcher at USC's Information Sciences Institute, founder and director of the Computer Science Laboratory at The Aerospace Corporation and a vice president at Trusted Information Systems. In 1994, Crocker was one of the founders and chief technology officer of CyberCash, Inc. He has also been an IETF security area director, a member of the Internet Architecture Board, chair of the Internet Corporation for Assigned Names and Numbers (ICANN) Security and Stability Advisory Committee, a board member of the Internet Society and numerous other Internet-related volunteer positions. Crocker is chair of the board of ICANN.\n\nFor this work, Crocker was awarded the 2002 IEEE Internet Award \"for leadership in creation of key elements in open evolution of Internet protocols\". In 2012, Crocker was inducted into the Internet Hall of Fame by the Internet Society.\n\nJon Postel (1943–1998) was a researcher at the Information Sciences Institute. He was editor of all early Internet standards specifications, such as the Request for Comments (RFC) series. His beard and sandals made him \"the most recognizable archetype of an Internet pioneer\".\n\nThe Internet Society's Postel Award is named in his honor, as is the Postel Center at Information Sciences Institute. His obituary was written by Vint Cerf and published as RFC 2468 in remembrance of Postel and his work. In 2012, Postel was inducted into the Internet Hall of Fame by the Internet Society.\n\nJoyce K. Reynolds (died 2015) was an American computer scientist and served as part of the editorial team of the Request For Comments series from 1987 to 2006. She performed the IANA function with Jon Postel until this was transferred to ICANN, then worked with ICANN in this role until 2001, while remaining an employee of ISI.\n\nAs Area Director of the User Services area, she was a member of the Internet Engineering Steering Group of the IETF from 1990 to March 1998.\n\nTogether with Bob Braden, she received the 2006 Postel Award in recognition of her services to the Internet. She is mentioned, along with a brief biography, in RFC 1336, \"Who's Who in the Internet\" (1992).\n\nDanny Cohen led several projects on real-time interactive applications over the ARPANet and the Internet starting in 1973. After serving on the computer science faculty at Harvard University (1969–1973) and Caltech (1976), he joined the Information Sciences Institute (ISI) at University of Southern California (USC). At ISI (1973–1993) he started many network related projects including, one to allow interactive, real-time speech over the ARPANet, packet-voice, packet-video, and Internet Concepts. In 1981 he adapted his visual flight simulator to run over the ARPANet, the first application of packet switching networks to real-time applications. In 1993, he worked on Distributed Interactive Simulation through several projects funded by United States Department of Defense. He is probably best known for his 1980 paper \"On Holy Wars and a Plea for Peace\"\nwhich adopted the terminology of endianness for computing.\n\nCohen was elected to the National Academy of Engineering in 2006 for contributions to the advanced design, graphics, and real-time network protocols of computer systems and as an IEEE Fellow in 2010 for contributions to protocols for packet switching in real-time applications. In 1993 he received a United States Air Force Meritorious Civilian Service Award. And in 2012, Cohen was inducted into the Internet Hall of Fame by the Internet Society.\n\nPaul V. Mockapetris (born 1948), while working with Jon Postel at the Information Sciences Institute (ISI) in 1983, proposed the Domain Name System (DNS) architecture. He was IETF chair from 1994 to 1996.\n\nMockapetris received the 1997 John C. Dvorak Telecommunications Excellence Award \"Personal Achievement - Network Engineering\" for DNS design and implementation, the 2003 IEEE Internet Award for his contributions to DNS, and the Distinguished Alumnus award from the University of California, Irvine. In May 2005, he received the ACM Sigcomm lifetime award. In 2012, Mockapetris was inducted into the Internet Hall of Fame by the Internet Society.\n\nDavid D. Clark (born 1944) is an American computer scientist. During the period of tremendous growth and expansion of the Internet from 1981 to 1989, he acted as chief protocol architect in the development of the Internet, and chaired the Internet Activities Board, which later became the Internet Architecture Board. He is currently a senior research scientist at the MIT Computer Science and Artificial Intelligence Laboratory.\n\nIn 1990 Clark was awarded the ACM SIGCOMM Award \"in recognition of his major contributions to Internet protocol and architecture.\" In 1998 he received the IEEE Richard W. Hamming Medal \"for leadership and major contributions to the\narchitecture of the Internet as a universal information medium\". In 2001 he was inducted as a Fellow of the Association for Computing Machinery for \"his preeminent role in the development of computer communication and the Internet, including architecture, protocols, security, and telecommunications policy\". In 2001, he was awarded the Telluride Tech Festival Award of Technology in Telluride, Colorado, and in 2011 the Lifetime Achievement Award from the Oxford Internet Institute, University of Oxford \"in recognition of his intellectual and institutional contributions to the advance of the Internet.\"\n\nSusan Estrada founded CERFnet, one of the original regional IP networks, in 1988. Through her leadership and collaboration with PSINet and UUnet, Estrada helped form the interconnection enabling the first commercial Internet traffic via the Commercial Internet Exchange. She wrote \"Connecting to the Internet\" in 1993 and she was inducted to the Internet Hall of Fame in 2014. She is on the Board of Trustees of the Internet Society.\n\nDavid L. Mills (born 1938) is an American computer engineer. Mills earned his PhD in Computer and Communication Sciences from the University of Michigan in 1971. While at Michigan he worked on the ARPA sponsored Conversational Use of Computers (CONCOMP) project and developed DEC PDP-8 based hardware and software to allow terminals to be connected over phone lines to an IBM System/360 mainframe computer.\n\nMills was the chairman of the Gateway Algorithms and Data Structures Task Force (GADS) and the first chairman of the Internet Architecture Task Force. He invented the Network Time Protocol (1981), the DEC LSI-11 based fuzzball router that was used for the 56 kbit/s NSFNET (1985), the Exterior Gateway Protocol (1984), and inspired the author of ping (1983). He is an emeritus professor at the University of Delaware.\n\nIn 1999 he was inducted as a Fellow of the Association for Computing Machinery, and in 2002, as a Fellow of the Institute of Electrical and Electronic Engineers (IEEE). In 2008, Mills was elected to the National Academy of Engineering (NAE). In 2013 he received the IEEE Internet Award \"For significant leadership and sustained contributions in the research, development, standardization, and deployment of quality time synchronization capabilities for the Internet.\"\n\nRadia Joy Perlman (born 1951) is the software designer and network engineer who developed the spanning-tree protocol which is fundamental to the operation of network bridges. She also played an important role in the development of link-state routing protocols such as IS-IS (which had a significant influence on OSPF). In 2010 she received the ACM SIGCOMM Award \"for her fundamental contributions to the Internet routing and bridging protocols that we all use and take for granted every day.\"\n\nDennis M. Jennings is an Irish physicist, academic, Internet pioneer, and venture capitalist. In 1984, the National Science Foundation (NSF) began construction of several regional supercomputing centers to provide very high-speed computing resources for the US research community. In 1985 NSF hired Jennings to lead the establishment of the National Science Foundation Network (NSFNET) to link five of the super-computing centers to enable sharing of resources and information. Jennings made three critical decisions that shaped the subsequent development of NSFNET: \n\nJennings was also actively involved in the start-up of research networks in Europe (European Academic Research Network, EARN - President; EBONE - Board member) and Ireland (HEAnet - initial proposal and later Board member). He chaired the Board and General Assembly of the Council of European National Top Level Domain Registries (CENTR) from 1999 to early 2001 and was actively involved in the start-up of the Internet Corporation for Assigned Names and Numbers (ICANN). He was a member of the ICANN Board from 2007 to 2010, serving as Vice-Chair in 2009-2010. In April 2014 Jennings was inducted into the Internet Hall of Fame.\n\nStephen \"Steve\" Wolff participated in the development of ARPANET while working for the U.S. Army. In 1986 he became Division Director for Networking and Communications Research and Infrastructure at the National Science Foundation (NSF) where he managed the development of NSFNET. He also conceived the Gigabit Testbed, a joint NSF-DARPA project to prove the feasibility of IP networking at gigabit speeds. His work at NSF transformed the fledgling internet from a narrowly focused U.S. government project into the modern Internet with scholarly and commercial interest for the entire world. In 1994 he left NSF to join Cisco as a technical manager in Corporate Consulting Engineering. In 2011 he became the CTO at Internet2.\n\nIn 2002 the Internet Society recognized Wolff with its Postel Award. When presenting the award, Internet Society (ISOC) President and CEO Lynn St.Amour said “…Steve helped transform the Internet from an activity that served the specific goals of the research community to a worldwide enterprise which has energized scholarship and commerce throughout the world.” The Internet Society also recognized Wolff in 1994 for his courage and leadership in advancing the Internet.\n\nSally Floyd is an American engineer recognized for her extensive contributions to Internet architecture and her work in identifying practical ways to control and stabilize Internet congestion. She invented the random early detection active queue management scheme, which has been implemented in nearly all commercially available routers, and devised the now-common method of adding delay jitter to message timers to avoid synchronization collisions. Floyd, with Vern Paxson, in 1997 identified the lack of knowledge of network topology as the major obstacle in understanding how the Internet works. This paper, \"Why We Don't Know How to Simulate the Internet\", was re-published as \"Difficulties in Simulating the Internet\" in 2001 and won the IEEE Communication Society's William R. Bennett Prize Paper Award.\n\nFloyd is also a co-author on the standard for TCP Selective acknowledgement (SACK), Explicit Congestion Notification (ECN), the Datagram Congestion Control Protocol (DCCP) and TCP Friendly Rate Control (TFRC).\n\nShe received the IEEE Internet Award in 2005 and the ACM SIGCOMM Award in 2007 for her contributions to congestion control. She has been involved in the Internet Advisory Board, and is one of the top-ten most cited researchers in computer science.\n\nVan Jacobson is an American computer scientist, best known for his work on TCP/IP network performance and scaling. His work redesigning TCP/IP's flow control algorithms (Jacobson's algorithm) to better handle congestion is said to have saved the Internet from collapsing in the late 1980s and early 1990s. He is also known for the TCP/IP Header Compression protocol described in RFC 1144: \"Compressing TCP/IP Headers for Low-Speed Serial Links\", popularly known as Van Jacobson TCP/IP Header Compression. He is co-author of several widely used network diagnostic tools, including traceroute, tcpdump, and pathchar. He was a leader in the development of the multicast backbone (MBone) and the multimedia tools vic, vat, and wb.\n\nFor his work, Jacobson received the 2001 ACM SIGCOMM Award for Lifetime Achievement, the 2003 IEEE Koji Kobayashi Computers and Communications Award, and was elected to the National Academy of Engineering in 2006. In 2012, Jacobson was inducted into the Internet Hall of Fame by the Internet Society.\n\nTheodor Holm \"Ted\" Nelson (born 1937) is an American sociologist and philosopher. In 1960 he founded Project Xanadu with the goal of creating a computer network with a simple user interface. Project Xanadu was to be a worldwide electronic publishing system using hypertext linking that would have created a universal library. In 1963 he coined the terms \"hypertext\" and \"hypermedia\". In 1974 he wrote and published two books in one, Computer Lib/Dream Machines, that has been hailed as \"the most important book in the history of new media.\" Sadly, his grand ideas from the 1960s and 1970s never became completed projects.\n\nTimothy John \"Tim\" Berners-Lee (born 1955) is a British physicist and computer scientist. In 1980, while working at CERN, he proposed a project using hypertext to facilitate sharing and updating information among researchers. While there, he built a prototype system named ENQUIRE. Back at CERN in 1989 he conceived of and, in 1990, together with Robert Cailliau, created the first client and server implementations for what became the World Wide Web. Berners-Lee is the director of the World Wide Web Consortium (W3C), a standards organization which oversees and encourages the Web's continued development, co-Director of the Web Science Trust, and founder of the World Wide Web Foundation.\n\nIn 1994, Berners-Lee became one of only six members of the World Wide Web Hall of Fame. In 2004, Berners-Lee was knighted by Queen Elizabeth II for his pioneering work. In April 2009, he was elected a foreign associate of the United States National Academy of Sciences, based in Washington, D.C. In 2012, Berners-Lee was inducted into the Internet Hall of Fame by the Internet Society.\n\nRobert Cailliau (, born 1947), is a Belgian informatics engineer and computer scientist who, working with Tim Berners-Lee and Nicola Pellow at CERN, developed the World Wide Web. In 2012 he was inducted into the Internet Hall of Fame by the Internet Society.\n\nNicola Pellow, one of the nineteen member of the \"WWW Project\" at CERN working with Tim Berners-Lee, is recognized for developing the first cross-platform internet browser, Line Mode Browser, that displayed web-pages on dumb terminals and was released in May 1991. She joined the project in November 1990, while an undergraduate math student enrolled in a sandwich course at Leicester Polytechnic (now De Montfort University). She left CERN at the end of August 1991, but returned after graduating in 1992, and worked with Robert Cailliau on MacWWW, the first web browser for the classic Mac OS.\n\nMark P. McCahill (born 1956) is an American programmer and systems architect. While working at the University of Minnesota he led the development of the Gopher protocol (1991), the effective predecessor of the World Wide Web, and contributed to the development and popularization of a number of other Internet technologies from the 1980s.\n\nMarc L. Andreessen (born 1971) is an American software engineer, entrepreneur, and investor. Working with Eric Bina while at NCSA, he co-authored Mosaic, the first widely used web browser. He is also co-founder of Netscape Communications Corporation.\n\nEric J. Bina (born 1964) is an American computer programmer. In 1993, together with Marc Andreessen, he authored the first version of Mosaic while working at NCSA at the University of Illinois at Urbana-Champaign. Mosaic is famed as the first killer application that popularized the Internet. He is also a co-founder of Netscape Communications Corporation.\n\nA plaque commemorating the \"Birth of the Internet\" was dedicated at a conference on the history and future of the internet on July 28, 2005 and is displayed at the Gates Computer Science Building, Stanford University. The text printed and embossed in black into the brushed bronze surface of the plaque reads:\n\n\n\n"}
{"id": "4940054", "url": "https://en.wikipedia.org/wiki?curid=4940054", "title": "List of archaeoastronomical sites by country", "text": "List of archaeoastronomical sites by country\n\nThis is a list of sites where claims for the use of archaeoastronomy have been made, sorted by country.\n\nClive Ruggles and Michel Cotte have edited a book on heritage sites of astronomy and archaeoastronomy that provides a list of the main archaeoastronomical sites around the world.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor a full list see the chapter on India in the ICOMOS book edited by Clive Ruggles and Michel Cotte. These sites include:\n\nJ.M. Malville and Rana P.B. Singh have done much work on the archaeoastronomy of sacred sites in India.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "363293", "url": "https://en.wikipedia.org/wiki?curid=363293", "title": "List of craters on Mercury", "text": "List of craters on Mercury\n\nThis is a list of named craters on Mercury, the innermost planet of the Solar System \"(for other features, see list of geological features on Mercury)\". Most Mercurian craters are named after famous writers, artists and composers. According to the rules by IAU's Working Group for Planetary System Nomenclature, all new craters must be named after an artist that was famous for more than fifty years, and dead for more than three years, before the date they are named. Craters larger than 250 km in diameter are referred to as \"basins\" \"(also see )\".\n\nAs of 2017, there are 397 named Mercurian craters, a small fraction of the total number of named Solar System craters, most of which are lunar, Martian and Venerian craters.\n\nOther, non-planetary bodies with numerous named craters include Callisto (141), Ganymede (131), Rhea (128), Vesta (90), Ceres (90), Dione (73), Iapetus (58), Enceladus (53), Tethys (50) and Europa (41). For a full list, \"see List of craters in the Solar System\".\n\nAs on the Moon and Mars, sequences of craters and basins of differing relative ages provide the best means of establishing stratigraphic order on Mercury. Overlap relations among many large mercurian craters and basins are clearer than those on the Moon. Therefore, as this map shows, we can build up many local stratigraphic columns involving both crater or basin materials and nearby plains materials.\n\nOver all of Mercury, the crispness of crater rims and the morphology of their walls, central peaks, ejecta deposits, and secondary-crater fields have undergone systematic changes with time. The youngest craters or basins in a local stratigraphic sequence have the sharpest, crispest appearance. The oldest craters consist only of shallow depressions with slightly raised, rounded rims, some incomplete. On this basis, five age categories of craters and basins have been mapped; the characteristics of each are listed in the explanation. In addition, secondary crater fields are preserved around proportionally far more craters and basins on Mercury than on the Moon or Mars, and are particularly useful in determining overlap relations and degree of modification.\n\nBecause only limited photographic evidence was available from \"Mariner 10\"s three flybys of the planet, these divisions are often tentative. The five crater groups, from youngest to oldest, are:\n\n\n\n"}
{"id": "4253950", "url": "https://en.wikipedia.org/wiki?curid=4253950", "title": "List of elements by stability of isotopes", "text": "List of elements by stability of isotopes\n\nAtomic nuclei consist of protons and neutrons, which attract each other through the nuclear force, while protons repel each other via the electric force due to their positive charge. These two forces compete, leading to some combinations of neutrons and protons being more stable than others. Neutrons stabilize the nucleus, because they attract protons, which helps offset the electrical repulsion between protons. As a result, as the number of protons increases, an increasing ratio of neutrons to protons is needed to form a stable nucleus; if too many or too few neutrons are present with regard to the optimum ratio, the nucleus becomes unstable and subject to certain types of nuclear decay. Unstable isotopes decay through various radioactive decay pathways, most commonly alpha decay, beta decay, or electron capture. Many other rare types of decay, such as spontaneous fission or cluster decay are known. (See radioactive decay for details.)\n\nOf the first 82 elements in the periodic table, 80 have isotopes considered to be stable. The 83rd element, bismuth, was traditionally regarded as having the heaviest stable isotope, bismuth-209, but in 2003 researchers in Orsay, France, measured the half-life of to be . Technetium, promethium (atomic numbers 43 and 61, respectively) and all the elements with an atomic number over 82 only have isotopes that are known to decompose through radioactive decay. No undiscovered elements are expected to be stable; therefore, lead is considered the heaviest stable element. However, it is possible that some isotopes that are now considered stable will be revealed to decay with extremely long half-lives (as with ). This list depicts what is agreed upon by the consensus of the scientific community as of 2016.\n\nFor each of the 80 stable elements, the number of the stable isotopes is given. Only 90 isotopes are expected to be perfectly stable, and an additional 163 are energetically unstable, but have never been observed to decay. Thus, 253 isotopes (nuclides) are stable by definition (including tantalum-180m, for which no decay has yet been observed). Those that may in the future be found to be radioactive are expected to have half-lives longer than 10 years (for example, xenon-134).\n\nOf the chemical elements, only one element (tin) has 10 such stable isotopes, one (xenon) has eight isotopes, four have seven isotopes, eight have six isotopes, ten have five isotopes, nine have four isotopes, five have three stable isotopes, 16 have two stable isotopes, and 26 have a single stable isotope.\n\nAdditionally, about 29 nuclides of the naturally occurring elements have unstable isotopes with a half-life larger than the age of the Solar System (~10 years or more). An additional four nuclides have half-lives longer than 100 million years, which is far less than the age of the solar system, but long enough for some of them to have survived. These 33 radioactive naturally occurring nuclides comprise the \"radioactive\" primordial nuclides. The total number of primordial nuclides is then 253 (the stable nuclides) \"plus\" the 33 radioactive primordial nuclides, for a \"total\" of 286 primordial nuclides. This number is subject to change if new shorter-lived primordials are identified on Earth.\n\nOne of the primordial nuclides is tantalum-180m, which is predicted to have a half-life in excess of 10 years, but has never been observed to decay. The even longer half-life of 7.7 x 10 years of tellurium-128 was measured by a unique method of detecting its radiogenic daughter xenon-128 and is the longest known experimentally measured half-life. Another notable example is the only naturally occurring isotope of bismuth, bismuth-209, which has been predicted to be unstable with a very long half-life, but has been observed to decay. Because of their long half-lives, such isotopes are still found on Earth in various quantities, and together with the stable isotopes they are called primordial isotopes. All the primordial isotopes are given in order of their decreasing abundance on Earth.. For a list of primordial nuclides in order of half-life, see list of nuclides.\n\n118 chemical elements are known to exist. All elements to element 94 are found in nature, and the remainder of the discovered elements are artificially produced, with isotopes all known to be highly radioactive with relatively short half-lives (see below). The elements in this list are ordered according to the lifetime of their most stable isotope. Of these, three elements (bismuth, thorium, and uranium) are primordial because they have half-lives long enough to still be found on the Earth, while all the others are produced either by radioactive decay or are synthesized in laboratories and nuclear reactors. Only 13 of the 38 known-but-unstable elements (assuming the total number of elements is 118) have isotopes with a half-life of at least 100 years. Every known isotope of the remaining 25 elements is highly radioactive; these are used in academic research and sometimes in industry and medicine. Some of the heavier elements in the periodic table may be revealed to have yet-undiscovered isotopes with longer lifetimes than those listed here.\n\nAbout 338 nuclides are found naturally on Earth. These comprise 253 stable isotopes, and with the addition of the 33 long-lived radioisotopes with half-lives longer than 100 million years, a total of 286 primordial nuclides, as noted above. The nuclides found naturally comprise not only the 286 primordials, but also include about 52 more short-lived isotopes (defined by a half-life less than 100 million years, too short to have survived from the formation of the Earth) that are daughters of primordial isotopes (such as radium from uranium); or else are made by energetic natural processes, such as carbon-14 made from atmospheric nitrogen by bombardment from cosmic rays.\n\nAn even number of protons or neutrons is more stable (higher binding energy) because of pairing effects, so even-even nuclides are much more stable than odd-odd. One effect is that there are few stable odd-odd nuclides: in fact only five are stable, with another four having half-lives longer than a billion years.\n\nAnother effect is to prevent beta decay of many even-even nuclides into another even-even nuclide of the same mass number but lower energy, because decay proceeding one step at a time would have to pass through an odd-odd nuclide of higher energy. (Double beta decay directly from even-even to even-even, skipping over an odd-odd nuclide, is only occasionally possible, and is a process so strongly hindered that it has a half-life greater than a billion times the age of the universe.) This makes for a larger number of stable even-even nuclides, up to three for some mass numbers, and up to seven for some atomic (proton) numbers and at least four for all stable even-Z elements beyond iron except for strontium.\n\nSince a nucleus with an odd number of protons is relatively less stable, odd-numbered elements tend to have fewer stable isotopes. Of the 26 \"monoisotopic\" elements that have only a single stable isotope, all but one have an odd atomic number — the single exception being beryllium. In addition, no odd-numbered element has more than two stable isotopes, while every even-numbered element with stable isotopes, except for helium, beryllium, and carbon, has at least three.\n\nThe following tables give the elements with primordial nuclides, which means that the element may still be identified on Earth from natural sources, having been present since the Earth was formed out of the solar nebula. Thus, none are shorter-lived daughters of longer-lived parental primordials, such as radon. Two nuclides which have half-lives long enough to be primordial, but have not yet been conclusively observed as such (Pu and Sm), have been excluded.\n\nThe tables of elements are sorted in order of decreasing number of nuclides associated with each element. (For a list sorted entirely in terms of half-lives of nuclides, with mixing of elements, see List of nuclides.) Stable and unstable (marked \"decays\") nuclides are given, with symbols for unstable (radioactive) nuclides in italics. Note that the sorting does not quite give the elements purely in order of stable nuclides, since some elements have a larger number of long-lived unstable nuclides, which place them ahead of elements with a larger number of stable nuclides. By convention, nuclides are counted as \"stable\" if they have never been observed to decay by experiment or from observation of decay products (extremely long-lived nuclides unstable only in theory, such as tantalum-180m, are counted as stable).\n\nThe first table is for even-atomic numbered elements, which tend to have far more primordial nuclides, due to the stability conferred by proton-proton pairing. A second separate table is given for odd-atomic numbered elements, which tend to have far fewer stable and long-lived (primordial) unstable nuclides.\n\n\n"}
{"id": "7119834", "url": "https://en.wikipedia.org/wiki?curid=7119834", "title": "List of extraterrestrial volcanoes", "text": "List of extraterrestrial volcanoes\n\nThis is a list of active, dormant and extinct volcanoes located beyond planet Earth. They may be designated mons (mountain), patera (an irregular crater) or tholus (small mountain or hill) in accordance with the International Astronomical Union's rules for planetary nomenclature. Many of them are nameless.\n\nIo, a moon of the planet Jupiter, is the most volcanically active body in the Solar System. Its volcanoes are believed to eject sulfur and sulfur dioxide, as well as basaltic and ultramafic silicate lavas.\n\nMars has many shield volcanoes, including the largest known volcano of the Solar System, but they are all dormant if not extinct.\n\nOn Venus, volcanic features are very numerous and quite diverse, but, like on Mars, none is known to be currently active. These volcanoes range from several to several hundred kilometers in diameter; a majority of them are shield volcanoes. In addition, Venus has unusual types of volcanoes: pancake domes and scalloped margin domes. Most small volcanoes on Venus are nameless.\n\nDue to the low viscosity of most lunar lava, volcanic mountains were seldom created. Instead, basaltic lava flooded large areas, which became lunar maria. Shield volcanoes are known from a few areas on the Moon; they are called lunar domes. Some areas of the Moon are covered with a usually dark coating, which is interpreted as pyroclastic deposits. Sometimes they form a dark halo around rilles. See also:\n\nMany of Mercury's basins contain smooth plains, like the lunar mare, that are believed likely to be filled with lava flows. Collapse structures possibly indicative of volcanism have been found in some craters. Eleven volcanic domes were identified in Mariner 10 images, including a 1.4-km high dome near the centre of Odin Planitia.\n\n\n\n"}
{"id": "41777152", "url": "https://en.wikipedia.org/wiki?curid=41777152", "title": "List of seaweeds and marine flowering plants of Australia (temperate waters)", "text": "List of seaweeds and marine flowering plants of Australia (temperate waters)\n\nThe list of seaweeds and marine flowering plants of Australia (temperate waters) is a list of marine species that form a part of the flora of Australia.\n\nThe geographical range is from Perth, Western Australia to New South Wales, and those tropical species which are also found in this range may also be listed here.\n\nThe listed organisms are generally identifiable to the naked eye. Many microscopic algae also inhabit this region.\n\nRanges are generally given relating to Australian waters. If listed as endemic, they have been found only in the listed range. Others may have much greater ranges.\n\nFamily Leathesiacae\n\nFamily Chordariaceae\n\nFamily Splachnidiaceae\n\nFamily Stypocaulaceae\n\nFamily Cladostephaceae\n\nFamily Dictyotaceae\n\nFamily Sporochnaceae\n\nFamily Scytosiphonaceae\n\nFamily Punctariaceae\n\nFamily Lessoniaceae\n\nFamily Alariaceae\n\nFamily Notheiaceae\n\nFamily Durvillaeaceae\n\nFamily Hormosiraceae\n\nFamily Seirococcaceae\n\nFamily Cystoseiraceae\n\nFamily Sargassaceae\n\nFamily Bangiaceae\n\nFamily Liagoraceae\n\nFamily Galaxauraceae\n\nFamily Gelidiaceae\n\nFamily Peyssonneliaceae\n\nFamily Polyidaceae\n\nFamily Halymeniaceae\n\nFamily Kallymeniaceae\n\nFamily Phyllophoraceae\n\nFamily Nemastomataceae\n\nFamily Gigartinaceae\n\nFamily Dicranemiaceae\n\nFamily Sarcodiaceae\n\nFamily Acrotylaceae\n\nFamily Areschougiaceae\n\nFamily Plocamiaceae\n\nFamily Phacelocarpaceae\n\nFamily Cystocloniaceae\n\nFamily Mychodeaceae\n\nFamily Hypnaeaceae\n\nFamily Bonnemaisoniaceae\n\nFamily Graciliariaceae\n\nFamily Corallinaceae\n\nFamily Rhodymeniaceae\n\nFamily Champiaceae\n\nFamily Ceramiaceae\n\nFamily Delesseriaceae\n\nFamily Dasyaceae\n\nFamily Rhodomelaceae\n\nFamily Ulvaceae\n\nFamily Cladophoraceae\n\nFamily Anadyomenaceae\n\nFamily Valoniaceae\n\nFamily Codiaceae\n\nFamily Udoteaceae\n\nFamily Caulerpaceae\n\nFamily Bryopsidaceae\n\nFamily Polyphysaceae\n\nFamily Hydrocharitaceae\n\nFamily Potamogetonaceae\n\nFamily Posidoniaceae\n\nFamily Cymodoceaceae\n\nFamily Zosteraceae\n\n\n\n\n"}
{"id": "12687622", "url": "https://en.wikipedia.org/wiki?curid=12687622", "title": "Marina Ratner", "text": "Marina Ratner\n\nMarina Evseevna Ratner (; October 30, 1938 – July 7, 2017) was a professor of mathematics at the University of California, Berkeley who worked in ergodic theory. Around 1990, she proved a group of major theorems concerning unipotent flows on homogeneous spaces, known as Ratner's theorems. Ratner was elected to the American Academy of Arts and Sciences in 1992, awarded the Ostrowski Prize in 1993 and elected to the National Academy of Sciences the same year. In 1994, she was awarded the John J. Carty Award from the National Academy of Sciences.\n\nBorn in Moscow, Russian SFSR to a Jewish family, her father was a plant physiologist and mother, a chemist. Ratner's mother was fired from work in the 1940s for writing to her mother in Israel, then considered an enemy of the Soviet state. Ratner gained an interest in mathematics in her fifth grade. She then studied at the Moscow State University in 1956 where she was inspired by A.N. Kolmogorov and his group. Her graduate studies were under Yakov G. Sinai, also a student of Kolmogorov. She emigrated from the Soviet Union in 1971 after obtaining a Ph.D. to Israel and taught at the Hebrew University 1971–1975. She began to work with Rufus Bowen at Berkeley and later emigrated to the United States and became a professor of mathematics at Berkeley. Her work included proofs of conjectures dealing with unipotent flows on quotients of Lie groups made by S. G. Dani and M. S. Raghunathan.\n"}
{"id": "31761672", "url": "https://en.wikipedia.org/wiki?curid=31761672", "title": "Model year (computer modeling)", "text": "Model year (computer modeling)\n\nThe term model year in computer modeling is used for calculated equations describing one calendar year of data. If a climate model, for example, is calculating the climate from 2015 to 2020, the computer has to calculate 5 model years, however it most likely takes much less time for the computer to do so.\n\nThe term is used in descriptions for models like climateprediction.net, which has produced about 125 million model years of data until May 2011.\n"}
{"id": "53363170", "url": "https://en.wikipedia.org/wiki?curid=53363170", "title": "Nikolaj Iljitsch Baranov", "text": "Nikolaj Iljitsch Baranov\n\nNikolaj Iljitsch Baranov (sometimes Baranoff) (1887, Oryol-1981, London) was a Russian entomologist who specialised in Diptera.\nHis collection of Palearctic Tachinidae is held by the Smithsonian Institution Washington D.C..Baranov described many new species.He worked as an entomologist at the Institute of Hygiene in Zagreb.\n\npartial list\n\n"}
{"id": "51457884", "url": "https://en.wikipedia.org/wiki?curid=51457884", "title": "Norwegian Scientific Expedition to Tristan Da Cunha 1937-1938", "text": "Norwegian Scientific Expedition to Tristan Da Cunha 1937-1938\n\nThe Norwegian Scientific Expedition to Tristan Da Cunha was a scientific and cultural exploration of the most remote inhabited archipelago in the world, in the south Atlantic Ocean, 2,000 kilometres (1,200 mi) from the nearest inhabited land, Saint Helena. The expedition arrived on the island in December 1937 and left in March 1938.\n\nCaptained by botanist Erling Christophersen, the thirteen man crew included three University of Oslo Ph.D. students conducting research for their dissertations, which were published shortly after their return; these included sociologist Peter A. Munch, ornithologist Yngvar Hagen, and phycologist Egil Baardseth. Also among the crew were a geologist, a marine biologist, an ichthyologist, a dentist and a doctor. A late addition to the crew was topological surveyor Allan Crawford, a British engineer who was recruited by the captain en route to Cape Town, and who would later come to be regarded as a leading authority on Tristan Da Cunha.\n\nBased on observations made during the voyage, Christophersen published \"Tristan da Cunha, the Lonely Isle\" (1938) and the comprehensive \"Norwegian Scientific Expedition to Tristan Da Cunha, 1937-1938\" (1945), while the Ph.D. students published their work and achieved their degrees. Munch published \"Sociology of Tristan da Cunha\" (1946), Hagen published \"Birds of Tristan da Cunha\" (1952), and Baardseth published \"The Marine Algae of Tristan da Cunha\" (1941).\n\n"}
{"id": "15278157", "url": "https://en.wikipedia.org/wiki?curid=15278157", "title": "Nuclear clock", "text": "Nuclear clock\n\nA nuclear clock is a notional clock that would use the frequency of a nuclear transition as its reference frequency, in the same manner as an atomic clock uses the frequency of an electronic transition in an atom's shell. Such a clock is expected to be more accurate than the best current atomic clocks, with a fractional instability at the 10 level.\n\nThe only nuclear transition suitable as a reference frequency is\nthe gamma decay of thorium-229m, a nuclear isomer of\nthorium-229 and the lowest-energy nuclear isomer known. The\ntransition is expected to be in the vacuum ultraviolet, which\nwould make it accessible to laser excitation.\n\nIn 2016, the first direct detection of the isomeric transition has\nconstrained its energy to be between 6.3 and 18.3 eV.\n"}
{"id": "44763458", "url": "https://en.wikipedia.org/wiki?curid=44763458", "title": "Particle mass analyser", "text": "Particle mass analyser\n\nParticle mass analyser is a measurement technique for classifying aerosol particles according to their mass-to-charge ratio.\n\nTechniques exist for classifying (selecting) aerosol particles in the sub 1,000 nm range according to electrical mobility using devices such as differential mobility analysers.\n\nIn electrical mobility measurement, aerosol particles are classified according to their aerodynamic drag-charge ratio. If the particle is non-spherical, the electrical-mobility diameter will not correspond to any measurable physical dimensions of the particle. (For a spherical particle, the electrical-mobility diameter will correspond to physically measurable diameter.)\n\nAn alternative technique classifies particles according to their mass-to-charge ratio, using opposing electrical and centrifugal forces. This allows the classifier to select particles of a specified mass-to-charge ratio independent of particle shape. \n\nFurther work on the technique used centrifugal and electrostatic forces to classify particles according to their mass-to-charge ratio.\n"}
{"id": "522347", "url": "https://en.wikipedia.org/wiki?curid=522347", "title": "Pelagic zone", "text": "Pelagic zone\n\nThe pelagic zone consists of the water column of the open ocean, and can be further divided into regions by depth. The word \"pelagic\" is derived . The pelagic zone can be thought of in terms of an imaginary cylinder or water column that goes from the surface of the sea almost to the bottom. Conditions differ deeper in the water column such that as pressure increases with depth, the temperature drops and less light penetrates. Depending on the depth, the water column, rather like the Earth's atmosphere, may be divided into different layers.\n\nThe pelagic zone occupies 1,330 million km (320 million mi) with a mean depth of and maximum depth of . Fish that live in the pelagic zone are called pelagic fish. Pelagic life decreases with increasing depth. It is affected by light intensity, pressure, temperature, salinity, the supply of dissolved oxygen and nutrients, and the submarine topography, which is called bathymetry. In deep water, the pelagic zone is sometimes called the open-ocean zone and can be contrasted with water that is near the coast or on the continental shelf. In other contexts, coastal water not near the bottom is still said to be in the pelagic zone.\n\nThe pelagic zone can be contrasted with the benthic and demersal zones at the bottom of the sea. The benthic zone is the ecological region at the very bottom of the sea. It includes the sediment surface and some subsurface layers. Marine organisms living in this zone, such as clams and crabs, are called benthos. The demersal zone is just above the benthic zone. It can be significantly affected by the seabed and the life that lives there. Fish that live in the demersal zone are called demersal fish, and can be divided into benthic fish, which are denser than water so they can rest on the bottom, and benthopelagic fish, which swim in the water column just above the bottom. Demersal fish are also known as bottom feeders and groundfish.\n\nDepending on how deep the sea is, the pelagic zone can extend to five vertical regions in the ocean. From the top down, these are:\n\n\"From the surface (MSL) down to around \" \n\nThis is the illuminated zone at the surface of the sea where enough light is available for photosynthesis. Nearly all primary production in the ocean occurs here. Consequently, plants and animals are largely concentrated in this zone. Examples of organisms living in this zone are plankton, floating seaweed, jellyfish, tuna, many sharks and dolphins.\n\n\"From down to around \"\n\nThe most abundant organisms thriving into the mesopelagic zone are heterotrophic bacteria. Examples of animals that live here are swordfish, squid, \"Anarhichadidae\" or \"wolffish\" and some species of cuttlefish. Many organisms that live in this zone are bioluminescent. Some creatures living in the mesopelagic zone rise to the epipelagic zone at night to feed.\n\n\"From down to around \"\n\nThe name stems . At this depth, the ocean is pitch black, apart from occasional bioluminescent organisms, such as anglerfish. No living plant exists here. Most animals living here survive by consuming the detritus falling from the zones above, which is known as \"marine snow\", or, like the marine hatchetfish, by preying on other inhabitants of this zone. Other examples of this zone's inhabitants are giant squid, smaller squids and the grimpoteuthis or \"dumbo octopus\". The giant squid is hunted here by deep-diving sperm whales.\n\n\"From around down to above the ocean floor\"\n\nThe name is derived (a holdover from the times when the deep ocean, or abyss, was believed to be bottomless). Very few creatures live in the cold temperatures, high pressures and complete darkness of this depth. Among the species found in this zone are several species of squid; echinoderms including the basket star, swimming cucumber, and the sea pig; and marine arthropods including the sea spider. Many of the species living at these depths are transparent and eyeless because of the total lack of light in this zone.\n\n\"The deep water in ocean trenches\"\n\nThe name is derived from the realm of Hades, the Greek underworld. However, many organisms live in hydrothermal vents in this and other zones. Some define the hadopelagic as waters below , whether in a trench or not.\n\nThe pelagic ecosystem is based on phytoplankton. Phytoplankton manufacture their own food using a process of photosynthesis. Because they need sunlight, they inhabit the upper, sunlit epipelagic zone, which includes the coastal or neritic zone. Biodiversity diminishes markedly in the deeper zones below the epipelagic zone as dissolved oxygen diminishes, water pressure increases, temperatures become colder, food sources become scarce, and light diminishes and finally disappears.\n\nPelagic birds, also called oceanic birds, live on the open sea, rather than around waters adjacent to land or around inland waters. Pelagic birds feed on planktonic crustaceans, squid and forage fish. Examples are the Atlantic puffin, macaroni penguins, sooty terns, shearwaters, and Procellariiformes such as the albatross, Procellariidae and petrels.\n\nThe term seabird includes birds which live around the sea adjacent to land, as well as pelagic birds.\n\nPelagic fish live in the water column of coastal, ocean, and lake waters, but not on or near the bottom of the sea or the lake. They can be contrasted with demersal fish, which live on or near the bottom, and coral reef fish.\n\nThese fish are often migratory forage fish, which feed on plankton, and the larger fish that follow and feed on the forage fish. Examples of migratory forage fish are herring, anchovies, capelin, and menhaden. Examples of larger pelagic fish which prey on the forage fish are billfish, tuna, and oceanic sharks.\n\nSome examples of pelagic invertebrates include krill, copepods, jellyfish, decapod larvae, hyperiid amphipods, rotifers and cladocerans.\n\nThorson's rule states that benthic marine invertebrates at low latitudes tend to produce large numbers of eggs developing to widely dispersing pelagic larvae, whereas at high latitudes such organisms tend to produce fewer and larger lecithotrophic (yolk-feeding) eggs and larger offspring.\n\n\"Pelamis platura\", the pelagic sea snake, is the only one of the 65 species of marine snakes to spend its entire life in the pelagic zone. It bears live young at sea and is helpless on land. The species sometimes forms aggregations of thousands along slicks in surface waters. The pelagic sea snake is the world’s most widely distributed snake species.\n\nMany species of sea turtles spend the first years of their lives in the pelagic zone, moving closer to shore as they reach maturity.\n\n"}
{"id": "11451041", "url": "https://en.wikipedia.org/wiki?curid=11451041", "title": "Prodigal Genius: The Life of Nikola Tesla", "text": "Prodigal Genius: The Life of Nikola Tesla\n\nProdigal Genius: The Life of Nikola Tesla () is a 1944 book by John Joseph O'Neill detailing the life of Nikola Tesla.\n\nWritten by Pulitzer Prize-winning author John J. O'Neill, the life of Nikola Tesla details the life of a pioneer in electrical engineering. O'Neill was a close friend of Tesla, whom he had met as a boy and remained in contact with.\n\nThe book covers, among other topics, the story of Tesla's father's inspiration for his career in engineering, shows his theories of electricity that went against the scientific establishment, explores the friendships of Tesla, investigates the story of Tesla's lost Nobel Prize, and explains Tesla's investigations of the paranormal.\n\n"}
{"id": "5241800", "url": "https://en.wikipedia.org/wiki?curid=5241800", "title": "Recognition primed decision", "text": "Recognition primed decision\n\nRecognition-primed decision (RPD) is a model of how people make quick, effective decisions when faced with complex situations. In this model, the decision maker is assumed to generate a possible course of action, compare it to the constraints imposed by the situation, and select the first course of action that is not rejected. RPD has been described in diverse groups including Whitewater kayaking Trauma nurses, fireground commanders, chess players, commercial Whitewater river guides and stock market traders. It functions well in conditions of time pressure, and in which information is partial and goals poorly defined. The limitations of RPD include the need for extensive experience among decision-makers (in order to correctly recognize the salient features of a problem and model solutions) and the problem of the failure of recognition and modeling in unusual or misidentified circumstances. It appears to be a valid model for how human decision-makers make decisions.\n\nThe RPD model identifies a reasonable reaction as the first one that is immediately considered. RPD combines two ways of developing a decision; the first is recognizing which course of action makes sense, and the second, evaluating the course of action through imagination to see if the actions resulting from that decision make sense. However, the difference of being experienced or inexperienced plays a major factor in the decision-making processes. \n\nRPD reveals a critical difference between experts and novices when presented with recurring situations. Experienced people will generally be able to come up with a quicker decision because the situation may match a prototypical situation they have encountered before. Novices, lacking this experience, must cycle through different possibilities, and tend to use the first course of action that they believe will work. The inexperienced also have the tendencies of using trial and error through their imagination.\n\nThere are three variations in RPD strategy. In Variation 1, decision makers recognize the situation as typical: a scenario where both the situational detail and the detail of relevant courses of action are known. Variation 1 is therefore essentially an “If… then…” reaction. A given situation will lead to an immediate course of action as a function of the situation's typicality. More experienced decision makers are more likely to have the knowledge of both prototypical situations and established courses of action that is required for an RPD strategy to qualify as Variation 1. \n\nVariation 2 occurs when the decision maker diagnoses an unknown situation to choose from a known selection of courses of action. Variation 2 takes the form of “If (???)… then…,” a phrase which implies the decision maker's specific knowledge of available courses of action but lack of knowledge regarding the parameters of the situation. In order to prevent situational complications and the accrual of misinformation, the decision maker models possible details of the situation carefully and then chooses the most relevant known course of action. Experienced decision makers are more likely to correctly model the situation, and are thus more likely to more quickly choose more appropriate courses of action.\n\nIn Variation 3, the decision maker is knowledgeable of the situation but unaware of the proper course of action. The decision maker therefore implements a mental trial and error simulation to develop the most effective course of action. Variation 3 takes the form of “If… then… (???)” wherein the decision maker models outcomes of new or uncommon courses of action. The decision maker will cycle through different courses of action until a course of action appears appropriate to the goals and priorities of the situation. Due to the time constraint fundamental to the RPD model, the decision maker will choose the first course of action which appears appropriate to the situation. Experienced decision makers are likely to develop a viable course of action more quickly because their expert knowledge can rapidly be used to disqualify inappropriate courses of action.\n\nRecognition primed decision making is highly relevant to the leaders or officers of organizations that are affiliated with emergency services such as fire fighters, search and rescue units, police, and other emergency services. It is applied to both the experienced and the inexperienced, and how they manage their decision making processes. The Recognition primed decision making model is developed as samples for organizations on how important decisions can affect important situations which may either save lives or take lives. The model developed can be used as a study for organizations to fill in the gaps and to determine which type of RPD variation is more applicable to the organization.\n\n\n"}
{"id": "42406424", "url": "https://en.wikipedia.org/wiki?curid=42406424", "title": "Recycling antimatter", "text": "Recycling antimatter\n\nRecycling antimatter pertains to recycling antiprotons and antihydrogen atoms.\n"}
{"id": "11027988", "url": "https://en.wikipedia.org/wiki?curid=11027988", "title": "Slide chart", "text": "Slide chart\n\nA slide chart is a hand-held device, usually of paper, cardboard, or plastic, for conducting simple calculations or looking up information.\nA circular slide chart is sometimes referred to as a wheel chart or volvelle.\n\nUnlike other hand-held mechanical calculating devices such as slide rules and addiators, which have been replaced by electronic calculators and computer software, wheel charts and slide charts have survived to the present time. There are a number of companies who design and manufacture these devices.\n\nUnlike the general-purpose mechanical calculators, slide charts are usually devoted to carrying out a particular specialized calculation, or displaying information on a single product or a particular process. For example, the \"CurveEasy\" wheel chart displays information related to spherical geometry calculations and the Prestolog calculator is used for cost/profit calculations. Another example of a wheel chart is the planisphere, which shows the location of stars in the sky for a given location, date, and time.\n\nSlide charts are often associated with particular sports, political campaigns or commercial companies. For example, a pharmaceutical company may create wheel charts printed with their company name and product information for distribution to medical practitioners.\n\nSlide charts are common collectables.\n\n\n\n"}
{"id": "48477239", "url": "https://en.wikipedia.org/wiki?curid=48477239", "title": "Teodor Bordeianu", "text": "Teodor Bordeianu\n\nTeodor Bordeianu (n. February 16, 1902, Marșenița today Ukraine – d. March 19, 1969, Bucharest) was an agronomist and Romanian growers, member of Romanian Academy.\n\n"}
{"id": "399085", "url": "https://en.wikipedia.org/wiki?curid=399085", "title": "The Fifth Discipline", "text": "The Fifth Discipline\n\nThe Fifth Discipline: The Art and Practice of the Learning Organization is a book by Peter Senge (a senior lecturer at MIT) focusing on group problem solving using the systems thinking method in order to convert companies into learning organizations. The five disciplines represent approaches (theories and methods) for developing three core learning capabilities: fostering aspiration, developing reflective conversation, and understanding complexity.\n\nThe five disciplines of what the book refers to as a \"learning organization\" discussed in the book are:\n\n\nSenge describes extensively the role of what it refers to as \"mental models,\" which he says are integral in order to \"focus on the openness needed to unearth shortcomings\" in perceptions. The book also focuses on \"team learning\" with the goal of developing \"the skills of groups of people to look for the larger picture beyond individual perspectives.\" In addition to these principles, the author stresses the importance of \"personal mastery\" to foster \"the personal motivation to continually learn how [...] actions affect [the] world.\"\n\nIn addition to \"disciplines,\" which Senge suggests are beneficial to what he describes as a \"learning organization,\" Senge also posits several perceived deleterious habits or mindsets, which he refers to as \"learning disabilities.\"\n\n\n\nIn 1997, \"Harvard Business Review\" identified \"The Fifth Discipline\" as one of the seminal management books of the previous 75 years.\n\nThe Federal Reserve Bank of Minneapolis also reviewed the book.\n\n"}
{"id": "5585834", "url": "https://en.wikipedia.org/wiki?curid=5585834", "title": "The Panda's Thumb (book)", "text": "The Panda's Thumb (book)\n\nThe Panda's Thumb: More Reflections in Natural History (1980) is a collection of 31 essays by the Harvard University paleontologist Stephen Jay Gould. It is the second volume culled from his 27-year monthly column \"This View of Life\" in \"Natural History\" magazine.\nRecurring themes of the essays are \nevolution and its teaching, science biography, probabilities and common sense.\n\nThe title essay (of 1978, originally titled \"The panda's peculiar thumb\") presents the paradox that poor design is a better argument for evolution than good design, as illustrated by the anatomy of the panda's \"thumb\"—which is not a thumb at all—but an extension of the radial sesamoid. Topics addressed in other essays include the female brain, the Piltdown Man hoax, Down syndrome, and the relationship between dinosaurs and birds.\n\n\"The Panda's Thumb\" won the 1981 U.S. National Book Award in Science.\n\n\n"}
{"id": "14627006", "url": "https://en.wikipedia.org/wiki?curid=14627006", "title": "Tor Ørvig", "text": "Tor Ørvig\n\nTor Ørvig (1916 – 1994) was a Norwegian-born Swedish paleontologist who explored the histology of early vertebrates. He was professor at the Swedish Museum of Natural History in Stockholm and member of the Royal Swedish Academy of Sciences.\n"}
{"id": "9365268", "url": "https://en.wikipedia.org/wiki?curid=9365268", "title": "Web-based taxonomy", "text": "Web-based taxonomy\n\nWeb-based taxonomy is the effort by taxonomists to use the World Wide Web in order to create unified, consensus taxonomies of life on Earth.\n\nIn his 2002 paper on the subject, H. Charles J. Godfray called for the creation of Web-based organisations to collect all the accumulated literature on a taxonomic group into a centralized knowledge base and make this data available through the Web as a unified taxonomy, so that it can be more easily examined and revised. Such a platform would be owned and maintained by a taxonomic working group, governed by an editor or an editorial board. An example of such a platform is FishBase.\n\nThe notion of Web-based consensus taxonomies remains controversial because, as two Australian researchers pointed out, taxonomic names are not fixed but hypotheses, and therefore in constant change.\n\n\n"}
{"id": "519323", "url": "https://en.wikipedia.org/wiki?curid=519323", "title": "Whole Earth Catalog", "text": "Whole Earth Catalog\n\nThe Whole Earth Catalog (WEC) was an American counterculture magazine and product catalog published by Stewart Brand several times a year between 1968 and 1972, and occasionally thereafter, until 1998. The magazine featured essays and articles, but was primarily focused on product reviews. The editorial focus was on self-sufficiency, ecology, alternative education, \"do it yourself\" (DIY), and holism, and featured the slogan \"access to tools\". While WEC listed and reviewed a wide range of products (clothing, books, tools, machines, seeds, etc.), it did not sell any of the products directly. Instead, the vendor's contact information was listed alongside the item and its review. This is why, while not a regularly published periodical, numerous editions and updates were required to keep price and availability information up to date.\n\nThe title \"Whole Earth Catalog\" came from a previous project by Stewart Brand. In 1966, he initiated a public campaign to have NASA release the then-rumored satellite photo of the sphere of Earth as seen from space, the first image of the \"Whole Earth.\" He thought the image might be a powerful symbol, evoking a sense of shared destiny and adaptive strategies from people. The Stanford-educated Brand, a biologist with strong artistic and social interests, believed that there was a groundswell of commitment to thoroughly renovating American industrial society along ecologically and socially just lines, whatever they might prove to be.\n\nAndrew Kirk in \"Counterculture Green\" notes that the \"Whole Earth Catalog\" was preceded by the \"Whole Earth Truck Store\". The WETS was a 1963 Dodge truck: In 1968, Brand, who was then 29, and his wife Lois embarked \"on a commune road trip\" with the truck, hoping to tour the country doing educational fairs. The truck was not only a store, but also an alternative lending library and a mobile microeducation service.\n\nKevin Kelly, who would edit later editions of the catalog, summarizes the very early history this way:\n\nThe \"Truck Store\" finally settled into its permanent location in Menlo Park, California. Instead of bringing the store to the people, Brand decided to create \"accumulatively larger versions of his tool catalog\" and sell it by mail so the people could contact the vendors directly.\n\nUsing the most basic typesetting and page-layout tools, Brand and his colleagues created the first issue of \"The Whole Earth Catalog\" in 1968. In subsequent issues, its production values gradually improved. Its outsize pages measured 11×14 inches (28×36 cm). Later editions were more than an inch thick. The early editions were published by the Portola Institute, headed by Richard Raymond. The so-called \"Last Whole Earth Catalogue\" (June 1971) won the first U.S. National Book Award in category Contemporary Affairs.\nIt was the first time a catalog had ever won such an award. \nBrand's intent with the catalog was to provide education and \"access to tools\" so a reader could \"find his own inspiration, shape his own environment, and share his adventure with whoever is interested.\"\n\nJ. Baldwin was a young designer and instructor of design at colleges around the San Francisco Bay (San Francisco State University [then San Francisco State College], the San Francisco Art Institute, and the California College of the Arts [then California College of Arts and Crafts]). As he recalled in the film \"Ecological Design\" (1994), \"Stewart Brand came to me because he heard that I read catalogs. He said, 'I want to make this thing called a \"whole Earth\" catalog so that anyone on Earth can pick up a telephone and find out the complete information on anything. ... That's my goal.'\" Baldwin served as the chief editor of subjects in the areas of technology and design, both in the catalog itself and in other publications which arose from it.\n\nTrue to his 1966 vision, Brand's publishing efforts were suffused with an awareness of the importance of ecology, both as a field of study and as an influence upon the future of humankind and emerging human awareness.\n\nFrom the opening page of the 1969 \"Catalog\":\n\nThe WHOLE EARTH CATALOG functions as an evaluation and access device. With it, the user should know better what is worth getting and where and how to do the getting.\n\nAn item is listed in the CATALOG if it is deemed:\n\nCATALOG listings are continually revised according to the experience and suggestions of CATALOG users and staff.\nWe are as gods and might as well get good at it. So far, remotely done power and glory—as via government, big business, formal education, church—has succeeded to the point where gross defects obscure actual gains. In response to this dilemma and to these gains a realm of intimate, personal power is developing—power of the individual to conduct his own education, find his own inspiration, shape his own environment, and share his adventure with whoever is interested. Tools that aid this process are sought and promoted by the WHOLE EARTH CATALOG.\n\nThe 1968 catalog divided itself into seven broad sections:\n\n\nWithin each section, the best tools and books the editors could find were collected and listed, along with images, reviews and uses, prices, and suppliers. The reader was also able to order some items directly through the catalog.\n\nLater editions changed a few of the headings, but generally kept the same overall framework.\n\nThe \"Catalog\" used a broad definition of \"tools.\" There were informative tools, such as books, maps, professional journals, courses, and classes. There were well-designed special-purpose utensils, including garden tools, carpenters' and masons' tools, welding equipment, chainsaws, fiberglass materials, tents, hiking shoes, and potters' wheels. There were even early synthesizers and personal computers.\n\nThe \"Catalog\"'s publication coincided with a great wave of convention-challenging experimentalism and a do-it-yourself attitude associated with \"the counterculture,\" and tended to appeal not only to the intelligentsia of the movement, but to creative, hands-on, and outdoorsy people of many stripes. Some of the ideas in the \"Catalog\" were developed during Brand's visits to Drop City.\n\nWith the \"Catalog\" opened flat, the reader might find the large page on the left full of text and intriguing illustrations from a volume of Joseph Needham's \"Science and Civilization in China\", showing and explaining an astronomical clock tower or a chain-pump windmill, while on the right-hand page are a review of a beginners' guide to modern technology (\"The Way Things Work\") and a review of \"The Engineers' Illustrated Thesaurus\". On another spread, the verso reviews books on accounting and moonlighting jobs, while the recto bears an article in which people tell the story of a community credit union they founded. Another pair of pages depict and discuss different kayaks, inflatable dinghies, and houseboats.\n\nThree books were serialized in the pages of the WEC, printing a couple of paragraphs per page. This made reading the catalog a page-by-page experience.\n\nSteve Jobs compared \"The Whole Earth Catalog\" to Internet search engine Google in his June 2005 Stanford University commencement speech. \"When I was young, there was an amazing publication called \"The Whole Earth Catalog\", which was one of the bibles of my generation ... It was sort of like Google in paperback form, 35 years before Google came along. It was idealistic and overflowing with neat tools and great notions.\" During the commencement speech, Jobs also quoted the farewell message placed on the back cover of the 1974 edition of the \"Catalog\": \"Stay hungry. Stay foolish.\"\n\nKevin Kelly made a similar comparison in 2008:\n\nFor this new countercultural movement, information was a precious commodity. In the '60s, there was no Internet; no 500 cable channels. [ ... The \"WEC\"] was a great example of user-generated content, without advertising, before the Internet. Basically, Brand invented the blogosphere long before there was any such thing as a blog. [ ... ] No topic was too esoteric, no degree of enthusiasm too ardent, no amateur expertise too uncertified to be included. [ ... ] This I am sure about: it is no coincidence that the \"Whole Earth Catalogs\" disappeared as soon as the web and blogs arrived. Everything the \"Whole Earth Catalogs\" did, the web does better.\n\nLooking back and discussing attitudes evident in the early editions of the catalog, Brand wrote, \"At a time when the New Left was calling for grassroots \"political\" (i.e., referred) power, \"Whole Earth\" eschewed politics and pushed grass-roots \"direct\" power—tools and skills.\"\n\nThe broad interpretation of \"tool\" coincided with that given by the designer, philosopher, and engineer Buckminster Fuller, though another thinker admired by Brand and some of his cohorts was Lewis Mumford, who had written about words as tools. Early editions reflected the considerable influence of Fuller, particularly his teachings about \"whole systems,\" \"synergetics,\" and efficiency or reducing waste. By 1971, Brand and his co-workers were already questioning whether Fuller's sense of direction might be too anthropocentric. New information arising in fields like ecology and biospherics was persuasive.\n\nBy the mid-1970s, much of the Buddhist economics viewpoint of E. F. Schumacher, as well as the activist interests of the biological species preservationists, had tempered the overall enthusiasm for Fuller's ideas in the catalog. Still later, the amiable-architecture ideas of people like Christopher Alexander and similar community-planning ideas of people like Peter Calthorpe further tempered the engineering-efficiency tone of Fuller's ideas.\n\nAn important shift in philosophy in the \"Catalogs\" occurred in the early 1970s, when Brand decided that the early stance of emphasizing individualism should be replaced with one favoring \"community\". He had originally written that \"a realm of intimate, personal power is developing\"; regarding this as important in some respects (to wit, the soon-emerging potentials of personal computing), Brand felt that the overarching project of humankind had more to do with living within natural systems, and this is something we do in common, interactively.\n\nAs an early indicator of the general Zeitgeist, the catalog's first edition preceded the original Earth Day by nearly two years. The idea of Earth Day occurred to Senator Gaylord Nelson, its instigator, \"in the summer of 1969 while on a conservation speaking tour out west,\" where the Sierra Club was active, and where young minds had been broadened and stimulated by such influences as the catalog.\n\nDespite this popular and critical success, particularly among a generation of young hippies and survivalists, the catalog was not intended to continue in publication for long, just long enough for the editors to complete a good overview of the available tools and resources, and for the word, and copies, to get out to everyone who needed them.\n\nAfter 1972 the catalog was published sporadically. Updated editions of \"The Last Whole Earth Catalog\" appeared periodically from 1971 to 1975, but only a few fully new catalogs appeared. In 1974 the \"Whole Earth Epilog\" was published, which was intended as a \"volume 2\" to the \"Last Whole Earth Catalog\". In 1980, \"The Next Whole Earth Catalog\" () was published; it was so well received that an updated second edition was published in 1981.\n\nThere were two editions in the 1980s of the \"Whole Earth Software Catalog\", a compendium for which Doubleday had bid $1.4 million for the trade paperback rights.\n\nIn 1986, \"The Essential Whole Earth Catalog\" () was published, and in 1988 the \"WEC\" was published on CD-ROM using an early form of hypertext developed by Apple Computer called \"HyperCard\". In 1988, there was a \"WEC\" dedicated to Communications Tools. A \"Whole Earth Ecolog\" was published in 1990, devoted exclusively to environmental topics. Around this time there were special \"WEC\"s on other topics (e.g., \"The Fringes of Reason\" in 1989).\n\nThe last \"full\" \"WEC\", entitled \"The Millennium Whole Earth Catalog\" (), was published in 1994.\n\nA slender, but still on A3 paper. \"30th Anniversary Celebration WEC\" was published in 1998 as part of Issue 95 of the \"Whole Earth\" magazine (); it reprinted the original \"WEC\" along with new material. An important aspect of this copy of the first \"WEC\" was a limitation placed on it by book publishers who \"begged [\"Whole Earth\"] not to reprint the Catalog with their names anywhere near books they no longer carry\". As a result, all such information was placed at the back of the catalog. This placement hampered a valuable function of the \"WEC\": nudging publishers to keep featured seminal works in print.\n\nFrom 1974 to 2003, the Whole Earth principals published a magazine, known originally as \"CoEvolution Quarterly\". When the short-lived \"Whole Earth Software Review\" (a supplement to \"The Whole Earth Software Catalog\") failed, it was merged in 1985 with \"CoEvolution Quarterly\" to form the \"Whole Earth Review\" (edited at different points by Jay Kinney, \nKevin Kelly, and Howard Rheingold), later called \"Whole Earth Magazine\" and finally just \"Whole Earth\". The last issue, number 111 (edited by Alex Steffen), was meant to be published in Spring 2003, but funds ran out. The Point Foundation, which owned \"Whole Earth\", closed its doors later that year. \n\nThe Whole Earth website continues the \"WEC\" legacy of concepts in popular discourse, medical self-care, community building, bioregionalism, environmental restoration, nanotechnology, and cyberspace.\n\nRecognizing the \"developed country\" focus of the original WEC, groups in several developing countries have created \"catalogs\" of their own to be more relevant to their countries. One such effort was an adaptation of the WEC (called the \"Liklik Buk\") written and published in the late 1970s in Papua New Guinea; by 1982 this had been enlarged, updated, and translated (as \"Save Na Mekem\") into the Pidgin language used throughout Melanesia, and updates of the English \"Liklik Buk\" were published in 1986 and 2003.\n\nIn the United States, the book \"Domebook One\" was a direct spin-off of the WEC. Lloyd Kahn, Shelter editor of the WEC, borrowed WEC production equipment for a week in 1970 and produced the first book on building geodesic domes. A year later, in 1971, Kahn again borrowed WEC equipment (an IBM Selectric Composer typesetting machine and a Polaroid MP-5 camera on an easel), and spent a month in the Santa Barbara Mountains producing \"Domebook 2\", which went on to sell 165,000 copies. With production of DB 2, Kahn and his company Shelter Publications followed Stewart Brand's move to nationwide distribution by Random House.\n\nIn 1973, Kirsten Grimstad and Susan Rennie are part of a research project at Berkeley University and publish a feminist catalog inspired by the Whole Earth Catalog, the New Woman's Survival Catalog, which gathers feminist initiatives in different domains (art, communication, work, money, self-help, self-defense…) in the USA.\n\nIn 1969, a store which was inspired by (but not financially connected with) \"The Whole Earth Catalog,\" called the Whole Earth Access opened in Berkeley, California. It closed in 1998. In 1970 a store called the \"Whole Earth Provision Co.\", inspired by the catalogue, opened in Austin, Texas. It now has eight stores in Austin, Houston, Dallas, Southlake, and San Antonio.\n\nIn late 2006, Worldchanging released their 600-page compendium of solutions, \"Worldchanging: A User's Guide to the 21st Century\", which Bill McKibben, in an article in the \"New York Review of Books\" called \"The Whole Earth Catalog retooled for the iPod generation.\" The editor of Worldchanging has since acknowledged the Catalog as a prime inspiration.\n\n\"Whole Arctic Catalog\" was written by Pamela Richot and Published in \"Backet 3: At Extremes\" in 2015 to draw attention to threats to the arctic region specifically, similarly to how \"The Whole Earth Catalog\" drew attention to global environmental threats.\n\nStewart Brand and \"The Whole Earth Catalog\" are both subjects of interest to scholars. Notable examples include works by Theodore Roszak, Howard Rheingold, Fred Turner, John Markoff, Andrew Kirk, Sam Binkley and Felicity Scott. The Stanford University Library System has a \"Whole Earth\" archive in their Department of Special Collections.\n\nIn 1970, on April Fool's Day, the Whole Earth Restaurant opened at UC Santa Cruz. It was an early source of \"whole foods\" in Northern California until it closed in 2002.\n\nA 2010 issue of the political art magazine made by the Adbusters Media Foundation was titled \"The Whole Brain Catalog\", which features a parody cover with a small human brain in place of the earth, and many references to the 1960s counter culture movement. The tagline read \"Access to Therapies\" rather than \"Access to Tools\".\n\nOn April 17, 2018 My Morning Jacket frontman Jim James announced the release of his third solo album \"Uniform Distortion\", which he stated was inspired by \"The\" \"Whole Earth Catalog.\"\n\nKevin Kelly, mentioned above for his role in editing later editions of the \"Whole Earth Catalog\", maintains a web site—Cool-Tools.org—that publishes reviews of \"the best/cheapest tools available. Tools are defined broadly as anything that can be useful. This includes hand tools, machines, books, software, gadgets, websites, maps, and even ideas.\" He also published a large format book in 2013—\"Cool Tools A Catalog of Possibilities\"—which draws on the many reviews published over the years on that web site. The format, size, and style of the book reflect and pay homage to the original \"Whole Earth Catalog\".\n\n"}
{"id": "46432477", "url": "https://en.wikipedia.org/wiki?curid=46432477", "title": "Women in chemistry", "text": "Women in chemistry\n\nThis is a list of women chemists. It should include those who have been important to the development or practice of chemistry. Their research or application has made significant contributions in the area of basic or applied chemistry.\n\nFive women have won the Nobel Prize in Chemistry (listed above), awarded annually since 1901 by the Royal Swedish Academy of Sciences. Marie Curie was the first woman to receive the prize in 1911, which was her second Nobel Prize (she also won the prize in physics in 1903, along with Pierre Curie and Henri Becquerel - making her the only woman to be award two Nobel prizes). Her prize in chemistry was for her \"discovery of the elements radium and polonium, by the isolation of radium and the study of the nature and compounds of this remarkable element.\" Irene Joliot-Curie, Marie's daughter, became the second woman to be awarded this prize in 1935 for her discovery of artificial radioactivity. Dorothy Hodgkin won the prize in 1964 for the development of protein crystallography. Among her significant discoveries are the structures of penicillin and vitamin B12. Forty five years later, Ada Yonath shared the prize with Venkatraman Ramakrishnan and Thomas A. Steitz for the study of the structure and function of the ribosome.\n\n\n\n\n"}
