{"id": "477556", "url": "https://en.wikipedia.org/wiki?curid=477556", "title": "Agnes Meyer Driscoll", "text": "Agnes Meyer Driscoll\n\nAgnes Meyer Driscoll (July 24, 1889 – September 16, 1971), known as \"Miss Aggie\" or \"Madame X\", was an American\ncryptanalyst during both World War I and World War II. Edwin T. Layton described her as \"without peer as a cryptanalyst\".\n\nBorn in Illinois in 1889, Agnes May Meyer moved with her family to Westerville, Ohio in 1895 where her father, Gustav Meyer, had taken a job teaching music at Otterbein College. In 1909 he donated the family home to the Anti-Saloon League, which had recently moved its headquarters to Westerville.\n\nMeyer attended Otterbein College from 1907 to 1909. In 1911, she received a Bachelor of Arts degree from the Ohio State University, having majored in mathematics and physics and studied foreign languages, statistics, and music. She was fluent in English, French, German, Latin, and Japanese. From her earliest days as a college student, she pursued technical and scientific studies. After graduation, she moved to Amarillo, Texas, where she lived from 1911 to 1918 and worked as director of music at a military academy, and, later, chair of the mathematics department at the local high school.\n\nOn June 22, 1918, about one year after America entered World War I, Agnes Meyer enlisted in the United States Navy – America had just started allowing women to enlist. She was recruited at the highest possible rank of chief yeoman and after a stint in the Postal Cable and Censorship Office she was assigned to the Code and Signal section of the Director of Naval Communications. After the war ended she made use of an option to continue working at her post as a civilian. Except for a two-year hiatus, when she worked for a private firm, she would remain a leading cryptanalyst for the U.S. Navy until 1949.\n\nHer efforts were not limited to manual systems; she was involved also in the emerging machine technology of the time, which was being applied both to making and breaking ciphers. In her first days in the Code and Signal section, she co-developed one of the U.S. Navy's cipher machines, the \"CM.\" This cipher machine would become a standard enciphering device for the Navy in the 1920s. In 1923, she solved a puzzle published in a magazine that was advertised as impossible. The creator, Edward Hebern of the fledgling Hebern Electric Code Company, was attempting to create a cipher machine; he offered Agnes a job as technical advisor which she left the Navy to accept. She worked on developing an early cipher machine. Although Hebern's company ultimately failed, its work in rotor technology would affect machine cryptography for years to come. Agnes returned to the navy in the spring of 1924. In August 1924 she married Michael Driscoll, a Washington, D.C. lawyer.\n\nIn early 1935, Agnes Driscoll led the attack on the Japanese M-1 cipher machine (also known to the U.S. as the ORANGE machine), used to encrypt the messages of Japanese naval attaches around the world. At the same time she sponsored the introduction of early machine support for cryptanalysis against Japanese naval code systems.\n\nIn her thirty-year career, Agnes Driscoll and Lieutenant Joseph Rochefort broke Japanese Navy manual codes—the Red Book Code in 1926 after three years of work, and the Blue Book Code in 1930, and in 1940 she made critical inroads into JN-25, the Japanese fleet's operational code, which the U.S. Navy exploited after the attack on Pearl Harbor for the rest of the Pacific War. She would be unable to finish this work, however, because she was transferred to a team working to break the German naval Enigma cipher.\n\nAfter getting the work against JN-25 started, Driscoll headed up a new group to attack the German Enigma ciphers using a catalog approach. Unfortunately the U.S. and U.K. did not communicate effectively and her approach both was fruitless and had been tried by the British, who determined that it was unlikely to work. Ultimately this work was superseded by the US-UK cryptologic exchanges of 1942–43. She worked under Laurance Safford and Joseph Rochefort.\n\nIn 1943 she worked with a team to break the Japanese cipher Coral. It was broken two months later, although Driscoll is said to have had little influence on the project.\n\nIn 1945 she appears to have worked on attacking Russian ciphers.\n\nDriscoll was part of the navy contingent that joined the new national cryptologic agencies, firstly the Armed Forces Security Agency in 1949 and then the National Security Agency in 1952. While with the Armed Forces Security Agency she may have contributed to attacking a cipher called Venona.\n\nShe retired from Armed Forces Security Agency in 1959.\n\nShe died in 1971 and is buried in Arlington National Cemetery.\n\nIn 2000 she was inducted into the National Security Agency's Hall of Honor.\n\n"}
{"id": "15286717", "url": "https://en.wikipedia.org/wiki?curid=15286717", "title": "Andrew Fraknoi", "text": "Andrew Fraknoi\n\nAndrew Fraknoi (born 1948) is a retired professor of astronomy recognized for his lifetime of work using everyday language to make astronomy more accessible and popular for both students and the general public. In 2017 Fraknoi retired from his position as Chair of the Department of Astronomy at Foothill College. In retirement he continues to teach through the Fromm Institute for Lifelong Learning and the Osher Lifelong Learning Institute at San Francisco State University, to give public lectures, and to continue to add to his body of written work. He is the recipient of numerous awards and honors in his field.\n\nFraknoi continues to serve on the Board of Trustees of the Search for Extraterrestrial Intelligence (SETI) Institute, a scientific and educational organization. He is also an elected fellow of the California Academy of Sciences, vice chair of the Lick Observatory Council, University of California's astronomical observatory, and a fellow of the Committee for Skeptical Inquiry. He has a special interest in debunking astrology and other pseudosciences connected to astronomy.\n\nAndrew Fraknoi was born in Hungary in 1948. Eight years later, following the Hungarian Revolution of 1956, he and his family fled their home in Budapest. They spent almost a year in an Austrian refugee camp and finally resettled in New York City. He entered his first American school at age 11, unable to speak English. Comic books, with their pictures and simple language, became his preferred entry point to learning. His initial interest was super hero comic books, and then comics with outer space themes. “This isn’t just comic books – this is real,\" he recalls thinking about space.\n\nFraknoi graduated from the Bronx High School of Science in 1966. He earned his A.B. in Astronomy (with a minor in Physics) from Harvard University in 1970, and his M.A. in Astronomy from University of California, Berkeley in 1972.\n\nFraknoi held the position of Chair of the Department of Astronomy at Foothill College from 1992-2017. He also taught astronomy and physics at other institutions including San Francisco State University, City College of San Francisco, Cañada College, and several campuses of the University of California Extension Division. Fraknoi served as the Executive Director of the Astronomical Society of the Pacific from 1978 to 1992, edited its popular magazine \"Mercury\", both expanding circulation and reaching out to lay people as well as teachers. In this role he also established the newsletter \"The Universe in the Classroom\" specifically for teachers. He is the founder and was director of \"Project ASTRO\", which sets up partnerships between volunteer astronomers and 4th-9th grade teachers; each astronomer \"adopts\" one classroom for a year, visits at least four times, and works with the teacher to do hands-on activities in astronomy. The program is still operating in sites around the country. Later he founded and directed \"Family ASTRO\", a project to design activities, kits and games to help families share the excitement of astronomical discovery. \n\nFraknoi is recognized for both his multi-dimensional approach, and his innovation, in making astronomy more accessible to all. His popular interdisciplinary course on Albert Einstein's life and work, \"Physics for Poets\" (nick-named \"Einstein Without Tears\"), won the 2005 “Innovation of the Year” award from the League for Innovation. In this course students learn about areas of modern physics that Einstein had a role in creating or changing, and then read novels, stories, and poems, and hear music influenced by Einstein's work and ideas. According to Thuy Thi Nguyen, president of Foothill College at the time of Fraknoi's retirement, the college sent a memo to the student body to warn them that spring semester 2017 would be their last chance to attend this very popular course. Fraknoi also created and offered various other courses for non-science majors. \n\nIn 2007 Fraknoi was the narrator for Gustav Holst's “The Planets” for the California Symphony Orchestra, a role he repeated with the Peninsula Symphony in 2017. He holds a long-time interest in astronomically correct science fiction, which he also uses in his teaching and writing. He has compiled an extensive resource with examples of scientifically accurate science fiction. He also is a science fiction author in his own right with published stories in 2 science fiction anthologies. \n\nSince 1999, Fraknoi has organized and moderated the Silicon Valley Astronomy Lecture Series where noted astronomers from around California and the nation give nontechnical public talks on new developments in our exploration of the universe in the large Smithwick Theater at Foothill College. Cosponsored by NASA's Ames Research Center, the SETI Institute, and the Astronomical Society of the Pacific, the talks, attended by 400 to 900+ people each time, have featured Nobel laureates, members of the National Academy of Sciences, and many other distinguished scientists. Some lectures have been taped and are available on YouTube. \n\nFraknoi has served on the Board of Trustees of the SETI Institute, a scientific and educational organization devoted to the search for life in the universe, since its inception in 1985. From 2010 to 2012, he was vice-chair of the Board and served on the program committee planning the first and second SETIcon, a national weekend public conferences devoted to the scientific quest for our counterparts among the stars. In 2013, he was elected to the Board of Trustees of the Friends of the Lick Observatory, now called the Lick Observatory Council.\n\nIn his retirement Fraknoi continues to teach classes at both the Fromm Institute for Lifelong Learning through University of San Francisco, and the Osher Lifelong Learning Institute at San Francisco State University.\n\nFraknoi is the author or co-author of 14 books in the field of astronomy. He was the lead author of \"Voyages through the Universe\", an introductory college astronomy textbook published by Brooks-Cole, which went through three editions. In the 1980s, he co-edited with Byron Preiss two collections of science articles and science fiction stories, \"The Universe\" and \"The Planets.\" With Sidney Wolff, Fraknoi founded and was co-editor of the first on-line journal devoted to astronomy education, \"Astronomy Education Review\". He edited two collections of resources for K-12 teachers, \"The Universe at Your Fingertips\" and \"More Universe at Your Fingertips\" published through the Astronomical Society of the Pacific. Additionally, he is the lead author of the 2016 college textbook \"Astronomy\", published by OpenStax as a free book for college students around the world, part of a project at Rice University (supported by the Bill and Melinda Gates Foundation and the William and Flora Hewlett Foundation) to make college more affordable.\n\nHe also authored multiple resources for young people. He is the co-author of the richly illustrated 2017 children’s book about eclipses \"When the Sun Goes Dark\", that came out just ahead of the North American solar eclipse in August 2017. In 2007, his first children's book \"Wonderful World of Space\" was published by Disney. When asked about the book in a 2008 podcast interview, Fraknoi explained, \"This has been a fun project. My son, who was 13 at the time, and I got a chance to write a picture book on astronomy for Disney and the challenge was how can we convey some of these modern ideas, including the Big Bang, to kids who are in 4th or 5th grade.\" His 2015 book, \"Solar Science\", published by the National Science Teachers Association, is full of 45 hands-on activities about the Sun, the seasons, the Moon, eclipses, and more.\n\nFraknoi frequently writes articles on interdisciplinary topics, such as using music, poetry, or science fiction to teach science. He has published a list of science fiction stories using good astronomy, as well as a resource guide to music inspired by astronomy. Fraknoi himself has had two stories published in science fiction anthologies. The story \"Cave in Arsia Mons\" is published in the book \"Building Red: Mission Mars,\" edited by Janet Cannon, and the story \"Supernova Rhythm\" is published in the book \"Science Fiction by Scientists,\" edited by Mike Brotherton and published by Springer.\n\nFraknoi has been a frequent radio, television and podcast guest explaining astronomical developments in everyday language. According to his published biography at Wonderfest, the science education organization that awarded him their 2002 Carl Sagan Prize for Science Popularization, Fraknoi \"appeared for over 20 years on the Jim Eason Show on KGO or KSFO radio and [as] a regular guest on both the \"Pete Wilson Show\" (later the \"Gil Gross Show\") on KGO and Michael Krasny’s \"Forum\" program on KQED. Nationally, he has been heard regularly on Science Friday and \"Weekend All Things Considered\" on National Public Radio. He has given over 400 public lectures on topics ranging from the death of stars to the origin of the universe.\" His television appearances include \"The Today Show\", \"CBS Morning News\", and \"Larry King Live\". He also posts frequently on his own blog \"Exploring the Universe\" http://fraknoi.blogspot.com/.\n\nFraknoi has been the recipient of many notable awards throughout his career. In 1994 he was awarded the American Astronomical Society's Annenberg Foundation Award -- then the highest honor in the field of astronomy education, as well as the Klumpke-Roberts Award of The Astronomical Society of the Pacific (given for a lifetime of contributions to popularizing astronomy). In 2002 he received the Carl Sagan Prize for Science Popularization. In 2007 he was named California Professor of the Year by the Carnegie Foundation for the Advancement of Teaching and the Council for Advancement and Support of Education. Additionally he was conferred the Astronomical Society of the Pacific's 2007 Richard H. Emmons award and the American Institute of Physics' 2007 Andrew Gemant Award (given for a lifetime of contributions to the intersection of physics and culture). In 2011 he was elected an Honorary Member by the Royal Astronomical Society of Canada and in 2013 was conferred the Faraday Science Communicator Award. Asteroid 4859 was named Asteroid Fraknoi by the International Astronomical Union \"to honor his work in sharing the excitement of modern astronomy with students, teachers and the public\".\n\nFraknoi has demonstrated, through a lifetime of work, his commitment to advancing public understanding of astronomy and science using everyday language. He said in an interview after being named 2007 California Professor of the Year: \"I believe that an understanding of our place in the wider universe and the methods of science are part of the birthright of everyone living on our planet... Yet, the way science is taught in this country can often discourage non-science majors from taking a life-long interest, or even a course-long interest, in science. My philosophy is to show students that science is engaging, human, and part of our cultural heritage.\" In 2013 Fraknoi received the Faraday Science Communicator Award. Michael Faraday, for whom the award was named, was an influential 19th century scientist and a great advocate for rigorous, skeptical thinking and recognizing how easy it is for the mind to deceive itself. Fraknoi was quoted on receipt of the award, \"I too try to encourage students and the public to examine claims at the fringes of science with skepticism and fact-based thinking\".\n\nFraknoi and his wife live in San Francisco. They have one son.\n\n"}
{"id": "30269772", "url": "https://en.wikipedia.org/wiki?curid=30269772", "title": "Causal patch", "text": "Causal patch\n\nA causal patch is a region of spacetime connected within the relativistic framework of causality (causal light cones).\n\nAfter Leonard Susskind proposed the black hole complementarity conjecture for black holes in quantum gravity, he realized it would also apply to a de Sitter universe with a positive cosmological constant with the cosmological horizon in place of the event horizon. The region within the horizon is the causal patch, and it is self-contained. This means we may neglect what happens beyond the cosmological horizon. A consequence of this radical conjecture is that the total number of states of the universe is finite.\n\n"}
{"id": "57644912", "url": "https://en.wikipedia.org/wiki?curid=57644912", "title": "Changing of the Gods", "text": "Changing of the Gods\n\nChanging of the Gods: Feminism and the End of Traditional Religions is a non-fiction book written by Naomi Goldenberg.\n\nA feminist take on traditional male dominated religions.\n"}
{"id": "58009847", "url": "https://en.wikipedia.org/wiki?curid=58009847", "title": "Christel Kammerer", "text": "Christel Kammerer\n\nChristel Kammerer invented the idea of Flextime, allowing workers to alter their respective start and finish times at work as long as they completed the required amount of daily/weekly required total hours. Kammerer was a German management consultant. \n"}
{"id": "3696813", "url": "https://en.wikipedia.org/wiki?curid=3696813", "title": "Classical-map hypernetted-chain method", "text": "Classical-map hypernetted-chain method\n\nThe classical-map hypernetted-chain method (CHNC method) is a method used in many-body theoretical physics for interacting uniform electron liquids in two and three dimensions, and for interacting hydrogen plasmas. The method extends the famous hypernetted-chain method (HNC) introduced by J. M. J van Leeuwen et al. to quantum fluids as well. The classical HNC, together with the Percus–Yevick approximation, are the two pillars which bear the brunt of most calculations in the theory of interacting classical fluids. Also, HNC and PY have become important in providing basic reference schemes in the theory of fluids, and hence they are of great importance to the physics of many-particle systems.\n\nThe HNC and PY integral equations provide the pair distribution functions of the particles in a classical fluid, even for very high coupling strengths. The coupling strength is measured by the ratio of the potential energy to the kinetic energy. In a classical fluid, the kinetic energy is proportional to the temperature. In a quantum fluid, the situation is very complicated as one needs to deal with quantum operators, and matrix elements of such operators, which appear in various perturbation methods based on Feynman diagrams. The CHNC method provides an approximate \"escape\" from these difficulties, and applies to regimes beyond perturbation theory. In Robert B. Laughlin's famous Nobel Laureate work on the fractional quantum Hall effect, an HNC equation was used within a classical plasma analogy.\nIn the CHNC method, the pair-distributions of the interacting particles are calculated using a mapping which ensures that the quantum mechanically correct non-interacting pair distribution function is recovered when the Coulomb interactions are switched off. The value of the method lies in its ability to calculate the \"interacting\" pair distribution functions \"g\"(\"r\") at zero and finite temperatures. Comparison of the calculated \"g\"(\"r\") with results from Quantum Monte Carlo show remarkable agreement, even for very strongly correlated systems.\n\nThe interacting pair-distribution functions obtained from CHNC have been used to calculate the exchange-correlation energies, Landau parameters of Fermi liquids and other quantities of interest in many-body physics and density functional theory, as well as in the theory of hot plasmas.\n\n\n"}
{"id": "33131237", "url": "https://en.wikipedia.org/wiki?curid=33131237", "title": "Clavaviridae", "text": "Clavaviridae\n\nClavaviridae is a family of double-stranded viruses that infect archaea. This family was first described by the team led by D. Prangishvili in 2010. There is one genus in this family (\"Clavavirus\"). Within this genus, only a single species has been described to date: \"Aeropyrum pernix bacilliform virus 1\".\n\nThe name is derived from the Latin word \"clava\" meaning stick.\n\nThe virons are bacilliform in shape and 143 nanometers (nm) in length and 15.8 nm in diameter. One end is pointed and the other is rounded. The structure of the APBV1 virion has been solved by cryo-electron microscopy to near-atomic resolution, revealing how the helical particle is built from an alpha-helical major capsid protein with a unique structural fold.\n\nThe genome is a circular double-stranded DNA molecule of 5.3 kb. It does not integrate into the host genome.\n\nInfection with this virus does not cause host cell lysis.\n"}
{"id": "62332", "url": "https://en.wikipedia.org/wiki?curid=62332", "title": "Deep sea fish", "text": "Deep sea fish\n\nDeep-sea fish are fish that live in the darkness below the sunlit surface waters, that is below the epipelagic or photic zone of the sea. The lanternfish is, by far, the most common deep-sea fish. Other deep sea fishes include the flashlight fish, cookiecutter shark, bristlemouths, anglerfish, viperfish, and some species of eelpout.\n\nOnly about 2% of known marine species inhabit the pelagic environment. This means that they live in the water column as opposed to the benthic organisms that live in or on the sea floor. Deep-sea organisms generally inhabit bathypelagic (1000–4000m deep) and abyssopelagic (4000–6000m deep) zones. However, characteristics of deep-sea organisms, such as bioluminescence can be seen in the mesopelagic (200–1000m deep) zone as well. The mesopelagic zone is the disphotic zone, meaning light there is minimal but still measurable. The oxygen minimum layer exists somewhere between a depth of 700m and 1000m deep depending on the place in the ocean. This area is also where nutrients are most abundant. The bathypelagic and abyssopelagic zones are aphotic, meaning that no light penetrates this area of the ocean. These zones make up about 75% of the inhabitable ocean space.\n\nThe epipelagic zone (0–200m) is the area where light penetrates the water and photosynthesis occurs. This is also known as the photic zone. Because this typically extends only a few hundred meters below the water, the deep sea, about 90% of the ocean volume, is in darkness. The deep sea is also an extremely hostile environment, with temperatures that rarely exceed 3 °C (37.4 °F) and fall as low as −1.8 °C (28.76 °F) (with the exception of hydrothermal vent ecosystems that can exceed 350 °C, or 662 °F), low oxygen levels, and pressures between 20 and 1,000 atmospheres (between 2 and 100 megapascals).\n\nIn the deep ocean, the waters extend far below the epipelagic zone, and support very different types of pelagic fishes adapted to living in these deeper zones.\n\nIn deep water, marine snow is a continuous shower of mostly organic detritus falling from the upper layers of the water column. Its origin lies in activities within the productive photic zone. Marine snow includes dead or dying plankton, protists (diatoms), fecal matter, sand, soot and other inorganic dust. The \"snowflakes\" grow over time and may reach several centimetres in diameter, travelling for weeks before reaching the ocean floor. However, most organic components of marine snow are consumed by microbes, zooplankton and other filter-feeding animals within the first 1,000 metres of their journey, that is, within the epipelagic zone. In this way marine snow may be considered the foundation of deep-sea mesopelagic and benthic ecosystems: As sunlight cannot reach them, deep-sea organisms rely heavily on marine snow as an energy source.\n\nSome deep-sea pelagic groups, such as the lanternfish, ridgehead, marine hatchetfish, and lightfish families are sometimes termed \"pseudoceanic\" because, rather than having an even distribution in open water, they occur in significantly higher abundances around structural oases, notably seamounts and over continental slopes. The phenomenon is explained by the likewise abundance of prey species which are also attracted to the structures.\n\nHydrostatic pressure increases by 1 atmosphere for every 10m in depth. Deep-sea organisms have the same pressure within their bodies as is exerted on them from the outside, so they are not crushed by the extreme pressure. Their high internal pressure, however, results in the reduced fluidity of their membranes because molecules are squeezed together. Fluidity in cell membranes increases efficiency of biological functions, most importantly the production of proteins, so organisms have adapted to this circumstance by increasing the proportion of unsaturated fatty acids in the lipids of the cell membranes. In addition to differences in internal pressure, these organisms have developed a different balance between their metabolic reactions from those organisms that live in the epipelagic zone. David Wharton, author of \"Life at the Limits: Organisms in Extreme Environments\", notes \"Biochemical reactions are accompanied by changes in volume. If a reaction results in an increase in volume, it will be inhibited by pressure, whereas, if it is associated with a decrease in volume, it will be enhanced\". This means that their metabolic processes must ultimately decrease the volume of the organism to some degree.\n\nMost fish that have evolved in this harsh environment are not capable of surviving in laboratory conditions, and attempts to keep them in captivity have led to their deaths. Deep-sea organisms contain gas-filled spaces (vacuoles). Gas is compressed under high pressure and expands under low pressure. Because of this, these organisms have been known to blow up if they come to the surface.\n\nThe fish of the deep-sea are among the strangest and most elusive creatures on Earth. In this deep dark unknown lie many unusual creatures that have yet to be studied. Since many of these fish live in regions where there is no natural illumination, they cannot rely solely on their eyesight for locating prey and mates and avoiding predators; deep-sea fish have evolved appropriately to the extreme sub-photic region in which they live. Many of these organisms are blind and rely on their other senses, such as sensitivities to changes in local pressure and smell, to catch their food and avoid being caught. Those that aren't blind have large and sensitive eyes that can use bioluminescent light. These eyes can be as much as 100 times more sensitive to light than human eyes. Also, to avoid predation, many species are dark to blend in with their environment.\n\nMany deep-sea fish are bioluminescent, with extremely large eyes adapted to the dark. Bioluminescent organisms are capable of producing light biologically through the agitation of molecules of luciferin, which then produce light. This process must be done in the presence of oxygen. These organisms are common in the mesopelagic region and below (200m and below). More than 50% of deep-sea fish as well as some species of shrimp and squid are capable of bioluminescence. About 80% of these organisms have photophores – light producing glandular cells that contain luminous bacteria bordered by dark colorings. Some of these photophores contain lenses, much like those in the eyes of humans, which can intensify or lessen the emanation of light. The ability to produce light only requires 1% of the organism's energy and has many purposes: It is used to search for food and attract prey, like the anglerfish; claim territory through patrol; communicate and find a mate; and distract or temporarily blind predators to escape. Also, in the mesopelagic where some light still penetrates, some organisms camouflage themselves from predators below them by illuminating their bellies to match the color and intensity of light from above so that no shadow is cast. This tactic is known as counter illumination.\n\nThe lifecycle of deep-sea fish can be exclusively deep water although some species are born in shallower water and sink upon maturation. Regardless of the depth where eggs and larvae reside, they are typically pelagic. This planktonic — drifting — lifestyle requires neutral buoyancy. In order to maintain this, the eggs and larvae often contain oil droplets in their plasma. When these organisms are in their fully matured state they need other adaptations to maintain their positions in the water column. In general, water's density causes upthrust — the aspect of buoyancy that makes organisms float. To counteract this, the density of an organism must be greater than that of the surrounding water. Most animal tissues are denser than water, so they must find an equilibrium to make them float. Many organisms develop swim bladders (gas cavities) to stay afloat, but because of the high pressure of their environment, deep-sea fishes usually do not have this organ. Instead they exhibit structures similar to hydrofoils in order to provide hydrodynamic lift. It has also been found that the deeper a fish lives, the more jelly-like its flesh and the more minimal its bone structure. They reduce their tissue density through high fat content, reduction of skeletal weight — accomplished through reductions of size, thickness and mineral content — and water accumulation makes them slower and less agile than surface fish.\n\nDue to the poor level of photosynthetic light reaching deep-sea environments, most fish need to rely on organic matter sinking from higher levels, or, in rare cases, hydrothermal vents for nutrients. This makes the deep-sea much poorer in productivity than shallower regions. Also, animals in the pelagic environment are sparse and food doesn’t come along frequently. Because of this, organisms need adaptations that allow them to survive. Some have long feelers to help them locate prey or attract mates in the pitch black of the deep ocean. The deep-sea angler fish in particular has a long fishing-rod-like adaptation protruding from its face, on the end of which is a bioluminescent piece of skin that wriggles like a worm to lure its prey. Some must consume other fish that are the same size or larger than them and they need adaptations to help digest them efficiently. Great sharp teeth, hinged jaws, disproportionately large mouths, and expandable bodies are a few of the characteristics that deep-sea fishes have for this purpose. The gulper eel is one example of an organism that displays these characteristics.\n\nFish in the different pelagic and deep water benthic zones are physically structured, and behave in ways, that differ markedly from each other. Groups of coexisting species within each zone all seem to operate in similar ways, such as the small mesopelagic vertically migrating plankton-feeders, the bathypelagic anglerfishes, and the deep water benthic rattails. \"\n\nRay finned species, with spiny fins, are rare among deep sea fishes, which suggests that deep sea fish are ancient and so well adapted to their environment that invasions by more modern fishes have been unsuccessful. The few ray fins that do exist are mainly in the Beryciformes and Lampriformes, which are also ancient forms. Most deep sea pelagic fishes belong to their own orders, suggesting a long evolution in deep sea environments. In contrast, deep water benthic species, are in orders that include many related shallow water fishes.\n\nBelow the epipelagic zone, conditions change rapidly. Between 200 metres and about 1000 metres, light continues to fade until there is almost none. Temperatures fall through a thermocline to temperatures between 3.9 °C (39 °F) and 7.8 °C (46 °F). This is the twilight or mesopelagic zone. Pressure continues to increase, at the rate of one atmosphere every 10 metres, while nutrient concentrations fall, along with dissolved oxygen and the rate at which the water circulates.\"\n\nSonar operators, using the newly developed sonar technology during World War II, were puzzled by what appeared to be a false sea floor 300–500 metres deep at day, and less deep at night. This turned out to be due to millions of marine organisms, most particularly small mesopelagic fish, with swimbladders that reflected the sonar. These organisms migrate up into shallower water at dusk to feed on plankton. The layer is deeper when the moon is out, and can become shallower when clouds pass over the moon. This phenomenon has come to be known as the deep scattering layer.\n\nMost mesopelagic fish make daily vertical migrations, moving at night into the epipelagic zone, often following similar migrations of zooplankton, and returning to the depths for safety during the day. These vertical migrations often occur over large vertical distances, and are undertaken with the assistance of a swimbladder. The swimbladder is inflated when the fish wants to move up, and, given the high pressures in the messoplegic zone, this requires significant energy. As the fish ascends, the pressure in the swimbladder must adjust to prevent it from bursting. When the fish wants to return to the depths, the swimbladder is deflated. Some mesopelagic fishes make daily migrations through the thermocline, where the temperature changes between 50 °F (10 °C) and 69 °F (20 °C), thus displaying considerable tolerances for temperature change.\n\nThese fish have muscular bodies, ossified bones, scales, well developed gills and central nervous systems, and large hearts and kidneys. Mesopelagic plankton feeders have small mouths with fine gill rakers, while the piscivores have larger mouths and coarser gill rakers. The vertically migratory fish have swimbladders.\n\nMesopelagic fish are adapted for an active life under low light conditions. Most of them are visual predators with large eyes. Some of the deeper water fish have tubular eyes with big lenses and only rod cells that look upwards. These give binocular vision and great sensitivity to small light signals. This adaptation gives improved terminal vision at the expense of lateral vision, and allows the predator to pick out squid, cuttlefish, and smaller fish that are silhouetted against the gloom above them.\n\nMesopelagic fish usually lack defensive spines, and use colour to camouflage themselves from other fish. Ambush predators are dark, black or red. Since the longer, red, wavelengths of light do not reach the deep sea, red effectively functions the same as black. Migratory forms use countershaded silvery colours. On their bellies, they often display photophores producing low grade light. For a predator from below, looking upwards, this bioluminescence camouflages the silhouette of the fish. However, some of these predators have yellow lenses that filter the (red deficient) ambient light, leaving the bioluminescence visible.\n\nThe brownsnout spookfish, a species of barreleye, is the only vertebrate known to employ a mirror, as opposed to a lens, to focus an image in its eyes.\n\nSampling via deep trawling indicates that lanternfish account for as much as 65% of all deep sea fish biomass. Indeed, lanternfish are among the most widely distributed, populous, and diverse of all vertebrates, playing an important ecological role as prey for larger organisms. The estimated global biomass of lanternfish is 550 - 660 million metric tonnes, several times the entire world fisheries catch. Lanternfish also account for much of the biomass responsible for the deep scattering layer of the world's oceans. Sonar reflects off the millions of lanternfish swim bladders, giving the appearance of a false bottom.\n\nBigeye tuna are an epipelagic/mesopelagic species that eats other fish. Satellite tagging has shown that bigeye tuna often spend prolonged periods cruising deep below the surface during the daytime, sometimes making dives as deep as 500 metres. These movements are thought to be in response to the vertical migrations of prey organisms in the deep scattering layer.\n\nBelow the mesopelagic zone it is pitch dark. This is the midnight (or bathypelagic zone), extending from 1000 metres to the bottom deep water benthic zone. If the water is exceptionally deep, the pelagic zone below 4000 metres is sometimes called the lower midnight (or abyssopelagic zone).\n\nConditions are somewhat uniform throughout these zones; the darkness is complete, the pressure is crushing, and temperatures, nutrients and dissolved oxygen levels are all low.\n\nBathypelagic fish have special adaptations to cope with these conditions – they have slow metabolisms and unspecialized diets, being willing to eat anything that comes along. They prefer to sit and wait for food rather than waste energy searching for it. The behaviour of bathypelagic fish can be contrasted with the behaviour of mesopelagic fish. Mesopelagic fish are often highly mobile, whereas bathypelagic fish are almost all lie-in-wait predators, normally expending little energy in movement.\n\nThe dominant bathypelagic fishes are small bristlemouth and anglerfish; fangtooth, viperfish, daggertooth and barracudina are also common. These fishes are small, many about 10 centimetres long, and not many longer than 25 cm. They spend most of their time waiting patiently in the water column for prey to appear or to be lured by their phosphors. What little energy is available in the bathypelagic zone filters from above in the form of detritus, faecal material, and the occasional invertebrate or mesopelagic fish. About 20 percent of the food that has its origins in the epipelagic zone falls down to the mesopelagic zone, but only about 5 percent filters down to the bathypelagic zone.\n\nBathypelagic fish are sedentary, adapted to outputting minimum energy in a habitat with very little food or available energy, not even sunlight, only bioluminescence. Their bodies are elongated with weak, watery muscles and skeletal structures. Since so much of the fish is water, they are not compressed by the great pressures at these depths. They often have extensible, hinged jaws with recurved teeth. They are slimy, without scales. The central nervous system is confined to the lateral line and olfactory systems, the eyes are small and may not function, and gills, kidneys and hearts, and swimbladders are small or missing.\n\nThese are the same features found in fish larvae, which suggests that during their evolution, bathypelagic fish have acquired these features through neoteny. As with larvae, these features allow the fish to remain suspended in the water with little expenditure of energy.\n\nDespite their ferocious appearance, these beasts of the deep are mostly miniature fish with weak muscles, and are too small to represent any threat to humans.\n\nThe swimbladders of deep sea fish are either absent or scarcely operational, and bathypelagic fish do not normally undertake vertical migrations. Filling bladders at such great pressures incurs huge energy costs. Some deep sea fishes have swimbladders which function while they are young and inhabit the upper epipelagic zone, but they wither or fill with fat when the fish move down to their adult habitat.\n\nThe most important sensory systems are usually the inner ear, which responds to sound, and the lateral line, which responds to changes in water pressure. The olfactory system can also be important for males who find females by smell.\nBathypelagic fish are black, or sometimes red, with few photophores. When photophores are used, it is usually to entice prey or attract a mate. Because food is so scarce, bathypelagic predators are not selective in their feeding habits, but grab whatever comes close enough. They accomplish this by having a large mouth with sharp teeth for grabbing large prey and overlapping gill rakers which prevent small prey that have been swallowed from escaping.\n\nIt is not easy finding a mate in this zone. Some species depend on bioluminescence. Others are hermaphrodites, which doubles their chances of producing both eggs and sperm when an encounter occurs. The female anglerfish releases pheromones to attract tiny males. When a male finds her, he bites on to her and never lets go. When a male of the anglerfish species \"Haplophryne mollis\" bites into the skin of a female, he releases an enzyme that digests the skin of his mouth and her body, fusing the pair to the point where the two circulatory systems join up. The male then atrophies into nothing more than a pair of gonads. This extreme sexual dimorphism ensures that, when the female is ready to spawn, she has a mate immediately available.\n\nMany forms other than fish live in the bathypelagic zone, such as squid, large whales, octopuses, sponges, brachiopods, sea stars, and echinoids, but this zone is difficult for fish to live in.\n\nSampling via deep trawling indicates that lanternfish account for as much as 65% of all deep-sea fish biomass. Indeed, lanternfish are among the most widely distributed, populous, and diverse of all vertebrates, playing an important ecological role as prey for larger organisms. With an estimated global biomass of 550 - 660 million metric tons, several times the entire world fisheries catch, lanternfish also account for much of the biomass responsible for the deep scattering layer of the world's oceans. In the Southern Ocean, Myctophids provide an alternative food resource to krill for predators such as squid and the king penguin. Although these fish are plentiful and prolific, currently only a few commercial lanternfish fisheries exist: These include limited operations off South Africa, in the sub-Antarctic, and in the Gulf of Oman.\n\nA 2006 study by Canadian scientists has found five species of deep-sea fish – blue hake, spiny eel – to be on the verge of extinction due to the shift of commercial fishing from continental shelves to the slopes of the continental shelves, down to depths of 1600 meters. The slow reproduction of these fish – they reach sexual maturity at about the same age as human beings – is one of the main reasons that they cannot recover from the excessive fishing.\n\n\n\n\n"}
{"id": "24617662", "url": "https://en.wikipedia.org/wiki?curid=24617662", "title": "Dependent statement", "text": "Dependent statement\n\nIn grammar, a dependent statement is a statement converted into a noun clause, normally, in English, by the addition of \"that\" at the beginning, and made dependent on another clause (e.g. as subject or object). For example, the statement \"I had saved his brother\" appears as object of the verb \"knew\" in the following quotation:\n\nI would he knew \"that I had saved his brother!\" (Shakespeare).\n\nThe statement \"They were unprepared\" is made subject of \"is\" in the following:\n\n\"That they were unprepared\" is obvious.\n\nFurther examples:\n\nMethoughts \"that I had broken from the Tower\" (Shakespeare).\n\nHis majesty hath straitly given in charge \"that no man shall have private conference, of what degree soever, with his brother\" (Shakespeare).\n\nYou shall confess \"that you are both deceived\" (Shakespeare).\n\n"}
{"id": "55377516", "url": "https://en.wikipedia.org/wiki?curid=55377516", "title": "Dual systems model", "text": "Dual systems model\n\nThe dual systems model, also known as the maturational imbalance model, is a theory arising from developmental cognitive neuroscience which posits that increased risk-taking during adolescence is a result of a combination of heightened reward sensitivity and immature impulse control. In other words, the appreciation for the benefits arising from the success of an endeavor is heightened, but the appreciation of the risks of failure lags behind.\n\nThe dual systems model hypothesizes that early maturation of the socioemotional system (including brain regions like the striatum) increases adolescents' attraction for exciting, pleasurable, and novel activities during a time when cognitive control systems (including brain regions like the prefrontal cortex) are not fully developed and thus cannot regulate these appetitive, and potentially hazardous, impulses. The temporal gap in the development of the socioemotional and cognitive control systems creates a period of heightened vulnerability to risk-taking during mid-adolescence. In the dual systems model, \"reward sensitivity\" and \"cognitive control\" refer to neurobiological constructs that are measured in studies of brain structure and function. Other models similar to the dual systems model are the maturational imbalance model, the driven dual systems model, and the triadic model.\n\nThe dual systems model arose out of evidence from developmental cognitive neuroscience providing insight into how patterns of brain development could explain aspects of adolescent decision-making. In 2008, Laurence Steinberg's laboratory at Temple University and BJ Casey's laboratory at Cornell separately proposed similar dual systems theories of adolescent risky decision-making. Casey et al. termed their model the maturational imbalance model.\n\nBoth the dual systems model and the maturational imbalance model conceive of a slower developing cognitive control system that matures through late adolescence. The dual systems model proposes an inverted-U shape development of the socioemotional system, such that reward responsivity increases in early adolescence and declines thereafter. The maturational imbalance model portrays a socioemotional system that reaches its peak around mid-adolescence and then plateaus into adulthood. Further, the dual systems model proposes that the development of the cognitive control and socioemotional systems is independent whereas the maturational imbalance proposes that the maturation of the cognitive control system leads to dampening of socioemotional responsivity.\n\nRecently, another variation of the dual systems model was proposed called the \"driven dual systems model\". This model proposes an inverted-U shaped trajectory of socioemotional system responsivity, similar to the dual systems model, but hypothesizes a cognitive control trajectory that plateaus in mid-adolescence. This cognitive control trajectory differs from that proposed by the dual systems model and maturational imbalance model which continues to increase into the early 20s. Similar to the driven dual systems model, a model has been proposed including a hyperactive socioemotional system that undermines the regulatory ability of the cognitive control system. These later models hypothesize that cognitive control development is completed by mid-adolescence and attribute increased risk-taking during adolescence to the hyperarousal of the socioemotional system. The dual systems model and maturational imbalance model propose that cognitive control development continues into early adulthood and that increased risk-taking in adolescence is attributable to a developmental imbalance where the socioemotional system is at its peak of development but the cognitive control system developmental trajectory lags behind.\n\nThe \"triadic model\", which includes a third brain system responsible for emotion processing and primarily implicating the amygdala. The triadic model proposes that this emotion system increases impulsivity during adolescence by increasing the perceived cost of delaying decision-making. This model posits that impulsivity and risk seeking in adolescence is due to a combination of hyperactive reward systems causing adolescents to approach appetitive stimuli, emotion processing systems causing adolescents to enhance perceived costs of delaying behaviors and reduce avoidance of potentially negative stimuli, and an underdeveloped cognitive control system that is unable to regulate reward-seeking behaviors.\n\nRisk-taking in certain, but not all, domains peaks during adolescence. Most notably, mortality and morbidity rates increase significantly from childhood to adolescence despite the fact that physical and mental capabilities increase during this period. The primary cause for this increase in mortality/morbidity among adolescents is preventable injury. According to the Center for Disease Control, in 2014 about 40% of all adolescent deaths (ages 15–19 years) were caused by unintentional accidents. From 1999 to 2006, almost one-half of all adolescent deaths (ages 12–19 years) were due to accidental injury. Of these unintentional injuries, about 2/3 are due to motor vehicle accidents, followed by unintentional poisoning, unintentional drowning, other land transportation accidents, and unintentional discharge of firearms.\n\nThe dual systems model proposes that mid-adolescence is the time of highest biological propensity for risk-taking, but that older adolescents may exhibit higher levels of real-world risk-taking (e.g., binge drinking is most common during the early 20s) not due to greater propensity for risk-taking but due to greater opportunity. For example, individuals in their early 20s compared to mid-adolescence have less adult supervision, greater financial resources, and greater legal privileges. The dual systems model looks to experimental paradigms in developmental neuroscience for evidence of this greater biological propensity for risk-taking.\n\nThere is also a consistent relation between age and crime with adolescents and young adults being more likely to engage in violent and non-violent crime. These findings are linked to increases in sensation-seeking, which is the tendency to seek out novel, exciting, and rewarding stimuli, during adolescence, and continued development of impulse control, which is the ability to regulate one's behavior. The dual systems model points to brain development as a mechanism for this association.\n\nAcross many species including humans, rodents, and nonhuman primates, adolescents demonstrate peaks in reward-seeking behaviors. For example, adolescent rats are more sensitive than adult rats to rewarding stimuli and show enhanced behavioral responses to novelty and peers. Adolescent humans show peaks in self-reported sensation-seeking, increased neural activation to monetary and social rewards, greater temporal discounting of delayed rewards, and heightened preferences for primary rewards (e.g., sweet substances).\n\nSensation-seeking is a type of reward seeking involving the tendency to seek out novel, exciting, and rewarding stimuli. Sensation-seeking has been found to increase in preadolescence, peak in mid-adolescence, and decline in early adulthood.\n\nImpulsivity has been found to exhibit a different developmental trajectory than reward or sensation seeking. Impulsivity gradually declines with age in a linear fashion. Around mid-adolescence when impulsivity and sensation-seeking are at their peak is the theoretical peak age for risk-taking according to the dual systems model.\n\nAdolescent risk-taking is more likely to occur in the presence of peers compared to adults. Animal studies have found that adolescent mice, but not adult mice, consume more alcohol in the presence of peers than when alone. In humans, the presence of peers has been found to result in increased activation in the striatum and orbitofrontal cortex risk-taking, and activation in these regions predicted subsequent risk-taking among adolescents but not adults. Age differences in activation of the striatum and frontal cortex have been interpreted to suggest heightened risk-taking in the presence of peers is due to the influence of peers on reward processing rather than the influence of peers on cognitive control.\n\nThe term socioemotional brain network or system (also known as the ventral affective system) refers to the striatum as well as the medial and orbital prefrontal cortices.\n\nEvidence from rodent studies indicates the dopaminergic system, the pathway connecting the ventral tegmental area to the nucleus accumbens and olfactory tubercle, plays a critical role in the brain's reward circuitry and the dopamine-rich striatum has been implicated as a key contributor to reward sensitivity in the brain.\n\nDuring puberty, the dopaminergic system undergoes significant reorganization. Increased dopamine projections from mesolimbic areas (e.g., the striatum) to the prefrontal cortex have been observed during mid- and late-adolescence. These projections are pruned decline in early adulthood. Adolescent-specific peaks in dopamine receptors in the striatum have been observed in humans and rodents. Additionally, dopamine concentrations projecting to the prefrontal cortex increase into adolescence as do the dopamine projections from the prefrontal cortex to the striatum (namely the nucleus accumbens).\n\nThe striatum has been linked to reward processing, learning, and motivation.\n\nNeuroimaging studies using functional magnetic resonance imaging (fMRI) have shown that the ventral striatum is more active among adolescents compared to children and adults when receiving monetary rewards, primary rewards, and social rewards. Peaks in striatal activity as associated with increased self-reported risk-taking.\n\nSome studies have found that striatum activity is blunted compared to children and adults when anticipating rewards, which has been linked to greater risk-taking behaviors. The theory linking this hypoactivation to greater risk-taking is that adolescents experience less gratifying experience from anticipating rewards and they are therefore motivated to seek out more reward-inducing experiences to achieve the same level of reward sensation as other age groups.\n\nAlthough evidence exists for both adolescent hyper-responsiveness to rewards and hypo-responsiveness to rewards, the field of developmental neuroscience has generally converged on the view of hyper-responsiveness. In other words, that is, that adolescents are motivated, in part, to engage in greater reward-seeking behaviors because of developmental changes in the striatum that contribute to hypersensitivity to reward.\n\nThe cognitive control system refers to the lateral prefrontal, lateral parietal, and anterior cingulate cortices. The most commonly investigated region is the prefrontal cortex which undergoes substantial development throughout adolescence. The development of the prefrontal cortex has been implicated in the ability to regulate behavior and engage in inhibitory control.\n\nAs a result of synaptic pruning and myelination of the prefrontal cortex, improvements in executive functions have been observed during adolescence.\n\nDuring development, the brain undergoes overproduction of neurons and their synaptic connections and then prunes those that are unnecessary for optimal functioning. This developmental process results in grey matter reduction over development. During adolescence, this pruning process is specialized with some areas losing approximately half of their synaptic connections but others showing little change. Total grey matter volume undergoes substantial pruning starting around puberty. The process of grey matter loss (i.e., maturation) occurs differentially in different brain regions with frontal and occipital poles losing grey matter early, but the prefrontal cortex losing grey matter only at the end of adolescence.\n\nIn addition to synaptic pruning, the brain undergoes myelination, which influences the speed of information flow across brain regions. Myelination involves neuronal axons connecting certain brain areas to become insulated with a white, fatty substance called myelin that increases the speed and efficiency of transmission along axons. Myelination increases dramatically during adolescence. Myelination contributes to the developmental thinning or reduction in grey matter in the prefrontal cortex during adolescence.\n\nEvidence supporting the dual systems model theory of delayed maturation of the cognitive control system is supported by evidence of structural changes like cortical thinning as well as less diffuse activation of frontal regions during inhibitory control tasks from adolescence to adulthood. Regardless of age, increased activation of the prefrontal cortex is related to better performance on response inhibition tasks.\n\nThree primary experimental paradigms are used to study reward behavior in adolescents (1) passive receipt of reward, (2) reward conditional on task performance, and (3) decision-making selecting different types of reward options.\n\nPassive exposure tasks generally involve exposing the participant to pleasant stimuli (e.g., monetary reward, attractive faces). These paradigms also involve exposure to negative stimuli for the purposes of comparison (e.g., monetary loss, angry faces). Although these tasks are more commonly used to investigate emotion processing rather than reward, some studies have used a slot-machine passive task to target reward circuitry in the brain. Faces have also been used as reward for motivational paradigms. Passive exposure tasks have been found to activate the striatum and orbitofrontal cortex, with striatal activation greater in adolescents in response to rewarding stimuli but orbitofrontal activation greater in adults in response to negative stimuli.\n\nReward tied to task performance typically involves participants being asked to complete a task in order to obtain a reward (and sometimes to avoid losing a reward). Task performance is not necessarily directly related to the reward. Examples of this type of task are the Pirate's paradigm, monetary incentive delay (MID) task, Iowa Gambling Task, Balloon Analogue Risk Task (BART), and Columbia Card Task, among others. Differences in activation to anticipation of reward versus preparation to try and achieve reward have been reported on performance related reward tasks.\n\nReward decision-making tasks involve participants being asked to choose among different options of reward. Sometimes the rewards differ on probability, magnitude, or type of reward (e.g., social versus monetary). These tasks are typically conceived to not have a correct or incorrect response, but rather to have decision-making based on the participants' preference. Examples of decision making tasks include delay discounting tasks and the Driving Game. During feedback on decision-making tasks, greater striatal activation to rewarding outcomes has been observed in adolescents compared to adults.\n\nCommon response inhibition tasks are the Go/No-Go, Flanker, Stroop, Stop Signal, and anti-saccade tasks. Individuals who perform well on these tasks generally activate the prefrontal cortex to a greater extent than individuals who perform poorly on these tasks. Performance on these tasks improves with age.\n\nThe Go/No-Go task requires participants to respond, usually by pressing a button or a key on a computer keyboard, to a designated cue or withhold a response, by not pressing the button/key, to a different designated cue. Variants of this task include alphabet letters, shapes, and faces.\n\nThe Flanker task typically involves presentation of a target flanked by non-target stimuli that is either in the same direction as the target (congruent) or in the opposite direction of a target (incongruent) or neither direction (neutral). Participants have to respond to the direction of the target ignoring the non-target stimuli.\n\nStroop tasks require participants to respond to one facet of the presented stimuli (e.g., read the word) but ignore another competing facet (e.g., ignore a contradictory color).\n\nThe Stop Signal task is similar to the Go/No-Go task in that participants see a cue indicating a go trial. For stop trials, participants see the go cue but then are presented with the stop signal (typically a sound) indicating they should not respond to the go trial. Presenting the stop signal after the go cue makes this task more difficult than traditional Go/No-Go tasks.\n\nAnti-saccade tasks typically require participants to fixate on a motionless target. A stimulus is then presented on one side of the target and the participant is asked to make a saccade (either move their eyes or respond with a button press) in the direction away from the stimulus.\n\nAdolescent developmental immaturity and culpability were central to three US Supreme Court cases: \"Roper v. Simmons\", \"Graham v. Florida\", and \"Miller v.\" \"Alabama\". Prior to \"Roper\" in 2005, the Supreme Court had relied on common sense standards to determine adolescent culpability. For example, in \"Thompson v. Oklahoma\", the Court prohibited capital punishment for individuals under the age of 16 stating that \"Contemporary standards of decency confirm our judgment that such a young person is not capable of acting with the degree of culpability that can justify the ultimate penalty.\" In \"Roper\", however, the Court looked to developmental science as rationale for abolishing capital punishment for juveniles. In 2010, the Court ruled life without parole was unconstitutional for juveniles in \"Graham\" and in 2012 the Court ruled that States could not mandate life without parole for juveniles even in the case of homicide in \"Miller.\" In \"Miller\", the Court stated \"It is increasingly clear that adolescent brains are not yet fully mature in regions and systems related to higher-order executive functions such as impulse control, planning ahead, and risk avoidance.\"\n\nThere have been some critiques of the dual systems model purporting that: (1) there are too few studies to provide enough empirical support for the model; (2) empirical evidence are contrary to the model, namely that activation of the ventral affective or socioemotional system has been associated with adaptive behavior and activation of the cognitive control system has been associated with maladaptive functioning; and (3) the model is overly simplistic in its description of brain development. Respondents continue to assert the value in the dual systems model despite acknowledging that it is a simplified model of real-world behavior.\n\nOthers have criticized the triadic model for a lack of evidence implicating the emotion processing systems (e.g., the amygdala) in risk-taking during adolescence.\n"}
{"id": "524396", "url": "https://en.wikipedia.org/wiki?curid=524396", "title": "Ecosystem diversity", "text": "Ecosystem diversity\n\nEcosystem diversity deals with the variations in ecosystems within a geographical location and its overall impact on human existence and the environment.\n\nEcosystem diversity is a type of biodiversity. It is the variation in the ecosystems found in a region or the variation in ecosystems over the whole planet. Biodiversity is important because it clears out our water, changes out climate, and provides us with food. Ecological diversity includes the variation in both terrestrial and aquatic ecosystems. Ecological diversity can also take into account the variation in the complexity of a biological community, including the number of different niches, the number of trophic levels and other ecological processes. An example of ecological diversity on a global scale would be the variation in ecosystems, such as deserts, forests, grasslands, wetlands and oceans. Ecological diversity is the largest scale of biodiversity, and within each ecosystem, there is a great deal of both species and genetic diversity.\n\nDiversity in the ecosystem is significant to human existence for a variety of reasons. Ecosystem diversity boosts the availability of oxygen via the process of photosynthesis amongst plant organisms domiciled in the habitat. Diversity In an aquatic environment helps in the purification of water by plant varieties for use by humans. Diversity increases plant varieties which serves as a good source for medicines and herbs for human use. A lack of diversity in the ecosystem produces an opposite result.\n\nSome examples of ecosystems that are rich in diversity are:\nEcological diversity around the world can be directly linked to the evolutionary and selective pressures that constrain the diversity outcome of the ecosystems within different niches. Tundras, Rainforests, coral reefs and deciduous forests all are formed as a result of evolutionary pressures. Even seemingly small evolutionary interactions can have large impacts on the diversity of the ecosystems throughout the world. One of the best studied cases of this is of the honey bee's interaction with angiosperms on every continent in the world except Antartica. \n\nIn 2010 Robert Brodschneider, and Karl Crailsheim conducted a study about the health and nutrition in honey bee colonies, the study conducted focused on: overall colony health, adult nutrition, and larva nutrition as a function of the effect of pesticides, monocultures and genetically modified crops to see if the anthropogenically created problems can have an effect pollination levels. The results indicate that human activity does have a role in the destruction of the fitness of the bee colony. The extinction or near extinction of these pollinators would result in many plants that feed humans on a wide scale, needing alternative pollination methods. Crop pollinating insects are worth annually 14.6 billion to the US economy and cost to hand pollinate over insect pollination will cost an estimated 5,715-$7,135 per hectare additionally. Not only will there be a cost increase but also an decrease in colony fitness, leading to a decrease in genetic diversity, which studies have shown has a direct link to the long term survival of the honey bee colonies. \n\nAccording to a study, there are over 50 plants that are dependent on bee pollination, many of these being key staples to feeding the world. Another study conducted states that as a direct result of a lack of plant diversity, will lead to a decline in the bee population fitness, and a low bee colony fitness has impacts on the fitness of plant ecosystem diversity. By allowing for bee pollination and working to reduce anthropogenically harmful footprints, bee pollination can increase flora growth genetic diversity and create a unique ecosystem that is highly diverse and can provide a habitat and niche for many other organisms to thrive. Due to the evolutionary pressures of bees being located on six out of seven continents, there can be no denying the impact of pollinators on the ecosystem diversity. The pollen collected by the bees is harvested and used as an energy source for winter time, this act of collecting pollen from local plants also has a more important effect of facilitating the movement of genes between organisms. \n\nThe new evolutionary pressures that are largely anthropogenically catalyzed can potentially cause wide spread collapse of ecosystems. In the north Atlantic sea, a study was conducted that followed the effects of the human interaction on surrounding ocean habitats. They found that in there was no habitat or trophic level that in some way was effected negatively by human interaction, and that much of the diversity of life was being stunted as a result.\n\n"}
{"id": "48408726", "url": "https://en.wikipedia.org/wiki?curid=48408726", "title": "Einstein's Cosmos", "text": "Einstein's Cosmos\n\nEinstein's Cosmos: How Albert Einstein's Vision Transformed Our Understanding of Space and Time is a popular science book by Michio Kaku first published in 2004. In the book Kaku discusses Albert Einstein's work, life and concepts such as E=mc² as well as special relativity.\n"}
{"id": "49641828", "url": "https://en.wikipedia.org/wiki?curid=49641828", "title": "Enneasartorite", "text": "Enneasartorite\n\nEnneasartorite is a very rare mineral with formula TlPbAsS. It belongs to sartorite homologous series. It is related to other recently approved minerals of the sartorite series: hendekasartorite and heptasartorite. All come from Lengenbach quarry in Switzerland, which is famous for thallium sulfosalts. Enneasartorite is chemically similar to edenharterite and hutchinsonite.\n"}
{"id": "8255950", "url": "https://en.wikipedia.org/wiki?curid=8255950", "title": "Esagu", "text": "Esagu\n\n'eSagu' is a web-based personalized agro-advisory system which uses Information Technology to solve the unscientific agricultural practices. Sagu means cultivation in Telugu-local language of Telangana Andhra Pradesh, the region in which the project started.\nE-Sagu means electronic cultivation.\nIt exploits the advances in Information Technology to build a cost-effective agricultural information dissemination system to disseminate expert agriculture knowledge to the farming community to improve the crop productivity.\n\nIt is a joint research project of Media lab asia and IIIT Hyderabad\n\n\n"}
{"id": "1924100", "url": "https://en.wikipedia.org/wiki?curid=1924100", "title": "Fire ecology", "text": "Fire ecology\n\nFire ecology is a scientific discipline concerned with natural processes involving fire in an ecosystem and the ecological effects, the interactions between fire and the abiotic and biotic components of an ecosystem, and the role as an ecosystem process. Many ecosystems, particularly prairie, savanna, chaparral and coniferous forests, have evolved with fire as an essential contributor to habitat vitality and renewal. Many plant species in fire-affected environments require fire to germinate, establish, or to reproduce. Wildfire suppression not only eliminates these species, but also the animals that depend upon them.\n\nCampaigns in the United States have historically molded public opinion to believe that wildfires are always harmful to nature. This view is based on the outdated belief that ecosystems progress toward an equilibrium and that any disturbance, such as fire, disrupts the harmony of nature. More recent ecological research has shown, however, that fire is an integral component in the function and biodiversity of many natural habitats, and that the organisms within these communities have adapted to withstand, and even to exploit, natural wildfire. More generally, fire is now regarded as a 'natural disturbance', similar to flooding, wind-storms, and landslides, that has driven the evolution of species and controls the characteristics of ecosystems.\n\nFire suppression, in combination with other human-caused environmental changes, may have resulted in unforeseen consequences for natural ecosystems. Some large wildfires in the United States have been blamed on years of fire suppression and the continuing expansion of people into fire-adapted ecosystems, but climate change is more likely responsible. Land managers are faced with tough questions regarding how to restore a natural fire regime, but allowing wildfires to burn is the least expensive and likely most effective method.\n\nA fire regime describes the characteristics of fire and how it interacts with a particular ecosystem. Its \"severity\" is a term that ecologists use to refer to the impact that a fire has on an ecosystem. Ecologists can define this in many ways, but one way is through an estimate of plant mortality. Fire can burn at three levels. Ground fires will burn through soil that is rich in organic matter. Surface fires will burn through dead plant material that is lying on the ground. Crown fires will burn in the tops of shrubs and trees. Ecosystems generally experience a mix of all three.\n\nFires will often break out during a dry season, but in some areas wildfires may also commonly occur during a time of year when lightning is prevalent. The frequency over a span of years at which fire will occur at a particular location is a measure of how common wildfires are in a given ecosystem. It is either defined as the average interval between fires at a given site, or the average interval between fires in an equivalent specified area.\n\nDefined as the energy released per unit length of fireline (kW m), wildfire intensity can be estimated either as\n\n\nFires can affect soils through heating and combustion processes. Depending on the temperatures of the soils caused by the combustion processes, different effects will happen- from evaporation of water at the lower temperature ranges, to the combustion of soil organic matter and formation of pyrogenic organic matter, otherwise known as charcoal.\n\nFires can cause changes in soil nutrients through a variety of mechanisms, which include oxidation, volatilization, erosion, and leaching by water, but the event must usually be of high temperatures in order of significant loss of nutrients to occur. However, quantity of nutrients available in soils are usually increased due to the ash that is generated, and this is made quickly available, as opposed to the slow release of nutrients by decomposition. Rock spalling (or thermal exfoliation) accelerates weathering of rock and potentilly the release of some nutrients.\n\nIncrease in the pH of the soil following a fire is commonly observed, most likely due to the formation of calcium carbonate, and the subsequent decomposition of this calcium carbonate to calcium oxide when temperatures get even higher. It could also be due to the increased cation content in the soil due to the ash, which temporarily increases soil pH. Microbial activity in the soil might also increase due to the heating of soil and increased nutrient content in the soil, though studies have also found complete loss of microbes on the top layer of soil after a fire. Overall, soils become more basic (higher pH) following fires because of acid combustion. By driving novel chemical reactions at high temperatures, fire can even alter the texture and structure of soils by affecting the clay content and the soil's porosity.\n\nRemoval of vegetation following a fire can cause several effects on the soil, such as increasing the temperatures of the soil during the day due to increased solar radiation on the soil surface, and greater cooling due to loss of radiative heat at night. Fewer leaves to intercept rain will also cause more rain to reach the soil surface, and with fewer plants to absorb the water, the amount of water content in the soils might increase. However, it might be seen that ash can be water repellent when dry, and therefore water content and availability might not actually increase.\n\nPlants have evolved many adaptations to cope with fire. Of these adaptations, one of the best-known is likely \"pyriscence\", where maturation and release of seeds is triggered, in whole or in part, by fire or smoke; this behaviour is often erroneously called \"serotiny\", although this term truly denotes the much broader category of seed release activated by any stimulus. All pyriscent plants are serotinous, but not all serotinous plants are pyriscent (some are necriscent, hygriscent, xeriscent, soliscent, or some combination thereof). On the other hand, germination of seed activated by trigger is not to be confused with pyriscence; it is known as \"physiological dormancy\".\n\nIn chaparral communities in Southern California, for example, some plants have leaves coated in flammable oils that encourage an intense fire. This heat causes their fire-activated seeds to germinate (an example of dormancy) and the young plants can then capitalize on the lack of competition in a burnt landscape. Other plants have smoke-activated seeds, or fire-activated buds. The cones of the Lodgepole pine (\"Pinus contorta\") are, conversely, pyriscent: they are sealed with a resin that a fire melts away, releasing the seeds. Many plant species, including the shade-intolerant giant sequoia (\"Sequoiadendron giganteum\"), require fire to make gaps in the vegetation canopy that will let in light, allowing their seedlings to compete with the more shade-tolerant seedlings of other species, and so establish themselves. Because their stationary nature precludes any fire avoidance, plant species may only be fire-intolerant, fire-tolerant or fire-resistant.\n\nFire-intolerant plant species tend to be highly flammable and are destroyed completely by fire. Some of these plants and their seeds may simply fade from the community after a fire and not return; others have adapted to ensure that their offspring survives into the next generation. \"Obligate seeders\" are plants with large, fire-activated seed banks that germinate, grow, and mature rapidly following a fire, in order to reproduce and renew the seed bank before the next fire.\nSeeds may contain the receptor protein KAI2, that is activated by the growth hormones karrikin released by the fire.\nFire-tolerant species are able to withstand a degree of burning and continue growing despite damage from fire. These plants are sometimes referred to as \"resprouters.\" Ecologists have shown that some species of resprouters store extra energy in their roots to aid recovery and re-growth following a fire. For example, after an Australian bushfire, the Mountain Grey Gum tree (\"Eucalyptus cypellocarpa\") starts producing a mass of shoots of leaves from the base of the tree all the way up the trunk towards the top, making it look like a black stick completely covered with young, green leaves.\n\nFire-resistant plants suffer little damage during a characteristic fire regime. These include large trees whose flammable parts are high above surface fires. Mature ponderosa pine (\"Pinus ponderosa\") is an example of a tree species that suffers virtually no crown damage under a naturally mild fire regime, because it sheds its lower, vulnerable branches as it matures.\n\nLike plants, animals display a range of abilities to cope with fire, but they differ from most plants in that they must avoid the actual fire to survive. Although birds are vulnerable when nesting, they are generally able to escape a fire; indeed they often profit from being able to take prey fleeing from a fire and to recolonize burned areas quickly afterwards. Some anthropological and ethno-ornithological evidence suggests that certain species of fire-foraging raptors may engage in intentional fire propagation to flush out prey. Mammals are often capable of fleeing a fire, or seeking cover if they can burrow. Amphibians and reptiles may avoid flames by burrowing into the ground or using the burrows of other animals. Amphibians in particular are able to take refuge in water or very wet mud. Some arthropods also take shelter during a fire, although the heat and smoke may actually attract some of them, to their peril. Microbial organisms in the soil vary in their heat tolerance but are more likely to be able to survive a fire the deeper they are in the soil. A low fire intensity, a quick passing of the flames and a dry soil will also help. An increase in available nutrients after the fire has passed may result in larger microbial communities than before the fire. The generally greater heat tolerance of bacteria relative to fungi makes it possible for soil microbial population diversity to change following a fire, depending on the severity of the fire, the depth of the microbes in the soil, and the presence of plant cover. Certain species of fungi, such as \"Cylindrocarpon destructans\" appear to be unaffected by combustion contaminants, which can inhibit re-population of burnt soil by other microorganisms, and therefore have a higher chance of surviving fire disturbance and then recolonizing and out-competing other fungal species afterwards.\n\nFire behavior is different in every ecosystem and the organisms in those ecosystems have adapted accordingly. One sweeping generality is that in all ecosystems, fire creates a mosaic of different habitat patches, with areas ranging from those having just been burned to those that have been untouched by fire for many years. This is a form of ecological succession in which a freshly burned site will progress through continuous and directional phases of colonization following the destruction caused by the fire. Ecologists usually characterize succession through the changes in vegetation that successively arise. After a fire, the first species to re-colonize will be those with seeds are already present in the soil, or those with seeds are able to travel into the burned area quickly. These are generally fast-growing herbaceous plants that require light and are intolerant of shading. As time passes, more slowly growing, shade-tolerant woody species will suppress some of the herbaceous plants. Conifers are often early successional species, while broad leaf trees frequently replace them in the absence of fire. Hence, many conifer forests are themselves dependent upon recurring fire.\n\nDifferent species of plants, animals, and microbes specialize in exploiting different stages in this process of succession, and by creating these different types of patches, fire allows a greater number of species to exist within a landscape. Soil characteristics will be a factor in determining the specific nature of a fire-adapted ecosystem, as will climate and topography.\n\nMild to moderate fires burn in the forest understory, removing small trees and herbaceous groundcover. High-severity fires will burn into the crowns of the trees and kill most of the dominant vegetation. Crown fires may require support from ground fuels to maintain the fire in the forest canopy (passive crown fires), or the fire may burn in the canopy independently of any ground fuel support (an active crown fire). High-severity fire creates complex early seral forest habitat, or snag forest with high levels of biodiversity. When a forest burns frequently and thus has less plant litter build-up, below-ground soil temperatures rise only slightly and will not be lethal to roots that lie deep in the soil. Although other characteristics of a forest will influence the impact of fire upon it, factors such as climate and topography play an important role in determining fire severity and fire extent. Fires spread most widely during drought years, are most severe on upper slopes and are influenced by the type of vegetation that is growing.\n\nIn Canada, forests cover about 10% of the land area and yet harbor 70% of the country’s bird and terrestrial mammal species. Natural fire regimes are important in maintaining a diverse assemblage of vertebrate species in up to twelve different forest types in British Columbia. Different species have adapted to exploit the different stages of succession, regrowth and habitat change that occurs following an episode of burning, such as downed trees and debris. The characteristics of the initial fire, such as its size and intensity, cause the habitat to evolve differentially afterwards and influence how vertebrate species are able to use the burned areas.\n\nShrub fires typically concentrate in the canopy and spread continuously if the shrubs are close enough together. Shrublands are typically dry and are prone to accumulations of highly volatile fuels, especially on hillsides. Fires will follow the path of least moisture and the greatest amount of dead fuel material. Surface and below-ground soil temperatures during a burn are generally higher than those of forest fires because the centers of combustion lie closer to the ground, although this can vary greatly. Common plants in shrubland or chaparral include manzanita, chamise and Coyote Brush.\n\nCalifornia shrubland, commonly known as chaparral, is a widespread plant community of low growing species, typically on arid sloping areas of the California Coast Ranges or western foothills of the Sierra Nevada. There are a number of common shrubs and tree shrub forms in this association, including salal, toyon, coffeeberry and Western poison oak. Regeneration following a fire is usually a major factor in the association of these species.\n\nFynbos shrublands occur in a small belt across South Africa. The plant species in this ecosystem are highly diverse, yet the majority of these species are obligate seeders, that is, a fire will cause germination of the seeds and the plants will begin a new life-cycle because of it. These plants may have coevolved into obligate seeders as a response to fire and nutrient-poor soils. Because fire is common in this ecosystem and the soil has limited nutrients, it is most efficient for plants to produce many seeds and then die in the next fire. Investing a lot of energy in roots to survive the next fire when those roots will be able to extract little extra benefit from the nutrient-poor soil would be less efficient. It is possible that the rapid generation time that these obligate seeders display has led to more rapid evolution and speciation in this ecosystem, resulting in its highly diverse plant community.\n\nGrasslands burn more readily than forest and shrub ecosystems, with the fire moving through the stems and leaves of herbaceous plants and only lightly heating the underlying soil, even in cases of high intensity. In most grassland ecosystems, fire is the primary mode of decomposition, making it crucial in the recycling of nutrients. It has been hypthesized that fire only recently became the primary mode of decomposition in many grassland ecosystems after the removal or extinction of large migratory herds of browsing or grazing megafauna driven by predator pressure. In this view, in the absence of functional communities of large migratory herds of herbivorous megafauna and attendant predators, overuse of fire to maintain grassland ecosystems may lead to excessive oxidation, loss of carbon, and desertification in susceptible climates.\n\nIn the savanna of South Africa, recently burned areas have new growth that provides palatable and nutritious forage compared to older, tougher grasses. This new forage attracts large herbivores from areas of unburned and grazed grassland that has been kept short by constant grazing. On these unburned \"lawns\", only those plant species adapted to heavy grazing are able to persist; but the distraction provided by the newly burned areas allows grazing-intolerant grasses to grow back into the lawns that have been temporarily abandoned, so allowing these species to persist within that ecosystem.\n\nMuch of the southeastern United States was once open longleaf pine forest with a rich understory of grasses, sedges, carnivorous plants and orchids. The above maps shows that these ecosystems (coded as pale blue) had the highest fire frequency of any habitat, once per decade or less. Without fire, deciduous forest trees invade, and their shade eliminates both the pines and the understory. Some of the typical plants associated with fire include Yellow Pitcher Plant and Rose pogonia. The abundance and diversity of such plants is closely related to fire frequency. Rare animals such as gopher tortoises and indigo snakes also depend upon these open grasslands and flatwoods. Hence, the restoration of fire is a priority to maintain species composition and biological diversity.\n\nAlthough it may seem strange, many kinds of wetlands are also influenced by fire. This usually occurs during periods of drought. In landscapes with peat soils, such as bogs, the peat substrate itself may burn, leaving holes that refill with water as new ponds. Fires that are less intense will remove accumulated litter and allow other wetland plants to regenerate from buried seeds, or from rhizomes. Wetlands that are influenced by fire include coastal marshes, wet prairies, peat bogs, floodplains, prairie marshes and flatwoods. Since wetlands can store large amounts of carbon in peat, the fire frequency of vast northern peatlands is linked to processes controlling the carbon dioxide levels of the atmosphere, and to the phenomenon of global warming.\n\nFire serves many important functions within fire-adapted ecosystems. Fire plays an important role in nutrient cycling, diversity maintenance and habitat structure. The suppression of fire can lead to unforeseen changes in ecosystems that often adversely affect the plants, animals and humans that depend upon that habitat. Wildfires that deviate from a historical fire regime because of fire suppression are called \"uncharacteristic fires\".\n\nIn 2003, southern California witnessed powerful chaparral wildfires. Hundreds of homes and hundreds of thousands of acres of land went up in flames. Extreme fire weather (low humidity, low fuel moisture and high winds) and the accumulation of dead plant material from 8 years of drought, contributed to a catastrophic outcome. Although some have maintained that fire suppression contributed to an unnatural buildup of fuel loads, a detailed analysis of historical fire data has showed that this may not have been the case. Fire suppression activities had failed to exclude fire from the southern California chaparral. Research showing differences in fire size and frequency between southern California and Baja has been used to imply that the larger fires north of the border are the result of fire suppression, but this opinion has been challenged by numerous investigators and is no longer supported by the majority of fire ecologists.\n\nOne consequence of the fires in 2003 has been the increased density of invasive and non-native plant species that have quickly colonized burned areas, especially those that had already been burned in the previous 15 years. Because shrubs in these communities are adapted to a particular historical fire regime, altered fire regimes may change the selective pressures on plants and favor invasive and non-native species that are better able to exploit the novel post-fire conditions.\n\nThe Boise National Forest is a US national forest located north and east of the city of Boise, Idaho. Following several uncharacteristically large wildfires, an immediately negative impact on fish populations was observed, posing particular danger to small and isolated fish populations. In the long term, however, fire appears to rejuvenate fish habitats by causing hydraulic changes that increase flooding and lead to silt removal and the deposition of a favorable habitat substrate. This leads to larger post-fire populations of the fish that are able to recolonize these improved areas. But although fire generally appears favorable for fish populations in these ecosystems, the more intense effects of uncharacteristic wildfires, in combination with the fragmentation of populations by human barriers to dispersal such as weirs and dams, \"will\" pose a threat to fish populations.\n\nRestoration ecology is the name given to an attempt to reverse or mitigate some of the changes that humans have caused to an ecosystem. Controlled burning is one tool that is currently receiving considerable attention as a means of restoration and management. Applying fire to an ecosystem may create habitats for species that have been negatively impacted by fire suppression, or fire may be used as a way of controlling invasive species without resorting to herbicides or pesticides. However, there is debate as to what state managers should aim to restore their ecosystems to, especially as to whether \"natural\" means pre-human or pre-European. Native American use of fire, not natural fires, historically maintained the diversity of the savannas of North America. When, how, and where managers should use fire as a management tool is a subject of debate.\n\nA combination of heavy livestock grazing and fire-suppression has drastically altered the structure, composition, and diversity of the shortgrass prairie ecosystem on the Great Plains, allowing woody species to dominate many areas and promoting fire-intolerant invasive species. In semi-arid ecosystems where the decomposition of woody material is slow, fire is crucial for returning nutrients to the soil and allowing the grasslands to maintain their high productivity.\n\nAlthough fire can occur during the growing or the dormant seasons, managed fire during the dormant season is most effective at increasing the grass and forb cover, biodiversity and plant nutrient uptake in shortgrass prairies. Managers must also take into account, however, how invasive and non-native species respond to fire if they want to restore the integrity of a native ecosystem. For example, fire can only control the invasive spotted knapweed (\"Centaurea maculosa\") on the Michigan tallgrass prairie in the summer, because this is the time in the knapweed's life cycle that is most important to its reproductive growth.\n\nMixed conifer forests in the United States Sierra Nevada used to have fire return intervals that ranged from 5 years up to 300 years, depending on the local climate. Lower elevations had more frequent fire return intervals, whilst higher and wetter elevations saw much longer intervals between fires. Native Americans tended to set fires during fall and winter, and land at a higher elevation was generally occupied by Native Americans only during the summer.\n\nThe decline of habitat area and quality has caused many species populations to be red-listed by the International Union for Conservation of Nature. According to a study on forest management of Finnish boreal forests, improving the habitat quality of areas outside reserves can help in conservation efforts of endangered deadwood-dependent beetles. These beetles and various types of fungi both need dead trees in order to survive. Old growth forests can provide this particular habitat. However, most Fennoscandian boreal forested areas are used for timber and therefore are unprotected. The use of controlled burning and tree retention of a forested area with deadwood was studied and its effect on the endangered beetles. The study found that after the first year of management the number of species increased in abundance and richness compared to pre-fire treatment. The abundance of beetles continued to increase the following year in sites where tree retention was high and deadwood was abundant. The correlation between forest fire management and increased beetle populations shows a key to conserving these red-listed species.\n\nMuch of the old growth eucalypt forest in Australia is designated for conservation. Management of these forests is important because species like \"Eucalyptus grandis\" rely on fire to survive. There are a few eucalypt species that do not have a lignotuber, a root swelling structure that contains buds where new shoots can then sprout. During a fire a lignotuber is helpful in the reestablishment of the plant. Because some eucalypts do not have this particular mechanism, forest fire management can be helpful by creating rich soil, killing competitors, and allowing seeds to be released.\n\nFire policy in the United States involves the federal government, individual state governments, tribal governments, interest groups, and the general public. The new federal outlook on fire policy parallels advances in ecology and is moving towards the view that many ecosystems depend on disturbance for their diversity and for the proper maintenance of their natural processes. Although human safety is still the number one priority in fire management, new US government objectives include a long-term view of ecosystems. The newest policy allows managers to gauge the relative values of private property and resources in particular situations and to set their priorities accordingly.\n\nOne of the primary goals in fire management is to improve public education in order to suppress the \"Smokey Bear\" fire-suppression mentality and introduce the public to the benefits of regular natural fires.\n\n\n"}
{"id": "29742984", "url": "https://en.wikipedia.org/wiki?curid=29742984", "title": "Fossil hash", "text": "Fossil hash\n\nHash fossils are not actually one fossil, they are many fossils in the same rock. The term hash fossil describes the fossil formed when all the organic material in an environment falls to the ocean floor and fossilizes, hence the name \"hash\". When you look at a hash fossil, you actually are looking at a piece of ancient seabed. In hash fossils you can usually find the pieces of corals, crinoids, bryzoans, and brachiopods. Also you can rarely find a piece of a trilobite. Hash fossils are very common, and are most easily found in limestone.\n\n"}
{"id": "30863843", "url": "https://en.wikipedia.org/wiki?curid=30863843", "title": "G-parity", "text": "G-parity\n\nIn theoretical physics, G-parity is a multiplicative quantum number that results from the generalization of C-parity to multiplets of particles.\n\n\"C\"-parity applies only to neutral systems; in the pion triplet, only π has \"C\"-parity. On the other hand, strong interaction does not see electrical charge, so it cannot distinguish amongst π, π and π. We can generalize the \"C\"-parity so it applies to all charge states of a given multiplet: \nwhere \"η\" = ±1 are the eigenvalues of \"G\"-parity. The \"G\"-parity operator is defined as \n\nwhere formula_3 is the \"C\"-parity operator, and \"I\" is the operator associated with the 2nd component of the isospin \"vector\". \"G\"-parity is a combination of charge conjugation and a π rad (180°) rotation around the 2nd axis of isospin space. Given that charge conjugation and isospin are preserved by strong interactions, so is \"G\". Weak and electromagnetic interactions, though, are not invariant under \"G\"-parity. \n\nSince \"G\"-parity is applied on a whole multiplet, charge conjugation has to see the multiplet as a neutral entity. Thus, only multiplets with an average charge of 0 will be eigenstates of \"G\", that is \n\n(see Q, B, Y). \n\nIn general\nwhere \"η\" is a \"C\"-parity eigenvalue, and \"I\" is the isospin. For fermion-antifermion systems, we have\nwhere \"S\" is the total spin, \"L\" the total orbital angular momentum quantum number. For boson–antiboson systems we have \n\n\n"}
{"id": "19587932", "url": "https://en.wikipedia.org/wiki?curid=19587932", "title": "Gao Xing", "text": "Gao Xing\n\nGao Xing (; born 1974) is a Chinese amateur astronomer from Ürümqi, Xinjiang, China. He built Xingming Observatory (星明天文台) in 2006 and discovered Comet C/2008 C1 (Chen-Gao) on February 1, 2008 with Chen Tao from Jiangsu and Comet P/2009 L2 (Yang-Gao) on June 15, 2009 with Yang Rui from Hangzhou, Zhejiang and Comet C/2015 F5 (SWAN-Xingming) on April 4, 2015 with Guoyou Sun from Wenzhou, Zhejiang. China and hence won the Edgar Wilson Award for 2008. In the night on February 26, 2009, he discovered a nova in Sagittarius in the Galaxy's central part at night with his partner Sun Guoyou from Wenzhou. Gao reported his new discovery to the International Astronomical Union on May 29 and acquired the identification.In the night on October 3, 2010, he discovered a new Supernova in NGC5430 at night with his partner Sun Guoyou.He also discovered several SOHO comets and NEAT Asteroids. Currently, he is working as a physics teacher at the Urumqi No.1 High School. He also has a daughter.\n\n\n"}
{"id": "887836", "url": "https://en.wikipedia.org/wiki?curid=887836", "title": "Gene doping", "text": "Gene doping\n\nGene doping is the hypothetical non-therapeutic use of gene therapy by athletes in order to improve their performance in those sporting events which prohibit such applications of genetic modification technology, and for reasons other than the treatment of disease. , there is no evidence that gene doping has been used for athletic performance-enhancement in any sporting events. Gene doping would involve the use of gene transfer to increase or decrease gene expression and protein biosynthesis of a specific human protein; this could be done by directly injecting the gene carrier into the person, or by taking cells from the person, transfecting the cells, and administering the cells back to the person.\n\nThe historical development of interest in gene doping by athletes and concern about the risks of gene doping and how to detect it moved in parallel with the development of the field of gene therapy, especially with the publication in 1998 of work on a transgenic mouse overexpressing insulin-like growth factor 1 that was much stronger than normal mice, even in old age, preclinical studies published in 2002 of a way to deliver erythropoietin (EPO) via gene therapy, and publication in 2004 of the creation of a \"marathon mouse\" with much greater endurance than normal mice, created by delivering the gene expressing PPAR gamma to the mice. The scientists generating these publications were all contacted directly by athletes and coaches seeking access to the technology. The public became aware of that activity in 2006 when such efforts were part of the evidence presented in the trial of a German coach.\n\nScientists themselves, as well as bodies including the World Anti-Doping Agency (WADA), the International Olympic Committee, and the American Association for the Advancement of Science, started discussing the risk of gene doping in 2001, and by 2003 WADA had added gene doping to the list of banned doping practices, and shortly thereafter began funding research on methods to detect gene doping.\n\nGenetic enhancement includes manipulation of genes or gene transfer by healthy athletes for the purpose of physically improving their performance. Genetic enhancement includes gene doping and has potential for abuse among athletes, all while opening the door to political and ethical controversy.\n\nThe history of concern about the potential for gene doping follows the history of gene therapy, the medical use of genes to treat diseases, which was first clinically tested in the 1990s. Interest by the athletic community was especially spurred by the creation in a university lab of a \"mighty mouse\", created by administering a virus carrying the gene expressing insulin-like growth factor 1 to mice; the mice were stronger and remained strong even as they aged, without exercise. The lab had been seeking treatments for muscle wasting diseases, but when their work was made public, the lab was inundated with calls from athletes seeking the treatment, with one coach offering his whole team. The scientist told \"The New York Times\" in 2007: \"I was quite surprised, I must admit. People would try to entice me, saying things like, 'It'll help advance your research.' Some offered to pay me.\" He also told the \"Times\" that every time similar research is published he gets calls and that he explains that, even should the treatment became ready for use in people, which would take years, there would be serious risks, including death; he also said that even after he explains this, the athletes still want it.\n\nIn 1999, the field of gene therapy was set back when Jesse Gelsinger died in a gene therapy clinical trial, suffering a massive inflammatory reaction to the drug. This led regulatory authorities in the US and Europe to increase safety requirements in clinical trials even beyond the initial restrictions that had been put in place at the beginning of the biotechnology era to deal with the risks of recombinant DNA.\n\nIn June 2001, Theodore Friedmann, one of the pioneers of gene therapy, and Johann Olav Koss an Olympic gold medallist in speed skating, published a paper that was the first public warning about gene doping. Also in June 2001, a Gene Therapy Working Group, convened by the Medical Commission of the International Olympic Committee noted that \"we are aware that there is the potential for abuse of gene therapy medicines and we shall begin to establish procedures and state-of-the-art testing methods for identifying athletes who might misuse such technology\".\n\nResearch was published in 2002 about a preclinical gene therapy called Repoxygen, which delivered the gene encoding erythropoietin (EPO) as a potential treatment for anemia. The scientists from that company also received calls from athletes and coaches. In that same year the World Anti-Doping Agency held its first meeting to discuss the risk of gene doping, and the US The President's Council on Bioethics discussed gene doping in the context of human enhancement at several sessions.\n\nIn 2003, the field of gene therapy took a step forward and a step back; first gene therapy drug was approved, Gendicine, which was approved in China for the treatment of certain cancers, but children in France who had seemingly been effective treated with gene therapy for severe combined immunodeficiency (non-human) began developing leukemia. In 2003 the BALCO scandal became public, in which chemists, trainers and athletes conspired to evade doping controls with new and undetectable doping substances. In 2003 the World Doping Agency proactively added gene doping to the list of banned doping practices. Also in 2003, a symposium convened by the American Association for the Advancement of Science focused on the issue.\n\nResearch published in 2004 showing that mice given gene therapy coding for a protein called PPAR gamma had about double the endurance of untreated mice and were dubbed \"marathon mice\"; those scientists received calls from athletes and coaches. Also in 2004 the World Anti-Doping Agency began to fund research to detect gene doping, and formed a permanent expert panel to advise it on risks and to guide the funding.\n\nIn 2006 interest from athletes in gene doping received widespread media coverage due its mention during the trial of a German coach who was accused and found guilty of giving his athletes performance enhancing drugs without their knowledge; an email in which the coach attempted to obtain Repoxygen was read in open court by a prosecutor. This was the first public disclosure that athletes were interested in gene doping.\n\nIn 2011 the second gene therapy drug was approved; Neovasculgen, which delivers the gene encoding VEGF, was approved in Russia to treat peripheral artery disease.\n\nIn 2012 Glybera, a treatment for a rare inherited disorder, became the first treatment to be approved for clinical use in either Europe or the United States.\n\nAs the field of gene therapy has developed, the risk of gene doping becoming a reality has increased with it.\n\nThere are numerous genes of interest as agents for gene doping. They include erythropoietin, insulin-like growth factor 1, human growth hormone, myostatin, vascular endothelial growth factor, fibroblast growth factor, endorphin, enkephalin and alpha-actinin-3.\n\nThe risks of gene doping would be similar to those of gene therapy: immune reaction to the native protein leading to the equivalent of an genetic disease, massive inflammatory response, cancer, and death, and in all cases, these risks would be undertaken for short-term gain as opposed to treating a serious disease.\n\nAlpha-actinin-3 is found only in skeletal muscle in humans, and has been identified in several genetic studies as having a different polymorphism in world-class athletes compared with normal people. One form that causes the gene to make more protein is found in sprinters and is related to increased power; another form that causes the gene to make less protein is found in endurance athletes. Gene doping agents could be designed with either polymorphism, or for endurance athletes, some DNA construct that interfered with expression like a small interfering RNA.\n\nMyostatin is a protein responsible for inhibiting muscle differentiation and growth. Removing the myostatin gene or otherwise limiting its expression leads to an increase in muscle size and power. This has been demonstrated in knockout mice lacking the gene that were dubbed \"Schwarzenegger mice\". Humans born with defective genes can also serve as \"knockout models\"; a German boy with a mutation in both copies of the myostatin gene was born with well-developed muscles. The advanced muscle growth continued after birth, and the boy could lift weights of 3 kg at the age of 4. In work published in 2009, scientists administered follistatin via gene therapy to the quadriceps of non-human primates, resulting in local muscle growth similar to the mice.\n\nErythropoietin is a glycoprotein that acts as a hormone, controlling red blood cell production. Athletes have injected the EPO protein as a performance-enhancing substance for many years (blood doping). When the additional EPO increases the production of red blood cells in circulation, this increases the amount of oxygen available to muscle, enhancing an athlete's endurance. Recent studies suggest it may be possible to introduce another EPO gene into an animal in order to increase EPO production endogenously. EPO genes have been successfully inserted into mice and monkeys, and were found to increase hematocrits by as much as 80 percent in those animals. However, the endogenous and transgene derived EPO elicited autoimmune responses in some animals in the form of severe anemia.\n\nInsulin-like growth factor 1 is a protein involved in the mediation of the growth hormone. Administration of IGF-1 to mice has resulted in more muscle growth and quicker muscle and nerve regeneration. If athletes were to use this the sustained production of IGF-1 could cause heart disease and cancer.\n\nModulating the levels of proteins that affect psychology are also potential goals for gene doping; for example pain perception depends on endorphins and enkephalins, response to stress depends on BDNF, and an increase in synthesis of monamines could improve the mood of athletes. Preproenkephalin has been administered via gene therapy using a replication-deficient herpes simplex virus, which targets nerves, to mice with results good enough to justify a Phase I clinical trial in people with terminal cancer with uncontrolled pain. Adopting that approach for athletes would be problematic since the pain deadening would likely be permanent.\n\nVEGF has been tested in clinical trials to increase blood flow and has been considered as a potential gene doping agent; however long term follow up of the clinical trial subjects showed poor results. The same is true of fibroblast growth factor. Glucagon-like peptide-1 increases the amount of glucose in the liver and has been administered via gene therapy to the livers of mouse models of diabetes and was shown to increase gluconeogenesis' for athletes this would make more energy available and reduce the buildup of lactic acid.\n\nThe World Anti-Doping Agency (WADA) is the main regulatory organization looking into the issue of the detection of gene doping. Both direct and indirect testing methods are being researched by the organization. Directly detecting the use of gene therapy usually requires the discovery of recombinant proteins or gene insertion vectors, while most indirect methods involve examining the athlete in an attempt to detect bodily changes or structural differences between endogenous and recombinant proteins.\n\nIndirect methods are by nature more subjective, as it becomes very difficult to determine which anomalies are proof of gene doping, and which are simply natural, though unusual, biological properties. For example, Eero Mäntyranta, an Olympic cross country skier, had a mutation which made his body produce abnormally high amounts of red blood cells. It would be very difficult to determine whether or not Mäntyranta's red blood cell levels were due to an innate genetic advantage, or an artificial one.\n\nA 2016 review found that about 120 DNA polymorphisms had been identified in the literature related to some aspect of athletic performance, 77 related to endurance and 43 related to power. 11 had been replicated in three or more studies and six were identified in genome-wide association studies, but 29 had not been replicated in at least one study.\n\nThe 11 replicated markers were:\n\nThe six GWAS markers were:\n\nThe World Anti-Doping Agency (WADA) determined that non therapeutic form of genetic manipulation for enhancement of athletic performance is not allowed in sport. The WADA code implemented guidelines to determine if said technology should be prohibited in sport. If two of the three conditions are met, then the technology is prohibited in sport; harmful to one's health, performance enhancing, and/or against the \"spirit of sport\". The high risks associated with gene therapy can be outweighed by the potential save the lives of individuals with diseases. According to Alain Fischer, who was involved in clinical trials of gene therapy in children with severe combined immunodeficiency, \"Only people who are dying would have reasonable grounds for using it. Using gene therapy for doping is ethically unacceptable and scientifically stupid.\" As seen with past cases, including the steroid tetrahydrogestrinone THG, athletes may choose to incorporate risky genetic technologies into their training regimes.\n\nThe mainstream perspective is that gene doping is dangerous and unethical, as is any application of a therapeutic intervention for non-therapeutic or enhancing purposes, and that it compromises the ethical foundation of medicine and the spirit of sport. Others, who support human enhancement on broader grounds, or who see a false dichotomy between \"natural\" and \"artificial\" or a denial of the role of technology in improving athletic performance, do not oppose or support gene doping.\n\n"}
{"id": "50655274", "url": "https://en.wikipedia.org/wiki?curid=50655274", "title": "Genius by Stephen Hawking", "text": "Genius by Stephen Hawking\n\nGenius by Stephen Hawking is a television series aired on PBS hosted by Stephen Hawking. It premiered on May 18, 2016 and ended on June 1, 2016, with only one season being produced before Hawking's death in 2018.\n\n\"Genius by Stephen Hawking\" is a series that first aired on PBS on Wednesdays, from May 18 to June 1, 2016. Professor Stephen Hawking challenges volunteers to think like geniuses and solve some of humanity's most enduring questions. \"What, Why, Where, are We\" (as in humanity) are covered, as well as \"Are We Alone\" and \"Can We Time Travel\". Generally three volunteers with these questions are followed as they try to find the answers to their questions. The idea is to teach the volunteers and watchers how to think like a genius. All are G-rated TV hour episodes originally aired in pairs.\n\n"}
{"id": "14449116", "url": "https://en.wikipedia.org/wiki?curid=14449116", "title": "History of timekeeping devices", "text": "History of timekeeping devices\n\nFor thousands of years, devices have been used to measure and keep track of time. The current sexagesimal system of time measurement dates to approximately 2000  from the Sumerians.\n\nThe Egyptians divided the day into two 12-hour periods, and used large obelisks to track the movement of the sun. They also developed water clocks, which were probably first used in the Precinct of Amun-Re, and later outside Egypt as well; they were employed frequently by the Ancient Greeks, who called them \"clepsydrae\". The Zhou dynasty is believed to have used the outflow water clock around the same time, devices which were introduced from Mesopotamia as early as 2000.\n\nOther ancient timekeeping devices include the candle clock, used in ancient China, ancient Japan, England and Mesopotamia; the timestick, widely used in India and Tibet, as well as some parts of Europe; and the hourglass, which functioned similarly to a water clock. The sundial, another early clock, relies on shadows to provide a good estimate of the hour on a sunny day. It is not so useful in cloudy weather or at night and requires recalibration as the seasons change (if the gnomon was not aligned with the Earth's axis).\n\nThe earliest known clock with a water-powered escapement mechanism, which transferred rotational energy into intermittent motions, dates back to 3rd century in ancient Greece; Chinese engineers later invented clocks incorporating mercury-powered escapement mechanisms in the 10th century, followed by Iranian engineers inventing water clocks driven by gears and weights in the 11th century.\n\nThe first mechanical clocks, employing the verge escapement mechanism with a foliot or balance wheel timekeeper, were invented in Europe at around the start of the 14th century, and became the standard timekeeping device until the pendulum clock was invented in 1656. The invention of the mainspring in the early 15th century allowed portable clocks to be built, evolving into the first pocketwatches by the 17th century, but these were not very accurate until the balance spring was added to the balance wheel in the mid 17th century.\n\nThe pendulum clock remained the most accurate timekeeper until the 1930s, when quartz oscillators were invented, followed by atomic clocks after World War 2. Although initially limited to laboratories, the development of microelectronics in the 1960s made quartz clocks both compact and cheap to produce, and by the 1980s they became the world's dominant timekeeping technology in both clocks and wristwatches.\n\nAtomic clocks are far more accurate than any previous timekeeping device, and are used to calibrate other clocks and to calculate the International Atomic Time; a standardized civil system, Coordinated Universal Time, is based on atomic time.\n\nMany ancient civilizations observed astronomical bodies, often the Sun and Moon, to determine times, dates, and seasons. The first calendars may have been created during the last glacial period, by hunter-gatherers who employed tools such as sticks and bones to track the phases of the moon or the seasons. Stone circles, such as England's Stonehenge, were built in various parts of the world, especially in Prehistoric Europe, and are thought to have been used to time and predict seasonal and annual events such as equinoxes or solstices. As those megalithic civilizations left no recorded history, little is known of their calendars or timekeeping methods. Methods of sexagesimal timekeeping, now common in both Western and Eastern societies, are first attested nearly 4,000 years ago in Mesopotamia and Egypt. Mesoamericans similarly modified their usual vigesimal counting system when dealing with calendars to produce a 360-day year.\n\nThe oldest known sundial is from Egypt; it dates back to around 1500 (19th Dynasty), and was discovered in the Valley of the Kings in 2013. Sundials have their origin in shadow clocks, which were the first devices used for measuring the parts of a day. Ancient Egyptian obelisks, constructed about 3500, are also among the earliest shadow clocks.\nEgyptian shadow clocks divided daytime into 12 parts with each part further divided into more precise parts. One type of shadow clock consisted of a long stem with five variable marks and an elevated crossbar which cast a shadow over those marks. It was positioned eastward in the morning, and was turned west at noon. Obelisks functioned in much the same manner: the shadow cast on the markers around it allowed the Egyptians to calculate the time. The obelisk also indicated whether it was morning or afternoon, as well as the summer and winter solstices. A third shadow clock, developed c. 1500, was similar in shape to a bent T-square. It measured the passage of time by the shadow cast by its crossbar on a non-linear rule. The \"T\" was oriented eastward in the mornings, and turned around at noon, so that it could cast its shadow in the opposite direction.\n\nAlthough accurate, shadow clocks relied on the sun, and so were useless at night and in cloudy weather. The Egyptians therefore developed a number of alternative timekeeping instruments, including water clocks, and a system for tracking star movements. The oldest description of a water clock is from the tomb inscription of the 16th-century Egyptian court official Amenemhet, identifying him as its inventor. There were several types of water clocks, some more elaborate than others. One type consisted of a bowl with small holes in its bottom, which was floated on water and allowed to fill at a near-constant rate; markings on the side of the bowl indicated elapsed time, as the surface of the water reached them. The oldest-known waterclock was found in the tomb of pharaoh Amenhotep I (1525–1504), suggesting that they were first used in ancient Egypt. Another Egyptian method of determining the time during the night was using plumb-lines called merkhets. In use since at least 600, two of these instruments were aligned with Polaris, the north pole star, to create a north–south meridian. The time was accurately measured by observing certain stars as they crossed the line created with the \"merkhets\".\n\nWater clocks, or clepsydrae, were commonly used in Ancient Greece following their introduction by Plato, who also invented a water-based alarm clock. One account of Plato's alarm clock describes it as depending on the nightly overflow of a vessel containing lead balls, which floated in a columnar vat. The vat held a steadily increasing amount of water, supplied by a cistern. By morning, the vessel would have floated high enough to tip over, causing the lead balls to cascade onto a copper platter. The resultant clangor would then awaken Plato's students at the Academy. Another possibility is that it comprised two jars, connected by a siphon. Water emptied until it reached the siphon, which transported the water to the other jar. There, the rising water would force air through a whistle, sounding an alarm. The Greeks and Chaldeans regularly maintained timekeeping records as an essential part of their astronomical observations.\n\nGreek astronomer, Andronicus of Cyrrhus, supervised the construction of the Tower of the Winds in Athens in the 1st century.\n\nIn Greek tradition, clepsydrae were used in court; later, the Romans adopted this practice, as well. There are several mentions of this in historical records and literature of the era; for example, in \"Theaetetus\", Plato says that \"Those men, on the other hand, always speak in haste, for the flowing water urges them on\". Another mention occurs in Lucius Apuleius' \"The Golden Ass\": \"The Clerk of the Court began bawling again, this time summoning the chief witness for the prosecution to appear. Up stepped an old man, whom I did not know. He was invited to speak for as long as there was water in the clock; this was a hollow globe into which water was poured through a funnel in the neck, and from which it gradually escaped through fine perforations at the base\". The clock in Apuleius's account was one of several types of water clock used. Another consisted of a bowl with a hole in its centre, which was floated on water. Time was kept by observing how long the bowl took to fill with water.\n\nAlthough clepsydrae were more useful than sundials—they could be used indoors, during the night, and also when the sky was cloudy—they were not as accurate; the Greeks, therefore, sought a way to improve their water clocks. Although still not as accurate as sundials, Greek water clocks became more accurate around 325, and they were adapted to have a face with an hour hand, making the reading of the clock more precise and convenient. One of the more common problems in most types of clepsydrae was caused by water pressure: when the container holding the water was full, the increased pressure caused the water to flow more rapidly. This problem was addressed by Greek and Roman horologists beginning in 100, and improvements continued to be made in the following centuries. To counteract the increased water flow, the clock's water containers—usually bowls or jugs—were given a conical shape; positioned with the wide end up, a greater amount of water had to flow out in order to drop the same distance as when the water was lower in the cone. Along with this improvement, clocks were constructed more elegantly in this period, with hours marked by gongs, doors opening to miniature figurines, bells, or moving mechanisms. There were some remaining problems, however, which were never solved, such as the effect of temperature. Water flows more slowly when cold, or may even freeze.\n\nBetween 270 and 500, Hellenistic (Ctesibius, Hero of Alexandria, Archimedes) and Roman horologists and astronomers began developing more elaborate mechanized water clocks. The added complexity was aimed at regulating the flow and at providing fancier displays of the passage of time. For example, some water clocks rang bells and gongs, while others opened doors and windows to show figurines of people, or moved pointers, and dials. Some even displayed astrological models of the universe.\n\nAlthough the Greeks and Romans did much to advance water clock technology, they still continued to use shadow clocks. The mathematician and astronomer Theodosius of Bithynia, for example, is said to have invented a universal sundial that was accurate anywhere on Earth, though little is known about it. Others wrote of the sundial in the mathematics and literature of the period. Marcus Vitruvius Pollio, the Roman author of \"De Architectura\", wrote on the mathematics of gnomons, or sundial blades. During the reign of Emperor Augustus, the Romans constructed the largest sundial ever built, the Solarium Augusti. Its gnomon was an obelisk from Heliopolis. Similarly, the obelisk from Campus Martius was used as the gnomon for Augustus's zodiacal sundial. Pliny the Elder records that the first sundial in Rome arrived in 264, looted from Catania, Sicily; according to him, it gave the incorrect time until the markings and angle appropriate for Rome's latitude were used—a century later.\n\nAccording to Callisthenes, the Persians were using water clocks in 328 to ensure a just and exact distribution of water from qanats to their shareholders for agricultural irrigation. The use of water clocks in Iran, especially in Zeebad, dates back to 500. Later they were also used to determine the exact holy days of pre-Islamic religions, such as the \"Nowruz\", \"Chelah\", or \"Yaldā\" – the shortest, longest, and equal-length days and nights of the years. The water clocks used in Iran were one of the most practical ancient tools for timing the yearly calendar.\n\nWater clocks, or \"Fenjaan\", in Persia reached a level of accuracy comparable to today's standards of timekeeping. The fenjaan was the most accurate and commonly used timekeeping device for calculating the amount or the time that a farmer must take water from a qanat or well for irrigation of the farms, until it was replaced by more accurate current clock. Persian water clocks were a practical and useful tool for the qanat's shareholders to calculate the length of time they could divert water to their farm. The qanat was the only water source for agriculture and irrigation so a just and fair water distribution was very important. Therefore, a very fair and clever old person was elected to be the manager of the water clock, and at least two full-time managers were needed to control and observe the number of fenjaans and announce the exact time during the days and nights.\n\nThe fenjaan was a big pot full of water and a bowl with small hole in the center. When the bowl become full of water, it would sink into the pot, and the manager would empty the bowl and again put it on the top of the water in the pot. He would record the number of times the bowl sank by putting small stones into a jar.\n\nThe place where the clock was situated, and its managers, were collectively known as \"khaneh fenjaan\". Usually this would be the top floor of a public-house, with west- and east-facing windows to show the time of sunset and sunrise. There was also another time-keeping tool named a \"staryab\" or astrolabe, but it was mostly used for superstitious beliefs and was not practical for use as a farmers' calendar. The Zeebad Gonabad water clock was in use until 1965 when it was substituted by modern clocks.\n\nJoseph Needham speculated that the introduction of the outflow clepsydra to China, perhaps from Mesopotamia, occurred as far back as the 2nd millennium, during the Shang Dynasty, and at the latest by the 1st millennium. By the beginning of the Han Dynasty, in 202, the outflow clepsydra was gradually replaced by the inflow clepsydra, which featured an indicator rod on a float. To compensate for the falling pressure head in the reservoir, which slowed timekeeping as the vessel filled, Zhang Heng added an extra tank between the reservoir and the inflow vessel. Around 550 AD, Yin Gui was the first in China to write of the overflow or constant-level tank added to the series, which was later described in detail by the inventor Shen Kuo. Around 610, this design was trumped by two Sui Dynasty inventors, Geng Xun and Yuwen Kai, who were the first to create the balance clepsydra, with standard positions for the steelyard balance. Joseph Needham states that:\nThe term 'clock' encompasses a wide spectrum of devices, ranging from wristwatches to the Clock of the Long Now. The English word \"clock\" is said to derive from the Middle English \"clokke\", Old North French \"cloque\", or Middle Dutch \"clocke\", all of which mean \"bell\", and are derived from the Medieval Latin \"clocca\", also meaning bell. Indeed, bells were used to mark the passage of time; they marked the passage of the hours at sea and in abbeys.\n\nThroughout history, clocks have had a variety of power sources, including gravity, springs, and electricity. Mechanical clocks became widespread in the 14th century, when they were used in medieval monasteries to keep the regulated schedule of prayers. The clock continued to be improved, with the first pendulum clock being designed and built in the 17th century.\n\nThe earliest mention of candle clocks comes from a Chinese poem, written in 520 by You Jianfu. According to the poem, the graduated candle was a means of determining time at night. Similar candles were used in Japan until the early 10th century.\n\nThe candle clock most commonly mentioned and written of is attributed to King Alfred the Great. It consisted of six candles made from 72 pennyweights of wax, each high, and of uniform thickness, marked every inch (2.54 cm). As these candles burned for about four hours, each mark represented 20 minutes. Once lit, the candles were placed in wooden framed glass boxes, to prevent the flame from extinguishing.\n\nThe most sophisticated candle clocks of their time were those of Al-Jazari in 1206. One of his candle clocks included a dial to display the time and, for the first time, employed a bayonet fitting, a fastening mechanism still used in modern times. Donald Routledge Hill described Al-Jazari's candle clocks as follows:\n\nA variation on this theme were oil-lamp clocks. These early timekeeping devices consisted of a graduated glass reservoir to hold oil — usually whale oil, which burned cleanly and evenly — supplying the fuel for a built-in lamp. As the level in the reservoir dropped, it provided a rough measure of the passage of time.\n\nIn addition to water, mechanical, and candle clocks, incense clocks were used in the Far East, and were fashioned in several different forms. Incense clocks were first used in China around the 6th century; in Japan, one still exists in the Shōsōin, although its characters are not Chinese, but Devanagari. Due to their frequent use of Devanagari characters, suggestive of their use in Buddhist ceremonies, Edward H. Schafer speculated that incense clocks were invented in India. Although similar to the candle clock, incense clocks burned evenly and without a flame; therefore, they were more accurate and safer for indoor use.\n\nSeveral types of incense clock have been found, the most common forms include the incense stick and incense seal. An incense stick clock was an incense stick with calibrations; most were elaborate, sometimes having threads, with weights attached, at even intervals. The weights would drop onto a platter or gong below, signifying that a certain amount of time had elapsed. Some incense clocks were held in elegant trays; open-bottomed trays were also used, to allow the weights to be used together with the decorative tray. Sticks of incense with different scents were also used, so that the hours were marked by a change in fragrance. The incense sticks could be straight or spiraled; the spiraled ones were longer, and were therefore intended for long periods of use, and often hung from the roofs of homes and temples. In Japan, a geisha was paid for the number of \"senkodokei\" (incense sticks) that had been consumed while she was present, a practice which continued until 1924.\n\nIncense seal clocks were used for similar occasions and events as the stick clock; while religious purposes were of primary importance, these clocks were also popular at social gatherings, and were used by Chinese scholars and intellectuals. The seal was a wooden or stone disk with one or more grooves etched in it into which incense was placed. These clocks were common in China, but were produced in fewer numbers in Japan. To signal the passage of a specific amount of time, small pieces of fragrant woods, resins, or different scented incenses could be placed on the incense powder trails. Different powdered incense clocks used different formulations of incense, depending on how the clock was laid out. The length of the trail of incense, directly related to the size of the seal, was the primary factor in determining how long the clock would last; all burned for long periods of time, ranging between 12 hours and a month.\n\nWhile early incense seals were made of wood or stone, the Chinese gradually introduced disks made of metal, most likely beginning during the Song dynasty. This allowed craftsmen to more easily create both large and small seals, as well as design and decorate them more aesthetically. Another advantage was the ability to vary the paths of the grooves, to allow for the changing length of the days in the year. As smaller seals became more readily available, the clocks grew in popularity among the Chinese, and were often given as gifts. Incense seal clocks are often sought by modern-day clock collectors; however, few remain that have not already been purchased or been placed on display at museums or temples.\n\nSundials had been used for timekeeping since Ancient Egypt. Ancient dials were nodus-based with straight hour-lines that indicated unequal hours—also called temporary hours—that varied with the seasons. Every day was divided into 12 equal segments regardless of the time of year; thus, hours were shorter in winter and longer in summer. The sundial was further developed by Muslim astronomers. The idea of using hours of equal length throughout the year was the innovation of Abu'l-Hasan Ibn al-Shatir in 1371, based on earlier developments in trigonometry by Muhammad ibn Jābir al-Harrānī al-Battānī (Albategni). Ibn al-Shatir was aware that \"using a gnomon that is parallel to the Earth's axis will produce sundials whose hour lines indicate equal hours on any day of the year\". His sundial is the oldest polar-axis sundial still in existence. The concept appeared in Western sundials starting in 1446.\n\nFollowing the acceptance of heliocentrism and equal hours, as well as advances in trigonometry, sundials appeared in their present form during the Renaissance, when they were built in large numbers. In 1524, the French astronomer Oronce Finé constructed an ivory sundial, which still exists; later, in 1570, the Italian astronomer Giovanni Padovani published a treatise including instructions for the manufacture and laying out of mural (vertical) and horizontal sundials. Similarly, Giuseppe Biancani's \"Constructio instrumenti ad horologia solaria\" (c. 1620) discusses how to construct sundials.\n\nSince the hourglass was one of the few reliable methods of measuring time at sea, it is speculated that it was used on board ships as far back as the 11th century, when it would have complemented the magnetic compass as an aid to navigation. However, the earliest unambiguous evidence of their use appears in the painting \"Allegory of Good Government\", by Ambrogio Lorenzetti, from 1338. From the 15th century onwards, hourglasses were used in a wide range of applications at sea, in churches, in industry, and in cooking; they were the first dependable, reusable, reasonably accurate, and easily constructed time-measurement devices. The hourglass also took on symbolic meanings, such as that of death, temperance, opportunity, and Father Time, usually represented as a bearded, old man. Though also used in China, the hourglass's history there is unknown. The Portuguese navigator Ferdinand Magellan used 18 hourglasses on each ship during his circumnavigation of the globe in 1522.\n\nThe earliest instance of a liquid-driven escapement was described by the Greek engineer Philo of Byzantium (fl. 3rd century) in his technical treatise \"Pneumatics\" (chapter 31) where he likens the escapement mechanism of a washstand automaton with those as employed in (water) clocks. Another early clock to use escapements was built during the 7th century in Chang'an, by Tantric monk and mathematician, Yi Xing, and government official Liang Lingzan. An astronomical instrument that served as a clock, it was discussed in a contemporary text as follows:\n[It] was made in the image of the round heavens and on it were shown the lunar mansions in their order, the equator and the degrees of the heavenly circumference. Water, flowing into scoops, turned a wheel automatically, rotating it one complete revolution in one day and night. Besides this, there were two rings fitted around the celestial sphere outside, having the sun and moon threaded on them, and these were made to move in circling orbit ... And they made a wooden casing the surface of which represented the horizon, since the instrument was half sunk in it. It permitted the exact determinations of the time of dawns and dusks, full and new moons, tarrying and hurrying. Moreover, there were two wooden jacks standing on the horizon surface, having one a bell and the other a drum in front of it, the bell being struck automatically to indicate the hours, and the drum being beaten automatically to indicate the quarters. All these motions were brought about by machinery within the casing, each depending on wheels and shafts, hooks, pins and interlocking rods, stopping devices and locks checking mutually.\n\nSince Yi Xing's clock was a water clock, it was affected by temperature variations. That problem was solved in 976 by Zhang Sixun by replacing the water with mercury, which remains liquid down to . Zhang implemented the changes into his clock tower, which was about tall, with escapements to keep the clock turning and bells to signal every quarter-hour. Another noteworthy clock, the elaborate Cosmic Engine, was built by Su Song, in 1088. It was about the size of Zhang's tower, but had an automatically rotating armillary sphere—also called a celestial globe—from which the positions of the stars could be observed. It also featured five panels with mannequins ringing gongs or bells, and tablets showing the time of day, or other special times. Furthermore, it featured the first known endless power-transmitting chain drive in horology. Originally built in the capital of Kaifeng, it was dismantled by the Jin army and sent to the capital of Yanjing (now Beijing), where they were unable to put it back together. As a result, Su Song's son Su Xie was ordered to build a replica.\nThe clock towers built by Zhang Sixun and Su Song, in the 10th and 11th centuries, respectively, also incorporated a striking clock mechanism, the use of clock jacks to sound the hours. A striking clock outside of China was the Jayrun Water Clock, at the Umayyad Mosque in Damascus, Syria, which struck once every hour. It was constructed by Muhammad al-Sa'ati in the 12th century, and later described by his son Ridwan ibn al-Sa'ati, in his \"On the Construction of Clocks and their Use\" (1203), when repairing the clock. In 1235, an early monumental water-powered alarm clock that \"announced the appointed hours of prayer and the time both by day and by night\" was completed in the entrance hall of the Mustansiriya Madrasah in Baghdad.\n\nThe first geared clock was invented in the 11th century by the Arab engineer Ibn Khalaf al-Muradi in Islamic Iberia; it was a water clock that employed a complex gear train mechanism, including both segmental and epicyclic gearing, capable of transmitting high torque. The clock was unrivalled in its use of sophisticated complex gearing, until the mechanical clocks of the mid-14th century. Al-Muradi's clock also employed the use of mercury in its hydraulic linkages, which could function mechanical automata. Al-Muradi's work was known to scholars working under Alfonso X of Castile, hence the mechanism may have played a role in the development of the European mechanical clocks. Other monumental water clocks constructed by medieval Muslim engineers also employed complex gear trains and arrays of automata. Like the earlier Greeks and Chinese, Arab engineers at the time also developed a liquid-driven escapement mechanism which they employed in some of their water clocks. Heavy floats were used as weights and a constant-head system was used as an escapement mechanism, which was present in the hydraulic controls they used to make heavy floats descend at a slow and steady rate.\n\nA mercury clock, described in the \"Libros del saber de Astronomia\", a Spanish work from 1277 consisting of translations and paraphrases of Arabic works, is sometimes quoted as evidence for Muslim knowledge of a mechanical clock. However, the device was actually a compartmented cylindrical water clock, which the Jewish author of the relevant section, Rabbi Isaac, constructed using principles described by a philosopher named \"Iran\", identified with Heron of Alexandria (fl. 1st century AD), on how heavy objects may be lifted.\n\nClock towers in Western Europe in the Middle Ages were also sometimes striking clocks. The most famous original still standing is possibly St Mark's Clock on the top of St Mark's Clocktower in St Mark's Square in Venice, assembled in 1493 by the clockmaker Gian Carlo Rainieri from Reggio Emilia. In 1497, Simone Campanato moulded the great bell on which every definite time-lapse is beaten by two mechanical bronze statues (h. 2,60 m.) called \"Due Mori\" (\"Two Moors\"), handling a hammer. Possibly earlier (1490) is the Prague Astronomical Clock by clockmaster Jan Růže (also called Hanuš) – according to another source this device was assembled as early as 1410 by clockmaker Mikuláš of Kadaň and mathematician Jan Šindel. The allegorical parade of animated sculptures rings on the hour every day.\n\nDuring the 11th century in the Song Dynasty, the Chinese astronomer, horologist and mechanical engineer Su Song created a water-driven astronomical clock for his clock tower of Kaifeng City. It incorporated an escapement mechanism as well as the earliest known endless power-transmitting chain drive, which drove the armillary sphere.\n\nContemporary Muslim astronomers also constructed a variety of highly accurate astronomical clocks for use in their mosques and observatories, such as the water-powered astronomical clock by Al-Jazari in 1206, and the astrolabic clock by Ibn al-Shatir in the early 14th century. The most sophisticated timekeeping astrolabes were the geared astrolabe mechanisms designed by Abū Rayhān Bīrūnī in the 11th century and by Muhammad ibn Abi Bakr in the 13th century. These devices functioned as timekeeping devices and also as calendars.\nA sophisticated water-powered astronomical clock was built by Al-Jazari in 1206. This castle clock was a complex device that was about high, and had multiple functions alongside timekeeping. It included a display of the zodiac and the solar and lunar paths, and a pointer in the shape of the crescent moon which travelled across the top of a gateway, moved by a hidden cart and causing doors to open, each revealing a mannequin, every hour. It was possible to reset the length of day and night in order to account for the changing lengths of day and night throughout the year. This clock also featured a number of automata including falcons and musicians who automatically played music when moved by levers operated by a hidden camshaft attached to a water wheel.\n\nThe earliest medieval European clockmakers were Catholic monks. Medieval religious institutions required clocks because they regulated daily prayer- and work-schedules strictly, using various types of time-telling and recording devices, such as water clocks, sundials and marked candles, probably in combination. When mechanical clocks came into use, they were often wound at least twice a day to ensure accuracy. Monasteries broadcast important times and durations with bells, rung either by hand or by a mechanical device, such as by a falling weight or by rotating beater.\n\nAlthough the mortuary inscription of Pacificus, archdeacon of Verona, records that he constructed a night clock (\"horologium nocturnum\") as early as 850, his clock has been identified as being an observation tube used to locate stars with an accompanying book of astronomical observations, rather than a mechanical or water clock, an interpretation supported by illustrations from medieval manuscripts.\n\nThe religious necessities and technical skill of the medieval monks were crucial factors in the development of clocks, as the historian Thomas Woods writes:\nThe appearance of clocks in writings of the 11th century implies that they were well known in Europe in that period. In the early 14th-century, the Florentine poet Dante Alighieri referred to a clock in his \"Paradiso\"; the first known literary reference to a clock that struck the hours. Giovanni da Dondi, Professor of Astronomy at Padua, presented the earliest detailed description of clockwork in his 1364 treatise \"Il Tractatus Astrarii\". This has inspired several modern replicas, including some in London's Science Museum and the Smithsonian Institution. Other notable examples from this period were built in Milan (1335), Strasbourg (1354), Lund (1380), Rouen (1389), and Prague (1462).\n\nSalisbury cathedral clock, dating from about 1386, is one of the oldest working clocks in the world, and may be the oldest. It still has most of its original parts, although its original verge and foliot timekeeping mechanism is lost, having been converted to a pendulum, which was replaced by a replica verge in 1956. It has no dial, as its purpose was to strike a bell at precise times. The wheels and gears are mounted in an open, box-like iron frame, measuring about square. The framework is held together with metal dowels and pegs. Two large stones, hanging from pulleys, supply the power. As the weights fall, ropes unwind from the wooden barrels. One barrel drives the main wheel, which is regulated by the escapement, and the other drives the striking mechanism and the air brake.\n\nNote also Peter Lightfoot's Wells Cathedral clock, constructed c. 1390. The dial represents a geocentric view of the universe, with the Sun and Moon revolving around a central fixed Earth. It is unique in having its original medieval face, showing a philosophical model of the pre-Copernican universe. Above the clock is a set of figures, which hit the bells, and a set of jousting knights who revolve around a track every 15 minutes. The clock was converted to pendulum-and-anchor escapement in the 17th century, and was installed in London's Science Museum in 1884, where it continues to operate. Similar astronomical clocks, or \"horologes\", survive at Exeter, Ottery St Mary, and Wimborne Minster.\nOne clock that has not survived is that of the Abbey of St Albans, built by the 14th-century abbot Richard of Wallingford. It may have been destroyed during Henry VIII's Dissolution of the Monasteries, but the abbot's notes on its design have allowed a full-scale reconstruction. As well as keeping time, the astronomical clock could accurately predict lunar eclipses, and may have shown the Sun, Moon (age, phase, and node), stars and planets, as well as a wheel of fortune, and an indicator of the state of the tide at London Bridge. According to Thomas Woods, \"a clock that equaled it in technological sophistication did not appear for at least two centuries\". Giovanni de Dondi was another early mechanical clockmaker whose clock did not survive, but his work has been replicated based on the designs. De Dondi's clock was a seven-faced construction with 107 moving parts, showing the positions of the Sun, Moon, and five planets, as well as religious feast days. Around this period, mechanical clocks were introduced into abbeys and monasteries to mark important events and times, gradually replacing water clocks which had served the same purpose.\n\nDuring the Middle Ages, clocks primarily served religious purposes; the first employed for secular timekeeping emerged around the 15th century. In Dublin, the official measurement of time became a local custom, and by 1466 a public clock stood on top of the Tholsel (the city court and council chamber). It was the first of its kind to be clearly recorded in Ireland, and would only have had an hour hand. The increasing lavishness of castles led to the introduction of turret clocks. A 1435 example survives from Leeds castle; its face is decorated with the images of the Crucifixion of Jesus, Mary and St George.\n\nEarly clock dials showed hours: the display of minutes and seconds evolved later. A clock with a minutes dial is mentioned in a 1475 manuscript, and clocks indicating minutes and seconds existed in Germany in the 15th century. Timepieces which indicated minutes and seconds were occasionally made from this time on, but this was not common until the increase in accuracy made possible by the pendulum clock and, in watches, by the spiral balance spring. The 16th-century astronomer Tycho Brahe used clocks with minutes and seconds to observe stellar positions.\n\nThe Ottoman engineer Taqi al-Din described a weight-driven clock with a verge-and-foliot escapement, a striking train of gears, an alarm, and a representation of the moon's phases in his book \"The Brightest Stars for the Construction of Mechanical Clocks\" (\"Al-Kawākib al-durriyya fī wadh' al-bankāmat al-dawriyya\"), written around 1556.\n\nThe concept of the wristwatch goes back to the production of the very earliest watches in the 16th century. Elizabeth I of England received a wristwatch from Robert Dudley in 1571, described as an arm watch. From the beginning, wrist watches were almost exclusively worn by women, while men used pocket-watches up until the early 20th century. This was not just a matter of fashion or prejudice; watches of the time were notoriously prone to fouling from exposure to the elements, and could only reliably be kept safe from harm if carried securely in the pocket. When the waistcoat was introduced as a manly fashion at the court of Charles II in the 17th century, the pocket watch was tucked into its pocket. Prince Albert, the consort to Queen Victoria, introduced the 'Albert chain' accessory, designed to secure the pocket watch to the man's outergarment by way of a clip. By the mid nineteenth century, most watchmakers produced a range of wristwatches, often marketed as bracelets, for women.\n\nWristwatches were first worn by military men towards the end of the nineteenth century, when the importance of synchronizing manoeuvres during war without potentially revealing the plan to the enemy through signalling was increasingly recognized. It was clear that using pocket watches while in the heat of battle or while mounted on a horse was impractical, so officers began to strap the watches to their wrist. The Garstin Company of London patented a 'Watch Wristlet' design in 1893, although they were probably producing similar designs from the 1880s. Clearly, a market for men's wristwatches was coming into being at the time. Officers in the British Army began using wristwatches during colonial military campaigns in the 1880s, such as during the Anglo-Burma War of 1885.\n\nDuring the Boer War, the importance of coordinating troop movements and synchronizing attacks against the highly mobile Boer insurgents was paramount, and the use of wristwatches subsequently became widespread among the officer class. The company Mappin & Webb began production of their successful 'campaign watch' for soldiers during the campaign at the Sudan in 1898 and ramped up production for the Boer War a few years later.\nThese early models were essentially standard pocket-watches fitted to a leather strap, but by the early 20th century, manufacturers began producing purpose-built wristwatches. The Swiss company, Dimier Frères & Cie patented a wristwatch design with the now standard wire lugs in 1903. In 1904, Alberto Santos-Dumont, an early aviator, asked his friend, a French watchmaker called Louis Cartier, to design a watch that could be useful during his flights.\n\nThe impact of the First World War dramatically shifted public perceptions on the propriety of the man's wristwatch, and opened up a mass market in the post-war era. The creeping barrage artillery tactic, developed during the War, required precise synchronization between the artillery gunners and the infantry advancing behind the barrage. Service watches produced during the War were specially designed for the rigours of trench warfare, with luminous dials and unbreakable glass. Wristwatches were also found to be needed in the air as much as on the ground: military pilots found them more convenient than pocket watches for the same reasons as Santos-Dumont had. The British War Department began issuing wristwatches to combatants from 1917.\nThe company H. Williamson Ltd., based in Coventry, was one of the first to capitalize on this opportunity. During the company's 1916 AGM it was noted that \"...the public is buying the practical things of life. Nobody can truthfully contend that the watch is a luxury. It is said that one soldier in every four wears a wristlet watch, and the other three mean to get one as soon as they can.\" By the end of the War, almost all enlisted men wore a wristwatch, and after they were demobilized, the fashion soon caught on – the British \"Horological Journal\" wrote in 1917 that \"...the wristlet watch was little used by the sterner sex before the war, but now is seen on the wrist of nearly every man in uniform and of many men in civilian attire.\" Within a decade, sales of wristwatches had outstripped those of pocket watches.\n\nIn the late 17th and 18th Centuries, equation clocks were made, which allowed the user to see or calculate apparent solar time, as would be shown by a sundial. Before the invention of the pendulum clock, sundials were the only accurate timepieces. When good clocks became available, they appeared inaccurate to people who were used to trusting sundials. The annual variation of the equation of time made a clock up to about 15 minutes fast or slow, relative to a sundial, depending on the time of year. Equation clocks satisfied the demand for clocks that always agreed with sundials. Several types of equation clock mechanism were devised. which can be seen in surviving examples, mostly in museums.\n\nInnovations to the mechanical clock continued, with miniaturization leading to domestic clocks in the 15th century, and personal watches in the 16th. In the 1580s, the Italian polymath Galileo Galilei investigated the regular swing of the pendulum, and discovered that it could be used to regulate a clock. Although Galileo studied the pendulum as early as 1582, he never actually constructed a clock based on that design. The first pendulum clock was designed and built by Dutch scientist Christiaan Huygens, in 1656. Early versions erred by less than one minute per day, and later ones only by 10 seconds, very accurate for their time.\n\nIn England, the manufacturing of pendulum clocks was soon taken up. The longcase clock (also known as the grandfather clock) was first created to house the pendulum and works by the English clockmaker William Clement in 1670 or 1671; this became feasible after Clement invented the anchor escapement mechanism in about 1670. Before then, pendulum clocks used the older verge escapement mechanism, which required very wide pendulum swings of about 100°. To avoid the need for a very large case, most clocks using the verge escapement had a short pendulum. The anchor mechanism, however, reduced the pendulum's necessary swing to between 4° to 6°, allowing clockmakers to use longer pendulums with consequently slower beats. These required less power to move, caused less friction and wear, and were more accurate than their shorter predecessors. Most longcase clocks use a pendulum about a metre (39 inches) long to the center of the bob, with each swing taking one second. This requirement for height, along with the need for a long drop space for the weights that power the clock, gave rise to the tall, narrow case.\n\nClement also introduced the pendulum suspension spring in 1671. The concentric minute hand was added to the clock by Daniel Quare, a London clock-maker, and the Second Hand was introduced.\n\nThe Jesuits were another major contributor to the development of pendulum clocks in the 17th and 18th centuries, having had an \"unusually keen appreciation of the importance of precision\". In measuring an accurate one-second pendulum, for example, the Italian astronomer Father Giovanni Battista Riccioli persuaded nine fellow Jesuits \"to count nearly 87,000 oscillations in a single day\". They served a crucial role in spreading and testing the scientific ideas of the period, and collaborated with contemporary scientists, such as Huygens.\n\nThe invention of the mainspring in the early 15th century allowed portable clocks to be built, evolving into the first pocketwatches by the 17th century, but these were not very accurate until the balance spring was added to the balance wheel in the mid 17th century. Some dispute remains as to whether British scientist Robert Hooke (his was a straight spring) or Dutch scientist Christiaan Huygens was the actual inventor of the balance spring. Huygens was clearly the first to use a spiral balance spring, the form used in virtually all watches to the present day. The addition of the balance spring made the balance wheel a harmonic oscillator like the pendulum in a pendulum clock, which oscillated at a fixed resonant frequency and resisted oscillating at other rates. This innovation increased watches' accuracy enormously, reducing error from perhaps several hours per day to perhaps 10 minutes per day, resulting in the addition of the minute hand to the watch face around 1680 in Britain and 1700 in France.\n\nLike the invention of pendulum clock, Huygens' spiral hairspring (balance spring) system of portable timekeepers, helped lay the foundations for the modern watchmaking industry. The application of the spiral balance spring for watches ushered in a new era of accuracy for portable timekeepers, similar to that which the pendulum had introduced for clocks. From its invention in 1675 by Christiaan Huygens, the spiral hairspring (balance spring) system for portable timekeepers, still used in mechanical watchmaking industry today.\n\nIn 1675, Huygens and Robert Hooke invented the spiral balance, or the hairspring, designed to control the oscillating speed of the balance wheel. This crucial advance finally made accurate pocket watches possible. This resulted in a great advance in accuracy of pocket watches, from perhaps several hours per day to 10 minutes per day, similar to the effect of the pendulum upon mechanical clocks. The great English clockmaker, Thomas Tompion, was one of the first to use this mechanism successfully in his pocket watches, and he adopted the minute hand which, after a variety of designs were trialled, eventually stabilised into the modern-day configuration.\n\nThe Rev. Edward Barlow invented the rack and snail striking mechanism for striking clocks, which was a great improvement over the previous mechanism. The repeating clock, that chimes the number of hours (or even minutes) was invented by either Quare or Barlow in 1676. George Graham invented the deadbeat escapement for clocks in 1720.\n\nMarine chronometers are clocks used at sea as time standards, to determine longitude by celestial navigation.\nA major stimulus to improving the accuracy and reliability of clocks was the importance of precise time-keeping for navigation. The position of a ship at sea could be determined with reasonable accuracy if a navigator could refer to a clock that lost or gained less than about 10 seconds per day. The marine chronometer would have to keep the time of a fixed location—usually Greenwich Mean Time—allowing seafarers to determine longitude by comparing the local high noon to the clock. This clock could not contain a pendulum, which would be virtually useless on a rocking ship.\nAfter the Scilly naval disaster of 1707 where four ships ran aground due to navigational mistakes, the British government offered a large prize of £20,000, equivalent to millions of pounds today, for anyone who could determine longitude accurately. The reward was eventually claimed in 1761 by Yorkshire carpenter John Harrison, who dedicated his life to improving the accuracy of his clocks.\n\nIn 1735 Harrison built his first chronometer, which he steadily improved on over the next thirty years before submitting it for examination. The clock had many innovations, including the use of bearings to reduce friction, weighted balances to compensate for the ship's pitch and roll in the sea and the use of two different metals to reduce the problem of expansion from heat.\n\nThe chronometer was trialled in 1761 by Harrison's son and by the end of 10 weeks the clock was in error by less than 5 seconds.\n\nIn 1815, Sir Francis Ronalds (1788-1873) of London published the forerunner of the electric clock, the electrostatic clock. It was powered with dry piles, a high voltage battery with extremely long life but the disadvantage of its electrical properties varying with the weather. He trialled various means of regulating the electricity and these models proved to be reliable across a range of meteorological conditions.\n\nAlexander Bain, a Scottish clock and instrument maker, was the first to invent and patent the electric clock in 1840. On January 11, 1841, Alexander Bain along with John Barwise, a chronometer maker, took out another important patent describing a clock in which an electromagnetic pendulum and an electric current is employed to keep the clock going instead of springs or weights. Later patents expanded on his original ideas.\n\nThe piezoelectric properties of crystalline quartz were discovered by Jacques and Pierre Curie in 1880. The first quartz crystal oscillator was built by Walter G. Cady in 1921, and in 1927 the first quartz clock was built by Warren Marrison and J. W. Horton at Bell Telephone Laboratories in Canada. The following decades saw the development of quartz clocks as precision time measurement devices in laboratory settings—the bulky and delicate counting electronics, built with vacuum tubes, limited their practical use elsewhere. In 1932, a quartz clock able to measure small weekly variations in the rotation rate of the Earth was developed. The National Bureau of Standards (now NIST) based the time standard of the United States on quartz clocks from late 1929 until the 1960s, when it changed to atomic clocks. In 1969, Seiko produced the world's first quartz wristwatch, the Astron. Their inherent accuracy and low cost of production has resulted in the subsequent proliferation of quartz clocks and watches.\n\nAtomic clocks are the most accurate timekeeping devices in practical use today. Accurate to within a few seconds over many thousands of years, they are used to calibrate other clocks and timekeeping instruments.\n\nThe idea of using atomic transitions to measure time was first suggested by Lord Kelvin in 1879, although it was only in the 1930s with the development of Magnetic resonance that there was a practical method for doing this. A prototype ammonia maser device was built in 1949 at the U.S. National Bureau of Standards (NBS, now NIST). Although it was less accurate than existing quartz clocks, it served to demonstrate the concept.\nThe first accurate atomic clock, a caesium standard based on a certain transition of the caesium-133 atom, was built by Louis Essen in 1955 at the National Physical Laboratory in the UK. Calibration of the caesium standard atomic clock was carried out by the use of the astronomical time scale \"ephemeris time\" (ET).\n\nThe International System of Units standardized its unit of time, the second, on the properties of cesium in 1967. SI defines the second as 9,192,631,770 cycles of the radiation which corresponds to the transition between two electron spin energy levels of the ground state of the Cs atom. The cesium atomic clock, maintained by the National Institute of Standards and Technology, is accurate to 30 billionths of a second per year. Atomic clocks have employed other elements, such as hydrogen and rubidium vapor, offering greater stability—in the case of hydrogen clocks—and smaller size, lower power consumption, and thus lower cost (in the case of rubidium clocks).\n\nThe first professional clockmakers came from the guilds of locksmiths and jewellers. Clockmaking developed from a specialized craft into a mass production industry over many years.\n\nParis and Blois were the early centres of clockmaking in France. French clockmakers such as Julien Le Roy, clockmaker of Versailles, were leaders in case design and ornamental clocks. Le Roy belonged to the fifth generation of a family of clockmakers, and was described by his contemporaries as \"the most skillful clockmaker in France, possibly in Europe\". He invented a special repeating mechanism which improved the precision of clocks and watches, a face that could be opened to view the inside clockwork, and made or supervised over 3,500 watches. The competition and scientific rivalry resulting from his discoveries further encouraged researchers to seek new methods of measuring time more accurately.\n\nBetween 1794 and 1795, in the aftermath of the French Revolution, the French government briefly mandated decimal clocks, with a day divided into 10 hours of 100 minutes each. The astronomer and mathematician Pierre-Simon Laplace, among other individuals, modified the dial of his pocket watch to decimal time. A clock in the Palais des Tuileries kept decimal time as late as 1801, but the cost of replacing all the nation's clocks prevented decimal clocks from becoming widespread. Because decimalized clocks only helped astronomers rather than ordinary citizens, it was one of the most unpopular changes associated with the metric system, and it was abandoned.\n\nIn Germany, Nuremberg and Augsburg were the early clockmaking centers, and the Black Forest came to specialize in wooden cuckoo clocks.\nThe English became the predominant clockmakers of the 17th and 18th centuries. The main centres of the British industry were in the City of London, the West End of London, Soho where many skilled French Huguenots settled and later in Clerkenwell. The Worshipful Company of Clockmakers was established in 1631 as one of the Livery Companies of the City of London.\n\nThomas Tompion was the first English clockmaker with an international reputation and many of his pupils went on to become great horologists in their own right, such as George Graham who invented the deadbeat escapement, orrery and mercury pendulum, and his pupil Thomas Mudge who created the first lever escapement. Famous clockmakers of this period included Joseph Windmills, Simon de Charmes who established the De Charmes clockmaker firm and Christopher Pinchbeck who invented the alloy pinchbeck.\n\nLater famous horologists included John Arnold who made the first practical and accurate modern watch by refining Harrison's chronometer, Thomas Earnshaw who was the first to make these available to the public, Daniel Quare, who invented a repeating watch movement, a portable barometer and introduced the concentric minute hand.\n\nQuality control and standards were imposed on clockmakers by the Worshipful Company of Clockmakers, a guild which licensed clockmakers for doing business. By the rise of consumerism in the late 18th century, clocks, especially pocket watches, became regarded as fashion accessories and were made in increasingly decorative styles. By 1796, the industry reached a high point with almost 200,000 clocks being produced annually in London, however by the mid-19th century the industry had gone into steep decline from Swiss competition.\n\nSwitzerland established itself as a clockmaking center following the influx of Huguenot craftsmen, and in the 19th century, the Swiss industry \"gained worldwide supremacy in high-quality machine-made watches\". The leading firm of the day was Patek Philippe, founded by Antoni Patek of Warsaw and Adrien Philippe of Bern.\n\n\n\n\n"}
{"id": "24936955", "url": "https://en.wikipedia.org/wiki?curid=24936955", "title": "How to Build a Robot Army", "text": "How to Build a Robot Army\n\nHow to Build a Robot Army: Tips on Defending Planet Earth Against Alien Invaders, Ninjas, Monsters, and Zombies is a semi-satirical non-fiction book by Daniel Wilson published in December 2007.\n\nDaniel H. Wilson’s third book is obviously fiction, but contains a lot of useful information nonetheless. \"How to Build a Robot Army: Tips on Defending Planet Earth Against Alien Invaders, Ninjas, and Zombies\" not only deals with how to reprogram household robots to defend against aliens but literally every imaginable monster. From vampires to Godzilla Wilson has you covered. With section titles such as \"How to Slay a Vampire Clan\"\nand \"How to Repel Godzilla\" there is no adversary that this book would leave you unprepared for. Although Annalee Newitz would beg to differ and \"go out on a limb and claim that Earth is desperately unprepared for any kind of alien attack.\" Even though there is a section in the book that deals specifically with that scenario, \"How to Thwart an Alien Invasion\". Despite its apocalyptic aura the book is an easy read. The sections are all short and to the point with little to no flowery language but a lot of humor mixed in. Each section goes into describing what sort of attack you are facing and the monsters involved. Then looks at a typical personal robot that is available for reprogramming. The reprogramming goes in depth but again is easy to follow and stays on point to the hostile situation.\n\nThis book, \"How to Build a Robot Army: Tips on Defending Planet Earth Against Alien Invaders, Ninjas, and Zombies\", is similar to Daniel H. Wilson’s other works because it is heavily laden with accurate and possibly useful information. The easy-to-follow prose that Wilson uses in this book not only fully outlines how to properly reprogram a multitude of robotic devices but also entices young readers. In 2009 \"How to Build a Robot Army: Tips on Defending Planet Earth Against Alien Invaders, Ninjas, and Zombies\" was awarded the “2009 Quick Pick for reluctant Young Adult Readers” by the American Library Association (ALA). Obviously the book gets kids reading because it easily engages them. As Susan Lee Stutler says in her article “From The Twilight Zone to Avatar: Science Fiction Engages the Intellect, Touches the Emotions, and Fuels the Imagination of Gifted Learners” science fiction is interesting not only to gifted learners but all children because “Science fiction is rife with connections to the real world”{link to footnote} and the multitude of layers within each story. \"How to Build a Robot Army: Tips on Defending Planet Earth Against Alien Invaders, Ninjas, and Zombies\" is not a purely instructional book about robotics programming but follows many story lines each involving artificial intelligence. To bring up Stutler again science fiction is plainly this: “Advanced technology, artificial intelligence, extrasensory perception, mind control, fantastic future worlds, anti-utopian societies, strange extraterrestrials, intrepid travels through time and space, large-scale catastrophe, and the problem solving that comes with it—these are the elements of science fiction.” and Wilson’s \"How to Build a Robot Army\" includes all of these and more, the more being Godzilla.\n\n• Chosen by the American Library Association (ALA) as a “2009 Quick Pick for Reluctant Young Adult Readers.\"\n"}
{"id": "4635854", "url": "https://en.wikipedia.org/wiki?curid=4635854", "title": "Hypothetical protein", "text": "Hypothetical protein\n\nIn biochemistry, a hypothetical protein is a protein whose existence has been predicted, but for which there is a lack of experimental evidence that it is expressed in vivo. Sequencing of several genomes has resulted in numerous predicted open reading frames to which functions cannot be readily assigned. These proteins, either orphan or conserved hypothetical proteins, make up ~ 20% to 40% of proteins encoded in each newly sequenced genome. Even when there is enough evidence that the product of the gene is expressed, by techniques such as microarray and mass-spectrometry, it is difficult to assign a function to it given its lack of identity to protein sequences with annotated biochemical function. Nowadays, most protein sequences are inferred from computational analysis of genomic DNA sequence. Hypothetical proteins are created by gene prediction software during genome analysis. When the bioinformatic tool used for the gene identification finds a large open reading frame without a characterised homologue in the protein database, it returns \"hypothetical protein\" as an annotation remark.\n\nThe function of a hypothetical protein can be predicted by domain homology searches with various confidence levels. Conserved domains are available in the hypothetical proteins which need to be compared with the known family domains by which hypothetical protein could be classified into particular protein families even though they have not been in vivo investigated. The function of hypothetical protein could also be predicted by homology modelling, in which hypothetical protein has to align with known protein sequence whose three dimensional structure is known and by modelling method if structure predicted then the capability of hypothetical protein to function could be ascertained computationally.Further, approaches to annotate function to hypothetical proteins include determination of 3-dimensional structure of these proteins by structural genomics initiatives, understanding the nature and mode of prosthetic group/metal ion binding, fold similarity with other proteins of known functions and annotating possible catalytic site and regulatory site. Structure prediction with biochemical function assessment by screening for various substrate is another promising approach to annotate function\n\n\n\n"}
{"id": "47402328", "url": "https://en.wikipedia.org/wiki?curid=47402328", "title": "Index of branches of science", "text": "Index of branches of science\n\nScience (from Latin \"scientia\", meaning \"knowledge\") is a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.\n\nModern science is typically divided into three major branches that consist of the natural sciences (e.g. biology, chemistry, physics), which study nature in the broadest sense; the social sciences (e.g. psychology, sociology, economics) which study people and societies; and the formal sciences (e.g. mathematics, logic, theoretical computer science), which study abstract concepts. There is disagreement, however, on the formal sciences being a science as they do not rely on empirical evidence. Disciplines that use science, such as engineering and medicine, are described as applied sciences.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "408703", "url": "https://en.wikipedia.org/wiki?curid=408703", "title": "Intercropping", "text": "Intercropping\n\nIntercropping is a multiple cropping practice involving growing two or more crops in proximity. The most common goal of intercropping is to produce a greater yield on a given piece of land by making use of resources or ecological processes that would otherwise not be utilized by a single crop.\n\nCareful planning is required, taking into account the soil, climate, crops, and varieties. It is particularly important not to have crops competing with each other for physical space, nutrients, water, or sunlight. Examples of intercropping strategies are planting a deep-rooted crop with a shallow-rooted crop, or planting a tall crop with a shorter crop that requires partial shade. Inga alley cropping has been proposed as an alternative to the ecological destruction of slash-and-burn farming.\n\nWhen crops are carefully selected, other agronomic benefits are also achieved.\n\nPlanting two crops in close proximity can especially be beneficial when the two plants interact in a way that increases one or both of the plant's fitness (and therefore yield). For example, plants that are prone to tip over in wind or heavy rain (lodging-prone plants), may be given structural support by their companion crop. Climbing plants can also benefit from structural support. Some plants are used to suppress weeds or provide nutrients. Delicate or light-sensitive plants may be given shade or protection, or otherwise wasted space can be utilized. An example is the tropical multi-tier system where coconut occupies the upper tier, banana the middle tier, and pineapple, ginger, or leguminous fodder, medicinal or aromatic plants occupy the lowest tier.\n\nIntercropping of compatible plants can also encourage biodiversity, by providing a habitat for a variety of insects and soil organisms that would not be present in a single-crop environment. These organisms may provide crops valuable nutrients, such as through nitrogen fixation.\n\nThere are several ways in which increasing crop diversity may help improve pest management. For example, such practices may limit outbreaks of crop pests by increasing predator biodiversity. Additionally, reducing the homogeneity of the crop can potentially increase the barriers against biological dispersal of pest organisms through the crop.\n\nThere are several ways pests can be controlled through intercropping:\nThe degree of spatial and temporal overlap in the two crops can vary somewhat, but both requirements must be met for a cropping system to be an intercrop. Numerous types of intercropping, all of which vary the temporal and spatial mixture to some degree, have been identified. These are some of the more significant types:\n\nIntercropping to reduce pest damage in agriculture, has been deployed with varying success. For example, while trap cropping has reduced pest densities at a commercial in experiments, it often fails to decrease pest densities deployed in large scale commercial landscapes. Furthermore, increasing crop diversity through intercropping does not necessarily increase the presence of the predators of crop pests. In a systematic review of the literature, in 2008, in the studies examined, predators of pests tended only increased under crop diversification strategies in 53 percent of studies, and crop diversification only led to increased yield in only 32% of the studies.\n\n\n"}
{"id": "8174234", "url": "https://en.wikipedia.org/wiki?curid=8174234", "title": "List of prizes, medals, and awards for women in science", "text": "List of prizes, medals, and awards for women in science\n\nThis is a list of awards, prizes, and medals for women in science (and the STEM (Science, technology, engineering, and mathematics) fields generally).\n\n\n\n\n\n\n\"See also Prizes, Awards, and Honors for Women Mathematicians\"\n\n\n\n\n\n"}
{"id": "32107215", "url": "https://en.wikipedia.org/wiki?curid=32107215", "title": "Loyal Blaine Aldrich", "text": "Loyal Blaine Aldrich\n\nLoyal Blaine Aldrich (November 20, 1884 – February 11, 1965) was an American astrophysicist and astronomer of the Smithsonian Institution. Upon graduation from the University of Wisconsin in 1907, Aldrich became a Smithsonian Astrophysical Observatory assistant to Charles Greeley Abbot. The observatory conducted astrophysical research on solar radiation and the amount of energy from the sun that strikes the outer edge of the earth's atmosphere. Abbot became director of the observatory in 1907 and established solar observing stations in the United States, South America, and Africa to carry out research on solar radiation. Aldrich became director of the observatory from 1942 to 1955. Harvard University astronomy department chairman Fred Lawrence Whipple became director of the observatory when Aldrich retired.\n\nAldrich married Elizabeth Stanley (born October 9, 1896). Their son, Stanley Loyal Aldrich, assisted his father operating the observing station in the Chilean Andes until returning to the United States in 1957 so his daughters might receive education at Windham, Maine, where he taught high school mathematics.\n\nAldrich assisted Abbott's mapping of the infrared solar spectrum and carried out systematic studies of variation in solar radiation, its relation to the sunspot cycle, and its effect on weather variation. He also studied the nature of atmospheric transmission and absorption and assisted Abbot perfecting various standardised instruments now widely used for measuring the sun's heat.\n\n"}
{"id": "48610825", "url": "https://en.wikipedia.org/wiki?curid=48610825", "title": "Lúcia Mendonça Previato", "text": "Lúcia Mendonça Previato\n\nLúcia Mendonça Previato (born 1949) is a Brazilian biologist. She was awarded the L'Oréal-UNESCO Awards for Women in Science in 2004 for her research into preventing Chagas disease.\n\nLúcia Mendonça Previato was born in 1949. She is married to Jose Osvaldo Previato and has two children, Anna and Peter. She graduated from St. Ursula University and earned her Doctorate at the Federal University of Rio de Janeiro in Microbiology and Immunology. She is also a recipient of the TWAS Prize in Biology.\n"}
{"id": "2231135", "url": "https://en.wikipedia.org/wiki?curid=2231135", "title": "Melonite", "text": "Melonite\n\nMelonite is a telluride of nickel; it is a metallic mineral. Its chemical formula is NiTe. It is opaque and white to reddish-white in color, oxidizing in air to a brown tarnish. \n\nIt was first described from the Melones and Stanislaus mine in Calaveras County, California in 1866, by Frederick Augustus Genth.\n\nMelonite occurs as trigonal crystals, which cleave in a (0001) direction. It has a specific gravity of 7.72 and a hardness of 1–1.5 (very soft).\n\n\n"}
{"id": "21172177", "url": "https://en.wikipedia.org/wiki?curid=21172177", "title": "Military taxonomy", "text": "Military taxonomy\n\nMilitary taxonomy encompasses the domains of weapons, equipment, organizations, strategies, and tactics. The use of taxonomies in the military extends beyond its value as an indexing tool or record-keeping template.\n\nMilitary theorist Carl von Clausewitz stressed the significance of grasping the fundamentals of any situation in the \"blink of an eye\" (\"coup d'œil\"). In a military context, the astute tactician can immediately grasp a range of implications and can begin to anticipate plausible and appropriate courses of action. Clauzewitz' conceptual \"blink\" represents a tentative ontology which organizes a set of concepts within a domain.\n\nA conventional military taxonomy might be an hierarchical set of classifications for a given set of objects; and the progress of reasoning is developed from the general to the more specific. In such taxonomic schema, a conflative term is always a polyseme.\n\nIn contrast, a less conventional approach might employ an open-ended contextual military taxonomy—a taxonomy holding only with respect to a specific context; and the progress of reasoning is developed form the specific to the more general.\n\nA taxonomy of terms to describe various types of military operations is fundamentally affected by the way all elements are defined and addressed—not unlike framing.\n\nIn terms of a specific military operation, a taxonomic approach based on differentiation and categorization of the entities participating would produce results which were quite different from an approach based on functional objective of an operation (such as peacekeeping, disaster relief, or counter-terrorism). An incidental advantage which flows from give-and-take in refining taxonomic terms more accurately and efficiently becomes more than a worthwhile objective in terms of anticipated outcomes or results. In today's nontraditional operations, the discussion about fundamentals also generates greater precision in how the defense and security community understands and discusses integrated operations.\n\nMilitary taxonomy in Japan is circumscribed by Japan's pacifist post-war constitution. For example, this affects classification of the \"Hyūga\" class helicopter carriers, which are ships of the Japan Maritime Self-Defense Force (JMSDF).\n\nThis type of helicopter carrier was formally identified as a helicopter destroyer (DDH) to comply with explicit constitutional limitations written in Article 9 of the Japanese Constitution.\n\nThe two ships of this class, the JS \"Hyūga\" and the JS \"Ise\" resemble a light aircraft carrier or amphibious assault ship such as the Italian Navy's 13,850-ton \"Giuseppe Garibaldi\", the Spanish Navy's 17,000-ton \"Principe de Asturias\" or the Royal Navy's 21,000-ton \"Invincible\"-class carriers. According to a PBS documentary, JS \"Hyūga\" is the \"first Japanese aircraft carrier built since WWII;\" but this label is controversial. A taxonomic label of \"aircraft carrier\" is legally proscribed.\n\nEach ship in this class has attracted media and Diet attention because of its resemblance to an aircraft carrier. Until the 1970s, US Navy taxonomy categorized large-scale flattops as \"attack aircraft carriers\" and small flattops as \"antisubmarine aircraft carriers.\" In Japan, the constitutional prohibition against having \"attack\" aircraft carries has been construed to encompass small aircraft carriers but not helicopter carriers.\n\nA uniquely Japanese taxonomic template is applied to these ships and to their missions, which are limited to \"military operations other than war\" (MOOTW).\n\nA number of military strategies can be parsed using a taxonomy model. The comparative theoretical framework might posit a range of criteria, e.g., the character of envisaged political goals, the type of military strategy preferred, and the scope of forces engaged; and this template suggests discrete modes of force. The taxonomy-model analysis suggests a useful depiction of the spectrum of the use of military force in a political context.\n\nIn the 21st century, the ambit of a subset taxonomy of terrorism would include terms related to terrorists, terrorist groups, terrorist attacks, weapons, venues, and characteristics of terrorists and terrorist groups.\n\nTaxonomies offer useful, but incomplete means of structuring information.\n\nTaxonomies are a necessary but not sufficient condition for adequate evaluation of a given data set. While the taxonomic categorizations and sub-categorizations do enhance understanding, it may be significant that the lack of detail in describing objects or elements creates room for ambiguity.\n\n\n"}
{"id": "18493301", "url": "https://en.wikipedia.org/wiki?curid=18493301", "title": "NASA Research Park", "text": "NASA Research Park\n\nNASA Research Park is a research park run by NASA which is developing a world-class, shared-use research and development campus in association with government entities, academia, industry and non-profit organisations. It is situated near San Jose, California. NASA Research Park was approved by NASA HQ in the fall of 2002 and over the past decade has grown into the research park it is today with 61 tenants and partners.\n\nThe U.S. Congress originally established Ames Research Center (Ames) in 1939 as the Ames Aeronautical Laboratory under the National Advisory Committee for Aeronautics (NACA). Ames eventually grew to occupy approximately at Moffett Field adjacent to the Naval Air Station Moffett Field in Santa Clara County, California, in the center of the region that would, in the 1990s, become known worldwide as Silicon Valley. In 1958, Congress created NASA with the \"National Aeronautics and Space Act\" of 1958, 42 U.S.C. § 2451 \"et seq\". The Ames Aeronautical Laboratory was renamed Ames Research Center and became a NASA field center.\n\nAmes is nearing 70 years of age, which it calls \"Seven Decades of Innovation,\" highlighted with major accomplishments in aeronautics and space. From the 1940s through the 1990s, Ames scientists and engineers demonstrated excellence in flight research in many areas including variable stability aircraft, guidance and control displays, boundary-layer control, vertical and short takeoff and landing aircraft, and rotorcraft. Ames developed the swept wing design and the conical camber, now considered in the design of every supersonic aircraft.\n\nAmes developed and operated critical facilities including flight simulators and wind tunnels, pushing the frontiers of computers and the arcjets facility to test materials at very high temperatures, which were critical to high-speed aircraft development and space vehicle re-entry. Ames largest contribution to the early space program for human missions was solving the problem of getting astronauts safely back to Earth, through the development of the blunt body design for re-entry vehicles.\n\nAmes assisted the development of Apollo, developed and operated the Pioneer Missions (the first spacecraft to travel through the asteroid belts, observe Jupiter and Saturn and Venus), and developed the tiltrotor aircraft. The diversity of accomplishments led to the focus in the 1990s on Ames becoming the high-tech center of NASA. In those days in NASA parlance, Ames became the Center of Excellence for Information Technologies, taking the lead in human centered computing, a major interdisciplinary effort to develop means of optimizing the performance of mixed human and computer systems. These new technologies were critical for both aeronautics and space operations with ground-based operators, astronauts (or pilots/controllers in the air traffic management system) and robots functioning collaboratively to maximize mission science return, productivity and safety. This human centered computing focus developed the expertise for Ames to become the lead for all supercomputing in NASA, and in 2005 Ames operated the world's fastest supercomputer, partnered with SGI and Intel.\n\nIn the 1990s, following its historic excellence in life and space sciences, Ames developed a focused new program called Astrobiology to search for the origins of life in the universe. Ames leads NASA's Kepler Mission, a spacecraft designed to find Earth-sized planets in other galaxies that may be in or near habitable zones, distances from a star where liquid water could exist on a planet's surface. Ames developed SOFIA, the new Stratospheric Observatory for Infrared Astronomy, using a Boeing 747 aircraft that will study the universe for the next twenty years in the infrared spectrum.\n\nConcurrent with the outstanding innovations in science and technology, Ames has become the leader in NASA for innovative partnerships with universities and industry, both onsite and in distance collaborations. The opportunity for this new partnering became available in the early 1990s, with the potential for R&D partners to move into the property obtained from the transfer of Navy Moffett Field land to NASA.\n\nFrom its establishment in 1939, Ames shared the land generally known as Moffett Field with the United States Navy, jointly using the major airfield on the property. In the 1930s the Navy developed Moffett Field originally for the home of the famous \"Lighter than Air Era of American Military History,\" housing and operating large-scale airships. Through the years a number of different military organizations, including the United States Air Force, used the Moffett Field facilities, and in the late 1980s the Navy operated the base.\n\nWith the enactment of the Base Realignment and Closure Act in 1991, Congress directed the Navy to close and vacate the Naval Air Station at Moffett Field. Under the framework of the \"Federal Property Administrative Services Act\" of 1949, 40 U.S.C. §471, NASA successfully negotiated custody of most of the Navy property, with the strong support of the local governments surrounding Moffett Field and the U.S. Congressmen from the area, especially Rep. Norman Mineta. The decision was properly approved through the federal government process to transfer the property to NASA and disestablish the Naval Air Station Moffett Field. The United States Department of Defense decided to retain control of of military housing at Moffett Field. In 1994, the Department of the Navy transferred approximately to NASA. This transfer created a unique opportunity for NASA to provide stewardship for the entire 800-hectare (2,000 acre) site, except the military housing.\n\nPrior to obtaining control of Moffett Field, NASA prepared the Moffett Field Comprehensive Use Plan (CUP) to implement its management program for the newly expanded Ames. An Environmental Assessment (EA) and Finding of No Significant Impact accompanied the plan. The EA established under the CUP allows for the development of up to approximately 102,000 square meters (1.1 million square feet) of new construction.\n\nThe transfer of Moffett Field to Ames supplied the impetus to consider various new uses of the property for NASA's benefit. Ames leaders began discussions with federal, state and community leaders for potential reuse ideas.\n\nIn November 1996, the neighboring cities of Mountain View and Sunnyvale formed the Community Advisory Committee to study and provide input to Ames about the best reuses of Moffett Field. Ames developed a six-point initiative, which outlined program goals and reuse concepts for the development of the former Navy base that basically focused on university and industry building on NASA property as R&D collaborative partners. In 1997, after extensive public outreach and public meetings, the final report advisory committee endorsed NASA's six-point initiative, which established the plans to develop what became the NASA Research Park.\n\nAmes leaders reviewed studies of research parks worldwide and continued to work with the neighboring communities in preparing its preferred development plan. In 1998, Ames and the cities of Sunnyvale and Mountain View signed a memorandum of understanding to work jointly on development. Also, a number of major universities were involved in planning their potential roles in development. In mid-1998, Ames leaders presented their plan to NASA HQ and secured approval to proceed.\n\nOn December 8, 1998, NASA unveiled its visionary concept for a shared-use R&D and education campus for collaborations among government, industry, academia and non-profit organizations at a national press conference with NASA Administrator Dan Goldin. Over the next year, MOUs for planning development were signed with the University of California, Carnegie Mellon University, San Jose State University and Foothill-DeAnza Community College.\n\nIn addition to federal, state and community leaders' inputs, Ames worked closely with a number of economic development and industry organizations in focused groups by industry: information technology, bio-technology and others to understand the needs of Silicon Valley high-tech industry. In 1999, this vision was outlined in an Economic Development Concept Workbook, which won the 2000 American Planning Association Award.\n\nNASA's goal is to develop a world-class, shared-use research and development campus in association with government entities, academia, industry and nonprofits. The NADP/EIS provides a framework to guide the use, renovation, management and development of facilities at Ames over the next 20 years to achieve that goal. The NRP supports NASA's overall mission in three areas: advancing NASA's research leadership; facilitating science and technology education; and creating a unique community of researchers, students and educators.\n\nNASA's recent vision and mission statements recognize that not from NASA alone, not from industry alone and not from universities alone will tomorrow's innovations emerge. They will come from the integration of these different segments, each making the most of their unique attributes—NASA's focus on high-risk, long-term research; industry's ability to react quickly with applied technologies; and the universities' expertise in educating and providing a vibrant workforce for the future.\n\nThe Vision for Space Exploration (VSE) announced in 2004 requires NASA to reach out and partner with all kinds of relevant organizations to sustain the long-term vision. The NRP has and is continuing to bring together outstanding diverse partners, assisting the pursuit of the VSE and other NASA programs. Through the interaction of academia, industry and nonprofit organizations at a robust federal laboratory, a unique community of researchers, students and educators with a shared mission to advance human knowledge will be created. This is the goal of the NRP.\n\nIn October 2011, the President issued a Presidential Memorandum \"Accelerating Technology Transfer and Commercialization of Federal Research in Support of High-Growth Businesses\" that directs federal agencies to \"Facilitate Commercialization through Local and Regional Partnerships. Agencies must take steps to enhance successful technology innovation networks by fostering increased Federal laboratory engagement with external partners, including universities, industry consortia, economic development entities, and State and local governments.\" and \"to use existing authorities, such as Enhanced Use Leasing or Facility Use Agreements, to locate applied research and business support programs, such as incubators and research parks, on or near Federal laboratories and other research facilities to further technology transfer and commercialization. I encourage agencies with Federal laboratories and other research facilities to engage in public-private partnerships in those technical areas of importance to the agency's mission with external partners to strengthen the commercialization activities in their local region.\" NRP has implemented this directive through its program of onsite and offsite industry and academic partnerships.\n\n2000 American Planning Association, Northern California Chapter.\nEarly in the planning process for NRP, NASA prepared its \"NRP Economic Development Concept Workbook\" used to brief key stakeholders in Silicon Valley and test real estate development concepts with end user technology companies.\n\n2000-2001 National Research Council.\nAs part of the planning process for NRP, NASA commissioned the highly regarded National Research Council to review the NRP concept. The National Research Council conducted a workshop and published its findings in \"A Review of New Initiatives at NASA Ames Research Center: Summary of a Workshop.\" A key finding was that the proposed NSP would represent a \"new model for Industry-Government Partnerships.\"\n\n2003 General Services Administration Award.\nThe General Services Administration (GSA) awarded its Seventh Annual GSA Achievement Award for Real Property Innovation to NASA for its NASA Ames Development Plan that established NRP. In making the agency's award, GSA Administrator Stephan A. Perry stated that \"as our country changes, our mandate for excellence is creating an every more responsive government to serve our citizens better...(the) NASA Ames Development Plan will provide an integrated, dynamic research and development community.\"\n\n2008 Deal of Year.\nThe San Jose/Silicon Valley Business Journal gave its top \"Deal of the Year\" award to NASA and Google for their landmark ground lease of 42 acres for an office/R&D development.\n\n2009 National Research Council.\nEight years later, the National Research Council reviewed NASA Research Park as part of its influential \"Understanding Research, Science, and Technology Parks: Global Best Practices: Report of a Symposium.\" Citing NRP as an example of global best practices, the Council found that NRP had made \"great progress, exceeding expectations and enacting NASA plans with remarkable effectiveness.\"\n\n2011 Keynote Speaker Association of University Research Parks.\nMr. Michael Marlaire, Director of NASA Research Park was invited to give a keynote speech at the 2011 annual convention of the Association of University Research Parks in New Orleans, LA. With an international membership, this pretisgious association's mission is to foster innovation, commercialization and economic growth in a global economy through university, industry and government partnerships.\n2013 BBC TV Feature.\nNRP and its innovative tenants were featured in the lead episode of BBC TV's Horizons: An Insight into the Future of Global Business. The show can be viewed at the BBC TV website: http://www.bbc.com/specialfeatures/horizonsbusiness/episode/space-innovation/?autoplay=true\n\n2015 Best Civic Project.\nThe San Jose/Silicon Valley Business Journal gave its top \"Best Civic Project of the Year\" award to NASA and Google for their landmark ground lease of the 1,000 acre Moffett Federal Airfield.\n\nThe Business Plan reviewed current partners and suggested future partners based on the core technology areas being pursued by Ames. Current and future NRP partners will contribute to breakthroughs in the areas in which NASA and Ames possess traditional competencies, such as aeronautics, air transportation management, robotics and information technology. While these core areas consist of technologies that are relatively mature, they require further advancement to enable the next generation of space flight and air travel. The development of these technologies is a key driver behind NASA's mission; as such Ames will seek to create partnerships that support these areas as described below.\n\nAmes has pioneered the concept of small spacecraft and their potential for accelerating NASA's progress in exploring the moon and solar system. By inserting a mix of microsatellites and miniature landers into NASA's existing plans for robotic lunar exploration, the agency will be able to make great strides in a shorter timeframe and at a modest cost.\n\nIn May 2006, NASA's Exploration Systems Directorate assigned Ames the responsibility for developing small spacecraft missions to support agency exploration goals. Underlying this decision was the expectation that many of NASA's goals could be achieved with targeted, low-cost proposals in the $50 to $100 million range—much lower than traditional NASA missions.\n\nThe success of this program will depend in part upon partnerships with industry. Increased collaboration with the NRP's university-led Center for Robotics and Space Exploration (CREST) will aid in the development of specific concepts for future small spacecraft missions, and will accelerate the advancement of the supporting technology for these missions.\n\nCloser relationships among NASA, industry partners, and academic institutions will allow for a degree of progress in this area that would otherwise be impossible. Adaptation to the scale of small, low-cost missions will be difficult for both NASA and industry, but can be achieved in short order in a collaborative environment. Furthermore, partners, such as Google, may contribute non-traditional yet valuable expertise to the venture of developing computer systems for the new low-cost payloads.\n\nOne of the NRP partners, m2mi (machine-to-machine intelligence) developed the third ever in NASA, Cooperative Research and Development Agreement or CRADA, that will combine their unique capabilities in software technology, sensors, Global Systems awareness, adaptive control and commercialization capabilities with Ames' expertise in nanosensors, wireless networks and nanosatellite technologies to develop a Fifth Generation (5G) (VOIP-Video-Data-Wireless-m2mi) telecommunications system. A large number of these nanosats (a constellation) will be placed in low earth orbit (LEO) to provide the first ever, Fifth Generation (5G) Telecommunications system to enable Internet Protocol (IP) based services to the global user community.\n\nThe next generation of space exploration systems will require a much greater degree of system intelligence than is currently available. The ability of systems to engage in autonomous decision making and to adapt to changes in the environment will enable NASA to expand its operations in austere environments, reduce costs of operations, and increase safety. NASA and Ames have been leaders in the development of these kinds of systems. However, by leveraging the base of knowledge in the commercial world, and by working with partners at the cutting edge of this technology, NASA can advance its science base in automated learning, intelligent execution and adaptive control beyond what is currently possible. There is great potential with Carnegie Mellon University's expertise in this area along with many Silicon Valley businesses.\n\nNASA's spacecraft, landers, and other exploration systems are heavily reliant upon computers, sensors and information technology. By partnering with the top firms in these areas, Ames will be better positioned to integrate these technologies into future NASA architectures. Google, Cisco, and Apprion are examples of companies currently in negotiation with Ames, and could add a great deal of value to this area once brought on board. Other potential partners include Hewlett-Packard, Cisco, Intel, AMD, and a host of other IT firms located in the San Francisco Bay Area.\n\nAmes has played a significant role in the development of the TPS system for the Space Shuttle and possesses the baseline capabilities needed to develop systems for future spacecraft. NASA can greatly benefit by reaching out to partners beyond the pool of traditional suppliers of orbital thermal protection technology. For example, Ames will reach out to major chemical companies such as Dow Chemical, Du-Pont and BASF, to involve them in the search for next-generation heat shielding solutions. Such collaboration will greatly enhance the pool of talent available to advance technology in this area, and will advance the technology for future space transportation systems. In addition, nanotechnology materials development has great potential in the NRP as a number of companies and universities are interested, including the UCSC planned construction of a new Bio-Info-Nano R&D Lab.\n\nTo ensure the highest degree of safety and efficiency, future space exploration will require a greater understanding of the impact of low and zero-gravity environments upon human physiology. The effects of prolonged exposure to the low-gravity of the moon, for example, will challenge the ability of future explorers to maintain a significant lunar presence. As a result, NASA must continue to charge forward in developing technology and processes that overcome this challenge. Areas of current collaboration in the NRP include advanced muscle augmentation and bone density growth. Ames is currently working with NRP partners Changene and Tibion to advance these technologies. Ames has also been working with firms such as Bigelow and Hamilton Standard on water filtration devices for use on board future spacecraft or orbital habitats. The San Francisco Bay Area hosts over one-third of the world's biotech companies and discussions are underway with the potential for a biotech cluster in the NRP.\n\nOn September 30, 2005, NASA and Google announced a Memorandum of Understanding (MOU) at a national press conference to pursue R&D collaborations with Ames in the areas of: large-scale data management; massively distributed computing; Bio-Info-Nano Convergence; and R&D activities to encourage the entrepreneurial space industry and plan to build of new facilities. In 2006, NASA and Google signed a major Space Act Agreement for Research and Development Collaboration with planned continuing new R&D annexes being added. In 2007, Google announced their Lunar X PRIZE, a $30 million international competition to safely land a robot on the surface of the Moon, travel 500 meters over the lunar surface, and send images and data back to the Earth. In 2008, Google Inc. and NASA signed a long-term ground lease for 42 acres in NRP. The San Jose Business Journal awarded the NASA/Google ground lease its \"Deal of the Year\" award in 2008. In late 2012 Google broke ground to construct up to 1.2 million in new office/R&D facilities near its Googleplex in Mountain View, CA. Google released details of its planned construction of its new campus to Vanity Fair in February 2013 and simultaneously issued a press statement confirming construction plans to the media. The announcement has been reported internationally, including the Wall Street Journal in the United States.\n\nSingularity University was jointly founded by Dr. Peter H. Diamandis and Dr. Ray Kurzweil. The concept of a new university that could leverage the power of exponential technologies to solve humanity's grand challenges was proposed by Diamandis to Kurzweil and to International Space University colleagues Dr. Robert D. Richards and Michael Simpson in April 2007. An exploratory meeting was held at NASA Research Park, Moffett Field in November 2007, followed by a Founders Conference on September 20, 2008 also hosted at NASA. Singularity University opened in the summer of 2009 and has attracted widespread press coverage of its classes, competitions, and events. It hosted over 150 students in 2011 and has a faculty and staff of 40 located at NRP.\n\nIn 2002, Carnegie Mellon University established a branch campus in Silicon Valley to connect its many distinctive technology education programs to the innovative business community at the epicenter of the 21st century technology revolution. The university's Silicon Valley Campus offers master's programs in Software Engineering, Software Management, Engineering & Technology Innovation Management, and Information Technology, as well as a bicoastal Ph.D. program in Electrical and Computer Engineering with a focus in Mobility offered in conjunction with the new CyLab Mobility Research Center. Over 600 graduate-level students have received degrees at the Silicon Valley campus located in two historic buildings at NRP.\n\nThe Navy has the responsibility to remediate the PCB hazard in the metal outer structure of the Hangar One building. The exterior skin is in the process of being removed, and the course of action for the remaining structural skeleton has yet to be publicly announced. The long and public process - of developing and analyzing courses of action - is ongoing. Many have expressed hope that this dynamic historic icon of the South Bay Area can be saved and utilized for a public purpose.\n\nParts of the research park lands are on a superfund site. However, NASA studies have shown that the lands are still usable and only require a particular type of construction.\n\n\n\"A Review of the New Initiatives at the NASA Ames Research Center: A Summary of a Workshop\" \nAuthors: Charles W. Wessner, Editor, Steering Committee for Government-Industry Partnership for the Development of New Technologies, Board on Science, Technology, and Economic Policy, National Research Council\nConsolidated Appropriations Resolution, 2003 \n(Enrolled as Agreed to or Passed by Both House and Senate) \nENHANCED-USE LEASE OF REAL PROPERTY DEMONSTRATION \n"}
{"id": "52480271", "url": "https://en.wikipedia.org/wiki?curid=52480271", "title": "OpenLB", "text": "OpenLB\n\nOpenLB is an object-oriented implementation of the lattice Boltzmann methods (LBM). It is the first implementation of a generic platform for LBM programming, which is shared with the open source community (GPLv2).\nThe code is written in C++ and is used by application programmers as well as developers, with the ability to implement custom models\nOpenLB supports complex data structures that allow simulations in complex geometries and parallel execution using MPI and OpenMP on high-performance computers.\nThe source code uses the concepts of interfaces and templates, so that efficient, direct and intuitive implementations of the LBM become possible.\nThe efficiency and scalability has been checked and proved by code reviews.\nA user manual and a source code documentation by DoxyGen are available on the project page.\n\"OpenLB\" is being constantly developed. By now the following features are implemented:\n\nAutomated grid generation is one of the great advantages of \"OpenLB\" over other CFD software packages. The main advantages are listed below:\nThe automatic grid generation can assume both an STL file as well as primitive geometries. For the geometry, a uniform and rectangular grid is created which encloses the entire space of the geometry. The superfluous grid cells are then removed and the remaining cuboids are shrunk to fit the given geometry. Finally, the grid is distributed to different threads or processors for the parallel execution of the simulation. The boundary conditions and start values can be set using material numbers.\n\n\n"}
{"id": "17347546", "url": "https://en.wikipedia.org/wiki?curid=17347546", "title": "Otto Hahn Medal", "text": "Otto Hahn Medal\n\nThe Otto Hahn Medal () is awarded by the Max Planck Society to young scientists and researchers in both the natural and social sciences. The prestigious award takes its name from the German chemist and Nobel Prize laureate Otto Hahn, who served as the first president of the Max Planck Society from 1948 to 1960.\n\nThe medal is awarded annually to a maximum of thirty junior scientists in recognition of outstanding scientific achievement. Up to ten awardees are selected in each of three thematized sections: 1) Biological-Medical, 2) Chemical-Physical-Engineering, and 3) Social Science-Humanities. It is accompanied by a monetary award of €7,500. Medalists are awarded during a ceremony at the General Meeting of the Max Planck Society, taking place annually in alternating locales in Germany.\n\n\n"}
{"id": "39078347", "url": "https://en.wikipedia.org/wiki?curid=39078347", "title": "Peter R. Last", "text": "Peter R. Last\n\nPeter Robert Last is an Australian ichthyologist, curator of the Australian National Fish Collection and a senior principal research scientist at CSIRO Marine and Atmospheric Research (CMAR) in Hobart, Tasmania. He is an elasmobranch expert and has described many new species of shark.\n\nLast is the co-author of \"Sharks and Rays of Australia\" and co-author of \"A revision of the Australian handfishes (Lophiiformes: Brachionichthyidae), with descriptions of three new genera and nine new species\".\n"}
{"id": "37618233", "url": "https://en.wikipedia.org/wiki?curid=37618233", "title": "Ping Tao Li", "text": "Ping Tao Li\n\nPing Tao Li (; born 1936) is a Chinese botanist who co-authored articles in the \"Flora of China\".\n\n"}
{"id": "29098174", "url": "https://en.wikipedia.org/wiki?curid=29098174", "title": "Puli Space Technologies", "text": "Puli Space Technologies\n\nPuli Space Technologies (named after the \"puli\", a small Hungarian dog breed) is a Hungarian company established by individuals in June 2010 in order to take part in Google Lunar X Prize Challenge and other competitions, and further to facilitate development of space industry in Hungary, to promote scientific thinking and encourage students to choose scientific careers.\n\nBy the end of August 2010, the Team finished a \"Letter of Intent\", and by October 2010 the fundraising started. On 11 January 2011 the team announced it transferred the registration fee. The registration (together with the last eight teams) was announced on 17 February 2011.\n\nThe Puli spacecraft was planned to be finished by 2014. It was planned to be launched by a commercial carrier rocket, and to reach the Moon on its own.\n\nIn the team's early concept traveling on the lunar surface was planned to be achieved by changing the spherical lander's center of mass, making it roll. Later they redesigned the rover to run on four \"wheg\" (wheel of legs) structures. During 2012 the second, bigger rover of that concept (Iteration 2) was constructed and underwent running tests.\n\n"}
{"id": "50444778", "url": "https://en.wikipedia.org/wiki?curid=50444778", "title": "RRS Sir David Attenborough", "text": "RRS Sir David Attenborough\n\nRRS \"Sir David Attenborough\" is a research vessel owned by the Natural Environment Research Council, to be operated by the British Antarctic Survey for the purposes of both research and logistic support. In this, the ship is intended to replace a pair of existing vessels, and . The vessel is named after famed broadcaster and naturalist David Attenborough, and is currently being fitted out.\n\nIn 2014, the UK Government announced funding for the construction of a new polar research vessel for the British Antarctic Survey (BAS) to replace a pair of existing ships. This new ship was intended not only to be fully equipped with the latest instrumentation for the purposes of carrying out research in polar regions, for which it would have an improved icebreaking capability and greater endurance over the existing polar research vessel, but also to serve as a logistic support vessel for BAS teams in inshore locations.\n\nBAS contracted Houlder Ltd to undertake the basic design in which suggestions for the final configuration of the new ship were taken. Following the consultation period, in 2015, Rolls-Royce Holdings was selected to execute the detailed design and Cammell Laird in Birkenhead was selected as the preferred bidder to construct the ship.\n\nThe ship will be about long, with a beam of about . The draught will be about with a planned cruising speed of with a range of at that speed. She will be capable of carrying a helicopter and will have a capacity for approximately of cargo. Accommodation will be provided for 30 crew and 60 research staff.\n\n\"Sir David Attenborough\" will have a twin-shaft hybrid diesel-electric propulsion system. The vessel's power plant will consist of two 6-cylinder Bergen B33:45L6A and two 9-cylinder Bergen B33:45L9A main diesel generators, a harbor generator and two 2,500kW battery systems. The power plant, which can run with different configurations depending on the mission and operating conditions, produces electricity to four asynchronous electric motors driving two 5-bladed controllable pitch propellers. This gives \"Sir David Attenborough\" a maximum speed of in open water and ability to break up to thick level ice at a speed of . At an economical cruising speed of , she will have an operating range of . For maneuvering and dynamic positioning, the vessel will have four Tees White Gill thrusters, two in the bow and two in the stern.\n\nThe vessel will be strengthened according to the International Association of Classification Societies (IACS) \"Unified Requirements for Polar Class Ships\". Her ice class, Polar Class 4, is intended for year-round operation in thick first-year ice which may include old ice inclusions. However, her propulsion system is rated for \"Polar Class 5\" which is intended for medium first-year ice.\n\nThe first steel for the construction of the ship was cut in July 2016. The keel laying ceremony for the ship, yard number 1390, took place on 17 October 2016.\n\nThe ship will be constructed by combining individually fabricated blocks, much like the \"Queen Elizabeth\"-class aircraft carriers. The majority of the blocks will be manufactured by Cammell Laird at Birkenhead, but due to a tight schedule, the stern of the ship (named 'Block 10') was fabricated at the A&P Group on the River Tyne. The section was transported to Merseyside on a barge in August 2017. The stern section was loaded onto the barge by heavy lifting company ALE, using self-propelled modular trailers (SPMT). The same procedure in reverse was then used to get the hull segment on to the slipway at Birkenhead. The hull of \"Sir David Attenborough\" was named by her namesake and launched on 14 July 2018. She was moved into a wet dock for the addition of her superstructure and fitting out. The ship is scheduled to be completed by October 2018.\n\nIn March 2016, the Natural Environment Research Council (NERC) announced that members of the public were being asked to suggest names for the ship. Names previously used would not be eligible, but otherwise it was open to suggestions. The NERC stated that they would have the final say, and that the most popular name in the poll would not necessarily be the one used.\n\nFormer BBC Radio Jersey presenter James Hand jokingly suggested RRS \"Boaty McBoatface\". This quickly became the most popular choice and was the runaway winner when the poll closed, with 124,109 votes. The name has been described as a homage to \"Hooty McOwlface\", an owl named through an \"Adopt-A-Bird\" program in 2012 that became popular on the internet.\n\nOn 6 May 2016, British science minister Jo Johnson announced that the choice had been made to name the ship after naturalist Sir David Attenborough, but that \"Boaty McBoatface\" would be the name of one of \"David Attenborough\" remotely controlled submersibles.\n\nA petition calling for Sir David Attenborough to change his name to Sir Boaty McBoatface \"in the interest of democracy and humour\" soon received over 3,800 signatures.\n\nIn response to the poll, the Science and Technology Committee, a select committee of the House of Lords, announced that they were to review the process by which the ship was named. NERC chief executive Professor Duncan Wingham and NERC head of communications Julia Maddock faced the committee on 10 May. Professor James Wilsdon, an outreach director at Sheffield University, told MPs that he voted for \"Boaty McBoatface\". Despite the controversy, NERC directors felt that their poll was a successful initiative in that it generated a lot of publicity regarding their organisation and research mission among the lay public.\n\nSpanish Internet trolls promoted the choice \"Blas de Lezo\", a Spanish Admiral who gave a humiliating defeat to the British Navy in 1741. The organisers removed the option, which gathered more than 38,000 votes.\n\nOn 12 May 2016, Google released a natural language parser named \"Parsey McParseface\" in reference to the boat naming contest, as part of their open-source SyntaxNet neural network framework.\n\nIn September 2016, the US Air Force formally named its new stealth bomber currently under development as the Northrop Grumman B-21 \"Raider\". The name was chosen through a survey of airmen; among the rejected names was \"Stealthy McStealthface\".\n\nOn 5 August 2016, Cartoon Network's long running animated series, \"Regular Show\", released an episode titled \"Spacey McSpaceTree\".\n\nIn 2017 the BBC Three (online) series \"Pls Like\", Liam Williams who wrote and starred, uses the vlogger name \"Vloggy McVlogface\". \"Pls Like\" is a comedy mockumentary about the search for the next star vlogger.\n\nOn 22 March 2016 the 0729 South West Trains Portsmouth to Waterloo service was playfully named Trainy McTrainface and in July 2017 the same name was chosen in an online poll for a new engine on Stockholm-Gothenburg line.\n\nIn May 2017, the Hunstanton Sea Life Sanctuary in the UK announced that the first Humboldt penguin chick to hatch in more than a decade had been named Fluffy McFluffyface by the sanctuary staff.\n\nIn November 2017, it was announced that one of six new ferries for use in Sydney Harbour, Australia would be named \"Ferry McFerryface\".\n"}
{"id": "53843955", "url": "https://en.wikipedia.org/wiki?curid=53843955", "title": "Rashmi C. Desai", "text": "Rashmi C. Desai\n\nRashmi C. Desai from the University of Toronto, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Topical Group on Statistical and Nonlinear Physics in 2001, for \"applications of statistical mechanics to materials science, including: phase separation and ordering kinetics in systems with competing interactions, Langmuir films, ferromagnetic films, epitaxially grown solid films, order-order transitions in polymers.\"\n"}
{"id": "4615920", "url": "https://en.wikipedia.org/wiki?curid=4615920", "title": "Regulating factors", "text": "Regulating factors\n\nIn population ecology, a regulating factor is something that keeps a population at equilibrium (neither increasing nor decreasing in size over time).\n\nAn example of a regulating factor would be food supply. If the population increases to a certain size, there will be less food for each organism. This will lead to fewer births (a decrease in fecundity) and more deaths, making a negative growth rate. As there are now fewer animals, the amount of food for each organism will increase, meaning the growth rate will become positive. This would lead to a large population size again, and the cycle would start over. Therefore, food is a regulating factor in this scenario, as food supply keeps the population at relative equilibrium.\n\nAll regulating factors are density-dependent, meaning they keep populations at equilibrium by counteracting fluctuations in population size per unit area (or per unit volume for species living within three dimensional environments, such as water). Other regulating factors of the human population at present are drinking water supply, amount of arable land (obviously a more fundamental term for food), air pollution and prevalence of communicable disease. The major regulating factor for the human population in current times is inadequacy of safe drinking water, since waterborne disease is the principal environmental cause of mortality.\n\n"}
{"id": "3345681", "url": "https://en.wikipedia.org/wiki?curid=3345681", "title": "Research design", "text": "Research design\n\nA research design is the set of methods and procedures used in collecting and analyzing measures of the variables specified in the research problem research. The design of a study defines the study type (descriptive, correlation, semi-experimental, experimental, review, meta-analytic) and sub-type (e.g., descriptive-longitudinal case study), research problem, hypotheses, independent and dependent variables, experimental design, and, if applicable, data collection methods and a statistical analysis plan. A research design is a framework that has been created to find answers to research questions.\n\nThere are many ways to classify research designs, but sometimes the distinction is artificial and other times different designs are combined. Nonetheless, the list below offers a number of useful distinctions between possible research designs. A research design is an arrangement of conditions or collections.\n\n\nSometimes a distinction is made between \"fixed\" and \"flexible\" designs. In some cases, these types coincide with quantitative and qualitative research designs respectively, though this need not be the case. In fixed designs, the design of the study is fixed before the main stage of data collection takes place. Fixed designs are normally theory-driven; otherwise, it is impossible to know in advance which variables need to be controlled and measured. Often, these variables are measured quantitatively. Flexible designs allow for more freedom during the data collection process. One reason for using a flexible research design can be that the variable of interest is not quantitatively measurable, such as culture. In other cases, the theory might not be available before one starts the research.\n\nThe choice of how to group participants depends on the research hypothesis and on how the participants are sampled. In a typical experimental study, there will be at least one \"experimental\" condition (e.g., \"treatment\") and one \"control\" condition (\"no treatment\"), but the appropriate method of grouping may depend on factors such as the duration of measurement phase and participant characteristics:\n\n\nConfirmatory research tests \"a priori\" hypotheses — outcome predictions that are made before the measurement phase begins. Such \"a priori\" hypotheses are usually derived from a theory or the results of previous studies. The advantage of confirmatory research is that the result is more meaningful, in the sense that it is much harder to claim that a certain result is generalizable beyond the data set. The reason for this is that in confirmatory research, one ideally strives to reduce the probability of falsely reporting a coincidental result as meaningful. This probability is known as α-level or the probability of a type I error. \n\nExploratory research, on the other hand, seeks to generate \"a posteriori\" hypotheses by examining a data-set and looking for potential relations between variables. It is also possible to have an idea about a relation between variables but to lack knowledge of the direction and strength of the relation. If the researcher does not have any specific hypotheses beforehand, the study is exploratory with respect to the variables in question (although it might be confirmatory for others). The advantage of exploratory research is that it is easier to make new discoveries due to the less stringent methodological restrictions. Here, the researcher does not want to miss a potentially interesting relation and therefore aims to minimize the probability of rejecting a \"real\" effect or relation; this probability is sometimes referred to as β and the associated error is of type II. In other words, if the researcher simply wants to see whether some measured variables could be related, he would want to increase the chances of finding a significant result by lowering the threshold of what is deemed to be \"significant\".\n\nSometimes, a researcher may conduct exploratory research but report it as if it had been confirmatory ('Hypothesizing After the Results are Known', HARKing—see Hypotheses suggested by the data); this is a questionable research practice bordering on fraud.\n\nA distinction can be made between state problems and process problems. State problems aim to answer what the state of a phenomenon is at a given time, while process problems deal with the change of phenomena over time. Examples of state problems are the level of mathematical skills of sixteen-year-old children or the level, computer skills of the elderly, the depression level of a person, etc. Examples of process problems are the development of mathematical skills from puberty to adulthood, the change in computer skills when people get older and how depression symptoms change during therapy.\n\nState problems are easier to measure than process problems. State problems just require one measurement of the phenomena of interest, while process problems always require multiple measurements. Research designs such as repeated measurements and longitudinal study are needed to address process problems.\n\nIn an experimental design, the researcher actively tries to change the situation, circumstances, or experience of participants (manipulation), which may lead to a change in behaviour or outcomes for the participants of the study. The researcher randomly assigns participants to different conditions, measures the variables of interest and tries to control for confounding variables. Therefore, experiments are often highly fixed even before the data collection starts.\n\nIn a good experimental design, a few things are of great importance. First of all, it is necessary to think of the best way to operationalize the variables that will be measured, as well as which statistical methods would be most appropriate to answer the research question. Thus, the researcher should consider what the expectations of the study are as well as how to analyse any potential results. Finally, in an experimental design, the researcher must think of the practical limitations including the availability of participants as well as how representative the participants are to the target population. It is important to consider each of these factors before beginning the experiment. Additionally, many researchers employ power analysis before they conduct an experiment, in order to determine how large the sample must be to find an effect of a given size with a given design at the desired probability of making a Type I or Type II error.\n\nNon-experimental research designs do not involve a manipulation of the situation, circumstances or experience of the participants. Non-experimental research designs can be broadly classified into three categories. First, in relational designs, a range of variables are measured. These designs are also called correlation studies because correlation data are most often used in the analysis. Since correlation does not imply causation, such studies simply identify co-movements of variables. Correlational designs are helpful in identifying the relation of one variable to another, and seeing the frequency of co-occurrence in two natural groups (\"See correlation and dependence\"). The second type is comparative research. These designs compare two or more groups on one or more variable, such as the effect of gender on grades. The third type of non-experimental research is a longitudinal design. A longitudinal design examines variables such as performance exhibited by a group or groups over time. \"See Longitudinal study.\"\n\nFamous case studies are for example the descriptions about the patients of Freud, who were thoroughly analysed and described.\n\nBell (1999) states “a case study approach is particularly appropriate for individual researchers because it gives an opportunity for one aspect of a problem to be studied in some depth within a limited time scale”.\n\nThis type of research is involved with a group, organization, culture, or community. Normally the researcher shares a lot of time with the group.\n\nGrounded theory research is a systematic research process that works to develop \"a process, and action or an interaction about a substantive topic\".\n\n"}
{"id": "19822852", "url": "https://en.wikipedia.org/wiki?curid=19822852", "title": "Residential cluster development", "text": "Residential cluster development\n\nA Residential Cluster Development, or open space development, is the grouping of residential properties on a development site in order to use the extra land as open space, recreation or agriculture. It is increasingly becoming popular in subdivision development because it allows the developer to spend much less on land and obtain much the same price per unit as for detached houses. The shared garden areas can be a source of conflict however. Claimed advantages include more green/public space, closer community, and an optimal storm water management. Cluster development often encounters planning objections. \n\nAccording to William H. Whyte, the author of “Cluster Development” there are two types of cluster development. Townhouse development and super development. Examples of townhouse development include Morrell Park, Philadelphia, Pennsylvania, Hartshone in Richmond, and Dudley Square in Shreveport. Examples of Super Development include Reston, Virginia, Crofton, Maryland, and Americana Fairfax in Virginia.\n\nIn many ways cluster development has been practiced since the earliest communities — from the medieval village to the New England town. However, it wasn’t formalized as a modern concept until the onset of suburban sprawl and ubiquity of detached house developments. The idea of a Cluster development was created as the alternative to the ‘conventional subdivision’. The first conscious application of a Cluster development was in Radburn, New Jersey in 1928. Though it was based on English planning and Ebenezer Howard’s Garden Cities movement, it used principles of cluster development. Following Radburn, many other towns in New Jersey applied those principles to their planning notably the ‘village green’ in Hillsborough, NJ and Brunswick Hill in South Brunswick. In the rest of the country the use of cluster development grew in principally in Maryland and Virginia; notably in Reston and American Fairfax County.\n\nCurrently cluster development is applied all over the United States. There is particularly a strong push for it in the Midwestern states that have had significant problems with large lot suburban sprawl, such as Minnesota, Illinois, Ohio, and Wisconsin.\n\nCluster Development, also known as conservation development, is a site planning approach that is an alternative to conventional subdivision development. It is a practice of Low Impact Development that groups residential properties in a proposed subdivision closer together in order to utilize the rest of the land for open space, recreation or agriculture. Cluster development differs from a planned unit development (PUD) due to the fact that a PUD contains a mix of residential, commercial, industrial, or other uses, whereas the cluster development primarily focuses on residential area. \n\nThe purpose of cluster development is to:\n\nThanks to there being more porous ground coverings and fewer impervious surfaces such as asphalt and concrete, the risk of flooding and erosion from stormwater is reduced. Economical benefits of cluster development can include there being less infrastructure to build— fewer roads, sewers, and utility lines. The higher density of the clusters of housing also tends to mean more efficiency for services such as public transit, and can also promote increased bicycle usage and the encouragement of pedestrians. The extra open space made available by this type of development leaves room for parks, trails, and community-supported agriculture.\n\nFollowing World War II, migration from the cities to suburbs became a dominant trend in America. People were acquiring any land they could find; as a result developers attempted to squeeze as many lots as they could on development sites. Communities then developed zoning regulations to limit the number of units and density allowed on a site. Though this zoning protected land for communities and to an extent preserved land from development, it was what ultimately led to the suburban sprawl as we know it today. It is this zoning that cluster development attempts to amend, and is the primary issue it faces.\n\nMost municipalities have established zoning which restricts developers, planning boards and communities to use only this conventional subdivision development. Thus, the practice of traditional development is difficult to change because of the set standard, familiarly of the procedure, and the fear of undertaking something new. In response to this, groups such as the American Planning Association have developed a model ordinance that provides the framework for cluster development. This ordinance is not difficult to implement administratively, but politically, it is problematic because of conservative resistance.\n\nPeople's perception of personal space has a large part to do with this resistance. Many cases people chose to live in suburbs with the intention of having a large lot property; therefore, it is hard to convince those individuals to live closer together. Convincing people to accept small lot sizes and higher density living remains one of the biggest obstacles of cluster development. This obstacle can be mostly overcome with proper site design, which grants homes unobstructed views and effective private space. As well as educating people about the benefits of having better community and open space can serve as encouragement to change perceptions.\n\nAn additional obstacle to cluster development is the difficulty for creating small lot sizes when no municipal sewer system is in place. When septic systems are used, enough land needs to be available on each building lot for a leach field (sometimes land is required for two leach fields, the additional land set aside as a back-up). The amount of land needed is in proportion to the size of the septic system and the soil conditions, which must allow for the percolation of wastewater safely into the ground. In areas where well water is used, additional lot area may be required to sufficiently separate the well from the leach field. This can lead to minimum lot sizes of or more, making cluster development difficult.\n\nHowever, installing a package wastewater treatment plant (WWTP) for the development (which acts as a small cluster plant, eliminating the need for individual septic tanks) or using biofilters with each septic system make smaller lot sizes acceptable. In addition, providing a package WWTP reduces the diameter and depth of collection sewer lines for the cluster, thus reducing the overall cost of infrastructure. \n\nThe final primary issue with cluster development is the issue of dealing with open, recreational, and agricultural space. These areas serve as benefits in many respects but are also issues that are required to be dealt with. The maintenance of open and recreational space requires the formation of home owners associations that necessitate fees for taxes, insurance and general upkeep. This would not be necessary under a typical subdivision, but people would have their own maintenance expenses. As to agriculture: people enjoying living next to it until there is a need to apply fertilizer or pesticides. This fact cannot be avoided, but through proper use of cluster development, there can be wider gaps and barriers between agricultural land and residential properties, which would limit exposure to unwanted byproducts.\n\nThe model ordinance for cluster development is section 4.7 in the Smart Growth Codes, issued by the American Planning Association. Along with introducing the concept of residential cluster development, the ordinance outlines the process of application, site planning and implementation. \n\nThe primary requisites for application of cluster development are that all principal or accessory uses are allowed and that multifamily dwelling, duplexes, and townhouses are permitted. As well the application of maximal lot coverage, floor area ratios, building height, and parking requirements to the entire site as opposed to the individual lot. Provisions of a cluster development require that the site is at least 2 to and there is no minimum to lot dimensions; furthermore each house can be no more than from the street with yard that is at least . There also needs to be the ability to place more than one principal building on each lot, and lastly no less than 25% of the site is used for open space. \n\nIncluded in the application, the site plan is required to consist of the street and sidewalk layout, the maximum number and type of dwelling units proposed, and how much area they will occupy, with calculations; as well as the area of parking, open space, and other accessories. To calculate the permitted amount of dwellings, one must measure the gross area of the site in acres and tenths of an acre, then subtract the gross area of the public and private streets and public dedicated improvement; the remainder will be the build able area. Then divide the net build able area by the smallest minimum lot size; round this number to the nearest lower number and the figure will be the maximum number of units.\n\nThere are various distinct design features in cluster development notably: the consideration of natural features/topography, smaller lot size, the use of cul-de-sacs, and the use of certain waste/storm water management techniques.\n\nAlong with site design, waste/storm water management design features are a principle aspect of cluster development. Through the maximizing of over land water flow and the strategic use of landforms and plants to slow, hold, and treat runoff it is possible to handle the majority of storm water. As well, there many options in dealing with waste water; techniques such as community drain fields, irrigation systems, and package plants can dramatically reduce the cost of infrastructure, and improve the environment.\n\n\n"}
{"id": "1888490", "url": "https://en.wikipedia.org/wiki?curid=1888490", "title": "Rheopecty", "text": "Rheopecty\n\nRheopecty or rheopexy is the rare property of some non-Newtonian fluids to show a time-dependent increase in viscosity (time-dependent viscosity); the longer the fluid undergoes shearing force, the higher its viscosity. Rheopectic fluids, such as some lubricants, thicken or solidify when shaken. The opposite and much more common type of behaviour, in which fluids become less viscous the longer they undergo shear, is called thixotropy.\n\nExamples of rheopectic fluids include gypsum pastes and printer inks. In the body synovial fluid exhibits the extraordinary property of inverse thixotropy or rheopexy.\n\nThere is ongoing research into new ways to make and use rheopectic materials. There is great interest in possible military uses of this technology. Moreover, the high end of the sports market has also begun to respond to it. Body armor and combat vehicle armor are key areas where efforts are being made to use rheopectic materials. Work is also being done to use these materials in other kinds of protective equipment, which is seen as potentially useful to reduce apparent impact stress in athletics, motor sports, transportation accidents, and all forms of parachuting. In particular, footwear with rheopectic shock absorption is being pursued as a dual-use technology that can provide better support to those who must frequently run, leap, climb, or descend.\n\nAn incorrect example often used to demonstrate rheopecty is cornstarch mixed with water, which is a very viscous, white fluid. It is a cheap and simple demonstrator, which can be picked up by hand as a semi-solid, but flows easily when not under pressure. However, cornstarch in water is actually a dilatant fluid, since it does not show the time-dependent, shear-induced change required in order to be labeled rheopectic. These terms are often and easily confused since the terms are rarely used; a true rheopectic fluid would when shaken be liquid at first, becoming thicker as shaking continued.\n\nJust as the opposite behaviour of becoming thinner with time is thixotropism (time dependent pseudoplastic behaviour), rheopectic behaviour may be described as time-dependent dilatant behaviour.\n"}
{"id": "17051408", "url": "https://en.wikipedia.org/wiki?curid=17051408", "title": "Rudolph Schild", "text": "Rudolph Schild\n\nRudolph E. Schild (born January 10, 1940) is an astrophysicist at the Harvard-Smithsonian Center for Astrophysics, who has been active since the mid-1960s. He has authored or contributed to over 250 papers, of which 150 are in refereed journals. He is married to mezzo-soprano Jane Struss, who teaches voice at Longy School of Music.\n\nSchild is a proponent of \"magnetospheric eternally collapsing objects\" (MECOs), an alternative to black holes. These results are most often published in the fringe \"Journal of Cosmology\", an astronomy journal edited by Schild himself, while his other research is published in mainstream astronomy journals such as \"MNRAS\" and the \"Astronomical Journal\".\n"}
{"id": "12133237", "url": "https://en.wikipedia.org/wiki?curid=12133237", "title": "Science and Technology Committee (House of Lords)", "text": "Science and Technology Committee (House of Lords)\n\nThe Science and Technology Committee is a select committee of the House of Lords in the Parliament of the United Kingdom. It has a broad remit \"to consider science and technology\".\n\nAs of July 2017, the membership of the committee is as follows:\n\n"}
{"id": "732315", "url": "https://en.wikipedia.org/wiki?curid=732315", "title": "Scientific wager", "text": "Scientific wager\n\nA scientific wager is a wager whose outcome is settled by scientific method. They typically consist of an offer to pay a certain sum of money on the scientific proof or disproof of some currently-uncertain statement. Some wagers have specific date restrictions for collection, but many are open. Wagers occasionally exert a powerful galvanizing effect on society and the scientific community.\n\nNotable scientists who have made scientific wagers include Stephen Hawking and Richard Feynman. Stanford Linear Accelerator has an open book containing about 35 bets in particle physics dating back to 1980; many are still unresolved.\n\n\n"}
{"id": "27592", "url": "https://en.wikipedia.org/wiki?curid=27592", "title": "Statistical assumption", "text": "Statistical assumption\n\nStatistics, like all mathematical disciplines, does not infer valid conclusions from nothing. Inferring interesting conclusions about real statistical populations almost always requires some background assumptions. Those assumptions must be made carefully, because incorrect assumptions can generate wildly inaccurate conclusions.\n\nHere are some examples of statistical assumptions.\n\n\nThere are two approaches to statistical inference: \"model-based inference\" and \"design-based inference\". Both approaches rely on some statistical model to represent the data-generating process. In the model-based approach, the model is taken to be initially unknown, and one of the goals is to select an appropriate model for inference. In the design-based approach, the model is taken to be known, and one of the goals is to ensure that the sample data are selected randomly enough for inference.\n\nStatistical assumptions can be put into two classes, depending upon which approach to inference is used.\n\nThe model-based approach is the most commonly used in statistical inference; the design-based approach is used mainly with survey sampling. With the model-based approach, all the assumptions are effectively encoded in the model.\n\nGiven that the validity of any conclusion drawn from a statistical inference depends on the validity of the assumptions made, it is clearly important that those assumptions should be reviewed at some stage. Some instances—for example where data are lacking—may require that researchers judge whether an assumption is reasonable. Researchers can expand this somewhat to consider what effect a departure from the assumptions might produce. Where more extensive data are available, various types of procedures for statistical model validation are available—e.g. for regression model validation.\n\n\n"}
{"id": "1915402", "url": "https://en.wikipedia.org/wiki?curid=1915402", "title": "The Discovery of Grounded Theory", "text": "The Discovery of Grounded Theory\n\nThe Discovery of Grounded Theory is a 1967 book () by Barney Glaser and Anselm Strauss on grounded theory.\n\nAfter their success with \"Awareness of Dying\", Glaser and Strauss decided to write a book on methodology. \"The Discovery of Grounded Theory\" was meant to invite and motivate people to use the newly developed methodology. Unlike later works, it does not provide much advice on how to put the theory into practice.\n\nThe authors had several goals in mind when writing the book:\n\n"}
{"id": "7365819", "url": "https://en.wikipedia.org/wiki?curid=7365819", "title": "Undaunted Courage", "text": "Undaunted Courage\n\nUndaunted Courage: Meriwether Lewis, Thomas Jefferson, and the Opening of the American West (), written by Stephen Ambrose, is a 1996 biography of Meriwether Lewis of the Lewis and Clark Expedition. The book is based on journals and letters written by Lewis, William Clark, Thomas Jefferson and the members of the Corps of Discovery. While most of the book is dedicated to the expedition, several chapters are also devoted to Lewis's early life as a Virginia planter and Jefferson's personal secretary, and his later life as governor of the Louisiana Territory before his untimely death in 1809.\n\nThe book outlines the expedition in detail including the route, interactions with Native Americans, scientific discoveries, wildlife, and landscape. As a biography the book is focused entirely on Lewis, and Clark, Sacagawea and the others are addressed principally in their interactions with Lewis. The expedition, and Lewis' life as a whole, is placed within the broader context of Jefferson's presidency, the opening of the American west, and early Indian Policy. The text is supplemented by maps and illustrations, including some drawn by Lewis himself.\n\nThe book was a #1. New York Times Bestseller in 1996.\n\nIn May 2014, HBO announced plans to produce a six-hour miniseries based on the book starring Casey Affleck as Meriwether Lewis and Matthias Schoenaerts as William Clark.\n\n"}
{"id": "4792160", "url": "https://en.wikipedia.org/wiki?curid=4792160", "title": "VALS", "text": "VALS\n\nVALS (\"Values and Lifestyles\") is a proprietary research methodology used for psychographic market segmentation. VALS is a way of viewing people on the basis of their attitudes, needs, wants, beliefs and demographics. By using psychology to analyze and predict consumer preferences and choices, the VALS system constructs a link between personality traits and buying behavior.The system identifies current and future opportunities by segmenting the consumer market place on the basis of the personality traits that drive consumer behavior\n\nVALS was developed in 1978 by social scientist and consumer futurist Arnold Mitchell and his colleagues at SRI International. It was immediately embraced by advertising agencies and is currently offered as a product of SRI's consulting services division. VALS draws heavily on the work of Harvard sociologist David Riesman and psychologist Abraham Maslow.\n\nMitchell used statistics to identify attitudinal and demographic questions that helped categorize adult American consumers into one of nine lifestyle types: survivors (4%), sustainers (7%), belongers (35%), emulators (9%), achievers (22%), I-am-me (5%), experiential (7%), societally conscious (9%), and integrated (2%). The questions were weighted using data developed from a sample of 1,635 Americans and their significant others, who responded to an SRI International survey in 1980.\n\nThe main dimensions of the VALS framework are resources (the vertical dimension) and primary motivation (the horizontal dimension). The vertical dimension segments people based on the degree to which they are innovative and have resources such as income, education, self-confidence, intelligence, leadership skills, and energy. The horizontal dimension represents primary motivations and includes three distinct types:\n\n\nAt the top of the rectangle are the Innovators, who have such high resources that they could have any of the three primary motivations. At the bottom of the rectangle are the Survivors, who live complacently and within their means without a strong primary motivation of the types listed above. The VALS Framework gives more details about each of the groups.\n\nResearchers faced some problems with the VALS method, and in response, SRI developed the VALS2 programme in 1978; additionally, SRI significantly revised it in 1989. VALS2 places less emphasis on activities and interests and more on a psychological base to tap relatively enduring attitudes and values. The VALS2 program has two dimensions. The first dimension, Self-orientation, determines the type of goals and behaviours that individuals will pursue, and refers to patterns of attitudes and activities which help individuals reinforce, sustain, or modify their social self-image. This is a fundamental human need. \n\nThe second dimension, Resources, reflects the ability of individuals to pursue their dominant self-orientation and includes full-range of physical, psychological, demographic, and material means such as self-confidence, interpersonal skills, inventiveness, intelligence, eagerness to buy, money, position, education, etc. According to VALS 2, a consumer purchases certain products and services because the individual is a specific type of person. The purchase is believed to reflect a consumer’s lifestyle, which is a function of self–orientation and resources. \n\nIn 1991, the name VALS2 was switched back to VALS, because of brand equity. \n\nPsychographic segmentation has been criticized by well-known public opinion analyst and social scientist Daniel Yankelovich, who says psychographics are \"very weak\" at predicting people's purchases, making it a \"very poor\" tool for corporate decision-makers. \n\nThe VALS Framework has also been criticized as too culturally specific for international use. \n\nThe following types correspond to VALS segments of US adults based on two concepts for understanding consumers: primary motivation and resources.\n\n\nThere is little to no evidence of the use of this framework on a wide basis by any other industry other than the ad industry as indicated by SRI on what little it shares on its website (go to the SRI website for more info).\n\n\n\n"}
{"id": "33809236", "url": "https://en.wikipedia.org/wiki?curid=33809236", "title": "Yevgeny Korotkevich", "text": "Yevgeny Korotkevich\n\nYevgeny Sergeyevich Korotkevich () (1918 – 1 February 1994) was a Soviet scientist and polar explorer, Hero of Socialist Labor, and Doctor of Geographical Sciences. He was one of the leading scientists of the Arctic and Antarctic Research Institute (AARI) in the field of glaciology and geography of polar countries.\n\nYevgeny Korotkevich participated in and headed many Arctic expeditions. In 1947-1955, he was engaged in scientific research on New Siberian Islands, Franz Josef Land, Novaya Zemlya, Severnaya Zemlya, Spitzbergen, Kara Sea islands, Taymyr Peninsula. Since 1955, Korotkevich's scientific and organizational activity was closely associated with the studies of the Antarctic nature. He took part in the 1st Soviet Antarctic Expedition and then headed other three Antarctic expeditions.\n\nYevgeny Korotkevich was AARI's deputy director and head of Soviet Antarctic Expedition for a long time. He was also executive editor of the Information Bulletin of the Soviet Antarctic Expedition. Yevgeny Korotkevich authored more than 200 scientific works and oversaw preparation and publishing of series of reference books on the nature of polar regions. He took part in the drafting of the two-volume \"Atlas of the Antarctica\" in the 1970s.\n\nYevgeny Korotkevich was a vice president of the Russian Geographical Society and was twice elected vice president of the International Scientific Committee on Antarctic Research. He was also a member of scientific boards at several research institutes in Saint Petersburg.\n\nYevgeny Korotkevich was a veteran of the Great Patriotic War, Meritorious Worker of Science and Technology of the RSFSR, and twice recipient of the USSR State Prize.\n"}
