{"id": "20057406", "url": "https://en.wikipedia.org/wiki?curid=20057406", "title": "Aargletschers", "text": "Aargletschers\n\nThe Aargletschers, literally \"Aare-Glaciers\", are a system of glaciers located at the sources of the Aare river in the Bernese Alps, Switzerland. The original name has no plural-\"s\", as in German the plural of \"gletscher\" is only marked by changes of the article. The Aargletschers are constituted by two distinct partial glacier systems:\nGrimselsee and Oberaarsee are recent reserve lakes. The \"...-Aar-Horns\" are summits of more than 3600 metres above sea level, two of them even above 4000 metres.\n\n\n"}
{"id": "46718276", "url": "https://en.wikipedia.org/wiki?curid=46718276", "title": "Aggregation-induced emission", "text": "Aggregation-induced emission\n\nAggregation-induced emission (AIE) is an abnormal phenomenon that is observed with certain organic luminophores.\nMost organic compounds have planar structures and higher photoemission efficiencies in solution than in the solid state. However, some organic luminophores have freely-rotating groups that consume energy and promote radiation-less decay after they are excited in solution. When these luminophores aggregate or crystallize and the free rotation of those groups is restricted in the solid-state, the photoluminescence efficiency (i.e. quantum yield) becomes higher than in the solution phase.\n\nThe phenomenon in which organic luminophores show higher photoluminescence efficiency in the aggregated state than in solution is called aggregation-induced emission enhancement (AIEE). Some luminophores, e.g., diketopyrrolopyrrole-based and sulfonamide-based luminophores, only display enhanced emission upon entering the crystalline state. That is, these luminophores are said to exhibit crystallization-induced emission enhancement (CIEE).\n"}
{"id": "1113929", "url": "https://en.wikipedia.org/wiki?curid=1113929", "title": "Agrology", "text": "Agrology\n\nAgrology (from Greek , \"agros\", \"field, tilled land\"; and , \"-logia\") is the branch of soil science dealing with the production of crops. The use of the term is most active in Canada. Use of the term outside Canada is sporadic but significant. The term appears especially well established in Russia and China, with agrologists on university faculty lists and agrology curricula.\n\nAgrology is synonymous with agricultural science when used in Canada, is nearly synonymous with the U.S. term \"agronomy\", and has a meaning related to agricultural soil science when used outside Canada.\n\nThe term agrologist was coined by Dr. J. B. Harrington and adopted in 1946 to fill the need in Canada to have a term to denote \"provincial agriculturalist\". The title of Professional Agrologist is conferred on persons with at least a Bachelor's Degree in Agriculture and who can demonstrate the qualities needed to responsibly teach, practise, or conduct experiments and research in the agricultural sciences. According to the Agricultural Institute of Canada website, an agrologist can also hold a degree in a field related to agriculture, or in some provinces pass rigorous prescribed examinations to attain a professional designation. There are about 5000 agrologists in Canada .\n\nAgrology designations are managed by separate governing bodies in each province, with each operating under its own legislation. For example, within British Columbia the term \"agrology\" is defined by an Act of the Legislature passed in 2003 and adopted in 2004 entitled the Agrologists Act. This Act authorizes the self-governing body, the British Columbia Institute of Agrologists and those practising agrology within British Columbia do so under the following definition.\n\n\"Agrology\" means using agricultural and natural sciences and agricultural and resource economics, including collecting or analyzing data or carrying out research or assessments, to design, evaluate, advise on, direct or otherwise provide professional support to\n\n(a) the cultivation, production, improvement, processing or marketing of aquatic or terrestrial plants or animals, or\n\n(b) the classification, management, use, conservation, protection, restoration, reclamation or enhancement of aquatic or terrestrial ecosystems that are affected by, sustain, or have the potential to sustain the cultivation or production of aquatic or terrestrial plants or animals;\n\nBCIA, the governing body for Agrologists in British Columbia, has over 1000 registered members.\n\nThe Registrars of Professional Agrologists across Canada adopted the following definition of Agrology in May 2007.\n\nAgrology is the practice of bioresource sciences to provide knowledge and advice to support the development of the agriculture sector and the health of the society, environment, and economy.\n\nOutside of Canada, the term agrology is synonymous with soil science and is not in common usage in English-speaking countries.\n\nTwo national member societies (Canadian, American) of the International Union of Soil Sciences (IUSS) maintain and publish glossaries of scientific terms. Other soil science societies defer to the American glossary. The term agrology is not in use. Edaphology or crop edaphology in combination with soil management would be the preferred approach used by soil scientists to concisely describe soil science as it applies to crop production.\n\n, no dictionary definition of agrology is yet consistent with the Canadian use of the term and dictionary definitions fall into one of four categories.\n\n\n\n\n\n"}
{"id": "11045543", "url": "https://en.wikipedia.org/wiki?curid=11045543", "title": "Belgian marble", "text": "Belgian marble\n\nBelgian marble is the name given to limestone extracted in Wallonia, southern Belgium. \n\nIt is quarried around the cities of Namur, Dinant, Tournai, Basecles, Theux, and Mazy/Golzinne. \n\nThe rock is actually not a true marble (a metamorphic rock), but a type of limestone (a calcareous sedimentary rock).\n\nBelgian marbles are available in solid dark greys or blacks; and in polychroms of red, grey, and/or pink. After polishing slabs with several colors exhibit natural decorative patterns. \n\nNamed Belgian marbles include:\n\nBelgian marble has been quarried, cut, and finished as a building stone, stone cladding, and stone veneer since the Ancient Roman era, in Roman Gaul and Rome, such as in the Basilica of Junius Bassus. It has been used in important European religious and secular buildings since the Renaissance, including the Palazzo Pitti and Palace of Versailles. \n\n"}
{"id": "22859001", "url": "https://en.wikipedia.org/wiki?curid=22859001", "title": "Biermann battery", "text": "Biermann battery\n\nIn astrophysics, the Biermann battery is a process by which a weak seed magnetic field can be generated from zero initial conditions. The relative motion between electrons and ions is driven by rotation. The process was discovered by Ludwig Biermann in 1950.\n\nA simple derivation of the effect starts with the momentum equation for the free electron fluid, keeping only the electric field and pressure force:\n\nformula_1.\n\nFor phenomena on sufficiently slow time scales, the left-hand side of the above equation can be neglected. That leads to the Ohm's law for the electric field:\n\nformula_2.\n\nThe other terms in the general Ohm's law are neglected here since they typically vanish for zero magnetic field and do not spontaneously generate field. Inserting the above electric field into Faraday's law of induction gives an equation for the magnetic field:\n\nformula_3\n\nThis shows that magnetic fields develop \"spontaneously\", that is, from initially being zero, when there are non-parallel gradients in electron temperature and density. \n"}
{"id": "52869832", "url": "https://en.wikipedia.org/wiki?curid=52869832", "title": "Centre for Studies in Social Sciences, Calcutta", "text": "Centre for Studies in Social Sciences, Calcutta\n\nCentre for Studies in Social Sciences, Calcutta (CSSSC) is a social science and humanities research and teaching institute in Kolkata, West Bengal, India. \n\nEstablished in 1973 jointly by the Indian Council of Social Science Research and Government of West Bengal, the Centre is one of the top social sciences think tanks of India. The centre was founded by Professor S. Nurul Hasan, when he was the education minister of India. Professor Barun De was appointed as its first director.\n\nThe centre specializes in post-colonial, subaltern studies and cultural studies research.\n\nThe museum, called Jadunath Sarkar Resource Centre and Museum, houses an extensive collection of vernacular medium primary and secondary literature.\n\nThe Centre is administered by a chairman, director and registrar.\n\nInitially located in Jadunath Bhavan, the former residence of Sir Jadunath Sarkar at 10, Jadunath Sarkar Road (earlier Lake Terrace), Calcutta, the research centre is now located in a new building in Patuli, Calcutta. The resource centre and museum continue to remain in the historian's former residence.\n\n\n"}
{"id": "10648303", "url": "https://en.wikipedia.org/wiki?curid=10648303", "title": "Chambliss Amateur Achievement Award", "text": "Chambliss Amateur Achievement Award\n\nThe Chambliss Amateur Achievement Award is awarded by the American Astronomical Society for an achievement in astronomical research made by an amateur astronomer resident in North America. The prize is named after Carlson R. Chambliss of Kutztown University, who donated the funds to support the prize. The award will consist of a 224-gram (½-lb) silver medal and $1,000 cash. \n\nSource: American Astronomical Society\n\n\n"}
{"id": "14259138", "url": "https://en.wikipedia.org/wiki?curid=14259138", "title": "Charles Jules Edmée Brongniart", "text": "Charles Jules Edmée Brongniart\n\nCharles Jules Edmée Brongniart (11 February 1859, in Paris – 18 April 1899, in Paris) was a French entomologist and paleontologist.\n\nA pioneer in the field of paleoentomology, he made important contributions towards the understanding of insect evolution. He is remembered for his studies of Late Carboniferous fauna found at Commentry, France.\n\n\n"}
{"id": "929970", "url": "https://en.wikipedia.org/wiki?curid=929970", "title": "Controversies in autism", "text": "Controversies in autism\n\nDiagnoses of autism have become more frequent since the 1980s, which has led to various controversies about both the cause of autism and the nature of the diagnoses themselves. Whether autism has mainly a genetic or developmental cause, and the degree of coincidence between autism and intellectual disability, are all matters of current scientific controversy as well as inquiry.\n\nScientific consensus holds that vaccines do not cause autism, but popular rumors and an article in a respected scientific journal, \"The Lancet\", provoked concern among parents. The \"Lancet\" article was retracted for making false claims and because its author was found to be on the payroll of litigants against vaccine manufacturers.\n\nMost recent reviews of epidemiology estimate a prevalence of one to two cases per 1,000 people for autism, and about six per 1,000 for ASD; because of inadequate data, these numbers may underestimate the true prevalence of autism spectrum disorder (ASD). ASD averages a 4.3:1 male-to-female ratio. The number of children on the autism spectrum has increased dramatically since the 1980s, at least partly due to changes in diagnostic practice; it is unclear whether prevalence has actually increased; and as-yet-unidentified environmental risk factors cannot be ruled out. The risk of autism is associated with several prenatal factors, including advanced parental age and diabetes in the mother during pregnancy. ASD is associated with several genetic disorders and epilepsy. Autism is also associated with intellectual disability.\n\nThe role of genetic influence on ASD has been heavily researched over the past few years. ASD is considered to have polygenic traits since there is not a single risk factor, but multiple ones.\n\nMultiple twin and family studies have been conducted in order to observe any genetic influence in diagnosing ASD. The chance of both twins having ASD was significantly higher in identical twins than fraternal twins, concluding that ASD is heritable. A reoccurring finding is that \"de novo\" (new mutation) copy number variants are a primary cause of ASD - they alter synaptic functions; germ line mutations can produce \"de novo\" CNVs. These mutations can only be passed on to offspring; this explains the phenomenon that occurs when the child has symptoms of ASD, but the parents have no symptoms or history of ASD. \"De novo\" variants differ from person to person i.e one variant can cause ASD in one person, whereas another person would need multiple variants to cause the same disorder. Loss of function variants occur in 16-18% of ASD diagnoses, which is nearly double the normal population. These loss of function variants reduce function in the protein neurexin, which connects neurons at the synapse and is important for neurological development; deletion mutations of neurexin are also very common in people with autism, as well as other neurological disorders like schizophrenia, bipolar disorder, and ADHD.\n\nGut microbiome has a relation to ASD. Excessive \"Clostridia\" spp. was found in children with ASD and gastrointestinal difficulties; \"Clostridia\" spp produces propionic acid which is impaired or in excess in people with ASD Specifically, \"C. tetani\" and \"C. histolyticum\" are two species of this bacteria that affect people with ASD. \"C. tetani\" produces tetanus neurotoxin in the intestinal tract; \"C. histolyticum\" is a toxin producer that is abundant in people diagnosed with ASD. Both of these could contribute to neurological symptoms.\n\nThere is also controversy over the Nature vs. Nurture debate. According to family studies, genetic and environmental factors have an equal influence on risk of ASD.\n\nSeveral false claims have been made with regard to autism and vaccinations, leading notably to the MMR vaccine controversy, the thiomersal controversy, and the theory of vaccine overload. A 2011 journal article described the MMR-autism connection as \"the most damaging medical hoax of the last 100 years\".\n\nIn 1999, the Centers for Disease Control (CDC) and the American Academy of Pediatrics (AAP) asked vaccine makers to remove the organomercury compound thiomersal (spelled \"thimerosal\" in the U.S.) from vaccines as quickly as possible, and thiomersal has been phased out of U.S. and European vaccines, except for some preparations of influenza vaccine, meningococcal vaccine, and trace amounts in the tetanus-diphtheria (Td) vaccine. Under the FDA Modernization Act (FDAMA) of 1997, the FDA conducted a comprehensive review of the use of thimerosal in childhood vaccines. Conducted in 1999, this review found no evidence of harm from the use of thimerosal as a vaccine preservative, other than local hypersensitivity reactions. The CDC and the AAP followed the precautionary principle, which assumes that there is no harm in exercising caution even if it later turns out to be unwarranted, but their 1999 action sparked confusion and controversy that has diverted attention and resources away from efforts to determine the causes of autism. Since 2000, the thiomersal in child vaccines has been alleged to contribute to autism, and thousands of parents in the United States have pursued legal compensation from a federal fund. A 2004 Institute of Medicine (IOM) committee favored rejecting any causal relationship between thiomersal-containing vaccines and autism. Autism incidence rates increased steadily even after thiomersal was removed from childhood vaccines. Currently there is no accepted scientific evidence that exposure to thiomersal is a factor in causing autism.\n\nIn the UK, the MMR vaccine was the subject of controversy after publication in \"The Lancet\" of a 1998 paper by Andrew Wakefield, \"et al.\" This article reported a study of 12 children mostly with onset autism spectrum disorders soon after administration of the vaccine. During a 1998 press conference, Wakefield suggested that giving children the vaccines in three separate doses would be safer than a single vaccination. This suggestion was not supported by his paper, nor by several subsequent peer-reviewed studies that failed to show any association between the vaccine and autism. It later emerged that Wakefield had received funding from litigants against vaccine manufacturers and that Wakefield had not informed colleagues or medical authorities of his conflict of interest. Had this been known, the paper would not have been published in \"The Lancet\" the way that it was. Wakefield has been heavily criticized on scientific grounds and for triggering a decline in vaccination rates, as well as on ethical grounds for the way the research was conducted. In 2004 the MMR-and-autism interpretation of the paper was formally retracted by 10 of Wakefield's 12 co-authors, and in 2010 \"The Lancet\"s editors fully retracted the paper.\n\nThe CDC, the IOM of the National Academy of Sciences, and the UK National Health Service have all concluded that there is no evidence of a link between the MMR vaccine and autism. In 2009, \"The Sunday Times\" reported that Wakefield had manipulated patient data and misreported results in his 1998 paper, creating the appearance of a link with autism. A 2011 article in the British Medical Journal described how the data in the study had been falsified by Wakefield so it would arrive at a predetermined conclusion. An accompanying editorial in the same journal described Wakefield's work as an \"elaborate fraud\" which led to lower vaccination rates, putting hundreds of thousands of children at risk and diverting energy and money away from research into the true cause of autism.\n\nFollowing the belief that individual vaccines caused autism was the idea of vaccine overload, which claims that too many vaccines at once may overwhelm or weaken a child's immune system and lead to adverse effects. Vaccine overload became popular after the Vaccine Injury Compensation Program accepted the case of nine year old Hannah Poling. Hannah had encephalopathy putting her on the autism spectrum disorder, which was believed to have worsened after getting multiple vaccines at nineteen months old. There have been multiple cases reported similar to this one, which led to the belief that vaccine overload caused autism. However, scientific studies show that vaccines do not overwhelm the immune system. In fact, conservative estimates predict that the immune system can respond to thousands of viruses simultaneously. It is known that vaccines constitute only a tiny fraction of the pathogens already naturally encountered by a child in a typical year. Common fevers and middle ear infections pose a much greater challenge to the immune system than vaccines do. Other scientific findings support the idea that vaccinations, and even multiple concurrent vaccinations, do not weaken the immune system or compromise overall immunity because autism is not an immune-mediated disease.\n\nSome celebrities have spoken out on their views that autism is related to vaccination, including: Jenny McCarthy, Kristin Cavallari, Toni Braxton, Jim Carrey, and Robert F. Kennedy, Jr. Kennedy in particular published the book \"Thimerosal: Let the Science Speak: The Evidence Supporting the Immediate Removal of Mercury--A Known Neurotoxin--From Vaccines.\" \n\nMcCarthy, one of the most outspoken celebrities on the topic, has said her son Evan's autism diagnosis was a result of the MMR vaccine. She authored \"Louder than Words: A Mother's Journey in Healing Autism\" and co-authored \"Healing and Preventing Autism.\" She also founded an organization called Generation Rescue, which provides resources for families affected by autism. In a September 2015 CNN Presidential debate, Donald Trump claimed to know a 2-year-old who recently got a combined vaccine, developed a tremendous fever and now is on the Autism Spectrum.\n\nThe percentage of autistic individuals who also meet criteria for intellectual disability has been reported as anywhere from 25% to 70%, a wide variation illustrating the difficulty of assessing autistic intelligence. For PDD-NOS the association with intellectual disability is much weaker. The diagnosis of Asperger's excludes clinically significant delays in mental or cognitive skills.\n\nA 2007 study suggested that Raven's Progressive Matrices (RPM), a test of abstract reasoning, may be a better indicator of intelligence for autistic children than the more commonly used Wechsler Intelligence Scale for Children (WISC). Researchers suspected that the WISC relied too heavily on language to be an accurate measure of intelligence for autistic individuals. Their study revealed that the neurotypical children scored similarly on both tests, but the autistic children fared far better on the RPM than on the WISC. The RPM measures abstract, general and fluid reasoning, an ability autistic individuals have been presumed to lack. A 2008 study found a similar effect, but to a much lesser degree and only for individuals with IQs less than 85 on the Wechsler scales.\n\nFacilitated communication gained immediate attention and was used by many hopeful parents of individuals with autism when it was first introduced during the early 1990s by Douglas Biklen, a professor at Syracuse University. However, controversies shortly arose after several reports of sexual abuse were made by individuals using the keyboard. There became a heightened concern regarding who was actually typing the messages – the individual with autism or the facilitator?\n\nSimilar studies were conducted starting in 1993 that argued facilitated communication was an ineffective technique. One reason was that autistic individuals were not able to communicate any better than they would have independently. Another reason was because facilitators’ seemed to have an influence on the messages being typed by individuals with autism. In 1994, the American Psychological Association officially announced that there was “no scientifically demonstrated support for its efficacy.\" \n\nA recent study done in 2014 examined the role of facilitators in eleven cases where children have been using facilitated communication for more than one year. They agreed that facilitated communication was not valid because the children with autism performed better on test questions only when the facilitator was also aware of the questions. If the facilitator did not know the test questions or the keyboard was out of sight, those with autism were not able to answer the questions either, suggesting that facilitators unconsciously play a role in the messages being typed. Much of the empirical research done during the early 1990s up to now argue that facilitated communication is an invalid alternative treatment to autism.\n\nDespite what the scientific community claims, there are still many people using facilitated communication. In fact, Syracuse University still offers a program called Institute on Communication and Inclusion that trains facilitators to use this technique, what they now call supported typing. They claim that new methods such as video-eye tracking, linguistics analysis, evidence of speech before and during typing, and message passing have demonstrated that it is the individual with autism doing the typing, not the facilitator.\n\nA study done in 2014 stated that the changes made to facilitated communication since its introduction, such as the use of multiple facilitators for one individual, has made this technique more valid such that some individuals with autism have been able to type with zero support. In 2012, the Autism National Committee announced a position statement that approves the use of facilitated communication as it “has already proven to be profoundly beneficial in the lives of many people by opening the doors to reliable, trusted, and respected symbolic communication for the first time.\"\n\nThe autism rights movement (ARM) is a social movement that encourages autistic people, their caregivers and society to adopt a position of neurodiversity, accepting autism as a variation in functioning rather than a mental disorder to be cured. The ARM advocates a variety of goals including a greater acceptance of autistic behaviors; therapies that teach autistic individuals coping skills rather than therapies focused on imitating behaviors of neurotypical peers; the creation of social networks and events that allow autistic people to socialize on their own terms; and the recognition of the Autistic community as a minority group.\n\nAutism rights or neurodiversity advocates believe that the autism spectrum is genetic and should be accepted as a natural expression of the human genome. This perspective is distinct from two other likewise distinct views: (1) the mainstream perspective that autism is caused by a genetic defect and should be addressed by targeting the autism gene(s) and (2) the perspective that autism is caused by environmental factors like vaccines and pollution and could be cured by addressing environmental causes.\n\n\"Curing\" or \"treating\" autism is a controversial and politicized issue. Doctors and scientists are not sure of the cause(s) of autism yet many organizations like Autism Research Institute and Autism Speaks advocate researching a cure. Members of the various autism rights organizations view autism as a way of life rather than as a disease and thus advocate acceptance over a search for a cure. Some advocates believe that common therapies for the behavioral and language differences associated with autism, like applied behavior analysis, are not only misguided but also unethical.\n\nThe \"anti-cure perspective\" endorsed by the movement is a view that autism is not a disorder, but a normal occurrence—an alternate variation in brain wiring or a less common expression of the human genome. Advocates of this perspective believe that autism is a unique way of being that should be validated, supported and appreciated rather than shunned, discriminated against or eliminated. They believe quirks and uniqueness of autistic individuals should be tolerated as the differences of any minority group should be tolerated and that efforts to eliminate autism should not be compared, for example, to curing cancer but instead to the antiquated notion of curing left-handedness. The ARM is a part of the larger disability rights movement, and as such acknowledges the social model of disability. Within the model, struggles faced by autistic people are viewed as discrimination rather than deficiencies.\n\nJohn Elder Robison was a discussant for the Autism Social, Legal, and Ethical Research Special Interest Group at the 2014 International Meeting for Autism Research (IMFAR). He ended up taking the group to task, stating that the autism science community is headed for disaster if it does not change course on several factors – and noting for context the larger size of the US autistic community in proportion to other minority groups such as Jewish or Native American communities.\n\nRobison asserted that autistic people need to be the ones providing oversight and governance for autism research. He condemned the use of words like \"cure\". He pointed out that researchers' explicit or implicit efforts to eradicate autistic people are a formula for disaster and need to stop. He also affirmed that memoirs and narratives written by autistic people are more trustworthy than writing about autism by nonautistics.\n\nAlthough the 2013 fifth revision of the \"Diagnostic and Statistical Manual of Mental Disorders\" (DSM-5) has more specificity, it also has reports of more limited sensitivity. Due to the changes to the DSM and the lessening of sensitivity, there is the possibility that individuals who were diagnosed with autistic spectrum disorders (ASD) using the fourth revision (DSM-IV-TR) will not receive the same diagnosis with the DSM-5.\n\nFrom the 933 individuals that were evaluated, 39 percent of the samples that were diagnosed with an ASD using the DSM-IV-TR criteria did not meet the DSM-5 criteria for that disorder. Essentially, the DSM-5 criteria no longer classified them with having ASD, deeming them without a diagnosis. It was likely that individuals that exhibited higher cognitive functioning and had other disorders, such as Asperger's or pervasive developmental disorder not otherwise specified (PDD-NOS), were completely excluded from the criteria. Also, it is more probable that younger children who do not exhibit the entirety of the symptoms and characteristics of ASD are more at risk of being excluded by the new criteria since they could have Asperger's as Asperger's disorder does not usually show symptoms until later in childhood. Because the onset age is different in Asperger's from autism, grouping together the disorders does not typically allow or distinguish the differentiating ages of onset, which is problematic in diagnosing. It is evident, through the various studies, that the amount of people being diagnosed will be significantly diminished as well, which is prominently due to the DSM-5's new criteria.\n"}
{"id": "55933272", "url": "https://en.wikipedia.org/wiki?curid=55933272", "title": "Damer (crater)", "text": "Damer (crater)\n\nDamer is a crater on Mercury at latitude 36.36 N, longitude 115.81 W and is inside the Shakespeare quadrangle. It is 60 km wide and was named after the English sculptor Anne Seymour Damer, and the name was approved on June 18, 2013 by the IAU.\n\nDamer is located east of the ray crater Degas and west of Gibran, NNE is the crater Whitman. Rays from the Degas protrude into the northernmost rim and north of the crater. The crater features a circular central peak with a hollow in the middle.\n"}
{"id": "22032653", "url": "https://en.wikipedia.org/wiki?curid=22032653", "title": "Dapeng dialect", "text": "Dapeng dialect\n\nDapeng dialect () is a Chinese dialect, a variant of Cantonese with a strong Hakka influence that was originally only spoken on the Dapeng Peninsula of Shenzhen, Guangdong, China. The Chinese diaspora has spread the dialect to places with large populations whose ancestral roots are originally from Dapeng, Shenzhen, Guangdong. Today, their descendants living in Hong Kong, as well among overseas Chinese living in the Randstad region of The Netherlands, Portsmouth, UK and New York City, United States have a lot of Dapeng dialect speakers.\n\nThe dialect is a form of junhua, created as a lingua franca by soldiers at the Dapeng Fortress, who spoke various forms of Cantonese and Hakka. Despite strong influence from Hakka, some, including Lau Chun-Fat, have classified it as a Guan–Bao dialect.\n"}
{"id": "9346431", "url": "https://en.wikipedia.org/wiki?curid=9346431", "title": "Death by Black Hole", "text": "Death by Black Hole\n\nDeath by Black Hole: And Other Cosmic Quandaries is a 2007 popular science book written by Neil deGrasse Tyson. It is an anthology of several of Tyson's most popular articles, and was featured in an episode of \"The Daily Show with Jon Stewart\".\n\n\"Death by Black Hole\" is divided into seven sections: The Nature of Knowledge, The Knowledge of Nature, Ways and Means of Nature, The Meaning of Life, When the Universe Turns Bad, Science and Culture, and Science and God.\n\nSection 1 comprises five chapters:\n\n"}
{"id": "56983545", "url": "https://en.wikipedia.org/wiki?curid=56983545", "title": "Depocenter", "text": "Depocenter\n\nA depocenter or depocentre in geology is the part of a sedimentary basin where a particular rock unit has its maximum thickness. Depending on the controls on subsidence and the sedimentary environment the location of basin depocenters may vary with time, such as in active rift basins as extensional faults grow, link or become abandoned.\n"}
{"id": "51944", "url": "https://en.wikipedia.org/wiki?curid=51944", "title": "Dichroism", "text": "Dichroism\n\nIn optics, a dichroic material is either one which causes visible light to be split up into distinct beams of different wavelengths (colours) (not to be confused with dispersion), \"or\" one in which light rays having different polarizations are absorbed by different amounts.\n\nThe original meaning of \"dichroic\", from the Greek \"dikhroos\", two-coloured, refers to any optical device which can split a beam of light into two beams with differing wavelengths. Such devices include mirrors and filters, usually treated with optical coatings, which are designed to reflect light over a certain range of wavelengths, and transmit light which is outside that range. An example is the dichroic prism, used in some camcorders, which uses several coatings to split light into red, green and blue components for recording on separate CCD arrays, however it is now more common to have a Bayer filter to filter individual pixels on a single CCD array. This kind of dichroic device does not usually depend on the polarization of the light. The term \"dichromatic\" is also used in this sense.\n\nThe second meaning of \"dichroic\" refers to the property of a material in which light in different polarization states traveling through it experiences a different absorption coefficient; this is also known as diattenuation. When the polarization states in question are right and left-handed circular polarization, it is then known as circular dichroism. Since the left- and right-handed circular polarizations represent two spin angular momentum (SAM) states, in this case for a photon, this dichroism can also be thought of as Spin Angular Momentum Dichroism.\n\nIn some crystals, the strength of the dichroic effect varies strongly with the wavelength of the light, making them appear to have different colours when viewed with light having differing polarizations. This is more generally referred to as pleochroism, and the technique can be used in mineralogy to identify minerals. In some materials, such as herapathite (iodoquinine sulfate) or Polaroid sheets, the effect is not strongly dependent on wavelength.\n\nDichroism, in the second meaning above, occurs in liquid crystals due to either the optical anisotropy of the molecular structure or the presence of impurities or the presence of dichroic dyes. The latter is also called a \"guest–host effect\".\n\n"}
{"id": "40206496", "url": "https://en.wikipedia.org/wiki?curid=40206496", "title": "E-belt asteroids", "text": "E-belt asteroids\n\nThe E-belt asteroids were the population of a hypothetical extension of the primordial asteroid belt proposed as the source of most of the basin-forming lunar impacts during the Late Heavy Bombardment.\n\nThe E-belt model was developed by William F. Bottke, David Vokrouhlicky, David Minton, David Nesvorný, Alessandro Morbidelli, Ramon Brasser, Bruce Simonson and Harold Levison. It describes the dynamics of an inner band of the early asteroid belt within the framework of the Nice model.\n\nThe extended-belt asteroids were located between the current inner boundary of the asteroid belt and the orbit of Mars with semi-major axis ranging from 1.7 to 2.1 astronomical units (AU). In the current Solar System most orbits in this region are unstable due to the presence of the ν secular resonance. However, prior to the giant planet migration described in the Nice model the outer planets would have been in a more compact configuration with nearly circular orbits. With the planets in this configuration the ν secular resonance would be located outside the asteroid belt. Stable orbits would have existed inside 2.1 AU and the inner edge of the primordial asteroid belt would have been defined by Mars-crossing orbits.\n\nDuring the migration of the giant planets the ν secular resonance would have moved inward as Saturn moved outward. Upon reaching its current location near 2.1 AU the ν secular resonance and other related resonances would destabilize the orbits of the E-belt asteroids. Most would be driven onto planet-crossing orbits as their eccentricities and inclinations increased. Over a period of 400 million years impacts of the E-belt asteroids yield an estimated 9-10 of the 12 basin-forming lunar impacts attributed to the Late Heavy Bombardment.\n\nAs their orbits evolved many of the E-belt asteroids would have acquired orbits similar to those of the Hungaria asteroids with high inclinations and semimajor axis between 1.8 and 2.0 AU. Because orbits in this region are dynamically sticky these objects would form a quasi-stable reservoir. As this population of the E-belt asteroids leaked from this reservoir they would produce a long-lived tail of impacts after the traditional end of the late heavy bombardment at 3.7 billion years ago. A remnant representing roughly 0.1–0.4% of the original E-belt asteroids would remain as the current Hungaria asteroids.\n\nEvidence for the Moon does not support comets from the outer planetesimal belt as the source of the basin-forming lunar impacts. The size frequency distribution (SFD) of ancient lunar craters is a similar to the SFD of main belt asteroids instead of that of comets. Samples recovered from the Moon containing impact melts have a range of ages rather than the sharp spike expected if comets produced the LHB. Analysis of highly siderophile elements in these samples shows a better match for impactors from the inner Solar System than for comets.\nStudies of the dynamics of the main asteroid belt during giant planet migration have significantly limited the number of impactors originating from this region. A rapid alteration of Jupiter's and Saturn's orbits is necessary to reproduce the current orbital distribution. This scenario removes only 50% of the asteroids from the main belt producing 2–3 basins on the Moon.\n\nExamination of samples recovered from the Moon indicates that the impactors were thermally evolved objects. E-type asteroids, an example of this type, are uncommon in the main belt but become more common toward the inner belt and would be expected to be most common in the E-belt. The Hungaria asteroids, which are a remnant of the E-belt in this model, contain a sizable fraction of E-type asteroids.\n\nThe decay of the population of E-belt asteroids captured onto Hungaria like orbits produces a long-lived tail of impacts which continues past the LHB. The continuation of the bombardment is predicted to generate basin-forming impacts on the Earth and Chicxulub-sized craters on the Earth and Moon. Impact craters on the Moon and impact spherule beds found on the Earth dated to this period are consistent with these predictions.\n\nThe E-belt model predicts a remnant population will remain on Hungaria-like orbits. The initial population of E-belt asteroids was calculated based on the population of potential basin-forming impactors remaining among the Hungaria asteroids. The result was consistent with calculations based on the recent estimates of the orbital density of the main asteroid belt before the planetary migration.\n"}
{"id": "4833901", "url": "https://en.wikipedia.org/wiki?curid=4833901", "title": "Ecological indicator", "text": "Ecological indicator\n\nEcological indicators are used to communicate information about ecosystems and the impact human activity has on ecosystems to groups such as the public or government policy makers. Ecosystems are complex and ecological indicators can help describe them in simpler terms that can be understood and used by non-scientists to make management decisions. For example, the number of different beetle taxa found in a field can be used as an indicator of biodiversity.\n\nMany different types of indicators have been developed. They can be used to reflect a variety of aspects of ecosystems, including biological, chemical and physical. Due to this variety, the development and selection of ecological indicators is a complex process.\n\nUsing ecological indicators is a pragmatic approach since direct documentation of changes in ecosystems as related to management measures, is cost and time intensive. For example, it would be expensive and time-consuming to count every bird, plant and animal in a newly restored wetland to see if the restoration was a success. Instead, a few indicator species can be monitored to determine the success of the restoration.\n\nThe terms ecological indicator and environmental indicator are often used interchangeably. However, ecological indicators are actually a sub-set of environmental indicators. Generally, environmental indicators provide information on pressures on the environment, environmental conditions and societal responses. Ecological indicators refer only to ecological processes; however, sustainability indicators are seen as increasingly important for managing humanity's coupled human-environmental systems.\n\nEcological indicators play an important role in evaluating policy regarding the environment.\n\nIndicators contribute to evaluation of policy development by:\n\nBased on the United Nations convention to combat desertification and convention for biodiversity, indicators are planned to be built in order to evaluate the evolution of the factors. For instance, for the CCD, the Unesco-funded Observatoire du Sahara et du Sahel (OSS) has created the Réseau d'Observatoires du Sahara et du Sahel (ROSELT) (website ) as a network of cross-Saharan observatories to establish ecological indicators.\n\nThere are limitations and challenges to using indicators for evaluating policy programs.\n\nFor indicators to be useful for policy analysis, it is necessary to be able to use and compare indicator results on different scales (local, regional, national and international). Currently, indicators face the following spatial limitations and challenges:\n\n\nIndicators also face other limitations and challenges, such as:\n\n\n"}
{"id": "1007362", "url": "https://en.wikipedia.org/wiki?curid=1007362", "title": "Enculturation", "text": "Enculturation\n\nEnculturation is the process by which people learn the dynamics of their surrounding culture and acquire values and norms appropriate or necessary in that culture and worldviews. As part of this process, the influences that limit, direct, or shape the individual (whether deliberately or not) include parents, other adults, and peers. If successful, enculturation results in competence in the language, values, and rituals of the culture.\n\nEnculturation is related to socialization. In some academic fields, socialization refers to the deliberate shaping of the individual. In others, the word may cover both deliberate and informal enculturation.\n\nConrad Phillip Kottak (in \"Window on Humanity\") writes:\n\n\"Enculturation\" is sometimes referred to as \"acculturation\", a word recently used to more distinctively refer only to exchanges of cultural features with foreign cultures. Note that this is a recent development, as \"acculturation\" in some literatures has the same meaning as \"enculturation\".\n\n\n\n"}
{"id": "12776956", "url": "https://en.wikipedia.org/wiki?curid=12776956", "title": "Engineer's Ring", "text": "Engineer's Ring\n\nThe Engineer's Ring is a ring worn by members of the United States Order of the Engineer, a fellowship of engineers who must be a certified Professional Engineer or graduated from an accredited engineering program (or be within one academic year of graduation to participate). The ring is usually a stainless steel band worn on the little finger of the dominant hand. This is so that it makes contact with all work done by the engineer. Rings used to be cast in iron in the most unattractive and simple form to show the nature of work. The ring is symbolic of the oath taken by the wearer, and symbolizes the unity of the profession in its goal of benefitting mankind. The stainless steel from which the ring is made depicts the strength of the profession.\n\nStarting in 1970, it is inspired by the original Canadian Iron Ring ceremony that started in 1922. Engineers receive the ring after taking an oath known as The Obligation of The Engineer, during a ring ceremony. Only those who have met the standards of professional engineering training or experience are able to accept the Obligation, which is voluntarily received for life.\n\nThe required oath, taken immediately before accepting the Engineer's Ring, is known as \"The Obligation of the Engineer\" and is as follows:\n\n\n"}
{"id": "4169791", "url": "https://en.wikipedia.org/wiki?curid=4169791", "title": "Flavorist", "text": "Flavorist\n\nA flavorist, also known as flavor chemist, is someone who uses chemistry to engineer artificial and natural flavors. The tools and materials used by flavorists are almost the same as that used by perfumers with the exception that flavorists seek to mimic or modify both the olfactory and gustatory properties of various food products rather than creating just abstract smells. As well, the materials and chemicals that a flavorist utilizes for flavor creation must be safe for human consumption.\n\nThe profession of flavorists came about when affordable refrigeration for the home spurred of food processing technology, which could affect the quality of the flavor of the food. In some cases these technologies can remove naturally occurring flavors. To remedy the flavor loss, the food processing industry created the flavor industry. The chemists that resolved the demand of the food processing industry became known as \"flavorists\". \n\nEducational requirements for the profession known as flavorist are varied. Flavorists are often graduated either in Chemistry, Biology or Food Science up to PhDs obtained in subjects such as Biochemistry and Chemistry. Because, however, the training of a flavorist is mostly done on-the-job and specifically at a known as a flavor house, this training is similar to the apprentice system.\n\nLocated in Versailles (France), ISIPCA French School offers two years of high-standard education in food flavoring including 12 months traineeship in a . This education program provides students with solid background in flavor formulation, flavor application, and flavor chemistry (analysis and sensory).\n\nThe British Society of Flavourists together with Reading University provide, every year, a three-week flavorist training course for flavorists from all around the world.\n\nIn the United States, a flavorist can join the Society of Flavor Chemists, which meets in New Jersey, Cincinnati, Chicago, and the West Coast 6 to 8 times a year. To be an apprentice flavorist in the society, one must pass an apprenticeship within a flavor house for five years. To be a certified member with voting rights, one must pass a seven-year program. Each level is verified by a written and oral test of the Membership Committee. As an alternative to training under a flavorist, rather than the above-mentioned cases, a 10-year independent option is available.\n\nIn the United Kingdom, a flavorist can join The British Society of Flavourists, which meets near the London area. To acquire membership, applicants must be sponsored by at least two voting members, shall not be under thirty years of age, and shall have been engaged as a creative flavorist for a period of at least ten years. To be an associate member, applicants must be either a full-time creative flavorist with at least four years' experience, a flavor application chemist, or a food technologist responsible for flavor blending, assessment, and evaluation for a period of at least five years, or a person of such standing in the flavor-producing or using industries as satisfies the Membership Committee that he/she is eligible for membership. An Associate Member must be proposed by two voting members. To be a student member, the applicant must be a new entrant to the flavor industry, not yet able to qualify as an Associate, and proposed by one voting member. To be an affiliate member, applicants must be Technical and Marketing Consultants, Commercial and Technical Managers having a direct relationship to the flavoring industry, and sponsored by three voting members.\n\nPamela Low, a flavorist at Arthur D. Little and 1951 graduate of the University of New Hampshire with a microbiology degree, developed the original flavor for Cap'n Crunch in 1963 — recalling a recipe of brown sugar and butter her grandmother served over rice at her home in Derry, New Hampshire.\n\nRobert (Bob) Reinhart developed a technique in the manufacture of Cap'n Crunch, using oil in its recipe as a flavor delivery mechanism — which initially presented problems in having the cereal bake properly. The cereal required innovation of a special baking process as it was one of the first cereals to use the oil coating method to deliver its flavoring.\n\nHaving arrived at the flavor coating for Cap'n Crunch, Low described it as giving the cereal a quality she called \"want-more-ishness\". After her death in 2007, the Boston Globe called Low \"the mother of Cap'n Crunch.\" At Arthur D. Little, Low had also worked on the flavors for Heath, Mounds and Almond Joy candy bars.\n\n\n"}
{"id": "4101904", "url": "https://en.wikipedia.org/wiki?curid=4101904", "title": "Flux balance analysis", "text": "Flux balance analysis\n\nFlux balance analysis (FBA) is a mathematical method for simulating metabolism in genome-scale reconstructions of metabolic networks. In comparison to traditional methods of modeling, FBA is less intensive in terms of the input data required for constructing the model. Simulations performed using FBA are computationally inexpensive and can calculate steady-state metabolic fluxes for large models (over 2000 reactions) in a few seconds on modern personal computers.\n\nFBA finds applications in bioprocess engineering to systematically identify modifications to the metabolic networks of microbes used in fermentation processes that improve product yields of industrially important chemicals such as ethanol and succinic acid. It has also been used for the identification of putative drug targets in cancer and pathogens, rational design of culture media, and more recently host–pathogen interactions. The results of FBA can be visualized using flux maps similar to the image on the right, which illustrates the steady-state fluxes carried by reactions in glycolysis. The thickness of the arrows is proportional to the flux through the reaction.\n\nFBA formalizes the system of equations describing the concentration changes in a metabolic network as the dot product of a matrix of the stoichiometric coefficients (the stoichiometric matrix S) and the vector v of the unsolved fluxes. The right-hand side of the dot product is a vector of zeros representing the system at steady state. Linear programming is then used to calculate a solution of fluxes corresponding to the steady state.\n\nSome of the earliest work in FBA dates back to the early 1980s. Papoutsakis demonstrated that it was possible to construct flux balance equations using a metabolic map. It was Watson, however, who first introduced the idea of using linear programming and an objective function to solve for the fluxes in a pathway. The first significant study was subsequently published by Fell and Small, who used flux balance analysis together with more elaborate objective functions to study the constraints in fat synthesis.\n\nFBA is not computationally intensive, taking on the order of seconds to calculate optimal fluxes for biomass production for a typical network (around 2000 reactions). This means that the effect of deleting reactions from the network and/or changing flux constraints can be sensibly modelled on a single computer.\n\nA frequently used technique to search a metabolic network for reactions that are particularly critical to the production of biomass. By removing each reaction in a network in turn and measuring the predicted flux through the biomass function, each reaction can be classified as either essential (if the flux through the biomass function is substantially reduced) or non-essential (if the flux through the biomass function is unchanged or only slightly reduced).\n\nPairwise reaction deletion of all possible pairs of reactions is useful when looking for drug targets, as it allows the simulation of multi-target treatments, either by a single drug with multiple targets or by drug combinations. Double deletion studies can also quantify the synthetic lethal interactions between different pathways providing a measure of the contribution of the pathway to overall network robustness.\n\nGenes are connected to enzyme-catalyzed reactions by Boolean expressions known as Gene-Protein-Reaction expressions (GPR). Typically a GPR takes the form (Gene A AND Gene B) to indicate that the products of genes A and B are protein sub-units that assemble to form the complete protein and therefore the absence of either would result in deletion of the reaction. On the other hand, if the GPR is (Gene A OR Gene B) it implies that the products of genes A and B are isozymes.\n\nTherefore, it is possible to evaluate the effect of single or multiple gene deletions by evaluation of the GPR as a Boolean expression. If the GPR evaluates to false, the reaction is constrained to zero in the model prior to performing FBA. Thus gene knockouts can be simulated using FBA.\n\nThe utility of reaction inhibition and deletion analyses becomes most apparent if a gene-protein-reaction matrix has been assembled for the network being studied with FBA. The gene-protein-reaction matrix is a binary matrix connecting genes with the proteins made from them. Using this matrix, reaction essentiality can be converted into gene essentiality indicating the gene defects which may cause a certain disease phenotype or the proteins/enzymes which are essential (and thus what enzymes are the most promising drug targets in pathogens). However, the gene-protein-reaction matrix does not specify the Boolean relationship between genes with respect to the enzyme, instead it merely indicates an association between them. Therefore, it should be used only if the Boolean GPR expression is unavailable.\n\nThe effect of inhibiting a reaction, rather than removing it entirely, can be simulated in FBA by restricting the allowed flux through it. The effect of an inhibition can be classified as lethal or non-lethal by applying the same criteria as in the case of a deletion where a suitable threshold is used to distinguish “substantially reduced” from “slightly reduced”. Generally the choice of threshold is arbitrary but a reasonable estimate can be obtained from growth experiments where the simulated inhibitions/deletions are actually performed and growth rate is measured.\n\nTo design optimal growth media with respect to enhanced growth rates or useful by-product secretion, it is possible to use a method known as Phenotypic Phase Plane analysis. PhPP involves applying FBA repeatedly on the model while co-varying the nutrient uptake constraints and observing the value of the objective function (or by-product fluxes). PhPP makes it possible to find the optimal combination of nutrients that favor a particular phenotype or a mode of metabolism resulting in higher growth rates or secretion of industrially useful by-products. The predicted growth rates of bacteria in varying media have been shown to correlate well with experimental results. as well as to define precise minimal media for the culture of \"Salmonella typhimurium\".\n\nIn contrast to the traditionally followed approach of metabolic modeling using coupled ordinary differential equations, flux balance analysis requires very little information in terms of the enzyme kinetic parameters and concentration of metabolites in the system. It achieves this by making two assumptions, steady state and optimality. The first assumption is that the modeled system has entered a steady state, where the metabolite concentrations no longer change, i.e. in each metabolite node the producing and consuming fluxes cancel each other out. The second assumption is that the organism has been optimized through evolution for some biological goal, such as optimal growth or conservation of resources. The steady-state assumption reduces the system to a set of linear equations, which is then solved to find a flux distribution that satisfies the steady-state condition subject to the stoichiometry constraints while maximizing the value of a pseudo-reaction (the objective function) representing the conversion of biomass precursors into biomass.\n\nThe steady-state assumption dates to the ideas of material balance developed to model the growth of microbial cells in fermenters in bioprocess engineering. During microbial growth, a substrate consisting of a complex mixture of carbon, hydrogen, oxygen and nitrogen sources along with trace elements are consumed to generate biomass.\nThe material balance model for this process becomes:\n\nIf we consider the system of microbial cells to be at steady state then we may set the accumulation term to zero and reduce the material balance equations to simple algebraic equations. In such a system, substrate becomes the input to the system which is consumed and biomass is produced becoming the output from the system. The material balance may then be represented as:\n\nMathematically, the algebraic equations can be represented as a dot product of a matrix of coefficients and a vector of the unknowns. Since the steady-state assumption puts the accumulation term to zero. The system can be written as:\n\nExtending this idea to metabolic networks, it is possible to represent a metabolic network as a stoichiometry balanced set of equations. Moving to the matrix formalism, we can represent the equations as the dot product of a matrix of stoichiometry coefficients (stoichiometric matrix formula_5) and the vector of fluxes formula_6 as the unknowns and set the right hand side to 0 implying the steady state.\n\nMetabolic networks typically have more reactions than metabolites and this gives an under-determined system of linear equations containing more variables than equations. The standard approach to solve such under-determined systems is to apply linear programming.\n\nLinear programs are problems that can be expressed in canonical form:\n\nwhere x represents the vector of variables (to be determined), c and b are vectors of (known) coefficients, \"A\" is a (known) matrix of coefficients, and formula_9 is the matrix transpose. The expression to be maximized or minimized is called the \"objective function\" (cx in this case). The inequalities \"A\"x ≤ b are the constraints which specify a convex polytope over which the objective function is to be optimized.\n\nLinear Programming requires the definition of an objective function. The optimal solution to the LP problem is considered to be the solution which maximizes or minimizes the value of the objective function depending on the case in point. In the case of flux balance analysis, the objective function Z for the LP is often defined as biomass production. Biomass production is simulated by an equation representing a lumped reaction that converts various biomass precursors into one unit of biomass.\n\nTherefore, the canonical form of a Flux Balance Analysis problem would be:\n\nwhere formula_6 represents the vector of fluxes (to be determined), formula_5 is a (known) matrix of coefficients. The expression to be maximized or minimized is called the \"objective function\" (formula_13 in this case). The inequalities formula_14 and formula_15 define, respectively, the minimal and the maximal rates of flux for every reaction corresponding to the columns of the formula_5 matrix. These rates can be experimentally determined to constrain and improve the predictive accuracy of the model even further or they can be specified to an arbitrarily high value indicating no constraint on the flux through the reaction.\n\nThe main advantage of the flux balance approach is that it does not require any knowledge of the metabolite concentrations, or more importantly, the enzyme kinetics of the system; the homeostasis assumption precludes the need for knowledge of metabolite concentrations at any time as long as that quantity remains constant, and additionally it removes the need for specific rate laws since it assumes that at steady state, there is no change in the size of the metabolite pool in the system. The stoichiometric coefficients alone are sufficient for the mathematical maximization of a specific objective function.\n\nThe objective function is essentially a measure of how each component in the system contributes to the production of the desired product. The product itself depends on the purpose of the model, but one of the most common examples is the study of total biomass. A notable example of the success of FBA is the ability to accurately predict the growth rate of the prokaryote \"E. coli\" when cultured in different conditions. In this case, the metabolic system was optimized to maximize the biomass objective function. However this model can be used to optimize the production of any product, and is often used to determine the output level of some biotechnologically relevant product. The model itself can be experimentally verified by cultivating organisms using a chemostat or similar tools to ensure that nutrient concentrations are held constant. Measurements of the production of the desired objective can then be used to correct the model.\n\nA good description of the basic concepts of FBA can be found in the freely available supplementary material to Edwards et al. 2001 which can be found at the Nature website. Further sources include the book \"Systems Biology\" by B. Palsson dedicated to the subject and a useful tutorial and paper by J. Orth. Many other sources of information on the technique exist in published scientific literature including Lee et al. 2006, Feist et al. 2008, and Lewis et al. 2012.\n\nThe key parts of model preparation are: creating a metabolic network without gaps, adding constraints to the model, and finally adding an objective function (often called the Biomass function), usually to simulate the growth of the organism being modelled.\n\nMetabolic networks can vary in scope from those describing a single pathway, up to the cell, tissue or organism. The main requirement of a metabolic network that forms the basis of an FBA-ready network is that it contains no gaps. This typically means that extensive manual curation is required, making the preparation of a metabolic network for flux-balance analysis a process that can take months or years. However, recent advances such as so-called gap-filling methods can\nreduce the required time to weeks or months.\n\nSoftware packages for creation of FBA models include Pathway Tools/MetaFlux, Simpheny, and MetNetMaker.\n\nGenerally models are created in BioPAX or SBML format so that further analysis or visualization can take place in other software although this is not a requirement.\n\nA key part of FBA is the ability to add constraints to the flux rates of reactions within networks, forcing them to stay within a range of selected values. This lets the model more accurately simulate real metabolism. The constraints belong to two subsets from a biological perspective; boundary constraints that limit nutrient uptake/excretion and internal constraints that limit the flux through reactions within the organism. In mathematical terms, the application of constraints can be considered to reduce the solution space of the FBA model. In addition to constraints applied at the edges of a metabolic network, constraints can be applied to reactions deep within the network. These constraints are usually simple; they may constrain the direction of a reaction due to energy considerations or constrain the maximum speed of a reaction due to the finite speed of all reactions in nature.\n\nOrganisms, and all other metabolic systems, require some input of nutrients. Typically the rate of uptake of nutrients is dictated by their availability (a nutrient that is not present cannot be absorbed), their concentration and diffusion constants (higher concentrations of quickly-diffusing metabolites are absorbed more quickly) and the method of absorption (such as active transport or facilitated diffusion versus simple diffusion).\n\nIf the rate of absorption (and/or excretion) of certain nutrients can be experimentally measured then this information can be added as a constraint on the flux rate at the edges of a metabolic model. This ensures that nutrients that are not present or not absorbed by the organism do not enter its metabolism (the flux rate is constrained to zero) and also means that known nutrient uptake rates are adhered to by the simulation. This provides a secondary method of making sure that the simulated metabolism has experimentally verified properties rather than just mathematically acceptable ones.\n\nIn principle, all reactions are reversible however in practice reactions often effectively occur in only one direction. This may be due to significantly higher concentration of reactants compared to the concentration of the products of the reaction. But more often it happens because the products of a reaction have a much lower free energy than the reactants and therefore the forward direction of a reaction is favored more.\n\nFor ideal reactions,\n\nFor certain reactions a thermodynamic constraint can be applied implying direction (in this case forward)\n\nRealistically the flux through a reaction cannot be infinite (given that enzymes in the real system are finite) which implies that,\n\nCertain flux rates can be measured experimentally (formula_20) and the fluxes within a metabolic model can be constrained, within some error (formula_21), to ensure these known flux rates are accurately reproduced in the simulation.\n\nFlux rates are most easily measured for nutrient uptake at the edge of the network. Measurements of internal fluxes is possible using radioactively labelled or NMR visible metabolites.\n\nConstrained FBA-ready metabolic models can be analyzed using software such as the COBRA toolbox(available implementations in MATLAB and Python), SurreyFBA, or the web-based FAME. Additional software packages have been listed elsewhere. A comprehensive review of all such software and their functionalities has been recently reviewed.\n\nAn open-source alternative is available in the R (programming language) as the packages abcdeFBA or sybil for performing FBA and other constraint based modeling techniques.\n\nFBA can give a large number of mathematically acceptable solutions to the steady-state problem formula_23. However solutions of biological interest are the ones which produce the desired metabolites in the correct proportion. The objective function defines the proportion of these metabolites. For instance when modelling the growth of an organism the objective function is generally defined as biomass. Mathematically, it is a column in the stoichiometry matrix the entries of which place a \"demand\" or act as a \"sink\" for biosynthetic precursors such as fatty acids, amino acids and cell wall components which are present on the corresponding rows of the S matrix. These entries represent experimentally measured, dry weight proportions of cellular components. Therefore, this column becomes a lumped reaction that simulates growth and reproduction. Therefore, the accuracy of experimental measurements plays an essential role in the correct definition of the biomass function and makes the results of FBA biologically applicable by ensuring that the correct proportion of metabolites are produced by metabolism.\n\nWhen modeling smaller networks the objective function can be changed accordingly. An example of this would be in the study of the carbohydrate metabolism pathways where the objective function would probably be defined as a certain proportion of ATP and NADH and thus simulate the production of high energy metabolites by this pathway.\n\nLinear programming can be used to find a single optimal solution. The most common biological optimization goal for a whole-organism metabolic network would be to choose the flux vector formula_24 that maximises the flux through a biomass function composed of the constituent metabolites of the organism placed into the stoichiometric matrix and denoted formula_25 or simply formula_26\n\nIn the more general case any reaction can be defined and added to the biomass function with either the condition that it be maximised or minimised if a single “optimal” solution is desired. Alternatively, and in the most general case, a vector formula_28 can be introduced, which defines the weighted set of reactions that the linear programming model should aim to maximise or minimise,\n\nIn the case of there being only a single separate biomass function/reaction within the stoichiometric matrix formula_28 would simplify to all zeroes with a value of 1 (or any non-zero value) in the position corresponding to that biomass function. Where there were multiple separate objective functions formula_28 would simplify to all zeroes with weighted values in the positions corresponding to all objective functions.\n\nThe analysis of the null space of matrices is implemented in software packages specialized for matrix operations such as Matlab and Octave. Determination of the null space of formula_32 tells us all the possible collections of flux vectors (or linear combinations thereof) that balance fluxes within the biological network. The advantage of this approach becomes evident in biological systems which are described by differential equation systems with many unknowns. The velocities in the differential equations above - formula_33 and formula_34 - are dependent on the reaction rates of the underlying equations. The velocities are generally taken from the Michaelis–Menten kinetic theory, which involves the kinetic parameters of the enzymes catalyzing the reactions and the concentration of the metabolites themselves. Isolating enzymes from living organisms and measuring their kinetic parameters is a difficult task, as is measuring the internal concentrations and diffusion constants of metabolites within an organism. Therefore, the differential equation approach to metabolic modeling is beyond the current scope of science for all but the most studied organisms. FBA avoids this impediment by applying the homeostatic assumption, which is a reasonably approximate description of biological systems.\n\nAlthough FBA avoids that biological obstacle, the mathematical issue of a large solution space remains. FBA has a two-fold purpose. Accurately representing the biological limits of the system and returning the flux distribution closest to the natural fluxes within the target system/organism. Certain biological principles can help overcome the mathematical difficulties. While the stoichiometric matrix is almost always under-determined initially (meaning that the solution space to formula_35 is very large), the size of the solution space can be reduced and be made more reflective of the biology of the problem through the application of certain constraints on the solutions.\n\nThe success of FBA and the realization of its limitations has led to extensions that attempt to mediate the limitations of the technique.\n\nThe optimal solution to the flux-balance problem is rarely unique with many possible, and equally optimal, solutions existing. Flux variability analysis (FVA), built into some analysis software, returns the boundaries for the fluxes through each reaction that can, paired with the right combination of other fluxes, estimate the optimal solution.\n\nReactions which can support a low variability of fluxes through them are likely to be of a higher importance to an organism and FVA is a promising technique for the identification of reactions that are important.\n\nWhen simulating knockouts or growth on media, FBA gives the final steady-state flux distribution. This final steady state is reached in varying time-scales. For example, the predicted growth rate of \"E. coli\" on glycerol as the primary carbon source did not match the FBA predictions; however, on sub-culturing for 40 days or 700 generations, the growth rate adaptively evolved to match the FBA prediction.\n\nSometimes it is of interest to find out what is the immediate effect of a perturbation or knockout, since it takes time for regulatory changes to occur and for the organism to re-organize fluxes to optimally utilize a different carbon source or circumvent the effect of the knockout. MOMA predicts the immediate sub-optimal flux distribution following the perturbation by minimizing the distance (Euclidean) between the wild-type FBA flux distribution and the mutant flux distribution using quadratic programming. This yields an optimization problem of the form.\n\nformula_36\n\nwhere formula_37 represents the wild-type (or unperturbed state) flux distribution and formula_38 represents the flux distribution on gene deletion that is to be solved for. This simplifies to:\n\nformula_39\n\nThis is the MOMA solution which represents the flux distribution immediately post-perturbation.\n\nROOM attempts to improve the prediction of the metabolic state of an organism after a gene knockout. It follows the same premise as MOMA that an organism would try to restore a flux distribution as close as possible to the wild-type after a knockout. However it further hypothesizes that this steady state would be reached through a series of transient metabolic changes by the regulatory network and that the organism would try to minimize the number of regulatory changes required to reach the wild-type state. Instead of using a distance metric minimization however it uses a mixed integer linear programming method.\n\nDynamic FBA attempts to add the ability for models to change over time, thus in some ways avoiding the strict steady state condition of pure FBA. Typically the technique involves running an FBA simulation, changing the model based on the outputs of that simulation, and rerunning the simulation. By repeating this process an element of feedback is achieved over time.\n\nFBA provides a less simplistic analysis than Choke Point Analysis while requiring far less information on reaction rates and a much less complete network reconstruction than a full dynamic simulation would require. In filling this niche, FBA has been shown to be a very useful technique for analysis of the metabolic capabilities of cellular systems.\n\nUnlike choke point analysis which only considers points in the network where metabolites are produced but not consumed or vice versa, FBA is a true form of metabolic network modelling because it considers the metabolic network as a single complete entity (the stoichiometric matrix) at all stages of analysis. This means that network effects, such as chemical reactions in distant pathways affecting each other, can be reproduced in the model. The upside to the inability of choke point analysis to simulate network effects is that it considers each reaction within a network in isolation and thus can suggest important reactions in a network even if a network is highly fragmented and contains many gaps.\n\nUnlike dynamic metabolic simulation, FBA assumes that the internal concentration of metabolites within a system stays constant over time and thus is unable to provide anything other than steady-state solutions. It is unlikely that FBA could, for example, simulate the functioning of a nerve cell. Since the internal concentration of metabolites is not considered within a model, it is possible that an FBA solution could contain metabolites at a concentration too high to be biologically acceptable. This is a problem that dynamic metabolic simulations would probably avoid. One advantage of the simplicity of FBA over dynamic simulations is that they are far less computationally expensive, allowing the simulation of large numbers of perturbations to the network. A second advantage is that the reconstructed model can be substantially simpler by avoiding the need to consider enzyme rates and the effect of complex interactions on enzyme kinetics.\n"}
{"id": "11305204", "url": "https://en.wikipedia.org/wiki?curid=11305204", "title": "Frederic Rousseau", "text": "Frederic Rousseau\n\nFrederic Rousseau is a Flemish Belgian molecular biologist and researcher at the KU Leuven (Leuven, Belgium). Together with Joost Schymkowitz he is group leader at the VIB Switch Laboratory, KU Leuven. His research interest is on essential cellular processes where functional regulation is governed by protein conformational switches that have to be actively controlled to ensure cell viability\n\nHe obtained a PhD at the University of Cambridge (Cambridge, United Kingdom) in 2001. He did a post-doctorate work at the EMBL in Heidelberg Germany from 2001 until 2003. Rousseau is a VIB Group leader since 2003.\n\n"}
{"id": "3898328", "url": "https://en.wikipedia.org/wiki?curid=3898328", "title": "Frostwork", "text": "Frostwork\n\nIn geology, frostwork is a type of speleothem (cave formation) with acicular (\"needle-like\") growths almost always composed of aragonite (a polymorph of calcite) or calcite replaced aragonite. It is a variety of anthodite. In some caves frostwork may grow on top of cave popcorn or boxwork.\n\nIn architecture frost-work or frostwork refers to a style of rustication carved with a vertically-oriented pattern evoking hanging pond-weed or algae, or icicles. It is mainly found in garden architecture, where water is to flow over or near the surface. Other decorative arts may use the term for other decorative patterns imitating frost or ice.\nThe origin of frostwork is somewhat controversial. Formation of cave frostwork has been attributed to moist, circulating air which, containing dissolved calcium carbonate, drifted against rock surfaces and coated them with the delicate crystals. Frostwork has also been attributed to water seepage from cave passageways in which there are relatively high evaporation rates.\n\nNotable frostwork deposits are found in a number of caves in the Black Hills region of South Dakota, USA, most notable in Wind Cave National Park and Jewel Cave National Monument and Timpanogos Cave in Utah. Perhaps the most extensive displays known are found in Lechuguilla Cave, New Mexico, USA.\n\n"}
{"id": "198572", "url": "https://en.wikipedia.org/wiki?curid=198572", "title": "Georg Wilhelm Steller", "text": "Georg Wilhelm Steller\n\nGeorg Wilhelm Steller (10 March 1709 – 14 November 1746) was a German botanist, zoologist, physician and explorer, who worked in Russia and is considered a pioneer of Alaskan natural history.\n\nSteller was born in Windsheim, near Nuremberg in Germany, son to a Lutheran cantor named Johann Jakob Stöhler (after 1715, Stöller), and studied at the University of Wittenberg. He then traveled to Russia as a physician on a troop ship returning home with the wounded. He arrived in Russia in November 1734. He met the naturalist Daniel Gottlieb Messerschmidt (1685–1735) at the Imperial Academy of Sciences. Two years after Messerschmidt's death, Steller married his widow and acquired notes from his travels in Siberia not handed over to the Academy.\n\nSteller knew about Vitus Bering’s Second Kamchatka Expedition, which had left Saint Petersburg in February 1733. He volunteered to join it and was accepted. He then left St Petersburg in January 1738 with his wife, who decided to stay in Moscow and go no farther. Steller met Johann Georg Gmelin in Yeniseisk in January 1739. Gmelin recommended that Steller take his place in the planned exploration of Kamchatka. Steller embraced that role and finally reached Okhotsk and the main expedition in March 1740 as Bering's ships, the \"St. Peter\" and \"St. Paul\", were nearing completion.\nIn September 1740, the expedition sailed to the Kamchatka Peninsula with Bering and his two expeditionary vessels sailing around the peninsula's south tip and up to Avacha Bay on the Pacific coast. Steller went ashore on the east coast of Kamchatka to spend the winter in Bolsherechye, where he helped to organize a local school and began exploring Kamchatka. When Bering summoned him to join the voyage in search of America and the strait between the two continents, serving in the role of scientist and physician, Steller crossed the peninsula by dog sled. After Bering's \"St. Peter\" was separated from its sister ship the \"St. Paul\" in a storm, Bering continued to sail east, expecting to find land soon. Steller, reading sea currents and flotsam and wildlife, insisted they should sail northeast. After considerable time lost, they turned northeast and made landfall in Alaska at Kayak Island on Monday 20 July 1741. Bering wanted to stay only long enough to take on fresh water. Steller argued Captain Bering into giving him more time for land exploration and was granted 10 hours. During this time, as the first non-native to have set foot upon Alaskan soil, Steller became the first European naturalist to describe a number of North American plants and animals, including a jay later named Steller's jay.\n\nOf the six species of birds and mammals that Steller discovered during the voyage, two are extinct (Steller's sea cow and the spectacled cormorant) and three are endangered or in severe decline (Steller's sea lion, Steller's eider and Steller's sea eagle). The sea cow, in particular, a massive northern relative of the dugong, lasted only 27 years after Steller discovered and named it, a limited population that quickly became victim of overhunting by the Russian crews that followed in Bering's wake.\n\nSteller's jay is one of the few species named after Steller that is not currently endangered. In his brief encounter with the bird, Steller was able to deduce that the jay was kin to the American blue jay, a fact which seemed proof that Alaska was indeed part of North America.\n\nAlthough Steller tried to treat the crew's growing scurvy epidemic with leaves and berries he had gathered, officers scorned his proposal. Steller and his assistant were some of the very few who did not suffer from the ailment. On the return journey, with only 12 members of the crew able to move and the rigging rapidly failing, the expedition was shipwrecked on what later became known as Bering Island. Almost half of the crew had perished from scurvy during the voyage. Steller nursed the survivors, including Bering, but the aging captain could not be saved and died. The remaining men made camp with little food or water, a situation made only worse by frequent raids by Arctic foxes. Despite the hardships the crew endured, Steller studied the flora, fauna, and topography of the island in great detail. Of particular note were the only detailed behavioral and anatomical observations of Steller's sea cow, a large sirenian mammal that once ranged across the Northern Pacific during the Ice Ages, but whose surviving relict population was confined to the shallow kelp beds around the Commander Islands, and which was driven to extinction within 30 years of discovery by Europeans.\n\nBased on these and other observations, Steller later wrote \"De Bestiis Marinis\" (‘On the Beasts of the Sea’), describing the fauna of the island, including the northern fur seal, the sea otter, Steller's sea lion, Steller's sea cow, Steller's eider and the spectacled cormorant. Steller claimed the only recorded sighting of the marine cryptid Steller's sea ape.\n\nIn early 1742 the crew used salvaged material from the St. Peter to construct a new vessel to return to Avacha Bay and nicknamed it \"The Bering\". Steller spent the next two years exploring the Kamchatka peninsula. Because of his sympathies for the native Kamchatkans, he was accused of fomenting rebellion and was recalled to Saint Petersburg. At one point he was put under arrest and made to return to Irkutsk for a hearing. He was freed and again turned west toward St. Petersburg, but along the way he came down with a fever and died at Tyumen.\n\nHis journals, which reached the Academy and were later published by Peter Simon Pallas, were used by other explorers of the North Pacific, including Captain Cook.\n\nGeorg Steller described a number of animals and plants, some of which bear his name, either in the common name or scientific:\n\nThere is a secondary school in Anchorage, Alaska named after him: Steller Secondary School.\n\n"}
{"id": "16805361", "url": "https://en.wikipedia.org/wiki?curid=16805361", "title": "Goeppert-Mayer (crater)", "text": "Goeppert-Mayer (crater)\n\nGoeppert-Mayer is a crater on the planet Venus. It is in diameter and lies above an escarpment at the edge of a ridge belt in Southern Ishtar Terra. West of the crater the scarp has more than one kilometer (0.6 miles) of relief.\n"}
{"id": "52218453", "url": "https://en.wikipedia.org/wiki?curid=52218453", "title": "Gradient-enhanced kriging", "text": "Gradient-enhanced kriging\n\nGradient-Enhanced Kriging (GEK) is a surrogate modeling technique used in engineering. A surrogate model (alternatively known as a metamodel, response surface or emulator) is a prediction of the output of an expensive computer code.\nThis prediction is based on a small number of evaluations of the expensive computer code.\n\nAdjoint solvers are now becoming available in a range of Computational Fluid Dynamics (CFD) solvers, such as Fluent, OpenFOAM, SU2 and US3D. Originally developed for optimization, adjoint solvers are now finding more and more use in uncertainty quantification.\n\nAn adjoint solver allows one to compute the gradient of the quantity of interest with respect to all design parameters at the cost of one additional solve. This, potentially, leads to a linear speedup: the computational cost of constructing an accurate surrogate decrease, and the resulting computational speedup formula_1 scales linearly with the number formula_2 of design parameters.\n\nThe reasoning behind this linear speedup is straightforward. Assume we run formula_3 primal solves and formula_3 adjoint solves, at a total cost of formula_5. This results in formula_6 data; formula_3 values for the quantity of interest and formula_2 partial derivatives in each of the formula_3 gradients. Now assume that each partial derivative provides as much information for our surrogate as a single primal solve. Then, the total cost of getting the same amount of information from primal solves only is formula_6. The speedup is the ratio of these costs:\n\nA linear speedup has been demonstrated for a fluid-structure interaction problem and for a transonic airfoil.\n\nOne issue with adjoint-based gradients in CFD is that they can be particularly noisy.\nWhen derived in a Bayesian framework, GEK allows one to incorporate not only the gradient information, but also the uncertainty in that gradient information.\n\nWhen using GEK one takes the following steps:\n\n\nOnce the surrogate has been constructed it can be used in different ways, for example for surrogate-based Uncertainty Quantification (UQ) or optimization.\n\nIn a Bayesian framework, we use Bayes' Theorem to predict the Kriging mean and covariance conditional on the observations. When using GEK, the observations are usually the results of a number of computer simulations. GEK can be interpreted as a form of Gaussian process regression.\n\nAlong the lines of, we are interested in the output formula_12 of our computer simulation, for which we assume the normal prior probability distribution:\n\nwith prior mean formula_14 and prior covariance matrix formula_15. The observations formula_16 have the normal likelihood:\n\nwith formula_18 the observation matrix and formula_19 the observation error covariance matrix, which contains the observation uncertainties. After applying Bayes' Theorem we obtain a normally distributed posterior probability distribution, with Kriging mean:\n\nand Kriging covariance:\n\nwhere we have the gain matrix:\n\nIn Kriging, the prior covariance matrix formula_15 is generated from a covariance function. One example of a covariance function is the Gaussian covariance:\n\nwhere we sum over the dimensions formula_25 and formula_26 are the input parameters. The hyperparameters formula_14, formula_28 and formula_29 can be estimated from a Maximum Likelihood Estimate (MLE).\nThere are several ways of implementing GEK. The first method, indirect GEK, defines a small but finite stepsize formula_30, and uses the gradient information to append synthetic data to the observations formula_16, see for example. Indirect Kriging is sensitive to the choice of the step-size formula_30 and cannot include observation uncertainties.\n\nDirect GEK is a form of co-Kriging, where we add the gradient information as co-variables. This can be done by modifying the prior covariance formula_15 or by modifying the observation matrix formula_18; both approaches lead to the same GEK predictor. When we construct direct GEK through the prior covariance matrix, we append the partial derivatives to formula_16, and modify the prior covariance matrix formula_15 such that it also contains the derivatives (and second derivatives) of the covariance function, see for example \nThe main advantages of direct GEK over indirect GEK are: 1) we do not have to choose a step-size, 2) we can include observation uncertainties for the gradients in formula_19, and 3) it is less susceptible to poor conditioning of the gain matrix formula_38.\n\nAnother way of arriving at the same direct GEK predictor is to append the partial derivatives to the observations formula_16 and include partial derivative operators in the observation matrix formula_18, see for example.\n\nCurrent gradient-enhanced kriging methods do not scale well with the number of sampling points due to the rapid growth in the size of the correlation matrix, where new information is added for each sampling point in each direction of the design space. Furthermore, they do not scale well with the number of independent variables due to the increase in the number of hyperparameters that needs to be estimated.\nTo address this issue, a new gradient-enhanced surrogate model approach that drastically reduced the number of hyperparameters through the use of the partial-least squares method that maintains accuracy is developed. In addition, this method is able to control the size of the correlation matrix by adding only relevant points defined through the information provided by the partial-least squares method. For more details, see .\nThis approach is implemented into the Surrogate Modeling Toolbox (SMT) in Python (https://github.com/SMTorg/SMT), and it runs on Linux, macOS, and Windows. SMT is distributed under the New BSD license.\n\nAs an example, let us consider the flow over a transonic airfoil. The airfoil is operating at a Mach number of 0.8 and an angle of attack of 1.25 degrees. We assume that the shape of the airfoil is uncertain; the top and the bottom of the airfoil might have shifted up or down due to manufacturing tolerances. In other words, the shape of the airfoil that we are using might be slightly different from the airfoil that we designed.\n\nOn the right we see the reference results for the drag coefficient of the airfoil, based on a large number of CFD simulations. Note that the lowest drag, which corresponds to 'optimal' performance, is close to the undeformed 'baseline' design of the airfoil at (0,0).\n\nAfter designing a sampling plan (indicated by the gray dots) and running the CFD solver at those sample locations, we obtain the Kriging surrogate model. The Kriging surrogate is close to the reference, but perhaps not as close as we would desire.\n\nIn the last figure, we have improved the accuracy of this surrogate model by including the adjoint-based gradient information, indicated by the arrows, and applying GEK.\n\nGEK has found the following applications:\n\n"}
{"id": "50018085", "url": "https://en.wikipedia.org/wiki?curid=50018085", "title": "International Center for Biosaline Agriculture", "text": "International Center for Biosaline Agriculture\n\nICBA (International Center for Biosaline Agriculture) is an international, non-profit agricultural research and development center which focuses on the closely linked issues of water, environment, income, and food security. ICBA conducts research and development programs that aim to improve agricultural productivity and sustainability in marginal environments.\n\nIts headquarters are in Dubai, United Arab Emirates and it is a founding member of the Association of International Research and Development Centers for Agriculture (AIRCA). It is also a member of Middle East and North Africa Network of Water Centers of Excellence and the Asia-Pacific Association of Agricultural Research Institutions (APAARI).\n\nIn 1992, the Islamic Development Bank (IDB) initiated a series of expert consultations which outlined the objectives and activities of the new institute. In November 1992, the IDB Board of Executive Directors approved financing for start-up operations. Subsequent consultations between the Bank and the General Secretariat of the Gulf Cooperation Council led to the selection of the United Arab Emirates as a host country for the new center.\n\nIn 1996, the IDB and the Government of the UAE signed an agreement to establish ICBA as a formal entity. In 1997, the Municipality of Dubai donated 100 hectares of land at Al Ruwayyah, where ICBA's head office is now based. The center became operational in 1999.\n\nICBA’s founding donors were the Islamic Development Bank (IDB), the OPEC Fund for International Development (OFID), the Arab Fund for Economic and Social Development (AFESD), and the Government of the United Arab Emirates (UAE).\n\nCurrently the Center is funded by the three core donors: the Ministry of Climate Change and Environment of the UAE (formerly the Ministry of Environment and Water); the Environment Agency - Abu Dhabi; and the Islamic Development Bank. The Center has additional support from several development agencies and international donors such as the International Fund for Agricultural Development (IFAD), the International Atomic Energy Agency (IAEA), the UAE Government, Dubai and Abu Dhabi Municipalities, and the private sector in Oman and Saudi Arabia.\n\nInitially ICBA focused solely on biosaline environments but its applied research for development now aims to address agricultural challenges in all marginal environments. Its tasks include assessment of natural resources, climate change adaptation, crop productivity and diversification, aquaculture, bioenergy and policy analysis.\n\nICBA works on a number of technological developments, including the use of conventional and non-conventional water (such as saline, treated wastewater, industrial water, agricultural drainage, and seawater); water and land management technologies; and remote sensing and modeling for climate change adaptation.\n\nThe Center conducts research on a variety of food crops which hold promise of being resilient, salt tolerant and water efficient. For example, ICBA has been conducting widespread research on quinoa, a multi-purpose, high-protein grain crop which is drought and salt tolerant. ICBA initiated research on a number of quinoa germplasm accessions since 2006–07. The selections from these trials are being introduced to farmers in several countries, e.g. in the United Arab Emirates in Abu Dhabi.\n\nICBA scientists are also working to increase the salinity tolerance of hardier traditional crops such as date palms, sorghum and millet.\n\nAnother field of research lies with \"Salicornia\" and other halophytes (salt-loving plants) that remove salts from saline soils and water. \"Salicornia’s\" oil-bearing seeds produce biofuel, cosmetic oil and edible vegetable oil. Its straws can also be used as animal fodder, and its vegetable tips can be harvested for human consumption.\nThe geographic scope of ICBA has grown since its inception. It now focusses its work on water-scarce countries of the Middle East and North Africa (MENA) including Gulf Cooperation Council countries (GCC), Central Asia and the Caucasus (CAC) and South and Southwest Asia. It is currently expanding to include countries of sub-Saharan Africa (SSA).\n\n"}
{"id": "8818410", "url": "https://en.wikipedia.org/wiki?curid=8818410", "title": "International Commission on Mathematical Instruction", "text": "International Commission on Mathematical Instruction\n\nThe International Commission on Mathematical Instruction (ICMI) is a commission of the International Mathematical Union and is an internationally acting organization focussing on mathematics education. ICMI was founded in 1908 at the International Congress of Mathematicians (ICM) in Rome and aims to improve teaching standards around the world, through programs, workshops and initiatives and publications. It aims to work a great deal with developing countries, to increase teaching standards and education which can improve life quality and aid the country.\n\nICMI was founded at the ICM, and mathematician Felix Klein was elected first president of the organisation. Henri Fehr and Charles Laisant created the international research journal \"L'Enseignement Mathématique\" in 1899, and from early on this journal became the official organ of ICMI. A bulletin is published twice a year by ICMI, and from December 1995 this bulletin has been available at the organisation's official website, in their 'digital library'.\n\nIn the years between World War I and World War II there was little activity in the organization, but in 1952 ICMI was reconstituted. At this time the organization was reorganized, and it became an official commission of the International Mathematical Union (IMU). As a scientific organization, IMU is a member of the International Council for Science (ICSU). Although ICMI follows the general principles of IMU and ICSU, the organization has a large degree of autonomy.\n\nAll countries that are members of IMU are automatically members of ICMI; membership is also possible for non-IMU members. Currently, there are 90 member states of ICMI. Each member state has the right to appoint a national representative.\n\nAs a commission, ICMI has two main bodies:\n\nTogether, these two constitute the General Assembly (GA) of ICMI. The GA is summoned every four years in connection with the International Congress on Mathematical Education, ICME. The executive committee is appointed by the general assembly of IMU for four-year terms.\n\nThese include multi-national organisations, which are independent from ICMI and have interests in the field of mathematics.\n\nThere are currently four multinational Mathematical Education Societies:\n\n\nAnd six international Study Groups which have obtained affiliation with ICMI:\n\nInternational Congress on Mathematical Education\n\nThe International Congress on Mathematical Education (ICME) is an international event which is held every four years under the auspices of ICMI. The congress looks at the development of mathematical education across the world.\n\nThe last ICME was held in Hamburg in 2016 (link to website:http://icme13.org/). The next 2020 ICME will be held in Shanghai, China\n\nICMI Regional Conferences\n\nICMI sometimes offers financial as well as moral support to facilitate the organisation of regional meetings these have to be related to mathematical education. Precedence goes to less affluent countries. \nAFRICME: The Africa Regional Congress of ICMI on Mathematical Education was launched in 2005 and aims at offering a forum for mathematics educators throughout Africa.\n\nCIAEM: The Conferencia interamericana de educación matemática — Inter-American Conference on Mathematical Education organised by the Comité Interamericano de Educación Matemática - Inter-American Committee on Mathematical Education to promote discussion amongst Latin-American countries.\n\nEARCOME: is the name given to the ICMI-East Asia Regional Conferences in Mathematics Education. The South East Asia Conferences on Mathematics Education (SEACME) series began in 1978 in Manila. In addition there have been two ICMI-China Regional Conferences on Mathematics Education, in Beijing (1991) and Shanghai (1994). The EARCOME series has replaced the SEACME series.\n\nEMF Launched by the French Sub-Commission of ICMI on the occasion of the World Mathematical Year 2000, the series of Espace Mathématique Francophone conferences is built on a notion of \"region\" defined in linguistic rather than geographical terms, French being a common language among participants.\nOther ICMI Regional Conferences occur on a more ad hoc basis. \nApproval of a Conference as an ICMI Regional Conference\nTwo main aspects are that:\n\n• The conference should be genuinely international, and not just a national activity;\n\n• The conference should aim for high standards of scientific quality, with a planning and organizing structure that assures this.\n\nThere are a variety of various publications made by or under the auspices of ICMI, some resulting directly from activities organised by the Commission, examples include: \n• The ICMI Bulletin\n• ICMI News – The ICMI electronic newsletter \n• ICME Proceedings\n• ICMI Studies Publications\n• Proceedings from other ICMI Conferences\n• L’Enseignement Mathématique\n\nEach ICMI Study addresses an issue or topic of particular significance in contemporary mathematics education, and is conducted by an international team of leading scholars and practitioners in that domain. The best contributing professionals from around the world are then invited to a carefully planned and structured international conference/workshop. Beyond the productive interaction and collaborations occasioned by this event, the main product is a Study volume, which are published in the New ICMI Study Series (NISS) by Springer Science+Business Media.\n\nFrom 2000 onwards ICMI has been presenting the Felix Klein Award and the Hans Freudenthal Award. These prizes recognise outstanding achievement in mathematics education research. In 2013 the ICMI Emma Castelnuovo Award for Excellence in the Practice of Mathematics Education was created.\n\n\nICMI Emma Castelnuovo/Award\n\nIn 2013 the ICMI Emma Castelnuovo Award for Excellence in the Practice of Mathematics Education was created. The award recognizes outstanding achievements in the practice of mathematics education. The award is named after Emma Castelnuovo, an Italian mathematics educator born in 1913 to celebrate her 100th birthday and honour her pioneering work.\n\nThe award honours persons, groups, projects, institutions or organizations engaged in the development and implementation of exceptional and influential work in the practice of mathematics education, including: classroom teaching, curriculum development, instructional design (of materials or pedagogical models), teacher preparation programs and/or field projects with a shown influence on schools, districts, regions or countries.\n\nThe award consists of a medal and a certificate accompanied by a citation and will be awarded only once every four years, delivered at the International Congress on Mathematical Education (ICME).\n\nAt each ICME, the medals and certificates of the awards are presented at the Opening Ceremony. Furthermore, the awardees are invited to present special lectures at the Congress.\n\nThe Capacity & Networking Project is an international initiative to support mathematics education in the developing world and is a joint initiative of the international bodies of mathematicians (IMU) and mathematics educators (ICMI) in conjunction with UNESCO and International Congress of Industrial and Applied Mathematics, ICIAM. The project is a response to the report: Current Challenges in Basic Mathematics Education (UNESCO, 2011). CANP aims to enhance mathematics education at all levels in developing countries so that their people are capable of meeting the challenges these countries face. The first program was held in Bamako in Mali in September, 2011. The follow up meeting took place in Senegal in 2012. The second program was held in Costa Rica in 2012 and created a successful regional network.\n\nThe Klein Project was launched in 2008 and aims to support mathematics teachers to connect the mathematics they teach, to the field of mathematics, while taking into account the evolution of this field over the last century. The Klein Project is inspired by Felix Klein’s famous book, Elementary Mathematics from an Advanced\nStandpoint, published in 1908 and 1909. The project will have two main outputs: a book published in several languages and a blog which includes many materials for mathematics teachers to be used in the class room.\n\n"}
{"id": "1018499", "url": "https://en.wikipedia.org/wiki?curid=1018499", "title": "International Near-Earth Asteroid Survey", "text": "International Near-Earth Asteroid Survey\n\nThe International Near-Earth Asteroid Survey (INAS) was an astronomical survey, organized and co-ordinated by prolific American astronomer Eleanor Helin during the 1980s. It is considered to be the international extension of the Planet-Crossing Asteroid Survey (PCAS). While PCAS operated exclusively from the U.S. Palomar Observatory in California, INAS attempted to encourage and stimulate worldwide interest in asteroids, and to expand the sky coverage and the discovery and recovery of near-Earth objects around the world.\n\nThe IAU's Minor Planet Center credits INAS with the discovery of 8 minor planets in 1986 (compared to 20 discoveries made by PCAS during 1993–1994). One of the discoveries was the 7-kilometer sized main-belt asteroid 4121 Carlin.\n\n"}
{"id": "9162613", "url": "https://en.wikipedia.org/wiki?curid=9162613", "title": "Jerry March", "text": "Jerry March\n\nJerry March, Ph.D. (August 1, 1929 – December 25, 1997) was an American organic chemist and a professor of chemistry at Adelphi University. \n\nMarch authored the \"March's Advanced Organic Chemistry\" text, which is considered to be a pillar of graduate-level organic chemistry texts. The book was prepared in its fifth edition at the time of his death. \n\n"}
{"id": "165320", "url": "https://en.wikipedia.org/wiki?curid=165320", "title": "Jodrell Bank Observatory", "text": "Jodrell Bank Observatory\n\nThe Jodrell Bank Observatory (originally the Jodrell Bank Experimental Station and from 1966 to 1999, the Nuffield Radio Astronomy Laboratories; ) hosts a number of radio telescopes, and is part of the Jodrell Bank Centre for Astrophysics at the University of Manchester. The observatory was established in 1945 by Bernard Lovell, a radio astronomer at the University of Manchester to investigate cosmic rays after his work on radar during the Second World War. It has since played an important role in the research of meteors, quasars, pulsars, masers and gravitational lenses, and was heavily involved with the tracking of space probes at the start of the Space Age. The managing director of the observatory is Professor Simon Garrington.\n\nThe main telescope at the observatory is the Lovell Telescope, which is the third largest steerable radio telescope in the world. There are three other active telescopes at the observatory; the Mark II, and and 7 m diameter radio telescopes. Jodrell Bank Observatory is the base of the Multi-Element Radio Linked Interferometer Network (MERLIN), a National Facility run by the University of Manchester on behalf of the Science and Technology Facilities Council.\n\nThe observatory, the Jodrell Bank Visitor Centre and an arboretum, are in the civil parish of Lower Withington and the rest of the site is in Goostrey civil parish, near Goostrey and Holmes Chapel, Cheshire, North West England. The observatory is reached from the A535. The Crewe to Manchester Line passes right by the site, and Goostrey station is a short distance away. In 2018, the observatory became a candidate for UNESCO World Heritage site status.\n\nJodrell Bank was first used for academic purposes in 1939 when the University of Manchester's Department of Botany purchased three fields from the Leighs. It is named from a nearby rise in the ground, Jodrell Bank, which was named after William Jauderell whose descendants lived at the mansion that is now Terra Nova School. The site was extended in 1952 by the purchase of a farm from George Massey on which the Lovell Telescope was built.\n\nThe site was first used for astrophysics in 1945, when Bernard Lovell used some equipment left over from World War II, including a gun laying radar, to investigate cosmic rays. The equipment was a GL II radar system working at a wavelength of 4.2 m, provided by J. S. Hey. He intended to use the equipment in Manchester but electrical interference from the trams on Oxford Road prevented him from doing so. He moved the equipment to Jodrell Bank, south of the city, on 10 December 1945. Lovell's main research was transient radio echoes, which he confirmed were from ionized meteor trails by October 1946. The first staff were Alf Dean and Frank Foden who observed meteors observed with the naked eye while Lovell observed the electromagnetic signal using equipment. The first time Lovell turned the radar on – 14 December 1945 – the Geminids meteor shower was at a maximum.\n\nOver the next few years, Lovell accumulated more ex-military radio hardware, including a portable cabin, known as a \"Park Royal\" in the military (see Park Royal Vehicles). The first permanent building was near to the cabin and was named after it.\n\nJodrell Bank is primarily used for investigating radio waves from the planets and stars.\n\nA searchlight was loaned to Jodrell Bank in 1946 by the army; a broadside array was constructed on its mount by J. Clegg. It consisted of a number of Yagi antennas. It was used for astronomical observations in October 1946.\n\nOn 9 and 10 October 1946, the telescope observed ionisation in the atmosphere caused by meteors in the Giacobinids meteor shower. When the antenna was turned by 90 degrees at the maximum of the shower, the number of detections dropped to the background level, proving that the transient signals detected by radar were from meteors. The telescope was then used to determine the radiant points for meteors. This was possible as the echo rate is at a minimum at the radiant point, and a maximum at 90 degrees to it. The telescope and other receivers on the site, studied auroral streamers that were visible in early August 1947.\n\nThe Transit Telescope was a parabolic reflector zenith telescope built in 1947. At the time, it was the world's largest radio telescope. It consisted of a wire mesh suspended from a ring of scaffold poles, which focussed radio signals on a focal point above the ground. The telescope mainly looked directly upwards, but the direction of the beam could be changed by small amounts by tilting the mast to change the position of the focal point. The focal mast was changed from timber to steel before construction was complete.\n\nThe telescope was replaced by the steerable Lovell Telescope and the Mark II telescope was subsequently built at the same location.\n\nThe telescope could map a ± 15-degree strip around the zenith at 72 and 160 MHz, with a resolution at 160 MHz of 1 degree. It discovered radio noise from the Great Nebula in Andromeda—the first definite detection of an extragalactic radio source—and the remains of Tycho's Supernova in the radio frequency; at the time it had not been discovered by optical astronomy.\n\nThe \"Mark I\" telescope, now known as the Lovell Telescope, was the world's largest steerable dish radio telescope, in diameter, when it was constructed in 1957; it is now the third largest, after the Green Bank telescope in West Virginia and the Effelsberg telescope in Germany. Part of the gun turret mechanisms from the battleships \"HMS Revenge\" and \"HMS Royal Sovereign\" were reused in the telescope's motor system. The telescope became operational in mid-1957, in time for the launch of the Soviet Union's Sputnik 1, the world's first artificial satellite. The telescope was the only one able to track Sputnik's booster rocket by radar; first locating it just before midnight on 12 October 1957.\n\nIn the following years, the telescope tracked various space probes. Between 11 March and 12 June 1960, it tracked the United States' NASA-launched Pioneer 5 probe. The telescope sent commands to the probe, including those to separate it from its carrier rocket and turn on its more powerful transmitter when the probe was eight million miles away. It received data from the probe, the only telescope in the world capable of doing so. In February 1966, Jodrell Bank was asked by the Soviet Union to track its unmanned moon lander Luna 9 and recorded on its facsimile transmission of photographs from the moon's surface. The photographs were sent to the British press and published before the Soviets made them public.\n\nIn 1969, the Soviet Union's Luna 15 was also tracked. A recording of the moment when Jodrell Bank's scientists observed the mission was released on 3 July 2009.\n\nWith the support of Sir Bernard Lovell, the telescope tracked Russian satellites. Satellite and space probe observations were shared with the US Department of Defense satellite tracking research and development activity at Project Space Track.\n\nTracking space probes only took a fraction of the Lovell telescope's observing time and the remainder used for scientific observations including using radar to measure the distance to the moon and to Venus; observations of astrophysical masers around star-forming regions and giant stars; observations of pulsars (including the discovery of millisecond pulsars and the first pulsar in a globular cluster); observations of quasars and gravitational lenses (including the detection of the first gravitational lens and the first Einstein ring). The telescope has been used for SETI observations.\n\nThe Mark II is an elliptical radio telescope, with a major axis and a minor axis of . It was constructed in 1964. As well as operating as a standalone telescope, it has been used as an interferometer with the Lovell Telescope, and is now primarily used as part of the MERLIN project.\nThe Mark III telescope, the same size as the Mark II, was constructed to be transportable but it was never moved from Wardle, near Nantwich, where it was used as part of MERLIN. It was built in 1966 and decommissioned in 1996.\n\nThe Mark IV, V and VA telescope proposals were put forward in the 1960s through to the 1980s to build even larger radio telescopes.\n\nThe Mark IV proposal was for a diameter standalone telescope, built as a national project.\n\nThe Mark V proposal was for a moveable telescope. The concept of this proposal was for a telescope on a 3/4-mile long railway line adjoining Jodrell Bank but concerns about future levels of interference meant that a site in Wales would have been preferable. Design proposals by Husband and Co and Freeman Fox, who had designed the Parkes Observatory telescope were put forward.\n\nThe Mark VA was similar to the Mark V but with a smaller dish of and a design using prestressed concrete, similar to the Mark II (the previous two designs more closely resembled the Lovell telescope).\n\nNone of the proposed telescopes was constructed, although design studies were carried out and scale models were made, partly because of the changing political climate, partly to the financial constraints of astronomical research in the UK and it became necessary to upgrade the Lovell Telescope to the Mark IA, which overran in terms of cost.\n\nA 50 ft (15 m) alt-azimuth dish was constructed in 1964 for astronomical research and to track the Zond 1, Zond 2, Ranger 6 and Ranger 7 space probes and Apollo 11. After an accident that irreparably damaged the 50 ft telescope's surface, it was demolished in 1982 and replaced with a more accurate telescope, the \"42 ft\". The 42 ft (12.8 m) dish is mainly used to observe pulsars, and continually monitors the Crab Pulsar.\n\nWhen the 42 ft was installed, a smaller dish, the \"7 m\" (actually 6.4 m, or 21 ft, in diameter) was installed and is used for undergraduate teaching. The 42 ft and 7 m telescopes were originally used at the Woomera Rocket Testing Range in Australia. The 7 m was originally constructed in 1970 by Marconi Company.\n\nA Polar Axis telescope was built in 1962. It had a circular 50 ft (15.2 m) dish on a polar mount, and was mostly used for moon radar experiments. It has been decommissioned. An reflecting optical telescope was donated to the observatory in 1951 but was not used much, and was donated to the Salford Astronomical Society around 1971.\n\nThe Multi-Element Radio Linked Interferometer Network (MERLIN) is an array of radio telescopes spread across England and the Welsh borders. The array is run from Jodrell Bank on behalf of the Science and Technology Facilities Council as a National Facility. The array consists of up to seven radio telescopes and includes the Lovell Telescope, the Mark II, Cambridge, Defford, Knockin, Darnhall, and Pickmere (previously known as Tabley). The longest baseline is and MERLIN can operate at frequencies between 151 MHz and 24 GHz. At a wavelength of 6 cm (5 GHz frequency), MERLIN has a resolution of 50 milliarcseconds which is comparable to that of the HST at optical wavelengths.\n\nJodrell Bank has been involved with Very Long Baseline Interferometry (VLBI) since the late 1960s; the Lovell telescope took part in the first transatlantic interferometer experiment in 1968, with other telescopes at Algonquin and Penticton in Canada. The Lovell Telescope and the Mark II telescopes are regularly used for VLBI with telescopes across Europe (the European VLBI Network), giving a resolution of around 0.001 arcseconds.\n\nIn April 2011, Jodrell Bank was named as the location of the control centre for the planned Square Kilometre Array, or SKA Project Office (SPO). The SKA is planned by a collaboration of 20 countries and when completed, is intended to be the most powerful radio telescope ever built. In April 2015 it was announced that Jodrell Bank would be the permanent home of the SKA headquarters for the period of operation expected for the telescope (over 50 years).\n\nThe Jodrell Bank Centre for Astrophysics, of which the Observatory is a part, is one of the largest astrophysics research groups in the UK. About half of the research of the group is in the area of radio astronomy—including research into pulsars, the Cosmic Microwave Background Radiation, gravitational lenses, active galaxies and astrophysical masers. The group also carries out research at different wavelengths, looking into star formation and evolution, planetary nebulae and astrochemistry.\n\nThe first director of Jodrell Bank was Bernard Lovell, who established the observatory in 1945. He was succeeded in 1980 by Sir Francis Graham-Smith, followed by Professor Rod Davies around 1990 and Professor Andrew Lyne in 1999. Professor Phil Diamond took over the role on 1 October 2006, at the time when the Jodrell Bank Centre for Astrophysics was formed. Prof Ralph Spencer was Acting Director during 2009 and 2010. In October 2010, Prof. Albert Zijlstra became Director of the Jodrell Bank Centre for Astrophysics. Professor Lucio Piccirillo was the Director of the Observatory from Oct 2010 to Oct 2011 when Prof Simon Garrington became its managing director.\n\nIn May 2017 Jodrell Bank entered into a partnership with the Breakthrough Listen initiative and will share information with Jodrell Bank’s team, who wish to conduct an independent SETI search via its 76-m radio telescope and e-MERLIN array.\n\nThere is an active development programme researching and constructing telescope receivers and instrumentation. The observatory has been involved in the construction of several Cosmic Microwave Background experiments, including the Tenerife Experiment, which ran from the 1980s to 2000, and the amplifiers and cryostats for the Very Small Array. It has also constructed the front-end modules of the 30 and 44 GHz receivers for the Planck spacecraft. Receivers were also designed at Jodrell Bank for the Parkes Telescope in Australia.\n\nA visitors' centre opened on 19 April 1971 by the Duke of Devonshire, attracted around 120,000 visitors per year. It covered the history of Jodrell Bank and had a 3D theatre hosting simulated trips to Mars.\nAsbestos in the visitors' centre buildings led to its demolition in 2003 leaving a remnant of its far end. A marquee was set up in its grounds while a new science centre was planned. The plans were shelved when Victoria University of Manchester and UMIST merged to become the University of Manchester in 2004, leaving the interim centre, which received around 70,000 visitors a year.\nIn October 2010, work on a new visitor centre started and the Jodrell Bank Discovery Centre opened on 11 April 2011. It includes an entrance building, the Planet Pavilion, a Space Pavilion for exhibitions and events, a glass-walled cafe with a view of the Lovell Telescope and an outdoor dining area, an education space, and landscaped gardens including the Galaxy Maze. A large orrery was installed in 2013.\n\nThe visitor centre is open to daily and organises public outreach events, including public lectures, star parties, and \"ask an astronomer\" sessions.\n\nA path around the Lovell telescope is approximately 20 m from the telescope's outer railway, information boards explain how the telescope works and the research that is done with it.\n\nThe arboretum, created in 1972, houses the UK's national collections of crab apple \"Malus\" and mountain ash \"Sorbus\" species, and the Heather Society's \"Calluna\" collection. The arboretum also has a small scale model of the solar system, the scale is approximately 1:5,000,000,000. At Jodrell Bank, as part of the \"SpacedOut\" project, is the Sun in a 1:15,000,000 scale model of the solar system covering Britain.\n\nOn 7 July 2010, it was announced that the observatory was being considered for the 2011 United Kingdom Tentative List for World Heritage Site status. It was announced on 22 March 2011 that it was on the UK government's shortlist. In January 2018, it became the UK's candidate for World Heritage status.\n\nIn July 2011 the visitor centre and observatory hosted \"Jodrell Bank Live\" – a rock concert with bands including The Flaming Lips, British Sea Power, Wave Machines, OK GO and Alice Gold. On 23 July 2012 Elbow performed live at the observatory and filmed a documentary of the event and the facility which was released as a live CD/DVD of the concert.\n\nOn 31 August 2013 Jodrell Bank hosted a concert performed by the Halle Orchestra to commemorate what would have been Lovell's 100th birthday. As well as a number of operatic performances during the day, the evening Halle performance saw numbers such as themes from Star Trek, Star Wars and Doctor Who among others. The main Lovell telescope was rotated to face the onlooking crowd and used as a huge projection screen showing various animated planetary effects. During the interval the 'screen' was used to show a history of Lovell's work and Jodrell Bank.\n\nThere is an astronomy podcast from the observatory, named \"The Jodcast\". The BBC television programme \"Stargazing Live\" is hosted in the control room of the observatory. The programme has had four series, in January 2011, 2012, 2013 and 2014.\n\nIn July 2018, the observatory will hold Bluedot, a music and science festival, featuring Public Service Broadcasting, The Chemical Brothers, Jim Al-Khalili and Richard Dawkins.\n\nJodrell Bank's visitor centre had a planetarium until 2003, when it and much else was demolished because of risk of asbestos. The planetarium projector was one-ended, with one star ball, and it could not show stars near the south pole.\n\nCurrently (as at 2018) they only have a small inflatable planetarium dome.\n\nA new planetarium with a computer-generated display will be built in the arboretum, starting in February or March 2019.\n\nOn 3 March 2008, it was reported that Britain's Science and Technology Facilities Council (STFC), faced with an £80 million shortfall in its budget, was considering withdrawing its planned £2.7 million annual funding of Jodrell Bank's e-MERLIN project. The project, which aims to replace the microwave links between Jodrell Bank and a number of other radio telescopes with high-bandwidth fibre-optic cables, greatly increasing the sensitivity of observations, is seen as critical to the survival of the establishment in its present form. Sir Bernard Lovell was quoted as saying \"It will be a disaster … The fate of the Jodrell Bank telescope is bound up with the fate of e-MERLIN. I don't think the establishment can survive if the e-MERLIN funding is cut\".\n\nOn Monday 14 April 2008, Cheshire's 106.9 Silk FM unveiled to its listeners their own campaign song to save Jodrell Bank, entitled \"The Jodrell Bank Song\" and sung by a group dubbed \"The Astronomers\". Along with the song, the Silk FM team also produced a music video filmed in front of the iconic Lovell telescope. Silk FM released the song for download from Monday 21 April 2008. All proceeds went towards saving Jodrell Bank.\n\nOn 9 July 2008, it was reported that, following an independent review, the STFC had reversed its initial position and would after all guarantee funding of £2.5 million annually for three years.\n\nJodrell Bank has been mentioned in several popular works of fiction, including Doctor Who (\"The Tenth Planet\", \"Remembrance of the Daleks\", \"The Poison Sky\", \"The Eleventh Hour\"). It was intended to be a filming location for \"Logopolis\" (Tom Baker's final \"Doctor Who\" serial) but budget restrictions prevented this and another location with a superimposed model of a radio telescope was used instead. It was also mentioned in \"The Hitchhiker's Guide to the Galaxy\" (as well as \"The Hitchhiker's Guide to the Galaxy\" film), \"The Creeping Terror\" and \"Meteor\". Jodrell Bank also featured heavily in the music video to \"Electric Light Orchestra\"'s 1983 single \"Secret Messages\". The Prefab Sprout song Technique (from debut album Swoon) opens with the line \"Her husband works at Jodrell Bank/He's home late in the morning\".\n\nThe Observatory is the site of several episodes in the novel \"Boneland\", by Alan Garner (2012), and the central character, Colin Whisterfield, is an astrophysicist on its staff.\n\nSince 13 July 1988 the Lovell Telescope has been designated as a Grade I listed building. On 10 July 2017 the Mark II Telescope was also designated at the same grade. On the same date five other buildings on the site were designated at Grade II; namely the Searchlight Telescope, the Control Building, the Park Royal Building, the Electrical Workshop, and the Link Hut. Grade I is the highest of the three grades of listing, and is applied to buildings that are of \"exceptional interest\", and Grade II, the lowest grade, is applied to buildings \"of special interest\".\n\n\n\n"}
{"id": "6076253", "url": "https://en.wikipedia.org/wiki?curid=6076253", "title": "Joseph Altman", "text": "Joseph Altman\n\nJoseph Altman (1925 – 2016) was an American biologist who worked in the field of neurobiology.\n\nBorn in Hungary to a Jewish family, he survived the Holocaust and migrated with his family via Germany and Australia to the United States. In these places, he sought employment as a librarian and used the opportunity to inform himself reading books about psychology, human behavior, psychoanalysis, and human brain structure. In New York, where he married his first wife Elizabeth Altman, he became a graduate student in psychology in the laboratory of Hans-Lukas Teuber, earning a PhD.,in 1959 from New York University. That degree launched his scientific career, first as a postdoctoral fellow at Columbia University, next at the Massachusetts Institute of Technology, and finally at Purdue University. During his career, he collaborated closely with his second wife, Shirley A. Bayer. From the early 1960s to 2016, he published many articles in peer-reviewed journals, books, monographs, and online free books that emphasized developmental processes in brain anatomy and function.\n\nJoseph Altman discovered adult neurogenesis, the creation of new neurons in the adult brain, in the 1960s. As an independent investigator at MIT, his results were largely ignored in favor of Pasko Rakic's findings that neurogenesis is limited to pre-natal development. By the late 1990s, a paradigm shift had occurred. The fact that the brain can create new neurons even into adulthood was rediscovered by Elizabeth Gould in 1999, leading it to be one of the hottest fields in neuroscience. Adult neurogenesis has recently been proven to occur in the dentate gyrus, olfactory bulb and striatum through the measurement of Carbon-14—the levels of which changed during nuclear bomb testing throughout the 20th century—in postmortem human brains.\n\n\n\n"}
{"id": "52012421", "url": "https://en.wikipedia.org/wiki?curid=52012421", "title": "K band (infrared)", "text": "K band (infrared)\n\nIn infrared astronomy, the K band is an atmospheric transmission window centered on 2.2 μm (in the near-infrared 136 THz range).\n"}
{"id": "37899152", "url": "https://en.wikipedia.org/wiki?curid=37899152", "title": "List of Ciconiiformes by population", "text": "List of Ciconiiformes by population\n\nThis is a list of Ciconiiformes species by global population. While numbers are estimates, they have been made by the experts in their fields. For more information on how these estimates were ascertained, see Wikipedia's articles on population biology and population ecology.\n\nThis list is incomprehensive, as not all Ciconiiformes have had their numbers quantified. The classification of this order of birds is currently in flux; the classification for this list is aligned with the IUCN's current position, but may change in the future.\nFurthermore, some species currently listed below are no longer considered Ciconiformes; they have been reclassified under Pelecaniformes (e.g. the Black-crowned night heron, listed below). Click the species page for a more definitive classification.\n"}
{"id": "59174283", "url": "https://en.wikipedia.org/wiki?curid=59174283", "title": "List of higher viral taxa", "text": "List of higher viral taxa\n\nThis is an alphabetical list of biological virus higher taxa; it includes those realms, subrealms, kingdoms, subkingdoms, phyla, subphyla, classes, subclasses, orders, and suborders listed by the ICTV 2018 report.\n\nFor a list of individual species, see List of virus species.\n\nFor a list of virus genera, see List of genera of viruses.\n\nFor a list of family-level viral taxa, see List of virus families and subfamilies.\n\nFor a taxonomic list, see Taxonomic list of viruses.\n\n\n"}
{"id": "1278990", "url": "https://en.wikipedia.org/wiki?curid=1278990", "title": "List of the 72 names on the Eiffel Tower", "text": "List of the 72 names on the Eiffel Tower\n\nOn the Eiffel Tower, seventy-two names of French scientists, engineers, and mathematicians are engraved in recognition of their contributions. Gustave Eiffel chose this \"invocation of science\" because of his concern over the protests against the tower. The engravings are found on the sides of the tower under the first balcony, in letters about 60 cm high, and originally painted in gold.\nThe engraving was painted over at the beginning of the twentieth century and restored in 1986–1987 by Société Nouvelle d'exploitation de la Tour Eiffel, the company contracted by the city of Paris to operate the Tower. The repainting of 2010–2011 restored the letters to their original gold colour.\n\nThere are also names of the engineers who helped build the tower and design its architecture on the top of the tower on a plaque, where a laboratory was built as well.\n\nThe list is split in four parts (for each side of the tower). The sides have been named after the parts of Paris that each side faces:\n\n\n\nIn the table below are all the names on the four sides.\n\nThe list contains no women. The list has been criticized for excluding the name of Sophie Germain, a noted French mathematician whose work on the theory of elasticity was used in the construction of the tower itself. In 1913, John Augustine Zahm suggested that Germain was excluded because she was a woman.\n\n14 hydraulic engineers and scholars are listed on the Eiffel Tower. Eiffel acknowledged most of the leading scientists in the field. Henri Philibert Gaspard Darcy is missing; some of his work did not come into wide use until the 20th century. Also missing are Antoine Chézy, who was less famous; Joseph Valentin Boussinesq, who was early in his career at the time; and mathematician Évariste Galois.\n\n\n"}
{"id": "6565890", "url": "https://en.wikipedia.org/wiki?curid=6565890", "title": "Maximum density", "text": "Maximum density\n\nThe maximum density of a substance is the highest attainable density of the substance under given conditions.\n\nAlmost all known substances undergo thermal expansion in response to heating, meaning that a given mass of substance contracts to a low volume at low temperatures, when little thermal energy is present. Substances, especially fluids in which intermolecular forces are weak, also undergo compression upon the application of pressure. Nearly all substances therefore reach a density maximum at very low temperatures and very high pressures, characteristic properties of the solid state of matter.\n\nAn especially notable irregular maximum density is that of water, which reaches a density peak at . This has important ramifications in Earth's ecosystem.\n\n"}
{"id": "14241428", "url": "https://en.wikipedia.org/wiki?curid=14241428", "title": "Mineralization (biology)", "text": "Mineralization (biology)\n\nIn biology, mineralization refers to a process where an inorganic substance precipitates in an organic matrix. This may be due to normal biological processes that take place during the life of an organism such as the formation of bones, egg shells, teeth, coral, and other exoskeletons. This term may also refer to abnormal processes that result in kidney and gall stones.\n\nMineralization can be subdivided into different categories depending on the following: the organisms or processes that create chemical conditions necessary for mineral formation, the origin of the substrate at the site of mineral precipitation, and the degree of control that the substrate has on crystal morphology, composition, and growth. These subcategories include: biomineralization, organomineralization, and inorganic mineralization, which can be subdivided further. However, usage of these terms vary widely in scientific literature because there are no standardized definitions.The following definitions are based largely on a paper written by Dupraz et al. (2009), which provided a framework for differentiating these terms.\n\nBiomineralization, biologically-controlled mineralization, occurs when crystal morphology, growth, composition, and location is completely controlled by the cellular processes of a specific organism. Examples include the shells of invertebrates, such as molluscs and brachiopods. Additionally, mineralization of collagen provides the crucial compressive strength for the bones, cartilage, and teeth of vertebrates.\n\nThis type of mineralization includes both biologically-induced mineralization and biologically-influenced mineralization.\n\nInorganic mineralization is a completely abiotic process. Chemical conditions necessary for mineral formation develop via environmental processes, such as evaporation or degassing. Furthermore, the substrate for mineral deposition is abiotic (i.e. contains no organic compounds) and there is no control on crystal morphology or composition. Examples of this type of mineralization include cave formations, such as stalagmites and stalactites.\n\nBiological mineralization can also take place as a result of fossilization. \nSee also calcification.\n\nBone mineralization occurs in human body by cells called osteoblasts.\n"}
{"id": "51310415", "url": "https://en.wikipedia.org/wiki?curid=51310415", "title": "NGC 161", "text": "NGC 161\n\nNGC 161 is a lenticular galaxy in the Cetus constellation. It was discovered on November 21, 1886, by Lewis A. Swift.\n\n"}
{"id": "28017775", "url": "https://en.wikipedia.org/wiki?curid=28017775", "title": "Nitrospirae", "text": "Nitrospirae\n\nNitrospirae is a phylum of bacteria. It contains only one class, Nitrospira, which itself contains one order (Nitrospirales) and one family (Nitrospiraceae). It includes multiple genera, such as \"Nitrospira\", the largest. The first member of this phylum, \"Nitrospira marina\", was discovered in 1985. The second member, \"Nitrospira moscoviensis\", was discovered in 1995.\n\nThe phylogeny based on the work of the All-Species Living Tree Project.\n\nThe currently accepted taxonomy is based on the List of Prokaryotic names with Standing in Nomenclature (LSPN) and the National Center for Biotechnology Information (NCBI).\n\n\nNotes:\n♠ Strain found at the National Center for Biotechnology Information (NCBI) but not listed in the List of Prokaryotic names with Standing in Nomenclature (LPSN)\n\n\n"}
{"id": "23053", "url": "https://en.wikipedia.org/wiki?curid=23053", "title": "Periodic table", "text": "Periodic table\n\nThe periodic table, or periodic table of elements, is a tabular arrangement of the chemical elements, ordered by their atomic number, electron configuration, and recurring chemical properties, whose structure shows \"periodic trends\". Generally, within one row (period) the elements are metals to the left, and non-metals to the right, with the elements having similar chemical behaviours placed in the same column. Table rows are commonly called periods and columns are called groups. Six groups have accepted names as well as assigned numbers: for example, group 17 elements are the halogens; and group 18 are the noble gases. Also displayed are four simple rectangular areas or blocks associated with the filling of different atomic orbitals.\n\nThe organization of the periodic table can be used to derive relationships between the various element properties, but also the predicted chemical properties and behaviours of undiscovered or newly synthesized elements. Russian chemist Dmitri Mendeleev was the first to publish a recognizable periodic table in 1869, developed mainly to illustrate periodic trends of the then-known elements. He also predicted some properties of unidentified elements that were expected to fill gaps within the table. Most of his forecasts proved to be correct. Mendeleev's idea has been slowly expanded and refined with the discovery or synthesis of further new elements and the development of new theoretical models to explain chemical behaviour. The modern periodic table now provides a useful framework for analyzing chemical reactions, and continues to be widely used in chemistry, nuclear physics and other sciences.\n\nAll the elements from atomic numbers 1 (hydrogen) through 118 (oganesson) have been either discovered or synthesized, completing the first seven rows of the periodic table. The first 98 elements exist in nature, although some are found only in trace amounts and others were synthesized in laboratories before being found in nature. Elements 99 to 118 have only been synthesized in laboratories or nuclear reactors. The synthesis of elements having higher atomic numbers is currently being pursued: these elements would begin an eighth row, and theoretical work has been done to suggest possible candidates for this extension. Numerous synthetic radionuclides of naturally occurring elements have also been produced in laboratories.\n\nEach chemical element has a unique atomic number (\"Z\") representing the number of protons in its nucleus. Most elements have differing numbers of neutrons among different atoms, with these variants being referred to as isotopes. For example, carbon has three naturally occurring isotopes: all of its atoms have six protons and most have six neutrons as well, but about one per cent have seven neutrons, and a very small fraction have eight neutrons. Isotopes are never separated in the periodic table; they are always grouped together under a single element. Elements with no stable isotopes have the atomic masses of their most stable isotopes, where such masses are shown, listed in parentheses.\n\nIn the standard periodic table, the elements are listed in order of increasing atomic number \"Z\" (the number of protons in the nucleus of an atom). A new row (\"period\") is started when a new electron shell has its first electron. Columns (\"groups\") are determined by the electron configuration of the atom; elements with the same number of electrons in a particular subshell fall into the same columns (e.g. oxygen and selenium are in the same column because they both have four electrons in the outermost p-subshell). Elements with similar chemical properties generally fall into the same group in the periodic table, although in the f-block, and to some respect in the d-block, the elements in the same period tend to have similar properties, as well. Thus, it is relatively easy to predict the chemical properties of an element if one knows the properties of the elements around it.\n\n, the periodic table has 118 confirmed elements, from element 1 (hydrogen) to 118 (oganesson). Elements 113, 115, 117 and 118, the most recent discoveries, were officially confirmed by the International Union of Pure and Applied Chemistry (IUPAC) in December 2015. Their proposed names, nihonium (Nh), moscovium (Mc), tennessine (Ts) and oganesson (Og) respectively, were announced by the IUPAC in June 2016 and made official in November 2016.\n\nThe first 94 elements occur naturally; the remaining 24, americium to oganesson (95–118), occur only when synthesized in laboratories. Of the 94 naturally occurring elements, 83 are primordial and 11 occur only in decay chains of primordial elements. No element heavier than einsteinium (element 99) has ever been observed in macroscopic quantities in its pure form, nor has astatine (element 85); francium (element 87) has been only photographed in the form of light emitted from microscopic quantities (300,000 atoms).\n\nA \"group\" or \"family\" is a vertical column in the periodic table. Groups usually have more significant periodic trends than periods and blocks, explained below. Modern quantum mechanical theories of atomic structure explain group trends by proposing that elements within the same group generally have the same electron configurations in their valence shell. Consequently, elements in the same group tend to have a shared chemistry and exhibit a clear trend in properties with increasing atomic number. In some parts of the periodic table, such as the d-block and the f-block, horizontal similarities can be as important as, or more pronounced than, vertical similarities.\n\nUnder an international naming convention, the groups are numbered numerically from 1 to 18 from the leftmost column (the alkali metals) to the rightmost column (the noble gases). Previously, they were known by roman numerals. In America, the roman numerals were followed by either an \"A\" if the group was in the s- or p-block, or a \"B\" if the group was in the d-block. The roman numerals used correspond to the last digit of today's naming convention (e.g. the group 4 elements were group IVB, and the group 14 elements were group IVA). In Europe, the lettering was similar, except that \"A\" was used if the group was before group 10, and \"B\" was used for groups including and after group 10. In addition, groups 8, 9 and 10 used to be treated as one triple-sized group, known collectively in both notations as group VIII. In 1988, the new IUPAC naming system was put into use, and the old group names were deprecated.\n\nSome of these groups have been given trivial (unsystematic) names, as seen in the table below, although some are rarely used. Groups 3–10 have no trivial names and are referred to simply by their group numbers or by the name of the first member of their group (such as \"the scandium group\" for group 3), since they display fewer similarities and/or vertical trends.\n\nElements in the same group tend to show patterns in atomic radius, ionization energy, and electronegativity. From top to bottom in a group, the atomic radii of the elements increase. Since there are more filled energy levels, valence electrons are found farther from the nucleus. From the top, each successive element has a lower ionization energy because it is easier to remove an electron since the atoms are less tightly bound. Similarly, a group has a top-to-bottom decrease in electronegativity due to an increasing distance between valence electrons and the nucleus. There are exceptions to these trends: for example, in group 11, electronegativity increases farther down the group.\n\nA \"period\" is a horizontal row in the periodic table. Although groups generally have more significant periodic trends, there are regions where horizontal trends are more significant than vertical group trends, such as the f-block, where the lanthanides and actinides form two substantial horizontal series of elements.\n\nElements in the same period show trends in atomic radius, ionization energy, electron affinity, and electronegativity. Moving left to right across a period, atomic radius usually decreases. This occurs because each successive element has an added proton and electron, which causes the electron to be drawn closer to the nucleus. This decrease in atomic radius also causes the ionization energy to increase when moving from left to right across a period. The more tightly bound an element is, the more energy is required to remove an electron. Electronegativity increases in the same manner as ionization energy because of the pull exerted on the electrons by the nucleus. Electron affinity also shows a slight trend across a period. Metals (left side of a period) generally have a lower electron affinity than nonmetals (right side of a period), with the exception of the noble gases.\n\nSpecific regions of the periodic table can be referred to as \"blocks\" in recognition of the sequence in which the electron shells of the elements are filled. Each block is named according to the subshell in which the \"last\" electron notionally resides. The s-block comprises the first two groups (alkali metals and alkaline earth metals) as well as hydrogen and helium. The p-block comprises the last six groups, which are groups 13 to 18 in IUPAC group numbering (3A to 8A in American group numbering) and contains, among other elements, all of the metalloids. The d-block comprises groups 3 to 12 (or 3B to 2B in American group numbering) and contains all of the transition metals. The f-block, often offset below the rest of the periodic table, has no group numbers and comprises lanthanides and actinides.\n\nAccording to their shared physical and chemical properties, the elements can be classified into the major categories of metals, metalloids and nonmetals. Metals are generally shiny, highly conducting solids that form alloys with one another and salt-like ionic compounds with nonmetals (other than noble gases). A majority of nonmetals are coloured or colourless insulating gases; nonmetals that form compounds with other nonmetals feature covalent bonding. In between metals and nonmetals are metalloids, which have intermediate or mixed properties.\n\nMetal and nonmetals can be further classified into subcategories that show a gradation from metallic to non-metallic properties, when going left to right in the rows. The metals may be subdivided into the highly reactive alkali metals, through the less reactive alkaline earth metals, lanthanides and actinides, via the archetypal transition metals, and ending in the physically and chemically weak post-transition metals. Nonmetals may be simply subdivided into the polyatomic nonmetals, being nearer to the metalloids and show some incipient metallic character; the essentially nonmetallic diatomic nonmetals, nonmetallic and the almost completely inert, monatomic noble gases. Specialized groupings such as refractory metals and noble metals, are examples of subsets of transition metals, also known and occasionally denoted.\n\nPlacing elements into categories and subcategories based just on shared properties is imperfect. There is a large disparity of properties within each category with notable overlaps at the boundaries, as is the case with most classification schemes. Beryllium, for example, is classified as an alkaline earth metal although its amphoteric chemistry and tendency to mostly form covalent compounds are both attributes of a chemically weak or post-transition metal. Radon is classified as a nonmetallic noble gas yet has some cationic chemistry that is characteristic of metals. Other classification schemes are possible such as the division of the elements into mineralogical occurrence categories, or crystalline structures. Categorizing the elements in this fashion dates back to at least 1869 when Hinrichs wrote that simple boundary lines could be placed on the periodic table to show elements having shared properties, such as metals, nonmetals, or gaseous elements.\n\nThe electron configuration or organisation of electrons orbiting neutral atoms shows a recurring pattern or periodicity. The electrons occupy a series of electron shells (numbered 1, 2, and so on). Each shell consists of one or more subshells (named s, p, d, f and g). As atomic number increases, electrons progressively fill these shells and subshells more or less according to the Madelung rule or energy ordering rule, as shown in the diagram. The electron configuration for neon, for example, is 1s 2s 2p. With an atomic number of ten, neon has two electrons in the first shell, and eight electrons in the second shell; there are two electrons in the s subshell and six in the p subshell. In periodic table terms, the first time an electron occupies a new shell corresponds to the start of each new period, these positions being occupied by hydrogen and the alkali metals.\n\nSince the properties of an element are mostly determined by its electron configuration, the properties of the elements likewise show recurring patterns or periodic behaviour, some examples of which are shown in the diagrams below for atomic radii, ionization energy and electron affinity. It is this periodicity of properties, manifestations of which were noticed well before the underlying theory was developed, that led to the establishment of the periodic law (the properties of the elements recur at varying intervals) and the formulation of the first periodic tables.\n\nAtomic radii vary in a predictable and explainable manner across the periodic table. For instance, the radii generally decrease along each period of the table, from the alkali metals to the noble gases; and increase down each group. The radius increases sharply between the noble gas at the end of each period and the alkali metal at the beginning of the next period. These trends of the atomic radii (and of various other chemical and physical properties of the elements) can be explained by the electron shell theory of the atom; they provided important evidence for the development and confirmation of quantum theory.\n\nThe electrons in the 4f-subshell, which is progressively filled across the lanthanide series, are not particularly effective at shielding the increasing nuclear charge from the sub-shells further out. The elements immediately following the lanthanides have atomic radii that are smaller than would be expected and that are almost identical to the atomic radii of the elements immediately above them. Hence hafnium has virtually the same atomic radius (and chemistry) as zirconium, and tantalum has an atomic radius similar to niobium, and so forth. This is known as the lanthanide contraction. The effect of the lanthanide contraction is noticeable up to platinum (element 78), after which it is masked by a relativistic effect known as the inert pair effect. The d-block contraction, which is a similar effect between the d-block and p-block, is less pronounced than the lanthanide contraction but arises from a similar cause.\n\nThe first ionization energy is the energy it takes to remove one electron from an atom, the second ionization energy is the energy it takes to remove a second electron from the atom, and so on. For a given atom, successive ionization energies increase with the degree of ionization. For magnesium as an example, the first ionization energy is 738 kJ/mol and the second is 1450 kJ/mol. Electrons in the closer orbitals experience greater forces of electrostatic attraction; thus, their removal requires increasingly more energy. Ionization energy becomes greater up and to the right of the periodic table.\n\nLarge jumps in the successive molar ionization energies occur when removing an electron from a noble gas (complete electron shell) configuration. For magnesium again, the first two molar ionization energies of magnesium given above correspond to removing the two 3s electrons, and the third ionization energy is a much larger 7730 kJ/mol, for the removal of a 2p electron from the very stable neon-like configuration of Mg. Similar jumps occur in the ionization energies of other third-row atoms.\n\nElectronegativity is the tendency of an atom to attract a shared pair of electrons. An atom's electronegativity is affected by both its atomic number and the distance between the valence electrons and the nucleus. The higher its electronegativity, the more an element attracts electrons. It was first proposed by Linus Pauling in 1932. In general, electronegativity increases on passing from left to right along a period, and decreases on descending a group. Hence, fluorine is the most electronegative of the elements, while caesium is the least, at least of those elements for which substantial data is available.\n\nThere are some exceptions to this general rule. Gallium and germanium have higher electronegativities than aluminium and silicon respectively because of the d-block contraction. Elements of the fourth period immediately after the first row of the transition metals have unusually small atomic radii because the 3d-electrons are not effective at shielding the increased nuclear charge, and smaller atomic size correlates with higher electronegativity. The anomalously high electronegativity of lead, particularly when compared to thallium and bismuth, appears to be an artifact of data selection and data availability. Methods of calculation other than the Pauling method show the normal periodic trends for these elements.\n\nThe electron affinity of an atom is the amount of energy released when an electron is added to a neutral atom to form a negative ion. Although electron affinity varies greatly, some patterns emerge. Generally, nonmetals have more positive electron affinity values than metals. Chlorine most strongly attracts an extra electron. The electron affinities of the noble gases have not been measured conclusively, so they may or may not have slightly negative values.\n\nElectron affinity generally increases across a period. This is caused by the filling of the valence shell of the atom; a group 17 atom releases more energy than a group 1 atom on gaining an electron because it obtains a filled valence shell and is therefore more stable.\n\nA trend of decreasing electron affinity going down groups would be expected. The additional electron will be entering an orbital farther away from the nucleus. As such this electron would be less attracted to the nucleus and would release less energy when added. In going down a group, around one-third of elements are anomalous, with heavier elements having higher electron affinities than their next lighter congenors. Largely, this is due to the poor shielding by d and f electrons. A uniform decrease in electron affinity only applies to group 1 atoms.\n\nThe lower the values of ionization energy, electronegativity and electron affinity, the more metallic character the element has. Conversely, nonmetallic character increases with higher values of these properties. Given the periodic trends of these three properties, metallic character tends to decrease going across a period (or row) and, with some irregularities (mostly) due to poor screening of the nucleus by d and f electrons, and relativistic effects, tends to increase going down a group (or column or family). Thus, the most metallic elements (such as caesium and francium) are found at the bottom left of traditional periodic tables and the most nonmetallic elements (oxygen, fluorine, chlorine) at the top right. The combination of horizontal and vertical trends in metallic character explains the stair-shaped dividing line between metals and nonmetals found on some periodic tables, and the practice of sometimes categorizing several elements adjacent to that line, or elements adjacent to those elements, as metalloids.\n\nFrom left to right across the four blocks of the long- or 32-column form of the periodic table are a series of linking or bridging groups of elements, located approximately between each block. These groups, like the metalloids, show properties in between, or that are a mixture of, groups to either side. Chemically, the group 3 elements, scandium, yttrium, lanthanum and actinium behave largely like the alkaline earth metals or, more generally, \"s\" block metals but have some of the physical properties of \"d\" block transition metals. Lutetium and lawrencium, at the end of the end of the \"f\" block, may constitute another linking or bridging group. Lutetium behaves chemically as a lanthanide but shows a mix of lanthanide and transition metal physical properties. Lawrencium, as an analogue of lutetium, would presumably display like characteristics. The coinage metals in group 11 (copper, silver, and gold) are chemically capable of acting as either transition metals or main group metals. The volatile group 12 metals, zinc, cadmium and mercury are sometimes regarded as linking the \"d\" block to the \"p\" block. Notionally they are \"d\" block elements but they have few transition metal properties and are more like their \"p\" block neighbors in group 13. The relatively inert noble gases, in group 18, bridge the most reactive groups of elements in the periodic table—the halogens in group 17 and the alkali metals in group 1.\n\nIn 1789, Antoine Lavoisier published a list of 33 chemical elements, grouping them into gases, metals, nonmetals, and earths. Chemists spent the following century searching for a more precise classification scheme. In 1829, Johann Wolfgang Döbereiner observed that many of the elements could be grouped into triads based on their chemical properties. Lithium, sodium, and potassium, for example, were grouped together in a triad as soft, reactive metals. Döbereiner also observed that, when arranged by atomic weight, the second member of each triad was roughly the average of the first and the third; this became known as the Law of Triads. German chemist Leopold Gmelin worked with this system, and by 1843 he had identified ten triads, three groups of four, and one group of five. Jean-Baptiste Dumas published work in 1857 describing relationships between various groups of metals. Although various chemists were able to identify relationships between small groups of elements, they had yet to build one scheme that encompassed them all.\n\nIn 1857, German chemist August Kekulé observed that carbon often has four other atoms bonded to it. Methane, for example, has one carbon atom and four hydrogen atoms. This concept eventually became known as valency; different elements bond with different numbers of atoms.\n\nIn 1862, Alexandre-Emile Béguyer de Chancourtois, a French geologist, published an early form of periodic table, which he called the telluric helix or screw. He was the first person to notice the periodicity of the elements. With the elements arranged in a spiral on a cylinder by order of increasing atomic weight, de Chancourtois showed that elements with similar properties seemed to occur at regular intervals. His chart included some ions and compounds in addition to elements. His paper also used geological rather than chemical terms and did not include a diagram; as a result, it received little attention until the work of Dmitri Mendeleev.\n\nIn 1864, Julius Lothar Meyer, a German chemist, published a table with 44 elements arranged by valency. The table showed that elements with similar properties often shared the same valency. Concurrently, English chemist William Odling published an arrangement of 57 elements, ordered on the basis of their atomic weights. With some irregularities and gaps, he noticed what appeared to be a periodicity of atomic weights among the elements and that this accorded with \"their usually received groupings\". Odling alluded to the idea of a periodic law but did not pursue it. He subsequently proposed (in 1870) a valence-based classification of the elements.\n\nEnglish chemist John Newlands produced a series of papers from 1863 to 1866 noting that when the elements were listed in order of increasing atomic weight, similar physical and chemical properties recurred at intervals of eight; he likened such periodicity to the octaves of music. This so termed Law of Octaves was ridiculed by Newlands' contemporaries, and the Chemical Society refused to publish his work. Newlands was nonetheless able to draft a table of the elements and used it to predict the existence of missing elements, such as germanium. The Chemical Society only acknowledged the significance of his discoveries five years after they credited Mendeleev.\n\nIn 1867, Gustavus Hinrichs, a Danish born academic chemist based in America, published a spiral periodic system based on atomic spectra and weights, and chemical similarities. His work was regarded as idiosyncratic, ostentatious and labyrinthine and this may have militated against its recognition and acceptance.\n\nRussian chemistry professor Dmitri Mendeleev and German chemist Julius Lothar Meyer independently published their periodic tables in 1869 and 1870, respectively. Mendeleev's table was his first published version; that of Meyer was an expanded version of his (Meyer's) table of 1864. They both constructed their tables by listing the elements in rows or columns in order of atomic weight and starting a new row or column when the characteristics of the elements began to repeat.\n\nThe recognition and acceptance afforded to Mendeleev's table came from two decisions he made. The first was to leave gaps in the table when it seemed that the corresponding element had not yet been discovered. Mendeleev was not the first chemist to do so, but he was the first to be recognized as using the trends in his periodic table to predict the properties of those missing elements, such as gallium and germanium. The second decision was to occasionally ignore the order suggested by the atomic weights and switch adjacent elements, such as tellurium and iodine, to better classify them into chemical families.\n\nMendeleev published in 1869, using atomic weight to organize the elements, information determinable to fair precision in his time. Atomic weight worked well enough to allow Mendeleev to accurately predict the properties of missing elements.\n\nFollowing the discovery, in 1911, by Ernest Rutherford of the atomic nucleus, it was proposed that the integer count of the nuclear charge is identical to the sequential place of each element in the periodic table. In 1913, Henry Moseley using X-ray spectroscopy confirmed this proposal experimentally. Moseley determined the value of the nuclear charge of each element, and showed that Mendeleev's ordering actually places the elements in sequential order by nuclear charge. Nuclear charge is identical to proton count, and determines the value of the atomic number (Z) of each element. Using atomic number gives a definitive, integer-based sequence for the elements. Moseley predicted, in 1913, that the only elements still missing between aluminium (Z=13) and gold (Z=79) were Z = 43, 61, 72, and 75, all of which were later discovered. The atomic number is the absolute definition of an element, and gives a factual basis for the ordering of the periodic table. The periodic table is used to predict the properties of new synthetic elements before they are produced and studied.\n\nIn 1871, Mendeleev published his periodic table in a new form, with groups of similar elements arranged in columns rather than in rows, and those columns numbered I to VIII corresponding with the element's oxidation state. He also gave detailed predictions for the properties of elements he had earlier noted were missing, but should exist. These gaps were subsequently filled as chemists discovered additional naturally occurring elements. It is often stated that the last naturally occurring element to be discovered was francium (referred to by Mendeleev as \"eka-caesium\") in 1939. Plutonium, produced synthetically in 1940, was identified in trace quantities as a naturally occurring element in 1971.\n\nThe popular periodic table layout, also known as the common or standard form (as shown at various other points in this article), is attributable to Horace Groves Deming. In 1923, Deming, an American chemist, published short (Mendeleev style) and medium (18-column) form periodic tables. Merck and Company prepared a handout form of Deming's 18-column medium table, in 1928, which was widely circulated in American schools. By the 1930s Deming's table was appearing in handbooks and encyclopaedias of chemistry. It was also distributed for many years by the Sargent-Welch Scientific Company.\n\nWith the development of modern quantum mechanical theories of electron configurations within atoms, it became apparent that each period (row) in the table corresponded to the filling of a quantum shell of electrons. Larger atoms have more electron sub-shells, so later tables have required progressively longer periods.\n\nIn 1945, Glenn Seaborg, an American scientist, made the suggestion that the actinide elements, like the lanthanides, were filling an f sub-level. Before this time the actinides were thought to be forming a fourth d-block row. Seaborg's colleagues advised him not to publish such a radical suggestion as it would most likely ruin his career. As Seaborg considered he did not then have a career to bring into disrepute, he published anyway. Seaborg's suggestion was found to be correct and he subsequently went on to win the 1951 Nobel Prize in chemistry for his work in synthesizing actinide elements.\n\nAlthough minute quantities of some transuranic elements occur naturally, they were all first discovered in laboratories. Their production has expanded the periodic table significantly, the first of these being neptunium, synthesized in 1939. Because many of the transuranic elements are highly unstable and decay quickly, they are challenging to detect and characterize when produced. There have been controversies concerning the acceptance of competing discovery claims for some elements, requiring independent review to determine which party has priority, and hence naming rights. In 2010, a joint Russia–US collaboration at Dubna, Moscow Oblast, Russia, claimed to have synthesized six atoms of tennessine (element 117), making it the most recently claimed discovery. It, along with nihonium (element 113), moscovium (element 115), and oganesson (element 118), are the four most recently named elements, whose names all became official on 28 November 2016.\n\nThe modern periodic table is sometimes expanded into its long or 32-column form by reinstating the footnoted f-block elements into their natural position between the s- and d-blocks. Unlike the 18-column form this arrangement results in \"no interruptions in the sequence of increasing atomic numbers\". The relationship of the f-block to the other blocks of the periodic table also becomes easier to see. Jensen advocates a form of table with 32 columns on the grounds that the lanthanides and actinides are otherwise relegated in the minds of students as dull, unimportant elements that can be quarantined and ignored. Despite these advantages the 32-column form is generally avoided by editors on account of its undue rectangular ratio (compared to a book page ratio), and the familiarity of chemists with the modern form (as introduced by Seaborg).\n\nWithin 100 years of the appearance of Mendeleev's table in 1869, Edward G. Mazurs had collected an estimated 700 different published versions of the periodic table. As well as numerous rectangular variations, other periodic table formats have been shaped, for example, like a circle, cube, cylinder, building, spiral, lemniscate, octagonal prism, pyramid, sphere, or triangle. Such alternatives are often developed to highlight or emphasize chemical or physical properties of the elements that are not as apparent in traditional periodic tables.\nA popular alternative structure is that of Otto Theodor Benfey (1960). The elements are arranged in a continuous spiral, with hydrogen at the centre and the transition metals, lanthanides, and actinides occupying peninsulas.\n\nMost periodic tables are two-dimensional; three-dimensional tables are known to as far back as at least 1862 (pre-dating Mendeleev's two-dimensional table of 1869). More recent examples include Courtines' Periodic Classification (1925), Wringley's Lamina System (1949),\nGiguère's Periodic helix (1965) and Dufour's Periodic Tree (1996). Going one further, Stowe's Physicist's Periodic Table (1989) has been described as being four-dimensional (having three spatial dimensions and one colour dimension).\n\nThe various forms of periodic tables can be thought of as lying on a chemistry–physics continuum. Towards the chemistry end of the continuum can be found, as an example, Rayner-Canham's \"unruly\" Inorganic Chemist's Periodic Table (2002), which emphasizes trends and patterns, and unusual chemical relationships and properties. Near the physics end of the continuum is Janet's Left-Step Periodic Table (1928). This has a structure that shows a closer connection to the order of electron-shell filling and, by association, quantum mechanics. A somewhat similar approach has been taken by Alper, albeit criticized by Eric Scerri as disregarding the need to display chemical and physical periodicity. Somewhere in the middle of the continuum is the ubiquitous common or standard form of periodic table. This is regarded as better expressing empirical trends in physical state, electrical and thermal conductivity, and oxidation numbers, and other properties easily inferred from traditional techniques of the chemical laboratory. Its popularity is thought to be a result of this layout having a good balance of features in terms of ease of construction and size, and its depiction of atomic order and periodic trends.\n\nSimply following electron configurations, hydrogen (electronic configuration 1s) and helium (1s) should be placed in groups 1 and 2, above lithium (1s2s) and beryllium (1s2s). While such a placement is common for hydrogen, it is rarely used for helium outside of the context of electron configurations: When the noble gases (then called \"inert gases\") were first discovered around 1900, they were known as \"group 0\", reflecting no chemical reactivity of these elements known at that point, and helium was placed on the top of that group, as it did share the extreme chemical inertness seen throughout the group. As the group changed its formal number, many authors continued to assign helium directly above neon, in group 18; one of the examples of such placing is the current IUPAC table.\n\nHydrogen's chemical properties are not very close to those of the alkali metals, which occupy group 1. On this basis it is sometimes placed elsewhere. A common alternative is at the top of group 17 given hydrogen's strictly univalent and largely non-metallic chemistry, and the strictly univalent and non-metallic chemistry of fluorine (the element otherwise at the top of group 17). Sometimes, to show hydrogen has properties corresponding to both those of the alkali metals and the halogens, it is shown at the top of the two columns simultaneously. Another suggestion is above carbon in group 14: placed that way, it fits well into the trends of increasing ionization potential values and electron affinity values, and is not too far from the electronegativity trend, even though hydrogen cannot show the tetravalence characteristic of the heavier group 14 elements. Finally, hydrogen is sometimes placed separately from any group; this is based on its general properties being different from those of the elements in any other group. The other period 1 element, helium, is sometimes placed separately from any group as well. The property that distinguishes helium from the rest of the noble gases (even though the extraordinary inertness of helium is extremely close to that of neon and argon) is that in its closed electron shell, helium has only two electrons in the outermost electron orbital, while the rest of the noble gases have eight.\n\nAlthough scandium and yttrium are always the first two elements in group 3, the identity of the next two elements is not completely settled. They are commonly lanthanum and actinium, and less often lutetium and lawrencium. The two variants originate from historical difficulties in placing the lanthanides in the periodic table, and arguments as to where the \"f\" block elements start and end. It has been claimed that such arguments are proof that, \"it is a mistake to break the [periodic] system into sharply delimited blocks\". A third variant shows the two positions below yttrium as being occupied by the lanthanides and the actinides.\n\nChemical and physical arguments have been made in support of lutetium and lawrencium but the majority of authors seem unconvinced. Most working chemists are not aware there is any controversy. In December 2015 an IUPAC project was established to make a recommendation on the matter.\n\nLanthanum and actinium are commonly depicted as the remaining group 3 members. It has been suggested that this layout originated in the 1940s, with the appearance of periodic tables relying on the electron configurations of the elements and the notion of the differentiating electron. The configurations of caesium, barium and lanthanum are [Xe]6s, [Xe]6s and [Xe]5d6s. Lanthanum thus has a 5d differentiating electron and this establishes it \"in group 3 as the first member of the d-block for period 6\". A consistent set of electron configurations is then seen in group 3: scandium [Ar]3d4s, yttrium [Kr]4d5s and lanthanum [Xe]5d6s. Still in period 6, ytterbium was assigned an electron configuration of [Xe]4f5d6s and lutetium [Xe]4f5d6s, \"resulting in a 4f differentiating electron for lutetium and firmly establishing it as the last member of the f-block for period 6\". Later spectroscopic work found that the electron configuration of ytterbium was in fact [Xe]4f6s. This meant that ytterbium and lutetium—the latter with [Xe]4f5d6s—both had 14 f-electrons, \"resulting in a d- rather than an f- differentiating electron\" for lutetium and making it an \"equally valid candidate\" with [Xe]5d6s lanthanum, for the group 3 periodic table position below yttrium. Lanthanum has the advantage of incumbency since the 5d electron appears for the first time in its structure whereas it appears for the third time in lutetium, having also made a brief second appearance in gadolinium.\n\nIn terms of chemical behaviour, and trends going down group 3 for properties such as melting point, electronegativity and ionic radius, scandium, yttrium, lanthanum and actinium are similar to their group 1–2 counterparts. In this variant, the number of \"f\" electrons in the most common (trivalent) ions of the f-block elements consistently matches their position in the f-block. For example, the f-electron counts for the trivalent ions of the first three f-block elements are Ce 1, Pr 2 and Nd 3.\n\nIn other tables, lutetium and lawrencium are the remaining group 3 members. Early techniques for chemically separating scandium, yttrium and lutetium relied on the fact that these elements occurred together in the so-called \"yttrium group\" whereas La and Ac occurred together in the \"cerium group\". Accordingly, lutetium rather than lanthanum was assigned to group 3 by some chemists in the 1920s and 30s. Several physicists in the 1950s and '60s favoured lutetium, in light of a comparison of several of its physical properties with those of lanthanum. This arrangement, in which lanthanum is the first member of the f-block, is disputed by some authors since lanthanum lacks any f-electrons. It has been argued that this is not valid concern given other periodic table anomalies—thorium, for example, has no f-electrons yet is part of the f-block. As for lawrencium, its gas phase atomic electron configuration was confirmed in 2015 as [Rn]5f7s7p. Such a configuration represents another periodic table anomaly, regardless of whether lawrencium is located in the f-block or the d-block, as the only potentially applicable p-block position has been reserved for nihonium with its predicted configuration of [Rn]5f6d7s7p.\n\nChemically, scandium, yttrium and lutetium (and presumably lawrencium) behave like trivalent versions of the group 1–2 metals. On the other hand, trends going down the group for properties such as melting point, electronegativity and ionic radius, are similar to those found among their group 4–8 counterparts. In this variant, the number of \"f\" electrons in the gaseous forms of the f-block atoms usually matches their position in the f-block. For example, the f-electron counts for the first five f-block elements are La 0, Ce 1, Pr 3, Nd 4 and Pm 5.\n\nA few authors position all thirty lanthanides and actinides in the two positions below yttrium (usually via footnote markers).\nThis variant, which is the IUPAC-agreed version, emphasizes similarities in the chemistry of the 15 lanthanide elements (La–Lu), possibly at the expense of ambiguity as to which elements occupy the two group 3 positions below yttrium, and a 15-column wide \"f\" block (there can only be 14 elements in any row of the \"f\" block).\n\nThe definition of a transition metal, as given by IUPAC, is an element whose atom has an incomplete d sub-shell, or which can give rise to cations with an incomplete d sub-shell. By this definition all of the elements in groups 3–11 are transition metals. The IUPAC definition therefore excludes group 12, comprising zinc, cadmium and mercury, from the transition metals category.\n\nSome chemists treat the categories \"d-block elements\" and \"transition metals\" interchangeably, thereby including groups 3–12 among the transition metals. In this instance the group 12 elements are treated as a special case of transition metal in which the d electrons are not ordinarily involved in chemical bonding. The 2007 report of mercury(IV) fluoride (HgF), a compound in which mercury would use its d electrons for bonding, has prompted some commentators to suggest that mercury can be regarded as a transition metal. Other commentators, such as Jensen, have argued that the formation of a compound like HgF can occur only under highly abnormal conditions; indeed, its existence is currently disputed. As such, mercury could not be regarded as a transition metal by any reasonable interpretation of the ordinary meaning of the term.\n\nStill other chemists further exclude the group 3 elements from the definition of a transition metal. They do so on the basis that the group 3 elements do not form any ions having a partially occupied d shell and do not therefore exhibit any properties characteristic of transition metal chemistry. In this case, only groups 4–11 are regarded as transition metals. Though the group 3 elements show few of the characteristic chemical properties of the transition metals, they do show some of their characteristic physical properties (on account of the presence in each atom of a single d electron).\n\nAlthough all elements up to oganesson have been discovered, of the elements above hassium (element 108), only copernicium (element 112), nihonium (element 113), and flerovium (element 114) have known chemical properties, and only for copernicium is there enough evidence for a conclusive categorisation at present. The other elements may behave differently from what would be predicted by extrapolation, due to relativistic effects; for example, flerovium has been predicted to possibly exhibit some noble-gas-like properties, even though it is currently placed in the carbon group. The current experimental evidence still leaves open the question of whether flerovium behaves more like a metal or a noble gas.\n\nIt is unclear whether new elements will continue the pattern of the current periodic table as period 8, or require further adaptations or adjustments. Seaborg expected the eighth period to follow the previously established pattern exactly, so that it would include a two-element s-block for elements 119 and 120, a new g-block for the next 18 elements, and 30 additional elements continuing the current f-, d-, and p-blocks, culminating in element 168, the next noble gas. More recently, physicists such as Pekka Pyykkö have theorized that these additional elements do not follow the Madelung rule, which predicts how electron shells are filled and thus affects the appearance of the present periodic table. There are currently several competing theoretical models for the placement of the elements of atomic number less than or equal to 172. In all of these it is element 172, rather than element 168, that emerges as the next noble gas after oganesson, although these must be regarded as speculative as no complete calculations have been done beyond element 122.\n\nThe number of possible elements is not known. A very early suggestion made by Elliot Adams in 1911, and based on the arrangement of elements in each horizontal periodic table row, was that elements of atomic weight greater than circa 256 (which would equate to between elements 99 and 100 in modern-day terms) did not exist. A higher—more recent—estimate is that the periodic table may end soon after the island of stability, which is expected to centre around element 126, as the extension of the periodic and nuclides tables is restricted by proton and neutron drip lines. Other predictions of an end to the periodic table include at element 128 by John Emsley, at element 137 by Richard Feynman, and at element 155 by Albert Khazan.\n\nThe Bohr model exhibits difficulty for atoms with atomic number greater than 137, as any element with an atomic number greater than 137 would require 1s electrons to be travelling faster than \"c\", the speed of light. Hence the non-relativistic Bohr model is inaccurate when applied to such an element.\n\nThe relativistic Dirac equation has problems for elements with more than 137 protons. For such elements, the wave function of the Dirac ground state is oscillatory rather than bound, and there is no gap between the positive and negative energy spectra, as in the Klein paradox. More accurate calculations taking into account the effects of the finite size of the nucleus indicate that the binding energy first exceeds the limit for elements with more than 173 protons. For heavier elements, if the innermost orbital (1s) is not filled, the electric field of the nucleus will pull an electron out of the vacuum, resulting in the spontaneous emission of a positron. This does not happen if the innermost orbital is filled, so that element 173 is not necessarily the end of the periodic table.\n\nThe many different forms of periodic table have prompted the question of whether there is an optimal or definitive form of periodic table. The answer to this question is thought to depend on whether the chemical periodicity seen to occur among the elements has an underlying truth, effectively hard-wired into the universe, or if any such periodicity is instead the product of subjective human interpretation, contingent upon the circumstances, beliefs and predilections of human observers. An objective basis for chemical periodicity would settle the questions about the location of hydrogen and helium, and the composition of group 3. Such an underlying truth, if it exists, is thought to have not yet been discovered. In its absence, the many different forms of periodic table can be regarded as variations on the theme of chemical periodicity, each of which explores and emphasizes different aspects, properties, perspectives and relationships of and among the elements.\n\n\n"}
{"id": "36224143", "url": "https://en.wikipedia.org/wiki?curid=36224143", "title": "Planetary science", "text": "Planetary science\n\nPlanetary science or, more rarely, planetology, is the scientific study of planets (including Earth), moons, and planetary systems (in particular those of the Solar System) and the processes that form them. It studies objects ranging in size from micrometeoroids to gas giants, aiming to determine their composition, dynamics, formation, interrelations and history. It is a strongly interdisciplinary field, originally growing from astronomy and earth science, but which now incorporates many disciplines, including planetary geology (together with geochemistry and geophysics), cosmochemistry, atmospheric science, oceanography, hydrology, theoretical planetary science, glaciology, and exoplanetology. Allied disciplines include space physics, when concerned with the effects of the Sun on the bodies of the Solar System, and astrobiology.\n\nThere are interrelated observational and theoretical branches of planetary science. Observational research can involve a combination of space exploration, predominantly with robotic spacecraft missions using remote sensing, and comparative, experimental work in Earth-based laboratories. The theoretical component involves considerable computer simulation and mathematical modelling.\n\nPlanetary scientists are generally located in the astronomy and physics or Earth sciences departments of universities or research centres, though there are several purely planetary science institutes worldwide. There are several major conferences each year, and a wide range of peer-reviewed journals. In the case of some exclusive planetary scientists, many of whom are in relation to the study of dark matter, they will seek a private research centre and often initiate partnership research tasks.\n\nThe history of planetary science may be said to have begun with the Ancient Greek philosopher Democritus, who is reported by Hippolytus as saying The ordered worlds are boundless and differ in size, and that in some there is neither sun nor moon, but that in others, both are greater than with us, and yet with others more in number. And that the intervals between the ordered worlds are unequal, here more and there less, and that some increase, others flourish and others decay, and here they come into being and there they are eclipsed. But that they are destroyed by colliding with one another. And that some ordered worlds are bare of animals and plants and all water.\n\nIn more modern times, planetary science began in astronomy, from studies of the unresolved planets. In this sense, the original planetary astronomer would be Galileo, who discovered the four largest moons of Jupiter, the mountains on the Moon, and first observed the rings of Saturn, all objects of intense later study. Galileo's study of the lunar mountains in 1609 also began the study of extraterrestrial landscapes: his observation \"that the Moon certainly does not possess a smooth and polished surface\" suggested that it and other worlds might appear \"just like the face of the Earth itself\".\n\nAdvances in telescope construction and instrumental resolution gradually allowed increased identification of the atmospheric and surface details of the planets. The Moon was initially the most heavily studied, as it always exhibited details on its surface, due to its proximity to the Earth, and the technological improvements gradually produced more detailed lunar geological knowledge. In this scientific process, the main instruments were astronomical optical telescopes (and later radio telescopes) and finally robotic exploratory spacecraft.\n\nThe Solar System has now been relatively well-studied, and a good overall understanding of the formation and evolution of this planetary system exists. However, there are large numbers of unsolved questions, and the rate of new discoveries is very high, partly due to the large number of interplanetary spacecraft currently exploring the Solar System.\n\nThis is both an observational and a theoretical science. Observational researchers are predominantly concerned with the study of the small bodies of the Solar System: those that are observed by telescopes, both optical and radio, so that characteristics of these bodies such as shape, spin, surface materials and weathering are determined, and the history of their formation and evolution can be understood.\n\nTheoretical planetary astronomy is concerned with dynamics: the application of the principles of celestial mechanics to the Solar System and extrasolar planetary systems.\n\nThe best known research topics of planetary geology deal with the planetary bodies in the near vicinity of the Earth: the Moon, and the two neighbouring planets: Venus and Mars. Of these, the Moon was studied first, using methods developed earlier on the Earth.\n\nGeomorphology studies the features on planetary surfaces and reconstructs the history of their formation, inferring the physical processes that acted on the surface. Planetary geomorphology includes the study of several classes of surface features:\n\nThe history of a planetary surface can be deciphered by mapping features from top to bottom according to their deposition sequence, as first determined on terrestrial strata by Nicolas Steno. For example, stratigraphic mapping prepared the Apollo astronauts for the field geology they would encounter on their lunar missions. Overlapping sequences were identified on images taken by the Lunar Orbiter program, and these were used to prepare a lunar stratigraphic column and geological map of the Moon.\nOne of the main problems when generating hypotheses on the formation and evolution of objects in the Solar System is the lack of samples that can be analysed in the laboratory, where a large suite of tools are available and the full body of knowledge derived from terrestrial geology can be brought to bear. Direct samples from the Moon, asteroids and Mars are present on Earth, removed from their parent bodies and delivered as meteorites. Some of these have suffered contamination from the oxidising effect of Earth's atmosphere and the infiltration of the biosphere, but those meteorites collected in the last few decades from Antarctica are almost entirely pristine.\n\nThe different types of meteorites that originate from the asteroid belt cover almost all parts of the structure of differentiated bodies: meteorites even exist that come from the core-mantle boundary (pallasites). The combination of geochemistry and observational astronomy has also made it possible to trace the HED meteorites back to a specific asteroid in the main belt, 4 Vesta.\n\nThe comparatively few known Martian meteorites have provided insight into the geochemical composition of the Martian crust, although the unavoidable lack of information about their points of origin on the diverse Martian surface has meant that they do not provide more detailed constraints on theories of the evolution of the Martian lithosphere. As of July 24, 2013 65 samples of Martian meteorites have been discovered on Earth. Many were found in either Antarctica or the Sahara Desert.\n\nDuring the Apollo era, in the Apollo program, 384 kilograms of lunar samples were collected and transported to the Earth, and 3 Soviet Luna robots also delivered regolith samples from the Moon. These samples provide the most comprehensive record of the composition of any Solar System body beside the Earth. The numbers of lunar meteorites are growing quickly in the last few years – as of\nApril 2008 there are 54 meteorites that have been officially classified as lunar.\nEleven of these are from the US Antarctic meteorite collection, 6 are from the Japanese\nAntarctic meteorite collection, and the other 37 are from hot desert localities in Africa,\nAustralia, and the Middle East. The total mass of recognized lunar meteorites is close to\n50 kg.\n\nSpace probes made it possible to collect data in not only the visible light region, but in other areas of the electromagnetic spectrum. The planets can be characterized by their force fields: gravity and their magnetic fields, which are studied through geophysics and space physics.\n\nMeasuring the changes in acceleration experienced by spacecraft as they orbit has allowed fine details of the gravity fields of the planets to be mapped. For example, in the 1970s, the gravity field disturbances above lunar maria were measured through lunar orbiters, which led to the discovery of concentrations of mass, mascons, beneath the Imbrium, Serenitatis, Crisium, Nectaris and Humorum basins.\n\nIf a planet's magnetic field is sufficiently strong, its interaction with the solar wind forms a magnetosphere around a planet. Early space probes discovered the gross dimensions of the terrestrial magnetic field, which extends about 10 Earth radii towards the Sun. The solar wind, a stream of charged particles, streams out and around the terrestrial magnetic field, and continues behind the magnetic tail, hundreds of Earth radii downstream. Inside the magnetosphere, there are relatively dense regions of solar wind particles, the Van Allen radiation belts.\n\nGeophysics includes seismology and tectonophysics, geophysical fluid dynamics, mineral physics, geodynamics, mathematical geophysics, and geophysical surveying.\nPlanetary geodesy, (also known as planetary geodetics) deals with the measurement and representation of the planets of the Solar System, their gravitational fields and geodynamic phenomena (polar motion in three-dimensional, time-varying space. The science of geodesy has elements of both astrophysics and planetary sciences. The shape of the Earth is to a large extent the result of its rotation, which causes its equatorial bulge, and the competition of geologic processes such as the collision of plates and of vulcanism, resisted by the Earth's gravity field. These principles can be applied to the solid surface of Earth (orogeny; Few mountains are higher than , few deep sea trenches deeper than that because quite simply, a mountain as tall as, for example, , would develop so much pressure at its base, due to gravity, that the rock there would become plastic, and the mountain would slump back to a height of roughly in a geologically insignificant time. Some or all of these geologic principles can be applied to other planets besides Earth. For instance on Mars, whose surface gravity is much less, the largest volcano, Olympus Mons, is high at its peak, a height that could not be maintained on Earth. The Earth geoid is essentially the figure of the Earth abstracted from its topographic features. Therefore, the Mars geoid is essentially the figure of Mars abstracted from its topographic features. Surveying and mapping are two important fields of application of geodesy.\n\nThe atmosphere is an important transitional zone between the solid planetary surface and the higher rarefied ionizing and radiation belts. Not all planets have atmospheres: their existence depends on the mass of the planet, and the planet's distance from the Sun — too distant and frozen atmospheres occur. Besides the four gas giant planets, almost all of the terrestrial planets (Earth, Venus, and Mars) have significant atmospheres. Two moons have significant atmospheres: Saturn's moon Titan and Neptune's moon Triton. A tenuous atmosphere exists around Mercury.\n\nThe effects of the rotation rate of a planet about its axis can be seen in atmospheric streams and currents. Seen from space, these features show as bands and eddies in the cloud system, and are particularly visible on Jupiter and Saturn.\n\nPlanetary science frequently makes use of the method of comparison to give a greater understanding of the object of study. This can involve comparing the dense atmospheres of Earth and Saturn's moon Titan, the evolution of outer Solar System objects at different distances from the Sun, or the geomorphology of the surfaces of the terrestrial planets, to give only a few examples.\n\nThe main comparison that can be made is to features on the Earth, as it is much more accessible and allows a much greater range of measurements to be made. Earth analogue studies are particularly common in planetary geology, geomorphology, and also in atmospheric science.\n\n\n\n\nSmaller workshops and conferences on particular fields occur worldwide throughout the year.\n\nThis non-exhaustive list includes those institutions and universities with major groups of people working in planetary science. Alphabetical order is used.\n\n\n\n\n\n"}
{"id": "3799749", "url": "https://en.wikipedia.org/wiki?curid=3799749", "title": "Porosimetry", "text": "Porosimetry\n\nPorosimetry is an analytical technique used to determine various quantifiable aspects of a material's porous nature, such as pore diameter, total pore volume, surface area, and bulk and absolute densities.\n\nThe technique involves the intrusion of a non-wetting liquid (often mercury) at high pressure into a material through the use of a porosimeter. The pore size can be determined based on the external pressure needed to force the liquid into a pore against the opposing force of the liquid's surface tension.\n\nA force balance equation known as Washburn's equation for the above material having cylindrical pores is given as:\n\nSince the technique is usually performed within a vacuum, the initial gas pressure is zero. The contact angle of mercury with most solids is between 135° and 142°, so an average of 140° can be taken without much error. The surface tension of mercury at 20 °C under vacuum is 480 mN/m. With the various substitutions, the equation becomes:\n\nAs pressure increases, so does the cumulative pore volume. From the cumulative pore volume, one can find the pressure and pore diameter where 50% of the total volume has been added to give the median pore diameter.\n"}
{"id": "12561056", "url": "https://en.wikipedia.org/wiki?curid=12561056", "title": "Quantum dimer models", "text": "Quantum dimer models\n\nQuantum dimer models were introduced to model the physics of resonating valence bond (RVB) states in lattice spin systems. The only degrees of freedom retained from the motivating spin systems are the valence bonds, represented as dimers which live on the lattice bonds. In typical dimer models, the dimers do not overlap (\"hardcore constraint\").\n\nTypical phases of quantum dimer models tend to be valence bond crystals. However, on non-bipartite lattices, RVB liquid phases possessing topological order and fractionalized spinons also appear. The discovery of topological order in quantum dimer models (more than a decade after the models were introduced) has led to new interest in these models.\n\nClassical dimer models have been studied previously in statistical physics, in particular by P. W. Kasteleyn (1961) and\nM. E. Fisher (1961).\n\nExact solution for classical dimer models on planar graphs:\n\n\nIntroduction of model; early literature:\n\n\nTopological order in quantum dimer model on non-bipartite lattices:\n\n\nTopological order in quantum spin model on non-bipartite lattices:\n\n"}
{"id": "44654167", "url": "https://en.wikipedia.org/wiki?curid=44654167", "title": "Radial head fracture", "text": "Radial head fracture\n\nRadial head fractures are a type of elbow fracture.\n\nIt is not clear if removing fluid from the joint by joint aspiration affects outcomes.\n"}
{"id": "712662", "url": "https://en.wikipedia.org/wiki?curid=712662", "title": "Saturn V Instrument Unit", "text": "Saturn V Instrument Unit\n\nThe Saturn V Instrument Unit is a ring-shaped structure fitted to the top of the Saturn V rocket's third stage (S-IVB) and the Saturn IB's second stage (also an S-IVB). It was immediately below the SLA \"(Spacecraft/Lunar Module Adapter)\" panels that contained the Lunar Module. The Instrument Unit contains the guidance system for the Saturn V rocket. Some of the electronics contained within the Instrument Unit are a digital computer, analog flight control computer, emergency detection system, inertial guidance platform, control accelerometers and control rate gyros. The instrument unit (IU) for Saturn V was designed by NASA at Marshall Space Flight Center (MSFC) and was developed from the Saturn I IU. NASA's contractor to manufacture the Saturn V Instrument Unit was International Business Machines (IBM).\n\nOne of the unused Instrument Units is currently on display at the Steven F. Udvar-Hazy Center in Chantilly, Virginia. The plaque for the Unit has the following inscription:\n\nThe Saturn V rocket, which sent astronauts to the Moon, used inertial guidance, a self-contained system that guided the rocket's trajectory. The rocket booster had a guidance system separate from those on the command and lunar modules. It was contained in an instrument unit like this one, a ring located between the rocket's third stage and the command and lunar modules. The ring contained the basic guidance system components—a stable platform, accelerometers, a digital computer, and control electronics—as well as radar, telemetry, and other units.\n\nThe instrument unit's stable platform was based on an experimental unit for the German V-2 rocket of World War II. The Bendix Corporation produced the platform, while IBM designed and built the unit's digital computer.\n\n\nThere was no Instrument Unit for Saturn I Block I boosters (SA-1 to SA-4). Guidance and control equipment was carried in canisters on top of the S-I first stage, and included the ST-90 stabilized platform, made by Ford Instrument Company and used in the Jupiter missile.\n\nThe IU made its debut with SA-5, the first Saturn I Block II launch. The first version of the IU was in diameter and high, and was both designed and built by MSFC. Guidance, telemetry, tracking and power components were contained in four pressurized, cylindrical containers attached like spokes to a central hub.\n\nMSFC flew version 2 of the IU on SA-8, 9 and 10. Version 2 was the same diameter as version 1, but only high. Instead of pressurized containers, the components were hung on the inside of the cylindrical wall, achieving a reduction in weight.\n\nThe last version, number 3, was in diameter and tall. It was designed by MSFC but manufactured by IBM in their factory at Huntsville, and flew on all Saturn IB and Saturn V launches. This is the version that is on display in Washington, Huntsville, Houston, and the Apollo/Saturn V Center.\n\nSaturn Apollo flight profiles varied considerably by mission. All missions began, however, with liftoff under power of the first stage. To more smoothly control engine ignition, thrust buildup and liftoff of the vehicle, restraining arms provided support and hold down at four points around the base of the S-IC stage. A gradual controlled release was accomplished during the first six inches of vertical motion.\n\nAfter clearing the launch tower, a flight program stored in the launch vehicle digital computer (LVDC) commanded a roll of the vehicle to orient it so that the subsequent pitch maneuver pointed the vehicle in the desired azimuth. The roll and pitch commands were controlled by the stored program, and were not affected by navigation measurements. Until the end of the S-IC burn, guidance commands were functions only of time.\n\nFirst stage cutoff and stage separation were commanded when the IU received a signal that the tank's fuel level had reached a predetermined point. Guidance during the second and third stage burns depended both on time and navigation measurements, in order to achieve the target orbit using the minimum fuel.\n\nSecond stage engine cutoff was commanded by the IU at a pre-determined fuel level, and the stage was separated. By this time, the vehicle had reached its approximate orbital altitude, and the third stage burn was just long enough to reach a circular parking orbit.\n\nDuring manned Apollo missions, the vehicle coasted in Earth orbit for 2-4 passes as the crew performed checks of systems status and other tasks, and as ground stations tracked the vehicle. During the hour and a half after launch, tracking stations around the world had refined estimates of the vehicle's position and velocity, collectively known as its state vector. The latest estimates were relayed to the guidance systems in the IU, and to the Command Module Computer in the spacecraft. When the Moon, Earth, and vehicle were in the optimum geometrical configuration, the third stage was reignited to put the vehicle into a translunar orbit. For Apollo 15, for example, this burn lasted 5 minutes 55 seconds.\n\nAfter translunar injection came the maneuver called transposition, docking, and extraction. This was under crew control, but the IU held the S-IVB/IU vehicle steady while the Command/Service Module (CSM) first separated from the vehicle, rotated 180 degrees, and returned to dock with the Lunar Module (LM). When the CSM and LM had \"hard docked\" (connected by a dozen latches), the rearranged spacecraft separated from the S-IVB/IU.\n\nThe last function of the IU was to command the very small maneuver necessary to keep the S-IVB/IU out of the way of the spacecraft. On some missions the S-IVB/IU went into high Earth or Solar orbit, while on others it was crashed into the Moon; seismometers were left on the Moon during Apollo 11, 12, 14, 15, and 16, and the S-IVB/IUs of Apollo 13, 14, 15, 16, and 17 were directed to crash. These impacts provided impulses that were recorded by the seismometer network to yield information about the geological structure of the Moon.\n\nThe IU consists of six subsystems: structure, guidance and control, environmental control, emergency detection, radio communications (for telemetry, tracking, and command), and power.\n\nThe basic IU structure is a short cylinder, 36 inches high and in diameter, fabricated of an aluminum alloy honeycomb sandwich material thick. The cylinder is manufactured in three 120-degree segments, which are joined by splice plates into an integral structure. The top and bottom edges are made from extruded aluminum channels bonded to the honeycomb sandwich. This type of construction was selected for its high strength to weight ratio, acoustical insulation, and thermal conductivity properties. The IU supported the components mounted on its inner wall and the weight of the Apollo spacecraft above (the Lunar Module, the Command Module, the Service Module, and the Launch Escape Tower). To facilitate handling the IU before it was assembled into the Saturn, the fore and aft protective rings, 6 inches tall and painted blue, were bolted to the top and bottom channels. These were removed in the course of stacking the IU into the Saturn vehicle. The structure was manufactured by North American Rockwell in Tulsa, Oklahoma. Edward A. Beasley was the I.U. Program Manager.\n\nThe IU is divided into 24 locations, which are marked on the interior by numbers 1-24 on the aluminum surface just above the blue flange.\n\nThe Saturn V launch vehicle was guided by navigation, guidance, and control equipment located in the IU. A space stabilized platform (the ST-124-M3 inertial platform at location 21) measured acceleration and attitude. A launch vehicle digital computer (LVDC at location 19) solved guidance equations, and an analog flight control computer (location 16) issued commands to steer the vehicle.\n\nThe attitude of the vehicle was defined in terms of three axes:\n\n\nThe ST-124-M3 inertial platform contains three gimbals: the outer gimbal (which can rotate 360° about the roll or X axis of the vehicle), the middle gimbal (which can rotate ±45° about the yaw or Z axis of the vehicle), and the inner or inertial gimbal (which can rotate 360° about the pitch or Y axis of the vehicle). The inner gimbal is a platform to which are fixed several components:\n\n\nThe angular positions of gimbals on their axes were measured by resolvers, which sent their signals to the LVDA. The LVDA was the input/output device for the LVDC. It performed the necessary processing of signals to make these signals acceptable to the LVDC.\n\nThe instantaneous attitude of the vehicle was compared with the desired vehicle attitude in the LVDC. Attitude correction signals from the LVDC were converted into control commands by the flight control computer. The required thrust direction was obtained by gimbaling the engines in the propelling stage to change the thrust direction of the vehicle. Gimbaling of these engines was accomplished through hydraulic actuators. In the first and second stages (S-IC and S-II), the four outboard engines were gimbaled to control roll, pitch, and yaw. Since the third (S-IVB) stage has only one engine, an auxiliary propulsion system was used for roll control during powered flight. The auxiliary propulsion system provides complete attitude control during coast flight of the S-IVB/IU stage.\n\nThe environmental control system (ECS) maintains an acceptable operating environment for the IU equipment during preflight and flight operations. The ECS is composed of the following:\n\n\nThermal conditioning panels, also called cold plates, were located in both the IU and S-IVB stage (up to sixteen in each stage). Each cold plate contains tapped bolt holes in a grid pattern which provides flexibility of component mounting.\n\nThe cooling fluid circulated through the TCS was a mixture of 60 percent methanol and 40 percent demineralized water by weight. Each cold plate was capable of dissipating at least 420 watts.\n\nDuring flight, heat generated by equipment mounted on the cold plates was dissipated to space by a sublimation heat exchanger. Water from a reservoir (water accumulator) was exposed to the low temperature and pressure environment of space, where it first freezes and then sublimates, taking heat from the heat exchanger and transferring it to the water molecules which escape to space in gaseous state. Water/methanol was cooled by circulation through the heat exchanger.\n\nBefore flight, ground support equipment (GSE) supplies cooled, filtered ventilating air to the IU, entering via the large duct in the middle of the umbilical panel (location 7), and branching into two ducts at the top that are carried around the IU in the cable rack. Downward pointing vents from these ducts release ventilating air to the interior of the IU. During fueling, gaseous nitrogen was supplied instead of air, to purge any propellant gases that might otherwise accumulate in the IU.\n\nTo reduce errors in sensing attitude and velocity, designers cut friction to a minimum in the platform gyros and accelerometers by floating the bearings on a thin film of dry nitrogen. The nitrogen was supplied from a sphere holding 2 cu ft (56.6 l) of gas at 3,000 psig (pounds per square inch gauge, i.e. psi above one atmosphere) (20,7 MPa). This sphere is 21 inches (0,53 m) in diameter and is mounted at location 22, to the left of the ST-124-M3. Gas from the supply sphere passes through a filter, a pressure regulator, and a heat exchanger before flowing through the bearings in the stable platform.\n\nThe hazardous gas detection system monitors the presence of hazardous gases in the IU and S-IVB stage forward compartments during vehicle fueling. Gas was sampled at four locations: between panels 1 and 2, 7 and 8, 13 and 14, and 19 and 20. Tubes lead from these locations to location 7, where they were connected to ground support equipment (external to the IU) which can detect hazardous gases.\n\nThe emergency detection system (EDS) sensed initial development of conditions in the flight vehicle during the boost phases of flight which could cause vehicle failure. The EDS reacted to these emergency situations in one of two ways. If breakup of the vehicle were imminent, an automatic abort sequence would be initiated. If, however, the emergency condition were developing slowly enough or were of such a nature that the flight crew can evaluate it and take action, only visual indications were provided to the flight crew. Once an abort sequence had been initiated, either automatically or manually, it was irrevocable and ran to completion.\n\nThe EDS was distributed throughout the vehicle and includes some components in the IU. There were nine EDS rate gyros installed at location 15 in the IU. Three gyros monitored each of the three axes (pitch, roll and yaw), providing triple redundancy. The control signal processor (location 15) provided power to and received inputs from the nine EDS rate gyros. These inputs were processed and sent to the EDS distributor (location 14) and to the flight control computer (location 16). The EDS distributor served as a junction box and switching device to furnish the spacecraft display panels with emergency signals if emergency conditions existed. It also contained relay and diode logic for the automatic abort sequence. An electronic timer (location 17) was activated at liftoff and 30 seconds later energized relays in the EDS distributor which allowed multiple engine shutdown. This function was inhibited during the first 30 seconds of launch, to preclude the vehicle falling back into the launch area. While the automatic abort was inhibited, the flight crew can initiate a manual abort if an angular-overrate or two-engine-out condition arose.\n\nThe IU communicated by radio continually to ground for several purposes. The measurement and telemetry system communicated data about internal processes and conditions on the Saturn V. The tracking system communicated data used by the Mission Ground Station (MGS) to determine vehicle location. The radio command system allowed the MGS to send commands up to the IU.\n\nApproximately 200 parameters were measured on the IU and transmitted to the ground, in order to\n\n\nParameters measured include acceleration, angular velocity, flow rate, position, pressure, temperature, voltage, current, frequency, and others. Sensor signals were conditioned by amplifiers or converters located in measuring racks. There are four measuring racks in the IU at locations 1, 9, and 15 and twenty signal conditioning modules in each. Conditioned signals were routed to their assigned telemetry channel by the measuring distributor at location 10. There were two telemetry links. In order for the two IU telemetry links to handle approximately 200 separate measurements, these links must be shared. Both frequency sharing and time sharing multiplexing techniques were used to accomplish this. The two modulation techniques used were pulse code modulation/frequency modulation (PCM/FM) and frequency modulation/frequency modulation (FM/FM).\n\nTwo Model 270 time sharing multiplexers (MUX-270) were used in the IU telemetry system, mounted at locations 9 and 10. Each one operates as a 30×120 multiplexer (30 primary channels, each sampled 120 times per second) with provisions for submultiplexing individual primary channels to form 10 subchannels each sampled at 12 times per second. Outputs from the MUX-270 go to the PCM/DDAS assembly model 301 at location 12, which in turn drives the 245.3 MHz PCM VHF transmitter.\n\nThe FM/FM signals were carried in 28 subcarrier channels and transmitted by a 250.7 MHz FM transmitter.\n\nBoth the FM/FM and the PCM/FM channels were coupled to the two telemetry antennas on opposite sides of the IU outside locations 10 and 22.\n\nC-band radar transponders carried by the IU provided tracking data to the ground which were used to determine the vehicle's trajectory. The transponder received coded or single pulse interrogation from ground stations and transmitted a single-pulse reply in the same frequency band (5.4 to 5.9 GHz). A common antenna was used for receiving and transmitting. The C-band transponder antennas are outside locations 11 and 23, immediately below CCS PCM omni receive antennas.\n\nThe command communications system (CCS) provided for digital data transmission from ground stations to the LVDC. This communications link was used to update guidance information or command certain other functions through the LVDC. Command data originated in the Mission Control Center, Houston, and was sent to remote stations for transmission to the launch vehicle. Command messages were transmitted from the ground at 2101.8 MHz. The received message was passed to the command decoder (location 18), where it was checked for authenticity before being passed to the LVDC. Verification of message receipt was accomplished through the IU PCM telemetry system. The CCS system used five antennas:\n\n\nPower during flight originated with four silver-zinc batteries with a nominal voltage of 28±2 vdc. Battery D10 sat on a shelf at location 5, batteries D30 and D40 were on shelves in location 4, and battery D20 was at location 24. Two power supplies converted the unregulated battery power to regulated 56 vdc and 5 vdc. The 56 vdc power supply was at location 1 and provided power to the ST-124-M3 platform electronic assembly and the accelerometer signal conditioner. The 5 vdc power supply at location 12 provided 5 ±.005 vdc to the IU measuring system.\n\nThese images show the development of the IU. The first four Saturn launches did not have an IU, but used guidance, telemetry and other equipment installed on top of the first stage.\n\nThe first IU flew on the fifth Saturn launch, SA-5, and was in diameter and high. The components it carried were in pressurized containers. This version flew on SA-5, SA-6 and SA-7. The IU carried by missions SA-8, -9, and -10 was only high, and was not pressurized.\n\nWith the Saturn IB and Saturn V launches, a third version was used, in diameter and high. Comparison of these photographs of the Instrument Unit shows that the configuration of components carried by this version changed, depending on the mission. Some equipment was deleted (e.g. the Azusa tracking system was deleted from later IUs), some equipment was added (e.g. a fourth battery for longer missions), and other components were moved around.\n\nThese images also show that some components (e.g. batteries, the ST-124 inertial platform) were installed in the IU after it had been stacked in the VAB on top of the S-IVB third stage.\n\n\n\n\n\n\n\n"}
{"id": "27014288", "url": "https://en.wikipedia.org/wiki?curid=27014288", "title": "Science in History", "text": "Science in History\n\nScience in History is a four-volume book by scientist and historian John Desmond Bernal, published in 1954. It was the first comprehensive attempt to analyse the reciprocal relations of science and society throughout history. It was originally published in London by Watts. There were three editions up to 1969 an. It was republished by MIT Press in 1971 and is still in print.\n\nIt is one of the sources for the idea - considered erroneous by modern historians - that Medieval Christianity had returned to the pre-scientific notion of a Flat Earth:\n\n\n"}
{"id": "58681668", "url": "https://en.wikipedia.org/wiki?curid=58681668", "title": "Sur les épaules de Darwin", "text": "Sur les épaules de Darwin\n\nSur les épaules de Darwin (in English \"on the shoulders of Darwin\") is a one-hour science radio program broadcast by France Inter in France since 2010. it is also distributed globally as a podcast. It is presented and written by Jean-Claude Ameisen.\n\n\n"}
{"id": "34066587", "url": "https://en.wikipedia.org/wiki?curid=34066587", "title": "Technological transitions", "text": "Technological transitions\n\nTechnological innovations have occurred throughout history and rapidly increased over the modern age. New technologies are developed and co-exist with the old before supplanting them. Transport offers several examples; from sailing to steam ships to automobiles replacing horse-based transportation. Technological transitions (TT) describe how these technological innovations occur and are incorporated into society. Alongside the technological developments TT considers wider societal changes such as “user practices, regulation, industrial networks (supply, production, distribution), infrastructure, and symbolic meaning or culture”. For a technology to have use, it must be linked to social structures human agency and organisations to fulfil a specific need. Hughes refers to the ‘seamless web’ where physical artefacts, organisations, scientific communities, and social practices combine. A technological system includes technical and non-technical aspects, and it a major shift in the socio-technical configurations (involving at least one new technology) is when a technological transition occurs.\n\nWork on technological transitions draws on a number of fields including history of science, technology studies, and evolutionary economics. The focus of evolutionary economics is on economic change, but as a driver of this technological change has been considered in the literature. Joseph Schumpeter, in his classic \"Theory of Economic Development\" placed the emphasis on non-economic forces as the driver for growth. The human actor, the entrepreneur is seen as the cause of economic development which occurs as a cyclical process. Schumpeter proposed that radical innovations were the catalyst for Kondratiev cycles.\n\nThe Russian economist Kondratiev proposed that economic growth operated in boom and bust cycles of approximately 50 year periods. These cycles were characterised by periods of expansion, stagnation and recession. The period of expansion is associated with the introduction of a new technology, e.g. steam power or the microprocessor. At the time of publication, Kondratiev had considered that two cycles had occurred in the nineteenth century and third was beginning at the turn of the twentieth. Modern writers, such as Freeman and Perez outlined five cycles in the modern age:\n\n\nFreeman and Perez proposed that each cycle consists of pervasive technologies, their production and economic structures that support them. Termed ‘techno-economic paradigms’, they suggest that the shift from one paradigm to another is the result of emergent new technologies. \n\nFollowing the recent economic crisis, authors such as Moody and Nogrady have suggested that a new cycle is emerging from the old, centred on the use of sustainable technologies in a resource depleted world.\n\nThomas Kuhn described how a paradigm shift is a wholesale shift in the basic understanding of a scientific theory. Examples in science include the change of thought from miasma to germ theory as a cause of disease. Building on this work, Giovanni Dosi developed the concept of ’technical paradigms’ and ‘technological trajectories’. In considering how engineers work, the technical paradigm is an outlook on the technological problem, a definition of what the problems and solutions are. It charts the idea of specific progress. By identifying the problems to be solved the paradigm exerts an influence on technological change. The pattern of problem solving activity and the direction of progress is the technological trajectory. In similar fashion, Nelson and Winter (,)defined the concept of the ‘technological regime’ which directs technological change through the beliefs of engineers of what problems to solve. The work of the actors and organisations is the result of organisational and cognitive routines which determines search behaviour. This places boundaries and also trajectories (direction) to those boundaries.\n\nIn analysing (historic) cases of technological transitions researchers from the systems in transition branch of transitions research have used a multi-level perspective (MLP) as a heuristic model to understand changes in socio-technical systems. () Innovation system approaches traditionally focus on the production side. A socio-technical approach combines the science and technology in devising a production, with the application of the technology in fulfilling a societal function. Linking the two domains are the distribution, infrastructure and markets of the product. This approach considers a transition to be multi-dimensional as technology is only one aspect. \n\nThe MLP proposes three analytical levels: the niche, regime and landscape. \n\nNiche (Micro-level)\nRadical innovations occur at the niche level. These act as ‘safe havens’ for fledgling technologies to develop, largely free from market pressures which occur at the regime level. The US Military has acted as niche for major twentieth century technologies such as the aircraft, radio and the internet. More recently, California’s Silicon Valley has provided an arena for ICT focused technologies to emerge. Some innovations will challenge the existing regime while others fail. \n\nRegime (Meso-level)\nThe socio-technical regime, as defined by Geels, includes a web of inter-linking actors across different social groups and communities following a set of rules. In effect, the established practices of a given system. Seven dimensions have been identified in the socio-technical regime: technology, user practices and application, the symbolic meaning of technology, infrastructure, policy and techno-scientific knowledge. Change does occur at the regime level but it is normally slow and incremental unlike the radical change at the niche level. The actors who constitute the existing regime are set to gain from perpetuating the incumbent technology at the expense of the new. This is known as ‘lock-in’.\n\nLandscape (Macro-level)\nExogenous to the previous levels is the socio-technical landscape. A broad range of factors are contained here, such as economic pressures, cultural values, social trends, wars and environmental issues. Change occurs at an even slower rate than at the regime level. \n\nA transition is said to happen when a regime shift has occurred. This is the result of the interplay between the three levels. Regimes are relatively inert and resistant to change being structured to incremental innovation following established trajectories. As such, transitions are difficult to achieve. The current regime is typically suffering internal issues. Pressure from the landscape level may cause ‘cracks’ or ‘windows of opportunity’ through which innovations at the niche level may initially co-exist with the established technology before achieving ascendency. Once the technology has fully embedded into society the transition is said to be completed.\n\nThe MLP has been used in describing a range of historic transitions in socio-technical regimes for mobility, sanitation, food, lighting and so on. While early research focused on historical transitions, a second strand of research was more focused on transitions to sustainable technologies in key sectors such as transport, energy and housing.\n\nGeels presented three historical transitions on system innovation relating to modes of transportation. The technological transition from sailing ships to steamships in the UK will be summarised and shown in the context of a wider system innovation. \n\nGreat Britain was the world’s leading naval power in the nineteenth century, and led the way in the transition from sail to steam. At first, the introduction of steam technology co-existed with the current regime. Steam tugs assisted sail ships into port and hybrid steam / sail ships appeared. Landscape developments create the necessity for improvements in the technology. A demand for trans-Atlantic emigration was prompted by the Irish potato famine, European political instability and the lure of gold in California. The requirement for such arduous journeys had prompted a wealth of innovations at the niche level in steamship-development. From the late 1880s, as steamship technology improved and costs dropped, the new technology was widely diffused and a new regime established. The changes go beyond a technological transition as it involved new ship management and fleet management practices, new supporting infrastructures and new functionalities.\n\nThe nature of transitions varies and the differing qualities result in multiple pathways occurring. Geels and Schot defined five transition paths:\n\n\nSix characteristics of technological transitions have been identified.,\n\n\"Transitions are co-evolutionary and multi-dimensional\"\nTechnological developments occur intertwined with societal needs, wants and uses. A technology is adopted and diffused based on this interplay between innovation and societal requirements. Co-evolution has different aspects. As well as the co-evolution of technology and society, aspects between science, technology, users and culture have been considered.\n\"Multi-actors are involved\"\nScientific and engineering communities are central to the development of a technology, but a wide range of actors are involved in a transition. This can include organisations, policy-makers, government, NGOs, special interest groups and others.\n\n\"Transitions occur at multiple levels\"\nAs shown in the MLP transitions occur through the interplay of processes at different levels. \n\n\"Transitions are a long-term process\"\nComplete system-change takes time and can be decades in the making. Case studies show them to be between 40 and 90 years.\n\n\"Transitions are radical\"\nFor a true transition to occur the technology has to be a radical innovation. \n\n\"Change is Non-linear\"\nThe rate of change will vary over time. For example, the pace of change may be slow at the gestation period (at the niche level) but much more rapid when a breakthrough is occurring.\n\nDiffusion of an innovation is the concept of how it is picked up by society, at what rate and why. Everett (1962).The diffusion of a technological innovation into society can be considered in distinct phases. Pre-development is the gestation period where the new technology has yet to make an impact. Take-off is when the process of a system shift is beginning. A breakthrough is occurring when fundamental changes are occurring in existing structures through the interplay of economic, social and cultural forces. Once the rate of change has decreased and a new balance is achieved, stabilization is said to have occurred. A full transition involves an overhaul of existing rules and change of beliefs which takes time, typically spanning at least a generation. This process can be speeded-up through seismic, unforeseen events such as war or economic strife. \n\nGeels proposed a similar four phased approach which draws on the multi-level perspective (MLP) developed by Dutch scholars. Phases one sees the emergence of a novelty, born from the existing regime. Development then occurs in the niche level at phase two. As before, breakthrough then occurs at phase three. In the parlance of the MLP the new technology, having been developed at the niche level, is in competition with the established regime. To breakthrough and achieve wide diffusion, external factors – ‘windows of opportunity’ are required.\n\nA number of possible circumstances can act as windows of opportunity for the diffusion of new technologies: \n\n\nAlongside external influences, internal drivers catalyse diffusion. These include economic factors such as the price performance ration. Socio-technical perspectives focus on the links between disparate social and technological elements. Following the breakthrough, the final phases see the new technology supersede the old.\n\nThe study of technological transitions has an impact beyond academic interest. The transitions referred to in the literature may relate to historic processes, such as the transportation transitions studied by Geels, but system changes are required to achieve a safe transition to a low carbon-economy. (). Current structural problems are apparent in a range of sectors. Dependency on oil is problematic in the energy sector due to availability, access and contribution to greenhouse gas (GHG) emissions. Transportation is a major user of energy causing significant emission of GHGs. Food production will need to keep pace with an ever-growing world population while overcoming challenges presented by global warming and transportation issues. Incremental change has provided some improvements but a more radical transition is required to achieve a more sustainable future. \n\nDeveloped from the work on technological transitions is the field of transition management. Within this is an attempt to shape the direction of change complex socio-technical systems to more sustainable patterns. Whereas work on technological transitions is largely based on historic processes, proponents of transition management seek to actively steer transitions in progress.\n\nGenus and Coles outlined a number of criticisms against the analysis of technological transitions, in particular when using the MLP. Empirical research on technological transitions occurring now has been limited, with the focus on historic transitions. Depending on the perspective on transition case studies they could be presented as having occurred on a different transition path to what was shown. For example, the bicycle could be considered an intermediate transport technology between the horse and the car. Judged from shorter different time-frame this could appear a transition in its own right. Determining the nature of a transition is problematic; when it started and ended, or whether one occurred in the sense of a radical innovation displacing an existing socio-technical regime. The perception of time casts doubt on whether a transition has occurred. If viewed over a long enough period even inert regimes may demonstrate radical change in the end. The MLP has also been criticised by scholars studying sustainability transitions using Social Practice Theories.\n\n"}
{"id": "5217031", "url": "https://en.wikipedia.org/wiki?curid=5217031", "title": "The Information Society", "text": "The Information Society\n\nThe Information Society is a peer-reviewed academic journal on sociology, that was established in 1981. It is published five times per year by Routledge and covers topics related to information technologies and changes in society and culture. According to the \"Journal Citation Reports\", the journal has a 2014 impact factor of 1.048, ranking it 31st out of 85 journals in the category \"Information Science & Library Science\".\n"}
{"id": "58444", "url": "https://en.wikipedia.org/wiki?curid=58444", "title": "Timeline of communication technology", "text": "Timeline of communication technology\n\nTimeline of communication technology\n\n\n\n"}
