{"id": "20927521", "url": "https://en.wikipedia.org/wiki?curid=20927521", "title": "7th meridian east", "text": "7th meridian east\n\nThe meridian 7° east of Greenwich is a line of longitude that extends from the North Pole across the Arctic Ocean, Europe, Africa, the Atlantic Ocean, the Southern Ocean, and Antarctica to the South Pole.\n\nThe 7th meridian east forms a great circle with the 173rd meridian west.\n\nStarting at the North Pole and heading south to the South Pole, the 7th meridian east passes through:\n\n"}
{"id": "7301415", "url": "https://en.wikipedia.org/wiki?curid=7301415", "title": "AISTS", "text": "AISTS\n\nThe International Academy of Sport Science and Technology (French: \"Académie internationale des sciences et techniques du sport\", AISTS) is a non-profit foundation based in Lausanne, Switzerland, the Olympic Capital. Each year the AISTS delivers over 900 hours of continuing education, workshops, seminars and projects to the sports industry. The AISTS is committed to professionalising sports management through continuing education, applied research and an engaging platform for industry connections and is ranked the number 1 sports management programme in the world (Eduniversal 2015/16).\n\nThe International Academy of Sports Science and Technology is focused on sport management worldwide by offering sports management programs through sports management graduate programs, continuing professional education, and applied research in sport.\n\nThe AISTS mission is to apply knowledge to the study of sports, incorporating the disciplines of management, economics, technology, medicine, biology, law, logistics, sociology, and ethics. The AISTS supports sport innovation, technology transfer and business development.\n\nThe academy’s activities are built upon three pillars:\n\nThe main service offered by the academy is its Master of Advanced Studies in Sport Administration and Technology (MAS); a 15 month postgraduate master’s programme training sport executives.\n\nIn addition, continuing education seminars for sports management worldwide are organized under the lead of the academy including the “Sport Event Management and Organisation Seminar” (SEMOS), and the “Sustainable Sport and Events Open Module” (SSE). In addition the AISTS offers Risk Management Conferences on various topics including Security Risks at Sport Events and Sport Concussion for International Sport Governing Bodies.\n\nThe AISTS is incorporated as a foundation (Art 80ss, Swiss law), which forms a network of the following founding members:\n\nAISTS was founded in 2000 by the International Olympic Committee (IOC), the Swiss Federal Institute of Technology in Lausanne (EPFL), the International Institute for Management Development, the University of Lausanne, the University of Geneva, the Swiss Graduate School of Public Administration (IDHEAP), the École hôtelière de Lausanne (EHL), the City of Lausanne and the Canton of Vaud.\n\nAISTS has the support of the International Olympic Committee for sports management worldwide, and has strong links to the worldwide Olympic Movement.\n\nAISTS delivers knowledge from human sciences, life sciences and engineering sciences. AISTS acts as an advisor and manager to sporting and non-sporting organizations such as the International Olympic Committee, international sports federations (Ski, Volleyball, Basketball, etc.), and national sports federations.\n\nAISTS provides sports management graduate programs by the Master of Advanced Studies in Sport Administration and Technology (MAS)\n\nThe AISTS MAS (Master of Advanced Studies) in Sports Administration and Technology is a sports management graduate program in sports management. Training is offered in Sports Management & Economics, Technology, Law, Sociology, and sports Medicine. A multidisciplinary master's degree co-signed by the Swiss Federal Institute of Technology in Lausanne, the University of Geneva and the University of Lausanne. Every year, it attracts graduate students and professionals from all continents (85% of the students are not Swiss). MAS - Master of Advanced Studies in Sports Administration and Technology\n\nOver two days, the Sustainable Sport & Events Learning Module provides an understanding of the current challenges and opportunities of sustainability in sport. This understanding will be reinforced through a number of case studies, which offer concrete examples of what leading sports organisations, such as FIFA, the International Olympic Committee and the International Automobile Federation are achieving in this area. Participants will also gain insight into the best practices adopted by the American major leagues such as the NFL, NBA and MLB. Throughout both days, group work and discussions will challenge the existing ideas of sustainability in sport, with a particular focus on the business case for sustainability while simultaneously addressing the demands of multiple stakeholders.\n\nThe AISTS SEMOS module offers an overview of the key tasks that sports managers need to successfully plan, communicate and operate when organising sport events, providing a 360° view and behind the scenes look at the mechanics of sport events. Bringing together leaders of International Sport Federations and other experts of the AISTS network, including the International Olympic Committee and sport event organisers, AISTS SEMOS is both pertinent and practical, and recognises the increased expectations of spectators, media and sponsors at major events.\n\n\n"}
{"id": "13406872", "url": "https://en.wikipedia.org/wiki?curid=13406872", "title": "Acousto-optical spectrometer", "text": "Acousto-optical spectrometer\n\nAn acousto-optical spectrometer (AOS) is based on the diffraction of light by ultrasonic waves. A piezoelectric transducer, driven by the RF signal (from the receiver), generates an acoustic wave in a crystal (the so-called Bragg-cell). This acoustic wave modulates the refractive index and induces a phase grating. The Bragg-cell is illuminated by a collimated laser beam. The angular dispersion of the diffracted light represents a true image of the IF-spectrum according to the amplitude and wavelengths of the acoustic waves in the crystal. The spectrum is detected by using a single linear diode array (CCD), which is placed in the focal plane of an imaging optics. Depending on the crystal and the focal length of the imaging optics, the resolution of this type of spectrometer can be varied.\n\n"}
{"id": "58345806", "url": "https://en.wikipedia.org/wiki?curid=58345806", "title": "Adrenopause", "text": "Adrenopause\n\nAdrenopause is the decline in secretion and levels of adrenal androgens such as dehydroepiandrosterone (DHEA) and dehydroepiandrosterone sulfate (DHEA-S) from the zona reticularis of the adrenal glands with age. Levels of adrenal androgens start to increase around age 7 or 8 years (adrenarche), peak in early adulthood around age 20 to 25 years, and decrease at a rate of approximately 2% per year thereafter, eventually reaching levels of 10 to 20% of those of young adults by age 80 years. It is caused by the progressive apoptosis of adrenal androgen-secreting cells and hence involution of the zona reticularis. It is analogous to andropause in men and menopause in women, the abrupt or gradual decline in production of sex hormones from the gonads with age.\n\nDHEA can be supplemented or taken as a medication in the form of prasterone to replace adrenal androgens later in life if it is desired. Some clinical studies have found benefits of DHEA supplementation in the elderly and people with adrenal insufficiency.\n"}
{"id": "56036206", "url": "https://en.wikipedia.org/wiki?curid=56036206", "title": "Ann Hardy", "text": "Ann Hardy\n\nAnn Hardy (\"née\" Haley, born 20 April 1933) is an American computer programmer and entrepreneur, best known for her pioneering work on computer time-sharing systems while working at Tymshare from 1966 onwards.\n\nHardy was born in Chicago, Illinois. Her father had a small advertising agency and her mother, Ruth H. Ewing, was a high school math teacher and homemaker. Hardy was the eldest of five children in a conservative Methodist family. She grew up in Evanston, Illinois.\n\nIn 1951, Hardy graduated from Evanston Township High School. In 1955, Hardy graduated from Pomona College with a degree in physical education. She chose physical education because it was the only degree program which allowed her to take math and science courses after her desire to major in chemistry was stymied by the chemistry department head who was opposed to having women in the lab. Following her graduation, she took chemistry classes at Columbia University, but ultimately decided that a career in physical therapy had little appeal. On the advice of a friend working for IBM, she took the company's Programmer Aptitude Test when searching for a job.\n\nIn 1956, Hardy entered the programming field after taking IBM's Programmer Aptitude Test. Her official title was System Service Girl, which is equivalent to the position of a current day system engineer. Hardy then switched to programming and worked in IBM Research, which was in Poughkeepsie, and then in Ossining, New York. She worked at IBM for five years.\n\nA job on the STRETCH supercomputer project led to an offer to work at the Lawrence Radiation Laboratory in 1962. Hardy was one of a team of five who worked on the Livermore STRETCH's Fortran compiler from 1963 to 1966.\n\nIn February 1966, after her husband got a job at IBM in the Bay Area, Hardy got a job at Tymshare, a newly formed time-sharing company in Los Altos, California. She worked for Tymshare from 1966 to 1985.\n\nHardy worked on some of the first time-sharing systems and computer networks, used by a variety of corporations and government agencies. In 1968 she, with her husband Norm Hardy and LaRoy Tymes, first used minicomputers to log onto mainframe computers. She eventually rose to vice-president, the first woman in that role. Though Hardy was solely responsible for the writing the code for Tymshare's time-sharing product, many of her male colleagues assumed her husband had written the OS. It wasn't until she was in the hospital giving birth to her first child, and her coworkers encountered problems that they did not know how to fix, that they began to defer to her as the expert on the system she had written.\n\nAfter Tymshare was acquired by McDonnell Douglas in 1984, she left to found KeyLogic, which sold the timesharing hardware and software developed at Tymshare, under a licensing arrangement, until changing market conditions forced its closure in the early 1990s. She subsequently co-founded Agorics, which focuses on web-based marketplace applications.\n\nHardy's experiences are representative of the state of gendered labor in the field of computing during the mid to late 20th century: despite having technical skills and proving their value as workers, women in computing were continually undervalued as the field rose in power and importance. While at IBM, at one point Hardy learned that she was being paid less than one-half of the salary lowest ranked man who reported to her. As historians like Nathan Ensmenger, Janet Abbate, and Marie Hicks have shown, women programmers were denied credit for their work and historically submerged, leading to an inaccurate view of men's and women's roles in the field.\n\nIn 2004, Hardy retired. She currently serves as co-chair of the Software Industry Special Interest Group at the Computer History Museum.\n\nHardy married, and later divorced, Norman Hardy, also an alumnus of IBM and Tymshare. She has two daughters, born in 1968 and 1970; one became an environmentalist and the other a costume designer.\n\n"}
{"id": "2904730", "url": "https://en.wikipedia.org/wiki?curid=2904730", "title": "Araroba powder", "text": "Araroba powder\n\nAraroba powder is a drug occurring in the form of a yellowish-brown powder, varying considerably in tint, which derives an alternative name, Goa powder, from the Portuguese colony of Goa, where it appears to have been introduced about the year 1852, and is also known as Bahia powder.\n\nThe tree which yields it is the \"Andira Araroba\" of the natural order \"Leguminosae\". It is met with in great abundance in certain forests in the province of Bahia, preferring as a rule low and humid spots. The tree is from 80 to 100 ft. high and has large imparipinnate leaves, the leaflets of which are oblong, about 12 in. long and 1 in. broad, and somewhat truncate at the apex. The flowers are papilionaceous, of a purple color and arranged in panicles.\n\nThe Goa powder or araroba is contained in the trunk, filling crevices in the heartwood. It is a morbid product in the tree, and yields to hot chloroform 50% of a substance known officially as chrysarobin. It occurs as a micro-crystalline, odorless, tasteless powder, very slightly soluble in either water or alcohol; it also occurs in rhubarb root. This complex mixture contains pure chrysarobin, di-chrysarobin methylether, di-chrysarobin. Chrysarobin is a methyl trioxyanthracene and exists as a glucoside in the plant, but is gradually oxidized to chrysophanic acid (a dioxy-methyl anthraquinone) and glucose. This strikes a blood-red color in alkaline solutions, and may therefore cause much alarm if administered to a patient whose urine is alkaline. The British pharmacopoeia had an ointment containing one part of chrysarobin and 24 of benzoated lard.\n\nBoth internally and externally the drug is a powerful irritant. The general practice amongst modern dermatologists is to use only chrysophanic acid, which may be applied externally and given by the mouth in doses of about one grain in cases of psoriasis and chronic eczema. The drug is a feeble antiparasitic, and has been used locally in the treatment of ringworm. It stains the skin and linen a deep yellow or brown, a coloration which may be removed by caustic alkali in weak solution.\n"}
{"id": "10181793", "url": "https://en.wikipedia.org/wiki?curid=10181793", "title": "Azeotrope tables", "text": "Azeotrope tables\n\nThis page contains tables of azeotrope data for various binary and ternary mixtures of solvents. The data include the composition of a mixture by weight (in binary azeotropes, when only one fraction is given, it is the fraction of the second component), the boiling point (b.p.) of a component, the boiling point of a mixture, and the specific gravity of the mixture. Boiling points are reported at a pressure of 760 mm Hg unless otherwise stated. Where the mixture separates into layers, values are shown for upper (U) and lower (L) layers.\n\nThe data were obtained from Lange's 10th edition and CRC 44th edition unless otherwise noted (see color code table).\n\nA list of 15825 binary and ternary mixtures was collated and published by the ACS. An azeotrope databank is also available online through Edinburgh University\n\nTables of various ternary azeotropes (that is azeotropes consisting of three components). Fraction percentages are given by weight.\n"}
{"id": "24280214", "url": "https://en.wikipedia.org/wiki?curid=24280214", "title": "Bayesian cognitive science", "text": "Bayesian cognitive science\n\nBayesian Cognitive Science (also known as Computational Cognitive Science) is a rapidly growing approach to cognitive science concerned with the rational analysis of cognition through the use of Bayesian inference and cognitive modeling. The term \"computational\" refers to the computational level of analysis as put forth by David Marr.\n\nThis work often consists of testing the hypothesis that cognitive systems behave like rational Bayesian agents in particular types of tasks. Past work has applied this idea to categorization, language, motor control, sequence learning, reinforcement learning and theory of mind. At other times, Bayesian rationality is \"assumed\", and the goal is to infer the knowledge that agents have, and the mental representations that they use.\n\nIt is important to contrast this with the ordinary use of Bayesian inference in cognitive science, which is independent of rational modeling (see e.g. Michael Lee's work).\n\n\n"}
{"id": "34856488", "url": "https://en.wikipedia.org/wiki?curid=34856488", "title": "Boundary friction", "text": "Boundary friction\n\nBoundary friction occurs when a surface is at least partially wet, but not so lubricated that there is no direct friction between two surfaces.\n\nWhen two consistent, unlubricated surfaces slide against each other, there is a specific, predictable amount of friction that occurs. This amount increases as velocity does, but only up to a certain point. That increase generally follows what is known as a Stribeck curve, after Richard Stribeck. On the other hand, if the two surfaces are completely lubricated, there is no direct friction or rubbing at all. In real life, though, there is often a situation where the surfaces are not completely dry, but also not so lubricated that they do not touch.\n\nThis \"boundary friction\" produces various effects, like an increase in lubrication through the generation of shearing forces, or an oscillation effect during motion, as the friction increases and decreases.\n\nFor example, one can experience vibration when trying to brake on a partially damp road, or a cold glass that is slowly condensing moisture can be lifted until it spontaneously slides across the surface it is resting on.\n"}
{"id": "10094760", "url": "https://en.wikipedia.org/wiki?curid=10094760", "title": "Bureau-shaping model", "text": "Bureau-shaping model\n\nBureau-shaping is a rational choice model of bureaucracy and a response to the budget-maximization model. It argues that rational officials will not want to maximize their budgets, but instead to shape their agency so as to maximize their personal utilities from their work. For instance, bureaucrats would prefer to work in small, elite agencies close to political power centres and doing interesting work, rather than to run large-budget agencies with many staff but also many risks and problems. For the same reasons, and to avoid risks, the bureau-shaping model also predicts that senior government bureaucrats will often favour either 'agencification' to other public sector bodies (as in the UK 'Next Steps' programme) or off-loading functions to contractors and privatization. In the health and social work fields, officials will favour 'deinstitutionalization' and 'care in the community'. The model was developed by Patrick Dunleavy from the London School of Economics in \"Democracy, Bureaucracy and Public Choice\" (London: Pearson Education, 1991, reissued 2001).\n\nIt was propounded in response to William Niskanen's harsh criticism of Public Bureaucracies in his Budget Maximising Model. The Niskanen model predicts that in representative democracies, public bureaucracies will not only generate allocative inefficiency (by oversupplying public goods) but also x-inefficiency (by producing public goods inefficiently). It is evident that the Niskanen model is heavily reliant on an American institutional milieu. Patrick Dunleavy, a British political scientist who set out to demolish the public choice arguments on bureaucracy, came instead in the end to develop a public choice model of bureaucratic behaviour which combines elements of Peacock’s insight with the original American model. The Dunleavy (1985, p. 300) model of public bureaucracy is built on six basic assumptions. The first three are consistent with Niskanen’s model:\n\n(i) bureau policies are set by bureaucrats interacting with the government;\n\n(ii) governments largely depend on information from bureaus about the costs and value of producing within given ranges of output; and\n\n(iii) bureaucrats maximise their personal utilities (by satisfying \"self-regarding, relatively hard-edged preferences\") when making official decisions.\n\nAdded to these are two 4 assumptions which greatly weaken the budget-maximising conclusion. These are that a bureau’s aggregate policy behaviour is set by some combination of individual decisions made by its officials, although the actual combination that results may be an outcome desired by no bureau member; and that, within broad limits, officials’ influence on bureau policy is always correlated with rank and those nearest the top of bureaus are the most influential. Dunleavy therefore discards Niskanen’s assumption that a bureau’s behaviour will be wholly in line with the preferences of a single senior bureaucrat. In a bureau, where no individual has complete hegemony, budget maximisation is a collective, not an individual good. Rational utility maximising individuals will thus tend to favour strategies that directly advance their personal interests ahead of strategies that advance the collective good. The interaction of the maximising activities of individuals within a bureau will not necessarily lead to budget maximizing.\n"}
{"id": "51080574", "url": "https://en.wikipedia.org/wiki?curid=51080574", "title": "Chemical safety", "text": "Chemical safety\n\nChemical safety is the practice of handling chemicals in a safe manner, minimizing the hazard to public and personal health.\n\nChemical safety is the practice of minimizing risk of exposure to chemicals in any environment.\n\nDespite efforts to design in and maintain safety, accidents happen.\n\nGovernments regulate workplace health and safety.\n\n\n\n"}
{"id": "47278", "url": "https://en.wikipedia.org/wiki?curid=47278", "title": "Cognitive bias", "text": "Cognitive bias\n\nA cognitive bias is a systematic pattern of deviation from norm or rationality in judgment. Individuals create their own \"subjective social reality\" from their perception of the input. An individual's construction of social reality, not the objective input, may dictate their behaviour in the social world. Thus, cognitive biases may sometimes lead to perceptual distortion, inaccurate judgment, illogical interpretation, or what is broadly called irrationality.\n\nSome cognitive biases are presumably adaptive. Cognitive biases may lead to more effective actions in a given context. Furthermore, allowing cognitive biases enable faster decisions which can be desirable when timeliness is more valuable than accuracy, as illustrated in heuristics. Other cognitive biases are a \"by-product\" of human processing limitations, resulting from a lack of appropriate mental mechanisms (bounded rationality), or simply from a limited capacity for information processing.\n\nA continually evolving list of cognitive biases has been identified over the last six decades of research on human judgment and decision-making in cognitive science, social psychology, and behavioral economics. Kahneman and Tversky (1996) argue that cognitive biases have efficient practical implications for areas including clinical judgment, entrepreneurship, finance, and management.\n\nBias arises from various processes that are sometimes difficult to distinguish. These include\n\n\nThe notion of cognitive biases was introduced by Amos Tversky and Daniel Kahneman in 1972 and grew out of their experience of people's \"innumeracy\", or inability to reason intuitively with the greater orders of magnitude. Tversky, Kahneman and colleagues demonstrated several replicable ways in which human judgments and decisions differ from rational choice theory. Tversky and Kahneman explained human differences in judgement and decision making in terms of heuristics. Heuristics involve mental shortcuts which provide swift estimates about the possibility of uncertain occurrences. Heuristics are simple for the brain to compute but sometimes introduce \"severe and systematic errors.\"\n\nFor example, the representativeness heuristic is defined as the tendency to \"judge the frequency or likelihood\" of an occurrence by the extent of which the event \"resembles the typical case\". The \"Linda Problem\" illustrates the representativeness heuristic (Tversky & Kahneman, 1983). Participants were given a description of \"Linda\" that suggests Linda might well be a feminist (e.g., she is said to be concerned about discrimination and social justice issues). They were then asked whether they thought Linda was more likely to be a \"(a) bank teller\" or a \"(b) bank teller and active in the feminist movement\". A majority chose answer (b). This error (mathematically, answer (b) cannot be more likely than answer (a)) is an example of the \"conjunction fallacy\"; Tversky and Kahneman argued that respondents chose (b) because it seemed more \"representative\" or typical of persons who might fit the description of Linda. The representativeness heuristic may lead to errors such as activating stereotypes and inaccurate judgments of others (Haselton et al., 2005, p. 726).\n\nAlternatively, critics of Kahneman and Tversky such as Gerd Gigerenzer argue that heuristics should not lead us to conceive of human thinking as riddled with irrational cognitive biases, but rather to conceive rationality as an adaptive tool that is not identical to the rules of formal logic or the probability calculus. Nevertheless, experiments such as the \"Linda problem\" grew into the heuristics and biases research program which spread beyond academic psychology into other disciplines including medicine and political science.\n\nBiases can be distinguished on a number of dimensions. \nFor example,\nSome biases reflect a subject's \"motivation\", for example, the desire for a positive self-image leading to egocentric bias and the avoidance of unpleasant cognitive dissonance. Other biases are due to the particular way the brain perceives, forms memories and makes judgments. This distinction is sometimes described as \"hot cognition\" versus \"cold cognition\", as motivated reasoning can involve a state of arousal.\n\nAmong the \"cold\" biases,\n\n\nThe fact that some biases reflect motivation, and in particular the motivation to have positive attitudes to oneself accounts for the fact that many biases are self-serving or self-directed (e.g., illusion of asymmetric insight, self-serving bias). There are also biases in how subjects evaluate in-groups or out-groups; evaluating in-groups as more diverse and \"better\" in many respects, even when those groups are arbitrarily-defined (ingroup bias, outgroup homogeneity bias).\n\nSome cognitive biases belong to the subgroup of attentional biases which refer to the paying of increased attention to certain stimuli. It has been shown, for example, that people addicted to alcohol and other drugs pay more attention to drug-related stimuli. Common psychological tests to measure those biases are the Stroop task and the dot probe task.\n\nIndividuals' susceptibility to some types of cognitive biases can be measured by the Cognitive Reflection Test (CRT) developed by Frederick (2005).\n\nThe following is a list of the more commonly studied cognitive biases:\n\nA 2012 \"Psychological Bulletin\" article suggests that at least 8 seemingly unrelated biases can be produced by the same information-theoretic generative mechanism. It is shown that noisy deviations in the memory-based information processes that convert objective evidence (observations) into subjective estimates (decisions) can produce regressive conservatism, the belief revision (Bayesian conservatism), illusory correlations, illusory superiority (better-than-average effect) and worse-than-average effect, subadditivity effect, exaggerated expectation, overconfidence, and the hard–easy effect.\n\nMany social institutions rely on individuals to make rational judgments.\n\nThe securities regulation regime largely assumes that all investors act as perfectly rational persons. In truth, actual investors face cognitive limitations from biases, heuristics, and framing effects.\n\nA fair jury trial, for example, requires that the jury ignore irrelevant features of the case, weigh the relevant features appropriately, consider different possibilities open-mindedly and resist fallacies such as appeal to emotion. The various biases demonstrated in these psychological experiments suggest that people will frequently fail to do all these things. However, they fail to do so in systematic, directional ways that are predictable.\n\nCognitive biases are also related to the persistence of superstition, to large social issues such as prejudice, and they also work as a hindrance in the acceptance of scientific non-intuitive knowledge by the public.\n\nHowever, in some academic disciplines, the study of bias is very popular. For instance, bias is a wide spread phenomenon and well studied, because most decisions that concern the minds and hearts of entrepreneurs are computationally intractable\n\nSimilar to Gigerenzer (1996), Haselton et al. (2005) state the content and direction of cognitive biases are not \"arbitrary\" (p. 730). Moreover, cognitive biases can be controlled. Debiasing is a technique which aims to decrease biases by encouraging individuals to use controlled processing compared to automatic processing. In relation to reducing the FAE, monetary incentives and informing participants they will be held accountable for their attributions have been linked to the increase of accurate attributions. Training has also shown to reduce cognitive bias. Morewedge and colleagues (2015) found that research participants exposed to one-shot training interventions, such as educational videos and debiasing games that taught mitigating strategies, exhibited significant reductions in their commission of six cognitive biases immediately and up to 3 months later.\n\nCognitive bias modification refers to the process of modifying cognitive biases in healthy people and also refers to a growing area of psychological (non-pharmaceutical) therapies for anxiety, depression and addiction called cognitive bias modification therapy (CBMT). CBMT is sub-group of therapies within a growing area of psychological therapies based on modifying cognitive processes with or without accompanying medication and talk therapy, sometimes referred to as applied cognitive processing therapies (ACPT). Although cognitive bias modification can refer to modifying cognitive processes in healthy individuals, CBMT is a growing area of evidence-based psychological therapy, in which cognitive processes are modified to relieve suffering from serious depression, anxiety, and addiction. CBMT techniques are technology assisted therapies that are delivered via a computer with or without clinician support. CBM combines evidence and theory from the cognitive model of anxiety, cognitive neuroscience, and attentional models.\n\nThere are criticisms against theories of cognitive biases based on the fact that both sides in a debate often claim each other's thoughts to be in human nature and the result of cognitive bias, while claiming their own viewpoint as being the correct way to \"overcome\" cognitive bias. This is not due simply to debate misconduct but is a more fundamental problem that stems from psychology's making up of multiple opposed cognitive bias theories that can be non-falsifiably used to explain away any viewpoint.\n\n\n"}
{"id": "9223", "url": "https://en.wikipedia.org/wiki?curid=9223", "title": "Economics", "text": "Economics\n\nEconomics () is the social science that studies the production, distribution, and consumption of goods and services.\n\nEconomics focuses on the behaviour and interactions of economic agents and how economies work. Microeconomics analyzes basic elements in the economy, including individual agents and markets, their interactions, and the outcomes of interactions. Individual agents may include, for example, households, firms, buyers, and sellers. Macroeconomics analyzes the entire economy (meaning aggregated production, consumption, savings, and investment) and issues affecting it, including unemployment of resources (labour, capital, and land), inflation, economic growth, and the public policies that address these issues (monetary, fiscal, and other policies). See glossary of economics.\n\nOther broad distinctions within economics include those between positive economics, describing \"what is\", and normative economics, advocating \"what ought to be\"; between economic theory and applied economics; between rational and behavioural economics; and between mainstream economics and heterodox economics.\n\nEconomic analysis can be applied throughout society, in business, finance, health care, and government. Economic analysis is sometimes also applied to such diverse subjects as crime, education, the family, law, politics, religion, social institutions, war, science, and the environment.\n\nThe discipline was renamed in the late 19th century, primarily due to Alfred Marshall, from \"political economy\" to \"economics\" as a shorter term for \"economic science\". At that time, it became more open to rigorous thinking and made increased use of mathematics, which helped support efforts to have it accepted as a science and as a separate discipline outside of political science and other social sciences.\n\nThere are a variety of modern definitions of economics; some reflect evolving views of the subject or different views among economists. Scottish philosopher Adam Smith (1776) defined what was then called political economy as \"an inquiry into the nature and causes of the wealth of nations\", in particular as:\n\nJean-Baptiste Say (1803), distinguishing the subject from its public-policy uses, defines it as the science \"of\" production, distribution, and consumption of wealth. On the satirical side, Thomas Carlyle (1849) coined \"the dismal science\" as an epithet for classical economics, in this context, commonly linked to the pessimistic analysis of Malthus (1798). John Stuart Mill (1844) defines the subject in a social context as:\n\nAlfred Marshall provides a still widely cited definition in his textbook \"Principles of Economics\" (1890) that extends analysis beyond wealth and from the societal to the microeconomic level:\n\nLionel Robbins (1932) developed implications of what has been termed \"[p]erhaps the most commonly accepted current definition of the subject\":\n\nRobbins describes the definition as not \"classificatory\" in \"pick[ing] out certain \"kinds\" of behaviour\" but rather \"analytical\" in \"focus[ing] attention on a particular \"aspect\" of behaviour, the form imposed by the influence of scarcity.\" He affirmed that previous economists have usually centred their studies on the analysis of wealth: how wealth is created (production), distributed, and consumed; and how wealth can grow. But he said that economics can be used to study other things, such as war, that are outside its usual focus. This is because war has as the goal winning it (as a sought after end), generates both cost and benefits; and, resources (human life and other costs) are used to attain the goal. If the war is not winnable or if the expected costs outweigh the benefits, the deciding actors (assuming they are rational) may never go to war (a decision) but rather explore other alternatives. We cannot define economics as the science that studies wealth, war, crime, education, and any other field economic analysis can be applied to; but, as the science that studies a particular common aspect of each of those subjects (they all use scarce resources to attain a sought after end).\n\nSome subsequent comments criticized the definition as overly broad in failing to limit its subject matter to analysis of markets. From the 1960s, however, such comments abated as the economic theory of maximizing behaviour and rational-choice modelling expanded the domain of the subject to areas previously treated in other fields. There are other criticisms as well, such as in scarcity not accounting for the macroeconomics of high unemployment.\n\nGary Becker, a contributor to the expansion of economics into new areas, describes the approach he favours as \"combin[ing the] assumptions of maximizing behaviour, stable preferences, and market equilibrium, used relentlessly and unflinchingly.\" One commentary characterizes the remark as making economics an approach rather than a subject matter but with great specificity as to the \"choice process and the type of social interaction that [such] analysis involves.\" The same source reviews a range of definitions included in principles of economics textbooks and concludes that the lack of agreement need not affect the subject-matter that the texts treat. Among economists more generally, it argues that a particular definition presented may reflect the direction toward which the author believes economics is evolving, or should evolve.\n\nMicroeconomics examines how entities, forming a market structure, interact within a market to create a market system. These entities include private and public players with various classifications, typically operating under scarcity of tradable units and light government regulation. The item traded may be a tangible product such as apples or a service such as repair services, legal counsel, or entertainment.\n\nIn theory, in a free market the aggregates (sum of) of \"quantity demanded\" by buyers and \"quantity supplied\" by sellers may reach economic equilibrium over time in reaction to price changes; in practice, various issues may prevent equilibrium, and any equilibrium reached may not necessarily be morally equitable. For example, if the supply of healthcare services is limited by external factors, the equilibrium price may be unaffordable for many who desire it but cannot pay for it.\n\nVarious market structures exist. In perfectly competitive markets, no participants are large enough to have the market power to set the price of a homogeneous product. In other words, every participant is a \"price taker\" as no participant influences the price of a product. In the real world, markets often experience imperfect competition.\n\nForms include monopoly (in which there is only one seller of a good), duopoly (in which there are only two sellers of a good), oligopoly (in which there are few sellers of a good), monopolistic competition (in which there are many sellers producing highly differentiated goods), monopsony (in which there is only one buyer of a good), and oligopsony (in which there are few buyers of a good). Unlike perfect competition, imperfect competition invariably means market power is unequally distributed. Firms under imperfect competition have the potential to be \"price makers\", which means that, by holding a disproportionately high share of market power, they can influence the prices of their products.\n\nMicroeconomics studies individual markets by simplifying the economic system by assuming that activity in the market being analysed does not affect other markets. This method of analysis is known as partial-equilibrium analysis (supply and demand). This method aggregates (the sum of all activity) in only one market. General-equilibrium theory studies various markets and their behaviour. It aggregates (the sum of all activity) across \"all\" markets. This method studies both changes in markets and their interactions leading towards equilibrium.\n\nIn microeconomics, production is the conversion of inputs into outputs. It is an economic process that uses inputs to create a commodity or a service for exchange or direct use. Production is a flow and thus a rate of output per period of time. Distinctions include such production alternatives as for consumption (food, haircuts, etc.) vs. investment goods (new tractors, buildings, roads, etc.), public goods (national defence, smallpox vaccinations, etc.) or private goods (new computers, bananas, etc.), and \"guns\" vs \"butter\".\n\nOpportunity cost is the economic cost of production: the value of the next best opportunity foregone. Choices must be made between desirable yet mutually exclusive actions. It has been described as expressing \"the basic relationship between scarcity and choice\". For example, if a baker uses a sack of flour to make pretzels one morning, then the baker cannot use either the flour or the morning to make bagels instead. Part of the cost of making pretzels is that neither the flour nor the morning are available any longer, for use in some other way. The opportunity cost of an activity is an element in ensuring that scarce resources are used efficiently, such that the cost is weighed against the value of that activity in deciding on more or less of it. Opportunity costs are not restricted to monetary or financial costs but could be measured by the real cost of output forgone, leisure, or anything else that provides the alternative benefit (utility).\n\nInputs used in the production process include such primary factors of production as labour services, capital (durable produced goods used in production, such as an existing factory), and land (including natural resources). Other inputs may include intermediate goods used in production of final goods, such as the steel in a new car.\n\nEconomic efficiency measures how well a system generates desired output with a given set of inputs and available technology. Efficiency is improved if more output is generated without changing inputs, or in other words, the amount of \"waste\" is reduced. A widely accepted general standard is Pareto efficiency, which is reached when no further change can make someone better off without making someone else worse off.\nThe production–possibility frontier (PPF) is an expository figure for representing scarcity, cost, and efficiency. In the simplest case an economy can produce just two goods (say \"guns\" and \"butter\"). The PPF is a table or graph (as at the right) showing the different quantity combinations of the two goods producible with a given technology and total factor inputs, which limit feasible total output. Each point on the curve shows potential total output for the economy, which is the maximum feasible output of one good, given a feasible output quantity of the other good.\n\nScarcity is represented in the figure by people being willing but unable in the aggregate to consume \"beyond the PPF\" (such as at \"X\") and by the negative slope of the curve. If production of one good \"increases\" along the curve, production of the other good \"decreases\", an inverse relationship. This is because increasing output of one good requires transferring inputs to it from production of the other good, decreasing the latter.\n\nThe slope of the curve at a point on it gives the trade-off between the two goods. It measures what an additional unit of one good costs in units forgone of the other good, an example of a \"real opportunity cost\". Thus, if one more Gun costs 100 units of butter, the opportunity cost of one Gun is 100 Butter. \"Along the PPF\", scarcity implies that choosing \"more\" of one good in the aggregate entails doing with \"less\" of the other good. Still, in a market economy, movement along the curve may indicate that the choice of the increased output is anticipated to be worth the cost to the agents.\n\nBy construction, each point on the curve shows \"productive efficiency\" in maximizing output for given total inputs. A point \"inside\" the curve (as at \"A\"), is feasible but represents \"production inefficiency\" (wasteful use of inputs), in that output of \"one or both goods\" could increase by moving in a northeast direction to a point on the curve. Examples cited of such inefficiency include high unemployment during a business-cycle recession or economic organization of a country that discourages full use of resources. Being on the curve might still not fully satisfy allocative efficiency (also called Pareto efficiency) if it does not produce a mix of goods that consumers prefer over other points.\n\nMuch applied economics in public policy is concerned with determining how the efficiency of an economy can be improved. Recognizing the reality of scarcity and then figuring out how to organize society for the most efficient use of resources has been described as the \"essence of economics\", where the subject \"makes its unique contribution.\"\n\nSpecialization is considered key to economic efficiency based on theoretical and empirical considerations. Different individuals or nations may have different real opportunity costs of production, say from differences in stocks of human capital per worker or capital/labour ratios. According to theory, this may give a comparative advantage in production of goods that make more intensive use of the relatively more abundant, thus \"relatively\" cheaper, input.\n\nEven if one region has an absolute advantage as to the ratio of its outputs to inputs in every type of output, it may still specialize in the output in which it has a comparative advantage and thereby gain from trading with a region that lacks any absolute advantage but has a comparative advantage in producing something else.\n\nIt has been observed that a high volume of trade occurs among regions even with access to a similar technology and mix of factor inputs, including high-income countries. This has led to investigation of economies of scale and agglomeration to explain specialization in similar but differentiated product lines, to the overall benefit of respective trading parties or regions.\n\nThe general theory of specialization applies to trade among individuals, farms, manufacturers, service providers, and economies. Among each of these production systems, there may be a corresponding \"division of labour\" with different work groups specializing, or correspondingly different types of capital equipment and differentiated land uses.\n\nAn example that combines features above is a country that specializes in the production of high-tech knowledge products, as developed countries do, and trades with developing nations for goods produced in factories where labour is relatively cheap and plentiful, resulting in different in opportunity costs of production. More total output and utility thereby results from specializing in production and trading than if each country produced its own high-tech and low-tech products.\n\nTheory and observation set out the conditions such that market prices of outputs and productive inputs select an allocation of factor inputs by comparative advantage, so that (relatively) low-cost inputs go to producing low-cost outputs. In the process, aggregate output may increase as a by-product or by design. Such specialization of production creates opportunities for gains from trade whereby resource owners benefit from trade in the sale of one type of output for other, more highly valued goods. A measure of gains from trade is the \"increased income levels\" that trade may facilitate.\n\nPrices and quantities have been described as the most directly observable attributes of goods produced and exchanged in a market economy. The theory of supply and demand is an organizing principle for explaining how prices coordinate the amounts produced and consumed. In microeconomics, it applies to price and output determination for a market with perfect competition, which includes the condition of no buyers or sellers large enough to have price-setting power.\n\nFor a given market of a commodity, \"demand\" is the relation of the quantity that all buyers would be prepared to purchase at each unit price of the good. Demand is often represented by a table or a graph showing price and quantity demanded (as in the figure). Demand theory describes individual consumers as rationally choosing the most preferred quantity of each good, given income, prices, tastes, etc. A term for this is \"constrained utility maximization\" (with income and wealth as the constraints on demand). Here, utility refers to the hypothesized relation of each individual consumer for ranking different commodity bundles as more or less preferred.\n\nThe law of demand states that, in general, price and quantity demanded in a given market are inversely related. That is, the higher the price of a product, the less of it people would be prepared to buy (other things unchanged). As the price of a commodity falls, consumers move toward it from relatively more expensive goods (the substitution effect). In addition, purchasing power from the price decline increases ability to buy (the income effect). Other factors can change demand; for example an increase in income will shift the demand curve for a normal good outward relative to the origin, as in the figure. All determinants are predominantly taken as constant factors of demand and supply.\n\n\"Supply\" is the relation between the price of a good and the quantity available for sale at that price. It may be represented as a table or graph relating price and quantity supplied. Producers, for example business firms, are hypothesized to be \"profit maximizers\", meaning that they attempt to produce and supply the amount of goods that will bring them the highest profit. Supply is typically represented as a function relating price and quantity, if other factors are unchanged.\n\nThat is, the higher the price at which the good can be sold, the more of it producers will supply, as in the figure. The higher price makes it profitable to increase production. Just as on the demand side, the position of the supply can shift, say from a change in the price of a productive input or a technical improvement. The \"Law of Supply\" states that, in general, a rise in price leads to an expansion in supply and a fall in price leads to a contraction in supply. Here as well, the determinants of supply, such as price of substitutes, cost of production, technology applied and various factors inputs of production are all taken to be constant for a specific time period of evaluation of supply.\n\nMarket equilibrium occurs where quantity supplied equals quantity demanded, the intersection of the supply and demand curves in the figure above. At a price below equilibrium, there is a shortage of quantity supplied compared to quantity demanded. This is posited to bid the price up. At a price above equilibrium, there is a surplus of quantity supplied compared to quantity demanded. This pushes the price down. The model of supply and demand predicts that for given supply and demand curves, price and quantity will stabilize at the price that makes quantity supplied equal to quantity demanded. Similarly, demand-and-supply theory predicts a new price-quantity combination from a shift in demand (as to the figure), or in supply.\n\nFor a given quantity of a consumer good, the point on the demand curve indicates the value, or marginal utility, to consumers for that unit. It measures what the consumer would be prepared to pay for that unit. The corresponding point on the supply curve measures marginal cost, the increase in total cost to the supplier for the corresponding unit of the good. The price in equilibrium is determined by supply and demand. In a perfectly competitive market, supply and demand equate marginal cost and marginal utility at equilibrium.\n\nOn the supply side of the market, some factors of production are described as (relatively) \"variable\" in the short run, which affects the cost of changing output levels. Their usage rates can be changed easily, such as electrical power, raw-material inputs, and over-time and temp work. Other inputs are relatively \"fixed\", such as plant and equipment and key personnel. In the long run, all inputs may be adjusted by management. These distinctions translate to differences in the elasticity (responsiveness) of the supply curve in the short and long runs and corresponding differences in the price-quantity change from a shift on the supply or demand side of the market.\n\nMarginalist theory, such as above, describes the consumers as attempting to reach most-preferred positions, subject to income and wealth constraints while producers attempt to maximize profits subject to their own constraints, including demand for goods produced, technology, and the price of inputs. For the consumer, that point comes where marginal utility of a good, net of price, reaches zero, leaving no net gain from further consumption increases. Analogously, the producer compares marginal revenue (identical to price for the perfect competitor) against the marginal cost of a good, with \"marginal profit\" the difference. At the point where marginal profit reaches zero, further increases in production of the good stop. For movement to market equilibrium and for changes in equilibrium, price and quantity also change \"at the margin\": more-or-less of something, rather than necessarily all-or-nothing.\n\nOther applications of demand and supply include the distribution of income among the factors of production, including labour and capital, through factor markets. In a competitive labour market for example the quantity of labour employed and the price of labour (the wage rate) depends on the demand for labour (from employers for production) and supply of labour (from potential workers). Labour economics examines the interaction of workers and employers through such markets to explain patterns and changes of wages and other labour income, labour mobility, and (un)employment, productivity through human capital, and related public-policy issues.\n\nDemand-and-supply analysis is used to explain the behaviour of perfectly competitive markets, but as a standard of comparison it can be extended to any type of market. It can also be generalized to explain variables across the economy, for example, total output (estimated as real GDP) and the general price level, as studied in macroeconomics. Tracing the qualitative and quantitative effects of variables that change supply and demand, whether in the short or long run, is a standard exercise in applied economics. Economic theory may also specify conditions such that supply and demand through the market is an efficient mechanism for allocating resources.\n\nPeople frequently do not trade directly on markets. Instead, on the supply side, they may work in and produce through \"firms\". The most obvious kinds of firms are corporations, partnerships and trusts. According to Ronald Coase, people begin to organize their production in firms when the costs of doing business becomes lower than doing it on the market. Firms combine labour and capital, and can achieve far greater economies of scale (when the average cost per unit declines as more units are produced) than individual market trading.\n\nIn perfectly competitive markets studied in the theory of supply and demand, there are many producers, none of which significantly influence price. Industrial organization generalizes from that special case to study the strategic behaviour of firms that do have significant control of price. It considers the structure of such markets and their interactions. Common market structures studied besides perfect competition include monopolistic competition, various forms of oligopoly, and monopoly.\n\nManagerial economics applies microeconomic analysis to specific decisions in business firms or other management units. It draws heavily from quantitative methods such as operations research and programming and from statistical methods such as regression analysis in the absence of certainty and perfect knowledge. A unifying theme is the attempt to optimize business decisions, including unit-cost minimization and profit maximization, given the firm's objectives and constraints imposed by technology and market conditions.\n\nUncertainty in economics is an unknown prospect of gain or loss, whether quantifiable as risk or not. Without it, household behaviour would be unaffected by uncertain employment and income prospects, financial and capital markets would reduce to exchange of a single instrument in each market period, and there would be no communications industry. Given its different forms, there are various ways of representing uncertainty and modelling economic agents' responses to it.\n\nGame theory is a branch of applied mathematics that considers strategic interactions between agents, one kind of uncertainty. It provides a mathematical foundation of industrial organization, discussed above, to model different types of firm behaviour, for example in an solipsistic industry (few sellers), but equally applicable to wage negotiations, bargaining, contract design, and any situation where individual agents are few enough to have perceptible effects on each other. In behavioural economics, it has been used to model the strategies agents choose when interacting with others whose interests are at least partially adverse to their own.\n\nIn this, it generalizes maximization approaches developed to analyse market actors such as in the supply and demand model and allows for incomplete information of actors. The field dates from the 1944 classic \"Theory of Games and Economic Behavior\" by John von Neumann and Oskar Morgenstern. It has significant applications seemingly outside of economics in such diverse subjects as formulation of nuclear strategies, ethics, political science, and evolutionary biology.\n\nRisk aversion may stimulate activity that in well-functioning markets smooths out risk and communicates information about risk, as in markets for insurance, commodity futures contracts, and financial instruments. Financial economics or simply finance describes the allocation of financial resources. It also analyses the pricing of financial instruments, the financial structure of companies, the efficiency and fragility of financial markets, financial crises, and related government policy or regulation.\n\nSome market organizations may give rise to inefficiencies associated with uncertainty. Based on George Akerlof's \"Market for Lemons\" article, the paradigm example is of a dodgy second-hand car market. Customers without knowledge of whether a car is a \"lemon\" depress its price below what a quality second-hand car would be. Information asymmetry arises here, if the seller has more relevant information than the buyer but no incentive to disclose it. Related problems in insurance are adverse selection, such that those at most risk are most likely to insure (say reckless drivers), and moral hazard, such that insurance results in riskier behaviour (say more reckless driving).\n\nBoth problems may raise insurance costs and reduce efficiency by driving otherwise willing transactors from the market (\"incomplete markets\"). Moreover, attempting to reduce one problem, say adverse selection by mandating insurance, may add to another, say moral hazard. Information economics, which studies such problems, has relevance in subjects such as insurance, contract law, mechanism design, monetary economics, and health care. Applied subjects include market and legal remedies to spread or reduce risk, such as warranties, government-mandated partial insurance, restructuring or bankruptcy law, inspection, and regulation for quality and information disclosure.\n\nThe term \"market failure\" encompasses several problems which may undermine standard economic assumptions. Although economists categorize market failures differently, the following categories emerge in the main texts.\n\nInformation asymmetries and incomplete markets may result in economic inefficiency but also a possibility of improving efficiency through market, legal, and regulatory remedies, as discussed above.\n\nNatural monopoly, or the overlapping concepts of \"practical\" and \"technical\" monopoly, is an extreme case of \"failure of competition\" as a restraint on producers. Extreme economies of scale are one possible cause.\n\nPublic goods are goods which are under-supplied in a typical market. The defining features are that people can consume public goods without having to pay for them and that more than one person can consume the good at the same time.\n\nExternalities occur where there are significant social costs or benefits from production or consumption that are not reflected in market prices. For example, air pollution may generate a negative externality, and education may generate a positive externality (less crime, etc.). Governments often tax and otherwise restrict the sale of goods that have negative externalities and subsidize or otherwise promote the purchase of goods that have positive externalities in an effort to correct the price distortions caused by these externalities. Elementary demand-and-supply theory predicts equilibrium but not the speed of adjustment for changes of equilibrium due to a shift in demand or supply.\n\nIn many areas, some form of price stickiness is postulated to account for quantities, rather than prices, adjusting in the short run to changes on the demand side or the supply side. This includes standard analysis of the business cycle in macroeconomics. Analysis often revolves around causes of such price stickiness and their implications for reaching a hypothesized long-run equilibrium. Examples of such price stickiness in particular markets include wage rates in labour markets and posted prices in markets deviating from perfect competition.\n\nSome specialized fields of economics deal in market failure more than others. The economics of the public sector is one example. Much environmental economics concerns externalities or \"public bads\".\n\nPolicy options include regulations that reflect cost-benefit analysis or market solutions that change incentives, such as emission fees or redefinition of property rights.\n\nPublic finance is the field of economics that deals with budgeting the revenues and expenditures of a public sector entity, usually government. The subject addresses such matters as tax incidence (who really pays a particular tax), cost-benefit analysis of government programmes, effects on economic efficiency and income distribution of different kinds of spending and taxes, and fiscal politics. The latter, an aspect of public choice theory, models public-sector behaviour analogously to microeconomics, involving interactions of self-interested voters, politicians, and bureaucrats.\n\nMuch of economics is positive, seeking to describe and predict economic phenomena. Normative economics seeks to identify what economies \"ought\" to be like.\n\nWelfare economics is a normative branch of economics that uses microeconomic techniques to simultaneously determine the allocative efficiency within an economy and the income distribution associated with it. It attempts to measure social welfare by examining the economic activities of the individuals that comprise society.\n\nMacroeconomics examines the economy as a whole to explain broad aggregates and their interactions \"top down\", that is, using a simplified form of general-equilibrium theory. Such aggregates include national income and output, the unemployment rate, and price inflation and subaggregates like total consumption and investment spending and their components. It also studies effects of monetary policy and fiscal policy.\n\nSince at least the 1960s, macroeconomics has been characterized by further integration as to micro-based modelling of sectors, including rationality of players, efficient use of market information, and imperfect competition. This has addressed a long-standing concern about inconsistent developments of the same subject.\n\nMacroeconomic analysis also considers factors affecting the long-term level and growth of national income. Such factors include capital accumulation, technological change and labour force growth.\n\n\"Growth economics\" studies factors that explain economic growth – the increase in output \"per capita\" of a country over a long period of time. The same factors are used to explain differences in the \"level\" of output \"per capita\" \"between\" countries, in particular why some countries grow faster than others, and whether countries converge at the same rates of growth.\n\nMuch-studied factors include the rate of investment, population growth, and technological change. These are represented in theoretical and empirical forms (as in the neoclassical and endogenous growth models) and in growth accounting.\n\nThe economics of a depression were the spur for the creation of \"macroeconomics\" as a separate discipline field of study. During the Great Depression of the 1930s, John Maynard Keynes authored a book entitled \"The General Theory of Employment, Interest and Money\" outlining the key theories of Keynesian economics. Keynes contended that aggregate demand for goods might be insufficient during economic downturns, leading to unnecessarily high unemployment and losses of potential output.\n\nHe therefore advocated active policy responses by the public sector, including monetary policy actions by the central bank and fiscal policy actions by the government to stabilize output over the business cycle.\nThus, a central conclusion of Keynesian economics is that, in some situations, no strong automatic mechanism moves output and employment towards full employment levels. John Hicks' IS/LM model has been the most influential interpretation of \"The General Theory\".\n\nOver the years, understanding of the business cycle has branched into various research programmes, mostly related to or distinct from Keynesianism. The neoclassical synthesis refers to the reconciliation of Keynesian economics with neoclassical economics, stating that Keynesianism is correct in the short run but qualified by neoclassical-like considerations in the intermediate and long run.\n\nNew classical macroeconomics, as distinct from the Keynesian view of the business cycle, posits market clearing with imperfect information. It includes Friedman's permanent income hypothesis on consumption and \"rational expectations\" theory, led by Robert Lucas, and real business cycle theory.\n\nIn contrast, the new Keynesian approach retains the rational expectations assumption, however it assumes a variety of market failures. In particular, New Keynesians assume prices and wages are \"sticky\", which means they do not adjust instantaneously to changes in economic conditions.\n\nThus, the new classicals assume that prices and wages adjust automatically to attain full employment, whereas the new Keynesians see full employment as being automatically achieved only in the long run, and hence government and central-bank policies are needed because the \"long run\" may be very long.\n\nThe amount of unemployment in an economy is measured by the unemployment rate, the percentage of workers without jobs in the labour force. The labour force only includes workers actively looking for jobs. People who are retired, pursuing education, or discouraged from seeking work by a lack of job prospects are excluded from the labour force. Unemployment can be generally broken down into several types that are related to different causes.\n\nClassical models of unemployment occurs when wages are too high for employers to be willing to hire more workers. Wages may be too high because of minimum wage laws or union activity. Consistent with classical unemployment, frictional unemployment occurs when appropriate job vacancies exist for a worker, but the length of time needed to search for and find the job leads to a period of unemployment.\n\nStructural unemployment covers a variety of possible causes of unemployment including a mismatch between workers' skills and the skills required for open jobs. Large amounts of structural unemployment can occur when an economy is transitioning industries and workers find their previous set of skills are no longer in demand. Structural unemployment is similar to frictional unemployment since both reflect the problem of matching workers with job vacancies, but structural unemployment covers the time needed to acquire new skills not just the short term search process.\n\nWhile some types of unemployment may occur regardless of the condition of the economy, cyclical unemployment occurs when growth stagnates. Okun's law represents the empirical relationship between unemployment and economic growth. The original version of Okun's law states that a 3% increase in output would lead to a 1% decrease in unemployment.\n\nMoney is a \"means of final payment\" for goods in most price system economies, and is the unit of account in which prices are typically stated. Money has general acceptability, relative consistency in value, divisibility, durability, portability, elasticity in supply, and longevity with mass public confidence. It includes currency held by the nonbank public and checkable deposits. It has been described as a social convention, like language, useful to one largely because it is useful to others. In the words of Francis Amasa Walker, a well-known 19th-century economist, \"Money is what money does\" (\"Money is \"that\" money does\" in the original).\n\nAs a medium of exchange, money facilitates trade. It is essentially a measure of value and more importantly, a store of value being a basis for credit creation. Its economic function can be contrasted with barter (non-monetary exchange). Given a diverse array of produced goods and specialized producers, barter may entail a hard-to-locate double coincidence of wants as to what is exchanged, say apples and a book. Money can reduce the transaction cost of exchange because of its ready acceptability. Then it is less costly for the seller to accept money in exchange, rather than what the buyer produces.\n\nAt the level of an economy, theory and evidence are consistent with a positive relationship running from the total money supply to the nominal value of total output and to the general price level. For this reason, management of the money supply is a key aspect of monetary policy.\n\nGovernments implement fiscal policy to influence macroeconomic conditions by adjusting spending and taxation policies to alter aggregate demand. When aggregate demand falls below the potential output of the economy, there is an output gap where some productive capacity is left unemployed. Governments increase spending and cut taxes to boost aggregate demand. Resources that have been idled can be used by the government.\n\nFor example, unemployed home builders can be hired to expand highways. Tax cuts allow consumers to increase their spending, which boosts aggregate demand. Both tax cuts and spending have multiplier effects where the initial increase in demand from the policy percolates through the economy and generates additional economic activity.\n\nThe effects of fiscal policy can be limited by crowding out. When there is no output gap, the economy is producing at full capacity and there are no excess productive resources. If the government increases spending in this situation, the government uses resources that otherwise would have been used by the private sector, so there is no increase in overall output. Some economists think that crowding out is always an issue while others do not think it is a major issue when output is depressed.\n\nSceptics of fiscal policy also make the argument of Ricardian equivalence. They argue that an increase in debt will have to be paid for with future tax increases, which will cause people to reduce their consumption and save money to pay for the future tax increase. Under Ricardian equivalence, any boost in demand from tax cuts will be offset by the increased saving intended to pay for future higher taxes.\n\nInternational trade studies determinants of goods-and-services flows across international boundaries. It also concerns the size and distribution of gains from trade. Policy applications include estimating the effects of changing tariff rates and trade quotas. International finance is a macroeconomic field which examines the flow of capital across international borders, and the effects of these movements on exchange rates. Increased trade in goods, services and capital between countries is a major effect of contemporary globalization.\n\nThe distinct field of \"development economics\" examines economic aspects of the economic development process in relatively low-income countries focusing on structural change, poverty, and economic growth. Approaches in development economics frequently incorporate social and political factors.\n\nEconomic systems is the of economics that studies the methods and institutions by which societies determine the ownership, direction, and allocation of economic resources. An \"economic system\" of a society is the unit of analysis.\n\nAmong contemporary systems at different ends of the organizational spectrum are socialist systems and capitalist systems, in which most production occurs in respectively state-run and private enterprises. In between are mixed economies. A common element is the interaction of economic and political influences, broadly described as political economy. \"Comparative economic systems\" studies the relative performance and behaviour of different economies or systems.\n\nThe U.S. Export-Import Bank defines a Marxist–Leninist state as having a centrally planned economy. They are now rare; examples can still be seen in Cuba, North Korea and Laos.\n\nContemporary economics uses mathematics. Economists draw on the tools of calculus, linear algebra, statistics, game theory, and computer science. Professional economists are expected to be familiar with these tools, while a minority specialize in econometrics and mathematical methods.\n\nMainstream economic theory relies upon \"a priori\" quantitative economic models, which employ a variety of concepts. Theory typically proceeds with an assumption of \"ceteris paribus\", which means holding constant explanatory variables other than the one under consideration. When creating theories, the objective is to find ones which are at least as simple in information requirements, more precise in predictions, and more fruitful in generating additional research than prior theories. While neoclassical economic theory constitutes both the dominant or orthodox theoretical as well as methodological framework, economic theory can also take the form of other schools of thought such as in heterodox economic theories.\n\nIn microeconomics, principal concepts include supply and demand, marginalism, rational choice theory, opportunity cost, budget constraints, utility, and the theory of the firm. Early macroeconomic models focused on modelling the relationships between aggregate variables, but as the relationships appeared to change over time macroeconomists, including new Keynesians, reformulated their models in microfoundations.\n\nThe aforementioned microeconomic concepts play a major part in macroeconomic models – for instance, in monetary theory, the quantity theory of money predicts that increases in the growth rate of the money supply increase inflation, and inflation is assumed to be influenced by rational expectations. In development economics, slower growth in developed nations has been sometimes predicted because of the declining marginal returns of investment and capital, and this has been observed in the Four Asian Tigers. Sometimes an economic hypothesis is only \"qualitative\", not \"quantitative\".\n\nExpositions of economic reasoning often use two-dimensional graphs to illustrate theoretical relationships. At a higher level of generality, Paul Samuelson's treatise \"Foundations of Economic Analysis\" (1947) used mathematical methods beyond graphs to represent the theory, particularly as to maximizing behavioural relations of agents reaching equilibrium. The book focused on examining the class of statements called \"operationally meaningful theorems\" in economics, which are theorems that can conceivably be refuted by empirical data.\n\nEconomic theories are frequently tested empirically, largely through the use of econometrics using economic data. The controlled experiments common to the physical sciences are difficult and uncommon in economics, and instead broad data is observationally studied; this type of testing is typically regarded as less rigorous than controlled experimentation, and the conclusions typically more tentative. However, the field of experimental economics is growing, and increasing use is being made of natural experiments.\n\nStatistical methods such as regression analysis are common. Practitioners use such methods to estimate the size, economic significance, and statistical significance (\"signal strength\") of the hypothesized relation(s) and to adjust for noise from other variables. By such means, a hypothesis may gain acceptance, although in a probabilistic, rather than certain, sense. Acceptance is dependent upon the falsifiable hypothesis surviving tests. Use of commonly accepted methods need not produce a final conclusion or even a consensus on a particular question, given different tests, data sets, and prior beliefs.\n\nCriticisms based on professional standards and non-replicability of results serve as further checks against bias, errors, and over-generalization, although much economic research has been accused of being non-replicable, and prestigious journals have been accused of not facilitating replication through the provision of the code and data. Like theories, uses of test statistics are themselves open to critical analysis, although critical commentary on papers in economics in prestigious journals such as the \"American Economic Review\" has declined precipitously in the past 40 years. This has been attributed to journals' incentives to maximize citations in order to rank higher on the Social Science Citation Index (SSCI).\n\nIn applied economics, input-output models employing linear programming methods are quite common. Large amounts of data are run through computer programs to analyse the impact of certain policies; IMPLAN is one well-known example.\n\nExperimental economics has promoted the use of scientifically controlled experiments. This has reduced the long-noted distinction of economics from natural sciences because it allows direct tests of what were previously taken as axioms. In some cases these have found that the axioms are not entirely correct; for example, the ultimatum game has revealed that people reject unequal offers.\n\nIn behavioural economics, psychologist Daniel Kahneman won the Nobel Prize in economics in 2002 for his and Amos Tversky's empirical discovery of several cognitive biases and heuristics. Similar empirical testing occurs in neuroeconomics. Another example is the assumption of narrowly selfish preferences versus a model that tests for selfish, altruistic, and cooperative preferences. These techniques have led some to argue that economics is a \"genuine science\".\n\nThe professionalization of economics, reflected in the growth of graduate programmes on the subject, has been described as \"the main change in economics since around 1900\". Most major universities and many colleges have a major, school, or department in which academic degrees are awarded in the subject, whether in the liberal arts, business, or for professional study.\n\nIn the private sector, professional economists are employed as consultants and in industry, including banking and finance. Economists also work for various government departments and agencies, for example, the national Treasury, Central Bank or Bureau of Statistics.\n\nThe Nobel Memorial Prize in Economic Sciences (commonly known as the Nobel Prize in Economics) is a prize awarded to economists each year for outstanding intellectual contributions in the field.\n\nEconomics is one social science among several and has fields bordering on other areas, including economic geography, economic history, public choice, energy economics, , family economics and institutional economics.\n\nLaw and economics, or economic analysis of law, is an approach to legal theory that applies methods of economics to law. It includes the use of economic concepts to explain the effects of legal rules, to assess which legal rules are economically efficient, and to predict what the legal rules will be. A seminal article by Ronald Coase published in 1961 suggested that well-defined property rights could overcome the problems of externalities.\n\nPolitical economy is the interdisciplinary study that combines economics, law, and political science in explaining how political institutions, the political environment, and the economic system (capitalist, socialist, mixed) influence each other. It studies questions such as how monopoly, rent-seeking behaviour, and externalities should impact government policy. Historians have employed \"political economy\" to explore the ways in the past that persons and groups with common economic interests have used politics to effect changes beneficial to their interests.\n\nEnergy economics is a broad scientific subject area which includes topics related to energy supply and energy demand. Georgescu-Roegen reintroduced the concept of entropy in relation to economics and energy from thermodynamics, as distinguished from what he viewed as the mechanistic foundation of neoclassical economics drawn from Newtonian physics. His work contributed significantly to thermoeconomics and to ecological economics. He also did foundational work which later developed into evolutionary economics.\n\nThe sociological subfield of economic sociology arose, primarily through the work of Émile Durkheim, Max Weber and Georg Simmel, as an approach to analysing the effects of economic phenomena in relation to the overarching social paradigm (i.e. modernity). Classic works include Max Weber's \"The Protestant Ethic and the Spirit of Capitalism\" (1905) and Georg Simmel's \"The Philosophy of Money\" (1900). More recently, the works of Mark Granovetter, Peter Hedstrom and Richard Swedberg have been influential in this field.\n\nEconomic writings date from earlier Mesopotamian, Greek, Roman, Indian subcontinent, Chinese, Persian, and Arab civilizations. Economic precepts occur throughout the writings of the Boeotian poet Hesiod and several economic historians have described Hesiod himself as the \"first economist\". Other notable writers from Antiquity through to the Renaissance include Aristotle, Xenophon, Chanakya (also known as Kautilya), Qin Shi Huang, Thomas Aquinas, and Ibn Khaldun. Joseph Schumpeter described Aquinas as \"coming nearer than any other group to being the \"founders' of scientific economics\" as to monetary, interest, and value theory within a natural-law perspective.\n\nTwo groups, later called \"mercantilists\" and \"physiocrats\", more directly influenced the subsequent development of the subject. Both groups were associated with the rise of economic nationalism and modern capitalism in Europe. Mercantilism was an economic doctrine that flourished from the 16th to 18th century in a prolific pamphlet literature, whether of merchants or statesmen. It held that a nation's wealth depended on its accumulation of gold and silver. Nations without access to mines could obtain gold and silver from trade only by selling goods abroad and restricting imports other than of gold and silver. The doctrine called for importing cheap raw materials to be used in manufacturing goods, which could be exported, and for state regulation to impose protective tariffs on foreign manufactured goods and prohibit manufacturing in the colonies.\n\nPhysiocrats, a group of 18th-century French thinkers and writers, developed the idea of the economy as a circular flow of income and output. Physiocrats believed that only agricultural production generated a clear surplus over cost, so that agriculture was the basis of all wealth. Thus, they opposed the mercantilist policy of promoting manufacturing and trade at the expense of agriculture, including import tariffs. Physiocrats advocated replacing administratively costly tax collections with a single tax on income of land owners. In reaction against copious mercantilist trade regulations, the physiocrats advocated a policy of \"laissez-faire\", which called for minimal government intervention in the economy.\n\nAdam Smith (1723–1790) was an early economic theorist. Smith was harshly critical of the mercantilists but described the physiocratic system \"with all its imperfections\" as \"perhaps the purest approximation to the truth that has yet been published\" on the subject.\n\nThe publication of Adam Smith's \"The Wealth of Nations\" in 1776, has been described as \"the effective birth of economics as a separate discipline.\" The book identified land, labour, and capital as the three factors of production and the major contributors to a nation's wealth, as distinct from the physiocratic idea that only agriculture was productive.\n\nSmith discusses potential benefits of specialization by division of labour, including increased labour productivity and gains from trade, whether between town and country or across countries. His \"theorem\" that \"the division of labor is limited by the extent of the market\" has been described as the \"core of a theory of the functions of firm and industry\" and a \"fundamental principle of economic organization.\" To Smith has also been ascribed \"the most important substantive proposition in all of economics\" and foundation of resource-allocation theory – that, under competition, resource owners (of labour, land, and capital) seek their most profitable uses, resulting in an equal rate of return for all uses in equilibrium (adjusted for apparent differences arising from such factors as training and unemployment).\n\nIn an argument that includes \"one of the most famous passages in all economics,\" Smith represents every individual as trying to employ any capital they might command for their own advantage, not that of the society, and for the sake of profit, which is necessary at some level for employing capital in domestic industry, and positively related to the value of produce. In this:\n\nThe Rev. Thomas Robert Malthus (1798) used the concept of diminishing returns to explain low living standards. Human population, he argued, tended to increase geometrically, outstripping the production of food, which increased arithmetically. The force of a rapidly growing population against a limited amount of land meant diminishing returns to labour. The result, he claimed, was chronically low wages, which prevented the standard of living for most of the population from rising above the subsistence level. Economist Julian Lincoln Simon has criticized Malthus's conclusions.\n\nWhile Adam Smith emphasized the production of income, David Ricardo (1817) focused on the distribution of income among landowners, workers, and capitalists. Ricardo saw an inherent conflict between landowners on the one hand and labour and capital on the other. He posited that the growth of population and capital, pressing against a fixed supply of land, pushes up rents and holds down wages and profits. Ricardo was the first to state and prove the principle of comparative advantage, according to which each country should specialize in producing and exporting goods in that it has a lower \"relative\" cost of production, rather relying only on its own production. It has been termed a \"fundamental analytical explanation\" for gains from trade.\n\nComing at the end of the classical tradition, John Stuart Mill (1848) parted company with the earlier classical economists on the inevitability of the distribution of income produced by the market system. Mill pointed to a distinct difference between the market's two roles: allocation of resources and distribution of income. The market might be efficient in allocating resources but not in distributing income, he wrote, making it necessary for society to intervene.\n\nValue theory was important in classical theory. Smith wrote that the \"real price of every thing ... is the toil and trouble of acquiring it\". Smith maintained that, with rent and profit, other costs besides wages also enter the price of a commodity. Other classical economists presented variations on Smith, termed the 'labour theory of value'. Classical economics focused on the tendency of any market economy to settle in a final stationary state made up of a constant stock of physical wealth (capital) and a constant population size.\n\nMarxist (later, Marxian) economics descends from classical economics. It derives from the work of Karl Marx. The first volume of Marx's major work, \"Das Kapital\", was published in German in 1867. In it, Marx focused on the labour theory of value and the theory of surplus value which, he believed, explained the exploitation of labour by capital. The labour theory of value held that the value of an exchanged commodity was determined by the labour that went into its production and the theory of surplus value demonstrated how the workers only got paid a proportion of the value their work had created.\n\nAt the dawn as a social science, economics was defined and discussed at length as the study of production, distribution, and consumption of wealth by Jean-Baptiste Say in his \"Treatise on Political Economy or, The Production, Distribution, and Consumption of Wealth\" (1803). These three items are considered by the science only in relation to the increase or diminution of wealth, and not in reference to their processes of execution. Say's definition has prevailed up to our time, saved by substituting the word \"wealth\" for \"goods and services\" meaning that wealth may include non-material objects as well. One hundred and thirty years later, Lionel Robbins noticed that this definition no longer sufficed, because many economists were making theoretical and philosophical inroads in other areas of human activity. In his \"Essay on the Nature and Significance of Economic Science\", he proposed a definition of economics as a study of a particular aspect of human behaviour, the one that falls under the influence of scarcity, which forces people to choose, allocate scarce resources to competing ends, and economize (seeking the greatest welfare while avoiding the wasting of scarce resources). For Robbins, the insufficiency was solved, and his definition allows us to proclaim, with an easy conscience, education economics, safety and security economics, health economics, war economics, and of course, production, distribution and consumption economics as valid subjects of the economic science.\"\n\nCiting Robbins: \"Economics is the science which studies human behavior as a relationship between ends and scarce means which have alternative uses\". After discussing it for decades, Robbins' definition became widely accepted by mainstream economists, and it has opened way into current textbooks. Although far from unanimous, most mainstream economists would accept some version of Robbins' definition, even though many have raised serious objections to the scope and method of economics, emanating from that definition. Due to the lack of strong consensus, and that production, distribution and consumption of goods and services is the prime area of study of economics, the old definition still stands in many quarters.\n\nA body of theory later termed \"neoclassical economics\" or \"marginalism\" formed from about 1870 to 1910. The term \"economics\" was popularized by such neoclassical economists as Alfred Marshall as a concise synonym for \"economic science\" and a substitute for the earlier \"political economy\". This corresponded to the influence on the subject of mathematical methods used in the natural sciences.\n\nNeoclassical economics systematized supply and demand as joint determinants of price and quantity in market equilibrium, affecting both the allocation of output and the distribution of income. It dispensed with the labour theory of value inherited from classical economics in favour of a marginal utility theory of value on the demand side and a more general theory of costs on the supply side. In the 20th century, neoclassical theorists moved away from an earlier notion suggesting that total utility for a society could be measured in favour of ordinal utility, which hypothesizes merely behaviour-based relations across persons.\n\nIn microeconomics, neoclassical economics represents incentives and costs as playing a pervasive role in shaping decision making. An immediate example of this is the consumer theory of individual demand, which isolates how prices (as costs) and income affect quantity demanded. In macroeconomics it is reflected in an early and lasting neoclassical synthesis with Keynesian macroeconomics.\n\nNeoclassical economics is occasionally referred as \"orthodox economics\" whether by its critics or sympathizers. Modern mainstream economics builds on neoclassical economics but with many refinements that either supplement or generalize earlier analysis, such as econometrics, game theory, analysis of market failure and imperfect competition, and the neoclassical model of economic growth for analysing long-run variables affecting national income.\n\nNeoclassical economics studies the behaviour of individuals, households, and organizations (called economic actors, players, or agents), when they manage or use scarce resources, which have alternative uses, to achieve desired ends. Agents are assumed to act rationally, have multiple desirable ends in sight, limited resources to obtain these ends, a set of stable preferences, a definite overall guiding objective, and the capability of making a choice. There exists an economic problem, subject to study by economic science, when a decision (choice) is made by one or more resource-controlling players to attain the best possible outcome under bounded rational conditions. In other words, resource-controlling agents maximize value subject to the constraints imposed by the information the agents have, their cognitive limitations, and the finite amount of time they have to make and execute a decision. Economic science centres on the activities of the economic agents that comprise society. They are the focus of economic analysis.\n\nAn approach to understanding these processes, through the study of agent behaviour under scarcity, may go as follows:\n\nThe continuous interplay (exchange or trade) done by economic actors in all markets sets the prices for all goods and services which, in turn, make the rational managing of scarce resources possible. At the same time, the decisions (choices) made by the same actors, while they are pursuing their own interest, determine the level of output (production), consumption, savings, and investment, in an economy, as well as the remuneration (distribution) paid to the owners of labour (in the form of wages), capital (in the form of profits) and land (in the form of rent). Each period, as if they were in a giant feedback system, economic players influence the pricing processes and the economy, and are in turn influenced by them until a steady state (equilibrium) of all variables involved is reached or until an external shock throws the system toward a new equilibrium point. Because of the autonomous actions of rational interacting agents, the economy is a complex adaptive system.\n\nKeynesian economics derives from John Maynard Keynes, in particular his book \"The General Theory of Employment, Interest and Money\" (1936), which ushered in contemporary macroeconomics as a distinct field. The book focused on determinants of national income in the short run when prices are relatively inflexible. Keynes attempted to explain in broad theoretical detail why high labour-market unemployment might not be self-correcting due to low \"effective demand\" and why even price flexibility and monetary policy might be unavailing. The term \"revolutionary\" has been applied to the book in its impact on economic analysis.\n\nKeynesian economics has two successors. Post-Keynesian economics also concentrates on macroeconomic rigidities and adjustment processes. Research on micro foundations for their models is represented as based on real-life practices rather than simple optimizing models. It is generally associated with the University of Cambridge and the work of Joan Robinson.\n\nNew-Keynesian economics is also associated with developments in the Keynesian fashion. Within this group researchers tend to share with other economists the emphasis on models employing micro foundations and optimizing behaviour but with a narrower focus on standard Keynesian themes such as price and wage rigidity. These are usually made to be endogenous features of the models, rather than simply assumed as in older Keynesian-style ones.\n\nThe Chicago School of economics is best known for its free market advocacy and monetarist ideas. According to Milton Friedman and monetarists, market economies are inherently stable if the money supply does not greatly expand or contract. Ben Bernanke, former Chairman of the Federal Reserve, is among the economists today generally accepting Friedman's analysis of the causes of the Great Depression.\n\nMilton Friedman effectively took many of the basic principles set forth by Adam Smith and the classical economists and modernized them. One example of this is his article in the 13 September 1970 issue of \"The New York Times Magazine\", in which he claims that the social responsibility of business should be \"to use its resources and engage in activities designed to increase its profits ... (through) open and free competition without deception or fraud.\"\n\nOther well-known schools or trends of thought referring to a particular style of economics practised at and disseminated from well-defined groups of academicians that have become known worldwide, include the Austrian School, the Freiburg School, the School of Lausanne, post-Keynesian economics and the Stockholm school. Contemporary mainstream economics is sometimes separated into the Saltwater approach of those universities along the Eastern and Western coasts of the US, and the Freshwater, or Chicago-school approach.\n\nWithin macroeconomics there is, in general order of their appearance in the literature; classical economics, Keynesian economics, the neoclassical synthesis, post-Keynesian economics, monetarism, new classical economics, and supply-side economics. Alternative developments include ecological economics, constitutional economics, institutional economics, evolutionary economics, dependency theory, structuralist economics, world systems theory, econophysics, feminist economics and biophysical economics.\n\nAccording to various random and anonymous surveys of members of the American Economic Association, economists have agreement about the following propositions by percentage:>\n\n\n\"The dismal science\" is a derogatory alternative name for economics devised by the Victorian historian Thomas Carlyle in the 19th century. It is often stated that Carlyle gave economics the nickname \"the dismal science\" as a response to the late 18th century writings of The Reverend Thomas Robert Malthus, who grimly predicted that starvation would result, as projected population growth exceeded the rate of increase in the food supply. However, the actual phrase was coined by Carlyle in the context of a debate with John Stuart Mill on slavery, in which Carlyle argued for slavery, while Mill opposed it.\n\nSome economists, like John Stuart Mill or Léon Walras, have maintained that the production of wealth should not be tied to its distribution.\n\nIn \"The Wealth of Nations\", Adam Smith addressed many issues that are currently also the subject of debate and dispute. Smith repeatedly attacks groups of politically aligned individuals who attempt to use their collective influence to manipulate a government into doing their bidding. In Smith's day, these were referred to as factions, but are now more commonly called special interests, a term which can comprise international bankers, corporate conglomerations, outright oligopolies, monopolies, trade unions and other groups.\n\nEconomics \"per se\", as a social science, is independent of the political acts of any government or other decision-making organization; however, many policymakers or individuals holding highly ranked positions that can influence other people's lives are known for arbitrarily using a plethora of economic concepts and rhetoric as vehicles to legitimize agendas and value systems, and do not limit their remarks to matters relevant to their responsibilities. The close relation of economic theory and practice with politics is a focus of contention that may shade or distort the most unpretentious original tenets of economics, and is often confused with specific social agendas and value systems.\n\nNotwithstanding, economics legitimately has a role in informing government policy. It is, indeed, in some ways an outgrowth of the older field of political economy. Some academic economic journals have increased their efforts to gauge the consensus of economists regarding certain policy issues in hopes of effecting a more informed political environment. Often there exists a low approval rate from professional economists regarding many public policies. Policy issues featured in one survey of American Economic Association economists include trade restrictions, social insurance for those put out of work by international competition, genetically modified foods, curbside recycling, health insurance (several questions), medical malpractice, barriers to entering the medical profession, organ donations, unhealthy foods, mortgage deductions, taxing internet sales, Wal-Mart, casinos, ethanol subsidies, and inflation targeting.\n\nIn \"Steady State Economics\" 1977, leading ecological economist and steady-state theorist Herman Daly argues that there exist logical inconsistencies between the emphasis placed on economic growth and the limited availability of natural resources.\n\nIssues like central bank independence, central bank policies and rhetoric in central bank governors discourse or the premises of macroeconomic policies (monetary and fiscal policy) of the state, are focus of contention and criticism.\n\nDeirdre McCloskey has argued that many empirical economic studies are poorly reported, and she and Stephen Ziliak argue that although her critique has been well-received, practice has not improved. This latter contention is controversial.\n\nA 2002 International Monetary Fund study assessed the national economic growth predictions from \"Consensus Forecasts\" in the 1990s. Of the 60 different national recessions that occurred, only 2 (3%) were predicted a year in advance.\n\nEconomics has been subject to criticism that it relies on unrealistic, unverifiable, or highly simplified assumptions, in some cases because these assumptions simplify the proofs of desired conclusions. Examples of such assumptions include perfect information, profit maximization and rational choices. The field of information economics includes both mathematical-economical research and also behavioural economics, akin to studies in behavioural psychology.\n\nNevertheless, prominent mainstream economists such as Keynes and Joskow have observed that much of economics is conceptual rather than quantitative, and difficult to model and formalize quantitatively. In a discussion on oligopoly research, Paul Joskow pointed out in 1975 that in practice, serious students of actual economies tended to use \"informal models\" based upon qualitative factors specific to particular industries. Joskow had a strong feeling that the important work in oligopoly was done through informal observations while formal models were \"trotted out \"ex post\"\". He argued that formal models were largely not important in the empirical work, either, and that the fundamental factor behind the theory of the firm, behaviour, was neglected.\n\nIn recent years, feminist critiques of neoclassical economic models gained prominence, leading to the formation of feminist economics. Contrary to common conceptions of economics as a positive and objective science, feminist economists call attention to the social construction of economics and highlight the ways in which its models and methods reflect masculine preferences. Primary criticisms focus on failures to account for: the selfish nature of actors (homo economicus); exogenous tastes; the impossibility of utility comparisons; the exclusion of unpaid work; and the exclusion of class and gender considerations. Feminist economics developed to address these concerns, and the field now includes critical examinations of many areas of economics including paid and unpaid work, economic epistemology and history, globalization, household economics and the care economy. In 1988, Marilyn Waring published the book \"If Women Counted\", in which she argues that the discipline of economics ignores women's unpaid work and the value of nature; according to Julie A. Nelson, \"If Women Counted\" \"showed exactly how the unpaid work traditionally done by women has been made invisible within national accounting systems\" and \"issued a wake-up call to issues of ecological sustainability.\" Bjørnholt and McKay argue that the financial crisis of 2007–08 and the response to it revealed a crisis of ideas in mainstream economics and within the economics profession, and call for a reshaping of both the economy, economic theory and the economics profession. They argue that such a reshaping should include new advances within feminist economics that take as their starting point the socially responsible, sensible and accountable subject in creating an economy and economic theories that fully acknowledge care for each other as well as the planet.\n\nPhilip Mirowski observes that:\n\nIn a series of peer-reviewed journal and conference papers and books published over a period of several decades, John McMurtry has provided extensive criticism of what he terms the \"unexamined assumptions and implications [of economics], and their consequent cost to people's lives.\"\n\nNassim Nicholas Taleb and Michael Perelman are two additional scholars who criticized conventional or mainstream economics. Taleb opposes most economic theorizing, which in his view suffers acutely from the problem of overuse of Plato's Theory of Forms, and calls for cancellation of the Nobel Memorial Prize in Economics, saying that the damage from economic theories can be devastating. Michael Perelman provides extensive criticism of economics and its assumptions in all his books (and especially his books published from 2000 to date), papers and interviews.\n\nDespite these concerns, mainstream graduate programs have become increasingly technical and mathematical.\n\n\nGeneral:\n\n\n\n"}
{"id": "2736602", "url": "https://en.wikipedia.org/wiki?curid=2736602", "title": "El Perú (book)", "text": "El Perú (book)\n\nEl Perú: Itinerarios de Viajes is an expansive written work covering a variety of topics in the natural history of Peru, written by the prominent Italian-born Peruvian geographer and scientist Antonio Raimondi in the latter half of the 19th century. The work was compiled from extensive and detailed notes Raimondi took while criss-crossing the country, studying the nation's geography, geology, meteorology, botany, zoology, ethnography, and archaeology; \"El Perú\" focuses to some extent on each of these topics and others. The first volume was published in 1874; several more volumes were published both before Raimondi's death and posthumously from his notes, the last being released in 1913, making a five volume set. The volumes are a classic example of exploration scholarship, and form one of the earliest and broadest scientific reviews of Peru's natural and cultural heritage.\n\n\n"}
{"id": "51213082", "url": "https://en.wikipedia.org/wiki?curid=51213082", "title": "Ernest Allard", "text": "Ernest Allard\n\nErnest (e) Allard (1820 - 1900) was a French entomologist who specialised in Coleoptera. He is not to be confused with the Belgian entomologist Vincent Allard (1921-1994).\n\nAllard's collection was acquired by René Oberthür and is now held by Muséum national d'histoire naturelle, in Paris and Museum Koenig in Bonn. He was a Member of the Société entomologique de France.\n\n\n\n"}
{"id": "51531534", "url": "https://en.wikipedia.org/wiki?curid=51531534", "title": "Filippo Cavolini", "text": "Filippo Cavolini\n\nFilippo Cavolini (8 April 1756, Vico Equense – 15 March 1810, Naples ) was an Italian marine biologist.\n\nThe son of Nicola Cavolini, a Neapolitan lawyer, and Angela Auriemma, Filippo Cavolini left a legal career to devote himself to natural history. He became the Professor of Zoology at the University of Naples and Director of the Zoological Museum.\nHis research style, similar to that of Lazzaro Spallanzani, led to important results in the arena of marine biology and botany (\"Memorie per servire alla storia de' polipi marini\" -1785- \"Memoria per servire alla generazione dei pesci e dei granchi\" - 1787).\nHe died following an assault by a soldier while he was researching by boat in the Gulf of Naples: he fell into the sea and died a few days later from pneumonia.\nThe opisthobranch family Cavoliniidae honours his name.\n\n"}
{"id": "44086673", "url": "https://en.wikipedia.org/wiki?curid=44086673", "title": "Global Health and Vaccination Research", "text": "Global Health and Vaccination Research\n\nGlobal Health and Vaccination Research (GLOBVAC) is a programme of the Research Council of Norway. Its primary objective is to support high-quality research with potential for high impact that can contribute to sustainable improvements in health and health equity for poor people in low- and lower-middle income countries (LMIC). Norwegians institutions may apply for grants in collaboration with researchers in other countries.\n\nAs all programmes of the Research Council of Norway it is led by a programme board appointed by the Division Board for Society and Health at the Research Council. The programme board for the period 2011 - 2014 is chaired by Peter Smith (epidemiologist). Other members include Prof. Nelson Sewankambo and Prof. Rifat Atun\n\nIt prioritizes projects in the following thematic areas:\n\n"}
{"id": "50970290", "url": "https://en.wikipedia.org/wiki?curid=50970290", "title": "Glossary of structural engineering", "text": "Glossary of structural engineering\n\n\"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.\"\nThis glossary of structural engineering terms pertains specifically to structural engineering and its sub-disciplines. Please see glossary of engineering for a broad overview of the major concepts of engineering.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "83909", "url": "https://en.wikipedia.org/wiki?curid=83909", "title": "Graham's law", "text": "Graham's law\n\nGraham's law of effusion (also called Graham's law of diffusion) was formulated by Scottish physical chemist Thomas Graham in 1848. Graham found experimentally that the rate of effusion of a gas is inversely proportional to the square root of the mass of its particles. This formula can be written as:\nwhere:\n\nGraham's law states that the rate of diffusion or of effusion of a gas is inversely proportional to the square root of its molecular weight. Thus, if the molecular weight of one gas is four times that of another, it would diffuse through a porous plug or escape through a small pinhole in a vessel at half the rate of the other (heavier gases diffuse more slowly). A complete theoretical explanation of Graham's law was provided years later by the kinetic theory of gases. Graham's law provides a basis for separating isotopes by diffusion—a method that came to play a crucial role in the development of the atomic bomb.\n\nGraham's law is most accurate for molecular effusion which involves the movement of one gas at a time through a hole. It is only approximate for diffusion of one gas in another or in air, as these processes involve the movement of more than one gas.\n\nIn the same conditions of temperature and pressure, the molar mass is proportional to the mass density. Therefore the rate of diffusion of different gases is inversely proportional to the square root of their mass densities.\n\nLet gas 1 be H and gas 2 be O. (This example is solving for the ratio between the rates of the two gases)\n\nTherefore, hydrogen molecules effuse four times faster than those of oxygen.\n\nUsing the formula of gaseous diffusion, the ratio of rate of diffusion of NH and HCl gas was obtained practically as 1.46 +_ 0.01.\n\nGraham's Law can also be used to find the approximate molecular weight of a gas if one gas is a known species, and if there is a specific ratio between the rates of two gases (such as in the previous example). The equation can be solved for the unknown molecular weight.\n\nGraham's law was the basis for separating U from U found in natural uraninite (uranium ore) during the Manhattan Project to build the first atomic bomb. The United States government built a gaseous diffusion plant in Clinton, Tennessee at the cost of $100 million (~$7.7 billion in 2014 dollars). In this plant, uranium from uranium ore was first converted to uranium hexafluoride and then forced repeatedly to diffuse through porous barriers, each time becoming a little more enriched in the slightly lighter U isotope.\n\nGraham's research on the diffusion of gases was triggered by his reading about the observation of German chemist Johann Döbereiner that hydrogen gas diffused out of a small crack in a glass bottle faster than the surrounding air diffused in to replace it. Graham measured the rate of diffusion of gases through plaster plugs, through very fine tubes, and through small orifices. In this way he slowed down the process so that it could be studied quantitatively. He first stated in 1831 that the rate of effusion of a gas is inversely proportional to the square root of its density, and later in 1848 showed that this rate is inversely proportional to the square root of the molar mass. Graham went on to study the diffusion of substances in solution and in the process made the discovery that some apparent solutions actually are suspensions of particles too large to pass through a parchment filter. He termed these materials colloids, a term that has come to denote an important class of finely divided materials.\n\nAround the time Graham did his work, the concept of molecular weight was being established largely through the measurements of gases. Daniel Bernoulli suggested in 1738 in his book Hydrodynamica that heat increases in proportion to the velocity, and thus kinetic energy, of gas particles. Italian physicist Amedeo Avogadro also suggested in 1811 that equal volumes of different gases contain equal numbers of molecules. Thus, the relative molecular weights of two gases are equal to the ratio of weights of equal volumes of the gases. Avogadro's insight together with other studies of gas behaviour provided a basis for later theoretical work by Scottish physicist James Clerk Maxwell to explain the properties of gases as collections of small particles moving through largely empty space.\n\nPerhaps the greatest success of the kinetic theory of gases, as it came to be called, was the discovery that for gases, the temperature as measured on the Kelvin (absolute) temperature scale is directly proportional to the average kinetic energy of the gas molecules. Graham's law for diffusion could thus be understood as a consequence of the molecular kinetic energies being equal at the same temperature.\n\nThe rationale of the above can be summed up as follows:\n\nKinetic energy of each type of particle (in this example, Hydrogen and Oxygen, as above) within the system is equal, as defined by thermodynamic temperature:\n\nformula_5\n\nWhich can be simplified and rearranged to:\n\nformula_6\n\nor:\n\nformula_7\n\nErgo, when constraining the system to the passage of particles through an area, Graham's Law appears as written at the start of this article.\n\n"}
{"id": "51115461", "url": "https://en.wikipedia.org/wiki?curid=51115461", "title": "H-500 Holon", "text": "H-500 Holon\n\nThe H-500 project is an outline planning scheme in accordance with the Planning and Building Law for South Holon, Israel. The area of the project is approximately 4,080 dunams and it is the largest, undeveloped land reserve remaining in Holon. The scheme is bounded in the north by Sderot Yerushalayim and the Kiryat Sharet and Kiryat Pinhas Ayalon neighbourhoods, in the east by Highway no. 4, in the south by the border with the city of Rishon LeZion and in the west by Highway 20 (Israel) (the Ayalon Highway).\n\nThe scheme (Holon H-500) adapts the land zoning, and the deployment and height of construction, to the restrictions deriving from NOPS 2/4 (the National Outline Planning Scheme for the development of Ben Gurion Airport), which fixed the location of the flight paths adjacent to and above the site of the H-500 scheme. The restrictions deriving from the Airport's activity are expressed in a limit of the final height of the buildings in the project (a maximum of 22 stories), height limits for masts and antennas and also restrictions relating to the erection of cranes during construction. In addition, the use of acoustic elements for noise insulation is prescribed, as are provisions intended to minimise the hazards deriving from flying birds that might endanger passing aircraft.\n\nAccording to the scheme, in the version that has been deposited (Holon H-500), a green area of 1,200 dunams or thereabouts is planned for the development of a metropolitan sand dune park at the centre of the site. The centre of the park, which is called the \"preserved heart\", will include an area of not less than 400 dunams of natural sand dunes in order to preserve the unique ecology that includes a wide range of rare plants, the like of which is not to be found elsewhere along the coast. The ring around the \"preserved heart\" will form the outer envelope of the park and will be zoned for municipal recreation and leisure activity.\n\nFurther to the hearing of oppositions to the deposited version of the scheme (Holon H-500), the district planning and building committee decided to extend the \"preserved heart\" of the park at the expense of the area for future planning. The 1,500 dwelling units that were allotted in an area for future planning according to the deposited version of the scheme were moved by the district committee's decision to the residential domains, so that their allotment will not be dependent on changes to the Airport flight paths.\n\n\n\n\nThe sand dunes of South Holon have been a focus of real estate activity since the beginning of the last century. At that time, before the establishment of the Holon Local Council, the area was defined as being under galilee jurisdiction, that is to say that it was not under the jurisdiction of any local authority, adjacent to the Arab village of Yazur. The first outline scheme that was prepared for the area by the planning agencies during the British Mandate was called the \"Agrobank And Neighbourhood Regional Outline Road Scheme\".\n\nThe scheme was provisionally approved on October 30, 1939 and finally on January 14, 1942 (its approval was published in the Official Gazette, no. 1164). The purpose of the scheme was to delineate the road network for the new neighbourhoods that were to be built around the Agrobank neighbourhood, situated to the north, adjacent to Mikveh Israel.\n\nIn the 1930s private individuals and commercial companies started buying large tracts of land totalling hundreds of dunams from the Arab landowners. Parcellation plans were prepared in accordance with the Mandatory City Building Ordinance, which divided the large tracts of land into small parcels of between 300 and 500 m² each, creating neighbourhoods that incorporated roads and plots for public purposes, in conformity with the Mandatory outline scheme for Holon, R/128, the preparation of which began in the early 1940s\n\nThe parcels resulting from the parcellation plans were entered in the Land Registers and came to be traded on the open market in Israel and also abroad. At that time great importance was attached to the redemption of the land and for that reason many parcels were bought by the Jews of Europe, USA and South Africa, with the assistance of public leaders who were harnessed to the task on visiting Jewish communities abroad. Thus, for example, Dr Haim Kugel, who went on to become the first Mayor of Holon, helped in the land redemption when he visited the Jewish community in South Africa. For that reason there are still many owners (the heirs of the original owners) of land within the boundaries of the scheme who are non-residents.\n\nThe following are examples of the historical parcellation of land in South Holon:\n\n\nThe first outline planning scheme for the City of Holon, H/1, dating from 1958, cancelled all the Mandatory plans for the construction of new residential neighbourhoods in South Holon and zoned the whole area for planning again as an \"area for reparcellation\" (fresh consolidation and partition). The H-500 scheme is the third and most recent of the outline planning schemes (following H/1 and Amendment no. 3 to H/1) and it proposes a new planning scheme for the whole area.\n\nThe Moledet neighbourhood is included within the boundaries of the H-500 scheme in the area that is zoned for the sand dune park. The neighbourhood was established in 1930 as the first residential suburb of South Holon and it is one of the first five neighbourhoods (together with Agrobank, Greene, Kiryat Avoda and Am) that combined into the Holon Local Council in 1940.\n\nThe Moledet land was purchased by a company called Hamizrach, headed by the engineer, Alexander Hissin. The centre of the neighbourhood was the Water Tower. The first residents were eight immigrant families from Yemen but by the end of 1934, just four years later, the population of the neighbourhood had reached about 100 families.\n\nBecause of Moledet's remoteness from other Jewish settlements and its proximity to the Arab villages of Yazur and Bayt Dajan, the neighbourhood was attacked by Arabs from Jaffa in the great Arab revolt between 1936 and 1939 and almost completely abandoned. Its residents went to live temporarily at the Mikveh Israel Agricultural School.\n\nThe Security Road, which was paved in 1948 as an alternative to the Jaffa – Jerusalem Road (now Highway no. 44), with the object of connecting Tel Aviv to the settlements of the South and Jerusalem, ran through Moledet.\n\nDozens of families now live in Moledet, alongside a few businesses and workshops. All the buildings in the neighbourhood, apart from the Water Tower and Security Road that have been zoned for preservation, are to be cleared according to the H-500 scheme (see below – the Clearance of Givat Holon and Moledet).\n\nGivat Holon is located within the boundaries of the H-500 scheme in the area that is zoned for the sand dune park. This small suburb is located to the south of Kiryat Sharet in the heart of the sand dune area, alongside the road leading to the site of the municipal abattoir.\n\nGivat Holon includes several dozen homes. The buildings were constructed at the beginning of the 1940s and remain in their original format. Although according to outline scheme H/1 (Amendment no. 3), which was approved in 1978, the area was zoned as residential, in order to enlarge construction in the area it was necessary to prepare a detailed plan. Because of the limitations deriving from the neighbourhood's location in the area affected by the Airport, it was not possible to develop the neighborhood and it has therefore remained in its original state, without modern infrastructure.\n\nThe residents of Givat Holon took legal proceedings against Holon Municipality and in June 2014 they were awarded judgement requiring the Municipality to grant them proper municipal services, like garbage removal and the provision of refuse bins, and the installation of lampposts and street signs.\n\nGivat Holon is due to be cleared as part of the H-500 scheme (see below – the Clearance of Givat Holon and Moledet).\n\nThe Abattoir tract is a 254 dunam piece of land that is included in the H-500 scheme. The land was expropriated from private owners in 1961 further to authority obtained by the Mayor of Holon in accordance with the Land Ordinance for the establishment of a regional abattoir and associated industrial facilities. To that end a joint company was established by the Tel Aviv and Holon Municipalities (80% Tel Aviv and 20% Holon) in order to promote the venture and bear its costs, including the compensation payable to the private landowners.\n\nFurther to the expropriation proceedings, in 1966 a town planning scheme (H-142) for the construction of an abattoir and associated industries was approved. According to the scheme it was permitted to build an abattoir and meat processing facilities.\n\nThe aforegoing might today sound like a figment of the imagination: to expropriate 250 dunams of land for an abattoir in the sandhills of South Holon, big enough to serve the whole Middle East. Nevertheless, at that time, the venture did appear to justify such a large-scale expropriation. Not one of the private landowners objected to the venture and even had anyone objected, it is doubtful whether he would have had any chance of succeeding against Holon's then legendary mayor, Pinchas Ayalon.\n\nIn fact, apart from one building that was constructed at the end of the 1960s, which served as a municipal abattoir until the beginning of the 1990s, the rest of the land remained empty and undeveloped. The abattoir building was blown up in June 2015 as part of an exercise by the Home Front Command.\n\nThe landowners in the abattoir domain, Tel Aviv and Holon Municipalities, are expected to obtain rights in the scope of the H-500 scheme for the clearance of the abattoir (section 2.2.4 of the scheme regulations).\n\nThe H-500 scheme lays down directions for the preservation of the Security Road and the historic sites associated with it in accordance with the municipal preservation plan. The scheme nevertheless prescribes the clearance of Givat Holon and Moledet because of the limitations of using the area, which is affected by the flight paths according to the Ben Gurion Airport national outline scheme. After clearance, the neighborhoods will become part of the municipal sand dune park.\n\nThe deposited plan contains merely general provisions for the compensation of the residents of Moledet, the clearance to be financed from building rights that have been allotted specifically for the purpose, totalling 800 dwelling units (designated for the clearance of three centres: the refuse site, the abattoir and Moledet). The scheme further provides that the total compensation will be determined by a real estate appraiser to be appointed by the local planning and building committee.\n\nFurther to appeals that were filed against the district planning and building committee's decision (which the residents of Givat Holon also joined), which culminated in the decision of the appeal subcommittee of the national council of May 15, 2014, several operative decisions were made in respect of the clearance of Givat Holon (from which inferences may be drawn in respect of the clearance of Moledet) as follows:\n\n\n\nJoseph Raiten, The Holon H-500 Trade Arena, http://www.h-500.com/\n"}
{"id": "42478293", "url": "https://en.wikipedia.org/wiki?curid=42478293", "title": "Harry Brailovsky Alperowits", "text": "Harry Brailovsky Alperowits\n\nHarry Urad Brailovsky Alperowitz (born in Mexico City on May 30, 1946) is a biologist. He earned his BA, MA and Ph.D. in biological sciences at the Faculty of Sciences, National Autonomous University of Mexico. His main academic interest is the taxonomy, biology, and biogeography of Coreidae, especially those found in Mexico (Hemiptera: Heteroptera).\n\nBrailovsky has published over 200 academic works on Coreoidea describing over 660 new species.\n"}
{"id": "36549035", "url": "https://en.wikipedia.org/wiki?curid=36549035", "title": "Helmholtz flow", "text": "Helmholtz flow\n\nHelmholtz flow is a term used in fluid mechanics for flow with free streamlines or vortex sheets. \n"}
{"id": "1820023", "url": "https://en.wikipedia.org/wiki?curid=1820023", "title": "Henry Woodward (geologist)", "text": "Henry Woodward (geologist)\n\nHenry Bolingbroke Woodward (24 November 1832 – 6 September 1921) was an English geologist and paleontologist known for his research on fossil crustaceans and other arthropods.\n\nWoodward was born Norwich, England on 24 November 1832 and was educated at Norwich School.\n\nHe became assistant in the geological department of the British Museum in 1858, and in 1880 keeper of that department. He became Fellow of the Royal Society in 1873, LL.D (St Andrews) in 1878, president of the Geological Society of London (1894–1896). He was awarded the Murchison Medal in 1884 and Wollaston Medal in 1906. Woodward was president of the Geologists' Association for the years 1873 and 1874, president of the Malacological Society in 1893–1895, president of the Museums Association for the year 1900, and president of the Palaeontographical Society from 1895 (upon the death of incumbent president T. H. Huxley) to his own death in 1921.\n\nHe published a \"Monograph of the British Fossil Crustacea, Order Merostomata\" (Palaeontograph. Soc. 1866-1878); \"A Monograph of Carboniferous Trilobites\" (Pal. Soc. 1883-1884), and many articles in scientific journals. He was editor of the \"Geological Magazine\" from its commencement in 1864 and sole editor from July 1865 until the end of 1918. Woodward's collection of shells, manuscripts and casts of fossil vertebrates can be found in the archives of the Cambridge University Museum of Zoology.\n\nHenry's father, Samuel Woodward, was a noted geologist and antiquary. Henry's brother Bernard Bolingbroke Woodward became a noted librarian and antiquary while his brother Samuel Pickworth Woodward became a professor of geology and natural history. His nephews were Bernard Barham Woodward, a British malacologist and a member of staff at the British Museum and the Natural History Museum and Horace Bolingbroke Woodward, who was Vice-president of the Geological Society and a Fellow of the Royal Society.\n\nHenry Woodward had two sons, both of whom died before he did; the eldest, Henry Page Woodward was also a noted geologist who worked in Australia. Henry's second son, Martin- a student of T. H. Huxley alongside H. G. Wells- was a promising zoologist killed in a boating accident. Henry also had five daughters, two of whom- Alice B. Woodward and Gertrude Mary Woodward - worked in biological illustration, although Alice was primarily known for her children's book illustrations.\n"}
{"id": "13855813", "url": "https://en.wikipedia.org/wiki?curid=13855813", "title": "Ionization cone", "text": "Ionization cone\n\nIonization cones are cones of material extending out from spiral galaxies. They are visible because of their emissions which are believed to be from re-emission of photons produced by nuclear activity within the galaxy itself.\n\nThere is not yet a scientific consensus on the mechanics of such cones.\n"}
{"id": "37642238", "url": "https://en.wikipedia.org/wiki?curid=37642238", "title": "Irensaga Montes", "text": "Irensaga Montes\n\nThe Irensaga Montes is a range of mountains on Titan, the largest moon of the planet Saturn. The range is located near Titan's equator, between 5-6° south and 210-214° east. It is located within the Adiri region, just west of the landing site of the Huygens probe.\n\nThe Irensaga Montes is named after Irensaga, one of the White Mountains in J. R. R. Tolkien's fictional world of Middle-earth. The name follows a convention that Titanean mountains are after mountains in Tolkien's work. It was formally announced on November 13, 2012.\n"}
{"id": "2616800", "url": "https://en.wikipedia.org/wiki?curid=2616800", "title": "Jack Sarfatti", "text": "Jack Sarfatti\n\nJack Sarfatti (born September 14, 1939) is an American theoretical physicist. Working largely outside academia, most of Sarfatti's publications revolve around quantum physics and consciousness. He argues for retrocausality, that mind is crucial to the structure of matter, and that physics—which he calls the \"conceptual art of the late 20th Century\"—has replaced philosophy as the unifying force between science and art.\n\nSarfatti was a leading member of the Fundamental Fysiks Group, an informal group of physicists in California in the 1970s who, according to historian of science David Kaiser, helped to inspire some of the investigations into quantum physics that underly parts of quantum information science. Sarfatti co-wrote \"Space-Time and Beyond\" (1975; credited to Bob Toben and Fred Alan Wolf) and has self-published several books. \n\nSarfatti was born in Brooklyn, New York to Hyman and Millie Sarfatti. His father was born in Kastoria, Greece, and moved to New York as a child with his family.\n\nRaised in the Midwood neighborhood, Sarfatti attended Midwood High School where he was in the All City Chorus and Math Club, graduating in 1956. In \"Destiny Matrix\" (2002), Sarfatti wrote that, when he was 13, he received at least one telephone call from a voice that said it was a conscious computer on a spaceship. The voice said he had been identified as \"one of 400 bright young receptive minds,\" and that he would be picked up shortly from his building's fire escape. He and several friends waited, he wrote, but nothing happened.\n\nSarfatti has also alleged that numismatist and convicted child sex offender Walter H. Breen (who later offered guest workshops sponsored by Sarfatti at meetings of the Physics/Consciousness Research Group at the Esalen Institute in 1976) secured his admission to Cornell University while coordinating Sandia National Laboratories-funded parapsychological research studies of New York City gifted children (so-called \"superkids,\" including Sarfatti and Robert Bashlow) in William Herbert Sheldon's Constitutional Laboratory at Columbia Medical School from 1953 to 1956. The studies allegedly included immersion in New York science fiction fandom, a connection facilitated by Breen.\n\nIn 1960, he obtained his B.A in physics from Cornell University, where he was also a lead tenor in the \"Cornell Savoyards\". Three years later, he published his first paper, \"Quantum-Mechanical Correlation Theory of Electromagnetic Fields,\" in \"Nuovo Cimento\", the journal of the Italian Physical Society. Following graduate studies at Cornell and Brandeis University, he obtained an M.S. in physics in 1967 from the University of California, San Diego before receiving a Ph.D. in the discipline in 1969 from the University of California, Riverside, where he studied under Fred Cummings; his dissertation was \"Gauge Invariance in the Theory of Superfluidity.\" He and Cummings co-wrote a paper, \"Beyond the Hartree-Fock Theory in Superfluid Helium,\" published in \"Physica Scripta\" in 1970.\n\nFrom 1967 to 1971, Sarfatti was an assistant professor of physics at San Diego State University. During the 1971–1972 academic year, he held a research fellowship at Birkbeck, University of London, where he worked with David Bohm. He also studied at the Cornell Space Science Center, the UK Atomic Energy Research Establishment, and the Max Planck Institute for Physics in Munich. In 1973–1974 he conducted research into mini black holes at the International Centre for Theoretical Physics in Trieste. At around this time he decided to leave academia, seeing it as too sterile.\n\nAccording to Kaiser, Sarfatti's politics have leaned to the right since the early 1980s, when he became dependent on a cadre of \"politically conservative thinkers who were drawn to certain New Age ideas\" for research funding following the dissolution of his relationship with Werner Erhard. He endorses the Cultural Marxism conspiracy theory and perceives the majority of faculty at American universities as constituting \"the enemy within.\" In the 1980s, he worked with Lawry Chickering at the Institute for Contemporary Studies, a neoconservative think tank. For over forty years, Sarfatti has maintained a close friendship with conservative talk show host and medical anthropologist Michael Savage.\n\nSarfatti became a leading member of the Fundamental Fysiks Group, an informal group of physicists that conducted weekly discussions at the Lawrence Berkeley National Laboratory in the 1970s. The group—\"very smart and very playful,\" according to David Kaiser—was founded by Elizabeth Rauscher and included Henry Stapp, Fred Alan Wolf, Nick Herbert, Fritjof Capra, John Clauser, Philippe Eberhard, Saul-Paul Sirag and George Weissman.\n\nSeveral held academic posts, but others had been left unemployed when the post-war boom in physics ended in 1968–1972. Physics, too, had changed; students were taught little or no philosophy and metaphysics. The Fundamental Fysiks Group, with PhDs in theoretical physics, made names for themselves writing about consciousness, metaphysics and quantum mysticism.\nQuantum theory—particularly Bell's theorem and the concept of quantum entanglement—had raised questions about parapsychology and telepathy. Kaiser argues that Sarfatti and the group kept several of these apparently fringe ideas alive. For example, they believed they could develop faster-than-light communication, discussions that led to the no-cloning theorem, which became part of quantum cryptography. The group similarly kept Bell's theorem alive, which eventually led to quantum information science. According to historian Robert P. Crease and physicist Alfred Scharff Goldhaber, apart from one brief mention in 1966, Bell's theorem \"did not enter mainstream physics textbooks until after the Fundamental Fysiks Group had left its impact.\"\n\nKaiser writes that there was significant government interest in telepathy and remote viewing. The Central Intelligence Agency and Defense Intelligence Agency set up a program called ESPionage, financing research conducted by the Stanford Research Institute (SRI), where Sarfatti and the Fundamental Fysiks Group became what Kaiser calls its \"house theorists.\" The group became celebrities in San Francisco. \"City of San Francisco Magazine\" devoted two pages to them in 1975, shortly after the magazine was acquired by the film director Francis Ford Coppola. The spread included a photograph of Sarfatti, Saul-Paul Sirag, Fred Alan Wolf and Nick Herbert, and discussed them \"going into trances, working at telepathy, [and] dipping into their subconscious in experiments toward psychic mobility.\" In 1979 Sarfatti was featured on the cover of \"North Beach Magazine\".\n\nIn 1974 Sarfatti and the Fundamental Fysiks Group were hired by the Stanford Research Institute to help with its research into Uri Geller. Geller, an Israeli, maintained that he could bend spoons and control watches using only his thoughts. The SRI studies, led by laser physicists Russell Targ and Harold Puthoff, began in November 1972 and resulted in a paper in \"Nature\" in October 1974. According to Kaiser, SRI asked Sarfatti and the group to use quantum theory, and specifically Bell's theorem, to explain what Geller appeared to be doing. Joseph Hanlon wrote in \"New Scientist\" at the time that the SRI tests had been conducted in a \"circus atmosphere,\" with Geller in control.\n\nSarfatti and Fred Wolf helped to organize a series of tests at Birkbeck College, London, led by John Hasted. On June 21 and 22, 1974, Hasted and Sarfatti joined David Bohm, Arthur Koestler, Arthur C. Clarke, and two of Geller's associates, Ted Bastin and Brendan O'Regan, to watch Geller appear to bend four brass Yale keys and a 1 cm disk, affect a Geiger counter and deflect a compass needle. Hanlon wrote that any good magician could have bent the keys, no matter how closely the observers believed they were watching. Sarfatti issued press releases saying he believed Geller had demonstrated psychokinetic ability, statements picked up by \"Science News\" and the international media. Hasted, Bohm, Bastin and O'Regan described the experiments in \"Nature\" in April 1975. Sarfatti retracted his view in December that year after watching magician James Randi perform the same trick.\n\nOutside government, groups within the human potential movement were also interested in quantum theory. Werner Erhard, founder of Erhard Seminars Training (EST), moved to the Bay Area and came into contact with Sarfatti and Fred Alan Wolf. In January 1975 Erhard and the physicists formally set up a non-profit think tank, the Physics–Consciousness Research Group, with Sarfatti as president and Saul-Paul Sirag vice-president. Funded by Erhard, they held lectures, published pamphlets, and staged an opera in a Bay Area park about quantum physics and the brain.\n\nErhard introduced Sarfatti to Michael Murphy of the Esalen Institute in Big Sur, California. In January 1976 Sarfatti and the Physics–Consciousness Research Group gathered there for a month-long conference on physics and consciousness. Sarfatti was the conference's intellectual director, and wrote to major figures asking them to address it. Gary Zukav's best-selling \"The Dancing Wu Li Masters\" (1979) was organized around his attendance at this conference; he and Sarfatti were roommates in North Beach at the time. The conference apart, the Esalen group held regular workshops on quantum theory, with physicists mixing lectures with yoga and sessions in the hot tubs.\n\nThe new ideas were not invariably welcome within mainstream academic physics. According to Kaiser, Samuel Goudsmit, editor of the prestigious \"Physical Review\", formally banned discussion of the interpretation of quantum mechanics, drawing up special instructions to referees to reject material that even hinted at the philosophical debate. The new material was distributed instead in alternative media. One such publication was a hand-typed newsletter called \"Epistemological Letters\", published by a Swiss Foundation. Several eminent physicists and philosophers published their material there—including the Irish physicist John Bell, the originator of Bell's theorem—as well as Sarfatti and other members of the Physics-Consciousness Research Group.\n\nSarfatti's local celebrity in San Francisco continued throughout the 1980s with seminars on physics and consciousness in the Caffe Trieste on Vallejo Street, North Beach. In 1993 the novelist Herbert Gold called the café \"Sarfatti's Cave,\" after Plato's cave\n\nIn 2010 Sarfatti was among 30 people invited to join a working group, the 100-Year Starship study, financed by the Defense Advanced Research Projects Agency and NASA's Ames Research Center, to discuss how interstellar space flight might be achieved.\n\n\n"}
{"id": "285544", "url": "https://en.wikipedia.org/wiki?curid=285544", "title": "John Money", "text": "John Money\n\nJohn William Money (8 July 1921 – 7 July 2006) was a psychologist, sexologist and author, specializing in research into sexual identity and biology of gender. He was one of the first scientists to study the psychology of sexual fluidity and how the societal constructs of \"gender\" affect an individual. Recent academic studies have criticized Money's work in many respects, particularly in regards to his involvement with the sex-reassignment of David Reimer and his eventual suicide. Money's writing has been translated into many languages, and includes around 2,000 articles, books, chapters and reviews. He received around 65 honors, awards, and degrees in his lifetime.\n\nBorn in Morrinsville, New Zealand, to a family of English and Welsh descent, Money initially studied psychology at Victoria University of Wellington, graduating with a double master's degree in psychology and education in 1944. Money was a junior member of the psychology faculty at the University of Otago in Dunedin, but in 1947, at the age of 26, he emigrated to the United States to study at the Psychiatric Institute of the University of Pittsburgh. He left Pittsburgh and earned his PhD from Harvard University in 1952. He was married briefly in the 1950s but had no children.\n\nMoney proposed and developed several theories and related terminology, including gender identity, gender role, gender-identity/role, and lovemap. He coined the term \"paraphilia\" (appearing in the DSM-III) to replace \"perversions\" and introduced the term \"sexual orientation\" in place of \"sexual preference\", arguing that attraction is not necessarily a matter of free choice. \nMoney was a professor of pediatrics and medical psychology at Johns Hopkins University from 1951 until his death. He also established the Johns Hopkins Gender Identity Clinic in 1965 along with Claude Migeon who was the head of pediatric endocrinology at Johns Hopkins. The hospital began performing sexual reassignment surgery in 1966. At Johns Hopkins, Money was also involved with the Sexual Behaviors Unit, which ran studies on sex-reassignment surgery. He received the Magnus Hirschfeld Medal in 2002 from the German Society for Social-Scientific Sexuality Research.\n\nMoney was an early supporter of New Zealand's arts, both literary and visual. He was a noted friend and supporter of author Janet Frame. In 2002, as his Parkinson's disease worsened, Money donated a substantial portion of his art collection to the Eastern Southland Art Gallery in Gore, New Zealand. In 2003, the New Zealand Prime Minister, Helen Clark, opened the John Money wing at the Eastern Southland Gallery.\n\nMoney died 7 July 2006, one day before his 85th birthday, in Towson, Maryland, of complications from Parkinson's disease.\n\nMoney was the co-editor of a 1969 book \"Transsexualism and Sex Reassignment\", which helped bring more acceptance to sexual reassignment surgery and transgender individuals.\n\nMoney introduced numerous definitions related to gender in journal articles in the 1950s, many of them as a result of his studies of Hermaphroditism.\n\nMoney's definition of gender is based on his understanding of sex differences among human beings. According to Money, the fact that one sex produces ova and the other sex produces sperm is the irreducible criterion of sex difference. However, there are other \"sex-derivative differences\" that follow in the wake of this primary dichotomy.\n\nThese differences involve the way urine is expelled from the human body and other questions of sexual dimorphism. According to Money's theory, \"sex-adjunctive differences\" are typified by the smaller size of females and their problems in moving around while nursing infants. This then makes it more likely that the males do the roaming and hunting.\n\n\"Sex-arbitrary differences\" are those that are purely conventional: for example, color selection (baby blue for boys, pink for girls). Some of the latter differences apply to life activities, such as career opportunities for men versus women.\n\nFinally, Money created the now-common term \"gender role\" which he differentiated from the concept of the more traditional terminology \"sex role\". This grew out of his studies of hermaphrodites. According to Money, the genitalia and erotic sexual roles were now, by his definition, to be included under the more general term \"gender role\" including all the non-genital and non-erotic activities that are defined by the conventions of society to apply to males or to females.\n\nIn his studies of hermaphrodites, Money found that there are six variables that define sex. While in the average person all six would line up unequivocally as either all \"male\" or \"female\", in hermaphrodites any one or more than one of these could be inconsistent with the others, leading to various kinds of anomalies. In his seminal 1955 paper he defined these factors as:\n\nand added, \n\nHe then defined gender role as \n\nMoney made the concept of \"gender\" a broader, more inclusive concept than one of masculine/feminine. For him, gender included not only one's status as a man or a woman, but was also a matter of personal recognition, social assignment, or legal determination; not only on the basis of one's genitalia but also on the basis of somatic and behavioral criteria that go beyond genital differences.\n\nIn 1972, Money presented his theories in \"Man & Woman, Boy & Girl\", a college-level, mainstream textbook. The book featured David Reimer (see below) as an example of gender reassignment.\n\nIn this book (Oxford 1988: 116), Money develops a conception of 'bodymind,' as a way for scientists, in developing a science about sexuality, to move on from the platitudes of dichotomy between nature versus nurture, innate versus the acquired, biological versus the social, and psychological versus the physiological. He suggests that all of these capitalize on the ancient, pre-Platonic, pre-biblical conception of body versus the mind, and the physical versus the spiritual. In coining the term \"bodymind\", in this sense, Money wishes to move beyond these very ingrained principles of our folk or vernacular psychology.\n\nMoney also develops here (Oxford 1988: 114–119) a view of \"Concepts of Determinism,\" which, transcultural, transhistorical, and universal, all people have in common, sexologically or otherwise. These include pairbondage, troopbondage, abidance, ycleptance, foredoomance, with these coping strategies: adhibition (engagement), inhibition, explication.\n\nMoney suggests that the concept of threshold (Oxford 1988: 115) – the release or inhibition of sexual (or other) behavior – is most useful for sex research as a substitute for any concept of motivation. Moreover, it confers the distinct advantage of having continuity and unity to what would otherwise be a highly disparate and varied field of research. It also allows for the classification of sexual behavior. For Money, the concept of threshold has great value because of the wide spectrum to which it applies. \"It allows one to think developmentally or longitudinally, in terms of stages or experiences that are programmed serially, or hierarchically, or cybernetically (i.e. regulated by mutual feedback).\" (Oxford 1988: 116)\n\nDuring his professional life, Money was respected as an expert on sexual behavior, especially known for his views that gender was learned rather than innate. However, it was later revealed that his most famous case of David Reimer was fundamentally flawed. In 1966, a botched circumcision left eight-month-old Reimer without a penis. Money persuaded the baby's parents that sex reassignment surgery would be in Reimer's best interest. At the age of 22 months, Reimer underwent an orchidectomy, in which his testicles were surgically removed. He was reassigned to be raised as female and given the name Brenda. Money further recommended hormone treatment to which the parents agreed. Money then recommended a surgical procedure to create an artificial vagina, which the parents refused. Money published a number of papers reporting the reassignment as successful.\n\nFor several years, Money reported on Reimer's progress as the \"John/Joan case\", describing apparently successful female gender development and using this case to support the feasibility of sex reassignment and surgical reconstruction even in non-intersex cases. Notes by a former student at Money's laboratory state that, during the yearly follow-up visits, Reimer's parents routinely lied to staff about the success of the procedure. Reimer's twin brother, Brian, later developed schizophrenia.\n\nDavid Reimer's case came to international attention in 1997 when he told his story to Milton Diamond, an academic sexologist who persuaded Reimer to allow him to report the outcome in order to dissuade physicians from treating other infants similarly. Soon after, Reimer went public with his story, and John Colapinto published a widely disseminated and influential account in \"Rolling Stone\" magazine in December 1997.\n\nOn July 1, 2002, Brian was found dead from an overdose of antidepressants. On May 4, 2004, after suffering years of severe depression, financial instability, and marital troubles, David committed suicide by shooting himself in the head with a sawed-off shotgun at the age of 38. Reimer's parents have stated that Money's methodology was responsible for the deaths of both of their sons.\n\nMoney argued that media response to the exposé was due to right-wing media bias and \"the antifeminist movement\". He said his detractors believed \"masculinity and femininity are built into the genes so women should get back to the mattress and the kitchen\". However, intersex activists also criticized Money, stating that the unreported failure had led to the surgical reassignment of thousands of infants as a matter of policy. Privately, Money was mortified by the case, colleagues said, and as a rule did not discuss it. Money's own views also developed and changed over the years.\n\nJohn Money was critical in debates on chronophilias, especially pedophilia. He stated that both sexual researchers and the public do not make distinctions between affectional pedophilia and sadistic pedophilia. Money asserted that affectional pedophilia was about love and not sex.\n\nMoney held the view that affectional pedophilia is caused by a surplus of parental love that became erotic, and is not a behavioral disorder. Rather, he took the position that heterosexuality is another example of a societal and therefore superficial, ideological concept.\n\n\n"}
{"id": "4007751", "url": "https://en.wikipedia.org/wiki?curid=4007751", "title": "Kuwait Institute for Scientific Research", "text": "Kuwait Institute for Scientific Research\n\nKuwait Institute for Scientific Research (KISR) is an organization in Kuwait which engages in scientific and applied research for several purposes, including preserving the environment, serving the economy, advising the government of scientific issues, and others.\n\nKISR was established in 1967 by the Arabian Oil Company. The institute was founded to carry out applied research in petroleum, arid-zone agriculture and marine biology. In 1973, an amiri decree was made by Jaber Al-Ahmad Al-Jaber Al-Sabah to organize KISR under the responsibility of the Council of Ministers. The objectives of KISR were also reorganized. Another amiri decree was made in 1981 to establish KISR as an independent public institution although it was launched as a semi-official entity.\n\nEnvironmental Management; Coastal Management and Atmospheric Pollution; Urban Infrastructure Development; Advanced Systems; Arid land Agriculture and Greenery; Aquaculture, Fisheries, and Marine Environment; Biotechnology\nRenewable Energy\n\nPetroleum Production; Petrochemical Processes; Petroleum Refining\n\nHydrology; Water Management; Desalination; Wastewater\n\nEconomic Studies; Quantitative Methods and Modelling\n\nSoftware Development Section(SDS), Technology and Applications for Special Needs Section(TASNS), and Geographic Information system Section (GIS)\n\nIntroduction\n\nThe Petroleum Research and Studies Center (PRSC) of the Kuwait Institute for Scientific Research (KISR) was established in 2000. The Center was originally a division of KISR under the name “Division of Petroleum, Petrochemicals and Materials”. The Petroleum Division had been providing services and research studies for Kuwait and the regional petroleum industry for over 25 years. The Division was established in 1967 with the main aim of conducting research and providing consultancy and technical support to the local industry.\n\nThe main goal of PRSC is to become an R&D arm for the petroleum industry in the State of Kuwait. PRSC provides applied research projects and technical services for the Petroleum Industry. PRSC also works to acquire new technologies for the oil industry in order to enhance the overall development in Kuwait. The Center is a source of information and provides technical on-job formal training for its research staff as well as oil sector staff. The PRSC is staffed with personnel in a wide spectrum of technical fields related to oil production, petroleum refining, corrosion and petrochemicals.\n\nThe PRSC focuses its research work and technical studies on:\n\n\nPRSC is located in the city of Ahmadi with sixteen research facilities and two large halls for pilot plants. The PRSC has also managed KOC’s Central Core Store Laboratories since 1997. In addition, PRSC manages several field and mobile laboratories.\n"}
{"id": "51150873", "url": "https://en.wikipedia.org/wiki?curid=51150873", "title": "LEDA 89996", "text": "LEDA 89996\n\nLEDA 89996, also known by its 2MASS designation 2MASS J04542829-6625280, is a spiral galaxy. It is located within the Dorado constellation and appears very close to the Large Magellanic Cloud.\n\nThe galaxy was observed by the Hubble Telescope in 6 July 2015 and is similar in appearance to the Milky Way being spiral shaped with winding spiral arms. The darker patches between the arms is dust and gas. Lots of new stars form in this area making the spirals appear very bright.\n"}
{"id": "821148", "url": "https://en.wikipedia.org/wiki?curid=821148", "title": "Level of measurement", "text": "Level of measurement\n\nLevel of measurement or scale of measure is a classification that describes the nature of information within the values assigned to variables. Psychologist Stanley Smith Stevens developed the best-known classification with four levels, or scales, of measurement: nominal, ordinal, interval, and ratio. This framework of distinguishing levels of measurement originated in psychology and is widely criticized by scholars in other disciplines. Other classifications include those by Mosteller and Tukey, and by Chrisman.\n\nStevens proposed his typology in a 1946 \"Science\" article titled \"On the theory of scales of measurement\". In that article, Stevens claimed that all measurement in science was conducted using four different types of scales that he called \"nominal,\" \"ordinal,\" \"interval,\" and \"ratio,\" unifying both \"qualitative\" (which are described by his \"nominal\" type) and \"quantitative\" (to a different degree, all the rest of his scales). The concept of scale types later received the mathematical rigour that it lacked at its inception with the work of mathematical psychologists Theodore Alper (1985, 1987), Louis Narens (1981a, b), and R. Duncan Luce (1986, 1987, 2001). As Luce (1997, p. 395) wrote:\nTo give a better overview the values in 'Mathematical Operators', 'Advanced operations' and 'Central tendency' are only the ones this level of measurement introduces. The complete list includes the values of previous levels. This is inverted for the 'Measure property'. \nThe nominal type differentiates between items or subjects based only on their names or (meta-)categories and other qualitative classifications they belong to; thus dichotomous data involves the construction of classifications as well as the classification of items. Discovery of an exception to a classification can be viewed as progress. Numbers may be used to represent the variables but the numbers do not have numerical value or relationship: for example, a globally unique identifier.\n\nExamples of these classifications include gender, nationality, ethnicity, language, genre, style, biological species, and form. In a university one could also use hall of affiliation as an example. Other concrete examples are \n\nNominal scales were often called qualitative scales, and measurements made on qualitative scales were called qualitative data. However, the rise of qualitative research has made this usage confusing. The numbers in nominal measurement are assigned as labels and have no specific numerical value or meaning. No form of arithmetic computation (+, −, ×, etc.) may be performed on nominal measures. The nominal level is the lowest measurement level used from a statistical point of view.\n\nEquality and other operations that can be defined in terms of equality, such as inequality and set membership, are the only non-trivial operations that generically apply to objects of the nominal type.\n\nThe mode, i.e. the \"most common\" item, is allowed as the measure of central tendency for the nominal type. On the other hand, the median, i.e. the \"middle-ranked\" item, makes no sense for the nominal type of data since ranking is meaningless for the nominal type.\n\nThe ordinal type allows for rank order (1st, 2nd, 3rd, etc.) by which data can be sorted, but still does not allow for relative \"degree of difference\" between them. Examples include, on one hand, dichotomous data with dichotomous (or dichotomized) values such as 'sick' vs. 'healthy' when measuring health, 'guilty' vs. 'not-guilty' when making judgments in courts, 'wrong/false' vs. 'right/true' when measuring truth value, and, on the other hand, non-dichotomous data consisting of a spectrum of values, such as 'completely agree', 'mostly agree', 'mostly disagree', 'completely disagree' when measuring opinion.\n\nThe median, i.e. \"middle-ranked\", item is allowed as the measure of central tendency; however, the mean (or average) as the measure of central tendency is not allowed. The mode is allowed.\n\nIn 1946, Stevens observed that psychological measurement, such as measurement of opinions, usually operates on ordinal scales; thus means and standard deviations have no validity, but they can be used to get ideas for how to improve operationalization of variables used in questionnaires. Most psychological data collected by psychometric instruments and tests, measuring cognitive and other abilities, are ordinal, although some theoreticians have argued they can be treated as interval or ratio scales. However, there is little prima facie evidence to suggest that such attributes are anything more than ordinal (Cliff, 1996; Cliff & Keats, 2003; Michell, 2008). In particular, IQ scores reflect an ordinal scale, in which all scores are meaningful for comparison only. There is no absolute zero, and a 10-point difference may carry different meanings at different points of the scale.\n\nThe interval type allows for the \"degree of difference\" between items, but not the ratio between them. Examples include \"temperature\" with the Celsius scale, which has two defined points (the freezing and boiling point of water at specific conditions) and then separated into 100 intervals, \"date\" when measured from an arbitrary epoch (such as AD), \"percentage\" such as a percentage return on a stock, \"location\" in Cartesian coordinates, and \"direction\" measured in degrees from true or magnetic north. Ratios are not meaningful since 20 °C cannot be said to be \"twice as hot\" as 10 °C, nor can multiplication/division be carried out between any two dates directly. However, \"ratios of differences\" can be expressed; for example, one difference can be twice another. Interval type variables are sometimes also called \"scaled variables\", but the formal mathematical term is an affine space (in this case an affine line).\n\nThe mode, median, and arithmetic mean are allowed to measure central tendency of interval variables, while measures of statistical dispersion include range and standard deviation. Since one can only divide by \"differences\", one cannot define measures that require some ratios, such as the coefficient of variation. More subtly, while one can define moments about the origin, only central moments are meaningful, since the choice of origin is arbitrary. One can define standardized moments, since ratios of differences are meaningful, but one cannot define the coefficient of variation, since the mean is a moment about the origin, unlike the standard deviation, which is (the square root of) a central moment.\n\nThe ratio type takes its name from the fact that measurement is the estimation of the ratio between a magnitude of a continuous quantity and a unit magnitude of the same kind (Michell, 1997, 1999). A ratio scale possesses a meaningful (unique and non-arbitrary) zero value. Most measurement in the physical sciences and engineering is done on ratio scales. Examples include mass, length, duration, plane angle, energy and electric charge. In contrast to interval scales, ratios are now meaningful because having a non-arbitrary zero point makes it meaningful to say, for example, that one object has \"twice the length.\" Very informally, many ratio scales can be described as specifying \"how much\" of something (i.e. an amount or magnitude) or \"how many\" (a count). The Kelvin temperature scale is a ratio scale because it has a unique, non-arbitrary zero point called absolute zero. \n\nThe geometric mean and the harmonic mean are allowed to measure the central tendency, in addition to the mode, median, and arithmetic mean. The studentized range and the coefficient of variation are allowed to measure statistical dispersion. All statistical measures are allowed because all necessary mathematical operations are defined for the ratio scale.\n\nWhile Stevens's typology is widely adopted, it is still being challenged by other theoreticians, particularly in the cases of the nominal and ordinal types (Michell, 1986).\n\nDuncan (1986) objected to the use of the word \"measurement\" in relation to the nominal type, but Stevens (1975) said of his own definition of measurement that \"the assignment can be any consistent rule. The only rule not allowed would be random assignment, for randomness amounts in effect to a nonrule\". However, so-called nominal measurement involves arbitrary assignment, and the \"permissible transformation\" is any number for any other. This is one of the points made in Lord's (1953) satirical paper \"On the Statistical Treatment of Football Numbers\".\n\nThe use of the mean as a measure of the central tendency for the ordinal type is still debatable among those who accept Stevens's typology. Many behavioural scientists use the mean for ordinal data, anyway. This is often justified on the basis that the ordinal type in behavioural science is in fact somewhere between the true ordinal and interval types; although the interval difference between two ordinal ranks is not constant, it is often of the same order of magnitude.\n\nFor example, applications of measurement models in educational contexts often indicate that total scores have a fairly linear relationship with measurements across the range of an assessment. Thus, some argue that so long as the unknown interval difference between ordinal scale ranks is not too variable, interval scale statistics such as means can meaningfully be used on ordinal scale variables. Statistical analysis software such as SPSS requires the user to select the appropriate measurement class for each variable. This ensures that subsequent user errors cannot inadvertently perform meaningless analyses (for example correlation analysis with a variable on a nominal level).\n\nL. L. Thurstone made progress toward developing a justification for obtaining the interval type, based on the law of comparative judgment. A common application of the law is the analytic hierarchy process. Further progress was made by Georg Rasch (1960), who developed the probabilistic Rasch model that provides a theoretical basis and justification for obtaining interval-level measurements from counts of observations such as total scores on assessments.\n\nTypologies aside from Stevens's typology has been proposed. For instance, Mosteller and Tukey (1977), Nelder (1990) described continuous counts, continuous ratios, count ratios, and categorical modes of data. See also Chrisman (1998), van den Berg (1991).\n\nMosteller and Tukey noted that the four levels are not exhaustive and proposed:\n\nFor example, percentages (a variation on fractions in the Mosteller-Tukey framework) do not fit well into Stevens’s framework: No transformation is fully admissible.\n\nNicholas R. Chrisman introduced an expanded list of levels of measurement to account for various measurements that do not necessarily fit with the traditional notions of levels of measurement. Measurements bound to a range and repeating (like degrees in a circle, clock time, etc.), graded membership categories, and other types of measurement do not fit to Stevens's original work, leading to the introduction of six new levels of measurement, for a total of ten:\n\nWhile some claim that the extended levels of measurement are rarely used outside of academic geography, graded membership is central to fuzzy set theory, while absolute measurements include probabilities and the plausibility and ignorance in Dempster-Shafer theory. Cyclical ratio measurements include angles and times. Counts appear to be ratio measurements, but the scale is not arbitrary and fractional counts are commonly meaningless. Log-interval measurements are commonly displayed in stock market graphics. All these types of measurements are commonly used outside academic geography, and do not fit well to Stevens' original work.\n\nThe theory of scale types is the intellectual handmaiden to Stevens's \"operational theory of measurement\", which was to become definitive within psychology and the behavioral sciences, despite Michell's characterization as its being quite at odds with measurement in the natural sciences (Michell, 1999). Essentially, the operational theory of measurement was a reaction to the conclusions of a committee established in 1932 by the British Association for the Advancement of Science to investigate the possibility of genuine scientific measurement in the psychological and behavioral sciences. This committee, which became known as the \"Ferguson committee\", published a Final Report (Ferguson, et al., 1940, p. 245) in which Stevens's sone scale (Stevens & Davis, 1938) was an object of criticism:\n\nThat is, if Stevens's \"sone\" scale genuinely measured the intensity of auditory sensations, then evidence for such sensations as being quantitative attributes needed to be produced. The evidence needed was the presence of \"additive structure\" – a concept comprehensively treated by the German mathematician Otto Hölder (Hölder, 1901). Given that the physicist and measurement theorist Norman Robert Campbell dominated the Ferguson committee's deliberations, the committee concluded that measurement in the social sciences was impossible due to the lack of concatenation operations. This conclusion was later rendered false by the discovery of the theory of conjoint measurement by Debreu (1960) and independently by Luce & Tukey (1964). However, Stevens's reaction was not to conduct experiments to test for the presence of additive structure in sensations, but instead to render the conclusions of the Ferguson committee null and void by proposing a new theory of measurement:\n\nStevens was greatly influenced by the ideas of another Harvard academic, the Nobel laureate physicist Percy Bridgman (1927), whose doctrine of \"operationism\" Stevens used to define measurement. In Stevens's definition, for example, it is the use of a tape measure that defines length (the object of measurement) as being measurable (and so by implication quantitative). Critics of operationism object that it confuses the relations between two objects or events for properties of one of those of objects or events (Hardcastle, 1995; Michell, 1999; Moyer, 1981a,b; Rogers, 1989).\n\nThe Canadian measurement theorist William Rozeboom (1966) was an early and trenchant critic of Stevens's theory of scale types.\n\nAnother issue is that the same variable may be a different scale type depending on how it is measured and on the goals of the analysis. For example, hair color is usually thought of as a nominal variable, since it has no apparent ordering. However, it is possible to order colors (including hair colors) in various ways, including by hue; this is known as colorimetry. Hue is an interval level variable.\n\n\n"}
{"id": "29493962", "url": "https://en.wikipedia.org/wiki?curid=29493962", "title": "List of American Medical Association journals", "text": "List of American Medical Association journals\n\nThere are thirteen medical journals published by the American Medical Association (AMA). The \"Journal of the American Medical Association (JAMA)\", along with \"JAMA Network Open\" and eleven specialty journals, compose the JAMA Network family of journals. The journals share a common website, archives and other means of access (such as RSS feeds), have common policies on publishing and public relations, and pool their editorial resources in producing the AMA Manual of Style.\n\nThey also operate a common webpage, \"For The Media\", that provides free access to news releases about the latest research published in AMA journals to credentialed journalists prior to official publication dates (pre-embargo content), as well as access to all related pre-embargo (pre-publication) news releases and video news release scripts.\n\nBefore they were rebranded as the JAMA Network in 2013, the AMA's stable of journals were referred to as \"JAMA and the Archives journals\" (for example, this is how the AMA Manual of Style formerly referred to them), because the specialty journals used to have titles on the pattern of \"Archives of [Specialty]\". \n\nJAMA Network recently created three new journals: \"JAMA Oncology\" in 2015, \"JAMA Cardiology\" in 2016, and \"JAMA Network Open\" in 2018. \n\n\n"}
{"id": "37968870", "url": "https://en.wikipedia.org/wiki?curid=37968870", "title": "List of Piciformes by population", "text": "List of Piciformes by population\n\nThis is a list of Piciformes species by global population. While numbers are estimates, they have been made by the experts in their fields. For more information on how these estimates were ascertained, see Wikipedia's articles on population biology and population ecology.\n\nThis list is not comprehensive, as not all Piciformes have had their numbers quantified.\n"}
{"id": "10657512", "url": "https://en.wikipedia.org/wiki?curid=10657512", "title": "List of Sinclair QL software", "text": "List of Sinclair QL software\n\nThis is a list of software titles produced for the Sinclair QL personal computer.\n\nNotation: Program name (purpose), publisher, first release\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "8542010", "url": "https://en.wikipedia.org/wiki?curid=8542010", "title": "List of humanities journals", "text": "List of humanities journals\n\nThe following is a partial list of humanities journals, for academic study and research in the humanities There are thousands of humanities journals in publication, and many more have been published at various points in the past. The list given here is far from exhaustive, and contains only the most influential, currently publishing journals in each field. As a rule of thumb, each field should be represented by about 5 examples, chosen for their current academic importance.\n\n\"Note\": there are many important academic magazines that are not true peer-reviewed journals. They are not listed here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "49185383", "url": "https://en.wikipedia.org/wiki?curid=49185383", "title": "Lo and Behold, Reveries of the Connected World", "text": "Lo and Behold, Reveries of the Connected World\n\nLo and Behold, Reveries of the Connected World is a 2016 American documentary film directed by Werner Herzog. In it, Herzog ponders the existential impact of the Internet, robotics, artificial intelligence, the Internet of Things, and more on human life. The film premiered at the 2016 Sundance Film Festival, and was sponsored by the company NetScout. The film contains interviews with Bob Kahn, Elon Musk, Sebastian Thrun, Ted Nelson, and other leaders of the technology world.\n\n\nHerzog narrates over footage of the University of California at Los Angeles, \"the birthplace of the Internet,\" then comes to the first piece of Internet equipment ever to be installed. From here, the film explores the beneficial opportunities the Internet has afforded humans.\nHerzog interviews a family that has been harassed online after the death of their daughter. They express their grief.\nAn institute where no electronic equipment is allowed within a 3-mile radius is examined and the society of people living in this area expresses their experience. Eventually, the film comes to a group of people that are afflicted with an electromagnetism sensitivity condition who begrudgingly have to live in this area.\nThen the film comes to Elon Musk and his quest to send humans to Mars. Artificial intelligence is touched upon and the film comes to focus on how robots could become replacements for human interaction in the future.\n\nAt the end of the film, Herzog poses the question to multiple interviewees, \"Can the Internet dream of itself?\"\n\n\n\"Lo and Behold\" has received generally favorable reviews from critics. On review aggregator website Rotten Tomatoes, the film has a 94% approval rating based on 61 reviews, with an average rating of 7.5 out of 10. On Metacritic, the film has a weighted average score of 78 out of 100 based on seven critics, indicating \"generally favorable reviews\".\n\n"}
{"id": "5048374", "url": "https://en.wikipedia.org/wiki?curid=5048374", "title": "Mittelwerk", "text": "Mittelwerk\n\nMittelwerk (German for \"Central Works\") was a German World War II factory built underground in the Kohnstein to avoid Allied bombing. It used slave labor from the Mittelbau-Dora concentration camp to produce V-2 ballistic missiles, V-1 flying bombs, and other weapons.\n\nOn the night of 17/18 August 1943, RAF bombers carried out Operation Hydra against the Peenemünde Army Research Center where V-2 development and production was being carried out.\n\nOn 19 October 1943, the German limited company was issued War Contract No. 0011-5565/43 by General Emil Leeb, head of the Army Weapons Office, for 12,000 A-4 missiles at 40,000 Reichsmarks each.<br>\nMittelwerk GmbH also headed sites for V-2 rocket development and testing at Schlier (Project Zement) and Lehesten. Beginning in May 1944, Georg Rickhey was the Mittelwerk general manager, Albin Sawatzki was the Mittelwerk technical director over both Arthur Rudolph's Technical Division (with deputy Karl Seidenstuecker) and Hans Lindenberg's 50 engineers of the quality control group located at Ilfeld. Other Mittelwerk/Ilfield engineers included Magnus von Braun in turbopump production, Guenther Haukohl who supervised V-2 production after helping design the assembly line, Eric Ball (assembly line), Hans Fridrich, Hans Palaoro and Rudolph Schlidt. The facility had a communications staff under Captain Dr Kühle, an Administrative Division run by Börner under Mittelwerk board member Otto Karl Bersch, and a Prisoner Labor Supply office (\"Brozsat\"). Hannelore Bannasch was Sawatzki's secretary.\n\nIn July 1944, Hans Kammler ordered the North Works (\"Nordwerke\") to use cross-tunnels 1-20 for a Junkers jet and piston engine factory, leaving cross-tunnels 21-46 for Mittelwerk GmbH. During February–April 1945, the Nordhausen plant built Taifun anti-aircraft missiles and Heinkel He 162 jet fighters and put into operation a liquid oxygen plant. The plant was the \"Eber\" project and used equipment evacuated from the Watten bunker and elsewhere to build Heylandt liquid oxygen generators; the 15 generators were nearly complete when the site was captured. The Mittelwerk also contained equipment for producing jet fuel, and in an emergency 1944 decentralization program (named \"Geilenbergprogramm\" after Edmund Geilenberg) started the \"Cuckoo\" project, an underground oil plant to be \"carved out of the \" North of the Mittelwerk.\n\nAdditional plans for V-2 rocket plants (the Southern Works near Friedrichshafen and the Eastern Works near Riga) were never fulfilled.\n\nV-1 flying bomb assembly began during October/November 1944 in the South end of tunnel A. At the end of January 1945, 51 V-1s were shipped from a dispersed Fieseler factory in Upper Bavaria (code name Cham) to the Nordhausen plant for completion. After a second V-1 factory at Burg was closed, the Mittelwerk \"Werk II\" in February 1945 was the only factory producing V-1 flying bombs, and a total of 2,275 V-1s were built by Werk II from September 1944 until April 1945.\n\nAlthough there has long been speculation about other \"exotic\" weaponry being constructed or stored at Mittelwerk, evidence of this is scarce. For example, Richard Overy notes in \"The Bombing War - Europe 1939-1945\" (2013): \"There is some evidence that small spherical bombs containing radioactive waste were stored in the Mittelbau-Dora works [...], but it is not conclusive.\"\n\nIn late February 1945, the Allied Chiefs of Staff discussed a proposed attack on the Nordhausen plant with a highly flammable petroleum-soap mixture that had been used in the Pacific theatre to deeply penetrate buried strongpoints and scourge them with intense heat.\nThe area was attacked with conventional bombs by RAF Bomber Command on the 3 & 4 April. What were believed to be barracks were attacked on 3 April but they actually contained forced labor workers. The attack of 4 April hit the barracks and the town of Nordhausen. The Mittelbau-Dora forced labor was evacuated on 4 April, and scientists evacuated to the \"Alpenfestung\" (). Hitler had made an order, the \"Demolitions on Reich Territory Decree\", which ordered the destruction of any infrastructure that might be of use to the Allies but it was deliberately ignored by Speer and the Nordhausen plant was evacuated without damage.\n\nHaving been warned to \"expect something a little unusual in the Nordhausen area\", and after previously entering the Nordhausen plant from the North through the Junkers Nordwerke, U.S. 3rd Armored Division and 104th Infantry Divisions reached the city of Nordhausen on 11 April 1945 and discovered the dead and sick of the Boelcke Kaserne barracks.\n\nCasualties of the V-2 rocket are estimated at 2,541 killed and 5,923 injured. By contrast, of the roughly 60,000 people who passed through Mittelbau-Dora and its subcamps, an estimated 20,000 died either at the camp or at places they were subsequently transported to: 350 were hanged (including 200 for sabotage), many others died from exhaustion, cold, malnutrition or disease. Some were murdered by guards. The total also includes 1,300 to 1,500 prisoners killed by British bombs in early April.\n\nOn 22 May 1945, US Army Special Mission V-2 shipped the first trainload of rocket parts for use in projects such as Operation Sandy, Operation Blossom and, at the White Sands Proving Grounds, the Hermes project.\nThe Nordhausen area was to become part of the Soviet zone of occupation, and Soviet Army officers arrived to tour the Nordhausen plant on 26 May 1945. In June 1945, the US Army left the Nordhausen plant as required by JCS Directive 1067/14, with parts, machine tools, and documents (including blueprints for the projected A-9/A-10 intercontinental missile) left for the Soviets. The Red Army occupied the Mittelwerk on 5 July 1945 and demolished both of the entrances of the tunnel system in mid-1948.\n\nThe 1947 Dora Trial convicted SS Officers and concentration camp kapos, while 3 scientists of the V-2 rocket program were implicated (2 after the trial) and exonerated of Nazi war crimes at the Mittelwerk. On 19 May 1947, the former head of the Mittelwerk facility, Georg Rickhey, was extradited to Germany from Wright Field in the U.S. and acquitted of war crimes at the Dora Trial. Arthur Rudolph, after immigrating to the U.S. and playing key roles in the Pershing missile and Apollo programs, was forced to renounce his U.S. citizenship and return to Germany; the West German government, citing the statute of limitations, never charged him and eventually granted him citizenship. Wernher von Braun, the Technical Director of a separate facility at Peenemünde Army Research Center, visited the Mittelwerk on 25 January 1944, and a 1991 author alleged he witnessed Mittelwerk and Buchenwald war crimes.\n\nAfter a new entrance tunnel had been dug to former rail Tunnel A in 1995, 710 meters of the tunnel system were opened for visitors. Large parts of the system are flooded by ground water, while other parts have collapsed. After the reunification of Germany the tunnels were frequently looted by treasure seekers who gained access via the private mine in the north of the Kohnstein.\n\nWilli Kramer, a German archaeologist and scientist who dived in the tunnel system in 1992 and 1998, estimated that 70 tons of material was stolen. Access through these entrances was not secured until 2004, when the mine went into insolvency.\n\n\n \n"}
{"id": "27706498", "url": "https://en.wikipedia.org/wiki?curid=27706498", "title": "Nahal Amud", "text": "Nahal Amud\n\nNahal Amud (), also known as the Wadi Amud, is a stream in the Upper Galilee region of Israel that flows into the Sea of Galilee.\n\nThe source of the stream, Ramat Dalton, is located 800 meters above sea level. Its drainage basin includes the peaks of Mount Canaan (955 meters) and Mount Meron (1,204 meters) and flows south through eastern Galilee to the northwest part of the Sea of Galilee – a height of less than 200 meters below sea level.\n\nThe stream is named after a pillar that rises high above ground and is located near a channel of the stream near Kibbutz Hukok. The gorge that forms the channel at this point holds many caves once inhabited by \"Homo heidelbergensis\" and later by Neanderthal Man such as the cave at Zuttiyeh and the Amud cave. They were the object of the first paleoanthropological excavations in Mandatory Palestine in 1925–1926. The caves contained hominin remains as well as Mousterian and Acheulean artifacts.\n\nMost of Nahal Amud (8923 dunams) was declared a nature reserve in 1972.\n\n"}
{"id": "4055832", "url": "https://en.wikipedia.org/wiki?curid=4055832", "title": "Nonlocal Lagrangian", "text": "Nonlocal Lagrangian\n\nIn field theory, a nonlocal Lagrangian is a Lagrangian, a type of functional formula_1 containing terms that are \"nonlocal\" in the fields formula_2, i.e. not polynomials or functions of the fields or their derivatives evaluated at a single point in the space of dynamical parameters (e.g. space-time). Examples of such nonlocal Lagrangians might be:\n\nActions obtained from nonlocal Lagrangians are called \"nonlocal actions\". The actions appearing in the fundamental theories of physics, such as the Standard Model, are local actions; nonlocal actions play a part in theories that attempt to go beyond the Standard Model and also in some effective field theories. Nonlocalization of a local action is also an essential aspect of some regularization procedures. Noncommutative quantum field theory also gives rise to nonlocal actions.\n"}
{"id": "1212705", "url": "https://en.wikipedia.org/wiki?curid=1212705", "title": "Online consultation", "text": "Online consultation\n\nOnline consultations or e-consultations refer to an exchange between government and citizens using the Internet. They are one form of online deliberation. Further, online consultation consists in using the Internet to ask a group of people their opinion on one or more specific topics, allowing for trade-offs between participants. Generally, an agency consults a group of people to get their thoughts on an issue when a project or a policy is being developed or implemented, e.g. to identify or access options, or to evaluate ongoing activities. This enables governments to draft more citizen-centered policy.\n\nAs the Internet gains popularity with the public for voicing opinion, citizen participation in policy development through cyberspace is changing the face of democracy. The rise of the Internet has given way to buzzwords such as e-democracy, referring to citizen participation in politics, government issues and policy development through electronic technologies and the Internet, and eGovernment, pertaining to providing citizens with government information and services online. Online consultation is an extension of these concepts. Through online engagement, government is enabled to hold interactive dialogues with the public as they have a more direct route to citizen opinion via the Internet.\n\nThe California Report Card (CRC) facilitates online consultation, working to support collaboration between citizens and the government though the Internet. Gavin Newsom, Lt. Governor of California, and the Center for Information Technology Research in the Interest of Society at University of California, Berkeley jointly launched the CRC in January 2014. The CRC allows for Californians to vote online on six timely issues. The site then redirects users to an electronic \"cafe\" using Principal Component Analysis. In the \"cafe\", participants can textually submit their own suggestions and assess the ideas of other users. The CRC offers a means of connecting the public to the California government. \n\nWhile this definition is framed in the Canadian context, other countries like the UK, Denmark, Scotland, and Australia can also be considered leaders in the field. These and many other countries are integrating online consultations and engagement using various methods and for a range of purposes. The European Union also utilises online consultations. These complement face-to-face consultations and help to create greater transparency of the democratic process. Online consultations are also increasingly being used by the United Nations and its Specialized Agencies. The Food and Agriculture Organization of the UN hosts online consultations to allow for more inclusive drafting processes of policy guidelines, reports and strategy papers. The Global Forum on Food Security and Nutrition (FSN Forum) is tasked with carrying out many of these consultations.\n\nOnline consultations and engagement activities can utilize: \n\n\n"}
{"id": "22300361", "url": "https://en.wikipedia.org/wiki?curid=22300361", "title": "Protoscholastic writing", "text": "Protoscholastic writing\n\nProtoscholastic writing re-introduced space between words into the punctuation of western languages. It replaced liturgical models starting in the 7th century and by the 14th century it was the standard. The protoscholastic mode of writing was primarily introduced for faster decoding of written texts for scholars, rather than for oral communication (the focus of liturgical models).\n"}
{"id": "597280", "url": "https://en.wikipedia.org/wiki?curid=597280", "title": "Robert J. Cenker", "text": "Robert J. Cenker\n\n<!--*****************************************************************\n"}
{"id": "7517319", "url": "https://en.wikipedia.org/wiki?curid=7517319", "title": "Rule induction", "text": "Rule induction\n\nRule induction is an area of machine learning in which formal rules are extracted from a set of observations. The rules extracted may represent a full scientific model of the data, or merely represent local patterns in the data.\n\nSome major rule induction paradigms are:\n\nSome rule induction algorithms are:\n"}
{"id": "11009131", "url": "https://en.wikipedia.org/wiki?curid=11009131", "title": "Short-tailed chinchilla", "text": "Short-tailed chinchilla\n\nThe short-tailed chinchilla (\"Chinchilla chinchilla\", formerly known as \"Chinchilla brevicaudata\"), also called the Bolivian, Peruvian, or royal chinchilla, is an endangered species of rodent. Their original range included the Andes Mountains of Argentina, Chile, Peru, and Bolivia. The rodents were exploited for their luxurious fur, causing their numbers to dwindle greatly.\n\nChinchillas’ bodies measure between 28 and 49 cm long and weigh around 38 to 50 ounces. They have short front legs and long, powerful hind legs that aid in climbing and jumping. Short-tailed chinchillas have thicker necks and shoulders and have much shorter tails than their long-tailed relatives.\n\nIn the wild, chinchillas burrow under rocks or in the ground for shelter. They mostly live in colder climates for which they are well adapted because of their dense fur. They mostly feed upon vegetation. They are social animals living in colonies or herds; chinchillas usually have litters of one or two offspring.\n\nMany chinchillas are bred in captivity for their fur, which is very fine and dense, and is in high demand in the fur industry. Commercial hunting began in 1829 and increased every year, by about half a million skins annually, as fur and skin demand increased in the United States and Europe. “The continuous and intense harvesting rate, however, was not sustainable and the number of chinchillas hunted declined until the resource was considered economically extinct by 1917\". Hunting chinchillas became illegal in 1929, but laws were not effectively enforced until 1983.\n\nBecause of the impending extinction of short-tailed chinchillas, conservation measures were implemented in the 1890s in Chile. However, these measures were unregulated. The 1910 treaty between Chile, Bolivia, Argentina, and Peru brought the first international efforts to ban hunting and commercialization of chinchillas. Unfortunately, this effort led to great price increases, thus led to the further decline of the remaining populations. The first successful protection law passed in Chile was not until 1929. Today, both the short-tailed and long-tailed chinchillas are listed as “endangered” both in Chile and by the IUCN. Because of successful reproduction in captive environments, chinchillas are less hunted in the wild.\n"}
{"id": "8378107", "url": "https://en.wikipedia.org/wiki?curid=8378107", "title": "Simon model", "text": "Simon model\n\nIn applied probability theory, the Simon model is a class of stochastic models that results in a power-law distribution function. It was proposed by Herbert A. Simon to account for the wide range of empirical distributions following a power-law. It models the dynamics of a system of elements with associated counters (e.g., words and their frequencies in texts, or nodes in a network and their connectivity formula_1). In this model the dynamics of the system is based on constant growth via addition of new elements (new instances of words) as well as incrementing the counters (new occurrences of a word) at a rate proportional to their current values.\n\nTo model this type of network growth as described above, Bornholdt and Ebel considered a network with formula_2 nodes, and each node with connectivities formula_3, formula_4. These nodes\nform classes formula_5 of formula_6 nodes with identical connectivity formula_1.\nRepeat the following steps:\n\n(i) With probability formula_8 add a new node and attach a link to it from an arbitrarily chosen node.\n\n(ii) With probability formula_9 add one link from an arbitrary node to a node formula_10 of class formula_5 chosen with a probability proportional to formula_12.\n\nFor this stochastic process, Simon found a stationary solution exhibiting power-law scaling, formula_13, with exponent formula_14\n\n(i) Barabási-Albert (BA) model can be mapped to the subclass formula_15 of Simon's model, when using the simpler probability for a node being\nconnected to another node formula_16 with connectivity formula_3 formula_18 (same as the preferential attachment at BA model). In other words, the Simon model describes a general class of stochastic processes that can result in a scale-free network, appropriate to capture Pareto and Zipf's laws.\n\n(ii) The only free parameter of the model formula_8 reflects the relative\ngrowth of number of nodes versus the number of links. In general formula_8 has small values; therefore, the scaling exponents can be predicted to be formula_21. For instance, Bornholdt and Ebel studied the linking dynamics of World Wide Web, and predicted the scaling exponent as formula_22, which was consistent with observation.\n\n(iii) The interest in the scale-free model comes from its ability to describe the topology of complex networks. The Simon model does not have an underlying network structure, as it was designed to describe events whose frequency follows a power-law. Thus network measures going beyond the degree distribution such\nas the average path length, spectral properties, and clustering coefficient, cannot be obtained from this mapping.\n\nThe Simon model is related to generalized scale-free models with growth and preferential attachment properties. For more reference, see.\n"}
{"id": "52387920", "url": "https://en.wikipedia.org/wiki?curid=52387920", "title": "Society for Economic Botany", "text": "Society for Economic Botany\n\nThe Society for Economic Botany is an international learned society covering the field of economic botany. It was established in 1959. Its official journal is \"Economic Botany\", published on their behalf by Springer Science+Business Media and the New York Botanical Garden Press. The society also publishes a biannual newsletter, \"Plants and People\". The president is Sunshine Brosi (Frostburg, MD, United States). The society organizes annual meetings at different locations around the world, where it awards the prize of Distinguished Economic Botanist to particularly meritorious individuals.\n\nRecent recipients of the Distinguished Economic Botanist prize include:\nPast presidents of the society include:\n"}
{"id": "2877892", "url": "https://en.wikipedia.org/wiki?curid=2877892", "title": "Space Race (TV series)", "text": "Space Race (TV series)\n\nSpace Race is a BBC docudrama series first shown in Britain on BBC2 between 14 September and 5 October 2005, chronicling the major events and characters in the American/Soviet space race up to the first landing of a man on the moon. It focuses on Sergei Korolev, the Soviet chief rocket designer, and Wernher von Braun, his American counterpart. The series was a joint effort between British, German, American and Russian production teams.\n\n\nWe see the results of Wernher von Braun's work on the V-2 for the Nazis at Mittelwerk and Peenemünde, and his final activities within Germany during the last years of the Second World War, as both American and Soviet forces race to capture German rocket technology. When the Americans gain the upper hand by recovering von Braun and most of his senior staff, along with all their technical documents and much other materiel, we see Sergei Korolev's release from the Gulag to act as the Soviets' rocketry expert alongside former colleague Valentin Glushko, and how he is set to work bringing Soviet rocket technology up to date with that of von Braun, working with what material and personnel are left after von Braun's escape to the US.\n\nAs the Cold War intensifies, Korolev is asked to build a rocket capable of carrying a five-ton warhead to America - he designs and constructs the R-7 Semyorka, the first ICBM, and is later allowed to use it to launch the first satellite, Sputnik 1, quickly following up with the rushed Sputnik 2. Meanwhile, von Braun struggles to persuade the US government to allow him to launch his own satellite - after Sputnik's launch and the failure of the US Navy to launch a Vanguard satellite, he is finally allowed to launch the first American satellite, Explorer 1. Korolev announces that the Americans have evened the score and that they are in a space race, which they intend to win. At the end of the episode, two men are shown walking down a corridor, one of them wearing a spacesuit.\n\nBoth the Americans and Soviets are planning manned space flight, and we see both sides preparing to do so with the development of the Vostok programme (USSR) and Project Mercury (USA). As well as basic details about the capsules and their delivery vehicles, we also see some of the selection and training of the Russian cosmonauts, and rather less of that of their counterparts in the US. After difficulties and failures on both sides, including a side story about a catastrophic failure of one of the first Russian ballistic missiles, the Soviets succeed in putting Yuri Gagarin into space first, with the Americans putting Alan Shepard up shortly afterwards.\n\nBoth sides now plan to put a man on the Moon - the Americans pull ahead in the space race with Project Gemini, but then suffer a disaster with the Apollo 1 fire. Meanwhile, despite a few notable successes such as the first space walk by Alexei Leonov, the Soviet space programme struggles to keep up amid internal strife. Glushko and Korolev permanently fall out in an argument about fuel; Korolev turns to Nikolai Kuznetsov to develop engines instead. Kuznetsov delivers the NK-33, very efficient but much less powerful than the Americans' F-1. The Soviet program suffers further blows when Korolev dies during surgery, Gagarin dies in a jet crash, Soyuz 1 crashes and kills Vladimir Komarov, and the prototype booster for the moon shot, the N-1 rocket, fails to successfully launch. In America, von Braun has continuing difficulties with the Saturn V, especially combustion instability in the large F-1 engine, but these are ultimately overcome almost by brute force at great expense, and the rocket successfully launches the first manned lunar mission, Apollo 8, and the first manned lunar landing, Apollo 11. The final episode finishes with brief textual summaries of the remaining careers of the various people involved.\n\nBBC filmed the \"Space Race\" in and around the town of Sibiu, Transylvania, a region of Romania. Romania has signed the EU co-production treaty which allows for EU co-productions. Compared to other locations, Romania attracted BBC with unspoiled natural locations, experienced crews and moderately priced production facilities.\n\nThe series was filmed with the Panasonic SDX 900 DVCPro 50 professional camcorder. This allowed keeping to the speedy shooting schedule and provided the ‘gritty’ look appropriate to the time period. Shot in widescreen 25fps progressive mode, the series deliver rich, filmic feel, which compares favourably with high definition.\n\n\nMost of the historical and technological data presented in the series is heavily simplified, and sometimes contains outright untruths or errors. The series would best be described and interpreted as giving a general impression of the subject matter, rather than rigorous factual account.\n\n\nThe series repeats the claim Korolev was denounced by Glushko several times. There are no known documents substanitating this statement. Glushko had been imprisoned himself before Korolev was arrested and had been sentenced to eight years in a prison camp “for participating in sabotage organization.” He was retained to work for the NKVD to develop aircraft jet boosters. In 1942, at Glushko’s request, NKVD transferred Korolev from another prison to Glushko’s OKB.\n\n\nA companion book to the series was written by Deborah Cadbury.\n\n\n\n"}
{"id": "1729618", "url": "https://en.wikipedia.org/wiki?curid=1729618", "title": "Stasis (fiction)", "text": "Stasis (fiction)\n\nA stasis or stasis field, in science fiction, is a confined area of space in which time has been stopped or the contents have been rendered motionless.\n\nA stasis field is imagined to be a region in which a stasis process is in effect. Stasis fields in fictional settings often have several common characteristics. These include infinite or nearly infinite rigidity, making them \"unbreakable objects\" and a perfect or nearly-perfect reflective surface. Most science fiction plots rely on a physical device to establish this region. When the device is deactivated, the stasis field collapses, and the stasis effect ends.\n\nTime is often suspended in stasis fields. Such fields thus have the additional property of protecting non-living materials from deterioration. This time dilation can be, from an in-universe perspective, absolute; something thrown into the field, has the field triggered and then reactivated, would fly out as if nothing had happened. Storylines using such fields often have materials as well as living beings surviving thousands or millions of years beyond their normal lifetimes. The property also allows for such plot devices as booby traps, containing, for instance, a nuclear bomb. Once out of the stasis field, the trap is sprung. In such a situation, to avoid the protagonist from seeing what is in the field, the story line would not allow normal beings to see something protected by a stasis field.\n\nOne use of stasis fields is as a form of suspended animation: to let passengers and cargoes (normally of spacecraft or sleeper ships) avoid having to experience extremely long periods of time by \"skipping over\" large sections of it. They may also be used, such as in \"The Night's Dawn Trilogy\", as protection against the effects of extreme acceleration.\n\nThere are real phenomena that cause time dilation similar that of a stasis field. Extremely high velocities approaching light speed or immensely powerful gravitational fields such as those existing near the event horizons of black holes will cause time to progress more slowly. However, there is no known theoretical way of causing such time dilation independently of such conditions.\n\nThe noted science fiction author Larry Niven used the concept of stasis fields and stasis boxes to a great extent directly or indirectly throughout his many novels and short stories set in the Known Space series. Niven's stasis fields followed conductive surfaces when established, and the resulting frozen space became a completely invulnerable and perfectly reflective object. They were often used as emergency protective devices. They could also be used to create a weapon called a variable sword, a length of extremely fine wire in a stasis field that makes it able to easily cut through normal matter. For more information, see Slaver stasis field.\n\nA more limited form of stasis field is the \"bobble\", found in Vernor Vinge's Peace Authority setting. A bobble is always perfectly spherical and exists for a fixed period of time that is set when the bobble is first created. The duration of a bobble effect cannot be changed. Bobble generators were initially used as weapons, removing their targets from the field of combat.\n\nAnother example of a stasis field exists in Joe Haldeman's \"The Forever War\", where stasis field generators are carried by troops to create conditions where melee weapons become the only viable means of combat. Inside the field, no object can travel faster than 16.3 m/s, which includes electrons, photons, and the field itself. Soldiers inside the field must be wearing suits with a special coating, or all electrical activity within their body would stop and the soldiers would die. In the novel, the main character defeats an enemy army, which has besieged a small remaining contingent of human troops on a moon, by arming a nuclear bomb inside the field and then moving the field away from the bomb. Once the bomb is revealed, its electrical activity resumes, and it promptly detonates. That vaporises the surrounding army, and a large chunk of the ground beneath the field. The soldiers emerge some days later to see if their trick worked and find themselves alone at the bottom of a large crater, their enemy destroyed.\n\nIn Peter F. Hamilton’s \"The Night’s Dawn\" trilogy (1996-1999), “zero-tau pods” — powered containers inside which time halts — are an important narrative device.\n\nIn the computer strategy game \"StarCraft\", the Arbiter unit can, through a combination of Protoss technology and the Arbiter's psionic power, create a stasis field that traps all units in the affected area in blue \"crystals\" of stopped time, taking them out of the fight and rendering them invulnerable for 30 seconds, thus allowing both offensive and defensive applications.\n\nThe \"Dead Space\" series has the main character Isaac Clarke carry a wrist-mounted tachyon-based stasis module, used to slow enemy Necromorphs to a crawl for a short period of time. He adapted its use to fight Necromorphs; it was used previously by technicians to slow down malfunctioning equipment that moves at dangerously high speeds, such as doors. Medical use of the technology is later seen in \"Dead Space 2\", with stasis beds; the protagonist had also been kept in stasis for the majority of the time between games.\n\nThe game \"Mass Effect\" has a biotic power simply called \"Stasis\" that can trap an enemy in a stasis field rendering them immobile as well as invincible to all forms of damage. The duration of this effect is usually dependent on the user's skill level.\n\nIn the Star Wars RPG series \"\", Jedi who follow the path of the light are able to use \"Stasis\" powers, using the force to alter time and freeze an enemy in place. Unlike true stasis, this stasis allows external events to affect the victim so someone held by stasis can be killed while unable to retaliate. The original game also uses a similar effect when Dark Jedi trap party members to engage the player in a duel.\n\nIn the \"Justice League Unlimited\" episode \"The Cat and the Canary\", Green Arrow uses a stunner to put himself into a form of stasis while fighting Wildcat. It was an attempt to end Wildcat's cage fighting career by falsely convincing him he killed Green Arrow during their fight.\n\nIn the \"Invader Zim\" episode \"Walk For Your Lives\", Zim creates a time stasis field and uses it on Dib as an experiment to show to the Tallest, as a result Dib can move only very slowly. Also produced is an explosion, which is also exploding very slowly, Zim decides to throw Dib into the explosion to cause it to speed up. The explosion then explodes at normal speed. \n\nThe Space themed MMO \"Eve Online\" features a weapon called a stasis webifier. When activated against an enemy ship it reduces the target's speed, making them easier to hit and keep in range. The weapon affords no protection to its target, and multiple 'webs' can be used on a ship at once, effectively stopping it dead.\n\nIn \"Half Life\", the protagonist Gordon Freeman is put into a state of Stasis after a brief discussion with the G-Man. A similar incidence happens to Adrian Shepherd at the end of \"\", when the G-Man puts him into a state of stasis \"for further evaluation\".\n\nIn \"Portal\", Chell, the protagonist, is dragged away at the end of the game and put in stasis for many years, until she is awakened at the beginning of \"Portal 2\".\n\nIn \"Project Eden\" one character is frozen in stasis for 15 years. Stasis can also be used offensively to slow down enemies.\n\nIn the first episode of \"Red Dwarf\", \"The End\", third-class technician of the mining ship \"Red Dwarf\", Dave Lister, is put into a stasis booth as punishment for not revealing the whereabouts of his unquarantined cat. However, during his time in stasis, lethal radiation leaks into the ship as a result of a malfunction, killing all the crew (aside from his cat, which was in the cargo hold). Lister is then revived three million years later by the ship's computer, Holly, when the high radiation levels have subsided.\n\nIn Catherine Asaro's Skolian Empire books, the Skolians use quasis to freeze time during interstellar travel.\n"}
{"id": "36275564", "url": "https://en.wikipedia.org/wiki?curid=36275564", "title": "Swakopmund tracking station", "text": "Swakopmund tracking station\n\nSwakopmund tracking station is a Chinese space tracking station in Swakopmund, Namibia, South-West Africa which is used for the Chinese manned space programme. The full name of the station, according to the Namibian Ministry of Education, is China Telemetry, Tracking and Command Station. The station, which opened in 2001, tracks the re-entry of Chinese manned space vehicles.\n\nThe station was built as a result of an agreement signed in October 2000 between the Namibian and Chinese governments. It was built by Windhoek Consulting Engineers at an estimated cost of $12 million Namibian dollars and opened in July 2001. The site is north of Swakopmund and is a 150m x 85m compound surrounded by a 2m high wall. There are two antennae for communicating with spacecraft, one 5m in diameter and the other 9m. The latter is 16m high.\n\nIn March 2012 it was announced that it had participated in six launches. It was involved in the 29 June 2012 descent of Shenzhou 9, the first manned craft to dock with Chinese space station Tiangong 1, and the craft with the first Chinese woman in space, Liu Yang. The ground track of the descending craft passed over the station on its way to land in Inner Mongolia.\n\nThe station is part of the Chinese Tracking, Telemetry, Command and Communications System which uses S band communications. It is one of three overseas stations - the other two are in Karachi, Pakistan and Malindi, Kenya. The programme also uses Yuanwang tracking ships and Tianlian relay satellites.\n"}
{"id": "13344206", "url": "https://en.wikipedia.org/wiki?curid=13344206", "title": "Walter Riedel", "text": "Walter Riedel\n\nWalter J H \"Papa\" Riedel was a German engineer who was the head of the Design Office of the Army Research Centre Peenemünde and the Chief Designer of the A4 (V-2) ballistic rocket. The crater Riedel on the Moon was co-named for him and the German rocket pioneer Klaus Riedel.\n\nEmployed by the Heylandt Company from 27 February 1928, in December 1929 Riedel was assigned responsibility for the development of rocket motors using liquid propellants, initially in collaboration with Max Valier who had joined the company at that date. Riedel took over full responsibility for the rocket motor development in 1930, after Valier’s untimely death (following a rocket motor explosion during a test using paraffin oil (kerosene) as fuel instead of ethyl alcohol) In 1934, research and development of the Heylandt Company was taken over by the Army and amalgamated with the Wernher von Braun Group at the Army Proving Grounds at Kummersdorf, near Berlin, in order to carry out research and development of long-range rocket missiles. In March 1936, Wernher von Braun and Walter Riedel began consideration of much larger rockets than the A3 (under development at that time), which was merely a test vehicle and could not carry any payload. Along with Walter Dornberger, plans were drawn up for a more suitable and better equipped test site for large rockets at Peememünde, to take the place of the rather confined Kummersdorf. From 17 May 1937, following the transfer of the rocket activities from Kummersdorf to the Army’s new rocket establishment at Peenemünde, Riedel headed the Technical Design Office as Chief Designer of the A4 (V2) ballistic rocket After the air raid by the British Royal Air Force (Operation Hydra) on Peenemünde in August 1943, the transfer of the development facility was ordered to a location giving better protection from air attack. The air raid had cost the lives of Dr Walter Thiel (Propulsion Chief) and Erich Walther (Chief of Maintenance for the workshops), two leading men at the Peenemünde Army facilities. In mid-September 1943, Riedel and two others surveyed the Austrian Alps for a new site for rocket development to replace that at Peenemünde. The chosen location was at Ebensee, on the southern end of the Traunsee, 100 km east of Salzburg. The site consisted of a system of galleries driven into the mountains, and received the code name Zement (Cement). Work on the site started at the beginning of 1944 and was intended to be completed in October 1945. From 1 October 1943, Riedel was responsible for supervising the transfer, to Ebensee, of the Peenemünde development facility. From 29 May 1945 to 20 September 1945, following the end of World War II, Riedel was held in protective custody (Sicherheitshaft) at the US Third Army’s internment camp at Deggendorf (situated between Regensburg and Passau). From 1 November 1945 to 10 March 1946 he was employed by the Ministry of Supply (MoS) Establishment at Altenwalde (near Cuxhaven), and from 11 March to 31 July 1946 at the MoS Establishment at Trauen (near Braunschweig). After the Trauen Establishment was disbanded, Riedel emigrated to England, to work initially (from 1947) at the Royal Aircraft Establishment, Farnborough and later (from 1948 until his death in 1968) at the MoS Rocket Propulsion Department (RPD) in Westcott (near Aylesbury, Buckinghamshire). In 1957, Riedel became a British citizen.\n\nAn alleged CIA memorandum of February 9, 1953 places him in Los Angeles as a founding director of the California Committee for Saucer(UFOs) Investigation, unless there is another former Nazi rocket designer with the same name who worked at the Reich's Army Research Centre Peenemünde. In the CIA document, he tells the agent that he has been living in the US for a number of years as part of Operation Paper Clip, a program that helped Nazi scientists to resettle in the US in exchange for their research work. Werner Von Braun of NASA fame is the most well known of the Paperclip club. \n\nhttps://www.cia.gov/library/readingroom/docs/DOC_0000015355.pdf \nWalter Riedel died while visiting East Berlin in East Germany.\n"}
{"id": "8838251", "url": "https://en.wikipedia.org/wiki?curid=8838251", "title": "World Academy of Art and Science", "text": "World Academy of Art and Science\n\nThe World Academy of Art and Science (WAAS) is an international non-governmental scientific organization, a world network of more than 700 individual fellows from more than 80 countries. Fellows are elected for distinguished accomplishments in the sciences, arts and the humanities. The Academy strives to promote the growth of knowledge, enhance public awareness of the social consequences and policy implications of that growth, and provide \"leadership in thought that leads to action\". The spirit of the academy can be expressed in the words of Albert Einstein \"The creations of our mind shall be a blessing and not a curse to mankind\".\n\nThe idea of founding the Academy and a set of world scientific and youth scientist and science journalist associations was proposed in an article in \"Time\" magazine on 1/10/1938 by philosopher and liberal-libertarian Etienne Gilson and during his tours of several scientific associations and universities in Europe and the Americas in the 1940s. He also proposed a world association of Liberal parties, what became www.liberal-international.org, to champion secular democracy and pave the way for better scientific communication and education in a time of growing totalitarianism.\n\nThe idea was echoed in the 1950s by leading scientists who were concerned about the potential for misuse of scientific discoveries. They included Albert Einstein, J. Robert Oppenheimer, and Joseph Rotblat who had been involved in the development of the atomic bomb; Bertrand Russell philosopher and pacifist; Joseph Needham, a co-founder of UNESCO; Lord Boyd Orr, the First Director General of the Food and Agriculture Organization; George Brock Chisholm, the First Director General of the World Health Organization; John A. Fleming, former President of the International Council of Scientific Unions; as well as Hermann Joseph Muller, Harold C. Urey, Francis Perrin, Panchanan Maheshwari, Theodore Monod, Detlev Bronk, Harold Lasswell and other scholars and public figures. The Academy was organized in December 1960 with the aim of creating an informal world association of the highest scientific and ethical norms and standards.\n\nOriginally established in Geneva, Switzerland, in 1960, in 2011 WAAS incorporated as a non-profit organization in the State of California, USA. The Academy maintains offices in Napa, California; Zagreb, Croatia; Bucharest, Romania; and Pondicherry, India. It has a special division for South Eastern Europe.\n\n\nAt present the Academy's membership consists of 730 Fellows, Associate Fellows and Junior Fellows from all parts of the world, representing virtually all fields of science, social sciences, arts and humanities, as well as educators, political leaders, diplomats and heads of leading international NGOs. Fellows are nominated by existing members and directly elected by the members. To be a Fellow of the Academy is to be a member of the global civil society, concerned for the welfare of the increasingly interconnected global civilization with a demonstrated commitment to addressing issues of global importance. Principal criteria for election include:\n\nElection in the WAAS is considered as one of the highest honors that can be accorded a scientist. Fellows include leading artists, politicians, statesmen, former heads of various countries, heads of research institutes and international organizations, Nobel Prize winners and business leaders. Fellows of the Academy are entitled to use the post-nominal letters FWAAS.\n\nIn its approach to the challenges confronting humanity today, the Academy seeks to apply a comprehensive, integrated, human-centered conception of Reliable Knowledge encompassing seven dimensions. The framework is based on the premise that actions based on partial, piecemeal approaches are inadequate to address the pressing problems confronting humanity today and most often lead to unexpected side-effects generative of new and more complex problems.\n\nThe central objective of the Academy’s activities is to develop ideas, strategies and initiatives to promote a New Paradigm of Human Development appropriate to the needs of the 21st Century. WAAS’ current programming focuses on addressing challenges in the fields of democracy, economy and employment, ecology, education, global governance and rule of law, nuclear disarmament, peace and international security, scientific and technological development and application.\nCurrent programs of the Academy include specific initiatives focusing on\n\nThe Academy’s founders intended that WAAS should function as \"an informal World University at the highest scientific and ethical level, in which deep human understanding and fullest sense of responsibility will meet.\" Although some modest beginnings were made to establish a few regional centers of excellence, political conditions at the height of the Cold War, as well as limitations in transport and global communication, posed insurmountable obstacles to realize this vision at the time. Rapid advances in telecommunications and educational technology over the past decade created the opportunity to realize the vision of WAAS Founders.\n\nThe World University Consortium in 2013 is a major initiative of the Academy to design a global system of higher education attuned to emerging opportunities and challenges.\n\nFollowing a major international conference on Global Higher Education organized by the Academy at the University of California at Berkeley in October 2013, the World University Consortium was incorporated in California as a non-governmental organization to promote accessible, affordable, quality education at the global level founded on universal human values, relevant to the emerging needs of the 21st century, oriented to promote full employment, and capable of developing the full potential of each individual student for personal accomplishment and contribution to the collective progress of humanity.\n\nWUC is intended to become a global network and umbrella group working to facilitate educational partnerships and linkages with interested stakeholders with shared goals, to explore creative solutions to enhance the reach, quality and relevance of higher education globally, to provide a platform for new ideas, trans-disciplinary perspectives and integrated courseware intended to address global opportunities and challenges, and to develop ways to draw upon the extensive knowledge and experience acquired by organizations outside the traditional field of academia which possess essential knowledge for addressing real-world problems.\n\n\nWAAS works in partnership with a wide range of leading international institutions including CERN (the world’s leading research institute in nuclear physics at Geneva), Club of Madrid (an association of nearly 100 former heads of state), Club of Rome (author of the famous environmental report on the Limits to Growth), Fundación Cultura de Paz (Madrid, established by Federico Mayor Zaragoza, former Director General of UNESCO), Green Cross International (established by former USSR President Mikhail Gorbachev to promote global ecology), Institute for Cultural Diplomacy (Berlin), Inter-University Centre, Dubrovnik (international consortium of 170 leading universities), Library of Alexandria (Egypt), The Mother’s Service Society (social science research institute based in Pondicherry, India), Nizami Ganjavi International Centre (cultural institute in Baku, Azerbaijan), Pugwash Conferences on Science and World Affairs (on nuclear disarmament).\n\nThe Academy is managed by a 22-member board of trustees and a seven-member Executive Committee. The four principal officers are:\nHeitor Gurgulino de Souza, President – formerly Secretary General of International Association of University Presidents and Rector of United Nations University.\n\nGarry Jacobs, Chief Executive Officer – Vice President of the Mother’s Service Society, formerly Chairman of the Board of the World Academy and Member Secretary of the International Commission on Peace & Food.\n\nWinston Nagan, Chairman of the Board – Professor of Law at University of Florida and former Chairman of Amnesty International (USA).\n\nIvo Šlaus, Honorary President – Dean of Dag Hammarskjold University College of International Relations (Zagreb) and former President of WAAS.\n\n\n\n"}
