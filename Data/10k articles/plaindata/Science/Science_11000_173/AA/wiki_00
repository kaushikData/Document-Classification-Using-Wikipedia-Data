{"id": "1574954", "url": "https://en.wikipedia.org/wiki?curid=1574954", "title": "A Treatise on the Astrolabe", "text": "A Treatise on the Astrolabe\n\nA Treatise on the Astrolabe is a medieval instruction manual on the astrolabe by Geoffrey Chaucer. It is notable for being written in prose, in English and for describing a scientific instrument.\n\nThe \"Treatise\" is considered the \"oldest work in English written upon an elaborate scientific instrument\". It is admired for its clarity in explaining difficult concepts—although modern readers lacking an actual astrolabe may find the details of the astrolabe difficult to understand. Robinson believes that it indicates that had Chaucer written more freely composed prose it would have been superior to his translations of Boece and Melibee.\n\nChaucer’s exact source is undetermined but most of his ‘conclusions’ go back, directly or indirectly, to \"Compositio et Operatio Astrolabii\", a Latin translation of Messahala's Arabic treatise of the 8th century. His description of the instrument amplifies Messahala’s and Chaucer’s indebtedness to Messahala was recognised by John Selden and established by Walter William Skeat. Mark Harvey Liddell held Chaucer drew on \"De Sphaera\" of John de Sacrobosco for the substance of his astronomical definitions and descriptions, but the non-correspondence in language suggests the probable use of an alternative compilation. A collotype facsimile of the second part of Messahala’s Latin text (the portion which is parallel to Chaucer's) is found in Skeat’s \"Treatise On The Astrolabe\". and in Gunther's \"Chaucer and Messahalla on the Astrolabe\".\n\nThe work is written in free flowing contemporary (1391) English, today commonly referred to as middle English. Chaucer explains this departure from the norm thus:\n\nChaucer proceeds to labour the point somewhat:\n\nHe continues to explain that it easier for a child to understand things in his own language than struggle with unfamiliar grammar, a commonplace idea today but radical in the fourteenth century. Finally, he appeals to Royalty (his wife was a lady-in-waiting to Edward III's queen and sister to John of Gaunt's wife) in an early version of the phrase \"the King's English\":\n\nSkeat identifies 22 manuscripts of varying quality. The best he labels \"A\", \"B\" and \"C\" which are MS. Dd. 3.53 (part 2) in the Cambridge University Library, MS. E Museo 54 in the Bodleian Library and MS. Rawlinson, Misc. 1262 also in the Bodleian. \"A\" and \"B\" were apparently written by the same scribe, but \"A\" has been corrected by another hand. Skeat observes that the errors are just those described in \"Chaucers Wordes unto Adam, His Owne Scriveyn\": \n\n\"A\" has indeed been rubbed and scraped then corrected by another hand. This latter scribe Skeat believes to be a better writer than the first. To this second writer was the insertion of diagrams entrusted. \"A\" and \"B\" were apparently written in London about the year 1400, that is some 9 years after the original composition. Manuscript \"C\" is also early, perhaps 1420 and closely agrees with \"A\".\n\nChaucer opens with the words \"Lyte Lowys my sone\". In the past a question arose whether the Lowys was Chaucer's son or some other child he was in close contact with. Kittredge suggested that it could be Lewis Clifford, a son of a friend and possible a godson of Chaucer's. As evidence he advanced that Lewis Clifford died in October 1391, the year of the composition, which could explain its abandonment. Robinson reports though the finding of a document by Professor Manly \"recently\" (to 1957) which links one Lewis Chaucer with Geoffrey's eldest child Thomas Chaucer. The likelihood therefore is that the dedication can be taken at face value.\n\nChaucer had an eye to the wider public as well. In the prologue he says:\n\nThe work was planned to have an introduction and five sections:\n\nPart 1 is complete and extant. Part 2 is also extant with certain caveats described below. Part 3, if it ever existed, is not extant as part of the Treatise. Part 4 was, in the opinion of Skeat, probably never written. Part 5 also was probably never written which Skeat approves of. Indeed, he draws attention to Chaucer's comment at the end of conclusion 4:\n\nThe whole of this section describes the form of an astrolabe. The astrolabe is based on a large plate (\"The moder\" or \"mother\") which is arranged to hang vertically from a thumb ring. It has \"a large hool, that resceiveth in hir wombe the thin plates\". The back of the astrolabe is engraved with various scales (see Skeat's sketch below). Mounted on the back is a sighting rule (Skeat's fig 3, below) \"a brod rule, that hath on either end a square plate perced with certein holes\". To hold it all together there is a \"pyn\" with a \"littel wegge\" (wedge) as shown below at Skeat's fig 7. Into the \"womb\" various thin plates can be inserted which are designed for a particular place: \"compowned after the latitude of Oxenforde\". These plates show the star map. Surmounting them is a \"riet\" or \"rete\" which is a pierced framework carrying the major stars shown at fig 9. Outside all is another rule, this time not with sighting holes, mounted on the common pivot, see fig 6.\n\nPart 2 consists of around 40 propositions or descriptions of things that can be done with the astrolabe. The exact number is uncertain since of the later propositions some are of disputed or doubtful authenticity. Skeat accepts that propositions 1-40 are unambiguously genuine. Robinson generally follows Skeat's reasoning. These first 40 propositions form the cannon of part 2, the propositions that follow are usually labeled \"Supplementary Propositions\".\n\nThe astrolabe was a sophisticated precision instrument. With it one could determine the date, time (when the sky was clear), the position of stars, the passage of the zodiac, latitude on the earth's surface, tides and basic surveying. Care must be taken not to dismiss the astrological aspects; as well as any mystical interpretation astrological terminology was used for what today would be recognized as astronomy. Determining when the sun entered a house (or sign) of the zodiac was a precise determination of the calendar.\n\nSkeat produced a number of sketches to accompany his edition:\n\nThe stars listed on the rim of the rete of the drawings in the \"Treatise\" are given below with their modern names:\n\nFootnotes\n\nCitations\n\nBibliography\n\n"}
{"id": "24244719", "url": "https://en.wikipedia.org/wiki?curid=24244719", "title": "Aeronautical and Astronautical Research Laboratory", "text": "Aeronautical and Astronautical Research Laboratory\n\nThe Aeronautical/Astronautical Research Laboratory (AARL) is an aerospace engineering research facility operated by Ohio State University. It is the principal research facility of the College of Engineering's Department of Aerospace and Astronautical Engineering. It is located on the grounds of Ohio State University Airport, in Columbus, Ohio. \n\nThe research activities conducted at AARL are based around a compressed air system that is capable of holding 1500 cubic feet (42.5 m) of air at 2560 psia (17.65MPa), a 2 MW on site DC power station, and vacuum system capable of holding suction at .15 psia (1.01 kPa). \nThe AARL houses four active wind tunnels (dimensions of test section given in name):\n\nComputers complement wind tunnel and all other experimentation at AARL. LabVIEW is extensively used for experiment documentation and control, while FLUENT and the Ohio Supercomputer Center are used for computational fluid dynamics experimentation.\n\nThe most prominent area of research at AARL is jet engine test cell design. Under the patronage of GE Aviation, Ohio State Engineering has been at the forefront of jet engine test cell design for over 25 years. To test jet engine test cell designs, scale models of the interior of the building proposed to hold the test cell, as well as the test cell itself are reconstructed at AARL. The models are incredibly detailed, going as far as to contain scale furniture. Any number of scale jet engines can then be placed in the model, including a 1/12 scale model of the GE90, the largest jet engine ever built.\n\nThe AARL houses numerous other research facilities, including the Mechanical Engineering Department's Gas Dynamics and Turbulence Laboratory(GDTL) and Gas Turbine Lab. GDTL works cooperatively with NASA Glenn Research Center and Wright-Patterson Air Force Base researching aeroacoustics and high reynolds number flow related to propulsion. The Gas Turbine Lab is an R&D laboratory that focuses on the development of gas turbine aircraft engines and has an annual budget of over $4 million. \n"}
{"id": "23646085", "url": "https://en.wikipedia.org/wiki?curid=23646085", "title": "Astronomical rings", "text": "Astronomical rings\n\nAstronomical rings (Latin: annuli astronomici), also known as Gemma's rings, are an early astronomical instrument. The instrument consists of three rings, each representing the celestial equator, declination, and the meridian.\n\nIt can be used as a sun dial to tell time, if the approximate latitude and season is known, or to tell latitude, if the time is known or observed (at solar noon). It may be considered to be a simplified, portable armillary sphere, or a more complex form of astrolabe.\n\nParts of the instrument go back to instruments made and used by ancient Greek astronomers. Gemma Frisius combined several of the instruments into a small, portable, astronomical-ring instrument. He first published the design in 1534, and in Petrus Apianus's \"Cosmographia\" in 1539. These ring instruments combined terrestrial and celestial calculations.\n\nFixed astronomical rings are mounted on a plinth, like armillary spheres, and can be used as sundials.\n\nThe dial is suspended from a cord or chain; the suspension point on the vertical meridian ring can be changed to match the local latitude. The time is read off on the equatorial ring; in the example below, the center bar is twisted until a sunray passes through a small hole and falls on the horizontal equatorial ring.\n\nA sunring or farmer's ring is a latitude-specific simplification of astronomical rings. On one-piece sunrings, the time and month scale is marked on the inside of the ring; a sunbeam passing through a hole in the ring lights a point on this scale. Newer sunrings are often made in two parts, one of which slides to set the month; they are usually less accurate.\n\nIn 1610, Edward Wright created the sea ring, which mounted a universal ring dial over a magnetic compass. This permitted mariners to determine the time and magnetic variation in a single step.\n\nThe three rings are oriented with respect to the local meridian, the planet's equator, and a celestial object. The instrument itself can be used as a plumb bob to align it with the vertical. The instrument is then rotated until a single light beam passes through two points on the instrument. This fixes the orientation of the instrument in all three axes.\n\nThe angle between the vertical and the light beam gives the solar elevation. The solar elevation is a function of latitude, time of day, and season. Any one of these variables can be determined using astronomical rings, if the other two are known.\n\nThe altitude of the sun does not change much in a single day at the poles (where the sun rises and sets once a year), so rough measurements of solar altitude don't vary with time of day at high latitudes.\n\nWhen the solar time is exactly noon, or known from another clock, the instrument can be used to determine the time of year.\n\nThe meridional ring can function as the gnomon, when the rings are used as a sundial. A horizontal line aligned on a meridian with a gnomon facing the noon-sun is termed a meridian line and does not indicate the time, but instead the day of the year. Historically they were used to accurately determine the length of the solar year. A fixed meridional ring on its own can be used as a , which can be read only at noon.\n\nWhen the shadow of the rings are aligned so that they appear to be in the same, or nearly the same, place, the meridian identifies itself.\n\nThe \"meridian\" ring is placed vertically, then rotated (relative to the celestial object) until it is parallel to the local north-south line. The whole ring is thus parallel to the circle of longitude passing through the place where the user is standing.\n\nBecause the instrument is often supported by the meridional ring, it is often the outermost ring, as it is in the traveller's rings illustrated above. There, a sliding suspension shackle is attached to the top of the meridional ring, from which the whole device can be suspended. The meridional ring is marked in degrees of latitude (0-90, for each hemisphere). When properly used, the pointer on the support points to the latitude of the instrument's location. This tilts the equatorial ring so that it lies at the same angle to the vertical as the local equator.\n\nThe \"equatorial ring\" occupies a plane parallel to the celestial equator, at right angles to the meridian. It is aligned by\n\n\nOften equipped with a graduated scale, it can be used to measure right ascension. On the traveller's sundial shown above, it is the inner ring.\n\nThis ring is sometimes engraved with the months on one side and corresponding zodiac signs on the outside; very similar to an astrolabe. \nOthers have been found to be engraved with two twelve-hour time scales. Each twelve-hour scale is stretched over 180 degrees and numbered by hour with hashes every 20 minutes and smaller hashes every four minutes. The inside displays a calendrical scale with the names of the months indicated by their first letters, with a mark to show every 5 days and other marks to represent single days. On these, the outside of the ring is engraved with the corresponding symbols of the zodiac signs. The position of the symbol indicates the date of the entry of the sun into this particular sign. The vernal equinox is marked at March 15 and the autumnal equinox is marked at September 10.\n\nThe \"declination ring\" is moveable, and rotates on pivots set in the meridian ring. An imaginary line connecting these pivots is parallel to the Earth's axis. The declination \"ring\" of the traveller's sundial above is not a ring at all, but an oblong loop with a slider for setting the season.\n\nThis ring is often equipped with vanes and pinholes for use as the alidade of a dioptra (see image). It can be used to measure declination.\n\nThis ring is also often marked with the zodiac signs and twenty-five stars, similar to the astrolabe.\n"}
{"id": "49664704", "url": "https://en.wikipedia.org/wiki?curid=49664704", "title": "Bernard Dell", "text": "Bernard Dell\n\nBernard Dell (born 1949) is an Australian botanist. He is a research director at Murdoch University developing strategic research partnerships with China and nearby countries. His research spans the disciplines of agriculture and forestry. He is the author of books and academic papers on these subjects and is the recipient of a Murdoch University Excellence in Research Award (2012), the China Friendship Award (2014) and the Chiang Mai University Award (2015).\n"}
{"id": "28341716", "url": "https://en.wikipedia.org/wiki?curid=28341716", "title": "Bravo Glacier", "text": "Bravo Glacier\n\nBravo Glacier is a glacier on Greenwich Island in the South Shetland Islands, Antarctica extending 2.1 km in northwest-southeast direction and 2.3 km in southwest-northeast direction and draining the south slopes of Dryanovo Heights to flow southeastwards into Shopski Cove.\n\nThe feature was probably named after an officer in the Chilean ship \"Lientur\" that took part in the 1952-53 Chilean survey of Yankee Harbour.\n\nThe glacier is centred at (Bulgarian mapping in 2005 and 2009).\n\n\n\n"}
{"id": "47022356", "url": "https://en.wikipedia.org/wiki?curid=47022356", "title": "Brief Candle in the Dark: My Life in Science", "text": "Brief Candle in the Dark: My Life in Science\n\nBrief Candle in the Dark: My Life in Science is the second volume of the autobiographical memoir by British evolutionary biologist Richard Dawkins. It was published in English in September 2015.\n\nThe first volume, titled \"An Appetite for Wonder: The Making of a Scientist\", tells the first thirty-five years of his life, until the publication of \"The Selfish Gene\" in 1976. In \"Brief Candle in the Dark\", Richard Dawkins continues his autobiography until an academic party for and \"... on my seventieth birthday...\". He explains his inspirations, ideas, encounters and history. He mentions some of his 'heroes' such as Charles Darwin, Peter Medawar, Niko Tinbergen, Bill Hamilton, John Maynard Smith, Douglas Adams, Carl Sagan and David Attenborough.\n\nHe develops subjects such as his scientific work, travels and conferences, his Royal Institution Christmas Lecture (\"Growing Up in the Universe\", in 1991), his work as Professor for the Public Understanding of Science in Oxford, his documentaries (such as \"The Root of All Evil?\") as well as his personal life and his books.\n\n\"Kirkus Reviews\" found the book \"an impressive overview\". Dwight Garner wrote that the book \"presents a public life more than a private one.\" Steven Shapin wrote that the book is \"a loose and multiply digressive collection of reminiscences, anecdotes, addenda, quotes from admirers, and extended quotes from himself.\"\n"}
{"id": "44639789", "url": "https://en.wikipedia.org/wiki?curid=44639789", "title": "Burke and Wills Plant Camp", "text": "Burke and Wills Plant Camp\n\nBurke and Wills Plant Camp is a heritage-listed campsite near Betoota, Shire of Diamantina, Queensland, Australia. It is also known as Return Camp 46 and Burke and Wills Camp R46. It was added to the Queensland Heritage Register on 11 December 2008.\n\nThe Burke and Wills Plant Camp on Durrie Station is associated with explorers Robert O'Hara Burke and William John Wills. On the 3 April 1861, on their return trip from the Gulf of Carpentaria, the Burke and Wills expedition were approximately north of the present day Birdsville. That evening Wills made his last astronomical observations and then \"planted\" his instruments and associated equipment, presumably in a shallow hole, on an unnamed creek near return camp XLVI (also known Camp R46 or Plant Camp). This return trip was a desperate race to reach their depot camp number LXV - the site of the famous Dig Tree located close to the border with South Australia and near the present Nappa Merrie Homestead.\n\nBurke, the leader of the expedition, was born in 1821 in County Galway, Ireland, of Protestant gentry. Following an education at the Woolwich Academy, the young Burke served as a lieutenant in the Austrian cavalry and later the Irish Mounted Constabulary, before emigrating to Australia in 1853. After several postings with the Victorian Police, Burke was appointed to lead the Victorian Exploring Expedition, a position he had anxiously and diligently pursued. Wills, born in Devon England in 1834, trained in medicine. He migrated to Australia in 1853 and after a short stint working as a shepherd at Deniliquin, New South Wales, Wills assisted his father with his medical practice at Ballarat, Victoria. Wills later studied surveying and astronomy, and accompanied the expedition as astronomer, surveyor, and third-in-command, behind camel master George Landells.\n\nThe Victorian Exploring Expedition was undertaken in the spirit of previous epics such as Edward Eyre's journey between Western Australia and South Australia in 1840, and Ludwig Leichhardt's 1844-1845 trek from the Darling Downs in Queensland to Port Essington near Darwin. Organised by the Royal Society of Victoria and supported by the government, the expedition's objectives were hazy, beyond the desire to make the already-prosperous colony mightier by taking the lead in exploration efforts. The expedition would prove extremely expensive both financially and in lives, yet was ultimately successful in its bid to beat South Australia's John McDouall Stuart to the first north-south crossing of the continent.\n\nOn 20 August 1860, a large crowd in Melbourne provided a farewell for the explorers. The party comprised fifteen men, twenty-six Indian camels with their drivers, packhorses, wagons, food and supplies. In early October the party reached Menindee on the Darling River. Burke had already begun to shed baggage, determined to travel light and fast, leaving behind much of the equipment and some provisions at Balranald. Burke left more provisions at Menindee. He also quarrelled with Landells, who subsequently resigned. Burke promoted Wills to second-in-charge and engaged local man William Wright as third officer. Despite having received clear instructions that he was to establish his main base camp at Cooper's Creek, Burke pressed on quickly with an advance party of eight, leaving the remainder of the men and stores under the charge of Wright. Wright lingered at Menindee for three months, contrary to Burke's orders to proceed to Coopers' Creek as soon as possible, and Wright failed to arrive at Cooper's Creek with the reserve provisions and transport before Burke returned from the Gulf of Carpentaria. On 6 December 1860 Burke and his seven men established Camp LXV (65) at Cooper's Creek, having halted at several other spots along the Creek during the preceding three-and-a-half weeks while searching for the best location for a longer-term depot. Here Burke split his party once again, and on 16 December pressed on to the Gulf. This northern party was made up of Burke as party leader, Wills as second-in-command and astronomer, navigator and meteorologist, King as attendant to the party's camels, and Gray as camp organizer. Brahe, Thomas McDonough, William Patten and Dost Mahomet were instructed by Burke to wait for at least three months (the more cautious Wills preferred four months), before retracing their steps homewards via the Darling River. During their stay at Camp LXV, Brahe's party erected a timber stockade to protect themselves and their supplies from the unwanted interest of local Aborigines.\n\nFollowing a line to the northeast of Stuart's proposed track, Burke and his team arrived at the Little Bynoe River on 11 February 1861. By this time, they were 57 days out from Cooper's Creek and 13 days over that planned. As they were running low on stores and concerned about meeting the remainder of the exploration party at Cooper's Creek, Burke and Wills left Gray and King at Camp 119 and pushed further north through the mangroves in an attempt to reach the Gulf of Carpentaria coast. However, after three days of difficult travel, they returned south picking up Gray and King.\n\nThe return trip from the Gulf of Carpentaria became a desperate race to reach the depot camp on Cooper's Creek on low rations and with failing camels. On 3 April 1861, the Burke and Wills Expedition were approximately north of present-day Birdsville. Burke on that day gave an order \"for leaving behind everything but the grub and just what we carry on our backs.\" That evening Wills made his last astronomical observations and then, according to the notes in his astronomical journal, \"planted\" his equipment in a shallow hole on an unnamed creek near Camp RXLVI (Camp R46).\n\nBetween February 14 and April 21, the four men, with Gray sickening, continued to struggle back towards Cooper's Creek. Gray became increasingly ill suffering from \"pain in his head and limbs\". His condition continued to deteriorate until he died on the morning of April 17.\n\nOn the morning of 21 April 1861, William Brahe and his party of three others including the seriously-injured Patten, left for Menindee. Burke's party reached Camp LXV that same evening with his two surviving though exhausted camels and with perilously low stores. Brahe had left messages carved into the Dig Tree pointing to the cache of supplies. Subsequent events would prove to be a litany of missed opportunities and failure to communicate. A week after their departure from Camp LXV, Brahe met with Wright's party, heading north and finally en route to Cooper's Creek. Brahe and Wright then returned to the camp, but having noticed no evidence of Burke's return on 21 April, left no further messages or indications and retraced their steps southwards. Brahe arrived in Melbourne late in June 1861 to report on the missing men. On April 23 Burke's trio began the westward journey to Blanchewater Station, near Mount Hopeless in South Australia, preferring to take this direction over the longer, but known, journey to Menindee. Twice they were forced back to Cooper's Creek due to lack of water to the west. They then remained along the Creek throughout June. On 30 May 1861 Wills had returned to Camp LXV. He found no evidence of Brahe's return in early May, and placed his journals and a new note in the buried cache for fear of accidents, and returned to his companions waiting further along Cooper's Creek. After surviving on ground nardoo, which was slowly poisoning them due to their lack of expertise in its preparation in leaching out the toxins, Wills died alone on the banks of Cooper's Creek on 27 or 28 June. He had insisted that his companions head further down the creek to seek assistance from the Aborigines. A day or two later, Burke also succumbed, having sent King on to look for help. With the assistance of Aborigines, King survived along the watercourse until found on 15 September 1861 by Alfred William Howitt's search party.\n\nWhen it became known that the well-publicised Burke and Wills expedition had foundered, a number of search parties were quickly organised. Howitt left from Melbourne, John McKinlay from Adelaide, Frederick Walker from Rockhampton, and William Landsborough from Brisbane. Howitt's party, which included Brahe and King, arrived at Camp LXV on 13 September 1861. The Royal Commission was told that they found the depot as Mr Brahe had left it, the plant untouched, and nothing removed of the useless things lying about, but a piece of leather. The party located Wills' remains where his body had been covered by King, some miles downstream of Camp LXV. They buried Wills on 18 September 1861, and inscribed a tree. Field books, notebooks and various small articles were recovered. Three days later and approximately away, Howitt found Burke's remains near Innamincka Waterhole, north of Innamincka) in South Australia. Burke was buried wrapped in a Union Jack, under a box tree on the south-eastern bank of Cooper's Creek. Howitt blazed this tree at the head of Burke's grave. The Royal Geographical Society, organised to promote exploration, awarded Burke a posthumous RGS Founder's Medal in 1862. Wills was awarded nothing, as the Society's policy was to award only one medal to an exploration party. King received a gold watch, as did McKinlay, Landsborough and Walker for leading their various search parties. These search parties helped open up vast areas of inland Australia for settlement, as a result of the increased knowledge of the country they brought back with them.\n\nA Commission of Enquiry was organised to investigate the circumstances surrounding the deaths of Burke and Wills. The enquiry was held in Melbourne in November and December 1861. The commissioners interviewed thirteen people connected to the expedition, including the only survivor John King, and examined a range of evidence including Wills' journal. The findings of the enquiry were published in a report released at the end of January 1862. While critical of Brahe's decision to leave the depot camp before he was rejoined by Burke, Wills and King, the commission reported that the many of the calamities that befell the expedition might have been averted, including their deaths, if expedition leader Burke kept a regular journal and gave written orders to his men.\n\nIn 2010, a student of the University of Queensland, Nick Hadnutt, published his PhD thesis which undertook a detailed analysis of artefacts found at the site when compared with the known dates of the expedition using primary and secondary sources to confirm the authenticity of the site. The thesis concluded that the site was indeed Burke and Wills Camp 46R.\n\nThe Burke and Wills Plant Camp site is located on an unnamed creek on Durrie station, approximately two hours drive north of Birdsville. The archaeological record of the Plant Camp has two key components in two different locations. The majority of the European artefacts found to date were located at the terminus of the intermittent unnamed creek and a claypan. Artefacts have been found all along the unnamed creek in varying concentrations. All artefacts are located within the influence of the drainage pattern of the unnamed creek, however artefacts are mostly found on the high points within this local environment. The depth of the artefacts also varies with the majority being found close to the surface or within the top of soil.\n\nA number of artefacts have been collected by a number of researchers who have visited the site including canvas/leather sewing kit needles, thrust block, duck bill leather sewing pliers, percussion cap, nipples and bullets consistent with the calibre, vintage and make of weapons taken on the Burke and Wills Expedition. These artefacts have been compared with the \"List of Stores III\" from the expedition and found to be consistent with goods that were officially supplied.\n\nThe remainder of the site consists of two scarred trees located and east along the creek from the main artefact concentration. The origin of the scars on these trees remains unknown, but it is possible that these are the remains of blazes associated with the Burke and Wills expedition, providing a visible marker for the location of the Plant Camp for other explorers, or in the case of this expedition, to any potential rescuers.\n\nEvidence of indiscriminate looting of archaeological artefacts is visible throughout the Plant Camp site. More extensive excavations have also been undertaken at the base of the two scarred trees thought to be associated with the Burke and Wills expedition. Both tree locations show evidence of extensive shovel testing or turning of soil - presumably the digger was searching for buried instruments of other artefacts. Ongoing disturbance is a key threat facing Plant Camp as the process of retrieving surface artefacts without sufficient concern for their provenance and the indiscriminate digging up of artefacts removes them from their archaeological context and thus limits our ability to obtain reliable information about the Burke and Wills expedition.\n\nBurke and Wills \"Plant Camp\" was listed on the Queensland Heritage Register on 11 December 2008 having satisfied the following criteria.\n\nThe Burke and Wills Plant Camp is significant in demonstrating the evolution or pattern of Queensland's history as evidence of the nineteenth century interest in scientific exploration. The Burke and Wills expedition, and the subsequent search parties sent to find the lost explorers, contributed to the opening up of Queensland and large portions of inland Australia to pastoralism. The process of pastoral expansion commenced soon after the expedition concluded. Expansion into the largely unknown areas of western Queensland and the gulf forever changed these regions.\n\nThe Plant Camp is significant as a representation of the past in the present, providing tangible evidence of the first overland north-south crossing of the continent from settled areas in Victoria through Queensland to the Gulf of Carpentaria covering a distance of around . The place was the final camp established by the explorers and is representative of a key stage in the expedition when Burke and Wills decided to abandon most of their equipment.\n\nThe Plant Camp is significant for its potential to contain additional archaeological artefacts associated with the Burke and Wills expedition, particularly equipment associated with Wills' decision to \"plant\" or bury his astronomical equipment and Burke's order to leave behind everything except food and whatever they could carry on their backs. The Plant Camp has been identified through the presence of archaeological artefacts and two blazed trees situated along an unnamed ephemeral creek line on Durrie Station. Artefacts located to date include percussion caps, nipples, buckles, canvas/leather sewing kit needles, thrust block and duck bill leather sewing pliers a number of the brass fittings, including a clasp, escutcheon, latch and hinge all suggestive of a possible instrument case. These artefacts date to the same period (s-1860s) as the Burke and Wills expedition and are consistent with items listed in the detailed \"List of Stores III\" of goods officially supplied to the expedition.\n\nPlant Camp is significant for its research potential, especially future comparative analysis of archaeological artefacts with others documented at other Burke and Wills campsites. Comparative analysis could provide new and important information on the decisions made by the explorers throughout the expedition, including the rationing of supplies and the value placed on certain objects and items in their possession. For example, scientific analysis may reveal evidence as to why the explorers kept certain scientific instruments and abandoned more practical and useful items which could have increased their chances of success and ultimately survival. Comparative analysis of Plant Camp archaeological artefacts with other early exploration sites across Queensland may also benefit our understanding and knowledge of the conduct of those exploration events.\n\nThe Plant Camp has potential to substantiate or challenge documented accounts and myths surrounding the final days of the Burke and Wills expedition. Documentary evidence is limited to that provided in William Wills' journals and to that recorded by the Commission of Enquiry for the expedition held in 1862. Additional archaeological research at the Plant Camp may lead to a greater understanding of the reasons behind the ultimate failure of the expedition and enable a better understanding of the hardships endured by the explorers during the expedition.\n\nThe Plant Camp is significant for its special association with prominent Australian explorers Robert O'Hara Burke and William John Wills, who both died during the return journey in 1861.\n\n"}
{"id": "37521295", "url": "https://en.wikipedia.org/wiki?curid=37521295", "title": "California lunar sample displays", "text": "California lunar sample displays\n\nThe California lunar displays are two commemorative plaques consisting of small fragments of moon specimen brought back with the Apollo 11 and Apollo 17 lunar missions and given in the 1970s to the people of the state of California by United States President Richard Nixon as goodwill gifts.\n\nThe California Apollo 11 lunar sample display commemorative plaque display consists of four \"moon rock\" rice-size particle specimens that were collected by Apollo 11 astronauts Neil Armstrong and Buzz Aldrin in 1969 and a small California state flag that was taken to the moon and back on Apollo 11.\n\nThe 4 \"moon rocks\" weigh about 0.05 grams total and are entirely enveloped in a clear plastic button the size of a coin which is mounted to a wooden board approximately a foot square on a small podium pedestal display. The small podium plaque display also has mounted on it a small California state flag that had been taken to the moon and back, which lies directly below the \"moon rocks\". The California Apollo 11 lunar plaque display was given to the people of the state of California as a gift by President Richard Nixon. Similar lunar sample displays were distributed to all the other states of the United States and all the countries (at the time) of the world.\n\nThe California Apollo 17 lunar sample display commemorative style plaque, measuring 10 by 14 inches, consists of one \"moon rock\" particle specimen that was cut from lunar basalt 70017 and a California state flag. The basalt 70017 was collected by Apollo 17 astronaut Harrison Schmitt on the moon in 1972. Once lunar basalt 70017 was brought back to earth from the moon, the basalt moon rock was cut up into small fragments of approximately 1 gram. The specimen was encased in a plastic ball and mounted on the wooden plaque along with the California state flag which had been taken to the moon and back by the crew of Apollo 17. The plaque was then distributed in 1973 by President Richard Nixon to the state of California, as he did that year to the other 49 states (the same as for the Apollo 11 plaque gifts). This was done as a goodwill gesture to promote peace and harmony.\n\nThe California Apollo 17 lunar sample display was housed in the San Diego Aero-Space Museum. On February 22–23, 1978, the museum, which is part of the Panama-California Exposition Buildings (then called the Electric Building) in Balboa Park, San Diego, burned down. No one was injured, although a transient who sometimes lodged at the museum was missing. Two youths were seen in the area just before the fire broke out, who are believed to be connected to the start of the fire. Several one-of-a-kind aircraft were destroyed, including a reproduction of the Spirit of St. Louis, built in 1967. The loss of all the valuable artifacts was estimated at $4 million. Space-related objects that were lost included Gemini and Apollo spacecraft and space-related memorabilia and mementos. The California Apollo 17 lunar samples plaque display burned, but the 1 gram basalt \"moon rock\" stone in the Lucite ball that was mounted on the plaque survived, along with the small California flag that was mounted to the display. The damaged California Apollo 17 lunar samples plaque is now in storage at the San Diego Air & Space Museum.\n\nAccording to moon rocks researcher Robert Pearlman, the California Apollo 11 lunar samples display is also housed in the San Diego Air & Space Museum.\n\n"}
{"id": "27250934", "url": "https://en.wikipedia.org/wiki?curid=27250934", "title": "Chemical metallurgy", "text": "Chemical metallurgy\n\nChemical metallurgy is the science of obtaining metals from their ores, and of considering reactions of metals which are usually considered with an approach of disciplines belonging to chemistry. As such, it involves reactivity of metals which are also dealt in detail by electrochemistry and corrosion.\n\n"}
{"id": "5620190", "url": "https://en.wikipedia.org/wiki?curid=5620190", "title": "Class (locomotive)", "text": "Class (locomotive)\n\nClass (locomotive) refers to a group of locomotives built to a common design for a single railroad. Often members of a particular class had detail variations between individual examples, and these could lead to subclasses. Sometimes technical alterations (especially rebuilding, superheating, re-engining, etc.) move a locomotive from one class to another. Different railways had different systems, and sometimes one railway (or its successors) used different systems at different times and for different purposes, or applied those classifications inconsistently. Sometimes therefore it is not clear where one class begins and another ends. The result is a classic example of the Lumper splitter problem.\n\nAs locomotives became more numerous the need arose to deal with them in groups of similar engines rather than as named or numbered individuals. These groups were named \"classes\" and at first tended to reflect capability rather than design. For example, the Baltimore and Ohio Railroad grouped its roster into four classes before the Civil War, though they had by that point dozens of different designs.\n\nLater classes were based on design. A group of locomotives built off the same blueprints constituted a class, and if some of the locomotives in the class were sufficiently modified, a new class might be established for the modified examples. When electric locomotives were introduced, the same scheme was applied to them.\n\nSince steam and early electric locomotives were usually custom built, classes were assigned by the railroad, and each railroad had its own system. Mergers of lines and sales of locomotives brought about changes of class. Early diesels were often fitted into the locomotive class system, but since they were generally not custom built the use of manufacturer model designations overtook the class system and made it irrelevant, except for historical discussion.\n\nUsually the class system for a railroad was built on a simply hierarchy which assigned each class a code.\n\nThe first level was usually for the wheel arrangement and was usually coded by a letter of the alphabet. Different railroads used different codes, so that \"J\" on the New York Central Railroad meant a 4-6-4 (Hudson), while on the Norfolk and Western Railway it mean a 4-8-4 (Northern), and on the Baltimore and Ohio Railroad it denoted a 4-4-0 with a Wootten firebox.\n\nArticulated locomotives were handled through two different methods. On many railroads each wheel arrangement was assigned its own unique letter, due to the limited number of arrangements that had to be represented. On other railroads this was insufficient, and commonly articulated locomotives were represented with a two letter code, one letter for the arrangement of each half. Therefore, the Pennsylvania Railroad GG1 (UIC 2Co+Co2) is represented as if it were two ten-wheelers (4-6-0) coupled tail-to-tail.\n\nA sequence number was often added to distinguish different designs of the same wheel arrangement. As a rule the first design for a given arrangement had no sequence number, so that numbering stated at 1 with the second design for the wheel arrangement. For example, there were two main classes of 2-10-2 locomotives on the B&O, labelled \"S\" and \"S-1\".\n\nLetter suffixes were often used to indicate variants in a basic design. Sometimes these also referred to specific characteristics; for example, for many years the Pennsylvania Railroad used a \"s\" suffix to indicate superheating, while on the B&O a \"t\" suffix indicated an engine with an oversize tender.\n\nSub-classes are groups of locomotives that many have different mechanical characteristics between the sub-class of locomotives and the original design of those locomotives.\nFor example: when NSWGR C30 class suburban tank engines where displaced because of the electrification of Sydney suburban railways, many were converted to tender engines adding the prefix of “t” to C30. Some were even converted to superheighted engines, adding another prefix of “s” to either C30 or C30T to become C30S or C30TS.\n\nMost locomotives were given simple codes, but some classes were named, formally or not.\n\n\nThe old railway companies had various systems of classification. Taking the \"Big Four\" companies which operated from 1923 to 1947:\n\nThe class number was usually taken from the first member of each class, e.g. \"5700 Class\" or \"57XX Class\" for locomotives in the number series beginning 5700.\n\nEach class was given a letter or number but these were not very meaningful. For example, \"700\" Class locomotives were 0-6-0s, but so were \"Q\" Class engines. See:\n\nEach locomotive was given a power classification, e.g. \"3F\". However, many different classes would have the same power classification so this was not helpful for identifying classes.\n\nThe LNER's classification system was the most helpful to railway enthusiasts. Each wheel arrangement was given a letter (e.g. A for 4-6-2, B for 4-6-0, etc.) and this was followed by a number denoting the class (e.g. A1, B1, etc.).\n\n"}
{"id": "42124991", "url": "https://en.wikipedia.org/wiki?curid=42124991", "title": "Cynthia Friend", "text": "Cynthia Friend\n\nCynthia Friend, Ph.D. is a professor of Chemistry and Chemical Biology at Harvard University. She was the first female full professor of Chemistry at Harvard, attaining the position in 1989. Today she is the Theodore William Richards Professor of Chemistry and Professor of Materials Science, as well as a member of the editorial board of ACS Catalysis, Chemical Science, and the Journal of the American Chemical Society. She served as co-Editor-in-Chief of the Catalysis Science & Technology journal from 2010 until 2013. Her research focuses on controlling the chemical and physical properties of interfaces, by investigating important catalytic reactions and by making new materials with key chemical functionality. Her lab aims to develop solutions to important problems in energy usage and environmental chemistry.\n\nDr. Friend joined the Chemistry department of Harvard University in 1982 after doing her postdoctoral research at Stanford University and earning her Ph.D. from the University of California, Berkeley. Her previous leadership positions at Harvard include Associate Dean of the Faculty of Arts and Sciences (2002–05), Chair of the Department of Chemistry and Chemical Biology (2004–07), and Associate Director of the Materials Research Science and Engineering Center (2002-2011), and Radcliffe Trustee (1990–93). Dr. Friend also served as Associate Director of the Department of Energy’s SLAC National Accelerator Laboratory, at Stanford University (2011-2012) while on a leave from Harvard.\n\n\n"}
{"id": "293526", "url": "https://en.wikipedia.org/wiki?curid=293526", "title": "Dual loyalty", "text": "Dual loyalty\n\nIn politics, dual loyalty is loyalty to two separate interests that potentially conflict with each other.\n\nWhile nearly all examples of alleged \"dual loyalty\" are considered highly controversial, these examples point to the inherent difficulty in distinguishing between what constitutes a \"danger\" of dual loyalty – i.e., that there exists a pair of \"misaligned\" interests – versus what might be more simply a pair of \"partially aligned\" or even, according to the party being accused, a pair of \"fully aligned\" interests. For example, immigrants who still have feelings of loyalty to their country of origin will often insist that their two (or more) loyalties do not conflict. As Stanley A. Renshon at the Center for Immigration Studies notes,\nSome scholars refer to a growing trend of transnationalism and suggest that as societies become more heterogeneous and multi-cultural, the term \"dual loyalty\" increasingly becomes a meaningless bromide. According to the theory of transnationalism, migration (as well as other factors including improved global communication) produces new forms of identity that transcend traditional notions of physical and cultural space. Nina Glick Schiller, Linda Basch, and Cristina Blanc-Szanton define a process by which immigrants \"link together\" their country of origin and their country of settlement.\n\nThe transnationalist view is that \"dual loyalty\" is a potentially \"positive\" expression of multi-culturalism, and can contribute to the diversity and strength of civil society. While this view is popular in many academic circles, others are skeptical of this idea. As one paper describes it,\nBeyond its usage in particular instances, the term \"dual loyalty\" versus \"transnationalism\" continues to be the subject of much debate. As one academic writes:\nOther historical examples of actual or perceived \"dual loyalty\" include the following:\n"}
{"id": "19068598", "url": "https://en.wikipedia.org/wiki?curid=19068598", "title": "Electrohomeopathy", "text": "Electrohomeopathy\n\nElectrohomoeopathy (or Mattei cancer cure) is a derivative of homeopathy invented in the 19th century by Count Cesare Mattei. The name is derived from a combination of \"electro\" (referring to an electric bio-energy content supposedly extracted from plants and of therapeutic value, rather than electricity in its conventional sense) and \"homeopathy\" (referring to an alternative medicinal philosophy developed by Samuel Hahnemann in the 18th century). Electrohomeopathy has been defined as the combination of electrical devices and homeopathy.\n\nLucrative for its inventor and popular in the late nineteenth century, electrohomoeopathy has been described as \"utter idiocy\". Like all homeopathy, it is regarded by the medical and scientific communities as pseudoscience and its practice as quackery.\n\nElectrohomeopathy was devised by Cesare Mattei (1809–1896) in the latter part of the 19th century. Mattei, a nobleman living in a castle in the vicinity of Bologna, studied natural science, anatomy, physiology, pathology, chemistry and botany. He ultimately focused on the supposed therapeutic power of \"electricity\" in botanical extracts. Mattei made bold, unsupported claims for the efficacy of his treatments, including the claim that his treatments offered a nonsurgical alternative to cancer. His treatment regimens were met with scepticism by mainstream medicine:The electrohomeopathic system is an invention of Count Mattei who prates of \"red\", \"yellow\" and \"blue\", \"green\" and \"white\" electricity, a theory that, in spite of its utter idiocy, has attracted a considerable following and earned a large fortune for its chief promoter.\n\nNotwithstanding criticisms, including a challenge by the British medical establishment to the claimed success of his cancer treatments, electrohomeopathy (or Matteism, as it was sometimes known at the time) had adherents in Germany, France, the USA and the UK by the beginning of the 20th century; electrohomeopathy had been the subject of approximately 100 publications and there were three journals dedicated to it.\n\nRemedies are derived from what are said to be the active micro nutrients or mineral salts of certain plants. One contemporary account of the process of producing electrohomeopathic remedies was as follows:As to the nature of his remedies we learn ... that ... they are manufactured from certain herbs, and that the directions for the preparation of the necessary dilutions are given in the ordinary jargon of homeopathy. The globules and liquids, however, are \"instinct with a potent, vital, electrical force, which enables them to work wonders\". This process of \"fixing the electrical principle\" is carried on in the secret central chamber of a Neo-Moorish castle which Count Mattei has built for himself in the Bolognese Apennines ... The \"red electricity\" and \"white electricity\" supposed to be \"fixed\" in these \"vegetable compounds\" are in their very nomenclature and suggestion poor and miserable fictions.\n\nAccording to Mattei's own ideas however, every disease originates in the change of blood or of the lymphatic system or both, and remedies can therefore be mainly divided into two broad categories to be used in response to the dominant affected system. Mattei wrote that having obtained plant extracts, he was \"able to determine in the liquid vegetable electricity\". Allied to his theories and therapies were elements of Chinese medicine, of medical humours, of apparent Brownianism, as well as modified versions of Samuel Hahnemann's homeopathic principles. Electrohomeopathy has some associations with Spagyric medicine, a holistic medical philosophy claimed to be the practical application of alchemy in medical treatment, so that the principle of modern electrohomeopathy is that disease is typically multi-organic in cause or effect and therefore requires holistic treatment that is at once both complex and natural.\n\nA symposium took place in Bologna in 2008 to mark the 200th anniversary of the birth of Cesare Mattei, with attendees from India, Pakistan, Germany, UK, and the USA. Electrohomeopathy is practiced predominantly in India and Pakistan (RAJYA SABHA Parliamentary Bulletin- The Recognition of Electro Homeopathy System of Medicine Bill,\n2015 by E. M. Sudarsana Natchiappan, M. P), but there are also a number of electrohomeopathy organizations and institutions worldwide.\n\n\n"}
{"id": "28880277", "url": "https://en.wikipedia.org/wiki?curid=28880277", "title": "Escola d'Art i Superior de Disseny d'Olot", "text": "Escola d'Art i Superior de Disseny d'Olot\n\nEscola d'Art i Superior de Disseny d'Olot is the main Design and Arts & Crafts school of the region of Girona (Catalonia, Spain). Since its foundation, it has maintained an active participation in the social, educational, economic and cultural life. The school is in Olot, a city with a population of over 34,000 inhabitants.\n\nThe School was founded as Fine Arts School on 1 July 1783 by Bishop Tomas Lorenzana. In 1891 was renamed as Minor Fine Arts' School, and during the period 1934–1938 was designed as Landscape's School, depending on the Generalitat de Catalunya.\n\nDuring the 1986–87 academic year the official teaching of applied arts and crafts began, according to the official plan for 1963 schools; the school at this time depended on Olot's City Council.\n\nOn 20 June 1990 the school became part of the Catalonia official network of art schools with the name of Escola d'Arts Applicades i Oficis Artístics d'Olot; human resources and management switched to the Generalitat de Catalunya.\n\nSince the 2003–2004 academic year the school has become Escola d'Art i Superior de Disseny d'Olot, by completing the training with a Bachelor in Interior Design. Since the 2007–08 academic year the school also teaches a Bachelor of Graphic Design. Since 2010–11 it became one of seven ESDAP (Escola Superior de Disseny i Arts Plàstiques) and began to teach undergraduate Design (adapted to the European Higher Education Area).\n\nThe institution is at the \"Claustres del Carme\" building, the Carmelite Cloisters, which date back to the Renaissance.\n\n"}
{"id": "53413612", "url": "https://en.wikipedia.org/wiki?curid=53413612", "title": "FOSS Movement in India", "text": "FOSS Movement in India\n\nFOSS Movement in India refers to the campaign across the country during the 1990s and 2000s in particular, to promote Free and Open Source Software. It was marked by the existence of many Indian Linux User Groups (ILUGs) groups and Free Software User Groups (FSUGs) in different cities, town and other areas.\n\nThe prominent members of the campaign include the late Atul Chitnis, Prof Nagarjuna G. and others.\n\n"}
{"id": "20035423", "url": "https://en.wikipedia.org/wiki?curid=20035423", "title": "Flux transfer event", "text": "Flux transfer event\n\nA flux transfer event (FTE) occurs when a magnetic portal opens in the Earth's magnetosphere through which high-energy particles flow from the Sun. This connection, while previously thought to be permanent, has been found to be brief and very dynamic. The European Space Agency's four Cluster spacecraft and NASA's five THEMIS probes have flown through and surrounded these FTEs, measuring their dimensions and identifying the particles that are transferred between the magnetic fields.\n\nEarth's magnetosphere and the Sun's magnetic field are constantly pressed against one another on the dayside of Earth. Approximately every eight minutes, these fields briefly merge, forming a temporary \"portal\" between the Earth and the Sun through which high-energy particles such as solar wind can flow. The portal takes the shape of a magnetic cylinder about the width of Earth. Current observations place the portal at up to 4 times the size of Earth.\n\nSince Cluster and THEMIS have directly sampled FTEs, scientists can simulate FTEs on computers to predict how they might behave. Jimmy Raeder of the University of New Hampshire told his colleagues simulations show that the cylindrical portals tend to form above Earth's equator and then roll over Earth's winter pole. In December, FTEs roll over the north pole; in July they roll over the south pole.\n\nMagnetic fields similar to Earth's are common throughout known space and many undergo similar flux transfer events. During its second flyby of the planet on October 6, 2008, the NASA probe MESSENGER discovered that Mercury’s magnetic field shows a magnetic reconnection rate ten times higher than Earth's. Mercury's proximity to the Sun only accounts for about a third of the reconnection rate observed by MESSENGER and the cause of this discrepancy is not currently known.\n\nMost recently, it has been found that the same phenomenon, also known as a 'magnetic rope', can be observed at Saturn. The findings prove that at times Saturn \"behaves and interacts with the Sun in much the same way as Earth\". \n\n\n\n"}
{"id": "10553982", "url": "https://en.wikipedia.org/wiki?curid=10553982", "title": "GeoModeller", "text": "GeoModeller\n\nGeoModeller (old names include 3DWEG, Geomodeller3D) is a methodology and associated software tool for 3D geologic modelling developed by BRGM and Intrepid Geophysics over the last 20 years. The software is written using Open CASCADE in C++ for the engine (geometry, topology, viewers, data management, ...), Java for the GUI and data are stored in extensible mark-up language XML. GeoModeller has started to revolutionise the working practices, data standards and products of a geological survey as a whole. The software takes into account all structural geology data such as dip, dip directions, strike, hingelines and axialtrace to build the geometry of geological units.\n\nGeoModeller utilizes a Digital Terrain Model, surface geological linework, cross-sections, geophysical interpretation and drillhole borehole data to enable the geologist to construct cross sections, or 3D models. 3D Geostatistical interpolation (co-kriging) using all the data (location of interface, dip, direction, ...) produces a 3D implicit function representing a solid model. The model build may take in account if necessary a network of geologic faults. The model could be represented by triangulated objects each corresponding to one of the geological units present. Geologists can draw the model in their sections to obtain a fence diagram. The geologist can use their knowledge to add information in the 3D space until he obtain a 'right' model.\n\nIn geological and mining or oil exploration applications, seismic profiles as well as gravity and magnetic data are often available. Interpreted seismic cross-sections directly provide data that can be processed directly as geometric constraints for 3D modelling. On the other hand, gravity and magnetic data provide indirect constraints. Presently, a 3D geological model is considered as the initial state of a constrained inverse modelling of this data. That inversion is based on an iterative method, which is applied to a discrete version of the domain under study. This inversion formulation allows separate inversion of either gravity or magnetic data or simultaneous inversion of both datasets and tensor components of gravity and magnetic field . The final result is a probabilistic 3D geological model.\n\n\n"}
{"id": "4530453", "url": "https://en.wikipedia.org/wiki?curid=4530453", "title": "Georges Friedmann", "text": "Georges Friedmann\n\nGeorges Philippe Friedmann (; 13 May 1902 – 15 November 1977), was a French sociologist and philosopher, known for his influential work on the effects of industrial labor on individuals and his criticisms of the uncontrolled embrace of technological change in twentieth-century Europe and the United States.\n\nFriedmann was the last child of Adolphe Friedmann (1857-1922), a Berlin merchant, and Elizabeth Nathan (1871-1940). He was born in Paris, where his parents moved after their marriage in Berlin in 1882. They acquired French nationality in 1903.\n\nAfter a brief period studying industrial chemistry, Friedmann prepared for the philosophy \"agrégation\" at the prestigious Lycée Henri IV in Paris. He studied philosophy at the École Normale Supérieure from 1923-1926. He served as an assistant to the sociologist Célestin Bouglé at the Centre de documentation sociale, a social science research center at the ENS funded by the banker Albert Kahn and, later, the Rockefeller Foundation.\n\nUpon the death of his father in 1929, Friedmann inherited a fortune of 2.6 million francs, which enabled him to finance several of his young classmates' intellectual journals. Friedmann eventually donated a large part of the fortune to the Fondation Curie for cancer research. After his death, Degas paintings Friedmann had inherited from his father's collection were donated to the Louvre.\n\nFriedmann married his first wife, Hania Olszweska, a Polish Catholic, in 1937. The couple had one daughter, Liliane, born in 1941 in Toulouse. After Hania's death in 1957, Friedmann married Marcelle Rémond in 1960.\n\nAfter taking his family to Toulouse, Friedmann joined the French Resistance during World War II, when he was hunted by the Nazi Gestapo due to his Jewish heritage. He later wrote that he escaped the Gestapo in 1943, and was hidden in a school in Dordogne by a pair of young schoolteachers. Friedmann's journals from the war, published posthumously in 1987, recounted his experiences as a member of the resistance.\n\nHe received his \"Doctorat d'état\" in 1946, with his major thesis on mechanization in industrial production and minor thesis on Leibniz and Spinoza, both published as monographs.\n\nAt the ENS, Friedmann was close to the Philosophies group that opposed the influence of Henri Bergson and was influential in bringing Marx’s earlier philosophical texts to France, and included Georges Politzer, Norbert Gutermann, Paul Nizan, and Henri Lefebvre. The group's initial journal, \"Esprit,\" and its successor, \"Philosophies\", were funded by Friedmann's personal wealth.\n\nDuring the 1930s, Friedmann made several trips to the Soviet Union, where observed the Soviet industry and technology. His 1938 book, \"De la Sainte Russie à l’U.R.S.S.\" established him as an authority on Soviet society in France. But even his moderate criticisms of the U.S.S.R. and Stalin caused bitter conflict with members of the French Communist Party and began Friedmann’s move away from political activism.\n\nFriedmann’s doctoral thesis, published after the end of the war in 1946, examined the \"human problems\" of automation and mechanization European industrial production. A critical, historical overview of paradigms of industrial management, particularly scientific management, industrial psychology, and human relations, \"Problèmes humains du machinisme industriel\" examined social scientists’ efforts to \"humanize\" industrial labor that had been fragmented and de-skilled by industrialization and Taylorism. Friedmann argued that while these efforts were an improvement on the \"technicist ideology\" of management engineering, social science would not lead to significant changes in labor practices without class conflict and the transformation of the capitalist economic system.\n\nFriedmann’s book is considered a founding text of French \"sociologie du travail\", and his seminar influenced a number of younger sociologists, historians, and philosophers, including Edgar Morin and Alain Touraine. While holding various professorships and serving as the head of the French Centre national de la recherche scientifique (CNRS), Friedmann continued to travel extensively around the world, observing and publishing on labor practices and industrial models in the United States, Israel, and South America. His analysis of the nature of the Jewish people and Israeli society in \"The End of the Jewish People\"?, one of his few works to be translated into English, attracted media attention in the United States.\n\nFriedmann gradually shifted from emphasis on labor to a broader concern with \"technical civilization.\" His final book, \"La Puissance et la Sagesse\", a mixture of autobiography and reflection on contemporary society, modified his earlier Marxism and emphasized the importance of interiority and morality on humanizing postwar consumer society.\n\n\n\n"}
{"id": "31281413", "url": "https://en.wikipedia.org/wiki?curid=31281413", "title": "Gobar Times", "text": "Gobar Times\n\nGobar Times is a monthly environmental education magazine for the young adult, published by the Centre for Science and Environment. It is published along with \"Down to Earth\" as a supplement. The magazine was first published in May, 1998 and has widespread circulation across India and abroad. The icon of the magazine, Pandit Gobar Ganesh, the pondering panditji is an Indian elderly who takes the reader through current issues, subjects and ideas relating to the environment. He is the icon whose brains can be picked for anything on earth. The current editor of the magazine is Sumita Dasgupta.\n\n\"Gobar\" is the Hindi and Nepali word for \"cow dung\". It was chosen to capture the eco-philosophy and tradition of generating wealth from waste. Waste gobar serves as an insecticide and is used to plaster mud houses and is a waterproof coating for walls. It is also used to plaster floors to keep them cool. Gobar is the energy source for rural India. It is used as cooking fuel where people have no access to fuels like LPG. The greatest use of cow dung in India is in farming where it is used as natural manure for farmers' crops. In short, it's a mainstay of rural India, and an appropriate symbol for eco-friendly technology.\n\nAnil Agarwal the founder-director of Centre for Science and Environment, India’s leading environmental NGO, aptly called ‘Gobar’ the symbol that embodies the spirit of the Indian environmental movement. As he correctly reflected, the widespread and diverse use of gobar in Indian society stands up to every principle of good environmentalism.\n\nIn May 1992, the Society for Environmental Communications started India’s only science and environment fortnightly, \"Down To Earth\" (DTE). Over the years the magazine has informed and inspired people about environmental threats facing India and the world - a dimension underplayed in mainstream media. In May, 1998, \"Gobar Times\" was first published as a supplement to \"DTE\". \"Gobar Times\" reaches out to the young, encouraging them to take the lead and make a difference. It informs them and encourages them to save the environment by becoming an action-oriented, knowledgeable and aware community. The tone of the magazine is light-hearted, simple and thought-provoking. Every month the magazine comes up with an activity related center spread which can be pulled out and put up as a poster to encourage readers to become active participants. The magazine has recently entered into a partnership with the leading newspaper \"Hindustan Times\". Readers can now get a glimpse of what Pandit Gobar Ganesh has to say as they read HTNext.\n\nConceptualized by the New Delhi–based Centre for Science and Environment(CSE), The \"Gobar Times\" Green Schools Program allows self-assessment of the environmental practices of a school by its students, using the \"Green Schools Manual\". This inspection or survey of environmental practices at schools is a way for them to audit their immediate environment across the key areas of water, air, land, energy and waste.\n\n\nBy the end of it all the program gives the students an opportunity to give a report card to the school on its environmental performance and practices. They conduct the audit with the help of an interesting handbook called the 'Green Schools Program Manual' (2007 Edition). This manual gives them step by step guidelines on how to conduct an environmental audit in the school.\n\nThe purpose of the Green Schools Program is to encourage and support schools to build up an environmentally aware, active and skilled community of teachers, students and parents. The schools may use the manual as part of their environmental studies program or to conduct activities in eco-clubs. The data collected by students is compiled and rated such that it becomes a self-assessment tool of environmental practices followed by the school. The aim of the rating is, to understand what can be done to improve the schools performance on its environmental sustainability index and to implement changes over the coming years.\n\nThe \"Gobar Times\" Green Schools Awards is a platform to acknowledge and reward schools which have adopted the most innovative and effective practices to manage natural resources within their own premises. Schools send in their green reports to CSE. These reports undergo rigorous verification and selection and the best schools are awarded annually.\n\nThe top 10 schools of the country which have been working and have implemented tremendous improvement in their environmental practices are awarded the 'Change Makers' award. Schools participating for the first time are considered in the 'New Schools' category. They are judged basis involvement of students and teachers, and the quality of the reports submitted. Also schools which do not necessarily audit all areas but do exceptionally well in a single specific area of water, energy, land, air or waste apply for the coveted 'Best Managers' award.\n\nThe National Green Corps is a program of the Ministry of Environment and Forest, Government of India). State nodal offices and CSE partner in many states across India. As the Green Schools network is expanding, the awards are now also organized at the state level. The highest rank holders at the state level in different categories are nominated by state nodal agencies to compete for national awards with CSE in Delhi.\n\n"}
{"id": "54398673", "url": "https://en.wikipedia.org/wiki?curid=54398673", "title": "Group threat theory", "text": "Group threat theory\n\nGroup threat theory, also known as group position theory, is a sociological theory which proposes that the larger the size of an outgroup, the more the corresponding ingroup perceives it to threaten its own interests, resulting in the ingroup members having more negative attitudes toward the outgroup. It is based on the work of Herbert Blumer and Hubert M. Blalock, Jr. in the 1950s and 1960s, and has since been supported by multiple studies. Other studies, however, have not found support for the theory. Its predictions are contrary to those of the contact hypothesis, which posits that greater proximity between racial/ethnic groups increases harmony between them.\n"}
{"id": "36393507", "url": "https://en.wikipedia.org/wiki?curid=36393507", "title": "Industrial Bio-Test Laboratories", "text": "Industrial Bio-Test Laboratories\n\nIndustrial Bio-Test Laboratories (IBT Labs) was an American industrial product safety testing laboratory. IBT conducted significant quantities of research for pharmaceutical companies, chemical manufacturers and other industrial clients; at its height during the 1950s, 1960s, and 1970s, IBT operated the largest facility of its kind and performed more than one-third of all toxicology testing in the United States. IBT was later confirmed of engaging in extensive scientific misconduct, or more properly, fraud, which resulted in the indictment of its president and several top executives in 1981 and convictions in 1983. The revelations of misconduct by IBT Labs led to reforms in the regulation of pesticides in the United States and Canada.\n\nIBT was founded in 1953 by Dr. Joseph C. Calandra, an Italian American professor of pathology and biochemistry at Northwestern University. Calandra, the first of his family to pursue higher education, contributed to the concept of toxicologically innocuous doses during his tenure at Northwestern.\n\nDescribed by his colleagues as a \"man of high scientific standards who also knew how to make a dollar\", Calandra was an effective entrepreneur; his laboratory was contracted by the Department of Defense to evaluate irradiationally preserved food within its first year of operation.\n\nBy 1960, IBT reported that its professional staff included 12 biologists, five chemists, a mathematician, four physicians and a veterinarian, and that it employed 16 technicians. Calandra was president and director and John H. Kay was the associate director. At that time, it listed its research areas as \"industrial toxicology, food, drugs, cosmetics, pharmacology, radioisotopes, medical, dental, and veterinary products\".\n\nBy the early 1960s, IBT had attained a significant reputation for producing quality work at a reasonable price. \nAs of mid-decade, it had annual revenue estimated at nearly $2 million US.\n\nNalco Chemical acquired IBT in 1966 for an estimated $4.5 million. Utilizing the company's new financial prowess, Calandra oversaw considerable expansion of the company and constructed two new facilities. IBT began conducting tests on polychlorinated biphenyls (PCBs) in 1969. In 1970, Calandra began construction of a large $2 million laboratory and made two executive appointments: Dr. Moreno Keplinger as Manager of Toxicology and James Plank as Group Lead of Rat Toxicology.\n\nIn March 1971, IBT hired Dr. Paul L. Wright, one of Monsanto's toxicologists, to oversee its PCB testing. Dr. Donovan E. Gordon joined IBT as a pathologist in August 1971, and IBT finalized its safety analysis of PCBs in November 1971.\n\nIrregularities in IBT's data were discovered in April 1976 by Adrian Gross, an investigator at the Food and Drug Administration, whose aide retrieved one of the laboratory's naproxen studies that had been conducted for Syntex, a pharmaceutical company recently outed by a whistleblower. Gross nonetheless continued to read IBT's study, which would ineluctably strike him as being unrealistic and pique his interest. The FDA proceeded to probe IBT, and Gross personally inspected its facilities on April 11 and July 12, 1976. During Gross' physical inspection of the laboratory, he gained access to the study's raw safety data and found frequent references to an unknown acronym, \"TBD/TDA,\" which he said perplexed him until learning that it denoted a testing animal whose body had \"too badly decomposed.\"\n\nRevelations of suspected scientific misconduct would go on to be presented in March 1977 at Senate Subcommittee on Health and Scientific Research hearings held by U.S. Senator Ted Kennedy (D-MA), in which the integrity of safety data produced by IBT, as well as G. D. Searle & Company and Biometric Testing, Inc., was publicly called into question by FDA officials. Calandra vacated his role as president of the laboratory on March 25 and was succeeded by A. J. Frisque, who had previously worked as a research executive at Nalco. Plank parted ways with IBT in April, and Philip Smith, who would later testify against the laboratory, was \"fired and given 20 minutes to clean out his office\" in approximately June 1977. That same month, on June 2, IBT would also shred more of its internal documents, an action which Frisque characterized in a subsequent official statement as being \"entirely inadvertent.\"\n\nIBT was criminally implicated in 1977 for producing fraudulent studies on widely used household and industrial products, including Nemacur, Sencor, Naprosyn, and trichlorocarbanilide. The magnitude of IBT's scientific misconduct was considered to have been extensive: 618 of 867 (71%) of studies audited by the FDA were invalidated for having \"numerous discrepancies between the study conduct and data.\" Consequently, IBT would later be described as being \"at the center of one of the most far-reaching scandals in modern science, as thousands of its studies were revealed through EPA and FDA investigations to be fraudulent or grossly inadequate.\"\n\nAlthough the criminal case against IBT and its employees was proceeding, information about which chemicals were \"suspect\" because of IBT's misconduct was kept confidential by the Canadian and American government agencies responsible to regulating them.\n\nIn early 1980 the Regina Leader-Post obtained a Canadian government agency list of 106 chemicals about which there were concerns. The Regina Leader-Post also obtained a letter written Jan. 25, 1980 by R.O. Read, who was Chief of the Division of Additives and Pesticides\nin the Bureau of Chemical Safety Health Protection, a Branch of Canada's Health and Welfare Department, to, that said: \"All long-term rodent studies and multigenerational reproductive studies performed by IBT are considered invalid,\" and noted that the Canadian government had sent letters to a number of chemical companies pointing out that many had \"failed to submit the information required by the Environmental Protection Agency and the Canadian health protection branch.\" Documentation obtained from a variety of Canadian and American sources by the Regina Leader-Post included sloppy or inadequate record keeping that invalidated test results. In a number of cases, sick test animals were replaced with healthy ones, resulting in invalid test results.\n\nIn October 1983, three former officials of the company were convicted by a US federal jury of fabricating key product safety tests used to gain government approval for marketing two popular pesticides and two commonly used drugs. They were convicted after one of the longest criminal trials in US history, involving six months of testimony and nearly eleven days of jury deliberation. At the time it was regarded as \"the most massive scientific scandal in the history of this country and perhaps the world.\" The decision was subsequently appealed and upheld in \"United States of America v. Moreno L. Keplinger, Paul L. Wright, and James B. Plank\".\n\nFollowing the 1981 grand jury indictment of several key IBT officials, a federal trial began on April 4, 1983 to evaluate whether IBT had in fact committed scientific fraud.\n\nAccording to a May 13 article in \"The Wall Street Journal\", \"investigators charged that three big chemical companies—<nowiki>[</nowiki>Monsanto, Olin Corporation, and FMC Corporation<nowiki>]</nowiki>—knowingly submitted flawed data to the EPA in support of a widely used swimming pool chlorinator that was suspected of causing kidney and bladder problems.\" All three companies denied allegations of wrongdoing and reaffirmed the safety of their products.\n\nOn May 25, Cornelius Garrett testified that \"[no] research animals survived the entire testing period\" for either Sencor or Nemacure.\n\nGarrett also testified that Smith instructed him to falsify subject data, which he said was a \"common practice\".\n\nSmith testified that he was himself instructed by Wright to similarly falsify data on trichlorocarbanilide tests.\n\nCalandra requested a mistrial to have cardiac surgery and received approval on July 11. Keplinger, Plank and Wright resultingly argued that they should also receive a mistrial because his absence would undermine their case, but their request was denied on July 12.\n\nTestimony included that of an attorney retained by Nalco to defend IBT, Merrill Thompson, who refused to participate during earlier investigations of the laboratory. \"As I got into it and worked on these things more, and found out more, and worked with IBT people more, and looked at the evidence of other practices in the industry, I decided I couldn't defend IBT's practices,\" Thompson testified. Testimony drew to a close on September 22.\n\nClosing arguments concluded on October 4 and juror deliberation ensued. The case was decided by John Albert Nordberg on October 21, 1983.\n\nThe United States Environmental Protection Agency (EPA) announced on July 11, 1983 that 34 pesticides would be pulled from the market unless manufacturers provided additional safety data within 90 days, although an indefinite exception was made for those who committed to do so at a later time. Environmental organizations characterized the EPA's response as insufficient and negligent, expressing a desire for more decisive action. In September 1983, the agency revealed that it was still a \"long way from solving problems associated with the integrity of hundreds of studies produced by IBT and other large independent laboratories.\" Keith Schneider reported in the Winter 1983 edition of \"Amicus Journal\" that \"IBT performed over 2,000 key product safety tests approved by federal scientists to market 212 agricultural pesticides. After a seven-year review of its files, in 1983 the EPA reported that it had determined that only 16 percent of IBT's testing results were valid. Just a handful of the invalid studies have been replaced, the agency said, and most of the pesticides continue to be sprayed on fields and forests.\" Good Laboratory Practice regulation in the United States resulted from the episode.\n\nThe Law Reform Commission of Canada noted in a 1987 report that Canadian agriculture's increasing dependence on pesticides had led to inaction against many of those approved on the basis of IBT's research data. Agriculture Canada banned use of Reglone until further safety testing could be conducted and evaluated by Health and Welfare Canada, though application of the chemical continued illegally on a significant majority of lentils produced. Chlorbromuron, cyprazine, dinitramine, and phosphamidon were also banned \"because their manufacturers did not promise replacement studies,\" with Allidochlor and Captan being restricted.\n\nAccording to various reports, IBT's clientele included a wide variety of companies such as 3M, American Cyanamid, American Seed, Avitrol Corp, BFC, Black Leaf Chemical, Buckman, Casoron, Chemagro, Chevron, Ciba‑Geigy, Conrel, Diamond Shamrock, Dow Corning, DuPont, FMC, Glyco, Gulf, MGK, Mobay, Mobil, Monsanto, Montedison, Nissan, Noram, Olin, Penwalt, Procter & Gamble, PPG, Sandoz, Shell, Thompson-Hayward, Uniroyal, Upjohn, US Borax, Velsicol Chemical Company, Vertac Chemical Corporation, and Zoecon, as well as the United States Army, the Department of Defense, the Environmental Protection Agency, the Food and Drug Administration, the National Cancer Institute, and the World Health Organization.\n\nEric Francis and Marie-Monique Robin have cited IBT's research into the safety of polychlorinated biphenyl, which was not investigated, as also being potentially fraudulent.\nPhilip Smith later testified on October 28, 1991, during a court case against Monsanto, that he had observed PCB data falsification during his employment as an assistant toxicologist at IBT.\n\n\"The Ecologist\" claimed in a 2007 article that IBT had provided expert testimony against Douglas Gowan during a court case in which Monsanto allegedly sought to discredit and silence him over the wanton disposal of PCBs and other toxic waste at Brofiscin Quarry, Groes Faen.\n\nIn April, 1980 the Regina Leader-Post obtained and published a Canadian government list of more than 97 chemicals tested by IBT for which studies were considered \"suspect\". In June, 1980 an additional nine chemicals were added. \nAccording to Fagin and Lavelle in \"Toxic Deception\", Wright tampered with safety data for Machete and monosodium cyanurate, both Monsanto products. IBT performed safety tests on Dinoseb, which was later discovered to cause birth defects and then pulled from the U.S. market in 1986. IBT also originally evaluated the safety of atrazine, a herbicide now thought to be an endocrine disruptor, suspected carcinogen, and possible teratogen.\n\n\n"}
{"id": "2341527", "url": "https://en.wikipedia.org/wiki?curid=2341527", "title": "International Commission on Zoological Nomenclature", "text": "International Commission on Zoological Nomenclature\n\nThe International Commission on Zoological Nomenclature (ICZN) is an organization dedicated to \"achieving stability and sense in the scientific naming of animals\". Founded in 1895, it currently comprises 27 members from 19 countries, mainly practicing zoological taxonomists.\n\nThe ICZN is governed by the \"Constitution of the ICZN\", which is usually published together with the ICZN Code.\n\nMembers are elected by the Section of Zoological Nomenclature, established by the International Union of Biological Sciences (IUBS).\n\nThe regular term of service of a member of the Commission is 6 years. Members can be re-elected up to a total of three full six-year terms in a row. After 18 continuous years of elected service, a break of at least 3 years is prescribed before the member can stand again for election.\n\nSince 2014, the work of the Commission is supported by a small secretariat based at the National University of Singapore, in Singapore. Previously, the secretariat was based in London and funded by the International Trust for Zoological Nomenclature. The Commission assists the zoological community \"through generation and dissemination of information on the correct use of the scientific names of animals\".\n\nThe ICZN publishes the International Code of Zoological Nomenclature (usually referred to as \"the Code\" or \"the ICZN Code\"), a widely accepted convention containing the rules for the formal scientific naming of all organisms that are treated as animals. New editions of the Code are elaborated by the Editorial Committee appointed by the Commission. The 4th edition of the Code (1999) was edited by seven people.\n\nThe Commission also provides rulings on individual problems brought to its attention, as arbitration may be necessary in contentious cases, where strict adherence to the Code would interfere with stability of usage (e.g., see conserved name). These rulings are published in the Bulletin of Zoological Nomenclature. Starting in 2017, volumes 65 (2008) onwards of the \"Bulletin\" will be available online to subscribers in the BioOne journal database.\n\n\n"}
{"id": "28402107", "url": "https://en.wikipedia.org/wiki?curid=28402107", "title": "International System of Electrical and Magnetic Units", "text": "International System of Electrical and Magnetic Units\n\nThe International System of Electrical and Magnetic Units is an obsolete system of units used for measuring electrical and magnetic quantities. It was proposed as a system of practical international units by unanimous recommendation at the International Electrical Congress (Chicago, 1893), discussed at other Congresses, and finally adopted at the International Conference on Electric Units and Standards in London in 1908. It was rendered obsolete by the inclusion of electromagnetic units in the International System of Units (SI) at the 9th General Conference on Weights and Measures in 1948.\nThe link between electromagnetic units and the more familiar units of length, mass and time was first demonstrated by Gauss in 1832 with his measurement of the Earth's magnetic field, and the principle was extended to electrical measurements by Neumann in 1845. A complete system of metric electrical and magnetic units was proposed by Weber in 1851, based on the idea that electrical units could be defined solely in relation to absolute units of length, mass, and time. Weber's original proposal was based on a millimetre–milligram–second system of units.\n\nThe development of the electric telegraph (an invention of Gauss and Weber) demonstrated the need for accurate electrical measurements. At the behest of Thomson, the British Association for the Advancement of Science (B.A.) set up a committee in 1861, initially to examine standards for electrical resistance, which was expanded in 1862 to include other electrical standards. After two years of discussion, experiment and considerable differences of opinion, the committee decided to adapt Weber's approach to the CGS system of units, but used metre, gramme and second as their absolute units. However these units were both difficult to realize and (often) impractically small. To overcome these handicaps, the B.A. also proposed a set of \"practical\" or \"reproduceable\" units, which were not directly linked to the CGS system but which were, as near as experimental accuracy allowed, equal to multiples of the corresponding CGS units.\nThe B.A. had developed \"two\" sets of CGS units. The practical units were based on the electromagnetic set of units rather than the electrostatic set.\n\nThe B.A. system of practical units gained considerable international support, and was adopted – with one important modification – by the First International Conference of Electricians (Paris, 1881). The British Association had constructed an artefact representation of the ohm (a standard length of resistance wire which had a resistance of 10 CGS units of electric resistance, that is one ohm) whereas the international conference preferred a method of realization that could be repeated in different laboratories in different countries. The chosen method was based on the resistivity of mercury, by measuring the resistance of a column of mercury of specified dimensions (106 cm × 1 mm): however, the chosen length of column was almost 3 millimetres too short, leading to a difference of 0.28% between the new practical units and the CGS units which were supposedly their basis.\n\nThe anomaly was resolved at another international conference, in Chicago in 1893, by a correction in the definition of the ohm. The units agreed at this conference were termed \"international\" units, to distinguish them from their predecessors.\n\nThe 1893 system had three base units: the international ampere, the international ohm and the international volt.\n\nThe international units did not have the same formal legal status as the metre and the kilogram through the Metre Convention (1875), although several countries adopted the definition within their national laws (e.g., the United States, through Public Law 105 of July 12, 1894).\n\nThe 1893 system of units was overdefined, as can be seen from an examination of Ohm's law:\nBy Ohm's law, knowing any two of the physical quantities \"V\", \"I\" or \"R\" (potential difference, current or resistance) will define the third, and yet the 1893 system defines the units for all three quantities. With improvements in measurement techniques, it was soon recognised that\n\nThe solution came at an international conference in London in 1908. The essential point was to reduce the number of base units from three to two by redefining the international volt as a derived unit. There were several other modifications of less practical importance:\n\n\nWith advances in the theory of electromagnetism and in quantity calculus, it became apparent that a coherent absolute system of units could only include one electromagnetic base unit. The first such system was proposed by Giorgi in 1901: it used the ohm as the additional base unit in the MKS system, and so is often referred to as the MKSΩ system or the Giorgi system.\n\nAn additional problem with the CGS system of electrical units, pointed out as early as 1882 by Oliver Heaviside, was that they were not \"rationalized\", that is they failed to properly take account of permittivity and permeability as properties of a medium. Giorgi was also a great proponent of rationalization of the electrical units.\n\nThe choice of electrical unit for the base unit in a rationalized system depends only on practical considerations, particularly the ability to realize the unit accurately and reproducibly. The ampere rapidly gained support over the ohm, as many national standards laboratories were already realizing the ampere in absolute terms using ampere balances. The International Electrotechnical Commission (IEC) adopted the Giorgi system with the ampere replacing the ohm in 1935, and this choice of base units is often called the MKSA system.\n\nThe International Committee for Weights and Measures (CIPM) approved a new set of definitions for electrical units, based on the rationalized MKSA system, in 1946, and these were internationally adopted under the Metre Convention by the 9th General Conference on Weights and Measures in 1948. Under this system, which would become the International System of Units (SI), the ohm is a derived unit.\n\nThe SI definitions of the electrical units are formally equivalent to the 1908 international definitions, and so there should not have been any change in the size of the units. Nevertheless, the international ohm and the international volt were not usually realized in absolute terms but by reference to a standard resistance and a standard electromotive force respectively. The realizations recommended in 1908 are not exactly equivalent to the absolute definitions: recommended conversion factors are\nalthough slightly different factors may apply for individual standards in national measurement laboratories. As the international ampere was usually realized by means of an ampere balance rather than electrolytically, 1 A = 1 A. The conversion factor for the \"electrolytic\" ampere (A) can be calculated from modern values of the atomic weight of silver and the Faraday constant:\n\n\n"}
{"id": "57091071", "url": "https://en.wikipedia.org/wiki?curid=57091071", "title": "Intuitive statistics", "text": "Intuitive statistics\n\nIntuitive statistics, or folk statistics, refers to the cognitive phenomenon where organisms use data to make generalizations and predictions about the world. This can be a small amount of sample data or training instances, which in turn contribute to inductive inferences about either population-level properties, future data, or both. Inferences can involve revising hypotheses, or beliefs, in light of probabilistic data that inform and motivate future predictions. The informal tendency for cognitive animals to intuitively generate statistical inferences, when formalized with certain axioms of probability theory, constitutes statistics as an academic discipline.\n\nBecause this capacity can accommodate a broad range of informational domains, the subject matter is similarly broad and overlaps substantially with other cognitive phenomena. Indeed, some have argued that \"cognition as an intuitive statistician\" is an apt companion metaphor to the computer metaphor of cognition. Others appeal to a variety of statistical and probabilistic mechanisms behind theory construction and category structuring. Research in this domain commonly focuses on generalizations relating to number, relative frequency, risk, and any systematic signatures in inferential capacity that an organism (e.g., humans, or non-human primates) might have.\n\nIntuitive inferences can involve generating hypotheses from incoming sense data, such as categorization and concept structuring. Data are typically probabilistic and uncertainty is the rule, rather than the exception, in learning, perception, language, and thought. Recently, researchers have drawn from ideas in probability theory, philosophy of mind, computer science, and psychology to model cognition as a predictive and generative system of probabilistic representations, allowing information structures to support multiple inferences in a variety of contexts and combinations. This approach has been called a probabilistic language of thought because it constructs representations probabilistically, from pre-existing concepts to predict a possible and likely state of the world.\n\nStatisticians and probability theorists have long debated about the use of various tools, assumptions, and problems relating to inductive inference in particular. David Hume famously considered the problem of induction, questioning the logical foundations of how and why people can arrive at conclusions that extend beyond past experiences - both spatiotemporally and epistemologically. More recently, theorists have considered the problem by emphasizing techniques for arriving from data to hypothesis using formal content-independent procedures, or in contrast, by considering informal, content-dependent tools for inductive inference. Searches for formal procedures have led to different developments in statistical inference and probability theory with different assumptions, including Fisherian frequentist statistics, Bayesian inference, and Neyman-Pearson statistics.\n\nGerd Gigerenzer and David Murray argue that twentieth century psychology as a discipline adopted probabilistic inference as a unified set of ideas and ignored the controversies among probability theorists. They claim that a normative but incorrect view of how humans \"ought to think rationally\" follows from this acceptance. They also maintain, however, that the intuitive statistician metaphor of cognition is promising, and should consider different formal tools or heuristics as specialized for different problem domains, rather than a content- or context-free toolkit. Signal detection theorists and object detection models, for example, often use a Neyman-Pearson approach, whereas Fisherian frequentist statistics might aid cause-effect inferences.\n\nFrequentist inference focuses on the relative proportions or frequencies of occurrences to draw probabilistic conclusions. It is defined by its closely related concept, frequentist probability. This entails a view that \"probability\" is nonsensical in the absence of pre-existing data, because it is understood as a relative frequency that long-run samples would approach given large amounts of data. Leda Cosmides and John Tooby have argued that it is not possible to derive a probability without reference to some frequency of previous outcomes, and this likely has evolutionary origins: Single-event probabilities, they claim, are not observable because organisms evolved to intuitively understand and make statistical inferences from frequencies of prior events, rather than to \"see\" probability as an intrinsic property of an event.\n\nBayesian inference generally emphasizes the subjective probability of a hypothesis, which is computed as a posterior probability using Bayes' Theorem. It requires a \"starting point\" called a prior probability, which has been contentious for some frequentists who claim that frequency data are required to \"develop\" a prior probability, in contrast to taking a probability as an \"a priori\" assumption.\n\nBayesian models have been quite popular among psychologists, particularly learning theorists, because they appear to emulate the iterative, predictive process by which people learn and develop expectations from new observations, while giving appropriate weight to previous observations. Andy Clark, a cognitive scientist and philosopher, recently wrote a detailed argument in support of understanding the brain as a constructive Bayesian engine that is fundamentally action-oriented and predictive, rather than passive or reactive. More classic lines of evidence cited among supporters of Bayesian inference include conservatism, or the phenomenon where people modify previous beliefs \"toward\", but not all the way to, a conclusion implied by previous observations. This pattern of behavior is similar to the pattern of posterior probability distributions when a Bayesian model is conditioned on data, though critics argued that this evidence had been overstated and lacked mathematical rigor.\n\nAlison Gopnik more recently tackled the problem by advocating the use of Bayesian networks, or directed graph representations of conditional dependencies. In a Bayesian network, edge weights are conditional dependency strengths that are updated in light of new data, and nodes are observed variables. The graphical representation itself constitutes a model, or hypothesis, about the world and is subject to change, given new data.\n\nError management theory (EMT) is an application of Neyman-Pearson statistics to cognitive and evolutionary psychology. It maintains that the possible fitness costs and benefits of type I (false positive) and type II (false negative) errors are relevant to adaptively rational inferences, toward which an organism is expected to be biased due to natural selection. EMT was originally developed by Martie Haselton and David Buss, with initial research focusing on its possible role in sexual overperception bias in men and sexual underperception bias in women.\n\nThis is closely related to a concept called the \"smoke detector principle\" in evolutionary theory. It is defined by the tendency for immune, affective, and behavioral defenses to be hypersensitive and overreactive, rather than insensitive or weakly expressed. Randolph Nesse maintains that this is a consequence of a typical payoff structure in signal detection: In a system that is invariantly structured with a relatively low cost of false positives and high cost of false negatives, naturally selected defenses are expected to err on the side of hyperactivity in response to potential threat cues. This general idea has been applied to hypotheses about the apparent tendency for humans to apply agency to non-agents based on uncertain or agent-like cues. In particular, some claim that it is adaptive for potential prey to assume agency by default if it is even slightly suspected, because potential predator threats typically involve cheap false positives and lethal false negatives.\n\nHeuristics are efficient rules, or computational shortcuts, for producing a judgment or decision. The intuitive statistician metaphor of cognition led to a shift in focus for many psychologists, away from emotional or motivational principles and toward computational or inferential principles. Empirical studies investigating these principles have led some to conclude that human cognition, for example, has built-in and systematic errors in inference, or cognitive biases. As a result, cognitive psychologists have largely adopted the view that intuitive judgments, generalizations, and numerical or probabilistic calculations are systematically biased. The result is commonly an error in judgment, including (but not limited to) recurrent logical fallacies (e.g., the conjunction fallacy), innumeracy, and emotionally motivated shortcuts in reasoning. Social and cognitive psychologists have thus considered it \"paradoxical\" that humans can outperform powerful computers at complex tasks, yet be deeply flawed and error-prone in simple, everyday judgments.\n\nMuch of this research was carried out by Amos Tversky and Daniel Kahneman as an expansion of work by Herbert Simon on bounded rationality and satisficing. Tversky and Kahneman argue that people are regularly biased in their judgments under uncertainty, because in a speed-accuracy tradeoff they often rely on fast and intuitive heuristics with wide margins of error rather than slow calculations from statistical principles. These errors are called \"cognitive illusions\" because they involve systematic divergences between judgments and accepted, normative rules in statistical prediction.\n\nGigerenzer has been critical of this view, arguing that it builds from a flawed assumption that a unified \"normative theory\" of statistical prediction and probability exists. His contention is that cognitive psychologists neglect the diversity of ideas and assumptions in probability theory, and in some cases, their mutual incompatibility. Consequently, Gigerenzer argues that many cognitive illusions are not violations of probability theory \"per se\", but involve some kind of experimenter confusion between subjective probabilities with degrees of confidence and long-run outcome frequencies. Cosmides and Tooby similarly claim that different probabilistic assumptions can be more or less normative and rational in different types of situations, and that there is not general-purpose statistical toolkit for making inferences across all informational domains. In a review of several experiments they conclude, in support of Gigerenzer, that previous heuristics and biases experiments did not represent problems in an ecologically valid way, and that re-representing problems in terms of frequencies rather than single-event probabilities can make cognitive illusions largely vanish.\n\nTversky and Kahneman refuted this claim, arguing that making illusions disappear by manipulating them, whether they are cognitive or visual, does not undermine the initially discovered illusion. They also note that Gigerenzer ignores cognitive illusions resulting from frequency data, e.g., illusory correlations such as the hot hand in basketball. This, they note, is an example of an illusory positive autocorrelation that cannot be corrected by converted data to natural frequencies.\n\nFor adaptationists, EMT can be applied to inference under any informational domain where risk or uncertainty are present, such as predator avoidance, agency detection, or foraging. Researchers advocating this adaptive rationality view argue that evolutionary theory casts heuristics and biases in a new light, namely, as computationally efficient and ecologically rational shortcuts, or instances of adaptive error management.\n\nPeople often neglect base rates, or true actuarial facts about the probability or rate of a phenomenon, and instead give inappropriate amounts of weight to specific observations. In a Bayesian model of inference, this would amount to an underweighting of the prior probability, which has been cited as evidence against the appropriateness of a normative Bayesian framework for modeling cognition. Frequency representations can resolve base rate neglect, and some consider the phenomenon to be an experimental artifact, i.e., a result of probabilities or rates being represented as mathematical abstractions, which are difficult to intuitively think about. Gigerenzer speculates an ecological reason for this, noting that individuals learn frequencies through successive trials in nature. Tversky and Kahneman refute Gigerenzer's claim, pointing to experiments where subjects predicted a disease based on the presence vs. absence of pre-specified symptoms across 250 trials, with feedback after each trial. They note that base rate neglect was still found, despite the frequency formulation of subject trials in the experiment.\n\nAnother popular example of a supposed cognitive illusion is the conjunction fallacy, described in an experiment by Tversky and Kahneman known as the \"Linda problem.\" In this experiment, participants are presented with a short description of a person called Linda, who is 31 years old, single, intelligent, outspoken, and went to a university where she majored in philosophy, was concerned about discrimination and social justice, and participated in anti-nuclear protests. When participants were asked if it were more probable that Linda is (1) a bank teller, or (2) a bank teller and a feminist, 85% responded with option 2, even though it option 1 cannot be less probable than option 2. They concluded that this was a product of a representativeness heuristic, or a tendency to draw probabilistic inferences based on property similarities between instances of a concept, rather than a statistically structured inference.\n\nGigerenzer argued that the conjunction fallacy is based on a single-event probability, and would dissolve under a frequentist approach. He and other researchers demonstrate that conclusions from the conjunction fallacy result from ambiguous language, rather than robust statistical errors or cognitive illusions. In an alternative version of the Linda problem, participants are told that 100 people fit Linda's description and are asked how many are (1) bank tellers and (2) bank tellers and feminists. Experimentally, this version of the task appears to eliminate or mitigate the conjunction fallacy.\n\nThere has been some question about how concept structuring and generalization can be understood in terms of brain architecture and processes. This question is impacted by a neighboring debate among theorists about the nature of thought, specifically between connectionist and language of thought models. Concept generalization and classification have been modeled in a variety of connectionist models, or neural networks, specifically in domains like language learning and categorization. Some emphasize the limitations of pure connectionist models when they are expected to generalize future instances after training on previous instances. Gary Marcus, for example, asserts that training data would have to be completely exhaustive for generalizations to occur in existing connectionist models, and that as a result, they do not handle novel observations well. He further advocates an integrationist perspective between a language of thought, consisting of symbol representations and operations, and connectionist models than retain the distributed processing that is likely used by neural networks in the brain.\n\nIn practice, humans routinely make conceptual, linguistic, and probabilistic generalizations from small amounts of data. There is some debate about the utility of various tools of statistical inference in understanding the mind, but it is commonly accepted that the human mind is \"somehow\" an exceptionally apt prediction machine, and that action-oriented processes underlying this phenomenon, whatever they might entail, are at the core of cognition. Probabilistic inferences and generalization play central roles in concepts and categories and language learning, and infant studies are commonly used to understand the developmental trajectory of humans' intuitive statistical toolkit(s).\n\nDevelopmental psychologists such as Jean Piaget have traditionally argued that children do not develop the general cognitive capacities for probabilistic inference and hypothesis testing until concrete operational (age 7–11 years) and formal operational (age 12 years-adulthood) stages of development, respectively.\n\nThis is sometimes contrasted to a growing preponderance of empirical evidence suggesting that humans are capable generalizers in infancy. For example, looking-time experiments using expected outcomes of red and white ping pong ball proportions found that 8-month-old infants appear to make inferences about population characteristics from which the sample came, and vice versa when given population-level data. Other experiments have similarly supported a capacity for probabilistic inference with 6- and 11-month-old infants, but not in 4.5-month-olds.\n\nThe colored ball paradigm in these experiments did not distinguish the possibilities of infants' inferences based on quantity vs. proportion, which was addressed in follow-up research where 12-month-old infants seemed to understand proportions, basing probabilistic judgments - motivated by preferences for the more probable outcomes - on initial evidence of the proportions in their available options. Critics of the effectiveness of looking-time tasks allowed infants to search for preferred objects in single-sample probability tasks, supporting the notion that infants can infer probabilities of single events when given a small or large initial sample size. The researchers involved in these findings have argued that humans possess some statistically structured, inferential system during preverbal stages of development and prior to formal education.\n\nIt is less clear, however, how and why generalization is observed in infants: It might extend directly from detection and storage of similarities and differences in incoming data, or frequency representations. Conversely, it might be produced by something like general-purpose Bayesian inference, starting with a knowledge base that is iteratively conditioned on data to update subjective probabilities, or beliefs. This ties together questions about the statistical toolkit(s) that might be involved in learning, and how they apply to infant and childhood learning specifically.\n\nGopnik advocates the hypothesis that infant and childhood learning are examples of inductive inference, a general-purpose mechanism for generalization, acting upon specialized information structures (\"theories\") in the brain. On this view, infants and children are essentially proto-scientists because they regularly use a kind of scientific method, developing hypotheses, performing experiments via play, and updating models about the world based on their results. For Gopnik, this use of scientific thinking and categorization in development and everyday life can be formalized as models of Bayesian inference. An application of this view is the \"sampling hypothesis,\" or the view that individual variation in children's causal and probabilistic inferences is an artifact of random sampling from a diverse set of hypotheses, and flexible generalizations based on sampling behavior and context. These views, particularly those advocating general Bayesian updating from specialized theories, are considered successors to Piaget’s theory rather than wholesale refutations because they maintain its domain-generality, viewing children as randomly and unsystematically considering a range of models before selecting a probable conclusion.\n\nIn contrast to the general-purpose mechanistic view, some researchers advocate both domain-specific information structures and similarly specialized inferential mechanisms. For example, while humans do not usually excel at conditional probability calculations, the use of conditional probability calculations are central to parsing speech sounds into comprehensible syllables, a relatively straightforward and intuitive skill emerging as early as 8 months. Infants also appear to be good at tracking not only spatiotemporal states of objects, but at tracking properties of objects, and these cognitive systems appear to be developmentally distinct. This has been interpreted as domain specific toolkits of inference, each of which corresponds to separate types of information and has applications to concept learning.\n\nInfants use form similarities and differences to develop concepts relating to objects, and this relies on multiple trials with multiple patterns, exhibiting some kind of common property between trials. Infants appear to become proficient at this ability in particular by 12 months, but different concepts and properties employ different relevant principles of Gestalt psychology, many of which might emerge at different stages of development. Specifically, infant categorization at as early as 4.5 months involves iterative and interdependent processes by which exemplars (data) and their similarities and differences are crucial for drawing boundaries around categories. These abstract rules are statistical by nature, because they can entail common co-occurrences of certain perceived properties in past instances and facilitate inferences about their structure in future instances. This idea has been extrapolated by Douglas Hofstadter and Emmanuel Sander, who argue that because analogy is a process of inference relying on similarities and differences between concept properties, analogy and categorization are fundamentally the same process used for organizing concepts from incoming data.\n\nInfants and small children are not only capable generalizers of trait quantity and proportion, but of abstract rule-based systems such as language and music. These rules can be referred to as “algebraic rules” of abstract informational structure, and are representations of rule systems, or grammars. For language, creating generalizations with Bayesian inference and similarity detection has been advocated by researchers as a special case of concept formation. Infants appear to be proficient in inferring abstract and structural rules from streams of linguistic sounds produced in their developmental environments, and to generate wider predictions based on those rules.\n\nFor example, 9-month-old infants are capable of more quickly and dramatically updating their expectations when repeated syllable strings contain surprising features, such as rare phonemes. In general, preverbal infants appear to be capable of discriminating between grammars with which they have been trained with experience, and novel grammars. In 7-month-old infant looking-time tasks, infants seemed to pay more attention to unfamiliar grammatical structures than to familiar ones, and in a separate study using 3-syllable strings, infants appeared to similarly have generalized expectations based on abstract syllabic structure previously presented, suggesting that they used surface occurrences, or data, in order to infer deeper abstract structure. This was taken to support the “multiple hypotheses [or models]” view by the researchers involved.\n\nMultiple studies by Irene Pepperberg and her colleagues suggested that Grey parrots (\"Psittacus erithacus\") have some capacity for recognizing numbers or number-like concepts, appearing to understand ordinality and cardinality of numerals. Recent experiments also indicated that, given some language training and capacity for referencing recognized objects, they also have some ability to make inferences about probabilities and hidden object type ratios.\n\nExperiments found that when reasoning about preferred vs. non-preferred food proportions, capuchin monkeys were able to make inferences about proportions inferred by sequentially sampled data. Rhesus monkeys were similarly capable of using probabilistic and sequentially sampled data to make inferences about rewarding outcomes, and neural activity in the parietal cortex appeared to be involved in the decision-making process when they made inferences. In a series of 7 experiments using a variety of relative frequency differences between banana pellets and carrots, orangutans, bonobos, chimpanzees, and gorillas also appeared to guide their decisions based on the ratios favoring the banana pellets after this was established as their preferred food item.\n\nResearch on reasoning in medicine, or clinical reasoning, usually focuses on cognitive processes and/or decision-making outcomes among physicians and patients. Considerations include assessments of risk, patient preferences, and evidence-based medical knowledge. On a cognitive level, clinical inference relies heavily on interplay between abstraction, abduction, deduction, and induction. Intuitive \"theories,\" or knowledge in medicine, can be understood as prototypes in concept spaces, or alternatively, as semantic networks. Such models serve as a starting point for intuitive generalizations to be made from a small number of cues, resulting in the physician's tradeoff between the \"art and science\" of medical judgement. This tradeoff was captured in an artificially intelligent (AI) program called MYCIN, which outperformed medical students, but not experienced physicians with extensive practice in symptom recognition. Some researchers argue that despite this, physicians are prone to systematic biases, or cognitive illusions, in their judgment (e.g., satisficing to make premature diagnoses, confirmation bias when diagnoses are suspected \"a priori\").\n\nStatistical literacy and risk judgments have been described as problematic for physician-patient communication. For example, physicians frequently inflate the perceived risk of non-treatment, alter patients' risk perceptions by positively or negatively framing single statistics (e.g., 97% survival rate vs. 3% death rate), and/or fail to sufficiently communicate \"reference classes\" of probability statements to patients. The reference class is the object of a probability statement: If a psychiatrist says, for example, “this medication can lead to a 30-50% chance of a sexual problem,” it is ambiguous whether this means that 30-50% of patients will develop a sexual problem at some point, or if all patients will have problems in 30-50% of their sexual encounters.\n\nIn studies of base rate neglect, the problems given to participants often use base rates of disease prevalence. In these experiments, physicians and non-physicians are similarly susceptible to base rate neglect, or errors in calculating conditional probability. Here is an example from an empirical survey problem given to experienced physicians: Suppose that a hypothetical cancer had a prevalence of 0.3% in the population, and the true positive rate of a screening test was 50% with a false positive rate of 3%. Given a patient with a positive test result, what is the probability that the patient has cancer? When asked this question, physicians with an average of 14 years experience in medical practice ranged in their answers from 1-99%, with most answers being 47% or 50%. (The correct answer is 5%.) This observation of clinical base rate neglect and conditional probability error has been replicated in multiple empirical studies. Physicians' judgments in similar problems, however, improved substantially when the rates were re-formulated as natural frequencies.\n"}
{"id": "30597635", "url": "https://en.wikipedia.org/wiki?curid=30597635", "title": "John Coventry (constructor of philosophical instruments)", "text": "John Coventry (constructor of philosophical instruments)\n\nJohn Coventry (1735–1812) was an English constructor of scientific instruments. He made a reputation through the accuracy of his instruments.\n\nCoventry was born in Southwark. He worked with Benjamin Franklin and William Henly on electrical experiments, in the capacity of assistant.\n\nHe was the inventor of a new hygrometer, more accurate than any which had been previously in use. This instrument was generally employed by the chemists and other scientific men of his day. His telescopes were found to be more accurately adjusted than those usually employed, and the lenses with which they were fitted were more truly ground. His graduations were especially correct.\n\n"}
{"id": "16903748", "url": "https://en.wikipedia.org/wiki?curid=16903748", "title": "Kenneth Emory", "text": "Kenneth Emory\n\nKenneth Pike Emory (November 23, 1897 – January 2, 1992) was an American anthropologist who played a key role in shaping modern anthropology in Oceania. In the tradition of A. L. Kroeber and other pioneering anthropologists who trained him, Emory's works span all four major fields of anthropology: archaeology, physical anthropology, ethnography, and linguistics.\n\nWith fellow scientist Gerrit P. Wilder, Honolulu botanist, and Mrs. Wilder, historian; Dr. Armstrong Sperry and Dr. Stanley Ball, he was part of the Bishop Museum scientific research party who sailed to the South Pacific on Mr. and Mrs. Kellum schooner Kaimiloa. \nKenneth Pike Emory was born November 23, 1897 in Fitchburg, Massachusetts. He moved to Hawaii when he was two and grew up there, traveling first to Dartmouth and then continued his education afterward at Harvard then received his Phd. from Yale. While he was a high-school student, several archaeological digs in the Honolulu area piqued his interest in Polynesian artifacts and culture. Proselytizing in the first half of the nineteenth century by Roman Catholic, Protestant, and Mormon missionaries had been so successful that by the 1920s Polynesians had abandoned their ancestral gods in all but a few isolated places. When Emory realized this, he dedicated his life to finding and documenting as much pre-Christian Polynesian culture as he could. After attending Dartmouth College, he became associated with the Bishop Museum.\n\nIn 1924, with a group of Hawaiian scientists (including Gerrit P. Wilder, botanist; Mrs. Wilder, historian; Dr. Armstrong Sperry, writer and illustrator; Dr. Stanley Ball), he joined the four masted 170-foot 512 tons vessel Kaimiloa in Honolulu for a five-year expedition, reaching many of the then inaccessible spots of the Pacific. The vessel was a complete floating laboratory, possibly the most complete of any craft that has undertaken a similar trip. Bottles, crates, and boxes are stowed below along with gallons of preservatives for insects and plant specimens for the Bishop Museum.\nHe then spent the next 60 years roaming the Pacific, seeking out Polynesian settlement sites, excavating relics, and photographing petroglyphs. He sought out Polynesians who remembered the pre-Christian chants and rituals, and recorded them on film. By the 1950s, he had become the world's foremost expert on Polynesian culture.\n\nEmory theorized that Polynesians were descended from the Māori of New Zealand, and that Polynesian culture originated in Tonga and Samoa and migrated eastward through the Pacific to Tahiti, the Marquesas, and Hawaii. Emory believed, but did not attempt to prove, that Polynesians were capable of sailing great distances to all points of the compass. He argued that when the population of an island exceeded its capacity, a king or noble would outfit a large oceangoing vessel and set off to verify rumors of other habitable islands, sending back word of his discovery. Emory believed the Hawaiian Islands had been colonized by Society Islanders (Tahitians) in this way. With Kon-Tiki, Thor Heyerdahl proved that ancient mariners could have sailed westward across the Pacific; Emory replied that Peruvians might have gotten as far west as Easter Island, but its culture was overwhelmingly Polynesian. Others argued that even if Tahitians had found a new land mass such as Hawaii, they would have been unable to return to their point of origin. Emory disagreed, pointing out that contemporary copra schooners relied on wave direction, ocean currents, and seabirds to guide them to land, and Polynesian legends made frequent reference to celestial navigation. Besides: \"If they sailed south they were bound to hit islands whose inhabitants would know where the Society Islands lay.\" \n\nEmory's parents were from Massachusetts. By the time of his birth, Honolulu offered every amenity that could be found in any American city, and regular steamship service connected the city with San Francisco and other Pacific ports. Hawaiians accepted the inevitable presence of haole (Anglos) on their islands, partly because \"It is now much easier for (Hawaiians) to live . . . than in the old strenuous days when famine and war were never far off.\" In Hawaii intermarriage was relatively rare, but in Taiti intermarriage between French and Tahitians was quite common. Emory married a woman whose mother's family was Tahitian and whose father's was French; she considered Paris her second home.\n\nIn 1947, Emory spent time on Kapingamarangi, a remote Micronesian atoll, which, from his description, approached Rousseau's ideal society: \"This traditional lifestyle supported five hundred people on land . . . that did not total more than six-tenths of a square mile. There was no crime. . . . The people . . . were courteous, hospitable, hard-working, . . . and superbly adjusted to their environment.\n\nEmory died January 2, 1992 in Honolulu.\n\n\n\n\n"}
{"id": "35283807", "url": "https://en.wikipedia.org/wiki?curid=35283807", "title": "List of artistic occupations", "text": "List of artistic occupations\n\nThis is a list of artistic and creative occupations related to the creation of artistic displays.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "234906", "url": "https://en.wikipedia.org/wiki?curid=234906", "title": "List of integrals of trigonometric functions", "text": "List of integrals of trigonometric functions\n\nThese following is a list of integrals (antiderivative functions) of trigonometric functions. For antiderivatives involving both exponential and trigonometric functions, see List of integrals of exponential functions. For a complete list of antiderivative functions, see Lists of integrals. For the special antiderivatives involving trigonometric functions, see Trigonometric integral.\n\nGenerally, if the function formula_1 is any trigonometric function, and formula_2 is its derivative,\n\nIn all formulas the constant \"a\" is assumed to be nonzero, and \"C\" denotes the constant of integration.\n\n"}
{"id": "54200726", "url": "https://en.wikipedia.org/wiki?curid=54200726", "title": "List of investigational anxiolytics", "text": "List of investigational anxiolytics\n\nThis is a list of investigational anxiolytics, or anxiolytics that are currently under development for clinical use but are not yet approved. Chemical/generic names are listed first, with developmental code names, synonyms, and brand names in parentheses.\n\n\n\n\n\n\n\n"}
{"id": "21470", "url": "https://en.wikipedia.org/wiki?curid=21470", "title": "List of national anthems", "text": "List of national anthems\n\nMost nation-states have anthems, defined as \"a song, as of praise, devotion, or patriotism\"; most anthems are either marches or hymns in style. A hymn can become a national anthem by a provision in the state's constitution, by a law enacted by its legislature, or simply by tradition. A royal anthem is a patriotic song similar to a national anthem, but it specifically praises or prays for a monarch or royal dynasty. Such anthems are usually performed at public appearances by the monarch or during other events of royal importance. Some states use the royal anthem as the national anthem, such as the anthem of Jordan.\n\nThere are multiple claimants to the position of oldest national anthem. Among the national anthems, the first to be composed was the Dutch national anthem the \"Wilhelmus\", which was written between 1568 and 1572. The Japanese anthem, \"Kimigayo\", employs the oldest lyrics of any national anthem, taking its words from the \"Kokin Wakashū\", which was first published in 905, yet these words were not set to music until 1880. The first anthem to be officially adopted as such was the Spanish anthem \"Marcha Real\", in 1770; its origins remain unclear, being suggested to have sixteenth century Venetian origins, or even to have been composed by king Frederick the Great himself; it is also one of the few national anthems that has never had official lyrics. Anthems became increasingly popular among European states in the 18th century. For example, the British national anthem \"God Save the Queen\" was first performed under the title \"God Save the King\" in 1745. The French anthem \"La Marseillaise\" was written half a century later in 1792, and adopted in 1795.\n\nNational anthems are usually written in the most common language of the state, whether \"de facto\" or official. States with multiple national languages may offer several versions of their anthem. For instance, Switzerland's national anthem has different lyrics for each of the country's four official languages: French, German, Italian, and Romansh. One of New Zealand's two national anthems is commonly sung with the first verse in Māori (\"Aotearoa\") and the second in English (\"God Defend New Zealand\"). The tune is the same but the lyrics have different meanings. South Africa's national anthem is unique in that it is two different songs put together with five of the country's eleven official languages being used, in which each language comprises a stanza.\n\nOnly United Nations member states and observer states are included in this table. National anthems of sovereign states which are not UN members or observers are listed in a separate table below. An English translation of the title is provided in parentheses where appropriate.\n\nThis table includes anthems of \"de facto\" sovereign states which are not members or observers of the United Nations. Many of them have received little or no recognition from the international community; some are widely considered to be part of one of the countries listed above.\n\n\nGeneral\nSpecific\n"}
{"id": "4099213", "url": "https://en.wikipedia.org/wiki?curid=4099213", "title": "List of nursing schools in the United States", "text": "List of nursing schools in the United States\n\n\nLos Angeles Trade Technical College\n\n\n\n\n\n\n\nBachelor's degree programs or higher:\n\nAssociate degree programs:\n\n\n\n\n\n\n\n\n(( The William Paterson University)) School of Nursing, Wayne NJ\n\n\n\n\n\n!-- Please use proper formatting when adding to this list. (see Kentucky for example) -->\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "2227921", "url": "https://en.wikipedia.org/wiki?curid=2227921", "title": "MPTA-098", "text": "MPTA-098\n\nThe Main Propulsion Test Article (MPTA-098) was built by Rockwell International as a testbed for the definitive propulsion and fuel delivery systems for the U.S. Space Shuttle Program.\n\nNever intended for actual spaceflight, the MPTA consisted of the internal structure of a Space Shuttle orbiter aft-fuselage, a truss structure that simulated the basic structure and shape of an orbiter mid-fuselage and a complete Space Shuttle Main Engine (SSME) assembly, including all main propulsion system plumbing and the associated electrical systems.\n\nLater, the very different STA (Structural Test Article) was converted into a flightworthy orbiter, re-designated OV-099, and christened \"Challenger\". Rockwell and NASA thus retroactively re-designated the MPTA as MPTA-098, though it was never christened with a name.\n\nOn June 24, 1977, MPTA-098 was delivered by Rockwell International to the National Space Technology Laboratory (NSTL), in Hancock County, Mississippi, where it was mated with the Main Propulsion Test Article External Tank (MPTA-ET), mounted in a launch orientation and used for static engine tests.\n\nOn July 2, 1979, MPTA-098 suffered major structural damage due to a fractured fuel valve on Space Shuttle Main Engine number 2002. The fracture allowed hydrogen to leak into the enclosed aft compartment, raising the pressure to beyond the structural capability of the heat shield supports, severely damaging the structure.\n\nAfter extensive repairs were completed, testing resumed in September, but on November 4, a high-pressure oxidizer turbopump failed 9.7 seconds into a scheduled 510-second test. Finally, on December 17, 1979, a complete static firing was accomplished that included all three Space Shuttle Main Engines running at up to 100 percent of rated thrust for 554 seconds, exceeding the predicted maximum time that the SSMEs would burn during an operational shuttle launch.\n\nThe preliminary flight certification (PFC) program, which would clear the way for the SSMEs to be flown aboard manned vehicles, began in early 1980. A number of setbacks, including an overheating high-pressure turbopump that shut down an engine 4.6 seconds into a 544-second test on April 16, 1980, in July, the burn-through of a hydrogen preburner cancelled a 581-second test after 105 seconds and the structural failure of a flight-rated nozzle shut down a November 1980 test after 20 seconds, slowed progress dramatically. These failures led to a number of critical changes to the SSMEs and their associated systems.\n\nIn June 1980, due to the number of changes in the SSME design since the SSME installation on \"Columbia\", the three flight-rated SSMEs (numbers 2005, 2006 and 2007) which had performed successful individual 520-second mission demonstration test firings on the NSTL SSME test stand in early 1979, were removed from OV-102, shipped to NSTL, and successfully recertified. The engines were then shipped back to Kennedy Space Center and reinstalled on \"Columbia.\"\nOn January 17, 1981, with less than three months remaining before the scheduled STS-1 launch date, MPTA-098 successfully demonstrated a 625-second firing that included simulated abort profiles, completing the final PFC test and allowing the SSME design to be fully certified for flight, clearing the way for the launch of STS-1 on April 12, 1981.\n\nFrom 1981 until 1988, the MPTA-098 and MPTA-ET remained \"in-situ\" on the NSTL test stand, unused. In late 1988, the Essex Corporation used the thrust structure of the MPTA as the basis for an engineering development model for the proposed Shuttle-C launch vehicle. The model was used by NASA and Boeing at Kennedy Space Center and the Marshall Space Flight Center to conduct fit-checks and manufacturing engineering studies. The Shuttle-C program was cancelled by the United States Congress in 1990 and the model was disassembled.\n\nThe Main Propulsion Test Article, without truss work, is on display at the U.S. Space & Rocket Center, Visitor Information Center for NASA's Marshall Space Flight Center in Huntsville, Alabama, alongside the MPTA-ET which is mounted under the refurbished \"Pathfinder\" orbiter simulator.\n\n\n\n"}
{"id": "30179091", "url": "https://en.wikipedia.org/wiki?curid=30179091", "title": "Macroflora", "text": "Macroflora\n\nMacroflora is a term used for all the plants occurring in a particular area that are large enough to be seen with the naked eye. It is usually synonymous with the Flora and can be contrasted with the microflora, a term used for all the bacteria and other microorganisms in an ecosystem.\nMacroflora is also an informal term used by many palaeobotanists to refer to an assemblage of plant fossils as preserved in the rock. This is in contrast to the flora, which in this context refers to the assemblage of living plants that were growing in a particular area, whose fragmentary remains became entrapped within the sediment from which the rock was formed and thus became the macroflora.\n"}
{"id": "19603", "url": "https://en.wikipedia.org/wiki?curid=19603", "title": "Manhattan Project", "text": "Manhattan Project\n\nThe Manhattan Project was a research and development undertaking during World War II that produced the first nuclear weapons. It was led by the United States with the support of the United Kingdom and Canada. From 1942 to 1946, the project was under the direction of Major General Leslie Groves of the U.S. Army Corps of Engineers. Nuclear physicist Robert Oppenheimer was the director of the Los Alamos Laboratory that designed the actual bombs. The Army component of the project was designated the Manhattan District; \"Manhattan\" gradually superseded the official codename, Development of Substitute Materials, for the entire project. Along the way, the project absorbed its earlier British counterpart, Tube Alloys. The Manhattan Project began modestly in 1939, but grew to employ more than 130,000 people and cost nearly US$2 billion (about $ in dollars). Over 90% of the cost was for building factories and to produce fissile material, with less than 10% for development and production of the weapons. Research and production took place at more than 30 sites across the United States, the United Kingdom, and Canada.\n\nTwo types of atomic bombs were developed concurrently during the war: a relatively simple gun-type fission weapon and a more complex implosion-type nuclear weapon. The Thin Man gun-type design proved impractical to use with plutonium, and therefore a simpler gun-type called Little Boy was developed that used uranium-235, an isotope that makes up only 0.7 percent of natural uranium. Chemically identical to the most common isotope, uranium-238, and with almost the same mass, it proved difficult to separate the two. Three methods were employed for uranium enrichment: electromagnetic, gaseous and thermal. Most of this work was performed at the Clinton Engineer Works at Oak Ridge, Tennessee.\n\nIn parallel with the work on uranium was an effort to produce plutonium. After the feasibility of the world's first artificial nuclear reactor was demonstrated in Chicago at the Metallurgical Laboratory, it designed the X-10 Graphite Reactor at Oak Ridge and the production reactors in Hanford, Washington, in which uranium was irradiated and transmuted into plutonium. The plutonium was then chemically separated from the uranium, using the bismuth phosphate process. The Fat Man plutonium implosion-type weapon was developed in a concerted design and development effort by the Los Alamos Laboratory.\n\nThe project was also charged with gathering intelligence on the German nuclear weapon project. Through Operation Alsos, Manhattan Project personnel served in Europe, sometimes behind enemy lines, where they gathered nuclear materials and documents, and rounded up German scientists. Despite the Manhattan Project's tight security, Soviet atomic spies successfully penetrated the program.\n\nThe first nuclear device ever detonated was an implosion-type bomb at the Trinity test, conducted at New Mexico's Alamogordo Bombing and Gunnery Range on 16 July 1945. Little Boy and Fat Man bombs were used a month later in the atomic bombings of Hiroshima and Nagasaki, respectively. In the immediate postwar years, the Manhattan Project conducted weapons testing at Bikini Atoll as part of Operation Crossroads, developed new weapons, promoted the development of the network of national laboratories, supported medical research into radiology and laid the foundations for the nuclear navy. It maintained control over American atomic weapons research and production until the formation of the United States Atomic Energy Commission in January 1947.\n\nThe discovery of nuclear fission by German chemists Otto Hahn and Fritz Strassmann in 1938, and its theoretical explanation by Lise Meitner and Otto Frisch, made the development of an atomic bomb a theoretical possibility. There were fears that a German atomic bomb project would develop one first, especially among scientists who were refugees from Nazi Germany and other fascist countries. In August 1939, Hungarian-born physicists Leó Szilárd and Eugene Wigner drafted the Einstein–Szilárd letter, which warned of the potential development of \"extremely powerful bombs of a new type\". It urged the United States to take steps to acquire stockpiles of uranium ore and accelerate the research of Enrico Fermi and others into nuclear chain reactions. They had it signed by Albert Einstein and delivered to President Franklin D. Roosevelt. Roosevelt called on Lyman Briggs of the National Bureau of Standards to head the Advisory Committee on Uranium to investigate the issues raised by the letter. Briggs held a meeting on 21 October 1939, which was attended by Szilárd, Wigner and Edward Teller. The committee reported back to Roosevelt in November that uranium \"would provide a possible source of bombs with a destructiveness vastly greater than anything now known.\"\n\nThe Advisory Committee on Uranium became the National Defense Research Committee (NDRC) Committee on Uranium when that organization was formed on 27 June 1940. Briggs proposed spending $167,000 on research into uranium, particularly the uranium-235 isotope, and the recently discovered plutonium. On 28 June 1941, Roosevelt signed Executive Order 8807, which created the Office of Scientific Research and Development (OSRD), with Vannevar Bush as its director. The office was empowered to engage in large engineering projects in addition to research. The NDRC Committee on Uranium became the S-1 Section of the OSRD; the word \"uranium\" was dropped for security reasons.\n\nIn Britain, Frisch and Rudolf Peierls at the University of Birmingham had made a breakthrough investigating the critical mass of uranium-235 in June 1939. Their calculations indicated that it was within an order of magnitude of , which was small enough to be carried by a bomber of the day. Their March 1940 Frisch–Peierls memorandum initiated the British atomic bomb project and its Maud Committee, which unanimously recommended pursuing the development of an atomic bomb. In July 1940, Britain had offered to give the United States access to its scientific research, and the Tizard Mission's John Cockcroft briefed American scientists on British developments. He discovered that the American project was smaller than the British, and not as far advanced.\n\nAs part of the scientific exchange, the Maud Committee's findings were conveyed to the United States. One of its members, the Australian physicist Mark Oliphant, flew to the United States in late August 1941 and discovered that data provided by the Maud Committee had not reached key American physicists. Oliphant then set out to find out why the committee's findings were apparently being ignored. He met with the Uranium Committee and visited Berkeley, California, where he spoke persuasively to Ernest O. Lawrence. Lawrence was sufficiently impressed to commence his own research into uranium. He in turn spoke to James B. Conant, Arthur H. Compton and George B. Pegram. Oliphant's mission was therefore a success; key American physicists were now aware of the potential power of an atomic bomb.\n\nOn 9 October 1941, President Roosevelt approved the atomic program after he convened a meeting with Vannevar Bush and Vice President Henry A. Wallace. To control the program, he created a Top Policy Group consisting of himself—although he never attended a meeting—Wallace, Bush, Conant, Secretary of War Henry L. Stimson, and the Chief of Staff of the Army, General George C. Marshall. Roosevelt chose the Army to run the project rather than the Navy, because the Army had more experience with management of large-scale construction projects. He also agreed to coordinate the effort with that of the British, and on 11 October he sent a message to Prime Minister Winston Churchill, suggesting that they correspond on atomic matters.\n\nThe S-1 Committee held its meeting on 18 December 1941 \"pervaded by an atmosphere of enthusiasm and urgency\" in the wake of the attack on Pearl Harbor and the subsequent United States declaration of war upon Japan and then on Germany. Work was proceeding on three different techniques for isotope separation to separate uranium-235 from the more abundant uranium-238. Lawrence and his team at the University of California, Berkeley, investigated electromagnetic separation, while Eger Murphree and Jesse Wakefield Beams's team looked into gaseous diffusion at Columbia University, and Philip Abelson directed research into thermal diffusion at the Carnegie Institution of Washington and later the Naval Research Laboratory. Murphree was also the head of an unsuccessful separation project using gas centrifuges.\n\nMeanwhile, there were two lines of research into nuclear reactor technology, with Harold Urey continuing research into heavy water at Columbia, while Arthur Compton brought the scientists working under his supervision from Columbia, California and Princeton University to join his team at the University of Chicago, where he organized the Metallurgical Laboratory in early 1942 to study plutonium and reactors using graphite as a neutron moderator. Briggs, Compton, Lawrence, Murphree, and Urey met on 23 May 1942 to finalize the S-1 Committee recommendations, which called for all five technologies to be pursued. This was approved by Bush, Conant, and Brigadier General Wilhelm D. Styer, the chief of staff of Major General Brehon B. Somervell's Services of Supply, who had been designated the Army's representative on nuclear matters. Bush and Conant then took the recommendation to the Top Policy Group with a budget proposal for $54 million for construction by the United States Army Corps of Engineers, $31 million for research and development by OSRD and $5 million for contingencies in fiscal year 1943. The Top Policy Group in turn sent it to the President on 17 June 1942 and he approved it by writing \"OK FDR\" on the document.\n\nCompton asked theoretical physicist J. Robert Oppenheimer of the University of California, Berkeley, to take over research into fast neutron calculations—the key to calculations of critical mass and weapon detonation—from Gregory Breit, who had quit on 18 May 1942 because of concerns over lax operational security. John H. Manley, a physicist at the Metallurgical Laboratory, was assigned to assist Oppenheimer by contacting and coordinating experimental physics groups scattered across the country. Oppenheimer and Robert Serber of the University of Illinois examined the problems of neutron diffusion—how neutrons moved in a nuclear chain reaction—and hydrodynamics—how the explosion produced by a chain reaction might behave. To review this work and the general theory of fission reactions, Oppenheimer and Fermi convened meetings at the University of Chicago in June and at the University of California, Berkeley, in July 1942 with theoretical physicists Hans Bethe, John Van Vleck, Edward Teller, Emil Konopinski, Robert Serber, Stan Frankel, and Eldred C. Nelson, the latter three former students of Oppenheimer, and experimental physicists Emilio Segrè, Felix Bloch, Franco Rasetti, John Henry Manley, and Edwin McMillan. They tentatively confirmed that a fission bomb was theoretically possible.\n\nThere were still many unknown factors. The properties of pure uranium-235 were relatively unknown, as were those of plutonium, an element that had only been discovered in February 1941 by Glenn Seaborg and his team. The scientists at the Berkeley conference envisioned creating plutonium in nuclear reactors where uranium-238 atoms absorbed neutrons that had been emitted from fissioning uranium-235 atoms. At this point no reactor had been built, and only tiny quantities of plutonium were available from cyclotrons. Even by December 1943, only two milligrams had been produced. There were many ways of arranging the fissile material into a critical mass. The simplest was shooting a \"cylindrical plug\" into a sphere of \"active material\" with a \"tamper\"—dense material that would focus neutrons inward and keep the reacting mass together to increase its efficiency. They also explored designs involving spheroids, a primitive form of \"implosion\" suggested by Richard C. Tolman, and the possibility of autocatalytic methods, which would increase the efficiency of the bomb as it exploded.\n\nConsidering the idea of the fission bomb theoretically settled—at least until more experimental data was available—the Berkeley conference then turned in a different direction. Edward Teller pushed for discussion of a more powerful bomb: the \"super\", now usually referred to as a \"hydrogen bomb\", which would use the explosive force of a detonating fission bomb to ignite a nuclear fusion reaction in deuterium and tritium. Teller proposed scheme after scheme, but Bethe refused each one. The fusion idea was put aside to concentrate on producing fission bombs. Teller also raised the speculative possibility that an atomic bomb might \"ignite\" the atmosphere because of a hypothetical fusion reaction of nitrogen nuclei. Bethe calculated that it could not happen, and a report co-authored by Teller showed that \"no self-propagating chain of nuclear reactions is likely to be started.\" In Serber's account, Oppenheimer mentioned the possibility of this scenario to Arthur Compton, who \"didn't have enough sense to shut up about it. It somehow got into a document that went to Washington\" and was \"never laid to rest\".\n\nThe Chief of Engineers, Major General Eugene Reybold, selected Colonel James C. Marshall to head the Army's part of the project in June 1942. Marshall created a liaison office in Washington, D.C., but established his temporary headquarters on the 18th floor of 270 Broadway in New York, where he could draw on administrative support from the Corps of Engineers' North Atlantic Division. It was close to the Manhattan office of Stone & Webster, the principal project contractor, and to Columbia University. He had permission to draw on his former command, the Syracuse District, for staff, and he started with Lieutenant Colonel Kenneth Nichols, who became his deputy.\nBecause most of his task involved construction, Marshall worked in cooperation with the head of the Corps of Engineers Construction Division, Major General Thomas M. Robbins, and his deputy, Colonel Leslie Groves. Reybold, Somervell, and Styer decided to call the project \"Development of Substitute Materials\", but Groves felt that this would draw attention. Since engineer districts normally carried the name of the city where they were located, Marshall and Groves agreed to name the Army's component of the project the Manhattan District. This became official on 13 August, when Reybold issued the order creating the new district. Informally, it was known as the Manhattan Engineer District, or MED. Unlike other districts, it had no geographic boundaries, and Marshall had the authority of a division engineer. Development of Substitute Materials remained as the official codename of the project as a whole, but was supplanted over time by \"Manhattan\".\n\nMarshall later conceded that, \"I had never heard of atomic fission but I did know that you could not build much of a plant, much less four of them for $90 million.\" A single TNT plant that Nichols had recently built in Pennsylvania had cost $128 million. Nor were they impressed with estimates to the nearest order of magnitude, which Groves compared with telling a caterer to prepare for between ten and a thousand guests. A survey team from Stone & Webster had already scouted a site for the production plants. The War Production Board recommended sites around Knoxville, Tennessee, an isolated area where the Tennessee Valley Authority could supply ample electric power and the rivers could provide cooling water for the reactors. After examining several sites, the survey team selected one near Elza, Tennessee. Conant advised that it be acquired at once and Styer agreed but Marshall temporized, awaiting the results of Conant's reactor experiments before taking action. Of the prospective processes, only Lawrence's electromagnetic separation appeared sufficiently advanced for construction to commence.\n\nMarshall and Nichols began assembling the resources they would need. The first step was to obtain a high priority rating for the project. The top ratings were AA-1 through AA-4 in descending order, although there was also a special AAA rating reserved for emergencies. Ratings AA-1 and AA-2 were for essential weapons and equipment, so Colonel Lucius D. Clay, the deputy chief of staff at Services and Supply for requirements and resources, felt that the highest rating he could assign was AA-3, although he was willing to provide a AAA rating on request for critical materials if the need arose. Nichols and Marshall were disappointed; AA-3 was the same priority as Nichols' TNT plant in Pennsylvania.\n\nBush became dissatisfied with Colonel Marshall's failure to get the project moving forward expeditiously, specifically the failure to acquire the Tennessee site, the low priority allocated to the project by the Army and the location of his headquarters in New York City. Bush felt that more aggressive leadership was required, and spoke to Harvey Bundy and Generals Marshall, Somervell, and Styer about his concerns. He wanted the project placed under a senior policy committee, with a prestigious officer, preferably Styer, as overall director.\n\nSomervell and Styer selected Groves for the post, informing him on 17 September of this decision, and that General Marshall ordered that he be promoted to brigadier general, as it was felt that the title \"general\" would hold more sway with the academic scientists working on the Manhattan Project. Groves' orders placed him directly under Somervell rather than Reybold, with Colonel Marshall now answerable to Groves. Groves established his headquarters in Washington, D.C., on the fifth floor of the New War Department Building, where Colonel Marshall had his liaison office. He assumed command of the Manhattan Project on 23 September. Later that day, he attended a meeting called by Stimson, which established a Military Policy Committee, responsible to the Top Policy Group, consisting of Bush (with Conant as an alternate), Styer and Rear Admiral William R. Purnell. Tolman and Conant were later appointed as Groves' scientific advisers.\n\nOn 19 September, Groves went to Donald Nelson, the chairman of the War Production Board, and asked for broad authority to issue a AAA rating whenever it was required. Nelson initially balked but quickly caved in when Groves threatened to go to the President. Groves promised not to use the AAA rating unless it was necessary. It soon transpired that for the routine requirements of the project the AAA rating was too high but the AA-3 rating was too low. After a long campaign, Groves finally received AA-1 authority on 1 July 1944. According to Groves, \"In Washington you became aware of the importance of top priority. Most everything proposed in the Roosevelt administration would have top priority. That would last for about a week or two and then something else would get top priority\".\n\nOne of Groves' early problems was to find a director for Project Y, the group that would design and build the bomb. The obvious choice was one of the three laboratory heads, Urey, Lawrence, or Compton, but they could not be spared. Compton recommended Oppenheimer, who was already intimately familiar with the bomb design concepts. However, Oppenheimer had little administrative experience, and, unlike Urey, Lawrence, and Compton, had not won a Nobel Prize, which many scientists felt that the head of such an important laboratory should have. There were also concerns about Oppenheimer's security status, as many of his associates were Communists, including his brother, Frank Oppenheimer; his wife, Kitty; and his girlfriend, Jean Tatlock. A long conversation on a train in October 1942 convinced Groves and Nichols that Oppenheimer thoroughly understood the issues involved in setting up a laboratory in a remote area and should be appointed as its director. Groves personally waived the security requirements and issued Oppenheimer a clearance on 20 July 1943.\n\nThe British and Americans exchanged nuclear information but did not initially combine their efforts. Britain rebuffed attempts by Bush and Conant in 1941 to strengthen cooperation with its own project, codenamed Tube Alloys, because it was reluctant to share its technological lead and help the United States develop its own atomic bomb. An American scientist who brought a personal letter from Roosevelt to Churchill offering to pay for all research and development in an Anglo-American project was poorly treated, and Churchill did not reply to the letter. The United States as a result decided as early as April 1942 that if its offer was rejected, they should proceed alone. The British, who had made significant contributions early in the war, did not have the resources to carry through such a research program while fighting for their survival. As a result, Tube Alloys soon fell behind its American counterpart. and on 30 July 1942, Sir John Anderson, the minister responsible for Tube Alloys, advised Churchill that: \"We must face the fact that ... [our] pioneering work ... is a dwindling asset and that, unless we capitalise it quickly, we shall be outstripped. We now have a real contribution to make to a 'merger.' Soon we shall have little or none.\" That month Churchill and Roosevelt made an informal, unwritten agreement for atomic collaboration.\nThe opportunity for an equal partnership no longer existed, however, as shown in August 1942 when the British unsuccessfully demanded substantial control over the project while paying none of the costs. By 1943 the roles of the two countries had reversed from late 1941; in January Conant notified the British that they would no longer receive atomic information except in certain areas. While the British were shocked by the abrogation of the Churchill-Roosevelt agreement, head of the Canadian National Research Council C. J. Mackenzie was less surprised, writing \"I can't help feeling that the United Kingdom group [over] emphasizes the importance of their contribution as compared with the Americans.\" As Conant and Bush told the British, the order came \"from the top\".\n\nThe British bargaining position had worsened; the American scientists had decided that the United States no longer needed outside help, and they wanted to prevent Britain exploiting post-war commercial applications of atomic energy. The committee supported, and Roosevelt agreed to, restricting the flow of information to what Britain could use during the war—especially not bomb design—even if doing so slowed down the American project. By early 1943 the British stopped sending research and scientists to America, and as a result the Americans stopped all information sharing. The British considered ending the supply of Canadian uranium and heavy water to force the Americans to again share, but Canada needed American supplies to produce them. They investigated the possibility of an independent nuclear program, but determined that it could not be ready in time to affect the outcome of the war in Europe.\n\nBy March 1943 Conant decided that British help would benefit some areas of the project. James Chadwick and one or two other British scientists were important enough that the bomb design team at Los Alamos needed them, despite the risk of revealing weapon design secrets. In August 1943 Churchill and Roosevelt negotiated the Quebec Agreement, which resulted in a resumption of cooperation between scientists working on the same problem. Britain, however, agreed to restrictions on data on the building of large-scale production plants necessary for the bomb. The subsequent Hyde Park Agreement in September 1944 extended this cooperation to the postwar period. The Quebec Agreement established the Combined Policy Committee to coordinate the efforts of the United States, United Kingdom and Canada. Stimson, Bush and Conant served as the American members of the Combined Policy Committee, Field Marshal Sir John Dill and Colonel J. J. Llewellin were the British members, and C. D. Howe was the Canadian member. Llewellin returned to the United Kingdom at the end of 1943 and was replaced on the committee by Sir Ronald Ian Campbell, who in turn was replaced by the British Ambassador to the United States, Lord Halifax, in early 1945. Sir John Dill died in Washington, D.C., in November 1944 and was replaced both as Chief of the British Joint Staff Mission and as a member of the Combined Policy Committee by Field Marshal Sir Henry Maitland Wilson.\n\nWhen cooperation resumed after the Quebec agreement, the Americans' progress and expenditures amazed the British. The United States had already spent more than $1 billion ($ today), while in 1943, the United Kingdom had spent about £0.5 million. Chadwick thus pressed for British involvement in the Manhattan Project to the fullest extent and abandon any hopes of a British project during the war. With Churchill's backing, he attempted to ensure that every request from Groves for assistance was honored. The British Mission that arrived in the United States in December 1943 included Niels Bohr, Otto Frisch, Klaus Fuchs, Rudolf Peierls, and Ernest Titterton. More scientists arrived in early 1944. While those assigned to gaseous diffusion left by the fall of 1944, the 35 working with Lawrence at Berkeley were assigned to existing laboratory groups and stayed until the end of the war. The 19 sent to Los Alamos also joined existing groups, primarily related to implosion and bomb assembly, but not the plutonium-related ones. Part of the Quebec Agreement specified that nuclear weapons would not be used against another country without mutual consent. In June 1945, Wilson agreed that the use of nuclear weapons against Japan would be recorded as a decision of the Combined Policy Committee.\n\nThe Combined Policy Committee created the Combined Development Trust in June 1944, with Groves as its chairman, to procure uranium and thorium ores on international markets. The Belgian Congo and Canada held much of the world's uranium outside Eastern Europe, and the Belgian government in exile was in London. Britain agreed to give the United States most of the Belgian ore, as it could not use most of the supply without restricted American research. In 1944, the Trust purchased of uranium oxide ore from companies operating mines in the Belgian Congo. In order to avoid briefing US Secretary of the Treasury Henry Morgenthau Jr. on the project, a special account not subject to the usual auditing and controls was used to hold Trust monies. Between 1944 and the time he resigned from the Trust in 1947, Groves deposited a total of $37.5 million into the Trust's account.\n\nGroves appreciated the early British atomic research and the British scientists' contributions to the Manhattan Project, but stated that the United States would have succeeded without them. He also said that Churchill was \"the best friend the atomic bomb project had [as] he kept Roosevelt's interest up ... He just stirred him up all the time by telling him how important he thought the project was.\"\n\nThe British wartime participation was crucial to the success of the United Kingdom's independent nuclear weapons program after the war when the McMahon Act of 1946 temporarily ended American nuclear cooperation.\n\nThe day after he took over the project, Groves took a train to Tennessee with Colonel Marshall to inspect the proposed site there, and Groves was impressed. On 29 September 1942, United States Under Secretary of War Robert P. Patterson authorized the Corps of Engineers to acquire of land by eminent domain at a cost of $3.5 million. An additional was subsequently acquired. About 1,000 families were affected by the condemnation order, which came into effect on 7 October. Protests, legal appeals, and a 1943 Congressional inquiry were to no avail. By mid-November U.S. Marshals were tacking notices to vacate on farmhouse doors, and construction contractors were moving in. Some families were given two weeks' notice to vacate farms that had been their homes for generations; others had settled there after being evicted to make way for the Great Smoky Mountains National Park in the 1920s or the Norris Dam in the 1930s. The ultimate cost of land acquisition in the area, which was not completed until March 1945, was only about $2.6 million, which worked out to around $47 an acre. When presented with Public Proclamation Number Two, which declared Oak Ridge a total exclusion area that no one could enter without military permission, the Governor of Tennessee, Prentice Cooper, angrily tore it up.\n\nInitially known as the Kingston Demolition Range, the site was officially renamed the Clinton Engineer Works (CEW) in early 1943. While Stone & Webster concentrated on the production facilities, the architectural and engineering firm Skidmore, Owings & Merrill designed and built a residential community for 13,000. The community was located on the slopes of Black Oak Ridge, from which the new town of Oak Ridge got its name. The Army presence at Oak Ridge increased in August 1943 when Nichols replaced Marshall as head of the Manhattan Engineer District. One of his first tasks was to move the district headquarters to Oak Ridge although the name of the district did not change. In September 1943 the administration of community facilities was outsourced to Turner Construction Company through a subsidiary, the Roane-Anderson Company (for Roane and Anderson Counties, in which Oak Ridge was located). Chemical engineers, including William J. Wilcox Jr. and Warren Fuchs, were part of \"frantic efforts\" to make 10% to 12% enriched uranium 235, known as the code name \"tuballoy tetroxide\", with tight security and fast approvals for supplies and materials. The population of Oak Ridge soon expanded well beyond the initial plans, and peaked at 75,000 in May 1945, by which time 82,000 people were employed at the Clinton Engineer Works, and 10,000 by Roane-Anderson.\n\nFine-arts photographer, Josephine Herrick, and her colleague, Mary Steers, helped document the work at Oak Ridge.\n\nThe idea of locating Project Y at Oak Ridge was considered, but in the end it was decided that it should be in a remote location. On Oppenheimer's recommendation, the search for a suitable site was narrowed to the vicinity of Albuquerque, New Mexico, where Oppenheimer owned a ranch. In October 1942, Major John H. Dudley of the Manhattan Project was sent to survey the area, and he recommended a site near Jemez Springs, New Mexico. On 16 November, Oppenheimer, Groves, Dudley and others toured the site. Oppenheimer feared that the high cliffs surrounding the site would make his people feel claustrophobic, while the engineers were concerned with the possibility of flooding. The party then moved on to the vicinity of the Los Alamos Ranch School. Oppenheimer was impressed and expressed a strong preference for the site, citing its natural beauty and views of the Sangre de Cristo Mountains, which, it was hoped, would inspire those who would work on the project. The engineers were concerned about the poor access road, and whether the water supply would be adequate, but otherwise felt that it was ideal.\nPatterson approved the acquisition of the site on 25 November 1942, authorizing $440,000 for the purchase of the site of , all but of which were already owned by the Federal Government. Secretary of Agriculture Claude R. Wickard granted use of some of United States Forest Service land to the War Department \"for so long as the military necessity continues\". The need for land, for a new road, and later for a right of way for a power line, eventually brought wartime land purchases to , but only $414,971 was spent. Construction was contracted to the M. M. Sundt Company of Tucson, Arizona, with Willard C. Kruger and Associates of Santa Fe, New Mexico, as architect and engineer. Work commenced in December 1942. Groves initially allocated $300,000 for construction, three times Oppenheimer's estimate, with a planned completion date of 15 March 1943. It soon became clear that the scope of Project Y was greater than expected, and by the time Sundt finished on 30 November 1943, over $7 million had been spent.\nBecause it was secret, Los Alamos was referred to as \"Site Y\" or \"the Hill\". Birth certificates of babies born in Los Alamos during the war listed their place of birth as PO Box 1663 in Santa Fe. Initially Los Alamos was to have been a military laboratory with Oppenheimer and other researchers commissioned into the Army. Oppenheimer went so far as to order himself a lieutenant colonel's uniform, but two key physicists, Robert Bacher and Isidor Rabi, balked at the idea. Conant, Groves and Oppenheimer then devised a compromise whereby the laboratory was operated by the University of California under contract to the War Department.\n\nAn Army-OSRD council on 25 June 1942 decided to build a pilot plant for plutonium production in Red Gate Woods southwest of Chicago. In July, Nichols arranged for a lease of from the Cook County Forest Preserve District, and Captain James F. Grafton was appointed Chicago area engineer. It soon became apparent that the scale of operations was too great for the area, and it was decided to build the plant at Oak Ridge, and keep a research and testing facility in Chicago.\n\nDelays in establishing the plant in Red Gate Woods led Compton to authorize the Metallurgical Laboratory to construct the first nuclear reactor beneath the bleachers of Stagg Field at the University of Chicago. The reactor required an enormous amount of graphite blocks and uranium pellets. At the time, there was a limited source of pure uranium. Frank Spedding of Iowa State University were able to produce only two short tons of pure uranium. Additional three short tons of uranium metal was supplied by Westinghouse Lamp Plant which was produced in a rush with makeshift process. A large square balloon was constructed by Goodyear Tire to encase the reactor. On 2 December 1942, a team led by Enrico Fermi initiated the first artificial self-sustaining nuclear chain reaction in an experimental reactor known as Chicago Pile-1. The point at which a reaction becomes self-sustaining became known as \"going critical\". Compton reported the success to Conant in Washington, D.C., by a coded phone call, saying, \"The Italian navigator [Fermi] has just landed in the new world.\"\n\nIn January 1943, Grafton's successor, Major Arthur V. Peterson, ordered Chicago Pile-1 dismantled and reassembled at Red Gate Woods, as he regarded the operation of a reactor as too hazardous for a densely populated area. At the Argonne site, Chicago Pile-3, the first heavy water reactor, went critical on 15 May 1944. After the war, the operations that remained at Red Gate moved to the new site of the Argonne National Laboratory about away.\n\nBy December 1942 there were concerns that even Oak Ridge was too close to a major population center (Knoxville) in the unlikely event of a major nuclear accident. Groves recruited DuPont in November 1942 to be the prime contractor for the construction of the plutonium production complex. DuPont was offered a standard cost plus fixed-fee contract, but the President of the company, Walter S. Carpenter, Jr., wanted no profit of any kind, and asked for the proposed contract to be amended to explicitly exclude the company from acquiring any patent rights. This was accepted, but for legal reasons a nominal fee of one dollar was agreed upon. After the war, DuPont asked to be released from the contract early, and had to return 33 cents.\nDuPont recommended that the site be located far from the existing uranium production facility at Oak Ridge. In December 1942, Groves dispatched Colonel Franklin Matthias and DuPont engineers to scout potential sites. Matthias reported that Hanford Site near Richland, Washington, was \"ideal in virtually all respects\". It was isolated and near the Columbia River, which could supply sufficient water to cool the reactors that would produce the plutonium. Groves visited the site in January and established the Hanford Engineer Works (HEW), codenamed \"Site W\".\n\nUnder Secretary Patterson gave his approval on 9 February, allocating $5 million for the acquisition of of land in the area. The federal government relocated some 1,500 residents of White Bluffs and Hanford, and nearby settlements, as well as the Wanapum and other tribes using the area. A dispute arose with farmers over compensation for crops, which had already been planted before the land was acquired. Where schedules allowed, the Army allowed the crops to be harvested, but this was not always possible. The land acquisition process dragged on and was not completed before the end of the Manhattan Project in December 1946.\n\nThe dispute did not delay work. Although progress on the reactor design at Metallurgical Laboratory and DuPont was not sufficiently advanced to accurately predict the scope of the project, a start was made in April 1943 on facilities for an estimated 25,000 workers, half of whom were expected to live on-site. By July 1944, some 1,200 buildings had been erected and nearly 51,000 people were living in the construction camp. As area engineer, Matthias exercised overall control of the site. At its peak, the construction camp was the third most populous town in Washington state. Hanford operated a fleet of over 900 buses, more than the city of Chicago. Like Los Alamos and Oak Ridge, Richland was a gated community with restricted access, but it looked more like a typical wartime American boomtown: the military profile was lower, and physical security elements like high fences, towers, and guard dogs were less evident.\n\nCominco had produced electrolytic hydrogen at Trail, British Columbia, since 1930. Urey suggested in 1941 that it could produce heavy water. To the existing $10 million plant consisting of 3,215 cells consuming 75 MW of hydroelectric power, secondary electrolysis cells were added to increase the deuterium concentration in the water from 2.3% to 99.8%. For this process, Hugh Taylor of Princeton developed a platinum-on-carbon catalyst for the first three stages while Urey developed a nickel-chromia one for the fourth stage tower. The final cost was $2.8 million. The Canadian Government did not officially learn of the project until August 1942. Trail's heavy water production started in January 1944 and continued until 1956. Heavy water from Trail was used for Chicago Pile 3, the first reactor using heavy water and natural uranium, which went critical on 15 May 1944.\n\nThe Chalk River, Ontario, site was established to rehouse the Allied effort at the Montreal Laboratory away from an urban area. A new community was built at Deep River, Ontario, to provide residences and facilities for the team members. The site was chosen for its proximity to the industrial manufacturing area of Ontario and Quebec, and proximity to a rail head adjacent to a large military base, Camp Petawawa. Located on the Ottawa River, it had access to abundant water. The first director of the new laboratory was Hans von Halban. He was replaced by John Cockcroft in May 1944, who in turn was succeeded by Bennett Lewis in September 1946. A pilot reactor known as ZEEP (zero-energy experimental pile) became the first Canadian reactor, and the first to be completed outside the United States, when it went critical in September 1945, ZEEP remained in use by researchers until 1970. A larger 10 MW NRX reactor, which was designed during the war, was completed and went critical in July 1947.\n\nThe Eldorado Mine at Port Radium was a source of uranium ore.\n\nAlthough DuPont's preferred designs for the nuclear reactors were helium cooled and used graphite as a moderator, DuPont still expressed an interest in using heavy water as a backup, in case the graphite reactor design proved infeasible for some reason. For this purpose, it was estimated that of heavy water would be required per month. The \"P-9 Project\" was the government's code name for the heavy water production program. As the plant at Trail, which was then under construction, could produce per month, additional capacity was required. Groves therefore authorized DuPont to establish heavy water facilities at the Morgantown Ordnance Works, near Morgantown, West Virginia; at the Wabash River Ordnance Works, near Dana and Newport, Indiana; and at the Alabama Ordnance Works, near Childersburg and Sylacauga, Alabama. Although known as Ordnance Works and paid for under Ordnance Department contracts, they were built and operated by the Army Corps of Engineers. The American plants used a process different from Trail's; heavy water was extracted by distillation, taking advantage of the slightly higher boiling point of heavy water.\n\nThe key raw material for the project was uranium, which was used as fuel for the reactors, as feed that was transformed into plutonium, and, in its enriched form, in the atomic bomb itself. There were four known major deposits of uranium in 1940: in Colorado, in northern Canada, in Joachimsthal in Czechoslovakia, and in the Belgian Congo. All but Joachimstal were in allied hands. A November 1942 survey determined that sufficient quantities of uranium were available to satisfy the project's requirements. Nichols arranged with the State Department for export controls to be placed on uranium oxide and negotiated for the purchase of of uranium ore from the Belgian Congo that was being stored in a warehouse on Staten Island and the remaining stocks of mined ore stored in the Congo. He negotiated with Eldorado Gold Mines for the purchase of ore from its refinery in Port Hope, Ontario, and its shipment in 100-ton lots. The Canadian government subsequently bought up the company's stock until it acquired a controlling interest.\n\nWhile these purchases assured a sufficient supply to meet wartime needs, the American and British leaders concluded that it was in their countries' interest to gain control of as much of the world's uranium deposits as possible. The richest source of ore was the Shinkolobwe mine in the Belgian Congo, but it was flooded and closed. Nichols unsuccessfully attempted to negotiate its reopening and the sale of the entire future output to the United States with Edgar Sengier, the director of the company that owned the mine, Union Minière du Haut Katanga. The matter was then taken up by the Combined Policy Committee. As 30 percent of Union Minière's stock was controlled by British interests, the British took the lead in negotiations. Sir John Anderson and Ambassador John Winant hammered out a deal with Sengier and the Belgian government in May 1944 for the mine to be reopened and of ore to be purchased at $1.45 a pound. To avoid dependence on the British and Canadians for ore, Groves also arranged for the purchase of US Vanadium Corporation's stockpile in Uravan, Colorado. Uranium mining in Colorado yielded about of ore.\n\nMallinckrodt Incorporated in St. Louis, Missouri, took the raw ore and dissolved it in nitric acid to produce uranyl nitrate. Ether was then added in a liquid–liquid extraction process to separate the impurities from the uranyl nitrate. This was then heated to form uranium trioxide, which was reduced to highly pure uranium dioxide. By July 1942, Mallinckrodt was producing a ton of highly pure oxide a day, but turning this into uranium metal initially proved more difficult for contractors Westinghouse and Metal Hydrides. Production was too slow and quality was unacceptably low. A special branch of the Metallurgical Laboratory was established at Iowa State College in Ames, Iowa, under Frank Spedding to investigate alternatives. This became known as the Ames Project, and its Ames process became available in 1943.\n\nNatural uranium consists of 99.3% uranium-238 and 0.7% uranium-235, but only the latter is fissile. The chemically identical uranium-235 has to be physically separated from the more plentiful isotope. Various methods were considered for uranium enrichment, most of which was carried out at Oak Ridge.\n\nThe most obvious technology, the centrifuge, failed, but electromagnetic separation, gaseous diffusion, and thermal diffusion technologies were all successful and contributed to the project. In February 1943, Groves came up with the idea of using the output of some plants as the input for others.\n\nThe centrifuge process was regarded as the only promising separation method in April 1942. Jesse Beams had developed such a process at the University of Virginia during the 1930s, but had encountered technical difficulties. The process required high rotational speeds, but at certain speeds harmonic vibrations developed that threatened to tear the machinery apart. It was therefore necessary to accelerate quickly through these speeds. In 1941 he began working with uranium hexafluoride, the only known gaseous compound of uranium, and was able to separate uranium-235. At Columbia, Urey had Karl Cohen investigate the process, and he produced a body of mathematical theory making it possible to design a centrifugal separation unit, which Westinghouse undertook to construct.\n\nScaling this up to a production plant presented a formidable technical challenge. Urey and Cohen estimated that producing a kilogram (2.2 lb) of uranium-235 per day would require up to 50,000 centrifuges with rotors, or 10,000 centrifuges with rotors, assuming that 4-meter rotors could be built. The prospect of keeping so many rotors operating continuously at high speed appeared daunting, and when Beams ran his experimental apparatus, he obtained only 60% of the predicted yield, indicating that more centrifuges would be required. Beams, Urey and Cohen then began work on a series of improvements which promised to increase the efficiency of the process. However, frequent failures of motors, shafts and bearings at high speeds delayed work on the pilot plant. In November 1942 the centrifuge process was abandoned by the Military Policy Committee following a recommendation by Conant, Nichols and August C. Klein of Stone & Webster.\n\nElectromagnetic isotope separation was developed by Lawrence at the University of California Radiation Laboratory. This method employed devices known as calutrons, a hybrid of the standard laboratory mass spectrometer and cyclotron. The name was derived from the words \"California\", \"university\" and \"cyclotron\". In the electromagnetic process, a magnetic field deflected charged particles according to mass. The process was neither scientifically elegant nor industrially efficient. Compared with a gaseous diffusion plant or a nuclear reactor, an electromagnetic separation plant would consume more scarce materials, require more manpower to operate, and cost more to build. Nonetheless, the process was approved because it was based on proven technology and therefore represented less risk. Moreover, it could be built in stages, and rapidly reach industrial capacity.\nMarshall and Nichols discovered that the electromagnetic isotope separation process would require of copper, which was in desperately short supply. However, silver could be substituted, in an 11:10 ratio. On 3 August 1942, Nichols met with Under Secretary of the Treasury Daniel W. Bell and asked for the transfer of 6,000 tons of silver bullion from the West Point Bullion Depository. \"Young man,\" Bell told him, \"you may think of silver in tons but the Treasury will always think of silver in troy ounces!\" Eventually, were used.\n\nThe silver bars were cast into cylindrical billets and taken to Phelps Dodge in Bayway, New Jersey, where they were extruded into strips thick, wide and long. These were wound onto magnetic coils by Allis-Chalmers in Milwaukee, Wisconsin. After the war, all the machinery was dismantled and cleaned and the floorboards beneath the machinery were ripped up and burned to recover minute amounts of silver. In the end, only 1/3,600,000th was lost. The last silver was returned in May 1970.\n\nResponsibility for the design and construction of the electromagnetic separation plant, which came to be called Y-12, was assigned to Stone & Webster by the S-1 Committee in June 1942. The design called for five first-stage processing units, known as Alpha racetracks, and two units for final processing, known as Beta racetracks. In September 1943 Groves authorized construction of four more racetracks, known as Alpha II. Construction began in February 1943.\n\nWhen the plant was started up for testing on schedule in October, the 14-ton vacuum tanks crept out of alignment because of the power of the magnets, and had to be fastened more securely. A more serious problem arose when the magnetic coils started shorting out. In December Groves ordered a magnet to be broken open, and handfuls of rust were found inside. Groves then ordered the racetracks to be torn down and the magnets sent back to the factory to be cleaned. A pickling plant was established on-site to clean the pipes and fittings. The second Alpha I was not operational until the end of January 1944, the first Beta and first and third Alpha I's came online in March, and the fourth Alpha I was operational in April. The four Alpha II racetracks were completed between July and October 1944.\nTennessee Eastman was contracted to manage Y-12 on the usual cost plus fixed-fee basis, with a fee of $22,500 per month plus $7,500 per racetrack for the first seven racetracks and $4,000 per additional racetrack. The calutrons were initially operated by scientists from Berkeley to remove bugs and achieve a reasonable operating rate. They were then turned over to trained Tennessee Eastman operators who had only a high school education. Nichols compared unit production data, and pointed out to Lawrence that the young \"hillbilly\" girl operators were outperforming his PhDs. They agreed to a production race and Lawrence lost, a morale boost for the Tennessee Eastman workers and supervisors. The girls were \"trained like soldiers not to reason why\", while \"the scientists could not refrain from time-consuming investigation of the cause of even minor fluctuations of the dials.\"\n\nY-12 initially enriched the uranium-235 content to between 13% and 15%, and shipped the first few hundred grams of this to Los Alamos in March 1944. Only 1 part in 5,825 of the uranium feed emerged as final product. Much of the rest was splattered over equipment in the process. Strenuous recovery efforts helped raise production to 10% of the uranium-235 feed by January 1945. In February the Alpha racetracks began receiving slightly enriched (1.4%) feed from the new S-50 thermal diffusion plant. The next month it received enhanced (5%) feed from the K-25 gaseous diffusion plant. By August K-25 was producing uranium sufficiently enriched to feed directly into the Beta tracks.\n\nThe most promising but also the most challenging method of isotope separation was gaseous diffusion. Graham's law states that the rate of effusion of a gas is inversely proportional to the square root of its molecular mass, so in a box containing a semi-permeable membrane and a mixture of two gases, the lighter molecules will pass out of the container more rapidly than the heavier molecules. The gas leaving the container is somewhat enriched in the lighter molecules, while the residual gas is somewhat depleted. The idea was that such boxes could be formed into a cascade of pumps and membranes, with each successive stage containing a slightly more enriched mixture. Research into the process was carried out at Columbia University by a group that included Harold Urey, Karl P. Cohen, and John R. Dunning.\nIn November 1942 the Military Policy Committee approved the construction of a 600-stage gaseous diffusion plant. On 14 December, M. W. Kellogg accepted an offer to construct the plant, which was codenamed K-25. A cost plus fixed-fee contract was negotiated, eventually totaling $2.5 million. A separate corporate entity called Kellex was created for the project, headed by Percival C. Keith, one of Kellogg's vice presidents. The process faced formidable technical difficulties. The highly corrosive gas uranium hexafluoride would have to be used, as no substitute could be found, and the motors and pumps would have to be vacuum tight and enclosed in inert gas. The biggest problem was the design of the barrier, which would have to be strong, porous and resistant to corrosion by uranium hexafluoride. The best choice for this seemed to be nickel. Edward Adler and Edward Norris created a mesh barrier from electroplated nickel. A six-stage pilot plant was built at Columbia to test the process, but the Norris-Adler prototype proved to be too brittle. A rival barrier was developed from powdered nickel by Kellex, the Bell Telephone Laboratories and the Bakelite Corporation. In January 1944, Groves ordered the Kellex barrier into production.\n\nKellex's design for K-25 called for a four-story long U-shaped structure containing 54 contiguous buildings. These were divided into nine sections. Within these were cells of six stages. The cells could be operated independently, or consecutively within a section. Similarly, the sections could be operated separately or as part of a single cascade. A survey party began construction by marking out the site in May 1943. Work on the main building began in October 1943, and the six-stage pilot plant was ready for operation on 17 April 1944. In 1945 Groves canceled the upper stages of the plant, directing Kellex to instead design and build a 540-stage side feed unit, which became known as K-27. Kellex transferred the last unit to the operating contractor, Union Carbide and Carbon, on 11 September 1945. The total cost, including the K-27 plant completed after the war, came to $480 million.\n\nThe production plant commenced operation in February 1945, and as cascade after cascade came online, the quality of the product increased. By April 1945, K-25 had attained a 1.1% enrichment and the output of the S-50 thermal diffusion plant began being used as feed. Some product produced the next month reached nearly 7% enrichment. In August, the last of the 2,892 stages commenced operation. K-25 and K-27 achieved their full potential in the early postwar period, when they eclipsed the other production plants and became the prototypes for a new generation of plants.\n\nThe thermal diffusion process was based on Sydney Chapman and David Enskog's theory, which explained that when a mixed gas passes through a temperature gradient, the heavier one tends to concentrate at the cold end and the lighter one at the warm end. Since hot gases tend to rise and cool ones tend to fall, this can be used as a means of isotope separation. This process was first demonstrated by Klaus Clusius and Gerhard Dickel in Germany in 1938. It was developed by US Navy scientists, but was not one of the enrichment technologies initially selected for use in the Manhattan Project. This was primarily due to doubts about its technical feasibility, but the inter-service rivalry between the Army and Navy also played a part.\nThe Naval Research Laboratory continued the research under Philip Abelson's direction, but there was little contact with the Manhattan Project until April 1944, when Captain William S. Parsons, the naval officer in charge of ordnance development at Los Alamos, brought Oppenheimer news of encouraging progress in the Navy's experiments on thermal diffusion. Oppenheimer wrote to Groves suggesting that the output of a thermal diffusion plant could be fed into Y-12. Groves set up a committee consisting of Warren K. Lewis, Eger Murphree and Richard Tolman to investigate the idea, and they estimated that a thermal diffusion plant costing $3.5 million could enrich of uranium per week to nearly 0.9% uranium-235. Groves approved its construction on 24 June 1944.\n\nGroves contracted with the H. K. Ferguson Company of Cleveland, Ohio, to build the thermal diffusion plant, which was designated S-50. Groves's advisers, Karl Cohen and W. I. Thompson from Standard Oil, estimated that it would take six months to build. Groves gave Ferguson just four. Plans called for the installation of 2,142 diffusion columns arranged in 21 racks. Inside each column were three concentric tubes. Steam, obtained from the nearby K-25 powerhouse at a pressure of and temperature of , flowed downward through the innermost nickel pipe, while water at flowed upward through the outermost iron pipe. Isotope separation occurred in the uranium hexafluoride gas between the nickel and copper pipes.\n\nWork commenced on 9 July 1944, and S-50 began partial operation in September. Ferguson operated the plant through a subsidiary known as Fercleve. The plant produced just of 0.852% uranium-235 in October. Leaks limited production and forced shutdowns over the next few months, but in June 1945 it produced . By March 1945, all 21 production racks were operating. Initially the output of S-50 was fed into Y-12, but starting in March 1945 all three enrichment processes were run in series. S-50 became the first stage, enriching from 0.71% to 0.89%. This material was fed into the gaseous diffusion process in the K-25 plant, which produced a product enriched to about 23%. This was, in turn, fed into Y-12, which boosted it to about 89%, sufficient for nuclear weapons.\n\nAbout of uranium enriched to 89% uranium-235 was delivered to Los Alamos by July 1945. The entire 50 kg, along with some 50%-enriched, averaging out to about 85% enriched, were used in Little Boy.\nThe second line of development pursued by the Manhattan Project used the fissile element plutonium. Although small amounts of plutonium exist in nature, the best way to obtain large quantities of the element is in a nuclear reactor, in which natural uranium is bombarded by neutrons. The uranium-238 is transmuted into uranium-239, which rapidly decays, first into neptunium-239 and then into plutonium-239. Only a small amount of the uranium-238 will be transformed, so the plutonium must be chemically separated from the remaining uranium, from any initial impurities, and from fission products.\n\nIn March 1943, DuPont began construction of a plutonium plant on a site at Oak Ridge. Intended as a pilot plant for the larger production facilities at Hanford, it included the air-cooled X-10 Graphite Reactor, a chemical separation plant, and support facilities. Because of the subsequent decision to construct water-cooled reactors at Hanford, only the chemical separation plant operated as a true pilot. The X-10 Graphite Reactor consisted of a huge block of graphite, long on each side, weighing around , surrounded by of high-density concrete as a radiation shield.\n\nThe greatest difficulty was encountered with the uranium slugs produced by Mallinckrodt and Metal Hydrides. These somehow had to be coated in aluminum to avoid corrosion and the escape of fission products into the cooling system. The Grasselli Chemical Company attempted to develop a hot dipping process without success. Meanwhile, Alcoa tried canning. A new process for flux-less welding was developed, and 97% of the cans passed a standard vacuum test, but high temperature tests indicated a failure rate of more than 50%. Nonetheless, production began in June 1943. The Metallurgical Laboratory eventually developed an improved welding technique with the help of General Electric, which was incorporated into the production process in October 1943.\n\nWatched by Fermi and Compton, the X-10 Graphite Reactor went critical on 4 November 1943 with about of uranium. A week later the load was increased to , raising its power generation to 500 kW, and by the end of the month the first 500 mg of plutonium was created. Modifications over time raised the power to 4,000 kW in July 1944. X-10 operated as a production plant until January 1945, when it was turned over to research activities.\n\nAlthough an air-cooled design was chosen for the reactor at Oak Ridge to facilitate rapid construction, it was recognized that this would be impractical for the much larger production reactors. Initial designs by the Metallurgical Laboratory and DuPont used helium for cooling, before they determined that a water-cooled reactor would be simpler, cheaper and quicker to build. The design did not become available until 4 October 1943; in the meantime, Matthias concentrated on improving the Hanford Site by erecting accommodations, improving the roads, building a railway switch line, and upgrading the electricity, water and telephone lines.\nAs at Oak Ridge, the most difficulty was encountered while canning the uranium slugs, which commenced at Hanford in March 1944. They were pickled to remove dirt and impurities, dipped in molten bronze, tin, and aluminum-silicon alloy, canned using hydraulic presses, and then capped using arc welding under an argon atmosphere. Finally, they were subjected to a series of tests to detect holes or faulty welds. Disappointingly, most canned slugs initially failed the tests, resulting in an output of only a handful of canned slugs per day. But steady progress was made and by June 1944 production increased to the point where it appeared that enough canned slugs would be available to start Reactor B on schedule in August 1944.\n\nWork began on Reactor B, the first of six planned 250 MW reactors, on 10 October 1943. The reactor complexes were given letter designations A through F, with B, D and F sites chosen to be developed first, as this maximised the distance between the reactors. They would be the only ones constructed during the Manhattan Project. Some of steel, of concrete, 50,000 concrete blocks and 71,000 concrete bricks were used to construct the high building.\n\nConstruction of the reactor itself commenced in February 1944. Watched by Compton, Matthias, DuPont's Crawford Greenewalt, Leona Woods and Fermi, who inserted the first slug, the reactor was powered up beginning on 13 September 1944. Over the next few days, 838 tubes were loaded and the reactor went critical. Shortly after midnight on 27 September, the operators began to withdraw the control rods to initiate production. At first all appeared well but around 03:00 the power level started to drop and by 06:30 the reactor had shut down completely. The cooling water was investigated to see if there was a leak or contamination. The next day the reactor started up again, only to shut down once more.\n\nFermi contacted Chien-Shiung Wu, who identified the cause of the problem as neutron poisoning from xenon-135, which has a half-life of 9.2 hours. Fermi, Woods, Donald J. Hughes and John Archibald Wheeler then calculated the nuclear cross section of xenon-135, which turned out to be 30,000 times that of uranium. DuPont engineer George Graves had deviated from the Metallurgical Laboratory's original design in which the reactor had 1,500 tubes arranged in a circle, and had added an additional 504 tubes to fill in the corners. The scientists had originally considered this overengineering a waste of time and money, but Fermi realized that by loading all 2,004 tubes, the reactor could reach the required power level and efficiently produce plutonium. Reactor D was started on 17 December 1944 and Reactor F on 25 February 1945.\n\nMeanwhile, the chemists considered the problem of how plutonium could be separated from uranium when its chemical properties were not known. Working with the minute quantities of plutonium available at the Metallurgical Laboratory in 1942, a team under Charles M. Cooper developed a lanthanum fluoride process for separating uranium and plutonium, which was chosen for the pilot separation plant. A second separation process, the bismuth phosphate process, was subsequently developed by Seaborg and Stanly G. Thomson. This process worked by toggling plutonium between its +4 and +6 oxidation states in solutions of bismuth phosphate. In the former state, the plutonium was precipitated; in the latter, it stayed in solution and the other products were precipitated.\n\nGreenewalt favored the bismuth phosphate process due to the corrosive nature of lanthanum fluoride, and it was selected for the Hanford separation plants. Once X-10 began producing plutonium, the pilot separation plant was put to the test. The first batch was processed at 40% efficiency but over the next few months this was raised to 90%.\n\nAt Hanford, top priority was initially given to the installations in the 300 area. This contained buildings for testing materials, preparing uranium, and assembling and calibrating instrumentation. One of the buildings housed the canning equipment for the uranium slugs, while another contained a small test reactor. Notwithstanding the high priority allocated to it, work on the 300 area fell behind schedule due to the unique and complex nature of the 300 area facilities, and wartime shortages of labor and materials.\n\nEarly plans called for the construction of two separation plants in each of the areas known as 200-West and 200-East. This was subsequently reduced to two, the T and U plants, in 200-West and one, the B plant, at 200-East. Each separation plant consisted of four buildings: a process cell building or \"canyon\" (known as 221), a concentration building (224), a purification building (231) and a magazine store (213). The canyons were each long and wide. Each consisted of forty cells.\n\nWork began on 221-T and 221-U in January 1944, with the former completed in September and the latter in December. The 221-B building followed in March 1945. Because of the high levels of radioactivity involved, all work in the separation plants had to be conducted by remote control using closed-circuit television, something unheard of in 1943. Maintenance was carried out with the aid of an overhead crane and specially designed tools. The 224 buildings were smaller because they had less material to process, and it was less radioactive. The 224-T and 224-U buildings were completed on 8 October 1944, and 224-B followed on 10 February 1945. The purification methods that were eventually used in 231-W were still unknown when construction commenced on 8 April 1944, but the plant was complete and the methods were selected by the end of the year. On 5 February 1945, Matthias hand-delivered the first shipment of 80 g of 95%-pure plutonium nitrate to a Los Alamos courier in Los Angeles.\n\nIn 1943, development efforts were directed to a gun-type fission weapon with plutonium called Thin Man. Initial research on the properties of plutonium was done using cyclotron-generated plutonium-239, which was extremely pure, but could only be created in very small amounts. Los Alamos received the first sample of plutonium from the Clinton X-10 reactor in April 1944 and within days Emilio Segrè discovered a problem: the reactor-bred plutonium had a higher concentration of plutonium-240, resulting in up to five times the spontaneous fission rate of cyclotron plutonium. Seaborg had correctly predicted in March 1943 that some of the plutonium-239 would absorb a neutron and become plutonium-240.\n\nThis made reactor plutonium unsuitable for use in a gun-type weapon. The plutonium-240 would start the chain reaction too quickly, causing a predetonation that would release enough energy to disperse the critical mass with a minimal amount of plutonium reacted (a fizzle). A faster gun was suggested but found to be impractical. The possibility of separating the isotopes was considered and rejected, as plutonium-240 is even harder to separate from plutonium-239 than uranium-235 from uranium-238.\n\nWork on an alternative method of bomb design, known as implosion, had begun earlier under the direction of the physicist Seth Neddermeyer. Implosion used explosives to crush a subcritical sphere of fissile material into a smaller and denser form. When the fissile atoms are packed closer together, the rate of neutron capture increases, and the mass becomes a critical mass. The metal needs to travel only a very short distance, so the critical mass is assembled in much less time than it would take with the gun method. Neddermeyer's 1943 and early 1944 investigations into implosion showed promise, but also made it clear that the problem would be much more difficult from a theoretical and engineering perspective than the gun design. In September 1943, John von Neumann, who had experience with shaped charges used in armor-piercing shells, argued that not only would implosion reduce the danger of predetonation and fizzle, but would make more efficient use of the fissionable material. He proposed using a spherical configuration instead of the cylindrical one that Neddermeyer was working on.\nBy July 1944, Oppenheimer had concluded plutonium could not be used in a gun design, and opted for implosion. The accelerated effort on an implosion design, codenamed Fat Man, began in August 1944 when Oppenheimer implemented a sweeping reorganization of the Los Alamos laboratory to focus on implosion. Two new groups were created at Los Alamos to develop the implosion weapon, X (for explosives) Division headed by explosives expert George Kistiakowsky and G (for gadget) Division under Robert Bacher. The new design that von Neumann and T (for theoretical) Division, most notably Rudolf Peierls, had devised used explosive lenses to focus the explosion onto a spherical shape using a combination of both slow and fast high explosives.\n\nThe design of lenses that detonated with the proper shape and velocity turned out to be slow, difficult and frustrating. Various explosives were tested before settling on composition B as the fast explosive and baratol as the slow explosive. The final design resembled a soccer ball, with 20 hexagonal and 12 pentagonal lenses, each weighing about . Getting the detonation just right required fast, reliable and safe electrical detonators, of which there were two for each lens for reliability. It was therefore decided to use exploding-bridgewire detonators, a new invention developed at Los Alamos by a group led by Luis Alvarez. A contract for their manufacture was given to Raytheon.\n\nTo study the behavior of converging shock waves, Robert Serber devised the RaLa Experiment, which used the short-lived radioisotope lanthanum-140, a potent source of gamma radiation. The gamma ray source was placed in the center of a metal sphere surrounded by the explosive lenses, which in turn were inside in an ionization chamber. This allowed the taking of an X-ray movie of the implosion. The lenses were designed primarily using this series of tests. In his history of the Los Alamos project, David Hawkins wrote: \"RaLa became the most important single experiment affecting the final bomb design\".\n\nWithin the explosives was the thick aluminum pusher, which provided a smooth transition from the relatively low density explosive to the next layer, the thick tamper of natural uranium. Its main job was to hold the critical mass together as long as possible, but it would also reflect neutrons back into the core. Some part of it might fission as well. To prevent predetonation by an external neutron, the tamper was coated in a thin layer of boron. A polonium-beryllium modulated neutron initiator, known as an \"urchin\" because its shape resembled a sea urchin, was developed to start the chain reaction at precisely the right moment. This work with the chemistry and metallurgy of radioactive polonium was directed by Charles Allen Thomas of the Monsanto Company and became known as the Dayton Project. Testing required up to 500 curies per month of polonium, which Monsanto was able to deliver. The whole assembly was encased in a duralumin bomb casing to protect it from bullets and flak.\nThe ultimate task of the metallurgists was to determine how to cast plutonium into a sphere. The difficulties became apparent when attempts to measure the density of plutonium gave inconsistent results. At first contamination was believed to be the cause, but it was soon determined that there were multiple allotropes of plutonium. The brittle α phase that exists at room temperature changes to the plastic β phase at higher temperatures. Attention then shifted to the even more malleable δ phase that normally exists in the 300 °C to 450 °C range. It was found that this was stable at room temperature when alloyed with aluminum, but aluminum emits neutrons when bombarded with alpha particles, which would exacerbate the pre-ignition problem. The metallurgists then hit upon a plutonium-gallium alloy, which stabilized the δ phase and could be hot pressed into the desired spherical shape. As plutonium was found to corrode readily, the sphere was coated with nickel.\n\nThe work proved dangerous. By the end of the war, half the experienced chemists and metallurgists had to be removed from work with plutonium when unacceptably high levels of the element appeared in their urine. A minor fire at Los Alamos in January 1945 led to a fear that a fire in the plutonium laboratory might contaminate the whole town, and Groves authorized the construction of a new facility for plutonium chemistry and metallurgy, which became known as the DP-site. The hemispheres for the first plutonium pit (or core) were produced and delivered on 2 July 1945. Three more hemispheres followed on 23 July and were delivered three days later.\n\nBecause of the complexity of an implosion-style weapon, it was decided that, despite the waste of fissile material, an initial test would be required. Groves approved the test, subject to the active material being recovered. Consideration was therefore given to a controlled fizzle, but Oppenheimer opted instead for a full-scale nuclear test, codenamed \"Trinity\".\nIn March 1944, planning for the test was assigned to Kenneth Bainbridge, a professor of physics at Harvard, working under Kistiakowsky. Bainbridge selected the bombing range near Alamogordo Army Airfield as the site for the test. Bainbridge worked with Captain Samuel P. Davalos on the construction of the Trinity Base Camp and its facilities, which included barracks, warehouses, workshops, an explosive magazine and a commissary.\n\nGroves did not relish the prospect of explaining the loss of a billion dollars worth of plutonium to a Senate committee, so a cylindrical containment vessel codenamed \"Jumbo\" was constructed to recover the active material in the event of a failure. Measuring long and wide, it was fabricated at great expense from of iron and steel by Babcock & Wilcox in Barberton, Ohio. Brought in a special railroad car to a siding in Pope, New Mexico, it was transported the last to the test site on a trailer pulled by two tractors. By the time it arrived, however, confidence in the implosion method was high enough, and the availability of plutonium was sufficient, that Oppenheimer decided not to use it. Instead, it was placed atop a steel tower from the weapon as a rough measure of how powerful the explosion would be. In the end, Jumbo survived, although its tower did not, adding credence to the belief that Jumbo would have successfully contained a fizzled explosion.\n\nA pre-test explosion was conducted on 7 May 1945 to calibrate the instruments. A wooden test platform was erected from Ground Zero and piled with of TNT spiked with nuclear fission products in the form of an irradiated uranium slug from Hanford, which was dissolved and poured into tubing inside the explosive. This explosion was observed by Oppenheimer and Groves's new deputy commander, Brigadier General Thomas Farrell. The pre-test produced data that proved vital for the Trinity test.\n\nFor the actual test, the weapon, nicknamed \"the gadget\", was hoisted to the top of a steel tower, as detonation at that height would give a better indication of how the weapon would behave when dropped from a bomber. Detonation in the air maximized the energy applied directly to the target, and generated less nuclear fallout. The gadget was assembled under the supervision of Norris Bradbury at the nearby McDonald Ranch House on 13 July, and precariously winched up the tower the following day. Observers included Bush, Chadwick, Conant, Farrell, Fermi, Groves, Lawrence, Oppenheimer and Tolman. At 05:30 on 16 July 1945 the gadget exploded with an energy equivalent of around 20 kilotons of TNT, leaving a crater of Trinitite (radioactive glass) in the desert wide. The shock wave was felt over away, and the mushroom cloud reached in height. It was heard as far away as El Paso, Texas, so Groves issued a cover story about an ammunition magazine explosion at Alamogordo Field.\nOppenheimer later recalled that, while witnessing the explosion, he thought of a verse from the Hindu holy book, the \"Bhagavad Gita\" (XI,12):\n\nYears later he would explain that another verse had also entered his head at that time:\nIn June 1944, the Manhattan Project employed some 129,000 workers, of whom 84,500 were construction workers, 40,500 were plant operators and 1,800 were military personnel. As construction activity fell off, the workforce declined to 100,000 a year later, but the number of military personnel increased to 5,600. Procuring the required numbers of workers, especially highly skilled workers, in competition with other vital wartime programs proved very difficult. In 1943, Groves obtained a special temporary priority for labor from the War Manpower Commission. In March 1944, both the War Production Board and the War Manpower Commission gave the project their highest priority.\nTolman and Conant, in their role as the project's scientific advisers, drew up a list of candidate scientists and had them rated by scientists already working on the project. Groves then sent a personal letter to the head of their university or company asking for them to be released for essential war work. At the University of Wisconsin–Madison, Stanislaw Ulam gave one of his students, Joan Hinton, an exam early, so she could leave to do war work. A few weeks later, Ulam received a letter from Hans Bethe, inviting him to join the project. Conant personally persuaded Kistiakowsky to join the project.\n\nOne source of skilled personnel was the Army itself, particularly the Army Specialized Training Program. In 1943, the MED created the Special Engineer Detachment (SED), with an authorized strength of 675. Technicians and skilled workers drafted into the Army were assigned to the SED. Another source was the Women's Army Corps (WAC). Initially intended for clerical tasks handling classified material, the WACs were soon tapped for technical and scientific tasks as well. On 1 February 1945, all military personnel assigned to the MED, including all SED detachments, were assigned to the 9812th Technical Service Unit, except at Los Alamos, where military personnel other than SED, including the WACs and Military Police, were assigned to the 4817th Service Command Unit.\n\nAn Associate Professor of Radiology at the University of Rochester School of Medicine, Stafford L. Warren, was commissioned as a colonel in the United States Army Medical Corps, and appointed as chief of the MED's Medical Section and Groves' medical advisor. Warren's initial task was to staff hospitals at Oak Ridge, Richland and Los Alamos. The Medical Section was responsible for medical research, but also for the MED's health and safety programs. This presented an enormous challenge, because workers were handling a variety of toxic chemicals, using hazardous liquids and gases under high pressures, working with high voltages, and performing experiments involving explosives, not to mention the largely unknown dangers presented by radioactivity and handling fissile materials. Yet in December 1945, the National Safety Council presented the Manhattan Project with the Award of Honor for Distinguished Service to Safety in recognition of its safety record. Between January 1943 and June 1945, there were 62 fatalities and 3,879 disabling injuries, which was about 62 percent below the rate of private industry.\nA 1945 \"Life\" article estimated that before the Hiroshima and Nagasaki bombings \"probably no more than a few dozen men in the entire country knew the full meaning of the Manhattan Project, and perhaps only a thousand others even were aware that work on atoms was involved.\" The magazine wrote that the more than 100,000 others employed with the project \"worked like moles in the dark\". Warned that disclosing the project's secrets was punishable by 10 years in prison or a $10,000 ($ today) fine, they saw enormous quantities of raw materials enter factories with nothing coming out, and monitored \"dials and switches while behind thick concrete walls mysterious reactions took place\" without knowing the purpose of their jobs.\n\nOak Ridge security personnel considered any private party with more than seven people as suspicious, and residents—who believed that US government agents were secretly among them—avoided repeatedly inviting the same guests. Although original residents of the area could be buried in existing cemeteries, every coffin was reportedly opened for inspection. Everyone, including top military officials, and their automobiles were searched when entering and exiting project facilities. One Oak Ridge worker stated that \"if you got inquisitive, you were called on the carpet within two hours by government secret agents. Usually those summoned to explain were then escorted bag and baggage to the gate and ordered to keep going\".\n\nDespite being told that their work would help end the war and perhaps all future wars, not seeing or understanding the results of their often tedious duties—or even typical side effects of factory work such as smoke from smokestacks—and the war in Europe ending without the use of their work, caused serious morale problems among workers and caused many rumors to spread. One manager stated after the war:\n\nAnother worker told of how, working in a laundry, she every day held \"a special instrument\" to uniforms and listened for \"a clicking noise\". She learned only after the war that she had been performing the important task of checking for radiation with a geiger counter. To improve morale among such workers Oak Ridge created an extensive system of intramural sports leagues, including 10 baseball teams, 81 softball teams, and 26 football teams.\n\nVoluntary censorship of atomic information began before the Manhattan Project. After the start of the European war in 1939 American scientists began avoiding publishing military-related research, and in 1940 scientific journals began asking the National Academy of Sciences to clear articles. William L. Laurence of \"The New York Times\", who wrote an article on atomic fission in \"The Saturday Evening Post\" of 7 September 1940, later learned that government officials asked librarians nationwide in 1943 to withdraw the issue. The Soviets noticed the silence, however. In April 1942 nuclear physicist Georgy Flyorov wrote to Josef Stalin on the absence of articles on nuclear fission in American journals; this resulted in the Soviet Union establishing its own atomic bomb project.\n\nThe Manhattan Project operated under tight security lest its discovery induce Axis powers, especially Germany, to accelerate their own nuclear projects or undertake covert operations against the project. The government's Office of Censorship, by contrast, relied on the press to comply with a voluntary code of conduct it published, and the project at first avoided notifying the office. By early 1943 newspapers began publishing reports of large construction in Tennessee and Washington based on public records, and the office began discussing with the project how to maintain secrecy. In June the Office of Censorship asked newspapers and broadcasters to avoid discussing \"atom smashing, atomic energy, atomic fission, atomic splitting, or any of their equivalents. The use for military purposes of radium or radioactive materials, heavy water, high voltage discharge equipment, cyclotrons.\" The office also asked to avoid discussion of \"polonium, uranium, ytterbium, hafnium, protactinium, radium, rhenium, thorium, deuterium\"; only uranium was sensitive, but was listed with other elements to hide its importance.\n\nThe prospect of sabotage was always present, and sometimes suspected when there were equipment failures. While there were some problems believed to be the result of careless or disgruntled employees, there were no confirmed instances of Axis-instigated sabotage. However, on 10 March 1945, a Japanese fire balloon struck a power line, and the resulting power surge caused the three reactors at Hanford to be temporarily shut down. With so many people involved, security was a difficult task. A special Counter Intelligence Corps detachment was formed to handle the project's security issues. By 1943, it was clear that the Soviet Union was attempting to penetrate the project. Lieutenant Colonel Boris T. Pash, the head of the Counter Intelligence Branch of the Western Defense Command, investigated suspected Soviet espionage at the Radiation Laboratory in Berkeley. Oppenheimer informed Pash that he had been approached by a fellow professor at Berkeley, Haakon Chevalier, about passing information to the Soviet Union.\n\nThe most successful Soviet spy was Klaus Fuchs, a member of the British Mission who played an important part at Los Alamos. The 1950 revelation of his espionage activities damaged the United States' nuclear cooperation with Britain and Canada. Subsequently, other instances of espionage were uncovered, leading to the arrest of Harry Gold, David Greenglass, and Ethel and Julius Rosenberg. Other spies like George Koval and Theodore Hall remained unknown for decades. The value of the espionage is difficult to quantify, as the principal constraint on the Soviet atomic bomb project was a shortage of uranium ore. The consensus is that espionage saved the Soviets one or two years of effort.\n\nIn addition to developing the atomic bomb, the Manhattan Project was charged with gathering intelligence on the German nuclear energy project. It was believed that the Japanese nuclear weapons program was not far advanced because Japan had little access to uranium ore, but it was initially feared that Germany was very close to developing its own weapons. At the instigation of the Manhattan Project, a bombing and sabotage campaign was carried out against heavy water plants in German-occupied Norway. A small mission was created, jointly staffed by the Office of Naval Intelligence, OSRD, the Manhattan Project, and Army Intelligence (G-2), to investigate enemy scientific developments. It was not restricted to those involving nuclear weapons. The Chief of Army Intelligence, Major General George V. Strong, appointed Boris Pash to command the unit, which was codenamed \"Alsos\", a Greek word meaning \"grove\".\nThe Alsos Mission to Italy questioned staff of the physics laboratory at the University of Rome following the capture of the city in June 1944. Meanwhile, Pash formed a combined British and American Alsos mission in London under the command of Captain Horace K. Calvert to participate in Operation Overlord. Groves considered the risk that the Germans might attempt to disrupt the Normandy landings with radioactive poisons was sufficient to warn General Dwight D. Eisenhower and send an officer to brief his chief of staff, Lieutenant General Walter Bedell Smith. Under the codename Operation Peppermint, special equipment was prepared and Chemical Warfare Service teams were trained in its use.\n\nFollowing in the wake of the advancing Allied armies, Pash and Calvert interviewed Frédéric Joliot-Curie about the activities of German scientists. They spoke to officials at Union Minière du Haut Katanga about uranium shipments to Germany. They tracked down 68 tons of ore in Belgium and 30 tons in France. The interrogation of German prisoners indicated that uranium and thorium were being processed in Oranienburg, 20 miles north of Berlin, so Groves arranged for it to be bombed on 15 March 1945.\n\nAn Alsos team went to Stassfurt in the Soviet Occupation Zone and retrieved 11 tons of ore from WIFO. In April 1945, Pash, in command of a composite force known as T-Force, conducted Operation Harborage, a sweep behind enemy lines of the cities of Hechingen, Bisingen, and Haigerloch that were the heart of the German nuclear effort. T-Force captured the nuclear laboratories, documents, equipment and supplies, including heavy water and 1.5 tons of metallic uranium.\n\nAlsos teams rounded up German scientists including Kurt Diebner, Otto Hahn, Walther Gerlach, Werner Heisenberg, and Carl Friedrich von Weizsäcker, who were taken to England where they were interned at Farm Hall, a bugged house in Godmanchester. After the bombs were detonated in Japan, the Germans were forced to confront the fact that the Allies had done what they could not.\n\nStarting in November 1943, the Army Air Forces Materiel Command at Wright Field, Ohio, began Silverplate, the codename modification of B-29s to carry the bombs. Test drops were carried out at Muroc Army Air Field, California, and the Naval Ordnance Test Station at Inyokern, California. Groves met with the Chief of United States Army Air Forces (USAAF), General Henry H. Arnold, in March 1944 to discuss the delivery of the finished bombs to their targets. The only Allied aircraft capable of carrying the long Thin Man or the wide Fat Man was the British Avro Lancaster, but using a British aircraft would have caused difficulties with maintenance. Groves hoped that the American Boeing B-29 Superfortress could be modified to carry Thin Man by joining its two bomb bays together. Arnold promised that no effort would be spared to modify B-29s to do the job, and designated Major General Oliver P. Echols as the USAAF liaison to the Manhattan Project. In turn, Echols named Colonel Roscoe C. Wilson as his alternate, and Wilson became Manhattan Project's main USAAF contact. President Roosevelt instructed Groves that if the atomic bombs were ready before the war with Germany ended, he should be ready to drop them on Germany.\nThe 509th Composite Group was activated on 17 December 1944 at Wendover Army Air Field, Utah, under the command of Colonel Paul W. Tibbets. This base, close to the border with Nevada, was codenamed \"Kingman\" or \"W-47\". Training was conducted at Wendover and at Batista Army Airfield, Cuba, where the 393d Bombardment Squadron practiced long-distance flights over water, and dropping dummy pumpkin bombs. A special unit known as Project Alberta was formed at Los Alamos under Navy Captain William S. Parsons from Project Y as part of the Manhattan Project to assist in preparing and delivering the bombs. Commander Frederick L. Ashworth from Alberta met with Fleet Admiral Chester W. Nimitz on Guam in February 1945 to inform him of the project. While he was there, Ashworth selected North Field on the Pacific Island Tinian as a base for the 509th Composite Group, and reserved space for the group and its buildings. The group deployed there in July 1945. Farrell arrived at Tinian on 30 July as the Manhattan Project representative.\n\nMost of the components for Little Boy left San Francisco on the cruiser on 16 July and arrived on Tinian on 26 July. Four days later the ship was sunk by a Japanese submarine. The remaining components, which included six uranium-235 rings, were delivered by three C-54 Skymasters of the 509th Group's 320th Troop Carrier Squadron. Two Fat Man assemblies travelled to Tinian in specially modified 509th Composite Group B-29s. The first plutonium core went in a special C-54. A joint targeting committee of the Manhattan District and USAAF was established to determine which cities in Japan should be targets, and recommended Kokura, Hiroshima, Niigata, and Kyoto. At this point, Secretary of War Henry L. Stimson intervened, announcing that he would be making the targeting decision, and that he would not authorize the bombing of Kyoto on the grounds of its historical and religious significance. Groves therefore asked Arnold to remove Kyoto not just from the list of nuclear targets, but from targets for conventional bombing as well. One of Kyoto's substitutes was Nagasaki.\n\nIn May 1945, the Interim Committee was created to advise on wartime and postwar use of nuclear energy. The committee was chaired by Stimson, with James F. Byrnes, a former US Senator soon to be Secretary of State, as President Harry S. Truman's personal representative; Ralph A. Bard, the Under Secretary of the Navy; William L. Clayton, the Assistant Secretary of State; Vannevar Bush; Karl T. Compton; James B. Conant; and George L. Harrison, an assistant to Stimson and president of New York Life Insurance Company. The Interim Committee in turn established a scientific panel consisting of Arthur Compton, Fermi, Lawrence and Oppenheimer to advise it on scientific issues. In its presentation to the Interim Committee, the scientific panel offered its opinion not just on the likely physical effects of an atomic bomb, but on its probable military and political impact.\n\nAt the Potsdam Conference in Germany, Truman was informed that the Trinity test had been successful. He told Stalin, the leader of the Soviet Union, that the US had a new superweapon, without giving any details. This was the first official communication to the Soviet Union about the bomb, but Stalin already knew about it from spies. With the authorization to use the bomb against Japan already given, no alternatives were considered after the Japanese rejection of the Potsdam Declaration.\nOn 6 August 1945, a Boeing B-29 Superfortress (\"Enola Gay\") of the 393d Bombardment Squadron, piloted by Tibbets, lifted off from North Field, and Little Boy in its bomb bay. Hiroshima, the headquarters of the 2nd General Army and Fifth Division and a port of embarkation, was the primary target of the mission, with Kokura and Nagasaki as alternatives. With Farrell's permission, Parsons, the weaponeer in charge of the mission, completed the bomb assembly in the air to minimize the risks during takeoff. The bomb detonated at an altitude of with a blast that was later estimated to be the equivalent of 13 kilotons of TNT. An area of approximately was destroyed. Japanese officials determined that 69% of Hiroshima's buildings were destroyed and another 6–7% damaged. About 70,000 to 80,000 people, of whom 20,000 were Japanese combatants and 20,000 were Korean slave laborers, or some 30% of the population of Hiroshima, were killed immediately, and another 70,000 injured.\n\nOn the morning of 9 August 1945, a second B-29 (\"Bockscar\"), piloted by the 393d Bombardment Squadron's commander, Major Charles W. Sweeney, lifted off with Fat Man on board. This time, Ashworth served as weaponeer and Kokura was the primary target. Sweeney took off with the weapon already armed but with the electrical safety plugs still engaged. When they reached Kokura, they found cloud cover had obscured the city, prohibiting the visual attack required by orders. After three runs over the city, and with fuel running low, they headed for the secondary target, Nagasaki. Ashworth decided that a radar approach would be used if the target was obscured, but a last-minute break in the clouds over Nagasaki allowed a visual approach as ordered. The Fat Man was dropped over the city's industrial valley midway between the Mitsubishi Steel and Arms Works in the south and the Mitsubishi-Urakami Ordnance Works in the north. The resulting explosion had a blast yield equivalent to 21 kilotons of TNT, roughly the same as the Trinity blast, but was confined to the Urakami Valley, and a major portion of the city was protected by the intervening hills, resulting in the destruction of about 44% of the city. The bombing also crippled the city's industrial production extensively and killed 23,200–28,200 Japanese industrial workers and 150 Japanese soldiers. Overall, an estimated 35,000–40,000 people were killed and 60,000 injured.\n\nGroves expected to have another atomic bomb ready for use on 19 August, with three more in September and a further three in October. Two more Fat Man assemblies were readied, and scheduled to leave Kirtland Field for Tinian on 11 and 14 August. At Los Alamos, technicians worked 24 hours straight to cast another plutonium core. Although cast, it still needed to be pressed and coated, which would take until 16 August. It could therefore have been ready for use on 19 August. On 10 August, Truman secretly requested that additional atomic bombs not be dropped on Japan without his express authority. Groves suspended the third core's shipment on his own authority on 13 August.\n\nOn 11 August, Groves phoned Warren with orders to organize a survey team to report on the damage and radioactivity at Hiroshima and Nagasaki. A party equipped with portable Geiger counters arrived in Hiroshima on 8 September headed by Farrell and Warren, with Japanese Rear Admiral Masao Tsuzuki, who acted as a translator. They remained in Hiroshima until 14 September and then surveyed Nagasaki from 19 September to 8 October. This and other scientific missions to Japan would provide valuable scientific and historical data.\n\nThe necessity of the bombings of Hiroshima and Nagasaki became a subject of controversy among historians. Some questioned whether an \"atomic diplomacy\" would not have attained the same goals and disputed whether the bombings or the Soviet declaration of war on Japan was decisive. The Franck Report was the most notable effort pushing for a demonstration but was turned down by the Interim Committee's scientific panel. The Szilárd petition, drafted in July 1945 and signed by dozens of scientists working on the Manhattan Project, was a late attempt at warning President Harry S. Truman about his responsibility in using such weapons.\n\nSeeing the work they had not understood produce the Hiroshima and Nagasaki bombs amazed the workers of the Manhattan Project as much as the rest of the world; newspapers in Oak Ridge announcing the Hiroshima bomb sold for $1 ($ today}). Although the bombs' existence was public, secrecy continued, and many workers remained ignorant of their jobs; one stated in 1946, \"I don't know what the hell I'm doing besides looking into a ——— and turning a ——— alongside a ———. I don't know anything about it, and there's nothing to say\". Many residents continued to avoid discussion of \"the stuff\" in ordinary conversation despite it being the reason for their town's existence.\n\nIn anticipation of the bombings, Groves had Henry DeWolf Smyth prepare a history for public consumption. \"Atomic Energy for Military Purposes\", better known as the \"Smyth Report\", was released to the public on 12 August 1945. Groves and Nichols presented Army–Navy \"E\" Awards to key contractors, whose involvement had hitherto been secret. Over 20 awards of the Presidential Medal for Merit were made to key contractors and scientists, including Bush and Oppenheimer. Military personnel received the Legion of Merit, including the commander of the Women's Army Corps detachment, Captain Arlene G. Scheidenhelm.\n\nAt Hanford, plutonium production fell off as Reactors B, D and F wore out, poisoned by fission products and swelling of the graphite moderator known as the Wigner effect. The swelling damaged the charging tubes where the uranium was irradiated to produce plutonium, rendering them unusable. In order to maintain the supply of polonium for the urchin initiators, production was curtailed and the oldest unit, B pile, was closed down so at least one reactor would be available in the future. Research continued, with DuPont and the Metallurgical Laboratory developing a redox solvent extraction process as an alternative plutonium extraction technique to the bismuth phosphate process, which left unspent uranium in a state from which it could not easily be recovered.\n\nBomb engineering was carried out by the Z Division, named for its director, Dr. Jerrold R. Zacharias from Los Alamos. Z Division was initially located at Wendover Field but moved to Oxnard Field, New Mexico, in September 1945 to be closer to Los Alamos. This marked the beginning of Sandia Base. Nearby Kirtland Field was used as a B-29 base for aircraft compatibility and drop tests. By October, all the staff and facilities at Wendover had been transferred to Sandia. As reservist officers were demobilized, they were replaced by about fifty hand-picked regular officers.\n\nNichols recommended that S-50 and the Alpha tracks at Y-12 be closed down. This was done in September. Although performing better than ever, the Alpha tracks could not compete with K-25 and the new K-27, which had commenced operation in January 1946. In December, the Y-12 plant was closed, thereby cutting the Tennessee Eastman payroll from 8,600 to 1,500 and saving $2 million a month.\nNowhere was demobilization more of a problem than at Los Alamos, where there was an exodus of talent. Much remained to be done. The bombs used on Hiroshima and Nagasaki were like laboratory pieces; work would be required to make them simpler, safer and more reliable. Implosion methods needed to be developed for uranium in place of the wasteful gun method, and composite uranium-plutonium cores were needed now that plutonium was in short supply because of the problems with the reactors. However, uncertainty about the future of the laboratory made it hard to induce people to stay. Oppenheimer returned to his job at the University of California and Groves appointed Norris Bradbury as an interim replacement. In fact, Bradbury would remain in the post for the next 25 years. Groves attempted to combat the dissatisfaction caused by the lack of amenities with a construction program that included an improved water supply, three hundred houses, and recreation facilities.\n\nTwo Fat Man–type detonations were conducted at Bikini Atoll in July 1946 as part of Operation Crossroads to investigate the effect of nuclear weapons on warships. Able was detonated on 1 July 1946. The more spectacular Baker was detonated underwater on 25 July 1946.\n\nAfter the bombings at Hiroshima and Nagasaki, a number of Manhattan Project physicists founded the \"Bulletin of the Atomic Scientists\", which began as an emergency action undertaken by scientists who saw urgent need for an immediate educational program about atomic weapons. In the face of the destructiveness of the new weapons and in anticipation of the nuclear arms race several project members including Bohr, Bush and Conant expressed the view that it was necessary to reach agreement on international control of nuclear research and atomic weapons. The Baruch Plan, unveiled in a speech to the newly formed United Nations Atomic Energy Commission (UNAEC) in June 1946, proposed the establishment of an international atomic development authority, but was not adopted.\n\nFollowing a domestic debate over the permanent management of the nuclear program, the United States Atomic Energy Commission (AEC) was created by the Atomic Energy Act of 1946 to take over the functions and assets of the Manhattan Project. It established civilian control over atomic development, and separated the development, production and control of atomic weapons from the military. Military aspects were taken over by the Armed Forces Special Weapons Project (AFSWP). Although the Manhattan Project ceased to exist on 31 December 1946, the Manhattan District was not abolished until 15 August 1947.\n\nThe project expenditure through 1 October 1945 was $1.845 billion, equivalent to less than nine days of wartime spending, and was $2.191 billion when the AEC assumed control on 1 January 1947. Total allocation was $2.4 billion. Over 90% of the cost was for building plants and producing the fissionable materials, and less than 10% for development and production of the weapons.\n\nA total of four weapons (the Trinity gadget, Little Boy, Fat Man, and an unused bomb) were produced by the end of 1945, making the average cost per bomb around $500 million in 1945 dollars. By comparison, the project's total cost by the end of 1945 was about 90% of the total spent on the production of US small arms (not including ammunition) and 34% of the total spent on US tanks during the same period. Overall, it was the second most expensive weapons project undertaken by the United States in World War II, behind only the design and production of the Boeing B-29 Superfortress.\n\nThe political and cultural impacts of the development of nuclear weapons were profound and far-reaching. William Laurence of \"The New York Times\", the first to use the phrase \"Atomic Age\", became the official correspondent for the Manhattan Project in spring 1945. In 1943 and 1944 he unsuccessfully attempted to persuade the Office of Censorship to permit writing about the explosive potential of uranium, and government officials felt that he had earned the right to report on the biggest secret of the war. Laurence witnessed both the Trinity test and the bombing of Nagasaki and wrote the official press releases prepared for them. He went on to write a series of articles extolling the virtues of the new weapon. His reporting before and after the bombings helped to spur public awareness of the potential of nuclear technology and motivated its development in the United States and the Soviet Union.\n\nThe wartime Manhattan Project left a legacy in the form of the network of national laboratories: the Lawrence Berkeley National Laboratory, Los Alamos National Laboratory, Oak Ridge National Laboratory, Argonne National Laboratory, and Ames Laboratory. Two more were established by Groves soon after the war, the Brookhaven National Laboratory at Upton, New York, and the Sandia National Laboratories at Albuquerque, New Mexico. Groves allocated $72 million to them for research activities in fiscal year 1946–1947. They would be in the vanguard of the kind of large-scale research that Alvin Weinberg, the director of the Oak Ridge National Laboratory, would call Big Science.\n\nThe Naval Research Laboratory had long been interested in the prospect of using nuclear power for warship propulsion, and sought to create its own nuclear project. In May 1946, Nimitz, now Chief of Naval Operations, decided that the Navy should instead work with the Manhattan Project. A group of naval officers were assigned to Oak Ridge, the most senior of whom was Captain Hyman G. Rickover, who became assistant director there. They immersed themselves in the study of nuclear energy, laying the foundations for a nuclear-powered navy. A similar group of Air Force personnel arrived at Oak Ridge in September 1946 with the aim of developing nuclear aircraft. Their Nuclear Energy for the Propulsion of Aircraft (NEPA) project ran into formidable technical difficulties, and was ultimately cancelled.\n\nThe ability of the new reactors to create radioactive isotopes in previously unheard-of quantities sparked a revolution in nuclear medicine in the immediate postwar years. Starting in mid-1946, Oak Ridge began distributing radioisotopes to hospitals and universities. Most of the orders were for iodine-131 and phosphorus-32, which were used in the diagnosis and treatment of cancer. In addition to medicine, isotopes were also used in biological, industrial and agricultural research.\n\nOn handing over control to the Atomic Energy Commission, Groves bid farewell to the people who had worked on the Manhattan Project:\n\nIn 2014, the United States Congress passed a law providing for a national park dedicated to the history of the Manhattan Project. The Manhattan Project National Historical Park was established on 10 November 2015.\n\n\n\n\n"}
{"id": "40133941", "url": "https://en.wikipedia.org/wiki?curid=40133941", "title": "New Horizons 2", "text": "New Horizons 2\n\nNew Horizons 2 (also New Horizons II, NHII, or NH2) was a proposed mission to the trans-Neptunian objects by NASA. It was conceived as a planetary flyby mission in 2002, based on the \"New Horizons\" spacecraft, which was in development at the time. In March 2005, the proposal was not selected for further development because of a shortage of plutonium-238 needed for the radioisotope thermoelectric generator (RTG). The \"New Horizons 2\" study was funded out of the New Frontiers program, and was delivered to the U.S. Congress in June 2005.\n\n\"New Horizons 2\" was included in the tentative budget for the New Frontiers program missions. In 2004 the United States Senate Appropriations Committee provided additional funding for \"New Horizons 2\", a new Kuiper belt mission. As early as 2004 there was a conference on how to make the most use of \"New Horizons 2\" Uranus flyby.\n\nCandidate targets included 47171 Lempo, a system that, like Pluto–Charon, contains multiple bodies. The mission plan for Lempo also included flybys of Jupiter and Uranus, and perhaps four Kuiper belt objects (KBO). There was a lot of flexibility: even without a gravity assist any KBO within 50 AU and a 20-year flight time was possible. A flyby of Neptune's Triton was also considered, with 66652 Borasisi as a potential follow on. was also considered to be visited, having a similar flight plan as Lempo.\nAn selection of some trans-Neptunium objects\n\n\n"}
{"id": "32547305", "url": "https://en.wikipedia.org/wiki?curid=32547305", "title": "Odor amplifier", "text": "Odor amplifier\n\nThe odor amplifier is a fictional invention that is described in the 1970 novel \"Principles of American Nuclear Chemistry: A Novel\", by mechanical engineer Thomas A. McMahon. In the novel, it is invented by a character modeled on the real-life theoretical physicist Richard Feynman.\n\nIt is described as a device that looks like a medicine dropper suspended above a beaker. Small drops of metallic mercury fall from the dropper into the beaker below.\nThe odor amplifier, as described in the novel, is not feasible in real life. The metallic mercury droplets are not exposed in flight for long enough to acquire any perceptible odors, and, if small enough, they tend to bounce off of the liquid surface and out of the beaker. Other attempts to have similar devices (see Related devices, below) have been successful. Any device which could identify an odor and then generate more of it would also be plausible, given that the odor molecules in question were volatile enough to be easily detectable. \n\n"}
{"id": "1234444", "url": "https://en.wikipedia.org/wiki?curid=1234444", "title": "Pratiwi Sudarmono", "text": "Pratiwi Sudarmono\n\nPratiwi Pujilestari Sudarmono (born 31 July 1952) is an Indonesian scientist. She is currently professor of microbiology at the University of Indonesia, Jakarta.\n\nPratiwi Sudarmono received a master's degree from the University of Indonesia in 1977, and the Ph.D. in Molecular Biology from the University of Osaka, Japan, in 1984. She then started her scientific career as WHO grantee researching the molecular biology of \"Salmonella typhi\". From 1994 to 2000, she was head of the Department of Microbiology of the Medical Faculty of the University of Indonesia. From 2001 to 2002, she was a scholar in the Fulbright New Century Scholars Program.\n\nIn October 1985, she was selected to take part in the NASA Space Shuttle mission STS-61-H as a Payload Specialist. Taufik Akbar was her backup on the mission. However, after the Challenger disaster the deployment of commercial satellites like the Indonesian Palapa B-3 planned for the STS-61-H mission was canceled, thus the mission never took place. The satellite was later launched with a Delta rocket.\n\n"}
{"id": "4012709", "url": "https://en.wikipedia.org/wiki?curid=4012709", "title": "Proteomyxa", "text": "Proteomyxa\n\nProteomyxa is a name given by E. Ray Lankester to a group of Sarcodina. This is an obsolete group.\n\nMany of the species are endoparasites in living cells, mostly of algae or fungi, but not exclusively. At least two species of Pseudospora have been taken for reproductive stages in the life history of their hosts—whence indeed the generic name. Plasmodiophora brassicae gives rise to the disease known as Hanburies or fingers and toes in Cruciferae; Lymphosporidium causes a virulent epidemic among the American brook trout (Salvelinus fontinalis). Archerina boltoni is remarkable for containing a pair of chlorophyll corpuscles in each cell; no nucleus has been made out, but the chlorophyll bodies divide previous to fission. It is a fresh-water form. The cells of this species form loose aggregates or filoplasmodia, like those of Mikrogromia or Leydenia.\n"}
{"id": "35207083", "url": "https://en.wikipedia.org/wiki?curid=35207083", "title": "Quality Meat Scotland", "text": "Quality Meat Scotland\n\nQuality Meat Scotland (QMS) is an executive non-departmental public body of the Scottish Government. It promotes the red meat sector and markets the Protected Geographical Indication Scotch Beef and Scotch Lamb brands.\n\nIt was set up in 1990 to provide assurance to industry and consumers that animals produced for the food chain met certain standards.\n\n"}
{"id": "48502966", "url": "https://en.wikipedia.org/wiki?curid=48502966", "title": "RIKEN Quantitative Biology Center", "text": "RIKEN Quantitative Biology Center\n\nThe Quantitative Biology Center (QBiC) is a Strategic Research Center of the Japanese national research and development institute, RIKEN. In November 2014, they succeeded in making a translucent mouse in order to see its internal organs more clearly.\n\nQBiC is a systems biology research center. The center is led by director Toshio Yanagida and is divided into three research cores.\n\nThe Cell Dynamics Research Core houses the Laboratory for Cell Polarity Regulation, led by Yasushi Okada. Okada reported the first visualization of Mitochondrial Derived Vesicles (MDV) from mitochondrial protrusions using ultrafast super-resolution fluorescence imaging with spinning disk confocal microscope optics. This core also includes Shuichi Onami's Laboratory for Developmental Dynamics creator of the Biological Dynamics Markup Language (BDML). The Onami lab hosts the Systems Science of Biological Dynamics (SSBD) database.\n\nThe Computational Biology Research Core is a user of the K computer and developer of the MDGRAPE-4 supercomputer.\n\nThe Cell Design Research Core houses the Laboratory for Synthetic Biology which reported the see-through mouse. This core is also notable for housing Yoshihiro Shimizu's Laboratory for Cell-Free Protein Synthesis, developer of the PURE cell free protein expression system. Yo Takana's Laboratory for Integrated Biodevice, which created a battery from the electric organ of a torpedo ray.\n"}
{"id": "36307450", "url": "https://en.wikipedia.org/wiki?curid=36307450", "title": "Reactive transport modeling in porous media", "text": "Reactive transport modeling in porous media\n\nReactive transport modeling in porous media refers to the creation of computer models integrating chemical reaction with transport of fluids through the Earth's crust. Such models predict the distribution in space and time of the chemical reactions that occur along a flowpath. Reactive transport modeling in general can refer to many other processes, including reactive flow of chemicals through tanks, reactors, or membranes; particles and species in the atmosphere; gases exiting a smokestack; and migrating magma.\n\nReactive transport models are constructed to understand the composition of natural waters; the origin of economic mineral deposits; the formation and dissolution of rocks and minerals in geologic formations in response to injection of industrial wastes, steam, or carbon dioxide; and the generation of acidic waters and leaching of metals from mine wastes. They are often relied upon to predict the migration of contaminant plumes; the mobility of radionuclides in waste repositories; and the biodegradation of chemicals in landfills. When applied to the study of contaminants in the environments, they are known as fate and transport models.\n\nModern reactive transport modeling has arisen from several separate schools of thought. Hydrologists primarily concerned with the physical nature of mass transport assumed relatively simple reaction formulations, such as linear distribution coefficients or linear decay terms, which could be added to the advection-dispersion equation. By assuming linear, equilibrium sorption, for example, the advection-dispersion equation can be modified by a simple retardation factor and solved analytically. Such analytical solutions are limited to relatively simple flow systems and reactions.\n\nGeochemical models, on the other hand, have been developed to provide thermodynamic descriptions of multicomponent systems without regard to transport. Reaction path models were created, for instance, to describe the sequence of chemical reactions resulting from chemical weathering or hydrothermal alteration in batch systems, in terms of the overall reaction progress. By adopting the reference frame of a packet of fluid and treating reaction progress as travel time (or distance along a flowpath), however, a batch reaction path model could be thought of as describing advective transport through an aquifer.\n\nThe most sophisticated multi-component reactive transport models considered both reaction and transport. Early studies developed the theoretical basis of reactive transport models, and the numerical tools necessary to solve them, and applied them to problems of reactive contaminant transport and flow through reacting hydrothermal systems.\n\nReactive transport models have found increased application in recent years with improvements in the power of personal computers and modeling software.\n\nReactive transport models couple a large number chemical reactions with mass transport. Certain applications, such as geothermal energy production and ore deposit modeling, require the additional calculation of heat transfer. In modeling carbon sequestration and hydraulic fracturing, moreover, it may be necessary to describe rock deformation resulting from mineral growth or abnormally high fluid pressure. Description of transport through the unsaturated zone and multiphase flow modeling, as applied to transport of petroleum and natural gas; non-aqueous phase liquids (DNAPL or LNAPL); and supercritical carbon dioxide requires increasingly complex models which are prone to considerable uncertainty.\n\nIn many cases the processes simulated in reactive transport models are highly related. Mineral dissolution and precipitation, for example, can affect the porosity and permeability of the domain, which in turn affect the flow field and groundwater velocity. Heat transport greatly affects the viscosity of water and its ability to flow. Below are many of the physical and chemical processes which can be simulated with reactive transport models.\n\nGeochemical reactions:\n\nMass Transport:\n\nHeat transport:\n\nMedium deformation:\n\nSome of the simplest reactive transport problems can be solved analytically. Where equilibrium sorption is described by a linear distribution coefficient, for example, the sorbing solute's velocity is retarded relative to that of a nonreactive tracer; the relative velocities can be described with a retardation factor. Analytical solutions are exact solutions of the governing equations. \n\nComplex reactive transport problems are more commonly solved numerically. In this case, the governing equations are approximated so that they can be solved by computer algorithms. The governing equations, including both reaction and transport terms, can be solved simultaneously using a one-step or global implicit simulator. This technique is straightforward conceptually, but computationally very difficult.\n\nInstead of solving all the relevant equations together, the transport and chemical reaction equations can be solved separately. Operator splitting, as this technique is known, uses appropriate numerical techniques to solve the reaction and transport equations at each time step. Various methods exist, including the sequential non-iterative approach (SNIA), Strang splitting, and sequential iterative approach (SIA). Since the reaction and transport terms are handled separately, separate programs for batch reaction and transport can be linked together. Cross-linkable re-entrant software objects designed for this purpose readily enable construction of reactive transport models of any flow configuration.\n\nReactive transport modeling requires input from numerous fields, including hydrology, geochemistry and biogeochemistry, microbiology, soil physics, and fluid dynamics. The numerical formulation and solution of reactive transport problems can be especially difficult due to errors arising in the coupling process, beyond those inherent to the individual processes. Valocchi and Malmstead (1992), for example, reported on the potential errors arising from the operator splitting technique.\n\nEven in the absence of numerical difficulties, the general lack of knowledge available to practitioners creates uncertainty. Field sites are typically heterogeneous, both physically and chemically, and sampling is often sparse. The prevailing assumption of Fickian dispersion is often inadequate. Equilibrium constants and kinetic rate laws for relevant reactions are often poorly known. The complexity of many processes requires expertise in one or more of the above mentioned fields. Many processes, such as long-term nuclear waste storage, cannot be experimentally verified; reactive transport problems can only attempt to predict such long-term behavior. The current descriptions of multi-phase flow and mechanical deformation processes are still being developed.\n\n\n\n"}
{"id": "25986339", "url": "https://en.wikipedia.org/wiki?curid=25986339", "title": "Sheila Jasanoff", "text": "Sheila Jasanoff\n\nSheila Sen Jasanoff is an Indian American academic and significant contributor to the field of Science and Technology Studies.\n\nBorn in India, Jasanoff attended Radcliffe College, where she studied mathematics as an undergraduate, receiving her bachelor's degree in 1964. She then studied linguistics, receiving her M.A. at the University of Bonn (then part of West Germany). She returned to Harvard to complete a Ph.D. in linguistics in 1973, and a J.D. at Harvard Law School in 1976. She practiced environmental law in Boston from 1976 to 1978. She and her husband then accepted positions at Cornell University, where she became a pioneer in the newly emerging field of Science and Technology Studies. In 1998, Jasanoff joined the John F. Kennedy School of Government at Harvard University as a professor of public policy. In 2002, she became Pforzheimer Professor of Science and Technology Studies.\n\nJasanoff founded and directs the Program on Science, Technology, and Society at the John F. Kennedy School of Government at Harvard University. Her research focuses on science and the state in contemporary democratic societies. Her work is relevant to science & technology studies, comparative politics, law and society, political and legal anthropology, sociology and policy analysis. Jasanoff's research has considerable empirical breadth, spanning the United States, the United Kingdom, Germany, the European Union, and India, as well as emerging global regimes in areas such as climate and biotechnology.\n\nOne line of Jasanoff's work demonstrates how the political culture of different democratic societies influences how they assess evidence and expertise in policymaking. Her first book (with Brickman and Ilgen), \"Controlling Chemicals\" (1985), examines the regulation of toxic substances in the United States, Germany, and the United Kingdom. The book showed how the routines of decision making in these countries reflected different conceptions of what counts as evidence and of how expertise should operate in a policy context. In \"Designs on Nature: Science and Democracy in Europe and the United States\" (2005), she has shown how different societies employ different modes of public reasoning when making decisions involving science and technology. These differences, which in part reflect distinct \"civic epistemologies,\" are deeply embedded in institutions and shape how policy issues are framed and processed by the bureaucratic machinery of modern states.\n\nJasanoff has also contributed to scholarship on the interaction of science and law. \"Science at the Bar\" (1995), for example, reached beyond the prevailing diagnoses of structural incompatibilities between science and law to explore how these socially embedded institutions interact and, to a certain extent, mutually constitute each other. The concept of regulatory science, conducted for the purposes of meeting legally mandated standards, and the \"boundary\" drawing activities of science advisory committees are analyzed in \"The Fifth Branch\" (1990). More recently, she has explored the \"rise of the statistical victim\" in toxic torts, as the law with its individualistic orientation has increasingly encountered, and sought ways to accommodate, the statistical vision of such fields as epidemiology. In her work on science and law, as well as her research on science in the state, she takes an approach that links ideas from constitutional law, political theory, and science studies to consider the \"constitutional\" role of science in modern democratic states.\n\nJasanoff has considered the politics of science not only in a comparative but also in a global context. Examples include her work on the transnational aspects of the Bhopal disaster (\"Learning from Disaster\" 1994); her research on the formation and politics of global scientific advisory bodies such as the Intergovernmental Panel on Climate Change; and her research on national and global environmental movements (e.g., Earthy Politics, 2004).\n\nJasanoff also has contributed to building Science and Technology Studies as a field. Prior to moving to Harvard, she was the founding chair of the Department of Science & Technology Studies at Cornell University. She is also the founder of the Science & Democracy Network, a group of scholars interested in the study of science and the state in democratic societies that has met annually since 2002. Her research has been recognized with many awards, including the Bernal Prize from the Society for Social Studies of Science, a Guggenheim Fellowship, and the Albert O. Hirschman Prize from the Social Science Research Council.\n\nShe is married to Jay H. Jasanoff, and has two children, Maya Jasanoff, who is a professor in the Department of History at Harvard, and Alan Jasanoff, who is a professor in the Department of Biological Engineering at MIT\n\n"}
{"id": "10328644", "url": "https://en.wikipedia.org/wiki?curid=10328644", "title": "Shell grit", "text": "Shell grit\n\nShell grit is coarsely ground or broken seashells. It is used, among other things, by birds as a source of calcium for egg shell production, and to aid digestion.\n\nOther uses include protecting plants from slugs or snails and in aquariums.\n\n"}
{"id": "29838000", "url": "https://en.wikipedia.org/wiki?curid=29838000", "title": "Space Shuttle retirement", "text": "Space Shuttle retirement\n\nThe retirement of NASA's Space Shuttle fleet took place from March to July 2011. \"Discovery\" was the first of the three active space shuttles to be retired, completing its final mission on March 9, 2011; \"Endeavour\" did so on June 1. The final shuttle mission was completed with the landing of \"Atlantis\" on July 21, 2011, closing the 30-year Space Shuttle program.\n\nThe Shuttle was presented to the public in 1972 as a \"space truck\" which would, among other things, be used to build a United States space station in low earth orbit in the early 1990s and then be replaced by a new vehicle. When the concept of the U.S. space station evolved into that of the International Space Station, which suffered from long delays and design changes before it could be completed, the service life of the Space Shuttle was extended several times until 2011 when it was finally retired.\n\nIn 2010 the Shuttle was formally scheduled for retirement with \"Atlantis\" being taken out of service first after STS-132 in May of that year, but the program was once again extended when the two final planned missions were delayed until 2011. Later, one additional mission was added for \"Atlantis\" for July 2011, extending the program further. Counter-proposals to the shuttle's retirement were considered by Congress and the prime contractor United Space Alliance as late as spring 2010.\n\nHardware developed for the Space Shuttle met various ends with conclusion of the program, including donation, disuse and/or disposal, or reuse. An example of reuse, is that one of the three Multi-Purpose Logistics Module (MPLM) was converted to a permanent module for the International Space Station.\n\nOn April 12, 2011, NASA announced a selection of locations for the remaining Shuttle orbiters:\n<nowiki>*</nowiki> Prior to its move to New York, \"Enterprise\" was displayed at the Udvar-Hazy Center, Smithsonian Institution's National Air and Space Museum, Chantilly, Virginia, where \"Discovery\" has taken its place.\n\nMuseums and other facilities not selected to receive an orbiter were disappointed. Elected officials representing Houston, Texas, location of the Johnson Space Center; and Dayton, Ohio, location of National Museum of the United States Air Force called for Congressional investigations into the selection process, though no such action was taken. While local and Congressional politicians in Texas questioned if partisan politics played a role in the selection, former JSC Director Wayne Hale wrote \"Houston didn't get an orbiter because Houston didn't deserve it\" pointing to weak support from area politicians, media and residents, describing a \"sense of entitlement\". Chicago media questioned the decision not to include the Adler Planetarium in the list of facilities receiving orbiters, pointing to Chicago's 3rd-largest population in the United States. The chair of the NASA committee that made the selections pointed to the guidance from Congress that the orbiters go to facilities where the most people could see them, and the ties to the space program of Southern California (home to Edwards Air Force Base, where nearly half of shuttle flights have ended and home to the plants which manufactured the orbiters and the Space Shuttle Main Engines), the Smithsonian (curator of the nation's air and space artifacts), the Kennedy Space Center Visitor Complex (where all shuttle launches have originated, and a large tourist draw) and the Intrepid Sea-Air-Space Museum (which served as the recovery ship for Project Mercury and Project Gemini).\n\nIn August 2011 the NASA Inspector General released an audit of the display selection process; it highlighted issues which led to the final decision. The Museum of Flight in Seattle, Washington, March Field Air Museum, Riverside, California, Evergreen Aviation and Space Museum, McMinnville, Oregon, National Museum of the U.S. Air Force, Dayton, Ohio, San Diego Air and Space Museum, San Diego, Space Center Houston, Houston, Texas, Tulsa Air and Space Museum & Planetarium, Tulsa, Oklahoma and U.S. Space and Rocket Center, Huntsville, Alabama scored poorly on international access. Additionally Brazos Valley Museum of Natural History and the Bush Library at Texas A&M, in College Station, Texas scored poorly on museum attendance, regional population and was the only facility found to pose a significant risk in transporting an orbiter there. Overall, the California Science Center scored first and Brazos Valley Museum of Natural History scored last. The two most controversial locations which were not awarded an orbiter, Space Center Houston and National Museum of the U.S. Air Force, finished 2nd to last and near the middle of the list respectively. The report noted a scoring error, which if corrected would have placed the National Museum of the U.S. Air Force in a tie with the Intrepid Museum and Kennedy Visitor Complex (just below the California Science Center), although due to funding concerns the same decisions would have been made.\n\nThe Museum of Flight in Seattle, Washington was not selected to receive an orbiter but instead received the three–story Full Fuselage Trainer from the Space Vehicle Mockup Facility at Johnson Space Center in Houston, Texas. Museum officials, though disappointed, were able to allow the public to go inside the trainer, something not possible with an actual orbiter.\n\nIn addition to the challenge of transporting the large vehicles to the display site, placing the units on permanent display required considerable effort and cost. An article in the February 2012 issue of Smithsonian Magazine discussed the work performed on \"Discovery\". It involved removing the three main engines (they were slated to be reused on NASA's new Space Launch System); the windows were given to project engineers for analysis of how materials and systems fared after repeated space exposure; the communications modules were removed due to national–security concerns; and hazardous materials such as traces of propellants were thoroughly flushed from the plumbing. The total cost of preparation and delivery via a modified Boeing 747 was estimated at $26.5 million in 2011 dollars.\n\n\nNASA ran a program to donate thermal protection system tiles to schools and universities for $23.40 USD each (the fee for S&H). About 7000 tiles were available on a first-come, first-served basis, but limited to one per institution. Each orbiter incorporated over 24,000 tiles.\n\nAbout 42 reusable SSMEs (Pratt & Whitney RS-25/26) have been part of the STS program, with three used per orbiter per mission. The decision was made to retain all engines with plans to make use of them in future launch vehicles.\n\nWorn out engine nozzles are typically considered scrap, although nine nozzles were refurbished for display on the donated orbiters, so the actual engines can be retained by NASA.\n\nThree Shuttle arms were used by NASA; the arms of both \"Discovery\" and \"Atlantis\" will be left in place for their museum display. \"Endeavour\"'s arm is to be removed from the orbiter for separate display in Canada. The OBSS extension of \"Endeavour\"s arm was left on the International Space Station, for use with the station's robotic arm.\n\nIn December 2010, as NASA prepared for the STS program ending, an audit by the NASA Office of Inspector General (OIG) found that information technology had been sold or prepared for sale that still contained sensitive information. NASA OIG recommended NASA be more careful in the future.\n\nThe twin pads originally built for the Apollo program are now inactive. LC-39B was deactivated first on January 1, 2007. Three lightning towers were added to the pad and it was temporarily \"re-activated\" in April 2009 when \"Endeavour\" was placed on standby to rescue the STS-125 crew (the STS-125 mission was the last to visit the Hubble Space Telescope, which meant that the ISS was out of range) if needed; \"Endeavour\" was then moved over to LC-39A for STS-126. In October 2009 the prototype Ares I-X rocket was launched from 39B. The pad was then permanently deactivated and has since been dismantled and is being modified for the Space Launch System program, and possibly other launch vehicles. Like the Apollo structures before them, the shuttle structures were scrapped. 39A was deactivated in July 2011 after STS-135 was launched. On January 16, 2013, it was erroneously reported that NASA planned to abandon the pad, but the actual plan is to, like pad B, convert it for other rockets without dismantling it. If NASA did plan to permanently decommission the pads, they would have to restore them to their original Apollo-era appearance, as both pads are on the National Historic Register.\nIn December 2013 NASA announced that SpaceX would be the new tenant of pad 39A. SpaceX has since converted the pad to launch Falcon Heavy and manned Crew Dragon Falcon 9 flights. Following the destruction of Space Launch Complex 40 in an on-pad explosion in September 2016, SpaceX had to move all east coast launches to 39A while SLC-40 was being rebuilt. The first launch, Dragon resupply vehicle carried by a Falcon 9, occurred February 12, 2017. This flight was the first unmanned launch from Complex 39 since Skylab was launched in 1973. Once SLC-40 is back in action, SpaceX will finish modifying the pad for Falcon Heavy. Due to SLC-40s destruction, 39A had to be rushed into service, and activities such as dismantling the RSS were put on hold.\n\nAfter STS-135, the VAB was used as a storage shed for the decommissioned shuttles before they were sent to museums. High Bay 3 is now being gutted of all equipment and given upgraded platforms, to support the Space Launch System and potentially the SpaceX Falcon Heavy as well as other vehicles. After the shuttle was decommissioned, NASA opened the VAB for public tours, which ended on February 11, 2014 as NASA prepares the VAB for future launch vehicles.\n\nThe twin launch platforms are currently being modified for the Space Launch System with a large tower resembling those used in the Apollo program. Work is expected to be complete by 2016.\n\nThe Crawler-Transporters were used as the mobile part of the pad with the shuttles; the two vehicles were deactivated and are being upgraded for the SLS program. The crawlerways used for transporting launch vehicles from the VAB to the twin pads of KSC are also being extensively renovated for the SLS program.\n\nUsed to mate the shuttle on the Shuttle Carrier Aircraft, the Mate-Demate Devices were dismantled and scrapped.\n\nTwo modified Boeing 747s were used to fly the shuttles back to KSC when they landed at Edwards AFB. N911NA was retired on February 8, 2012 and is now a parts hulk for the Stratospheric Observatory for Infrared Astronomy. Beginning in September 2014, N911NA was loaned out to the Joe Davies Heritage Airpark, in Palmdale, CA, where it is on outdoor display next to a B-52. The other aircraft, N905NA was used to send \"Discovery\", \"Endeavour\" and \"Enterprise\" to their museums and in September 2012 was found to have few parts for SOFIA. It is currently a museum piece at the Johnson Space Center.\n\nUsed to retrieve the SRBs, MV \"Liberty Star\" and \"Freedom Star\" are now separated. \"Liberty Star\" was renamed as TV \"Kings Pointer\" and was transferred to the Merchant Marine Academy in New York for use as a training vessel. It will remain on call in case NASA needs it for further missions. \"Freedom Star\" was transferred to the James River Reserve Fleet on September 28, 2012 and placed under ownership of the United States Department of Transportation.\n\nThe buildings used to process the shuttles after each mission were decommissioned. OPF-1 was leased to Boeing in January 2014 for processing the X-37B spaceplane while OPF-3 is also used by Boeing for the manufacture and testing of the CST-100 spacecraft. OPF-2 also currently remains under lease by Boeing for its X-37B spaceplane.\n\nThe runway at KSC is currently used as a normal runway for the center and neighboring Cape Canaveral's daily operations. It may be used to land the X-37B and Sierra Nevada Dream Chaser spaceplanes.\nThe SLF received its first landing since \"Atlantis\" in June 2017 when the USAF X-37B landed on it at the end of almost 2 years in orbit.\n\nIn the 1980s, a planned successor to STS was called \"Shuttle II\", which encompassed a number of different ideas and was influenced by the Challenger disaster. At one point before retirement, extension of the Space Shuttle program for an additional five years while a replacement could be developed, was considered by the U.S. government. Some programs to provide access to space after the shuttle were the Lockheed Martin X-33, VentureStar, the Orbital Space Plane Program, and Ares I launcher (part of, and cancelled with Constellation). Some program from the 1980s were the NASA \"Advanced Manned Launch System\" program as well as the \"Future Space Transportation System\" program. There was a number of proposals for space access systems in the 1970s also, such as the Rockwell \"Star-raker\" Star-raker was a large single-stage to orbit design that used both rockets and ramjet for propulision. It was a contemporary to the Boeing \"Reusable Aerodynamic Space Vehicle\", which an all-rocket propulsion single-stage to orbit design.\n\nFor comparison to an earlier retirement, when the Saturn IB was last flown in 1975 for the Apollo-Soyuz Test Project, the Shuttle development program was already well underway. However, the Shuttle did not fly until 1981, which left a six-year gap in U.S. manned spaceflight. Because of this and other reasons, in particular, higher than expected Solar activity that caused Skylab's orbit to decay faster than hoped, the U.S. space station Skylab burned up in the atmosphere.\n\nThe Ares I was going to be NASA's manned spacecraft after STS, with Congress attempting to accelerate its development so it would be ready as early as 2016 for the ISS, in addition they attempted to delay retirement of the shuttle to reduce the time gap. However, Ares I was cancelled along with the rest of Constellation in 2010. The successor to STS with the cancellation would be combination of yet-to-be developed commercial crew spacecraft and the Orion plus the SLS\nFollowing the Space Shuttle \"Columbia\" disaster, in early 2003 President George W. Bush, announced his Vision for Space Exploration which called for the completion of the American portion of the International Space Station by 2010 (due to delays this would not happen until 2011), the retirement of the Space Shuttle fleet following its completion, to return to the moon by 2020 and one day to Mars. A new vehicle would need to be developed, it eventually was named the Orion spacecraft, a six-person variant would have serviced the ISS and a four-person variant would have traveled to the Moon. The Ares I would have launched Orion, and the Ares V heavy-lift vehicle (HLV) would have launched all other hardware. The Altair lunar lander would have landed crew and cargo onto the moon. The Constellation program experienced many cost overruns and schedule delays, and was openly criticized by the subsequent U.S. President, Barack Obama.\n\nIn February 2010, the Obama administration proposed eliminating public funds for the Constellation program and shifting greater responsibility of servicing the ISS to private companies. During a speech at the Kennedy Space Center on April 15, 2010, President Obama proposed the design selection of the new HLV that would replace the Ares-V but would not occur until 2015. The U.S. Congress drafted the NASA Authorization Act of 2010 and President Obama signed it into law on October 11 of that year. The authorization act officially cancelled the Constellation program.\n\nThe combination of Ares I and Orion was predicted to cost about 50 billion dollars. One of the issues with Ares I was the criticism of the second stage, which the post-cancellation Liberty proposal attempted to address by using a second stage from an Ariane 5. The Liberty proposal applied for but was not chosen for commercial crew. The other ongoing complaint was that it made more sense to make a man-rated version of the Atlas or Delta. The first crewed flight for Ares I was scheduled for March 2015, and one of its priorities was crew safety. One reason for the emphasis on safety was that it was envisioned in the aftermath of the Colombia disaster.\n\nU.S. astronauts have continued to access the ISS aboard the Russian Soyuz spacecraft. The Soyuz was chosen as the ISS lifeboat during the development of the International Space Station. The first NASA astronaut to launch on a Soyuz rocket was Norman Thagard, as part of the Shuttle-Mir program. Launching on March 14, 1995 on Soyuz TM-21, he visited the Mir Space Station however he returned to Earth on the Space Shuttle mission STS-71. The start of regular use of the Soyuz began as part of the international space station program, with William Shepherd launching on Soyuz TM-31 in October 2000. NASA has continued to take regular flights in the following two decades. NASA is currently contracted to use Soyuz seats until at least 2018.\n\nThe consideration of Soyuz as a lifeboat began in the aftermath of the dissolution of the Soviet Union. Russia proposed using the Soyuz as a life boat for what was still Space Station Freedom in late 1991, leading to further analysis of this concept in the early 1990s. One of the milestones was in 1992, when after three months of negotiations the heads of the two Space Agencies agreed to study applications of the Soyuz spacecraft.\n\nIn the course of a few decades NASA astronauts have flown on the following Soyuz versions:\n\nNASA also purchased several space modules from Russia including Spektr, Docking Module (Mir), Priroda, and Zarya.\nThe NASA Authorization Act of 2010 required a new heavy–lift vehicle design to be chosen within 90 days of its passing. The authorization act called this new HLV the Space Launch System (SLS). The Orion spacecraft was left virtually unchanged from its previous design. The Space Launch System will launch both Orion and other necessary hardware. The SLS is to be upgraded over time with more powerful versions. The initial version of SLS will be capable of lifting 70 metric tonnes into LEO. It is then planned to be upgraded in various ways to lift 105 tonnes, and then, eventually, 130 tonnes.\n\nExploration Flight Test 1 (EFT-1), an unmanned test flight of Orion's crew module, launched on December 5, 2014 on a Delta IV Heavy rocket. Exploration Mission-1 (EM-1) is the unmanned initial launch of SLS, planned for December 2019. The first manned flight of Orion and SLS, Exploration Mission-2 (EM-2) is to launch June 2022; it is a 10- to 14-day mission planned to place a crew of four into Lunar orbit. As of 2017, the destination for EM-3 is the Deep Space Gateway.\n\nThe ISS is planned to be funded until at least 2020. There has been discussion to extend it to 2028 or beyond. Until another U.S. crew vehicle is ready, crews will access the ISS exclusively aboard the Russian Soyuz spacecraft. The Soyuz was chosen as the ISS lifeboat during the development of the International Space Station, and has been one of the space taxis used by the international participants to this program. A Soyuz took Expedition 1, which included one U.S. astronaut in the year 2000. Previously the United States and Russia had collaborated on extended the Mir space station with the Shuttle-Mir program in the 1990s.\n\nAlthough the Orion spacecraft is oriented towards deep-space missions such as NEO visitation, it can also be used retrieve crew or supplies from the ISS if that task is needed once the spacecraft is operational. However, it is expected that the Commercial Crew Program (CCP) will produce a functioning manned space vehicle as early as 2018, providing an alternative to Orion or Soyuz. The delay is longer than expected because the Ares I was cancelled in 2010, leaving little time before the STS retired for something new to be ready for flight. U.S. Congress was aware a spaceflight gap could occur and accelerated funding in 2008 and 2009 in preparation for the retirement of the Shuttle. At that time the first crewed flight of the planned Ares I launcher would not have occurred until 2015, and its first use at ISS until 2016. Another option that has been analyzed is to adapt Orion to a man-rated heavy launch vehicle like the Delta IV Heavy. (see also Evolved Expendable Launch Vehicle) Another spacecraft evaluated by NASA, and also for commercial crew, was the ATK Liberty rocket, which looked similar to Ares I and was based on Space Shuttle (SRB) and Ariane 5 (core stage) components.\n\nThe Commercial Orbital Transportation Services (COTS) development program began in 2006 with the purpose of creating commercially operated automated cargo spacecraft to service the ISS. The program is a fixed–price milestone-based development program, meaning that each company that received a funded award had to have a list of milestones with a dollar value attached to them that they would not receive until after achieving the milestone. Private companies are also required to have some \"skin in the game\" which refers to raising additional private investment for their proposal.\n\nOn 23 December 2008, NASA awarded Commercial Resupply Services contracts to SpaceX and Orbital Sciences Corporation (later, this company and Alliant Techsystems become Orbital ATK). SpaceX will use its Falcon 9 rocket and Dragon spacecraft. Orbital Sciences will use its Antares rocket and Cygnus spacecraft. The first Dragon resupply mission occurred in May 2012. The first Cygnus resupply mission occurred in September 2013. The CRS program provides for all the projected U.S. cargo-transportation needs to the ISS, with the exception of a few vehicle–specific payloads to be delivered on the European ATV and the Japanese HTV.\n\nThe Commercial Crew Program (CCP) was initiated in 2010 with the purpose of creating commercially operated crew vehicles capable of delivering at least four astronauts to the ISS, staying docked for 180 days and then returning them to Earth. Like COTS, CCP is a fixed–price milestone-based developmental program that requires some private investment.\n\nIn the first phase of the program, NASA provided a total of $50 million divided among five U.S. companies, intended to foster research and development into human spaceflight concepts and technologies in the private sector. In 2011, during the second phase of the program, NASA provided $270 million divided among four companies. During the third phase of the program, NASA provided $1.1 billion divided among three companies. This phase of the CCP was expected to last from 3 June 2012 to 31 May 2014. The winners of that round were SpaceX's DragonRider (derived from the Dragon cargo vehicle), Boeing's CST-100 and Sierra Nevada's Dream Chaser. The United Launch Alliance is working on human-rating their Atlas V rocket as part of the latter two proposals. NASA wants to have two Commercial Crew vehicles in-service. These spacecraft are expected to begin delivering crew in 2018 or 2019.\n\n\n"}
{"id": "39590024", "url": "https://en.wikipedia.org/wiki?curid=39590024", "title": "Tautology (language)", "text": "Tautology (language)\n\nIn literary criticism and rhetoric, a tautology is a statement which repeats the same idea, using near-synonymous morphemes, words, or phrases, that is, \"saying the same thing twice\". Tautology and pleonasm are not consistently differentiated in the literature.\n\nLike pleonasm, it is often considered a fault of style when unintentional. On the other hand, an intentional repetition may be an effective way to emphasize a thought, or help the listener or reader understand a point. \n\nSometimes logical tautologies like \"Boys will be boys\" are conflated with language tautologies, but in general, a rhetorical tautology is not inherently true.\n\nThe word was coined in Hellenistic Greek from ταὐτός (\"the same\") plus λόγος (\"word/idea\"), and transmitted through 3rd-century Latin \"tautologia\" and French \"tautologie\". It first appears in English in the 16th century. The use in the term logical tautology was introduced in English by Wittgenstein in 1919, perhaps following Auguste Comte's usage in 1835.\n\n\nIntentional repetition of meaning intends to amplify or emphasize a particular, usually significant, fact about what is being discussed. For example, a gift is, by definition, free of charge; using the phrase \"free gift\" might emphasize that there are no hidden conditions or fine print, be it the expectation of money or reciprocation, or that the gift is being given by volition.\n\nThis is related to the rhetorical device of hendiadys, where one concept is expressed through the use of two descriptive words or phrases. For example, \"goblets and gold\" meaning wealth, or \"this day and age\" meaning the present time. Superficially these expressions may seem tautological, but they are stylistically sound because the repeated meaning is just a way to emphasize the same idea.\n\nThe use of tautologies is, however, usually unintentional. For example, the phrases \"mental telepathy\", \"planned conspiracies\", and \"small dwarfs\" imply that there are such things as \"physical telepathy, spontaneous conspiracies, and giant dwarfs.\")\n\nParallelism is not tautology, but rather a particular stylistic device. Much Old Testament poetry is based on parallelism: the same thing said twice, but in slightly different ways (Fowler puts it as pleonasm). However, modern biblical study emphasizes that there are subtle distinctions and developments between the two lines, such that they are usually not truly the \"same thing.\" Parallelism can be found wherever there is poetry in the Bible: Psalms, the Books of the Prophets, and in other areas as well.\n\n"}
{"id": "12615832", "url": "https://en.wikipedia.org/wiki?curid=12615832", "title": "The Brain That Changes Itself", "text": "The Brain That Changes Itself\n\nThe Brain That Changes Itself: Stories of Personal Triumph from the Frontiers of Brain Science is a book on neuroplasticity by psychiatrist and psychoanalyst Norman Doidge.\n\n\"The New York Times\" gave a mostly positive review of the book.\n\nIn contrast 'The International Journal of Psychoanalysis' published a negative book review essay critical of Doidge's writings. The review claims that neuroscience is irrelevant to the study of psychoanalysis.\n\n\n"}
{"id": "19512102", "url": "https://en.wikipedia.org/wiki?curid=19512102", "title": "Toddler's fracture", "text": "Toddler's fracture\n\nToddler's fractures or childhood accidental spiral tibial (CAST) fractures are bone fractures of the distal (lower) part of the shin bone (tibia) in toddlers (aged 9 months-3 years) and other young children (less than 8 years). The fracture is found in the distal two thirds of the tibia in 95% of cases, is undisplaced and has a spiral pattern. It occurs after low-energy trauma, sometimes with a rotational component.\n\nThe proposed mechanism involves shear stress and lack of displacement due to the periosteum that is relatively strong compared to the elastic bone in young children.\nTypical symptoms include pain, refusing to walk or bear weight and limping -bruising and deformity are absent. On clinical examination, there can be warmth and swelling over the fracture area, as well as pain on bending the foot upwards (dorsiflexion). The initial radiographical images may be inconspicuous (a faint oblique line) and often even completely normal. After 1–2 weeks however, callus formation develops. The condition can be mistaken for osteomyelitis, transient synovitis or even child abuse. Contrary to CAST fractures, non-accidental injury typically affect the upper two-thirds or midshaft of the tibia.\n\nOther possible fractures in this area, occurring in the cuboid, calcaneus, and fibula, can be associated or can be mistaken for a toddler's fracture. In some cases, an internal oblique radiography and radionuclide imaging can add information to anterior-posterior and lateral views. However, since treatment can also be initiated in the absence of abnormalities, this appears to have little value in most cases. It could be useful in special cases such as children with fever, those without a clear trauma or those in which the diagnosis remains unclear. Recently, ultrasound has been suggested as a helpful diagnostic tool.\n\nTreatment consist of a long leg orthopedic cast for several weeks.\n\nThe condition was initially recognised by Dunbar and co-workers in 1964. A new terminology has been proposed, which defines toddler's fracture as a subset of childhood accidental spiral tibial (CAST) fractures.\n"}
{"id": "46414597", "url": "https://en.wikipedia.org/wiki?curid=46414597", "title": "Varian Rule", "text": "Varian Rule\n\nThe Varian Rule holds that \"A simple way to forecast the future is to look at what rich people have today; middle-income people will have something equivalent in 10 years, and poor people will have it in an additional decade.\" It is attributed to Google’s chief economist Hal Varian. Andrew McAfee first called it \"the Varian Rule\" in the \"Financial Times\". An alternative interpretation put forth by The Guardian writer Evgeny Morozov is that \"Luxury is already here – it’s just not very evenly distributed.\"\n\nSome notable historical examples include anti-lock braking systems, electronic stability control and airbags which first appeared on high-end luxury vehicles \nbefore becoming commoditized on more mainstream automobiles.\n\nA more recent example is the Apple watch which is initially offered as a luxury item, but if the Varian Rule holds, it will be more accessible in the near future according to Paul Krugman, Nobel Laureate, and blogger and columnist for \"The New York Times\". But Jay Stanley, Senior Policy Analyst, of the American Civil Liberties Union Speech, Privacy & Technology Project disputes this conclusion based on personal privacy concerns.\n\n"}
{"id": "10042977", "url": "https://en.wikipedia.org/wiki?curid=10042977", "title": "Wind engineering", "text": "Wind engineering\n\nWind engineering is a subsets of mechanical engineering, structural engineering, meteorology, and applied physics to analyze the effects of wind in the natural and the built environment and studies the possible damage, inconvenience or benefits which may result from wind. In the field of engineering it includes strong winds, which may cause discomfort, as well as extreme winds, such as in a tornado, hurricane or heavy storm, which may cause widespread destruction. In the fields of wind energy and air pollution it also includes low and moderate winds as these are relevant to electricity production resp. dispersion of contaminants.\n\nWind engineering draws upon meteorology, fluid dynamics, mechanics, geographic information systems and a number of specialist engineering disciplines including aerodynamics, and structural dynamics. The tools used include atmospheric models, atmospheric boundary layer wind tunnels, and computational fluid dynamics models.\n\nWind engineering involves, among other topics:\n\nWind engineering may be considered by structural engineers to be closely related to earthquake engineering and explosion protection.\n\nSome sports stadiums such as Candlestick Park and Arthur Ashe Stadium are known for their strong, sometimes swirly winds, which affect the playing conditions.\n\nWind engineering as a separate discipline can be traced to the UK in the 1960s, when informal meetings were held at the National Physical Laboratory, the Building Research Establishment and elsewhere.\n\nThe design of buildings must account for wind loads, and these are affected by wind shear.\n\nFor engineering purposes, a power law wind-speed profile may be defined as follows:\n\nwhere:\n\nTypically, buildings are designed to resist a strong wind with a very long return period, such as 50 years or more. The design wind speed is determined from historical records using extreme value theory to predict future extreme wind speeds.\n\nThe advent of high rise tower blocks led to concerns regarding the wind nuisance caused by these buildings to pedestrians in their vicinity.\n\nA number of wind comfort and wind danger criteria were developed from 1971, based on different pedestrian activities such as:\nOther criteria classified a wind environment as completely unacceptable or dangerous.\n\nBuilding geometries consisting of one and two rectangular buildings have a number of well-known effects:\n\nFor more complex geometries, pedestrian wind comfort studies are required. These can use an appropriately scaled model in a boundary layer wind tunnel, or more recently there has been increased use of Computational Fluid Dynamics (CFD) techniques. The pedestrian level wind speeds for a given exceedance probability are calculated to allow for regional wind speeds statistics.\n\nThe vertical wind profile used in these studies varies according to the terrain in the vicinity of the buildings (which is may differ by wind direction), and is often grouped in categories such as:\n\nWind turbines are affected by wind shear. Vertical wind-speed profiles result in different wind speeds at the blades nearest to the ground level compared to those at the top of blade travel, and this in turn affects the turbine operation. The wind gradient can create a large bending moment in the shaft of a two bladed turbine when the blades are vertical. The reduced wind gradient over water means shorter and less expensive wind turbine towers can be used in shallow seas.\n\nFor wind turbine engineering, wind speed variation with height is often approximated using a power law:\n\nwhere:\n\nThe knowledge of wind engineering is used to analyze and design all high rise buildings, cable suspension bridges and cable-stayed bridges, electricity transmission towers and telecommunication towers and all other types of towers and chimneys. The wind load is the dominant load in the analysis of many tall buildings. So wind engineering is essential for the analysis and design of tall buildings. Again, wind load is a dominant load in the analysis and design of all long-span cable bridges.\n\n\n"}
{"id": "17037775", "url": "https://en.wikipedia.org/wiki?curid=17037775", "title": "Ângelo Moreira da Costa Lima", "text": "Ângelo Moreira da Costa Lima\n\nÂngelo Moreira da Costa Lima (1887–1964) was the foremost Brazilian entomologist of his time, and his still-consulted works continue to assure his place in the history of science as the \"Father\" of Brazilian entomology.\n\nCosta Lima, as he is called in Brazil, was born on June 29, 1887, in Rio de Janeiro, Brazil, to Valeriano Moreira da Costa Lima and Rosa Delfina Brum de Lima.\n\n\n"}
