{"id": "7853687", "url": "https://en.wikipedia.org/wiki?curid=7853687", "title": "Academic Word List", "text": "Academic Word List\n\nThe Academic Word List (AWL) was developed by Averil Coxhead at the School of Linguistics and Applied Language Studies at Victoria University of Wellington, New Zealand. The list contains 570 semantic fields which were selected because they appear with great frequency in a broad range of academic texts. The list does not include words that are in the most frequent 2000 words of English (the General Service List), thus many of the words are specific to academic contexts. However, a significant percentage of the vocabulary contained within the AWL is of general use; it is simply not of high enough frequency to be contained within the General Service List. Words such as \"area\", \"approach\", \"create\", \"similar\", and \"occur\", for example, are in sublist one, yet are words which one could expect to encounter in everyday life, in newspapers, on television, etc. The AWL was primarily made so that it could be used by teachers (especially teachers of English as a Second Language) as part of a programme preparing learners for tertiary level study or used by students working alone to learn the words most needed to study at colleges and universities.\n\nThe 570 words are divided into 10 sublists. The sublists are ordered such that the words in the first sublist are the most frequent words and those in the last sublist are the least frequent.\n\nThe is available on the .\n\n\n\n"}
{"id": "1009525", "url": "https://en.wikipedia.org/wiki?curid=1009525", "title": "Advanced Composition Explorer", "text": "Advanced Composition Explorer\n\nAdvanced Composition Explorer (ACE) is a NASA Explorers program Solar and space exploration mission to study matter comprising energetic particles from the solar wind, the interplanetary medium, and other sources.\n\nReal-time data from ACE is used by the NOAA Space Weather Prediction Center to improve forecasts and warnings of solar storms. The ACE robotic spacecraft was launched August 25, 1997, and entered a Lissajous orbit close to the Lagrangian point (which lies between the Sun and the Earth at a distance of some 1.5 million km from the latter) on December 12, 1997. The spacecraft is currently operating at that orbit. Because ACE is in a non-Keplerian orbit, and has regular station-keeping maneuvers, the orbital parameters in the adjacent information box are only approximate.\n\n, the spacecraft is still in generally good condition, and is projected to have enough propellant to maintain its orbit until 2024. NASA Goddard Space Flight Center managed the development and integration of the ACE spacecraft.\n\nACE observations allow the investigation of a wide range of fundamental problems in the following four major areas:\n\nA major objective is the accurate and comprehensive determination of the elemental and isotopic composition of the various samples of “source material” from which nuclei are accelerated. These observations have been used to:\n\nIsotopic “anomalies” in meteorites indicate that the solar system was not homogeneous when formed. Similarly, the Galaxy is neither uniform in space nor constant in time due to continuous stellar nucleosynthesis. \nACE measurements have been used to:\n\nSolar energetic particle, solar wind, and spectroscopic observations show that the elemental composition of the corona is differentiated from that of the photosphere, although the processes by which this occurs, and by which the solar wind is subsequently accelerated, are poorly understood. The detailed composition and charge–state data provided by ACE are used to:\n\nParticle acceleration is ubiquitous in nature and understanding its nature is one of the fundamental problems of space plasma astrophysics. The unique data set obtained by ACE measurements have been used to:\n\nThe Cosmic Ray Isotope Spectrometer covers the highest decade of the Advanced Composition Explorer’s energy interval, from 50 to 500 MeV/nucleon, with isotopic resolution for elements from Z ≈ 2 to 30. The nuclei detected in this energy interval are predominantly cosmic rays originating in our Galaxy. This sample of galactic matter investigates the nucleosynthesis of the parent material, as well as fractionation, acceleration, and transport processes that these particles undergo in the Galaxy and in the interplanetary medium. Charge and mass identification with CRIS is based on multiple measurements of dE/dx and total energy in stacks of silicon detectors, and trajectory measurements in a scintillating optical fiber trajectory (SOFT) hodoscope. The instrument has a geometrical factor of 250 cm sr for isotope measurements.\nThe Solar Isotope Spectrometer (SIS) provides high resolution measurements of the isotopic\ncomposition of energetic nuclei from He to Zn (Z = 2 to 30) over the energy range from ~10 to ~100 MeV/nucleon. During large solar events SIS measures the isotopic abundances of solar energetic particles to determine directly the composition of the solar corona and to\nstudy particle acceleration processes. During solar quiet times SIS measures the isotopes\nof low-energy cosmic rays from the Galaxy and isotopes of the anomalous cosmic ray component, which originates in the nearby interstellar medium. SIS has two telescopes composed of silicon solid-state detectors that provide measurements of the nuclear charge, mass, and\nkinetic energy of incident nuclei. Within each telescope, particle trajectories are measured\nwith a pair of two-dimensional silicon strip detectors instrumented with custom very-large-\nscale integrated (VLSI) electronics to provide both position and energy-loss measurements.\nSIS was especially designed to achieve excellent mass resolution under the extreme, high flux\nconditions encountered in large solar particle events. It provides a geometry factor of 40 cm sr, significantly greater than earlier solar particle isotope spectrometers.\n\nThe Ultra Low Energy Isotope Spectrometer (ULEIS) on the ACE spacecraft is an ultra-high-resolution mass spectrometer that measures particle composition and energy spectra of elements He–Ni with energies from ~45 keV/nucleon\nto a few MeV/nucleon. ULEIS investigates particles accelerated in solar energetic particle events, interplanetary shocks, and at the solar wind\ntermination shock. By determining energy spectra, mass composition, and their temporal variations\nin conjunction with other ACE instruments, ULEIS greatly improves our knowledge of solar\nabundances, as well as other reservoirs such as the local interstellar medium. ULEIS \ncombines the high sensitivity required to measure low particle fluxes, along with the capability to\noperate in the largest solar particle or interplanetary shock events. In addition to detailed information\nfor individual ions, ULEIS features a wide range of count rates for different ions and energies that allows accurate determination of particle fluxes and anisotropies over short (few minutes) time scales.\nThe Solar Energetic Particle Ionic Charge Analyzer (SEPICA) was the instrument on the Advanced Composition Explorer (ACE) that determined the ionic charge states of solar and interplanetary energetic particles in the energy range from ≈0.2 MeV nucl-1 to ≈5 MeV charge-1. The charge state of energetic ions contains key information to unravel source temperatures, acceleration, fractionation and transport processes for these particle populations. SEPICA had the ability to resolve individual charge states with a substantially larger geometric factor than its predecessor ULEZEQ on ISEE-1 and -3, on which SEPICA was based. To achieve these two requirements at the same time, SEPICA was composed of one high-charge resolution sensor section and two low- charge resolution, but large geometric factor sections.\n\nAs of 2008, this instrument is no longer functioning due to failed gas valves.\n\nThe Solar Wind Ion Composition Spectrometer (SWICS) and the Solar Wind Ions Mass Spectrometer (SWIMS) on ACE are instruments optimized for measurements of the chemical and isotopic composition of solar and interstellar matter. SWICS determined uniquely the chemical and ionic-charge composition of the solar wind, the thermal and mean speeds of all major solar wind ions from H through Fe at all solar wind speeds above 300 km s (protons) and 170 km s (Fe+16), and resolved H and He isotopes of both solar and interstellar sources. SWICS also measured the distribution functions of both the interstellar cloud and dust cloud pickup ions up to energies of 100 keV e. SWIMS measures the chemical, isotopic and charge state composition of the solar wind for every element between He and Ni. Each of the two instruments are time-of-flight mass spectrometers and use electrostatic analysis followed by the time-of-flight and, as required, an energy measurement.\n\nOn 23 August 2011, the SWICS time-of-flight electronics experienced an age- and radiation-induced hardware anomaly that increased the level of background in the composition data. To mitigate the effects of this background, the model for identifying ions in the data was adjusted to take advantage of only the ion energy-per-charge as measured by the electrostatic analyzer, and the ion energy as measured by solid state detectors. This has allowed SWICS to continue to deliver a subset of the data products that were provided to the public prior to the hardware anomaly, including ion charge state ratios of oxygen and carbon, and measurements of solar wind iron. The measurements of proton density, speed, and thermal speed by SWICS were not affected by this anomaly and continue to the present day.\n\nThe Electron, Proton, and Alpha Monitor (EPAM) instrument on the ACE spacecraft is designed to measure a broad range of energetic particles over nearly the full unit-sphere at high time resolution. Such measurements of ions and electrons in the range of a few tens of keV to several MeV are essential to understand the dynamics of solar flares, co-rotating interaction regions (CIR’s), interplanetary shock acceleration, and upstream terrestrial events. The large dynamic range of EPAM extends from about 50 keV to 5 MeV for ions, and 40 keV to about 350 keV for electrons. To complement its electron and ion measurements, EPAM is also equipped with a Composition Aperture (CA) which unambiguously identifies ion species reported as species group rates and/or individual pulse-height events. The instrument achieves its large spatial coverage through five telescopes oriented at various angles to the spacecraft spin axis. The low-energy particle measurements, obtained as time resolutions between 1.5 and 24 s, and the ability of the instrument to observe particle anisotropies in three dimensions make EPAM an excellent resource to provide the interplanetary context for studies using other instruments on the ACE spacecraft.\nThe Solar Wind Electron Proton Alpha Monitor (SWEPAM) experiment provides the bulk solar wind observations for the Advanced Composition Explorer (ACE). These observations provide the context for elemental and isotopic composition measurements made on ACE as well as allowing the direct examination of numerous solar wind phenomena such as coronal mass ejection, interplanetary shocks, and solar wind fine structure, with advanced, 3-D plasma instrumentation. They also provide an ideal data set for both heliospheric and magnetospheric multi-spacecraft studies where they can be used in conjunction with other, simultaneous observations from spacecraft such as Ulysses. The SWEPAM observations are made simultaneously with independent electron (SWEPAM-e) and ion (SWEPAM-i) instruments. In order to save costs for the ACE project, SWEPAM-e and SWEPAM-i are the recycled flight spares from the joint NASA/ESA Ulysses mission. Both instruments had selective refurbishment, modification, and modernization required to meet the ACE mission and spacecraft requirements. Both incorporate electrostatic analyzers whose fan-shaped fields of view sweep out all pertinent look directions as the spacecraft spins. \nThe magnetic field experiment on ACE provides continuous measurements of the local magnetic field in the interplanetary medium. These measurements are essential in the interpretation of simultaneous ACE observations of energetic and thermal particles distributions. The experiment consists of a pair of twin, boom- mounted, triaxial fluxgate sensors which are located 165 inches (=4.19 m) from the center of the spacecraft on opposing solar panels. The two triaxial sensors provide a balanced, fully redundant vector instrument and permit some enhanced assessment of the spacecraft's magnetic field. \nThe Advanced Composition Explorer (ACE) RTSW system is continuously monitoring the solar wind and producing warnings of impending major geomagnetic activity, up to one hour in advance. Warnings and alerts issued by NOAA allow those with systems sensitive to such activity to take preventative action. The RTSW system gathers solar wind and energetic particle data at high time resolution from four ACE instruments (MAG, SWEPAM, EPAM, and SIS), packs the data into a low-rate bit stream, and broadcasts the data continuously. NASA sends real-time data to NOAA each day when downloading science data. With a combination of dedicated ground stations (CRL in Japan and RAL in Great Britain), and time on existing ground tracking networks (NASA's DSN and the USAF's AFSCN), the RTSW system can receive data 24 hours per day throughout the year. The raw data are immediately sent from the ground station to the Space Weather Prediction Center in Boulder, Colorado, processed, and then delivered to its Space Weather Operations Center where they are used in daily operations; the data are also delivered to the CRL Regional Warning Center at Hiraiso, Japan, to the USAF 55th Space Weather Squadron, and placed on the World Wide Web. The data are downloaded, processed and dispersed within 5 min from the time they leave ACE. The RTSW system also uses the low-energy energetic particles to warn of approaching interplanetary shocks, and to help monitor the flux of high-energy particles that can produce radiation damage in satellite systems. \n\nThe figure shows the particle fluence (total flux over a given period of time) of oxygen at ACE for a time period just after solar minimum, the part of the 11-year solar cycle when solar activity is lowest. The lowest-energy particles come from the slow and fast solar wind, with speeds from about 300 to about 800 kilometers per second. Like the solar wind distribution of all ions, that of oxygen has a suprathermal tail of higher-energy particles; that is, in the frame of the bulk solar wind, the plasma has an energy distribution that is approximately a thermal distribution but has a notable excess above about 5 kiloelectron volts, as shown in Figure 1. The ACE team has made contributions to understanding the origins of these tails and their role in injecting particles into additional acceleration processes.\n\nAt energies higher than those of the solar wind particles, ACE observes particles from regions known as corotating interaction regions (CIRs). CIRs form because the solar wind is not uniform. Due to solar rotation, high-speed streams collide with preceding slow solar wind, creating shock waves at roughly 2–5 astronomical units (AU, the distance between Earth and the Sun) and forming CIRs. Particles accelerated by these shocks are commonly observed at 1 AU below energies of about 10 megaelectron volts per nucleon. ACE measurements confirm that CIRs include a significant fraction \nof singly charged helium formed when interstellar neutral helium is ionized.\n\nAt yet higher energies, the major contribution to the measured flux of particles is due to solar energetic particles (SEPs) associated with interplanetary (IP) shocks driven by fast coronal mass ejections (CMEs) and solar flares. Enriched abundances of helium-3 and helium ions show that the suprathermal tails are the main seed population for these SEPs. IP shocks traveling at speeds up to about 2000 kilometers per second accelerate particles from the suprathermal tail to 100 megaelectron volts per nucleon and more. IP shocks are particularly important because they can continue to accelerate particles as they pass over ACE and thus allow shock acceleration processes to be studied in situ.\n\nOther high-energy particles observed by ACE are anomalous cosmic rays (ACRs) that originate with neutral interstellar atoms that are ionized in the inner heliosphere to make “pickup” ions and are later accelerated to energies greater than 10 megaelectron volts per nucleon in the outer heliosphere. ACE also observes pickup ions directly; they are easily identified because they are singly charged. Finally, the highest-energy particles observed by ACE are the galactic cosmic rays (GCRs), thought to be accelerated by shock waves from supernova explosions in our galaxy.\n\nShortly after launch, the SEP sensors on ACE detected solar events that had unexpected characteristics. Unlike most large, \nshock-accelerated SEP events, these were highly enriched in iron and helium-3, as are the much smaller, flare-associated impulsive SEP events. Within the first year of operations, ACE found many of these “hybrid” events, which led to substantial discussion within the community as to what conditions could \ngenerate them.\n\nOne remarkable recent discovery in heliospheric physics has been the ubiquitous \npresence of suprathermal particles with common spectral shape. This shape unexpectedly occurs in the quiet solar wind; \nin disturbed conditions downstream from shocks, including CIRs; and elsewhere in the heliosphere. These observations have led Fisk and Gloeckler to suggest a novel mechanism for the particles’ acceleration.\n\nAnother discovery has been that the current solar cycle, as measured by sunspots, CMEs, and SEPs, has been much less magnetically active than the previous cycle. McComas et al. have shown that the dynamic pressures of the solar wind measured by the Ulysses satellite over all latitudes and by ACE in the ecliptic plane are correlated and were declining in time for about 2 decades. They concluded that the Sun had been undergoing global change that affected the overall heliosphere. Simultaneously, GCR intensities were increasing and in 2009 were the highest recorded during the past 50 years. GCRs have more difficulty reaching Earth when the Sun is more magnetically active, so the high GCR intensity in 2009 is consistent with a globally reduced dynamic pressure of the solar wind.\n\nACE also measures abundances of cosmic ray nickel-59 and cobalt-59 isotopes; these measurements indicate that a time longer than the half-life of nickel-59 with bound electrons (7.6 × 10 years) elapsed between the time nickel-59 was created in a supernova explosion and the time cosmic rays were accelerated. Such long delays indicate that cosmic rays come from the acceleration of old stellar or interstellar material rather than from fresh supernova ejecta. ACE also measures an iron-58/iron-56 ratio that is enriched over the same ratio in solar system material. These \nand other findings have led to a theory of the origin of cosmic rays in galactic superbubbles, formed in regions where many \nsupernovae explode within a few million years. Recent observations of a cocoon of freshly accelerated cosmic rays in the Cygnus superbubble by the Fermi gamma-ray observatory support this theory.\n\nOn February 11, 2015, the Deep Space Climate Observatory (DSCOVR)—with several similar instruments including a newer and more sensitive instrument to detect Earth-bound coronal mass ejections—successfully launched by NOAA and NASA aboard a SpaceX Falcon 9 launch vehicle from Cape Canaveral, Florida. The spacecraft arrived at L by 8 June 2015, just over 100 days after launch. Along with ACE, both will provide space weather data as long as ACE can continue to function.\n\n"}
{"id": "47888776", "url": "https://en.wikipedia.org/wiki?curid=47888776", "title": "Advanced Simulation Library", "text": "Advanced Simulation Library\n\nAdvanced Simulation Library (ASL) is free and open-source hardware-accelerated multiphysics simulation platform. It enables users to write customized numerical solvers in C++ and deploy them on a variety of massively parallel architectures, ranging from inexpensive FPGAs, DSPs and GPUs up to heterogeneous clusters and supercomputers. Its internal computational engine is written in OpenCL and utilizes matrix-free solution techniques. ASL implements variety of modern numerical methods, i.a. level-set method, lattice Boltzmann, immersed Boundary. Mesh-free, immersed boundary approach allows users to move from CAD directly to simulation, reducing pre-processing efforts and amount of potential errors. ASL can be used to model various coupled physical and chemical phenomena, especially in the field of computational fluid dynamics.\nIt is distributed under the free GNU Affero General Public License with an optional commercial license (which is based on the permissive MIT License).\n\nAdvanced Simulation Library is being developed by Avtech Scientific, an Israeli company. Its source code was released to the community on 14 May 2015, whose members packaged it for scientific sections of all major Linux distributions shortly thereafter. Subsequently, Khronos Group acknowledged the significance of ASL and listed it on its website among OpenCL-based resources.\n\n\n\n\nASL provides a range of features to solve number of problems - from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid mechanics and elasticity.\n\n\n"}
{"id": "12795884", "url": "https://en.wikipedia.org/wiki?curid=12795884", "title": "American Society of Plant Taxonomists", "text": "American Society of Plant Taxonomists\n\nThe American Society of Plant Taxonomists (ASPT) is a botanical organization formed in 1935 to \"foster, encourage, and promote education and research in the field of plant taxonomy, to include those areas and fields of study that contribute to and bear upon taxonomy and herbaria\", according to its bylaws. It is incorporated in the state of Wyoming, and its office is at the University of Wyoming, Department of Botany.\n\nThe ASPT publishes a quarterly botanical journal, \"Systematic Botany\", and the irregular series \"Systematic Botany Monographs\".\n\n"}
{"id": "49634150", "url": "https://en.wikipedia.org/wiki?curid=49634150", "title": "Aradite", "text": "Aradite\n\nAradite is a very rare mineral with formula BaCa[(SiO)(VO)](VO)F. Aradite and its phosphorus-analogue, zadovite, were found in paralavas (rocks formed due to pyrometamorphism) of the Hatrurim Formation. Both aradite and zadovite have structures similar to that of nabimusaite. Structure of all three minerals is related to that of hatrurite.\n"}
{"id": "6130928", "url": "https://en.wikipedia.org/wiki?curid=6130928", "title": "Arctic Lowlands", "text": "Arctic Lowlands\n\nThe 'Canadian Shield and the Innuitian region\nare located to the south of the Arctic Lowland plains. This is a region of tundra, a treeless plain, with a cold, dry climate and poorly drained soil. The Arctic Lowlands region is located in Nunavut and the Northwest Territories. The Arctic Lowlands are plains located in Canada. Plains are extensive areas of level or gently rolling land. In North America there is a large, flat interior Plain. They are also part of an area that is commonly referred to as the Arctic Archipelago, which occupies much of the central Canadian Arctic. They are made up of a series of islands located in Canada's far north, and remain frozen for most of the year. However, the Paleozoic sedimentary rock, from which the Lowlands are formed, contains lignite (a form of coal), oil, and natural gas deposits. Limestone is very abundant as well. The Arctic Lowlands have a small human population. The terrain is mostly ice, snow, rock, and it is full of marshes, especially in the winter. Animals that live in the area include polar bears, char, Arctic hares and Arctic foxes. This region is being affected by global warming. It is very cold and human life may be difficult. Commonly known as the Hudson Bay-Arctic Lowlands, the Hudson Bay contains over 50% water.\n"}
{"id": "3438662", "url": "https://en.wikipedia.org/wiki?curid=3438662", "title": "Average fixed cost", "text": "Average fixed cost\n\nIn economics, average fixed cost (AFC) is the fixed costs of production (FC) divided by the quantity (Q) of output produced. Fixed costs are those costs that must be incurred in fixed quantity regardless of the level of output produced.\n\nAverage fixed cost is a per-unit-of-output of fixed costs. As the total number of units of the good produced increases, the average fixed cost decreases because the same amount of fixed costs is being spread over a larger number of units of output.\n\nAverage variable cost plus average fixed cost equals average total cost:\n\nAssume a firm produces clothing. When the quantity of the output varies from 5 shirts to 10 shirts, fixed cost would be 30 dollars. In this case, average fixed cost of producing 5 shirts would be 30 dollars divided by 5 shirts, which is 6 dollars. In other words, when 5 shirts are produced, 30 dollars of fixed cost would spread and result in 6 dollars per shirt. Similarly, average fixed cost of producing 10 shirts would be 3 dollars derived from 30 dollars divided by 10 shirts.\nIn Example1, there was no information about average total cost and average variable cost. If the firm knows average total cost and average variable cost, it is possible to find the same result as Example 1. Because average total cost is average variable cost plus average fixed cost, average fixed cost is average total cost minus average variable cost. If producing 5 shirts generates average total cost of 11 dollars and average variable cost of 5 dollars, fixed cost would be 6 dollars. Similarly, the firm produces 10 shirts and average total cost and average variable cost is 10 dollars and 7 dollars respectively. In this case, average fixed cost would be 3 dollars.\n\n"}
{"id": "28836785", "url": "https://en.wikipedia.org/wiki?curid=28836785", "title": "Berw Fault", "text": "Berw Fault\n\nThe Berw Fault is a SW-NE trending fault in North Wales. It forms part of the Menai Strait Fault System, with the Dinorwic Fault and the Aber Dinlle Fault. It has a long history of movement with early ductile fabrics preserved from a sinistral (left lateral) strike-slip sense shear zone active at the end of the Precambrian and into the early Cambrian. Any Caledonian deformation is unclear but the fault zone was reactivated in the Carboniferous as a NW-throwing normal fault with seismic reflection data showing the formation of a half-graben in its hanging wall. There are no indications of inversion during the Variscan Orogeny, but the fault was reactivated in a normal sense during the Permian and Triassic and again during the Cenozoic with a sinistral strike-slip sense.\n\n"}
{"id": "49124285", "url": "https://en.wikipedia.org/wiki?curid=49124285", "title": "CTF3 (CERN)", "text": "CTF3 (CERN)\n\nCTF3 (CLIC Test Facility 3) was an electron accelerator facility built at CERN with the aim of demonstrating the key concepts of the Compact Linear Collider accelerator. The facility consisted in two electron beamlines to mimic the functionalities of the CLIC Drive Beam and Main Beam.\n\nThe facility stopped its operation in December 2016, and one of its beamlines has been converted into the new \"CERN Linear Electron Accelerator for Research\" (CLEAR) facility.\n\nThis page provides a general description of the facility with references to its main experimental program. More detailed informations can be found in the facility Design Report.\n\nThe facility implemented and demonstrated the feasibility of a scaled version of the CLIC Drive Beam: a 1.2 μs-long electron beam (bunched at 1.5 GHz, and with 4 A mean current) was generated and accelerated up to ~135 MeV in a ~80 m-long LINAC by using fully loaded accelerating structures powered by ~40 MW, 3 GHz RF pulses. The beam was then going through a simplified version of the Drive Beam Recombination Complex (DBRC): a system composed of a delay loop and a combiner ring allowed to recombine different part of the incoming beam to finally produce a 140 ns-long train of bunches at 12 GHz and with mean current as high as 28 A.\n\nA second electron beam of lower intensity (a few bunches of about 100 pC/bunch), called Probe Beam, was generated and accelerated up to 200 MeV in the so-called \"Concept d’Accélerateur Linéaire pour Faisceau d’Electrons Sonde\" (CALIFES) injector. The main purpose of the Probe Beam was to emulate the main, colliding beam of CLIC.\n\nThe two electron beams produced at CTF3 were used to demonstrate the two-beam acceleration concept in the Two-Beam-Module installed into the CLEX experimental area: the Drive Beam was decelerated in special Power Extraction and Transfer Structures (PETS), and the power produced used to accelerate the Probe Beam with gradients as high as ~145 MeV/m.\n\nThe facility served as test bed for other CLIC related R&D, for example:\n\nThanks to its ease of operation and versatility, the Probe Beam was used also for activities not directly connected to CLIC. This triggered the interest of various communities, and a workshop was organised to discuss possible re-use of such a beamline. In December 2016, while CTF3 was ending its operation, it was then decided to transform the Probe Beam in a new general purpose R&D facility under the name of CLEAR.\n\nCLEAR keeps providing testing capabilities for X-band accelerator technology, including CLIC, but it also allows to explore novel concepts as plasma acceleration, THz radiation production. Furthermore, it provides electron irradiation capabilities to space and medical communities.\n"}
{"id": "385275", "url": "https://en.wikipedia.org/wiki?curid=385275", "title": "Central Saint Martins", "text": "Central Saint Martins\n\nCentral Saint Martins, often abbreviated to CSM, is a public tertiary art school in London, England. It is a constituent college of the University of the Arts London. It offers full-time courses at foundation, undergraduate and postgraduate levels, and a variety of short and summer courses.\n\nIt was formerly known as Central Saint Martins College of Arts and Design, and before that as Central Saint Martins College of Art and Design.\n\nCentral Saint Martins College of Art and Design was formed in 1989 from the merger of the Central School of Art and Design, founded in 1896, and Saint Martin's School of Art, founded in 1854. Since 1986 both schools had been part of the London Institute, formed by the Inner London Education Authority to bring together seven London art, design, fashion and media schools. The London Institute became a legal entity in 1988, could award taught degrees from 1993, was granted university status in 2003 and was renamed University of the Arts London in 2004. It also includes Camberwell College of Arts, Chelsea College of Arts, the London College of Communication, the London College of Fashion and Wimbledon College of Arts.\n\nThe Drama Centre London, founded in 1963, joined Central Saint Martins in 1999 as an integral school, maintaining its name and teaching approaches. The Byam Shaw School of Art, founded in 1910, was merged into Central Saint Martins in 2003.\n\nThe Central School of Art and Design was established as the Central School of Arts and Crafts in 1896 by London County Council. It grew directly from the Arts and Crafts movement of William Morris and John Ruskin. The first principal, from 1896 until 1911, was William Richard Lethaby; a blue plaque in his memory was erected in 1957. The school was at first housed in Morley Hall, rented from the Regent Street Polytechnic. It moved to purpose-built premises in Southampton Row, in the London Borough of Camden, in 1908. In the same year the Royal Female School of Art, established in 1842, was merged into the school. Central became part of the London Institute in 1986, and merged with Saint Martin's in 1989.\n\nSaint Martin's School of Art was established in 1854 by Henry Mackenzie, vicar of the church of St Martin-in-the-Fields. It became independent from the church in 1859. Frank Martin became head of the sculpture department in 1952; he brought in young sculptors and recent graduates of the department as teachers. Among these, Anthony Caro was particularly influential. The group around him came to be known as the New Generation of British sculptors and the sculpture department of Saint Martin's became, in the words of Tim Scott: \"the most famous in the art world\". Saint Martin's became part of the London Institute in 1986, and merged with Central in 1989.\n\nThe Drama Centre London was founded in 1963 by a breakaway group of teachers and students from the Central School of Speech and Drama, led by John Blatchley, Yat Malmgren and Christopher Fettes. The school is a member of Drama UK and its undergraduate Acting course is accredited by Drama UK. The Drama Centre London merged with Central Saint Martins in 1999.\n\nByam Shaw School of Art was founded by the artists John Byam Shaw and Rex Vicat Cole in 1910 as a school of drawing and painting. It was originally located in Campden Street, Kensington, and moved to larger premises in Archway in 1990. It was subsumed by Central Saint Martins in 2003.\n\nIn 1998 the London Institute received a Queen's Anniversary Prize for the \"massive contribution\" of Central Saint Martins College of Art & Design to the growth of the fashion industry in Britain. The University of the Arts London received a Queen's Anniversary Prize in 2013, for the contribution of CSM industrial and product design graduates to commerce, industry and the design profession.\n\nCSM does not receive independent assessment in the \"Complete University Guide\" league tables, but is ranked as part of the University of the Arts London. In 2014 the university received an overall ranking of 67 out of 124 institutions, with a rank of 102 for graduate prospects and 123 for student satisfaction with teaching. In 2018 it was placed 83rd out of 129 universities, with a rank of 125 for student satisfaction. \n\nTeaching at Central Saint Martins is organised into nine programmes, which include acting, art, design, fashion, graphics and jewellery and textiles, as well as foundation courses.\n\nCentral Saint Martins moved to a converted warehouse complex on Granary Square at King's Cross in 2011. Most of the college is housed there, but it also uses the former Byam Shaw building in Elthorne Road, Archway, and premises in Richbell Place, Holborn.\n\nNotable past and present staff of Central Saint Martins include the theatre designer Maria Björnson.\n\nAmong the alumni of the school are the Turner Prize winner Pete Phipps, the musician Jarvis Cocker and many fashion designers, including Sarah Burton, John Galliano, Stella McCartney, Alexander McQueen, Zac Posen and Riccardo Tisci.\n"}
{"id": "45274057", "url": "https://en.wikipedia.org/wiki?curid=45274057", "title": "Couple interview", "text": "Couple interview\n\nA couple interview (or joint couple interview, or more broadly conjoint interview, joint interview or dyadic interview) is a method of qualitative research used in the social sciences, where two spouses are interviewed together. Such an interview is typically semi-structured or unstructured. Couple interviews are important in household research, often from a psychological, sociological, anthropological or social geographical perspective, and are also frequently used within health research. A couple interview is a form of joint interviewing (interviews involving two interviewees), the subject of a growing methodological research literature.\n\nThere is an ongoing methodological controversy over whether couples should ideally be interviewed together or apart. Bjørnholt and Farstad argue that the couple interview should be seen as a distinct form of the qualitative research interview, and argue that the couple interview has several advantages over individual interviews, in particular in \"solving the ethical problems of anonymity and consent among interviewees, and [resulting] in the production of rich data, including observational data,\" and in intra-couple dynamics and the interaction between the informants, as well as with the researcher(s), in the interview situation, which may also reveal controversies and areas of conflict, by providing a reflective space for both partners together, which enables them to challenge as well as to reinforce each other's accounts. They further argue that the researcher plays an important role, as couple interviews may be seen as an arena of \"family display,\" using a concept originally proposed by Janet Finch. The opportunity to observe shared storytelling is regarded as a widely documented advantage of joint interviewing.\n"}
{"id": "12531719", "url": "https://en.wikipedia.org/wiki?curid=12531719", "title": "Dinagat gymnure", "text": "Dinagat gymnure\n\nThe Dinagat gymnure (\"Podogymnura aureospinula\") is a species of mammal in the family Erinaceidae. It is endemic to the Philippines.\n\nIts natural habitat is subtropical or tropical dry forests. It is threatened by habitat loss.\n\n"}
{"id": "19537792", "url": "https://en.wikipedia.org/wiki?curid=19537792", "title": "Ekrem Ekinci", "text": "Ekrem Ekinci\n\nEkrem Ekinci is Professor of Chemistry and the rector of Işık University in Istanbul, Turkey.\n"}
{"id": "58817314", "url": "https://en.wikipedia.org/wiki?curid=58817314", "title": "Emma Kowal", "text": "Emma Kowal\n\nEmma Kowal is an Australian cultural and medical anthropologist, physician and scholar of science and technology studies. She is most well-known for her books \"Trapped in the Gap: Doing Good in Indigenous Australia\", and the co-edited volumes of \"Force, Movement, Intensity: The Newtonian Imagination in the Humanities and Social Sciences\" (with Ghassan Hage), \"Cryopolitics: Frozen Life in a Melting World\" (with Joanna Radin). \n\nShe received her Bachelor of Medicine and Bachelor of Surgery and a Bachelor of Arts in history and philosophy of science from University of Melbourne in 2000 and worked for a few years as a physician and a public health professional in the Northern Territories of Australia. She returned to the University of Melbourne to receive her PhD in public health anthropology in 2007. She is currently an Associate Professor in Anthropology at Deakin University.\n\nIn 2014, she received the Paul Bourke Award for Early Career Research from the Academy of the Social Sciences in Australia. She was the deputy director for the National Centre for Indigenous Genomics at Australian National University between 2013 and 2017.\n\nEmma Kowal has contributed to a large number of scholarly articles.\n"}
{"id": "8871974", "url": "https://en.wikipedia.org/wiki?curid=8871974", "title": "Ethnoornithology", "text": "Ethnoornithology\n\nEthnoornithology (also ethno-ornithology) is the study of the relationship between people and birds (from \"ethno-\" - relating to people and culture - and \"ornithology\" - the study of birds). It is a branch of ethnozoology and so of the wider field of ethnobiology. Ethnoornithology is an interdisciplinary subject and combines anthropological, cognitive and linguistic perspectives with natural scientific approaches to the description and interpretation of people's knowledge and use of birds. Like ethnoscience and other cognate terms, \"ethnoornithology\" is sometimes used narrowly to refer to people's practice rather than the study of that practice. The broader focus is on how birds are perceived, used and managed in human societies, including their use for food, medicine and personal adornment, as well as their use in divination and ritual. Applied ethnoornithological research is also starting to play an increasingly important role in the development of conservation initiatives.\n\nThe work of Ralph Bulmer in New Guinea, culminating in his collaboration with Ian Saem Majnep in writing \"Birds of My Kalam Country\" (1977), set a new standard for ethnoornithological research, and this book has deservedly become a classic of modern ethnoornithology.\n\nLike other branches of ethnozoology, ethnoornithology has been long undervalued as a resource for conservation, though this is now beginning to change. Mark Bonta's \"Seven Names for the Bellbird\" (2003), which highlights the importance of local traditions and practices relating to birds for the future of biodiversity conservation in Honduras, and Ricardo Rozzi´s \"Multi-ethnic Bird Guide of the Subantarctic Forests of South America\" (2003), which focuses on the integration of traditional ornithological knowledge and environmental ethics in southern Chile, provide good examples of this trend. Soma (2015) pointed out that ethnoornithological knowledge of falconers contribute to conservation for local avifauna (especially focusing on Kazakh eagle masters). This realisation is the basis for founding the Ethno-ornithology World Archive (EWA), a collaborative project between Oxford University (linking the Department of Zoology and School of Anthropology and Museum Ethnography) and BirdLife International.\n\nThe Society of Ethnobiology, which publishes the \"Journal of Ethnobiology\", provides a general forum for ethnobiological - including ethnoornthological - research. In January 2006 the Ethnoornithology Research & Study Group (ERSG) was established \"to provide a clearinghouse, information source and discussion point for people interested in the study of, research about and application of indigenous bird knowledge\".\n\n\n"}
{"id": "10024769", "url": "https://en.wikipedia.org/wiki?curid=10024769", "title": "Fiction theory", "text": "Fiction theory\n\nFiction Theory is a discipline that applies possible world theory to literature. Fiction theory scholars and critics have articulated various theses rooted in Saul Kripke’s application of modal logic to semantics. Drawing on concepts found in possible world theory, theorists of fiction study the relationships between textual worlds and the world outside the text. The overarching idea in fiction theory is that the relationships between the imaginary worlds of fiction and the actual world in which we live are complicated, and that one ought not dismiss fiction as simply stories that are not “true.” Theorists of fiction pose challenging questions about, and offer constructive ways of exploring, the often complex relations between the worlds of fiction and the “real” world in which we live. \n\nIn order to understand fiction theory, one must pose questions about fundamental terms such as text, narrative, literature, fiction, etc. Literature may be understood as a text that is self-consciously artistic, rather than as a text that is used as a medium through which to convey information, for example a newspaper article. Roman Jakobson, a Russian formalist and linguist, was one of the first individuals to discuss art as a way of communication that is intentionally aesthetic, and applied linguistics to analyses of literary texts. In his well-known communication model, Jakobson breaks apart a communicative act between an addressor and addressee into a message, code, context, and contact, with each part having its own function. According to Jakobson’s model, art is created when the message itself (which carries the poetic function) is stressed. \n\nThe French scholar Roland Barthes designed a system of five major codes that function as tools to analyze narrative texts in ways that move beyond examinations of plot and structure, thereby bringing to the surface the subtle ways a text becomes a literary narrative. \n\nJakobson’s model and Barthes’s codes offer critics a way to begin to explore the nature of a literary text through application of semiotics to narrative.\n\nWhile Jakobson and Barthes emphasize the intention of the speaker/writer, the philosopher Nelson Goodman examines the broader question of how we create imaginary worlds and categorizes our “Ways of Worldmaking” (the title of his book on this topic) into composition/decomposition, weighting, ordering, deletion/supplementation, and reformations. The scholar Marie-Laure Ryan is also concerned less with intention and more with the various ways that fictional worlds are related to the actual world outside the text. Ryan conceptualizes these relations in a framework of accessibility and has developed a typology of accessibility relations that establishes the extent to which fictional worlds are similar to or different from the actual world in which we live. The fictional world that most resembles the actual world is based on the “principle of minimal departure.” This idea was first articulated by John Searle and refers to the fundamental property of an imaginary world that is minimally different from the familiar world in which we live. Lubomir Dolezel has developed a similar typology based on modal operators that determine the narrative world.\n\nFrank Kermode, in his seminal text \"\", argues that ultimate meaning is derived from the end; successive events are predicated on previously established meaning. He articulates a concept of fiction based on this view of humans’ constant yearning for an ultimate end that will imbue with meaning everything that preceded it. In this discussion, Kermode distinguishes between fiction and myth. Fiction consists of stories all individuals create about their lives in order to keep on living in a world that makes few guarantees and is full of inexplicable phenomena. Kermode defines myth as a dangerous fiction used for exploitative purposes. The philosopher Hans Vaihinger has articulated similar ideas, putting forth the biological argument that human beings use fictions to help survive in a hostile environment, and that these fictions are so useful that it becomes most difficult, if not impossible, for us to stop ourselves from creating fictions. In Vaihinger’s language, fiction is something we treat “as if” it is true even when we know that it is not true, whereas myth is something we treat as true because we do not know it is false. \n\nLiterary critic Thomas Pavel argues that the fictional world deserves to be examined on its own terms rather than merely through the lens of mimesis. His thesis serves as a critique of Structuralism by its insistence on the idea that narrativity, as a fundamental aspect of fiction, removes the possibility of pure imitation of the actual world. Pavel’s theory thus departs from the main ideas of other fiction theorists because he separates literature from its referential relationship to the actual world. Following in the footsteps of the Austrian philosopher Alexius Meinong, Pavel asserts that fictional worlds demand tremendous respect for their ability to serve as powerful tools of knowledge rather than for their likeness to the actual world.\n\n\n\n"}
{"id": "48894400", "url": "https://en.wikipedia.org/wiki?curid=48894400", "title": "Giant Virus Finder", "text": "Giant Virus Finder\n\nThe Giant Virus Finder is a free bioinformatics software for finding giant viruses in metagenomes.\n\nThe Giant Virus Finder tool integrates and applies the Giant Virus Toplist, the list of the largest virus genomes. With the tool, giant viruses were found in diverse habitats, like the Great Rann of Kutch\n"}
{"id": "45210195", "url": "https://en.wikipedia.org/wiki?curid=45210195", "title": "Hadley (crater)", "text": "Hadley (crater)\n\nHadley is an impact crater in the Aeolis quadrangle of Mars, located at 19.5°S latitude and 203.1°W longitude, and is inside Terra Cimmeria. It is 119.0 km in diameter and was named after George Hadley, and the name was approved in 1973.\n\nNearby prominent craters include Graff to the west-southwest and almost northeast is Boeddicker. Not far from Hadley in the southeast is al-Qahira Vallis.\n\nIt owes no relation to a lunar feature known as Mons Hadley which is named after John Hadley.\n\nHadley is a triple crater formation whereas a smaller crater is in the middle and the southcentral portion and a small one in the southern part of the smaller one. Other surrounding craters are inside Hadley including one in the western rim and a tiny one near the first smaller crater inside Hadley. Southeast of Hadley's rim is a smaller unnamed crater that its depth is almost the same as the third smallest crater inside Hadley. Dunes are present on the floor of the crater and can be seen in the pictures on the left.\n\n"}
{"id": "34270521", "url": "https://en.wikipedia.org/wiki?curid=34270521", "title": "Henry Nathaniel Andrews", "text": "Henry Nathaniel Andrews\n\nHenry Nathaniel Andrews, Jr. (born June 15, 1910, Melrose, Massachusetts; d. March 3, 2002 Concord, New Hampshire) was an American paleobotanist recognized as an expert in plants of the Devonian and Carboniferous periods. He was a fellow of the Geological Society of America and the American Association for the Advancement of Science and was elected into the U.S. National Academy of Sciences in 1975. He was a professor at the Washington University in St. Louis from 1940 to 1964 and a paleobotanist at the Missouri Botanical Garden 1947 to 1964. From 1964 until his retirement 1975, Andrews worked at the University of Connecticut, where he served as head of the school's Botany department and later as head of the Systematics and Environmental Section.\n"}
{"id": "5260755", "url": "https://en.wikipedia.org/wiki?curid=5260755", "title": "Hubble – 15 Years of Discovery", "text": "Hubble – 15 Years of Discovery\n\nHubble – 15 Years of Discovery () is a book that formed part of the European Space Agency's 15th anniversary celebration activities for the 1990 launch of the NASA/ESA Hubble Space Telescope. Its main emphasis is on the exquisite Hubble images that have enabled astronomers to gain entirely new insights into the workings of a huge range of different astronomical objects. Hubble has provided the visual overview of the underlying astrophysical processes taking place in these objects, ranging from planets in the Solar System to galaxies in the young Universe. This book shows the close relationship between the results of great scientific value and of eye-catching beauty and artistic potential.\n\nThe book published by Springer has 120 pages, measures 30 x 25 cm and is in full-colour. It has been translated into Finnish, Portuguese and German. Some versions include a copy of the Hubble – 15 Years of Discovery documentary (distributed in 860,000 copies). \n\nThe book is authored by Lars Lindberg Christensen and Bob Fosbury. It is illustrated by Martin Kornmesser.\n\n"}
{"id": "51521375", "url": "https://en.wikipedia.org/wiki?curid=51521375", "title": "Hubert Hudson", "text": "Hubert Hudson\n\nHuberht Tor Hudson (17 September 1886 – 15 June 1942), commonly known as Hubert Hudson instead of by his actual first name (an Old English version of the name), was a navigating officer in the British Royal Navy, who took part in Ernest Shackleton's Imperial Trans-Antarctic Expedition to Antarctica.\n\nHudson joined the expedition whilst a 'mate' within the Royal Navy. He earned himself the nickname of 'Buddha', when the rest of the crew successfully tricked him into dressing-up in little more than a bedsheet for a 'fancy dress' party on the whaling station at South Georgia that was in reality anything but.\n\nDuring the expedition, Hudson was famed for his ability to catch penguins, which the crew ate as a source of food whilst trapped on the ice. It's also known that towards the end of the expedition, Hudson suffered a severe breakdown of mental morale, possibly due to a massive boil that he developed on his buttocks. His illness caused Frank Wild, the second-in-command, a lot of worry that he would not survive. However, Hudson pulled through and eventually recovered his health.\n\nUpon return from the expedition, Hudson took part in World War I, serving on 'mystery ships'.\n\nHe later also took part in World War II as a Royal Navy Reserve Convoy Commodore. Hudson died on 15 June 1942 on convoy HG84 when his ship, the merchant vessel PELAYO, was torpedoed by U552.\n"}
{"id": "4724116", "url": "https://en.wikipedia.org/wiki?curid=4724116", "title": "ISO 15926", "text": "ISO 15926\n\nThe ISO 15926 is a standard for data integration, sharing, exchange, and hand-over between computer systems.\n\nThe title, \"Industrial automation systems and integration—Integration of life-cycle data for process plants including oil and gas production facilities\", is regarded too narrow by the present ISO 15926 developers. Having developed a generic data model and Reference Data Library for process plants, it turned out that this subject is already so wide, that actually any state information may be modelled with it.\n\nIn 1991 a European Union ESPRIT-, named ProcessBase, started. The focus of this research project was to develop a data model for lifecycle information of a facility that would suit the requirements of the process industries. At the time that the project duration had elapsed, a consortium of companies involved in the process industries had been established: EPISTLE (European Process Industries STEP Technical Liaison Executive). Initially individual companies were members, but later this changed into a situation where three national consortia were the only members: PISTEP (UK), POSC/Caesar (Norway), and USPI-NL (Netherlands). (later PISTEP merged into POSC/Caesar, and USPI-NL was renamed to USPI).\n\nEPISTLE took over the work of the ProcessBase project. Initially this work involved a standard called ISO 10303-221 (referred to as \"STEP AP221\"). In that AP221 we saw, for the first time, an Annex M with a list of standard instances of the AP221 data model, including types of objects. These standard instances would be for reference and would act as a knowledge base with knowledge about the types of objects.\nIn the early nineties EPISTLE started an activity to extend Annex M to become a library of such object classes and their relationships: STEPlib. In the STEPlib activities a group of approx. 100 domain experts from all three member consortia, spread over the various expertises (e.g. Electrical, Piping, Rotating equipment, etc.), worked together to define the \"core classes\".\n\nThe development of STEPlib was extended with many additional classes and relationships between classes and published as Open Source data. Furthermore, the concepts and relation types from the AP221 and ISO 15926-2 data models were also added to the STEPlib dictionary. This resulted in the development of Gellish English, whereas STEPlib became the Gellish English dictionary. Gellish English is a structured subset of natural English and is a modeling language suitable for knowledge modeling, product modeling and data exchange. It differs from conventional modeling languages (meta languages) as used in information technology as it not only defines generic concepts, but also includes an English dictionary. The semantic expression capability of Gellish English was significantly increased by extending the number of relation types that can be used to express knowledge and information.\n\nFor modelling-technical reasons POSC/Caesar proposed another standard than ISO 10303, called ISO 15926. EPISTLE (and ISO) supported that proposal, and continued the modelling work, thereby writing Part 2 of ISO 15926. This Part 2 has official ISO IS (International Standard) status since 2003.\n\nPOSC/Caesar started to put together their own RDL (Reference Data Library). They added many specialized classes, for example for ANSI (American National Standards Institute) pipe and pipe fittings. Meanwhile, STEPlib continued its existence, mainly driven by some members of USPI. Since it was clear that it was not in the interest of the industry to have two libraries for, in essence, the same set of classes, the Management Board of EPISTLE decided that the core classes of the two libraries shall be merged into Part 4 of ISO 15926. This merging process has been finished. Part 4 should act as reference data for part 2 of ISO 15926 as well as for ISO 10303-221 and replaced its Annex M. On June 5, 2007 ISO 15926-4 was signed off as a TS (Technical Specification).\n\nIn 1999 the work on an earlier version of Part 7 started. Initially this was based on XML Schema (the only useful W3C Recommendation available then), but when Web Ontology Language (OWL) became available it was clear that provided a far more suitable environment for Part 7. Part 7 passed the first ISO ballot by the end of 2005, and an implementation project started. A formal ballot for TS (Technical Specification) was planned for December 2007. However, it was decided then to split Part 7 into more than one part, because the scope was too wide.\n\nISO 15926 has eleven parts (as of June 2009):\n\n\nThe model and the library are suitable for representing lifecycle information about technical installations and their components.\n\nThey can also be used for defining the terms used in product catalogs in e-commerce. Another, more limited, use of the standard is as a reference classification for harmonization purposes between shared databases and product catalogues that are not based on ISO 15926.\n\nThe purpose of ISO 15926 is to provide a Lingua Franca for computer systems, thereby integrating the information produced by them. Although set up for the process industries with large projects involving many parties, and involving plant operations and maintenance lasting decades, the technology can be used by anyone willing to set up a proper vocabulary of reference data in line with Part 4.\n\nIn Part 7 the concept of Templates is introduced. These are semantic constructs, using Part 2 entities, that represent a small piece of information. These constructs then are mapped to more efficient classes of n-ary relations that interlink the Nodes that are involved in the represented information.\n\nIn Part 8 the data model of Part 2 is mapped to OWL, and so are, in concept, the Reference Data of Part 4 and the templates of Part 7. For validation and reasoning purposes all are represented in First-Order Logic as well.\n\nIn Part 9 these Node and Template instances are stored in Façades. A Façade is an RDF quad store, set up to a standard schema and an API. Any Façade only stores the data for which the Façade owner is responsible.\n\nEach participating computer system maps its data from its internal format to such ISO-standard Node and Template instances. These are stored in a System Façade, each system its own Façade.\n\nData can be \"handed over\" from one Façade to another in cases where data custodianship is handed over (e.g. from a contractor to a plant owner, or from a manufacturer to the owners of the manufactured goods). Hand-over can be for a part of all data, whilst maintaining full referential integrity.\n\nFaçades can be set up for the consolidation of data by handing over data produced by various participating computer systems and stored in their System Façades. Examples are: a Façade for a project discipline, a project, a plant).\n\nDocuments are user-definable. They are defined in XML Schema and they are, in essence, only a structure containing cells that make reference to instances of Templates. This represents a view on all lifecycle data: since the data model is a 4D (space-time) model, it is possible to present the data that was valid at any given point in time, thus providing a true historical record. It is expected that this will be used for Knowledge Mining.\n\nData can be queried by means of SPARQL. In any implementation a restricted number of Façades can be involved, with different access rights. This is done by means of creating a CPF Server (= Confederation of Participating Façades). An Ontology Browser allows for access to one or more Façades in a given CPF, depending on the access rights.\n\nThere are a number of projects working on the extension of the ISO 15926 standard in different application areas.\n\nWithin the application of Capital Intensive projects, some cooperating implementation projects are running:\n\n\nFinalised projects include:\n\n\nThe Norwegian Oil Industry Association (OLF) has decided to use ISO 15926 (also known as the Oil and Gas Ontology) as the instrument for integrating data across disciplines and business domains for the Upstream Oil and Gas industry. It is seen as one of the enablers of what has been called the next (or second) generation of Integrated operations, where a better integration across companies is the goal.\n\nThe following projects are currently running (May 2009):\n\n\nFinalised projects include:\n\n\nOne of the main requirements was (and still is) that the scope of the data model covers the entire lifecycle of a facility (e.g. oil refinery) and its components (e.g. pipes, pumps and their parts, etc.). Since such a facility over such a long time entails many different types of activities on a myriad of different objects it became clear that a generic and data-driven data model would be required.\n\nA simple example will illustrate this. There are thousands of different types of physical objects in a facility (pumps, compressors, pipes, instruments, fluids, etc). Each of these has many properties. If all combinations would be modelled in a \"hard-coded\" fashion, the number of combinations would be staggering, and unmanageable.\n\nThe solution is a \"template\" that represents the semantics of: \"This object has a property of X yyyy\" (where yyyy is the unit of measure). Any instance of that template refers to the applicable reference data:\n\nWithout being able to make reference to those classes, via the Internet, it will be impossible to express this information.\n\n"}
{"id": "49926523", "url": "https://en.wikipedia.org/wiki?curid=49926523", "title": "Idiobiology", "text": "Idiobiology\n\nIdiobiology is a branch of biology which studies individual organisms, or the study of organisms as individuals.\n"}
{"id": "5255165", "url": "https://en.wikipedia.org/wiki?curid=5255165", "title": "Inca Roads (song)", "text": "Inca Roads (song)\n\n\"Inca Roads\" is the opening track of the Frank Zappa and The Mothers of Invention 1975 album, \"One Size Fits All\". The song features unusual time signatures, lyrics and vocals. The marimba-playing of Zappa's percussionist Ruth Underwood is featured prominently. The song was played in concert from 1973 to 1976, 1979 and 1988.\n\n\"Inca Roads\" for the most part explores the stereotypes of aliens encountering the Incan civilization. These themes, like the album cover of \"One Size Fits All\" seem to parody the spirituality of many progressive rock albums around the same era. The lyrics \"Did a vehicle come from somewhere out there, just to land in the Andes? Was it round and did it have a motor or was it something different?\" imply that a UFO is landing in the Andes mountains. As the song progresses, the lyrics become sillier and seem to mock the beginning of the song. An example of this is \"...or did someone build a place or leave a space for Chester's thing to land (Chester's thing... on Ruth). Did a booger-bear come from somewhere out there...\" The non-serious nature of these lyrics and even the music itself seem to be mocking other progressive rock bands and their possibly forced divine depth.\n\n\"Inca Roads\" uses mixed meter. The time signatures include , , , , , , , , , , , and possibly others.\n\nThe song starts with dominant vocals, drums, and marimba, but soon features a massive, iconic guitar solo performed by Zappa in late September 1974 at a live performance in Helsinki, Finland. An edited version of this solo recording (and part of the bass and drums accompaniment) was \"grafted\" onto the KCET track and forms the backbone of the OSFA version. Later, George Duke plays an equally complex solo in . On the video, Zappa is seen smiling gleefully, as he plays the backup chords. After a short marimba solo, \"Inca Roads\" reprises its snappy intro. The song ends with the lyrics \"On Ruth, on Ruth, that's Ruth!\" acknowledging Underwood for her leading on the marimba.\n\nIn an interview vocalist and keyboard player George Duke said that Zappa pushed for him to sing on \"Inca Roads\" and that beforehand Duke had no intentions of singing professionally and was only there to play keyboards. He went on to explain how Zappa had bought him a synthesizer (an instrument which Duke had disliked) and told him he could play around with it if he wanted. This led to Duke playing the synth part on \"Inca Roads\" as well.\n\nMany early LP copies contain a skip during \"Inca Roads\" at approximately 4:40 into the track (the end of the solo from Helsinki). This error was a manufacturing defect not caught during the test pressing stage. The album was recalled after the mistake was caught, but a significant number had already been sold. The highly complex nature of the music made it difficult to recognize the error without comparing it to the correct version.\n\n"}
{"id": "14854597", "url": "https://en.wikipedia.org/wiki?curid=14854597", "title": "Irving Geis", "text": "Irving Geis\n\nIrving Geis (October 18, 1908 – July 22, 1997) was an American artist who worked closely with biologists. Geis's hand-drawn work depicts many structures of biological macromolecules, such as DNA and proteins, including the first crystal structure of sperm whale myoglobin.\n\nGeis was born in New York City, and lived in Anderson, South Carolina for a time. He studied architecture at Georgia Tech from 1925 to 1927, and went on to get a Bachelor of Fine Arts at the University of Pennsylvania in 1929. From there he attended the University of South Carolina from 1932 to 1933, graduating with a degree in design and painting in the midst of the great depression.\n\nGeis served as a coauthor and illustrator of many biochemical books that were written by Albert Lehninger and Richard E. Dickerson, as well as the book \"How to Lie with Statistics\" by Darrell Huff. He was a frequent contributor to \"Scientific American\".\n\n"}
{"id": "4752147", "url": "https://en.wikipedia.org/wiki?curid=4752147", "title": "Isenthalpic process", "text": "Isenthalpic process\n\nAn isenthalpic process or isoenthalpic process is a process that proceeds without any change in enthalpy, \"H\"; or specific enthalpy, \"h\".\n\nIn a steady-state, steady-flow process, significant changes in pressure and temperature can occur to the fluid, and yet the process will be isenthalpic if there is no transfer of heat to or from the surroundings, no work done on or by the surroundings, and no change in the kinetic energy of the fluid. (If a steady-state, steady-flow process is analysed using a control volume, everything outside the control volume is considered to be the \"surroundings\".)\n\nThe throttling process is a good example of an isenthalpic process. Consider the lifting of a relief valve or safety valve on a pressure vessel. The specific enthalpy of the fluid inside the pressure vessel is the same as the specific enthalpy of the fluid as it escapes from the valve. With a knowledge of the specific enthalpy of the fluid and the pressure outside the pressure vessel, it is possible to determine the temperature and speed of the escaping fluid.\n\nIn an isenthalpic process:\n\nIsenthalpic processes on an ideal gas follow isotherms, since formula_3.\n\n\n\n \n"}
{"id": "1104121", "url": "https://en.wikipedia.org/wiki?curid=1104121", "title": "Joint Unmanned Combat Air Systems", "text": "Joint Unmanned Combat Air Systems\n\nJoint Unmanned Combat Air Systems, or J-UCAS, was the name for the joint U.S. Navy and U.S. Air Force unmanned combat air vehicle procurement project. The two vehicles involved in the project were the Boeing X-45 and Northrop Grumman X-47. J-UCAS was managed by the Defense Advanced Research Projects Agency. In the 2006 Quadrennial Defense Review, it was stated that the J-UCAS program would be terminated and instead a new long-range strategic bomber program, \"Next-Generation Bomber\", for the Air Force has been launched. The program was revitalized into a Navy-only program named UCAS-D.\n\nIt is unusual for DARPA to be involved with advanced development programs; the agency normally performs proof-of-concept demonstrations and then hands follow-on programs on to interested military services. Apparently this arrangement was driven from the office of the secretary of defense, the idea being that DARPA would be able to keep the development effort on track until advanced demonstrators were available, and then the program would have so much momentum that it would keep on going. The long list of US military UAV programs that have been bungled and dropped after much expense and effort, with some of them like the Hunter ending up effectively reaching operational service in spite of it, provoked the decision.\n\nOf course, the candidates for the J-UCAS program included developments of the follow-ons to the Boeing X-45A and the Northrop Grumman X-47 Pegasus. DARPA and Boeing had been working on the \"X-45B\", a scaled-up X-45A that was seen as the prototype for an operational machine that would reach service in 2008, and would carry a 1,590 kilogram (3,500 pound) warload to a combat radius of 1,665 kilometers (900 nautical miles). Two were to be built, but before any metal could be bent for the two X-45B prototypes planned, the Air Force redirected the effort to an even more capable machine, the \"X-45C\".\n\nThe X-45C, as currently envisioned, will be a flying wing powered by a single F404-GE-102D turbofan engine. Current specs include:\n\nThe payload and range specifications are as defined by J-UCAS requirements. The operational radius specification is for a strike to a predefined target and back home again. A secondary range specification dictates a two-hour loiter capability at a radius of 1,850 kilometers (1,000 NMI).\n\nPartly because of the pressure from Boeing, in the summer of 2003, Northrop Grumman formed an alliance with Lockheed Martin to help develop the \"X-47B\", a follow-on to the X-47 that would compete against Boeing efforts. The alliance, which repeats the successful teamup that won the F-35 Joint Strike Fighter effort, is focused on building a modular stealthy UCAV that could be adapted to a wide range of missions. It would have a speed of Mach 0.8 at 10,670 meters (35,000 feet) and endurance of up to 12 hours.\n\nThe goal of the J-UCAS effort is to select a single contractor to provide from 10 to 12 machines for operational evaluation in the 2007-2008 time frame. Current plans are to obtain two X-45Cs and two X-47Bs to perform a comparative evaluation and then select a winner for development in the 2010 time frame.\n\nSpecifications for the J-UCAS are still evolving. Right now, both services envision a stealthy machine with a pricetag of $10 to $15 million USD, which is actually modest for a sophisticated stealthy combat aircraft. The USAF envisions that J-UCAS will feature:\n\n\nThe USAF has envisioned an operational UCAV as being stored in broken-down form inside a container that can be airlifted, with the UCAV having a specified \"shelf life\" of 20 years. It would be removed from the container every few years for inspections and could be checked with an electronic test system. The Air Force would also like to use an operational UCAV as the basis of a \"penetrating jammer\" platform that would penetrate enemy airspace to blind hostile radars. It would replace the Grumman EA-6B Prowler manned electronic warfare aircraft in this role. A reconnaissance payload is also being considered. However, the Air Force wants to focus on the strike role first.\n\nOver the long term, the Air Force is interested in using a UCAV as a platform to carry directed-energy weapons, initially a \"high power microwave (HPM)\" weapon to fry adversary electronic systems. The HPM weapon would be \"fired\" out an aperture on the front of the aircraft, with electronic steering used to direct the beam over an arc covering about 45 degrees to either side of the UCAV. The HPM weapon could be followed by a high power laser weapon.\n\nThe Navy is interested in many of the features on the Air Force wish list, though the Navy has put reconnaissance and jamming at the top of the list and strike at the bottom, and seems to be indifferent to the containerization concept. Of course, a navalized UCAV would have a stronger airframe and landing gear for carrier takeoffs and landings; an arresting hook; and avionics for automated carrier approach and landing, along with a \"relative navigation system\" that will tell the UCAV where it is relative to the carrier.\n\nA big concern of program officials is to ensure that widespread use of UCAVs does not increase the number of friendly fire incidents or collateral damage to civilian targets. The evaluation program will investigate this matter in detail. One of the concepts now being given considerable thought is use of a manned aircraft, such as an F-15E Strike Eagle, as a UCAV \"mothership\", with the weapons systems officer in the back seat directing one or more UCAVs over high-speed datalink.\n\n"}
{"id": "2382824", "url": "https://en.wikipedia.org/wiki?curid=2382824", "title": "Jordanus (constellation)", "text": "Jordanus (constellation)\n\nJordanus (the \"Jordan River\") was a constellation introduced in 1612 (or 1613) by Petrus Plancius. \n\nOne end was in Canes Venatici and then it flowed through Leo Minor and Lynx and ended near Camelopardalis. This constellation was not adopted in the atlases of Johann Bode and fell into disuse.\n\n"}
{"id": "48813195", "url": "https://en.wikipedia.org/wiki?curid=48813195", "title": "Kirk (crater)", "text": "Kirk (crater)\n\nKirk Crater is the unofficial name given to a small crater on Pluto's largest moon Charon. The crater was discovered by the \"New Horizons\" space probe in 2015 during its flyby of Pluto and its moons. It was named after Captain Kirk from the \"Star Trek\" franchise and TV series. The crater is located in an area of the moon astronomers have named the Vulcan Planum.\n\n"}
{"id": "41555934", "url": "https://en.wikipedia.org/wiki?curid=41555934", "title": "Knowledge Based Software Assistant", "text": "Knowledge Based Software Assistant\n\nThe Knowledge Based Software Assistant (KBSA) was a research program funded by the United States Air Force. The goal of the program was to apply concepts from artificial intelligence to the problem of designing and implementing computer software. Software would be described by models in very high level languages (essentially equivalent to first order logic) and then transformation rules would transform the specification into efficient code. The air force hoped to be able to generate the software to control weapons systems and other command and control systems using this method. As software was becoming ever more critical to USAF weapons systems it was realized that improving the quality and productivity of the software development process could have significant benefits for the military, as well as for information technology in other major US industries.\n\nIn the early 1980s the United States Air Force realized that they had received significant benefits from applying artificial intelligence technologies to solving expert problems such as the diagnosis of faults in aircraft. The air force commissioned a group of researchers from the artificial intelligence and formal methods communities to develop a report on how such technologies might be used to aid in the more general problem of software development.\n\nThe report described a vision for a new approach to software development. Rather than define specifications with diagrams and manually transform them to code as was the current process, the KBSA vision was to define specifications in very high level languages and then to use transformation rules to gradually refine the specification into efficient code on heterogeneous platforms.\n\nEach step in the design and refinement of the system would be recorded as part of an integrated repository. In addition to the artifacts of software development the processes, the various definitions and transformations, would also be recorded in a way that they could be analyzed and also replayed later as needed. The idea was that each step would be a transformation that took into account various non-functional requirements for the implemented system. For example, requirements to use specific programming languages such as Ada or to harden code for real time mission critical fault tolerance.\n\nThe air force decided to fund further research on this vision through their Rome Air Development Center laboratory at Griffiss air force base in New York. The majority of the early research was conducted at the Kestrel Institute in Northern California (with Stanford University) and the Information Sciences Institute (ISI) in Southern California (with USC and UCLA). The Kestrel Institute focused primarily on the provably correct transformation of logical models to efficient code. ISI focused primarily on the front end of the process on defining specifications that could map to logical formalisms but were in formats that were intuitive and familiar to systems analysts. In addition, Raytheon did a project to investigate informal requirements gathering and Honeywell and Harvard University did work on underlying frameworks, integration, and activity coordination.\n\nAlthough not primarily funded by the KBSA program the MIT Programmer's Apprentice project also had many of the same goals and used the same techniques as KBSA.\n\nIn the later stages of the KBSA program (starting in 1991) researchers developed prototypes that were used on medium to large scale software development problems. Also, in these later stages the emphasis shifted from a pure KBSA approach to more general questions of how to use knowledge-based technology to supplement and augment existing and future computer-aided software engineering (CASE) tools. In these later stages there was significant interaction between the KBSA community and the object-oriented and software engineering communities. For example, KBSA concepts and researchers played an important role in the mega-programming and user centered software engineering programs sponsored by the Defense Advanced Research Projects Agency (DARPA). In these later stages the program changed its name to Knowledge-Based Software Engineering (KBSE). The name change reflected the different research goal, no longer to create a totally new all encompassing tool that would cover the complete software life cycle but to gradually work knowledge-based technology into existing tools. Companies such as Andersen Consulting (one of the largest system integrators and at the time vendor of their own CASE tool) played a major role in the program in these later stages.\n\nThe transformation rules that KBSA used were different than traditional rules for expert systems. Transformation rules matched against specification and implementation languages rather than against facts in the world. It was possible to specify transformations using patterns, wildcards, and recursion on both the right and left hand sides of a rule. The left hand expression would specify patterns in the existing knowledge base to search for. The right hand expression could specify a new pattern to transform the left hand side into. For example, transform a set theoretic data type into code using an Ada set library.\n\nThe initial purpose for transformation rules was to refine a high level logical specification into well designed code for a specific hardware and software platform. This was inspired by early work on theorem proving and automatic programming. However, researchers at the Information Sciences Institute (ISI) developed the concept of \"evolution transformations\". Rather than transforming a specification into code an evolution transformation was meant to automate various stereotypical changes at the specification level, for example developing a new superclass by extracting various capabilities from an existing class that can be shared more generally. Evolution transformations were developed at approximately the same time as the emergence of the software patterns community and the two groups shared concepts and technology. Evolution transformations were essentially what is known as refactoring in the object-oriented software patterns community.\n\nA key concept of KBSA was that all artifacts: requirements, specifications, transformations, designs, code, process models, etc. were represented as objects in a knowledge-based repository. The original KBSA report describes what was called a Wide Spectrum Language. The requirement was for a knowledge representation framework that could support the entire life cycle: requirements, specification, and code as well as the software process itself. The core representation for the knowledge base was meant to utilize the same framework although various layers could be added to support specific presentations and implementations.\n\nThese early knowledge-base frameworks were developed primarily by ISI and Kestrel building on top of Lisp and Lisp machine environments. The Kestrel environment was eventually turned into a commercial product called Refine which was developed and supported by a spin-off company from Kestrel called Reasoning Systems Incorporated.\n\nThe Refine language and environment also proved to be applicable to the problem of software reverse engineering: taking legacy code that is critical to the business but that lacks proper documentation and using tools to analyze it and transform it to a more maintainable form. With the growing concern of the Y2K problem reverse engineering was a major business concern for many large US corporations and it was a focus area for KBSA research in the 1990s.\n\nThere was significant interaction between the KBSA communities and the Frame language and object-oriented communities. The early KBSA knowledge-bases were implemented in object-based languages rather than object-oriented. Objects were represented as classes and sub-classes but it was not possible to define methods on the objects. In later versions of KBSA such as the Andersen Consulting Concept Demo the specification language was expanded to support message passing as well.\n\nKBSA took a different approach than traditional expert systems when it came to how to solve problems and work with users. In the traditional expert system approach the user answers a series of interactive questions and the system provides a solution. The KBSA approach left the user in control. Where as an expert system tried to, to some extent replace and remove the need for the expert the intelligent assistant approach in KBSA sought to re-invent the process with technology. This led to a number of innovations at the user interface level.\n\nAn example of the collaboration between the object-oriented community and KBSA was the architecture used for KBSA user interfaces. KBSA systems utilized a model-view-controller (MVC) user interface. This was an idea incorporated from Smalltalk environments. The MVC architecture was especially well suited to the KBSA user interface. KBSA environments featured multiple heterogeneous views of the knowledge-base. It might be useful to look at an emerging model from the standpoint of entities and relations, object interactions, class hierarchies, dataflow, and many other possible views. The MVC architecture facilitated this. With the MVC architecture the underlying model was always the knowledge base which was a meta-model description of the specification and implementation languages. When an analyst made some change via a particular diagram (e.g. added a class to the class hierarchy) that change was made at the underlying model level and the various views of the model were all automatically updated.\n\nOne of the benefits of using a transformation was that many aspects of the specification and implementation could be modified at once. For small scale prototypes the resulting diagrams were simple enough that basic layout algorithms combined with reliance on users to clean up diagrams was sufficient. However, when a transformation can radically redraw models with tens or even hundreds of nodes and links the constant updating of the various views becomes a task in itself. Researchers at Andersen Consulting incorporated work from the University of Illinois on graph theory to automatically update the various views associated with the knowledge base and to generate graphs that have minimal intersection of links and also take into account domain and user specific layout constraints.\n\nAnother concept used to provide intelligent assistance was automatic text generation. Early research at ISI investigated the feasibility of extracting formal specifications from informal natural language text documents. They determined that the approach was not viable. Natural language is by nature simply too ambiguous to serve as a good format for defining a system. However, natural language generation was seen to be feasible as a way to generate textual descriptions that could be read by managers and non-technical personnel. This was especially appealing to the air force since by law they required all contractors to generate various reports that describe the system from different points of view. Researchers at ISI and later Cogentext and Andersen Consulting demonstrated the viability of the approach by using their own technology to generate the documentation required by their air force contracts.\n"}
{"id": "37899283", "url": "https://en.wikipedia.org/wiki?curid=37899283", "title": "List of Columbiformes by population", "text": "List of Columbiformes by population\n\nThis is a list of Columbiformes species by global population. While numbers are estimates, they have been made by the experts in their fields. For more information on how these estimates were ascertained, see Wikipedia's articles on population biology and population ecology.\n\nThis list is not comprehensive, as not all Columbiformes have had their numbers quantified.\n"}
{"id": "1934103", "url": "https://en.wikipedia.org/wiki?curid=1934103", "title": "List of craters with ray systems", "text": "List of craters with ray systems\n\nThis is a list of craters with ray systems. In the following tables, the listed coordinates and the diameter are for the crater.\n\nThe following craters on Mercury possess ray systems.\n\nThe following craters on Mars possess ray systems (These were discovered in Thermal Emission Imaging System infrared images).\n\nThis table lists the lunar impact craters that have ray systems. Crater names followed by a letter are satellite craters associated with the primary crater of the same name.\n\n\n"}
{"id": "2864740", "url": "https://en.wikipedia.org/wiki?curid=2864740", "title": "List of lunar meteorites", "text": "List of lunar meteorites\n\nThis is a list of lunar meteorites. That is, meteorites that have been identified as having originated from Earth's Moon. \n\nWhere multiple meteorites are listed (e.g. NWA 4472/4485), they are believed to be pieces of the same original body. The mass shown is the total.\n\n\nSource: University of Washington in St Louis, Department of Earth and Planetary Science.\n\n\n"}
{"id": "24594156", "url": "https://en.wikipedia.org/wiki?curid=24594156", "title": "List of model checking tools", "text": "List of model checking tools\n\nThis article lists model checking tools and gives a synthetic overview their functionalities.\n\nThe following table includes model checkers that have\n(1) a web site from which it can be downloaded,\n(2) a declared license,\n(3) a description published in archived literature, and\n(4) a Wikipedia article describing it.\n\nIn the above table, the following abbreviations are used:\n\n\n\n\nThere exists a few papers that systematically compare various model checkers on a common case study. The comparison usually discusses the modelling tradeoffs faced when using the input languages of each model checker, as well as the comparison of performances of the tools when verifying correctness properties. One can mention:\n\n\n\n\n\n\n\n\nOther model checkers that do not yet have a Wikipedia page:\nand\n"}
{"id": "44555592", "url": "https://en.wikipedia.org/wiki?curid=44555592", "title": "List of people with Guillain–Barré syndrome", "text": "List of people with Guillain–Barré syndrome\n\nA number of notable people have been affected by the rare peripheral nervous system condition Guillain–Barré syndrome.\n\n\nNOTE: Franklin D. Roosevelt, The 32nd President of the United States, was stricken with a paralytic illness in 1921, at age 39. His main symptoms were fever; symmetric, ascending paralysis; facial paralysis; bowel and bladder dysfunction; numbness and hyperesthesia; and a descending pattern of recovery. He was left permanently paralyzed from the waist down. Roosevelt was diagnosed with \"infantile paralysis\" (paralytic polio) at the time, but his symptoms are more consistent with Guillain–Barré syndrome, which his doctors failed to consider as a diagnostic possibility. See Franklin D. Roosevelt's paralytic illness for more information.\n"}
{"id": "24619829", "url": "https://en.wikipedia.org/wiki?curid=24619829", "title": "List of prolific inventors", "text": "List of prolific inventors\n\nThomas Alva Edison was widely known as the America's most prolific inventor, even after his death in 1931. He held a total of 1,093 U.S. patents (1,084 utility patents and 9 design patents). In 2003, he was passed by Japanese inventor Shunpei Yamazaki. On February 26, 2008, Yamazaki was passed by Australian inventor Kia Silverbrook. Yamazaki passed Silverbrook in 2017.\n\nInventors with 200 or more worldwide utility patents are shown in the following table. While in many cases this is the number of utility patents granted by the United States Patent and Trademark Office, it may include utility patents granted by other countries, as noted by the source references for an inventor.\n\nThis table was current .\nThe columns are defined as follows:\n\nAs the average number of patents per inventor is around 3, some sources define prolific inventors as five times above the average (in terms of patents), leading to a threshold of 15 patents. However, this table currently has an arbitrary cut-off limit for inclusion of 200 patents. This is purely for practical reasons – there are tens of thousands of inventors with more than 15 patents. The threshold of 200 patents means that some famous prolific inventors such as Nikola Tesla are not included in this list, as Tesla had 111 patents.\n\nThis table is a sortable list of the most prolific inventors as measured by utility patents granted. It does not include other types of invention, such as inventions that were never applied for nor granted, for which there is no known source. Nor does the table attempt to measure the significance of an inventor and their inventions. The significance of inventions is often not apparent until many decades after the invention has been made. For recent inventors, it is not yet possible to determine their place in history.\n\nThe common symbol for inventiveness, the light bulb, is an example. The first incandescent light bulb was invented by British chemist Sir Humphry Davy in 1802. Many subsequent inventors improved Davy's invention prior to the successful commercialization of electric lighting by Thomas Edison in 1880, 78 years later. Electric lighting continued to be developed. Edison's carbon filament light bulb was made obsolete by the tungsten filament light bulb, invented in 1904 by Sándor Just and Franjo Hanaman. It is this that forms the popular conception of a light bulb, though there are other major forms of lighting. The principle of fluorescent lights was known since 1845, and various inventors, including Edison and Nikola Tesla worked on them without commercial success. Various improvements were made by many other inventors, until General Electric introduced \"fluorescent lumiline lamps\" commercially in 1938, first available to the public at the 1939 World's Fair. LED lamps also have a long history, with the first light-emitting diode (LED) invented in 1927 by Oleg Losev. LEDs were initially of low brightness, and have been used as indicator lamps and seven-segment displays since 1968. It wasn't until the development of high efficiency blue LEDs by Shuji Nakamura in the 1980s that white LEDs for lighting applications became practical. Although higher cost than incandescent light bulbs, LEDs have higher efficiency and longer life and may finally displace light bulbs in general lighting applications. In each case, more than 50 years passed between the initial invention and commercial success in general lighting applications.\n\nRankings of prolific inventors have been published at various times. However, until the patent records were digitized, these lists were very tedious to prepare, as many thousands of patent records had to be checked manually. Even after digitization, it is still not a simple process. While the USPTO keeps statistics for annual rankings of inventions assigned to companies, it no longer publishes rankings of individual inventors. The last such list was published by the USPTO in 1998. Also, patents predating 1976 have not yet been digitized in the USPTO records. This means that patents before 1976 will not be included in a USPTO search by inventor name, and the number of patents granted before 1976 must be added to current searches.\n\nIn January 1936, \"Popular Science\" published a list of the \"most prolific living inventors to be found in America today\".\n\nThomas Edison was not included in the list, as he died in 1931, five years earlier.\n\nOn December 4, 2000, Time Magazine published a list of the \"top five inventors\".\n\nThis list only included U.S. inventors, so omitted Canadian inventor George Albert Lyon, with 993 U.S. patents at the time of publication, Japanese inventor Shunpei Yamazaki, with 745 U.S. patents, and Béla Barényi, with 595 German patents. Also omitted were John F. O'Connor with 949 U.S. patents, and Carleton Ellis, with 753 U.S. patents at the time of publication.\n\nOn December 13, 2005 USA Today published a list of \"the top 10 living U.S. patent holders\":\nThis research was performed by ipIQ of Chicago (now \"The Patent Board\") and 1790 Analytics of New Jersey.\nThis list only considered living inventors, and thus did not include such prolific inventors as Thomas Edison, Melvin De Groote, and Elihu Thomson. This list included design patents, which are not patents for inventions.\n\nOn October 15, 2007 Condé Nast Portfolio Magazine published a list of \"the world's most prolific inventors alive\":\nThis research was performed by The Patent Board, a Chicago patent research and advisory firm.\nAs with the USA Today list, the Portfolio list only considered living inventors, and thus did not include such prolific inventors as Thomas Edison. This list also included design patents, which are not patents for inventions.\n\nOn 6 May 2011 Business Insider published an article titled: \"The Ten Greatest Inventors In The Modern Era\" containing the following list:\n\nThis list included living and dead inventors, and only included granted utility patents (patents for inventions).\n\nStrutpatent.com publishes a list of the \"Top 10 Inventors\" listing inventors ranked by US patents (of all types) issued since 1990:\n\nThis list included only patents granted since 1990, and includes design patents as well as utility patents.\n\nStrutpatent.com publishes weekly, monthly, and annual lists of the top ten categories, inventors and assignees of US patents since 2007. These lists include all patent types, not just patents for inventions (utility patents).\n\nThe top ten inventors of US patents for 2007:\n\nThe top ten inventors of US patents for 2008:\n\nThe top ten inventors of US patents for 2009:\n\nThe top ten inventors of US patents for 2010:\n\nThe top ten inventors of US patents for 2011:\n\nThe top ten inventors of US patents for 2012:\n\nThis table omitted Rick Allen Hamilton II. The USPTO database shows Hamilton was an inventor or co-inventor of 128 US patents granted in 2012, which would place Hamilton at 6th rank for 2012.\n\nDifferences in patent numbers between the various lists are due to several reasons:\n\n"}
{"id": "12268322", "url": "https://en.wikipedia.org/wiki?curid=12268322", "title": "List of women with ovarian cancer", "text": "List of women with ovarian cancer\n\nThis is a list of notable women who have or had ovarian cancer, whose illness attracted publicity.\n"}
{"id": "12306500", "url": "https://en.wikipedia.org/wiki?curid=12306500", "title": "Matthews correlation coefficient", "text": "Matthews correlation coefficient\n\nThe Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between −1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and −1 indicates total disagreement between prediction and observation. The statistic is also known as the phi coefficient. MCC is related to the chi-square statistic for a 2×2 contingency table\n\nwhere \"n\" is the total number of observations.\n\nWhile there is no perfect way of describing the confusion matrix of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures. Other measures, such as the proportion of correct predictions (also termed accuracy), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification.\n\nThe MCC can be calculated directly from the confusion matrix using the formula:\n\nIn this equation, \"TP\" is the number of true positives, \"TN\" the number of true negatives, \"FP\" the number of false positives and \"FN\" the number of false negatives. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.\n\nThe original formula as given by Matthews was:\n\nThis is equal to the formula given above. As a correlation coefficient, the Matthews correlation coefficient is the geometric mean of the regression coefficients of the problem and its dual. The component regression coefficients of the Matthews correlation coefficient are Markedness (Δp) and Youden's J statistic (Informedness or Δp'). Markedness and Informedness correspond to different directions of information flow and generalize Youden's J statistic, the formula_7p statistics and (as their geometric mean) the Matthews Correlation Coefficient to more than two classes.\n\nSome scientists claim the Matthews correlation coefficient to be the most informative single score to establish the quality of a binary classifier prediction in a confusion matrix context.\n\nLet us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2×2 \"contingency table\" or \"confusion matrix\", as follows:\n\nThe Matthews correlation coefficient has been generalized to the multiclass case. This generalization was called the formula_8 statistic (for K different classes) by the author, and defined in terms of a formula_9 confusion matrix formula_10\n\nWhen there are more than two labels the MCC will no longer range between -1 and +1. Instead the minimum value will be between -1 and 0 depending on the true distribution. The maximum value is always +1.\n\nAs explained by Davide Chicco in his paper \"Ten quick tips for machine learning in computational biology\" (BioData Mining, 2017), the Matthews correlation coefficient is more informative than other confusion matrix measures (such as F1 score and accuracy) in evaluating binary classification problems, because it takes into account the balance ratios of the four confusion matrix categories (true positives, true negatives, false positives, false negatives).\n\nThe paper explains, for \"Tip 8\":\n\nNote that the F1 score depends on which class is defined as the positive class. In the first example above, the F1 score is high because the majority class is defined as the positive class. Inverting the positive and negative classes results in the following confusion matrix:\n\nTP = 0, FP = 0; TN = 5, FN = 95\n\nThis gives an F1 score = 0%.\n\nThe MCC doesn't depend on which class is the positive one, which has the advantage over the F1 score to avoid incorrectly defining the positive class.\n\n"}
{"id": "250381", "url": "https://en.wikipedia.org/wiki?curid=250381", "title": "National Physical Laboratory (United Kingdom)", "text": "National Physical Laboratory (United Kingdom)\n\nThe National Physical Laboratory (NPL) is the national measurement standards laboratory for the United Kingdom, based at Bushy Park in Teddington, London, England. It comes under the management of the Department for Business, Innovation and Skills.\n\nThe National Physical Laboratory was established in 1900 at Bushy House \"to bring scientific knowledge to bear practically upon our everyday industrial and commercial life\". It grew to fill a large selection of buildings on the Teddington site. NPL procured a large state-of-the-art laboratory under a Private Finance Initiative contract in 1998. The construction, which was being undertaken by John Laing, and the maintenance of this new building, which was being undertaken by Serco, was transferred back to the DTI in 2004 after the private sector companies involved made losses of over £100m.\n\nThe laboratory was initially run by the UK government, with members of staff being part of the civil service. Administration of the NPL was contracted out in 1995 under a Government Owned Contractor Operated model, with Serco winning the bid and all staff transferred to their employ. Under this regime, overhead costs halved, third party revenues grew by 16% per annum, and the number of peer-reviewed research papers published doubled. It was decided in 2012 to change the operating model for NPL from 2014 onward to include academic partners and to establish a postgraduate teaching institute on site. The date of the changeover was later postponed for up to a year. The candidates for lead academic partner were the Universities of Edinburgh, Southampton, Strathclyde and Surrey with an alliance of the Universities of Strathclyde and Surrey chosen as preferred partners.\n\nIn January 2013 funding for a new £25m Advanced Metrology Laboratory was announced that will be built on the footprint of an existing unused building.\n\nThe operation of the laboratory transferred back to Department for Business, Innovation and Skills ownership on 1 January 2015.\n\nThe National Physical Laboratory is involved with new developments in metrology, such as researching metrology for, and standardising, nanotechnology. It is mainly based at the Teddington site, but also has a site in Huddersfield for dimensional metrology and an underwater acoustics facility at Wraysbury Reservoir.\n\nResearchers who have worked at NPL include: D. W. Dye who did important work in developing the technology of quartz clocks. The inventor Sir Barnes Wallis did early development work there on the \"Bouncing Bomb\" used in the \"Dam Busters\" wartime raids. H.J. Gough, one of the pioneers of research into metal fatigue, worked at NPL for 19 years from 1914 to 1938. Sydney Goldstein and Sir James Lighthill worked in NPL's aerodynamics division during World War II researching boundary layer theory and supersonic aerodynamics respectively. Dr Clifford Hodge also worked there and was engaged in research on semiconductors. Others who have spent time at NPL include Robert Watson-Watt, generally considered the inventor of radar, Oswald Kubaschewski, the father of computational materials thermodynamics and the numerical analyst James Wilkinson.\n\nNPL research has contributed to physical science, materials science, computing, and bioscience. Applications have been found in ship design, aircraft development, radar, computer networking and global positioning.\n\nThe first accurate atomic clock, a caesium standard based on a certain transition of the caesium-133 atom, was built by Louis Essen and Jack Parry in 1955 at NPL. Calibration of the caesium standard atomic clock was carried out by the use of the astronomical time scale \"ephemeris time\" (ET). This led to the internationally agreed definition of the latest SI second being based on atomic time.\n\nNPL has undertaken computer research since the mid-1940s. From 1945, Alan Turing led the design of the Automatic Computing Engine (ACE) computer. The ACE project was overambitious and floundered, leading to Turing's departure. Donald Davies took the project over and concentrated on delivering the less ambitious Pilot ACE computer, which first worked in May 1950. Among those who worked on the project was American computer pioneer Harry Huskey. A commercial spin-off, DEUCE was manufactured by English Electric Computers and became one of the best-selling machines of the 1950s.\n\nBeginning in the mid-1960s, Donald Davies and his team at the NPL pioneered packet switching, now the dominant basis for data communications in computer networks worldwide. Davies designed and proposed a national data network based on packet switching in his 1965 \"Proposal for the Development of a National Communications Service for On-line Data Processing\". Subsequently, the NPL team (Davies, Derek Barber, Roger Scantlebury, Peter Wilkinson, Keith Bartlett, and Brian Aldous) developed the concept into a local area network which operated from 1969 to 1986, and carried out work to analyse and simulate the performance of packet switching networks. Their research and practice influenced the ARPANET in the United States, the forerunner of the Internet, and other researchers in the UK and Europe.\n\nDirectors of NPL include a number of notable individuals.\n\nManaging Directors\n\nChief Executive Officers\n\n\n"}
{"id": "29534297", "url": "https://en.wikipedia.org/wiki?curid=29534297", "title": "Nature Farming", "text": "Nature Farming\n\n\"Nature Farming\" was established in 1936 by Mokichi Okada, the founder of the Church of World Messianity, an agricultural system originally called \"no fertilizer farming\" or in Japanese.\n\nOffshoots such as the Sekai Kyusei Kyo, promoting ‘Kyusei nature farming’, and the Mokichi Okada Association formed after his death to continue promoting the work in Japan and South-East Asia.\n\nZZ2, a farming conglomerate in South Africa has translated the term to Afrikaans, \"Natuurboerdery\".\n\nAccording to the International Nature Farming Research Center in Nagano, Japan, it is based on the theories that:\nThe term is sometimes used for an alternative farming philosophy of Masanobu Fukuoka.\n\nAnother Japanese farmer and philosopher, Masanobu Fukuoka, conceived of an alternative farming system in the 1930s separately from Okada and used the same Japanese characters to describe it. This is generally translated in English as \"Natural Farming\" although agriculture researcher Hu-lian Xu claims that \"nature farming\" is the correct literal translation of the Japanese term.\n\n\n"}
{"id": "255479", "url": "https://en.wikipedia.org/wiki?curid=255479", "title": "Operation Whetstone", "text": "Operation Whetstone\n\nThe United States's Whetstone nuclear test series was a group of 46 nuclear tests conducted in 1964-1965. These tests followed the \"Operation Niblick\" series and preceded the \"Operation Flintlock (nuclear test)\" series.\n"}
{"id": "159750", "url": "https://en.wikipedia.org/wiki?curid=159750", "title": "Pectin", "text": "Pectin\n\nPectin (from ', \"congealed, curdled\") is a structural heteropolysaccharide contained in the primary cell walls of terrestrial plants. It was first isolated and described in 1825 by Henri Braconnot. It is produced commercially as a white to light brown powder, mainly extracted from citrus fruits, and is used in food as a gelling agent, particularly in jams and jellies. It is also used in dessert fillings, medicines, sweets, as a stabilizer in fruit juices and milk drinks, and as a source of dietary fiber.\n\nIn plant biology, pectin consists of a complex set of polysaccharides (see below) that are present in most primary cell walls and are particularly abundant in the non-woody parts of terrestrial plants. Pectin is a major component of the middle lamella, where it helps to bind cells together, but is also found in primary cell walls. Pectin is deposited by exocytosis into the cell wall via vesicles produced in the golgi.\n\nThe amount, structure and chemical composition of pectin differs among plants, within a plant over time, and in various parts of a plant. Pectin is an important cell wall polysaccharide that allows primary cell wall extension and plant growth. During fruit ripening, pectin is broken down by the enzymes pectinase and pectinesterase, in which process the fruit becomes softer as the middle lamellae break down and cells become separated from each other. A similar process of cell separation caused by the breakdown of pectin occurs in the abscission zone of the petioles of deciduous plants at leaf fall.\n\nPectin is a natural part of the human diet, but does not contribute significantly to nutrition. The daily intake of pectin from fruits and vegetables can be estimated to be around 5 g if approximately 500 g of fruits and vegetables are consumed per day.\n\nIn human digestion, pectin binds to cholesterol in the gastrointestinal tract and slows glucose absorption by trapping carbohydrates. Pectin is thus a soluble dietary fiber. In non-obese diabetic (NOD) mice pectin has been shown to increase the incidence of diabetes.\n\nA study found that after consumption of fruit the concentration of methanol in the human body increased by as much as an order of magnitude due to the degradation of natural pectin (which is esterified with methyl alcohol) in the colon).\n\nPectin has been observed to have some function in DNA repair of plants. Pectinaceous surface pellicles, which are rich in pectin, create a mucilage layer that holds in dew that helps the cell repair its DNA.\n\nConsumption of pectin has been shown to slightly (3-7%) reduce blood LDL cholesterol levels. The effect depends upon the source of pectin; apple and citrus pectins were more effective than orange pulp fiber pectin. The mechanism appears to be an increase of viscosity in the intestinal tract, leading to a reduced absorption of cholesterol from bile or food. In the large intestine and colon, microorganisms degrade pectin and liberate short-chain fatty acids that have positive influence on health (prebiotic effect).\n\nPectins, also known as pectic polysaccharides, are rich in galacturonic acid. Several distinct polysaccharides have been identified and characterised within the pectic group. Homogalacturonans are linear chains of α-(1–4)-linked D-galacturonic acid. Substituted galacturonans are characterized by the presence of saccharide appendant residues (such as D-xylose or D-apiose in the respective cases of xylogalacturonan and apiogalacturonan) branching from a backbone of D-galacturonic acid residues. Rhamnogalacturonan I pectins (RG-I) contain a backbone of the repeating disaccharide: 4)-α-D-galacturonic acid-(1,2)-α-L-rhamnose-(1. From many of the rhamnose residues, sidechains of various neutral sugars branch off. The neutral sugars are mainly D-galactose, L-arabinose and D-xylose, with the types and proportions of neutral sugars varying with the origin of pectin.\n\nAnother structural type of pectin is rhamnogalacturonan II (RG-II), which is a less frequent, complex, highly branched polysaccharide. Rhamnogalacturonan II is classified by some authors within the group of substituted galacturonans since the rhamnogalacturonan II backbone is made exclusively of D-galacturonic acid units.\n\nIsolated pectin has a molecular weight of typically 60,000–130,000 g/mol, varying with origin and extraction conditions.\n\nIn nature, around 80 percent of carboxyl groups of galacturonic acid are esterified with methanol. This proportion is decreased to a varying degree during pectin extraction. The ratio of esterified to non-esterified galacturonic acid determines the behavior of pectin in food applications. This is why pectins are classified as high- vs. low-ester pectins (short HM vs. LM-pectins), with more or less than half of all the galacturonic acid esterified.\n\nThe non-esterified galacturonic acid units can be either free acids (carboxyl groups) or salts with sodium, potassium, or calcium. The salts of partially esterified pectins are called pectinates, if the degree of esterification is below 5 percent the salts are called pectates, the insoluble acid form, pectic acid.\n\nSome plants, such as sugar beet, potatoes and pears, contain pectins with acetylated galacturonic acid in addition to methyl esters. Acetylation prevents gel-formation but increases the stabilising and emulsifying effects of pectin.\n\nAmidated pectin is a modified form of pectin. Here, some of the galacturonic acid is converted with ammonia to carboxylic acid amide. These pectins are more tolerant of varying calcium concentrations that occur in use.\n\nTo prepare a pectin-gel, the ingredients are heated, dissolving the pectin. Upon cooling below gelling temperature, a gel starts to form. If gel formation is too strong, syneresis or a granular texture are the result, while weak gelling leads to excessively soft gels. Pectins gel according to specific parameters, such as sugar, pH and bivalent salts (especially Ca).\n\nIn high-ester pectins at soluble solids content above 60% and a pH-value between 2.8 and 3.6, hydrogen bonds and hydrophobic interactions bind the individual pectin chains together. These bonds form as water is bound by sugar and forces pectin strands to stick together. These form a 3-dimensional molecular net that creates the macromolecular gel. The gelling-mechanism is called a low-water-activity gel or sugar-acid-pectin gel.\n\nIn low-ester pectins, ionic bridges are formed between calcium ions and the ionised carboxyl groups of the galacturonic acid. This is idealised in the \"egg box-model\". Low-ester pectins need calcium to form a gel, and can do so at lower soluble solids and higher pH-values than high-ester pectins. Normally low-ester pectins form gels with a range of pH from 2.6 to 7.0 and with a soluble solids content between 10 and 70%.\n\nAmidated pectins behave like low-ester pectins but need less calcium and are more tolerant of excess calcium. Also, gels from amidated pectin are thermo-reversible; they can be heated and after cooling solidify again, whereas conventional pectin-gels will afterwards remain liquid.\n\nHigh-ester pectins set at higher temperatures than low-ester pectins. However, gelling reactions with calcium increase as the degree of esterification falls. Similarly, lower pH-values or higher soluble solids (normally sugars) increase gelling speeds. Suitable pectins can therefore be selected for jams and jellies, or for higher-sugar confectionery jellies.\n\nPears, apples, guavas, quince, plums, gooseberries, and oranges and other citrus fruits contain large amounts of pectin, while soft fruits, like cherries, grapes, and strawberries, contain small amounts of pectin.\n\nTypical levels of pectin in fresh plants are:\n\n\nThe main raw materials for pectin production are dried citrus peels or apple pomace, both by-products of juice production. Pomace from sugar beets is also used to a small extent.\n\nFrom these materials, pectin is extracted by adding hot dilute acid at pH-values from 1.5 – 3.5. During several hours of extraction, the protopectin loses some of its branching and chain length and goes into solution. After filtering, the extract is concentrated in a vacuum and the pectin is then precipitated by adding ethanol or isopropanol. An old technique of precipitating pectin with aluminium salts is no longer used (apart from alcohols and polyvalent cations, pectin also precipitates with proteins and detergents).\n\nAlcohol-precipitated pectin is then separated, washed and dried. Treating the initial pectin with dilute acid leads to low-esterified pectins. When this process includes ammonium hydroxide (NH(aq)), amidated pectins are obtained. After drying and milling, pectin is usually standardised with sugar and sometimes calcium salts or organic acids to have optimum performance in a particular application.\n\nThe main use for pectin (vegetable agglutinate) is as a gelling agent, thickening agent and stabilizer in food.\nThe classical application is giving the jelly-like consistency to jams or marmalades, which would otherwise be sweet juices. Pectin also reduces syneresis in jams and marmalades and increases the gel strength of low-calorie jams. For household use, pectin is an ingredient in gelling sugar (also known as \"jam sugar\") where it is diluted to the right concentration with sugar and some citric acid to adjust pH.\nIn some countries, pectin is also available as a solution or an extract, or as a blended powder, for home jam making.\nFor conventional jams and marmalades that contain above 60% sugar and soluble fruit solids, high-ester pectins are used. With low-ester pectins and amidated pectins, less sugar is needed, so that diet products can be made.\n\nPectin is used in confectionery jellies to give a good gel structure, a clean bite and to confer a good flavour release. Pectin can also be used to stabilize acidic protein drinks, such as drinking yogurt, to improve the mouth-feel and the pulp stability in juice based drinks and as a fat substitute in baked goods.\nTypical levels of pectin used as a food additive are between 0.5 and 1.0% – this is about the same amount of pectin as in fresh fruit.\n\nIn medicine, pectin increases viscosity and volume of stool so that it is used against constipation and diarrhea. Until 2002, it was one of the main ingredients used in Kaopectate a medication to combat diarrhea, along with kaolinite. It has been used in gentle heavy metal removal from biological systems. Pectin is also used in throat lozenges as a demulcent.\n\nIn cosmetic products, pectin acts as a stabilizer. Pectin is also used in wound healing preparations and specialty medical adhesives, such as colostomy devices.\n\nSriamornsak revealed that pectin could be used in various oral drug delivery platforms, e.g., controlled release systems, gastro-retentive systems, colon-specific delivery systems and mucoadhesive delivery systems, according to its intoxicity and low cost. It was found that pectin from different sources provides different gelling abilities, due to variations in molecular size and chemical composition. Like other natural polymers, a major problem with pectin is inconsistency in reproducibility between samples, which may result in poor reproducibility in drug delivery characteristics.\n\nIn ruminant nutrition, depending on the extent of lignification of the cell wall, pectin is up to 90% digestible by bacterial enzymes. Ruminant nutritionists recommend that the digestibility and energy concentration in forages be improved by increasing pectin concentration in the forage.\n\nIn cigars, pectin is considered an excellent substitute for vegetable glue and many cigar smokers and collectors use pectin for repairing damaged tobacco leaves on their cigars.\n\nYablokov \"et al.\", writing in \"\", quote research conducted by the Ukrainian Center of Radiation Medicine and the Belarusian Institute of Radiation Medicine and Endocrinology, concluded, regarding pectin's radioprotective effects, that \"adding pectin preparations to the food of inhabitants of the Chernobyl-contaminated regions promotes an effective excretion of incorporated radionuclides\" such as cesium-137. The authors reported on the positive results of using pectin food additive preparations in a number of clinical studies conducted on children in severely polluted areas, with up to 50% improvement over control groups.\n\nDuring the Second World War, Allied pilots were provided with maps printed on silk, for navigation in escape and evasion efforts. The printing process at first proved nearly impossible because the several layers of ink immediately ran, blurring outlines and rendering place names illegible until the inventor of the maps, Clayton Hutton, mixed a little pectin with the ink and at once the pectin coagulated the ink and prevented it from running, allowing small topographic features to be clearly visible.\n\nAt the Joint FAO/WHO Expert Committee Report on Food Additives and in the European Union, no numerical acceptable daily intake (ADI) has been set, as pectin is considered safe.\n\nIn the United States, pectin is generally recognized as safe for human consumption.\n\nIn the International Numbering System (INS), pectin has the number 440. In Europe, pectins are differentiated into the E numbers E440(i) for non-amidated pectins and E440(ii) for amidated pectins. There are specifications in all national and international legislation defining its quality and regulating its use.\n\nPectin was first isolated and described in 1825 by Henri Braconnot, though the action of pectin to make jams and marmalades was known long before. To obtain well-set jams from fruits that had little or only poor quality pectin, pectin-rich fruits or their extracts were mixed into the recipe.\n\nDuring the Industrial Revolution, the makers of fruit preserves turned to producers of apple juice to obtain dried apple pomace that was cooked to extract pectin.\n\nLater, in the 1920s and 1930s, factories were built that commercially extracted pectin from dried apple pomace and later citrus peel in regions that produced apple juice in both the USA and Europe.\n\nPectin was first sold as a liquid extract, but is now most often used as dried powder, which is easier than a liquid to store and handle.\n"}
{"id": "42294948", "url": "https://en.wikipedia.org/wiki?curid=42294948", "title": "Persephonella guaymasensis", "text": "Persephonella guaymasensis\n\nPersephonella guaymasensis is a thermophilic, hydrogen-oxidizing microaerophile first isolated from a deep-sea hydrothermal vent. It is strictly chemolithoautotrophic, microaerophilic, motile, 2-4 micrometres in size, rod-shaped, Gram-negative and non-sporulating. Its type strain is EX-H2.\n\n\n"}
{"id": "10096050", "url": "https://en.wikipedia.org/wiki?curid=10096050", "title": "RIKEN MDGRAPE-3", "text": "RIKEN MDGRAPE-3\n\nMDGRAPE-3 is an ultra-high performance petascale supercomputer system developed by the RIKEN research institute in Japan. It is a special purpose system built for molecular dynamics simulations, especially protein structure prediction.\n\nMDGRAPE-3 consists of 201 units of 24 custom MDGRAPE-3 chips (4,824 total), plus additional dual-core Intel Xeon processors (codename \"Dempsey\") which serve as host machines.\n\nIn June 2006 RIKEN announced its completion, achieving the petaFLOPS level of floating point arithmetic performance. This was more than three times faster than the 2006 version of the IBM Blue Gene/L system, which then led the TOP500 list of supercomputers at 0.28 petaFLOPS. Because it's not a general-purpose machine capable of running the LINPACK benchmarks, MDGRAPE-3 does not qualify for the TOP500 list.\n\n\n"}
{"id": "245796", "url": "https://en.wikipedia.org/wiki?curid=245796", "title": "Salyut 2", "text": "Salyut 2\n\nSalyut 2 (OPS-1) ( meaning \"Salute 2\") was a Soviet space station which was launched in 1973 as part of the Salyut programme. It was the first Almaz military space station to fly. Within two weeks of launch the station had lost attitude control and depressurised, leaving it unusable. Its orbit decayed and it re-entered the atmosphere on 28 May 1973, without any crews having visited it.\n\nSalyut 2 was an Almaz military space station. \nIt was designated part of the Salyut programme in order to conceal the existence of the two separate space station programmes.\n\nSalyut 2 was with a diameter of , and had an internal habitable volume of . At launch it had a mass of . A single aft-mounted docking port was intended for use by Soyuz spacecraft carrying cosmonauts to work aboard the station. Two solar arrays mounted at the aft end of the station near the docking port provided power to the station, generating a total of 3,120 watts of electricity. The station was equipped with 32 attitude control thrusters, as well as two RD-0225 engines, each capable of generating of thrust, for orbital manoeuvres.\n\nSalyut 2 was launched from Site 81/23 at the Baikonur Cosmodrome, atop a three-stage Proton-K rocket, serial number 283-01. The launch took place at 09:00:00 UTC on 3 April 1973, and successfully placed Salyut 2 into low Earth orbit. Upon reaching orbit, Salyut 2 was assigned the International Designator 1973-017A, whilst NORAD gave it the Satellite Catalog Number 06398. The third stage of the Proton-K rocket entered orbit along with Salyut 2. On 4 April, it was catalogued in a orbit, inclined at 51.4 degrees.\n\nThree days after the launch of Salyut 2, the Proton's spent third stage exploded, due to pressure changes within the tanks. This explosion resulted in a cloud of debris, some of which followed a similar trajectory to the station. Ten days later this debris struck the station, damaging the hull and causing depressurization. Both solar panels were torn free, removing the ability of the station to generate power and control its attitude.\n\nThree pieces of debris from the station were catalogued, and had decayed from orbit by 13 May. The remainder of the station reentered the atmosphere on May 28, 1973 over the Pacific Ocean.\n\nAn inquiry into the failure initially determined that a fuel line had burst, burning a hole in the station. The damage from the debris collision was only discovered later.\n\n"}
{"id": "411590", "url": "https://en.wikipedia.org/wiki?curid=411590", "title": "Science and technology studies", "text": "Science and technology studies\n\nScience and technology studies, or science, technology and society studies (both abbreviated STS) is the study of how society, politics, and culture affect scientific research and technological innovation, and how these, in turn, affect society, politics and culture.\n\nLike most interdisciplinary programs, STS emerged from the confluence of a variety of disciplines and disciplinary subfields, all of which had developed an interest—typically, during the 1960s or 1970s—in viewing science and technology as socially embedded enterprises. The key disciplinary components of STS took shape independently, beginning in the 1960s, and developed in isolation from each other well into the 1980s, although Ludwik Fleck's (1935) monograph \"Genesis and Development of a Scientific Fact\" anticipated many of STS's key themes. In the 1970s Elting E. Morison founded the STS program at Massachusetts Institute of Technology (MIT), which served as a model. By 2011, 111 STS research centres and academic programs were counted worldwide.\n\n\nDuring the 1970s and 1980s, leading universities in the US, UK, and Europe began drawing these various components together in new, interdisciplinary programs. For example, in the 1970s, Cornell University developed a new program that united science studies and policy-oriented scholars with historians and philosophers of science and technology. Each of these programs developed unique identities due to variation in the components that were drawn together, as well as their location within the various universities. For example, the University of Virginia's STS program united scholars drawn from a variety of fields (with particular strength in the history of technology); however, the program's teaching responsibilities—it is located within an engineering school and teaches ethics to undergraduate engineering students—means that all of its faculty share a strong interest in engineering ethics.\n\nA decisive moment in the development of STS was the mid-1980s addition of technology studies to the range of interests reflected in science. During that decade, two works appeared \"en seriatim\" that signaled what Steve Woolgar was to call the \"turn to technology\": \"Social Shaping of Technology\" (MacKenzie and Wajcman, 1985) and \"The Social Construction of Technological Systems\" (Bijker, Hughes and Pinch, 1987). MacKenzie and Wajcman primed the pump by publishing a collection of articles attesting to the influence of society on technological design. In a seminal article, Trevor Pinch and Wiebe Bijker attached all the legitimacy of the Sociology of Scientific Knowledge to this development by showing how the sociology of technology could proceed along precisely the theoretical and methodological lines established by the sociology of scientific knowledge. This was the intellectual foundation of the field they called the social construction of technology.\n\nThe \"turn to technology\" helped to cement an already growing awareness of underlying unity among the various emerging STS programs. More recently, there has been an associated turn to ecology, nature, and materiality in general, whereby the socio-technical and natural/material co-produce each other. This is especially evident in work in STS analyses of biomedicine (such as Carl May, Annemarie Mol, Nelly Oudshoorn, and Andrew Webster) and ecological interventions (such as Bruno Latour, Sheila Jasanoff, Matthias Gross, S. Lochlann Jain, and Jens Lachmund).\n\nThe subject has several professional associations.\n\nFounded in 1975, the Society for Social Studies of Science, initially provided scholarly communication facilities, including a journal (\"Science, Technology, and Human Values\") and annual meetings that were mainly attended by science studies scholars. The society has since grown into the most important professional association of science and technology studies scholars worldwide. The Society for Social Studies of Science members also include government and industry officials concerned with research and development as well as science and technology policy; scientists and engineers who wish to better understand the social embeddedness of their professional practice; and citizens concerned about the impact of science and technology in their lives. Proposals have been made to add the word \"technology\" to the association's name, thereby reflecting its stature as the leading STS professional society, that the name is long enough as it is.\n\nIn Europe, the European Association for the Study of Science and Technology (EASST) was founded in 1981 to \"stimulate communication, exchange and collaboration in the field of studies of science and technology\". Similarly, the European Inter-University Association on Society, Science and Technology (ESST) researches and studies science and technology in society, in both historical and contemporary perspectives.\n\nIn Asia several STS associations exist.\nIn Japan, the Japanese Society for Science and Technology Studies (JSSTS) was founded in 2001. The Asia Pacific Science Technology & Society Network (APSTSN) primarily has members from Australasia, Southeast and East Asia and Oceania.\n\nIn Latin America ESOCITE (Estudios Sociales de la Ciencia y la Tecnología) is the biggest association of Science and Technology studies. The study of STS (CyT in Spanish, CTS in Portuguese) here was shaped by authors like Amílcar Herrera and Jorge Sabato y Oscar Varsavsky in Argentina, José Leite Lopes in Brazil, Miguel Wionczek in Mexico, Francisco Sagasti in Peru, Máximo Halty Carrere in Uruguay and Marcel Roche in Venezuela.\n\nFounded in 1958, the Society for the History of Technology initially attracted members from the history profession who had interests in the contextual history of technology. After the \"turn to technology\" in the mid-1980s, the society's well-regarded journal (\"Technology and Culture\") and its annual meetings began to attract considerable interest from non-historians with technology studies interests.\n\nLess identified with STS, but also of importance to many STS scholars, are the History of Science Society, the Philosophy of Science Association, and the American Association for the History of Medicine.\n\nAdditionally, within the US there are significant STS-oriented special interest groups within major disciplinary associations, including the American Anthropological Association, the American Political Science Association, the National Women's Studies Association, and the American Sociological Association.\n\nNotable peer-reviewed journals in STS include: \nStudent journals in STS include: \nSocial constructions are human created ideas, objects, or events created by a series of choices and interactions. These interactions have consequences that change the perception that different groups of people have on these constructs. Some examples of social construction include class, race, money, and citizenship.\n\nThe following also alludes to the notion that not everything is set, a circumstance or result could potentially be one way or the other. According to the article \"What is Social Construction?\" by Laura Flores, \"Social construction work is critical of the status quo. Social constructionists about X tend to hold that:\nVery often they go further, and urge that:\nIn the past, there have been viewpoints that were widely regarded as fact until being called to question due to the introduction of new knowledge. Such viewpoints include the past concept of a correlation between intelligence and the nature of a human's ethnicity or race (X may not be at all as it is).\n\nAn example of the evolution and interaction of various social constructions within science and technology can be found in the development of both the high-wheel bicycle, or velocipede, and then of the bicycle. The velocipede was widely used in the latter half of the 19th century. In the latter half of the 19th century, a social need was first recognized for a more efficient and rapid means of transportation. Consequently, the velocipede was first developed, which was able to reach higher translational velocities than the smaller non-geared bicycles of the day, by replacing the front wheel with a larger radius wheel. One notable trade-off was a certain decreased stability leading to a greater risk of falling. This trade-off resulted in many riders getting into accidents by losing balance while riding the bicycle or being thrown over the handle bars.\n\nThe first \"social construction\" or progress of the velocipede caused the need for a newer \"social construction\" to be recognized and developed into a safer bicycle design. Consequently, the velocipede was then developed into what is now commonly known as the \"bicycle\" to fit within society's newer \"social construction,\" the newer standards of higher vehicle safety. Thus the popularity of the modern geared bicycle design came as a response to the first social construction, the original need for greater speed, which had caused the high-wheel bicycle to be designed in the first place. The popularity of the modern geared bicycle design ultimately ended the widespread use of the velocipede itself, as eventually it was found to best accomplish the social-needs/ social-constructions of both greater speed and of greater safety.\n\nTechnoscience is a subset of Science, Technology, and Society studies that focuses on the inseparable connection between science and technology. It states that fields are linked and grow together, and scientific knowledge requires an infrastructure of technology in order to remain stationary or move forward. Both technological development and scientific discovery drive one another towards more advancement. Technoscience excels at shaping human thought and behavior by opening up new possibilities that gradually or quickly come to be perceived as necessities.\n\n\"Technological action is a social process.\" Social factors and technology are intertwined so that they are dependent upon each other. This includes the aspect that social, political, and economic factors are inherent in technology and that social structure influences what technologies are pursued. In other words, \"technoscientific phenomena combined inextricably with social/political/ economic/psychological phenomena, so 'technology' includes a spectrum of artifacts, techniques, organizations, and systems.\" Winner expands on this idea by saying \"in the late twentieth century technology and society, technology and culture, technology and politics are by no means separate.\"\n\n\nDeliberative democracy is a reform of representative or direct democracies which mandates discussion and debate of popular topics which affect society. Deliberative Democracy is a tool for making decisions. Deliberative democracy can be traced back all the way to Aristotle’s writings. More recently, the term was coined by Joseph Bessette in his 1980 work \"Deliberative Democracy: The Majority Principle in Republican Government\", where he uses the idea in opposition to the elitist interpretations of the United States Constitution with emphasis on public discussion.\n\nDeliberative Democracy can lead to more legitimate, credible, and trustworthy outcomes. Deliberative Democracy allows for \"a wider range of public knowledge,\" and it has been argued that this can lead to \"more socially intelligent and robust\" science. One major shortcoming of deliberative democracy is that many models insufficiently ensure critical interaction.\n\nAccording to Ryfe, there are five mechanisms that stand out as critical to the successful design of deliberative democracy:\n\nRecently, there has been a movement towards greater transparency in the fields of policy and technology. Jasanoff comes to the conclusion that there is no longer a question of if there needs to be increased public participation in making decisions about science and technology, but now there needs to be ways to make a more meaningful conversation between the public and those developing the technology.\n\nAckerman and Fishkin offer an example of a reform in their paper \"Deliberation Day.\" The deliberation is to enhance public understanding of popular, complex, and controversial issues, through devices such as Fishkin’s Deliberative Polling. Although implementation of these reforms is unlikely in a large government situation such as the United States Federal Government. However, things similar to this have been implemented in small, local, governments like New England towns and villages. New England town hall meetings are a good example of deliberative democracy in a realistic setting.\n\nAn ideal Deliberative Democracy balances the voice and influence of all participants. While the main aim is to reach consensus, a deliberative democracy should encourage the voices of those with opposing viewpoints, concerns due to uncertainties, and questions about assumptions made by other participants. It should take its time and ensure that those participating understand the topics on which they debate. Independent managers of debates should also have substantial grasp of the concepts discussed, but must \"[remain] independent and impartial as to the outcomes of the process.\"\n\nIn 1968, Garrett Hardin popularised the phrase \"tragedy of the commons.\" It is an economic theory where rational people act against the best interest of the group by consuming a common resource. Since then, the tragedy of the commons has been used to symbolize the degradation of the environment whenever many individuals use a common resource. Although Garrett Hardin was not an STS scholar, the concept of tragedy of the commons still applies to science, technology and society.\n\nIn a contemporary setting, the Internet acts as an example of the tragedy of the commons through the exploitation of digital resources and private information. Data and internet passwords can be stolen much more easily than physical documents. Virtual spying is almost free compared to the costs of physical spying. Additionally, net neutrality can be seen as an example of tragedy of the commons in an STS context. The movement for net neutrality argues that the Internet should not be a resource that is dominated by one particular group, specifically those with more money to spend on Internet access.\n\nA counterexample to the tragedy of the commons is offered by Andrew Kahrl. Privatization can be a way to deal with the tragedy of the commons. However, Kahrl suggests that the privatization of beaches on Long Island, in an attempt to combat overuse of Long Island beaches, made the residents of Long Island more susceptible to flood damage from Hurricane Sandy. The privatization of these beaches took away from the protection offered by the natural landscape. Tidal lands that offer natural protection were drained and developed. This attempt to combat the tragedy of the commons by privatization was counter-productive. Privatization actually destroyed the public good of natural protection from the landscape.\n\nAlternative modernity is a conceptual tool conventionally used to represent the state of present western society. Modernity represents the political and social structures of the society, the sum of interpersonal discourse, and ultimately a snapshot of society's direction at a point in time. Unfortunately conventional modernity is incapable of modeling alternative directions for further growth within our society. Also, this concept is ineffective at analyzing similar but unique modern societies such as those found in the diverse cultures of the developing world. Problems can be summarized into two elements: inward failure to analyze growth potentials of a given society, and outward failure to model different cultures and social structures and predict their growth potentials.\n\nPreviously, modernity carried a connotation of the current state of being modern, and its evolution through European colonialism. The process of becoming \"modern\" is believed to occur in a linear, pre-determined way, and is seen by Philip Brey as a way of to interpret and evaluate social and cultural formations. This thought ties in with modernization theory, the thought that societies progress from \"pre-modern\" to \"modern\" societies.\n\nWithin the field of science and technology, there are two main lenses with which to view modernity. The first is as a way for society to quantify what it wants to move towards. In effect, we can discuss the notion of \"alternative modernity\" (as described by Andrew Feenberg) and which of these we would like to move towards. Alternatively, modernity can be used to analyze the differences in interactions between cultures and individuals. From this perspective, alternative modernities exist simultaneously, based on differing cultural and societal expectations of how a society (or an individual within society) should function. Because of different types of interactions across different cultures, each culture will have a different modernity.\n\nPace of Innovation is the speed at which technological innovation or advancement is occurring, with the most apparent instances being too slow or too rapid. Both these rates of innovation are extreme and therefore have effects on the people that get to use this technology.\n\n\"No innovation without representation\" is a democratic ideal of ensuring that everyone involved gets a chance to be represented fairly in technological developments.\n\n\nThe privileged positions of business and science refer to the unique authority that persons in these areas hold in economic, political, and technosocial affairs. Businesses have strong decision-making abilities in the function of society, essentially choosing what technological innovations to develop. Scientists and technologists have valuable knowledge, ability to pursue the technological innovations they want. They proceed largely without public scrutiny and as if they had the consent of those potentially affected by their discoveries and creations.\n\nLegacy thinking is defined as an inherited method of thinking imposed from an external source without objection by the individual, because it is already widely accepted by society.\n\nLegacy thinking can impair the ability to drive technology for the betterment of society by blinding people to innovations that do not fit into their accepted model of how society works. By accepting ideas without questioning them, people often see all solutions that contradict these accepted ideas as impossible or impractical. Legacy thinking tends to advantage the wealthy, who have the means to project their ideas on the public. It may be used by the wealthy as a vehicle to drive technology in their favor rather than for the greater good.\nExamining the role of citizen participation and representation in politics provides an excellent example of legacy thinking in society. The belief that one can spend money freely to gain influence has been popularized, leading to public acceptance of corporate lobbying. As a result, a self-established role in politics has been cemented where the public does not exercise the power ensured to them by the Constitution to the fullest extent. This can become a barrier to political progress as corporations who have the capital to spend have the potential to wield great influence over policy. Legacy thinking however keeps the population from acting to change this, despite polls from Harris Interactive that report over 80% of Americans feel that big business holds too much power in government. Therefore, Americans are beginning to try to steer away this line of thought, rejecting legacy thinking, and demanding less corporate, and more public, participation in political decision making.\n\nAdditionally, an examination of net neutrality functions as a separate example of legacy thinking. Starting with dial-up, the internet has always been viewed as a private luxury good. Internet today is a vital part of modern-day society members. They use it in and out of life every day. Corporations are able to mislabel and greatly overcharge for their internet resources. Since the American public is so dependent upon internet there is little for them to do. Legacy thinking has kept this pattern on track despite growing movements arguing that the internet should be considered a utility. Legacy thinking prevents progress because it was widely accepted by others before us through advertising that the internet is a luxury and not a utility. Due to pressure from grassroots movements the Federal Communications Commission (FCC) has redefined the requirements for broadband and internet in general as a utility. Now AT&T and other major internet providers are lobbying against this action and are in-large able to delay the onset of this movement due to legacy thinking’s grip on American culture and politics.\n\nFor example, those who cannot overcome the barrier of legacy thinking may not consider the privatization of clean drinking water as an issue. This is partially because access to water has become such a given fact of the matter to them. For a person living in such circumstances, it may be widely accepted to not concern themselves with drinking water because they have not needed to be concerned with it in the past. Additionally, a person living within an area that does not need to worry about their water supply or the sanitation of their water supply is less likely to be concerned with the privatization of water.\n\nThis notion can be examined through the thought experiment of \"veil of ignorance\". Legacy thinking causes people to be particularly ignorant about the implications behind the \"you get what you pay for\" mentality applied to a life necessity. By utilizing the \"veil of ignorance\", one can overcome the barrier of legacy thinking as it requires a person to imagine that they are unaware of their own circumstances, allowing them to free themselves from externally imposed thoughts or widely accepted ideas.\n\n\n\nSTS is taught in several countries. According to the STS wiki, STS programs can be found in twenty countries, including 45 programs in the United States, three programs in India, and eleven programs in the UK. STS programs can be found in Israel, Malaysia, and Taiwan. Some examples of institutions offering STS programs are Stanford University, Harvard University, the University of Oxford, Mines ParisTech, and Bar-Ilan University.\n\n\n"}
{"id": "4782859", "url": "https://en.wikipedia.org/wiki?curid=4782859", "title": "The Investigators (UK TV series)", "text": "The Investigators (UK TV series)\n\nThe Investigators was a BAFTA-nominated children's science program, presented by children, on Channel 4 in Great Britain. It showed various interesting experiments from how to blow a balloon up with yeast to building bridges. Other experiments included on the program were making salmon flavoured ice cream and making periscopes.\nIt aired on Saturday morning.\n\nPresenters included Adam Davies, Flo Carson, Rhys Williams, Constance Lound-Mcgowan, John Forster, Alice Govan, Jay Simpson, Camille St-Omer and Kevin Jarin.\n"}
{"id": "2338399", "url": "https://en.wikipedia.org/wiki?curid=2338399", "title": "V-weapons", "text": "V-weapons\n\nV-weapons, known in original German as Vergeltungswaffen (, German: \"retaliatory weapons\", \"reprisal weapons\"), were a particular set of long-range artillery weapons designed for strategic bombing during World War II, particularly terror bombing and/or aerial bombing of cities. They comprised the V-1, a pulsejet-powered cruise missile, the V-2, a liquid-fuelled ballistic missile (often referred to as V1 and V2), and the V-3 cannon. All of these weapons were intended for use in a military campaign against Britain, though only the V-1 and V-2 were so used in a campaign conducted 1944–5. After the invasion of Europe by the Allies, these weapons were also employed against targets on the mainland of Europe, mainly France and Belgium. The V-terror bombing killed approximately 18,000 people, mostly civilians. The cities London, Antwerp and Liège were the main targets.\n\nThey were part of the range of the so-called Wunderwaffen ( or 'wonderweapons') of Nazi Germany.\n\nAs early as 28 June 1940, a terror bombing rationale had been advanced for the A4 (V-2 rocket) being developed at a meeting between Army Ordnance Chief Emil Leeb and Commander-in-Chief of the Wehrmacht, Walther von Brauchitsch. Following the relative failure of the Baedeker Raids on Britain in 1942, development of both flying bomb and rocket accelerated, with Britain designated as the target. On September 29, 1943, Albert Speer publicly promised retribution against the mass bombing of German cities by a \"secret weapon\". Then the official 24 June 1944 Reich Propaganda Ministry announcement of the \"Vergeltungswaffe 1\" guided missile implied there would be another such weapon. After the first operational A-4 launch in September 1944, the rocket was renamed the V-2. (although no one knows exactly who gave it this name). However, the V-2 operations manual distributed to firing batteries continued to use the A-4 name for the rocket.\n\nBeginning in October 1943, launch sites for the V-1 were constructed in Northern France, along the coast from Calais to Le Havre. Aerial bombing attacks on these sites by the Allied airforce were only partially successful and by June 1944 they were ready for action. Prompted by the Normandy Landings of June 6, in the early morning of June 13, 1944, the first V-1 flying bomb attack was carried out on London. Ten missiles were launched of which four reached England. The first of these impacted near Swanscombe, causing no casualties. At Bethnal Green, however, a bridge was destroyed and six people killed and nine injured. After the 15th the attacks became sustained at a rate of about 100 a day. With the first attack the British put their pre-planned Operation Diver (after their codename \"Diver\" used for the V-1) into action.\n\nThe buzzing sound of the V-1's pulse jet engine was likened by some to \"a motor cycle in bad running order\". As it reached its target and dived, the sound of the propulsion unit spluttering and cutting out, followed by an eerie hush before impact, was quite terrifying, though the silence was also a warning to seek shelter (later V-1s were corrected to have the originally-intended power dive). At least one business in London advertised how quickly a patron could access a nearby shelter. Despite this, the cloudy and rainy conditions of June and July aided the effectiveness of the weapon and casualties were high. By late August a million and a half people had left London and the rate of work production was affected. By the late summer and autumn, however, increasingly effective countermeasures against the V-1 were taken and people started returning to London.\n\nA total of 9,251 V-1s were fired at targets in Britain, with the vast majority aimed at London; 2,515 reached the city, killing 6,184 civilians and injuring 17,981. Croydon to the south, on the flight path of the V1s suffered severely taking 142 hits.\n\nV-2 rocket launching sites were set up by the Germans around The Hague in the Netherlands on 6 September 1944. The first was launched from here against London on 8 September 1944 and took an estimated 5 minutes to fly the 200 miles from the Hague to London where it struck at 6.43pm on 8 September on Chiswick causing thirteen casualties. As the V2 explosions came without warning, the government initially attempted to conceal their cause by blaming them on defective gas mains. However, the public was not fooled and soon began sardonically referring to the V-2's as \"Flying gas pipes\".\n\nBy October the offensive became sustained. A particularly devastating strike was on 25 November 1944 when a V-2 exploded at the Woolworth's store in New Cross Road, killing 168 people and seriously injuring 121. Intercepting the supersonic V-2 missiles in flight proved virtually impossible and other counter measures, such as bombing the launch sites, were fairly ineffectual. Sustained bombardment continued until March 1945. The very last missiles arrived on 27 March 1945, with one of them killing 134 people and injuring 49 when it hit a block of flats in Stepney.\n\n1,115 V-2s were fired at the United Kingdom. The vast majority of them were aimed at London, though about 40 targeted (and missed) Norwich. They killed an estimated 2,754 people in London with another 6,523 injured. A further 2,917 service personnel were killed as a result of the V weapon campaign. Since the V-2 was supersonic and could not be heard (and was rarely seen) as it approached the target, its psychological effect \"suffered in comparison to the V-1\".\"\"\n\nThe V-weapon offensive ended in March 1945, with the last V-2 landing in Kent on March 27 and the last V-1 two days later. In terms of casualties their effects had been less than their inventors hoped or their victims feared, though the damage to property was extensive, with 20,000 houses a day being damaged at the height of the campaign, causing a massive housing crisis in south-east England in late 1944 and early 1945.\n\nThe existential horror of the V-2 attack on London is the theme of Thomas Pynchon's novel \"Gravity's Rainbow\".\n\nThe V-3 cannon, also designed to fire on London, was never used for this purpose due to Allied attacks on the launch facilities, specially the fortress of Mimoyecques, and the offensive in northern Europe in 1944, overrunning the launch sites. Consequently, its use was diverted, in the winter of 1944, to bombard Luxembourg, with minimal results.\n\n\n"}
{"id": "40593920", "url": "https://en.wikipedia.org/wiki?curid=40593920", "title": "Westphal balance", "text": "Westphal balance\n\nA Westphal balance (also known as a Mohr balance) is a scientific instrument for measuring the density of liquids.\n"}
{"id": "50001219", "url": "https://en.wikipedia.org/wiki?curid=50001219", "title": "William Cary (instrument maker)", "text": "William Cary (instrument maker)\n\nWilliam Cary (1759–16 November 1825) was an English scientific-instrument maker. Trained under Jesse Ramsden, he produced numerous scientific instruments including mechanical calculators, measuring instruments, telescopes, microscopes, navigation and survey equipment.\n\nWilliam Cary was born to George and Mary Cary. He had three brothers, the eldest George (ca. 1753-1830) was a haberdasher while the second brother John, was a mapmaker who also worked with William and the last, Francis (ca. 1756-1836) was an engraver. Cary learnt the skills for producing instruments as an apprentice of Jesse Ramsden (1735-1800).\n\nThe instruments made by William Cary were used around the world including Russia and India. He also made instruments for the English chemist William Hyde Wollaston. After his death in 1825, the firm was taken over by Charles Gould who may have trained briefly under William Cary.\n\n"}
