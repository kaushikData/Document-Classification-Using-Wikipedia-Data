{"id": "14861568", "url": "https://en.wikipedia.org/wiki?curid=14861568", "title": "3C 234", "text": "3C 234\n\n3C 234 is a Seyfert galaxy with a quasar-like appearance located in the constellation Leo Minor.\n\n"}
{"id": "40161223", "url": "https://en.wikipedia.org/wiki?curid=40161223", "title": "AMPHORA", "text": "AMPHORA\n\nAMPHORA (\"AutoMated Phylogenomic infeRence Application\") is an open-source bioinformatics workflow. AMPHORA2 uses 31 bacterial and 104 archaeal phylogenetic marker genes for inferring phylogenetic information from metagenomic datasets. Most of the marker genes are single copy genes, therefore AMPHORA2 is suitable for inferring the accurate taxonomic composition of bacterial and archaeal communities from metagenomic shotgun sequencing data.\n\nFirst AMPHORA was used for re-analysis of the Sargasso Sea metagenomic data in 2008, but recently there are more and more metagenomic datasets in the Sequence Read Archive waiting for analysis with AMPHORA2.\n\nAmphoraNet\n\nAmphoraVizu\n"}
{"id": "32100474", "url": "https://en.wikipedia.org/wiki?curid=32100474", "title": "Adventure (TV series)", "text": "Adventure (TV series)\n\nAdventure is a documentary television series that aired on CBS beginning in 1953. The series was produced in collaboration with the American Museum of Natural History and hosted by Charles Collingwood. The program consisted of interviews with scientists and academicians and films of anthropological expeditions.\n\nIndividuals appearing in interviews included historian Bernard DeVoto, biologist Alexander Fleming, and adventurer Sasha Siemel.\n\nMarcel LaFollette has written, \"Production approaches that are now standard practice on \"NOVA\" and the Discovery Channel derive, in fact, from experimentation by television pioneers like Lynn Poole and Don Herbert and such programs as \"Adventure\", \"Zoo Parade\", \"Science in Action\", and the Bell Telephone System’s science specials. These early efforts were also influenced by television’s love of the dramatic, refined during its first decade and continuing to shape news and public affairs programming, as well as fiction and fantasy, today.\" LaFollette included the program in her 2008 overview of early broadcasting devoted to science popularization.\n\nThe show began its run in May 1953 and was broadcast on late Sunday afternoon. It switched to early Sunday evening (6:00 pm to 7:00 pm) in June 1953. In October 1953, it returned to Sunday afternoon and remained there through July 1956.\n"}
{"id": "2062759", "url": "https://en.wikipedia.org/wiki?curid=2062759", "title": "Affine vector field", "text": "Affine vector field\n\nAn affine vector field (sometimes affine collineation or affine) is a projective vector field preserving geodesics and preserving the affine parameter. Mathematically, this is expressed by the following condition:\n\n"}
{"id": "8146294", "url": "https://en.wikipedia.org/wiki?curid=8146294", "title": "Allen Lowrie", "text": "Allen Lowrie\n\nAllen Lowrie (born 1948) is a West Australian botanist. He is recognised for his expertise on the genera \"Drosera\" and \"Stylidium\".\n\nLowrie, originally a businessman and inventor, first experienced the carnivorous flora of western Australia in the late sixties and studied it as an amateur. Over time, his hobby turned into a profession and Lowrie discovered and described numerous species (especially \"Drosera\", \"Byblis\" and \"Utricularia\"), partly together with Neville Marchant. From 1987 to 1998 he published \"Carnivorous Plants of Australia\" in three volumes; a fourth is in the making.\n\nLowrie lives in Duncraig, a Perth suburb, is married and has two daughters.\n"}
{"id": "22875437", "url": "https://en.wikipedia.org/wiki?curid=22875437", "title": "Anders Flodström", "text": "Anders Flodström\n\nAnders Flodström (born 1944) is a Swedish professor of materials physics at the Royal Institute of Technology. He was previously the rector of Linköping University from 1996 to 1999 and of the Royal Institute of Technology from 1999 to 2007 and University Chancellor of Sweden and head of the Swedish National Agency for Higher Education from 1 August 2007 to 30 June 2010. Since November 2012, Anders Flodström is the Chief Education Officer of EIT Digital and a member of the Management Committee of EIT Digital. \n\nFlodström was born in Söderhamn, Sweden. He studied engineering physics and electrical engineering in Linköping. In 1975, he was awarded a Ph.D. in physics in Linköping with the thesis \"Electronic structure of clean and oxygen covered aluminium and magnesium surfaces studied by photoelectron spectroscopy\". Flodström was also one of the initiators of the synchrotron facility MAX-Lab in Lund, where he served as a coordinator until 1985. In 1985 he was appointed professor of materials physics at the Royal Institute of Technology in Stockholm.\n"}
{"id": "3411345", "url": "https://en.wikipedia.org/wiki?curid=3411345", "title": "Archeus", "text": "Archeus\n\nIn alchemy, Archeus, or archaeus, is a term used generally to refer to the lowest and most dense aspect of the astral plane which presides over the growth and continuation of all living beings. The term was used by medieval Paracelsus and those after him, such as Jan Baptist van Helmont.\n\nTo define it, the philosophers maintained that the Archeus was the segment of the closest quadrant of the higher worlds which blends with some similarity to the highest vibrations of our physical world. Essentially it was seen as the \"gray area\" wherein matter, speaking parallel and not laterally, begins to transmute into spiritual energies. In effect it is the glue which binds the heavens to the material, and so allows the maxim \"As above, so below.\"\n\nApart from the Archeus, which is primarily a Platonic name for the subject, this sphere is also called the Anima Mundi, Soul of the World, Spirit of the World, The Transitive LVX, The Path of Saturn (connecting Malkuth and Yesod in the system of Jewish mysticism called the Kabbalah), the Earth Sphere and the Zone Girdling the Earth. It is also sometimes simply called the lower astral sphere, or the \"geographic\" region of it, as everything in the Archeus parallels physical manifestation.\n\nThe term was also used for the nature of fire, or the 'fire lodged in the center of the Earth', to which was ascribed the generation of metals and minerals, and which was believed to be the principle of life in vegetables.\n\nThe philosophy which discusses the Archeus in the most detail is medieval hermetic science, where we find the occult author Heinrich Cornelius Agrippa speaking at length about a previously purely platonic subject. This he likely derived from his teacher in hermetic science, a man by the name of Trithemius, who studied extensively under certain Neoplatonic philosophers.\n\nIt is worth mentioning that the Archeus can be broken down into four different ethers: Chemical, Life, Light, and Reflective. To discuss them briefly as certain mystics saw them, the Chemical Ether composes the substances within which energies responsible for the perpetuation of chemical actions in the world exist.\n\nThe Life Ether composes the substances through which the Vital Force exists and is transmitted, and which forms a matrix to hold in the Life Spark of a living thing. All living things contain both a chemical and life aura to them; the former, to the clairvoyant, is usually a subtle light-red flame. The latter is usually a static streaming of blue and white light.\n\nThe Light Ether is the highest ether at play in the physical world, and is the actual medium by which means the programmed virtues of objects travel down from the higher spheres of existence and impregnate their appropriate physical vessels. It is with the property of this ether that we are most concerned, for some of the greatest miracles in magic are accomplished by manipulating the virtues which objects and circumstances receive. It bears mentioning that it is through this Life Ether that the soul of a living thing is given unto a body.\n\nThe Reflective Ether does not so much importantly act upon the physical world, but does in occasion anyway. If the Akashic Library is to be seen as the Memory of God, then the Reflective Ether would be the memory of Earth. It is through the replay of such memories that so-called hauntings are often created. But the ethers may be reserved for a later lecture: in the mean time let us refer to the matter at hand.\n\nAs previously mentioned, the Light Ether brings virtues from higher spheres down into this one, and this requires elaboration. There is a chain of descent from the First Cause, which some call God, all the way to our physical world. In the First Cause all things are one, combined. As they emanate outwards from this First Cause they begin to divide according to Intelligent Design. Like begins to attract like, opposites begin to repel one another, and in a short time there is a vast array of different combinations of energies which may be called their own units.\n\n WISC\n"}
{"id": "361897", "url": "https://en.wikipedia.org/wiki?curid=361897", "title": "Astrophysics", "text": "Astrophysics\n\n\"Astrophysics\"' is the field of physics that studies the nature of astronomical objects, rather than their positions or motions in space. Among the objects studied are the Sun, other stars, galaxies, extrasolar planets, the interstellar medium and the cosmic microwave background. Their emissions are examined across all parts of the electromagnetic spectrum, and the properties examined include luminosity, density, temperature, and chemical composition. Because astrophysics is a very broad subject, \"astrophysicists\" typically apply many disciplines of physics, including mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics.\n\nIn practice, modern astronomical research often involves a substantial amount of work in the realms of theoretical and observational physics. Some areas of study for astrophysicists include their attempts to determine the properties of dark matter, dark energy, and black holes; whether or not time travel is possible, wormholes can form, or the multiverse exists; and the origin and ultimate fate of the universe. Topics also studied by theoretical astrophysicists include Solar System formation and evolution; stellar dynamics and evolution; galaxy formation and evolution; magnetohydrodynamics; large-scale structure of matter in the universe; origin of cosmic rays; general relativity and physical cosmology, including string cosmology and astroparticle physics.\n\nAlthough astronomy is as ancient as recorded history itself, it was long separated from the study of terrestrial physics. In the Aristotelian worldview, bodies in the sky appeared to be unchanging spheres whose only motion was uniform motion in a circle, while the earthly world was the realm which underwent growth and decay and in which natural motion was in a straight line and ended when the moving object reached its goal. Consequently, it was held that the celestial region was made of a fundamentally different kind of matter from that found in the terrestrial sphere; either Fire as maintained by Plato, or Aether as maintained by Aristotle.\nDuring the 17th century, natural philosophers such as Galileo, Descartes, and Newton began to maintain that the celestial and terrestrial regions were made of similar kinds of material and were subject to the same natural laws. Their challenge was that the tools had not yet been invented with which to prove these assertions.\n\nFor much of the nineteenth century, astronomical research was focused on the routine work of measuring the positions and computing the motions of astronomical objects. A new astronomy, soon to be called astrophysics, began to emerge when William Hyde Wollaston and Joseph von Fraunhofer independently discovered that, when decomposing the light from the Sun, a multitude of dark lines (regions where there was less or no light) were observed in the spectrum. By 1860 the physicist, Gustav Kirchhoff, and the chemist, Robert Bunsen, had demonstrated that the dark lines in the solar spectrum corresponded to bright lines in the spectra of known gases, specific lines corresponding to unique chemical elements. Kirchhoff deduced that the dark lines in the solar spectrum are caused by absorption by chemical elements in the Solar atmosphere. In this way it was proved that the chemical elements found in the Sun and stars were also found on Earth.\n\nAmong those who extended the study of solar and stellar spectra was Norman Lockyer, who in 1868 detected bright, as well as dark, lines in solar spectra. Working with the chemist, Edward Frankland, to investigate the spectra of elements at various temperatures and pressures, he could not associate a yellow line in the solar spectrum with any known elements. He thus claimed the line represented a new element, which was called helium, after the Greek Helios, the Sun personified.\n\nIn 1885, Edward C. Pickering undertook an ambitious program of stellar spectral classification at Harvard College Observatory, in which a team of woman computers, notably Williamina Fleming, Antonia Maury, and Annie Jump Cannon, classified the spectra recorded on photographic plates. By 1890, a catalog of over 10,000 stars had been prepared that grouped them into thirteen spectral types. Following Pickering's vision, by 1924 Cannon expanded the catalog to nine volumes and over a quarter of a million stars, developing the Harvard Classification Scheme which was accepted for worldwide use in 1922.\n\nIn 1895, George Ellery Hale and James E. Keeler, along with a group of ten associate editors from Europe and the United States, established \"The Astrophysical Journal: An International Review of Spectroscopy and Astronomical Physics\". It was intended that the journal would fill the gap between journals in astronomy and physics, providing a venue for publication of articles on astronomical applications of the spectroscope; on laboratory research closely allied to astronomical physics, including wavelength determinations of metallic and gaseous spectra and experiments on radiation and absorption; on theories of the Sun, Moon, planets, comets, meteors, and nebulae; and on instrumentation for telescopes and laboratories.\n\nAround 1920, following the discovery of the Hertsprung-Russell diagram still used as the basis for classifying stars and their evolution, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper \"The Internal Constitution of the Stars\". At that time, the source of stellar energy was a complete mystery; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein's equation \"E = mc\". This was a particularly remarkable development since at that time fusion and thermonuclear energy, and even that stars are largely composed of hydrogen (see metallicity), had not yet been discovered.\n\nIn 1925 Cecilia Helena Payne (later Cecilia Payne-Gaposchkin) wrote an influential doctoral dissertation at Radcliffe College, in which she applied ionization theory to stellar atmospheres to relate the spectral classes to the temperature of stars. Most significantly, she discovered that hydrogen and helium were the principal components of stars. Despite Eddington's suggestion, this discovery was so unexpected that her dissertation readers convinced her to modify the conclusion before publication. However, later research confirmed her discovery.\n\nBy the end of the 20th century, studies of astronomical spectra had expanded to cover wavelengths extending from radio waves through optical, x-ray, and gamma wavelengths. In the 21st century it further expanded to include observations based on gravitational waves.\n\nObservational astronomy is a division of the astronomical science that is concerned with recording data, in contrast with theoretical astrophysics, which is mainly concerned with finding out the measurable implications of physical models. It is the practice of observing celestial objects by using telescopes and other astronomical apparatus.\n\nThe majority of astrophysical observations are made using the electromagnetic spectrum.\n\n\nOther than electromagnetic radiation, few things may be observed from the Earth that originate from great distances. A few gravitational wave observatories have been constructed, but gravitational waves are extremely difficult to detect. Neutrino observatories have also been built, primarily to study our Sun. Cosmic rays consisting of very high energy particles can be observed hitting the Earth's atmosphere.\n\nObservations can also vary in their time scale. Most optical observations take minutes to hours, so phenomena that change faster than this cannot readily be observed. However, historical data on some objects is available, spanning centuries or millennia. On the other hand, radio observations may look at events on a millisecond timescale (millisecond pulsars) or combine years of data (pulsar deceleration studies). The information obtained from these different timescales is very different.\n\nThe study of our very own Sun has a special place in observational astrophysics. Due to the tremendous distance of all other stars, the Sun can be observed in a kind of detail unparalleled by any other star. Our understanding of our own Sun serves as a guide to our understanding of other stars.\n\nThe topic of how stars change, or stellar evolution, is often modeled by placing the varieties of star types in their respective positions on the Hertzsprung–Russell diagram, which can be viewed as representing the state of a stellar object, from birth to destruction.\n\nTheoretical astrophysicists use a wide variety of tools which include analytical models (for example, polytropes to approximate the behaviors of a star) and computational numerical simulations. Each has some advantages. Analytical models of a process are generally better for giving insight into the heart of what is going on. Numerical models can reveal the existence of phenomena and effects that would otherwise not be seen.\n\nTheorists in astrophysics endeavor to create theoretical models and figure out the observational consequences of those models. This helps allow observers to look for data that can refute a model or help in choosing between several alternate or conflicting models.\n\nTheorists also try to generate or modify models to take into account new data. In the case of an inconsistency, the general tendency is to try to make minimal modifications to the model to fit the data. In some cases, a large amount of inconsistent data over time may lead to total abandonment of a model.\n\nTopics studied by theoretical astrophysicists include: stellar dynamics and evolution; galaxy formation and evolution; magnetohydrodynamics; large-scale structure of matter in the universe; origin of cosmic rays; general relativity and physical cosmology, including string cosmology and astroparticle physics. Astrophysical relativity serves as a tool to gauge the properties of large scale structures for which gravitation plays a significant role in physical phenomena investigated and as the basis for black hole (\"astro\")physics and the study of gravitational waves.\n\nSome widely accepted and studied theories and models in astrophysics, now included in the Lambda-CDM model, are the Big Bang, cosmic inflation, dark matter, dark energy and fundamental theories of physics. Wormholes are examples of hypotheses which are yet to be proven (or disproven).\n\nThe roots of astrophysics can be found in the seventeenth century emergence of a unified physics, in which the same laws applied to the celestial and terrestrial realms. There were scientists who were qualified in both physics and astronomy who laid the firm foundation for the current science of astrophysics. In modern times, students continue to be drawn to astrophysics due to its popularization by the Royal Astronomical Society and notable educators such as prominent professors Lawrence Krauss, Subrahmanyan Chandrasekhar, Stephen Hawking, Hubert Reeves, Carl Sagan and Neil deGrasse Tyson. The efforts of the early, late, and present scientists continue to attract young people to study the history and science of astrophysics.\n\n\n"}
{"id": "53364762", "url": "https://en.wikipedia.org/wiki?curid=53364762", "title": "BES III", "text": "BES III\n\nThe Beijing Spectrometer III (BES III) is a particle physics experiment at the Beijing Electron–Positron Collider II (BEPC II) at the Institute of High Energy Physics (IHEP). It is designed to study the physics of charm, charmonium, and light hadron decays, as well studies of the tau lepton, tests of QCD and searches for physics beyond the Standard Model. The experiment started collecting data in the summer of 2008.\n\nBES III receives electron–positron collisions from BEPC II: a circular collider with a circumference of 240 m. BEPC II maintains a variable collision energy between 2 and 4.63 GeV, with a luminosity of 10 cm·s. Each of the beams contains 93 electron or positron bunches of length 1.5 cm and a total current of 0.91 A.\n\nThe BES III detector is a cylindrically symmetric 6-meter long and 7-meter diameter detector surrounding the interaction point of 2 beam pipe rings. It has 4 major detector layers: a main drift chamber (MDC), time-of-flight counter (TOF), cesium-iodide electromagnetic calorimeter (CsI EMC), and a muon counter (Muon Chamber, MC, µC). The inner three layers are inside of a 1 Tesla superconducting solenoid magnet.\n\nThe main drift chamber (MDC) is the first inner detector layer around the beam pipe and collision point. The MDC's main purpose is to measure the momentum and energy loss per unit distance (dE/dx) from charged particles. The chamber is 2.4 meters long and contains 6796 gold coated 25-micron tungsten signal wires that are arranged in 44 cylindrical layers. The half width of the inner 8 layers is 6 mm and the half width of the outer layers are 8.1 mm. Between the wires, a mixture of Helium and Propane gas are mixed together at a ratio of 60/40, designed to minimize multiple scattering and maintain a high dE/dx. Aluminum 110-micron wires are strung across the chamber for field shaping. As a charged particle passes through the chamber the gas becomes ionized along the path of the particle and the ions drift to the nearest wires. The particles path will be curved because of the magnetic field the solenoid creates. The amount of curvature allows for the momentum of the particle to be calculated. \n\nThe TOF, the second inner detector, makes time measurements that are used to assist in particle identification and as a fast trigger to reject cosmic rays. The detector is made out of two cylindrical layers of 88 2.4-meters long plastic scintillating bars. There is a photo-multiplying tube (PMT) at each end of the bars. The two PMTs are averaged and the travel time of the photons are removed. The electromagnetic calorimeter’s main purpose is to make energy and position measurements using Caesium Iodide scintillating crystals. The geometry of the crystals is 44 rings of 120 crystals along the axis of the cylinder with a 1.5 degree tilt. Two endcaps covering the end of the cylinder allows a total coverage of 93% of the space. Photo detectors are placed at the ends of each crystal. Photon and electron energies can be measured in the range of 20 MeV to 4.6 GeV. The muon identifier (MI) is composed of nine layers of Iron absorbers and resistive plate counters (RPC). RPC are composed of two separated Bakelite plastic plates with gas between them and enclosed in an Aluminium box. Having MI enables discrimination between muons and kaons.\n\nA crucial component of the detector is the trigger system that selects the useful collision data to save for analysis. Before the trigger, there are 40 million background events per second detected, which is reduced to around 4 thousand interesting collisions per second. The trigger is hardware based, and its design is predetermined by Monte Carlo simulations.\n"}
{"id": "34476487", "url": "https://en.wikipedia.org/wiki?curid=34476487", "title": "Bamberg (crater)", "text": "Bamberg (crater)\n\nBamberg is an impact crater in the Mare Acidalium quadrangle of Mars, located at 39.71 N and 356.9 E. It is 55.7 km in diameter and is named after the town Bamberg in Germany. CTX images and HiRISE images from the Mars Reconnaissance Orbiter have shown that the crater contains gullies. Martian gullies are believed to have formed through rather recent flows of liquid water.\n\nGullies are visible in the pictures below. On the basis of their form, aspects, positions, and location amongst and apparent interaction with features thought to be rich in water ice, many researchers believed that the processes carving the gullies involve liquid water. However, this remains a topic of active research. \nAs soon as gullies were discovered, researchers began to image many gullies over and over, looking for possible changes. By 2006, some changes were found. Later, with further analysis it was determined that the changes could have occurred by dry granular flows rather than being driven by flowing water. With continued observations many more changes were found in Gasa Crater and others. \nWith more repeated observations, more and more changes have been found; since the changes occur in the winter and spring, experts are tending to believe that gullies were formed from dry ice. Before-and-after images demonstrated the timing of this activity coincided with seasonal carbon-dioxide frost and temperatures that would not have allowed for liquid water. When dry ice frost changes to a gas, it may lubricate dry material to flow especially on steep slopes. In some years frost, perhaps as thick as 1 meter.\n\n\n"}
{"id": "1933105", "url": "https://en.wikipedia.org/wiki?curid=1933105", "title": "Capital deepening", "text": "Capital deepening\n\nCapital deepening is a situation where the capital per worker is increasing in the economy. This is also referred to as increase in the capital intensity. Capital deepening is often measured by the rate of change in capital stock per labour hour. Overall, the economy will expand, \"and\" productivity per worker will increase. However, according to some economic models, such as the Solow model, economic expansion will not continue indefinitely through capital deepening alone. This is partly due to diminishing returns and wear & tear (depreciation). Investment is also required to increase the amount of capital available to each worker in the system and thus increase the ratio of capital to labour. In other economic models, for example, the AK model or some models in endogenous growth theory, capital deepening can lead to sustained economic growth even without technological progress. Traditionally, in development economics, capital deepening is seen as a necessary but not sufficient condition for economic development of a country.\n\nCapital widening is the situation where the stock of capital is increasing at the same rate as the labour force and the depreciation rate, thus the capital per worker ratio remains constant. The economy will expand in terms of aggregate output, \"but\" productivity per worker will remain constant.\n\n"}
{"id": "10049748", "url": "https://en.wikipedia.org/wiki?curid=10049748", "title": "Cophasing", "text": "Cophasing\n\nIn astronomy, the term cophasing or phasing describes the process of controlling the individual segments in a segmented mirror or a telescope so that the segments form a larger composite mirroring surface. Cophasing implies precise, active control of three degrees of freedom of each individual segment mirror: translation along the optical axis (piston) and rotation about two axes perpendicular to the optical axis (tip-tilt).\n\nEach segment of the segmented telescope is a solid body having 6 degrees of freedom exposed to the gravitation force, wind blowing, and other mechanical forces. If the position of each segment is not controlled the resolution of the whole telescope will be the same as if telescope had the diameter equal to the size of one segment. To achieve a resolution commensurable with that of a monolithic telescope of the same diameter the segmented surface must be controlled with a precision better than formula_1 surface rms.\n\nProjects for future extremely large telescopes (ELTs) generally depend on the use of a segmented primary mirror. While the basic technologies required for segmented telescopes have been demonstrated for the 10m Keck telescope or GTC telescope, ELTs of diameters form 50 to 100 m represent a qualitative change with respect to wave front control related to segmentation in comparison with the current 10 meters technology.\n"}
{"id": "357339", "url": "https://en.wikipedia.org/wiki?curid=357339", "title": "Correctness (computer science)", "text": "Correctness (computer science)\n\nIn theoretical computer science, correctness of an algorithm is asserted when it is said that the algorithm is correct with respect to a specification. \"Functional\" correctness refers to the input-output behaviour of the algorithm (i.e., for each input it produces the expected output).\n\nA distinction is made between partial correctness, which requires that if an answer is returned it will be correct, and total correctness, which additionally requires that the algorithm terminates. Since there is no general solution to the halting problem, a total correctness assertion may lie much deeper. A termination proof is a type of mathematical proof that plays a critical role in formal verification because total correctness of an algorithm depends on termination.\n\nFor example, successively searching through integers 1, 2, 3, … to see if we can find an example of some phenomenon—say an odd perfect number—it is quite easy to write a partially correct program (using long division by two to check \"n\" as perfect or not). But to say this program is totally correct would be to assert something currently not known in number theory.\n\nA proof would have to be a mathematical proof, assuming both the algorithm and specification are given formally. In particular it is not expected to be a correctness assertion for a given program implementing the algorithm on a given machine. That would involve such considerations as limitations on computer memory.\n\nA deep result in proof theory, the Curry-Howard correspondence, states that a proof of functional correctness in constructive logic corresponds to a certain program in the lambda calculus. Converting a proof in this way is called \"program extraction\".\n\nHoare logic is a specific formal system for reasoning rigorously about the correctness of computer programs. It uses axiomatic techniques to define programming language semantics and argue about the correctness of programs through assertions known as Hoare triples.\n\nSoftware testing is any activity aimed at evaluating an attribute or capability of a program or system and determining that it meets its required results. Although crucial to software quality and widely deployed by programmers and testers, software testing still remains an art, due to limited understanding of the principles of software. The difficulty in software testing stems from the complexity of software: we can not completely test a program with moderate complexity. Testing is more than just debugging. The purpose of testing can be quality assurance, verification and validation, or reliability estimation. Testing can be used as a generic metric as well. Correctness testing and reliability testing are two major areas of testing. Software testing is a trade-off between budget, time and quality.\n\n\n"}
{"id": "12870634", "url": "https://en.wikipedia.org/wiki?curid=12870634", "title": "EAS3", "text": "EAS3\n\nEAS3 (EAS = Ein-Ausgabe-System) is a software toolkit for reading and writing structured binary data with geometry information and for postprocessing of these data. It is meant to exchange floating-point data according to IEEE standard between different computers, to modify them or to convert them into other file formats. It can be used for all kinds of structured data sets. It is mainly used in the field of direct numerical simulations.\n\nThe complete package consists of libraries intended for usage in own codes and a separate command-line tool. It is written in Fortran and C and runs on all POSIX operating systems. The libraries include different numerical algorithms and subroutines for reading and writing files in the binary EAS3 file format. The read/write routines are provided in Fortran and C. Implemented numerical methods include, for example, Fast Fourier transform, Thomas algorithm and interpolation routines. The libraries are also suitable for vector computers.\n\nEAS3 has been developed at the Institut für Aerodynamik und Gasdynamik (IAG) of the University of Stuttgart. The previous versions (EAS, EAS2) range back to the end of the 1980s, when computer power allowed the first spatial DNS computations. The upcoming amount of data required efficient handling and postprocessing. Typically, simulations were, and are still today, performed on a high-performance computer and afterwards postprocessed on other machines of opposite endianness. This required an endianness-independent file format for data handling.\n\nSince the publication of EAS3 in the 1999, the software has been developed continuously by members of the involved institutes. Since 2007, EAS3 is also available via the heise software directory. EAS3 is used by applications within the European PRACE project. The current version number is 1.6.7 from April, 2009.\n\nThe EAS3 file format is used to store floating point data in IEEE format and to exchange the files between different computer architectures (little/big endian). The data is organized as parameters with one parameter being a one-, two- or three-dimensional floating point array. Several of these parameters may be combined to one time step. This allows to store five-dimensional arrays. Data can be written in single-precision (32 Bit), double-precision (64 Bit) or quadruple-precision (128 Bit). Geometry information for the different directions are saved in the header of the file. It is also possible to store additional information in user defined arrays there. With the file size being limited only by the computer itself (e.g. file system), EAS3 files are suitable for large simulations and thus for high-performance computing.\n\nThe actual EAS3 executable is a command-line interface for alteration of EAS3 files. The implemented commands range from basic operations, e.g. simple computations, file operations, to rather complex operations like Fourier transformation or the computation of derivatives. Specific commands for DNS data are also available, e.g. the lambda2 vortex criterion. As the commands are read from standard input, EAS3 may be used in shell scripts for automated calls.\nOutline of important functions\n\nThe sources can be obtained directly from the CVS repository or one may download a zipped tar file. Makefiles for different machine types are included, providing an easy compilation. As linking of object files, created with different Fortran compilers can cause problems, binary packages (RPM, .deb) are not offered up to now.\n\nThe main profit for the programmer is the easy implementation of reading/writing large (>2GB) binary data sets. The library provides that the data is always written big endian. The resulting platform independence allows data exchange between different hardware architectures, e.g. supercomputers. The users benefits from the different methods provided for postprocessing, which can be automated using shell scripts.\n\nBeing specialized on structured grids may be a problem for some users. Up to now, only cartesian grids or a representation of the data in spectral space are implemented. Data in other types of data alignment, e.g. cylindrical coordinates, can be stored in EAS3 files but the existing postprocessing commands may not be used. As the usually used visualization programs do not support the EAS3 file format directly, it is often necessary to covert the data to the corresponding file format. Commands in the EAS3 program are given by a text interface, a graphical user interface does not exist. Completion of the commands in the EAS3 command line provides support for interactive usage but for an extensive help, the descriptions on the webpage are necessary.\n\nEAS3 is published under the MIT License. The MIT License is a free software license originating at the Massachusetts Institute of Technology (MIT). Specifically, it is a GPL-compatible permissive license, meaning that it permits reuse within proprietary software on the condition that the license is distributed with that software.\n\n\n\n\n"}
{"id": "2592596", "url": "https://en.wikipedia.org/wiki?curid=2592596", "title": "Ear (botany)", "text": "Ear (botany)\n\nAn ear is the grain-bearing tip part of the stem of a cereal plant, such as wheat or maize. It can also refer to \"a prominent lobe in some leaves\".\n\nThe ear is a spike, consisting of a central stem on which tightly packed rows of flowers grow. These develop into fruits containing the edible seeds. In corn, it is protected by leaves called husks.\n\nIn some species (including wheat), unripe ears contribute significantly to photosynthesis, in addition to the leaves lower down the plant.\n\nA parasite known as \"Anguina tritici\" (Ear Cockle) specifically affects the ears on wheat and rye by destroying the tissues and stems during growth. With the exception of North Africa and West Asia, the parasite has been eradicated in all countries by using the \ncrop rotation system.\n\n"}
{"id": "43006361", "url": "https://en.wikipedia.org/wiki?curid=43006361", "title": "Eternity: Our Next Billion Years", "text": "Eternity: Our Next Billion Years\n\nEternity: Our Next Billion Years is a non-fiction book which speculates about the future of mankind written by science writer Michael Hanlon. The book is a combination of non-fiction discussions based on science about what the future might look like, interspersed with more imaginative guesses about what life will look like thousands, and millions of years in the future.\n\"Eternity\" was published on November 25, 2008 by Palgrave Macmillan as part of the Macmillan Science series.\n\nCritical reception has been mixed. The \"SF Site\" gave a positive review and commented that the book was well suited to general audiences and was a good primer for people looking for an introduction to the book's themes. Michael Brooks was more critical of the work, as he felt that it \"covers some fascinating ground, but remains only superficially interesting, an hors d'oeuvre rather than the main course. Faced with the prospect of eternity, that's not enough to satisfy.\" \"The Globe and Mail\" was also mixed in their opinion, writing \"At its best, Hanlon's book offers the fascination and sense of wonder of good science fiction. At its worst, it reads like an earnest United Nations report on the challenges to be overcome in the 21st century. Luckily, there's more best than worst here.\"\n"}
{"id": "57576602", "url": "https://en.wikipedia.org/wiki?curid=57576602", "title": "Frances Colón", "text": "Frances Colón\n\nFrances Colón is an American science diplomat, serving ten years at the United States Department of State.\n\nColón grew up in San Juan, Puerto Rico and earned her doctorate in developmental neurobiology at Brandeis University and earned her B.S. in Biology in 1997 from the University of Puerto Rico.\n\nShe served nearly five years as Deputy Science and Technology Adviser to the Secretary of State (2012-2017). In that role, she became the highest-ranking Hispanic Scientist at the State Department. Prior to this role, she had been an adviser on science and the environment for the Western Hemisphere Affairs at the State Department and served as acting Science and Technology Adviser to Secretary of State John Kerry. In 2015, she represented the United States government as Vice Chair of the United Nations Commission on Science and Technology Development. Under Secretary of State Hillary Clinton, Colón led the Energy and Climate Partnership for the Americas (ECPA), an initiative announced by President Obama at the Summit of the Americas in April 2009 to accelerate sustainable energy in the Americas.\n\nColón is also an outspoken advocate for women and girls to pursue careers in science. During her time as Deputy Adviser, she oversaw the creation of the Networks of Diasporas in Engineering and Science (NODES) initiative to empower diasporas with science expertise to develop and influence effective policies and solve challenges in their countries of origin. As part of President Obama's White House \"Untold History of Women in STEM\" project, she shared the story of Puerto Rican scientist Ana Roqué de Duprey.\n"}
{"id": "36473068", "url": "https://en.wikipedia.org/wiki?curid=36473068", "title": "Gensim", "text": "Gensim\n\nGensim is a robust open-source vector space modeling and topic modeling toolkit implemented in Python. It uses NumPy, SciPy and optionally Cython for performance. Gensim is specifically designed to handle large text collections, using data streaming and efficient incremental algorithms, which differentiates it from most other scientific software packages that only target batch and in-memory processing.\n\nGensim includes implementations of tf-idf, random projections, word2vec and document2vec algorithms, hierarchical Dirichlet processes (HDP), latent semantic analysis (LSA, LSI, SVD) and latent Dirichlet allocation (LDA), including distributed parallel versions.\n\nSome of the online algorithms in Gensim were also published in the 2011 PhD dissertation \"Scalability of Semantic Analysis in Natural Language Processing\" of Radim Řehůřek, the creator of Gensim.\n\nGensim has been used and cited in over 800 commercial and academic applications, in a diverse array of disciplines from medicine to insurance claim analysis to patent search The software has been covered in several new articles, podcasts and interviews since 2009.\n\nThe open source code is developed and hosted on GitHub and a public support forum is maintained on Google Groups and Gitter.\n\nGensim is commercially supported by the company rare-technologies.com, who also provide student mentorships and academic thesis projects for Gensim via their Student Incubator programme.\n"}
{"id": "3221964", "url": "https://en.wikipedia.org/wiki?curid=3221964", "title": "Geography of Australian rules football", "text": "Geography of Australian rules football\n\nAustralian rules football is a sport played in many countries around the world at amateur level only. In 2016, about 106,000 people played in structured competitions outside of Australia and at least 20 leagues that are recognised by the game's governing body, exist outside Australia. In 2007 there was a total of 34,845 players. In contrast, there are over 800,000 players in Australia where the game is at its strongest; overseas players make up less than 2% of the total players worldwide.\n\nAustralian rules football is played professionally by men and women in Australia and is a major spectator sport in Australia and Nauru.\n\nThere have been several players in the VFL/AFL who were born outside Australia and since 1982, an increasing number of players have been recruited from outside Australia through initiatives such as the Irish experiment and more recently, international scholarship programs. Despite the amateur competitions outside of Australia, no player from these competitions has yet debuted in the AFL Premiership Season. Some have, however, featured in semi-professional competitions in Australia as well as in AFL pre-season practice matches. \n\nThe international growth of Australian rules in the 19th century and early 20th century was rapid, but it went into rapid decline following World War I. After World War II, the sport experienced a small amount of growth in the Pacific region, particularly in Nauru, Papua New Guinea and New Zealand.\n\nAustralian Football emerged as an international sport much later than other forms of football, such as soccer or rugby, but has grown substantially as an amateur sport in some countries since the 1980s. Initially, the sport grew with the Australian diaspora, aided by multiculturalism and assisted by exhibition matches and players who have converted to and from other football codes. In Papua New Guinea, New Zealand, South Africa, and the United States, there are many thousands of players. Canada, Japan, Denmark, and Sweden have also shown strong potential in the sport in the lead up to the 2008 Australian Football International Cup.\n\nThe AFL became the de facto international governing body for the sport when created the \"AFL International Development Committee\" and the IAFC was dissolved between 2002 and 2006.\n\nAustralian Football is played professionally by men, in Australia, and is the dominant spectator sport, with the exception of exhibition games staged in other countries.\n\nThe game is played in many countries, the Australian Football League and has more than 13 affiliated international governing bodies, AFL Canada, Danish Australian Football League, BARFL, AFL Japan, ARFLI, Nauru Australian Football Association, New Zealand AFL, USAFL, AFL South Africa, AFL PNG, AFL Samoa, Tonga Australian Football Association and AFL Germany. The league also has working relationships with bodies in additional countries, who have sent, or may in future send, teams to the International Cup. In 2010, a European association of 18 Countries was founded which later re-branded as AFL Europe. This association is affiliated to the Australian Football League, which funds the retention of a regional manager in Europe.\n\nAustralian rules football traditionally has seen its greatest support in Western Australia, South Australia, Victoria, Tasmania, the Northern Territory and the Riverina region of New South Wales.\n\nPrior to the establishment of the AFL in 1990, leagues were generally state-specific, with the Victorian Football League slowly beginning to expand prior to this point.\n\nSince becoming a national league, the AFL has continued its attempt to grow in the rest of Australia, with the success of teams in those areas helping to fuel interest in the game.\n\nAlmost as soon as the game was becoming established in Australia, it had spread to New Zealand and South Africa, initially because of the Otago Gold Rush and Witwatersrand Gold Rush. The game was further fuelled in South Africa by Australian soldiers in the First and Second Boer Wars.\n\nThere were reports of early competitions in England, Scotland, and Japan, started by expatriate Australians and servicemen.\n\nIn New Zealand, where proximity to Australia saw a formidable league, the sport quickly grew to a sizeable 115 clubs by the turn of the 20th century. As the game spread, it became known as \"Australasian Football\", with delegates from New Zealand added to the newly formed Australasian Football Council.\n\nIn 1908, New Zealand defeated both New South Wales and Queensland at the Jubilee Australasian Football Carnival, an event held to celebrate 50 years of Australian Football.\n\nWorld War I saw the game being played by Australian servicemen around the world, particularly in Egypt, and in Europe in France, Belgium, and England.\n\nFollowing the war, the game went into a sharp decline outside of Australia, with all international domestic competitions dying out. National teams and international competition in the sport became non-existent for three quarters of the 20th century. The return of many Australian expatriates from overseas gold fields and tours of duty, combined with Australia's low profile on the world stage, offered few opportunities for the game to grow during this time. With the withdrawal of its New Zealand delegates, the sport returned to the title of \"Australian Football\", governed by the Australian Football Council. Concerned primarily with the growth of their own domestic competitions, the Australian leagues and governing bodies made little effort to develop or promote the game until the 1950s, and the council's role was mainly to oversee the growing importance of interstate test matches.\n\nNevertheless, the longest running fixture outside of Australia, the annual Varsity match between Oxford University Australian Rules Football Club and Cambridge University in England, has been held since 1921, and has emerged into a fierce rivalry, worthy of half-blue status at Oxford. Apart from this match, however the game was rarely played in England.\n\nAustralian Football has been continuously played in the former Australian trustee mandate of Nauru, which began both senior and junior local competitions in the 1930s. Australian Football was also introduced to the Territory of New Guinea in 1944 and the Territory of Papua in 1948.\n\nWorld War II saw some servicemen play the game overseas, particularly in Malaysia, Indonesia, Egypt, and Algeria. During the Vietnam War, matches were even played by servicemen against the local Vietnamese.\n\nIn the 1960s, Australian leagues began to show some interest in expansion of the game outside of Australia. 1963 saw the first Australian rules football exhibition matches played in the United States. Australian state leagues began occasionally promoting themselves in this way over the following decades.\n\nIn 1967, it was reported in the VFL Record's \"Footy Facts\" column that Australian football clubs existed in Johannesburg, Pretoria, and Cape Town and that the VFL was optimistic about the future of the game in South Africa. Little is known of how or when these clubs had formed or what later became of them.\n\nSince 1967, there have been many matches between Australian and Irish teams, under various sets of hybrid, compromise rules. In 1984, the first official representative matches of International rules football were played, and these games have continued to be played annually each October, now attracting considerable public interest, drawing sizable crowds, and receiving regular television coverage. New Zealand resumed a local competition in 1974.\n\nBy 1975, Papua New Guinea had gained independence and test matches began to be played against teams from Australia. The first ever full international match involving Australia was played in 1977 at under 17 level between Australia and Papua New Guinea in Adelaide, with Australia taking the honours. Since then, Australia have been peerless in the sport and seldom compete at international level.\n\nDespite these advancements and others to the international aspects of the game, progress overseas is rarely covered in the Australian media.\n\nIn the late 1980s, successful VFL exhibition matches attracted large crowds and spawned fledgling local competitions in both Japan and Canada. The Australian media showed only a token interest in the matches in London and Japan involving VFL clubs. It was during this decade that the sport was first televised in North America and the United Kingdom.\n\nSome nationalities respond well to less formal means, however, and many trends in sporting activities are achieved outside formally organized programs. For instance, although Australian football was not formally established in Tonga until 2003 however informal matches had been introduced to schools as early as 1985.\n\nThe largest barriers to growth of Australian Football internationally have traditionally been distance, field availability, and player numbers. With a total of 36 players normally required for a game, and a cricket sized oval, organising games can be difficult in countries where space is a premium and devotees are spread widely. While these factors have not been a problem in Papua New Guinea or New Zealand, they did pose large problems to leagues in Europe, Asia, and the Americas. This disadvantage has been turned into an advantage with some organisers accepting modified versions of the game, such as nine-a-side, requiring fewer players and less space.\n\nIn the late 1980s, as organisers adapted, amateur leagues were established in Japan (1987), England, Denmark, and Canada (1989). In the case of Japan and Canada, these were directly sparked by VFL exhibition matches.\n\nIn the 1990s, the Australian diaspora had spread and amateur competition had grown in countries such as Sweden (1993), Germany (1995), the USA (1996), Argentina, Spain, Samoa (1997), and South Africa (1998), as well as a number of mainly expatriate teams, mainly based in South East Asia.\n\nDuring this time, the VFL expanded to become the AFL and began to command a greater national and international audience. Word of the sport grew out of AFL exhibition matches, cult television followings, and Internet communication. North American fans formed an organisation, AFANA, specifically to work for improved media coverage of Australian Football.\n\nThe traditionalists in the governing bodies of Australia (which became the AFL) were reluctant to sanction any games which were not played exactly according to the Laws of the Game, and the AFL initially did not recognise leagues that played the game on fields that did not closely match the proper dimensions, or had less than 16 players per side. Since the 1990s, these attitudes have changed somewhat, and the AFL and other development bodies have directly contributed to the development of the game overseas.\n\nThe International Australian Football Council (IAFC) was formed after football first featured at the Arafura Games in 1995. Since 1998, the Barassi International Australian Football Youth Tournament, endorsed by the AFL as part of its International Policy, has hosted several junior teams from other countries.\n\nSince 2000, fledgling competitions have been established in countries such as Ireland (2000), Tonga (2002), Scotland, France, and China (2005). Television and the internet have since helped to increase the awareness of the game outside of Australia.\n\nInspired by successful Arafura Games competitions, the inaugural Australian Football International Cup was held in Melbourne in 2002, an initiative of the IAFC and the AFL. The first International Cup also marked the beginnings of a very small media interest in the international aspects of the game in Australia.\n\nAt the 2002 International Cup, meetings held between the AFL, IAFC, and international teams saw a unanimous vote amongst member countries that the AFL become the \"de facto\" world governing body for the sport, with the leagues linked to the teams affiliating with the AFL. The IAFC's public relations officer, Brian Clarke, disputed this move and continued the organisation in name. This organisation was finally dissolved in 2005, dropping all public claims to being the world governing body for the sport and being replaced by the development organisation Aussie Rules International.\n\nIn recent years, the game has grown particularly strongly in Papua New Guinea and New Zealand. In percentage terms, their increases are high in comparison to the growth of the sport in Australia, and their total player numbers are at least 100,000, making senior competition involving Australia at open level unlikely for some time.\n\nIn 2004, a volunteer group known as World Footy News began documenting the growth of Australian football internationally through their website, becoming a major source of international football news, and for the first time providing a source of detailed coverage for the International Cups (2005 and 2008). Its website states that it \"was created to foster awareness of Australian Football around the globe and to aid communication between clubs, leagues and individuals playing and supporting Aussie Rules\". At various times between 2004 and 2007, other regularly updated sources included OziRulzGlobal, Fourth Quarter, and with slowly improving quantity, an International Leagues section of the AFL website.\n\nIn 2005, after eight years of growing domestic competition, the South African government declared Australian Football to be the sport for \"the new South Africa\", injecting government funding into the sport.\n\nIn 2006, Pakistan, Indonesia, Catalonia, Croatia, Norway, Bermuda and East Timor joined the list of playing nations.\n\nOn 3 July 2006, the AFL announced that it had formed an International Development Committee to support overseas leagues. The AFL hopes to develop the game in other countries to the point where Australian football is played at an international level by top-quality sides from around the world. The AFL plans to host the International Cup regularly every three or four years, beginning in 2008, the 150th anniversary of the code. Following the AFL's interest in the internationalisation of the game, coverage in the Australian media grew substantially.\n\nOn 14 April 2007, the Australian Institute of Sport Under 17 squad competed against the South African national Australian rules football team in the first international match between the two countries at North West Cricket Stadium in Potchefstroom, South Africa. The Australians won by a score of 162-12. In the same month, a massive junior program called \"FootyWILD\", similar to Auskick, was launched in the country.\n\nOn 25 April 2013, the first premiership match outside of Australia was held at Westpac Stadium in Wellington, New Zealand, between the Sydney Swans and the St Kilda Saints, and attracted a crowd of over 22,000 spectators. In the two subsequent years crowds at this event dwindled, and the matches in New Zealand were scrapped after 2015.\n\nThe first truly international competitor in Australian Football was New Zealand. In 1908, the Jubilee Australasian Football Carnival was held to celebrate the 50th anniversary of Australian rules football. New Zealand (then representing a total of 115 clubs) defeated both New South Wales and Queensland in the carnival, but lost to Victoria and Tasmania.\n\nThe 1995 Arafura Games, held in Darwin, Northern Territory, Australia became the first international sporting event to have Australian football as a competition sport, rather than a demonstration sport. Papua New Guinea won the gold medal and retained it in subsequent games. Other teams that have competed at Australian Rules in the games include Japan, Nauru, and a Northern Territory indigenous team. The International Australian football Council (IAFC) was formed after the 1995 Games.\n\nInspired by successful Arafura Games competitions, the inaugural Australian Football International Cup was held in Melbourne in 2002, as the last act of the IAFC, and held in conjunction with the AFL. The 2002 cup was contested by eleven teams from around the world, made up exclusively of non-Australians. Ireland won, defeating Papua New Guinea in the final.\n\nIn the interim years, Japan and New Zealand played an annual game as a curtain raiser to the AFL games. The New Zealand national team were victorious by 100 points in 2003, and so, in 2004, a club side from Auckland played the game, which Japan lost by two points. The amateur Australian Convicts also toured, playing several matches against sides from developing nations.\n\nThe second Australian Football International Cup was held in Melbourne in 2005, again under the guidance and funding of the AFL, with New Zealand defeating Papua New Guinea in the final. Third place went to the United States of America.\n\nIn 2001 The United States, Great Britain, Denmark and Ireland competed in the Atalantic Alliance Cup. This was fore runner to other European competitions starting with the EU Cup which became the Euro Cup and also the Central European Australian Football league Championships .\n\nIn 2006, Denmark, Sweden and Germany competed in a tri-nations series, which was planned to be repeated annually.\n\nThe third Australian Football International Cup was held in 2008 by the AFL in Melbourne, with a record 16 teams competing. Papua New Guinea won their first title, defeating New Zealand, and South Africa controversially defeated Ireland by 1 point to finish third.\n\nThe fourth Australian Football International Cup was held in 2011 by the AFL in Melbourne and Sydney, with a record 18 teams competing. Ireland won their second title by defeating Papua New Guinea who have appeared in every AFL International Cup grand final.\n\nOther international competitions that included some Australian expatriates are also held, including the EU Cup, which was first held in 2005 in London, featuring ten teams. In 2007 the Cup was held in Hamburg, with twelve teams.\n\nIn 2013 the East Asia Australian Football League was formed with Cambodia, Malaysia, Thailand, Vietnam, Singapore, Jakarta and Laos competing. \nAlso the South China Australian Football League consists of three teams from Hong Kong, Macau, Landau and Gangzhou.\n\nAlthough the AFL is regarded as the world governing body, it does not publish statistics for matches that it does not specifically sanction. By 2009, the only attempt to consolidate all world rankings was created by the World Footy News website, which for 2008 listed 22 countries, from Australia (1st) through to India (22nd). Detailed criteria were given as to whether a country qualified for consideration, though ultimately the rankings were listed as unofficial, and are only noteworthy because of the lack of any other system. The unofficial 2008 Australian Football World Rankings.\n\nA series of hybrid International rules matches between the Australian Football League's best professional players and a representative Gaelic football team from Ireland's Gaelic Athletic Association amateur players is staged annually. The rules are a compromise between the two codes, using a round ball and a rectangular field. The fierce tackling of the Australian code is allowed, although this has caused controversy with the Irish players. The series have remained evenly matched with the Irish using speed and athleticism, and the Australians strength and power — both inherent skills in their respective codes.\n\nThe International Australian football Council (IAFC) was formed in 1995 by a subset of playing countries to promote and develop Australian football internationally, before unanimously dissolving in favour of the AFL in 2002.\n\nAustralian football is not yet considered large enough internationally for a FIFA style governing body, so the Australian Football League is primarily responsible for funding and governance. In the mid-2000s, it provided around $30 million for development of the game in Australia and around A$500,000 annually for international development, with the following breakdown in 2005:\n\n\nIncluding AFL exhibition and NAB Cup matches, indigenous and AIS youth tours, International Cup funding and staff funding, this will have risen to around A$2,000,000 annually by 2008. Additional support for countries such as South Africa is leveraged through contacts with industry, and is increasingly adding to the total investment.\n\nMuch of the additional international promotion of the game is fuelled by exhibition matches, expatriate Australians, local leagues, and various AusAID projects. The internet is seen as a key tool in keeping diverse Australian football communities in contact.\n\nAlthough international football has a low profile within Australia, the issue is getting increased media exposure as several high-profile Australians have become advocates for international football. Former players and coaches that are involved in, have expressed interest in or are passionate about international footy at some stage include Ron Barassi, Kevin Sheedy, Jim Stynes, Paul Roos, Robert DiPierdomenico, Michael Long, Garry Lyon, Peter Schwab, Guy McKenna, Glenn Archer, Jason McCartney, Wayne Schwass, and Mal Michael. Current players who have expressed views or interest on the topic include David Rodan, Alipate Carlile, Jimmy Bartel, Jason Akermanis, Aaron Edwards, and Brad Moran. Former AFL players Mark Zanotti and John Ironmonger have been directly involved in living and establishing clubs overseas. Other non-players such as John So, Eddie McGuire, and Tiffany Cherry have also expressed interest in the media about the game being played or watched overseas.\n\nAustralia has had many women's leagues at both state and local level for decades. The first semi-professional women's Australian football league, the AFLW was founded in late 2016, after the AFL had conducted several exhibition matches over the previous few years. The inaugural season, held in 2017 was considered a huge success by the AFL and the league intends to expand from eight teams to 14 by the 2020 season.\n\nInternationally, women's Australian football is played at amateur level in several countries, particularly in the United Kingdom, North America and parts of Europe and Asia. and a dozen teams have represented their nations at one or more Australian Football International Cup since 2011. Ireland and Canada have dominated thus far, being the only teams to contest the women's grand final in the competition. Ireland's team, nicknamed the Banshees, are the most recent winners, defeating Canada by just four points in the grand final of the 2017 tournament.\n\nSeveral countries now have youth Australian rules programs in place. These countries include Papua New Guinea, New Zealand, Samoa, Tonga, Nauru, Denmark, South Africa, England, Indonesia, the United States, and Canada. The number of participants is quite high in PNG, RSA and NZ generally dependent on the level of AFL funding but some \"private\" endeavours in Canada and the UK have produced significant results.\n\nSince 1998, the Barassi International Australian Football Youth Tournament, endorsed by the Australian Football League as part of its International Policy, has hosted several of these nation's representative youth teams.\n\nThe first fully representative junior international Australian football outside of Australia was played between England and Denmark in Farum, Denmark, in October 2005. The Jakarta Bulldogs Australian Football Club, founded in 2006 by Alf Eddy, was an Australian Football Club made up of Under 18-year-old expatriate and local students in Jakarta. The team played against local teams such as the Pancawati Eagles, Depok Garudas, and the Jakarta Bintangs, and also travelled to Singapore and Malaysia in 2008 and 2009, respectively, for the Asian Australian Football Championships. The Bulldogs won the competition in both years.\nCurrently there is an increasing number of junior international Australian Football notably, North America, Scandinavia and the Pacific.\n\nAn AusAID funded project is South African junior development began in 2003, which is assisted by aid agency Australian Volunteers International in partnership with programs such as AFL Auskick, and sponsored by Tattersalls as well as the South African North West Academy of Sport.\n\nAnother funded junior project is Aussie Rules Schools UK, which is funded by Sport England and co-ordinated by AFL England and AFL Europe. This project has seen up to ten English schools adopt Aussie Rules as part of the school curriculum to combat obesity.\n\nIn February 2006, a joint project between the AFL, Melbourne Football Club, Melbourne City Council, and AusAID to post an Australian Youth Ambassador in Tianjin, a city of 10 million, about southeast of Beijing in an effort to kickstart Australian Football in China was announced.\n\nThere have been full-time development officers in Tonga and Samoa, as part of AusAid projects, since 2005.\n\nMichito Sakaki from Japan became the first international player to play at AFL level when selected to play for the Essendon Football Club against the Sydney Swans at an exhibition match at North Sydney Oval in February 2006. Mike Pyke, a former Canadian rugby player, was drafted to the Sydney Swans in 2009, and played his first game in Round 7 of 2009 against Geelong, becoming the first non-Irish international player to play an official league game.\n\nAustralia has recruited several Irish Gaelic footballers to play Aussie Rules. As Gaelic football is primarily an amateur competition and the AFL competition is professional, there is a strong financial lure. In the 1980s, the Melbourne Football Club recruited Jim Stynes, who would turn out to be the most successful Irish player in the history of the VFL/AFL, winning the Brownlow medal. At around the same time, the club recruited the Scot Sean Wight. In more recent years, the Sydney Swans recruited Irishman Tadhg Kennelly, who played in a premiership with the club and has also represented Ireland against Australia. Carlton Football Club experimented with brothers Setanta Ó hAilpín and Aisake Ó hAilpín. The Collingwood Football Club has recruited Martin Clarke, and the Brisbane Lions recruited Colm Begley and Brendan Quigley to their international rookie list. Due to increasing concern from the Gaelic Athletic Association, in 2006 the AFL made a deal with the GAA to limit the number of junior Gaelic drafts.\n\nAustralia has exported players to the NFL. Since the 1980s, many AFL players have tried out as American football punters. The special teams position requires the long range kicking skills often used by Australian football players, particularly those playing centre half-forward and full-forward. Although the punter position is one of the least valuable on an NFL team, punters and kickers have an average salary of around US$860,000 which surpasses the wages of AFL players, who average A$221,000. As the position is less physically demanding, it has also become attractive for players heading into retirement.\n\nList of Australian Football Leagues outside Australia\n\n\"News and Results Sites\"\n\n\"Fan sites\"\n\n\"Governing bodies\"\n\n\"International tournaments\"\n\n\"International leagues\"\n"}
{"id": "3514777", "url": "https://en.wikipedia.org/wiki?curid=3514777", "title": "Grapefruit diet", "text": "Grapefruit diet\n\nThe grapefruit diet, also known as the Hollywood Diet is a short-term fad diet that has existed in the United States since at least the 1930s. The diet is based on the claim that grapefruit has a fat-burning enzyme or similar property. The variations of the grapefruit diet that are too low in calories (below 800–1,000 calories a day), too low in carbohydrates, or too low in essential micronutrients are considered unhealthy and potentially dangerous. While eating half a grapefruit with every meal may be a good way to incorporate more fruit in the diet of a healthy person, grapefruit and grapefruit juice is harmful if the dieter is taking medicines that can interact with grapefruit juice or is allergic to citruses. This diet will not be beneficial to anyone over a long time as the extremely low calorie intake could lead to malnutrition and many health problems. The grapefruit diet also does not require exercise.\n\nThe grapefruit diet is a low-carb diet. It suggests that grapefruit helps burn body fat when eaten with foods high in dietary fat, which is why the grapefruit diet encourages consumption of meat, eggs, and other foods that are rich in fat and protein. A typical breakfast menu usually includes bacon and eggs. The grapefruit diet restricts consumption of carbohydrates by eliminating sugar, sweet fruits and vegetables, grains, and cereals. The grapefruit diet lasts for 10 to 12 days followed by 2 days off.\n\nThe grapefruit diet originated in the 1930s It was re-popularized in the 1980s and nicknamed the \"10-day, 10-pounds-off diet\".\n\nWeird Al Yankovic on his 1999 album \"Running with Scissors\" parodies both the grapefruit diet and the Cherry Poppin' Daddies' song \"Zoot Suit Riot\" in his song \"Grapefruit Diet\".\n\n"}
{"id": "55163921", "url": "https://en.wikipedia.org/wiki?curid=55163921", "title": "Henri Korn", "text": "Henri Korn\n\nHenri Korn is a neuroscientist with the Pasteur Institute. In 1992 he won the Richard Lounsbery Award jointly with Philippe Ascher for \"their discoveries of the mechanisms of synaptic transmission. Philippe Asher furthered knowledge regarding the properties of glutamate receptors which play an important role in trials, and Henri Korn brought to light the elementary liberation of neurotransmitter in quanta form in the central nervous system of vertebrates.\"\n"}
{"id": "4003918", "url": "https://en.wikipedia.org/wiki?curid=4003918", "title": "Huang Kun", "text": "Huang Kun\n\nHuang Kun () (September 2, 1919 – July 6, 2005), born in Beijing, an academician of the Chinese Academy of Sciences, was a well-known physicist in the People's Republic of China. He was awarded the State Preeminent Science and Technology Award (The highest science award in China) by the President of the People's Republic of China Jiang Zemin in 2001.\n\nBorn in Beijing, China, in 1919, Huang graduated from the Beijing-based Yenching University as a promising physicist. In 1948, he received a PhD degree from the H. H. Wills Physics Lab of Bristol University in the United Kingdom and continued his postdoctoral studies at Liverpool University where he coauthored the book of \"Dynamical Theory of Crystal Lattices\" with Max Born between 1949 and 1951. \n\nIn 1951, Huang returned to China to teach, and became a professor of physics at Peking University. In 1955, he became one of the first batch of academicians of the Chinese Academy of Sciences (CAS). After his retirement in 1983, Huang remained active in the research of semiconductors and was selected as the chairman of the Chinese Society of Physics between 1987 and 1991. He used to be the director of the institute of semiconductors of CAS. \n\nHuang made many founding contributions to the field of solid-state physics. His \"Dynamical Theory of Crystal Lattices\", which was a result of his collaboration with Nobel laureate German physicist Max Born, has become a classic work of modern physics, according to the announcement of the condolence committee. The Born–Huang approximation is partially named after him.\n"}
{"id": "20238890", "url": "https://en.wikipedia.org/wiki?curid=20238890", "title": "Implementation research", "text": "Implementation research\n\nImplementation research is the scientific study of barriers to and methods of promoting the systematic application of research findings in practice, including in public policy. Often, research projects focus on small scale pilot studies or laboratory-based experiments, and assume that findings can be generalised to roll out into a practice-based domain with few changes.\n\nImplementation research explores the challenges that are faced when generalizing research findings \"in the real world\", such as in the fields of healthcare or school-based education.\n\nIn the context of public health, the World Health Organization defines it as a form of research which \" addresses implementation bottlenecks, identifies optimal approaches for a particular setting, and promotes the uptake of research findings: ultimately, it leads to improved health care and its delivery.\" and it has been described to have four characteristics - systematic, multidisciplinary, contextual and complex.\nIt has been more broadly defined as “ the scientific inquiry into questions concerning implementation—the act of carrying an intention into effect, which in health research can be policies, programmes, or individual practices (collectively called interventions).”\n\nWhile a wide range of qualitative and quantitative research methods are used in implementation research in health but some of them have been developed more specifically for the purpose of implementation research . These are pragmatic trials, participatory action research, effectiveness-implementation hybrid trials and quality improvement studies.\n\nStaRI is the reporting standard for reporting implementation studies in public health.\n\n\n"}
{"id": "4760663", "url": "https://en.wikipedia.org/wiki?curid=4760663", "title": "Innumeracy (book)", "text": "Innumeracy (book)\n\nInnumeracy: Mathematical Illiteracy and its Consequences is a 1988 book by mathematician John Allen Paulos about \"innumeracy,\" a term he embraced to describe the mathematical equivalent of illiteracy: incompetence with numbers rather than words. Innumeracy is a problem with many otherwise educated and knowledgeable people. While many people would be ashamed to admit they are illiterate, there is very little shame in saying \"I'm a people person, not a numbers person.\" Or \"I always hated math\".\n\nPaulos speaks mainly of the common misconceptions in regard to numbers. He looks at real-world examples in stock scams, psychics, astrology, sports records, elections, sex discrimination, UFOs, insurance and law, lotteries and drug testing. Paulos discusses innumeracy with quirky anecdotes, scenarios and facts, encouraging readers in the end to look at their world in a more quantitative way. The book sheds light on the link between innumeracy and pseudoscience. For example, the fortune telling psychic's few correct and general observations are remembered over the many incorrect guesses. He also stresses the problem between the actual number of occurrences of various risks and popular perceptions of those risks happening. The problems of innumeracy come at a great cost to society. Topics include probability and coincidence, innumeracy in pseudoscience, statistics and trade-offs in society. For example, the danger of getting killed in a car accident is much greater than terrorism and this danger should be reflected in how we allocate our limited resources.\n\nJohn Allen Paulos (born July 4, 1945) is an American professor of mathematics at Temple University in Pennsylvania. He is a writer and speaker on mathematics and the importance of mathematical literacy. Paulos writes about many subjects, especially of the dangers of mathematical innumeracy; that is, the layperson's misconceptions about numbers, probability and logic. He has received awards in:\n2013 JPBM (Joint Policy Board for Mathematics) Award for Communicating Mathematics on a Sustained Basis to Large Audiences.\n2003 AAAS (American Association for the Advancement of Science) Award for Promoting the Public Understanding of Science and Technology.\n\nAs a reasons for writing the book he states:\nInnumeracy, an inability to deal comfortably with the fundamental notions of number and chance, plagues far too many otherwise knowledgeable citizens. The same people who cringe when words such as “imply” and “infer” are confused react without a trace of embarrassment to even the most egregious of numerical solecisms. I remember once listening to someone at a party drone on about the difference between “continually” and “continuously.” Later that evening we were watching the news, and the TV weathercaster announced that there was a 50 percent chance of rain for Saturday and a 50 percent chance for Sunday, and concluded that there was therefore a 100 percent chance of rain that weekend. The remark went right by the self-styled grammarian, and even after I explained the mistake to him, he wasn’t nearly as indignant as he would have been had the weathercaster left a dangling participle.\n\n\"Innumeracy\" made the New York Times best seller the year it came out in 1988. It was on the New York Times best seller for 18 weeks. There was a slightly revised addition in 2001. It received favorable reviews in the New York Times \"He takes us a couple of steps closer to numeracy, and it is all in all an enlightening place to be.\" The Chicago Tribune wrote \"Despite the title, which suggests yet another learned report documenting the sorry state of America's educational system, what Paulos provides is a readable romp across a varied mathematical landscape.\nIt serves as an excellent antidote to tedious classroom lectures on the difference between inverse and direct proportions.\" The LA Times review noted \"\"Paulos is very good at explaining all of this, though sometimes with a hectoring, bitter tone, for which he apologizes at the very end.\" The Christian Science Monitor review said \"Should you read \"Innumeracy\" if you enjoy reading math problems and reasoning them out? Yes, it's fun. Should you read it if you think you hate math and are turned off by math problems? Yes, you may even get turned on.\"\n"}
{"id": "28309002", "url": "https://en.wikipedia.org/wiki?curid=28309002", "title": "Isogenic human disease models", "text": "Isogenic human disease models\n\nIsogenic human disease models are a family of cells that are selected or engineered to accurately model the genetics of a specific patient population, \"in vitro\". They are provided with a genetically matched 'normal cell' to provide an isogenic system to research disease biology and novel therapeutic agents. They can be used to model any disease with a genetic foundation. Cancer is one such disease for which isogenic human disease models have been widely used.\n\nHuman isogenic disease models have been likened to 'patients in a test-tube', since they incorporate the latest research into human genetic diseases and do so without the difficulties and limitations involved in using non-human models.\n\nHistorically, cells obtained from animals, typically mice, have been used to model cancer-related pathways. However, there are obvious limitations inherent in using animals for modelling genetically determined diseases in humans. Despite a large proportion of genetic conservation between humans and mice, there are significant differences between the biology of mice and humans that are important to cancer research. For example, major differences in telomere regulation enable murine cells to bypass the requirement for telomerase upregulation, which is a rate-limiting step in human cancer formation. As another example, certain ligand-receptor interactions are incompatible between mice and humans. Additionally, experiments have demonstrated important and significant differences in the ability to transform cells, compared with cells of murine origin. For these reasons, it remains essential to develop models of cancer that employ human cells.\n\nIsogenic cell lines are created via a process called homologous gene-targeting. Targeting vectors that utilize homologous recombination are the tools or techniques that are used to knock-in or knock-out the desired disease-causing mutation or SNP (single nucleotide polymorphism) to be studied. Although disease mutations can be harvested directly from cancer patients, these cells usually contain many background mutations in addition to the specific mutation of interest, and a matched normal cell line is typically not obtained. Subsequently, targeting vectors are used to 'knock-in' or 'knock out' gene mutations enabling a switch in both directions; from a normal to cancer genotype; or vice versa; in characterized human cancer cell lines such as HCT116 or Nalm6.\n\nThere are several gene targeting technologies used to engineer the desired mutation, the most prevalent of which are briefly described, including key advantages and limitations, in the summary table below.\n\nHomologous recombination (HR) is a kind of genetic recombination in which genetic sequences are exchanged between two similar segments of DNA. HR plays a major role in eukaryotic cell division, promoting genetic diversity through the exchange between corresponding segments of DNA to create new, and potentially beneficial combinations of genes.\n\nHR performs a second vital role in DNA repair, enabling the repair of double-strand breaks in DNA which is a common occurrence during a cell's lifecycle. It is this process which is artificially triggered by the above technologies and bootstrapped in order to engender 'knock-ins' or 'knockouts' in specific genes5, 7.\n\nA recent key advance was discovered using AAV-homologous recombination vectors, which increases the low natural rates of HR in differentiated human cells when combined with gene-targeting vectors-sequences.\n\nFactors leading to the recent commercialization of isogenic human cancer cell disease models for the pharmaceutical industry and research laboratories are twofold.\n\nFirstly, successful patenting of enhanced targeting vector technology has provided a basis for commercialization of the cell-models which eventuate from the application of these technologies.\n\nSecondly, the trend of relatively low success rates in pharmaceutical RnD and the enormous costs have created a real need for new research tools that illicit how patient sub-groups will respond positively or be resistant to targeted cancer therapeutics based upon their individual genetic profile.\n\nThere are several companies working to address this need, a list of the key players and their technology offering is provided below.\n\n\n"}
{"id": "41904891", "url": "https://en.wikipedia.org/wiki?curid=41904891", "title": "List of MDPI academic journals", "text": "List of MDPI academic journals\n\nThis is a list of academic journals published by MDPI, a publisher listed on Jeffrey Beall's list of predatory open access publishing companies in 2014 but which was removed in 2015. As of April 2018, MDPI published 197 journals.\n\n"}
{"id": "49688036", "url": "https://en.wikipedia.org/wiki?curid=49688036", "title": "List of Orthodontic Functional Appliances", "text": "List of Orthodontic Functional Appliances\n\nThis is a comprehensive list of functional appliances that are used in the field of orthodontics. The functional appliances can be divided into fixed and removable. The fixed functional appliances have to be bonded to the teeth by an Orthodontist in their practice. A removable functional appliance does not need to be bonded on the teeth and can be removed by the patient. A removal appliance is usually used by patients who have high degree of compliance with their orthodontic treatment. Fixed appliances are able to produce very accurate movement in the teeth \n\nBoth fixed and removable functional appliances can be used to correct a malocclusion in 3 planes of spaces: Anterior-Posterior, Vertical and Transverse. In the Anterior-Posterior dimension, appliances such as Class II and Class III are used. Appliances used in transverse dimension are utilized to expand either the maxillary or the mandibular arch. Appliances used in vertical dimension are used to correct open or deep bite.\n\nIt is important to note that the dento-facial Orthopedics was mainly done in Europe initially as United States was introduced to Fixed Orthodontics by Edward Angle. Norman William Kingsley was the first person to show \"jumping the bite\" by using an anterior bite plate. Hotz then developed the Vorbissplate which was modification of Kingsley's plate. Wilhelm Roux is credited to be the first person who studied the effects of functional forces on Orthodontics in 1883. His workings were used by other dentists in future to study the dental orthopedics. His teachings became known as Roux Hypothesis, which Karl Haupl later expanded. Monobloc developed by Pierre Robin (surgeon) in 1902 is considered to be one of the first functional appliances in Orthodontics. The Monobloc was a modification of Ottolengui’s removable plate. In 1908, Viggo Andersen developed the Activator appliance. This was the first functional appliance to be widely accepted, especially in Europe. This appliance became the \"Norwegian\" system of treatment in Orthodontics in early 1900s.\n\nIn addition, in 1905 the Herbst Appliance was introduced by Emil Herbst. This appliance did not go through much evolution until the 1970s when Hans Pancherz revived interest in this appliance. In 1950s, Wilhem Balters modified Andersen's Activator appliance and gave the new appliance the name Bionator Appliance which was designed to produce forward positioning of the mandible. Positioner Appliance was developed by Harold Kesling in 1944 in order to aid the Orthodontic treatment during the finishing stage. The Frankel Appliance were developed by Rolf Frankel in 1957 for treatment of Class I, II, III Malocclusions . William Clark also developed Twin Block Appliance in 1978 which resembled Artur Martin Schwarz double plates that he developed in 1950s.\n\n\n\n\n\n\n\nSome of the components of removal appliances are retentive in nature. They are usually connecting by an acrylic component known as baseplate. The majority of the appliances include components such as \"Labial Bow\" and \"Adams Clasp\", both of these components are passive in nature. Labial bow is a wire attached to the baseplate which goes around the incisor teeth to provide retention of those teeth. Labial bow usually have U-Loops at the end to allow it to activate more. Adams clasps are used for retention of these removable appliances and are usually fabricated in the molar areas. They are usually manufactured from 0.7mm hard stainless steel wire (HSSW), or 0.6mm HSSW when planned for deciduous teeth. Removal of the appliance is usually performed by holding the bridge of this clasp. Other clasps that are usually used are C clasps on canines, Southend Clasp (on anteriors), Ball-ended clasp (primarily for use with the Twin Block system in the lower anteriors) and Plint clasp.\n\nActive components of removable appliances include springs which provides light forces on a tooth to move it orthodontically. Components such as Palatal Finger Springs, Buccal Canine Retractor, Z-Spring, T-Spring, Coffin Spring, Active Labial Bows (Mill's Bow or Roberts retractor), Screws and Elastics are all considered to be active components of the removable functional appliances. If a spring is moving one tooth it is made of 0.5mm thick stainless steel wire. The thickness increases to 0.6 or 0.7mm wire if it is to move more teeth or a larger/multi rooted tooth.\n\n\n\n\n\n\n"}
{"id": "37894917", "url": "https://en.wikipedia.org/wiki?curid=37894917", "title": "List of even-toed ungulates by population", "text": "List of even-toed ungulates by population\n\nThis is a list of even-toed ungulate species by estimated global population. This list is not comprehensive, as not all ungulates have had their numbers quantified.\n"}
{"id": "3310078", "url": "https://en.wikipedia.org/wiki?curid=3310078", "title": "List of pioneers in computer science", "text": "List of pioneers in computer science\n\nThis article presents a list of individuals who made transformative breakthroughs in the creation, development and imagining of what computers and electronics could do.\n\n~ Items marked with a tilde are circa dates.\n\n\n"}
{"id": "4506267", "url": "https://en.wikipedia.org/wiki?curid=4506267", "title": "List of weather records", "text": "List of weather records\n\nThis is a list of weather records, a list of the most extreme occurrences of weather phenomena for various categories. Many weather records are measured under specific conditions—such as surface temperature and wind speed—to keep consistency among measurements around the Earth. Each of these records is understood to be the record value officially observed, as these records may have been exceeded before modern weather instrumentation was invented, or in remote areas without an official weather station. This list does not include remotely sensed observations such as satellite measurements, since those values are not considered official records.\n\nThe standard measuring conditions for temperature are in the air, above the ground, and shielded from direct sunlight intensity (hence the term, x degrees \"in the shade\"). The following lists include all officially confirmed claims measured by those methods.\n\nTemperatures measured directly on the ground may exceed air temperatures by . The highest natural ground surface temperature ever recorded was at Furnace Creek, Death Valley, California, United States on 15 July 1972. In recent years a ground temperature of has been recorded in Port Sudan, Sudan. The theoretical maximum possible ground surface temperature has been estimated to be between for dry, darkish soils of low thermal conductivity.\n\nSatellite measurements of ground temperature taken between 2003 and 2009, taken with the MODIS infrared spectroradiometer on the Aqua satellite, found a maximum temperature of , which was recorded in 2005 in the Lut Desert, Iran. The Lut Desert was also found to have the highest maximum temperature in 5 of the 7 years measured (2004, 2005, 2006, 2007 and 2009). These measurements reflect averages over a large region and so are lower than the maximum point surface temperature.\n\nSatellite measurements of the surface temperature of Antarctica, taken between 1982 and 2013, found a coldest temperature of on 10 August 2010, at . Although this is not comparable to an air temperature, it is believed that the air temperature at this location would have been lower than the official record lowest air temperature of .\n\nAccording to the World Meteorological Organization's (WMO), the highest temperature ever recorded was on 10 July 1913 in Furnace Creek (Greenland Ranch), California, USA. According to the WMO this temperature may have been the result of \"a sandstorm that occurred at the time. Such a storm may have caused superheated surface materials to hit upon the temperature in the shelter.\"\n\nThe former highest official temperature on Earth (held for 90 years by ‘Aziziya, Libya) was reassessed in July 2012 by the WMO which published a report that invalidated the record.\n\nThere have been other unconfirmed reports of high temperatures, with readings as high as in the Flaming Mountains of China in 2008. However, these temperatures have never been confirmed, and are currently considered to have been recorder's errors, thus not being recognised as world records.\n\nChristopher C. Burt, the weather historian writing for Weather Underground who shepherded the Libya reading's 2012 disqualification, believes that the 1913 Death Valley reading is \"a myth\", and is at least too high. Burt proposes that the highest reliably recorded temperature on Earth could be at Death Valley, but is instead recorded on 30 June 2013. was recorded another four times: 20 July 1960, 18 July 1998, 20 July 2005, and 7 July 2007. On 21 July 2016, Mitribah in Kuwait also recorded a maximum temperature of , tying Death Valley's highest reliably recorded temperature on Earth, while Basra in Iraq reached that day. On 29 June 2017, the air at the airport of Ahvaz in Iran reached as well. In a second part to his analysis, he gave a list of 11 other occasions in which temperatures of or more were reliably measured as well as the highest reliably measured temperatures on each continent.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "24305930", "url": "https://en.wikipedia.org/wiki?curid=24305930", "title": "MCAG group", "text": "MCAG group\n\nThe MCAG group is a group of Mycobacteria. \n\nIt includes Mycobacterium chelonae and Mycobacterium abscessus.\n"}
{"id": "52202", "url": "https://en.wikipedia.org/wiki?curid=52202", "title": "Magic square", "text": "Magic square\n\nIn recreational mathematics and combinatorial design, a magic square is a formula_1 square grid (where is the number of cells on each side) filled with distinct positive integers in the range formula_2 such that each cell contains a different integer and the sum of the integers in each row, column and diagonal is equal. The sum is called the \"magic constant\" or \"magic sum\" of the magic square. A square grid with cells on each side is said to have \"order n\".\n\nThe mathematical study of magic squares typically deals with its construction, classification, and enumeration. Although completely general methods for producing all the magic squares of all orders do not exist, historically three general techniques have been discovered: by bordering method, by making composite magic squares, and by adding two preliminary squares. There are also more specific strategies like the continuous enumeration method that reproduces specific patterns. The magic squares are generally classified according to their order \"n\" as: odd if \"n\" is odd, evenly even (also referred to as \"doubly even\") if \"n\" = 4\"k\" (e.g. 4, 8, 12, and so on), oddly even (also known as \"singly even\") if \"n\" = 4\"k\" + 2 (e.g. 6, 10, 14, and so on). This classification is based on different techniques required to construct odd, evenly even, and oddly even squares. Beside this, depending on further properties, magic squares are also classified as associated magic squares, pan-diagonal magic squares, most-perfect magic square, and so on. More challengingly, attempts have also been made to classify all the magic squares of a given order as transformations of a smaller set of squares. Except for \"n\" ≤ 5, the enumeration of higher order magic squares is still an open challenge. The enumeration of most-perfect magic squares of any order was only accomplished in the late 20th century.\n\nIn regard to magic sum, the problem of magic squares only requires the sum of each row, column and diagonal to be equal, it does not require the sum to be a particular value. Thus, although magic squares may contain negative integers, they are just variations by adding or multiplying a negative number to every positive integer in the original square. Magic squares composed of integers formula_2 are also called \"normal magic squares\", in the sense that there are \"non-normal magic squares\" whose integers are not restricted to formula_2. However, in some places, \"\"magic squares\" is used as a general term to cover both the normal and non-normal ones, especially when non-normal ones are under discussion. Moreover, the term \"magic squares\"\" is sometimes also used to refer to various types of word squares.\n\nMagic squares have a long history, dating back to at least 190 BCE in China. At various times they have acquired magical or mythical significance, and have appeared as symbols in works of art. In modern times they have been generalized a number of ways, including using extra or different constraints, multiplying instead of adding cells, using alternate shapes or more than two dimensions, and replacing numbers with shapes and addition with geometric operations.\n\nThe third order magic square was known to Chinese mathematicians as early as 190 BCE, and explicitly given by the first century of the common era. The first datable instance of the fourth order magic square occur in 587 CE in India. Specimens of magic squares of order 3 to 9 appear in an encyclopedia from Baghdad \"circa\" 983, the \"Encyclopedia of the Brethren of Purity\" (\"Rasa'il Ihkwan al-Safa\"). By the end of 12th century, the general methods for constructing magic squares were well established. Around this time, some of these squares were increasingly used in conjunction with magic letters, as in Shams Al-ma'arif, for occult purposes. In India, all the fourth order pandiagonal magic squares were enumerated by Narayana in 1356. Magic squares were made known to Europe through translation of Arabic sources as occult objects during the Renaissance, and the general theory had to be re-discovered independent of prior developments in China, India, and Middle East. Also notable are the ancient cultures with a tradition of mathematics and numerology that did not discover the magic squares: Greeks, Babylonians, Egyptians, and Pre-Columbian Americans.\n\nWhile ancient references to the pattern of even and odd numbers in the 3×3 magic square appears in the \"I Ching\", the first unequivocal instance of this magic square appears in a 1st century book \"Da Dai Liji\" (Record of Rites by the Elder Dai). \n\nThe above magic squares of orders 3 to 9 are taken from Yang Hui's treatise, in which the Luo Shu principle is clearly evident. The order 5 square is a bordered magic square, with central 3×3 square formed according to Luo Shu principle. The order 9 square is a composite magic square, in which the nine 3×3 sub squares are also magic. After Yang Hui, magic squares frequently occur in Chinese mathematics such as in Ding Yidong's \"Dayan suoyin\" (circa 1300), Chen Dawei's \"Suanfa tongzong\" (1593), Fang Zhongtong's \"Shuduyan\" (1661) which contains magic circles, cubes and spheres, Zhang Chao's \"Xinzhai zazu\" (circa 1650), who published China's the first magic square of order ten, and lastly Bao Qishou's \"Binaishanfang ji\" (circa 1880), who gave various three dimensional magic configurations. However, despite being the first to discover the magic squares and getting a head start by several centuries, the Chinese development of the magic squares are much inferior compared to the Islamic, the Indian, or the European developments. The high point of Chinese mathematics that deals with the magic squares seems to be contained in the work of Yang Hui; but even as a collection of older methods, this work is much more primitive, lacking general methods for constructing magic squares of any order, compared to a similar collection written around the same time by the Byzantine scholar Manuel Moschopoulos. This is possibly because of the Chinese scholars' enthralment with the Lo Shu principle, which they tried to adapt to solve higher squares; and after Yang Hui and the fall of Yuan dynasty, their systematic purging of the foreign influences in Chinese mathematics.\n\nAlthough the early history of magic squares in Persia and Arabia is not known, it has been suggested that they were known in pre-Islamic times. It is clear, however, that the study of magic squares was common in medieval Islam, and it was thought to have begun after the introduction of chess into the region. The first datable appearance of magic square of order 3 occur in the alchemical works of Jābir ibn Hayyān (fl. c. 721– c. 815). While it is known that treatises on magic squares were written in the 9th century, the earliest extant treaties we have date from the 10th-century: one by Abu'l-Wafa al-Buzjani (circa 998) and another by Ali b. Ahmad al-Antaki (circa 987). These early treatise were purely mathematical, and the Arabic designation for magic squares is \"wafq al-a'dad\" which translates as \"harmonious disposition of the numbers\". By the end of 10th century, the two treatises by Buzjani and Antaki makes it clear that the Islamic mathematicians had understood how to construct bordered squares of any order as well as simple magic squares of small orders (\"n\" ≤ 6) which were used to make composite magic squares. A specimen of magic squares of orders 3 to 9 devised by Islamic mathematicians appear in an encyclopedia from Baghdad \"circa\" 983, the Rasa'il Ikhwan al-Safa (the Encyclopedia of the Brethren of Purity). The squares of order 3 to 7 from Rasa'il are given below:\n\nThe 11th century saw the finding of several ways to construct simple magic squares for odd and evenly-even orders; the more difficult case of evenly-odd case (\"n = 4k + 2\") was solved by Ibn al-Haytham with \"k\" even (circa 1040), and completely by the beginning of 12th century, if not already in the latter half of the 11th century. Around the same time, pandiagonal squares were being constructed. Treaties on magic squares were numerous in the 11th and 12th century. These later developments tended to be improvements on or simplifications of existing methods. From the 13th century on wards, magic squares were increasingly put to occult purposes. However, much of these later texts written for occult purposes merely depict certain magic squares and mention their attributes, without describing their principle of construction, with only some authors keeping the general theory alive. One such occultist was the Egyptian Ahmad al-Buni (circa 1225), who gave general methods on constructing bordered magic squares; some others were the 17th century Egyptian Shabramallisi and the 18th century Nigerian al-Kishnawi.\n\nThe magic square of order three was described as a child-bearing charm since its first literary appearances in the alchemical works of Jābir ibn Hayyān (fl. c. 721– c. 815) and al-Ghazālī (1058–1111) and it was preserved in the tradition of the planetary tables. The earliest occurrence of the association of seven magic squares to the virtues of the seven heavenly bodies appear in Andalusian scholar Ibn Zarkali's (known as Azarquiel in Europe) (1029–1087) \"Kitāb tadbīrāt al-kawākib\" (\"Book on the Influences of the Planets\"). A century later, the Egyptian scholar Ahmad al-Buni attributed mystical properties to magic squares in his highly influential book \"Shams al-Ma'arif\" (\"The Book of the Sun of Gnosis and the Subtleties of Elevated Things\"), which also describes their construction. This tradition about a series of magic squares from order three to nine, which are associated with the seven planets, survives in Greek, Arabic, and Latin versions. There are also references to the use of magic squares in astrological calculations, a practice that seems to have originated with the Arabs.\n\nThe 3×3 magic square has been a part of rituals in India since ancient times, and still is today. For instance, the Kubera-Kolam, a magic square of order three, is commonly painted on floors in India. It is essentially the same as the Lo Shu Square, but with 19 added to each number, giving a magic constant of 72. The 3×3 magic square first appears in India in \"Gargasamhita\" by Garga, who recommends its use to pacify the nine planets (\"navagraha\"). The oldest version of this text dates from 100 CE; however passage on planets could not have been written earlier than 400 CE. The first datable instance of 3×3 magic square in India occur in a medical text \"Siddhayog\" (ca. 900 CE) by Vrnda, which was prescribed to women in labor in order to have easy delivery. \n\nThe earliest unequivocal occurrence of magic square is found in a work called \"Kaksaputa\", composed by the alchemist Nagarjuna around 1st century CE. All of the squares given by Nagarjuna are 4×4 magic squares, and one of them is called \"Nagarjuniya\" after him. Nagarjuna gave a method of constructing 4×4 magic square using a primary skeleton square, given an odd or even magic sum. Incidentally, the special Nagarjuniya square cannot be constructed from the method he expounds. The Nagarjuniya square is given below, and has the sum total of 100.\n\nThe Nagarjuniya square is a pan-diagonal magic square. The Nagarjuniya square is made up of two arithmetic progressions starting from 6 and 16 with eight terms each, with a common difference between successive terms as 4. When these two progressions are reduced to the normal progression of 1 to 8, we obtain the adjacent square.\n\nThe oldest datable magic square in the world is found in an encyclopaedic work written by Varahamihira around 587 CE called \"Brhat Samhita\". The magic square is constructed for the purpose of making perfumes using 4 substances selected from 16 different substances. Each cell of the square represents a particular ingredient, while the number in the cell represents the proportion of the associated ingredient, such that the mixture of any four combination of ingredients along the columns, rows, diagonals, and so on, gives the total volume of the mixture to be 18. Although the book is mostly about divination, the magic square is given as a matter of combinatorial design, and no magical properties are attributed to it.\n\nThe square of Varahamihira as given above has sum of 18. Here the numbers 1 to 8 appear twice in the square. It is a pan-diagonal magic square. It is also an instance of most perfect magic square. Four different magic squares can be obtained by adding 8 to one of the two sets of 1 to 8 sequence. The sequence is selected such that the number 8 is added exactly twice in each row, each column and each of the main diagonals. One of the possible magic squares shown in the right side. This magic square is remarkable in that it is a 90 degree rotation of a magic square that appears in the 13th century Islamic world as one of the most popular magic squares. \n\nAround 12th-century, a 4×4 magic square was inscribed on the wall of Parshvanath temple in Khajuraho, India. Several Jain hyms teach how to make magic squares, although they are undatable.\n\nAs far as is known, the first systematic study of magic squares in India was conducted by Thakkar Pheru, a Jain scholar, in his \"Ganitasara Kaumudi\" (ca. 1315). This work contains a small section on magic squares which consists of nine verses. Here he gives a square of order four, and alludes to its rearrangement; classifies magic squares into three (odd, evenly even, and oddly even) according to its order; gives a square of order six; and prescribes one method each for constructing even and odd squares. For the even squares, Pheru divides the square into component squares of order four, and puts the numbers into cells according to the pattern of a standard square of order four. For odd squares, Pheru gives the method using horse move or knight's move. Although algorithmically different, it gives the same square as the De la Loubere's method.\n\nThe next comprehensive work on magic squares was taken up by Narayana Pandit, who in the fourteenth chapter of his \"Ganita Kaumudi\" (1356) gives general methods for their construction, along with the principles governing such constructions. It consists of 55 verses for rules and 17 verses for examples. Narayana gives a method to construct all the pan-magic squares of fourth order using knight's move; enumerates the number of pan-diagonal magic squares of order four, 384, including every variation made by rotation and reflection; three general methods for squares having any order and constant sum when a standard square of the same order is known; two methods each for constructing evenly even, oddly even, and odd squares when the sum is given. While Narayana describes one older method for each species of square, he claims the method of superposition for evenly even and odd squares and a method of interchange for oddly even squares to be his own invention. The superposition method was later re-discovered by De la Hire in Europe. In the last section, he conceives of other figures, such as circles, rectangles, and hexagons, in which the numbers may be arranged to possess properties similar to those of magic squares. Below are some of the magic squares constructed by Narayana: \n\nThe order 8 square is interesting in itself since it is an instance of the most-perfect magic square. Incidentally, Narayana states that the purpose of studying magic squares is to construct \"yantra\", to destroy the ego of bad mathematicians, and for the pleasure of good mathematicians. The subject of magic squares is referred to as \"bhadraganita\" and Narayana states that it was first taught to men by god Shiva.\n\nUnlike in Persia and Arabia, we have better documentation of how the magic squares were transmitted to Europe. Around 1315, influenced by Islamic sources, the Greek Byzantine scholar Manuel Moschopoulos wrote a mathematical treatise on the subject of magic squares, leaving out the mysticism of his Muslim predecessors, where he gave two methods for odd squares and two methods for evenly even squares. Moschopoulos was essentially unknown to the Latin Europe until the late 17th century, when Philippe de la Hire rediscovered his treatise in the Royal Library of Paris. However, he was not the first European to have written on magic squares; and the magic squares were disseminated to rest of Europe through Spain and Italy as occult objects. The early occult treaties that displayed the squares did not describe how they were constructed. Thus the entire theory had to be rediscovered.\n\nMagic squares had first appeared in Europe in \"Kitāb tadbīrāt al-kawākib\" (\"Book on the Influences of the Planets\") written by Ibn Zarkali of Toledo, Al-Andalus, as planetary squares by 11th century. The magic square of three was discussed in numerological manner in early 12th century by Jewish scholar Abraham ibn Ezra of Toledo, which influenced later Kabbalists. Ibn Zarkali's work was translated as \"Libro de Astromagia\" in the 1280s, due to Alfonso X of Castille. In the Alfonsine text, magic squares of different orders are assigned to the respective planets, as in the Islamic literature; unfortunately, of all the squares discussed, the Mars magic square of order five is the only square exhibited in the manuscript.\n\nMagic squares surface again in Florence, Italy in the 14th century. A 6×6 and a 9×9 square are exhibited in a manuscript of the \"Trattato d'Abbaco\" (Treatise of the Abacus) by Paolo Dagomari. It is interesting to observe that Paolo Dagomari, like Pacioli after him, refers to the squares as a useful basis for inventing mathematical questions and games, and does not mention any magical use. Incidentally, though, he also refers to them as being respectively the Sun's and the Moon's squares, and mentions that they enter astrological calculations that are not better specified. As said, the same point of view seems to motivate the fellow Florentine Luca Pacioli, who describes 3×3 to 9×9 squares in his work \"De Viribus Quantitatis\" by the end of 15th century.\n\nThe planetary squares had disseminated into northern Europe by the end of 15th century. For instance, the Cracow manuscript of \"Picatrix\" from Poland displays magic squares of orders 3 to 9. The same set of squares as in the Cracow manuscript later appears in the writings of Paracelsus in \"Archidoxa Magica\" (1567), although in highly garbled form. In 1514 Albrecht Dürer immortalized a 4×4 square in his famous engraving \"Melencolia I.\" Paracelsus' contemporary Heinrich Cornelius Agrippa von Nettesheim published his famous book \"De occulta philosophia\" in 1531, where he devoted a chapter to the planetary squares shown below. The same set of squares given by Agrippa reappear in 1539 in \"Practica Arithmetice\" by Girolamo Cardano. The tradition of planetary squares was continued into the 17th century by Athanasius Kircher in \"Oedipi Aegyptici\" (1653). In Germany, mathematical treaties concerning magic squares were written in 1544 by Michael Stifel in \" Arithmetica Integra\", who rediscovered the bordered squares, and Adam Riese, who rediscovered the continuous numbering method to construct odd ordered squares published by Agrippa. However, due to the religious upheavals of that time, these work were unknown to the rest of Europe.\n\nIn 1624 France, Claude Gaspard Bachet described the \"diamond method\" for constructing Agrippa's odd ordered squares in his book \"Problèmes Plaisants\". In 1691, Simon de la Loubère described the Indian continuous method of constructing odd ordered magic squares in his book \"Du Royaume de Siam\", which he had learned while returning from a diplomatic mission to Siam, which was faster than Bachet's method. In an attempt to explain its working, de la Loubere used the primary numbers and root numbers, and rediscovered the method of adding two preliminary squares. This method was further investigated by Abbe Poignard in \"Traité des quarrés sublimes\" (1704), and then later by Philippe de La Hire in \"Mémoires de l’Académie des Sciences\" for the Royal Academy (1705), and by Joseph Sauveur in \"Construction des quarrés magiques\" (1710). In the two treatise \"Des quarrez magiques\" and \"Table générale des quarrez magiques de quatre de côté\" published posthumously in 1693, Bernard Frenicle de Bessy demonstrated that there were exactly 880 distinct magic squares of order four and also gave methods to find magic squares of any even order. De la Hire also introduced concentric bordered square in 1705, while Sauveur introduced magic cubes and lettered squares, which was taken up later by Euler in 1776, who is often credited for devising them. In 1750 d'Ons-le-Bray rediscovered the method of constructing doubly even and singly even squares using bordering technique. By this time the earlier mysticism attached to the magic squares had completely vanished, and the subject was treated as a part of recreational mathematics.\n\nIn the 19th century, Bernard Violle gave a comprehensive treatment of magic squares in his three volume \"Traité complet des carrés magiques\" (1837—1838), which also described magic cubes, parallelograms, parallelopipeds, and circles. Pandiagonal squares were extensively studied by Andrew Hollingworth Frost, who learned it while in the town of Nasik, India, (thus calling them Nasik squares) in a series of articles: \"On the knight's path\" (1877), \"On the General Properties of Nasik Squares\" (1878), \"On the General Properties of Nasik Cubes\" (1878), \"On the construction of Nasik Squares of any order\" (1896). He showed that it is impossible to have normal singly-even pandiagonal magic square. Frederick A.P. Barnard constructed inlaid magic squares and other three dimensional magic figures like magic spheres and magic cylinders in \"Theory of magic squares and of magic cubes\" (1888). In 1897, Emroy McClintock published \"On the most perfect form of magic squares\", coining the words \"pandiagonal square\" and \"most perfect square\", which had previously been referred to as perfect, or diabolic, or Nasik.\n\nLegends dating from as early as 650 BC tell the story of the Lo Shu (洛书) or \"scroll of the river Lo\". According to the legend, there was at one time in ancient China a huge flood. While the great king Yu was trying to channel the water out to sea, a turtle emerged from it with a curious pattern on its shell: a 3×3 grid in which circular dots of numbers were arranged, such that the sum of the numbers in each row, column and diagonal was the same: 15. According to the legend, thereafter people were able to use this pattern in a certain way to control the river and protect themselves from floods. The Lo Shu Square, as the magic square on the turtle shell is called, is the unique normal magic square of order three in which 1 is at the bottom and 2 is in the upper right corner. Every normal magic square of order three is obtained from the Lo Shu by rotation or reflection.\n\nThere is a well-known 12th-century 4×4 magic square inscribed on the wall of the Parshvanath temple in Khajuraho, India.\n\nThis is known as the \"Chautisa Yantra\" since its magic sum is 34. It is one of the three 4×4 pandiagonal magic squares and is also an instance of the most-perfect magic square. The study of this square led to the appreciation of pandiagonal squares by European mathematicians in the late 19th century. Pandiagonal squares were referred to as Nasik squares or Jain squares in older English literature.\n\nThe order four magic square Albrecht Dürer immortalized in his 1514 engraving \"Melencolia I,\", referred to above, is believed to be the first seen in European art. The square associated with Jupiter appears as a talisman used to drive away melancholy. It is very similar to Yang Hui's square, which was created in China about 250 years before Dürer's time. The sum 34 can be found in the rows, columns, diagonals, each of the quadrants, the center four squares, and the corner squares (of the 4×4 as well as the four contained 3×3 grids). This sum can also be found in the four outer numbers clockwise from the corners (3+8+14+9) and likewise the four counter-clockwise (the locations of four queens in the two solutions of the 4 queens puzzle), the two sets of four symmetrical numbers (2+8+9+15 and 3+5+12+14), the sum of the middle two entries of the two outer columns and rows (5+9+8+12 and 3+2+15+14), and in four kite or cross shaped quartets (3+5+11+15, 2+10+8+14, 3+9+7+15, and 2+6+12+14). The two numbers in the middle of the bottom row give the date of the engraving: 1514. The numbers 1 and 4 at either side of the date correspond respectively to the letters \"A\" and \"D,\" which are the initials of the artist.\n\nDürer's magic square can also be extended to a magic cube.\n\nThe Passion façade of the Sagrada Família church in Barcelona, conceptualized by Antoni Gaudí and designed by sculptor Josep Subirachs, features a 4×4 magic square: The magic constant of the square is 33, the age of Jesus at the time of the Passion. Structurally, it is very similar to the Melancholia magic square, but it has had the numbers in four of the cells reduced by 1.\nWhile having the same pattern of summation, this is not a normal magic square because two numbers (10 and 14) are duplicated and two (12 and 16) are absent, failing the rule that an n x n magic square must contain each of the positive integers 1 through \"n\". Such squares are not generally mathematically interesting and may be described, in a slightly deprecative sense, as trivial. Lee Sallows has pointed out that, due to Subirachs's ignorance of magic square theory, the renowned sculptor made a needless blunder, and supports this assertion by giving several examples of non-trivial 4 x 4 magic squares showing the desired magic constant of 33.\n\nSimilarly to Dürer's magic square, the Sagrada Familia's magic square can also be extended to a magic cube.\n\nThe constant that is the sum of every row, column and diagonal is called the magic constant or magic sum, \"M.\" Every normal magic square has a constant dependent on the order , calculated by the formula formula_5, since the sum of formula_2 is formula_7 which when divided by the order is the magic constant. For normal magic squares of orders \"n\" = 3, 4, 5, 6, 7, and 8, the magic constants are, respectively: 15, 34, 65, 111, 175, and 260 (sequence in the OEIS).\n\nThe 1×1 magic square, with only one cell containing the number 1, is called \"trivial\", because it is typically not under consideration when discussing magic squares; but it is indeed a magic square by definition, if we regard a single cell as a square of order one.\n\nNormal magic squares of all sizes can be constructed except 2×2 (that is, where order \"n\" = 2).\n\nExcluding rotations and reflections, there is exactly one 3×3 magic square, exactly 880 4×4 magic squares, and exactly 275,305,224 5×5 magic squares. For the 6×6 case, there are estimated to be approximately 1.8 × 10 squares.\n\nIf we think of the numbers in the magic square as masses located in various cells, then the center of mass of a magic square coincides with its geometric center.\n\nThe \"moment of inertia\" of a magic square has been defined as the sum over all cells of the number in the cell times the squared distance from the center of the cell to the center of the square; here the unit of measurement is the width of one cell. (Thus for example a corner cell of a 3×3 square has a distance of formula_8 a non-corner edge cell has a distance of 1, and the center cell has a distance of 0.) Then all magic squares of a given order have the same moment of inertia as each other. For the order-3 case the moment of inertia is always 60, while for the order-4 case the moment of inertia is always 340. In general, for the \"n\"×\"n\" case the moment of inertia is formula_9\n\nDividing each number of the magic square by the magic constant will yield a doubly stochastic matrix, whose row sums and column sums equal to unity. However, unlike the doubly stochastic matrix, the diagonal sums of such matrices will also equal to unity. Thus, such matrices constitute a subset of doubly stochastic matrix. The Birkhoff–von Neumann theorem states that for any doubly stochastic matrix formula_10, there exists real numbers formula_11, where formula_12 and permutation matrices formula_13 such that\n\nThis representation may not be unique in general. By Marcus-Ree theorem, however, there need not be more than formula_15 terms in any decomposition. Clearly, this decomposition carries over to magic squares as well, since we can recover a magic square from a doubly stochastic matrix by multiplying it by the magic constant.\n\nWhile the classification of magic squares can be done in many way, some useful categories are given below. An \"n\"×\"n\" square array of integers 1, 2, ..., \"n\" is called:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are many ways to construct magic squares, but the standard (and most simple) way is to follow certain configurations/formulas which generate regular patterns. After a magic square has been constructed using some standard formula, the transformations described in the previous section can be applied to yield further magic squares.\nMagic squares exist for all values of \"n\", with only one exception: it is impossible to construct a magic square of order 2. Magic squares can be classified into three types according to their order: odd, doubly even (\"n\" divisible by four) and singly even (\"n\" even, but not divisible by four). Odd and doubly even magic squares are easy to generate; the construction of singly even magic squares is more difficult but several methods exist, including the LUX method for magic squares (due to John Horton Conway) and the Strachey method for magic squares.\n\nGroup theory was also used for constructing new magic squares of a given order from one of them.\nThe numbers of different \"n\"×\"n\" magic squares for \"n\" from 1 to 5, not counting rotations and reflections are: 1, 0, 1, 880, 275305224 . The number for \"n\" = 6 has been estimated to be \n\nCross-referenced to the above sequence, a new classification enumerates the magic tori that display these magic squares. The numbers of magic tori of order \"n\" from 1 to 5, are: 1, 0, 1, 255, 251449712 .\n\nIn the 19th century, Édouard Lucas devised the general formula for order 3 magic squares. Consider the following table made up of positive integers \"a\", \"b\" and \"c\":\n\nThese 9 numbers will be distinct positive integers forming a magic square so long as 0 < \"a\" < \"b\" < \"c\" − \"a\" and \"b\" ≠ 2\"a\". Moreover, every 3×3 square of distinct positive integers is of this form.\n\nIn 1997 Lee Sallows discovered that leaving aside rotations and reflections, then every distinct parallelogram drawn on the Argand diagram defines a unique 3×3 magic square, and vice versa, a result that had never previously been noted.\n\nA method for constructing magic squares of odd order was published by the French diplomat de la Loubère in his book, \"A new historical relation of the kingdom of Siam\" (Du Royaume de Siam, 1693), in the chapter entitled \"The problem of the magical square according to the Indians\". The method operates as follows:\n\nThe method prescribes starting in the central column of the first row with the number 1. After that, the fundamental movement for filling the squares is diagonally up and right, one step at a time. If a filled square is encountered, one moves vertically down one square instead, then continues as before. When an \"up and to the right\" move would leave the square, it is wrapped around to the last row or first column, respectively.\n\nStarting from other squares rather than the central column of the first row is possible, but then only the row and column sums will be identical and result in a magic sum, whereas the diagonal sums will differ. The result will thus be a semimagic square and not a true magic square. Moving in directions other than north east can also result in magic squares.\n\nDoubly even means that \"n\" is an even multiple of an even integer; or 4\"p\" (e.g. 4, 8, 12), where \"p\" is an integer.\n\nGeneric pattern\nAll the numbers are written in order from left to right across each row in turn, starting from the top left hand corner. Numbers are then either retained in the same place or interchanged with their diametrically opposite numbers in a certain regular pattern. In the magic square of order four, the numbers in the four central squares and one square at each corner are retained in the same place and the others are interchanged with their diametrically opposite numbers.\n\nA construction of a magic square of order 4 \nGo left to right through the square counting and filling in on the diagonals only. Then continue by going left to right from the top left of the table and fill in counting down from 16 to 1. As shown below.\n\nAn extension of the above example for Orders 8 and 12\nFirst generate a pattern table, where a '1' indicates selecting from the square where the numbers are written in order 1 to n (left-to-right, top-to-bottom), and a '0' indicates selecting from the square where the numbers are written in reverse order \"n\" to 1. For \"M\" = 4, the pattern table is as shown below (third matrix from left). When we shade the unaltered cells (cells with '1'), we get a criss-cross pattern.\n\nNote that a) there are equal number of '1's and '0's in each row and column; b) each row and each column are \"palindromic\"; c) the left- and right-halves are mirror images; and d) the top- and bottom-halves are mirror images (c & d imply b). The pattern table can be denoted using hexadecimals as (9, 6, 6, 9) for simplicity (1-nibble per row, 4 rows). The simplest method of generating the required pattern for higher ordered doubly even squares is to copy the generic pattern for the fourth order square in each four-by-four sub-squares.\n\nFor M = 8, possible choices for the pattern are (99, 66, 66, 99, 99, 66, 66, 99); (3C, 3C, C3, C3, C3, C3, 3C, 3C); (A5, 5A, A5, 5A, 5A, A5, 5A, A5) (2-nibbles per row, 8 rows). \n\nFor M = 12, the pattern table (E07, E07, E07, 1F8, 1F8, 1F8, 1F8, 1F8, 1F8, E07, E07, E07) yields a magic square (3-nibbles per row, 12 rows.) It is possible to count the number of choices one has based on the pattern table, taking rotational symmetries into account.\n\nThe earliest discovery of the superposition method was made by the Indian mathematician Narayana in the 14th century. The same method was later re-discovered and studied in early 18th century Europe by de la Loubere, Poignard, de La Hire, and Sauveur; and the method is usually referred to as de la Hire's method. Although Euler's work on magic square was unoriginal, he famously conjectured the impossibility of constructing the evenly odd ordered mutually orthogonal Graeco-Latin squares. This conjecture was disproved in the mid 20th century. For clarity of exposition, we have distinguished two important variations of this method.\n\nThis method consists of first constructing two preliminary squares, which when added gives the magic square. As a running example, we will consider a 3×3 magic square. We can uniquely label each number of the 3×3 square by a pair of numbers as\n\nwhere every pair of Greek and Latin alphabets, e.g. \"αa\", are meant to be added together, i.e. \"αa = α + a\". Here, (\"α\", \"β\", \"γ\") = (0, 3, 6) and (\"a\", \"b\", \"c\") = (1, 2, 3). The numbers 0, 3, and 6 are referred to as the \"root numbers\" while the numbers 1, 2, and 3 are referred to as the \"primary numbers\". An important general constraint to note here is that\n\n\nThus, the original square can now be split into two simpler squares:\n\nThe lettered squares are referred to as \"Greek square\" or \"Latin square\" if the they are filled with Greek or Latin letters, respectively. A magic square can be constructed by ensuring that the Greek and Latin squares are magic squares too, provided that the Greek and Latin alphabets are paired with each other only once. The converse of this statement is also often, but not always (e.g. bordered magic squares), true: A magic square can be decomposed into a Greek and a Latin square, which are themselves magic squares. Thus the method is useful for both synthesis as well as analysis of a magic square. Lastly, by examining the pattern in which the numbers are laid out in the finished square, it is often possible to come up with a faster algorithm to construct higher order squares that replicate the given pattern, without the necessity of creating the preliminary Greek and Latin squares.\n\nDuring the construction of the 3×3 magic square, the Greek and Latin squares with just three unique terms are much easier to deal with than the original square with nine different terms. The row sum and the column sum of the Greek square will be the same, \"α\" + \"β\" + \"γ\", if\n\n\nThis can be achieved by cyclic permutation of \"α\", \"β\", and \"γ\". Satisfaction of these two conditions ensures that the resulting square is a semi-magic square; and such Greek and Latin squares are said to be \"mutually orthogonal\" to each other. To construct a magic square, we should also ensure that the diagonals sum to magic constant. For this, we have a third condition:\n\n\nThe mutually orthogonal Greek and Latin squares that satisfy the first part of the third condition (that all letters appear in both the diagonals) are said to be \"mutually orthogonal doubly diagonal Graeco-Latin squares\". \n\nOdd squares: For the odd square, since \"α\", \"β\", and \"γ\" are in arithmetic progression, their sum is equal to the product of the square's order and the middle term, i.e. \"α\" + \"β\" + \"γ\" = 3 \"β\". Thus, the diagonal sums will be equal if we have \"β\"s in the main diagonal and \"α\", \"β\", \"γ\" in the skew diagonal. Similarly, for the Latin square. The resulting Greek and Latin squares and their combination will be as below. Note that the Latin square is just a rotation of the Greek square with the corresponding letters interchanged. Substituting the values of the Greek and Latin letters will give the 3×3 magic square.\n\nFor the odd squares, this method explains why the Siamese method (method of De la Loubere) and its variants work. This basic method can be used to construct odd ordered magic squares of higher orders. To summarise:\n\n\nA peculiarity of the construction method given above for the odd magic squares is that the middle number (\"n\" + 1)/2 will always appear at the center cell of the magic square. Since there are (\"n\" - 1)! ways to arrange the skew diagonal terms, we can obtain (\"n\" - 1)! Greek squares this way; same with the Latin squares. Also, since each Greek square can be paired with (\"n\" - 1)! Latin squares, and since for each of Greek square the middle term may be arbitrarily placed in the main diagonal or the skew diagonal (and correspondingly along the skew diagonal or the main diagonal for the Latin squares), we can construct a total of 2 × (\"n\" - 1)! × (\"n\" - 1)! magic squares using this method. For \"n\" = 3, 5, and 7, this will give 8, 1152, and 1,036,800 different magic squares, respectively. Dividing by 8 to neglect equivalent squares due to rotation and reflections, we obtain 1, 144, and 129,600 essentially different magic squares, respectively.\n\nAs another example, the construction of 5×5 magic square is given. Numbers are directly written in place of alphabets. The numbered squares are referred to as \"primary square\" or \"root square\" if they are filled with primary numbers or root numbers, respectively. The numbers are placed about the skew diagonal in the root square such that the middle column of the resulting root square has 0, 5, 10, 15, 20 (from bottom to top). The primary square is obtained by rotating the root square counter-clockwise by 90 degrees, and replacing the numbers. The resulting square is an associative magic square, in which every pair of numbers symmetrically opposite to the center sum up to the same value, 26. For e.g., 16+10, 3+23, 6+20, etc. In the finished square, 1 is placed at center cell of bottom row, and successive numbers are placed via elongated knight's move (two cells right, two cells down). When a collision occurs, the break move is to move one cell up. Also note that all the odd numbers occur inside the central diamond formed by 1, 5, 25 and 21, while the even numbers are placed at the corners. The occurrence of the even numbers can be deduced by copying the square to the adjacent sides. The even numbers from four adjacent squares will form a cross. \n\nA variation of the above example, where the skew diagonal sequence is taken in different order, is given below. The resulting magic square is the flipped version of the famous Agrippa's Mars magic square. It is an associative magic square and is the same as that produced by Moschopoulos's method. Here the resulting square starts with 1 placed in the cell which is to the right of the centre cell, and proceeds as De la Loubere's method, with downwards-right move. When a collision occurs, the break move is to shift two cells to the right.\n\nIn the previous examples, for the Greek square, the second row can be obtained from the first row by circularly shifting it to the right by one cell. Similarly, the third row is a circularly shifted version of the second row by one cell to the right; and so on. Likewise, the rows of the Latin square is circularly shifted to the left by one cell. Note that the row shifts for the Greek and Latin squares are in mutually opposite direction. It is possible to circularly shift the rows by more than one cell to create the Greek and Latin square. \n\n\nThis essentially re-creates the knight's move. All the letters will appear in both the diagonals, ensuring correct diagonal sum. Since there are \"n\"! permutations of the Greek letters by which we can create the first row of the Greek square, there are thus \"n\"! Greek squares that can be created by shifting the first row in one direction. Likewise, there are \"n\"! such Latin squares created by shifting the first row in the opposite direction. Since a Greek square can be combined with any Latin square with opposite row shifts, there are \"n\"! × \"n\"! such combinations. Lastly, since the Greek square can be created by shifting the rows either to the left or to the right, there are a total of 2 × \"n\"! × \"n\"! magic squares that can be formed by this method. For \"n\" = 5 and 7, since they are prime numbers, this method creates 28,800 and 50,803,200 pandiagonal magic squares. Dividing by 8 to neglect equivalent squares due to rotation and reflections, we obtain 3,600 and 6,350,400 equivalent squares. Further dividing by \"n\" to neglect equivalent panmagic squares due to cyclic shifting of rows or columns, we obtain 144 and 129,600 essentially different panmagic squares. For order 5 squares, these are the only panmagic square there are. The condition that the square's order not be divisible by 3 means that we cannot construct squares of orders 9, 15, 21, 27, and so on, by this method.\n\nIn the example below, the square has been constructed such that 1 is at the center cell. In the finished square, the numbers can be continuously enumerated by the knight's move (two cells up, one cell right). When collision occurs, the break move is to move one cell up, one cell left.The resulting square is a pandiagonal magic square. This square also has a further diabolical property that any five cells in quincunx pattern formed by any odd sub-square, including wrap around, sum to the magic constant, 65. For e.g., 13+7+1+20+24, 23+1+9+15+17, 13+21+10+19+2 etc. Also the four corners of any 5×5 square and the central cell, as well as the middle cells of each side together with the central cell, including wrap around, give the magic sum: 13+10+19+22+1 and 20+24+12+8+1. Lastly the four rhomboids that form elongated crosses also give the magic sum: 23+1+9+24+8, 15+1+17+20+12, 14+1+18+13+19, 7+1+25+22+10. \n\nWe can also combine the Greek and Latin squares constructed by different methods. In the example below, the primary square is made using knight's move. We have re-created the magic square obtained by De la Loubere's method. As before, we can form 8 × (\"n\" - 1)! × \"n\"! magic squares by this combination. For \"n\" = 5 and 7, this will create 23,040 and 29,030,400 magic squares. After dividing by 8 in order to neglect equivalent squares due to rotation and reflection, we get 2,880 and 3,628,800 squares. \n\nFor order 5 squares, these three methods give a complete census of the number of magic squares that can be constructed by the method of superposition. Neglecting the rotation and reflections, the total number of magic squares of order 5 produced by the superposition method is 144 + 3,600 + 2,880 = 6,624. \n\nEven squares: We can also construct even ordered squares in this fashion. Since there is no middle term among the Greek and Latin alphabets for even ordered squares, in addition to the first two constraint, for the diagonal sums to yield the magic constant, all the letters in the alphabet should appear in the main diagonal and the skew diagonal in the Greek and Latin square of even order.\n\nAn example of a 4×4 square is given below. For the given diagonal and skew diagonal in the Greek square, the rest of the cells can be filled using the condition that each letter appear only once in a row and a column.\n\nUsing these two Graeco-Latin squares, we can construct 2 × 4! × 4! = 1,152 magic squares. Dividing by 8 to eliminate equivalent squares due to rotation and reflections, we get 144 essentially different magic squares of order 4. These are the only magic squares constructible by the Euler method, since there are only two mutually orthogonal doubly diagonal Graeco-Latin squares of order 4.\n\nSimilarly, an 8×8 magic square can be constructed as below. Here the order of appearance of the numbers is not important; however the quadrants imitate the layout pattern of the 4×4 Graeco-Latin squares. \n\nEuler's method has given rise to the study of Graeco-Latin squares. Euler's method for constructing magic squares is valid for any order except 2 and 6.\n\nVariations: Magic squares constructed from mutually orthogonal doubly diagonal Graeco-Latin squares are interesting in themselves since the magic property emerges from the relative position of the alphabets in the square, and not due to any arithmetic property of the value assigned to them. This means that we can assign any value to the alphabets of such squares and still obtain a magic square. This is the basis for constructing squares that display some information (e.g. birthdays, years, etc) in the square and for creating \"reversible squares\". For example, we can display the number π ≈ 3·141592 at the bottom row of a 4×4 magic square using the Graeco-Latin square given above by assigning (\"α\", \"β\", \"γ\", \"δ\") = (10, 0, 90, 15) and (\"a\", \"b\", \"c\", \"d\") = (0, 2, 3, 4). We will obtain the following non-normal magic square with the magic sum 124:\nNarayana-De la Hire's method for odd square is the same as that of Euler's. However, for even squares, we drop the second requirement that each Greek and Latin letter appear only once in a given row or column. This allows us to take advantage of the fact that the sum of an arithmetic progression with an even number of terms is equal to the sum of two opposite symmetric terms multiplied by half the total number of terms. Thus, when constructing the Greek or Latin squares,\n\n\nAs a running example, if we take a 4×4 square, where the Greek and Latin terms have the values (\"α\", \"β\", \"γ\", \"δ\") = (0, 4, 8, 12) and (\"a\", \"b\", \"c\", \"d\") = (1, 2, 3, 4), respectively, then we have \"α\" + \"β\" + \"γ\" + \"δ\" = 2 (\"α\" + \"δ\") = 2 (\"β\" + \"γ\"). Similarly, \"a\" + \"b\" + \"c\" + \"d\" = 2 (\"a\" + \"d\") = 2 (\"b\" + \"c\"). This means that the complementary pair \"α\" and \"δ\" (or \"β\" and \"γ\") can appear twice in a column (or a row) and still give the desired magic sum. Thus, we can construct:\n\n\nIn the example given below, the main diagonal (from top left to bottom right) is filled with sequence ordered as \"α\", \"β\", \"γ\", \"δ\", while the skew diagonal (from bottom left to top right) filled in the same order. The remaining cells are then filled column wise such that the complementary letters appears only once within a row, but twice within a column. In the first column, since \"α\" appears on the 1st and 4th row, the remaining cells are filled with its complementary term \"δ\". Similarly, the empty cells in the 2nd column are filled with \"γ\"; in 3rd column \"β\"; and 4th column \"α\". Each Greek letter appears only once along the rows, but twice along the columns. As such, the row sums are \"α\" + \"β\" + \"γ\" + \"δ\" while the column sums are either 2 (\"α\" + \"δ\") or 2 (\"β\" + \"γ\"). Likewise for the Latin square, which is obtained by flipping the Greek square along the main diagonal and interchanging the corresponding letters.\n\nThe above example explains why the \"criss-cross\" method for doubly even magic square works. Another possible 4×4 magic square, which is also pan-diagonal as well as most-perfect, is constructed below using the same rule. However, the diagonal sequence is chosen such that all four letters \"α\", \"β\", \"γ\", \"δ\" appear inside the central 2×2 sub-square. Remaining cells are filled column wise such that each letter appears only once within a row. In the 1st column, the empty cells need to be filled with one of the letters selected from the complementary pair \"α\" and \"δ\". Given the 1st column, the entry in the 2nd row can only be \"δ\" since \"α\" is already there in the 2nd row; while, in the 3rd row the entry can only be \"α\" since \"δ\" is already present in the 3rd row. We proceed similarly until all cells are filled. The Latin square given below has been obtained by flipping the Greek square along the main diagonal and replacing the Greek alphabets with corresponding Latin alphabets. \n\nWe can use this approach to construct singly even magic squares as well. However, we have to be more careful in this case since the criteria of pairing the Greek and Latin alphabets uniquely is not automatically satisfied. Violation of this condition leads to some missing numbers in the final square, while duplicating others. Thus, here is an important proviso:\n\n\nBelow is a construction of a 6×6 magic square, where the numbers are directly given, rather than the alphabets. The second square is constructed by flipping the first square along the main diagonal. Here in the first column of the root square the 3rd cell is paired with its complement in the 4th cells. Thus, in the primary square, the numbers in the 1st and 6th cell of the 3rd row are same. Likewise, with other columns and rows. In this example the flipped version of the root square satisfies this proviso.\n\nAs another example of a 6×6 magic square constructed this way is given below. Here the diagonal entries are arranged differently. The primary square is constructed by flipping the root square about the main diagonal. In the second square the proviso for singly even square is not satisfied, leading to a non-normal magic square (third square) where the numbers 3, 13, 24, and 34 are duplicated while missing the numbers 4, 18, 19, and 33. \n\nThe last condition is a bit arbitrary and may not always need to be invoked, as in this example: \n\nAs one more example, we have generated an 8×8 magic square. Unlike the criss-cross pattern of the earlier section for evenly even square, here we have a checkered pattern for the altered and unaltered cells. Also, in each quadrant the odd and even numbers appear in alternating columns.\n\nVariations: A number of variations of the basic idea are possible: \"a complementary pair can appear \"n\"/2 times or less in a column\". That is, a column of a Greek square can be constructed using more than one complementary pair. This method allows us to imbue the magic square with far richer properties. The idea can also be extended to the diagonals too. An example of an 8×8 magic square is given below. In the finished square each of four quadrants are pan-magic squares as well, each quadrant with same magic constant 130.\n\nIn this method, the objective is to wrap a border around a smaller magic square which serves as a core. Let us consider the 3×3 square for example. Subtracting the middle number 5 from each number 1, 2, ..., 9, we obtain 0, ± 1, ± 2, ± 3, and ± 4, which we will refer to as bone numbers. The magic constant of a magic square, which we will refer to as the skeleton square, made by these bone numbers will be zero since adding all the rows of a magic square will give \"nM\" = Σ \"k\" = 0; thus \"M\" = 0. \n\nIt is not difficult to argue that the middle number should be placed at the center cell: let \"x\" be the number placed in the middle cell, then the sum of the middle column, middle row, and the two diagonals give Σ \"k\" + 3 \"x\" = 4 \"M\". Since Σ \"k\" = 3 \"M\", we have \"x\" = \"M\" / 3. Here \"M\" = 0, so \"x\" = 0. \n\nPutting the middle number 0 in the center cell, we want to construct a border such that the resulting square is magic. Let the border be given by:\n\nSince the sum of each row, column, and diagonals must be a constant (which is zero), we have \n\nThus, it must be that if we have chosen \"a\", \"b\", \"u\", and \"v\", then we have \"a*\" = - \"a\", \"b*\" = - \"b\", \"u*\" = - \"u\", and \"v*\" = - \"v\". But how should we choose \"a\", \"b\", \"u\", and \"v\"? We have the sum of the top row and the sum of the right column as \n\nSince 0 is an even number, there are only two ways that the sum of three integers will yield an even number: 1) if all three were even, or 2) if two were odd and one was even. Since in our choice of numbers we only have two even non-zero number (± 2 and ± 4), the first statement is false. Hence, it must be the case that the second statement is true: that two of the numbers are odd and one even. \n\nThe only way that both the above two equations can satisfy this parity condition simultaneously, and still be consistent with the set of numbers we have, is when \"u\" and \"v\" are odd. For on the contrary, if we had assumed \"u\" and \"a\" to be odd and \"v\" to be even in the first equation, then \"u*\" = - \"u\" will be odd in the second equation, making \"b\" odd as well, in order to satisfy the parity condition. But this requires three odd numbers (\"u\", \"a\", and \"b\"), contradicting the fact that we only have two odd numbers (± 1 and ± 3) which we can use. This proves that the odd bone numbers occupy the corners cells. When converted to normal numbers by adding 5, this implies that the corners of a 3×3 magic square are all occupied by even numbers. \n\nThus, taking \"u\" = 1 and \"v\" = 3, we have \"a\" = - 4 and \"b\" = - 2. Hence, the finished skeleton square will be as in the left. Adding 5 to each number, we get the finished magic square.\n\nSimilar argument can be used to construct larger squares. Since there does not exist a 2×2 magic square around which we can wrap a border to construct a 4×4 magic square, the next smallest order for which we can construct bordered square is the order 5.\n\nLet us consider the fifth order square. For this, we have a 3×3 magic core, around which we will wrap a magic border. The bone numbers to be used will be ± 5, ± 6, ± 7, ± 8, ± 9, ± 10, ± 11, and ± 12. Disregarding the signs, we have 8 bone numbers, 4 of which are even and 4 of which are odd. Let the magic border be given as\n\nAs before, we should \n\n\nIt is sufficient to determine the numbers \"u, v, a, b, c, d, e, f\" to describe the magic border. As before, we have the two constraint equations for the top row and right column:\n\nMultiple solutions are possible. The standard procedure is to \n\n\nThere are 28 ways of choosing two numbers from the set of 8 bone numbers for the corner cells \"u\" and \"v\". However, not all pairs are admissible. Among the 28 pairs, 16 pairs are made of an even and an odd number, 6 pairs have both as even numbers, while 6 pairs have them both as odd numbers. \n\nWe can prove that the corner cells \"u\" and \"v\" cannot have an even and an odd number. This is because if this were so, then the sums \"u\" + \"v\" and \"v\" + \"u*\" will be odd, and since 0 is an even number, the sums \"a\" + \"b\" + \"c\" and \"d\" + \"e\" + \"f\" should be odd as well. The only way that the sum of three integers will result in an odd number is when 1) two of them are even and one is odd, or 2) when all three are odd. Since the corner cells are assumed to be odd and even, neither of these two statements are compatible with the fact that we only have 3 even and 3 odd bone numbers at our disposal. This proves that \"u\" and \"v\" cannot have different parity. This eliminates 16 possibilities.\n\nNow consider the case when both \"u\" and \"v\" are even. The 6 possible pairs are: (6, 8), (6, 10), (6, 12), (8, 10), (8, 12), and (10, 12). Since the sums \"u\" + \"v\" and \"v\" + \"u*\" are even, the sums \"a\" + \"b\" + \"c\" and \"d\" + \"e\" + \"f\" should be even as well. The only way that the sum of three integers will result in an even number is when 1) two of them are odd and one is even, or 2) when all three are even. The fact that the two corner cells are even means that we have only 2 even numbers at our disposal. Thus, the second statement is not compatible with this fact. Hence, it must be the case that the first statement is true: two of the three numbers should be odd, while one be even. \n\nLet \"a, b, d, e\" be odd numbers while \"c\" and \"f\" be even numbers. Given the odd bone numbers at our disposal: ± 5, ± 7, ± 9, and ± 11, their differences range from \"D\" = { ± 2, ± 4, ± 6} while their sums range from \"S\" = {± 12, ± 14, ± 16, ± 18, ± 20}. It is also useful to have a table of their sum and differences for later reference. Now, given the corner cells (\"u\", \"v\"), we can check its admissibility by checking if the sums \"u\" + \"v\" + \"c\" and \"v\" + \"u*\" + \"f\" fall within the set \"D\" or \"S\". The admissibility of the corner numbers is a necessary but not a sufficient condition for the solution to exist.\n\nFor example, if we consider the pair (\"u\", \"v\") = (8, 12), then \"u\" + \"v\" = 20 and \"v\" + \"u*\" = 6; and we will have ± 6 and ± 10 even bone numbers at our disposal. Taking \"c\" = ± 6, we have the sum \"u\" + \"v\" + \"c\" to be 26 and 14, both of which do not fall within the sets \"D\" or \"S\". Likewise, taking \"c\" = ± 10, we have the sum \"u\" + \"v\" + \"c\" to be 30 and 10, both of which again do not fall within the sets \"D\" or \"S\". Thus, the pair (8, 12) is not admissible. By similar process of reasoning, we can also rule out the pair (6, 12).\n\nAs another example, if we consider the pair (\"u\", \"v\") = (10, 12), then \"u\" + \"v\" = 22 and \"v\" + \"u*\" = 2; and we will have ± 6 and ± 8 even bone numbers at our disposal. Taking \"c\" = ± 6, we have the sum \"u\" + \"v\" + \"c\" to be 28 and 16. While 28 does not fall within the sets \"D\" or \"S\", 16 falls in set \"S\". By inspection, we find that if (\"a\", \"b\") = (-7, -9), then \"a\" + \"b\" = -16; and it will satisfy the first constraint equation. Also, taking \"f\" = ± 8, we have the sum \"v\" + \"u*\" + \"f\" to be 10 and -6. While 10 does not fall within the sets \"D\" or \"S\", -6 falls in set \"D\". Since -7 and -9 have already been assigned to \"a\" and \"b\", clearly (\"d\", \"e\") = (-5, 11) so that \"d\" + \"e\" = 6; and it will satisfy the second constraint equation. \n\nLikewise, taking \"c\" = ± 8, we have the sum \"u\" + \"v\" + \"c\" to be 30 and 14. While 30 does not fall within the sets \"D\" or \"S\", 14 falls in set \"S\". By inspection, we find that if (\"a\", \"b\") = (-5, -9), then \"a\" + \"b\" = -14. Also, taking \"f\" = ± 6, we have the sum \"v\" + \"u*\" + \"f\" to be 8 and -4. While 8 does not fall within the sets \"D\" or \"S\", -4 falls in set \"D\". Clearly, (\"d\", \"e\") = (-7, 11) so that \"d\" + \"e\" = 4, and the second constraint equation will be satisfied.\n\nHence the corner pair (\"u\", \"v\") = (10, 12) is admissible; and it admits two solutions: (a, b, c, d, e, f) = (-7, -9, -6, -5, 11, -8) and (a, b, c, d, e, f) = ( -5, -9, -8, -7, 11, -6). The finished skeleton squares are given below. The magic square is obtained by adding 13 to each cells.\n\nUsing similar process of reasoning, we can construct the following table for the values of \"u, v, a, b, c, d, e, f\" expressed as bone numbers as given below. There are only 6 possible choices for the corner cells, which leads to 10 possible border solutions.\n\nGiven this group of 10 borders, we can construct 10×8×(3!) = 2880 essentially different bordered magic squares. Here the bone numbers ± 5, ..., ± 12 were consecutive. More bordered squares can be constructed if the numbers are not consecutive. Bernard Voille, in his \"Traité complet des carrés magiques\" (1837—1838), lists a total of 594 such magic borders for order 5 square; thus there are 594×8×(3!) = 171,072 essentially different bordered magic squares. It should be noted that the number of fifth order magic squares constructible via the bordering method is almost 25 times larger than via the superposition method.\n\nExhaustive enumeration of all the borders of a magic square of a given order, as done previously, is very tedious. As such a structured solution is often desirable, which allows us to construct a border for a square of any order. Below we give three algorithms for constructing border for odd, evenly even, and evenly odd squares. These continuous enumeration algorithms were discovered in 10th century by Islamic scholars; and their earliest surviving exposition comes from the two treatises by al-Buzjani and al-Antaki, although they themselves were not the discoverers.\n\nOdd ordered squares: The following is the algorithm given by al-Buzjani to construct a border for odd squares. A peculiarity of this method is that for order \"n\" square, the two adjacent corners comprise of numbers \"n - 1\" and \"n + 1\". \n\nStarting from the cell above the lower left corner, we put the numbers alternately in left column and bottom row until we arrive at the middle cell. The next number is written in the middle cell of the bottom row just reached, after which we fill the cell in the upper left corner, then the middle cell of the right column, then the upper right corner. After this, starting from the cell above middle cell of the right column already filled, we resume the alternate placement of the numbers in the right column and the top row. Once half of the border cells are filled, the other half are filled by numbers complementary to opposite cells. The subsequent inner borders is filled in the same manner, until the square of order 3 is filled.\n\nBelow is an example for 9th order square.\nEvenly even order: The following is the method given by al-Antaki. Consider an empty border of order \"n\" = 4\"k\" with \"k\" ≥ 3. The peculiarity of this algorithm is that the adjacent corner cells are occupied by numbers \"n\" and \"n - 1\". \n\nStarting at the upper left corner cell, we put the successive numbers by groups of four, the first one next to the corner, the second and the third on the bottom, and the fourth at the top, and so on until there remains in the top row (excluding the corners) six empty cells. We then write the next two numbers above and the next four below. We then fill the upper corners, first left then right. We place the next number below the upper right corner in the right column, the next number on the other side in the left column. We then resume placing groups of four consecutive numbers in the two columns as before. Once half of the border cells are filled, the other half are filled by numbers complementary to opposite cells.\n\nThe example below gives the border for order 16 square.\nFor order 8 square, we just begin directly with the six cells.\nEvenly odd order: For evenly odd order, we have the algorithm given by al-Antaki. Here the corner cells are occupied by \"n\" and \"n\" - 1. Below is an example of 10th order square.\n\nStart by placing 1 at the bottom row next to the left corner cell, then place 2 in the top row. After this, place 3 at the bottom row and turn around the border in anti-clockwise direction placing the next numbers, until \"n\" - 2 is reached on the right column. The next two numbers are placed in the upper corners (\"n\" - 1 in upper left corner and \"n\" in upper right corner). Then, the next two numbers are placed on the left column, then we resume the cyclic placement of the numbers until half of all the border cells are filled. Once half of the border cells are filled, the other half are filled by numbers complementary to opposite cells. \n\nThis is a method reminiscent of the Kronecker product of two matrices, that builds an \"nm\" × \"nm\" magic square from an \"n\" × \"n\" magic square and an \"m\" × \"m\" magic square. The \"product\" of two magic squares creates a magic square of higher order than the two multiplicands. Let the two magic squares be of orders \"m\" and \"n\". The final square will be of order \"m × n\". Divide the square of order \"m × n\" into \"m × m\" sub-squares, such that there are a total of \"n\" such sub-squares. In the square of order \"n\", reduce by 1 the value of all the numbers. Multiply these reduced values by \"m\", and place the results in the corresponding sub-squares of the \"m × n\" whole square. The squares of order \"m\" are added \"n\" times to the sub-squares of the final square. The peculiarity of this construction method is that each magic subsquare will have different magic sums. The square made of such magic sums from each magic subsquare will again be a magic square. The smallest composite magic square of order 9, composed of two order 3 squares is given below.\n\nThe next smallest composite magic squares of order 12, composed of magic squares order 3 and 4 are given below.\n\nWhen the square are of doubly even order, we can construct a composite magic square in a manner more elegant than the above process, in the sense that every magic subsquare will have the same magic constant. Let \"n\" be the order of the main square and \"m\" the order of the equal subsquares. The subsquares are filled one by one, in any order, with a continuous sequence of \"m\"/2 smaller numbers (i.e. numbers less than or equal to \"n\"/2) together with their complements to \"n\" + 1. Each subsquare as a whole will yield the same magic sum. The advantage of this type of composite square is that each subsquare is filled in the same way and their arrangement is arbitrary. Thus, the knowledge of a single construction of even order will suffice to fill the whole square. Furthermore, if the subsquares are filled in the natural sequence, then the resulting square will be pandiagonal. The magic sum of the subsquares is related to the magic sum of the whole square by formula_16 where \"k\" = \"n\"/\"m\". \n\nIn the examples below, we have divided the order 12 square into nine subsquares of order 4 filled each with eight smaller numbers and, in the corresponding bishop's cells (two cells diagonally across, including wrap arounds, in the 4×4 subsquare), their complements to \"n\" + 1 = 145. Each subsquare is a pandiagonal with magic constant 290; while the whole square on the left is also pandiagonal with magic constant 870.\n\nIn another example below, we have divided the order 12 square into four order 6 squares. Each of the order 6 squares are filled with eighteen small numbers and their complements using bordering technique given by al-Antaki. If we remove the shaded borders of the order 6 subsquares and form an order 8 square, then this order 8 square is again a magic square. In its full generality, we can take any \"m\"/2 smaller numbers together with their complements to \"n\" + 1 to fill the subsquares, not necessarily in continuous sequence.\n\nThis method is based on a 2006 published mathematical game called medjig (author: Willem Barink, editor: Philos-Spiele). The pieces of the medjig puzzle are squares divided in four quadrants on which the numbers 0, 1, 2 and 3 are dotted in all sequences. There are 18 squares, with each sequence occurring 3 times. The aim of the puzzle is to take 9 squares out of the collection and arrange them in a 3×3 \"medjig-square\" in such a way that each row and column formed by the quadrants sums to 9, along with the two long diagonals.\n\nThe medjig method of constructing a magic square of order 6 is as follows:\n\nExample:\n\nSimilarly, for any larger integer \"N\", a magic square of order 2\"N\" can be constructed from any \"N\" × \"N\" medjig-square with each row, column, and long diagonal summing to 3\"N\", and any \"N\" × \"N\" magic square (using the four numbers from 1 to 4\"N\" that equal the original number modulo \"N\").\n\nSimilar to the Sudoku and KenKen puzzles, solving partially completed has become a popular mathematical puzzle. Puzzle solving centers on analyzing the initial given values and possible values of the empty squares. One or more solution arises as the participant uses logic and permutation group theory to rule out all unsuitable number combinations.\n\nCertain extra restrictions can be imposed on magic squares. \n\nIf raising each number to the \"n\"th power yields another magic square, the result is a bimagic (n = 2), a trimagic (n = 3), or, in general, a multimagic square.\n\nA magic square in which the number of letters in the name of each number in the square generates another magic square is called an alphamagic square.\n\nThere are magic squares consisting entirely of primes. Rudolf Ondrejka (1928–2001) discovered the following 3×3 magic square of primes, in this case nine Chen primes:\n\nThe Green–Tao theorem implies that there are arbitrarily large magic squares consisting of primes.\n\nThe following \"reversible magic square\" has a magic constant of 264 both upside down and right way up:\nSometimes the rules for magic squares are relaxed, so that only the rows and columns but not necessarily the diagonals sum to the magic constant (this is usually called a semimagic square).\n\nIn heterosquares and antimagic squares, the 2\"n\" + 2 sums must all be \"different\".\n\nInstead of \"adding\" the numbers in each row, column and diagonal, one can apply some other operation. For example, a multiplicative magic square has a constant \"product\" of numbers. A multiplicative magic square can be derived from an additive magic square by raising 2 (or any other integer) to the power of each element, because the logarithm of the product of 2 numbers is the sum of logarithm of each. Alternatively, if any 3 numbers in a line are 2, 2 and 2, their product is 2, which is constant if \"a\"+\"b\"+\"c\" is constant, as they would be if \"a\", \"b\" and \"c\" were taken from ordinary (additive) magic square. For example, the original Lo-Shu magic square becomes:\n\nOther examples of multiplicative magic squares include:\n\nStill using Ali Skalli's non iterative method, it is possible to produce an infinity of multiplicative magic squares of complex numbers belonging to formula_17 set. On the example below, the real and imaginary parts are integer numbers, but they can also belong to the entire set of real numbers formula_18.\nThe product is: −352,507,340,640 − 400,599,719,520 \"i\".\n\nAdditive-multiplicative magic squares and semimagic squares satisfy properties of both ordinary and multiplicative magic squares and semimagic squares, respectively.\n\nIt is unknown if any additive-multiplicative magic squares smaller than 8×8 exist, but it has been proven that no 3×3 or 4×4 additive-multiplicative magic squares and no 3×3 additive-multiplicative semimagic squares exist.\n\nMagic squares may be constructed which contain geometric shapes instead of numbers. Such squares, known as geometric magic squares, were invented and named by Lee Sallows in 2001.\n\nIn the example shown the shapes appearing are two dimensional. It was Sallows discovery that \"all\" magic squares are geometric, the numbers that appear in numerical magic squares then being interpreted as a shorthand notation for indicating the lengths of straight line segments that are the geometric 'shapes' occurring in the square. That is, numerical magic squares are that special case of a geometric magic square using one dimensional shapes.\n\nIn 2017, following initial ideas of William Walkington and Inder Taneja, the first linear area magic square (L-AMS) was constructed by Walter Trump.\n\nOther shapes than squares can be considered. The general case is to consider a design with \"N\" parts to be magic if the \"N\" parts are labeled with the numbers 1 through \"N\" and a number of identical sub-designs give the same sum. Examples include magic dodecahedrons, magic triangles magic stars, and magic hexagons. Going up in dimension results in magic cubes and other magic hypercubes.\n\nEdward Shineman has developed yet another design in the shape of magic diamonds.\n\nPossible magic shapes are constrained by the number of equal-sized, equal-sum subsets of the chosen set of labels. For example, if one proposes to form a magic shape labeling the parts with {1, 2, 3, 4}, the sub-designs will have to be labeled with {1,4} and {2,3}.\n\nOver the years, many mathematicians, including Euler, Cayley and Benjamin Franklin have worked on magic squares, and discovered fascinating relations.\n\nIn 1992, Demirörs, Rafraf, and Tanik published a method for converting some magic squares into \"n\"-queens solutions, and vice versa.\n\nAs mentioned above, the set of normal squares of order three constitutes a single equivalence class-all equivalent to the Lo Shu square. Thus there is basically just one normal magic square of order 3. But the number of distinct normal magic squares rapidly increases for higher orders. There are 880 distinct magic squares of order 4 and 275,305,224 of order 5. These squares are respectively displayed on 255 magic tori of order 4, and 251,449,712 of order 5. The number of magic tori and distinct normal squares is not yet known for any higher order.\n\nAlgorithms tend to only generate magic squares of a certain type or classification, making counting all possible magic squares quite difficult. Traditional counting methods have proven unsuccessful, statistical analysis using the Monte Carlo method has been applied. The basic principle applied to magic squares is to randomly generate n × n matrices of elements 1 to n and check if the result is a magic square. The probability that a randomly generated matrix of numbers is a magic square is then used to approximate the number of magic squares.\n\nMore intricate versions of the Monte Carlo method, such as the exchange Monte Carlo, and Monte Carlo Backtracking have produced even more accurate estimations. Using these methods it has been shown that the probability of magic squares decreases rapidly as n increases. Using fitting functions give the curves seen to the right.\n\nMagic squares of order 3 through 9, assigned to the seven planets, and described as means to attract the influence of planets and their angels (or demons) during magical practices, can be found in several manuscripts all around Europe starting at least since the 15th century. Among the best known, the \"Liber de Angelis\", a magical handbook written around 1440, is included in Cambridge Univ. Lib. MS Dd.xi.45. The text of the \"Liber de Angelis\" is very close to that of \"De septem quadraturis planetarum seu quadrati magici\", another handbook of planetary image magic contained in the Codex 793 of the Biblioteka Jagiellońska (Ms BJ 793). The magical operations involve engraving the appropriate square on a plate made with the metal assigned to the corresponding planet, as well as performing a variety of rituals. For instance, the 3×3 square, that belongs to Saturn, has to be inscribed on a lead plate. It will, in particular, help women during a difficult childbirth.\n\nIn about 1510 Heinrich Cornelius Agrippa wrote \"De Occulta Philosophia\", drawing on the Hermetic and magical works of Marsilio Ficino and Pico della Mirandola. In its 1531 edition, he expounded on the magical virtues of the seven magical squares of orders 3 to 9, each associated with one of the astrological planets, much in the same way as the older texts did. This book was very influential throughout Europe until the counter-reformation, and Agrippa's magic squares, sometimes called kameas, continue to be used within modern ceremonial magic in much the same way as he first prescribed.\nThe most common use for these kameas is to provide a pattern upon which to construct the sigils of spirits, angels or demons; the letters of the entity's name are converted into numbers, and lines are traced through the pattern that these successive numbers make on the kamea.\nIn a magical context, the term \"magic square\" is also applied to a variety of word squares or number squares found in magical grimoires, including some that do not follow any obvious pattern, and even those with differing numbers of rows and columns. They are generally intended for use as talismans. For instance the following squares are: The Sator square, one of the most famous magic squares found in a number of grimoires including the \"Key of Solomon\"; a square \"to overcome envy\", from \"The Book of Power\"; and two squares from \"The Book of the Sacred Magic of Abramelin the Mage\", the first to cause the illusion of a superb palace to appear, and the second to be worn on the head of a child during an angelic invocation:\n\n\n\n\n"}
{"id": "394572", "url": "https://en.wikipedia.org/wiki?curid=394572", "title": "Max Planck Institute for the History of Science", "text": "Max Planck Institute for the History of Science\n\nThe Max Planck Institute for the History of Science in Berlin was established in March 1994. Its research is primarily devoted to a theoretically oriented history of science, principally of the natural sciences, but with methodological perspectives drawn from the cognitive sciences and from cultural history. All three departments of the Institute aim at the construction of a 'historical epistemology' of the sciences.\n\nHistorical epistemology deals with the historical development of knowledge and the technical, social, intellectual, and cultural processes surrounding the acquisition of knowledge in context. Building upon detailed studies from the history of particular sciences, historical epistemology investigates the emergence and evolution of key concepts such as 'number', 'force', 'motion', 'gene', 'organism', and 'field', as well as central categories and practices like 'representation', 'probability', 'causality', 'experiment', 'deduction', 'determinism', and 'objectivity'. The combination of highly specific historical inquiries within this more global framework of inquiry permits comparisons and generalizations spanning numerous disciplines.\n\nThe institute is affiliated with the Max Planck Society and is located in the Berlin neighborhood of Dahlem.\n\n"}
{"id": "24806944", "url": "https://en.wikipedia.org/wiki?curid=24806944", "title": "Metal-mesh optical filter", "text": "Metal-mesh optical filter\n\nMetal-mesh optical filters are optical filters made from stacks of metal meshes and dielectric. They are used as part of an optical path to filter the incoming light to allow frequencies of interest to pass while reflecting other frequencies of light.\n\nMetal-mesh filters have many applications for use in the far infrared (FIR) and submillimeter regions of the electromagnetic spectrum. These filters have been used in FIR and submillimeter astronomical instruments for over 4 decades, in which they serve two main purposes: bandpass or low-pass filters are cooled and used to lower the noise equivalent power of cryogenic bolometers (detectors) by blocking excess thermal radiation outside of the frequency band of observation, and bandpass filters can be used to define the observation band of the detectors. Metal-mesh filters can also be designed for use at 45° to split an incoming optical signal into several observation paths, or for use as a polarizing half wave plate.\n\nTransmission line theory can be applied to metallic meshes to understand how they work and the overall light transmission properties of groups of metallic meshes grouped together. Modeling the properties of these metallic meshes allows for reliable manufacture of filters with the desired transmission properties.\n\nIn 1967 Ulrich showed that the optical transmission properties of a metallic mesh can be modeled by considering the mesh to be a simple circuit element on a free space transmission line. To develop the theory of metallic meshes, he focused on the properties of two types of mesh structure: a metallic grid with square openings; and a grid of metallic squares supported on a thin dielectric substrate. Using the transmission line method, he then modeled the behavior of each of these meshes as either lumped inductance (square openings) or a lumped capacitance (free-standing squares). These two types of meshes are commonly referred to as inductive or capacitive meshes.\n\nThe theory developed by Ulrich to explain light transmission by metallic meshes makes a few assumptions and idealizations, which will be used here as well in explaining the theory. This theory is valid for thin meshes, i.e. formula_1).\n\nElectromagnetic theory of light can be used to describe how light incident on both capacitive and inductive metallic meshes will behave in transmission, reflection, and absorption.\n\nIf an incident plane wave of electromagnetic radiation hits a metallic grid of either type perpendicular to its path it will scatter, and the only propagating parts will be the zeroth order reflected wave and the zeroth order transmitted wave. The frequency of both of these electric fields will be equal, and the ratio of their amplitudes is formula_2, where formula_3 is the reflection coefficient, and formula_4 is the normalized frequency. If we assume that the incident wave had unit amplitude, we can add the incident wave to the transmitted scattered wave to get the total amplitude of the transmitted wave, formula_5:\n\nformula_6.\n\nSince we are neglecting losses, the amplitude squared of the reflected and transmitted waves must equal unity:\n\nformula_7.\n\nGiven these two relations, the phase of the reflection coefficient, formula_8, and the phase of the transmission coefficient formula_9can be simply related to the transmitted power, formula_10, which can be directly measured in experiments with metallic meshes.\n\nformula_11\n\nformula_12\n\nSolving these equations lets us find the amplitude of the scattered wave in terms of the phases of the reflected and transmitted waves:\n\nformula_13.\n\nThe result of drawing formula_2 vs. formula_15 in the complex plane is a unit half circle centered on the point formula_16 which is in the upper half-plane formula_17 for inductive grids and in the lower half-plane formula_18 for capacitive grids. At all frequencies formula_15 the transmitted and reflected waves are out of phase formula_20.\n\nUntil now, the theory has been general—whether the mesh was inductive or capacitive has not been specified. Since formula_5 and formula_2 are independent of polarization, we can apply Babinet's principle to the capacitive and inductive grids. Concisely, Babinet's principle states that if we swap the metallic parts of a grid for the gaps, (i.e., make a complementary mesh), then the sum of the transmitted wave from the original structure and the structure's complement must equal the original incident wave. Therefore, if we have complementary capacitive and inductive grids,\n\nformula_23.\n\nGiven the relations between the reflected and transmitted waves found earlier, this means that the transmitted wave in an inductive grid is equal to the negative of the reflected wave in a capacitive grid and vice versa, and also that the transmitted powers for capacitive and inductive grids sum to unity for a unit incident wave.\n\nformula_24\n\nformula_25\n\nformula_26.\n\nSolving for the exact form of formula_27 or formula_28 requires solving Maxwell's equations on the grids, which for the general case can only be solved numerically. However, in an inductive grid the metal is continuous, and hence DC currents can exist. Considering the limiting case of formula_29, the inductive grid must reflect the entire incident wave because of the boundary conditions for the electric field at the surface of a conductor. The relations derived above therefore show that a capacitive mesh will transmit the entire incident wave in this case.\n\nformula_30\n\nformula_31\n\nBecause the grids are complements of each other, these equations show that a capacitive mesh is a low pass filter and an inductive mesh is a high pass filter.\n\nUp until now, the theory has only been considering the ideal case where the grids are infinitely thin and perfectly conducting. In principle grids with finite dimensions could also absorb some of the incident radiation either through ohmic losses or losses in the dielectric supporting material.\n\nAssuming that the skin depth of the metal being used in the grids is much smaller than the thickness of the grid, the real part of the surface impedance of the metal is formula_32 where formula_33 is the conductivity of the metal and formula_34 is the skin depth of the metal. With a reflected wave formula_2, the change in the magnetic field amplitude across the grid is formula_36 because of surface currents on both sides of the grid. The average surface currents on both sides of the grid are formula_37.\n\nGiven the average surface current and the surface impedance, we could calculate the power dissipated as formula_38. However, because the actual extent of the metal in the grids is different between the capacitive and inductive grids and a flat sheet of metal, we need to introduce a factor formula_39 which is the ratio of the area the grid to that of a flat sheet. For capacitive grids, formula_40 and for inductive grids formula_41. This modifies the power dissipated to be formula_42. Using the definition of skin depth, the unitless absorptivity, formula_43 where formula_44 is the incident power, of the grid is\n\nformula_45.\n\nFor microwave and infrared radiation incident on copper, this unitless absorptivity comes out to be formula_46 to formula_47, which means that the initial assumption that absorption could be ignored in this ideal model was a good one. The dielectric losses can likewise be ignored.\n\nFor single layer metallic grids, the simple theory Ulrich laid out works quite well. The functions formula_48 and formula_49 can be determined by measuring the transmission through the filter, and the phases formula_50 and formula_51 can be measured by setting two identical grids variable distances apart and measuring the interference maximum of formula_52 as a function of separation. Measurements of very thin nearly ideal grids show the expected behavior and have very low absorptive loss.\n\nIn order to build filters out of metallic meshes with the desired properties, it is necessary to stack many metallic meshes together, and while the simple electromagnetic theory laid out above works well for one grid, it becomes more complicated when more than one element is introduced. However, these filters can be modeled as elements in a transmission line, which has easily calculable transmission properties.\n\nA transmission line model of metallic meshes is easy to work with, flexible, and is readily adapted for use in electronic modeling software. It not only handles the case of a single metallic grid, but is easily extended to many stacked grids.\n\nUnder the conditions of normal incidence and formula_53 the electric field across a metallic grid is continuous, but the magnetic field is not, so a transmission line with an admittance formula_54 between the two lines can be used to model the transmission and reflection from a metallic filter. If, for example, three identical grids were stacked, then there would be three admittance shunts in parallel across the transmission line. Using simple transmission line theory, the reflection coefficient formula_2 and transmission coefficient formula_5 are calculated to be\n\nformula_57\n\nformula_58\n\nwhich of course satisfy the original relation between the transmission and reflection coeffiecients:\n\nformula_6.\n\nIn a lossless circuit, the admittance becomes a purely imaginary susceptance, formula_60 where formula_61 is a real function of formula_15. Because of the complementary nature of the grids, we also know that formula_63.\n\nTo calculate the behavior of an ideal metallic grid, only formula_64 needs to be found. The standard approach is not to characterize the equivalent circuit by formula_64, but instead to parameterize it with values of formula_66, formula_67, and formula_68 which duplicate the transmission properties of the filters. At low frequencies, a reasonable model is to replace the shunt in the transmission line with a capacitor of value formula_69 for capacitive meshes and an inductor of value formula_70 for inductive meshes, where for complementary grids formula_71. However, at high frequencies this model fails to reflect the behavior of real metallic meshes correctly. The measured transmissions as formula_72 are\n\nformula_73\n\nformula_74.\n\nThe behavior of the transmission in the two limiting cases can be replicated with the transmission line model by adding an extra element. In addition, losses can be taken into account by adding another resistance formula_68. At resonance formula_76, the impedance of capacitors and inductors are formula_77. Typically, formula_78 and formula_79 have to be measured based on transmission properties of the grids, and both depend on the parameter formula_80. The formula_68 included in the 2-element equivalent circuit is consistent with the earlier calculation of absorptivity, which gives formula_82. The following table summarizes all the parameters to go from equivalent circuit parameters to expected reflection and transmission coeffecients.\n\nThe real power in this model is it allows prediction of the transmission properties of many metallic grids stacked together with spacers to form interference filters. Stacks of capacitive grids make a lowpass filter with a sharp frequency cutoff above which transmission is almost zero. Likewise, stacks of inductive grids make a highpass filter with a sharp frequency cutoff below which transmission is almost zero. Stacked inductive and capacitive meshes can be used to make bandpass filters.\n\nThe transmission line model gives the expected first-order transmission of the stacked metal mesh filters; however, it cannot be used to model transmission of light that is incident at an angle, loss in the supporting dielectric materials, or the transmission properties when formula_83 due to diffraction. To model those effects, scientists have used a cascade scattering matrix approach to model dielectric loss, and other modeling tools such as High Frequency Structure Simulator and Floquet mode analysis.\n\nThe manufacture of metal-mesh filters starts with photolithography of copper on a substrate, which allows fine control over the parameters formula_84, formula_85, and formula_86. The metallic grids are made of thin copper film on top of a dielectric substrate such as mylar or polypropylene. The copper is formula_87 thick, and the dielectric ranges from formula_88 to formula_89.\n\nThere are two ways to create a multi-layer metal-mesh filter. The first is to suspend the separate layers in supporting rings with a small gap which is either filled with air or under vacuum between the layers. However, these filters are mechanically delicate. The other way to build a multi-layer filer is to stack sheets of dielectric between the layers of metallic mesh and hot press the whole stack together. This results in a filter that is one solid piece. Hot pressed filters are mechanically robust and when impedance matched to vacuum show a pass-band fringe due to Fabry-Perot interference in the underlying dielectric material.\n\nThese filters have been used in FIR and submillimeter astronomical instruments for over 4 decades, in which they serve two main purposes: bandpass or low-pass filters are cooled and used to lower the noise equivalent power of cryogenic bolometers by blocking excess thermal radiation outside of the frequency band of observation, and bandpass filters can be used to define the observation band of the detectors. Metal-mesh filters can also be designed for use at 45° to split an incoming optical signal into several observation paths, or for use as a polarizing half wave plate.\n"}
{"id": "12162942", "url": "https://en.wikipedia.org/wiki?curid=12162942", "title": "Military Soyuz", "text": "Military Soyuz\n\nThe Soviet Union planned several military Soyuz spacecraft models. These versions were named \"Soyuz P\", \"Soyuz PPK\", \"Soyuz R\", \"Soyuz 7K-VI\", and \"Soyuz OIS (Orbital Research Station)\". However, none ever flew in space.\n\nThe Soyuz P (\"Perekhvatchik\", Interceptor) space interceptor and Soyuz R (\"Razvedki\", intelligence) command-reconnaissance spacecraft was proposed in December 1962 by Sergei Korolev.\n\nIn the initial draft project, the Soyuz P would use the Soyuz 9K rocket stage and Soyuz 11K tanker spacecraft to conduct a series of dockings and re-fueling operations. The complete complex would then could conduct intercepts of enemy satellites in orbits up to 6,000 km in altitude.\n\nThe Soyuz-R system consisted of two separately launched spacecraft, including the small orbital station 11F71 with photo-reconnaissance and electronic intelligence equipment and a Soyuz 7K-TK for crew transport.\n\nInitially the Soyuz P was designed for piloted inspection and destruction of enemy satellites. It was intended that the Soyuz would rendezvous with the target satellite. To minimize risk to the crew, a new version, Soyuz PPK (\"pilotiruemovo korablya-perekhvatchika\", manned interceptor spacecraft) was later proposed.\n\nThe Zvezda (star) station was based on a radically modified Soyuz. Objectives were manned earth observation, orbital inspection and destruction of enemy satellites. Zvezda would be powered by two plutonium radioisotope generators and had a recoilless gun for defense. It was designed for shooting in a vacuum and defending the military research spacecraft from enemy satellite inspector and interceptor satellites. The gun was aimed by maneuvering the entire spacecraft. A special gunsight was installed in the descent module for aiming the gun. A forward docking apparatus to allow docking with Almaz was also included. Work on Zvezda was canceled in 1967 with a single prototype in advanced stages of construction.\n\nCosmonaut training for the VI began in September 1966. The cosmonaut group selected included commander Pavel Popovich, pilot Alexei Gubarev, flight-engineers Yuri Artyukhin, Vladimir Gulyaev, Boris Belousov, and Gennadiy Kolesnikov. Popovich-Kolesnikov and Gubarev-Belousov were the prime crews, with the other engineers acting as reserves and then assigned to later crews.\n\nThe Soyuz OIS (Orbital Research Station) would consist of a separately-launched orbital block 11F731 OB-VI and a transport Soyuz 7K-S.\n\nThe Soyuz OB-VI would be launched for 30-day missions in a 51.6 degree orbit at 250 x 270 km. Power was provided by solar panels, and the payload included 700 to 1,000 kg of instrumentation. The total mass would be around 6,500 kg (14,300 lb).\n\nThe initial Soyuz 7K-S program was to consist of four unmanned, followed by two manned test flights, then two operational launches. Cosmonauts were assigned to the project in 1973.\n\nIn 1975 the project was canceled. \nAt that time the launch escape system for 7K-S was ready and was used for Apollo-Soyuz Test Project flights. Three complete vehicles were launched as unmanned test missions:\n\nThe Soyuz 7K-ST transport project was develop in parallel to the military 7K-S and was redesigned for a crew of three, eventually becoming the Soyuz-T used with the Salyut space stations.\n\n\n"}
{"id": "2931866", "url": "https://en.wikipedia.org/wiki?curid=2931866", "title": "Nordtvedt effect", "text": "Nordtvedt effect\n\nIn theoretical astrophysics, the Nordtvedt effect refers to the relative motion between the Earth and the Moon which would be observed if the gravitational self-energy of a body contributed differently to its gravitational mass than to its inertial mass. If observed, the Nordtvedt effect would violate the strong equivalence principle, which indicates that an object's movement in a gravitational field does not depend on its mass or composition.\n\nThe effect is named after Dr. Kenneth L. Nordtvedt, who first demonstrated that some theories of gravity suggest that massive bodies should fall at different rates, depending upon their gravitational self-energy.\n\nNordtvedt then observed that if gravity did in fact violate the strong equivalence principle, then the more-massive Earth should fall towards the Sun at a slightly different rate than the Moon, resulting in a polarization of the lunar orbit. To test for the existence (or absence) of the Nordtvedt effect, scientists have used the Lunar Laser Ranging Experiment, which is capable of measuring the distance between the Earth and the Moon with near-millimetre accuracy. Thus far, the results have failed to find any evidence of the Nordtvedt effect, demonstrating that if it exists, the effect is exceedingly weak. Subsequent measurements and analysis to even higher precision have improved constraints on the effect.. Measurements of Mercury's orbit by the MESSENGER Spacecraft have further refined the Nordvedt effect to be below of even smaller scale.\n\nA wide range of scalar-tensor theories have been found to naturally lead to a tiny effect only, at present epoch. This is due to a generic attractive mechanism that takes place during the cosmic evolution of the universe.\nOther screening mechanisms (chameleon, pressuron, Vainshtein etc.) could also be at play.\n\n\n"}
{"id": "1521108", "url": "https://en.wikipedia.org/wiki?curid=1521108", "title": "Paul Cameron", "text": "Paul Cameron\n\nPaul Drummond Cameron (born November 9, 1939) is an American psychologist and sex researcher. While employed at various institutions, including the University of Nebraska, he conducted research on passive smoking, but he is best known today for his claims about homosexuality. After a successful 1982 campaign against a gay rights proposal in Lincoln, Nebraska, he established the Institute for the Scientific Investigation of Sexuality (ISIS), now known as the Family Research Institute (FRI). As FRI's chairman, Cameron has written papers stating associations between homosexuality and the perpetration of child sexual abuse and reduced life expectancy.\n\nIn 1983, the American Psychological Association expelled Cameron for non-cooperation with an ethics investigation. Position statements issued by the American Sociological Association, Canadian Psychological Association, and the Nebraska Psychological Association have each accused Cameron of misrepresenting social science research.\n\nCameron was born in Pittsburgh, Pennsylvania, U.S., on November 9, 1939. His family moved shortly afterwards to Florida. He received a BA from Los Angeles Pacific College in 1961, an MA from California State University in Los Angeles the following year and a PhD from the University of Colorado at Boulder in 1966. He held posts as an assistant psychology professor at University of Wisconsin–Stout (1966–67) and Wayne State University (1967–68), before becoming an associate professor at the University of Louisville (1970–73) and the Fuller Graduate School of Psychology (part of the Fuller Theological Seminary) (1976–79). In 1979, he became an associate professor of Marriage and Family at the University of Nebraska.\n\nDuring this period, Cameron conducted research on a variety of topics, including the effects of passive smoking and the relation between pet ownership and happiness. In his 1978 book \"Sexual Gradualism\", he supported a middle ground between liberal and conservative Christian attitudes to sexuality, arguing that teenagers should avoid intercourse while experimenting with lower \"levels\" of sexual intimacy.\n\nIn 1980, Cameron left the University of Nebraska and took up private practice as a psychologist in Lincoln, Nebraska. In 1982, when the Lincoln city council asked residents to vote on a proposal to ban discrimination based on sexual orientation, Cameron led the opposition as chairman of the Committee to Oppose Special Rights for Homosexuals. Despite his earlier moderate position on teenage relationships, Cameron had come to take a hard-line stance on the topic of homosexuality. He has stated that his approach, emphasizing the harms he believed to be caused by homosexual behavior and its acceptance, was influenced by his work on the \"lethal\" behavior of smokers.\n\nDuring the campaign in Lincoln, Cameron delivered a speech at the University of Nebraska Lutheran chapel. This drew much attention after he stated that a four-year-old boy had suffered a brutal homosexual assault in a local mall. Police were unable to confirm the incident, and Cameron acknowledged that he had heard the story only as a rumor. On May 11, Lincoln voters rejected the proposed measure by a 4–1 margin.\n\nCameron says he was molested by a male at 4 years of age to have had a \"much more positive experience\" with a female a year later.\n\nHe says he started feeling sexual attraction to men at three years of age, but that he became heterosexual at 8–9 years of age.\n\nIn 1982, Cameron co-founded the Institute for the Scientific Investigation of Sexuality in Lincoln. Believing that earlier sex surveys, including those conducted by \"Playboy\" magazine, had overestimated the prevalence of homosexuality, Cameron set out in 1983 to conduct what he described as \"a fair sexuality poll, not one based on volunteers.\" His expectation was that the results would support his case for a ban on homosexual acts throughout the United States. Funding, according to Cameron, was provided by businessmen including several Nebraska chief executives. The 1983 ISIS Survey, an \"extensive self-administered questionnaire\", was offered to 9,129 adults in five U.S. cities, and 4,340 responses were received. In 1984, these were supplemented with data from 824 adults from Dallas.\n\nISIS was shortly afterwards renamed the Family Research Institute (FRI) and moved to Washington, D.C.. In 1995, FRI changed location again, this time to Colorado Springs, Colorado, where it remains based. In his capacity as FRI's chairman, Cameron has authored both popular pamphlets and scientific articles on the topic of homosexuality. The cover photograph for Cameron's pamphlet \"Child Molestation and Homosexuality\" depicted \"a young boy being pulled into a men's bathroom,\" while \"Murder, Violence and Homosexuality\" showed \"a little girl cowering beneath an arm wielding an ax.\" Many of Cameron's scientific articles have been based on the 1983–1984 ISIS survey, including a 1996 paper that concluded based on participants' answers concerning their teachers that homosexual teachers could influence their students to become homosexual.\n\nAnother of Cameron's conclusions, based partly on his studies of obituaries in gay newspapers, is that homosexuals as a group have a median age of death about 20 years lower than that of heterosexuals. After analyzing official data from Denmark, which allowed same-sex unions in 1989, and Norway, which allowed same-sex marriage in 2009, Cameron reported in 2007 that \"married lesbians lived to age 56 and married gay men to age 52.\" Cameron states that many victims of child sexual abuse are the same sex as their abusers – one FRI study on sexual abuse by foster parents in Illinois reported that 34% of perpetrators were guilty of same-sex abuse – and concludes that \"there is a strong, disproportionate association between child molestation and homosexuality.\"\n\nCameron was quoted in \"Rolling Stone\" as saying that homosexual sex was more pleasurable than most heterosexual sex, and as a result, if homosexuality were tolerated then it would become predominant within a few generations.\n\nCameron's publications have been cited as support by some groups who oppose same-sex marriage and allowing homosexuals to become foster or adoptive parents, including the Traditional Values Coalition. Cameron testified in the case \"Baker v. Wade\" (1985). In 1992, Gale Norton, then the Attorney General of Colorado, employed Cameron as a consultant when defending a law preventing the extension of civil rights legislation to homosexuals. Cameron's testimony went unused, and the law was struck down by the Supreme Court. Cameron campaigned against a gay-rights initiative in Maine in 2000, testified in favor of the failed Virginia Anti-Gay Adoption Bill in 2005, and opposed a 2007 Colorado bill intended to allow cohabiting couples to adopt. He was tricked into appearing in Sacha Baron Cohen's 2009 mockumentary film \"Brüno\".\nSouthern Poverty Law Center has classed the Family Research Institute as a hate group.\n\nCameron is married and has three children. His son, Kirk, has been involved with the Family Research Institute since 1983.\n\nThe American Psychological Association (APA) launched an investigation into Cameron after receiving complaints about his work from members. The APA President Max Seigel sent Cameron a letter on December 2, 1983 stating that the Board of Directors had decided to drop him from membership for failure to cooperate with their investigation. FRI has contended that Cameron had already resigned from the organization in November 1982, citing correspondence from before his formal expulsion. In a letter published in the March 1983 edition of the \"APA Monitor\", Cameron stated that his reasons for leaving included his opinion that the organization was becoming more of a \"liberal PAC\" than a professional society. The APA, however, does not allow the resignation of a member who is the subject of an ethics investigation. An APA spokesperson told \"The Boston Globe\" in 2005, \"We are concerned about Dr. Cameron because we do believe that his methodology is weak.\"\n\nIn 1984 the Nebraska Psychological Association issued a statement disassociating itself \"from the representations and interpretations of scientific literature offered by Dr. Paul Cameron.\" In 1986 the American Sociological Association passed a resolution stating, “The American Sociological Association officially and publicly states that Paul Cameron is not a sociologist, and condemns his consistent misrepresentation of sociological research.” This was based on a report from the ASA's Committee on the Status of Homosexuals in Sociology, which summarised Cameron's inflammatory statements and commented, \"It does not take great analytical abilities to suspect from even a cursory review of Cameron's writings that his claims have almost nothing to do with social science and that social science is used only to cover over another agenda. Very little of his work could find support from even a bad misreading of genuine social science investigation on the subject and some sociologists, such as Alan Bell, have been 'appalled' at the abuse of their work.\" In 1996, the Board of Directors of the Canadian Psychological Association approved a position statement disassociating the organisation from Cameron's work on sexuality, stating that he had \"consistently misinterpreted and misrepresented research on sexuality, homosexuality, and lesbianism.\"\n\nIn March 2015, Cameron sued the Polish gay rights organisation 'Pracownia Różnorodności' for calling him a 'homophobic liar'.\n\nAfter Cameron submitted affidavits to the U. S. District Court of Dallas in \"Baker v. Wade\" (1985), Judge Buchmeyer wrote in his opinion that Cameron had engaged in \"fraud\" and \"misrepresentations to this Court\" noting that, \"His sworn statement that 'homosexuals are approximately 43 times more apt to commit crimes than is the general population' is a total distortion of the Kinsey data upon which he relies – which, as is obvious to anyone who reads the report, concerns data from a non-representative sample of delinquent homosexuals (and Dr. Cameron compares this group to college and non-college heterosexuals).\" Buchmeyer's decision was reversed by the Fifth Circuit on other grounds, without mentioning Buchmeyer's finding that Cameron had made misrepresentations. FRI has disputed Judge Buchmeyer's assessment of Cameron's affidavits.\n\nEpidemiologists Morten Frisch and Henrik Brønnum-Hansen argue that Cameron was wrong to infer reduced life expectancy from the fact that deaths among homosexually married partners in Denmark and Norway occurred at a lower median age than those among heterosexually married partners: \"Because the age distribution among persons in same-sex marriages was considerably younger than that of people who had ever been heterosexually married, the average age at death among those who actually died during the observation period was, not surprisingly, considerably younger in the population of same-sex married persons.\" Their own analysis found that excess mortality in Danish same-sex marriages since 1995 was \"restricted to the first few years after a marriage, presumably reflecting preexisting illness at the time of marriage\". Similarly, critics have argued that obituaries in gay-themed newspapers, which Cameron used to estimate homosexual mortality, do not provide a representative sample of deaths and ignore surviving members of the same generation.\n\nCameron has also been criticized for placing responsibility for same-sex child sexual abuse on \"homosexuals\"; opponents state that someone who carries out such abuse need not have a homosexual orientation with respect to other adults. Gregory M. Herek, a psychologist specializing in prejudice against sexual minorities, charges that Cameron misrepresented the literature he had reviewed and cited to support his claims, such as a Groth and Birnbaum (1978) study in which none of the participating child molestors actually identified as homosexuals, and none of those who were bisexual claimed to prefer men over women. Furthermore, while Cameron assumed all the same-sex molestations were perpetrated by homosexuals, he did not assume all the opposite-sex molestations were perpetrated by heterosexuals; he included a \"bisexual correction\" only for opposite-sex molestations that effectively increased the number of perpetrators described as \"homosexual\" without changing the number described as \"heterosexual\".\n\nHerek noted that most of the Cameron group's academic publications in the past 15 years have been based on a survey study conducted in 1983 and 1984. The main survey was completed in seven U.S. cities and towns in 1983. Data were later added from a 1984 Dallas (TX) sample. Most of the Cameron group's papers have reported data from the combined samples. According to Herek, a critical review of the Cameron group's sampling techniques, survey methodology, and interpretation of results reveals at least six serious errors in their study. Herek concludes, \"an empirical study manifesting even one of these six weaknesses would be considered seriously flawed. In combination, the multiple methodological problems evident in the Cameron group's surveys mean that their results cannot even be considered a valid description of the specific group of individuals who returned the survey questionnaire. Because the data are essentially meaningless, it is not surprising that they have been virtually ignored by the scientific community.\" \"The Cameron group has published its empirical research in academic journals with low prestige and, at least in the case of Psychological Reports, with a low rejection rate. Other than the Cameron group itself, researchers have not cited their empirical studies as a source of ideas for new research on sexual orientation. Nor have scientists cited the group's papers to support assertions about the dangers to society posed by homosexuals.\"\n\nIn a widely publicized interview with Midweek Politics with David Pakman, Cameron compared homosexuality to drug use, a comparison that drew criticism from several gay rights blogs, websites, and \"The Huffington Post\".\n\n\n"}
{"id": "55807611", "url": "https://en.wikipedia.org/wiki?curid=55807611", "title": "Political Economy of Research and Innovation", "text": "Political Economy of Research and Innovation\n\nThe Political Economy of Research and Innovation (PERI) (or sometimes political economy of technoscience) is an emerging academic field at the interface of science and technology studies and political economy. It focuses on the production, distribution, and consumption of knowledge, and how these shape and are shaped by different political economies. Most scholars in this field have so-far focused on the two-way relationship between science, technology, and innovation and political economic processes, practices, and logics. \n\nIt has its origins in the critique of neoclassical or orthodox economics of science by scholars like Philip Mirowski, the 'economic turn' in science and technology studies (see social studies of finance and valuation studies), and innovation studies or science policy.\n\nExamples of the field include:\n"}
{"id": "9487795", "url": "https://en.wikipedia.org/wiki?curid=9487795", "title": "Relativistic electron beam", "text": "Relativistic electron beam\n\nRelativistic electron beams are streams of electrons moving at relativistic speeds. They are the lasing medium in free electron lasers to be used in atmospheric research conducted at entities such as the Pan-oceanic Environmental and Atmospheric Research Laboratory (PEARL) at the University of Hawaii and NASA.\n\nIt has been suggested that relativistic electron beams could be used to heat and accelerate the reaction mass in electrical rocket engines that Dr. Robert W. Bussard called quiet electric-discharge engines (QEDs).\n\n"}
{"id": "702761", "url": "https://en.wikipedia.org/wiki?curid=702761", "title": "Reverse psychology", "text": "Reverse psychology\n\nReverse psychology is a technique involving the assertion of a belief or behavior that is opposite to the one desired, with the expectation that this approach will encourage the subject of the persuasion to do what actually is desired. This technique relies on the psychological phenomenon of reactance, in which a person has a negative emotional reaction to being persuaded, and thus chooses the option which is being advocated against. This may work especially well on a person who is resistant by nature, while direct requests works best for people who are compliant. The one being manipulated is usually unaware of what is really going on.\n\nSusan Fowler, an author writes \"Beware that such strategies [of reverse psychology] can backfire. Children can sense manipulation a mile away.\" She instead recommends leading by example.\n\nThe psychology professor John Gottman advises against using reverse psychology on teens, with the presumption that they will rebel stating that \"don't try to use reverse psychology...such strategies are confusing, manipulative, dishonest, and they rarely work\".\n\nReverse psychology is often used on children due to their high tendency to respond with reactance, a desire to restore threatened freedom of action. Some parents feel that the best strategy is sometimes \"reverse psychology\": telling children to stay in the house when you really want them to choose to go outside and play. Questions have however been raised about such an approach when it is more than merely instrumental, in the sense that \"reverse psychology implies a clever manipulation of the misbehaving child\" and nothing more.\n\nClosely associated with reverse psychology in psychotherapy is the technique of \"the \"Paradoxical intervention\"...This technique has also been called 'prescribing the symptom' and 'antisuggestion. The therapist frames their message so that resistance to it promotes change.\n\nSuch interventions \"can have a similar impact as humor in helping clients cast their problems in a new light...By going \"with\", not against, the client's resistance, the therapist makes the behavior less attractive\". This is referred to as reframing. This means pretending to agree with clients thoughts and beliefs; to reaffirm them out loud to make the clients themselves realize their fallibility.\n\nWe value people and things unavailable to us - things we can't have. Common complaint men have about women is that they are attracted to men with a disagreeable personality. However, this is no less true about men's attraction towards women. Showing signs of confidence and independence is attractive for both sexes. Dating coach Neil Strauss says \"The dating dichotomy isn't actually between nice guys and mean guys. Or good guys and bad boys. It's between weak guys and strong guys.\nWomen are drawn to men who demonstrate strength—not necessarily physical strength, but the ability to make them feel safe.\" \n\nThe reason we want to be with emotionally abusive and unavailable people is because of unresolved issues from the unconcious mind. Being emotionally unavailable to your partner will damage the health of a long-term romantic relationship.\n\n\"In a world where it is expected that all things should be available ... less availability\" has emerged as a new selling point: \"by engaging in such a restricted anti-marketing ploy the brand has won kudos\". The result can be \"what the Japanese call a secret brand ... no regular retail outlets, no catalog, no web presence apart from a few cryptic mentions ... people like it because it's almost impossible to find\". Such an example of a brand catering to the customers exquisite taste is Cayce Pollard's \"The Gabriel Hounds\".\n\nTheodor Adorno and Max Horkheimer characterized the effect of the culture industry as \"psychoanalysis in reverse\". Their analysis began with the dialectic which operated in Germany when heirs of the Romantic movement became seekers of \"Strength through Joy\", only to have their movement co-opted by a combination of the mass media and National Socialism. A modern example begins with the \"fitness and jogging\" boom in the United States in the 1970s. The \"running craze\" at the Boston Marathon and in California, dialectically, was the thesis that one did not have to be \"Rocky\" in a sweaty gym to be physically fit, and that body acceptance was the key to effective aerobic training. The culture industry responded to the thesis with major advertising campaigns from Calvin Klein and others, using images featuring exceptionally toned models. People compared themselves to these models, which created a sense of competition, and many high school students avoid jogging because of the resultant body shame. \nThe culture industry mass-produces standardized material. This would not be dangerous if the material was meaningless, but it frequently offers and reinforces ideals and norms representing implied criticism of those who fail to match up. Empirical studies show that mass culture products can lower confidence and self-esteem, and cause humiliation among men and women whose particular characteristics fall outside the normalised range for appearance, behaviour, religion, ethnicity etc. Similarly, advertising frequently seeks to create a need to buy by showing differences between \"actual\" and \"ideal\" situations. The intention is usually to induce dissatisfaction with the present situation, and to induce expectations of satisfaction through the acquisition of products which will transform the actual reality into the idealized reality. Hence, if the peer group buys, all those who cannot afford the products will feel additional unhappiness and frustration until they eventually join the group. Thus, sometimes the process of advocacy for the one outcome really intends to produce the opposite outcome as the motivation for purchase. \nHowever, more often than not, the cause and effect is unintended. Marxist logic applied to the culture industry indicates that it is, \"per se\", a dialectic in which declining profit margins and increasing costs make investors anxious for \"sure things\". Repeating winning formulas and stereotyping create the lowest common denominator products with the lowest costs. But the less creative the input, the more likely it becomes that roles will be cast in ways which match, rather than challenge, common prejudices which can inadvertently (or quite deliberately) damage the esteem of those in the marginalized groups.\n\nClassic examples of reverse psychology in popular culture include a large, bright red button with a sign next to it saying \"Do not push\", or a sign saying \"Jump at your own risk\".\n\nThere are numerous examples of reverse psychology in fiction, cinema, and cartoons, including William Shakespeare's \"Julius Caesar\" where Mark Antony uses reverse psychology to get the town's people to cause a riot. Mark Anthony pretends to side with Brutus by complementing on his deeds which have led to Ceasar's murder, while actually inciting the crowd's anger.\n\nIn one of Joel Chandler Harris's Uncle Remus stories, Br'er Rabbit escaped from Br'er Fox by repeatedly pleading \"Please, Br'er Fox, don't fling me in that briar patch.\" \"The fox did so, which allowed the rabbit to escape: The Rabbit used 'reverse psychology' to outsmart the Fox.\"\n\nIn Edgar Allan Poe's \"The Cask of Amontillado\", Montresor uses reverse psychology to persuade Fortunato to enter his vaults. He says that Fortunato is too tired and should get some rest and that he should find someone else to help him with his wine tasting problem. Montresor knew that Fortunato would disagree and insisted on entering the vault, leading him into his death by immurement.\n\nThe Swedish fictional character Alfie Atkins uses reverse psychology in the children's book \"You're a Sly One, Alfie Atkins!\" from 1977. He exaggerates his own childishness in order to convince his older cousins to sit at the grown-up table.\n\n\n"}
{"id": "28712336", "url": "https://en.wikipedia.org/wiki?curid=28712336", "title": "Richard Hammond (physicist)", "text": "Richard Hammond (physicist)\n\nRichard Hammond is an Adjunct Professor at the University of North Carolina at Chapel Hill and the author of the book \"The Unknown Universe: The Origin of the Universe, Quantum Gravity, Wormholes, and Other Things Science Still Can't Explain\". He also works for the United States Army Research Laboratory as a theoretical physicist. He has authored several academic papers on general relativity and quantum mechanics.\n"}
{"id": "743909", "url": "https://en.wikipedia.org/wiki?curid=743909", "title": "Rotation period", "text": "Rotation period\n\nIn astronomy, the rotation period of a celestial object is the time that it takes to complete one revolution around its axis of rotation relative to the background stars. It differs from the planet's solar day, which includes an extra fractional rotation needed to accommodate the portion of the planet's orbital period during one day.\n\nFor solid objects, such as rocky planets and asteroids, the rotation period is a single value. For gaseous/fluid bodies, such as stars and gas giants, the period of rotation varies from the equator to the poles due to a phenomenon called differential rotation. Typically, the stated rotation period for a gas giant (Jupiter, Saturn, Uranus, Neptune) is its internal rotation period, as determined from the rotation of the planet's magnetic field. For objects that are not spherically symmetrical, the rotation period is, in general, not fixed, even in the absence of gravitational or tidal forces. This is because, although the rotation axis is fixed in space (by the conservation of angular momentum), it is not necessarily fixed in the body of the object itself. As a result of this, the moment of inertia of the object around the rotation axis can vary, and hence the rate of rotation can vary (because the product of the moment of inertia and the rate of rotation is equal to the angular momentum, which is fixed). Hyperion, a satellite of Saturn, exhibits this behaviour, and its rotation period is described as chaotic.\n\nEarth's rotation period relative to the Sun (its mean solar day) consists of 86,400 seconds of mean solar time, by definition. Each of these seconds is slightly longer than an SI second because Earth's solar day is now slightly longer than it was during the 19th century, due to tidal deceleration. The mean solar second between 1750 and 1892 was chosen in 1895 by Simon Newcomb as the independent unit of time in his Tables of the Sun. These tables were used to calculate the world's ephemerides between 1900 and 1983, so this second became known as the ephemeris second. The SI second was made equal to the ephemeris second in 1967.\n\nEarth's rotation period relative to the fixed stars, called its \"stellar day\" by the International Earth Rotation and Reference Systems Service (IERS), is seconds of mean solar time (UT1) Earth's rotation period relative to the precessing or moving mean vernal equinox, its \"sidereal day\", is seconds of mean solar time (UT1) Thus the sidereal day is shorter than the stellar day by about 8.4 ms. The length of the mean solar day in SI seconds is available from the IERS for the periods 1623–2005 and 1962–2005.\nRecently (1999–2005) the average annual length of the mean solar day in excess of 86400 SI seconds has varied between 0.3 ms and 1 ms, which must be added to both the stellar and sidereal days given in mean solar time above to obtain their lengths in SI seconds.\n\n"}
{"id": "1640288", "url": "https://en.wikipedia.org/wiki?curid=1640288", "title": "Scientific control", "text": "Scientific control\n\nA scientific control is an experiment or observation designed to minimize the effects of variables other than the independent variable. This increases the reliability of the results, often through a comparison between control measurements and the other measurements. Scientific controls are a part of the scientific method.\n\nIn controlled experiments, the same experiment is done in at least two parallel experiments that differ in only one way, with one experiment being the \"control arm\" and the other being the \"experimental arm\". \n\nControls eliminate alternate explanations of experimental results, especially experimental errors and experimenter bias. Many controls are specific to the type of experiment being performed, as in the molecular markers used in SDS-PAGE experiments, and may simply have the purpose of ensuring that the equipment is working properly. The selection and use of proper controls to ensure that experimental results are valid (for example, absence of confounding variables) can be very difficult. Control measurements may also be used for other purposes: for example, a measurement of a microphone's background noise in the absence of a signal allows the noise to be subtracted from later measurements of the signal, thus producing a processed signal of higher quality.\n\nFor example, if a researcher feeds an experimental artificial sweetener to sixty laboratory rats and observes that ten of them subsequently become sick, the underlying cause could be the sweetener itself or something unrelated. Other variables, which may not be readily obvious, may interfere with the experimental design. For instance, the artificial sweetener might be mixed with a dilutant and it might be the dilutant which causes the effect. To control for the effect of the dilutant, the same test is run twice; once with the artificial sweetener in the dilutant, and another done exactly the same way, but using the dilutant alone. Now the experiment is controlled for the dilutant and the experimenter can distinguish between sweetener, dilutant and non-treatment. Controls are most often necessary where a confounding factor cannot easily be separated from the primary treatments. For example, it may be necessary to use a tractor to spread fertilizer where there is no other practicable way to spread fertilizer. The simplest solution is to have a treatment where a tractor is driven over plots without spreading fertilizer and in that way the effects of tractor traffic are controlled.\n\nThe simplest types of control are negative and positive controls, and both are found in many different types of experiments. These two controls, when both are successful, are usually sufficient to eliminate most potential confounding variables: it means that the experiment produces a negative result when a negative result is expected, and a positive result when a positive result is expected.\n\nWhere there are only two possible outcomes, e.g. positive or negative, if the treatment group and the negative control both produce a negative result, it can be inferred that the treatment had no effect. If the treatment group and the negative control both produce a positive result, it can be inferred that a confounding variable is involved in the phenomenon under study, and the positive results are not solely due to the treatment.\n\nIn other examples, outcomes might be measured as lengths, times, percentages, and so forth. In the drug testing example, we could measure the percentage of patients cured. In this case, the treatment is inferred to have no effect when the treatment group and the negative control produce the same results. Some improvement is expected in the placebo group due to the placebo effect, and this result sets the baseline which the treatment must improve upon. Even if the treatment group shows improvement, it needs to be compared to the placebo group. If the groups show the same effect, then the treatment was not responsible for the improvement (because the same number of patients were cured in the absence of the treatment). The treatment is only effective if the treatment group shows more improvement than the placebo group.\n\nPositive controls are often used to assess test validity. For example, to assess a new test's ability to detect a disease (its sensitivity), then we can compare it against a different test that is already known to work. The well-established test is the positive control, since we already know that the answer to the question (whether the test works) is yes.\n\nSimilarly, in an enzyme assay to measure the amount of an enzyme in a set of extracts, a positive control would be an assay containing a known quantity of the purified enzyme (while a negative control would contain no enzyme). The positive control should give a large amount of enzyme activity, while the negative control should give very low to no activity.\n\nIf the positive control does not produce the expected result, there may be something wrong with the experimental procedure, and the experiment is repeated. For difficult or complicated experiments, the result from the positive control can also help in comparison to previous experimental results. For example, if the well-established disease test was determined to have the same effectiveness as found by previous experimenters, this indicates that the experiment is being performed in the same way that the previous experimenters did.\n\nWhen possible, multiple positive controls may be used — if there is more than one disease test that is known to be effective, more than one might be tested. Multiple positive controls also allow finer comparisons of the results (calibration, or standardization) if the expected results from the positive controls have different sizes. For example, in the enzyme assay discussed above, a standard curve may be produced by making many different samples with different quantities of the enzyme.\n\nIn randomization, the groups that receive different experimental treatments are determined randomly. While this does not ensure that there are no differences between the groups, it ensures that the differences are distributed equally, thus correcting for systematic errors.\n\nFor example, in experiments where crop yield is affected (e.g. soil fertility), the experiment can be controlled by assigning the treatments to randomly selected plots of land. This mitigates the effect of variations in soil composition on the yield.\n\nIn blind experiments, at least some information is withheld from participants in the experiments (but not the experimenter). For example, to evaluate the success of a medical treatment, an outside expert might be asked to examine blood samples from each of the patients without knowing which patients received the treatment and which did not. If the expert's conclusions as to which samples represent the best outcome correlates with the patients who received the treatment, this allows the experimenter to have much higher confidence that the treatment is effective.\n\nThe blinding eliminates effects such as confirmation bias and wishful thinking that might occur if the samples were evaluated by someone who knew which samples were in which group.\n\nIn double-blind experiments, at least some participants and some experimenters do not possess full information while the experiment is being carried out. Double-blind experiments are most often used in clinical trials of medical treatments, to verify that the supposed effects of the treatment are produced only by the treatment itself. Trials are typically randomized and double-blinded, with two (statistically) identical groups of patients being compared. The treatment group receives the treatment, and the control group receives a placebo. The placebo is the \"first\" blind, and controls for the patient expectations that come with taking a pill, which can have an effect on patient outcomes. The \"second\" blind, of the experimenters, controls for the effects on patient expectations due to unintentional differences in the experimenters' behavior. Since the experimenters do not know which patients are in which group, they cannot unconsciously influence the patients. After the experiment is over, they then \"unblind\" themselves and analyse the results.\n\nIn clinical trials involving a surgical procedure, a sham operated group is used to ensure that the data reflect the effects of the experiment itself, and are not a consequence of the surgery. In this case, double blinding is achieved by ensuring that the patient does not know whether their surgery was real or sham, and that the experimenters who evaluate patient outcomes are different from the surgeons and do not know which patients are in which group.\n\n"}
{"id": "4079010", "url": "https://en.wikipedia.org/wiki?curid=4079010", "title": "Scoring rule", "text": "Scoring rule\n\nIn decision theory, a score function, or scoring rule, measures the accuracy of probabilistic predictions. It is applicable to tasks in which predictions must assign probabilities to a set of mutually exclusive outcomes. The set of possible outcomes can be either binary or categorical in nature, and the probabilities assigned to this set of outcomes must sum to one (where each individual probability is in the range of 0 to 1). A score can be thought of as either a measure of the \"calibration\" of a set of probabilistic predictions, or as a \"cost function\" or \"loss function\".\n\nIf a cost is levied in proportion to a proper scoring rule, the minimal expected cost corresponds to reporting the true set of probabilities. Proper scoring rules are used in meteorology, finance, and pattern classification where a forecaster or algorithm will attempt to minimize the average score to yield refined, calibrated probabilities (i.e. accurate probabilities).\n\nSuppose formula_1 and formula_2 are two random variables defined on a sample space formula_3 with formula_4 and formula_5 as their corresponding density (mass) functions, in which formula_6 is a forecast target variable and formula_7 is the random variable generated from a forecast schema. Also, assume that the formula_8, for formula_9 is the realized value. A scoring rule is a function such as formula_10 (i.e., formula_11) which calculates the distance between formula_7 and formula_13.\n\nformula_11 is positively oriented if for two different probabilistic forecasts (such as formula_7 and formula_16), formula_17 means that formula_7 is a better probabilistic forecast than formula_16.\n\nExpected score is the expected value of the scoring rule over all possible values of the target variable. For example, for a continuous random variable we have\n\nformula_20\n\nThe expected score loss is the difference between the expected score for the target variable and the forecast:\n\nformula_21\n\nAssuming positive orientation, a scoring rule is considered to be strictly proper, if the value of the expected score loss is positive for all possible forecasts. In other words, based on a strictly proper score rule, a forecasting scheme must score best, if it suggests the target variable as the forecast, and if it scores best, the suggested forecast must be the target variable.\n\nAlthough scoring rules are introduced in probabilistic forecasting literature, the definition is general enough to consider non-probabilistic measures such as mean absolute error or mean square error as some specific scoring rules. The main characteristic of such scoring rules is formula_11 is just a function of the expected value of formula_7 (i.e., formula_24).\n\nAn example of probabilistic forecasting is in meteorology where a weather forecaster may give the probability of rain on the next day. One could note the number of times that a 25% probability was quoted, over a long period, and compare this with the actual proportion of times that rain fell. If the actual percentage was substantially different from the stated probability we say that the forecaster is poorly calibrated. A poorly calibrated forecaster might be encouraged to do better by a bonus system. A bonus system designed around a proper scoring rule will incentivize the forecaster to report probabilities equal to his personal beliefs.\n\nIn addition to the simple case of a binary decision, such as assigning probabilities to 'rain' or 'no rain', scoring rules may be used for multiple classes, such as 'rain', 'snow', or 'clear'.\n\nThe image to the right shows an example of a scoring rule, the logarithmic scoring rule, as a function of the probability reported for the event that actually occurred. One way to use this rule would be as a cost based on the probability that a forecaster or algorithm assigns, then checking to see which event actually occurs.\n\nA probabilistic forecaster or algorithm will return a probability vector formula_25 with a probability for each of the formula_26 outcomes. One usage of a scoring function could be to give a reward of formula_27 if the formula_26th event occurs. If a \"proper\" scoring rule is used, then the highest expected reward is obtained by reporting the true probability distribution. The use of a proper scoring rule encourages the forecaster to be honest to maximize the expected reward.\n\nA scoring rule is \"strictly proper\" if it is uniquely optimized by the true probabilities. Optimized in this case will correspond to maximization for the quadratic, spherical, and logarithmic rules but minimization for the Brier Score. This can be seen in the image at right for the logarithmic rule. Here, Event 1 is expected to occur with probability of 0.8, and the expected score (or reward) is shown as a function of the reported probability. The way to maximize the expected reward is to report the actual probability of 0.8 as all other reported probabilities will yield a lower expected score. This property holds because the logarithmic score is proper.\n\nThere are an infinite number of scoring rules, including entire parameterized families of proper scoring rules. The ones shown below are simply popular examples.\n\nThe logarithmic scoring rule is a local strictly proper scoring rule. This is also the negative of surprisal, which is commonly used as a scoring criterion in Bayesian Inference; the goal is to minimize expected surprise. This scoring rule has strong foundations in information theory.\n\nHere, the score is calculated as the logarithm of the probability estimate for the actual outcome. That is, a prediction of 80% that correctly proved true would receive a score of . This same prediction also assigns 20% likelihood to the opposite case, and so if the prediction proves false, it would receive a score based on the 20%: . The goal of a forecaster is to maximize the score and for the score to be as large as possible, and -0.22 is indeed larger than -1.6.\n\nIf one treats the truth or falsity of the prediction as a variable with value 1 or 0 respectively, and the expressed probability as , then one can write the logarithmic scoring rule as . Note that any logarithmic base may be used, since strictly proper scoring rules remain strictly proper under linear transformation. That is:\nis strictly proper for all formula_31.\n\nThe quadratic scoring rule is a strictly proper scoring rule\nwhere formula_33 is the probability assigned to the correct answer and formula_34 is the number of classes.\n\nThe Brier score, originally proposed by Glenn W. Brier in 1950, can be obtained by an affine transform from the quadratic scoring rule.\nWhere formula_36 when the formula_37th event is correct and formula_38 otherwise and formula_34 is the number of classes.\n\nAn important difference between these two rules is that a forecaster should strive to maximize the quadratic score yet minimize the Brier score. This is due to a negative sign in the linear transformation between them.\n\nThe spherical scoring rule is also a strictly proper scoring rule\n\nAll proper scoring rules are equal to weighted sums (integral with a non-negative weighting functional) of the losses in a set of simple two-alternative decision problems that \"use\" the probabilistic prediction, each such decision problem having a particular combination of associated cost parameters for false positive and false negative decisions. A \"strictly\" proper scoring rule corresponds to having a nonzero weighting for all possible decision thresholds. Any given proper scoring rule is equal to the expected losses with respect to a particular probability distribution over the decision thresholds; thus the choice of a scoring rule corresponds to an assumption about the probability distribution of decision problems for which the predicted probabilities will ultimately be employed, with for example the quadratic loss (or Brier) scoring rule corresponding to a uniform probability of the decision threshold being anywhere between zero and one.\n\nShown below on the left is a graphical comparison of the Logarithmic, Quadratic, and Spherical scoring rules for a binary classification problem. The x-axis indicates the reported probability for the event that actually occurred.\n\nIt is important to note that each of the scores have different magnitudes and locations. The magnitude differences are not relevant however as scores remain proper under affine transformation. Therefore, to compare different scores it is necessary to move them to a common scale. A reasonable choice of normalization is shown at the picture on the right where all scores intersect the points (0.5,0) and (1,1). This ensures that they yield 0 for a uniform distribution (two probabilities of 0.5 each), reflecting no cost or reward for reporting what is often the baseline distribution. All normalized scores below also yield 1 when the true class is assigned a probability of 1.\n\nA strictly proper scoring rule, whether binary or multiclass, after a positive-affine transformation remains a strictly proper scoring rule. That is, if formula_27 is a strictly proper scoring rule then formula_42 with formula_43 is also a strictly proper scoring rule.\n\nA proper scoring rule is said to be \"local\" if its value depends only on the probability formula_33. All binary scores are local because the probability assigned to the event that did not occur is directly producible as formula_45.\n\nAffine functions of the logarithmic scoring rule are the only strictly proper local scoring rules on a finite set that is not binary.\n\nThe expectation value of a proper scoring rule formula_46 can be decomposed into the sum of three components, called \"uncertainty\", \"reliability\", and \"resolution\", which characterize different attributes of probabilistic forecasts:\n\nIf a score is proper and negatively oriented (such as the Brier Score), all three terms are positive definite.\nThe uncertainty component is equal to the expected score of the forecast which constantly predicts the average event frequency.\nThe reliability component penalizes poorly calibrated forecasts, in which the predicted probabilities do not coincide with the event frequencies.\n\nThe equations for the individual components depend on the particular scoring rule.\nFor the Brier Score, they are given by\n\nwhere formula_51 is the average probability of occurrence of the binary event formula_52, and formula_53 is the conditional event probability, given formula_54, i.e. formula_55\n\n"}
{"id": "27083115", "url": "https://en.wikipedia.org/wiki?curid=27083115", "title": "Single-molecule FRET", "text": "Single-molecule FRET\n\nSingle molecule fluorescence resonance energy transfer (or smFRET) is a biophysical technique used to measure distances at the 1-10 nanometer scale in single molecules, typically biomolecules. It is an application of FRET wherein a pair of donor and acceptor fluorophores are excited and detected on a single molecule level. In contrast to \"ensemble FRET\" which provides the FRET signal of a high number of molecules, single-molecule FRET is able to resolve the FRET signal of each individual molecule. \n\nSingle molecule FRET measurements are typically performed on fluorescence microscopes, either using surface-immobilized or freely-diffusing molecules. Single FRET pairs are illuminated using intense light sources, typically lasers, in order to generate sufficient fluorescence signal to enable single molecule detection. Wide-field multiphoton microscopy is typically combined with total internal reflection fluorescence microscope (TIRF). This selectively excites FRET pairs on the surface of the measurement chamber and rejects noise from the bulk of the sample. Conversely, confocal microscopy minimizes background by focusing the fluorescence light onto a pinhole to reject out of focus light. The confocal volume has a diameter of around 220 nm, and therefore it must be scanned across if an image of the sample is needed. With confocal excitation, it is possible to measure much deeper into the sample than when using TIRF. Fluorescence signal is detected either using ultra sensitive CCD or scientific CMOS cameras for wide field microscopy or SPADs for confocal microscopy. Once the single molecule intensities vs. time are available the FRET efficiency can be computed for each FRET pair as a function of time and thereby it is possible to follow kinetic events on the single molecule scale and to build FRET histograms showing the distribution of states in each molecule. However, data from many FRET pairs must be recorded and combined in order to obtain general information about a sample.\n\nIn surface-immobilized experiments, biomolecules labeled with fluorescent tags are bound to the surface of the coverglass and images of fluorescence are acquired (typically by a CCD or scientific CMOS cameras). Data collection with cameras will produce movies of the specimen which must be processed to derive the single molecule intensities with time.\n\nAn advantage of surface-immobilized experiments is that many molecules can be observed in parallel for an extended period of time until photobleaching\n(typically 1-30 s). \nThis allows to conveniently study transitions taking place on slow time scales. A disadvantage is represented by the additional biochemical modifications\nneeded to link molecules to the surface and the perturbations that the surface can potentially exert on the molecular activity. \nIn addition, the maximum time resolution of single-molecule intensities is limited by the camera acquisition time (> 1 ms).\n\nSmFRET can also be used to study the conformations of molecules freely diffusing in a liquid sample. In freely-diffusing smFRET experiments (or diffusion-based smFRET), the same biomolecules are free to diffuse in solution while being excited by a small excitation volume (usually a diffraction-limited spot). Bursts of photons due a single-molecule crossing the excitation spot are acquired with SPAD detectors. The confocal spot is usually fixed in a given position (no scanning happens, and no image is acquired). Instead, the fluorescence photons emitted by individual molecules crossing the excitation volume are recorded and accumulated in order to build a distribution of different populations present in the sample. Depending on the complexity of this distribution, acquisition times varies from ~5 min to several hours. \n\nA distinctive advantage of setups employing SPAD detectors is that they are not limited by a \"frame rate\" or a fixed integration time like when using cameras. In fact, unlike cameras, SPADs produce a pulse every time a photon is detected, while an additional electronics is needed to \"timestamps\" each pulse with 10-50 ns resolution. The high time resolution of confocal single-molecule FRET measurements allows to potentially detect dynamics on time scales as low as 10 μs. However, detecting \"slow\" transitions on timescales longer than the diffusion time (typically ~1 ms), is more difficult than in surface-immobilized experiments and generally requires much longer acquisitions.\n\nNormally, the fluorescent emission of both donor and acceptor fluorophores is detected by two independent detectors and the FRET signal is computed from the ratio of intensities in the two channels. Some setup configurations further split each spectral channel (donor or acceptor) in two orthogonal polarizations (therefore requiring 4 detectors) and are able to measure both FRET and fluorescence anisotropy at the same time. In other configurations, 3 or 4 spectral channels are acquired at the same time in order to measure multiple FRET pairs at the same time. \nBoth CW or pulsed lasers can be used as excitation source. When using pulsed lasers, a suitable acquisition hardware can measure the photon arrival time with respect to the last laser pulse with picosecond resolution, in the so-called time-correlated single photon counting (TCSPC) acquisition. In this configuration each photon is characterized by a macro-time (i.e. a coarse 10-50 ns timestamp) and a micro-time (i.e. delay with respect the last laser pulse). The latter can be used to extract lifetime information and obtain the FRET signal.\n\nSmFRET allows for a more precise analysis of heterogeneous populations and has a few advantages when compared to ensemble FRET.\n\nOne benefit of studying distances in single molecules is that heterogeneous populations can be studied more accurately with values specific for each molecule rather than computing an average based on an ensemble. This allows for the study of specific homogeneous populations within a heterogeneous population. For example, if two existing homologous populations within a heterogeneous population have different FRET values, an ensemble FRET analysis will produce a weighted averaged FRET value to represent the population as a whole. Thus, the obtained FRET value does not produce data on the two distinct populations. In contrast, smFRET would be able to differentiate between the two populations and would allow analysis of the existing homologous populations.\n\nSmFRET also provides dynamic temporal resolution of an individual molecule that cannot be accomplished through ensemble FRET measurements. This allows smFRET to be used to study an RNA’s folding dynamics. Similar to protein folding, RNA folding goes through multiple interactions, folding pathways, and intermediates before reaching its native state. Ensemble FRET has the ability to detect well-populated transition states that accumulate in a population, but it lacks the ability to characterize intermediates that are short-lived and do not accumulate. This limit is addressed by smFRET which offers a direct way to observe the intermediates of single molecules regardless of accumulation. Therefore, smFRET demonstrates the ability to capture transient subpopulations in a heterogeneous environment.\n\nSmFRET is also shown to utilize a three-color system better than ensemble FRET. Using two acceptor fluorophores rather than one, FRET can observe multiple sites for correlated movements and spatial changes in any complex molecule. This is shown in the research on the Holliday Junction. SmFRET with the three-color system offers insights on synchronized movements of junction’s three helical sites and near non-existence of its parallel states. Ensemble FRET can use three-color system as well. However, any obvious advantages are outweighed by three-color system’s requirements which includes a clear separation of fluorophore signals. For a clear distinction of signal, FRET overlaps must be small but that also weakens FRET strength. SmFRET corrects its overlap limitations by using band-pass filters and dichroic mirrors which further the signal between two fluorescence acceptors and solve for any bleed through effects.\n\nA major application of smFRET is to analyze the minute biochemical nuances that facilitate protein folding. In recent years, multiple techniques have been developed to investigate single molecule interactions that are involved in protein folding and unfolding. Force-probe techniques, using atomic force microscopy and laser tweezers, have provided information on protein stability. smFRET allows researchers to investigate molecular interactions using fluorescence. Forster resonance energy transfer (FRET) was first applied to single molecules by Ha et al. and applied to protein folding in work by Hochstrasser, Weiss, et al. The benefit that smFRET as a whole has afforded to analyzing molecular interactions is the ability to test single molecule interactions directly without having to average ensembles of data. In protein folding analysis, ensemble experiments involve taking measurements of multiple proteins that are in various states of transition between their folded and unfolded state. When averaged, the protein structure that can be inferred from the ensemble of data only provides a rudimentary structural model of protein folding. However, true understanding of protein folding requires deciphering the sequence of structural events along the folding pathways between the folded and unfolded states. It is this particular branch of research that smFRET is highly applicable.\n\nFRET studies calculate corresponding FRET efficiencies as a result of time-resolved observation of protein folding events. These FRET efficiencies can then be used to infer distances between molecules as a function against time. As the protein transitions between the folded and unfolded states, the corresponding distances between molecules can indicate the sequence of molecular interactions that lead to protein folding.\n\nSingle-molecule FRET can also be applied to study the conformational changes of the relevant channel motifs in certain channels. For example, labeled tetrameric KirBac potassium channels were labeled with donor and acceptor fluorophores at particular sites in order to understand the structural dynamics within the lipid membrane, thus allowing them to generalize similar dynamics for similar motifs in other eukaryotic Kir channels or even cation channels in general. The use of smFRET in this experiment allows for visualization of the conformational changes that cannot be seen if the macroscopic measurements are simply averaged. This will lead to ensemble analysis rather than analysis of individual molecules and the conformational changes within, allowing us to generalize similar dynamics for similar motifs in other eukaryotic channels.\n\nThe structural dynamics of the KirBac channel was thoroughly analyzed in both the open and closed states, dependent on the presence of the ligand PIP2. Part of the results based on smFRET demonstrated the structural rigidity of the extracellular region. The selectivity filter and the outer loop of the selectivity filter region was labeled with fluorophores and conformational coupling was observed. The individual smFRET trajectories strongly demonstrated a FRET efficiency of around 0.8 with no fluctuations, regardless of the state of the channel.\n\nDespite making approximate estimates, a limitation of smFRET is the difficulty of obtaining the correct distance involved in energy transfer. Requiring an accurate distance estimate gives rise to a major challenge because the fluorescence of the donor and acceptor fluorophores as well as the energy transfer is dependent on the environment and how the dyes are oriented, which can vary depending on the flexibility of where the fluorophores are bound. This issue, however, is not particularly relevant when the distance estimation of the two fluorophores does not need to be determined with exact and absolute precision.\n\n"}
{"id": "14668858", "url": "https://en.wikipedia.org/wiki?curid=14668858", "title": "Sister Study", "text": "Sister Study\n\nThe Sister Study is a nationwide effort, conducted by the National Institute of Environmental Health Sciences, one of the National Institutes of Health of the U.S. Department of Health and Human Services to learn how the environment and genes may affect the chances of getting breast cancer. Over the next 10 years, the study will follow 50,000 sisters of women who have had breast cancer, in hopes of finding the environmental and genetic causes of the disease.\n\nSister Study participants are women ages 35 to 74. Women are eligible to participate if their sister (living or deceased), related to them by blood, had breast cancer; they have never had breast cancer themselves; and they live in the United States or Puerto Rico. The Sister Study is available in English and Spanish.\n\nOrganizations in partnership with the Sister Study include the American Cancer Society, the Intercultural Cancer Council, the National Center on Minority Health and Health Disparities of the National Institutes of Health, Sisters Network Inc., Susan G. Komen for the Cure, and Breast Cancer Network of Strength.\n\n"}
{"id": "38023022", "url": "https://en.wikipedia.org/wiki?curid=38023022", "title": "Sociological Methodology", "text": "Sociological Methodology\n\nSociological Methodology is an annual peer-reviewed academic journal that covers research methods in the field of sociology. The editor-in-chief is Duane F. Alwin (Pennsylvania State University). It was established in 1969 and is currently published by SAGE Publications on behalf of the American Sociological Association.\n\n\"Sociological Methodology\" is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", its 2017 impact factor is 1.85, ranking it 36th out of 146 journals in the category \"Sociology\".\n"}
{"id": "27576059", "url": "https://en.wikipedia.org/wiki?curid=27576059", "title": "The Penguin Dictionary of Curious and Interesting Numbers", "text": "The Penguin Dictionary of Curious and Interesting Numbers\n\nThe Penguin Dictionary of Curious and Interesting Numbers is a reference book for recreational mathematics and elementary number theory written by David Wells. The first edition was published in paperback by Penguin Books in 1986 in the UK, and a revised edition appeared in 1997 ().\n\nThe entries are arranged in increasing order of magnitude, with the exception of the first entry on −1 and \"i\". The book includes some irrational numbers below 10 but concentrates on integers, and has an entry for every integer up to 42. The final entry is for Graham's number.\n\nIn addition to the dictionary itself, the book includes a list of mathematicians in chronological sequence (all born before 1890), a short glossary, and a brief bibliography. The back of the book contains eight short tables \"for the benefit of readers who cannot wait to look for their own patterns and properties\", including lists of polygonal numbers, Fibonacci numbers, prime numbers, factorials, decimal reciprocals of primes, factors of repunits, and lastly the prime factorization and the values of the functions \"φ\"(\"n\"), \"d\"(\"n\") and \"σ\"(\"n\") for the first hundred integers. The book concludes with a conventional, alphabetical index.\n\nIn a review of several books in The \"College Mathematics Journal\", Brian Blank described it as \"a charming and interesting book\", and the \"Chicago Tribune\" described the revised edition as \"a fascinating book on all things numerical\". By contrast, Christopher Hirst called it \"a volume which none but propeller-heads will find either curious or interesting\" in a review of another book in \"The Independent\".\n\nBeside the serious mathematics and number theory, Wells occasionally makes humorous or playful comments on the numbers he is discussing. For example, his entry for the number 39 largely consists of a joke involving the interesting number paradox:\n\n"}
{"id": "7731163", "url": "https://en.wikipedia.org/wiki?curid=7731163", "title": "The Physics of Star Trek", "text": "The Physics of Star Trek\n\nThe Physics of Star Trek is a 1995 non-fiction book by the theoretical physicist Lawrence M. Krauss. It is the third book by Krauss, who later wrote a followup titled \"Beyond Star Trek\" in 1997.\n\nKrauss discusses the physics involved in various concepts and objects described in the \"Star Trek\" universe. He investigates the possibility of such things as inertial dampers and warp drive, and whether physics as we know it would allow such inventions. He also discusses time travel, light speed, pure energy beings, wormholes, teleportation, and other concepts that are staples of the \"Star Trek\" universe. The book includes a foreword by astrophysicist Stephen Hawking.\n\n\"The Physics of Star Trek\" was met with generally positive reviews. It became a national bestseller and sold more than 200,000 copies in the United States. As of 1998, it was being translated into 13 different languages. It was also the basis of a BBC television production.\n\nKrauss got the idea for writing the book from his publisher, who initially suggested it as a joke. Krauss dismissed the idea but later thought that using \"Star Trek\" might get people interested in real physics.\n\nThe hardcover edition was published in November 1995, and a paperback edition followed in September 1996. The book was revised and updated by Krauss in 2007. Krauss's next book, \"Beyond Star Trek: Physics from Alien Invasions to the End of Time\", was published in 1997.\n\n"}
{"id": "51410477", "url": "https://en.wikipedia.org/wiki?curid=51410477", "title": "Women in Antarctica", "text": "Women in Antarctica\n\nThere have been women in Antarctica and exploring the regions around Antarctica for many centuries. Oral tradition of Māori explorers reaching Antarctic waters as early as 650 CE, put women on the Antarctic map. The most celebrated \"first\" for women was in 1935 when Caroline Mikkelsen became the first woman to set foot on one of Antarctica's islands. Early male explorers, such as Richard Byrd, named areas of Antarctica after wives and female heads of state. As Antarctica moved from a place of exploration and conquest to a scientific frontier, women worked to be included in the sciences. The first countries to have female scientists working in Antarctica were the Soviet Union, South Africa and Argentina.\n\nBesides exploring and working as scientists, women have also played supportive roles as wives, fund-raisers, publicists, historians, curators and administrators of organizations and services that support Antarctic operations. Many early women on Antarctica were the wives of explorers. Some women worked with Antarctica from afar, crafting policies for a place they had never seen. Women who wished to have larger roles in Antarctica and on the continent itself had to \"overcome gendered assumptions about the ice and surmount bureaucratic inertia. As women began to break into fields in Antarctica, they found that it could be difficult to compete against men who already had the \"expeditioner experience\" needed for permanent science positions. Women who were qualified for expeditions or jobs in Antarctica were less likely to be selected than men, even after a 1995 study by Jane Mocellin showed that women cope better than men with the Antarctic environment.\n\nMost early policies and practices, including the construction and creation of Antarctic organizations were created initially by men. Women were originally excluded from early exploration in Antarctica based on the attitude that women could not handle the extremes in temperature or crisis situations. Vivian Fuchs, who was in charge of the British Antarctic Survey in the 1960s believed that women couldn't carry heavy equipment and that Antarctic facilities were unsuitable for women. The United States believed for many years that the climate of Antarctic was too harsh for women.\n\nAntarctica was seen by many men as a place where men could imagine themselves heroic conquerors. In Western culture, frontier territories are often associated with masculinity. Antarctica itself was envisioned by many male explorers as a \"virginal woman\" or \"monstrous feminine body\" to be conquered by men. Women were often \"invoked in terms of place naming and territorial conquest and later even encouraged to have babies in Antarctica.\" Using women as territorial conquest is probably at its most literal in the way that Argentina and Chile have flown pregnant women to Antarctica to give birth and stake a national claim to the area.\nSilvia Morella de Palma was the first woman to give birth in Antarctica, delivering 3.4 kg (7 lb 8 oz) Emilio Palma at the Argentine Esperanza base 7 January 1978.\n\nMen enjoyed having a space that was free of women and which in the late 1940s \"allowed them to continue the kind of male companionship and adventure they had enjoyed during the Second World War.\" In one news article about Antarctica written in 1958, the writer describes the use of dazzlement: \"On the womanless continent, the purpose of the dazzlement is not to catch the eye of a flirtatious blonde, but to attract spotters in the event that the explorers become lost in the frozen waste.\" Men's spaces in Antarctica resisted change. In the 1980s, there was an attempt by men to memorialize the \"Sistine ceiling\" of the Weddell hut in Antarctica as an Australian national heritage site of \"high significance.\" The \"Sistine ceiline\" was covered in 92 different pornographic pinups of women from the 1970s and 1980s. This represented a \"male's only club\" in which participants believed women would spoil the \"purity of a homosocial work--and play--environment.\" In 1983, the \"San Bernardino County Sun\" newspaper published an article about Antarctica stating that it \"is still one of the last macho redoubts, where men are men and women are superfluous.\" One scientist, Lyle McGinnis, who had been going to Antarctica since 1957 resented women in the field, saying that \"men never grouse,\" but he believed that women complained and needed \"comfort.\" Not all men felt that way. Other men felt that women's presence made life in Antarctica better and one male engineer stated that without women around, \"men are pigs.\" Sociologist Charles Moskos stated that as more women are introduced to a group, there is less aggression and a \"more civil culture develops.\"\n\nMany of the careers in Antarctica are in the sciences, and women faced barriers there as well. As women attempted to work in science, arguments using biological determinism, evolutionary psychology and popular notions of neurobiology were used as excuses as to why there were fewer women in the sciences. These arguments described how \"women are ill-adapted on evolutionary grounds for science and the competitive environment of the laboratory.\" Some women described feeling that they were \"a bit of a joke\" working in Antarctica, and felt that men regarded them as incapable.\n\nAntarctic exploration and science research was often facilitated by national navies, who often did not want women on their ships. The United States Navy used the excuse that \"sanitation facilities were too primitive\" on Antarctica as an excuse to bar women. The U.S. Navy also considered Antarctica a \"male-only bastion.\" Admiral George Dufek said in 1956 that \"women would join American Teams in the Antarctic over his dead body.\" He also believed that women's presence on Antarctica \"would wreck men's illusions of being heroes and frontiersmen.\" Military groups also were worried about \"sexual misconduct.\"\n\nAs women began to try to become part of Antarctic exploration and research, change was slow. An article run in \"The Daily Herald\" newspaper of Chicago in 1974 described women finally coming to Antarctica as integrating the \"land with a definite feminine touch.\" The article describes women's perfumed smells, ways of entertaining guests on Antarctica and the \"dainty feet\" of Caroline Mikkelsen. Eventually, however, both the \"presence and impact of female Antarctic researchers has increased rapidly.\"\n\nOral records from Oceania indicate that women explorers, Ui-te-rangiora around 650 CE and Te Ara-tanga-nuku in 1000 CE, may have traveled to the Antarctic regions. The first Western woman to visit the Antarctic region was Louise Séguin, who sailed on the \"Roland\" with Yves Joseph de Kerguelen in 1773.\n\nIn the early twentieth century, women were interested in going to Antarctica. When Ernest Shackleton advertised his 1914 Antarctic expedition, three women wrote to him, requesting to join, though the women never became part of the journey. In 1919, newspapers reported that women wanted to go to Antarctica, writing that \"several women were anxious to join, but their applications were refused.\" Later, in 1929, twenty-five women applied to the British, Australian and New Zealand Antarctic Research Expedition (BANZARE), and were also rejected. When a British Antarctic Expedition was proposed in 1937, 1,300 women applied to join. None of those 1,300 went to the frozen continent.\n\nWomen who were wives of explorers who were left behind \"endured years of loneliness and anxiety.\" Women like Kathleen Scott raised money for their husbands' journeys.\n\nThe first women involved in exploration of Antarctica were wives and companions of male travelers and explorers. Women accompanied men as \"whaling wives\" to Antarctic waters. The first women to see the continent of Antarctica was Norwegian Ingrid Christensen and her companion, Mathilde Wegger, both of whom were traveling with Christensen's husband. The first woman to step onto the land of Antarctica, an island, was Caroline Mikkelsen in 1935. Mikkelsen only briefly went ashore, and was also there with her husband. Later, after her husband died, Mikkelsen remarried and didn't talk about her experience in Antarctica in order \"to spare his feelings.\" Christensen went back to Antarctica three times after she first glimpsed the land. She eventually landed at Scullin Monolith, becoming the first woman to set foot on the Antarctic mainland, followed by her daughter, Augusta Sofie Christensen, and two other women: Lillemor Rachlew, and Solveig Widerøe. Because the women believed the landing wasn't an actual \"first,\" they didn't make much of their accomplishment.\n\nIn the years of 1946 and 1947, Jackie Ronne and Jennie Darlington were the first women to spend the year in Antarctica. When Ronne and Darlington decided to accompany their husbands in 1946 to Antarctica, men on the expedition \"signed a petition trying to stop it happening.\" Ronne worked as the mission's \"recorder.\" Ronne and Darlington both wrote about their experiences on the ice, and in the case of Darlington's book, about how conflict between team members also \"strained relations between the two women.\" One of the ways that Darlington tried to fit in with the men of the group was to make herself as \"inconspicuous within the group as possible.\" One man, first seeing Darlington arrive at the Antarctic base, \"fled in fright, thinking that he'd gone mad.\" Both women, upon returning from Antarctica downplayed their own roles, letting \"their husbands take most of the honour.\"\n\nIn 1948, the British diplomat, Margaret Anstee, was involved in the Falkland Islands Dependency Survey (FIDS) and helped make policy for the program.\n\nWomen scientists first began researching Antarctica from ships. The first woman scientist, Maria V. Klenova of the Soviet Union, worked on the ships \"Ob\" and \"Lena\" just off the Antarctic coastline in 1955 to 1956. Klenova's work helped create the first Antarctic atlas. Women served on Soviet Union ships going to Antarctica after 1963. The first women to visit a US station and the first to fly to Antarctica were Pat Hepinstall and Ruth Kelley, Pan Am flight attendants, who spent four hours on the ground at the McMurdo Station on October 15, 1957.\n\nOften women going to Antarctica had to be approved in both official and unofficial ways. An early candidate for becoming one of the first women scientists to go to Antarctica was geologist Dawn Rodley, who had been approved of not only by the expedition sponsor, Colin Bull, but also by the wives of the male team-members. Rodley was set to go in 1958, but the United States Navy, who were in charge of Operation Deep Freeze, refused to take her to Antarctica.\n\nThe Navy decided that sending a four-woman team would be acceptable, and Bull began to build a team including Lois Jones, Kay Lindsay, Eileen McSaveney and Terry Tickhill. These four women were part of the group who became the first women to visit the South Pole. Jones's team worked mainly in Wright Valley. After their return, Bull found that several of his male friends resented the addition of women and even called him a \"traitor\".\nThe first United States all-female team was led by Jones in 1969. Her team, which included the first women to set foot on the South Pole, were used by the navy as a publicity stunt, \"paraded around\" and called \"Powderpuff explorers\". The first United States woman to step into in the Antarctic interior in 1970 was engineer Irene C Peden, who also faced various barriers to her working on the continent. Peden describes how a \"mythology had been created about the women who'd gone to the coast -- that they had been a problem,\" and that since they had not published their work within the year, they were \"heavily criticized.\" Men in the Navy in charge of approving her trip to Antarctica were \"dragging their feet\", citing that there were not women's bathrooms available and that without another female companion, she would not be allowed to go. The admiral in charge of transportation to Antarctica suggested that Peden was trying to go there for adventure, or to find a husband, rather than for her research. Despite her setbacks, including not receiving critical equipment in Antarctica, Peden's research on the continent was successful.\n\nThe first U.S. woman to run an Antarctic research station was Mary Alice McWhinnie, who led the McMurdo Station in 1974 and was accompanied by a nun and biologist, Mary Odile Cahoon. United States women in 1978 were still using equipment and arctic clothing designed for men, although \"officials said that problem is being quickly remedied.\" American Ann Peoples became the manager of the Berg Field Center in 1986, becoming the first woman to serve in a \"significant leadership role\".\n\nBritish women had similar problems to the Americans. The director of the British Antarctic Survey (BAS) from 1959 to 1973 was Vivian Fuchs, who \"firmly believed that the inclusion of women would disrupt the harmony and scientific productivity of Antarctic stations.\" British women scientists started working on curating collections as part of the BAS prior to being allowed to visit Antarctica. Women who applied to the BAS were discouraged. A letter from BAS personnel sent to a woman who applied in the 1960s read, \"Women wouldn't like it in Antarctica as there are no shops and no hairdresser.\" The first BAS woman to go to Antarctica was Janet Thomson in 1983 who described the ban on women as a \"rather improper segregation.\" Women were still effectively barred from using UK bases and logistics in 1987. Women didn't winter-over at the Halley Research Station until 1996, forty years after the British station was established.\n\nArgentina sent four women scientists – biologist Irene Bernasconi, bacteriologist Maria Adela Caria, biologist Elena Martinez Fontes and algae expert Carmen Pujals – to Antarctica in 1969. Later, in 1978, Argentina sent a pregnant woman, Silvia Morello de Palma, to the Esperanza Base to give birth and to \"use the baby to stake [their] territorial claims\" to Antarctica.\n\nOnce Australia opened up travel to Antarctica to women, Elizabeth Chipman, who first worked as a typist at Casey Station in 1976, chronicled all of the women to travel there up to 1984. Chipman worked to find the names of all women who had ever been to or even near Antarctica and eventually donated 19 folio boxes of her research to the National Library of Australia.\n\nThe National Science Foundation (NSF) started long-range planning in 1978, looking towards facilities that could accommodate a population made up of 25% women. In the 1979-1980 season, there were only 43 women on the continent. By 1981, there were nearly one woman for every ten men in Antarctica. In 1983, the ratio was back to 20 men for every woman. In the 1980s, Susan Solomon's research in Antarctica on the ozone layer and the \"ozone hole\" causes her to gain \"fame and acclaim.\"\n\nIn Spain, Josefina Castellví, helped coordinate and also participated in her country's expedition to Antarctica in 1984. Later, after a Spanish base was constructed in 1988, Castellví was put in charge after the leader, Antoni Ballester had a stroke.\n\nThe first female station leader on Antarctica is Australian, Diana Patterson, head of Mawson Station in 1989. The first all-female over-wintering group is from Germany and spends the 1990-1991 winter at Georg von Neumayer, with the first German female station leader and medical doctor Monika Puskeppeleit. In 1991 In-Young Ahn is the first female leader of an Asian research station (King Sejong Station), and the first South Korean woman to step onto Antarctica.\n\nThere were approximately 180 women in Antarctica in the 1990-1991 season. Women from several different countries were regularly members of over-wintering teams by 1992. The first all-women expedition reached the South Pole in 1993. Diana Patterson, the first female station leader on Antarctica, saw a change happening in 1995. She felt that many of the sexist views of the past had given way so that women were judged not by the fact that they were women, but \"by how well you did your job.\"\n\nSocial scientist, Robin Burns, studied the social structures of Antarctica in the 1995-1996 season. She found that while many earlier women struggled, in 1995, there was more acceptance of women in Antarctica. Also by the mid 1990s, one of the station managers, Ann Peoples, felt that a tipping point had been reached and women on Antarctica became more normalized. There were still men in Antarctica who were not afraid to voice their opinion that women should not \"be on the ice,\" but many others enjoyed having \"women as colleagues and friends.\" Women around this time began to feel like it was \"taken for granted now that women go to the Antarctic.\"\n\nStudies done in the early 2000s showed that women's inclusion in Antarctic groups were beneficial overall. In the early 2000s, Robin Burns has found that female scientists who enjoyed their experience in Antarctica were ones who were able to finish their scientific work, to see through the project into completion.\n\nIn 2005, writer Gretchen Legler describes how there were many more women in Antarctica that year and that some are lesbians. International Women's Day in 2012 saw more than fifty women celebrating in Antarctica and who made up 70% of the International Antarctic Expedition. In 2013, when the Netherlands opened their first Antarctic Lab, Corina Brussaard was there to help set it up.\n\nHomeward Bound, is a 10 year program designed to encourage women's participation in science that planned to send the first large (78 member) all-women expedition to Antarctica in 2016. The first group consisted of 76 women and arrived in Antarctica for three weeks in December of 2016. Fabian Dattner and Jess Melbourne-Thomas founded the project and the Dattner Grant is providing funding, with each participant contributing $15,000 to the project. Homeward bound includes businesswomen and scientists who look at climate change and women's leadership. The plan is to create a network of 1,000 women who will become leaders in the sciences. The first voyage departed South America in December 2016 \n\nAn all-woman team of United Kingdom Army soldiers, called Exercise Ice Maiden, started recruiting members in 2015 to attempt to cross the continent under their own power in 2017. If successful, they will be the first military group of women to do so. Exercise Ice Maiden is meant to study \"female endurance in extreme conditions.\"\n\nCurrently, women make up 55% of membership in the Association of Polar Early Career Scientists (APECS). In 2016, nearly a third of all researchers at the South Pole were women. The Australian Antarctic Program (AAP) makes a \"conscious effort to recruit women.\"\n\nA social media network has recently been created \"Women in Polar Science\" it aims to connect women working in Arctic and Antarctic science and provides them with a platform to share an exchange knowledge, experiences and opportunities.\n\nWhen heavy equipment operator, Julia Uberuaga, first went to Antarctica in the late 70s, early 80s, she recalled that \"the men stared at her, or leered at her, or otherwise let her know she was unwelcome on the job.\" Rita Matthews, who went to Antarctica during the same period as Uberuaga said that the \"men were all over the place. There were some that would never stop going after you.\" In 1983, Marilyn Woody described living at McMurdo station and said, \"It makes your head spin, all this attention from all these men.\" Then she said, \"You realize you can put a bag over your head and they'll still fall in love with you.\"\n\nAnother scientist, Cynthia McFee, had been completely shut out of the \"male camaraderie\" at her location and had to deal with loneliness for long periods of time. Martha Kane, the second woman to overwinter at the South Pole, experienced \"negative pressure\" by the men with \"some viewing her as an interloper who had insinuated herself into a male domain.\"\n\nIn the 1990s, women experienced stigma in Antarctica. These women were labeled \"whores\" for interacting with men and those who did not interact with men were called \"dykes.\"\n\nIn the late 1990s and early 2000s, women felt that Antarctic operations were \"not at all sympathetic to the needs of mothers, and there is a deep concern lest a pregnant woman give birth in Antarctica.\"\n\nSexual harassment is still a problem for women working in Antarctica, with many women scientists fielding unwanted sexual advances over and over again. Women continue to be outnumbered in many careers in Antarctica, including fleet operations and trades.\n\nSome organizations, such as the Australian Antarctic Division, have created and adopted policies in order to combat sexual harassment and discrimination based on gender. The United States Antarctic Program (USAP) encourages women and minorities to apply.\n\nAmerican Lisa Densmore is the first woman to summit Mount Vinson in 1988. In 1993, American Ann Bancroft lead the first all woman expedition to the South Pole. Bancroft, and Norwegian, Liv Arnesen were the first women to ski across Antarctica in 2001.\n\nLynne Cox swam a mile in Antarctic water in 2003.\nMaria Leijerstam became the first person to cycle to the South Pole Station in 2013 on a recumbent tricyle.\n\nIn 1975, Eleanor Honnywill becomes the first woman to be awarded the Fuchs Medal from the British Antarctic Survey (BAS). The first woman to receive a Polar Medal was Virginia Fiennes, in 1986, who was honored for her work in the Transglobe Expedition. She was also the first woman to \"winter in both polar regions.\" Denise Allen is the first woman awarded the Australian Antarctic Medal in 1989.\n\n\n\n"}
