{"id": "34961993", "url": "https://en.wikipedia.org/wiki?curid=34961993", "title": "Alan Alda Center for Communicating Science", "text": "Alan Alda Center for Communicating Science\n\nThe Alan Alda Center for Communicating Science is a cross-disciplinary organization founded in 2009 within Stony Brook University's School of Journalism, in Stony Brook, New York.\n\nIts current director is Laura Lindenfield. Its goal is to help scientists learn to communicate more effectively with the public, including policymakers, students, funders and the media. It was inspired by Alan Alda, the actor, writer and science advocate, in whose honor it was renamed in 2013, and is supported by Brookhaven National Laboratory and Cold Spring Harbor Laboratory. The Center offers courses in communication that have been taken by more than 200 graduate students in the sciences and health professions at Stony Brook. It also conducts workshops at universities, laboratories and science meetings around the country. Many of its workshops use improvisational theater exercises to help scientists connect more directly with listeners and respond more spontaneously to their needs.\n\nIn 2012, Alda and the Center issued the \"Flame Challenge\", asking scientists to come up with the best explanation for a flame for an intended audience of 11-year-olds.\n\n"}
{"id": "44275062", "url": "https://en.wikipedia.org/wiki?curid=44275062", "title": "Amos Henry Worthen", "text": "Amos Henry Worthen\n\nAmos Henry Worthen (1813–1888) was an American geologist and paleontologist from Illinois. He was the second state geologist of Illinois and the first curator of the Illinois State Museum. He was a fellow of the American Association for the Advancement of Science and a member of the National Academy of Sciences.\n\n"}
{"id": "12528914", "url": "https://en.wikipedia.org/wiki?curid=12528914", "title": "Armenian birch mouse", "text": "Armenian birch mouse\n\nThe Armenian birch mouse (\"Sicista armenica\") is a species of rodent in the family Dipodidae.\n\nIt is a small rodent, like the mouse, the average weight of 10 g and up to 9 cm long, excluding the semi-prehensile tail, which slightly exceeds the length of the body. The body is brown, darker in the upper region.\n\nThe species shows nocturnal and feeds on seeds, berries and insects. Shifts in the ground with small jumps and can easily climb on the bushes and trees due to its semi-prehensile tail. The nest, oval shaped, is made of plant remains in a shallow hole dug by the animal itself.\n\nThe species is endemic to Armenia, found in mixed forests of coniferous and broadleaf trees in the area upstream of the river Marmarik.\n\nThe Zoological Society of London, on the basis of evolutionary uniqueness and smallness of the population, considers Armenian Birch Mouse one of the 100 species of mammals at greatest risk of extinction.\n"}
{"id": "10426660", "url": "https://en.wikipedia.org/wiki?curid=10426660", "title": "Association of Polar Early Career Scientists", "text": "Association of Polar Early Career Scientists\n\nThe Association of Polar Early Career Scientists (APECS) is a worldwide association of early career scientists (undergraduate and graduate students, postdocs, and early career faculty) interested in the polar regions and the cryosphere generally. Its mission is to raise the profile of polar scientists by providing a continuum of leadership that is both internationally and interdisciplinarily focused, and to stimulate collaborative projects. Several countries (Australia, Brazil, Bulgaria, Canada, Chile, Denmark, France, Germany, India, Italy, Norway, Poland, Portugal, Russia, South Africa, Sweden, the United Kingdom and the United States) have their own APECS chapters that focus on the needs and ideas of scholars country-wise.\n\nThe APECS website serves as the main contact point for APECS members and provides forums sharing news, connecting with other polar researchers, finding jobs, and announcing events relevant to polar research.\n\nAPECS is an endorsed International Polar Year (IPY) project and is considered one of the major legacies of IPY.\n\nA crucial event in the formation of APECS was a meeting in Sanga Saby, Sweden, in September 2007. This meeting saw founders and members representatives of two key initiatives combine together under the name of APECS: the International Polar Year Youth Steering Committee (YSC) formed in 2005 and including several national YSC's, and a formative APECS, formed in 2006.\n\nThe International Polar Year (IPY) Youth Steering Committee (YSC), was founded in 2005 by Amber Church and Tyler Kuhn (co-chairs, Canada), Melianie Raymond (New Zealand), Jenny Baeseman (USA), Hugues Lantuit (Germany), Elie Verleyen (Belgium) and Stef Bokhorst (The Netherlands). Its aims were to ensure that IPY's goals to include the next generation of polar researchers and the world's youth were met. The YSC was designed as a decentralized institution relying on national committees, which rapidly came to life in several countries, including Canada, Germany, New Zealand, Portugal, Sweden, and the United Kingdom, among others. Those committees rapidly gained independence and developed their own networks as exemplified by the creation of the UK Polar Network in 2007.\n\nA contributing factor to the success of the YSC's during the IPY was strong support from the IPY International Program Office (IPO), based in Cambridge, UK, who ensured that the goals of the YSC would be heard in the community of senior researchers. The roles of Dave Carlson and Rhian Salmon, in particular, were crucial. The YSC focus was the creation, fostering and promotion of activities geared towards youth by young researchers. It largely focused on the involvement of school children and young adults in polar literacy projects and strengthening the communication between students and young researchers. Progressively, a need for a broader, more encompassing organization specifically geared towards young researchers and early career scientists arose.\n\nDiscussions on IPY education and outreach forums, similar initiatives in other scientific realms and the encounter of like-minded people created an awareness of the need for an organization driven by and serving early career researchers, focused on science and career development, unlike the YSC.\n\nIn the Autumn of 2006 to address these needs, Jenny Baeseman (USA), Hugues Lantuit (Germany) and Rhian Salmon (UK) laid the grounds for the rationale, structure, connections and future activities of APECS. The acronym was coined at the time and a nascent APECS was launched massively in early 2007, at the start of the IPY with Jenny Baseman and Hugues Lantuit as co-directors. In March 2007, discussions were initiated by the directors with the International Arctic Science Committee (IASC) and the Scientific Committee on Antarctic Research (SCAR) to offer APECS' services as representative of early career researchers in polar science. This early version of APECS then started evolving to serve better the needs of early career researchers interested in the polar regions and the wider cryosphere.\n\nThe YSC's activities developed concurrently with many (if not most) of the early career researchers involved in both organizations. Its scope, however, was limited in time, since it mainly focused on creating activities during the IPY. The need to ensure the continuation of successful initiatives and activities after the IPY led to brainstorming on post-IPY legacy. At the same time, the increase in young researcher initiatives in polar science started to create some confusion in the scientific community, questioning the structure, coordination and even the relevance of such organizations.\n\nTo address these issues, a meeting was organized at Sanga Saby outside Stockholm, Sweden in September 2007 to bring together all these groups and prepare some long-term sustainable plans. The meeting was sponsored by the Swedish company Serla, the IPY IPO, and other international polar science entities The key outcomes of this meeting were the decisions to merge these groups, including the YSC, into one organization, retaining the name of APECS, and that APECS should adapt its structure to reflect better the multifaceted nature of its membership. This established APECS as a legacy of the YSC and other IPY projects. A new structure was launched at the end of the meeting including working groups, an advisory committee, an interim Council of the 24 attending participants (see below), an interim Executive Committee elected by the council Kriss Rokkan Iversen (Norway), Narelle Baker (UK), Hugues Lantuit (Germany), Dan Pringle (USA), and José Xavier (Portugal), and Jen Baeseman (USA) was appointed as an interim director. Kriss Rokkan Iversen received unanimous support of all voters and appointed by the Executive Committee as the interim President of APECS.\n\nThe Executive Committee and Director were charged with establishing APECS as an organization over the next 6–12 months. A report of activities in this period was made at the online APECS Council Meeting, 21 May 2008. Key progress included forming an international Advisory Committee of senior researchers and science administrators to provide guidance and support. A website was developed by in kind support from Iceland-based Arctic Portal through the generous support of director Halldór Jóhannsson. The website was established as a virtual home of APECS and amongst other features, includes study and job opportunities, meetings, news updates, and a discussion forum.\n\nThe executive committee met in March 2008 in Akureyri, Iceland to address strategic planning for APECS and draft the documents that will help sustain this organization for year to come; The Terms of Reference and the Rules of Procedure. This meeting was coordinated by Halldór Jóhannsson and supported by the University of Akureyri, Northern Research Forum and the Arctic Portal.\n\nThe ROP and TOR included a revision from the interim APECS structure to an open Council who elect an Executive Committee. The Council controls issues related to APECS governance and structure, and is expected to act on time scales of months – years. The Executive Committee is mandated by the Council with shorter time-scale decision making and running APECS on a day-to-day basis. (See the founding ROP and TOR for details.)\n\nThe organization now has an International Directorate Office hosted at the University of Tromsø, Norway.\n\nThe association represents people with a wide range of scientific expertise and interests including glaciology, geology, geodesy, anthropology, sociology, political science, atmospheric science, oceanography, polar biology, culture and heritage studies, linguistics, space studies, biogeochemistry, and paleontology.\n\nMembership in APECS is free and open to all early career scientists interested in natural and social sciences of the polar regions, from undergraduates through assistant professors or equivalent for non-academic positions. Participation by engineers and those interested in the cryosphere in general is also being sought. APECS encourages senior researchers to register on the APECS website and serve as mentors for the organization as well as post job openings and events at their institutions.\n\nAPECS will be sponsoring several early career gatherings at IPY events and major research conferences around the world during the International Polar Year.\n\n"}
{"id": "6404397", "url": "https://en.wikipedia.org/wiki?curid=6404397", "title": "Automotive X Prize", "text": "Automotive X Prize\n\nThe Progressive Insurance Automotive X PRIZE (PIAXP or AXP) was a set of competitions, programs and events, from the X Prize Foundation, to \"inspire a new generation of super-efficient vehicles that help break America's addiction to oil and stem the effects of climate change.\" Progressive Insurance was the title sponsor of the prize, the centerpiece of which is the Competition Division, within which a 10-million-dollar purse was divided between the winners of three competitions.\n\nThe essence of each competition was to design, build and race super-efficient vehicles that achieved 100 MPGe (2.35 liter/100 kilometer) efficiency, produced less than 200 grams/mile well-to-wheel CO equivalent emissions, and could be manufactured for the mass market. Within the Competition Division, there are two vehicle classes: Mainstream and Alternative. The mainstream class had a prize of $5 million. The alternate class had two separate prizes of $2.5 million, one for side-by-side seating and one for tandem seating.\n\nThe PIAXP has an Educational Program, funded by a $3.5 million grant from the United States Department of Energy, to engage students and the public in learning about advanced vehicle technologies, energy efficiency, climate change, alternative fuels, and the science, technology, engineering, and math behind efficient vehicle development.\n\nThe X Prize Foundation began work on the development of a competition to spur innovation in the automotive industry in 2005 and on 6 March 2006 announced that Mark Goodstein would join the Foundation as an Executive Director of the new prize.\n\nAt the 12 April 2007 announcement of the creation of the X Prize, the Foundation released draft Competition Guidelines, which were open for public comment from 2 April to 31 May 2007. The latest guidelines were published on 10 January 2009. The competition guidelines are the product of hundreds of volunteers of the AXP and world-class advisors.\n\nOn 7 April 2009, the X Prize Foundation announced that 111 teams had registered by the February 2009 deadline. By 20 October 2009, the design judging had winnowed the number of teams down to 43, with some publicly, and others quietly, withdrawing. The formal vehicle competition events began on 26 April 2010, and consisted of the remaining four stages: Shakedown (26 April – 7 May 2010), Knockout ( 16–30 June 2010), Finals ( 19–30 July 2010) and Validation (August 2010).\n\nThe winners of the competition were announced on 16 September 2010.\n\nWithin the Competition Division, there are two vehicle classes—Mainstream and Alternative—both of which have the same requirements for fuel economy and emissions, but differing design constraints. The Alternative class is further divided into tandem and side-by-side classes. Vehicles in the Mainstream Class must meet specifications that are derived from typical small, five-passenger, economy mixed-use vehicles. The Alternative Class has fewer performance and design restrictions and provides an outlet for innovation. Both classes allow entries that are modifications of an existing popular vehicle, provided that all PIAXP requirements are met.\n\nVehicles in both classes must have a fuel economy of 100 MPGe (21 kWh or 2.35 liters of petrol/100 kilometer) and produce less than 200 grams/mile CO emissions (measured well-to-wheel). For electric vehicles, the CO emissions requirement is a more binding constraint. Because CO emissions will be calculated assuming a national average of electricity sources projected to 2014, an all-electric car will have to achieve 114 MPGe in order to produce less than 200 grams/mile CO emissions. Further, electricity consumption is measured at the \"plug\" side of the battery charging device, so it would have to achieve 114 MPGe, assuming 100% efficient battery charging. If the charger were 85% efficient, this requirement would grow to 134 MPGe. In the other words, efficiency of electric cars should be not 21 but 16 kWh/100 km.\n\nVehicles in both classes also must have features expected of a modern automobile including an enclosed cabin with windshield and windows, operating windshield wipers, washers, headlights, horn, indicators, brake lights, reflective devices, rear and side-view mirrors, and seat belts. They must have the usual automotive controls, including accelerator pedal, brake pedal, steering mechanism (not necessarily a wheel) and indicators. They must be \"highway capable\", which is defined as the ability to maintain on a four percent uphill grade and to accelerate from to in less than 9 seconds. They must be able to brake from 60 to 0 mph in less than , meet existing noise standards and use tires that meet automotive or motorcycle (alternative class only and only if the vehicle is otherwise eligible to be classified as a motorcycle) standards. Both must meet the same set of static and dynamic stability requirements.\n\nThe mainstream vehicle must seat at least four adults with at least two side-by-side front seats, have at least of useful cargo space in one contiguous location not counting the passenger seats, accelerate from 0 to in 15 seconds or less, and be able to drive without refueling or recharging. The mainstream vehicle must have four or more wheels.\n\nThe alternate class vehicle must seat at least two people, accelerate from 0 to in 18 seconds or less, and be able to drive without refueling or recharging. The alternative vehicle has no minimum number of wheels, but it must remain upright when stopped with no driver inputs.\n\nWhile the main focus of PIAXP is fuel economy and carbon emissions, not safety, the vehicles must be \"production capable\". Therefore, the entries must either be fully compliant with the Federal Motor Vehicle Safety Standards (FMVSS) and other applicable National Highway Traffic Safety Administration (NHTSA) requirements or compliance has to be \"designed in\". For example, allowance for airbags in the designs is considered acceptable without actually installing the airbags. Teams are also required to submit a business plan which clearly demonstrates an ability to produce 10,000 vehicles per year. Note that teams are not required to be under-taking this plan, but the plan has to exist and the car has to be designed such that this plan is feasible.\n\nWith the lack of mainstream entrants from established automobile companies, a Demonstration Division was created so that automakers could at least display and promote their highest efficiency vehicles alongside the main competition. However, there were too few entrants by 1 March registration deadline, and this division was canceled. The only confirmed entrant was the Tesla Roadster, which had dropped out of the main competition.\n\nVehicles in the Demonstration Division would have met the same requirements as Mainstream Class vehicles in the Competition Division, except for MPGe and CO emissions. There was no Alternative Class equivalent in the Demonstration Division. These vehicles would have been stock vehicles, i.e., vehicles identical to those for sale or pre-production prototypes of vehicles intended for sales.\n\nVehicles in the Demonstration Division would have been tested in the same way as Competition Division vehicles and would have participated in the PIAXP competition events under the same rules in order to demonstrate and showcase their capabilities and performance.\n\nThe competition timeline was finalized as follows:\n\nAccepted teams must provide evidence that their vehicles are production capable, by providing a detailed Data Submissions covering four areas:\n\n\nThose that pass this hurdle will be invited to bring their vehicles to the competition events. The design judging was closed in October 2009, with 43 teams remaining.\n\nThe first three stages will be a “shake-down” period and performance in these stages will not count to final scores. The first step of the competition will be the review of technical reports, technical inspection of the vehicles and performance testing of safety elements to eliminate unsafe vehicles. The two-week shakedown took place at the Michigan International Speedway, from 26 April to 7 May 2010.\n\nThose that pass the initial technical and safety inspections and tests will participate in the remaining competition events. These include stage races, additional active safety performance tests and a dynamometer test.\n\nThe next stage will be a “knockout” qualifying event. To advance, vehicles must pass active safety performance tests, meet acceptable emission levels and demonstrate at least 67 MPGe on a test track. These were held at the Michigan International Speedway from 20–29 June 2010. After this stage, only 15 vehicles from 12 teams remained in the competition.\n\nThe final stages will determine the winning teams. To complete the race successfully, vehicles must maintain a minimum average speed (maximum allowable time) while meeting PIAXP requirements for fuel economy (90 MPGe) and emissions. The finals also include a tie breaking race; the vehicle with the best overall time while still meeting requirements will take the purse. The final stages are scheduled for 19–30 July 2010 at the Michigan International Speedway.\n\nThe few remaining teams were validated on dynamometers under laboratory conditions at the EPA Labs in Ann Arbor, Michigan and Argonne National Laboratory in Chicago, Illinois. This stage is scheduled for August 2010. The validation results count toward 50% of the final efficiency figures, which together must exceed 100 MPGe.\n\nEdison2 was an exception. The engines in both Very Light Cars failed before validation. Edison2 was allowed to circumvent the rules governing procedures and provided independent third-party validation reports instead of undergoing the same validation testing as other finalist teams.\n\nThe United States Department of Energy joined the PIAXP as a sponsor, funding an educational program targeted to young people. The first component of the Education Program is a web site created in partnership with Discovery Education, www.FuelOurFutureNow.com, featuring activities for K-12 students, as well as videos, virtual labs, and interactive resources intended for use in school and in the home.\n\nAlthough 111 teams registered for the PAIXP and paid the registration fee,\n\n\n\n\n\n\n"}
{"id": "34359107", "url": "https://en.wikipedia.org/wiki?curid=34359107", "title": "Bacillus phage AP50", "text": "Bacillus phage AP50\n\nBacillus phage AP50 (formerly Phage AP50) is a species of bacteriophage that infects \"Bacillus anthracis\" bacteria. Originally thought to be an RNA phage, it contains a DNA genome of about 14,000 base pairs in an icosahedral capsid with a two-layer capsid shell.\n"}
{"id": "44349530", "url": "https://en.wikipedia.org/wiki?curid=44349530", "title": "Balletto Glacier", "text": "Balletto Glacier\n\nBalletto Glacier is near the summit of Mount Kilimanjaro in Tanzania, on the southwest slope of the peak and is a small remnant of an icecap which once crowned the top of Mount Kilimanjaro. The glacier is situated at an elevation of between . Balletto Glacier is situated on the enormous rock wall known as the \"Breach Wall\" and is below Diamond Glacier. The two glaciers are connected by an enormous icicle which hangs down the rock face as much as .\n\n"}
{"id": "14697272", "url": "https://en.wikipedia.org/wiki?curid=14697272", "title": "Barton (crater)", "text": "Barton (crater)\n\nBarton crater is a 54-km (32-mi) diameter crater on Venus. It is the size at which craters on Venus begin to possess peak-rings instead of a single central peak. The floor of Barton crater is flat and radar-dark, indicating possible infilling by lava flows sometime following the impact. Barton's central peak-ring is discontinuous and appears to have been disrupted or separated during or following the cratering process. The crater is named after Clara Barton, the founder of the American Red Cross.\n"}
{"id": "29296569", "url": "https://en.wikipedia.org/wiki?curid=29296569", "title": "Brassboard", "text": "Brassboard\n\nA brassboard or brass board is an experimental or demonstration test model, intended for field testing outside the laboratory environment. A brassboard follows an earlier prototyping stage called a breadboard. A brassboard contains both the functionality and approximate physical configuration of the final operational product. Unlike breadboards, brassboards typically recreate geometric and dimensional constraints of the final system which are critical to its performance, as is the case in radio frequency systems. While representative of the physical layout of the production-grade product, a brassboard will not necessarily incorporate all final details, nor represent the physical size and quality level of the final deliverable product.\n\nExact definition of a brassboard depends on the industry and has changed with time. A 1992 guide book on proposal preparation defined a brassboard \"or\" a breadboard as \"a laboratory or shop working model that may or may not look like the final product or system, but that will operate in the same way as the final system\". The definition of a\nbreadboard was further narrowed to purely electronic systems, while a brassboard was treated as \"a similar arrangement for hydraulic, pneumatic or mechanically interconnected components\".\n\nIn modern system-on-a-chip prototyping, brassboard is defined as a second prototyping stage that follows engineering validation boards (EVB) and precedes wingboards and final pre-production samples. Typically, the board area decreases four times with each of these steps, so a brassboard is one fourth as large as an EVB, four times larger than a wingboard and around sixteen times larger than a production device. A modern brassboard printed circuit board typically contains ten conductive layers while a considerably larger EVB typically has eighteen (it needs larger and more sophisticated ground planes to overcome the effects of larger area and longer connecting tracks).\n\n"}
{"id": "13830523", "url": "https://en.wikipedia.org/wiki?curid=13830523", "title": "CNN Business", "text": "CNN Business\n"}
{"id": "47347009", "url": "https://en.wikipedia.org/wiki?curid=47347009", "title": "Canmethod", "text": "Canmethod\n\nThe C.A.N (Convenient, Attractive, Normal) approach to eating behavior reveals three different ways or strategies that consumer psychology can be used to provide an organized framework to help people make healthier food choices. The C is for convenient, the A is for attractive, and the N is for normal. The idea was researched and created by Brian Wansink, Director of the Cornell Food and Brand Lab and others at Cornell University. The researchers reviewed over 100 studies and found three things help people choose healthier foods. The food must be convenient (C), attractive (A) and normal (N), or CAN. The idea behind the CAN approach is to make foods like fruits and vegetables visible and easy to reach (convenient), nicely displayed (attractive), and look like a good choice (normal).\nIn a study, published in Psychology and Marketing, showed that when fruit is put in a nice bowl in your home on the counter—it becomes more convenient, attractive, and normal to grab a banana or orange, rather than the chocolate ice cream in the hidden back of the freezer. Therefore, the CAN idea helps make healthier eating choices easier.\nThe results of this study are intended to help consumers make healthier decisions by adjusting the way they select and serve their food. The authors suggest these organizational frameworks can be used to show how research can help transform consumer eating environments.\n\nAn article on cbsnews.com describes the research as more helpful then willpower alone to help lose weight and eat healthier.\n\n"}
{"id": "57410816", "url": "https://en.wikipedia.org/wiki?curid=57410816", "title": "Climate of Hope", "text": "Climate of Hope\n\nClimate of Hope: How Cities, Businesses, and Citizens can Save the Planet is a New York Times Bestselling environmental science book co-authored by environmentalist and former Sierra Club Executive Director Carl Pope and Public Health advocate and former mayor of New York City Michael Bloomberg. \n\nThe book is divided into six parts with two chapters each as well as a conclusion. The book switches between Pope and Bloomberg as the draw from their individual experiences within their respective careers as well as historic lessons and scientific evidence as they discuss the inevitable climate crisis of global warning. \n\nThey not only examine the problems at hand from a variety of perspectives but they also discuss solutions they have implemented and would like to see implemented and the barriers to such. Finally they look at how we as a society have the potential to solve this crisis utilizing innovative technology, policies and nature. \n\nClimate of Hope was published on April 18, 2017 by St Martin Publication Press and was a New York Times best selling nonfiction books of that year.\n\n"}
{"id": "45587776", "url": "https://en.wikipedia.org/wiki?curid=45587776", "title": "Comas Sola (crater)", "text": "Comas Sola (crater)\n\nComas Sola (sometimes as Comas Solá) is an impact crater on Mars, located in the Memnonia quadrangle at 19.59°S latitude and 168.51°W longitude, and is inside Terra Sirenum. It measures in diameter and was named after the Spanish Catalan astronomer Josep Comas Solá. The name was approved by IAU's Working Group for Planetary System Nomenclature in 1973.\n\nNearby named prominent craters include Burton to the north-northeast, Bernard to the southeast, Dejnev to the southwest, almost to the west is Williams, nearly the same size and northwest not far from the crater is the tiny Gratteri.\n\nSurrounding the southerm rim are two smaller unnamed craters, each with a central peak or mount, with the southwest touching the southwest rim, in the north rim, three tiny craters which also has a central mound. Almost touching the crater are a series of rilles and rifts known as Memnonia Fossae that were once fault lines\n\n"}
{"id": "27657824", "url": "https://en.wikipedia.org/wiki?curid=27657824", "title": "Counterion condensation", "text": "Counterion condensation\n\nThe counterion condensation phenomenon is commonly described by Manning’s\ntheory (Manning 1969), which assumes that counterions can condense \nonto polyions until the charged density between neighboring monomer charges \nalong the polyion chain is reduced below a certain critical value. In the model the \nreal polyion chain is replaced by an idealized line charge, where the polyion \nis represented by a uniformly charged thread of zero radius, infinite\nlength and finite charge density, and the condensed counterion layer is \nassumed to be in physical equilibrium with the ionic atmosphere surrounding \nthe polyion. The uncondensed mobile ions in the ionic atmosphere are treated \nwithin the Debye–Hückel (DH) approximation. The \nphenomenon of counterion condensation now takes place when the dimensionless \nCoulomb coupling strength \nwhere formula_2 represents the Bjerrum length and \nformula_3 the distance between neighboring charged monomers. \nIn this case the Coulomb interactions dominate over \nthe thermal interactions and counterion condensation is favored. For many standard \npolyelectrolytes, this phenomenon is relevant, since the \ndistance between neighboring monomer charges typically ranges between 2 and 3 Å and \nformula_4 7 Å in water.\n\nIn the case of the chondroitin sulfate (CS) systems, which is a major \nbiopolyelectrolyte controlling the frictional-compressive properties \nof articular cartilage, the Coulomb coupling strength is formula_5 \n1.4, which implies that according to Manning's theory counterion condensation \nshould take place. However, since Manning’s theory does not take into account \nthe molecular details of real polyion chains, like e.g. local solvation \neffects or atomic partial charge distributions, CS systems are a borderline \ncase and should be considered more carefully (Bathe 2005). Field-theoretic\ninvestigations (Baeurle 2009) have recently demonstrated that the phenomenon of \ncounterion condensation disappears in the limit of infinite dilution in solutions \nof low concentration of added salt, which is in opposition with the predictions \nof Manning’s theory but in conformity with Ostwald’s principle. By contrast at \nphysiological salt concentration, the phenomenon has been found to play a predominant \nrole in determining the frictional-compressive properties of articular cartilage\n(Baeurle 2009).\n\nThe counterion condensation originally only describes the behaviour of a charged rod. It competes here with Poisson-Boltzmann theory, which was shown to give less artificial results than the counterion condensation theories.\n"}
{"id": "3622099", "url": "https://en.wikipedia.org/wiki?curid=3622099", "title": "Definition of the situation", "text": "Definition of the situation\n\nThe definition of the situation is a fundamental concept in symbolic interactionism advanced by the American sociologist W. I. Thomas. It involves a proposal upon the characteristics of a social situation (e.g. norms, values, authority, participants' roles), and seeks agreement from others in a way that can facilitate social cohesion and social action. Conflicts often involve disagreements over definitions of the situation in question. This definition may thus become an area contested between different stakeholders (or by an ego's sense of self-identity).\n\nA definition of the situation is related to the idea of \"framing\" a situation. The construction, presentation, and maintenance of frames of interaction (i.e., social context and expectations), and identities (self-identities or group identities), are fundamental aspects of micro-level social interaction.\n\n\n"}
{"id": "17206677", "url": "https://en.wikipedia.org/wiki?curid=17206677", "title": "Dispositif", "text": "Dispositif\n\nDispositif is a term used by the French intellectual Michel Foucault, generally to refer to the various institutional, physical, and administrative mechanisms and knowledge structures which enhance and maintain the exercise of power within the social body.\n\nDispositif is translated variously, even in the same book, as 'device', 'machinery', 'apparatus', 'construction', and 'deployment'.\n\nFoucault uses the term in his 1977 \"The Confession of the Flesh\" interview, where he answers the question, \"What is the meaning or methodological function for you of this term, apparatus (dispositif)?\" as follows:\n\nThe German linguist Siegfried Jäger defines Foucault's \"dispositif\" as\n\nThe Danish philosopher Raffnsøe \"advances the 'dispositive' (le dispositif) as a key conception in Foucault's work\" and \"a resourceful approach to the study of contemporary societal problems.\" According to Raffnsøe, \"the dispositionally prescriptive level is a crucial aspect of social reality in organizational life, since it has a determining effect on what is taken for granted and considered real. Furthermore, it determines not only what is and can be considered possible but also what can even be imagined and anticipated as potentially realizable, as something one can hope for, or act to bring about\".\n\nThe Italian political philosopher Giorgio Agamben traces the trajectory of the term to Aristotle's \"oikonomia—the effective management of the household\" and the early Church Fathers' attempt to save the concept of the Trinity from the allegation of polytheism, as the triplicity of the God is his oikonomia. \n\nAgamben defines the \"apparatus\"/\"dispositif\" as\n\nThe Italian scholar Matteo Pasquinelli criticises Agamben's genealogy with these words\n\n"}
{"id": "1966611", "url": "https://en.wikipedia.org/wiki?curid=1966611", "title": "Fichtelite", "text": "Fichtelite\n\nFichtelite is a rare white mineral found in fossilized wood from Bavaria. It crystallizes in the monoclinic crystal system. It is a cyclic hydrocarbon: dimethyl-isopropyl-perhydrophenanthrene, CH. It is very soft with a Mohs hardness of 1, the same as talc. Its specific gravity is very low at 1.032, just slightly denser than water.\n\nIt was first described in 1841 and named for the location, Fichtelgebirge, Bavaria, Germany. It has been reported from fossilized pine wood from a peat bog and in organic-rich modern marine sediments.\n"}
{"id": "40868373", "url": "https://en.wikipedia.org/wiki?curid=40868373", "title": "Fire in the Blood (2013 film)", "text": "Fire in the Blood (2013 film)\n\nFire in the Blood is a 2013 documentary film by Dylan Mohan Gray depicting the intentional obstruction of access to low-cost antiretroviral drugs used in the treatment of HIV/AIDS to people in Africa and other parts the global south, driven by multinational pharmaceutical companies holding patent monopolies and various Western governments (above all those of the United States, European Union and Switzerland) consistently doing these companies' bidding. The film details how the battle against this genocidal blockade, estimated to have resulted in no less than ten- to twelve million completely unnecessary deaths, was fought and (at least temporarily) won.\n\n\"Fire in the Blood\" features contributions from former US President Bill Clinton, intellectual property activist James Love, global health reporter Donald McNeil, Jr. of \"The New York Times\", HIV/AIDS treatment activist Zackie Achmat, pioneering generic drugmaker Yusuf Hamied, former Pfizer executive-turned-whistleblower Peter Rost, Ugandan AIDS physician Peter Mugyenyi, and Nobel Prize-laureates Desmond Tutu and Joseph Stiglitz.\n\nThe film is narrated by Academy Award-winning actor William Hurt, who lent his voice to the film on a \"pro bono\" basis because he felt the story and subject matter were so important.\n\nIn November 2013, \"Fire in the Blood\" set a new all-time record for the longest theatrical run by any non-fiction feature film in Indian history (five weeks). It was also the first Indian non-fiction feature to be theatrically released in either the US or the UK.\n\nIn November 2018, legendary Australian journalist and documentary filmmaker John Pilger included \"Fire in the Blood\" among his selection of \"26 landmark documentary films of the past seven decades\".\n\nFilmmaker Dylan Mohan Gray first came to know of the issue in 2004, after he read an article in \"The Economist\" about the battle between pharmaceutical companies and the global public health community over access to lower-cost AIDS drugs for Africa. He decided to make the film three years later.\n\nThe film was shot on four continents from March 2008 to the end of 2010, while editing was completed in 2012.\n\nThe film was first released theatrically in Ireland on 21 February 2013, with the UK premiere the following day. It was released theatrically in the US on 7 September 2013 and in India on 11 October the same year, to outstanding reviews.<ref name=\"http://fireintheblood.com/reviews\">Official website: \"Reviews\" Re-linked 2014-11-01</ref>\n\nThe film was released on the iTunes UK Store in mid-2013, and worldwide on VOD via its website in 2014. It released in the UK and India on DVD in the first quarter of 2014.\n\nThus far the film has been broadcast on television in the following countries, beginning in late 2013: Finland (YLE), Israel (Yes (Israel)), Norway (NRK), Spain (TVE), Switzerland (SRF), Austria (ORF), Poland (Telewizja Polska), Ireland (TG4), Brazil (Globosat), Denmark (DR), the United States (Audience Network) and Japan (NHK). Transnational broadcasters include AJE, Al Jazeera Arabic and Al Jazeera Balkans, along with DStv and GOtv across Sub-Saharan Africa as part of the \"AfriDocs\" initiative.\n\n\"Fire in the Blood\" was the first Indian film to be selected for the World Cinema Documentary competition at the Sundance Film Festival and subsequently participated in over 100 leading film festivals in dozens of countries all over the world.\n\n\n\"Fire in the Blood\" received very positive critical notices, both to its North American premiere at the 2013 Sundance Film Festival, as well as to its subsequent theatrical releases in Ireland, Britain, the United States and India.\n\nThe influential film review aggregator website Rotten Tomatoes has given \"Fire in the Blood\" a \"92% fresh\" rating based on 24 reviews as of January 2014. This ranks the film within the 5-10% best-reviewed films of 2013.\n\nCertain critics, such as Gary Goldstein of the \"Los Angeles Times\", while acknowledging the enormous importance of the topic, felt the film should have taken a more emotional approach to its \"incendiary subject\". Many others, however, such as the legendary English critic Philip French, who in his review for \"The Observer\" described the film as \"quietly devastating\", praised Gray's choice in avoiding a polemical tone and allowing the material to speak for itself.\n\nWriting in \"Sight & Sound\", Ashley Clark called \"Fire in the Blood\" \"stirring\" and added \"Gray deserves credit for his own restraint... Such is the clarity of his ideological stance that any grandstanding would feel redundant.\"\n\nDavid Rooney of \"The Hollywood Reporter\" echoed this view, stating that \"the admirable balance between impassioned argument and clear-sighted reporting in Dylan Mohan Gray's chronicle of the why and how makes \"Fire in the Blood\" indispensable viewing\", adding that the \"very smart\", \"extremely moving\" film is \"a shocking account of international trade terrorism sanctioned by Western governments\" and \"a powerful documentary that demands to be seen by as wide an audience as possible.\"\n\nAuthor John le Carré (who became involved with the issue of pharmaceutical company abuses while researching his landmark 2001 novel \"The Constant Gardener\") called \"Fire in the Blood\" \"a blessing... full of conviction, passion and unanswerable argument\".\n\nAustralian-British journalist and documentary film maker, John Pilger wrote \"\"Fire in the Blood\" is one of the most powerful, important and humane documentaries I have ever seen. It's the story of ordinary people standing up to unaccountable power. The struggle to save millions from the ravages of untreated HIV is revealed as a struggle against the new lords of the world, transnational corporations, their greed and lies. Genuine hope is rare these days -- you'll find it in this film.\"\n\nFormer (2001–06) United Nations (UN) Special Envoy for HIV/AIDS in Africa Stephen Lewis said \"I was enraged as I watched, thinking of those years I spent as the Envoy, watching people die. [...] I rarely watch 'AIDS documentaries'; they're remarkably repetitive as a rule, largely uninspired and yielding almost nothing new. [Fire in the Blood] is in a wholly different category; a terrific, riveting documentary... dramatic, compelling, but most of all, wonderfully humane. [Gray is] a remarkably gifted documentary film-maker.\"\n\nSpecial screenings of \"Fire in the Blood\" for policymakers have been held in such fora as the United Nations headquarters in New York City and Vienna, the European Parliament in Brussels, the World Health Assembly and UNAIDS headquarters in Geneva, along with dedicated screenings for lawmakers in Washington and New Delhi, and one hosted by the Indian Ministry of External Affairs (MEA) for ambassadors and consuls accredited to India.\n\n\n"}
{"id": "401155", "url": "https://en.wikipedia.org/wiki?curid=401155", "title": "Franco Andrea Bonelli", "text": "Franco Andrea Bonelli\n\nFranco Andrea Bonelli (10 November 1784 – 18 November 1830) was an Italian ornithologist, entomologist and collector.\n\nVery little is known about the early life of Bonelli: he was born in Cuneo and was interested from an early age in the fauna which surrounded him, making collecting trips, preparing specimens and noting his observations.\n\nHe became a member of the Reale Società Agraria di Torino in 1807 when he presented his first studies relating to the Coleoptera of Piedmont. The high quality of these studies attracted the interest of the naturalists of his time.\n\nIn April 1810, George Vat was sent to Turin by the French government to reorganize the University of Turin and begin its fusion with the Impériale University founded by Napoleon. Vat was very impressed by Bonelli’s knowledge. Vat encouraged him to further his knowledge by coming to follow courses at the Natural History Museum in Paris. \nBonelli took this advice so as obtain a professor’s chair in the new university. In September 1810, he arrived in Paris.\n\nIn 1811, Bonelli was finally named professor of zoology at the University of Turin and keeper of the natural history museum of zoology. During his time at the university, he formed one of the largest ornithological collections in Europe.\n\nIn 1811, Bonelli wrote a \"Catalogue of the Birds of Piedmont\", in which he described 262 species. In 1815, he discovered the bird Bonelli's warbler (\"Phylloscopus bonelli\"), named by Louis Vieillot in 1819. In the same year, he discovered Bonelli's eagle (\"Hieraaetus fasciatus\") that was likewise named by Vieillot in 1822.\n\nThe successor of Bonelli at the Turin Museum was Carlo Giuseppe Gené.\n\nBonelli is most notable for his work on birds and on the beetle family Carabidae. Since he was an early worker on Coleoptera many of his genera later became Families, sub families and tribes. Also many of his genera survive.\n\nInstances are the:\n\n\n\nThe last two are founding works of entomology, introducing many new taxa.\n\n"}
{"id": "46729393", "url": "https://en.wikipedia.org/wiki?curid=46729393", "title": "Gibilmanna Observatory", "text": "Gibilmanna Observatory\n\nThe Gibilmanna Observatory is a research station used for a diverse range of studies set up and run by the Istituto Nazionale di Geofisica e Vulcanologia (INGV) and it is located on Cozzo Timpa Rossa at 1005 m.a.s.l. near Cefalù, a town in the district of Palermo, Italy.\n\nIt is one of the 120 stations of the Italian Magnetic Network measuring Earth magnetism field in Italy, and in 2005 INGV's Centro Nazionale Terremoti (CNT) selected it to set up the OBS Lab aimed at designing, manufacturing and managing the Ocean-Bottom Seismometer with Hydrophone (OBS/H), later deployed for monitoring the Marsili submarine volcano in the Tyrrhenian sea.\n\nIn addition, the observatory is one of the 110 Data Collection Platform (D.C.P.) stations of the Aeronautica Militare Italiana for automatic weather data gathering and transmission to the METEOSAT satellite.\nSince 1976 this station has also been equipped for Ionospheric research and it is the most southern station of its kind in Europe. Real time ionograms are recorded by the INGV own developed AIS-INGV ionosonde installed at the station and reported on the INGV ionospheric website.\n\n"}
{"id": "7214369", "url": "https://en.wikipedia.org/wiki?curid=7214369", "title": "Global distance test", "text": "Global distance test\n\nThe global distance test (GDT), also written as GDT TS to represent \"total score\", is a measure of similarity between two protein structures with identical amino acid sequences but different tertiary structures. It is most commonly used to compare the results of protein structure prediction to the experimentally determined structure as measured by X-ray crystallography or protein NMR. The metric is intended as a more accurate measurement than the more common RMSD metric, which is sensitive to outlier regions created by poor modeling of individual loop regions in a structure that is otherwise reasonably accurate. GDT_TS measurements are used as major assessment criteria in the production of results from the Critical Assessment of Structure Prediction (CASP), a large-scale experiment in the structure prediction community dedicated to assessing current modeling techniques and identifying their primary deficiencies. In general, the higher GDT_TS is, the better a given model is in comparison to reference structure.\n\nThe GDT score is calculated as the largest set of amino acid residues' alpha carbon atoms in the model structure falling within a defined distance cutoff of their position in the experimental structure. It is typical to calculate the GDT score under several cutoff distances, and scores generally increase with increasing cutoff. A plateau in this increase may indicate an extreme divergence between the experimental and predicted structures, such that no additional atoms are included in any cutoff of a reasonable distance.\n\nThe high accuracy version of the GDT measure is called GDT-HA. It uses smaller cut off distances (half the size of GDT_TS) and thus is more rigorous.\n\n\n"}
{"id": "2942638", "url": "https://en.wikipedia.org/wiki?curid=2942638", "title": "Gloss (optics)", "text": "Gloss (optics)\n\nGloss is an optical property which indicates how well a surface reflects light in a specular (mirror-like) direction. It is one of important parameters that are used to describe the visual appearance of an object. The factors that affect gloss are the refractive index of the material, the angle of incident light and the surface topography.\n\nApparent gloss depends on the amount of \"specular\" reflection – light reflected from the surface in an equal amount and the symmetrical angle to the one of incoming light – in comparison with \"diffuse\" reflection – the amount of light scattered into other directions.\n\nWhen light illuminates an object, it interacts with it in a number of ways:\n\n\nVariations in surface texture directly influence the level of specular reflection. Objects with a smooth surface, i.e. highly polished or containing coatings with finely dispersed pigments, appear shiny to the eye due to a large amount of light being reflected in a specular direction whilst rough surfaces reflect no specular light as the light is scattered in other directions and therefore appears dull. The image forming qualities of these surfaces are much lower making any reflections appear blurred and distorted.\n\nSubstrate material type also influences the gloss of a surface. Non-metallic materials, i.e. plastics etc. produce a higher level of reflected light when illuminated at a greater illumination angle due to light being absorbed into the material or being diffusely scattered depending on the colour of the material. Metals do not suffer from this effect producing higher amounts of reflection at any angle.\n\nThe Fresnel formula gives the specular reflectance, formula_1, for an unpolarized light of intensity formula_2, at angle of incidence formula_3, giving the intensity of specularly reflected beam of intensity formula_4, while the refractive index of the surface specimen is formula_5.\n\nThe Fresnel equation is given as follows : formula_6\n\nSurface roughness in micrometer range influences the specular reflectance levels. The diagram on the right depicts the reflection at an angle formula_3 on a rough surface with a characteristic roughness height formula_9. The path difference between rays reflected from the top and bottom of the surface bumps is:\n\nWhen the wavelength of the light is formula_11, the phase difference will be:\n\nIf formula_13 is small, the two beams (see Figure 1) are nearly in phase and therefore the specimen surface can be considered smooth. But when formula_14, then beams are not in phase and through interference, cancellation of each other will occur. Low intensity of specularly reflected light means the surface is rough and it scatters the light in other directions. If an arbitrary criterion for smooth surface is formula_15, then substitution into the equation above will produce:\n\nThis smooth surface condition is known as the Rayleigh criterion.\n\nThe earliest studies of gloss perception are attributed to Ingersoll who in 1914 examined the effect of gloss on paper. By quantitatively measuring gloss using instrumentation Ingersoll based his research around the theory that light is polarised in specular reflection whereas diffusely reflected light is non-polarized. The Ingersoll “glarimeter” had a specular geometry with incident and viewing angles at 57.5°. Using this configuration gloss was measured using a contrast method which subtracted the specular component from the total reflectance using a polarizing filter.\n\nIn the 1930s work by A. H. Pfund, suggested that although specular shininess is the basic (objective) evidence of gloss, actual surface glossy appearance (subjective) relates to the contrast between specular shininess and the diffuse light of the surrounding surface area (now called “contrast gloss” or “luster”).\n\nIf black and white surfaces of the same shininess are visually compared, the black surface will always appear glossier because of the greater contrast between the specular highlight and the black surroundings as compared to that with white surface and surroundings. Pfund was also the first to suggest that more than one method was needed to analyze gloss correctly.\n\nIn 1937 Hunter, as part of his research paper on gloss, described six different visual criteria attributed to apparent gloss. The following diagrams show the relationships between an incident beam of light, I, a specularly reflected beam, S, a diffusely reflected beam, D and a near-specularly reflected beam, B. \n\n\nDefined as the ratio of the light reflected from a surface at an equal but opposite angle to that incident on the surface.\n\n\nDefined as the gloss at grazing angles of incidence and viewing\n\n\nDefined as the ratio of the specularly reflected light to that diffusely reflected normal to the surface;\n\n\nDefined as a measure of the absence of haze or a milky appearance adjacent to the specularly reflected light: haze is the inverse of absence-of-bloom\n\n\nDefined as the sharpness of the specularly reflected light\n\n\nDefined as the uniformity of the surface in terms of visible texture and defects (orange peel, scratches, inclusions etc.)\n\nA surface can therefore appear very shiny if it has a well-defined specular reflectance at the specular angle. The perception of an image reflected in the surface can be degraded by appearing unsharp, or by appearing to be of low contrast. The former is characterised by the measurement of the distinctness-of-image and the latter by the haze or contrast gloss.\n\nIn his paper Hunter also noted the importance of three main factors in the measurement of gloss:\n\nFor his research he used a glossmeter with a specular angle of 45° as did most of the first photoelectric methods of that type, later studies however by Hunter and Judd in 1939, on a larger number of painted samples, concluded that the 60 degree geometry was the best angle to use so as to provide the closest correlation to a visual observation.\n\nStandardisation in gloss measurement was led by Hunter and ASTM (American Society for Testing and Materials) who produced ASTM D523 Standard test method for specular gloss in 1939. This incorporated a method for measuring gloss at a specular angle of 60°. Later editions of the Standard (1951) included methods for measuring at 20° for evaluating high gloss finishes, developed at the DuPont Company (Horning and Morse, 1947) and 85° (matte, or low, gloss).\nASTM has a number of other gloss-related standards designed for application in specific industries including the old 45° method which is used primarily now used for glazed ceramics, polyethylene and other plastic films. \n\nIn 1937, the paper industry adopted a 75° specular-gloss method because the angle gave the best separation of coated book papers. This method was adopted in 1951 by the Technical Association of Pulp and Paper Industries as TAPPI Method T480.\n\nIn the paint industry, measurements of the specular gloss are made according to International Standard ISO 2813 (BS 3900, Part 5, UK; DIN 67530, Germany; NFT 30-064, France; AS 1580, Australia; JIS Z8741, Japan, are also equivalent). This standard is essentially the same as ASTM D523 although differently drafted. \n\nStudies of polished metal surfaces and anodised aluminium automotive trim in the 1960s by Tingle, Potter and George led to the standardisation of gloss measurement of high gloss surfaces by goniophotometry under the designation ASTM E430. In this standard it also defined methods for the measurement of distinctness of image gloss and reflection haze.\n\n\n"}
{"id": "26525006", "url": "https://en.wikipedia.org/wiki?curid=26525006", "title": "Gozo Nature Museum", "text": "Gozo Nature Museum\n\nThe Gozo Nature Museum, formerly known as the Natural Science Museum, is a museum in Victoria, on the island of Gozo, Malta. It has been open to public since 1991. It is housed in a group of houses within the Cittadella, the oldest part of the city. These houses date back to various ages: the older one, which was an inn, to 1495; the other to the 17th century.\n\nThe Natural Science Museum shows collections \"relating to the Island’s geology, minerals, marine life, insects, local habitats and ecosystems\" as well as national plants (including the Maltese Rock Centaury), human and animal evolution.\nDuring later years, this building was used as an inn for visitors, and is mentioned in Thomas McGill’s “Handbook, or Guide, for Strangers visiting Malta” of 1839, and described as \nan excellent house of entertainment offering clean and comfortable beds and reasonably-priced dinners.\n\nDuring World War II the building served as a shelter for families who sought refuge during aerial bombings.\n\nThe buildings which house the museum are listed on the National Inventory of the Cultural Property of the Maltese Islands.\n\n\n"}
{"id": "55556786", "url": "https://en.wikipedia.org/wiki?curid=55556786", "title": "Hans Stubbe", "text": "Hans Stubbe\n\nHans Karl Oskar Stubbe (7 March 1902 - 14 May 1989) was a German agronomist and plant breeder. During the Second World War he was dismissed by the Nazi government from the Kaiser Wilhelm Institute for Breeding Research in Müncheberg in 1936. After the war he went to work in East Germany where he was the founding director of the Institute for Cultivated Plant Research (which started as the Institut für Kulturpflanzenforschung in Vienna) in Gatersleben. He stood up against the ideas of Trofim Lysenko and prevented East German genetics from being influenced by politics that had caused damage in the Soviet Union. \n\nStubbe was born in 1902 at Berlin where his father was a school inspector. He studied agriculture and biology at the University of Göttingen and the Agricultural University of Berlin. He became a student of Erwin Baur at the Institute for Inheritance Research in Berlin where he worked on a doctoral thesis on mutagenesis in 1929. He then joined the newly established Kaiser Wilhelm Institute for Breeding Research in Müncheberg but after about nine years he was dismissed when the Nazi party came into power. For a while he worked with Fritz von Wettstein at the Institut für Kulturpflanzenforschung in Vienna. Wettstein wished to recruit Elisabeth Schiemann to head the institute but Stubbe objected to the idea of men working under her. He made major expeditions to collect germplasm of wild and cultivated plants from around Europe both with civilian and military objectives. Stubbe worked on using X-rays to produce useful mutations in barley. Along with Gustav Becker and Kurt Mothes, Stubbe ensured that Lysenkoism did not take root in East Germany.\n\nDespite his anti-fascist views, Stubbe defended his friend Günther Niethammer and wrote a letter in 1947 exonerating the latter of any wilful participation with the Nazis at Auschwitz. After the Second World War Stubbe became director of the Institute for Cultivated Plant Research in Gatersleben. Stubbe died in 1989 in Zingst. He was succeeded at Gatersleben by his student Helmut Böhme.\n"}
{"id": "45224102", "url": "https://en.wikipedia.org/wiki?curid=45224102", "title": "Hooke (Martian crater)", "text": "Hooke (Martian crater)\n\nHooke Crater is an impact crater in the Argyre quadrangle on Mars at 45.2°S and 44.4°W and is 139.0 km in diameter and is in the north of Argyre Planitia lying next to the Nereidum Montes. Its name was approved in 1973, and it was named after Robert Hooke. The crater is the deepest point in the Argyre Plantia and in the Argyre quadrangle.\n\nNearby prominent crater is Galle to the southeast and further west is Halley. Surrounding Hooke are smaller named craters on the rim including Yungay to the north, Podor in the northeast and Wau to the east. Touching the northwest rim is Taza and the east rim is Camiri. NNE is the smaller Rengo. Other nearby minor craters include Luga and the tiny Gandu to the west and further southwest is Cypress.\n\nSome of the dunes have gullies on them. While these gullies may be a bit different then ones found on crater walls and other steep slopes, they have been thought by some to be caused by flowing water.\n\n"}
{"id": "22031754", "url": "https://en.wikipedia.org/wiki?curid=22031754", "title": "Intermediated research", "text": "Intermediated research\n\nIn finance, intermediated research is a type of fundamental analysis or investment analysis of a business to establish its value for investors that attempts to avoid commercial pressure or influence.\n\nMany banks and brokers provide research to the investment community, however this form of research suffers from a real or perceived lack of objectivity. For example, this research may be subject to influence from within a bank from, say, investment bankers keen to win company’s IPO mandate.\n\nAn alternative form is known as \"independent research\" or \"alternative research\", this is research that is that is not provided by a bank or broker. However, someone has to fund the costs of conducting the research, and when the funder is the company being researched, this form of independent research is termed “sponsored” or “company-paid” research. Unfortunately, with the subject company paying directly for these services, this form of research suffers from a real or perceived lack of objectivity, which reduces its value to investors and hence to the companies themselves.\n\nIntermediated research improves significantly upon the sponsored research model, by introducing important safeguards into the commercial and research process framework. The key structure is that the company has no choice in its allocation to a research provider; once matched with the research provider, the company is under a long term contract in order to minimize commercial pressure or influence; and the fees paid to the research provider are not sufficiently material for the company to be able to exert any influence over the content or conclusions of the research report. Collectively, these measures enhance the value of the report to investors and consequently, to the company itself.\n\nThe intermediated research framework includes the further protections; ensuring that the independent research firms involved have no investment banking or brokerage operations in their business models; conduct no traditional company-paid research; deliver a standard template for analysis of every company so that all standard topics are covered; and avoiding ratings and target prices in the reports, since these can risk becoming a focus of pressure exerted on the research provider by the subject company.\n"}
{"id": "7898478", "url": "https://en.wikipedia.org/wiki?curid=7898478", "title": "Jamal ad-Din (astronomer)", "text": "Jamal ad-Din (astronomer)\n\nJamal ad-Din Muḥammad ibn Ṭāhir ibn Muḥammad al‐Zaydī al‐Bukhārī (variously transcribed Jamal ud-Din, Jamal al-Din ( Beauty of Faith), etc., Chinese name Zhamaluding) was a 13th-century Persian astronomer. Originally from Bukhara, he entered the service of Kublai Khan around the 1250s to set up an Islamic Astronomical Bureau in his new capital Beijing, to operate in parallel with the traditional Chinese bureau. Kublai Khan thus maintained the bureaucratic structure, but allowed Chinese observations and predictions to be checked by respected Muslim scholars.\n\nHe is credited with having taken seven astronomical instruments to Kublai Khan, as a present from Hulagu Khan including a Persian astrolabe, a globe and an armillary sphere, in 1267. This is the earliest known reference to a spherical terrestrial globe of the Earth in Chinese astronomy.\n\nHe is associated with a zij in Persian which has been lost but was translated into Chinese in 1383 by Ma‐shayihei with the title \"Huihuilifa\" (Islamic calendar). This contained Ptolomaic tables based on new values and adjusted to Beijing and has been reconstructed in recent years.\n\nIn general, his activity didn't make much difference to Chinese astronomy. However Guo Shoujing did evidently gain the idea of the torquetum from him (which he didn't bring), and produced a simplified version which omitted ecliptic coordinates which were not used in China.\n\nIn 1286 he carried out a large-scale survey of the Yuan empire which was produced in 755 volumes as the \"Dayitongzh\". All but the introduction of this has been lost.\n\n\n"}
{"id": "3694890", "url": "https://en.wikipedia.org/wiki?curid=3694890", "title": "Jörgen Lehmann", "text": "Jörgen Lehmann\n\nJörgen Erik Lehmann (15 January 1898 – 26 December 1989) was a Danish-born Swedish physician and chemist best known for his discovery in the 1940s that para-amino salicylic acid (PAS) would make an excellent orally-available tuberculosis therapy. PAS was, together with streptomycin, the first efficacious anti-microbial therapy for tuberculosis and remained in clinical use for several decades. In 1941, Lehmann also developed the anti-coagulant dicumarol, which is used for the prevention of blood clots and in the treatment of deep venous thrombosis.\n\nLehmann studied under Torsten Thunberg, professor of physiology in Lund, who discovered the dehydrogenases. Lehmann was appointed professor of physiology in Aarhus in 1937, and became head of the central laboratory at the Sahlgrenska University Hospital in Gothenburg 1938. After retiring in 1963, Lehmann continued his research at the Nobel Laureate Arvid Carlsson's institution at the University of Gothenburg.\n\nJörgen Lehmann was son of Edvard Lehmann, professor of History of Religions at Lund University and grandnephew of the Danish politician Orla Lehmann.\n\n"}
{"id": "29407462", "url": "https://en.wikipedia.org/wiki?curid=29407462", "title": "Körber European Science Prize", "text": "Körber European Science Prize\n\nThe Körber European Science Prize is presented annually by the Körber Foundation in Hamburg honoring outstanding scientists working in Europe for their promising research projects. The 750,000 euro prize promotes research projects in the life sciences and physical sciences.\n\nThe prize was initiated by the entrepreneur Kurt A. Körber with the help of Reimar Lüst, the president of the Max Planck Society. The first award was in 1985. At first, European research teams were honored, but since 2005, only individuals qualify.\n\nCandidates for the prize need not be from Europe, but they must be living in Europe. Renowned scientists from all over Europe, grouped into two Search Committees, select promising candidates. The awards are annual and alternate between the life and physical sciences. Those who are shortlisted are then asked to submit a detailed proposal for a research project which is then judged in two rounds of assessment by the Search Committee. The work of the Search Committee is supported by international experts. A maximum of five candidates are subsequently recommended to the Trustee Committee which, based on a summary of expert assessments, previous publications and scientific career history, decides on the new prizewinner. A personal application is not allowed.\n\nAll prizewinners receive a certificate and 750,000 euro prize money. The prizewinners can keep 10 percent of the money for themselves and must spend the rest on research in Europe in three to five years. Aside from these restrictions they alone can decide how to use the money.\n\nThe prize is presented every year in the Great Hall of Hamburg City Hall in the presence of the Mayor of the Free and Hanseatic City of Hamburg and 600 guests from science, industry, politics, and society.\n\n\n"}
{"id": "3579261", "url": "https://en.wikipedia.org/wiki?curid=3579261", "title": "Laurance Doyle", "text": "Laurance Doyle\n\nLaurance R. Doyle (born 1953) is an American scientist who received his Ph.D. from the Ruprecht Karl University of Heidelberg. He has worked at the SETI Institute since 1987 where he is a principal investigator and astrophysicist. His main area of study has been the formation and detection of extrasolar planets, but he has also worked on communications theory. In particular he has written on how patterns in animal communication relate to humans with an emphasis on cetaceans.\n\nHe grew up on a dairy farm in Cambria, California and therefore, didn't have much access to information about stars. But by reading books at the local library, Doyle was able to develop his knowledge in astronomy, and eventually obtain his Bachelor's and Master's of Science degrees in astronomy from San Diego State University.\n\nHis first job was at the Jet Propulsion Laboratory as an imaging engineer, where he was in charge of analyzing pictures of Jupiter and Saturn sent from the spacecraft Voyager. He moved to Heidelberg, Germany, to help analyze images of Halley's Comet. He got his doctorate in Astrophysics at the University of Heidelberg.\n\nIn May 2005, he appeared on a National Geographic Channel special titled \"Extraterrestrial.\" He also appeared in the episode \"Will We Survive First Contact,\" of The Science Channel series titled \"Morgan Freeman's Through the Wormhole\".\n\nDoyle is currently seeking to compare dolphin whistles and baby babble in an attempt to make predictions about extraterrestrial communications. He believes that by measuring the complexity of communications for different species on Earth, we could get a good indication of how advanced an extraterrestrial signal is. His study determined that babies babble over 800 different sounds with the same amount of frequency as dolphins. As they grow older, those sounds decrease to around 50 and become more repetitious. The study found that baby dolphins develop similarly in regards to their whistling.\n\nDoyle is faculty at Principia College and the founding Director of Principia College's Institute for the Metaphysics of Physics, founded in 2014.\n\n"}
{"id": "34969080", "url": "https://en.wikipedia.org/wiki?curid=34969080", "title": "List of 21st-century Canadian tornadoes and tornado outbreaks", "text": "List of 21st-century Canadian tornadoes and tornado outbreaks\n\nThis is a list of notable tornadoes, tornado outbreaks, and tornado outbreak sequences that have occurred in Canada in the 21st century. (2001 through 2100). Due to increasing detection, particularly in the US and southern Canada, numbers of counted tornadoes have increased markedly in recent decades although number of actual tornadoes and counted significant tornadoes has not. In older events, the number of tornadoes officially counted is likely underestimated.\n\nOn average, there are around 80 confirmed and unconfirmed tornadoes that touch down in Canada each year, with most occurring in Southern Ontario, the southern Canadian Prairies and southern Quebec. Canada ranks as the second country in the world with the most tornadoes per year, after the US. The most common types are F0 to F2 in damage intensity level and usually result in minor structural damage to barns, wood fences, roof shingles, chimneys, uprooted or snapped tree limbs and downed power lines. Fewer than 5% of tornadoes in Canada are rated F3 or higher in intensity, where wind speeds are in excess of . Prior to April 1, 2013, Canada used a slightly modified Fujita scale, and as of that date the Enhanced Fujita scale, again slightly modified, was put into use to rate tornado intensity, based on the damage to buildings and vegetation.\n\nOntario, Alberta, Manitoba and Saskatchewan all average 15 tornadoes per season, followed by Quebec with fewer than 10. New Brunswick and the British Columbia Interior are also recognized tornado zones. All other provinces and territories have significantly less threat from tornadoes. The peak season in Canada is in the summer months when clashing air masses move north, as opposed to the spring season in the United States southern-central plains, although tornadoes in Canada have occurred in spring, fall and very rarely winter.\n\nThe reported increase in numbers of tornadoes in recent years may reflect more reporting by citizens and media involvement rather than an actual increase in tornado occurrence (although some natural increase has not been ruled out), in addition to better detection technology i.e. Doppler weather radar and satellite imagery. The upswing could also be attributed to other factors, such as improved aerial and ground damage assessment after the fact in sparsely populated areas (particularly the case in remote parts of the Canadian Prairies and Northern Ontario, for example), better trained spotter capabilities and increased use of digital recording devices by citizens. Tornadoes in Canada are enough of a threat for a public warning system to be in place, overseen by the national weather agency, Environment Canada (EC).\n\nFor a variety of reasons, such as Canada's lower population density and generally stronger housing construction due to the colder climate, Canadian tornadoes have historically caused far fewer fatalities than tornadoes in the United States. The deadliest tornado in Canadian history, the Regina Cyclone of June 30, 1912, does not even rank in the top 25 when compared to American tornado fatalities. Urban centres are not immune from the threat of severe tornadoes. Nine medium to large size Canadian cities have been hit by significant strength tornadoes (F3 or higher), which caused large-scale damage and fatalities: Regina (1912); Windsor (1946 and 1974); Sarnia (1953); Sudbury (1970); Woodstock (1979); London (1984); Barrie (1985); Edmonton (1987); Goderich (2011); and Ottawa-Gatineau (2018).\n\nAll figures for damages are in Canadian dollars.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "15891768", "url": "https://en.wikipedia.org/wiki?curid=15891768", "title": "List of New Zealand scientists", "text": "List of New Zealand scientists\n\nThis page is a list of New Zealand scientists with articles on Wikipedia and is necessarily incomplete.\n\n\n"}
{"id": "21185204", "url": "https://en.wikipedia.org/wiki?curid=21185204", "title": "List of distributed computing conferences", "text": "List of distributed computing conferences\n\nThis is a selected list of international academic conferences in the fields of distributed computing, parallel computing, and concurrent computing.\n\nThe conferences listed here are major conferences of the area; they have been selected using the following criteria:-\nFor the first criterion, references are provided; criteria 2–3 are usually clear from the name of the conference.\n\n\n\n"}
{"id": "17816236", "url": "https://en.wikipedia.org/wiki?curid=17816236", "title": "List of people considered father or mother of a scientific field", "text": "List of people considered father or mother of a scientific field\n\nThe following is a list of people who are considered a \"father\" or \"mother\" (or \"founding father\" or \"founding mother\") of a scientific field. Such people are generally regarded to have made the first significant contributions to and/or delineation of that field; they may also be seen as \"\"a\" rather than \"the\"\" father or mother of the field. Debate over who merits the title can be perennial. As regards science itself, the title has been bestowed on the ancient Greek philosophers Thaleswho attempted to explain natural phenomena without recourse to mythologyand Democritus, the atomist..\n\n!width=\"25%\"| Field\n!width=\"32%\"| Person/s\n!width=\"43%\"| Rationale\n</tr>\n"}
{"id": "56312951", "url": "https://en.wikipedia.org/wiki?curid=56312951", "title": "List of personal finance software", "text": "List of personal finance software\n\nThis is a list of personal financial management software. The first section is devoted to free and open-source software, and the second is for proprietary software.\n\n"}
{"id": "1385393", "url": "https://en.wikipedia.org/wiki?curid=1385393", "title": "Long tail", "text": "Long tail\n\nIn statistics and business, a long tail of some distributions of numbers is the portion of the distribution having a large number of occurrences far from the \"head\" or central part of the distribution. The distribution could involve popularities, random numbers of occurrences of events with various probabilities, etc. The term is often used loosely, with no definition or arbitrary definition, but precise definitions are possible.\n\nIn statistics, the term \"long-tailed distribution\" has a narrow technical meaning, and is a subtype of heavy-tailed distribution. Intuitively, a distribution is (right) long-tailed if, for any fixed amount, when a quantity exceeds a high level, it almost certainly exceeds it by at least that amount: large quantities are probably even larger. Note that there is no sense of \"the\" \"long tail\" of a distribution, but only the \"property\" of a distribution being long-tailed.\n\nIn business, the term \"long tail\" is applied to rank-size distributions or rank-frequency distributions (primarily of popularity), which often form power laws and are thus long-tailed distributions in the statistical sense. This is used to describe the retailing strategy of selling a large number of unique items with relatively small quantities sold of each (the \"long tail\")—usually in addition to selling fewer popular items in large quantities (the \"head\"). Sometimes an intermediate category is also included, variously called the \"body\", \"belly\", \"torso\", or \"middle\". The specific cutoff of what part of a distribution is \"the\" \"long tail\" is often arbitrary, but in some cases may be specified objectively; see segmentation of rank-size distributions.\n\nThe long tail concept has found some ground for application, research, and experimentation. It is a term used in online business, mass media, micro-finance (Grameen Bank, for example), user-driven innovation (Eric von Hippel), knowledge management, and social network mechanisms (e.g. crowdsourcing, crowdcasting, peer-to-peer), economic models, marketing (viral marketing), and IT Security threat hunting within a SOC (Information security operations center).\n\nA frequency distribution with a long tail has been studied by statisticians since at least 1946. The term has also been used in the finance and insurance business for many years. The work of Benoît Mandelbrot in the 1950s and later has led to him being referred to as \"the father of long tails\".\n\nThe long tail was popularized by Chris Anderson in an October 2004 \"Wired\" magazine article, in which he mentioned Amazon.com, Apple and Yahoo! as examples of businesses applying this strategy. Anderson elaborated the concept in his book \"\".\n\nThe distribution and inventory costs of businesses successfully applying a long tail strategy allow them to realize significant profit out of selling small volumes of hard-to-find items to many customers instead of only selling large volumes of a reduced number of popular items. The total sales of this large number of \"non-hit items\" is called \"the long tail\".\n\nGiven enough choice, a large population of customers, and negligible stocking and distribution costs, the selection and buying pattern of the population results in the demand across products having a power law distribution or Pareto distribution.\nIt is important to understand why some distributions are normal vs. long tail (power) distributions. Chris Anderson argues that while quantities such as human height or IQ follow a normal distribution, in scale-free networks with preferential attachments, power law distributions are created, i.e. because some nodes are more connected than others (like Malcolm Gladwell’s “mavens” in \"The Tipping Point\").\n\n\"The long tail\" is the name for a long-known feature of some statistical distributions (such as Zipf, power laws, Pareto distributions and general Lévy distributions). In \"long-tailed\" distributions a high-frequency or high-amplitude population is followed by a low-frequency or low-amplitude population which gradually \"tails off\" asymptotically. The events at the far end of the tail have a very low probability of occurrence.\n\nAs a rule of thumb, for such population distributions the majority of occurrences (more than half, and where the Pareto principle applies, 80%) are accounted for by the first 20% of items in the distribution. What is unusual about a long-tailed distribution is that the most frequently occurring 20% of items represent less than 50% of occurrences; or in other words, the least frequently occurring 80% of items are more important as a proportion of the total population.\n\nPower law distributions or functions characterize an important number of behaviors from nature and human endeavor. This fact has given rise to a keen scientific and social interest in such distributions, and the relationships that create them. The observation of such a distribution often points to specific kinds of mechanisms, and can often indicate a deep connection with other, seemingly unrelated systems. Examples of behaviors that exhibit long-tailed distribution are the occurrence of certain words in a given language, the income distribution of a business or the intensity of earthquakes (see: Gutenberg–Richter law).\n\nChris Anderson's and Clay Shirky's articles highlight special cases in which we are able to modify the underlying relationships and evaluate the impact on the frequency of events. In those cases the infrequent, low-amplitude (or low-revenue) events – the long tail, represented here by the portion of the curve to the right of the 20th percentile – can become the largest area under the line. This suggests that a variation of one mechanism (internet access) or relationship (the cost of storage) can significantly shift the frequency of occurrence of certain events in the distribution. The shift has a crucial effect in probability and in the customer demographics of businesses like mass media and online sellers.\n\nHowever, the long tails characterizing distributions such as the Gutenberg–Richter law or the words-occurrence Zipf's law, and those highlighted by Anderson and Shirky are of very different, if not opposite, nature: Anderson and Shirky refer to frequency-rank relations, whereas the Gutenberg–Richter law and the Zipf's law are probability distributions. Therefore, in these latter cases \"tails\" correspond to large-intensity events such as large earthquakes and most popular words, who dominate the distributions. By contrast, the long tails in the frequency-rank plots highlighted by Anderson and Shirky would rather correspond to short tails in the associated probability distributions, and therefore illustrate an opposite phenomenon compared to the Gutenberg–Richter and the Zipf's laws.\n\nUse of the phrase \"the long tail\" in business as \"the notion of looking at the tail itself as a new market\" of consumers was first coined by Chris Anderson. The concept drew in part from a February 2003 essay by Clay Shirky, \"Power Laws, Weblogs and Inequality\", which noted that a relative handful of weblogs have many links going into them but \"the long tail\" of millions of weblogs may have only a handful of links going into them. Anderson described the effects of the long tail on current and future business models beginning with a series of speeches in early 2004 and with the publication of a \"Wired\" magazine article in October 2004. Anderson later extended it into the book \"The Long Tail: Why the Future of Business is Selling Less of More\" (2006).\n\nAnderson argues that products in low demand or that have a low sales volume can collectively make up a market share that rivals or exceeds the relatively few current bestsellers and blockbusters, if the store or distribution channel is large enough. Anderson cites earlier research by Erik Brynjolfsson, Yu (Jeffrey) Hu, and Michael D. Smith, that showed that a significant portion of Amazon.com's sales come from obscure books that are not available in brick-and-mortar stores. The long tail is a potential market and, as the examples illustrate, the distribution and sales channel opportunities created by the Internet often enable businesses to tap that market successfully.\n\nIn his Wired article Anderson opens with an anecdote about creating a niche market for books on Amazon. He writes about a book titled Touching the Void about a near-death mountain climbing accident that took place in the Peruvian Andes. Anderson states the book got good reviews, but didn't have much commercial success. However, ten years later a book titled Into Thin Air by Jon Krakauer was published and Touching the Void began to sell again. Anderson realized that this was due to Amazon's recommendations. This created a niche market for those who enjoy books about mountain climbing even though it is not considered a popular genre supporting the long tail theory.\n\nAn Amazon employee described the long tail as follows: \"We sold more books today that didn't sell at all yesterday than we sold today of all the books that did sell yesterday.\"\n\nAnderson has explained the term as a reference to the tail of a demand curve. The term has since been \"re\"derived from an XY graph that is created when charting popularity to inventory. In the graph shown above, Amazon's book sales would be represented along the vertical axis, while the book or movie ranks are along the horizontal axis. The total volume of low popularity items exceeds the volume of high popularity items.\n\nIn his Wired article, Chris Anderson cites earlier research by Erik Brynjolfsson, Yu (Jeffrey) Hu, and Michael D. Smith, who first used a log-linear curve on an XY graph to describe the relationship between Amazon.com sales and sales ranking. They found that a large proportion of Amazon.com's book sales come from obscure books that were not available in brick-and-mortar stores. They then quantified the potential value of the long tail to consumers. In an article published in 2003, these authors showed that, while most of the discussion about the value of the Internet to consumers has revolved around lower prices, consumer benefit (a.k.a. consumer surplus) from access to increased product variety in online book stores is ten times larger than their benefit from access to lower prices online. Thus, the primary value of the internet to consumers comes from releasing new sources of value by providing access to products in the long tail.\n\nA study by Erik Brynjolfsson, Yu (Jeffrey) Hu, and Michael D. Smith finds that the long tail has grown longer over time, with niche books accounting for a larger share of total sales. Their analyses suggested that by 2008, niche books accounted for 36.7% of Amazon's sales while the consumer surplus generated by niche books has increased at least fivefold from 2000 to 2008. In addition, their new methodology finds that, while the widely used power laws are a good first approximation for the rank-sales relationship, the slope may not be constant for all book ranks, with the slope becoming progressively steeper for more obscure books.\n\nIn support of their findings, Wenqi Zhou and Wenjing Duan not only find a longer tail but also a fatter tail by an in-depth analysis on consumer software downloading pattern in their paper \"Online user reviews, product variety, and the long tail\". The demand for all products decreases, but the decrease for the hits is more pronounced, indicating the demand shifting from the hits to the niches over time. In addition, they also observe a superstar effect in the presence of the long tail. A small number of very popular products still dominates the demand.\n\nIn a 2006 working paper titled \"Goodbye Pareto Principle, Hello Long Tail\", Erik Brynjolfsson, Yu (Jeffrey) Hu, and Duncan Simester found that, by greatly lowering search costs, information technology in general and Internet markets in particular could substantially increase the collective share of hard-to-find products, thereby creating a longer tail in the distribution of sales.\n\nThey used a theoretical model to show how a reduction in search costs will affect the concentration in product sales. By analyzing data collected from a multi-channel retailing company, they showed empirical evidence that the Internet channel exhibits a significantly less concentrated sales distribution, when compared with traditional channels. An 80/20 rule fits the distribution of product sales in the catalog channel quite well, but in the Internet channel, this rule needs to be modified to a 72/28 rule in order to fit the distribution of product sales in that channel. The difference in the sales distribution is highly significant, even after controlling for consumer differences.\n\nThe key supply-side factor that determines whether a sales distribution has a long tail is the cost of inventory storage and distribution. Where inventory storage and distribution costs are insignificant, it becomes economically viable to sell relatively unpopular products; however, when storage and distribution costs are high, only the most popular products can be sold. For example, a traditional movie rental store has limited shelf space, which it pays for in the form of building overhead; to maximize its profits, it must stock only the most popular movies to ensure that no shelf space is wasted. Because online video rental provider (such as Amazon.com or Netflix) stocks movies in centralized warehouses, its storage costs are far lower and its distribution costs are the same for a popular or unpopular movie. It is therefore able to build a viable business stocking a far wider range of movies than a traditional movie rental store. Those economics of storage and distribution then enable the advantageous use of the long tail: for example, Netflix finds that in aggregate, \"unpopular\" movies are rented more than popular movies.\n\nAn \"MIT Sloan Management Review\" article titled \"From Niches to Riches: Anatomy of the Long Tail\" examined the long tail from both the supply side and the demand side and identifies several key drivers. On the supply side, the authors point out how e-tailers' expanded, centralized warehousing allows for more offerings, thus making it possible for them to cater to more varied tastes.\n\nOn the demand side, tools such as search engines, recommendation software, and sampling tools are allowing customers to find products outside their geographic area. The authors also look toward the future to discuss second-order, amplified effects of Long Tail, including the growth of markets serving smaller niches.\n\nNot all recommender systems are equal, however, when it comes to expanding the long tail. Some recommenders (i.e. certain collaborative filters) can exhibit a bias toward popular products, creating positive feedback, and actually reduce the long tail. A Wharton study details this phenomenon along with several ideas that may promote the long tail and greater diversity.\n\nA recent study conducted by Wenqi Zhou and Wenjing Duan further points out that the demand side factor (online user reviews) and the supply side factor (product variety) interplay to influence the long tail formation of user choices. Consumers' reliance on online user reviews to choose products is significantly influenced by the quantity of products available. Specifically, they find that the impacts of both positive and negative user reviews are weakened as product variety goes up. In addition, the increase in product variety reduces the impact of user reviews on popular products more than it does on niche products.\n\nThe \"crowds\" of customers, users and small companies that inhabit the long-tail distribution can perform collaborative and assignment work. Some relevant forms of these new production models are:\n\nThe demand-side factors that lead to the long tail can be amplified by the \"networks of products\" which are created by hyperlinked recommendations across products. An MIS Quarterly article by Gal Oestreicher-Singer and Arun Sundararajan shows that categories of books on Amazon.com which are more central and thus influenced more by their recommendation network have significantly more pronounced long-tail distributions. Their data across 200 subject areas shows that a doubling of this influence leads to a 50% increase in revenues from the least popular one-fifth of books.\n\nThe long-tail distribution applies at a given point in time, but over time the relative popularity of the sales of the individual products will change. Although the distribution of sales may appear to be similar over time, the positions of the individual items within it will vary. For example, new items constantly enter most fashion markets. A recent fashion-based model of consumer choice, which is capable of generating power law distributions of sales similar to those observed in practice, takes into account turnover in the relative sales of a given set of items, as well as innovation, in the sense that entirely new items become offered for sale.\n\nThere may be an optimal inventory size, given the balance between sales and the cost of keeping up with the turnover. An analysis based on this pure fashion model indicates that, even for digital retailers, the optimal inventory may in many cases be less than the millions of items that they can potentially offer. In other words, by proceeding further and further into the long tail, sales may become so small that the marginal cost of tracking them in rank order, even at a digital scale, might be optimised well before a million titles, and certainly before infinite titles. This model can provide further predictions into markets with long-tail distribution, such as the basis for a model for optimizing the number of each individual item ordered, given its current sales rank and the total number of different titles stocked.\n\nBefore a long tail works, only the most popular products are generally offered. When the cost of inventory storage and distribution fall, a wide range of products become available. This can, in turn, have the effect of reducing demand for the most popular products. For example, a small website that focuses on niches of content can be threatened by a larger website which has a variety of information (such as Yahoo) Web content. The big website covers more variety while the small website has only a few niches to choose from.\n\nThe competitive threat from these niche sites is reduced by the cost of establishing and maintaining them and the effort required for readers to track multiple small web sites. These factors have been transformed by easy and cheap web site software and the spread of RSS. Similarly, mass-market distributors like Blockbuster may be threatened by distributors like LoveFilm, which supply the titles that Blockbuster doesn't offer because they are not already very popular.\n\nSome of the most successful Internet businesses have used the long tail as part of their business strategy. Examples include eBay (auctions), Yahoo! and Google (web search), Amazon (retail), and iTunes Store (music and podcasts), amongst the major companies, along with smaller Internet companies like Audible (audio books) and LoveFilm (video rental). These purely digital retailers also have almost no marginal cost, which is benefiting the online services, unlike physical retailers that have fixed limits on their products. The internet can still sell physical goods, but at an unlimited selection and with reviews and recommendations. The internet has opened up larger territories to sell and provide its products without being confined to just the \"local Markets\" such as physical retailers like Target or even Walmart. With the digital and hybrid retailers there is no longer a perimeter on market demands.\n\nThe adoption of video games and massively multiplayer online games such as Second Life as tools for education and training is starting to show a long-tailed pattern. It costs significantly less to modify a game than it has been to create unique training applications, such as those for training in business, commercial flight, and military missions. This has led some to envision a time in which game-based training devices or simulations will be available for thousands of different job descriptions. \n\nThe banking business has used internet technology to reach an increasing number of customers. The most important shift in business model due to the long tail has come from the various forms of microfinance developed.\n\nAs opposed to e-tailers, micro-finance is a distinctly low technology business. Its aim is to offer very small credits to lower-middle to lower class and poor people, that would otherwise be ignored by the traditional banking business. The banks that have followed this strategy of selling services to the low-frequency long tail of the sector have found out that it can be an important niche, long ignored by consumer banks. The recipients of small credits tend to be very good payers of loans, despite their non-existent credit history. They are also willing to pay higher interest rates than the standard bank or credit card customer. It also is a business model that fills an important developmental role in an economy.\n\nGrameen Bank in Bangladesh has successfully followed this business model. In Mexico the banks Compartamos and Banco Azteca also service this customer demographic, with an emphasis on consumer credit. Kiva.org is an organization that provides micro credits to people worldwide, by using intermediaries called small microfinance organizations (S.M.O.'s)to distribute crowd sourced donations made by Kiva.org lenders.\n\nAccording to the user-driven innovation model, companies can rely on users of their products and services to do a significant part of the innovation work. Users want products that are customized to their needs. They are willing to tell the manufacturer what they really want and how it should work. Companies can make use of a series of tools, such as interactive and internet based technologies, to give their users a voice and to enable them to do innovation work that is useful to the company.\n\nGiven the diminishing cost of communication and information sharing (by analogy to the low cost of storage and distribution, in the case of e-tailers), long-tailed user driven innovation will gain importance for businesses.\n\nIn following a long-tailed innovation strategy, the company is using the model to tap into a large group of users that are in the low-intensity area of the distribution. It is their collaboration and aggregated work that results in an innovation effort. Social innovation communities formed by groups of users can perform rapidly the trial and error process of innovation, share information, test and diffuse the results.\n\nEric von Hippel of MIT's Sloan School of Management defined the user-led innovation model in his book \"Democratizing Innovation\". Among his conclusions is the insight that as innovation becomes more user-centered the information needs to flow freely, in a more democratic way, creating a \"rich intellectual commons\" and \"attacking a major structure of the social division of labor\".\n\nThe drive to build a market and obtain revenue from the consumer demographic of the long tail has led businesses to implement a series of long-tail marketing techniques, most of them based on extensive use of internet technologies. Among the most representative are:\n\nThe long tail has possible implications for culture and politics. Where the opportunity cost of inventory storage and distribution is high, only the most popular products are sold. But where the long tail works, minority tastes become available and individuals are presented with a wider array of choices. The long tail presents opportunities for various suppliers to introduce products in the niche category. These encourage the diversification of products. These niche products open opportunities for suppliers while concomitantly satisfying the demands of many individuals – therefore lengthening the tail portion of the long tail. In situations where popularity is currently determined by the lowest common denominator, a long-tail model may lead to improvement in a society's level of culture. The opportunities that arise because of the long tail greatly affect society's cultures because suppliers have unlimited capabilities due to infinite storage and demands that were unable to be met prior to the long tail are realized. At the end of the long tail, the conventional profit-making business model ceases to exist; instead, people tend to come up with products for varied reasons like expression rather than monetary benefit. In this way, the long tail opens up a large space for authentic works of creativity.\n\nTelevision is a good example of this: Chris Anderson defines long-tail TV in the context of \"content that is not available through traditional distribution channels but could nevertheless find an audience.\" Thus, the advent of services such as television on demand, pay-per-view and even premium cable subscription services such as HBO and Showtime open up the opportunity for niche content to reach the right audiences, in an otherwise mass medium. These may not always attract the highest level of viewership, but their business distribution models make that of less importance. As the opportunity cost goes down, the choice of TV programs grows and greater cultural diversity rises.\n\nOften presented as a phenomenon of interest primarily to mass market retailers and web-based businesses, the long tail also has implications for the producers of content, especially those whose products could not – for economic reasons – find a place in pre-Internet information distribution channels controlled by book publishers, record companies, movie studios, and television networks. Looked at from the producers' side, the long tail has made possible a flowering of creativity across all fields of human endeavour. One example of this is YouTube, where thousands of diverse videos – whose content, production value or lack of popularity make them inappropriate for traditional television – are easily accessible to a wide range of viewers.\n\nThe intersection of viral marketing, online communities and new technologies that operate within the long tail of consumers and business is described in the novel by William Gibson, \"Pattern Recognition\".\n\nIn military thinking, John Robb applies the long tail to the developments in insurgency and terrorist movements, showing how technology and networking allows the long tail of disgruntled groups and criminals to take on the nation state and have a chance to win.\n\nA 2008 study by Anita Elberse, professor of business administration at Harvard Business School, calls the long tail theory into question, citing sales data which shows that the Web magnifies the importance of blockbuster hits. On his blog, Chris Anderson responded to the study, praising Elberse and the academic rigor with which she explores the issue but drawing a distinction between their respective interpretations of where the \"head\" and \"tail\" begin. Elberse defined head and tail using percentages, while Anderson uses absolute numbers. Similar results were published by Serguei Netessine and Tom F. Tan, who suggest that head and tail should be defined by percentages rather than absolute numbers.\n\nAlso in 2008, a sales analysis of an unnamed UK digital music service by economist Will Page and high-tech entrepreneur Andrew Bud found that sales exhibited a log-normal distribution rather than a power law; they reported that 80% of the music tracks available sold no copies at all over a one-year period. Anderson responded by stating that the study's findings are difficult to assess without access to its data.\n\n\n"}
{"id": "56947379", "url": "https://en.wikipedia.org/wiki?curid=56947379", "title": "Magellan Rise (ocean plateau)", "text": "Magellan Rise (ocean plateau)\n\nMagellan Rise is an oceanic plateau in the Pacific Ocean, which covers a surface area of . There is another \"Magellan Rise\" west from the Marshall Islands as well.\n\nThe Magellan Rise has been called a large igneous province by Coffin and Endholm 2001 and was emplaced 145 million or 135-128 million years ago, possibly as a consequence of intense volcanism at a former triple junction. Alternatively, the Rise was formed by a mantle plume. Candidate mantle plumes are the Easter hotspot and the Foundation hotspot.\n\nThe volume of rocks in the Magellan Rise is about -. It apparently developed first on the Phoenix Plate before being transferred onto the Pacific Plate 125 million years ago. The Magellan Rise never rose to shallow depths at least since the Cretaceous, and the Rise is covered by sediments of Tithonian/Berriasian to Quaternary age.\n\n"}
{"id": "1022936", "url": "https://en.wikipedia.org/wiki?curid=1022936", "title": "Military education and training", "text": "Military education and training\n\nMilitary education and training is a process which intends to establish and improve the capabilities of military personnel in their respective roles. It begins with recruit training, proceeds to education and training specific to military roles, and may also include additional training during a military career. Military training may be voluntary or compulsory duty. \n\nThe primary form of military training is recruit training, which makes use of various conditioning techniques to resocialize trainees into the military system, ensure that they will obey all orders without hesitation, and teach basic military skills. The drill instructor has the task of making the service members fit for military use. (Resocialization is a sociological concept referring to the process of mentally and emotionally \"re-training\" a person so they can operate in a new environment, and involves changes to an individual's attitudes and behaviours.) \n\nAfter their recruit training, personnel may undergo further training specific to their military role, including the use of any specialist equipment. After this point, they are normally deemed fit for military service.\n\nMilitary personnel may continue to receive training during their career.\n\nLarger countries may have military academies, which combine military training with formal qualifications.\n\n"}
{"id": "1807405", "url": "https://en.wikipedia.org/wiki?curid=1807405", "title": "Nationalnyckeln till Sveriges flora och fauna", "text": "Nationalnyckeln till Sveriges flora och fauna\n\nNationalnyckeln till Sveriges flora och fauna (Swedish for \"National Key to Sweden's Flora and Fauna\") is a set of books, the first volume of which, \"Fjärilar: Dagfjärilar\" (Butterflies, 140 species), appeared on 25 April 2005. The publishing plan comprises 100,000 illustrations spread over more than 100 volumes, to appear over a period of 20 years, listing and providing popular scientific descriptions of all species of plants (flora) and animals (fauna) in Sweden. So large a work has never been published in the history of Swedish literature.\n\n\"Nationalnyckeln\" is a popular scientific account produced on contract from the Riksdag by the Swedish Species Information Centre (ArtDatabanken) at the Swedish University of Agricultural Sciences (SLU) in Uppsala.\n\n"}
{"id": "2819542", "url": "https://en.wikipedia.org/wiki?curid=2819542", "title": "Participatory action research", "text": "Participatory action research\n\nParticipatory action research (PAR) is an approach to research in communities that emphasizes participation and action. It seeks to understand the world by trying to change it, collaboratively and following reflection. PAR emphasizes collective inquiry and experimentation grounded in experience and social history. Within a PAR process, \"communities of inquiry and action evolve and address questions and issues that are significant for those who participate as co-researchers\". PAR contrasts with many research methods, which emphasize disinterested researchers and reproducibility of findings. \n\nPAR practitioners make a concerted effort to integrate three basic aspects of their work: participation (life in society and democracy), action (engagement with experience and history), and research (soundness in thought and the growth of knowledge). \"Action unites, organically, with research\" and collective processes of self-investigation. The way each component is actually understood and the relative emphasis it receives varies nonetheless from one PAR theory and practice to another. This means that PAR is not a monolithic body of ideas and methods but rather a pluralistic orientation to knowledge making and social change.\n\nPAR has multiple progenitors and resists definition. It is a broad tradition of collective self-experimentation backed up by evidential reasoning, fact-finding and learning. All formulations of PAR have in common the idea that research and action must be done 'with' people and not 'on' or 'for' people. It counters scientism by promoting the grounding of knowledge in human agency and social history (as in much of political economy). Inquiry based on PAR principles makes sense of the world through collective efforts to transform it, as opposed to simply observing and studying human behaviour and people's views about reality, in the hope that meaningful change will eventually emerge.\n\nPAR draws on a wide range of influences, both among those with professional training and those who draw on their life experience and those of their ancestors. Many draw on the work of Paulo Freire, new thinking on adult education research, the Civil Rights Movement, South Asian social movements such as the Bhoomi Sena, and key initiatives such as the Participatory Research Network created in 1978 and based in New Delhi. \"It has benefited from an interdisciplinary development drawing its theoretical strength from adult education,\nsociology, political economy, community psychology, community development, feminist studies, critical psychology, organizational development and more\". The Colombian sociologist Orlando Fals Borda and others organized the first explicitly PAR conference in Cartagena, Colombia in 1977. Based on his research with peasant groups in rural Boyaca and with other underserved groups, Fals Borda called for the 'community action' component to be incorporated into the research plans of traditionally trained researchers. His recommendations to researchers committed to the struggle for justice and greater democracy in all spheres, including the business of science, are useful for all researchers and echo the teaching from many schools of research:\n\nIn the UK and North America the work of Kurt Lewin and the Tavistock Institute in the 1940s has been influential. However alternative traditions of PAR, begin with processes that include more bottom-up organising and popular education than were envisaged by Lewin.\n\nPAR strategies to democratize knowledge making and ground it in real community needs and learning represent genuine efforts to overcome the ineffectiveness and elitism of conventional schooling and science, and the negative effects of market forces and industry on the workplace, community life and sustainable livelihoods. These principles and the ongoing evolution of PAR have had a lasting legacy in fields ranging from problem solving in the workplace to community development and sustainable livelihoods, education, public health, feminist research and civic engagement. It is important to note that these contributions are subject to many tensions and debates on key issues such as the role of \"clinical psychology\", \"critical social thinking\" and the pragmatic concerns of \"organizational learning\" in PAR theory and practice. Labels used to define each approach (PAR, critical PAR, action research, psychosociology, sociotechnical analysis, etc.) reflect these tensions and point to major differences that may outweigh the similarities. While a common denominator, the combination of participation, action and research reflects the fragile unity of traditions whose diverse ideological and organizational contexts kept them separate and largely ignorant of one another for several decades.\n\nThe following review focuses on traditions that incorporate the three pillars of PAR. Closely related approaches that overlap but do not bring the three components together are left out. Applied research, for instance, is not necessarily committed to participatory principles and may be initiated and controlled mostly by experts, with the implication that 'human subjects' are not invited to play a key role in science building and the framing of the research questions. As in mainstream science, this process \"regards people as sources of information, as having bits of isolated knowledge, but they are neither expected nor apparently assumed able to analyze a given social reality\". PAR also differs from participatory inquiry or collaborative research, contributions to knowledge that may not involve direct engagement with transformative action and social history. PAR, in contrast, has evolved from the work of activists more concerned with empowering marginalized peoples than with generating academic knowledge for its own sake. Lastly, given its commitment to the research process, PAR overlaps but is not synonymous with Action learning, Action Reflection Learning (ARL), participatory development and community development—recognized forms of problem solving and capacity building that may be carried out with no immediate concern for research and the advancement of knowledge.\n\nAction research in the workplace took its initial inspiration from Lewin's work on organizational development (and Dewey's emphasis on learning from experience). Lewin's seminal contribution involves a flexible, scientific approach to planned change that proceeds through a spiral of steps, each of which is composed of 'a circle of planning, action, and fact-finding about the result of the action', towards an organizational 'climate' of democratic leadership and responsible participation that promotes critical self-inquiry and collaborative work. These steps inform Lewin's work with basic skill training groups, T-groups where community leaders and group facilitators use feedback, problem solving, role play and cognitive aids (lectures, handouts, film) to gain insights into themselves, others and groups with a view to 'unfreezing' and changing their mindsets, attitudes and behaviours.\nLewin's understanding of action-research coincides with key ideas and practices developed at the influential Tavistock Institute (created in 1947)) in the UK and National Training Laboratories (NTL) in the US. An important offshoot of Tavistock thinking and practise is the sociotechnical systems perspective on workplace dynamics, guided by the idea that greater productivity or efficiency does not hinge on improved technology alone. Improvements in organizational life call instead for the interaction and 'joint optimization' of the social and technical components of workplace activity. In this perspective, the best match between the social and technical factors of organized work lies in principles of 'responsible group autonomy' and industrial democracy, as opposed to deskilling and top-down bureaucracy guided by Taylor's scientific management and linear chain of command.\n\nNTL played a central role in the evolution of experiential learning and the application of behavioral science to improving organizations. Process consultation, team building, conflict management, and workplace group democracy and autonomy have become recurrent themes in the prolific body of literature and practice known as organizational development (OD). As with 'action science', OD is a response to calls for planned change and 'rational social management' involving a normative human relations movement and approach to worklife in capital-dominated economies. Its principal goal is to enhance an organization's performance and the worklife experience, with the assistance of a consultant, a change agent or catalyst that helps the sponsoring organization define and solve its own problems, introduce new forms of leadership and change organizational culture and learning. Diagnostic and capacity-building activities are informed, to varying degrees, by psychology, the behavioural sciences, organizational studies, or theories of leadership and social innovation. Appreciative Inquiry (AI), for instance, is an offshoot of PAR based on positive psychology. Rigorous data gathering or fact-finding methods may be used to support the inquiry process and group thinking and planning. On the whole, however, science tends to be a means, not an end. Workplace and organizational learning interventions are first and foremost problem-based, action-oriented and client-centred.\n\nTavistock broke new ground in other ways as well, by meshing general medicine and psychiatry with Freudian and Jungian psychology and the social sciences to help the British army face various human resource problems. This gave rise to a field of scholarly research and professional intervention loosely known as psychosociology, particularly influential in France (CIRFIP). Several schools of thought and 'social clinical' practise belong to this tradition, all of which are critical of the experimental and expert mindset of social psychology. Most formulations of psychosociology share with OD a commitment to the relative autonomy and active participation of individuals and groups coping with problems of self-realization and goal effectiveness within larger organizations and institutions. In addition to this humanistic and democratic agenda, psychosociology uses concepts of psychoanalytic inspiration to address interpersonal relations and the interplay between self and group. It acknowledges the role of the unconscious in social behaviour and collective representations and the inevitable expression of transference and countertransference—language and behaviour that redirect unspoken feelings and anxieties to other people or physical objects taking part in the action inquiry.\n\nThe works of Balint, Jaques,) and Bion are turning points in the formative years of psychosociology. Commonly cited authors in France include Amado, Barus-Michel, Dubost, Enriquez, Lévy, Gaujelac, and Giust-Desprairies. Different schools of thought and practice include Mendel's action research framed in a 'sociopsychoanalytic' perspective and Dejours's psychodynamics of work, with its emphasis on work-induced suffering and defence mechanisms. Lapassade and Lourau's 'socianalytic' interventions focus rather on institutions viewed as systems that dismantle and recompose norms and rules of social interaction over time, a perspective that builds on the principles of institutional analysis and psychotherapy. and Martin's work on group psychoanalysis and theory of the collective 'skin-ego' is generally considered as the most faithful to the Freudian tradition. Key differences between these schools and the methods they use stem from the weight they assign to the analyst's expertise in making sense of group behaviour and views and also the social aspects of group behaviour and affect. Another issue is the extent to which the intervention is critical of broader institutional and social systems. The use of psychoanalytic concepts and the relative weight of effort dedicated to research, training and action also vary.\n\nPAR emerged in the postwar years as an important contribution to intervention and self-transformation within groups, organizations and communities. It has left a singular mark on the field of rural and community development, especially in the Global South. Tools and concepts for doing research with people, including \"barefoot scientists\" and grassroots \"organic intellectuals\" (see Gramsci), are now promoted and implemented by many international development agencies, researchers, consultants, civil society and local community organizations around the world. This has resulted in countless experiments in diagnostic assessment, scenario planning and project evaluation in areas ranging from fisheries and mining to forestry, plant breeding, agriculture, farming systems research and extension, watershed management, resource mapping, environmental conflict and natural resource management, land rights, appropriate technology, local economic development, communication, tourism, leadership for sustainability, biodiversity and climate change. This prolific literature includes the many insights and methodological creativity of participatory monitoring, participatory rural appraisal (PRA) and participatory learning and action (PLA) and all action-oriented studies of local, indigenous or traditional knowledge.\n\nOn the whole, PAR applications in these fields are committed to problem solving and adaptation to nature at the household or community level, using friendly methods of scientific thinking and experimentation adapted to support rural participation and sustainable livelihoods.\n\nIn education, PAR practitioners inspired by the ideas of critical pedagogy and adult education are firmly committed to the politics of emancipatory action formulated by Freire, with a focus on dialogical reflection and action as means to overcome relations of domination and subordination between oppressors and the oppressed, colonizers and the colonized. The approach implies that \"the silenced are not just incidental to the curiosity of the researcher but are the masters of inquiry into the underlying causes of the events in their world\". Although a researcher and a sociologist, Fals Borda also has a profound distrust of conventional academia and great confidence in popular knowledge, sentiments that have had a lasting impact on the history of PAR, particularly in the fields of development, literacy, counterhegemonic education as well as youth engagement on issues ranging from violence to criminality, racial or sexual discrimination, educational justice, healthcare and the environment.\n\nCommunity-based participatory research and service-learning are a more recent attempts to reconnect academic interests with education and community development. The Global Alliance on Community-Engaged Research is a promising effort to \"use knowledge and community-university partnership strategies for democratic social and environmental change and justice, particularly among the most vulnerable people and places of the world.\" It calls for the active involvement of community members and researchers in all phases of the action inquiry process, from defining relevant research questions and topics to designing and implementing the investigation, sharing the available resources, acknowledging community-based expertise, and making the results accessible and understandable to community members and the broader public. Service learning or education is a closely related endeavour designed to encourage students to actively apply knowledge and skills to local situations, in response to local needs and with the active involvement of community members. Many online or printed guides now show how students and faculty can engage in community-based participatory research and meet academic standards at the same time.\n\nCollaborative research in education is community-based research where pre-university teachers are the community and scientific knowledge is built on top of teachers' own interpretation of their experience and reality, with or without immediate engagement in transformative action.\n\nPAR has made important inroads in the field of public health, in areas such as disaster relief, community-based rehabilitation, accident prevention, hospital care and drug prevention.\n\nBecause of its link to radical democratic struggles of the Civil Rights Movement and other social movements in South Asia and Latin America (see above), PAR is seen as a threat to their authority by some established elites. An international alliance university-based participatory researchers, ICPHR, omit the word \"Action\", preferring the less controversial term \"participatory research\".\nPhotovoice is one of the strategies used in PAR and is especially useful in the public health domain. Keeping in mind the purpose of PAR, which is to benefit communities, Photovoice allows the same to happen through the media of photography. Photovoice considers helping community issues and problems reach policy makers as its primary goal.\n\nParticipatory programs within the workplace involve employees within all levels of a workplace organization, from management to front-line staff, in the design and implementation of health and safety interventions. Some research has shown that interventions are most successful when front-line employees have a fundamental role in designing workplace interventions. Success through participatory programs may be due to a number of factors. Such factors include a better identification of potential barriers and facilitators, a greater willingness to accept interventions than those imposed strictly from\nupper management, and enhanced buy-in to intervention design, resulting in greater sustainability though promotion and acceptance. When designing an intervention, employees are able to consider lifestyle and other behavioral influences into solution activities that go beyond the immediate workplace.\n\nFeminist research and women's development theory also contributed to rethinking the role of scholarship in challenging existing regimes of power, using qualitative and interpretive methods that emphasize subjectivity and self-inquiry rather than the quantitative approach of mainstream science.\n\nNovel approaches to PAR in the public sphere help scale up the engaged inquiry process beyond small group dynamics. Touraine and others thus propose a 'sociology of intervention' involving the creation of artificial spaces for movement activists and non-activists to debate issues of public concern. Citizen science is another recent move to expand the scope of PAR, to include broader 'communities of interest' and citizens committed to enhancing knowledge in particular fields. In this approach to collaborative inquiry, research is actively assisted by volunteers who form an active public or network of contributing individuals. Efforts to promote public participation in the works of science owe a lot to the revolution in information and communications technology (ICT). Web 2.0 applications support virtual community interactivity and the development of user-driven content and social media, without restricted access or controlled implementation. They extend principles of open-source governance to democratic institutions, allowing citizens to actively engage in wiki-based processes of virtual journalism, public debate and policy development. Although few and far between, experiments in open politics can thus make use of ICT and the mechanics of e-democracy to facilitate communications on a large scale, towards achieving decisions that best serve the public interest.\n\nIn the same spirit, discursive or deliberative democracy calls for public discussion, transparency and pluralism in political decision-making, lawmaking and institutional life. Fact-finding and the outputs of science are made accessible to participants and may be subject to extensive media coverage, scientific peer review, deliberative opinion polling and adversarial presentations of competing arguments and predictive claims. The methodology of Citizens' jury is interesting in this regard. It involves people selected at random from a local or national population who are provided opportunities to question 'witnesses' and collectively form a 'judgment' on the issue at hand.\n\nICTs, open politics and deliberative democracy usher in new strategies to engage governments, scientists, civil society organizations and interested citizens in policy-related discussions of science and technology. These trends represent an invitation to explore novel ways of doing PAR on a broader scale.\n\nCalls for norms of ethical conduct to guide the relationship between researchers and participants are many. Norms in research ethics involving humans include respect for the autonomy of individuals and groups to deliberate about a decision and act on it. This principle is usually expressed through the free, informed and ongoing consent of those participating in research (or those representing them in the case of persons lacking the capacity to decide). Another mainstream principle is the welfare of participants who should not be exposed to any unfavourable balance of benefits and risks with participation in research aimed at the advancement of knowledge, especially those that are serious and probable. Since privacy is a factor that contributes to people's welfare, confidentiality obtained through the collection and use of data that are anonymous (e.g. survey data) or anonymized tends to be the norm. Finally, the principle of justice—equal treatment and concern for fairness and equity—calls for measures of appropriate inclusion and mechanisms to address conflicts of interests.\n\nWhile the choice of appropriate norms of ethical conduct is rarely an either/or question, PAR implies a different understanding of what consent, welfare and justice entail. For one thing the people involved are not mere 'subjects' or 'participants'. They act instead as key partners in an inquiry process that may take place outside the walls of academic or corporate science. As Canada's Tri-Council Policy Statement: Ethical Conduct for Research Involving Humans suggests, PAR requires that the terms and conditions of the collaborative process be set out in a research agreement or protocol based on mutual understanding of the project goals and objectives between the parties, subject to preliminary discussions and negotiations. Unlike individual consent forms, these terms of reference (ToR) may acknowledge collective rights, interests and mutual obligations. While they are legalistic in their genesis, they are usually based on interpersonal relationships and a history of trust rather than the language of legal forms and contracts.\n\nAnother implication of PAR ethics is that partners must protect themselves and each other against potential risks, by mitigating the negative consequences of their collaborative work and pursuing the welfare of all parties concerned. This does not preclude battles against dominant interests. Given their commitment to social justice and transformative action, some PAR projects may be critical of existing social structures and struggle against the policies and interests of individuals, groups and institutions accountable for their actions, creating circumstances of danger.\n\nOn the matter of welfare, empowerment through recognition and 'being heard' may be more important to the research than are privacy and confidentiality. It is important to strike a balance between allowing privacy and confidentiality, and respect for individuals and groups who wish to be heard and identified for their contribution to research. The former may be hard to reconcile with PAR. The latter can be shown through proper quoting, acknowledgements, co-authorship, or the granting of intellectual property rights.\n\nBy definition, PAR is always a step into the unknown, raising new questions and creating new risks over time. Given its emergent properties and responsiveness to social context and needs, PAR cannot limit discussions and decisions about ethics to the design and proposal phase. Norms of ethical conduct and their implications may have to be revisited as the project unfolds. This has implications, both in resources and practice, for the ability to subject the research to true ethical oversight in the way that traditional research has come to be regulated.\n\nPAR offers a long history of experimentation with evidence-based and people-based inquiry, a groundbreaking alternative to mainstream positive science. As with positivism, the approach creates many challenges as well as debates on what counts as participation, action and research. Differences in theoretical commitments (Lewinian, Habermasian, Freirean, psychoanalytic, feminist, etc.) and methodological inclinations (quantitative, qualitative, mixed) are numerous and profound. This is not necessarily a problem, given the pluralistic value system built into PAR. Ways to better answer questions pertaining to PAR's relationship with science and social history are nonetheless key to its future.\n\nOne critical question concerns the problem-solving orientation of engaged inquiry—the rational means-ends focus of most PAR experiments as they affect organizational performance or material livelihoods, for instance. In the clinical perspective of French psychosociology, a pragmatic orientation to inquiry neglects forms of understanding and consciousness that are not strictly instrumental and rational. PAR must pay equal attention the interconnections of self-awareness, the unconscious and life in society.\n\nAnother issue, more widely debated, is scale—how to address broad-based systems of power and issues of complexity, especially those of another development on a global scale. How can PAR develop a macro-orientation to democratic dialogue and meet challenges of the 21st Century, by joining movements to support justice and solidarity on both local and global scales? By keeping things closely tied to local group dynamics, PAR runs the risk of substituting small-scale participation for genuine democracy and fails to develop strategies for social transformation on all levels. Given its political implications, community-based action research and its consensus ethos have been known to fall prey to powerful stakeholders and serve as Trojan horses to bring global and environmental restructuring processes directly to local settings, bypassing legitimate institutional buffers and obscuring diverging interests and the exercise of power during the process. Cooptation can lead to highly manipulated outcomes. Against this criticism, others argue that, given the right circumstances, it is possible to build institutional arrangements for joint learning and action across regional and national borders that can have impacts on citizen action, national policies and global discourses.\n\nThe role of science and scholarship in PAR is another source of difference. In the Lewinian tradition, \"there is nothing so practical as a good theory\". Accordingly, the scientific logic of developing theory, forming and testing hypotheses, gathering measurable data and interpreting the results plays a central role. While more clinically oriented, psychosociology in France also emphasizes the distinctive role of formal research and academic work, beyond problem solving in specific contexts. Many PAR practitioners critical of mainstream science and its overemphasis on quantitative data also point out that research based on qualitative methods may be theoretically-informed and rigorous in its own way. In other traditions, however, PAR keeps great distance from both academic and corporate science. Given their emphasis on pluralism and living knowledge, many practitioners of grassroots inquiry are critical of grand theory and advanced methods for collaborative inquiry, to the point of abandoning the word \"research\" altogether, as in participatory action learning. Others equate research with any involvement in reflexive practice aimed at assessing problems and evaluating project or program results against group expectations. As a result, inquiry methods tend to be soft and theory remains absent or underdeveloped. Practical and theoretical efforts to overcome this ambivalence towards scholarly activity are nonetheless emerging.\n\n\n"}
{"id": "65465", "url": "https://en.wikipedia.org/wiki?curid=65465", "title": "Petroleum engineering", "text": "Petroleum engineering\n\nPetroleum engineering is a field of engineering concerned with the activities related to the production of hydrocarbons, which can be either crude oil or natural gas. Exploration and production are deemed to fall within the \"upstream\" sector of the oil and gas industry. Exploration, by earth scientists, and petroleum engineering are the oil and gas industry's two main subsurface disciplines, which focus on maximizing economic recovery of hydrocarbons from subsurface reservoirs. Petroleum geology and geophysics focus on provision of a static description of the hydrocarbon reservoir rock, while petroleum engineering focuses on estimation of the recoverable volume of this resource using a detailed understanding of the physical behavior of oil, water and gas within porous rock at very high pressure.\n\nThe combined efforts of geologists and petroleum engineers throughout the life of a hydrocarbon accumulation determine the way in which a reservoir is developed and depleted, and usually they have the highest impact on field economics. Petroleum engineering requires a good knowledge of many other related disciplines, such as geophysics, petroleum geology, formation evaluation (well logging), drilling, economics, reservoir simulation, reservoir engineering, well engineering, artificial lift systems, completions and petroleum production engineering.\n\nRecruitment to the industry has historically been from the disciplines of physics, chemical engineering and mining engineering. Subsequent development training has usually been done within oil companies.\n\nThe profession got its start in 1914 within the American Institute of Mining, Metallurgical and Petroleum Engineers (AIME). The first Petroleum Engineering degree was conferred in 1915 by the University of Pittsburgh. Since then, the profession has evolved to solve increasingly difficult situations. Improvements in computer modeling, materials and the application of statistics, probability analysis, and new technologies like horizontal drilling and enhanced oil recovery, have drastically improved the toolbox of the petroleum engineer in recent decades. Automation, sensors, and robots are being used to propel the industry to more efficiency and safety. \n\nDeep-water, arctic and desert conditions are usually contended with. High temperature and high pressure (HTHP) environments have become increasingly commonplace in operations and require the petroleum engineer to be savvy in topics as wide-ranging as thermo-hydraulics, geomechanics, and intelligent systems.\n\nThe Society of Petroleum Engineers (SPE) is the largest professional society for petroleum engineers and publishes much technical information and other resources to support the oil and gas industry. It provides free online education (webinars), mentoring, and access to SPE Connect, an exclusive platform for members to discuss technical issues, best practices, and other topics. SPE members also are able to access the SPE Competency Management Tool to find knowledge and skill strengths and opportunities for growth. SPE publishes peer-reviewed journals, books, and magazines. SPE members receive a complimentary subscription to the \"Journal of Petroleum Technology\" and discounts on SPE's other publications. SPE members also receive discounts on registration fees for SPE organized events and training courses. SPE provides scholarships and fellowships to undergraduate and graduate students. \n\nAccording to the United States Department of Labor's Bureau of Labor Statics, petroleum engineers are required to have a bachelor's degree in engineering, generally a degree focused on petroleum engineering is preferred, but degrees in mechanical, chemical, and civil engineering are satisfactory as well. Petroleum engineering education is available at many universities in the United States and throughout the world - primarily in oil producing regions. \"U.S. News & World Report\" maintains a list of the Best Underdegraduate Petroleum Engineering Programs. SPE and some private companies offer training courses. Some oil companies have considerable in-house petroleum engineering training classes.\n\nPetroleum engineering has historically been one of the highest paid engineering disciplines, although there is a tendency for mass layoffs when oil prices decline and waves of hiring as prices rise. SPE annually conducts a salary survey. In 2017, SPE reported that the average SPE professional member reported earning USD $194,649 (including salary and bonus), and the average base pay was USD $151,122. The average base pay reported in 2016 was $143,006. Base pay and other compensation was on average was highest in the United States where the base pay was USD $174,283. Drilling and production engineers tended to make the best base pay, USD $160,026 for drilling engineers and USD $158,964 for production engineers. Base pay on average ranged from USD $96,382-174,283. There are still significant gender pay gaps, but in 2017 more than half of female respondents reported an increase in base pay received since 2016.\n\nIn 2016, the United States Department of Labor's Bureau of Labor Statistics reported the median pay for petroleum engineers was USD $18,230, or roughly $61.65 per hour. The same summary projects that there will be 15% job growth in this field from 2016 to 2026. Also in 2016, \"U.S. News & World Report\" named petroleum engineering the top college major in terms of highest median annual wages of college-educated workers (age 25-59). The 2010 National Association of Colleges and Employers survey showed petroleum engineers as the highest paid 2010 graduates, at an average annual salary of $125,220. For individuals with experience, salaries can range from $170,000 to $260,000. They make an average of $112,000 a year and about $53.75 per hour. In a 2007 article, Forbes.com reported that petroleum engineering was the 24th best paying job in the United States. \n\nPetroleum engineers divide themselves into several types:\n\n\n\n"}
{"id": "58832789", "url": "https://en.wikipedia.org/wiki?curid=58832789", "title": "Pluto Energetic Particle Spectrometer Science Investigation", "text": "Pluto Energetic Particle Spectrometer Science Investigation\n\nPluto Energetic Particle Spectrometer Science Investigation (PEPSSI), is an instrument on the \"New Horizons\" space probe to Pluto and beyond, it is designed to measure ions and electrons. Specifically, it is focused on measuring ions escaping from the atmosphere of Pluto during the 2015 flyby. It is one of seven major scientific instruments aboard the spacecraft. The spacecraft was launched in 2006, flewby Jupiter the following year, and went on flyby Pluto in 2015 where PEPSI was able to record and transmit back to Earth the planned data collections.\n\nPEPSSI is designed to help understand the rate of atmospheric loss from the atmosphere of Pluto into space, which is thought experience comet-like atmosphere loss into outer space. These ions blend in with the surrounding solar wind which passed by Pluto. During the flyby PEPSSI sent data back to Earth every day. During the journey to Pluto, PEPSSI was also used to record data about the interplanetary medium. Data about Jupiter and its magnetotail was also collected by PEPSSI during its 2007 flyby of that planet. Beyond Pluto and into the Kuiper belt, PEPSSI can be used to study how the solar wind interacts with interstellar wind, adding to the data pile from the Voyager's which also exited the solar system in a similar direction as the trajectory of New Horizons.\n\nPEPSSI is one of the seven major instruments on \"New Horizons\", and along with SWAP is designed to detect ions. Ions come in and pass through two foils, which when they pass through these foils they are timed, then they hit a solid state detector. The time of flight between the two foils helps determine the particles mass, and the detector measures the energy, and from this the composition of the particle can be determined within certain parameters. The instrument is designed to \"taste\" the atmosphere of Pluto, and design is oriented towards being low weight, low power, and understanding the nature of atmospheric loss from Pluto.\n\nPEPPSI is a compact low-power ion measurement device, and it is a time of flight type of instrument The design detects ions from about 10 keV to 1 MeV in a fan shaped160 degree by 12 degree arc. The device has a mass of 1.5 kg (3.31 lb) and can consume about 2.5 watts of electrical power. The ionized particles pass through two microchannel plates, with the time recorded for the time between these detections. After passing though this section, there is a solid state silicon detector. The design avoided using magnets for the time of flight section, which enhanced weight and/or power savings for the instrument.\n\nThe 160 by 12 degree field of view is covered by six detectors each with a 25 by 12 degree field of view. By noting which detector the particle has arrived at, its overall direction of input can be noted.\n\nTo meet the low power use and weight requirements, the device made use of application-specific integrated circuits (ASICs).\n\nPEPSSI is based on an instrument that was onboard the Discovery program's \"MESSENGER\" (planet Mercury orbiter, launched 2004/ended 2015) called the Energetic Particle Spectrometer. PEPSSI complements the focus of SWAP, which is oriented towards lower energy ions. Whereas PEPSSi measures ions with energies from about 10 keV to 1000 keV, SWAP measures ions from 25 to 7.5 keV. \n\nPEPSSI has an enhanced design to reduce weight and power consumption having to do with electron detection, with heritage going back the 1980s and 1990s. The instrument on MESSENGER was based on an instrument for that periods proposals for a Pluto flyby mission. The design can trace back to some heritage to instruments in the 1980s for detecting ions.\n\nSpecs:\n\n\n"}
{"id": "313371", "url": "https://en.wikipedia.org/wiki?curid=313371", "title": "Rip current", "text": "Rip current\n\nA rip current, often simply called a rip, or by the misnomer \"rip tide\", is a specific kind of water current which can occur near beaches with breaking waves. A rip is a strong, localized, and narrow current of water which moves directly away from the shore, cutting through the lines of breaking waves like a river running out to sea, and is strongest near the surface of the water.\n\nRip currents can be hazardous to people in the water. Swimmers who are caught in a rip current and who do not understand what is going on, and who may not have the necessary water skills, may panic, or exhaust themselves by trying to swim directly against the flow of water. Because of these factors, rips are the leading cause of rescues by lifeguards at beaches, and rips are the cause of an average of 46 deaths by drowning per year in the United States.\n\nA rip current is not the same thing as undertow, although some people use the term incorrectly when they often mean a rip current. Contrary to popular belief, neither rip nor undertow can pull a person down and hold them under the water. A rip simply carries floating objects, including people, out beyond the zone of the breaking waves.\n\nA rip current forms because wind and breaking waves push surface water towards the land, and this causes a slight rise in the water level along the shore. This excess water will tend to flow back to the open water via the route of least resistance. When there is a local area which is slightly deeper, or a break in an offshore sand bar or reef, this can allow water to flow offshore more easily, and this will initiate a rip current through that gap. \n\nWater that has been pushed up near the beach flows along the shore towards the outgoing rip as \"feeder currents\", and then the excess water flows out at a right angle to the beach, in a tight current called the \"neck\" of the rip. The \"neck\" is where the flow is most rapid. When the water in the rip current reaches outside of the lines of breaking waves, the flow disperses sideways, loses power, and dissipates in what is known as the \"head\" of the rip.\n\nRip currents can often occur on a gradually shelving shore where breaking waves approach the shore parallel to it, or where underwater topography encourages outflow at a specific area. Rip currents can form at the coasts of oceans, seas, and large lakes, whenever there are waves of sufficient energy. The location of rip currents can be difficult to predict; whereas some tend to recur always in the same places, others can appear and disappear suddenly at various locations along the beach. The appearance and disappearance of rip currents is dependent on the bottom topography and the exact direction that the surf and swells are coming in from.\n\nRip currents can potentially occur wherever there is strong longshore variability in wave breaking. This variability may be caused by such features as sandbars (as shown in the animated diagram), by piers and jetties, and even by crossing wave trains, and are often located in places such as where there is a gap in a reef or low area on a sandbar. Rip currents may deepen the channel through a sandbar once they have formed.\n\nRip currents are usually quite narrow, but tend to be more common, wider, and faster, when and where breaking waves are large and powerful. Local underwater topography makes some beaches more likely to have rip currents; a few beaches are notorious in this respect.\n\nAlthough \"rip tide\" is a misnomer, in areas of significant tidal range, rip currents may only occur at certain stages of the tide, when the water is shallow enough to cause the waves to break over a sand bar, but deep enough for the broken wave to flow over the bar. (In parts of the world with a big difference between high tide and low tide, and where the shoreline shelves gently, the distance between a bar and the shoreline may vary from a few feet to a mile or more, depending whether it is high tide or low tide.)\n\nA fairly common misconception is that rip currents can pull a swimmer down, under the surface of the water. This is not true, and in reality a rip current is strongest close to the surface, as the flow near the bottom is slowed by friction. \n\nThe surface of a rip current may appear to be a relatively smooth area of water, without any breaking waves, and this deceptive appearance may cause some beach goers to believe it is a suitable place to enter the water.\n\nA more detailed description involves radiation stress. This is the force (or momentum flux) exerted on the water column by the presence of the wave. As a wave shoals and increases in height prior to breaking, radiation stress increases. To balance this, the local mean surface level (the water level with the wave averaged out) drops; this is known as \"setdown\". As the wave breaks and continues to reduce in height, the radiation stress decreases. To balance this force, the mean surface increases — this is known as \"setup\". As a wave propagates over a sandbar with a gap (as shown in the lead image), the wave breaks on the bar, leading to setup. However, the part of the wave that propagates over the gap does not break, and thus setdown will continue. Thus, the mean surface over the bars is higher than that over the gap, and a strong flow issues outward through the gap.\n\nRip currents have a characteristic appearance, and, with some experience, they can be visually identified from the shore before entering the water. This is useful to lifeguards, swimmers, surfers, boaters, divers and other water users, who may need to avoid a rip, or in some cases make use of the current flow. Rip currents often look a bit like a road or a river running straight out to sea, and are easiest to notice and identify when the zone of breaking waves is viewed from a high vantage point. The following are some characteristics that can be used to visually identify a rip:\n\n\nThese characteristics are helpful in learning to recognize and understand the nature of rip currents so that a person can recognize the presence of rips before entering the water. In the United States, some beaches have signs created by the National Oceanic and Atmospheric Administration (NOAA) and United States Lifesaving Association, explaining what a rip current is and how to escape one. These signs are titled, \"Rip Currents; Break the Grip of the Rip\". Beachgoers can get information from lifeguards, who are always watching for rip currents, and who will move their safety flags so that swimmers can avoid rips.\n\nRip currents are a potential source of danger for people in shallow water with breaking waves in seas, oceans and lakes. Rip currents are the proximate cause of 80% of rescues carried out by beach lifeguards.\n\nRip currents typically flow at about 0.5 metres per second (1–2 feet per second), but they can be as fast as 2.5 metres per second (8 feet per second), which is faster than any human can swim. However, most rip currents are fairly narrow, and even the widest rip currents are not very wide; swimmers can easily exit the rip by swimming at a right angle to the flow, parallel to the beach. Swimmers who are unaware of this fact may exhaust themselves trying unsuccessfully to swim against the flow. The flow of the current also fades out completely at the head of the rip, outside the zone of the breaking waves, so there is a definite limit to how far the swimmer will be taken out to sea by the flow of a rip current.\n\nIn a rip current, death by drowning occurs when a person has limited water skills and panics, or when a swimmer persists in trying to swim to shore against a strong rip current, thus eventually becoming exhausted and unable to stay afloat.\n\nAccording to NOAA, over a 10-year average, rip currents cause 46 deaths annually in the United States, and 64 people died in rip currents in 2013. However, the United States Lifesaving Association \"estimates that the annual number of deaths due to rip currents on our nation's beaches exceeds 100.\"\n\nA study published in 2013 in Australia revealed that rips killed more people on Australian territory than bushfires, floods, cyclones and shark attacks combined.\n\nPeople caught in a rip current may notice that they are moving away from the shore quite rapidly. It is often not possible to swim directly back to shore against a rip current, so this is not recommended. Contrary to popular misunderstanding, a rip does not pull a swimmer under the water, it simply carries the swimmer away from the shore in a narrow band of moving water. The rip is like a moving treadmill, which the swimmer can get out of by swimming across the current, parallel to the shore, in either direction, until out of the rip current, which is usually not very wide. Once out of the rip, swimming back to shore is relatively easy in areas where waves are breaking and where floating objects and swimmers are being pushed towards the shore.\n\nAs an alternative, swimmers who are caught in a strong rip can relax and go with the flow (either floating or treading water) until the current dissipates beyond the surf line, and then they can signal for help, or swim back through the surf diagonally away from the rip and towards the shore.\n\nIt is necessary for coastal swimmers to understand the danger of rip currents, to learn how to recognize them and how to deal with them, and if possible to swim in only those areas where lifeguards are on duty.\n\nExperienced and knowledgeable water users, including surfers, body boarders, divers, surf lifesavers and kayakers, will sometimes use rip currents as a rapid and effortless means of transportation when they wish to get out beyond the breaking waves.\n\n\n"}
{"id": "18093238", "url": "https://en.wikipedia.org/wiki?curid=18093238", "title": "Scientific teaching", "text": "Scientific teaching\n\nScientific teaching is a pedagogical approach used in undergraduate science classrooms whereby teaching and learning is approached with the same rigor as science itself. \n\nAccording to a 2004 Policy Forum in \"Science\" magazine, \"scientific teaching involves active learning strategies to engage students in the process of science and teaching methods that have been systematically tested and shown to reach diverse students.\"\n\nThe 2007 volume \"Scientific Teaching\" lists three major tenets of scientific teaching:\n\nThese elements should underlie educational and pedagogical decisions in the classroom. The \"SCALE-UP\" learning environment is an example of applying the scientific teaching approach. In practice, scientific teaching employs a \"backward design\" approach. The instructor first decides what the students should know and be able to do (learning goals), then determines what would be evidence of student achievement of the learning goals, then designs assessments to measure this achievement. Finally, the instructor plans the learning activities, which should facilitate student learning through scientific discovery.\n\n"}
{"id": "43474682", "url": "https://en.wikipedia.org/wiki?curid=43474682", "title": "St. George Lane Fox-Pitt", "text": "St. George Lane Fox-Pitt\n\nSt. George William Lane Fox-Pitt (born 14 September 1856 in Malta, died 6 April 1932 in South Eaton Place) was a British electrical engineer and student of psychic phenomena.\n\nHis parents were Lieutenant General Augustus Henry Lane-Fox (1827–1900) and Alice Margaret (1828–1910, née Stanley). When his father's cousin Horace Pitt, 6th Baron Rivers, died, the family took the name \"Fox Pitt-Rivers\" on May 25, 1880.\n\nIn 1878, Fox-Pitt was granted a patent on a light bulb with a platinum-iridium filament. The patent also describes a system for power distribution using incandescent lamps in parallel.\n\nOn 12 December 1879, Charles Francis Brush founded the Anglo American Electric Light Company Ltd. in the UK and then acquired the patent rights to produce the Lane-Fox incandescent lamp in the same year. On 24 March 1880, he founded a new company, Anglo-American Brush Electric Light Corporation, which took over the previous one.\n\nAbout 1880, Fox-Pitt is said to have successfully experimented with charred plant fibers as the filament material. That was about the same time as the development of the light bulb with carbon filament by Edison in the United States. By 1883, Fox-Pitt had obtained further patents.\n\nFox-Pitt wrote books on the philosophy of science, education, and social problems. He was temporarily Vice President of the \"Moral Education League\" and organized the International Moral Education Congress. He was also one of the first active members of the Society for Psychical Research.\n\nIn 1891, he repurchased the patent rights of the Anglo-American Brush Electric Light Corporation and built himself a small lamp factory.\n\nIn 1898, he participated in a railway concession in Ecuador.\n\nIn 1899, he married Lady Edith Gertrude Douglas (1874–1963, the daughter of John Sholto Douglas, 9th Marquess of Queensberry).\n\n\n"}
{"id": "31058601", "url": "https://en.wikipedia.org/wiki?curid=31058601", "title": "Stoke Mandeville Stadium", "text": "Stoke Mandeville Stadium\n\nStoke Mandeville Stadium is the National Centre for Disability Sport in England. It is sited alongside Stoke Mandeville Hospital in Aylesbury in Buckinghamshire. Stoke Mandeville Stadium is owned by WheelPower, the national organisation for wheelchair sport. \n\nThe stadium developed out of the Stoke Mandeville Games — the forerunner of the Paralympic Games — founded in 1948 by Ludwig Guttmann. He was a neurosurgeon at the National Spinal Injuries Centre at Stoke Mandeville Hospital who recognised the value of exercise and competition in the rehabilitation of ex-members of the British armed forces. By 1961 Guttmann had founded the British Sports Association for the Disabled (now named English Federation of Disability Sport), expanding the concept of organising sport for men, women and children with disabilities and developing Stoke Mandeville Stadium into an international centre of disabled sport. The stadium was officially opened by Queen Elizabeth II on 2 August 1969.\n\nWhen Sir Ludwig Guttmann died in 1980 the Stadium was renamed Ludwig Guttmann Sports Centre for the Disabled. In 1993 the Stadium hosted the first international ex-service wheelchair games, organised by the Royal British Legion and opened by King Hussein and Queen Noor of Jordan. In 2001, following a £10 million refurbishment, it was again renamed as \"Stoke Mandeville Stadium\". The Paralympic mascot Mandeville is so named due to the legacy with the games.\n\nFacilities include a 400-metre outdoor running track, Cazenove Sports Hall, a 25m six-lane swimming pool, tennis courts and an indoor bowls arena. In addition the Stadium has its own \"Olympic Village\" accommodation for athletes and the Olympic Lodge Hotel and the Wolfson Conference Centre provide guest facilities.\n\nStoke Mandeville Stadium was one of the two venues of the VII Paralympic Games, the last of the Summer Paralympics not held in the same venue as the Summer Olympic Games.\n\n\n\n"}
{"id": "176349", "url": "https://en.wikipedia.org/wiki?curid=176349", "title": "Tom Van Flandern", "text": "Tom Van Flandern\n\nThomas C Van Flandern (June 26, 1940 – January 9, 2009) was an American astronomer and author specializing in celestial mechanics. Van Flandern had a career as a professional scientist, but was noted as an outspoken proponent of non-mainstream views related to astronomy, physics, and extra-terrestrial life. He also published the non-mainstream \"Meta Research Bulletin\". He died of colon cancer in Seattle, Washington.\n\nVan Flandern graduated from Saint Ignatius High School in Cleveland. While there, he helped start the Cleveland branch of Operation Moonwatch, an amateur science program initiated by the Smithsonian Astrophysical Observatory to track satellites. He also helped found a Moonwatchers team at Xavier University, and this team broke a tracking record in 1961.\n\nVan Flandern graduated from Xavier University cum laude with a B.S. in Mathematics in 1962 and was awarded a teaching fellowship at Georgetown University. He attended Yale University on a scholarship sponsored by the U.S. Naval Observatory (USNO), joining USNO in 1963. In 1969 he received a PhD in Astronomy from Yale, with a dissertation on lunar occultations.\n\nVan Flandern worked at the USNO until 1983, first becoming Chief of the Research Branch and later becoming Chief of the Celestial Mechanics Branch of the Nautical Almanac Office. His espousal of highly non-mainstream beliefs, particularly the exploded planet hypothesis, eventually led to his separation from the USNO. He later said, \"This forced me to the 'fringes,' areas of astronomy not accepted as credible by experts of the field\".\n\nFollowing his separation from the USNO, Van Flandern started a business organizing eclipse viewing expeditions, and promoting his non-mainstream views in a newsletter and web site. Shortly after his death in 2009, the asteroid 52266 Van Flandern was named in his honor because of his prediction and analysis of lunar occultations at the U.S. Naval Observatory and publications of papers on the dynamics of binary minor planets.\n\nDuring the mid-1970s, Van Flandern believed that lunar observations gave evidence of variation in Newton's gravitational constant (\"G\"), consistent with a speculative idea that had been put forward by Paul Dirac. In 1974, his essay \"A Determination of the Rate of Change of G\" was awarded second place by the Gravity Research Foundation.\nHowever, in later years, with new data available, Van Flandern himself admitted his findings were flawed, and the conclusions were contradicted by more accurate findings based on radio measurements with the Viking landers.\n\nVan Flandern and Henry Fliegel developed a compact algorithm to calculate a Julian date from a Gregorian date that would fit on a single IBM card. They described this in a letter to the editor of a computing magazine in 1968. This was available for use in business applications.\n\nWith Kenneth Pulkkinen, he published \"Low precision formulae for planetary positions\", in the Astrophysical Journal Supplement in 1979. The paper set a record for the number of reprints requested from that journal.\n\nIn 2003, developed the Van Flandern-Yang hypothesis with Xin-She Yang after observations made during the Solar eclipse of March 9, 1997.\n\nVan Flandern described in his book how he had become increasingly dissatisfied with the mainstream view of science by the early 1980s. He wrote\n\nIn his book, on blogs, lectures, newsletter and web site, Van Flandern focused on problems with cosmology and physics theories. He alleged that when experimental evidence is incompatible with mainstream scientific theories, that mainstream scientists refuse to acknowledge this to avoid jeopardizing their funding.\n\nVan Flandern espoused a set of principles for assessing ideas, and dubbed theories that he deemed compliant with these principles as \"Deep Reality Physics.\" He claimed that mainstream scientific theories, especially the prevailing theories regarding the big bang, solar system formation, relativity, and electrodynamics, left unanswered questions and therefore did not meet his criteria and often advocated his own replacement theories. Van Flandern's seven principles were:\n\nFollowing claims by David Dunham in 1978 to have detected satellites for some asteroids (notably 532 Herculina) by examining the light patterns during stellar occultations, Van Flandern and others began to report similar observations. His non-mainstream 1978 prediction that some asteroids have natural satellites, which was almost universally rejected, was proven correct when the \"Galileo\" spacecraft photographed Dactyl, a satellite of 243 Ida, during its flyby in 1993.\n\nIn 1976, while Van Flandern was employed by the USNO, he began to promote the belief that major planets sometimes explode. In his \"Exploded Planet Hypothesis 2000\" he lists as possible reasons for explosion either a runaway nuclear reaction in uranium in the core, a change of state as the planet cools down, creating a density phase change (like water to ice) and causing it to implode or explode, or absorption of heat from gravitons. In his book Van Flandern described the negative reception of his ideas about exploding planets among mainstream scientists. Van Flandern also speculated that the origin of the human species may well have been on the planet Mars, which he believed was once a moon of a now-exploded \"Planet V\".\n\nVan Flandern supported Le Sage's discredited theory of gravitation, according to which gravity is the result of a flux of invisible \"ultra-mundane corpuscles\" impinging on all objects from all directions at superluminal speeds. He gave public lectures in which he claimed that these particles could be used as a limitless source of free energy, and to provide superluminal propulsion for spacecraft. He also speculated that the ultra-mundane flux caused the explosion of a major planet once located between Mars and Jupiter.\n\nIn 1998 Van Flandern wrote a paper asserting that astronomical observations imply that gravity propagates at least twenty billion times faster than light, or even infinitely fast. These claims were dismissed by mainstream physicists.\n\nVan Flandern was a prominent advocate of the belief that certain geological features seen on Mars, especially the \"face at Cydonia\", are not of natural origin, but were produced by intelligent extra-terrestrial life, probably the inhabitants of a major planet once located where the asteroid belt presently exists, and which Van Flandern believed had exploded 3.2 million years ago. The claimed artificiality of the \"face\" was also the topic of a chapter of his 1993 book. He also gave lectures on the subject, and at the conclusion of the lectures he described his overall conception:\n\nWhen it was first imaged, and into the 21st century, the \"Face\" is near universally accepted to be an optical illusion, an example of pareidolia, and theories that it was an artificial artifact were considered to be pseudo-science. After analysis of the higher resolution Mars Global Surveyor data NASA stated that \"a detailed analysis of multiple images of this feature reveals a natural looking Martian hill whose illusory face-like appearance depends on the viewing angle and angle of illumination\".\n\nVan Flandern was a vocal opponent of the Big Bang model in cosmology, and supported instead a Steady-State cosmology. He compiled a list of what he regarded as problems for the Big Bang model. It began as a list of \"Top 10\" problems, then expanded to the \"Top 30\", and ultimately by 2008 had reached the \"Top 60\". In 2008 he was an organizer of a conference of individuals who oppose the Big Bang cosmological models. Van Flandern did not reject General Relativity as some have asserted, but rather rejected its geometrical interpretation. He said: \"General relativity has a geometric and a field interpretation. If angular momentum conservation is invoked in the geometric interpretation to explain experiments, the causality principle is violated. The field interpretation avoids this problem by allowing faster-than-light propagation in forward time.\"\n\n\n"}
{"id": "38532142", "url": "https://en.wikipedia.org/wiki?curid=38532142", "title": "Virtual colony count", "text": "Virtual colony count\n\nVirtual colony count (VCC) is a kinetic, 96-well microbiological assay originally developed to measure the activity of defensins. It has since been applied to other antimicrobial peptides including LL-37. It utilizes a method of enumerating bacteria called quantitative growth kinetics, which compares the time taken for a bacterial batch culture to reach a threshold optical density with that of a series of calibration curves. The name VCC has also been used to describe the application of quantitative growth kinetics to enumerate bacteria in cell culture infection models.\nAntimicrobial susceptibility testing (AST) can be done on 96-well plates by diluting the antimicrobial agent at varying concentrations in broth inoculated with bacteria and measuring the minimum inhibitory concentration that results in no growth. However, these methods cannot be used to study some membrane-active antimicrobial peptides, which are inhibited by the broth itself. The virtual colony count procedure takes advantage of this fact by first exposing bacterial cells to the active antimicrobial agent in a low-salt buffer for two hours, then simultaneously inhibiting antimicrobial activity and inducing exponential growth by adding broth. The growth kinetics of surviving cells can then be monitored using a temperature-controlled plate reader.\n\nThe method of enumeration of surviving cells used by VCC is termed quantitative growth kinetics (QGK). It relates the kinetic time taken for the turbidity of a bacterial batch microbiological culture in a well of a 96-well microplate to reach a threshold difference in turbidity to a 10-fold dilution series of calibration growth curves.\n\nQuantification of the number of viable cells is done using a process mathematically identical to quantitative real-time polymerase chain reaction (QPCR), except with QGK cells, rather than copies of PCR products, grow exponentially. The time taken to reach the threshold is called the \"threshold time\", T, which is equivalent to the QPCR value \"cycle time\" or C.\n\nThere are at least five processes that cause delays in threshold times in VCC assays:\n\n1. Adhesion, causing cells to stick to the microplate and possibly form biofilms. Unless these cells happen to be directly in the light path, their growth will not affect optical density readings.\n\n2. Cohesion, causing cells to aggregate into clumps of various sizes instead of a homogeneous suspension of individual cells undergoing binary fission. Cohesion can cause imprecision and fluctuations in T. Cohesive clumps may also be adhesive, leading to both imprecision due to cohesion and inaccuracy (increased T) due to adhesion.\n\n3. Bacteriostatic activity, causing cells to become unable to enter exponential growth even though they are not killed. Transient bacteriostatic activity can cause lag times, increasing T.\n\n4. The metabolic lag phase of bacterial growth. Such a lag phase would be expected to occur in the assay as cells growing slowly or not at all during the initial exposure to antimicrobial peptides in the low-salt buffer are shifted to exponential growth upon addition of twice-concentrated rich media. If this metabolic lag phase increases in the presence of the antimicrobial peptide, it could be considered a form of transient bacteriostatic activity in category 3, above, although other sources of transient bacteriostatic activity, such as a delay due to the time required for the repair of damaged cell structures such as cell walls or cell membranes, are possible.\n\n5. Bactericidal activity, or killing. Fewer surviving cells cause a delay in T as the survivors take longer to produce the same amount of turbidity through exponential growth. If all other processes causing increases in T are negligible, the VCC assay becomes a bactericidal assay and T can be used to enumerate viable bacteria by QGK. In this simplified case, VCC \"virtual survival\" results are equivalent to the \"survival\" results of a traditional colony count bactericidal assay.\n\nVCC was initially employed to quantify the antibacterial activity of peptides against six strains of \"Escherichia coli, Staphylococcus aureus, Bacillus cereus, and Enterobacter aerogenes\". Commonly, a standard Gram-negative and Gram-positive quality control strain are compared. \"Escherichia coli\" ATCC 25922 and \"Staphylococcus aureus\" ATCC 29213 have been used as the standard Gram-negative and Gram-positive strains, respectively. VCC has also been applied to \"Bacillus anthracis\", the causative agent of anthrax.\n\nThe initial virtual colony count study measured the activity of all six human alpha defensins concurrently on the same 96-well plate: HNP1, HNP2, HNP3, HNP4, HD5, and HD6. Subsequently, mutated forms of some of those six defensins were studied by VCC. A conserved glycine in a beta bulge in HNP2 was replaced with a series of D-amino acids resulting in VCC activity proportional to side chain hydrophobicity and charge. VCC showed that N-terminally acetylated and/or C-terminally amidated HNP2 activity is proportional to electrostatic charge. VCC results were again proportional to charge for a series of salt bridge-disrupting mutants, suggesting that the salt bridge is not required for HNP2 function. VCC measured the importance of N-terminal natural and artificial pro segments of the propeptide HNP1, dramatically altering activity against \"Escherichia coli\" and \"Staphylococcus aureus\". Enantiomer forms of HNP1, HNP4, HD5 and the beta defensin HBD2 composed entirely of D-amino acids suggested differing mechanisms of defensin activity against Gram-positive and Gram-negative bacteria. VCC results of dimerization-impaired monomer and tethered dimer forms of HNP1 demonstrated the importance of dimerization. Replacing the conserved glycine with L-alanine resulted in subtle VCC differences. Comprehensive alanine scanning mutagenesis of HNP1 and HD5 demonstrated the importance of bulky hydrophobic residues. These studies have recently been expanded to include additional beta defensins, theta defensins, and the human cathelicidin LL-37 and related peptides.\n\nVCC users are cautioned to transfer cells in a small volume such as 10 microliters beneath a larger volume such as 90 microliters, similar to the QGK calibration curves shown above and the calibration curves reported in the initial VCC publication, but unlike the experimental procedure used to test defensin activity in that same paper. The improved pipetting technique was described in 2011 in the study of the biosafety level 3 (BSL-3) pathogen \"Bacillus anthracis\". The original method published in 2005 involved the transfer of 50 microliters of cell suspensions to 50 microliters of liquid, which generates froth, bubbles and turbidity that is incompatible with a turbidimetric method when cells are transferred directly to the bottoms of the wells beneath the phosphate buffer solutions. Avoiding this problem by adding cell suspensions as droplets from above can cause aerosols that result in cross-contamination. Bioaerosols of hazardous bacteria can also pose safety risks that can be reduced by conducting experiments within a biosafety cabinet.\n\n"}
{"id": "323631", "url": "https://en.wikipedia.org/wiki?curid=323631", "title": "Wieferich prime", "text": "Wieferich prime\n\nIn number theory, a Wieferich prime is a prime number \"p\" such that \"p\" divides , therefore connecting these primes with Fermat's little theorem, which states that every odd prime \"p\" divides . Wieferich primes were first described by Arthur Wieferich in 1909 in works pertaining to Fermat's last theorem, at which time both of Fermat's theorems were already well known to mathematicians.\n\nSince then, connections between Wieferich primes and various other topics in mathematics have been discovered, including other types of numbers and primes, such as Mersenne and Fermat numbers, specific types of pseudoprimes and some types of numbers generalized from the original definition of a Wieferich prime. Over time, those connections discovered have extended to cover more properties of certain prime numbers as well as more general subjects such as number fields and the abc conjecture.\n\n, the only known Wieferich primes are 1093 and 3511 .\n\nThe stronger version of Fermat's little theorem, which a Wieferich prime satisfies, is usually expressed as a congruence relation . From the definition of the congruence relation on integers, it follows that this property is equivalent to the definition given at the beginning. Thus if a prime \"p\" satisfies this congruence, this prime divides the Fermat quotient formula_1. The following are two illustrative examples using the primes 11 and 1093:\n\nWieferich primes can be defined by other equivalent congruences. If \"p\" is a Wieferich prime, one can multiply both sides of the congruence by 2 to get . Raising both sides of the congruence to the power \"p\" shows that a Wieferich prime also satisfies , and hence for all . The converse is also true: for some implies that the multiplicative order of 2 modulo \"p\" divides gcd, φ, that is, and thus \"p\" is a Wieferich prime. This also implies that Wieferich primes can be defined as primes \"p\" such that the multiplicative orders of 2 modulo \"p\" and modulo \"p\" coincide: , (By the way, ord2 = 364, and ord2 = 1755).\n\nH. S. Vandiver proved that if and only if formula_4.\n\nIn 1902, W. F. Meyer proved a theorem about solutions of the congruence \"a\" ≡ 1 (mod \"p\"). Later in that decade Arthur Wieferich showed specifically that if the first case of Fermat's last theorem has solutions for an odd prime exponent, then that prime must satisfy that congruence for \"a\" = 2 and \"r\" = 2. In other words, if there exist solutions to \"x\" + \"y\" + \"z\" = 0 in integers \"x\", \"y\", \"z\" and \"p\" an odd prime with \"p\" ∤ \"xyz\", then \"p\" satisfies 2 ≡ 1 (mod \"p\"). In 1913, Bachmann examined the residues of formula_5. He asked the question when this residue vanishes and tried to find expressions for answering this question.\n\nThe prime 1093 was found to be a Wieferich prime by Waldemar Meissner in 1913 and confirmed to be the only such prime below 2000. He calculated the smallest residue of formula_6 for all primes \"p\" < 2000 and found this residue to be zero for \"t\" = 364 and \"p\" = 1093, thereby providing a counterexample to a conjecture by Grawe about the impossibility of the Wieferich congruence. E. Haentzschel later ordered verification of the correctness of Meissners congruence via only elementary calculations. Inspired by an earlier work of Euler, he simplified Meissners proof by showing that 1093 | (2 + 1) and remarked that (2 + 1) is a factor of (2 − 1). It was also shown that it is possible to prove that 1093 is a Wieferich prime without using complex numbers contrary to the method used by Meissner, although Meissner himself hinted at that he was aware of a proof without complex values.\n\nThe prime 3511 was first found to be a Wieferich prime by N. G. W. H. Beeger in 1922 and another proof of it being a Wieferich prime was published in 1965 by Guy. In 1960, Kravitz doubled a previous record set by Fröberg and in 1961 Riesel extended the search to 500000 with the aid of BESK. Around 1980, Lehmer was able to reach the search limit of 6. This limit was extended to over 2.5 in 2006, finally reaching 3. It is now known that if any other Wieferich primes exist, they must be greater than 6.7. \n\nIn 2007–2016, a search for Wieferich primes was performed by the distributed computing project Wieferich@Home. In 2011–2017, another search was performed by the PrimeGrid project, although later the work done in this project was claimed wasted. While these projects reached search bounds above 1, neither of them reported any sustainable results.\n\nIt has been conjectured (as for Wilson primes) that infinitely many Wieferich primes exist, and that the number of Wieferich primes below \"x\" is approximately log(log(\"x\")), which is a heuristic result that follows from the plausible assumption that for a prime \"p\", the degree roots of unity modulo \"p\" are uniformly distributed in the multiplicative group of integers modulo \"p\".\n\nThe following theorem connecting Wieferich primes and Fermat's last theorem was proven by Wieferich in 1909:\n\nThe above case (where \"p\" does not divide any of \"x\", \"y\" or \"z\") is commonly known as the first case of Fermat's last theorem (FLTI) and FLTI is said to fail for a prime \"p\", if solutions to the Fermat equation exist for that \"p\", otherwise FLTI holds for \"p\".\nIn 1910, Mirimanoff expanded the theorem by showing that, if the preconditions of the theorem hold true for some prime \"p\", then \"p\" must also divide . Granville and Monagan further proved that \"p\" must actually divide for every prime \"m\" ≤ 89. Suzuki extended the proof to all primes \"m\" ≤ 113.\n\nLet \"H\" be a set of pairs of integers with 1 as their greatest common divisor, \"p\" being prime to \"x\", \"y\" and \"x\" + \"y\", (\"x\" + \"y\") ≡ 1 (mod p), (\"x\" + \"ξy\") being the \"p\"th power of an ideal of \"K\" with \"ξ\" defined as cos 2\"π\"/\"p\" + \"i\" sin 2\"π\"/\"p\". \"K\" = Q(\"ξ\") is the field extension obtained by adjoining all polynomials in the algebraic number \"ξ\" to the field of rational numbers (such an extension is known as a number field or in this particular case, where \"ξ\" is a root of unity, a cyclotomic number field). \nFrom uniqueness of factorization of ideals in Q(ξ) it follows that if the first case of Fermat's last theorem has solutions \"x\", \"y\", \"z\" then \"p\" divides \"x\"+\"y\"+\"z\" and (\"x\", \"y\"), (\"y\", \"z\") and (\"z\", \"x\") are elements of \"H\".\nGranville and Monagan showed that (1, 1) ∈ \"H\" if and only if \"p\" is a Wieferich prime.\n\nA non-Wieferich prime is a prime \"p\" satisfying . J. H. Silverman showed in 1988 that if the abc conjecture holds, then there exist infinitely many non-Wieferich primes. More precisely he showed that the abc conjecture implies the existence of a constant only depending on \"α\" such that the number of non-Wieferich primes to base \"α\" with \"p\" less than or equal to a variable \"X\" is greater than log(\"X\") as \"X\" goes to infinity. Numerical evidence suggests that very few of the prime numbers in a given interval are Wieferich primes. The set of Wieferich primes and the set of non-Wieferich primes, sometimes denoted by \"W\" and \"W\" respectively, are complementary sets, so if one of them is shown to be finite, the other one would necessarily have to be infinite, because both are proper subsets of the set of prime numbers. It was later shown that the existence of infinitely many non-Wieferich primes already follows from a weaker version of the abc conjecture, called the \"ABC\"-(\"k\", \"ε\") \"conjecture\". Additionally, the existence of infinitely many non-Wieferich primes would also follow if there exist infinitely many square-free Mersenne numbers as well as if there exists a real number \"ξ\" such that the set {\"n\" ∈ N : λ(2 − 1) < 2 − \"ξ\"} is of density one, where the \"index of composition\" \"λ\"(\"n\") of an integer \"n\" is defined as formula_7 and formula_8, meaning formula_9 gives the product of all prime factors of \"n\".\n\nIt is known that the \"n\"th Mersenne number is prime only if \"n\" is prime. Fermat's little theorem implies that if is prime, then \"M\" is always divisible by \"p\". Since Mersenne numbers of prime indices \"M\" and \"M\" are co-prime,\nThus, a Mersenne prime cannot also be a Wieferich prime. A notable open problem is to determine whether or not all Mersenne numbers of prime index are square-free. If \"q\" is prime and the Mersenne number \"M\" is \"not\" square-free, that is, there exists a prime \"p\" for which \"p\" divides \"M\", then \"p\" is a Wieferich prime. Therefore, if there are only finitely many Wieferich primes, then there will be at most finitely many Mersenne numbers with prime index that are not square-free. Rotkiewicz showed a related result: if there are infinitely many square-free Mersenne numbers, then there are infinitely many non-Wieferich primes.\n\nSimilarly, if \"p\" is prime and \"p\" divides some Fermat number \"F\" , then \"p\" must be a Wieferich prime.\n\nIn fact, there exists a natural number \"n\" and a prime \"p\" that \"p\" divides formula_10 (where formula_11 is the \"n\"-th cyclotomic polynomial) if and only if \"p\" is a Wieferich prime. For example, 1093 divides formula_12, 3511 divides formula_13. Mersenne and Fermat numbers are just special situations of formula_10. Thus, if 1093 and 3511 are only two Wieferich primes, then all formula_10 are square-free except formula_12 and formula_13 (In fact, when there exists a prime \"p\" which \"p\" divides some formula_10, then it is a Wieferich prime); and clearly, if formula_10 is a prime, then it cannot be Wieferich prime. (Notice any odd prime \"p\" divides only one formula_10 and \"n\" divides , and if and only if the period length of 1/p in binary is \"n\", then \"p\" divides formula_10. Besides, if and only if \"p\" is a Wieferich prime, then the period length of 1/p and 1/p are the same (in binary). Otherwise, this is \"p\" times than that.)\n\nFor the primes 1093 and 3511, it was shown that neither of them is a divisor of any Mersenne number with prime index nor a divisor of any Fermat number, because 364 and 1755 are neither prime nor powers of 2.\n\nScott and Styer showed that the equation \"p\" – 2 = \"d\" has at most one solution in positive integers (\"x\", \"y\"), unless when \"p\" | 2 – 1 if \"p\" ≢ 65 (mod 192) or unconditionally when \"p\" | 2 – 1, where ord 2 denotes the multiplicative order of 2 modulo \"p\". They also showed that a solution to the equation ±\"a\" ± 2 = ±\"a\" ± 2 = \"c\" must be from a specific set of equations but that this does not hold, if \"a\" is a Wieferich prime greater than 1.25 x 10.\n\nJohnson observed that the two known Wieferich primes are one greater than numbers with periodic binary expansions (1092 = 010001000100=444; 3510 = 110110110110=6666). The Wieferich@Home project searched for Wieferich primes by testing numbers that are one greater than a number with a periodic binary expansion, but up to a \"bit pseudo-length\" of 3500 of the tested binary numbers generated by combination of bit strings with a bit length of up to 24 it has not found a new Wieferich prime.\n\nIt has been noted that the known Wieferich primes are one greater than mutually friendly numbers (the shared abundancy index being 112/39).\n\nIt was observed that the two known Wieferich primes are the square factors of all non-square free base-2 Fermat pseudoprimes up to 25. Later computations showed that the only repeated factors of the pseudoprimes up to 10 are 1093 and 3511. In addition, the following connection exists: \n\nFor all primes up to , only in two cases: and , where is the number of vertices in the cycle of 1 in the \"doubling diagram\" modulo . Here the doubling diagram represents the directed graph with the non-negative integers less than \"m\" as vertices and with directed edges going from each vertex \"x\" to vertex 2\"x\" reduced modulo \"m\". It was shown, that for all odd prime numbers either or .\n\nIt was shown that formula_24 and formula_25 if and only if where \"p\" is an odd prime and formula_26 is the fundamental discriminant of the imaginary quadratic field formula_27. Furthermore, the following was shown: Let \"p\" be a Wieferich prime. If , let formula_26 be the fundamental discriminant of the imaginary quadratic field formula_29 and if , let formula_26 be the fundamental discriminant of the imaginary quadratic field formula_31. Then formula_24 and formula_25 (\"χ\" and \"λ\" in this context denote Iwasawa invariants).\n\nFurthermore, the following result was obtained: Let \"q\" be an odd prime number, \"k\" and \"p\" are primes such that and the order of \"q\" modulo \"k\" is formula_34. Assume that \"q\" divides \"h\", the class number of the real cyclotomic field formula_35, the cyclotomic field obtained by adjoining the sum of a \"p\"-th root of unity and its reciprocal to the field of rational numbers. Then \"q\" is a Wieferich prime. This also holds if the conditions and are replaced by and as well as when the condition is replaced by (in which case \"q\" is a Wall–Sun–Sun prime) and the incongruence condition replaced by .\n\nA prime \"p\" satisfying the congruence 2 (mod \"p\") with small |\"A\"| is commonly called a \"near-Wieferich prime\" . Near-Wieferich primes with \"A\" = 0 represent Wieferich primes. Recent searches, in addition to their primary search for Wieferich primes, also tried to find near-Wieferich primes. The following table lists all near-Wieferich primes with |\"A\"| ≤ 10 in the interval [1, 3]. This search bound was reached in 2006 in a search effort by P. Carlisle, R. Crandall and M. Rodenkirch.\nThe sign +1 or -1 above can be easily predicted by Euler's criterion (and the second supplement to the law of quadratic reciprocity).\n\nDorais and Klyve used a different definition of a near-Wieferich prime, defining it as a prime \"p\" with small value of formula_36 where formula_37 is the Fermat quotient of 2 with respect to \"p\" modulo \"p\" (the modulo operation here gives the residue with the smallest absolute value). The following table lists all primes \"p\" ≤ with formula_38.\n\nThe two notions of nearness are related as follows. If formula_39, then by squaring, clearly formula_40. So if had been chosen with formula_41 small, then clearly formula_42 is also (quite) small, and an even number. However, when formula_43 is odd above, the related from before the last squaring was not \"small\". For example, with formula_44, we have formula_45 which reads extremely non-near, but after squaring this is formula_46 which is a near-Wieferich by the second definition.\n\nA \"Wieferich prime base a\" is a prime \"p\" that satisfies\nSuch a prime cannot divide \"a\", since then it would also divide 1.\n\nIt's a conjecture that for every natural number \"a\", there are infinitely many Wieferich primes in base \"a\".\n\nBolyai showed that if \"p\" and \"q\" are primes, \"a\" is a positive integer not divisible by \"p\" and \"q\" such that , , then . Setting \"p\" = \"q\" leads to . It was shown that if and only if .\n\nKnown solutions of for small values of \"a\" are: (checked up to 5 × 10)\n\nFor more information, see and. (Note that the solutions to \"a\" = \"b\" is the union of the prime divisors of \"k\" which does not divide \"b\" and the solutions to \"a\" = \"b\")\n\nThe smallest solutions of are\n\nThere are no known solutions of for \"n\" = 47, 72, 186, 187, 200, 203, 222, 231, 304, 311, 335, 347, 355, 435, 454, 542, 546, 554, 610, 639, 662, 760, 772, 798, 808, 812, 858, 860, 871, 983, 986, 1002, 1023, 1130, 1136, 1138, ...\n\nIt is a conjecture that there are infinitely many solutions of for every natural number \"a\".\n\nThe bases \"b\" < \"p\" which \"p\" is a Wieferich prime are (for \"b\" > \"p\", the solutions are just shifted by \"k\"·\"p\" for \"k\" > 0), and there are solutions < \"p\" of \"p\" and the set of the solutions congruent to \"p\" are {1, 2, 3, ..., \n\nThe least base \"b\" > 1 which prime(\"n\") is a Wieferich prime are\n\nWe can also consider the formula formula_47, (because of the generalized Fermat little theorem, formula_47 is true for all prime \"p\" and all natural number \"a\" such that both \"a\" and are not divisible by \"p\"). It's a conjecture that for every natural number \"a\", there are infinitely many primes such that formula_47.\n\nKnown solutions for small \"a\" are: (checked up to 4 × 10) \n\nA Wieferich pair is a pair of primes \"p\" and \"q\" that satisfy\nso that a Wieferich prime \"p\" ≡ 1 (mod 4) will form such a pair (\"p\", 2): the only known instance in this case is . There are only 7 known Wieferich pairs.\n\nStart with a(1) any natural number (>1), a(\"n\") = the smallest prime \"p\" such that (a(\"n\" − 1)) = 1 (mod \"p\") but \"p\" does not divide a(\"n\" − 1) − 1 or a(\"n\" − 1) + 1. (If \"p\" divides a(\"n\" − 1) − 1 or a(\"n\" − 1) + 1, then the solution is a trivial solution) It is a conjecture that every natural number \"k\" = a(1) > 1 makes this sequence become periodic, for example, let a(1) = 2:\n\nLet a(1) = 83:\n\nLet a(1) = 59 (a longer sequence):\n\nHowever, there are many values of a(1) with unknown status, for example, let a(1) = 3:\n\nLet a(1) = 14:\n\nLet a(1) = 39 (a longer sequence):\n\nIt is unknown that values for a(1) > 1 exist such that the resulting sequence does not eventually become periodic.\n\nWhen a(\"n\" − 1)=\"k\", a(\"n\") will be (start with \"k\" = 2): 1093, 11, 1093, 20771, 66161, 5, 1093, 11, 487, 71, 2693, 863, 29, 29131, 1093, 46021, 5, 7, 281, ?, 13, 13, 25633, 20771, 71, 11, 19, ?, 7, 7, 5, 233, 46145917691, 1613, 66161, 77867, 17, 8039, 11, 29, 23, 5, 229, 1283, 829, ?, 257, 491531, ?, ... (For \"k\" = 21, 29, 47, 50, even the next value is unknown)\n\nA Wieferich number is an odd natural number \"n\" satisfying the congruence 2 ≡ 1 (mod \"n\"), where denotes the Euler's totient function (according to Euler's theorem, 2 ≡ 1 (mod \"n\") for every odd natural number \"n\"). If Wieferich number \"n\" is prime, then it is a Wieferich prime. The first few Wieferich numbers are:\nIt can be shown that if there are only finitely many Wieferich primes, then there are only finitely many Wieferich numbers. In particular, if the only Wieferich primes are 1093 and 3511, then there exist exactly 104 Wieferich numbers, which matches the number of Wieferich numbers currently known.\n\nMore generally, a natural number \"n\" is a Wieferich number to base \"a\", if \"a\" ≡ 1 (mod \"n\").\n\nAnother definition specifies a Wieferich number as odd natural number \"n\" such that \"n\" and formula_50 are not coprime, where \"m\" is the multiplicative order of 2 modulo \"n\". The first of these numbers are:\nAs above, if Wieferich number \"q\" is prime, then it is a Wieferich prime.\n\nA weak Wieferich prime to base \"a\" is a prime \"p\" satisfies the condition\n\nEvery Wieferich prime to base \"a\" is also a weak Wieferich prime to base \"a\". If the base \"a\" is squarefree, then a prime \"p\" is a weak Wieferich prime to base \"a\" if and only if \"p\" is a Wieferich prime to base \"a\".\n\nSmallest weak Wieferich prime to base \"n\" are (start with \"n\" = 0)\n\nFor integer \"n\" ≥2, a Wieferich prime to base \"a\" with order \"n\" is a prime \"p\" satisfies the condition\n\nClearly, a Wieferich prime to base \"a\" with order \"n\" is also a Wieferich prime to base \"a\" with order \"m\" for all 2 ≤ \"m\" ≤ \"n\", and Wieferich prime to base \"a\" with order 2 is equivent to Wieferich prime to base \"a\", so we can only consider the \"n\" ≥ 3 case. However, there are no known Wieferich prime to base 2 with order 3. The first base with known Wieferich prime with order 3 is 9, where 2 is a Wieferich prime to base 9 with order 3. Besides, both 5 and 113 are Wieferich prime to base 68 with order 3.\n\nA Lucas–Wieferich prime associated with the pair of integers (\"P\", \"Q\") is a prime \"p\" such that \"U\"(\"P\", \"Q\") ≡ 0 (mod \"p\"), where \"U\"(\"P\", \"Q\") denotes the Lucas sequence of the first kind and \"ε\" equals the Legendre symbol formula_51. All Wieferich primes are Lucas-Wieferich primes associated with the pair (3, 2).\n\nLet \"Q\" = −1, \"P\" be any natural number, these primes are called P\"-Fibonacci–Wieferich primes or P\"-Wall–Sun–Sun primes, and if \"P\" = 1, they are called Fibonacci-Wieferich primes, and if \"P\" = 2, they are called Pell–Wieferich primes. For example, 241 is a Wieferich prime when \"P\" = 3, so it is a 3-Fibonacci-Wieferich prime or 3-Wall–Sun–Sun prime. In fact, 3 is an \"n\"-Fibonacci-Wieferich prime if and only if \"n\" congruent to 0, 4, or 5 (mod 9), like the traditional Wieferich primes, 3 is a base \"n\" Wieferich prime if and only if \"n\" congruent to 1 or 8 (mod 9).\n\nLet \"K\" be a global field, i.e. a number field or a function field in one variable over a finite field and let \"E\" be an elliptic curve. If \"v\" is a non-archimedean place of norm \"q\" of \"K\" and a ∈ K, with \"v\"(\"a\") = 0 then ≥ 1. \"v\" is called a \"Wieferich place\" for base \"a\", if > 1, an \"elliptic Wieferich place\" for base \"P\" ∈ \"E\", if \"NP\" ∈ \"E\" and a \"strong elliptic Wieferich place\" for base \"P\" ∈ \"E\" if \"nP\" ∈ \"E\", where \"n\" is the order of \"P\" modulo \"v\" and \"N\" gives the number of rational points (over the residue field of \"v\") of the reduction of \"E\" at \"v\".\n\n\n\n"}
