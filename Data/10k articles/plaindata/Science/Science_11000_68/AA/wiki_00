{"id": "14615689", "url": "https://en.wikipedia.org/wiki?curid=14615689", "title": "A476 road", "text": "A476 road\n\nThe A476 is a main road in Wales, linking Llanelli with the A483, the Swansea to Manchester Trunk road near the market town of Llandeilo.\n\nSettlements served by the road include:\n"}
{"id": "1349840", "url": "https://en.wikipedia.org/wiki?curid=1349840", "title": "A Letter to a Friend", "text": "A Letter to a Friend\n\nA Letter to a Friend (written 1656; published posthumously in 1690), by Sir Thomas Browne, the 17th century philosopher and physician, is a medical treatise of case-histories and witty speculations upon the human condition.\n\nIt is believed to be the source of a term Mary Leitao found in 2001 to describe her son's skin condition. She chose the name \"Morgellons disease\" from a skin condition described by Browne in \"Letter to a Friend\", thus:\n\nThere is, however, no suggestion that the symptoms described by Browne are linked to the alleged modern cases of Morgellons.\n\n"}
{"id": "5111684", "url": "https://en.wikipedia.org/wiki?curid=5111684", "title": "Alan T. Waterman Award", "text": "Alan T. Waterman Award\n\nThe Alan T. Waterman Award is the United States's highest honorary award for scientists no older than 40, or no more than 10 years past receipt of their Ph.D. It is awarded on a yearly basis by the National Science Foundation. In addition to the medal, the awardee receives a grant of $1,000,000 to be used at the institution of their choice over a period of five years for advanced scientific research. \n\nCongress established the annual award in August 1975 to mark the 25th Anniversary of the National Science Foundation and to honor its first Director, Alan T. Waterman. The annual award recognizes an outstanding young researcher in any field of science or engineering supported by the National Science Foundation. \n\nCandidates must be U.S. citizens or permanent residents. Prior to the 2018 competition, candidates must have been 35 years of age or younger or not more than 7 years beyond receipt of the Ph.D. degree by December 31 of the year in which they are nominated. As of the 2018 competition, these requirements were changed to 40 years of age or 10 years post-PhD. Candidates should have demonstrated exceptional individual achievements in scientific or engineering research of sufficient quality to place them at the forefront of their peers. Criteria include originality, innovation, and significant impact on the field. Potential candidates must be nominated and require four letters of reference, but none can be submitted from the nominee’s home institution. Solicitation announcements are sent to universities and colleges, scientific, engineering and other professional societies and organizations, members of the National Academy of Sciences and the National Academy of Engineering.\n\nCandidates are reviewed by the Alan T. Waterman Award committee, which is made up of 12 members, 8 rotators and 4 members \"ex officio\". The current \"ex officio\" members are Ralph Cicerone, President of the National Academy of Sciences, Subra Suresh, Director of the National Science Foundation, Steven C. Beering, Chairman of the National Science Board, and Charles M. Vest, President of the National Academy of Engineering. After review of the nominees, the Committee recommends the most outstanding candidate(s) to the Director of the National Science Foundation and the National Science Board, which then makes the final determination.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "58626404", "url": "https://en.wikipedia.org/wiki?curid=58626404", "title": "Alessandro Strumia", "text": "Alessandro Strumia\n\nAlessandro Strumia (born 26 December 1969) is a physicist at the University of Pisa.\n\nStrumia obtained his PhD in 1995 at the University of Pisa, where his doctoral advisor was Riccardo Barbieri. His thesis was titled \"Supersymmetric unification\".\n\nStrumia's research specialization is in physics beyond the Standard Model. In 1995 with Riccardo Barbieri and Lawrence J. Hall he studied flavour and CP violations, present in supersymmetric unified theories even in absence of any flavour or CP violation in the input for the soft-supersymmetry breaking parameters. He is one of the originators of the idea of Minimal Flavor Violation, a paradigm to characterize the effects of flavor transitions in new theories of particle physics.\nAfter the discovery of the Higgs boson, he computed the probability that the Higgs vacuum undergoes quantum tunnelling, finding that the universe is in a critical state which will eventually end in a cosmic collapse. After the OPERA experiment reported an observation of neutrinos apparently traveling faster than light, Strumia in collaboration with Gian Giudice and Sergey Sibiryakov showed that superluminal neutrinos would imply some anomalies in the velocities of electrons and muons as a result of quantum-mechanical effects. Such anomalies could be already ruled out from existing data on cosmic rays, thus contradicting the OPERA results. He joined the European Organization for Nuclear Research (CERN)'s theory division as a fellow in 2000, and as a member of the CMS Collaboration, he was a credited coauthor on the paper which announced the Higgs boson discovery; his primary affiliation was Estonia's National Institute of Chemical Physics and Biophysics. Along with Joseph Lykken and other collaborators, he later proposed the \"modified naturalness\" hypothesis for the Higgs boson's mass.\n\nWhile at CERN in June 2018, Strumia and Riccardo Torre worked on a new set of algorithms with which to evaluate the impact of published scientific research. Basing their investigation on PageRank used by Google, they proposed a similar system of ranking scientific papers and authors. Researchers had \"mixed reaction\", suggesting that it would be useful for \"lifetime achievement\" but possibly subject to \"transparency issues\". The \"simplicity\" of current methods of evaluation allows for gaming the system. The difference in Strumia and Torres' approach is that they include what they describe as \"second-generation\" and later-generation citations in their algorithms. Therefore, not only the original citations of the work are taken into account, but subsequent citations to derivative material also. They named their systems \"PaperRank\" and \"AuthorRank\". They also proposed a system called \"CitationCoin\" to reduce the effect of groups who \"inflate\" each other citation count.\n\nMarco Cirelli and Alessandro Strumia were amongst multiple teams that used digital photos from a conference presentation in 2008 in Stockholm for a subsequent publication. The slide showed a highly anticipated but yet unpublished measurement of the positron fraction in cosmic rays by the PAMELA collaboration.\n\nOn 28 September 2018, Strumia gave a presentation at CERN's first Workshop on High Energy Theory and Gender that provoked considerable controversy. Citing an analysis he had performed on data from the InSpire database, he rejected the idea that physics suffers from gender bias against women and claimed that male scientists were victims of discrimination. Strumia cited previous research which points to results commonly referred to as the \"gender-equality paradox\" — the observations that countries that score higher on measures of gender equality have a lower proportion of women in STEM fields.\n\nOn 30 September 2018 CERN published a short statement, removed the slides of Strumia's presentation from its conference website and on 1 October suspended him from his \"invited scientist\" position. A longer statement commenting on Strumia's talk, published on 2 October, received nearly 4000 signatures as of 13 October, including those of John Ellis, Howard Georgi and David Gross. One supporter of Strumia was the scientist and string theorist Luboš Motl.Physicist Sabine Hossenfelder cited papers addressing some of Strumia's conclusions and provided an alternative analysis, arguing that after accounting for disproportionately higher rates of women leaving the field the sex differences Strumia claims to have found become negligible.\n\n\n"}
{"id": "16067696", "url": "https://en.wikipedia.org/wiki?curid=16067696", "title": "Amen Clinics", "text": "Amen Clinics\n\nAmen Clinics is a group of mental and physical health clinics that work on the treatment of mood and behavior disorders. It was founded in 1989 by Daniel G. Amen a self-help guru and psychiatrist. The clinics perform clinical evaluations and brain SPECT (single photon emission computed tomography) imaging to diagnose and treat their patients. Amen Clinics uses SPECT scans, a type of brain-imaging technology, to measure neural activity through blood flow. It has a database of more than 100,000 functional brain scans from patients in 111 countries.\n\nAmen Clinics has locations in Newport Beach, California; San Francisco, California; Atlanta, Georgia; Reston, Virginia; Bellevue, Washington; and New York City.\n\nThe American Psychiatric Association have criticized the clinical appropriateness of Amen's use of brain scans, stating, \"the clinical utility of neuroimaging techniques for planning of individualized treatment has not yet been shown.\"\n\nAmen Clinics was founded in 1989. It has been using brain SPECT in an attempt to diagnose and treat psychiatric illness since 1991. Amen Clinics incorporates questionnaires, clinical histories, and clinical interviews in its practice. Some Amen Clinics locations also use quantitative electroencephalography as a diagnostic tool. Amen Clinics has scanned 50,000 people at an estimated cost of $170 million according to Daniel Amen.\n\nAs of 2014, Amen Clinics had a database of more than 100,000 functional brain scans. The subjects are from 111 countries with ages from 9 months to 101 years old. The database was funded in part by Seeds Foundation in Hong Kong, and developed by Daniel Amen with a team of researchers including Kristen Willeumier.\n\nAmen Clinics has worked to treat athletics-related brain damage for professional athletes, including current and 117 former National Football League players.\n\nAmen Clinics uses SPECT scans to measure blood flow and activity patterns in the brain. The company also uses diagnostics such as questionnaires, clinical histories, and clinical interviews. Amen Clinics claims that SPECT scans enable doctors to tailor treatment to individual patients' brains. A retrospective study released by Amen in 2010 showed that \"regional cerebral blood flow, as measured by SPECT, predicted stimulant response in 29 of 157 patients.\"\n\nHarriet Hall has written critically about SPECT scans in articles for Quackwatch and for the \"Science-Based Medicine\" website. Hall accuses the clinics of misrepresenting an unproven treatment as effective, of concealing important warning information, and of creating false hopes by promising things that can't be done. She dismisses the scans as \"pretty pictures\" and says that although Amen himself seems to believe in his approach, \"humans are very good at fooling themselves\".\n\nA 2011 paper co-authored by the neuroscientist Anjan Chatterjee discussed example cases that were found on the Amen Clinic's website. The paper noted that the example cases \"violate the standard of care\" because a normal clinical diagnosis would have been sufficient and functional neuroimaging was unnecessary. According to the American Psychiatric Association, \"the clinical utility of neuroimaging techniques for planning of individualized treatment has not yet been shown.\"\n"}
{"id": "7329655", "url": "https://en.wikipedia.org/wiki?curid=7329655", "title": "Archon X Prize", "text": "Archon X Prize\n\nThe Archon Genomics X PRIZE presented by Express Scripts for Genomics, the second X Prize offered by the X Prize Foundation, based in Playa Vista, California, was announced on October 4, 2006 stating that the prize of \"$10 million will be awarded to the first team to rapidly, accurately and economically sequence 100 whole human genomes to an unprecedented level of accuracy.\" The 30 day evaluation phase of the competition to begin on September 5, 2013, was canceled August 22, 2013 and this cancellation was debated on March 27, 2014.\n\nIn November 2011 the prize goals were stated as: \"The $10 million grand prize will be awarded to the team(s) able to sequence 100 human genomes within 30 days to an accuracy of 1 error per 1,000,000 bases, with 98% completeness, identification of insertions, deletions and rearrangements, and a complete haplotype, at an audited total cost of $1,000 per genome.\"\n\nThe $10 million was donated by Canadian geologist and philanthropist Stewart Blusson, who co-discovered the Ekati Diamond Mine. The name \"Archon\" is the name of Blusson's company, which refers to the type of lithosphere beneath northern Canada. Upon cancellation, the money was returned to the Blussons because no Master Team Agreements were in place.\n\nThe Archon X Prize in genomics began as a joint effort of the X Prize Foundation and the J. Craig Venter Science Foundation. The J. Craig Venter Science Foundation offered the $500,000 (US) Innovation in Genomics Science and Technology Prize in September 2003 aimed at stimulating development of less expensive and faster sequencing technology. To attract even more resources to this goal, Dr. Venter joined forces with the X Prize Foundation, wrapping his competition and prize purse into a later incarnation, The Archon Genomics X Prize presented by Express Scripts.\n\nThe 100 human genomes to be sequenced in this competition were donated by 100 centenarians (ages 100 or older) from all over the world, known as the 100 Over 100. Sequencing the genomes of the 100 Over 100 presented an unprecedented opportunity to identify those \"rare genes\" that protect against diseases, while giving researchers valuable clues to health and longevity. These centenarians’ genes would provide us with a window to the past, significantly impacting the future of healthcare. Although the contest is cancelled, the X PRIZE foundation collected blood samples and created cell-lines to preserve the DNA from more than 100 centenarians. Those genomes are expected to be sequenced nonetheless and put into an open data forum.\n\nThe only two pre-registered teams were Ion Torrent in Guilford, CT and Wyss Institute for Biologically Inspired Engineering in Boston MA. These teams were announced 23-Jul-2012 and 3-Oct-2012.\n\nThe result was going to be the world's first \"medical grade” genome, a critically needed clinical standard that would transform genomic research into usable medical information to improve patient diagnosis and treatment. This global competition was expected to inspire breakthrough genome sequencing innovations and technologies that would usher in a new era of personalized medicine.\n\nThe evaluation phase of the competition was officially set to begin on September 5, 2013, and conclude 30 days later on October 5, but was canceled August 22, 2013 because it was \"outpaced by innovation\". A public debate concerning the validity and potential implications of the cancellation was published March 27, 2014.\n"}
{"id": "27090911", "url": "https://en.wikipedia.org/wiki?curid=27090911", "title": "Arthropod bites and stings", "text": "Arthropod bites and stings\n\nMany species of arthropods (insects, arachnids and others) regularly or occasionally bite or sting human beings. Insect saliva contains anticoagulants and enzymes that cause local irritation and allergic reactions. Insect venoms can be delivered by their stingers, which often are modified ovipositors, or by their mouthparts. Insect, spider and scorpion venom can cause serious injury or death. Dipterans account for the majority of insect bites, while hymenopterans account for the majority of stings. Among arachnids spider bites and mite bites are the most common. Arthropods bite or sting humans for a number of reasons including feeding or defense. Arthropods are major vectors of human disease, with the pathogens typically transmitted by bites.\n\n\n\n\n\n\n\n\n\n"}
{"id": "29402982", "url": "https://en.wikipedia.org/wiki?curid=29402982", "title": "Carl Bovallius", "text": "Carl Bovallius\n\nCarl Erik Alexander Bovallius (or Bowallius) (31 July 1849 – 8 November 1907) was a Swedish biologist and archaeologist.\nBovallius became a student in Uppsala in 1868, and received a Ph.D. in 1875. Bovallius undertook, for scientific purposes, travel along the Swedish and Norwegian coasts.\n\nIn 1881-83, he made zoological and ethnographic studies in Latin America, to where he returned in the late 1890s.\n\nStarting in 1881, Carl Bovallius explored Central America, and especially Nicaragua, in search of ancient sites. He studied places like Ometepe, and Zapatera, and also researched the ethnography of local tribes.\n\nA species of snake, \"Rhinobothryum bovallii\", is named in his honor.\n"}
{"id": "2031489", "url": "https://en.wikipedia.org/wiki?curid=2031489", "title": "Channel stuffing", "text": "Channel stuffing\n\nChannel stuffing is a business practice in which a company, or a sales force within a company, inflates its sales figures by forcing more products through a distribution channel than the channel is capable of selling. Also known as trade loading, this can be the result of a company attempting to inflate its sales figures. Alternatively, it can be a consequence of a poorly managed sales force attempting to meet short term objectives and quotas in a way that is detrimental to a company in the long term.\n\nChannel stuffing has a number of long-term consequences for a company. Firstly, distributors will often return any unsold goods to the company, incurring a carrying cost and also developing a backlog of product inventory. Wildly fluctuating demand, combined with this excess inventory, leads to costly overtimes and factory shutdowns. Even mild channel stuffing can spiral out of control as sales works to make up for prior overselling. Discounts used to drive trade loading can greatly affect profits, and even help establish gray market channels as salesmen no longer adequately qualify their prospects. \n\nOccasionally, distribution channels such as large retailers have been known to identify the practice of channel stuffing in their suppliers, and use the phenomenon to their advantage. This is done by holding back on orders until the end of the suppliers' quota period. The supplier's sales force then panics, and sells a large amount of the product under more favorable terms than they would under ordinary circumstances. At the beginning of the next period, no new orders are placed, and, barring any action, the cycle is then repeated. This affects customers, with gluts and shortages as buyers turn to competing products. \n\nCorporations have been known to engage in channel stuffing and hide such activities from their investors. In the United States, the U.S. Securities and Exchange Commission has in some cases litigated against such corporations, and private class-action suits have been engaged. Channel stuffing might also be part of a broader set of financial improprieties.\n"}
{"id": "58080652", "url": "https://en.wikipedia.org/wiki?curid=58080652", "title": "Christine Charles", "text": "Christine Charles\n\nChristine Charles is a physicist at the Australian National University in Canberra, Australia, an inventor, researcher and science communicator. Her position at the Australian National University is director of the Space Plasma, Power and Propulsion Laboratory.\n\nCharles was born in Brittany, France, and studied engineering and applied physics at university in France. She completed a Ph. D in plasma physics and a bachelor of music in jazz from the Australian National University. Her specialist field is experimental expanding plasmas (hot ionized gases) and their use in electric propulsion, microelectronics and optoelectronics, astrophysical plasmas, and the development of fuel cells for the hydrogen economy. Charles invented the Helicon Double Layer Thruster, an electrode-less magneto-plasma space engine which could be used for keeping satellite stations in orbit, or interplanetary space travel for humans. \n\nCharles broadcasts and discusses her research through a range of media including television (ABC Catalyst and Discovery Channel), radio and public lectures. \n\nIn 2009 Charles received the Australian Institute of Physics Women in Physics Lecturer of the Year Award. In 2011 she was a finalist in the Australian Innovation Challenge and the World Technology Awards. In 2013 she was elected to the American Physical Society. In 2015 she received the Women in Industry Excellence in Engineering Award and was elected to the Australian Academy of Science.\n"}
{"id": "1429083", "url": "https://en.wikipedia.org/wiki?curid=1429083", "title": "Comparison of file transfer protocols", "text": "Comparison of file transfer protocols\n\nThis article lists communication protocols that are designed for file transfer over a telecommunications network.\n\nProtocols for shared file systems—such as 9P and the Network File System—are beyond the scope of this article, as are file synchronization protocols.\n\nA packet-switched network transmits data that is divided into units called \"packets\". A packet comprises a header (which describes the packet) and a payload (the data). The Internet is a packet-switched network, and most of the protocols in this list are designed for its protocol stack, the IP protocol suite.\n\nThey use one of two transport layer protocols: the Transmission Control Protocol (TCP) or the User Datagram Protocol (UDP). In the tables below, the \"Transport\" column indicates which protocol(s) the transfer protocol uses at the transport layer. Some protocols designed to transmit data over UDP also use a TCP port for oversight.\n\nThe \"Server port\" column indicates the port from which the server transmits data. In the case of FTP, this port differs from the listening port. Some protocols—including FTP, FTP Secure, FASP, and Tsunami—listen on a \"control port\" or \"command port\", at which they receive commands from the client.\n\nSimilarly, the encryption scheme indicated in the \"Encryption\" column applies to transmitted data only, and not to the authentication system.\n\nThe \"Managed\" column indicates whether the protocol is designed for managed file transfer (MFT). MFT protocols prioritise secure transmission in industrial applications that require such features as auditable transaction records, monitoring, and end-to-end data security. Such protocols may be preferred for electronic data interchange.\n\nIn the table below, the \"data port\" is the network port or range of ports through which the protocol transmits file data. The \"control port\" is the port used for the dialogue of commands and status updates between client and server.\n\nThe column \"Assigned by IANA\" indicates whether the port is listed in the Service Name and Transport Protocol Port Number Registry, which is curated by the Internet Assigned Numbers Authority (IANA). IANA devotes each port number in the registry to a specific service with a specific transport protocol. The table below lists the transport protocol in the \"Transport\" column.\n\nThe following protocols were designed for serial communication, mostly for the RS-232 standard. They are used for uploading and downloading computer files via modem or serial cable (e.g., by null modem or direct cable connection). UUCP is one protocol that can operate with either RS-232 or the Transmission Control Protocol as its transport. OBject EXchange is a protocol for binary object wireless transfer via the Bluetooth standard. Bluetooth was conceived as a wireless replacement for RS-232.\n\n\n"}
{"id": "23238482", "url": "https://en.wikipedia.org/wiki?curid=23238482", "title": "Cross-recurrence quantification", "text": "Cross-recurrence quantification\n\nCross-recurrence quantification (CRQ) is a non-linear method that quantifies how similarly two observed data series unfold over time. CRQ produces measures reflecting coordination, such as how often two data series have similar values or reflect similar system states (called percentage recurrence, or %REC), among other measures.\n"}
{"id": "6264305", "url": "https://en.wikipedia.org/wiki?curid=6264305", "title": "DARWARS", "text": "DARWARS\n\nDARWARS is a research program intended to accelerate the development and deployment of military training systems. These are envisioned as low-cost, mobile, web-centric, simulation-based, “lightweight” systems designed to take advantage of the ubiquitous presence of the PC and of new technology, including multi-player games, virtual worlds, off-the-shelf PC simulations, intelligent agents, and on-line communities. The project started in 2003.\n\nThe program is producing an architectural framework, including a set of web services, tools, and system interface definitions that facilitate the development of networked training systems. The scalable framework supports training for individuals, teams, or teams of teams (involving students at PCs interacting on a virtual battlefield). Training systems keep track of student performance in order to offer individual and group feedback. The program envisions an on-line community of students, instructors and developers around the DARWARS family of training systems, although, realistically the creators only hoped to get this kind of training started - not see it to that complete end.\n\nDARWARS is sponsored by the U.S. Defense Advanced Research Projects Agency (DARPA) and is co-sponsored by U.S. Joint Forces Command (JFCOM) and the United States Marine Corps Program Manager for Training Systems (PM TRASYS). The integration and architecture contractor for the DARWARS project is BBN Technologies. The program is the core DARPA's Training Superiority initiative.\n\nAs part of the DARWARS program, several training systems have been developed as exemplars of what is possible. Some of these systems have been deployed to meet training needs related to current U.S. military efforts. Among these is the DARWARS Ambush! tactical trainer and the language trainer Tactical IraqiTM.\n\n\"DARWARS Ambush!\" is a PC-based, networked, multiplayer training simulator, or, serious game. It provides military training based on experiences of personnel in the field, and includes capabilities to ensure the capture and dissemination of lessons learned. The initial application involved road-convoy-operations training, while subsequent applications include training for platoon level mounted infantry tactics, dismounted infantry operations, Rules-of-Engagement training, cross-cultural communications training, and other areas. The software was developed by Total Immersion Software, based on the technology of \"\".\n\n\"DARWARS Ambush!\" is generally restricted to U.S. Military and U.S. Government organizations and personnel. The \"DARWARS Ambush!\" software is free of charge. The costs of the underlying game engine and networking infrastructure is nominal compared to traditional simulation-training systems.\nThe most important innovation of DARWARS Ambush! is that it is user-authorable. Soldiers themselves can create new scenarios and training in a few hours or days without a contractor between them and their tactics, techniques and procedures.\n\nTactical Language & Culture Training Systems are PC-based courses that teach foreign languages and cultural knowledge needed to conduct tasks effectively and safely during both daily life and military missions. They are self-paced foreign-language training programs that use numerous research-based pedagogic and technologic innovations—including interactive 3D video game simulations—to teach what to say, how to say it, and when to say it. The courses to learn Iraqi, Pashto, French, and Dari {under development} are available to any member of the U.S. Armed Forces by free download from the company's website.\n\nGame After Ambush, which is based on VBS2 is scheduled to replace DARWARS Ambush! during 2009.\n\n\n"}
{"id": "24298014", "url": "https://en.wikipedia.org/wiki?curid=24298014", "title": "David Drewry", "text": "David Drewry\n\nDavid John Drewry (born 22 September 1947, in Grimsby) is a glaciologist and geophysicist who was described in the conferring of an honorary degree by Anglia Ruskin University in 1998 as having an \"outstanding reputation as an eminent scientist of international repute\". Drewry has also received several awards for his work. Since 1 July 2015 he is the Vice-President of the European University Association.\n\nDrewry was educated at Havelock Grammar School (now the Havelock Academy) in Grimsby. He then studied at Queen Mary College in east London, graduating with a BSc in Geography in 1969. He then studied in Cambridge, residing at Emmanuel College, Cambridge, and completed a PhD on Glaciology and Geophysics in 1974 with thesis titled \"Sub-ice relief and geology of East Antarctica\".\n\nFrom 1978 to 1983, Drewry was a Senior Research Assistant at the Scott Polar Research Institute, and then Assistant Director of Research in 1983. He was Director of the Scott Polar Research Institute from 1984 to 1987, and then Director of the British Antarctic Survey from 1987 to 1994. He is an Honorary Fellow of Emmanuel College, Cambridge.\n\nHe became the Vice-Chancellor Designate of the University of Hull in November 1999, being designated to take over on the planned retirement of the previous incumbent, David Dilks, in January 2000. On 1 September 2009, he was succeeded by Calie Pistorius, formerly the Vice-Chancellor of the University of Pretoria.\n\nDrewry has received several awards and honours for his work.\nDrewry Ice Stream in Ellsworth Land and Mount Drewry in the Queen Alexandra Range are named after him.\n\nDrewry is \"married to Gillian Elizabeth and lives in East Yorkshire and London\".\n\n"}
{"id": "33977176", "url": "https://en.wikipedia.org/wiki?curid=33977176", "title": "Edith Berkeley", "text": "Edith Berkeley\n\nEdith Berkeley (1875–1963) was a Canadian marine biologist.\n\nBerkeley completed a pre-medical course at the University of London, where she attended on scholarship.\n\nIn 1918, she gave up a paid position as zoology assistant at Columbia University to work as a volunteer for the Pacific Biological Station at Nanaimo in British Columbia Canada. As a volunteer she would be able to perform field work, whereas in a paid position, her work would be included under her husband's name. Though she was never officially on staff, her research on polychaetes brought prestige to the Station and established her as a world authority on the subject.\n\nHer husband Cyril Berkeley left his own research to help her in 1930. They wrote 34 papers together, and she published an additional 12 in her own name. Many organisms have been named after them.\n"}
{"id": "7939223", "url": "https://en.wikipedia.org/wiki?curid=7939223", "title": "Embodied embedded cognition", "text": "Embodied embedded cognition\n\nEmbodied embedded cognition (EEC) is a philosophical theoretical position in cognitive science, closely related to situated cognition, embodied cognition, embodied cognitive science and dynamical systems theory. The theory states that intelligent behaviour emerges from the interplay between brain, body and world. The world is not just the 'play-ground' on which the brain is acting. Rather, brain, body and world are equally important factors in the explanation of how particular intelligent behaviours come about in practice. There are concerns about whether EEC constitutes a novel and substantive approach to cognition or whether it is merely a manifestation of frustration with the classical cognitivist approach.\n\nEEC is divided into two aspects: embodiment and embeddedness (or situatedness).\n\n\"Embodiment\" refers to the idea that the body's internal milieu (a.o. homeostatic and hormonal states) heavily influences the higher 'cognitive' processes in the brain, presumably via the emotional system (see e.g. Antonio Damasio's theory of somatic markers). To put it simply: the state of your body is a direct factor of importance on the kinds of cognitive processes that may arise in the higher parts of your brain.\n\n\"Embeddedness\" refers to the idea that physical interaction between the body and the world strongly constrain the possible behaviours of the organism, which in turn influences (indeed, partly constitutes) the cognitive processes that emerge from the interaction between organism and world.\n\nThe theory is an explicit reaction to the currently dominant cognitivist paradigm, which states that cognitive systems are essentially computational-representational systems (like computer software), processing input and generating output (behaviour) on the basis of internal information processing. In cognitivism, the causal root of behaviour lies in the 'virtual' processes governed by the software that runs on our brains. The brain is purely the hardware on which the software is implemented. The body (sensors and actors) are purely input-output devices that are in service of the brain. The world is merely the play-ground (the object) in which the cognitive agent acts.\n\nIn contrast, EEC holds that the actual physical processes in body and in body-world interaction partly constitute whatever it is that we call 'the cognitive system' as a whole. Body, world and brain form a system. Together these system-parts 'cause' intelligent behaviour to arise as a system property. Dynamical Systems Theory is a way of modeling behaviour that teams up quite naturally with the theoretical concepts of EEC.\n\nCurrent discussions include:\n\nTheorists that inspired the EEC programme (but might not necessarily adhere to the above position) include:\n\n"}
{"id": "41282920", "url": "https://en.wikipedia.org/wiki?curid=41282920", "title": "Flow cytometry bioinformatics", "text": "Flow cytometry bioinformatics\n\nFlow cytometry bioinformatics is the application of bioinformatics to flow cytometry data, which involves storing, retrieving, organizing and analyzing flow cytometry data using extensive computational resources and tools.\nFlow cytometry bioinformatics requires extensive use of and contributes to the development of techniques from computational statistics and machine learning.\nFlow cytometry and related methods allow the quantification of multiple independent biomarkers on large numbers of single cells. The rapid growth in the multidimensionality and throughput of flow cytometry data, particularly in the 2000s, has led to the creation of a variety of computational analysis methods, data standards, and public databases for the sharing of results.\n\nComputational methods exist to assist in the preprocessing of flow cytometry data, identifying cell populations within it, matching those cell populations across samples, and performing diagnosis and discovery using the results of previous steps. For preprocessing, this includes compensating for spectral overlap, transforming data onto scales conducive to visualization and analysis, assessing data for quality, and normalizing data across samples and experiments.\nFor population identification, tools are available to aid traditional manual identification of populations in two-dimensional scatter plots (gating), to use dimensionality reduction to aid gating, and to find populations automatically in higher-dimensional space in a variety of ways.\nIt is also possible to characterize data in more comprehensive ways, such as the density-guided binary space partitioning technique known as probability binning, or by combinatorial gating.\nFinally, diagnosis using flow cytometry data can be aided by supervised learning techniques, and discovery of new cell types of biological importance by high-throughput statistical methods, as part of pipelines incorporating all of the aforementioned methods.\n\nOpen standards, data and software are also key parts of flow cytometry bioinformatics.\nData standards include the widely adopted Flow Cytometry Standard (FCS) defining how data from cytometers should be stored, but also several new standards under development by the International Society for Advancement of Cytometry (ISAC) to aid in storing more detailed information about experimental design and analytical steps.\nOpen data is slowly growing with the opening of the CytoBank database in 2010, and FlowRepository in 2012, both of which allow users to freely distribute their data, and the latter of which has been recommended as the preferred repository for MIFlowCyt-compliant data by ISAC.\nOpen software is most widely available in the form of a suite of Bioconductor packages, but is also available for web execution on the GenePattern platform.\n\nFlow cytometers operate by hydrodynamically focusing suspended cells so that they separate from each other within a fluid stream.\nThe stream is interrogated by one or more lasers, and the resulting fluorescent and scattered light is detected by photomultipliers.\nBy using optical filters, particular fluorophores on or within the cells can be quantified by peaks in their emission spectra.\nThese may be endogenous fluorophores such as chlorophyll or transgenic green fluorescent protein, or they may be artificial fluorophores covalently bonded to detection molecules such as antibodies for detecting proteins, or hybridization probes for detecting DNA or RNA.\n\nThe ability to quantify these has led to flow cytometry being used in a wide range of applications, including but not limited to:\n\nUntil the early 2000s, flow cytometry could only measure a few fluorescent markers at a time.\nThrough the late 1990s into the mid-2000s, however, rapid development of new fluorophores resulted in modern instruments capable of quantifying up to 18 markers per cell. \nMore recently, the new technology of mass cytometry replaces fluorophores with rare-earth elements detected by time of flight mass spectrometry, achieving the ability to measure the expression of 34 or more markers.\nAt the same time, microfluidic qPCR methods are providing a flow cytometry-like method of quantifying 48 or more RNA molecules per cell.\nThe rapid increase in the dimensionality of flow cytometry data, coupled with the development of high-throughput robotic platforms capable of assaying hundreds to thousands of samples automatically have created a need for improved computational analysis methods.\n\nFlow cytometry data is in the form of a large matrix of intensities over M wavelengths by N events. Most events will be a particular cell, although some may be doublets (pairs of cells which pass the laser closely together). For each event, the measured fluorescence intensity over a particular wavelength range is recorded.\n\nThe measured fluorescence intensity indicates the amount of that fluorophore in the cell, which indicates the amount that has bound to detector molecules such as antibodies. Therefore, fluorescence intensity can be considered a proxy for the amount of detector molecules present on the cell. A simplified, if not strictly accurate, way of considering flow cytometry data is as a matrix of M measurements of amounts of molecules of interest by N cells.\n\nThe process of moving from primary FCM data to disease diagnosis and biomarker discovery involves four major steps:\n\n\nSaving of the steps taken in a particular flow cytometry workflow is supported by some flow cytometry software, and is important for the reproducibility of flow cytometry experiments.\nHowever, saved workspace files are rarely interchangeable between software. \nAn attempt to solve this problem is the development of the Gating-ML XML-based data standard (discussed in more detail under the standards section), which is slowly being adopted in both commercial and open source flow cytometry software. The CytoML R package is also filling the gap by importing/exporting the Gating-ML that is compatible with FlowJo, CytoBank and FACS Diva softwares.\n\nPrior to analysis, flow cytometry data must typically undergo pre-processing to remove artifacts and poor quality data, and to be transformed onto an optimal scale for identifying cell populations of interest. Below are various steps in a typical flow cytometry preprocessing pipeline.\n\nWhen more than one fluorochrome is used with the same laser, their emission spectra frequently overlap. Each particular fluorochrome is typically measured using a bandpass optical filter set to a narrow band at or near the fluorochrome's emission intensity peak.\nThe result is that the reading for any given fluorochrome is actually the sum of that fluorochrome's peak emission intensity, and the intensity of all other fluorochromes' spectra where they overlap with that frequency band.\nThis overlap is termed spillover, and the process of removing spillover from flow cytometry data is called compensation.\n\nCompensation is typically accomplished by running a series of representative samples each stained for only one fluorochrome, to give measurements of the contribution of each fluorochrome to each channel.\nThe total signal to remove from each channel can be computed by solving a system of linear equations based on this data to produce a spillover matrix, which when inverted and multiplied with the raw data from the cytometer produces the compensated data.\nThe processes of computing the spillover matrix, or applying a precomputed spillover matrix to compensate flow cytometry data, are standard features of flow cytometry software.\n\nCell populations detected by flow cytometry are often described as having approximately log-normal expression.\nAs such, they have traditionally been transformed to a logarithmic scale.\nIn early cytometers, this was often accomplished even before data acquisition by use of a log amplifier.\nOn modern instruments, data is usually stored in linear form, and transformed digitally prior to analysis.\n\nHowever, compensated flow cytometry data frequently contains negative values due to compensation, and cell populations do occur which have low means and normal distributions.\nLogarithmic transformations cannot properly handle negative values, and poorly display normally distributed cell types.\nAlternative transformations which address this issue include the log-linear hybrid transformations Logicle and Hyperlog, as well as the hyperbolic arcsine and the Box-Cox.\n\nA comparison of commonly used transformations concluded that the biexponential and Box-Cox transformations, when optimally parameterized, provided the clearest visualization and least variance of cell populations across samples. However, a later comparison of the flowTrans package used in that comparison indicated that it did not parameterize the Logicle transformation in a manner consistent with other implementations, potentially calling those results into question.\n\nParticularly in newer, high-throughput experiments, there is a need for visualization methods to help detect technical errors in individual samples.\nOne approach is to visualize summary statistics, such as the empirical distribution functions of single dimensions of technical or biological replicates to ensure they are the similar.\nFor more rigor, the Kolmogorov–Smirnov test can be used to determine if individual samples deviate from the norm.\nThe Grubbs' test for outliers may be used to detect samples deviating from the group.\n\nA method for quality control in higher-dimensional space is to use probability binning with bins fit to the whole data set pooled together.\nThen the standard deviation of the number of cells falling in the bins within each sample can be taken as a measure of multidimensional similarity, with samples that are closer to the norm having a smaller standard deviation.\nWith this method, higher standard deviation can indicate outliers, although this is a relative measure as the absolute value depends partly on the number of bins.\n\nWith all of these methods, the cross-sample variation is being measured. However, this is the combination of technical variations introduced by the instruments and handling, and actual biological information that is desired to be measured. Disambiguating the technical and the biological contributions to between-sample variation can be a difficult to impossible task.\n\nParticularly in multi-centre studies, technical variation can make biologically equivalent populations of cells difficult to match across samples.\nNormalization methods to remove technical variance, frequently derived from image registration techniques, are thus a critical step in many flow cytometry analyses.\nSingle-marker normalization can be performed using landmark registration, in which peaks in a kernel density estimate of each sample are identified and aligned across samples.\n\nThe complexity of raw flow cytometry data (dozens of measurements for thousands to millions of cells) makes answering questions directly using statistical tests or supervised learning difficult. Thus, a critical step in the analysis of flow cytometric data is to reduce this complexity to something more tractable while establishing common features across samples. This usually involves identifying multidimensional regions that contain functionally and phenotypically homogeneous groups of cells. This is a form of cluster analysis. There are a range of methods by which this can be achieved, detailed below.\n\nThe data generated by flow-cytometers can be plotted in one or two dimensions to produce a histogram or scatter plot. The regions on these plots can be sequentially separated, based on fluorescence intensity, by creating a series of subset extractions, termed \"gates\". These gates can be produced using software, e.g. Flowjo, FCS Express, WinMDI, CytoPaint (aka Paint-A-Gate), VenturiOne, Cellcion, CellQuest Pro, Cytospec, Kaluza. or flowCore.\n\nIn datasets with a low number of dimensions and limited cross-sample technical and biological variability (e.g., clinical laboratories), manual analysis of specific cell populations can produce effective and reproducible results. However, exploratory analysis of a large number of cell populations in a high-dimensional dataset is not feasible. In addition, manual analysis in less controlled settings (e.g., cross-laboratory studies) can increase the overall error rate of the study. In one study, several computational gating algorithms performed better than manual analysis in the presence of some variation. However, despite the considerable advances in computational analysis, manual gating remains the main solution for the identification of specific rare cell populations that are not well-separated from other cell types.\n\nThe number of scatter plots that need to be investigated increases with the square of the number of markers measured (or faster since some markers need to be investigated several times for each group of cells to resolve high-dimensional differences between cell types that appear to be similar in most markers). To address this issue, principal component analysis has been used to summarize the high-dimensional datasets using a combination of markers that maximizes the variance of all data points. However, PCA is a linear method and is not able to preserve complex and non-linear relationships. More recently, two dimensional minimum spanning tree layouts have been used to guide the manual gating process. Density-based down-sampling and clustering was used to better represent rare populations and control the time and memory complexity of the minimum spanning tree construction process. More sophisticated dimension reduction algorithms are yet to be investigated.\n\nDeveloping computational tools for identification of cell populations has been an area of active research only since 2008. Many individual clustering approaches have recently been developed, including model-based algorithms (e.g., flowClust and FLAME), density based algorithms (e.g. FLOCK and SWIFT, graph-based approaches (e.g. SamSPECTRAL) and most recently, hybrids of several approaches (flowMeans and flowPeaks). These algorithms are different in terms of memory and time complexity, their software requirements, their ability to automatically determine the required number of cell populations, and their sensitivity and specificity. The FlowCAP (Flow Cytometry: Critical Assessment of Population Identification Methods) project, with active participation from most academic groups with research efforts in the area, is providing a way to objectively cross-compare state-of-the-art automated analysis approaches.\nOther surveys have also compared automated gating tools on several datasets.\n\nProbability binning is a non-gating analysis method in which flow cytometry data is split into quantiles on a univariate basis. \nThe locations of the quantiles can then be used to test for differences between samples (in the variables not being split) using the chi-squared test.\n\nThis was later extended into multiple dimensions in the form of frequency difference gating, a binary space partitioning technique where data is iteratively partitioned along the median. \nThese partitions (or bins) are fit to a control sample. \nThen the proportion of cells falling within each bin in test samples can be compared to the control sample by the chi squared test.\n\nFinally, cytometric fingerprinting uses a variant of frequency difference gating to set bins and measure for a series of samples how many cells fall within each bin. These bins can be used as gates and used for subsequent analysis similarly to automated gating methods.\n\nHigh-dimensional clustering algorithms are often unable to identify rare cell types that are not well separated from other major populations. Matching these small cell populations across multiple samples is even more challenging. In manual analysis, prior biological knowledge (e.g., biological controls) provides guidance to reasonably identify these populations. However, integrating this information into the exploratory clustering process (e.g., as in semi-supervised learning) has not been successful.\n\nAn alternative to high-dimensional clustering is to identify cell populations using one marker at a time and then combine them to produce higher-dimensional clusters. This functionality was first implemented in FlowJo. The flowType algorithm builds on this framework by allowing the exclusion of the markers. This enables the development of statistical tools (e.g. RchyOptimyx) that can investigate the importance of each marker and exclude high-dimensional redundancies.\n\nAfter identification of the cell population of interest, a cross sample analysis can be performed to identify phenotypical or functional variations that are correlated with an external variable (e.g., a clinical outcome). These studies can be partitioned into two main groups:\n\nIn these studies, the goal usually is to diagnose a disease (or a sub-class of a disease) using variations in one or more cell populations. For example, one can use multidimensional clustering to identify a set of clusters, match them across all samples, and then use supervised learning to construct a classifier for prediction of the classes of interest (e.g., this approach can be used to improve the accuracy of the classification of specific lymphoma subtypes). Alternatively, all the cells from the entire cohort can be pooled into a single multidimensional space for clustering before classification. This approach is particularly suitable for datasets with a high amount of biological variation (in which cross-sample matching is challenging) but requires technical variations to be carefully controlled.\n\nIn a discovery setting, the goal is to identify and describe cell populations correlated with an external variable (as opposed to the diagnosis setting in which the goal is to combine the predictive power of multiple cell types to maximize the accuracy of the results). Similar to the diagnosis use-case, cluster matching in high-dimensional space can be used for exploratory analysis but the descriptive power of this approach is very limited, as it is hard to characterize and visualize a cell population in a high-dimensional space without first reducing the dimensionality. Finally, combinatorial gating approaches have been particularly successful in exploratory analysis of FCM data. Simplified Presentation of Incredibly Complex Evaluations (SPICE) is a software package that can use the gating functionality of FlowJo to statistically evaluate a wide range of different cell populations and visualize those that are correlated with the external outcome. flowType and RchyOptimyx (as discussed above) expand this technique by adding the ability of exploring the impact of independent markers on the overall correlation with the external outcome. This enables the removal of unnecessary markers and provides a simple visualization of all identified cell types. In a recent analysis of a large (n=466) cohort of HIV+ patients, this pipeline identified three correlates of protection against HIV, only one of which had been previously identified through extensive manual analysis of the same dataset.\n\nFlow Cytometry Standard (FCS) was developed in 1984 to allow recording and sharing of flow cytometry data. Since then, FCS became the standard file format supported by all flow cytometry software and hardware vendors. The FCS specification has traditionally been developed and maintained by the International Society for Advancement of Cytometry (ISAC). Over the years, updates were incorporated to adapt to technological advancements in both flow cytometry and computing technologies with FCS 2.0 introduced in 1990, FCS 3.0 in 1997, and the most current specification FCS 3.1 in 2010. FCS used to be the only widely adopted file format in flow cytometry. Recently, additional standard file formats have been developed by ISAC.\n\nISAC is considering replacing FCS with a flow cytometry specific version of the Network Common Data Form (netCDF) file format.\nnetCDF is a set of freely available software libraries and machine independent data formats that support the creation, access, and sharing of array-oriented scientific data. In 2008, ISAC drafted the first version of netCDF conventions for storage of raw flow cytometry data.\n\nThe Archival Cytometry Standard (ACS) is being developed to bundle data with different components describing cytometry experiments. It captures relations among data, metadata, analysis files and other components, and includes support for audit trails, versioning and digital signatures. The ACS container is based on the ZIP file format with an XML-based table of contents specifying relations among files in the container. The XML Signature W3C Recommendation has been adopted to allow for digital signatures of components within the ACS container.\nAn initial draft of ACS has been designed in 2007 and finalized in 2010. Since then, ACS support has been introduced in several software tools including FlowJo and Cytobank.\n\nThe lack of gating interoperability has traditionally been a bottleneck preventing reproducibility of flow cytometry data analysis and the usage of multiple analytical tools. To address this shortcoming, ISAC developed Gating-ML, an XML-based mechanism to formally describe gates and related data (scale) transformations.\nThe draft recommendation version of Gating-ML was approved by ISAC in 2008 and it is partially supported by tools like FlowJo, the flowUtils, CytoML libraries in R/BioConductor, and FlowRepository. It supports rectangular gates, polygon gates, convex polytopes, ellipsoids, decision trees and Boolean collections of any of the other types of gates. In addition, it includes dozens of built in public transformations that have been shown to potentially useful for display or analysis of cytometry data. In 2013, Gating-ML version 2.0 was approved by ISAC's Data Standards Task Force as a Recommendation. This new version offers slightly less flexibility in terms of the power of gating description; however, it is also significantly easier to implement in software tools.\n\nThe Classification Results (CLR) File Format has been developed to exchange the results of manual gating and algorithmic classification approaches in a standard way in order to be able to report and process the classification. CLR is based in the commonly supported CSV file format with columns corresponding to different classes and cell values containing the probability of an event being a member of a particular class. These are captured as values between 0 and 1. Simplicity of the format and its compatibility with common spreadsheet tools have been the major requirements driving the design of the specification. Although it was originally designed for the field of flow cytometry, it is applicable in any domain that needs to capture either fuzzy or unambiguous classifications of virtually any kinds of objects.\n\nAs in other bioinformatics fields, development of new methods has primarily taken the form of free open source software, and several databases have been created for depositing open data.\n\nThe Bioconductor project is a repository of free open source software, mostly written in the R programming language.\nAs of July 2013, Bioconductor contained 21 software packages for processing flow cytometry data.\nThese packages cover most of the range of functionality described earlier in this article.\n\nGenePattern is a predominantly genomic analysis platform with over 200 tools for analysis of gene expression, proteomics, and other data. A web-based interface provides easy access to these tools and allows the creation of automated analysis pipelines enabling reproducible research. Recently, a GenePattern Flow Cytometry Suite has been developed in order to bring advanced flow cytometry data analysis tools to experimentalists without programmatic skills. It contains close to 40 open source GenePattern flow cytometry modules covering methods from basic processing of flow cytometry standard (i.e., FCS) files to advanced algorithms for automated identification of cell populations, normalization and quality assessment. Internally, most of these modules leverage functionality developed in BioConductor.\n\nMuch of the functionality of the Bioconductor packages for flow cytometry analysis has been packaged up for use with the GenePattern workflow system, in the form of the GenePattern Flow Cytometry Suite.\n\nFACSanadu is an open source portable application for visualization and analysis of FCS data. Unlike Bioconductor, it is an interactive program aimed at non-programmers for routine analysis. It supports standard FCS files as well as COPAS profile data.\n\nThe Minimum Information about a Flow Cytometry Experiment (MIFlowCyt), requires that any flow cytometry data used in a publication be available, although this does not include a requirement that it be deposited in a public database.\nThus, although the journals Cytometry Part A and B, as well as all journals from the Nature Publishing Group require MIFlowCyt compliance, there is still relatively little publicly available flow cytometry data.\nSome efforts have been made towards creating public databases, however.\n\nFirstly, CytoBank, which is a complete web-based flow cytometry data storage and analysis platform, has been made available to the public in a limited form.\nUsing the CytoBank code base, FlowRepository was developed in 2012 with the support of ISAC to be a public repository of flow cytometry data.\nFlowRepository facilitates MIFlowCyt compliance, and as of July 2013 contained 65 public data sets.\n\nIn 2012, the flow cytometry community has started to release a set of publicly available datasets. A subset of these datasets representing the existing data analysis challenges is described below. For comparison against manual gating, the FlowCAP-I project has released five datasets, manually gated by human analysts, and two of them gated by eight independent analysts. The FlowCAP-II project included three datasets for binary classification and also reported several algorithms that were able to classify these samples perfectly. FlowCAP-III included two larger datasets for comparison against manual gates as well as one more challenging sample classification dataset. As of March 2013, public release of FlowCAP-III was still in progress. The datasets used in FlowCAP-I, II, and III either have a low number of subjects or parameters. However, recently several more complex clinical datasets have been released including a dataset of 466 HIV-infected subjects, which provides both 14 parameter assays and sufficient clinical information for survival analysis.\n\nAnother class of datasets are higher-dimensional mass cytometry assays. A representative of this class of datasets is a study which includes analysis of two bone marrow samples using more than 30 surface or intracellular markers under a wide range of different stimulations. The raw data for this dataset is publicly available as described in the manuscript, and manual analyses of the surface markers are available upon request from the authors.\n\nDespite rapid development in the field of flow cytometry bioinformatics, several problems remain to be addressed.\n\nVariability across flow cytometry experiments arises from biological variation among samples, technical variations across instruments used, as well as methods of analysis.\nIn 2010, a group of researchers from Stanford University and the National Institutes of Health pointed out that while technical variation can be ameliorated by standardizing sample handling, instrument setup and choice of reagents, solving variation in analysis methods will require similar standardization and computational automation of gating methods.\nThey further opined that centralization of both data and analysis could aid in decreasing variability between experiments and in comparing results.\n\nThis was echoed by another group of Pacific Biosciences and Stanford University researchers, who suggested that cloud computing could enable centralized, standardized, high-throughput analysis of flow cytometry experiments.\nThey also emphasised that ongoing development and adoption of standard data formats could continue to aid in reducing variability across experiments.\nThey also proposed that new methods will be needed to model and summarize results of high-throughput analysis in ways that can be interpreted by biologists, as well as ways of integrating large-scale flow cytometry data with other high-throughput biological information, such as gene expression, genetic variation, metabolite levels and disease states.\n\n"}
{"id": "22021287", "url": "https://en.wikipedia.org/wiki?curid=22021287", "title": "Fran Balkwill", "text": "Fran Balkwill\n\nFrances Balkwill OBE (born 1952) is an English scientist, Professor of Cancer Biology at Queen Mary University of London, and author of children's books about scientific topics.\n\nBalkwill was born in south-west London in 1952, and was educated at Surbiton High School. She obtained a BSc in Cellular Pathology at the University of Bristol and a PhD on leukaemia cell biology in the Medical Oncology Department at St Bartholomew's Hospital under the late Gordon Hamilton-Fairley.\n\nShe worked for several years at the ICRF Lincoln's Inn Fields (now Cancer Research UK London Research Institute) on the effect of interferon on cancer. In 2000 she moved to Queen Mary, University of London, where she is Professor of Cancer Biology and Director of the Centre for Translational Oncology in the Cancer Research UK Clinical Cancer Centre.\n\nIn addition to her laboratory research, Balkwill has written many children's books about science, illustrated by Mic Rolph, and is Director of Centre of the Cell, a science education centre for children in the new Barts and The London Medical School building in Whitechapel, east London. Her books for children cover a range of topics in biology. For her first book on HIV and Aids, \"Staying Alive: Fighting HIV/AIDS\" (2002), Balkwill and Rolph travelled to South Africa to carry out research and to determine the educational needs of local communities. 19,000 free copies of the book were distributed throughout South Africa, funded by the publisher Cold Spring Harbor Laboratory Press.\n\nFrom 2008 to 2011, Balkwill served as Chairman of the Public Engagement Strategy Committee at the Wellcome Trust. In 2016 she was a judge for the Wellcome Book Prize.\n\nIn 2008 Balkwill was awarded an Order of the British Empire (OBE).\n\nBalkwill's books for children \"Cells are Us\" and \"Cell Wars\" won the Royal Society Young People Book Prize in 1991.\n\nShe won the EMBO Award for Communication in the Life Sciences in 2004. She was awarded the Michael Faraday Prize in 2005 \"for her outstanding work in communicating the concepts, facts and fascination of science in a way that appeals to children of all ages, backgrounds and nationalities, while at the same time maintaining a distinguished research career\". In 2006 she was awarded Fellowship of the Academy of Medical Sciences for her contributions to and achievements in medicine.\n\nIn 2015 she received an Honorary degree from the University of Bristol.\n\nIn 2017 she was awarded the Inspiring Leadership in Research Engagement Prize from Cancer Research UK.\n\nBalkwill has two children and two grandchildren. She is a keen piano player and enjoys writing, travel, and birdwatching.\n"}
{"id": "12066794", "url": "https://en.wikipedia.org/wiki?curid=12066794", "title": "Franz Anton von Scheidel", "text": "Franz Anton von Scheidel\n\nFranz Anton von Scheidel (1731–1801) was a German natural history artist, noted for his botanical illustrations of Nikolaus Joseph von Jacquin's \"Hortus botanicus Vindobonensis\" (Botanical Garden of Vienna), which was published in three fascicles from 1770 to 1776 and is a description of plants at the university's botanical garden. Included in this work are 300 hand-coloured copper engravings by Franz von Scheidel.\n\nVon Scheidel undertook several thousand drawings of plants, fish, birds and mammals for Jacquin and many other patrons.\n"}
{"id": "532323", "url": "https://en.wikipedia.org/wiki?curid=532323", "title": "G-type asteroid", "text": "G-type asteroid\n\nG-type asteroids are a relatively uncommon type of carbonaceous asteroid that makes up approximately 5% of asteroids. The most notable asteroid in this class is 1 Ceres.\n\nGenerally similar to the C-type objects, but contain a strong ultraviolet absorption feature below 0.5 μm. An absorption feature around 0.7 μm may also be present, which is indicative of phyllosilicate minerals such as clays or mica. \n\nIn the SMASS classification\nthe G-type corresponds to the Cgh and Cg types, depending on the presence or absence (respectively) of the absorption feature at 0.7 μm. The G-type, C-type and some rare types are sometimes collected together into a wider C-group of carbonaceous asteroids.\n\n\n"}
{"id": "7918775", "url": "https://en.wikipedia.org/wiki?curid=7918775", "title": "Hans J. Hofmann", "text": "Hans J. Hofmann\n\nHans J. Hofmann (3 October 1936, Kiel, Germany – 19 May 2010, Montreal, Quebec, Canada) \nwas a paleontologist, specializing in the study of Precambrian fossils using computer modelling and image analysis to quantify morphologic attributes.\n\nBorn in Germany, Hofmann immigrated to Montreal, Quebec, Canada and studied geology at McGill University, where he earned a Ph.D. under the supervision of T. H. Clark. He taught for three years at the University of Cincinnati and then worked at the Geological Survey of Canada. He was a professor in the geology department of the Université de Montréal for 31 years (1969–2000). He spent the last ten years of his life as a researcher in the Redpath Museum and an adjunct professor in the Department of Earth and Planetary Sciences at McGill.\n\nThe National Academy of Sciences noted him for, \"his pioneering discoveries of fossils that have illuminated life's early evolution, from Archean stromatolites and Proterozoic cyanobacteria, to the rise of multicellular organisms.\"\n\nHofmann’s contributions have shed light on the biologic, stratigraphic, and evolutionary significance of various stromatolites, microfossils, macrofossils, and trace fossils.\n\n"}
{"id": "27988760", "url": "https://en.wikipedia.org/wiki?curid=27988760", "title": "Heuristics in judgment and decision-making", "text": "Heuristics in judgment and decision-making\n\nIn psychology, heuristics are simple, efficient rules which people often use to form judgments and make decisions. They are mental shortcuts that usually involve focusing on one aspect of a complex problem and ignoring others. These rules work well under most circumstances, but they can lead to systematic deviations from logic, probability or rational choice theory. The resulting errors are called \"cognitive biases\" and many different types have been documented. These have been shown to affect people's choices in situations like valuing a house, deciding the outcome of a legal case, or making an investment decision. Heuristics usually govern automatic, intuitive judgments but can also be used as deliberate mental strategies when working from limited information.\n\nCognitive scientist Herbert A. Simon originally proposed that human judgments are limited by available information, time constraints, and cognitive limitations, calling this bounded rationality. In the early 1970s, psychologists Amos Tversky and Daniel Kahneman demonstrated three heuristics that underlie a wide range of intuitive judgments. These findings set in motion the heuristics and biases research program, which studies how people make real-world judgments and the conditions under which those judgments are unreliable. This research challenged the idea that human beings are rational actors, but provided a theory of information processing to explain how people make estimates or choices. This research, which first gained worldwide attention in 1974 with the \"Science\" paper \"Judgment Under Uncertainty: Heuristics and Biases\", has guided almost all current theories of decision-making, and although the originally proposed heuristics have been challenged in the further debate, this research program has changed the field by permanently setting the research questions.\n\nThis heuristics-and-biases tradition has been criticised by Gerd Gigerenzer and others for being too focused on how heuristics lead to errors. The critics argue that heuristics can be seen as rational in an underlying sense. According to this perspective, heuristics are good enough for most purposes without being too demanding on the brain's resources. Another theoretical perspective sees heuristics as fully rational in that they are rapid, can be made without full information and can be as accurate as more complicated procedures. By understanding the role of heuristics in human psychology, marketers and other persuaders can influence decisions, such as the prices people pay for goods or the quantity they buy.\n\nIn their initial research, Tversky and Kahneman proposed three heuristics—availability, representativeness, and anchoring and adjustment. Subsequent work has identified many more. Heuristics that underlie judgment are called \"judgment heuristics\". Another type, called \"evaluation heuristics\", are used to judge the desirability of possible choices.\n\nIn psychology, \"availability\" is the ease with which a particular idea can be brought to mind. When people estimate how likely or how frequent an event is on the basis of its availability, they are using the availability heuristic. When an infrequent event can be brought easily and vividly to mind, people tend to overestimate its likelihood. For example, people overestimate their likelihood of dying in a dramatic event such as a tornado or terrorism. Dramatic, violent deaths are usually more highly publicised and therefore have a higher availability. On the other hand, common but mundane events are hard to bring to mind, so their likelihoods tend to be underestimated. These include deaths from suicides, strokes, and diabetes. This heuristic is one of the reasons why people are more easily swayed by a single, vivid story than by a large body of statistical evidence. It may also play a role in the appeal of lotteries: to someone buying a ticket, the well-publicised, jubilant winners are more available than the millions of people who have won nothing.\n\nWhen people judge whether more English words begin with \"T\" or with \"K\", the availability heuristic gives a quick way to answer the question. Words that begin with \"T\" come more readily to mind, and so subjects give a correct answer without counting out large numbers of words. However, this heuristic can also produce errors. When people are asked whether there are more English words with \"K\" in the first position or with \"K\" in the third position, they use the same process. It is easy to think of words that begin with \"K\", such as \"kangaroo\", \"kitchen\", or \"kept\". It is harder to think of words with \"K\" as the third letter, such as \"lake\", or \"acknowledge\", although objectively these are three times more common. This leads people to the incorrect conclusion that \"K\" is more common at the start of words. In another experiment, subjects heard the names of many celebrities, roughly equal numbers of whom were male and female. The subjects were then asked whether the list of names included more men or more women. When the men in the list were more famous, a great majority of subjects incorrectly thought there were more of them, and vice versa for women. Tversky and Kahneman's interpretation of these results is that judgments of proportion are based on availability, which is higher for the names of better-known people.\n\nIn one experiment that occurred before the 1976 U.S. Presidential election, some participants were asked to imagine Gerald Ford winning, while others did the same for a Jimmy Carter victory. Each group subsequently viewed their allocated candidate as significantly more likely to win. The researchers found a similar effect when students imagined a good or a bad season for a college football team. The effect of imagination on subjective likelihood has been replicated by several other researchers.\n\nA concept's availability can be affected by how recently and how frequently it has been brought to mind. In one study, subjects were given partial sentences to complete. The words were selected to activate the concept either of hostility or of kindness: a process known as \"priming\". They then had to interpret the behavior of a man described in a short, ambiguous story. Their interpretation was biased towards the emotion they had been primed with: the more priming, the greater the effect. A greater interval between the initial task and the judgment decreased the effect.\n\nTversky and Kahneman offered the availability heuristic as an explanation for illusory correlations in which people wrongly judge two events to be associated with each other. They explained that people judge correlation on the basis of the ease of imagining or recalling the two events together.\n\nThe representativeness heuristic is seen when people use categories, for example when deciding whether or not a person is a criminal. An individual thing has a high \"representativeness\" for a category if it is very similar to a prototype of that category. When people categorise things on the basis of representativeness, they are using the representativeness heuristic. \"Representative\" is here meant in two different senses: the prototype used for comparison is representative of its category, and representativeness is also a relation between that prototype and the thing being categorised. While it is effective for some problems, this heuristic involves attending to the particular characteristics of the individual, ignoring how common those categories are in the population (called the \"base rates\"). Thus, people can overestimate the likelihood that something has a very rare property, or underestimate the likelihood of a very common property. This is called the base rate fallacy. Representativeness explains this and several other ways in which human judgments break the laws of probability.\n\nThe representativeness heuristic is also an explanation of how people judge cause and effect: when they make these judgements on the basis of similarity, they are also said to be using the representativeness heuristic. This can lead to a bias, incorrectly finding causal relationships between things that resemble one another and missing them when the cause and effect are very different. Examples of this include both the belief that \"emotionally relevant events ought to have emotionally relevant causes\", and magical associative thinking.\n\nA 1973 experiment used a psychological profile of Tom W., a fictional graduate student. One group of subjects had to rate Tom's similarity to a typical student in each of nine academic areas (including Law, Engineering and Library Science). Another group had to rate how likely it is that Tom specialised in each area. If these ratings of likelihood are governed by probability, then they should resemble the base rates, i.e. the proportion of students in each of the nine areas (which had been separately estimated by a third group). If people based their judgments on probability, they would say that Tom is more likely to study Humanities than Library Science, because there are many more Humanities students, and the additional information in the profile is vague and unreliable. Instead, the ratings of likelihood matched the ratings of similarity almost perfectly, both in this study and a similar one where subjects judged the likelihood of a fictional woman taking different careers. This suggests that rather than estimating probability using base rates, subjects had substituted the more accessible attribute of similarity.\n\nWhen people rely on representativeness, they can fall into an error which breaks a fundamental law of probability. Tversky and Kahneman gave subjects a short character sketch of a woman called Linda, describing her as, \"31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations\". People reading this description then ranked the likelihood of different statements about Linda. Amongst others, these included \"Linda is a bank teller\", and, \"Linda is a bank teller and is active in the feminist movement\". People showed a strong tendency to rate the latter, more specific statement as more likely, even though a conjunction of the form \"Linda is both \"X\" and \"Y\"\" can never be more probable than the more general statement \"Linda is \"X\"\". The explanation in terms of heuristics is that the judgment was distorted because, for the readers, the character sketch was representative of the sort of person who might be an active feminist but not of someone who works in a bank. A similar exercise concerned Bill, described as \"intelligent but unimaginative\". A great majority of people reading this character sketch rated \"Bill is an accountant who plays jazz for a hobby\", as more likely than \"Bill plays jazz for a hobby\".\n\nWithout success, Tversky and Kahneman used what they described as \"a series of increasingly desperate manipulations\" to get their subjects to recognise the logical error. In one variation, subjects had to choose between a logical explanation of why \"Linda is a bank teller\" is more likely, and a deliberately illogical argument which said that \"Linda is a feminist bank teller\" is more likely \"because she resembles an active feminist more than she resembles a bank teller\". Sixty-five percent of subjects found the illogical argument more convincing.\nOther researchers also carried out variations of this study, exploring the possibility that people had misunderstood the question. They did not eliminate the error. It has been shown that individuals with high CRT scores are significantly less likely to be subject to the conjunction fallacy. The error disappears when the question is posed in terms of frequencies. Everyone in these versions of the study recognised that out of 100 people fitting an outline description, the conjunction statement (\"She is \"X\" and \"Y\"\") cannot apply to more people than the general statement (\"She is \"X\"\").\n\nTversky and Kahneman asked subjects to consider a problem about random variation. Imagining for simplicity that exactly half of the babies born in a hospital are male, the ratio will not be exactly half in every time period. On some days, more girls will be born and on others, more boys. The question was, does the likelihood of deviating from exactly half depend on whether there are many or few births per day? It is a well-established consequence of sampling theory that proportions will vary much more day-to-day when the typical number of births per day is small. However, people's answers to the problem do not reflect this fact. They typically reply that the number of births in the hospital makes no difference to the likelihood of more than 60% male babies in one day. The explanation in terms of the heuristic is that people consider only how representative the figure of 60% is of the previously given average of 50%.\n\nRichard E. Nisbett and colleagues suggest that representativeness explains the \"dilution effect\", in which irrelevant information weakens the effect of a stereotype. Subjects in one study were asked whether \"Paul\" or \"Susan\" was more likely to be assertive, given no other information than their first names. They rated Paul as more assertive, apparently basing their judgment on a gender stereotype. Another group, told that Paul's and Susan's mothers each commute to work in a bank, did not show this stereotype effect; they rated Paul and Susan as equally assertive. The explanation is that the additional information about Paul and Susan made them less representative of men or women in general, and so the subjects' expectations about men and women had a weaker effect. This means unrelated and non-diagnostic information about certain issue can make relative information less powerful to the issue when people understand the phenomenon.\n\nRepresentativeness explains systematic errors that people make when judging the probability of random events. For example, in a sequence of coin tosses, each of which comes up heads (H) or tails (T), people reliably tend to judge a clearly patterned sequence such as HHHTTT as less likely than a less patterned sequence such as HTHTTH. These sequences have exactly the same probability, but people tend to see the more clearly patterned sequences as less representative of randomness, and so less likely to result from a random process. Tversky and Kahneman argued that this effect underlies the gambler's fallacy; a tendency to expect outcomes to even out over the short run, like expecting a roulette wheel to come up black because the last several throws came up red. They emphasised that even experts in statistics were susceptible to this illusion: in a 1971 survey of professional psychologists, they found that respondents expected samples to be overly representative of the population they were drawn from. As a result, the psychologists systematically overestimated the statistical power of their tests, and underestimated the sample size needed for a meaningful test of their hypotheses.\n\nAnchoring and adjustment is a heuristic used in many situations where people estimate a number. According to Tversky and Kahneman's original description, it involves starting from a readily available number—the \"anchor\"—and shifting either up or down to reach an answer that seems plausible. In Tversky and Kahneman's experiments, people did not shift far enough away from the anchor. Hence the anchor contaminates the estimate, even if it is clearly irrelevant. In one experiment, subjects watched a number being selected from a spinning \"wheel of fortune\". They had to say whether a given quantity was larger or smaller than that number. For instance, they might be asked, \"Is the percentage of African countries which are members of the United Nations larger or smaller than 65%?\" They then tried to guess the true percentage. Their answers correlated well with the arbitrary number they had been given. Insufficient adjustment from an anchor is not the only explanation for this effect. An alternative theory is that people form their estimates on evidence which is selectively brought to mind by the anchor.\n\nThe anchoring effect has been demonstrated by a wide variety of experiments both in laboratories and in the real world. It remains when the subjects are offered money as an incentive to be accurate, or when they are explicitly told not to base their judgment on the anchor. The effect is stronger when people have to make their judgments quickly. Subjects in these experiments lack introspective awareness of the heuristic, denying that the anchor affected their estimates.\n\nEven when the anchor value is obviously random or extreme, it can still contaminate estimates. One experiment asked subjects to estimate the year of Albert Einstein's first visit to the United States. Anchors of 1215 and 1992 contaminated the answers just as much as more sensible anchor years. Other experiments asked subjects if the average temperature in San Francisco is more or less than 558 degrees, or whether there had been more or fewer than 100,025 top ten albums by The Beatles. These deliberately absurd anchors still affected estimates of the true numbers.\n\nAnchoring results in a particularly strong bias when estimates are stated in the form of a confidence interval. An example is where people predict the value of a stock market index on a particular day by defining an upper and lower bound so that they are 98% confident the true value will fall in that range. A reliable finding is that people anchor their upper and lower bounds too close to their best estimate. This leads to an overconfidence effect. One much-replicated finding is that when people are 98% certain that a number is in a particular range, they are wrong about thirty to forty percent of the time.\n\nAnchoring also causes particular difficulty when many numbers are combined into a composite judgment. Tversky and Kahneman demonstrated this by asking a group of people to rapidly estimate the product 8 x 7 x 6 x 5 x 4 x 3 x 2 x 1. Another group had to estimate the same product in reverse order; 1 x 2 x 3 x 4 x 5 x 6 x 7 x 8. Both groups underestimated the answer by a wide margin, but the latter group's average estimate was significantly smaller. The explanation in terms of anchoring is that people multiply the first few terms of each product and anchor on that figure. A less abstract task is to estimate the probability that an aircraft will crash, given that there are numerous possible faults each with a likelihood of one in a million. A common finding from studies of these tasks is that people anchor on the small component probabilities and so underestimate the total. A corresponding effect happens when people estimate the probability of multiple events happening in sequence, such as an accumulator bet in horse racing. For this kind of judgment, anchoring on the individual probabilities results in an overestimation of the combined probability.\n\nPeople's valuation of goods, and the quantities they buy, respond to anchoring effects. In one experiment, people wrote down the last two digits of their social security numbers. They were then asked to consider whether they would pay this number of dollars for items whose value they did not know, such as wine, chocolate, and computer equipment. They then entered an auction to bid for these items. Those with the highest two-digit numbers submitted bids that were many times higher than those with the lowest numbers. When a stack of soup cans in a supermarket was labelled, \"Limit 12 per customer\", the label influenced customers to buy more cans. In another experiment, real estate agents appraised the value of houses on the basis of a tour and extensive documentation. Different agents were shown different listing prices, and these affected their valuations. For one house, the appraised value ranged from US$114,204 to $128,754.\n\nAnchoring and adjustment has also been shown to affect grades given to students. In one experiment, 48 teachers were given bundles of student essays, each of which had to be graded and returned. They were also given a fictional list of the students' previous grades. The mean of these grades affected the grades that teachers awarded for the essay.\n\nOne study showed that anchoring affected the sentences in a fictional rape trial. The subjects were trial judges with, on average, more than fifteen years of experience. They read documents including witness testimony, expert statements, the relevant penal code, and the final pleas from the prosecution and defence. The two conditions of this experiment differed in just one respect: the prosecutor demanded a 34-month sentence in one condition and 12 months in the other; there was an eight-month difference between the average sentences handed out in these two conditions. In a similar mock trial, the subjects took the role of jurors in a civil case. They were either asked to award damages \"in the range from $15 million to $50 million\" or \"in the range from $50 million to $150 million\". Although the facts of the case were the same each time, jurors given the higher range decided on an award that was about three times higher. This happened even though the subjects were explicitly warned not to treat the requests as evidence.\n\nAssessments can also be influenced by the stimuli provided. In one review, researchers found that if a stimuli is perceived to be important or carry \"weight\" to a situation, that people were more likely to attribute that stimuli as heavier physically.\n\n\"Affect\", in this context, is a feeling such as fear, pleasure or surprise. It is shorter in duration than a mood, occurring rapidly and involuntarily in response to a stimulus. While reading the words \"lung cancer\" might generate an affect of dread, the words \"mother's love\" can create an affect of affection and comfort. When people use affect (\"gut responses\") to judge benefits or risks, they are using the affect heuristic. The affect heuristic has been used to explain why messages framed to activate emotions are more persuasive than those framed in a purely factual way.\n\nThere are competing theories of human judgment, which differ on whether the use of heuristics is irrational. A \"cognitive laziness\" approach argues that heuristics are inevitable shortcuts given the limitations of the human brain. According to the \"natural assessments\" approach, some complex calculations are already done rapidly and automatically by the brain, and other judgments make use of these processes rather than calculating from scratch. This has led to a theory called \"attribute substitution\", which says that people often handle a complicated question by answering a different, related question, without being aware that this is what they are doing. A third approach argues that heuristics perform just as well as more complicated decision-making procedures, but more quickly and with less information. This perspective emphasises the \"fast and frugal\" nature of heuristics.\n\nAn \"effort-reduction framework\" proposed by Anuj K. Shah and Daniel M. Oppenheimer states that people use a variety of techniques to reduce the effort of making decisions.\n\nIn 2002 Daniel Kahneman and Shane Frederick proposed a process called attribute substitution which happens without conscious awareness. According to this theory, when somebody makes a judgment (of a \"target attribute\") which is computationally complex, a rather more easily calculated \"heuristic attribute\" is substituted. In effect, a difficult problem is dealt with by answering a rather simpler problem, without the person being aware this is happening. This explains why individuals can be unaware of their own biases, and why biases persist even when the subject is made aware of them. It also explains why human judgments often fail to show regression toward the mean.\n\nThis substitution is thought of as taking place in the automatic \"intuitive\" judgment system, rather than the more self-aware \"reflective\" system. Hence, when someone tries to answer a difficult question, they may actually answer a related but different question, without realizing that a substitution has taken place.\n\nIn 1975, psychologist Stanley Smith Stevens proposed that the strength of a stimulus (e.g. the brightness of a light, the severity of a crime) is encoded by brain cells in a way that is independent of modality. Kahneman and Frederick built on this idea, arguing that the target attribute and heuristic attribute could be very different in nature.\nKahneman and Frederick propose three conditions for attribute substitution:\n\nKahneman gives an example where some Americans were offered insurance against their own death in a terrorist attack while on a trip to Europe, while another group were offered insurance that would cover death of any kind on the trip. Even though \"death of any kind\" includes \"death in a terrorist attack\", the former group were willing to pay more than the latter. Kahneman suggests that the attribute of fear is being substituted for a calculation of the total risks of travel. Fear of terrorism for these subjects was stronger than a general fear of dying on a foreign trip. See Morewedge and Kahneman (2010), for a recent summary of attribute substitution.\n\nGerd Gigerenzer and colleagues have argued that heuristics can be used to make judgments that are accurate rather than biased. According to them, heuristics are \"fast and frugal\" alternatives to more complicated procedures, giving answers that are just as good.\n\nWarren Thorngate, an emeritus social psychologist, implemented 10 simple decision rules or heuristics in a simulation program as computer subroutines chose an alternative. He determined how often each heuristic selected alternatives with highest-through-lowest expected value in a series of randomly generated decision situations. He found that most of the simulated heuristics selected alternatives with highest expected value and almost never selected alternatives with lowest expected value. More information about the simulation can be found in his \"Efficient decision heuristics\" article (1980).\n\nPsychologist Benoît Monin reports a series of experiments in which subjects, looking at photographs of faces, have to judge whether they have seen those faces before. It is repeatedly found that attractive faces are more likely to be mistakenly labeled as familiar. Monin interprets this result in terms of attribute substitution. The heuristic attribute in this case is a \"warm glow\"; a positive feeling towards someone that might either be due to their being familiar or being attractive. This interpretation has been criticised, because not all the variance in familiarity is accounted for by the attractiveness of the photograph.\n\nLegal scholar Cass Sunstein has argued that attribute substitution is pervasive when people reason about moral, political or legal matters. Given a difficult, novel problem in these areas, people search for a more familiar, related problem (a \"prototypical case\") and apply its solution as the solution to the harder problem. According to Sunstein, the opinions of trusted political or religious authorities can serve as heuristic attributes when people are asked their own opinions on a matter. Another source of heuristic attributes is emotion: people's moral opinions on sensitive subjects like sexuality and human cloning may be driven by reactions such as disgust, rather than by reasoned principles. Sunstein has been challenged as not providing enough evidence that attribute substitution, rather than other processes, is at work in these cases.\n\n\n"}
{"id": "26476831", "url": "https://en.wikipedia.org/wiki?curid=26476831", "title": "History of randomness", "text": "History of randomness\n\nIn ancient history, the concepts of chance and randomness were intertwined with that of fate. Many ancient peoples threw dice to determine fate, and this later evolved into games of chance. At the same time, most ancient cultures used various methods of divination to attempt to circumvent randomness and fate.\n\nThe Chinese were perhaps the earliest people to formalize odds and chance 3,000 years ago. The Greek philosophers discussed randomness at length, but only in non-quantitative forms. It was only in the sixteenth century that Italian mathematicians began to formalize the odds associated with various games of chance. The invention of modern calculus had a positive impact on the formal study of randomness. In the 19th century the concept of entropy was introduced in physics.\nThe early part of the twentieth century saw a rapid growth in the formal analysis of randomness, and mathematical foundations for probability were introduced, leading to its axiomatization in 1933. At the same time, the advent of quantum mechanics changed the scientific perspective on determinacy. In the mid to late 20th-century, ideas of algorithmic information theory introduced new dimensions to the field via the concept of algorithmic randomness.\n\nAlthough randomness had often been viewed as an obstacle and a nuisance for many centuries, in the twentieth century computer scientists began to realize that the \"deliberate\" introduction of randomness into computations can be an effective tool for designing better algorithms. In some cases, such randomized algorithms are able to outperform the best deterministic methods.\n\nPre-Christian people along the Mediterranean threw dice to determine fate, and this later evolved into games of chance. There is also evidence of games of chance played by ancient Egyptians, Hindus and\nChinese, dating back to 2100 BC. The Chinese used dice before the Europeans, and have a long history of playing games of chance.\n\nOver 3,000 years ago, the problems concerned with the tossing of several coins were considered in the I Ching, one of the oldest Chinese mathematical texts, that probably dates to 1150 BC. The two principal elements yin and yang were combined in the I Ching in various forms to produce \"Heads and Tails\" permutations of the type HH, TH, HT, etc. and the Chinese seem to have been aware of Pascal's triangle long before the Europeans formalized it in the 17th century. However, Western philosophy focused on the non-mathematical aspects of chance and randomness until the 16th century.\n\nThe development of the concept of chance throughout history has been very gradual. Historians have wondered why progress in the field of randomness was so slow, given that humans have encountered chance since antiquity. Deborah Bennett suggests that ordinary people face an inherent difficulty in understanding randomness, although the concept is often taken as being obvious and self-evident. She cites studies by Kahneman and Tversky; these concluded that statistical principles are not learned from everyday experience because people do not attend to the detail necessary to gain such knowledge.\n\nThe Greek philosophers were the earliest Western thinkers to address chance and randomness. Around 400 BC, Democritus presented a view of the world as governed by the unambiguous laws of order and considered randomness as a subjective concept that only originated from the inability of humans to understand the nature of events. He used the example of two men who would send their servants to bring water at the same time to cause them to meet. The servants, unaware of the plan, would view the meeting as random.\n\nAristotle saw chance and necessity as opposite forces. He argued that nature had rich and constant patterns that could not be the result of chance alone, but that these patterns never displayed the machine-like uniformity of necessary determinism. He viewed randomness as a genuine and widespread part of the world, but as subordinate to necessity and order. Aristotle classified events into three types: \"certain\" events that happen necessarily; \"probable\" events that happen in most cases; and \"unknowable\" events that happen by pure chance. He considered the outcome of games of chance as unknowable.\n\nAround 300 BC Epicurus proposed the concept that randomness exists by itself, independent of human knowledge. He believed that in the atomic world, atoms would \"swerve\" at random along their paths, bringing about randomness at higher levels.\nFor several centuries thereafter, the idea of chance continued to be intertwined with fate. Divination was practiced in many cultures, using diverse methods. The Chinese analyzed the cracks in turtle shells, while the Germans, who according to Tacitus had the highest regards for lots and omens, utilized strips of bark. In the Roman Empire, chance was personified by the Goddess Fortuna. The Romans would partake in games of chance to simulate what Fortuna would have decided. In 49 BC, Julius Caesar allegedly decided on his fateful decision to cross the Rubicon after throwing dice.\n\nAristotle's classification of events into the three classes: \"certain\", \"probable\" and \"unknowable\" was adopted by Roman philosophers, but they had to reconcile it with deterministic Christian teachings in which even events unknowable to man were considered to be predetermined by God. About 960 Bishop Wibold of Cambrai correctly enumerated the 56 different outcomes (without permutations) of playing with three dice. No reference to playing cards has been found in Europe before 1350. The Church preached against card playing, and card games spread much more slowly than games based on dice. The Christian Church specifically forbade divination; and wherever Christianity went, divination lost most of its old-time power.\n\nOver the centuries, many Christian scholars wrestled with the conflict between the belief in free will and its implied randomness, and the idea that God knows everything that happens. Saints Augustine and Aquinas tried to reach an accommodation between foreknowledge and free will, but Martin Luther argued against randomness and took the position that God's omniscience renders human actions unavoidable and determined. In the 13th century, Thomas Aquinas viewed randomness not as the result of a single cause, but of several causes coming together by chance. While he believed in the existence of randomness, he rejected it as an explanation of the end-directedness of nature, for he saw too many patterns in nature to have been obtained by chance.\n\nThe Greeks and Romans had not noticed the magnitudes of the relative frequencies of the games of chance. For centuries, chance was discussed in Europe with no mathematical foundation and it was only in the 16th century that Italian mathematicians began to discuss the outcomes of games of chance as ratios. In his 1565 \"Liber de Lude Aleae\" (a gambler's manual published after his death) Gerolamo Cardano wrote one of the first formal tracts to analyze the odds of winning at various games.\n\nAround 1620 Galileo wrote a paper called \"On a discovery concerning dice\" that used an early probabilistic model to address specific questions. In 1654, prompted by Chevalier de Méré's interest in gambling, Blaise Pascal corresponded with Pierre de Fermat, and much of the groundwork for probability theory was laid. Pascal's Wager was noted for its early use of the concept of infinity, and the first formal use of decision theory. The work of Pascal and Fermat influenced Leibniz's work on the infinitesimal calculus, which in turn provided further momentum for the formal analysis of probability and randomness.\n\nThe first known suggestion for viewing randomness in terms of complexity was made by Leibniz in an obscure 17th-century document discovered after his death. Leibniz asked how one could know if a set of points on a piece of paper were selected at random (e.g. by splattering ink) or not. Given that for any set of finite points there is always a mathematical equation that can describe the points, (e.g. by Lagrangian interpolation) the question focuses on the way the points are expressed mathematically. Leibniz viewed the points as random if the function describing them had to be extremely complex. Three centuries later, the same concept was formalized as algorithmic randomness by A. N. Kolmogorov and Gregory Chaitin as the minimal length of a computer program needed to describe a finite string as random.\n\n\"The Doctrine of Chances\", the first textbook on probability theory was published in 1718 and the field continued to grow thereafter. The frequency theory approach to probability was first developed by Robert Ellis and John Venn late in the 19th century. \n\nWhile the mathematical elite was making progress in understanding randomness from the 17th to the 19th century, the public at large continued to rely on practices such as fortune telling in the hope of taming chance. Fortunes were told in a multitude of ways both in the Orient (where fortune telling was later termed an addiction) and in Europe by gypsies and others. English practices such as the reading of eggs dropped in a glass were exported to Puritan communities in North America.\n\nThe term entropy, which is now a key element in the study of randomness, was coined by Rudolf Clausius in 1865 as he studied heat engines in the context of the second law of thermodynamics. Clausius was the first to state \"entropy always increases\".\n\nFrom the time of Newton until about 1890, it was generally believed that if one knows the initial state of a system with great accuracy, and if all the forces acting on the system can be formulated with equal accuracy, it would be possible, in principle, to make predictions of the state of the universe for an infinitely long time. The limits to such predictions in physical systems became clear as early as 1893 when Henri Poincaré showed that in the three-body problem in astronomy, small changes to the initial state could result in large changes in trajectories during the numerical integration of the equations.\n\nDuring the 19th century, as probability theory was formalized and better understood, the attitude towards \"randomness as nuisance\" began to be questioned. Goethe wrote:\nThe tissue of the world\nis built from necessities and randomness;\nthe intellect of men places itself between both\nand can control them;\nit considers the necessity \nand the reason of its existence;\nit knows how randomness can be\nmanaged, controlled, and used.\nThe words of Goethe proved prophetic, when in the 20th century randomized algorithms were discovered as powerful tools. By the end of the 19th century, Newton's model of a mechanical universe was fading away as the statistical view of the collision of molecules in gases was studied by Maxwell and Boltzmann. Boltzmann's equation \"S\" = \"k\" log \"W\" (inscribed on his tombstone) first related entropy with logarithms.\n\nDuring the 20th century, the five main interpretations of probability theory (e.g., \"classical\", \"logical\", \"frequency\", \"propensity\" and \"subjective\") became better understood, were discussed, compared and contrasted. A significant number of application areas were developed in this century, from finance to physics. In 1900 Louis Bachelier applied Brownian motion to evaluate stock options, effectively launching the fields of financial mathematics and stochastic processes.\n\nÉmile Borel was one of the first mathematicians to formally address randomness in 1909, and introduced normal numbers. In 1919 Richard von Mises gave the first definition of algorithmic randomness via the impossibility of a gambling system. He advanced the frequency theory of randomness in terms of what he called \"the collective\", i.e. a random sequence. Von Mises regarded the randomness of a collective as an empirical law, established by experience. He related the \"disorder\" or randomness of a collective to the lack of success of attempted gambling systems. This approach led him to suggest a definition of randomness that was later refined and made mathematically rigorous by Alonzo Church by using computable functions in 1940. Richard von Mises likened the principle of the \"impossibility of a gambling system\" to the principle of the conservation of energy, a law that cannot be proven, but has held true in repeated experiments.\n\nVon Mises never totally formalized his rules for sub-sequence selection, but in his 1940 paper \"On the concept of random sequence\", Alonzo Church suggested that the functions used for place settings in the formalism of von Mises be computable functions rather than arbitrary functions of the initial segments of the sequence, appealing to the Church–Turing thesis on effectiveness.\n\nThe advent of quantum mechanics in the early 20th century and the formulation of the Heisenberg uncertainty principle in 1927 saw the end to the Newtonian mindset among physicists regarding the \"determinacy of nature\". In quantum mechanics, there is not even a way to consider all observable elements in a system as random variables \"at once\", since many observables do not commute.\nBy the early 1940s, the frequency theory approach to probability was well accepted within the Vienna circle, but in the 1950s Karl Popper proposed the propensity theory. Given that the frequency approach cannot deal with \"a single toss\" of a coin, and can only address large ensembles or collectives, the single-case probabilities were treated as propensities or chances. The concept of propensity was also driven by the desire to handle single-case probability settings in quantum mechanics, e.g. the probability of decay of a specific atom at a specific moment. In more general terms, the frequency approach can not deal with the probability of the death of a \"specific person\" given that the death can not be repeated multiple times for that person. Karl Popper echoed the same sentiment as Aristotle in viewing randomness as subordinate to order when he wrote that \"the concept of chance is not opposed to the concept of law\" in nature, provided one considers the laws of chance.\n\nClaude Shannon's development of Information theory in 1948 gave rise to the \"entropy view\" of randomness. In this view, randomness is the opposite of \"determinism\" in a stochastic process. Hence if a stochastic system has entropy zero it has no randomness and any increase in entropy increases randomness. Shannon's formulation defaults to Boltzmann's 19th century formulation of entropy in case all probabilities are equal. Entropy is now widely used in diverse fields of science from thermodynamics to quantum chemistry.\n\nMartingales for the study of chance and betting strategies were introduced by Paul Lévy in the 1930s and were formalized by Joseph L. Doob in the 1950s. The application of random walk hypothesis in financial theory was first proposed by Maurice Kendall in 1953. It was later promoted by Eugene Fama and Burton Malkiel.\n\nRandom strings were first studied by in the 1960s by A. N. Kolmogorov (who had provided the first axiomatic definition of probability theory in 1933), Chaitin and Martin-Löf. The algorithmic randomness of a string was defined as the minimum size of a program (e.g. in bits) executed on a universal computer that yields the string. Chaitin's Omega number later related randomness and the halting probability for programs.\n\nIn 1964, Benoît Mandelbrot suggested that most statistical models approached only a first stage of dealing with indeterminism, and that they ignored many aspects of real world turbulance. In his 1997 he defined \"seven states of randomness\" ranging from \"mild to wild\", with traditional randomness being at the mild end of the scale.\n\nDespite mathematical advances, reliance on other methods of dealing with chance, such as fortune telling and astrology continued in the 20th century. The government of Myanmar reportedly shaped 20th century economic policy based on fortune telling and planned the move of the capital of the country based on the advice of astrologers. White House Chief of Staff Donald Regan criticized the involvement of astrologer Joan Quigley in decisions made during Ronald Reagan's presidency in the 1980s. Quigley claims to have been the White House astrologer for seven years.\n\nDuring the 20th century, limits in dealing with randomness were better understood. The best-known example of both theoretical and operational limits on predictability is weather forecasting, simply because models have been used in the field since the 1950s. Predictions of weather and climate are necessarily uncertain. Observations of weather and climate are uncertain and incomplete, and the models into which the data are fed are uncertain. In 1961, Edward Lorenz noticed that a very small change to the initial data submitted to a computer program for weather simulation could result in a completely different weather scenario. This later became known as the butterfly effect, often paraphrased as the question: \"Does the flap of a butterfly’s wings in Brazil set off a tornado in Texas?\". A key example of serious practical limits on predictability is in geology, where the ability to predict earthquakes either on an individual or on a statistical basis remains a remote prospect.\n\nIn the late 1970s and early 1980s, computer scientists began to realize that the \"deliberate\" introduction of randomness into computations can be an effective tool for designing better algorithms. In some cases, such randomized algorithms outperform the best deterministic methods.\n"}
{"id": "50084188", "url": "https://en.wikipedia.org/wiki?curid=50084188", "title": "Indonesia National Science Olympiad", "text": "Indonesia National Science Olympiad\n\nThe Indonesian National Science Olympiad (Indonesian: \"Olimpiade Sains Nasional\") is a science competition for Indonesian students held by the Indonesian Ministry of Education and Culture. This Olympiad consists some competitions for elementary school (SD) students, junior high school (SMP) students, and senior high school (SMA) students. \n\nThe competition pits students from the thirty-four provinces of Indonesia, and winners of the competition are further selected to represent Indonesia in their respective subjects' International Science Olympiad. In addition, Indonesian public universities are required to accept medal-winners of the competition into their undergraduate programmes.\n\nIt was initiated in 2002 when Indonesia first became host of the International Physics Olympiad. The first ever national-stage competition was held in Yogyakarta, and in 2003 it was held in Balikpapan with improved rules and procedures.\n\nThe competition is divided into 3 levels:\n\nIn general, the overall competition is divided into four stages:\nIn 2016, over 320,000 students participated in the \"Olimpiade Sains Kabupaten\" across Indonesia, with 1,579 eventually making it into the national phase. In comparison, 420 medals were given out in 2017.\n\nAs of 2018, only two provinces have ever won the competition: DKI Jakarta and Central Java. Both provinces are commonly seen in the competition as dominant participants, oftentimes sending the largest delegations of students.\n\n"}
{"id": "2233078", "url": "https://en.wikipedia.org/wiki?curid=2233078", "title": "International Cometary Explorer", "text": "International Cometary Explorer\n\nThe International Cometary Explorer (ICE) spacecraft (designed and launched as the International Sun-Earth Explorer-3 (ISEE-3) satellite), was launched August 12, 1978, into a heliocentric orbit. It was one of three spacecraft, along with the mother/daughter pair of ISEE-1 and ISEE-2, built for the International Sun-Earth Explorer (ISEE) program, a joint effort by NASA and ESRO/ESA to study the interaction between the Earth's magnetic field and the solar wind.\n\nISEE-3 was the first spacecraft to be placed in a halo orbit at the Earth-Sun Lagrangian point. Renamed ICE, it became the first spacecraft to visit a comet, passing through the plasma tail of comet Giacobini-Zinner within about of the nucleus on September 11, 1985.\n\nNASA suspended routine contact with ISEE-3 in 1997, and made brief status checks in 1999 and 2008.\n\nOn May 29, 2014, two-way communication with the spacecraft was reestablished by the ISEE-3 Reboot Project, an unofficial group with support from the Skycorp company. On July 2, 2014, they fired the thrusters for the first time since 1987. However, later firings of the thrusters failed, apparently due to a lack of nitrogen pressurant in the fuel tanks. The project team initiated an alternative plan to use the spacecraft to \"collect scientific data and send it back to Earth,\" but on September 16, 2014, contact with the probe was lost.\n\nISEE-3 carries no cameras; instead, its instruments measure energetic particles, waves, plasmas, and fields.\n\nISEE-3 originally operated in a halo orbit about the Sun-Earth Lagrangian point, 235 Earth radii above the surface (about 1.5 million km, or 924,000 miles). It was the first artificial object placed at a so-called \"libration point\", entering orbit there on November 20, 1978, proving that such a suspension between gravitational fields was possible. It rotates at 19.76 rpm about an axis perpendicular to the ecliptic, to keep it oriented for its experiments, to generate solar power and to communicate with Earth.\n\nThe purposes of the mission were:\n\nAfter completing its original mission, ISEE-3 was re-tasked to study the interaction between the solar wind and a cometary atmosphere. On June 10, 1982, the spacecraft performed a maneuver which removed it from its halo orbit around the point and placed it in a transfer orbit. This involved a series of passages between Earth and the Sun-Earth Lagrangian point, through the Earth's magnetotail. Fifteen propulsive maneuvers and five lunar gravity assists resulted in the spacecraft being ejected from the Earth-Moon system and into a heliocentric orbit. Its last and closest pass over the Moon, on December 22, 1983, was only above the lunar surface; following this pass, the spacecraft was re-designated as the International Cometary Explorer (ICE).\n\nIts new orbit put it ahead of the Earth on a trajectory to intercept comet Giacobini-Zinner. On September 11, 1985, the craft passed through the comet's plasma tail.\n\nICE transited between the Sun and Comet Halley in late March 1986, when other spacecraft were near the comet on their early-March comet rendezvous missions. (This \"Halley Armada\" included Giotto, Vega 1 and 2, Suisei and Sakigake.) ICE flew through the tail; its minimum distance to the comet nucleus was . For comparison, Earth's minimum distance to Comet Halley in 1910 was .\n\nAn update to the ICE mission was approved by NASA in 1991. It defines a heliospheric mission for ICE consisting of investigations of coronal mass ejections in coordination with ground-based observations, continued cosmic ray studies, and the Ulysses probe. By May 1995, ICE was being operated under a low duty cycle, with some data-analysis support from the Ulysses project.\n\nOn May 5, 1997, NASA ended the ICE mission, leaving only a carrier signal operating. The ISEE-3/ICE downlink bit rate was nominally 2048 bits per second during the early part of the mission, and 1024 bit/s during the Giacobini-Zinner comet encounter. The bit rate then successively dropped to 512 bit/s (on December 9, 1985), 256 bit/s (on January 5, 1987), 128 bit/s (on January 24, 1989) and finally to 64 bit/s (on December 27, 1991). Though still in space, NASA donated the craft to the Smithsonian Museum.\n\nBy January 1990, ICE was in a 355-day heliocentric orbit with an aphelion of 1.03 AU, a perihelion of 0.93 AU and an inclination of 0.1 degree.\n\nIn 1999, NASA made brief contact with ICE to verify its carrier signal.\n\nOn September 18, 2008, NASA, with the help of KinetX, located ICE using the NASA Deep Space Network after discovering that it had not been powered off after the 1999 contact. A status check revealed that all but one of its 13 experiments were still functioning, and it still had enough propellant for of Δv.\n\nIt was determined to be possible to reactivate the spacecraft in 2014, when it again made a close approach to Earth, and scientists discussed reusing the probe to observe more comets in 2017 or 2018.\n\nSometime after NASA's interest in the ICE waned, others realized that the spacecraft might be steered to pass close to another comet. A team of engineers, programmers, and scientists began to study the feasibility and challenges involved.\n\nIn April 2014, its members formally announced their intentions to \"recapture\" the spacecraft for use, calling the effort the ISEE-3 Reboot Project. A team webpage said, \"We intend to contact the ISEE-3 (International Sun-Earth Explorer) spacecraft, command it to fire its engine and enter an orbit near Earth, and then resume its original mission... If we are successful we intend to facilitate the sharing and interpretation of all of the new data ISEE-3 sends back via crowd sourcing.\"\n\nOn May 15, the project reached its crowdfunding goal of US$125,000 on RocketHub, which was expected to cover the costs of writing the software to communicate with the probe, searching through the NASA archives for the information needed to control the spacecraft, and buying time on the dish antennas. The project then set a \"stretch goal\" of $150,000, which it also met with a final total of $159,502 raised.\n\nThe project members were working on deadline: if they got the spacecraft to change its orbit by late May or early June 2014, or in early July by using more fuel, it could use the Moon's gravity to get back into a useful halo orbit.\n\nEarlier in 2014, officials with the Goddard Space Flight Center said the Deep Space Network equipment necessary to transmit signals to the spacecraft had been decommissioned in 1999, and was too expensive to replace. However, project members were able to find documentation for the original equipment and were able to simulate the complex modulator/demodulator electronics using modern software-defined radio (SDR) techniques and open-source programs from the GNU Radio project. They obtained the needed hardware, an off-the-shelf SDR transceiver and power amplifier, and installed it on the 305-meter Arecibo dish antenna on May 19, 2014. Once they gained control of the spacecraft, the capture team planned to shift the primary ground station to the 21-meter dish located at Kentucky's Morehead State University Space Science Center. The 20-meter dish antenna in Bochum Observatory, Germany, would be a support station.\n\nAlthough NASA was not funding the project, it made advisors available and gave approval to try to establish contact. On May 21, 2014, NASA announced that it had signed a Non-Reimbursable Space Act Agreement with the ISEE-3 Reboot Project. \"This is the first time NASA has worked such an agreement for use of a spacecraft the agency is no longer using or ever planned to use again,\" officials said.\n\nOn May 29, 2014, the reboot team successfully commanded the probe to switch into Engineering Mode to begin to broadcast telemetry.\n\nOn June 26, project members using the Goldstone Deep Space Communications Complex DSS-24 antenna achieved synchronous communication and obtained the four ranging points needed to refine the spacecraft's orbital parameters.\nThe project team received approval from NASA to continue operations through at least July 16, and made plans to attempt the orbital maneuver in early July.\n\nOn July 2, the reboot project fired the thrusters for the first time since 1987. They spun up the spacecraft to its nominal roll rate, in preparation for the upcoming trajectory correction maneuver in mid-July.\n\nOn July 8, a longer sequence of thrusters firings failed, apparently due to loss of the nitrogen gas needed to pressurize the fuel tanks.\n\nOn July 24, the ISEE-3 Reboot Team announced that all attempts to change orbit using the ISEE-3 propulsion system had failed. Instead, the team said, the \"ISEE-3 Interplanetary Citizen Science Mission\" would gather data as the spacecraft flies by the Moon on August 10 and enters a heliocentric orbit similar to Earth's. The team began shutting down propulsion components to maximize the electrical power available for the science experiments.\n\nOn July 30, the team announced that it still planned to acquire data from as much of ISEE-3's 300-day orbit as possible. With five of the 13 instruments on the spacecraft still working, the science possibilities included listening for gamma-ray bursts, where observations from additional locations in the Solar System can be valuable. The team was also recruiting additional receiving sites around the globe to improve diurnal coverage, in order to upload additional commands while the spacecraft is close to Earth and later to receive data.\n\nOn August 10 at 18:16 UTC, the spacecraft passed about from the surface of the Moon. It will continue in its heliocentric orbit, and will return to the vicinity of Earth in 2031.\n\nOn September 25, 2014, the Reboot team announced that contact with the probe was lost on September 16. It is unknown whether contact can be reestablished because the probe's exact orbit is uncertain. The spacecraft's post-lunar flyby orbit takes it further from the Sun, causing electrical power available from its solar arrays to drop, and its battery failed in 1981. Reduced power could have caused the craft to enter a safe mode, from which it may be impossible to awaken without the precise orbital location information needed to point transmissions at the craft.\n\nThe ICE spacecraft is a barrel-like cylindrical shape covered by solar panels. Four long antennas protrude equidistant around the circumference of the spacecraft, spanning . It has a dry mass of and can generate nominal power of 173 watts.\n\nICE carries 13 scientific instruments to measure plasmas, energetic particles, waves, and fields. , five were known to be functional. It does not carry a camera or imaging system. Its detectors measure high energy particles such as X- and gamma-rays, solar wind, plasma and cosmic particles. A data handling system gathers the scientific and engineering data from all systems in the spacecraft and formats them into a serial stream for transmission. The transmitter output power is five watts.\n\n\n\n"}
{"id": "43984809", "url": "https://en.wikipedia.org/wiki?curid=43984809", "title": "Lewis and Clark Memorial Column", "text": "Lewis and Clark Memorial Column\n\nThe Lewis and Clark Memorial Column is an outdoor monument by artist Otto Schumann, dedicated to Meriwether Lewis and William Clark for their expedition and located at Washington Park in Portland, Oregon.\n\nThe sculpture, made of Snake River granite, is a Classical column with a sphere on top. It is approximately 34 feet, 6 inches tall, with a diameter of 2.5 feet. The obelisk sits on a square base that is approximately 5 feet, 5 inches tall. The base's sides display bronze seals for the states of Oregon, Washington, Idaho and Montana, which once comprised the Oregon Territory. An illuminated path leads up the monument. According to the Smithsonian Institution, the work is administered by City of Portland's Metropolitan Arts Commission.\n\nThe memorial was commissioned around 1902 by the Lewis & Clark Exposition Commission for approximately $10,500 as a \"gift of the people of Oregon in memory\" of the duo. President Theodore Roosevelt laid the first cornerstone on May 21, 1903, and the piece was completed and dedicated in 1908. It was surveyed and considered \"well maintained\" by Smithsonian's \"Save Outdoor Sculpture!\" program in November 1993.\n\n"}
{"id": "47338914", "url": "https://en.wikipedia.org/wiki?curid=47338914", "title": "Liber de orbe", "text": "Liber de orbe\n\nLiber de orbe was a Latin translation made in 1130s CE of an Arabic work attributed to the 8th century astrologer Mashallah ibn Athari. \n\nThe work's main topic is cosmology and is considered as one of the earliest works on Aristotelian physics available in Latin.\n\n"}
{"id": "17565789", "url": "https://en.wikipedia.org/wiki?curid=17565789", "title": "List of Colorado state symbols", "text": "List of Colorado state symbols\n\nThe following is a list of symbols of the U.S. state of Colorado.\n\n\n"}
{"id": "2618605", "url": "https://en.wikipedia.org/wiki?curid=2618605", "title": "List of benzo compounds", "text": "List of benzo compounds\n\nBenzo might also refer to: Benzodiazepine \nIn organic chemistry the addition of the prefix benzo to the name of a chemical compound indicates the addition of an even number of carbon atoms to an unsaturated or already aromatic compound by which a new aromatic ring is formed. Between the prefix \"benzo\" and the name of the parent compound then place of the addition of the extra carbon atoms is indicated by letters written between square brackets. Quite often the number of added carbon atoms is four, although sometimes two else will do the job as shown in the following table. The first entry also shows different routes to the name of the same molecule.\n"}
{"id": "19501639", "url": "https://en.wikipedia.org/wiki?curid=19501639", "title": "List of enterprise search vendors", "text": "List of enterprise search vendors\n\nThis is a list of enterprise search vendors.\n\n\n\n\n\n"}
{"id": "21447515", "url": "https://en.wikipedia.org/wiki?curid=21447515", "title": "List of healthcare journals", "text": "List of healthcare journals\n\nThis is a list of academic journals on health care.\n\n\n\n\n\n\n\n\n\n"}
{"id": "11486275", "url": "https://en.wikipedia.org/wiki?curid=11486275", "title": "List of towns and cities with 100,000 or more inhabitants/cityname: Z", "text": "List of towns and cities with 100,000 or more inhabitants/cityname: Z\n\n\n"}
{"id": "879815", "url": "https://en.wikipedia.org/wiki?curid=879815", "title": "Martin Spangberg", "text": "Martin Spangberg\n\nMartin Spangberg or Martyn Petrovich Shpanberg (Мартын Петрович Шпанберг) was a Danish naval lieutenant in Russian service who took part with his compatriot Vitus Bering in both Kamchatka expeditions as second in command. He is best known for finding a sea route to Japan and exploring the Kuril Islands (one of which, Shikotan, was renamed Shpanberg by the Russians in 1796). \n\nIn 1738, Spangberg was in command of the first Russian naval squadron to visit the island of Honshu in Japan. The Russians landed in a scenic area which is now part of the Rikuchu Kaigan National Park. Despite the prevalent policy of sakoku, the sailors were treated with politeness if not friendliness. This was the first diplomatic encounter of the Russians and the Japanese. he also made voyages in 1739 and 1742 to survey the coasts of Sakhalin, Japan and the Kuril Islands. Spangberg left a brief account of this expedition. He died in 1761.\n\nShpanberg Island in the Kurils and Shpanberg Island in the Nordenskiöld Archipelago were named after him.\n"}
{"id": "39227709", "url": "https://en.wikipedia.org/wiki?curid=39227709", "title": "Massive Online Analysis", "text": "Massive Online Analysis\n\nMassive Online Analysis (MOA) is a free open-source software project specific for data stream mining with concept drift. It is written in Java and developed at the University of Waikato, New Zealand.\n\nMOA is an open-source framework software that allows to build and run experiments\nof machine learning or data mining on evolving data streams. It includes a set of learners and stream generators that can be used from the Graphical User Interface (GUI), the command-line, and the Java API.\nMOA contains several collections of machine learning algorithms:\n\n\nThese algorithms are designed for large scale machine learning, dealing with concept drift, and big data streams in real time.\n\nMOA supports bi-directional interaction with Weka (machine learning). MOA is free software released under the GNU GPL.\n\n\n"}
{"id": "1654080", "url": "https://en.wikipedia.org/wiki?curid=1654080", "title": "Mephisto (automaton)", "text": "Mephisto (automaton)\n\nMephisto was the name given to a chess-playing \"pseudo-automaton\" built in 1876. Unlike The Turk and Ajeeb it had no hidden operator, instead being remotely controlled by electromechanical means.\n\nConstructed by Charles Godfrey Gumpel (c.1835 - 1921), an Alsatian manufacturer of artificial limbs, it took some 6 or 7 years to build and was first shown in 1878 at Gumpel's home in Leicester Square, London. Mephisto was mainly operated by chess master Isidor Gunsberg.\n\n\"Mephisto\" consisted of a life-size figure of an elegant devil, with one foot rendered as a cloven hoof, dressed in red velvet and seated in an armchair in front of an unenclosed, open-sided table. This table set-up was provided to reassure the player that there were no compartments beneath the board where a man could be hidden (as in \"The Turk\"). In addition, the public was invited to inspect the contraption before each exhibition, with the intention of demonstrating that there was no player inside. The chessboard was noted as having had indentations on each square that held the bases of the chessmen to prevent them from moving unintentionally. The figure of Mephisto itself was bolted to the table at the chest to enable its arm full reach across the board.\n\nIt was the first automaton to win a Chess tournament when it was entered in the Counties Chess Association in London in 1878 and at one time had its own chess club. In 1879 Mephisto, with Gunsberg, went on tour, defeating every male player. When playing ladies, however, Mephisto would first obtain a winning position before losing the game then courteously offer to shake their hand afterwards.\n\nWhen Mephisto was shown at the Paris Exposition of 1889 it was operated by Jean Taubenhaus. After 1889 it was dismantled and its subsequent whereabouts are unknown.\n\n\"Mephisto\" was later used as the name of a top-line dedicated chess computer which won the World Microcomputer Chess Championship in the years 1985-1990. The name is now used by the consumer electronics company Saitek on its line of standalone chess computers.\n\n"}
{"id": "33787267", "url": "https://en.wikipedia.org/wiki?curid=33787267", "title": "Nuclear resonance vibrational spectroscopy", "text": "Nuclear resonance vibrational spectroscopy\n\nNuclear resonance vibrational spectroscopy is a synchrotron-based technique that probes vibrational energy levels. The technique, often called NRVS, is specific for samples that contain nuclei that respond to Mössbauer spectroscopy, most commonly iron. The method exploits the high resolution offered by synchrotron light sources, which enables the resolution of vibrational fine structure, especially those vibrations that are coupled to the position of the Fe centre(s). The method is popularly applied to problems in bioinorganic chemistry, materials science, and geophysics. A novel aspect of the method is the ability to determine the 3D-trajectory of iron atoms within vibrational modes, providing a unique appraisal of DFT-prediction accuracy. Other names for this method include nuclear inelastic scattering (NIS), nuclear inelastic absorption (NIA), nuclear resonant inelastic x-ray scattering (NRIXS), and phonon assisted Mössbauer effect.\n\nIn the experimental setup, X-rays are released from the particle beam by an undulator; a high-resolution monochromator produces a beam with small energy dispersion (typically 1.0 meV). The sample is irradiated with photons chosen around the resonance of the Mössbauer isotope and further information is provided for the specific isotope. Typical parameters for the experimental scan are –20 meV below recoil-free resonance energy to +100 meV above it. The number of scans (often recorded for 5 seconds every 0.2 meV) depends on the amount of Mössbauer-active nuclei in the sample. The number of photons absorbed by the sample at any wavelength are measured by detecting the fluorescence emitted from the excited atom with an avalanche photodiode detector. The resulting raw spectrum contains a high-intensity resonance at corresponds to the nuclear excited state of the probed nucleus. For bulk samples, the technique detects natural abundance Fe. For many dilute or biological samples, the sample is often enriched in Fe.\n"}
{"id": "7577556", "url": "https://en.wikipedia.org/wiki?curid=7577556", "title": "Oort constants", "text": "Oort constants\n\nThe Oort constants (discovered by Jan Oort) formula_1 and formula_2 are empirically derived parameters that characterize the local rotational properties of our galaxy, the Milky Way, in the following manner:\n\nwhere formula_4 and formula_5 are the rotational velocity and distance to the Galactic center, respectively, measured at the position of the Sun, and and are the velocities and distances at other positions in our part of the galaxy. As derived below, and depend only on the motions and positions of stars in the solar neighborhood. As of 2018, the most accurate values of these constants are formula_1 = 15.3 ± 0.4 km s kpc and formula_2 = -11.9 ± 0.4 km s kpc. From the Oort constants, it is possible to determine the orbital properties of the Sun, such as the orbital velocity and period, and infer local properties of the Galactic disk, such as the mass density and how the rotational velocity changes as a function of radius from the Galactic center.\n\nBy the 1920s, a large fraction of the astronomical community had recognized that some of the diffuse, cloud-like objects, or nebulae, seen in the night sky were collections of stars located beyond our own, local collection of star clusters. These \"galaxies\" had diverse morphologies, ranging from ellipsoids to disks. The concentrated band of starlight that is the visible signature of the Milky Way was indicative of a disk structure for our galaxy; however, our location within our galaxy made structural determinations from observations difficult.\n\nClassical mechanics predicted that a collection of stars could be supported against gravitational collapse by either random velocities of the stars or their rotation about its center of mass. For a disk-shaped collection, the support should be mainly rotational. Depending on the mass density, or distribution of the mass in the disk, the rotation velocity may be different at each radius from the center of the disk to the outer edge. A plot of these rotational velocities against the radii at which they are measured is called a rotation curve. For external disk galaxies, one can measure the rotation curve by observing the Doppler shifts of spectral features measured along different galactic radii, since one side of the galaxy will be moving towards our line of sight and one side away. However, our position in the Galactic midplane of the Milky Way, where dust in molecular clouds obscures most optical light in many directions, made obtaining our own rotation curve technically difficult until the discovery of the 21 cm hydrogen line in the 1930s.\n\nTo confirm the rotation of our galaxy prior to this, in 1927 Jan Oort derived a way to measure the Galactic rotation from just a small fraction of stars in the local neighborhood. As described below, the values he found for formula_1 and formula_2 proved not only that the Galaxy was rotating but also that it rotates differentially, or as a fluid rather than a solid body.\n\nConsider a star in the midplane of the Galactic disk with Galactic longitude formula_10 at a distance formula_11 from the Sun. Assume that both the star and the Sun have circular orbits around the center of the Galaxy at radii of formula_12 and formula_13 from the galactic center and rotational velocities of formula_14 and formula_15, respectively. The motion of the star along our line of sight, or \"radial velocity\", and motion of the star across the plane of the sky, or \"transverse velocity\", as observed from the position of the Sun are then:\n\nWith the assumption of circular motion, the rotational velocity is related to the angular velocity by formula_17 and we can substitute this into the velocity expressions:\n\nFrom the geometry in Figure 1, one can see that the triangles formed between the galactic center, the Sun, and the star share a side or portions of sides, so the following relationships hold and substitutions can be made:\n\nand with these we get\n\nTo put these expressions only in terms of the known quantities formula_10 and formula_11, we take a Taylor expansion of formula_23 about formula_13.\n\nAdditionally, we take advantage of the assumption that the stars used for this analysis are \"local\", i.e. formula_26 is small, and the distance d to the star is smaller than formula_12 or formula_13, and we take:\nSo:\n\nUsing the sine and cosine half angle formulae, these velocities may be rewritten as:\n\nWriting the velocities in terms of our known quantities and two coefficients formula_1 and formula_2 yields:\n\nwhere\n\nAt this stage, the observable velocities are related to these coefficients and the position of the star. It is now possible to relate these coefficients to the rotation properties of the galaxy. For a star in a circular orbit, we can express the derivative of the angular velocity with respect to radius in terms of the rotation velocity and radius and evaluate this at the location of the Sun:\n\nso\n\nformula_1 is the Oort constant describing the shearing motion and formula_2 is the Oort constant describing the rotation of the Galaxy. As described below, one can measure formula_1 and formula_2 from plotting these velocities, measured for many stars, against the galactic longitudes of these stars.\n\nAs mentioned in an intermediate step in the derivation above:\n\nTherefore, we can write the Oort constants formula_1 and formula_2 as:\n\nThus, the Oort constants can be expressed in terms of the radial and transverse velocities, distances, and galactic longitudes of objects in our Galaxy - all of which are, in principle, observable quantities.\n\nHowever, there are a number of complications. The simple derivation above assumed that both the Sun and the object in question are traveling on circular orbits about the Galactic center. This is not true for the Sun (the Sun's velocity relative to the local standard of rest is approximately 13.4 km/s), and not necessarily true for other objects in the Milky Way either. The derivation also implicitly assumes that the gravitational potential of the Milky Way is axisymmetric and always directed towards the center. This ignores the effects of spiral arms and the Galaxy's bar. Finally, both transverse velocity and distance are notoriously difficult to measure for objects which are not relatively nearby.\n\nSince the non-circular component of the Sun's velocity is known, it can be subtracted out from our observations to compensate. We do not know, however, the non-circular components of the velocity of each individual star we observe, so they cannot be compensated for in this way. But, if we plot transverse velocity divided by distance against galactic longitude for a large sample of stars, we know from the equations above that they will follow a sine function. The non-circular velocities will introduce scatter around this line, but with a large enough sample the true function can be fit for and the values of the Oort constants measured, as shown in figure 2. formula_1 is simply the amplitude of the sinusoid and formula_2 is the vertical offset from zero. Measuring transverse velocities and distances accurately and without biases remains challenging, though, and sets of derived values for formula_1 and formula_2 frequently disagree.\n\nMost methods of measuring formula_1 and formula_2 are fundamentally similar, following the above patterns. The major differences usually lie in what sorts of objects are used and details of how distance or proper motion are measured. Oort, in his original 1927 paper deriving the constants, obtained formula_1 = 31.0 ± 3.7 km s kpc. He did not explicitly obtain a value for formula_2, but from his conclusion that the Galaxy was nearly in Keplerian rotation (as in example 2 below), we can presume he would have gotten a value of around -10 km s kpc. These differ significantly from modern values, which is indicative of the difficulty of measuring these constants. Measurements of formula_1 and formula_2 since that time have varied widely; in 1964 the IAU adopted formula_1 = 15 km s kpc and formula_2 = -10 km s kpc as standard values. Although more recent measurements continue to vary, they tend to lie near these values.\n\nThe Hipparcos satellite, launched in 1989, was the first space-based astrometric mission, and its precise measurements of parallax and proper motion have enabled much better measurements of the Oort constants. In 1997 Hipparcos data were used to derive the values formula_1 = 14.82 ± 0.84 km s kpc and formula_2 = -12.37 ± 0.64 km s kpc. The Gaia spacecraft, launched in 2013, is an updated successor to Hipparcos; which allowed new levels of accuracy in measuring four Oort constants formula_1 = 15.3 ± 0.4 km s kpc, formula_2 = -11.9 ± 0.4 km s kpc, formula_62 = -3.2 ± 0.4 km s kpc and formula_63 = -3.3 ± 0.6 km s kpc.\n\nWith the Gaia values, we find\nThis value of Ω corresponds to a period of 226 million years for the sun's present neighborhood to go around the Milky Way. However, the time it takes for the sun to go around the Milky Way (a galactic year) may be longer because (in a simple model) it is circulating around a point further from the centre of the galaxy where Ω is smaller (see Sun#Orbit in Milky Way).\n\nThe values in km s kpc can be converted into milliarcseconds per year by dividing by 4.740. This gives the following values for the average proper motion of stars in our neighborhood at different galactic longitudes, after correction for the effect due to the sun's velocity with respect to the local standard of rest:\n\nThe motion of the sun towards the solar apex in Hercules adds a generally westward component to the observed proper motions of stars around Vela or Centaurus and a generally eastward component for stars around Cygnus or Cassiopeia. This effect falls off with distance, so the values in the table are more representative for stars that are further away. On the other hand, more distant stars or objects will not follow the table, which is for objects in our neighborhood. For example, Sagittarius A*, the radio source at the centre of the galaxy, will have a proper motion of approximately Ω or 5.7 mas/y southwestward (with a small adjustment due to the sun's motion toward the solar apex) even though it is in Sagittarius. Note that these proper motions cannot be measured against \"background stars\" (because the background stars will have similar proper motions), but must be measured against more stationary references such as quasars.\n\nThe Oort constants can greatly enlighten one as to how the Galaxy rotates. As one can see formula_1 and formula_2 are both functions of the Sun's orbital velocity as well as the first derivative of the Sun's velocity. As a result, formula_1 describes the shearing motion in the disk surrounding the Sun, while formula_2 describes the angular momentum gradient in the solar neighborhood, also referred to as vorticity.\n\nTo illuminate this point, one can look at three examples that describe how stars and gas orbit within the Galaxy giving intuition as to the meaning of formula_1 and formula_2. These three examples are solid body rotation, Keplerian rotation and constant rotation over different annuli. These three types of rotation are plotted as a function of radius (formula_12), and are shown in Figure 3 as the green, blue and red curves respectively. The grey curve is approximately the rotation curve of the Milky Way.\n\nTo begin, let one assume that the rotation of the Milky Way can be described by solid body rotation, as shown by the green curve in Figure 3. Solid body rotation assumes that the entire system is moving as a rigid body with no differential rotation. This results in a constant angular velocity, formula_72, which is independent of formula_12. Following this we can see that velocity scales linearly with formula_12, formula_75, thus\nUsing the two Oort constant identities, one then can determine what the formula_1 and formula_2 constants would be,\n\nThis demonstrates that in solid body rotation, there is no shear motion, i.e. formula_80, and the vorticity is just the angular rotation, formula_81. This is what one would expect because there is no difference in orbital velocity as radius increases, thus no stress between the annuli. Also, in solid body rotation, the only rotation is about the center, so it is reasonable that the resulting vorticity in the system is described by the only rotation in the system. One can actually measure and find that is non-zero (formula_82 km s kpc.). Thus the galaxy does not rotate as a solid body in our local neighborhood, but may in the inner regions of the Galaxy.\n\nThe second illuminating example is to assume that the orbits in the local neighborhood follow a Keplerian orbit, as shown by the blue line in Figure 3. The orbital motion in a Keplerian orbit is described by,\nwhere formula_84 is the Gravitational Constant, and formula_85 is the mass enclosed within radius formula_86. The derivative of the velocity with respect to the radius is,\n\nThe Oort constants can then be written as follows,\n\nFor values of Solar velocity, formula_89 km/s, and radius to the Galactic center, formula_90 kpc, the Oort's constants are formula_91 km s kpc, and formula_92 km s kpc. However, the observed values are formula_82 km s kpc and formula_94 km s kpc. Thus, Keplerian rotation is not the best description the Milky Way rotation. Furthermore, although this example does not describe the local rotation, it can be thought of as the limiting case that describes the minimum velocity an object can have in a stable orbit.\n\nThe final example is to assume that the rotation curve of the Galaxy is flat, i.e. formula_95 is constant and independent of radius, formula_86. The rotation velocity is in between that of a solid body and of Keplerian rotation, and is the red dottedline in Figure 3. With a constant velocity, it follows that the radial derivative of formula_95 is 0, \nand therefore the Oort constants are,\n\nUsing the local velocity and radius given in the last example, one finds formula_100 km s kpc and formula_101 km s kpc. This is close to the actual measured Oort constants and tells us that the constant-speed model is the closest of these three to reality in the solar neighborhood. But in fact, as mentioned above, formula_102 is negative, meaning that at our distance, speed decreases with distance from the centre of the galaxy.\n\nWhat one should take away from these three examples, is that with a remarkably simple model, the rotation of the Milky Way can be described by these two constants. The first two examples are used as constraints to the Galactic rotation, for they show the fastest and slowest the Galaxy can rotate at a given radius. The flat rotation curve serves as an intermediate step between the two rotation curves, and in fact gives the most reasonable Oort constants as compared to current measurements.\n\nOne of the major uses of the Oort constants is to calibrate the galactic rotation curve. A relative curve can be derived from studying the motions of gas clouds in the Milky Way, but to calibrate the actual absolute speeds involved requires knowledge of V. We know that:\n\nSince R can be determined by other means (such as by carefully tracking the motions of stars near the Milky Way's central supermassive black hole), knowing formula_1 and formula_2 allows us to determine V.\n\nIt can also be shown that the mass density formula_106 can be given by:\n\nSo the Oort constants can tell us something about the mass density at a given radius in the disk. They are also useful to constrain mass distribution models for the Galaxy. As well, in the epicyclic approximation for nearly circular stellar orbits in a disk, the epicyclic frequency formula_108 is given by formula_109, where formula_72 is the angular velocity. Therefore, the Oort constants can tell us a great deal about motions in the galaxy.\n"}
{"id": "154242", "url": "https://en.wikipedia.org/wiki?curid=154242", "title": "PH meter", "text": "PH meter\n\nA pH meter is a scientific instrument that measures the hydrogen-ion activity in water-based solutions, indicating its acidity or alkalinity expressed as pH. The pH meter measures the difference in electrical potential between a pH electrode and a reference electrode, and so the pH meter is sometimes referred to as a \"potentiometric pH meter\". The difference in electrical potential relates to the acidity or pH of the solution. The pH meter is used in many applications ranging from laboratory experimentation to quality control.\nThe rate and outcome of chemical reactions taking place in water often depends on the acidity of the water, and it is therefore useful to know the acidity of the water, typically measured by means of a pH meter. Knowledge of pH is useful or critical in many situations, including chemical laboratory analyses. pH meters are used for soil measurements in agriculture, water quality for municipal water supplies, swimming pools, environmental remediation; brewing of wine or beer; manufacturing, healthcare and clinical applications such as blood chemistry; and many other applications.\n\nAdvances in the instrumentation and in detection have expanded the number of applications in which pH measurements can be conducted. The devices have been miniaturized, enabling direct measurement of pH inside of living cells. In addition to measuring the pH of liquids, specially designed electrodes are available to measure the pH of semi-solid substances, such as foods. These have tips suitable for piercing semi-solids, have electrode materials compatible with ingredients in food, and are resistant to clogging.\n\nPotentiometric pH meters measure the voltage between two electrodes and display the result converted into the corresponding pH value. They comprise a simple electronic amplifier and a pair of electrodes, or alternatively a combination electrode, and some form of display calibrated in pH units. It usually has a glass electrode and a reference electrode, or a combination electrode. The electrodes, or probes, are inserted into the solution to be tested.\n\nThe design of the electrodes is the key part: These are rod-like structures usually made of glass, with a bulb containing the sensor at the bottom. The glass electrode for measuring the pH has a glass bulb specifically designed to be selective to hydrogen-ion concentration. On immersion in the solution to be tested, hydrogen ions in the test solution exchange for other positively charged ions on the glass bulb, creating an electrochemical potential across the bulb. The electronic amplifier detects the difference in electrical potential between the two electrodes generated in the measurement and converts the potential difference to pH units. The magnitude of the electrochemical potential across the glass bulb is linearly related to the pH according to the Nernst equation.\n\nThe reference electrode is insensitive to the pH of the solution, being composed of a metallic conductor, which connects to the display. This conductor is immersed in an electrolyte solution, typically potassium chloride, which comes into contact with the test solution through a porous ceramic membrane. The display consists of a voltmeter, which displays voltage in units of pH.\n\nOn immersion of the glass electrode and the reference electrode in the test solution, an electrical circuit is completed, in which there is a potential difference created and detected by the voltmeter. The circuit can be thought of as going from the conductive element of the reference electrode to the surrounding potassium-chloride solution, through the ceramic membrane to the test solution, the hydrogen-ion-selective glass of the glass electrode, to the solution inside the glass electrode, to the silver of the glass electrode, and finally the voltmeter of the display device. The voltage varies from test solution to test solution depending on the potential difference created by the difference in hydrogen-ion concentrations on each side of the glass membrane between the test solution and the solution inside the glass electrode. All other potential differences in the circuit do not vary with pH and are corrected for by means of the calibration.\n\nFor simplicity, many pH meters use a combination probe, constructed with the glass electrode and the reference electrode contained within a single probe. A detailed description of combination electrodes is given in the article on glass electrodes.\n\nThe pH meter is calibrated with solutions of known pH, typically before each use, to ensure accuracy of measurement. To measure the pH of a solution, the electrodes are used as probes, which are dipped into the test solutions and held there sufficiently long for the hydrogen ions in the test solution to equilibrate with the ions on the surface of the bulb on the glass electrode. This equilibration provides a stable pH measurement.\n\nDetails of the fabrication and resulting microstructure of the glass membrane of the pH electrode are maintained as trade secrets by the manufacturers. However, certain aspects of design are published. Glass is a solid electrolyte, for which alkali-metal ions can carry current. The pH-sensitive glass membrane is generally spherical to simplify manufacture of a uniform membrane. These membranes are up to 0.4 millimeters in thickness, thicker than original designs, so as to render the probes durable. The glass has silicate chemical functionality on its surface, which provides binding sites for alkali-metal ions and hydrogen ions from the solutions. This provides an ion-exchange capacity in the range of 10 to 10 mol/cm. Selectivity for hydrogen ions (H) arises from a balance of ionic charge, volume requirements versus other ions, and the coordination number of other ions. Electrode manufacturers have developed compositions that suitably balance these factors, most notably lithium glass.\n\nThe silver chloride electrode is most commonly used as a reference electrode in pH meters, although some designs use the saturated calomel electrode. The silver chloride electrode is simple to manufacture and provides high reproducibility. The reference electrode usually consists of a platinum wire that has contact with a silver / silver chloride mixture, which is immersed in a potassium chloride solution. There is a ceramic plug, which serves as a contact to the test solution, providing low resistance while preventing mixing of the two solutions.\n\nWith these electrode designs, the voltmeter is detecting potential differences of ±1400 millivolts. The electrodes are further designed to rapidly equilibrate with test solutions to facilitate ease of use. The equilibration times are typically less than one second, although equilibration times increase as the electrodes age.\n\nBecause of the sensitivity of the electrodes to contaminants, cleanliness of the probes is essential for accuracy and precision. Probes are generally kept moist when not in use with a medium appropriate for the particular probe, which is typically an aqueous solution available from probe manufacturers. Probe manufacturers provide instructions for cleaning and maintaining their probe designs. For illustration, one maker of laboratory-grade pH gives cleaning instructions for specific contaminants: general cleaning (15-minute soak in a solution of bleach and detergent), salt (hydrochloric acid solution followed by sodium hydroxide and water), grease (detergent or methanol), clogged reference junction (KCl solution), protein deposits (pepsin and HCl, 1% solution), and air bubbles.\n\nThe German Institute for Standardization publishes a standard for pH measurement using pH meters, DIN 19263.\n\nVery precise measurements necessitate that the pH meter is calibrated before each measurement. More typically calibration is performed once per day of operation. Calibration is needed because the glass electrode does not give reproducible electrostatic potentials over longer periods of time.\n\nConsistent with principles of good laboratory practice, calibration is performed with at least two standard buffer solutions that span the range of pH values to be measured. For general purposes, buffers at pH 4.00 and pH 10.00 are suitable. The pH meter has one calibration control to set the meter reading equal to the value of the first standard buffer and a second control to adjust the meter reading to the value of the second buffer. A third control allows the temperature to be set. Standard buffer sachets, available from a variety of suppliers, usually document the temperature dependence of the buffer control. More precise measurements sometimes require calibration at three different pH values. Some pH meters provide built-in temperature-coefficient correction, with temperature thermocouples in the electrode probes. The calibration process correlates the voltage produced by the probe (approximately 0.06 volts per pH unit) with the pH scale. Good laboratory practice dictates that, after each measurement, the probes are rinsed with distilled water or deionized water to remove any traces of the solution being measured, blotted with a scientific wipe to absorb any remaining water, which could dilute the sample and thus alter the reading, and then immersed in a storage solution suitable for the particular probe type.\n\npH meters range from simple and inexpensive pen-like devices to complex and expensive laboratory instruments with computer interfaces and several inputs for indicator and temperature measurements to be entered to adjust for the variation in pH caused by temperature. The output can be digital or analog, and the devices can be battery-powered or rely on line power. Some versions use telemetry to connect the electrodes to the voltmeter display device.\n\nSpecialty meters and probes are available for use in special applications, such as harsh environments and biological microenvironments. There are also holographic pH sensors, which allow pH measurement colorimetrically, making use of the variety of pH indicators that are available. Additionally, there are commercially available pH meters based on solid state electrodes, rather than conventional glass electrodes.\n\nThe concept of pH was defined in 1909 by S. P. L. Sørensen, and electrodes were used for pH measurement in the 1920s.\n\nIn October 1934 Arnold Orville Beckman registered the first patent for a complete chemical instrument for the measurement of pH, U.S. Patent No. 2,058,761, for his \"acidimeter\", later renamed the pH meter. Beckman developed the prototype as an assistant professor of chemistry at the California Institute of Technology, when asked to devise a quick and accurate method for measuring the acidity of lemon juice for the California Fruit Growers Exchange (Sunkist). On April 8, 1935, Beckman's renamed National Technical Laboratories focused on the manufacture of scientific instruments, with the Arthur H. Thomas Company as a distributor for its pH meter. In its first full year of sales, 1936, the company sold 444 pH meters for $60,000 in sales. In years to come, the company sold millions of the units. In 2004 the Beckman pH meter was designated an ACS National Historic Chemical Landmark in recognition of its significance as the first commercially successful electronic pH meter.\n\nThe Radiometer Corporation of Denmark was founded in 1935, and began marketing a pH meter for medical use around 1936, but \"the development of automatic pH-meters for industrial purposes was neglected. Instead American instrument makers successfully developed industrial pH-meters with a wide variety of applications, such as in breweries, paper works, alum works, and water treatment systems.\"\n\nIn the 1970s Jenco Electronics of Taiwan designed and manufactured the first portable digital pH meter. This meter was sold under the label of the Cole-Parmer Corporation.\n\nSpecialized manufacturing is required for the electrodes, and details of their design and construction are typically trade secrets. However, with purchase of suitable electrodes, a standard multimeter can be used to complete the construction of the pH meter. However, commercial suppliers offer voltmeter displays that simplify use, including calibration and temperature compensation.\n\n\n"}
{"id": "663904", "url": "https://en.wikipedia.org/wiki?curid=663904", "title": "Pragmatic theory of truth", "text": "Pragmatic theory of truth\n\nA pragmatic theory of truth is a theory of truth within the philosophies of pragmatism and pragmaticism. Pragmatic theories of truth were first posited by Charles Sanders Peirce, William James, and John Dewey. The common features of these theories are a reliance on the \"pragmatic maxim\" as a means of clarifying the meanings of difficult concepts such as \"truth\"; and an emphasis on the fact that \"belief\", \"certainty\", \"knowledge\", or \"truth\" is the result of an inquiry.\n\nPragmatic theories of truth developed from the earlier ideas of ancient philosophy, the Scholastics, and Immanuel Kant. Pragmatic ideas about truth are often confused with the quite distinct notions of \"logic and inquiry\", \"judging what is true\", and \"truth predicates\".\n\nIn one classical formulation, truth is defined as the good of logic, where logic is a normative science, that is, an inquiry into a \"good\" or a \"value\" that seeks knowledge of it and the means to achieve it. In this view, truth cannot be discussed to much effect outside the context of inquiry, knowledge, and logic, all very broadly considered.\n\nMost inquiries into the character of truth begin with a notion of an informative, meaningful, or significant element, the truth of whose information, meaning, or significance may be put into question and needs to be evaluated. Depending on the context, this element might be called an \"artefact\", \"expression\", \"image\", \"impression\", \"lyric\", \"mark\", \"performance\", \"picture\", \"sentence\", \"sign\", \"string\", \"symbol\", \"text\", \"thought\", \"token\", \"utterance\", \"word\", \"work\", and so on. Whatever the case, one has the task of judging whether the bearers of information, meaning, or significance are indeed truth-bearers. This judgment is typically expressed in the form of a specific \"truth predicate\", whose positive application to a sign, or so on, asserts that the sign is true.\n\nConsidered within the broadest horizon, there is little reason to imagine that the process of judging a \"work\", that leads to a predication of false or true, is necessarily amenable to formalization, and it may always remain what is commonly called a \"judgment call\". But there are indeed many well-circumscribed domains where it is useful to consider disciplined forms of evaluation, and the observation of these limits allows for the institution of what is called a \"method\" of judging truth and falsity.\n\nOne of the first questions that can be asked in this setting is about the relationship between the significant performance and its reflective critique. If one expresses oneself in a particular fashion, and someone says \"that's true\", is there anything useful at all that can be said in general terms about the relationship between these two acts? For instance, does the critique add value to the expression criticized, does it say something significant in its own right, or is it just an insubstantial echo of the original sign?\n\nTheories of truth may be described according to several dimensions of description that affect the character of the predicate \"true\". The truth predicates that are used in different theories may be classified by the number of things that have to be mentioned in order to assess the truth of a sign, counting the sign itself as the first thing.\n\nIn formal logic, this number is called the \"arity\" of the predicate. The kinds of truth predicates may then be subdivided according to any number of more specific characters that various theorists recognize as important.\n\n\nSeveral qualifications must be kept in mind with respect to any such radically simple scheme of classification, as real practice seldom presents any pure types, and there are settings in which it is useful to speak of a theory of truth that is \"almost\" \"k\"-adic, or that \"would be\" \"k\"-adic if certain details can be abstracted away and neglected in a particular context of discussion. That said, given the generic division of truth predicates according to their arity, further species can be differentiated within each genus according to a number of more refined features.\n\nThe truth predicate of interest in a typical correspondence theory of truth tells of a relation between representations and objective states of affairs, and is therefore expressed, for the most part, by a dyadic predicate. In general terms, one says that a representation is \"true of\" an objective situation, more briefly, that a sign is true of an object. The nature of the correspondence may vary from theory to theory in this family. The correspondence can be fairly arbitrary or it can take on the character of an \"analogy\", an \"icon\", or a \"morphism\", whereby a representation is rendered true of its object by the existence of corresponding elements and a similar structure.\n\nVery little in Peirce's thought can be understood in its proper light without understanding that he thinks all thoughts are signs, and thus, according to his theory of thought, no thought is understandable outside the context of a sign relation. Sign relations taken collectively are the subject matter of a theory of signs. So Peirce's \"\", his theory of sign relations, is key to understanding his entire philosophy of pragmatic thinking and thought.\n\nIn his contribution to the article \"Truth and Falsity and Error\" for Baldwin's \"Dictionary of Philosophy and Psychology\" (1901), Peirce defines truth in the following way:\nTruth is that concordance of an abstract statement with the ideal limit towards which endless investigation would tend to bring scientific belief, which concordance the abstract statement may possess by virtue of the confession of its inaccuracy and one-sidedness, and this confession is an essential ingredient of truth. (Peirce 1901, see \"Collected Papers\" (CP) 5.565).\nThis statement emphasizes Peirce's view that ideas of approximation, incompleteness, and partiality, what he describes elsewhere as \"fallibilism\" and \"reference to the future\", are essential to a proper conception of truth. Although Peirce occasionally uses words like \"concordance\" and \"correspondence\" to describe one aspect of the pragmatic sign relation, he is also quite explicit in saying that definitions of truth based on mere correspondence are no more than \"nominal\" definitions, which he follows long tradition in relegating to a lower status than \"real\" definitions.\nThat truth is the correspondence of a representation with its object is, as Kant says, merely the nominal definition of it. Truth belongs exclusively to propositions. A proposition has a subject (or set of subjects) and a predicate. The subject is a sign; the predicate is a sign; and the proposition is a sign that the predicate is a sign of that of which the subject is a sign. If it be so, it is true. But what does this correspondence or reference of the sign, to its object, consist in? (Peirce 1906, CP 5.553).\nHere Peirce makes a statement that is decisive for understanding the relationship between his pragmatic definition of truth and any theory of truth that leaves it solely and simply a matter of representations corresponding with their objects. Peirce, like Kant before him, recognizes Aristotle's distinction between a \"nominal definition\", a definition in name only, and a \"real definition\", one that states the function of the concept, the reason for conceiving it, and so indicates the \"essence\", the underlying \"substance\" of its object. This tells us the sense in which Peirce entertained a \"correspondence theory of truth\", namely, a purely nominal sense. To get beneath the superficiality of the nominal definition it is necessary to analyze the notion of correspondence in greater depth.\n\nIn preparing for this task, Peirce makes use of an allegorical story, omitted here, the moral of which is that there is no use seeking a conception of truth that we cannot conceive ourselves being able to capture in a humanly conceivable concept. So we might as well proceed on the assumption that we have a real hope of comprehending the answer, of being able to \"handle the truth\" when the time comes. Bearing that in mind, the problem of defining truth reduces to the following form:\nNow thought is of the nature of a sign. In that case, then, if we can find out the right method of thinking and can follow it out — the right method of transforming signs — then truth can be nothing more nor less than the last result to which the following out of this method would ultimately carry us. In that case, that to which the representation should conform, is itself something in the nature of a representation, or sign — something noumenal, intelligible, conceivable, and utterly unlike a thing-in-itself. (Peirce 1906, CP 5.553).\nPeirce's theory of truth depends on two other, intimately related subject matters, his theory of \"sign relations\" and his theory of \"inquiry\". Inquiry is a special case of \"semiosis\", a process that transforms signs into signs while maintaining a specific relationship to an object, which object may be located outside the trajectory of signs or else be found at the end of it. Inquiry includes all forms of belief revision and logical inference, including \"scientific method\", what Peirce here means by \"the right method of transforming signs\". A sign-to-sign transaction relating to an object is a transaction that involves three parties, or a relation that involves three roles. This is called a \"ternary or triadic relation\" in logic. Consequently, pragmatic theories of truth are largely expressed in terms of triadic truth predicates.\n\nThe statement above tells us one more thing: Peirce, having started out in accord with Kant, is here giving notice that he is parting ways with the Kantian idea that the ultimate object of a representation is an unknowable \"thing-in-itself\". Peirce would say that the object is knowable, in fact, it is known in the form of its representation, however imperfectly or partially.\n\n\"Reality\" and \"truth\" are coordinate concepts in pragmatic thinking, each being defined in relation to the other, and both together as they participate in the time evolution of inquiry. Inquiry is not a disembodied process, nor the occupation of a singular individual, but the common life of an unbounded community.\nThe real, then, is that which, sooner or later, information and reasoning would finally result in, and which is therefore independent of the vagaries of me and you. Thus, the very origin of the conception of reality shows that this conception essentially involves the notion of a COMMUNITY, without definite limits, and capable of a definite increase of knowledge. (Peirce 1868, CP 5.311).\n\nDifferent minds may set out with the most antagonistic views, but the progress of investigation carries them by a force outside of themselves to one and the same conclusion. This activity of thought by which we are carried, not where we wish, but to a foreordained goal, is like the operation of destiny. No modification of the point of view taken, no selection of other facts for study, no natural bent of mind even, can enable a man to escape the predestinate opinion. This great law is embodied in the conception of truth and reality. The opinion which is fated to be ultimately agreed to by all who investigate, is what we mean by the truth, and the object represented in this opinion is the real. That is the way I would explain reality. (Peirce 1878, CP 5.407).\nWilliam James's version of the pragmatic theory is often summarized by his statement that \"the 'true' is only the expedient in our way of thinking, just as the 'right' is only the expedient in our way of behaving.\" By this, James meant that truth is a quality the value of which is confirmed by its effectiveness when applying concepts to actual practice (thus, \"pragmatic\"). James's pragmatic theory is a synthesis of correspondence theory of truth and coherence theory of truth, with an added dimension. Truth is verifiable to the extent that thoughts and statements correspond with actual things, as well as \"hangs together,\" or coheres, fits as pieces of a puzzle might fit together, and these are in turn verified by the observed results of the application of an idea to actual practice. James said that \"all true processes must lead to the face of directly verifying sensible experiences somewhere.\" He also extended his pragmatic theory well beyond the scope of scientific verifiability, and even into the realm of the mystical: \"On pragmatic principles, if the hypothesis of God works satisfactorily in the widest sense of the word, then it is 'true.' \"\n\"Truth, as any dictionary will tell you, is a property of certain of our ideas. It means their 'agreement', as falsity means their disagreement, with 'reality'. Pragmatists and intellectualists both accept this definition as a matter of course. They begin to quarrel only after the question is raised as to what may precisely be meant by the term 'agreement', and what by the term 'reality', when reality is taken as something for our ideas to agree with.\"\nWilliam James (1907) begins his chapter on \"Pragmatism's Conception of Truth\" in much the same letter and spirit as the above selection from Peirce (1906), noting the nominal definition of truth as a plausible point of departure, but immediately observing that the pragmatist's quest for the meaning of truth can only begin, not end there.\n\"The popular notion is that a true idea must copy its reality. Like other popular views, this one follows the analogy of the most usual experience. Our true ideas of sensible things do indeed copy them. Shut your eyes and think of yonder clock on the wall, and you get just such a true picture or copy of its dial. But your idea of its 'works' (unless you are a clockmaker) is much less of a copy, yet it passes muster, for it in no way clashes with reality. Even though it should shrink to the mere word 'works', that word still serves you truly; and when you speak of the 'time-keeping function' of the clock, or of its spring's 'elasticity', it is hard to see exactly what your ideas can copy.\"\nJames exhibits a knack for popular expression that Peirce seldom sought, and here his analysis of correspondence by way of a simple thought experiment cuts right to the quick of the first major question to ask about it, namely: To what extent is the notion of correspondence involved in truth covered by the ideas of analogues, copies, or iconic images of the thing represented? The answer is that the iconic aspect of correspondence can be taken literally only in regard to sensory experiences of the more precisely eidetic sort. When it comes to the kind of correspondence that might be said to exist between a symbol, a word like \"works\", and its object, the springs and catches of the clock on the wall, then the pragmatist recognizes that a more than nominal account of the matter still has a lot more explaining to do.\n\nInstead of truth being ready-made for us, James asserts we and reality jointly \"make\" truth. This idea has two senses: (1) truth is mutable, (often attributed to William James and F.C.S. Schiller); and (2) truth is relative to a conceptual scheme (more widely accepted in Pragmatism).\n\n(1) Mutability of truth\n\n\"Truth\" is not readily defined in Pragmatism. Can beliefs pass from being \"true\" to being \"untrue\" and back? For James, beliefs are not true until they have been made true by verification. James believed propositions become true over the long term through proving their utility in a person's specific situation. The opposite of this process is not falsification, but rather the belief ceases to be a \"live option.\" F.C.S. Schiller, on the other hand, clearly asserted beliefs could pass into and out of truth on a situational basis. Schiller held that truth was relative to specific problems. If I want to know how to return home safely, the true answer will be whatever is useful to solving that problem. Later on, when faced with a different problem, what I came to believe with the earlier problem may now be false. As my problems change, and as the most useful way to solve a problem shifts, so does the property of truth.\n\nC.S. Peirce considered the idea that beliefs are true at one time but false at another (or true for one person but false for another) to be one of the \"seeds of death\" by which James allowed his pragmatism to become \"infected.\" For Peirce the pragmatic view implies theoretical claims should be tied to verification processes (i.e. they should be subject to test). They shouldn't be tied to our specific problems or life needs. Truth is defined, for Peirce, as what \"would\" be the ultimate outcome (not any outcome in real time) of inquiry by a (usually scientific) community of investigators. William James, while agreeing with this definition, also characterized truthfulness as a species of the good: if something is true it is trustworthy and reliable and will remain so in every conceivable situation. Both Peirce and Dewey connect the definitions of truth and warranted assertability. Hilary Putnam also developed his internal realism around the idea a belief is true if it is ideally justified in epistemic terms. About James' and Schiller's view, Putnam says:\n\nRorty has also weighed in against James and Schiller:\n\n(2) Conceptual relativity\n\nWith James and Schiller we make things true by verifying them—a view rejected by most pragmatists. However, nearly all pragmatists do accept the idea there can be no truths without a conceptual scheme to express those truths. That is,\n\nF.C.S. Schiller used the analogy of a chair to make clear what he meant by the phrase that truth is made: just as a carpenter \"makes\" a chair out of existing materials and doesn't \"create\" it out of nothing, truth is a transformation of our experience—but this doesn't imply reality is something we're free to construct or imagine as we please.\n\nJohn Dewey, less broadly than William James but much more broadly than Charles Peirce, held that inquiry, whether scientific, technical, sociological, philosophical or cultural, is self-corrective over time \"if\" openly submitted for testing by a community of inquirers in order to clarify, justify, refine and/or refute proposed truths. In his \"Logic: The Theory of Inquiry\" (1938), Dewey gave the following definition of inquiry:\nInquiry is the controlled or directed transformation of an indeterminate situation into one that is so determinate in its constituent distinctions and relations as to convert the elements of the original situation into a unified whole. (Dewey, p. 108).\nThe index of the same book has exactly one entry under the heading \"truth\", and it refers to the following footnote:\nThe best definition of \"truth\" from the logical standpoint which is known to me is that by Peirce: \"The opinion which is fated to be ultimately agreed to by all who investigate is what we mean by the truth, and the object represented in this opinion is the real [CP 5.407]. (Dewey, 343 \"n\").\nDewey says more of what he understands by \"truth\" in terms of his preferred concept of \"warranted assertibility\" as the end-in-view and conclusion of inquiry (Dewey, 14–15).\n\nSeveral objections are commonly made to pragmatist account of truth, of either sort.\n\nFirst, due originally to Bertrand Russell (1907) in a discussion of James's theory, is that pragmatism mixes up the notion of truth with \"epistemology\". Pragmatism describes an \"indicator\" or a \"sign\" of truth. It really cannot be regarded as a theory of the \"meaning\" of the word \"true\". There's a difference between \"stating an indicator\" and \"giving the meaning\". For example, when the streetlights turn on at the end of a day, that's an indicator, a sign, that evening is coming on. It would be an obvious mistake to say that the word \"evening\" just means \"the time that the streetlights turn on\". In the same way, while it might be an \"indicator\" of truth, that a proposition is part of that perfect science at the ideal limit of inquiry, that just isn't what \"true\" \"means\".\n\nRussell's objection is that pragmatism mixes up an \"indicator\" of truth with the \"meaning\" of the predicate 'true'. There is a difference between the two and pragmatism confuses them. In this pragmatism is akin to Berkeley's view that to be is to be perceived, which similarly confuses an indication or proof of that something exists with the meaning of the word 'exists', or with what it is for something to exist.\n\nOther objections to pragmatism include how we define what it means to say a belief \"works\", or that it is \"useful to believe\". The vague usage of these terms, first popularized by James, has led to much debate.\n\nA final objection is that pragmatism of James's variety entails relativism. What is useful for \"you\" to believe might not be useful for \"me\" to believe. It follows that \"truth\" for you is different from \"truth\" for me (and that the relevant facts don't matter). This is relativism.\n\nA viable, more sophisticated consensus theory of truth, a mixture of Peircean theory with speech-act theory and social theory, is that presented and defended by Jürgen Habermas, which sets out the universal pragmatic conditions of ideal consensus and responds to many objections to earlier versions of a pragmatic, consensus theory of truth. Habermas distinguishes explicitly between factual consensus, i.e. the beliefs that happen to hold in a particular community, and rational consensus, i.e. consensus attained in conditions approximating an \"ideal speech situation\", in which inquirers or members of a community suspend or bracket prevailing beliefs and engage in rational discourse aimed at truth and governed by the force of the better argument, under conditions in which all participants in discourse have equal opportunities to engage in constative (assertions of fact), normative, and expressive speech acts, and in which discourse is not distorted by the intervention of power or the internalization of systematic blocks to communication.\n\nRecent Peirceans, Cheryl Misak, and Robert B. Talisse have attempted to formulate Peirce's theory of truth in a way that improves on Habermas and provides an epistemological conception of deliberative democracy.\n\n"}
{"id": "24112931", "url": "https://en.wikipedia.org/wiki?curid=24112931", "title": "Proprietary community", "text": "Proprietary community\n\nA community is distinguished from a loose group of individuals by an integrative system of organization that establishes both individual, private resources and common, public resources, and that organizes the activities required for the community’s continuity.\n\nA proprietary community is a special type of community in which a single owner leases units to multiple tenants.\n\nIn \"The Art of Community\", anthropologist Spencer MacCallum defines community as follows:\n\n\"A community is an occupation by two or more persons of a place divided into private and common areas according to a system of relations which defines and allocates responsibility for the performance of all activities that might be required for its continuity.\" (p. 3)\n\n\"A proprietary community is a community administered as a proprietary enterprise in which the relations of every member of the community are formed directly with the proprietary authority.\" (p. 5)\n\nProprietary communities are thus distinguished from other types of community such as private communities, voluntary communities, and intentional communities by the fact that none of these latter types of community are necessarily organized on a proprietary basis. For example, residential communes, Amish communities, and Israeli kibbutzim are voluntary, but not proprietary. Importantly, proprietary communities are also distinguished from private communities such as home owners' associations, which operate on political principles (democratic voting by the multiple owners), not on proprietary principles (which require a single owner who leases units to multiple tenants). Examples of proprietary communities include hotels, marinas, office buildings, industrial parks, entertainment complexes, and ever-larger and more complex combinations of these.\n\nIn \"The Art of Community\" and other works MacCallum argues that the property relations in a community fundamentally determine the physical structure and dynamics of the community. He writes that proprietary leasehold communities provide an optimal incentive system for communities by internalizing externalities and solving many of the coordination and cooperation problems that plague contemporary societies.\n\nThe first truly proprietary communities developed in the nineteenth century in the form of forerunners of the modern customer-service-oriented hotel. The twentieth century saw a veritable explosion of proprietary community types, and of ever-increasing generality and scale (for example, Disney World, The Venetian in Las Vegas, and Masdar City in the UAE). However, to date these proprietary communities have only existed within the framework of a larger nation-state—not as fully generalized independent sovereign jurisdictions. Since the 1960s, there have been various attempts to create such sovereign proprietary communities—unsuccessfully to date. A new initiative to develop experimental new societies is being spearheaded by the Seasteading Institute, founded by Patri Friedman (grandson of economist Milton Friedman), and funded by PayPal founder Peter Thiel.\n\n\n\n\n\n\n"}
{"id": "5963176", "url": "https://en.wikipedia.org/wiki?curid=5963176", "title": "Pyotr Gorlov", "text": "Pyotr Gorlov\n\nPyotr Nikolayevich Gorlov (; 11 May 1839 in Irkutsk, Irkutsk Governorate, Russian Empire – 20 November 1915) was a geologist and engineer who explored coal deposits in the Donets Basin, the Caucasus, Central Asia and Ussuri Krai. He founded the city of Horlivka; the city has a monument in honor of him.\n\n"}
{"id": "20572438", "url": "https://en.wikipedia.org/wiki?curid=20572438", "title": "Ricardo Miledi", "text": "Ricardo Miledi\n\nRicardo Miledi (15 September, 1927 – 18 December, 2017) was a Mexican neuroscientist.\n\nOne of seven children, many of whom settled in the US, Miledi embodied the essence of a global scientist working with the brightest and the best from across the world. Research and discovery was as paramount to him as was bringing forward the next generation of scientists. Miledi devoted time continuously over his career to enabling PhD students to succeed and adopt a discripline of inquiry and ingenuity. He was forever approachable and had a wonderfully engaging personality that belied his global status.\n\nMost recently he was a Distinguished Professor at University of California, Irvine having joined the faculty in the early 1980s where he was known for work on the role of calcium in neurotransmitter release. He received undergraduate and medical degrees at National Autonomous University of Mexico. While in medical school, he decided that he would make a \"terrible clinician\", as \"he imagined that he would end up seeing only one patient per week, because he would always be too interested in every unknown detail of the case, trying to work out how medicines might act.\" As a result, when required to perform social service as a component of his training in medical school, he chose a research fellowship at the Instituto Nacional de Cardiología under Arturo Rosenblueth. There, he studied the electrical origins of ventricular fibrillation and became skilled at delicate laboratory work.\n\nIn 1955, he spent a summer at the Marine Biological Laboratory at Wood's Hole. There, he began his study of synapses in the common squid and began to see the importance of calcium in synaptic transmission. Around 1956/1957 Miledi conducted research in Canberra, Australia.\n\nIn 1958, he met frequent collaborator Noble Laureate Bernard Katz, who offered him a position in the Department of Biophysics at University College London. There, he studied the release of Acetylcholine(ACh) and the expression of its receptors. In the early 1960s, he again became interested in the role of calcium. He found that \"in zero-Ca medium, the nerve impulse still fully invades the nerve terminal, but does not release any neurotransmitter. And then as soon as you give a little Ca , you get neurotransmitter release.\" He and Katz published a paper establishing the major role of Ca in ACh release. Further work with squid contributed to an even better understanding of the role of Ca in neurotransmitter release.\n\nMiledi was elected as a fellow to the British Royal Society in 1970.\n\nDuring the early 1970s Miledi was a frequent research scientist during the summer months at the Stazione Zooalogica in Naples, Italy ostensibly as the local squid made excellent research specimen.\n\nHis awards include the Royal Medal (1998), the Prince of Asturias Award (1999), and the Society for Neuroscience's Ralph W. Gerard Prize for outstanding contributions to the field (2010).\n"}
{"id": "8679999", "url": "https://en.wikipedia.org/wiki?curid=8679999", "title": "Selective dissemination of information", "text": "Selective dissemination of information\n\nSelective dissemination of information (\"SDI\") was originally a phrase related to library and information science. SDI refers to tools and resources used to keep a user informed of new resources on specified topics. \n\nSDI services pre-date the world wide web, and the term itself is somewhat dated. Contemporary analogous systems for SDI services include alerts, current awareness tools or trackers. These systems provide automated searches that inform the user of the availability of new resources meeting the user's specified keywords and search parameters. Alerts can be received a number of ways, including email, RSS feeds, voice mail, Instant messaging, and text messaging. \n\nSelective dissemination of information was a concept first described by Hans Peter Luhn of IBM in the 1950s. Software was developed in many companies and in government to provide this service in the 1950s and 60s, which allowed distribution of items recently published in abstract journals to be routed to individuals who are likely to be interested in the contents. For example, the system at Ft. Monmouth automatically sent out (by mail) a different set of abstracts to each of about 1,000 scientists and engineers in the army depending on what they were working on. The selection was based on an \"interest profile,\" a list of keywords that described their interests. In some organizations, the 'interest profile' was much more than a simple list of keywords. Librarians or information professionals conducted extensive interviews with their clients to establish a fairly complex profile for each individual. Based on these profiles, the information professionals would then distribute selectively appropriate information to their clients. This labour-intensive operation, while initially costly, over time was made less so.\nA survey at the time (1970s) indicated that a large number of projects were affected by the SDI service. The software was developed by Edward Housman at the Signal Corps Research Laboratories Technical Information Division.\n\nHensley, C. B. 1963. Selective Dissemination of Information (SDI): state of the art in May, 1963. \"AFIPS '63, Proceedings of the May 21-23, 1963 Spring Joint Computer Conference\": 257-262. Accessed December 11, 2012, DOI: 10.1145/1461551.1461584\n\nConnor, J. H. 1967. Selective Dissemination of Information - review of literature and issues.\n\"The Library Quarterly\" 37 (4): 373-391. Accessed December 11, 2015, URL: https://www.jstor.org/stable/4305823.\n"}
{"id": "57642066", "url": "https://en.wikipedia.org/wiki?curid=57642066", "title": "Society for the Study of Human Development", "text": "Society for the Study of Human Development\n\nThe Society for the Study of Human Development (abbreviated SSHD) is a United States-based international learned society dedicated to interdisciplinary research on human development. It was founded in 1998 when a group of scholars met at the Radcliffe Institute for Advanced Study. Its first meeting was held in November 1999. The current president of the society is David Henry Feldman (Tufts University), and the president-elect is Carolyn Aldwin (Oregon State University). The society's official journal is \"Research in Human Development\".\nFormer presidents of the SSHD include:\n"}
{"id": "4165915", "url": "https://en.wikipedia.org/wiki?curid=4165915", "title": "Spectronic 20", "text": "Spectronic 20\n\nThe Spectronic 20 is a brand of single-beam spectrophotometer, designed to operate in the visible spectrum across a wavelength range of 340 nm to 950 nm, with a spectral bandpass of 20 nm. It is designed for quantitative absorption measurement at single wavelengths. Because it measures the transmittance or absorption of visible light through a solution, it is sometimes referred to as a colorimeter. The name of the instrument is a trademark of the manufacturer.\n\nDeveloped by Bausch & Lomb and launched in 1953, the Spectronic 20 was the first low-cost UV-Vis spectrophotometer. It rapidly became an industry standard due to its low cost, durability and ease of use, and has been referred to as an \"iconic lab spectrophotometer\". Approximately 600,000 units were sold over its nearly 60 year production run. It has been the most widely used spectrophotometer worldwide. Production was discontinued in 2011 when it was replaced by the Spectronic 200, but the Spectronic 20 is still in common use. It is sometimes referred to as the \"Spec 20\".\n\nThe Bausch & Lomb Spectronic 20 colorimeter uses a diffraction grating monochromator combined with a system for the detection, amplification, and measurement of light wavelengths in the 340 nm to 950 nm range.\n\nAs shown in the schematic optical diagram (see left), polychromatic light from a source in the system passes through lenses which are reflected and dispersed by the diffraction grating to restrict the range of light wavelengths. This restricted range of wavelengths is then passed through the sample to be measured. The intensity of the transmitted light is determined by a phototube detector. Mechanical movement of the diffraction grating by means of the cam attached to the wavelength control enables the user to select for various wavelengths. This is the \"λ knob\", wherein λ refers to wavelength of light used for the measurement.\n\nMany substances absorb light in the ultraviolet - visible light range. Absorption at any particular wavelength in the ultraviolet visible range is proportional to the concentration of the substances in the solution or other medium, in accord with the Beer-Lambert relationship. In a practical sense, the Beer-Lambert relationship can be stated as:\n\nin which \"A\" is the absorbance measured by the instrument, ε is the molar absorption coefficient of the sample, \"l\" is the pathlength of the light beam through the sample, and c is the concentration of the substance in the solution or medium. The Spectronic 20 is thereby commonly used for quantitative determination of the concentration of a substance of interest. The Spectronic 20 measures the absorbance of light at a pre-determined concentration, and the concentration is calculated from the Beer-Lambert relationship.\n\nThe absorbance of the light is the base 10 logarithm of the ratio of the Transmittance of the pure solvent to the transmittance of the sample, and so the two absorbance and transmittance can be interconverted. Either transmittance or absorbance can therefore be plotted versus concentration using measurements from the Spectronic 20. Plotting a curve using percent transmittance of light yields an exponential curve. However, absorbance is linearly related to concentration, and so absorbance is often preferred for plotting a standard curve. This type of standard curve relates the concentration of the solution (on the x-axis) to measures of its absorbance (y-axis).\n\nTo obtain such a curve, a series of dilutions of known concentration of a solution are prepared and readings are obtained for each of the dilutions (see plot at left). In this plot, the slope of the line is the product ε x \"l\". By measuring a series of standards and creating the standard curve, it is possible to quantify the amount or concentration of a substance within a sample by determining the absorbance on the Spec 20 and finding the corresponding concentration on the calibration curve. Alternatively, the logarithm of percent transmittance can be plotted versus concentration to create a standard curve using the same procedure.\n\nThe absorbance measured by the Spectronic 20 is the sum of the absorbance of each of the constituents of the solution. Therefore, the Spectronic 20 can be used to analyze more complex solutions. For example, if a sample solution has two light-absorbing compounds in it, then the user performs measurements at two different wavelengths and constructs standard curves for each compound. Then the concentration of each compound can be calculated algebraically.\n\nThe Spectronic 20 can be used for turbidimetric measurements. In microbiological work, the turbidity of a liquid culture of bacterial cells relates to the cell count, and OD600 measurements can be conducted for this purpose using the Spectronic 20. Likewise the turbidity of water suspensions of clays and other particles of size suitable for light scattering can be quantitatively determined by means of a Spectronic 20. In the past, the Spectronic 20 was used for clinical diagnostic purposes.\n\nBefore testing a sample, the Spectronic 20 is calibrated using a blank solution, which is the pure solvent that is used in the experimental sample. It is typically water or an organic solvent. In this calibration, the transmittance is set at 100% using the calibration knob of the instrument (the amplifier control knob in the figure at right). The instrument can also optionally be calibrated with a stock solution of a sample at a concentration known to have an absorbance of 2 or else vendor supplied standards, using the light absorption knob in the diagram shown at right. After calibration, the user places a 1/2 inch test tube or cuvette containing the sample solution to be measured into the sample compartment. Calibration is repeated each time the wavelength is changed. It or a standard reference sample is generally used to periodically check for drift. To measure wavelengths above 650 nm, the bottom of the instrument is opened, and a red filter and a red-sensitive photocell is installed.\n\nThe original design of the Spectronic 20 utilized an analog dial for readout of transmission from 100%T to 1%T (top scale), 0A - 2A (lower scale). Using the original instrument requires manual setting of the wavelength and making readings from a moving-needle analog display.\n\nThe Spectronic 20D (launched in 1985) and later the 20D+ replaced the analog dial with a red digital LED readout, offering greater precision in the readout, if not greater accuracy in the actual reading. A side-by-side comparison of the features of the 20+ and 20D+ is available in the 2001 operating manual.\n\nThe Spectronic 20 was replaced by the Spectronic 200 in the Thermo Scientific spectrophotometer product line in 2011. The Spectronic 200 utilizes an array detector and digital control of the measured wavelength, while retaining the characteristic λ knob of the Spec 20 for setting the wavelength. In addition to replicating the user modes of the Spec 20D+ (which it can emulate on a color LCD screen) the Spec 200 accommodates both test-tubes and square cuvettes without needing to install an adapter. Software modes described in the Spectronic 200's specifications include scanning, four wavelength simultaneous measurement, and quantitative analysis with up to four standards, in contrast to the SPEC 20D+ which offered only single point calibration.\n\nOriginally introduced by Bausch & Lomb in 1953, the product line was sold to Milton Roy in 1985. Milton Roy sold its instrument group to Life Sciences International, renamed Spectronic instruments, Inc. in 1995. Spectronics Instruments was purchased by Thermo Optek in 1997, renamed Spectronic-Unicam in 2001 and Thermo-Spectronic in 2002. In 2003 the product line was moved to Madison, WI and the brand renamed to Thermo Electron.\n\nWith the merger of Thermo Electron and Fisher Scientific in 2006 the brand changed to Thermo Scientific, and remained such until the end of the production run. Spectronic 20 instruments found in labs today may bear any of the Bausch and Lomb, Milton Roy, Spectronic, Thermo Electron or Thermo Scientific brand names.\n\nThe Spectronic 20 is apparently one of the few lab instruments to remain intact after the destruction of the laboratory in the movie \"Back to the Future\".\n\n"}
{"id": "32752335", "url": "https://en.wikipedia.org/wiki?curid=32752335", "title": "Sputnik 40", "text": "Sputnik 40\n\nSputnik 40 (, ), also known as Sputnik Jr, PS-2 and Radio Sputnik 17 (RS-17), was a Franco-Russian amateur radio satellite which was launched in 1997 to commemorate the fortieth anniversary of the launch of Sputnik 1, the world's first artificial satellite. A one-third scale model of Sputnik 1, Sputnik 40 was deployed from the Mir space station on 3 November 1997. Built by students, the spacecraft was constructed at the Polytechnic Laboratory of Nalchik in Kabardino-Balkaria, whilst its transmitter was assembled by Jules Reydellet College in Réunion with technical support from AMSAT-France.\n\nSputnik 40 was launched, along with a backup spacecraft and the X-Mir inspection satellite, aboard Progress M-36 at 15:08 UTC on 5 October 1997. A Soyuz-U carrier rocket placed the spacecraft into orbit, flying from 1/5 at the Baikonur Cosmodrome in Kazakhstan: the same launch pad used by Sputnik 1. Progress M-36 docked to Mir on 8 October, and the satellites were transferred to the space station. At 04:05 UTC on 3 November, during an extra-vehicular activity, Sputnik 40 was deployed by cosmonauts Anatoly Solovyev and Pavel Vinogradov.\n\nOn 4 November, the day after it was deployed, Sputnik 40 was in a low Earth orbit with a perigee of , an apogee of , an inclination of 51.6 degrees, and a period of 92.13 minutes. The satellite was given the International Designator 1997-058C, and was catalogued by the United States Space Command as 24958. It ceased operations on 29 December 1997 when its batteries expired, and subsequently decayed from orbit on 21 May 1998. The backup satellite remained aboard Mir, and was destroyed when Mir was deorbited on 23 March 2001.\n\n"}
{"id": "32078809", "url": "https://en.wikipedia.org/wiki?curid=32078809", "title": "Trophic species", "text": "Trophic species\n\nTrophic species are a group of organisms that are aggregated according to their common trophic (feeding) positions in a food web or food chain. Trophic species have identical prey and a shared set of predators in the food web. This means that members of a trophic species share many of the same kinds of ecological functions. The idea of trophic species was first devised by Joel Cohen and Frederick Briand in 1984 to redefine assessment of the ratio of predators to prey within a food web. The category may include species of plant, animal, a combination of plant and animal, and biological stages of an organism. The reassessment grouped similar species according to habit rather than genetics. This resulted in a ratio of predator to prey in food webs is generally 1:1. By assigning groups in a trophic manner, relationships are linear in scale. This allows for predicting the proportion of different trophic links in a community food web.\n"}
{"id": "42176435", "url": "https://en.wikipedia.org/wiki?curid=42176435", "title": "Visualizing Energy Resources Dynamically on the Earth", "text": "Visualizing Energy Resources Dynamically on the Earth\n\nVERDE (Visualizing Energy Resources Dynamically on the Earth) is a visualization and analysis capability of the United States Department of Energy (DOE). The system, developed and maintained by Oak Ridge National Laboratory (ORNL), provides wide-area situational understanding of the U.S. electric grid. Enabling grid monitoring, weather impacts prediction and analysis, VERDE supports preparedness and response to potentially large outage events. As a real-time geo-visualization capability, it characterizes the dynamic behavior of the grid over interconnects giving views into bulk transmission lines as well as county-level power distribution status. By correlating grid behaviors with cyber events, the platform also enables a method to link cyber-to-infrastructure dependencies.\n\nVERDE integrates different data elements from other available on-line services, databases, and social media. The Tennessee Valley Authority (TVA) and other major utilities spanning multiple regions across the electric grid interconnection provide real-time status of their systems. Social media sources such as Twitter provide additional data-sources for visualization and analyses.\n\nThe VERDE software, which was developed by the Computational Sciences and Engineering Division (CSED) of ORNL, is used outside of the DOE for a number of related national security requirements.\n\n\n\n"}
{"id": "54156721", "url": "https://en.wikipedia.org/wiki?curid=54156721", "title": "Walkaway (Doctorow novel)", "text": "Walkaway (Doctorow novel)\n\nWalkaway is an adult science fiction novel by Cory Doctorow, published by Head of Zeus and Tor Books in April 2017.\n\nSet in our near-future, it is a story of walking away from \"non-work\", and surveillance and control by a brutal, immensely rich oligarchical elite; love and romance; a post-scarcity gift economy; revolution and eventual war; and a means of finally ending death.\n\nIn a world of non-work, ruined by human-created climate change and pollution, and where people are under surveillance and ruled over by a mega-rich elite, Hubert, Etc, his friend Seth, and Natalie, decide that they have nothing to lose by turning their backs and walking away from the everyday world or \"default reality\".\n\nWith the advent of 3D printing – and especially the ability to use these to fabricate even better fabricators – and with machines that can search for and reprocess waste or discarded materials, they no longer have need of Default for the basic essentials of life, such as food, clothing and shelter.\n\nAs more and more people choose to \"walkaway\", the ruling elite do not take these social changes sitting down. They use the military, police and mercenaries to attack and disrupt the walkaways' new settlements.\n\nOne thing that the elite are especially interested in is scientific research that the walkaways are carrying out which could finally put an end to death – and all this leads to revolution and eventual war.\n\n\nWriting on \"NPR\" online, Jason Sheehan is of the opinion that Doctorow's writing is \"super weird in the best possible way.\"\n\nThe reviewer says that \"Walkaway\" is a remarkable \"story of a utopia in progress, as messy as every new thing ever is, told in the form of people talking to each other, arguing with each other and working together to solve problems. It's all about the deep, disturbing, recognizable weirdness of the future that must come from the present we have already made for ourselves, trying to figure out what went wrong and what comes next.\"\n\nIn \"The Verge\", Adi Robertson writes: \"\"Walkaway\" imagines a future shaped by the same problems and possibilities Doctorow's been playing with for years: the threat of ubiquitous surveillance and artificial scarcity, and the promise that almost any technology can be repurposed and turned against its creator\".\n\nRobertson goes on to say that \"one of Doctorow's core themes in \"Walkaway\" is subverting what he described as the popular 'man against man against nature' pulp plot.\"\n\nIn \"Ars Technica\", Sean Gallagher writes that \"the future is open source in this optimistic sci-fi disaster epic full of big ideas.\" He and Doctorow see \"Walkaway\" as a prequel to the author's debut novel \"Down and Out in the Magic Kingdom\".\n\nGallagher concludes that \"Walkaway\" \"shows us a world trying to make things right after having made all the wrong decisions about how to use technology. But Walkaway executes that move beautifully. And like all great performances, it's worth witnessing over and over again.\"\n\n"}
{"id": "49226157", "url": "https://en.wikipedia.org/wiki?curid=49226157", "title": "Weak gravity conjecture", "text": "Weak gravity conjecture\n\nThe weak gravity conjecture (WGC) is a conjecture regarding the strength gravity can have in a theory of quantum gravity relative to the gauge forces in that theory. It roughly states that gravity should be the weakest force in any consistent theory of quantum gravity.\n\n"}
