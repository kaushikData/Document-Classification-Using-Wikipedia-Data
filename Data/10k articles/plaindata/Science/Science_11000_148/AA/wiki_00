{"id": "20667382", "url": "https://en.wikipedia.org/wiki?curid=20667382", "title": "2nd parallel north", "text": "2nd parallel north\n\nThe 2nd parallel north is a circle of latitude that is 2 degrees north of the Earth's equatorial plane. It crosses the Atlantic Ocean, Africa, the Indian Ocean, Southeast Asia, the Pacific Ocean and South America.\n\nStarting at the Prime Meridian and heading eastwards, the parallel 2° north passes through:\n\n"}
{"id": "25877262", "url": "https://en.wikipedia.org/wiki?curid=25877262", "title": "Bara Shigri Glacier", "text": "Bara Shigri Glacier\n\nBara Shigri is the largest glacier located in the state of Himachal Pradesh, India, Bara-Sigri glacier which is the second longest glacier in Himalaya (Indian side) after Gangotri, both are around 30 km long. The glacier is located in the Chandra Valley of Lahaul. The glacier feeds the Chenab River.\nThe name comes from the Lahaul dialect, where Bara means big and Shigri means glacier.\n\nAccording to Hugh Whistler's 1924 writing, \"Shigri is applied par-excellence to one particular glacier that emerges from the mountains on the left bank of the Chenab. It is said to be several miles long, and the snout reaches right down to the river, lying athwart the customary road from Kullu to Spiti.\" Estimates differ as to the breadth of the glacier where it is crossed, as owing to its movement and roughness no two caravans cross it in exactly the same way, but it is not less than a mile wide. In 1836 this glacier dammed the Chenab River, causing the formation of a large lake, which eventually broke loose and carried devastation down the valley.\n\nThe Bara Shigri glacier attracted much attention for many years because of the valuable antimony deposits found there. The glacier was first surveyed in 1906 by H. Walker and E.H. Pascoe of the Geological Survey of India. In 1955, the Geological Survey of India sponsored an expedition to this glacier as part of the Indian programme for the International Geophysical Year 1956-57, when a number of Himalayan glaciers were examined and their snout position fixed.\n\nThe Bara Shigri glacier flows northwards and debouches into the Chenab River where its southerly course is deflected westwards, close to the Spiti border. The glacier's altitude is between and , and its length has been recently surveyed and mapped. The glacier is so heavily covered with surface moraine that ice is not visible for long stretches except along the crevices and in the ablation areas. Across the Bara Shigri is another glacier known as Chhota Shigri. It is a comparatively smaller glacier and does not reach down to the bed of the Chenab, but it is steep, slippery, and difficult to cross.\n"}
{"id": "5772401", "url": "https://en.wikipedia.org/wiki?curid=5772401", "title": "Big Joe (Project Mercury)", "text": "Big Joe (Project Mercury)\n\nBig Joe was a subprogram of America's Project Mercury space program. It launched a single flight using an Atlas launch vehicle with a boilerplate Mercury capsule. The purpose of the Big Joe program was to prove the ablative heat shield which would be needed for the re-entry of orbital Mercury missions. The flight, Big Joe 1, occurred on September 9, 1959.\n\nThe Big Joe name has been attributed to Maxime Faget at NASA's Langley Research Center in Hampton, Virginia. It was a progression of the smaller test booster Little Joe. Faget also coined the Little Joe name basing it on its four large fins which reminded him a roll of four in craps.\n\nThe official Mercury mission numbering designation was a two-letter designation which corresponded to the launch vehicle type, followed by a dash then a one- or two-character designation of the flight/test number. The official designation for Big Joe 1 was BJ-1.\n\nThe launch vehicle used for BJ-1 (628/10-D) had a slightly different numbering scheme than those used for the later Mercury-Atlas flights.\n\n"}
{"id": "28105864", "url": "https://en.wikipedia.org/wiki?curid=28105864", "title": "Biogeography of Deep-Water Chemosynthetic Ecosystems", "text": "Biogeography of Deep-Water Chemosynthetic Ecosystems\n\nThe Biogeography of Deep-Water Chemosynthetic Ecosystems is a field project of the Census of Marine Life programme (CoML). The main aim of ChEss is to determine the biogeography of deep-water chemosynthetic ecosystems at a global scale and to understand the processes driving these ecosystems. ChEss addresses the main questions of CoML on diversity, abundance and distribution of marine species, focusing on deep-water reducing environments such as hydrothermal vents, cold seeps, whale falls, sunken wood and areas of low oxygen that intersect with continental margins and seamounts.\n\nDeep-sea hydrothermal vents and their associated fauna were first discovered along the Galapagos Rift in the eastern Pacific in 1977. Vents are now known to occur along all active mid ocean ridges and back-arc spreading centres, from fast to ultra-slow spreading ridges. The interest in chemosynthetic environments was strengthened by the discovery of chemosynthetic-based fauna at cold seeps along the base of the Florida Escarpment in 1983. Cold seeps occur along active and passive continental margins. More recently, the study of chemosynthtetic fauna has extended to the communities that develop in other reducing habitats such as whale falls, sunken wood and areas of oxygen minima when they intersect with the margin or seamounts.\nSince the first discovery of hydrothermal vents, more than 600 species have been described from vents and seeps. This is equivalent of 1 new description every 2 weeks(!). As biologists, geochemists, and physicists combine research efforts in these systems, new species will certainly be discovered. Moreover, because of the extreme conditions of the vent and seep habitat, certain species may have specific physiological adaptations with interesting results for the biochemical and medical industry.\n\nThese globally distributed, ephemeral and insular habitats that support endemic faunas offer natural laboratories for studies on dispersal, isolation and evolution. Here, hydrographic and topographic controls on biodiversity and biogeography might be much more readily resolved than in systems where climate and human activity obscure their role. In addition, hydrothermal vents have been suggested to be the habitat of the origin of life. These hypotheses are being used by ChEss researchers in collaboration with NASA to develop programmes to search for life in planets or moons of the outer space.\n\nOnly a small fraction of the global ridge system (~65000 km) and of the vast continental margin regions have been explored and their communities described. It is the aim of ChEss to improve the knowledge on the diversity, abundance and distribution of species from vents, seeps and other reducing habitats at a global scale, understanding the abiotic and biotic processes that shape and maintain these ecosystems and their biogeography.\n\nMain ChEss Science Questions\n\n\"Objective 1. To create a centralised database\"\n\nTo create a centralised database, ChEssBase, of deep-water vent, cold seep, whalefall and OMZ species. ChEssBase is a web-based database that incorporates archived and newly collected biological material. The database is geo- and bio-referenced. ChEssBase is available online and has been integrated with OBIS.\n\n\"Objective 2. To develop a long-term field programme\"\n\nTo develop a long-term field programme to locate potential vent and seep sites and continue research on whalefalls and OMZ sites. The field programme aims to explain the main gaps in our knowledge of the diversity, abundance and distribution of chemosynthetic species globally. A limited number of target areas have been selected where specific scientific questions relevant to biogeographical issues will be answered.\n\nThe target areas have been grouped into two categories. Category I, combined areas: Area A: Equatorial Atlantic Belt region; Area B: the SE Pacific region; Area C: NZ region; Area D: the Arctic and Antarctic regions, within the International Polar Year. Category II, specific areas: 1 – The ice-covered Gakkel Ridge, 2 – the (ultra)-slow ridges of the Norwegian-Greenland Sea, 3 – the northern MAR between the Iceland and Azores hot-spots; 4 – the Brazilian continental margin, 5 – the East Scotia Ridge and Bransfield Strait, 6 – the SW Indian Ridge, 7 – the Central Indian Ridge.\nDuring the field programme, ChEss will promote the development and refinement of deep-towed, remotely operated (ROV) and autonomous underwater (AUV) vehicle technologies to locate, map and sample new chemosynthetic systems. Using optical, chemical and acoustic techniques, ChEss researchers hope to gain a better understanding of not only biogeographical patterns, but to determine the processes driving these ecosystems.\n\nDuring the field programme, ChEss will promote the development and refinement of deep-towed, remotely operated (ROV) and autonomous underwater (AUV) vehicle technologies to locate, map and sample new chemosynthetic systems. Using optical, chemical and acoustic techniques, ChEss researchers hope to gain a better understanding of not only biogeographical patterns, but to determine the processes driving these ecosystems.\n\n\"Objective 3: Outreach and Education\"\n\nChEss has multi-lingual education pages related to vents, seeps and whalefalls. There is a dedicated page for key outreach initiatives such as live cruise diaries, open days, schools activities etc.\n\nChEss has joined forces with the other deep-sea CoML projects and this has resulted in the creation of the DEep-Sea Education and Outreach group (DESEO) that has produced a book \"Deeper than Light\" published in 5 languages.\n\n"}
{"id": "24697657", "url": "https://en.wikipedia.org/wiki?curid=24697657", "title": "Bioresource engineering", "text": "Bioresource engineering\n\nBioresource engineering is similar to biological engineering, except that it is based on biological and/or agricultural feedstocks. Bioresource engineering is more general and encompasses a wider range of technologies and various elements such as biomass, biological waste treatment, bioenergy, biotransformations, bioresource systems analysis, bioremediation and technologies associated with Thermochemical conversion technologies such as combustion, pyrolysis, gasification, catalysis, etc.\n\nBioresource engineering also contains biochemical conversion technologies such as aerobic methods, anaerobic digestion, microbial growth processes, enzymatic methods, and composting. Products include fibre, fuels, feedstocks, fertilisers, building materials, polymers and other industrial products, and management products e.g. modelling, systems analysis, decisions, and support systems. \nBioresource engineering is a discipline that is usually very similar to environmental engineering.\n\nThe impact of urbanization and increasing demand for food, water and land presents bioresource engineers with the task of bridging the gap between the biological world and traditional engineering. Agricultural and bioresource engineers attempt to develop efficient and environmentally sensitive methods of producing food, fiber, timber, bio-based products and renewable energy sources for an ever-increasing world population. Some of the research in bioresource engineering include machine vision, vehicle modification, wastewater irrigation, irrigation water management, stormwater management, inside natural environment for animals and plants, sensors, non-point source pollution and animal manure management.\nA biosynthesis of silver nanoparticles (NPs) mediated by fungal proteins of Coriolus versicolor has been undertaken for the first time last year. Hydrogels have been used to separate As(V) from water.\n\nFounded in 1925, the ATCC (American Type Culture Collection) is a nonprofit and research organization, whose mission focuses on the acquisition, production, and development of standard reference microorganisms, cell lines and other materials for research in life sciences. ATCC has collected a wide range of biological items for research. Their holdings include molecular genomics tools, microorganisms and bioproducts.\n\n\n"}
{"id": "27900956", "url": "https://en.wikipedia.org/wiki?curid=27900956", "title": "Bistra Glacier", "text": "Bistra Glacier\n\nBistra Glacier (, ‘Lednik Bistra’ \\'led-nik 'bis-tra\\) is long and wide glacier on the northwest side of Imeon Range on Smith Island in the South Shetland Islands, Antarctica. It is situated southwest of Chuprene Glacier and northwest of Dragoman Glacier, drains the west slopes of Mount Foster and the north slopes of Slaveykov Peak, and flows northwestwards of Zavet Saddle to enter Drake Passage south of Garmen Point.\n\nThe glacier is named after the settlements of Bistra in northeastern Bulgaria.\n\nBistra Glacier is located at . Bulgarian mapping in 2009.\n\n\n\n"}
{"id": "1932484", "url": "https://en.wikipedia.org/wiki?curid=1932484", "title": "Brinkmann coordinates", "text": "Brinkmann coordinates\n\nBrinkmann coordinates (named for Hans Brinkmann) are a particular coordinate system for a spacetime belonging to the family of pp-wave metrics. In terms of these coordinates, the metric tensor can be written as\n\nwhere formula_2, the coordinate vector field dual to the covector field formula_3, is a null vector field. Indeed, geometrically speaking, it is a null geodesic congruence with vanishing optical scalars. Physically speaking, it serves as the wave vector defining the direction of propagation for the pp-wave.\n\nThe coordinate vector field formula_4 can be spacelike, null, or timelike at a given event in the spacetime, depending upon the sign of formula_5 at that event. The coordinate vector fields formula_6 are both spacelike vector fields. Each surface formula_7 can be thought of as a wavefront.\n\nIn discussions of exact solutions to the Einstein field equation, many authors fail to specify the intended range of the coordinate variables formula_8. Here we should take\n\nformula_9\n\nto allow for the possibility that the pp-wave develops a null curvature singularity.\n\n"}
{"id": "18591272", "url": "https://en.wikipedia.org/wiki?curid=18591272", "title": "Cafarsite", "text": "Cafarsite\n\nCafarsite (Ca(Ti,Fe,Fe,Mn)(AsO)·4HO) is a rare calcium iron arsenite mineral. Manganese and titanium occur with iron in the formula.\n\nIt was first described in 1966 for an occurrence in the Binn Valley, Valais, Switzerland. Its name is from the composition, \"ca\"lcium, \"f\"errum (iron), and \"ars\"enic. It has also been reported from Piedmont, Italy and the Hemlo gold mine in the Thunder Bay District, Ontario, Canada.\n"}
{"id": "5360316", "url": "https://en.wikipedia.org/wiki?curid=5360316", "title": "Catagmatic", "text": "Catagmatic\n\nIn pre-modern medicine, the term catagmatic generally referred to any treatment purported to heal bone fractures, by promoting the formation of a callus.\n\nThe principal catagmatics were Armenian bole, gum tragacanth, osteocolla, Cyprus nuts, frankincense, aloes, and acacia.\n\nThe word comes from the Greek \"καταγμα\", \"fracture\".\n"}
{"id": "12005987", "url": "https://en.wikipedia.org/wiki?curid=12005987", "title": "Charles N. F. Brisout", "text": "Charles N. F. Brisout\n\nCharles Nicolas François Brisout de Barneville (22 July 1822, in Paris – 2 May 1893, in St. Germain-en-Laye) was a French entomologist who specialised in Orthoptera and Coleoptera.\n\nHe was President of the Société entomologique de France in 1873. His collections are kept by the Society.\n"}
{"id": "18139667", "url": "https://en.wikipedia.org/wiki?curid=18139667", "title": "Climate Code Red", "text": "Climate Code Red\n\nClimate Code Red: The Case for Emergency Action is a 2008 book which presents scientific evidence that the global warming crisis is worse than official reports and national governments have so far indicated. The book argues that we are facing a \"sustainability emergency\" that requires a clear break from business-as-usual politics. The authors explain that emergency action to address climate change is not so much a radical idea as an indispensable course we must embark upon. \"Climate Code Red\" draws heavily on the work of a large number of climate scientists, including James E. Hansen.\n\nThe key themes of \"Climate Code Red\" are:\n\nCo-author David Spratt is a Melbourne businessman, climate-policy analyst, and co-founder of the Carbon Equity network, and director of the Breakthrough - National Centre for Climate Restoration. Co-author Philip Sutton is convener of the Greenleap Strategic Institute and Assistant Convenor of the Climate Emergency Network.\n\nThe book was launched by the Governor of Victoria, Professor David de Kretser in Parliament House in Melbourne, Victoria, on July 17, 2008.\n\n\n"}
{"id": "435099", "url": "https://en.wikipedia.org/wiki?curid=435099", "title": "Conditional expectation", "text": "Conditional expectation\n\nIn probability theory, the conditional expectation, conditional expected value, or conditional mean of a random variable is its expected value – the value it would take “on average” over an arbitrarily large number of occurrences – given that a certain set of \"conditions\" is known to occur. If the random variable can take on only a finite number of values, the “conditions” are that the variable can only take on a subset of those values. More formally, in the case when the random variable is defined over a discrete probability space, the \"conditions\" are a partition of this probability space. \n\nWith multiple random variables, for one random variable to be mean independent of all others both individually and collectively means that each conditional expectation equals the random variable's (unconditional) expected value. This always holds if the variables are independent, but mean independence is a weaker condition.\n\nDepending on the nature of the conditioning, the conditional expectation can be either a random variable itself or a fixed value. With two random variables, if the expectation of a random variable formula_1 is expressed conditional on another random variable formula_2 without a particular value of formula_2 being specified, then the expectation of formula_1 conditional on formula_2, denoted formula_6, is a function of the random variable formula_2 and hence is itself a random variable. Alternatively, if the expectation of formula_1 is expressed conditional on the occurrence of a particular value of formula_2, denoted formula_10, then the conditional expectation formula_11 is a fixed value.\n\nThis concept generalizes to any probability space using measure theory.\n\nIn modern probability theory the concept of conditional probability is defined in terms of conditional expectation.\n\n\"Example 1\". Consider the roll of a fair and let \"A\" = 1 if the number is even (i.e. 2, 4, or 6) and \"A\" = 0 otherwise. Furthermore, let \"B\" = 1 if the number is prime (i.e. 2, 3, or 5) and \"B\" = 0 otherwise.\nThe unconditional expectation of A is formula_12. But the expectation of A \"conditional\" on B = 1 (i.e., conditional on the die roll being 2, 3, or 5) is formula_13, and the expectation of A conditional on B = 0 (i.e., conditional on the die roll being 1, 4, or 6) is formula_14. Likewise, the expectation of B conditional on A = 1 is formula_15, and the expectation of B conditional on A = 0 is formula_16.\n\n\"Example 2\". Suppose we have daily rainfall data (mm of rain each day) collected by a weather station on every day of the ten–year (3652–day) period from Jan 1, 1990 to Dec 31, 1999. The unconditional expectation of rainfall for an unspecified day is the average of the rainfall amounts of those 3652 days. The \"conditional\" expectation of rainfall for an otherwise unspecified day known to be (conditional on being) in the month of March is the average of daily rainfall over all 310 days of the ten–year period that falls in March. And the conditional expectation of rainfall conditional on days dated March 2 is the average of the rainfall amounts that occurred on the ten days with that specific date.\n\nThe related concept of conditional probability dates back at least to Laplace who calculated conditional distributions. It was Andrey Kolmogorov who in 1933 formalized it using the Radon–Nikodym theorem. In works of Paul Halmos and Joseph L. Doob from 1953, conditional expectation was generalized to its modern definition using sub-σ-algebras.\n\nIn classical probability theory the conditional expectation of formula_1 given an event formula_18 (which may be the event formula_19 for a random variable formula_2) is the average of formula_1 over all outcomes in formula_18, that is\n\nWhere formula_24 is the cardinality of formula_18.\n\nThe sum above can be grouped by different values of formula_26, to get a sum over the range formula_27 of formula_1\n\nIn modern probability theory, when formula_18 is an event with strictly positive probability, it is possible to give a similar formula. This is notably the case for a discrete random variable formula_2 and for formula_10 in the range of formula_2 if the event formula_18 is formula_19. Let formula_36 be a probability space, formula_1 is a random variable on that probability space, and formula_38 an event with strictly positive probability formula_39. Then the conditional expectation of formula_1 given the event formula_18 is\n\nwhere formula_27 is the range of formula_1 and formula_45 is the probability measure defined, for each set formula_46, as formula_47, the conditional probability of formula_46 given formula_18.\n\nWhen formula_50 (for instance if formula_2 is a continuous random variable and formula_18 is the event formula_19, this is in general the case), the Borel–Kolmogorov paradox demonstrates the ambiguity of attempting to define the conditional probability knowing the event formula_18. The above formula shows that this problem transposes to the conditional expectation. So instead one only defines the conditional expectation with respect to a σ-algebra or a random variable.\n\nIf \"Y\" is a discrete random variable on the same probability space formula_36 having range formula_56, then the conditional expectation of \"X\" with respect to \"Y\" is the random variable formula_57 on formula_56 defined by\n\nThere is a closely related function from formula_60 to formula_56 defined by\n\nThis function, which is different from the previous one, is the conditional expectation of \"X\" with respect to the σ-algebra generated by \"Y\". The two are related by\n\nAs mentioned above, if \"Y\" is a continuous random variable, it is not possible to define formula_64 by this method. As explained in the Borel–Kolmogorov paradox, we have to specify what limiting procedure produces the set \"Y\" = \"y\". If the event space formula_60 has a distance function, then one procedure for doing so is as follows. Define the set formula_66. Assume that each formula_67 is \"P\"-measurable and that formula_68 for all formula_69. Then conditional expectation with respect to formula_67 is well-defined. Take the limit as formula_71 tends to 0 and define\n\nReplacing this limiting process by the Radon–Nikodym derivative yields an analogous definition that works more generally.\n\nConsider the following:\n\nSince formula_77 is a subalgebra of formula_76, the function formula_74 is usually not formula_77-measurable, thus the existence of the integrals of the form formula_81, where formula_82 and formula_83 is the restriction of formula_84 to formula_77, cannot be stated in general. However, the local averages formula_86 can be recovered in formula_87 with the help of the conditional expectation. A conditional expectation of \"X\" given formula_77, denoted as formula_89, is any formula_77-measurable function formula_91 which satisfies:\n\nfor each formula_93.\n\nThe existence of formula_89 can be established by noting that formula_95 for formula_96is a finite measure on formula_97 that is absolutely continuous with respect to formula_84. If formula_99 is the natural injection from formula_77 to formula_76, then formula_102 is the restriction of formula_103 to formula_77 and formula_105 is the restriction of formula_84 to formula_77. Furthermore, formula_108 is absolutely continuous with respect to formula_109, because the condition\nimplies\n\nThus, we have \nwhere the derivatives are Radon–Nikodym derivatives of measures.\n\nConsider, in addition to the above,\n\nLet formula_115 be a formula_116-measurable function such that, for every formula_116-measurable function formula_118,\n\nThen the random variable formula_120, denoted as formula_121, is a conditional expectation of \"X\" given formula_2.\n\nThis definition is equivalent to defining the conditional expectation with respect to the sub-formula_123-field of formula_76 (see above) defined by the pre-image of \"Σ\" by \"Y\". If we define\n\nthen\n\n\nIn the definition of conditional expectation that we provided above, the fact that formula_2 is a \"real\" random element is irrelevant. Let formula_113 be a measurable space, where formula_116 is a σ-algebra on formula_140. \"A formula_140-valued random element\" is a measurable function formula_142, i.e. formula_143 for all formula_144. The \"distribution\" of formula_2 is the probability measure formula_146 defined as the pushforward measure formula_147, that is, such that formula_148.\n\nTheorem. If formula_149 is an integrable random variable, then there exists a unique integrable random element formula_150, defined formula_151 almost surely, such that\n\nfor all formula_153.\n\nProof sketch. Let formula_154 be such that formula_155. Then formula_156 is a signed measure which is absolutely continuous with respect to formula_151. Indeed formula_158 means exactly that formula_159, and since the integral of an integrable function on a set of probability 0 is 0, this proves absolute continuity. The Radon–Nikodym theorem then proves the existence of a density of formula_156 with respect to formula_151. This density is formula_162. formula_163\n\nComparing with conditional expectation with respect to sub-σ-algebras, it holds that\n\nWe can further interpret this equality by considering the abstract change of variables formula to transport the integral on the right hand side to an integral over Ω:\n\nThe equation means that the integrals of formula_1 and the composition formula_167 over sets of the form formula_168, for formula_153, are identical.\n\nThis equation can be interpreted to say that the following diagram is commutative \"on average\".\n\nWhen \"X\" and \"Y\" are both discrete random variables, then the conditional expectation of \"X\" given the event \"Y\" = \"y\" can be considered as function of \"y\" for \"y\" in the range of \"Y\":\n\nwhere formula_27 is the range of \"X\".\n\nIf \"X\" is a continuous random variable, while \"Y\" remains a discrete variable, the conditional expectation is\n\nwith formula_173 (where \"f(\"x, y\") gives the joint density of \"X\" and \"Y\") being the conditional density of \"X\" given \"Y\" = \"y\".\n\nIf both \"X\" and \"Y\" are continuous random variables, then the conditional expectation is\n\nwhere formula_175 (where \"f\"(\"y\") gives the density of \"Y\").\n\nAll the following formulas are to be understood in an almost sure sense. The σ-algebra formula_77 could be replaced by a random variable formula_177.\n\n\nLet formula_181. Then formula_1 is independent of formula_183, so we get that\nThus the definition of conditional expectation is satisfied by the constant random variable formula_185, as desired.\n\n\n"}
{"id": "40836275", "url": "https://en.wikipedia.org/wiki?curid=40836275", "title": "Cosmic age problem", "text": "Cosmic age problem\n\nThe cosmic age problem is a historical problem in astronomy concerning the age of the universe. The problem was that at various times in the 20th century, some objects in the universe were estimated to be older than the time elapsed since the Big Bang, as estimated from measurements of the expansion rate of the universe known as the Hubble constant, denoted H. (This is more correctly called the Hubble parameter, since it generally varies with time).\nIf so, this would represent a contradiction, since objects such as galaxies, stars and planets could not have existed in the extreme temperatures and densities shortly after the Big Bang.\n\nSince around 1997–2003, the problem is believed to be solved by most cosmologists: modern cosmological measurements lead to a precise estimate of the age of the universe (i.e. time since the Big Bang) of 13.8 billion years, and recent age estimates for the oldest objects are either younger than this, or consistent allowing for measurement uncertainties.\n\nFollowing theoretical developments of the Friedmann equations by Alexander Friedmann and Georges Lemaître in the 1920s, and the discovery of the expanding universe by Edwin Hubble in 1929, it was immediately clear that tracing this expansion backwards in time predicts that the universe had almost zero size at a finite time in the past. This concept, initially known as the \"Primeval Atom\" by Lemaitre, was later elaborated into the modern Big Bang theory. If the universe had expanded at a constant rate in the past, the age of the universe now (i.e. the time since the Big Bang) is simply the inverse of the Hubble constant, often known as the \"Hubble time\". For Big Bang models with zero cosmological constant and positive matter density, the actual age must be somewhat younger than this Hubble time; typically the age would be between 66% and 90% of the Hubble time, depending on the density of matter.\n\nHubble's early estimate of his constant was 550 (km/s)/Mpc, and the inverse of that is 1.8 billion years. It was believed by many geologists such as Arthur Holmes in the 1920s that the Earth was probably over 2 billion years old, but with large uncertainty. The possible discrepancy between the ages of the Earth and the universe was probably one motivation for the development of the Steady State theory in 1948 as an alternative to the Big Bang; in the (now obsolete) steady state theory, the universe is infinitely old and on average unchanging with time. The steady state theory postulated spontaneous creation of matter to keep the average density constant as the universe expands, and therefore most galaxies still have an age less than 1/H. However, if H had been 550 (km/s)/Mpc, our Milky Way galaxy would be exceptionally large compared to most other galaxies, so it could well be much older than an average galaxy, therefore eliminating the age problem.\n\nIn the 1950s, two substantial errors were discovered in Hubble's extragalactic distance scale: first in 1952, Walter Baade discovered there were two classes of Cepheid variable star. Hubble's sample comprised different classes nearby and in other galaxies, and correcting this error made all other galaxies twice as distant as Hubble's values, thus doubling the Hubble time. A second error was discovered by Allan Sandage and coworkers: for galaxies beyond the Local Group, Cepheids were too faint to observe with Hubble's instruments, so Hubble used the brightest stars as distance indicators. Many of Hubble's \"brightest stars\" were actually HII regions or clusters containing many stars, which caused another underestimation of distances for these more distant galaxies. Thus, in 1958 Sandage published the first reasonably accurate measurement of the Hubble constant, at 75 (km/s)/Mpc, which is close to modern estimates of 68–74 (km/s)/Mpc.\n\nThe age of the Earth (actually the Solar System) was first accurately measured around 1955 by Clair Patterson at 4.55 billion years, essentially identical to the modern value. For H ~ 75 (km/s)/Mpc, the inverse of H is 13.0 billion years; so after 1958 the Big Bang model age was comfortably older than the Earth.\n\nHowever, in the 1960s and onwards, new developments in the theory of stellar evolution enabled age estimates for large star clusters called globular clusters: these generally gave age estimates of around 15 billion years, with substantial scatter. Further revisions of the Hubble constant by Sandage and Gustav Tammann in the 1970s gave values around 50–60 (km/s)/Mpc, and an inverse of 16-20 billion years, consistent with globular cluster ages.\n\nHowever, in the late 1970s to early 1990s, the age problem re-appeared: new estimates of the Hubble constant gave higher values, with Gerard de Vaucouleurs estimating values 90–100 (km/s)/Mpc, while Marc Aaronson and co-workers gave values around 80-90  (km/s)/Mpc. Sandage and Tammann continued to argue for values 50-60, leading to a period of controversy sometimes called the \"Hubble wars\". The higher values for H appeared to predict a universe younger than the globular cluster ages, and gave rise to some speculations during the 1980s that the Big Bang model was seriously incorrect.\n\nThe age problem was eventually thought to be resolved by several developments between 1995-2003: firstly, a large program with the Hubble space telescope measured the Hubble constant at 72 (km/s)/Mpc with 10 percent uncertainty. Secondly, measurements of parallax by the Hipparcos spacecraft in 1995 revised globular cluster distances upwards by 5-10 percent; this made their stars brighter than previously estimated and therefore younger, shifting their age estimates down to around 12-13 billion years. Finally, from 1998-2003 a number of new cosmological observations including supernovae, cosmic microwave background observations and large galaxy redshift surveys led to the acceptance of dark energy and the establishment of the Lambda-CDM model as the standard model of cosmology. The presence of dark energy implies that the universe was expanding more slowly at around half its present age than today, which makes the universe older for a given value of the Hubble constant. The combination of the three results above essentially removed the discrepancy between estimated globular cluster ages and the age of the universe.\n\nMore recent measurements from WMAP and the Planck spacecraft lead to an estimate of the age of the universe of 13.80 billion years with only 0.3 percent uncertainty (based on the standard Lambda-CDM model), and modern age measurements for globular clusters and other objects are currently smaller than this value (within the measurement uncertainties). A substantial majority of cosmologists therefore believe the age problem is now resolved.\n\n"}
{"id": "36278887", "url": "https://en.wikipedia.org/wiki?curid=36278887", "title": "Countries applying biometrics", "text": "Countries applying biometrics\n\nThis list is only partial. Multiple other countries in Asia and Africa, including China (PRC), collect fingerprints and other biometrics from foreign visitors upon entry.\n\nVisitors intending to visit Australia and returning Australian residents with an ePassport may have to submit to biometric authentication as part of the Smartgate system, linking individuals to their visas and passports. Biometric data are collected from some visa applicants by Immigration particularly in cases of Refugee or Humanitarian Visas.\nAustralia is the first country to introduce a Biometrics Privacy Code, which is established and administered by the Biometrics Institute. The Biometrics Institute Privacy Code forms part of Australian privacy legislation. The Code includes privacy standards that are at least equivalent to the Australian National Privacy Principles (NPPs) in the Privacy Act and also incorporates higher standards of privacy protection in relation to certain acts and practices. Only members of the Biometrics Institute are eligible to subscribe to this Code. Biometrics Institute membership, and thus subscription to this Code, is voluntary.\n\nSince the beginning of the 20th century, Brazilian citizens have had user ID cards. The decision by the Brazilian government to adopt fingerprint-based biometrics was spearheaded by Dr. Felix Pacheco at Rio de Janeiro, at that time capital of the Federative Republic. Dr. Pacheco was a friend of Dr. Juan Vucetich, who invented one of the most complete tenprint classification systems in existence. The Vucetich system was adopted not only in Brazil, but also by most of the other South American countries. The oldest and most traditional ID Institute in Brazil (Instituto de Identificação Félix Pacheco) was integrated at DETRAN (Brazilian equivalent to DMV) into the civil and criminal AFIS system in 1999.\n\nEach state in Brazil is allowed to print its own ID card, but the layout and data are the same for all of them. The ID cards printed in Rio de Janeiro are fully digitized using a 2D bar code with information which can be matched against its owner off-line. The 2D bar code encodes a color photo, a signature, two fingerprints, and other citizen data. This technology was developed in 2000 in order to enhance the safety of the Brazilian ID cards.\n\nBy the end of 2005, the Brazilian government started the development of its new passport. The new documents started to be released by the beginning of 2007, in Brasília. The new passport included several security features, like Laser perforation, UV hidden symbols, security layer over variable data etc. Brazilian citizens will have their signature, photo, and 10 rolled fingerprints collected during passport requests. All of the data is planned to be stored in ICAO E-passport standard. This allows for contactless electronic reading of the passport content and Citizens ID verification since fingerprint templates and token facial images will be available for automatic recognition.\n\nCanada has begun research into the use of biometric technology in the area of border security and immigration (Center for Security Sciences, Public Security Technical Program, Biometrics Community of Practice). At least one program, the NEXUS program operated jointly by the Canada Border Services Agency and U.S. Customs and Border Protection, is already operational. It is a functioning example of biometric technology, specifically \"iris recognition biometric technology\" used for border control and security for air travellers. Canada is also the home for the world's biggest biometric access control company called Bioscrypt Inc..\n\nFrance introduced biometric border control system known as Parafe on 3 August 2007 at some of its Shengen borders. The system allows those enrolled to utilise automated \"airlock\" barriers to cross the border. French passports issued after June 2009 are pre-enrolled, while European Union, European Economic Area, Swiss passport holders and family member of European Union citizens can enroll at Charles de Gaulle (Terminals 1 and 2E) and Paris Orly airports. The barriers are currently in use at Orly, Charles de Gaulle, Marseille airports and on Eurostar services between London and Paris\n\nThe Gambia Biometric Identification System (GAMBIS) allowed for the issuance of Gambia's first biometric identity documents in July 2009. An individual's data, including their biometric information (thumbprints) is captured in the database. A National Identification Number (NIN), unique to each applicant applying for a card, is issued to the applicant. Biometric documents issued for Gambia include national identity cards, residential permits, non-Gambian ID cards and driver licenses.\n\nThe biometrics market in Germany experienced enormous growth between the years 2005 and 2009. \"The market size will increase from approximately 120 million € (2004) to 377 million €\" (2009). \"The federal government will be a major contributor to this development\". In particular, the biometric procedures of fingerprint and facial recognition can profit from the government project.\nIn May 2005 the German Upper House of Parliament approved the implementation of the ePass, a passport issued to all German citizens which contain biometric technology. The ePass has been in circulation since November 2005, and contains a chip that holds a digital photograph and one fingerprint from each hand, usually of the index fingers, though others may be used if these fingers are missing or have extremely distorted prints. \"A third biometric identifier – iris scans – could be added at a later stage\".\nAn increase in the prevalence of biometric technology in Germany is an effort to not only keep citizens safe within German borders but also to comply with the current US deadline for visa-waiver countries to introduce biometric passports. In addition to producing biometric passports for German citizens, the German government has put in place new requirements for visitors to apply for visas within the country. \"Only applicants for long-term visas, which allow more than three months' residence, will be affected by the planned biometric registration program. The new work visas will also include fingerprinting, iris scanning, and digital photos\".\n\nGermany is also one of the first countries to implement biometric technology at the Olympic Games to protect German athletes. \"The Olympic Games is always a diplomatically tense affair and previous events have been rocked by terrorist attacks—most notably when Germany last held the Games in Munich in 1972 and 11 Israeli athletes were killed\".\n\nBiometric technology was first used at the Olympic Summer Games in Athens, Greece in 2004. \"On registering with the scheme, accredited visitors will receive an ID card containing their fingerprint biometrics data that will enable them to access the 'German House'. Accredited visitors will include athletes, coaching staff, team management and members of the media\".\n\nAs a protest against the increasing use of biometric data, the influential hacker group Chaos Computer Club published a fingerprint of German Minister of the Interior Wolfgang Schäuble in the March 2008 edition of its magazine \"Datenschleuder\". The magazine also included the fingerprint on a film that readers could use to fool fingerprint readers.\n\nThe government of India funds a number of social welfare schemes focused towards the poor and most vulnerable sections of society. Aadhaar and its platform offers an opportunity to streamline their welfare delivery mechanisms and improve transparency and good governance.\nIn the recent Supreme Court of India verdict AADHAR has to be mandatorily linked to Permanent Account Number (Tax Identification Number of India) \n\nThe UIDAI issues Aadhaar number to residents only after de-duplicating their demographic and biometric attributes against its entire database. Aadhaar authentication enables elimination of duplicates under various schemes and is expected to generate substantial savings to the government exchequer. It also provides the government with accurate data on beneficiaries, enables direct benefit programs, and allows the government departments/service providers to coordinate and optimize various schemes. Aadhaar will enable implementing agencies to verify beneficiaries and ensure targeted delivery of benefits. All these activities will lead to:\n\nCurbing Leakages through Targeted Delivery: Welfare programs where beneficiaries are required to be confirmed before the service delivery, stand to benefit from UIDAI’s authentication services. This will result in curbing leakages and ensuring that services are delivered to the intended beneficiaries only. Examples include subsidized food and kerosene delivery to Public Distribution System (PDS) beneficiaries, worksite attendance of Mahatma Gandhi National Rural Employment Guarantee Scheme (MGNREGS) beneficiaries, etc.\n\nImproving Efficiency and Efficacy: With the Aadhaar platform providing accurate and transparent information about the service delivery mechanism, government can improve disbursement systems and utilize scarce development funds more effectively and efficiently including better human resource utilisation involved in the service delivery network.\n\nFor Residents\n\nAadhaar system provides single source online identity verification across the country for the residents. Once residents enrol, they can use the Aadhaar number to authenticate and establish their identity multiple times using electronic means. It eliminates the hassle of repeatedly providing supporting identity documents each time a resident wishes to access services such as opening a bank account, obtaining driving license, etc. By providing a portable proof of identity that can be verified through Aadhaar authentication on-line anytime, anywhere, the Aadhaar system enables mobility to millions of people who migrate from one part of the country to another.\n\nAbout UIDAI\n\nThe Unique Identification Authority of India (UIDAI) is a statutory authority established under the provisions of the Aadhaar (Targeted Delivery of Financial and other Subsidies, benefits and services) Act, 2016 (\"Aadhaar Act 2016\") on 12 July 2016 by the Government of India, under the Ministry of Electronics and Information Technology (MeitY).\n\nPrior to its establishment as a statutory authority, UIDAI was functioning as an attached office of the then Planning Commission (now NITI Aayog) vied its Gazette Notification No.-A-43011/02/2009-Admn.I dated 28 January 2009. Later, on 12 September 2015, the Government revised the Allocation of Business Rules to attach the UIDAI to the Department of Electronics & Information Technology (DeitY) of the then Ministry of Communications and Information Technology.\n\nUIDAI was created with the objective to issue Unique Identification numbers (UID), named as \"Aadhaar\", to all residents of India that is (a) robust enough to eliminate duplicate and fake identities, and (b) can be verified and authenticated in an easy, cost-effective way. The first UID number was issued on 29 September 2010 to a resident of Nandurbar, Maharashtra. The Authority has so far issued more than 111 crore (>1.11 Billion) Aadhaar numbers to the residents of India.\n\nUnder the Aadhaar Act 2016, UIDAI is responsible for Aadhaar enrollment and authentication, including operation and management of all stages of Aadhaar life cycle, developing the policy, procedure and system for issuing Aadhaar numbers to individuals and perform authentication and also required to ensure the security of identity information and authentication records of individuals.\n\nBiometrics are being used extensively in Iraq to catalogue as many Iraqis as possible providing Iraqis with a verifiable identification card, immune to forgery. During account creation, the collected biometrics information is logged into a central database which then allows a user profile to be created. Even if an Iraqi has lost their ID card, their identification can be found and verified by using their unique biometric information. Additional information can also be added to each account record, such as individual personal history.\n\nThe Israeli government has passed a bill calling for the creation of a biometric database of all Israeli residents; the database will contain their fingerprints and facial contours. Upon enrolling, a resident would be issued a new form of an identification card, containing these biometrics as well as being harder to counterfeit and having a smart-card chip with a digital certificate, allowing for digital signatures. In addition, smart-chip passports can be issued. The law is currently in its trial period, during which enrollment is optional; pending on successful trial, enrollment would be mandatory for all residents upon issuance or replacement of \"Teudot Zehut\" .\n\nOpponents of the proposed law, including prominent Israeli scientists and security experts, warned that the existence of such a database could damage both civil liberties and state security, because any leaks could be used by criminals or hostile individuals against Israeli residents.\n\nSince 2013, Italy has deployed a number of ePassport gates at major Italian airports allowing EU, EEA and Swiss citizens with a biometric passport to cross the Schengen border in a totally automated way.\n\nItaly has standardized protocols in use by police forces.\n\nStarting 21 September 2009, all new Dutch passports and ID cards must include the holder's fingerprints. Since 26 August 2006, Dutch passports have included an electronic chip containing the personal details of the holder and a digitised passport photograph. The chip holds following data: your name (first name(s) and surname); the document number; your nationality, date of birth and sex; the expiry date; the country of issue; and your personal ID number (Dutch tax and social security (SoFi) number).\n\nSince 28 August 2006, under EU regulation '2252/2004' all EU member states have been obliged to include a digital image of the holder's passport photograph.\n\nSmartGate was launched by the New Zealand government at Auckland International Airport on Thursday 3 December 2009. The program is available at Auckland, Wellington and Christchurch international airports for arriving travellers, and also for travellers departing from Auckland (with plans to extend the program for departures from Wellington and Christchurch by mid-2011).\n\nThe kiosk and gate system will allow all New Zealand and Australian electronic passport holders over 18 to clear passport control without needing to have their identity checked by a Customs officer. The system uses \"advanced facial software\" which \"compares your face with the digital copy of your photo in your ePassport chip\".\n\nDeputy comptroller of customs John Secker said SmartGate represented probably the biggest single development in border processing in New Zealand in the past two decades.\nPeople will have a choice whether they want to use the system or go through normal passport control.\n\nThe introduction of biometric passports to Norway began in 2005 and supplied by Setec, a Gemplus International company. The chips used in Norway employ security features recommended by the International Civil Aviation Organization (ICAO) although security and privacy concerns from civil liberties groups remain.\n\nIn 2007 the Norwegian government launched a ‘multi-modal’ biometric enrolment system supplied by Motorola. Motorola's new system enabled multiple public agencies to digitally capture and store fingerprints, 2D facial images and signatures for passports and visas.\n\nSteria and IDEX ASA are two of the leading biometrics software and product manufacturers in Norway.\n\nTanzania began its biometric voter registration (BVR) system in February 2015 and planned to use it first in a constitutional referendum and subsequent general elections.\n\nA bill on biometric passports was approved by Ukraine's Parliament on February 15, 2012. These passports are to be used for foreign travel only. Internal passports so far don't have biometric information. An updated version of the law was passed by the Verkhovna Rada of Ukraine on November 20, 2012, taking into account proposals made by the president, and was subsequently signed by the president. The new biometric passports are one of the conditions placed on Ukraine by the European Union on the path to easing its visa requirements and eventually moving towards a visa-free regime with the EU. These national ID cards will be valid for 10 years and issued at the age of 14, instead of at the age of 16, as previous Ukrainian passports were. Verkhovna Rada Commissioner for Human Rights, Valeriya Lutkovska, had criticized the law as it “does not comply with the Constitution of Ukraine and European standards in the sphere of personal data protection, and might infringe on human rights and freedom.” The law was also criticized by Oleksandr Hladun from the Institute for Demography and Social Studies of Ukraine. He said the adoption such a law on a unified demographic register threatens “to create a police state” in Ukraine, where any information can be collected about any person and used without the person’s knowledge.\n\nAccording to the new law, each Ukrainian citizen, regardless of his or her age, is obliged to obtain such a passport. The previous version of the Ukrainian passports will remain valid until their official expiration dates.\n\nUkraine's law on biometric passports came into force on December 6, 2012. The document foresees the introduction of electronic passports containing electronic chips with biometric information for traveling abroad, according to the standards of the International Civil Aviation Organization (ICAO). According to the law, the passports of Ukrainian citizens will be produced in the form of cards with contactless smart chips and issued no later than 30 calendar days from the date of the submission of a relevant application. The electronic passports will include the name of the state, the name of the document, the full name of the holder, the holder's gender, citizenship, date of birth, and a unique number in the register, the number of the document, the document's date of expiration, the date of issue of the document, the name of the agency that issued the document, the place of birth, a photo and the signature of the holder. Information about the parents or the guardian of the holder may be included to the document upon a relevant written request. At the president's request, a new provision has been included in the law that says that people who refuse to enter their personal information in an electronic chip due to their religious beliefs have the right to refuse to use this document or the insertion of such information into the chip.\n\nThe United Nations High Commissioner for Refugees has suggested that the new law does not fully comply with international standards.\n\nThe United Kingdom utilises biometrics in its EPassport gates. UK, EEA and Swiss citizens aged 18 or over, in addition to people enrolled in its registered traveller scheme, are able to use automated frontier gates at key UK airports. Passport officers remain in place at all EPassport gates in case a passport cannot be read or a traveller requires further screening.\n\nFingerprint scanners have been used in some schools to facilitate the subtraction of funds from an account financed by parents for the payment of school dinners. By using such a system, nutritional reports can be produced for parents to survey a child's intake. This has raised questions from liberty groups, which claim that the system removes freedom of choice from young people. Other concerns arise from the possibility of data leaking from the providers of school meals to interest groups that provide health services, such as the NHS and insurance groups, which may have a detrimental effect on the ability of individuals to enjoy equality of access to services.\n\nStarting in 2005, US passports with facial (image-based) biometric data were scheduled to be produced. Privacy activists in many countries have criticized the technology's use for the potential harm to civil liberties, privacy, and the risk of identity theft. Currently, there is some apprehension in the United States (and the European Union) that the information can be \"skimmed\" and identify people's citizenship remotely for criminal intent, such as kidnapping. \n\nThe US Department of Defense (DoD) Common Access Card, is an ID card issued to all US Service personnel and contractors on US Military sites. This card contains biometric data and digitized photographs. It also has laser-etched photographs and holograms to add security and reduce the risk of falsification. There have been over 10 million of these cards issued.\n\nAccording to Jim Wayman, director of the National Biometric Test Center at San Jose State University, Walt Disney World is the nation's largest single commercial application of biometrics. However, the US-VISIT program will very soon surpass Walt Disney World for biometrics deployment.\n\nThe United States records all fingerprints and a picture of foreign airline passengers visiting the U.S. (excepting Canadians), keeping it in databases for seventy-five years. It is suggested that such information should be shared among the U.S. and other countries that have similar systems. Privacy advocates are worried about data leakage and misuse.\n\nNEXUS is a joint Canada-United States program operated by the Canada Border Services Agency and U.S. Customs and Border Protection. It is designed to expedite travel cross the US-Canada border and makes use of biometric authentication technology, specifically \"iris recognition biometric technology\". It permits pre-approved members of the program to use self-serve kiosks at airports, reserved lanes at land crossings, or by phoning border officials when entering by water.\n\nAs of February 2018, Utah requires applicants for non-REAL ID driver licenses to provide fingerprints, and California and Colorado fingerprint all driver license applicants.\n\nThe number of countries adopting biometrics for voting registration or voting authentication has steadily increased. As of 2016, half of the countries in Africa and Latin America use this technology in elections. According to the International IDEA’s ICTs in Elections Database, 35 per cent of over 130 surveyed Electoral Commissions is capturing biometric data (such as fingerprints or photos) as part of their voter registration process.\n"}
{"id": "1157235", "url": "https://en.wikipedia.org/wiki?curid=1157235", "title": "Cutin", "text": "Cutin\n\nCutin is one of two waxy polymers that are the main components of the plant cuticle, which covers all aerial surfaces of plants. The other major cuticle polymer is cutan, which is much more readily preserved in the fossil record. Cutin consists of omega hydroxy acids and their derivatives, which are interlinked via ester bonds, forming a polyester polymer of indeterminate size. \n\nThere are two major monomer families of cutin, the C16 and C18 families. The C16 family consists mainly of 16-hydroxy palmitic acid and 9,16- or 10,16-dihydroxypalmitic acid. The C18 family consists mainly of 18-hydroxy oleic acid, 9,10-epoxy-18-hydroxy stearic acid, and 9,10,18-trihydroxystearate.\n"}
{"id": "2153281", "url": "https://en.wikipedia.org/wiki?curid=2153281", "title": "Dark star (Newtonian mechanics)", "text": "Dark star (Newtonian mechanics)\n\nA dark star is a theoretical object compatible with Newtonian mechanics that, due to its large mass, has a surface escape velocity that equals or exceeds the speed of light. Whether light is affected by gravity under Newtonian mechanics is unclear but if it were accelerated the same way as projectiles, any light emitted at the surface of a dark star would be trapped by the star's gravity, rendering it dark, hence the name. Dark stars are analogous to black holes in general relativity.\n\nDuring 1783 geologist John Michell wrote a letter to Henry Cavendish outlining the expected properties of dark stars, published by The Royal Society in their 1784 volume. Michell calculated that when the escape velocity at the surface of a star was equal to or greater than lightspeed, the generated light would be gravitationally trapped so that the star would not be visible to a distant astronomer.\n\nMichell's idea for calculating the number of such \"invisible\" stars anticipated 20th century astronomers' work: he suggested that since a certain proportion of double-star systems might be expected to contain at least one \"dark\" star, we could search for and catalogue as many double-star systems as possible, and identify cases where only a single circling star was visible. This would then provide a statistical baseline for calculating the amount of other unseen stellar matter that might exist in addition to the visible stars.\n\nMichell also suggested that future astronomers might be able to identify the surface gravity of a distant star by seeing how far the star's light was shifted to the weaker end of the spectrum, a precursor of Einstein's 1911 gravity-shift argument. However, Michell cited Newton as saying that blue light was less energetic than red (Newton thought that more massive particles were associated with bigger wavelengths), so Michell's predicted spectral shifts were in the wrong direction. It is difficult to tell whether Michell's careful citing of Newton's position on this may have reflected a lack of conviction on Michell's part over whether Newton was correct or just academic thoroughness.\n\nIn 1796, the mathematician Pierre-Simon Laplace promoted the same idea in the first and second editions of his book \"Exposition du système du Monde\", independently of Michell.\n\nBecause of the development of the wave theory of light, Laplace may have removed it from later editions as light became to be thought of as a massless wave, and therefore not influenced by gravity and as a group, physicists dropped the idea although the German physicist, mathematician, and astronomer Johann Georg von Soldner continued with Newton's corpuscular theory of light as late as 1804.\n\nDark stars and black holes both have a surface escape velocity equal or greater than lightspeed, and a critical radius of \"r\" ≤ 2\"M\".\n\nHowever, the dark star is capable of emitting indirect radiation – outward-aimed light and matter can leave the \"r\" = 2\"M\" surface briefly before being recaptured, and while outside the critical surface, can interact with other matter, or be accelerated free from the star through such interactions. A dark star, therefore, has a rarefied atmosphere of \"visiting particles\", and this ghostly halo of matter and light can radiate, albeit weakly. Also as Faster than light speeds are possible in Newtonian mechanics, it is possible for particles to escape.\n\n\n\n"}
{"id": "11198203", "url": "https://en.wikipedia.org/wiki?curid=11198203", "title": "Demographic momentum", "text": "Demographic momentum\n\nDemographic momentum is the tendency for growing populations to continue growing after a fertility decline because of their young age distribution. This is important because once this happens a country moves to a different stage in the demographic transition model.\n\nEven in the face of extreme measures aimed at lowering reproductive rates, the population will continue to grow due to a large proportion of its population entering its reproductive years.\n\nFor example, when China first introduced its one-child policy, population growth continued regardless. Even though the number of children born reduced dramatically, the sheer number of maturing youth was significant. In 1979 when the one-child policy entered into force, the number of people becoming adults was based on the number of births around the 1950s, not 1979. As a result, the Chinese population maintained the same momentum of increase as for the past 20 years. It is only now that the Chinese population has reached a somewhat stabilized population growth.\n\n"}
{"id": "23103356", "url": "https://en.wikipedia.org/wiki?curid=23103356", "title": "Dimorphic root system", "text": "Dimorphic root system\n\nA dimorphic root system is a plant root system with two distinct root forms, which are adapted to perform different functions. One of the most common manifestations is in plants with both a taproot, which grows straight down to the water table, from which it obtains water for the plant; and a system of lateral roots, which obtain minerals.\n"}
{"id": "23307031", "url": "https://en.wikipedia.org/wiki?curid=23307031", "title": "Douglas SASSTO", "text": "Douglas SASSTO\n\nDouglas Aircraft's SASSTO, short for \"Saturn Application Single Stage to Orbit\", was a single-stage-to-orbit (SSTO) reusable launch system designed by Philip Bono's team in 1967. SASSTO was a study in minimalist designs, a launcher with the specific intent of repeatedly placing a Gemini capsule in orbit for the lowest possible cost. The SASSTO booster was based on the layout of the S-IVB upper stage from the Saturn family, modified with a plug nozzle. Although the SASSTO design was never followed up at Douglas, it is widely referred to in newer studies for SSTO launchers, notably the MBB \"Beta\" design, which was largely an updated version of SASSTO.\n\nIn 1962 NASA sent out a series of studies on post-Apollo launch needs, which generally assumed very large launchers for a manned mission to Mars. At Douglas, makers of the S-IVB, Philip Bono led a team that studied a number of very large liquid-fueled boosters as a way to lower the cost of space exploration. His designs were based on an economy of scale which makes larger rockets more economical than smaller ones as the structure accounts for less and less of the overall weight of the launcher. At some point the dry weight of the launcher becomes lower than the payload it can launch, after which increases in payload fraction are essentially free. However, this point is crossed at relatively large vehicle sizes - Bono's original OOST study from 1963 was over long - and this path to lower costs only makes sense if there is an enormous amount of payload that needs to be launched.\n\nAfter designing a number of such vehicles, including ROOST and the ROMBUS/Ithacus/Pegasus series, Bono noticed that the S-IVB stage, then just starting to be used operationally, was very close to being able to reach orbit on its own if launched from the ground. Intrigued, Bono started looking at what missions a small S-IVB-based SSTO could accomplish, realizing that it would be able to launch a manned Gemini capsule if it was equipped with some upgrades, notably an aerospike engine that would improve the specific impulse and provide altitude compensation. He called the design \"SASSTO\", short for \"Saturn Application Single-Stage To Orbit\".\n\nThese same upgrades would also have the side-effect of lowering the weight of the SASSTO compared to the original S-IVB, while at the same time increasing its performance. Thus the study also outlined a number of ways that it could be used in place of the S-IV in existing Saturn IB and Saturn V stacks, increasing their performance. When used with the existing Saturn I lower stage, it would improve payload to low earth orbit from 35,000 to , or if the landing gear were removed and it was expended like the S-IVB. SASSTO would thus give NASA a short-term inexpensive manned launch capability, while also offering improved heavy-launch capability on the existing Saturn infrastructure.\n\nSASSTO required a number of new technologies, however, which made development risky. In particular, the performance of the aerospike engine had to be considerably higher than the J-2 it would replace, yet also offer the ability to be restarted multiple times as the single engine was used for launch, de-orbit and landing. Of particular note was the final landing burn, which required the engines to be restarted at during the descent. The vehicle's weight was also greatly reduced, almost by half, which would not have been trivial considering the relatively good performance of the S-IVB design.\n\nAlthough the SASSTO claimed the S-IVB as its starting point, this was a conceit, and the vehicle had little in common with the S-IVB except its size.\n\nThe internal fuel tankage was considerably different than in the S-IV. The LH2 was no longer cylindrical, but spherical, and moved to the forward location in the fuselage. The LOX tankage, originally on top of the LH2, was re-positioned into a series of smaller spherical tanks arranged in a ring below the LH2. The tanks were all moved forward within the airframe compared to the engine, all of these changes being made in order to reduce changes in the center of gravity as the fuel was burned off. The fuselage section immediately above the engine was necked down, forming what appeared to be a larger single plug. The upper section of the fuselage, over the top of the hydrogen tank, was likewise necked down.\nIn order to increase the amount of LH2 being carried, given the fixed dimensions, SASSTO proposed freezing 50% of the fuel to produce a slush hydrogen mixture. This improvement was not uncommon in designs of the era, although it was not until the 1990s that any serious development work on the concept was carried out.\n\nThe rearmost portion of the spacecraft was a single large plug nozzle, fed by a series of 36 injectors operating at 1500 psia, producing of thrust. Since plug nozzles gain efficiency as they grow larger, the 465 sec specific impulse (compared to the J-2's 425) was not particularly aggressive. The engine also served as the primary heat shield, actively cooled by liquid hydrogen that was then dumped overboard.\n\nFour landing legs extended from fairings on the fuselage sides, retracting to a point about even with the \"active\" portion of the engine area. Four clusters of small maneuvering engines were located between the legs, about half-way from front to back along the fuselage. A series of six smaller tanks arranged in the gaps between the LOX and LH2 tanks fed the maneuvering engines.\n\nSASSTO delivered of cargo to a orbit when launched due east from the Kennedy Space Center. Empty weight was , considerably lighter than the S-IVB's , and gross lift off weight was . The typical payload was the Gemini, which was covered with a large aerodynamic fairing.\n\nRe-entry maneuverability was through a blunt-body lifting profile, similar to the Apollo CSM. The cross-range was limited, about , and there was basically no maneuverability at all on final approach. There was enough fuel for about 10 seconds of hovering and small maneuvers to select a flat landing spot. Because SASSTO was the same basic size as the S-IVB, Douglas proposed transporting it in the existing Aero Spacelines Super Guppy after landing at either Wendover Air Force Base in Utah, or Fort Bliss outside El Paso, Texas.\n\nDietrich Koelle used SASSTO as the starting point for a similar development at Messerschmitt-Bölkow-Blohm in the late 1960s. Unlike Bono's version, Koelle used as much existing technology and materials as possible, while abandoning the need for the specific S-IVB sizing. The result was a slightly larger spacecraft, the Beta, that launched of payload without the use of slush fuel, advanced lightweight construction, or a real aerospike engine. As part of the Beta proposal, Koelle pointed out that even the existing S-IVB could reach orbit, with zero payload, if equipped with a high-pressure LOX/LH2 engine of 460 Isp.\n\nGary Hudson later pointed out that such an engine existed, the Space Shuttle Main Engine, using a SSME-powered S-IVB as a thought experiment to demonstrate the real-world feasibility of SSTO launchers. This study was part of his \"Phoenix\" series of launchers, all similar to the SASSTO.\n\n\n"}
{"id": "14540905", "url": "https://en.wikipedia.org/wiki?curid=14540905", "title": "Einstellung effect", "text": "Einstellung effect\n\nEinstellung is the development of a mechanized state of mind. Often called a problem solving set, Einstellung refers to a person's predisposition to solve a given problem in a specific manner even though better or more appropriate methods of solving the problem exist. The Einstellung effect is the negative effect of previous experience when solving new problems. The Einstellung effect has been tested experimentally in many different contexts. The most famous example (which led to Luchins and Luchins' coining of the term) is the Luchins' water jar experiment, in which subjects were asked to solve a series of water jar problems. After solving many problems which had the same solution, subjects applied the same solution to later problems even though a simpler solution existed (Luchins, 1942). Other experiments on the Einstellung effect can be found in \"The Effect of Einstellung on Compositional Processes\" and \"Rigidity of Behavior, A Variational Approach to the Effect of Einstellung\".\n\nEinstellung literally means \"setting\" or \"installation\" as well as a person's \"attitude\" in German. Related to Einstellung is what is referred to as an Aufgabe (literally, \"task\" in German). The Aufgabe is the situation which could potentially invoke the Einstellung effect. It is a task which creates a tendency to execute a previously applicable behavior. In the Luchins and Luchins experiment a water jar problem served as the Aufgabe, or task.\n\nAnother phenomenon similar to Einstellung is functional fixedness (Duncker 1945). Functional fixedness is an impaired ability to discover a new use for an object, owing to the subject's previous use of the object in a functionally dissimilar context. It can also be deemed a cognitive bias that limits a person to using an object only in the way it is traditionally used. Duncker also pointed out that the phenomenon occurs not only with physical objects, but also with mental objects or concepts (a point which lends itself nicely to the phenomenon of Einstellung effect).\n\nThe Einstellung effect occurs when a person is presented with a problem or situation that is similar to problems they have worked through in the past. If the solution (or appropriate behavior) to the problem/situation has been the same in each past experience, the person will likely provide that same response, without giving the problem too much thought, even though a more appropriate response might be available. Essentially, the Einstellung effect is one of the human brain's ways of finding an appropriate solution/behavior as efficiently as possible. Note, however, that although finding the solution is efficient, the solution found might not necessarily be the \"most\" appropriate solution.\n\nThe water jar test, first described in Abraham Luchins 1942 classic experiment, is a commonly cited example of an Einstellung situation. The experiment's participants were given the following problem: you have 3 water jars, each with the capacity to hold a different, fixed amount of water; figure out how to measure a certain amount of water using these jars. It was found that subjects used methods that they had used previously to find the solution even though there were quicker and more efficient methods available. The experiment shines light on how mental sets can hinder the solving of novel problems. \n\nIn Luchins experiment, subjects were divided into two groups. The experimental group was given five practice problems, followed by 4 critical test problems. The control group did not have the five practice problems. All of the practice problems and some of the critical problems had only one solution, which was \"B minus A minus 2·C.” For example, one is given Jar A capable of holding 21 units of water, B capable of holding 127, and C capable of holding 3. If an amount of 100 units must be measured out, the solution is to fill up Jar B and pour out enough water to fill A once and C twice. \n\nOne of the critical problems was called the extinction problem. The extinction problem was a problem that could not be solved using the previous solution B − A − 2C. In order to answer the extinction problem correctly, one had to solve the problem directly and generate a novel solution. An incorrect solution to the extinction problem indicated the presence of the Einstellung effect. The problems after the extinction problem again had two possible solutions. These post-extinction problems helped determine the recovery of the subjects from the Einstellung effect. \n\nThe critical problems could be solved using this solution (B − A − 2C) or a shorter solution (A − C or A + C). For example, subjects were instructed to get 18 units of water from jars with capacities 15, 39, and 3. Despite the presence of a simpler solution (A + C), subjects in the experimental group tended to give the lengthier solution in lieu of the shorter one. Instead of simply filling up Jars A and C, most subjects from the experimental group preferred the previous method of B − A − 2C, whereas virtually all of the control group used the simpler solution. When Luchins and Luchins gave experimental group subjects the warning, \"Don't be blind,\" over half of them used the simplest solution to the remaining problems. Thus, this warning helped reduce the prevalence of the Einstellung effect among the experimental group. \n\nThe results of the water jars experiment illustrates the concept of Einstellung. The majority of the experimental subjects adopted a mechanized state of mind and relied on mental sets formed through previous experience. However, the experimental subjects would have been more efficient if they had employed the direct method of solving the problem rather than applying the same solution from previous examples.\n\nThe Einstellung effect can be supported by theories of inductive reasoning. In a nutshell, inductive reasoning is the act of inferring a rule based on a finite number of instances. Most experiments on human inductive reasoning involve showing subjects a card with an object (or multiple objects, or letters, etc.) on it. The objects can vary in number, shape, size, color, etc., and the subject's job is to answer (initially by guessing) \"yes\" or \"no\" whether (or not) the card is a positive instance of the rule (which must be inferred by the subject). Over time, the subjects do tend to learn the rule, but the question is \"how\"? Kendler and Kendler (1962) proposed that older children and adults tend to exhibit \"noncontinuity theory\"; that is, the subjects tend to pick a reasonable rule and assume it to be true until it proves false. Regarding Einstellung effect, one can view noncontinuity theory as a way of explaining the tendency to maintain a specific behavior until it fails to work. In the water-jar problem, subjects generated a specific rule because it seemed to work in all situations; when they were given problems for which the same solution worked, but a better solution was possible, they still gave their \"tried and true\" response. Where theories of inductive reasoning tend to diverge from the idea of Einstellung effect is when analyzing the fact that, even after an instance where the Einstellung rule failed to work, many subjects reverted to the old solution when later presented with a problem for which it did work (again, this problem also had a better solution). One way to explain this observation is that in actuality subjects know (consciously) that the same solution might not always work, yet since they were presented with so many instances where it did work, they still tend to test that solution before any other (and so if it works, it will be the first solution found).\n\nNeurologically, the idea of synaptic plasticity, which is an important neurochemical explanation of memory, can help to understand the Einstellung effect. Specifically, Hebbian theory (which in many regards is the neuroscience equivalent of original associationist theories) is one explanation of synaptic plasticity (Hebb, 1949). It states that when two associated neurons frequently fire together – while infrequently firing apart from one another – the strength of their association tends to become stronger (making future stimulation of one neuron even more likely to stimulate the other). Since the frontal lobe is most often attributed with the roles of planning and problem solving, if there is a neurological pathway which is fundamental to the understanding of Einstellung effect, the majority of it most likely falls within the frontal lobe. Essentially, a Hebbian explanation of Einstellung could be as follows: stimuli are presented in such a way that the subject recognizes him or herself as being in a situation which he or she has been in before. That is, the subject sees, hears, smells, etc., an environment which is akin to an environment which he or she has been in before. The subject then must process the stimuli which are presented in such a way that he or she exhibits a behavior which is appropriate for the situation (be it run, throw, eat, etc.). Because neural growth is, at least in part, due to the associations between two events/ideas, it follows that the more a given stimulus is followed by a specific response, the more likely that in the future that stimulus will invoke the same response. Regarding the Luchins’ experiment, the stimulus presented was a water-jar problem (or to be more technical, the stimulus was a piece of paper which had words and numbers on it which, when interpreted correctly, portray a water-jar problem) and the invoked response was B − A − 2C. While it is a bit of a stretch to assume that there is a direct connection between a \"water-jar problem\" and \"B\" − \"A\" − 2\"C\" within the brain, it is not unreasonable to assume that the specific neural connections which are active during a water-jar problem-state and those that are active when one thinks “take the second term, subtract the first term, then subtract two of the third term” tend to increase in the amount of overlap as more and more instances where B − A − 2C works are presented.\n\nThe following experiments were designed to gauge the effect of different stressful situations on the Einstellung effect. Overall, these experiments show that stressful situations increase the prevalence of the Einstellung effect.\n\nLuchins gave an elementary-school class a set of water jar problems. In order to create a stressful situation, experimenters told the students that the test would be timed, that the speed and accuracy of the test would be reviewed by their principal and teachers, and that the test would affect their grades. To further agitate the students during the test, experimenters were instructed to comment on how much slower the children were compared to children in lower grades. The experimenters observed anxious, stressed, and sometimes tearful faces during the experiment.\n\nThe results of the experiment indicated that the stressful speed test situation increased rigidity. Luchins found that only three of the ninety-eight students tested were able to solve the extinction problem, and only two students used the direct method for the critical problems. The same experiment conducted under non-stress conditions showed 70% rigidity during the test problems and 58% failure of the extinction problem, while the anxiety-inducing situation showed 98% and 97% respectively.\n\nThe speed test was performed with college students as well, which yielded similar results. Even when college students were told ahead of time to use the direct method in order to avoid mistakes made by children, the college students continued to exhibit rigidity under time pressure. The results of these studies showed that the emphasis on speed increased the Einstellung effect on the water jar problems.\n\nLuchins also instructed subjects to draw a solution through a maze without crossing any of the maze’s lines. The maze was either traced normally or traced using the mirror reflection of the maze. If the subject drew over the lines of the figure, they had to start at the beginning, which was disadvantageous since the subject was told that their score depended on the time and smoothness of the solution. The mirror-tracing situation was the stressful situation, and the normal tracing was the non-stressful, control situation. Experimenters observed that the mirror-tracing task caused more drawing outside the boundaries, increased overt signs of stress and anxiety, and required more time to accurately complete. The mirror-tracing situation produced 89% Einstellung solution on the first two criticals instead of the 71% observed for normal tracing. In addition, 55% of the subjects failed with the mirror while only 18% failed without the mirror.\n\nIn 1951, Solomon gave both stutterers and fluent-speakers a hidden-word test, an arithmetical test, and a mirror-maze test. Experimenters called the hidden-word test a “speech test” to increase stutterer anxiety. There were no marked differences between the stutterers and the fluent-speakers for the arithmetical and mirror-maze tests. However, the results reveal a significant difference between the performance of the stutterers and the fluent-speakers on the \"speech test.\" On the first two critical problems, 58 percent of the stutterers gave Einstellung solutions whereas only 4 percent of the fluent speakers showed Einstellung effects.\n\nThe original Luchins and Luchins experiment tested nine-, ten-, eleven-, and twelve-year-olds for the Einstellung effect. The older groups showed more Einstellung effects than the younger groups in general. However, this initial study did not control for differences in educational level and intelligence. \n\nTo remedy this problem, Ross (1952) conducted a study on middle-aged (mean 37.3 years) and older adults (mean 60.8 years). The adults were grouped according to the I.Q., years of schooling, and occupation. Ross administered five Einstellung tests including the arithmetical (water jar) test, the maze test, the hidden-word test, and two other tests. For every test, the middle-aged group performed better than the older group. For example, 65% of the older adults failed the extinction task of the arithmetical test, whereas only 29% of the middle-aged adults failed the extinction problem. \n\nLuchins devised another experiment to determine the difference between Einstellung effects in children and in adults. In this study, 140 fifth-graders (mean 10.5 years) were compared to 79 college students (mean 21 years) and 21 adults (mean 43 years). Einstellung effects prior to the extinction task increased with age: the observed Einstellung effects for the extinction task were 56, 68, and 69 percent for young adults, children, and older adults respectively. This implies that there exists a curvilinear relationship between age and the recovery from the Einstellung Effect. A similar experiment conducted by Heglin in 1955, also found this relationship when the three age groups were equated for I.Q.\n\nTherefore, the initial manifestation of the Einstellung effect on the arithmetic test increases with age. However, the recovery from the Einstellung effect is greatest for young adults (average age 21 years) and decreases as you move away from this age.\n\nIn Luchins and Luchins original experiment with 483 children, they found that boys demonstrated less Einstellung effects than girls. The experimental difference was only significant for the group that was instructed to write “Don’t be blind” on their papers after the sixth problem (the DBB group). “Don’t be blind” was meant as a reminder to pay attention and guard against rigidity for the sixth problem. However, this message was interpreted in many different ways including thinking of the message as just some more words to remember. The alternative interpretations occurred more frequently in girls and increased with IQ score within the female group. This difference in interpretation of DBB may account for the fact that the male DBB group showed more direct solutions than their female counterparts.\n\nTo determine sex differences in adults, Luchins gave college students the maze Einstellung test. The female group showed slightly more (although not statistically significant) Einstellung effects than the male group. Other studies have provided conflicting data about the sex differences in the Einstellung effect.\n\nLuchins and Luchins looked at the relationship between the intelligence quotient (IQ) and the Einstellung effects for the children in their original experiment. They found that there was a statistically insignificant negative relationship between the Einstellung Effect and Intelligence. In general, large Einstellung effects were observed for all subject groups regardless of IQ score. When Luchins and Luchins looked at the IQ range for children who did and did not demonstrate Einstellung effects, they spanned from 51 to 160 and from 75 to 155 respectively. These ranges show a slight negative correlation between intelligence and Einstellung effects.\n\n\n"}
{"id": "20514516", "url": "https://en.wikipedia.org/wiki?curid=20514516", "title": "Endcapping", "text": "Endcapping\n\nIn chromatography, endcapping refers to the replacement of accessible silanol groups in a bonded stationary phase by trimethylsilyl groups.\n\nEndcapping technology prevents the tailing of a polar compound's peak and shows very high durability even with an alkaline mobile phase because of the strong film covering the stationary phase surface.\n"}
{"id": "33214442", "url": "https://en.wikipedia.org/wiki?curid=33214442", "title": "George Woodward Warder", "text": "George Woodward Warder\n\nGeorge Woodward Warder (May 20, 1848 – February 8, 1907) was a poet, philosopher and author from Missouri, USA.\n\nWarder was a student at the University of Missouri. At the age of eighteen, he was practicing law. He later became a successful lawyer with an interest in banking. In 1878, Warder moved to Kansas City, Missouri, continuing his practice of law as well as other financial enterprises. He invested in real estate and construction.\n\nWarder was also a poet and philosopher. He wrote many books, such as \"Poetic Fragments, or, College Poems\" (1873), \"Eden Dell, or, Love's Wanderings and other Poems\" (1878), \"Utopian Dreams and Lotus Leaves\" (1885) and \"After Which All Things, or, Footprints and Shadows\" (1895). \n\nWarder also developed his own cosmology theory. He authored \"The New Cosmogony\" (1898), \"Invisible Light, or Electric Theory of Creation\" (1899), \"The Cities of the Sun\" (1901),\" The Stairway to the Stars\" (1902), \"The Universe a Vast Electric Organism\" (1903). Warder believed that the universe was an electrical creation and that electricity plays a more important role in the universe than is generally accepted. His views can be seen as a predecessor to plasma cosmology.\n\n"}
{"id": "1846548", "url": "https://en.wikipedia.org/wiki?curid=1846548", "title": "International Celestial Reference System", "text": "International Celestial Reference System\n\nThe International Celestial Reference System (ICRS) is the current standard celestial reference system adopted by the International Astronomical Union (IAU). Its origin is at the barycenter of the Solar System, with axes that are intended to be \"fixed\" with respect to space. ICRS coordinates are approximately the same as equatorial coordinates: the mean pole at J2000.0 in the ICRS lies at 17.3±0.2 mas in the direction 12 h and 5.1±0.2 mas in the direction 18 h. The mean equinox of J2000.0 is shifted from the ICRS right ascension origin by 78±10 mas (direct rotation around the polar axis).\n\nThe defining extragalactic reference frame of the ICRS is the International Celestial Reference Frame (currently ICRF2) based on hundreds of extra-galactic radio sources, mostly quasars, distributed around the entire sky. Because they are so distant, they are apparently stationary to our current technology, yet their positions can be measured with the utmost accuracy by Very Long Baseline Interferometry (VLBI). The positions of most are known to 0.001 arcsecond or better, which is orders of magnitude more precise than the best optical measurements. At optical wavelengths, the ICRS is currently realized by the Hipparcos Celestial Reference Frame (HCRF), a subset of about 100,000 stars in the Hipparcos Catalogue. A more accurate optical realization of the ICRS (\"Gaia\"-CRF2), based on the observation by the \"Gaia\" spacecraft of almost 500,000 extragalactic objects believed to be quasars, is under preparation.\n\n\n\n"}
{"id": "52916384", "url": "https://en.wikipedia.org/wiki?curid=52916384", "title": "Ion interaction chromatography", "text": "Ion interaction chromatography\n\nIon interaction chromatography (ion-pair chromatography) is a laboratory technique for separating ions with chromatography. In this technique ions are mixed with ion pairing reagents (IPR) . The analyte combines with its reciprocal ion in the IRP, this corresponds to retention time. Often organic salts are selected to pair with solute(s). The formation of this pair affects the interaction of the pair with the mobile phase and the stationary phase. \n\n"}
{"id": "414736", "url": "https://en.wikipedia.org/wiki?curid=414736", "title": "Isochoric process", "text": "Isochoric process\n\nAn isochoric process, also called a constant-volume process, an isovolumetric process, or an isometric process, is a thermodynamic process during which the volume of the closed system undergoing such a process remains constant. An isochoric process is exemplified by the heating or the cooling of the contents of a sealed, inelastic container: The thermodynamic process is the addition or removal of heat; the isolation of the contents of the container establishes the closed system; and the inability of the container to deform imposes the constant-volume condition. The isochoric process here should be a quasi-static process.\n\nAn isochoric thermodynamic process is characterized by constant volume, i.e., Δ\"V\" = 0.\nThe process does no pressure-volume work, since such work is defined by\nwhere \"P\" is pressure. The sign convention is such that positive work is performed by the system on the environment.\n\nIf the process is not quasi-static, the work can perhaps be done in a volume constant thermodynamic process.\n\nFor a reversible process, the first law of thermodynamics gives the change in the system's internal energy:\n\nReplacing work with a change in volume gives\n\nSince the process is isochoric, \"dV\" = 0, the previous equation now gives\n\nUsing the definition of specific heat capacity at constant volume,\n\nIntegrating both sides yields\n\nWhere \"c\" is the specific heat capacity at constant volume, \"T\" is the initial temperature and \"T\" is the final temperature. We conclude with:\n\nOn a pressure volume diagram, an isochoric process appears as a straight vertical line. Its thermodynamic conjugate, an isobaric process would appear as a straight horizontal line.\n\nIf an ideal gas is used in an isochoric process, and the quantity of gas stays constant, then the increase in energy is proportional to an increase in temperature and pressure. Take for example a gas heated in a rigid container: the pressure and temperature of the gas will increase, but the volume will remain the same.\n\nThe ideal Otto cycle is an example of an isochoric process when it is assumed that the burning of the gasoline-air mixture in an internal combustion engine car is instantaneous. There is an increase in the temperature and the pressure of the gas inside the cylinder while the volume remains the same.\n\nThe noun \"isochor\" and the adjective \"isochoric\" are derived from the Greek words ἴσος (\"isos\") meaning \"equal\", and χῶρος (\"choros\") meaning \"space.\"\n\n"}
{"id": "10848731", "url": "https://en.wikipedia.org/wiki?curid=10848731", "title": "Jalkr (crater)", "text": "Jalkr (crater)\n\nJalkr is a bright crater on Jupiter's moon Callisto measuring 74 km across (in the lower part of the image). This an example of a central dome impact crater. A smaller degraded crater in the upper part of the image is called Audr.\n"}
{"id": "41919078", "url": "https://en.wikipedia.org/wiki?curid=41919078", "title": "José Arechavaleta", "text": "José Arechavaleta\n\nJosé Arechavaleta (27 September 1838, Urioste, near Bilbao – 16 June 1912, Montevideo) was a Spanish-born pharmacist, geologist and naturalist, active in Uruguay.\n\nAt the age of 18, he emigrated to Uruguay, where he made the acquaintanceship of botanist José Ernesto Gibert (1818–1886). In 1862 he obtained a degree in pharmacy, and later taught classes in botany, zoology and natural history at the University of the Republic in Montevideo. In 1892 he was appointed general director of the natural history museum in Montevideo, a position he kept until his death in 1912.\n\nIn Uruguay, he studied and collected insects from all parts of the country, being particularly interested in Coleoptera. As a botanist, he published a major work on grasses native to the country. He is credited with establishing bacteriology laboratories at the University Institute of Experimental Hygiene. In 1899 the plant genus \" Arechavaletaia\" was named in his honor by Carlos Luigi Spegazzini.\n\nSome botanical species were named in Arechavaleta's honor:\n"}
{"id": "40630969", "url": "https://en.wikipedia.org/wiki?curid=40630969", "title": "Karl Deutsch Award (international relations)", "text": "Karl Deutsch Award (international relations)\n\nThe Karl Deutsch Award is an award in the field of international relations to prominent scholars under 40 or within ten years of defending their doctoral dissertation. It was named after Karl Deutsch and was established in 1981 by the International Studies Association (ISA). The award is presented annually to a scholar who is judged to have made the most significant contribution to the study of International Relations and Peace Research by the means of publication.\n\nIt should not be confused with the Karl Deutsch Award awarded by the International Political Science Association, considered as the highest award in the field of comparative politics.\n\nRecipients must be a current member of ISA and under age 40, or within ten years of defending their doctoral dissertation.\n\nA $500.00 (USD) cash prize is awarded to the recipient. Previous nominees are eligible for the award so long as they meet the merit and age requirements.\n\nHere is a list of the past recipients.\n\n"}
{"id": "58112175", "url": "https://en.wikipedia.org/wiki?curid=58112175", "title": "Kids Code Jeunesse", "text": "Kids Code Jeunesse\n\nKids Code Jeunesse (KCJ) is a Canadian not for profit organization based in Montreal, Quebec which helps children in Canada have an opportunity to learn computational thinking through code. This organization, founded in 2013 by Kate Arthur, is primarily used by children, teachers, and parents with the skills needed for the modern society. KCJ wants more creatures than producers. The intent is not to replace existing courses with computer science, but rather complement them with coding. Kids Code Jeunesse uses intuitive, open source, free tools such as Scratch and Trinket. Its education materials are developed for children aged 5 to 12 years old. By targeting a younger age group, KCJ is helping boost the children's abilities in communication, collaboration, problem-solving, and creative thinking.\n\nKCJ has partnered with local, national, and international partners in order to raise awareness across Canada in both official languages.\n\nIn 2016, in partnership with Code Club U.K., KCJ licensed the rights to Code Club Canada, which runs volunteer-led Code Clubs for free across Canada. There are now over 150 Code Clubs registered, throughout every province and territory. These clubs, set up at schools, libraries and community centers for a minimum of 8 weeks host free coding activities for children aged 7-12.\n\nIn 2016, Kids Code Jeunesse, in partnership with Lighthouse Labs, embarked on a national campaign to inspire teachers to incorporate the basics of coding and computational thinking into their classrooms. From April to December 2018, KCJ will be hosting free, full-day Code Create Teach workshops which will provide K-12 educators with the tools to help them teach their students how to experiment with technology. Two workshops are planned for each Canadian province and territory allowing KCJ to reach over 1500 teachers. During the workshops, KCJ instructors provide tips and guidelines to bring coding into the classroom, as well as combine unplugged activities with hands-on coding activities. These methods gives attendees the opportunity to connect with other teachers in purposeful and learner-driven way. Following the Code Create Teach workshops, each teacher is given a free classroom kit of micro:bits, a pocket-sized programmable microcontroller designed to make learning and teaching code easy and fun. \n\nIn 2017, KCJ was contracted for CodeMTL, a project designed to KCJs coding workshops to over 65 schools in the Commission Scolaire de Montréal, Quebec's largest school board. \n\nIn January 2018, the Honourable Navdeep Bains, Canada’s Minister of Innovation, Science and Economic Development announced that Kids Code Jeunesse was one the recipients of the inaugural CanCode program. This program is part of the Canadian Government's Innovation and Skills Plan which has the stated intention to invest $50 million by March 2019 to increase the opportunities for children and teachers to master digital skills. With the funds received, Kids Code Jeunesse has been able to extend the training it provides to Canada's youth and aims to support over 70,000 children and 2000 teachers. \n\n"}
{"id": "27773710", "url": "https://en.wikipedia.org/wiki?curid=27773710", "title": "Kozyrev mirror", "text": "Kozyrev mirror\n\nA Kozyrev mirror, in Russian esoteric literature from 1990s, is a device made from aluminum (sometimes from glass, or reflecting mirror-like material) spiral shape surfaces, which, according to a non-proved hypothesis, are able to focus different types of radiation including that coming from biological objects. They are named after the famous astronomer Nikolai Aleksandrovich Kozyrev, though they were neither invented nor described by him.\n\nKozyrev mirrors were used in experiments related to extrasensory perception (ESP), conducted in the Institute of Experimental Medicine of Siberia, division of the Russian Academy of Sciences. Humans, allocated into the cylindrical spirals (usually 1.5 rotations clockwise, made of polished aluminum) allegedly experienced anomalous psycho-physical sensations, which had been recorded in the minutes of the research experiments.\n\nKozyrev mirrors were shown in a documentary on the Russian state TV channel and articles about them were published in tabloid newspapers in Russia and Ukraine but not in scientific journals.\n\nThere is a claim that during one of early experiments in the arctic village of Dixon, scientists placed an ancient symbol of Trinity into a mirror installation. Almost immediately there formed a field of force around the setup. The experiment was led by Vlail Kaznacheev MD, PhD, Academic Member of the Russian Academy of Medical Science .\n\nA 1998 Russian patent, RU2122446, \"Device for the correction of man's psychosomatic diseases\", relates to Kozyrev mirrors.\n\nIn 2014 a Czech Republic group attempted, but failed, to crowdfund 17,000-20,000 Euros for development of Kozyrev mirrors.\n"}
{"id": "50695684", "url": "https://en.wikipedia.org/wiki?curid=50695684", "title": "List of Cotabato provincial symbols", "text": "List of Cotabato provincial symbols\n\nThe following is a List of Cotabato provincial symbols. Most symbols were designated by the virtue of Provincial Ordinance No. 540 which took effect in September 1, 1914.\n"}
{"id": "10497382", "url": "https://en.wikipedia.org/wiki?curid=10497382", "title": "List of Kansas state symbols", "text": "List of Kansas state symbols\n\nThe following is a list of symbols of the U.S. state of Kansas.\n\n\n\n"}
{"id": "1590049", "url": "https://en.wikipedia.org/wiki?curid=1590049", "title": "List of TRS-80 clones", "text": "List of TRS-80 clones\n\nThe following is a list of clones of Tandy's TRS-80 model I and III home computers:\n"}
{"id": "50285537", "url": "https://en.wikipedia.org/wiki?curid=50285537", "title": "List of extinct animals of Romania", "text": "List of extinct animals of Romania\n\nMammals\nBirds\nReptiles\n\nMammals\n\nBirds\nFish\n"}
{"id": "462547", "url": "https://en.wikipedia.org/wiki?curid=462547", "title": "List of meteorologists", "text": "List of meteorologists\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "55294115", "url": "https://en.wikipedia.org/wiki?curid=55294115", "title": "List of the Mesozoic life of Wyoming", "text": "List of the Mesozoic life of Wyoming\n\nThis list of the Mesozoic life of Wyoming contains the various prehistoric life-forms whose fossilized remains have been reported from within the US state of Wyoming and are between 252.17 and 66 million years of age.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "39359993", "url": "https://en.wikipedia.org/wiki?curid=39359993", "title": "Miller's rule (optics)", "text": "Miller's rule (optics)\n\nIn optics, Miller's rule is an empirical rule which gives an estimate of the order of magnitude of the nonlinear coefficient.\n\nMore formally, it states that the coefficient of the second order electric susceptibility response (formula_1) is proportional to the product of the first-order susceptibilities (formula_2) at the three frequencies which formula_1 is dependent upon. The proportionality coefficient is known as Miller's coefficient formula_4.\n\nThe first order susceptibility response is given by:\n\nwhere:\n\nFor simplicity, we can define formula_11, and hence rewrite formula_2:\n\nThe second order susceptibility response is given by:\n\nwhere formula_16 is the first anharmonicity coefficient.\nIt is easy to show that we can thus express formula_1 in terms of a product of formula_2\n\nThe constant of proportionality between formula_1 and the product of formula_2 at three different frequencies is Miller's coefficient:\n"}
{"id": "24333655", "url": "https://en.wikipedia.org/wiki?curid=24333655", "title": "New South Wales Institute for Educational Research Award for Outstanding Educational Research", "text": "New South Wales Institute for Educational Research Award for Outstanding Educational Research\n\nThe New South Wales Institute for Educational Research Award for Outstanding Educational Research is awarded annually by the New South Wales Institute for Educational Research to recognize outstanding individuals who have produced outstanding doctoral theses in educational research. The decision to introduce the awards system was made at the Annual General Meeting of the Institute in 1971 and the awards scheme itself commenced in 1972. Rebecca Fleming writes in the history of the Institute that \"the research awards have served and continue to serve as a tangible way in which the Institute can recognize high quality educational research\". Recent awardees include Professor Anne Bamford, Mr William Chivers, Dr Arthur Michael-Kelly, Dr James Page, Dr Tai Peseta, Professor Wayne Sawyer, Dr Kerry-Ann O'Sullivan, and Dr Ruth Wajnryb.\n"}
{"id": "81983", "url": "https://en.wikipedia.org/wiki?curid=81983", "title": "Pioneer 0", "text": "Pioneer 0\n\nPioneer 0 (also known as Thor-Able 1) was a failed United States space probe that was designed to go into orbit around the Moon, carrying a television camera, a micrometeorite detector and a magnetometer, as part of the first International Geophysical Year (IGY) science payload. It was designed by the United States Air Force (USAF) as the first satellite in the Pioneer program and was one of the first attempted launches beyond Earth orbit by any country, but the rocket failed shortly after launch. The probe was intended to be called Pioneer (or Pioneer 1), but the launch failure precluded that name.\n\nThe spacecraft consisted of a thin cylindrical midsection with a squat truncated cone frustum of high on each side. The cylinder was in diameter and the height from the top of one cone to the top of the opposite cone was 76 cm. Along the axis of the spacecraft and protruding from the end of the lower cone was an solid propellant injection rocket and rocket case, which formed the main structural member of the spacecraft. Eight small low-thrust solid propellant velocity adjustment rockets were mounted on the end of the upper cone in a ring assembly which could be jettisoned after use. A magnetic dipole antenna also protruded from the top of the upper cone. The shell was composed of laminated plastic and was painted with a pattern of dark and light stripes to help regulate temperature.\n\nThe scientific instrument package had a mass of and consisted of:\n\nThe spacecraft was powered by nickel-cadmium batteries for ignition of the rockets, silver cell batteries for the television system, and mercury batteries for the remaining circuits. Radio transmission was on 108.06 MHz, a standard frequency used by satellites in the International Geophysical Year, through an electric dipole antenna for telemetry and doppler information and a magnetic dipole antenna for the television system. Ground commands were received through the electric dipole antenna at 115 MHz. The spacecraft was to be spin-stabilized at 1.8 revolutions per second, the spin direction approximately perpendicular to the geomagnetic meridian planes of the trajectory.\n\nPioneer 0 was launched on Thor missile number 127 at 12:18:00 UTC on August 17, 1958 by the United States Air Force, only 4 minutes after the scheduled launch time. It was destroyed by an explosion of the first stage of the Thor booster, 73.6 seconds after lift-off at altitude, 16 km downrange over the Atlantic Ocean. The failure was suspected to be due to a turbopump bearing that came loose, causing the liquid oxygen pump to stop. The abrupt loss of thrust caused the Thor to lose attitude control and pitch downward, which caused the LOX tank to rupture from aerodynamic loads and resulting in complete destruction of the launch vehicle. Erratic telemetry signals were received from the payload and upper stages for 123 seconds after the explosion, and the upper stages were tracked to impact in the ocean. The original plan was for the spacecraft to travel for 2.6 days to the Moon at which time a TX-8-6 solid propellant motor would fire to put it into a lunar orbit which was to nominally last for about two weeks. Air Force officials stated that they were not surprised at the failure, adding that \"it would have been more of a shock had the mission succeeded\".\n\nIt was the only mission in the Pioneer program carried out by the United States Air Force, as subsequent missions were conducted by NASA.\n\n"}
{"id": "38076364", "url": "https://en.wikipedia.org/wiki?curid=38076364", "title": "Polish Sociological Review", "text": "Polish Sociological Review\n\nThe Polish Sociological Review is a quarterly peer-reviewed academic journal published by the Polish Sociological Association. It covers diverse areas of sociology, especially social theory, social structure, social change, culture and politics in global perspective. The journal publishes articles in English.\nPublished by Polish Sociological Association.\n\nThe \"Polish Sociological Review\" is abstracted and indexed in:\nAccording to the \"Journal Citation Reports\", the journal has a 2011 impact factor of 0.071, ranking it 132nd out of 137 journals in the category \"Sociology\".\n\nEditorial board\n\nEditor in Chief\nKrzysztof Zagórski\npsr@pts.org.pl\nStatistical Editor\nKatarzyna Piotrowska\nManaging Editor\nHanna Mokrzycka\npsr@pts.org.pl\nEditorial Board\nZygmunt Bauman (University of Leeds), Zbigniew Bokszański (University of Łódź), Henryk Domański (Polish Academy of Sciences), Irina Tomescu-Dubrow (Polish Academy of Sciences, Ohio State University), Sven Eliaeson (University\nof Uppsala), Brian Green (Keen College), Michal Illner (Czech Academy of Sciences), Jarosław Kilias (University of Warsaw), Martin Krygier (University of New South Wales), Jan Kubik (Rutgers University), Joanna Kurczewska (Polish\nAcademy of Sciences), György Lengyel (Budapest University of Economic Sciences), Rachel Lovell (Case Western Reserve University), Arvydas Matulionis (Lithuanian Social Research Centre), Barbara Misztal (University of Leicester),\nJanusz Mucha (AGH University), Michał Nowosielski (Institute for Western Affairs), David Ost (Hobart William Smith Colleges), Jan Pakulski (University of Tasmania), Grażyna Skąpska (Jagiellonian University), Kazimierz M. Słomczyński\n"}
{"id": "7311044", "url": "https://en.wikipedia.org/wiki?curid=7311044", "title": "Riesel Sieve", "text": "Riesel Sieve\n\nRiesel Sieve is a distributed computing project, running in part on the BOINC platform. Its aim is to prove that 509,203 is the smallest Riesel number, by finding a prime of the form for all odd smaller than 509,203.\n\nAt the start of the project in August 2003, there were less than 509,203 for which no prime was known. , 52 of these had been eliminated by Riesel Sieve or outside persons; the largest prime found by this project is 502,573 × 2 − 1 of 2,162,000 digits, and it is known that for none of the remaining there is a prime with \"n\" < 8,000,000. (For = 342,847 and 444,637, there is even no prime with \"n\" < 10,000,000.\n\nThe project proceeds in the same way as other prime-hunting projects like GIMPS or Seventeen or Bust: sieving eliminates pairs (\"k\", \"n\") with small factors, and then a deterministic test, in this case the Lucas-Lehmer-Riesel test based on the Lucas-Lehmer test, is used to check primality of numbers without small factors. Users can choose whether to sieve or to run LLR tests on candidates sieved by other users; heavily-optimised sieving software is available.\n\nRiesel Sieve maintains lists of the primes that have been found and the \"k\" whose status is still unknown.\n\nFrom 2010 onward, the investigation has been taken over by another distributed computing project, PrimeGrid.\n\n"}
{"id": "18417310", "url": "https://en.wikipedia.org/wiki?curid=18417310", "title": "Rule complex", "text": "Rule complex\n\nA rule complex is a set consisting of rules and/or other rule complexes. This is a generalization of a set of rules, and provides a tool to investigate and describe how rules can function as values, norms, judgmental or prescriptive rules, and meta-rules. Also possible is to examine objects consisting of rules such as roles, routines, algorithms, models of reality, social relationships, and institutions. In game theory, rules and rule complexes can be used to define the behavior and interactions of the players (although in generalized game theory, the rules are not necessarily static. Rule complexes are especially associated with sociologist Tom R. Burns and Anna Gomolinska and the Uppsala Theory Circle.\n\nIn this setting, a rule is type of knowledge (in the sense of epistemic logic (see Fagin, 2003)) formalized as a set of premises or conditions, a set of justifications, and a set of conclusions (this may be written as a triple, a rule formula_1). Elements of \"X\" should hold, and of \"Y\" may hold. If \"Y\", the justifications, do not hold, then the rule cannot be applied. If \"X\", the premises, obtain and the justifications are not known to not apply, then the rule is applied, and formula_2 is concluded. If \"X\" and \"Y\" are empty, then the rule is axiomatic (a \"fact\" or unconditional directive). Thus, rules can be seen as the basic objects of knowledge.\n\nFormally, a rule complex is the class which contains all finite sets of rules, is closed under set-theoretical union and power set, and preserves inclusion:\n\n\nThis means that for rule complexes formula_8 and formula_7, formula_11 are also rule complexes. A complex formula_12 is a subcomplex of the complex formula_13 if formula_14 or formula_12 may be obtained from formula_13 by deleting some rules from formula_13 and/or redundant parentheses (Burns, 2005).\n\nsocially embedded games: A granular computing perspective. In S. K. Pal, L. Polkowski and A. Skowron (eds). \"Rough-Neural Computing: Techniques for Computing with Words\", Springer, Berlin Heidelberg, pages 411–434.\n"}
{"id": "55679432", "url": "https://en.wikipedia.org/wiki?curid=55679432", "title": "Sexual orientation, gender identity, and military service", "text": "Sexual orientation, gender identity, and military service\n\nSexual orientation, gender identity, and military service may refer to:\n"}
{"id": "13120429", "url": "https://en.wikipedia.org/wiki?curid=13120429", "title": "Sodality (social anthropology)", "text": "Sodality (social anthropology)\n\nIn social anthropology, a sodality is a non-kin group organized for a specific purpose (economic, cultural, or other), and frequently spanning villages or towns . \n\nSodalities are often based on common age or gender, with all-male sodalities more common than all-female. One aspect of a sodality is that of a group \"representing a certain level of achievement in the society, much like the stages of an undergraduate's progress through college [university]\" . \n\nIn the anthropological literature, the Mafia in Sicily has been described as a sodality . Other examples include Maasai war camps, and Crow and Cheyenne military associations, groups that were not much unlike today's Veterans of Foreign Wars or American Legion.\n\nThe term was first used with this meaning by Elman Service (no doubt drawing on the sodality vs. modality distinction used in some Christian churches), as part of his band-tribe-chiefdom-state model for the progression of political integration. It defined an organization that occurred across bands, and therefore was a part of a tribe, rather than a band, which was composed of only kin.\n\nArjun Appadurai uses the concept of sodalities to describe what he views as the collective, cultural dimension and function of the imagination given the globalization of electronic mass media and transnational migration. For Appadurai, sodalities, much like what he terms \"localities\" or \"neighborhoods\", are cultural groups or spaces that mediate globalized cultural flows and, importantly, create possibilities for \"translocal social action that would otherwise be hard to imagine\" (p.8). In other words, sodalities are generative social spaces for agency, imagination, and social action.\n\n\n3. Winick, Charles, <Dictionary of Anthropology>, Littlefield Adams Quality Paperback, 1970 \n4. Appadurai, Arjun. 1996. <Modernity at Large: Cultural Dimensions of Globalization> Minneapolis: University of Minnesota Press.\n\n"}
{"id": "17872875", "url": "https://en.wikipedia.org/wiki?curid=17872875", "title": "Spice (oceanography)", "text": "Spice (oceanography)\n\nIn oceanography the term spice refers to spatial variations in the temperature and salinity of seawater whose effects on density cancel each other. Such \"density compensated\" thermohaline variability is ubiquitous in the upper ocean. Warmer, saltier water is more spicy while cooler, less salty water is more minty. For a density ratio of 1, all the thermohaline variability is spice, and there are no density fluctuations.\n"}
{"id": "53842277", "url": "https://en.wikipedia.org/wiki?curid=53842277", "title": "Steven M. George", "text": "Steven M. George\n\nSteven M. George from the University of Colorado, Boulder, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Division of Chemical Physics in 1997, for \"advancements in our understanding of gas-surface energy transfer dynamics, surface kinetics and diffusion processes, environmental chemistry at gas-surface interfaces, heterogeneous catalysis, and chemically controlled eptiaxy of novel thin film materials.\"\n"}
{"id": "24379037", "url": "https://en.wikipedia.org/wiki?curid=24379037", "title": "Telecollaboration", "text": "Telecollaboration\n\nTelecollaboration is a form of network-based language teaching which emerged in language teaching in the 1990s. It refers to the pedagogic practice of bringing together classes of foreign language learners through computer-mediated communication for the purpose of improving their language skills, intercultural communicative competence and digital literacies. Telecollaboration, also increasingly referred to as online intercultural exchange (OIE), is recognized as a field of computer-assisted language learning as it relates to the use of technology in language learning. Outside the field of language education this type of pedagogic practice is increasingly being used to internationalize the curriculum and offer students the possibility to engage with peers in other parts of the world in collaborative online projects. Different terms are used to refer to this practice, for example virtual exchange, collaborative online international learning (COIL), and globally networked learning.\n\nTelecollaboration is based on sociocultural views of learning inspired by Vygotskian theories of learning as a social activity.\n\nGuth and Helm (2010) built on the pedagogy of telecollaboration by expanding on its traditional practices via incorporating Web 2.0 tools in online collaborative projects. This enriched practice widely became known as telecollaboration 2.0. Telecollaboration 2.0, being a completely new phase, serves to achieve nearly the same goals of telecollaboration. A distinctive feature of Telecollaboration 2.0, however, lies in prioritizing promoting the development and mastery of new online literacies. Although telecollaboration and telecollaboration 2.0 are used interchangeably, the latter slightly differs in affording \"a complex context for language education as it involves the simultaneous use and development\" of intercultural competencies, internationalize classrooms and promotes authentic intercultural communication among partnering schools/students.\n\nThere are several different 'models' of telecollaboration which have been extensively described in the literature. The first models to be developed were based on the partnering of foreign language students with \"native speakers\" of the target language, usually by organizing exchanges between two classes of foreign language students studying one another's languages. The most well established models are the eTandem and the Cultura, and eTwinning models.\n\neTandem, which developed from the face to face Tandem Learning approach, has been widely adopted by individual learners who seek partners on the many available educational websites which offer to help find partners and suggest activities for tandem partners to engage in. However, the eTandem model has also been used for class-to-class telecollaboration projects where teachers establish specific objectives, tasks, and/or topics for discussion.\nThe Teletandem model is based on eTandem and was developed in Brazil, but focuses on oral communication through VOIP tools such as Skype and Google Hangouts. Until recent years, however, telecollaboration has generally used asynchronous communication tools.\n\nThe Cultura project was developed by teachers of French as a foreign language at MIT in the late 1990s with the aim of making culture the focus of their foreign language class. This model takes its inspiration from the words of the Russian philosopher Mikhail Bakhtin: \"It is only in the eyes of another culture that foreign culture reveals itself fully and profoundly ... A meaning only reveals its depths once it has encountered and come into contact with another foreign meaning\" (as cited in Furstenberg, Levet, English, & Maillet, 2001, p. 58). Cultura is based on the notion and process of cultural comparison and entails students analysing cultural products in class with their teachers and interacting with students of the target languages and cultures through which they develop a deeper understanding of each other's culture, attitudes, representations, values, and frames of reference.\n\nThe eTwinning project, which essentially is a network of schools and educators within the European Union and part of Eramus+, contrasts with its earlier counterparts in not setting specific guidelines apropos of language use, themes or structure. This model serves as a broad platform for schools within the EU to exchange information and share materials online, and provides a virtual space for countless pedagogical opportunities where teachers and students collectively learn, communicate and collaborate using a foreign language.\nQuintessentially, eTwinning has the following four objectives: 1. setting up a collaborative network among European schools by connecting them via Web 2.0 tools; 2. encouraging educators and students to collaborate with their counterparts in other European countries; 3. fostering a learning environment in which European identity is integrated with multilingualism and multiculturalism; 4. continuously developing educators' professional skills \"in the pedagogical and collaborative use of ICT\". eTwinning has thus proven to be a strong model for telecollaboration in recent years, since it enables the authentic use of foreign language among virtual partners, i.e. teachers and students. Not surprisingly, eTwinning projects have become increasingly recognized at various educational institutions across the continent.\nEach of the telecollaborative models discussed above has its strengths and weaknesses:\n"}
{"id": "7989919", "url": "https://en.wikipedia.org/wiki?curid=7989919", "title": "Thermomechanical analysis", "text": "Thermomechanical analysis\n\nThermomechanical analysis (TMA) is a technique used in thermal analysis, a branch of materials science which studies the properties of materials as they change with temperature.\n\nThermomechanical analysis is a subdiscipline of the thermomechanometry (TM) technique.\n\nThermomechanometry is the measurement of a change of a dimension or a mechanical property of the sample while it is subjected to a temperature regime. An associated thermoanalytical method is thermomechanical analysis. A special related technique is thermodilatometry (TD), the measurement of a change of a dimension of the sample with a negligible force acting on the sample while it is subjected to a temperature regime. The associated thermoanalytical method is thermodilatometric analysis (TDA).\n\nTDA is often referred to as zero force TMA. The temperature regime may be heating, cooling at a rate of temperature change that can include stepwise temperature changes, linear rate of change, temperature modulation with a set frequency and amplitude, free (uncontrolled) heating or cooling, or maintaining a constant increase in temperature. The sequence of temperatures with respect to time may be predetermined (temperature programmed) or sample controlled (controlled by a feedback signal from the sample response).\n\nThermomechanometry includes several variations according to the force and the way the force is applied.\n\nStatic force TM (sf-TM) is when the applied force is constant; previously called TMA with TD as the special case of zero force.\n\nDynamic force TM (df-TM) is when the force is changed as for the case of a typical stress–strain analysis; previously called TMA with the term dynamic meaning any alteration of the variable with time, and not to be confused with dynamic mechanical analysis (DMA).\n\nModulated force TM (mf-TM) is when the force is changed with a frequency and amplitude; previously called DMA. The term modulated is a special variant of dynamic, used to be consistent with modulated temperature differential scanning calorimetry (mt-DSC) and other situations when a variable is imposed in a cyclic manner.\n\nMechanical testing seeks to measure mechanical properties of materials using various test specimen and fixture geometries using a range of probe types.\n\nMeasurement is desired to take place with minimal disturbance of the material being measured. Some characteristics of a material can be measured without disturbance, such as dimensions, mass, volume, density. However, measurement of mechanical properties normally involves disturbance of the system being measured.\n\nThe measurement often reflects the combined material and measuring device as the system. Knowledge of a structure can be gained by imposing an external stimulus and measuring the response of the material with a suitable probe. The external stimulus can be a stress or strain, however in thermal analysis the influence is often temperature.\n\nThermomechanometry is where a stress is applied to a material and the resulting strain is measured while the material is subjected to a controlled temperature program. The simplest mode of TM is where the imposed stress is zero. No mechanical stimulus is imposed upon the material, the material response is generated by a thermal stress, either by heating or cooling.\n\nZero force TM (a variant of sf-TM or TD) measures the response of the material to changes in temperature and the basic change is due to activation of atomic or molecular phonons. Increased thermal vibrations produce thermal expansion characterized by the coefficient of thermal expansion (CTE) that is the gradient of the graph of dimensional change versus temperature.\n\nCTE depends upon thermal transitions such as the glass transition. CTE of the glassy state is low, while at the glass transition temperature (Tg) increased degrees of molecular segmental motion are released so CTE of the rubbery state is high. Changes in an amorphous polymer may involve other sub-Tg thermal transitions associated with short molecular segments, side-chains and branches. The linearity of the sf-TM curve will be changed by such transitions.\n\nOther relaxations may be due to release of internal stress arising from the non-equilibrium state of the glassy amorphous polymer. Such stress is referred to as thermal aging. Other stresses may be as a result of moulding pressures, extrusion orientation, thermal gradients during solidification and externally imparted stresses.\n\nSemi-crystalline polymers are more complex than amorphous polymers, since the crystalline regions are interspersed with amorphous regions. Amorphous regions in close association to the crystals or contain common molecules as tie molecules have less degrees of freedom than the bulk amorphous phase. These immobilised amorphous regions are called the rigid amorphous phase. CTE of the rigid amorphous phase is expected to be lower than that of the bulk amorphous phase.\n\nThe crystallite are typically not at equilibrium and they may contain different polymorphs. The crystals re-organize during heating so that they approach the equilibrium crystalline state. Crystal re-organization is a thermally activated process. Further crystallization of the amorphous phase may take place. Each of these processes will interfere with thermal expansion of the material.\n\nThe material may be a blend or a two-phase block or graft copolymer. If both phases are amorphous then two Tg will be observed if the material exists as two phases. If one Tg is exhibited then it will be between the Tg of the components and the resultant Tg will likely be described by a relationship such as the Flory-Fox or Kwei equations.\n\nIf one of the components is semi-crystalline then the complexity of a pure crystalline phase and either one or two amorphous phases will result. If both components are semi-crystalline then the morphology will be complex since both crystal phases will likely form separately, though with influence on each other.\n\nCross-linking will restrict the molecular response to temperature change since degree of freedom for segmental motions are reduced as molecules become irreversibly linked. Cross-linking chemically links molecules, while crystallinity and fillers introduce physical constraints to motion. Mechanical properties such as derived from stress-strain testing are used to calculate crosslink density that is usually expressed as the molar mass between crosslinks (Mc).\n\nThe sensitivity of zero stress TMA to crosslinking is low since the structure receives minimum disturbance. Sensitivity to crosslinks requires high strain such that the segments between crosslinks become fully extended.\n\nZero force TM will only be sensitive to changes in the bulk that are expressed as a change in a linear dimension of the material. The measured change will be the resultant of all processes occurring as the temperature is changed. Some of the processes will be reversible, others irreversible, and others time-dependent. The methodology must be chosen to best detect, distinguish and resolve the thermal expansion or contractions observable.\n\nThe TM instrument need only apply sufficient stress to keep the probe in contact with the specimen surface, but it must have high sensitivity to dimensional change. The experiment must be conducted at a temperature change rate slow enough for the material to approach thermal equilibrium throughout. While the temperature should be the same throughout the material it will not necessarily be at thermal equilibrium in the context of molecular relaxations.\n\nThe temperature of the molecules relative to equilibrium is expressed as the fictive temperature. The fictive temperature is the temperature at which the unrelaxed molecules would be at equilibrium.\n\nTM is sufficient for zero stress experiments since superimposition of a frequency to create a dynamic mechanical experiment will have no effect since there is no stress other than a nominal contact stress. The material can be best characterized by an experiment in which the original material is first heated to the upper temperature required, then the material should be cooled at the same rate, followed by a second heating scan.\n\nThe first heating scan provides a measure of the material with all of its structural complexities. The cooling scan allows and measures the material as the molecules lose mobility, so it is going from an equilibrium state and gradually moving away from equilibrium as the cooling rate exceeds the relaxation rate. The second heating scan will differ from the first heating scan because of thermal relaxation during the first scan and the equilibration achieved during the cooling scan. A second cooling scan followed by a third heating scan can be performed to check on the reliability of the prior scans. Different heating and cooling rates can be used to produce different equilibrations. Annealing at specific temperatures can be used to provide different isothermal relaxations that can be measured by a subsequent heating scan.\n\nThe sf-TM experiments duplicate experiments that can be performed using differential scanning calorimetry (DSC). A limitation of DSC is that the heat exchange during a process or due to the heat capacity of the material cannot be measured over long times or at slow heating or cooling rates since the finite quantity of heat exchanges will be dispersed over too long a time to be detected. The limitation does not apply to sf-TM since the dimensional change of the material can be measured over any time. The constraint is the practical time for the experiment. The application of multiple scans is shown above to distinguish reversible from irreversible changes. Thermal cycling and annealing steps can be added to provide complex thermal programs to test various attributes of a material as more becomes known about the material.\n\nModulated temperature TM (mt-TM) has been used as an analogous experiment to modulated-temperature DSC (mtDSC). The principle of mt-TM is similar to the DSC analogy. The temperature is modulated as the TM experiment proceeds. Some thermal processes are reversible, such as the true CTE, while others such as stress relief, orientation randomization and crystallization are irreversible within the conditions of the experiment. The modulation conditions should be different from mt-DSC since the sample and test fixture and enclosure is larger thus requiring longer equilibration time. mt-DSC typically uses a period of 60 s, amplitude 0.5-1.0 °C and average heating or cooling rate of 2 °C·min-1. MT-TMA may have a period of 1000 s with the other parameters similar to mt-DSC. These conditions will require long scan times.\n\nAnother experiment is an isothermal equilibration where the material is heated rapidly to a temperature where relaxations can proceed more rapidly. Thermal aging can take several hours or more under ideal conditions. Internal stresses may relax rapidly. TM can be used to measure the relaxation rates and hence characteristic times for these events, provides they are within practical measurements times available for the instrument. Temperature is the variable that can be changed to bring relaxations into measurable time ranges.\n\nTable 1. Typical zero-stress thermomechanometry parameters\n\nCreep and stress relaxation measures the elasticity, viscoelasticity and viscous behaviour of materials under a selected stress and temperature. Tensile geometry is the most common for creep measurements. A small force is initially imparted to keep the specimen aligned and straight. The selected stress is applied rapidly and held constant for the required time; this may be 1 h or more. During application of force the elastic property is observed as an immediate elongation or strain. During the constant force period the time dependent elastic response or viscoelasticity, together with the viscous response, result in further increase in strain.\n\nThe force is removed rapidly, though the small alignment force is maintained. The recovery measurement time should be four times the creep time, so in this example the recovery time should be 4 h. Upon removal of the force the elastic component results in an immediate contraction. The viscoelastic recovery is exponential as the material slowly recovers some of the previously imparted creep strain. After recovery there is a permanent unrecovered strain due to the viscous component of the properties.\n\nAnalysis of the data is performed using the four component viscoelastic model where the elements are represented by combinations of springs and dashpots. The experiment can be repeated using different creep forces. The results for varying forces after the same creep time can be used to construct isochronal stress–strain curves. The creep and recovery experiment can be repeated under different temperatures. The creep–time curves measured at various temperatures can be extended using the time-temperature-superposition principle to construct a creep and recovery mastercurve that extends the data to very long and very short times. These times would be impractical to measure directly. Creep at very long timeframes is important for prediction of long term properties and product lifetimes. A complementary property is stress relaxation, where a strain is applied and the corresponding stress change is measured. The mode of measurement is not directly available with most thermomechanical instruments. Stress relaxation is available using any standard universal test instruments, since their mode of operation is application of strain, while the stress is measured.\n\nExperiments where the force is changed with time are called dynamic force thermomechanometry (df-TM). This use of the term dynamic is distinct from the situation where the force is periodically changed with time, typically following a sine relationship, where the term modulated is recommended. Most thermomechanical instruments are force controlled, that is they apply a force, then measure a resulting change in a dimension of the test specimen. Usually a constant strain rate is used for stress–strain measurements, but in the case of df-TM the stress will be applied at a chosen rate.\n\nThe result of a stress-strain analysis is a curve that will reveal the modulus (hardness) or compliance (softness, the reciprocal of modulus). The modulus is the slope of the initial linear region of the stress–strain curve. Various ways of selecting the region to calculate gradient are used such as the initial part of the curve, another is to select a region defined by the secant to the curve. If the test material is a thermoplastic a yield zone may be observed and a yield stress (strength) calculated. A brittle material will break before it yields. A ductile material will further deform after yielding. When the material breaks a break stress (ultimate stress) and break strain are calculated. The area under the stress–strain curve is the energy required to break (toughness).\n\nThermomechanical instruments are distinct in that they can measure only small changes in linear dimension (typically 1 to 10 mm) so it is possible to measure yield and break properties for small specimens and those that do not change dimensions very much before exhibiting these properties.\n\nA purpose of measuring a stress–strain curve is to establish the linear viscoelastic region (LVR). LVR is this initial linear part of a stress–strain curve where an increase in stress is accompanied by a proportional increase in strain, that is the modulus is constant and the change in dimension is reversible. A knowledge of LVR is a prerequisite for any modulated force thermomechanometry experiments. Conduct of complex experiments should be preceded by preliminary experiments with a limited range of variables to establish the behaviour of the test material for selection of further instrument configuration and operating parameters.\n\nModulated temperature conditions are where the temperature is changed in a cyclic manner such as in a sine, isothermal-heating, isothermal-cooling or heat-cool. The underlying temperature can increase, decrease or be constant. Modulated temperature conditions enable separation of the data into reversing data that is in-phase with the temperature changes, and non-reversing that is out-of-phase with the temperature changes. Sf-TM is required since the force should be constant while the temperature is modulated, or at least constant for each modulation period.\n\nA reversing properties is coefficient of thermal expansion. Non-reversing properties are thermal relaxations, stress relief and morphological changes that occur during heating, causing the material to approach thermal equilibrium.\n\n"}
