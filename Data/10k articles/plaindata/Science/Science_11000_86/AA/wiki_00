{"id": "45449734", "url": "https://en.wikipedia.org/wiki?curid=45449734", "title": "Adolphe Hercule de Graslin", "text": "Adolphe Hercule de Graslin\n\nAdolphe Hercule de Graslin (11 April 1802, Chateaux de Malitourne, Flée, Sarthe –\n31 May 1882, Malitourne) was a French entomologist.\n\nAdolphe Hercule de Graslin specialised in Lepidoptera. He was a founding Member of the Société Entomologique de France. His collection was acquired by Charles Oberthür.\n\nWith Jean Alphonse Boisduval and Jules Pierre Rambur \"Collection iconographique et historique des chenilles; ou, Description et figures des chenilles d'Europe, avec l'histoire de leurs métamorphoses, et des applications à l'agriculture\", Paris, Librairie encyclopédique de Roret, 1832 ( in Gallica & in Google Livres).\n\n"}
{"id": "48701676", "url": "https://en.wikipedia.org/wiki?curid=48701676", "title": "Amazon Basin (sedimentary basin)", "text": "Amazon Basin (sedimentary basin)\n\nThe Amazon Basin is a major large sedimentary basin located roughly at the middle and lower course of the Amazon River, south the Guiana Shield and north of the Central Brazilian Shield. It is bound to the west by the Púrus Arch, separating the Amazon Basin from the Solimões Basin and in the east by the Gurupá Arch, separating the basin from the Marajó Basin. The basin developed on a rift that originated possibly about 550 million years ago during the Cambrian. Parts of the rift were reactivated during the opening of the South Atlantic.\n\nThe basin has an elongated shape with a WSW-ENE orientation. It long axis runs from the vicinity of Manaus to the area near the confluence of Xingu River with the Amazon River.\n"}
{"id": "6296791", "url": "https://en.wikipedia.org/wiki?curid=6296791", "title": "Astrarium", "text": "Astrarium\n\nAn astrarium, also called a planetarium, is the mechanical representation of the cyclic nature of astronomical objects in one timepiece. It is an astronomical clock.\n\nThe first astraria were mechanical devices. Archimedes is said to have used a primitive version that could predict the positions of the Sun, the Moon, and the planets. On May 17, 1902, an archaeologist named Valerios Stais discovered that a lump of oxidated material, which had been recovered from a shipwreck near Antikythera, held within it a mechanism with cogwheels. This mechanism, known as the Antikythera mechanism, was recently redated to end of the 2nd century BCE. Extensive study of the fragments, using X-rays, has revealed enough details (gears, pinions, crank) to enable researchers to build partial replicas of the original device. Engraved on the major gears are the names of the planets, which leaves little doubt as to the intended use of the mechanism.\n\nBy the end of the Roman Empire, the know-how and science behind this piece of clockwork was lost.\n\nThe first modern documented astrarium clock was completed in 1364 by Giovanni de' Dondi (1318–1388), a scholar and physician of the Middle Ages interested in astronomy and horology. The original clock, consisting of 107 wheels and pinions, has been lost, perhaps during the sacking of Mantua in 1630, but de' Dondi left detailed descriptions which have survived, enabling a reconstruction of the clock. It displays the mean time, sidereal, (or star), time and the motions of the sun, moon and the five then-known planets Venus, Mars, Saturn, Mercury, and Jupiter. It was conceived according to a Ptolemaic conception of the solar system. De' Dondi was inspired by his father Jacopo who designed the astronomical clock in the Piazzi dei Signori, Padua, in 1344 - one of the first of its type.\n\nIn later ages more astraria were built. A famous example is the one built in 1774 by Eise Eisinga from Dronrijp, Friesland, the Netherlands. It displayed all the planets and was fixed to the ceiling in a house in Franeker where it can still be visited.\n\nIn modern times, the astrarium has grown into a tourist attraction as a commercially exploited planetarium-showing in IMAX theaters. With such presentations as 'The history of the Universe', as well as other astronomical phenomena.\n\n\n\n"}
{"id": "508390", "url": "https://en.wikipedia.org/wiki?curid=508390", "title": "Brownian motor", "text": "Brownian motor\n\nBrownian motors are nano-scale or molecular devices by which thermally activated processes (chemical reactions) are controlled and used to generate directed motion in space and to do mechanical or electrical work. These tiny engines operate in an environment where viscosity dominates inertia, and where thermal noise makes moving in a specific direction as difficult as walking in a hurricane: the forces impelling these motors in the desired direction are minuscule in comparison with the random forces exerted by the environment. Because this type of motor is so strongly dependent on random thermal noise, Brownian motors are feasible only at the nanometer scale.\n\nThe term \"Brownian motor\" was originally coined by Peter Hänggi in 1995: A distinct feature of a Brownian motor is—in contrast to a molecular motor—that the output response is typically coupled only loosely to the input perturbation and action of fluctuations.\n\nIn biology, many protein-based molecular motors in the cell may in fact be Brownian motors. These molecular motors convert the chemical energy present in ATP into mechanical energy. One example of a Brownian motor would be an ATPase motor that hydrolyzes ATP to generate fluctuating anisotropic energetic potentials. The anisotropic potentials along the path would bias the motion of a particle (like an ion or polypeptide); the result would essentially be diffusion of a particle whose net motion is strongly biased in one direction. The translocation of the particle would only be loosely coupled to hydrolysis of ATP.\n\nThe dynamics and activity of Brownian motors are current topics of study in theoretical and experimental biophysics. Brownian motors are sometimes modeled using the Fokker-Planck equation or with Monte Carlo methods. Many researchers are presently engaged in understanding how molecular-scale motors operate in environments with non-negligible thermal noise. The thermodynamics of such motors is constrained by the ramifications of the Fluctuation Theorems, Pumping Quantization Theorems, and Pumping-Restriction Theorems. \n\n\n"}
{"id": "4409068", "url": "https://en.wikipedia.org/wiki?curid=4409068", "title": "C. R. Narayan Rao", "text": "C. R. Narayan Rao\n\nC.R. Narayan Rao (15 August 1882 – 2 January 1960) was an Indian zoologist and herpetologist. He was among the founding editors of the journal \"Current Science\". In recognition of his pioneering work on Indian amphibians, the genus \"Raorchestes\" was named after him.\n\nBorn in Coimbatore, he studied in Bellary and at the Madras Christian College under Professor Henderson who headed the department of zoology. After obtaining his graduate and post-graduate degrees and a gold medal for proficiency, he obtained a diploma in teaching. He taught in Coimbatore and Ernakulam, before moving to the Central College in Bangalore where he organized the department of zoology and headed it until his retirement in 1937.\n\nHis role in science and research is considered significant since he was involved in the integration of research into university education. Along with Sir Martin Onslow Forster and other Indian scientists he helped found the journal \"Current Science\" in July 1932 along the lines of the journal \"Nature\". He was its first editor. In one of his first editorials, he pleaded for the coordination of scientific activities in India, a plea that helped create the Indian Academy of Sciences.\n\nProfessor Rao specialized on frogs and their taxonomy. He named and described several frog species, and his work on the Archenteric and Segmentation Cavities of frogs are regarded as important contributions to our understanding of amphibian development. He described the new Microhylid genus \"Ramanella\". The genus Raorchestes is named in his honour.\n\nProfessor Rao presided over the zoology section of the Indian Science Congress in 1938 at Lahore. His account of the ovarian ovum of the slender loris was presented to the Royal Society by James Peter Hill in the latter's Croonian Lecture.\n\n\n"}
{"id": "34322150", "url": "https://en.wikipedia.org/wiki?curid=34322150", "title": "Charles James (chemist)", "text": "Charles James (chemist)\n\nCharles James (27 April 1880 – 10 December 1928) was a chemist of British origin working in the United States. After studying under William Ramsay at University College London, he joined the New Hampshire College of Agriculture and the Mechanic Arts (now the University of New Hampshire). He became a professor and head of the chemistry department, separating and identifying rare-earth elements by fractional precipitation and crystallization. He isolated element 71, later named lutetium.\n\nIn 1999 the American Chemical Society recognized Charles James's work in chemical separations as a National Historic Chemical Landmark.\n\n "}
{"id": "14794138", "url": "https://en.wikipedia.org/wiki?curid=14794138", "title": "Chippie", "text": "Chippie\n\nChippie (\"hr2-Computermagazin\") was a German radio program. It was one of the first programs on computer topics, produced by the Hessischer Rundfunk (Hessian Broadcasting).\n\n\"Chippie\" started in 1990. At first it was broadcast together with the youth magazine \"Radio unfrisiert\", who won the Civis media prize that year. Later it got its own one-hour slot. The show was hosted by Claudia Bultje and Patrick Conley. Topics on the program were, for example: \"Computer in Theater, Opera and Rock Concert\" (2 May 1992), \"Computer and Money\" (5 September 1992), \"Computers and Sex\" (24 October 1992) and \"Data Networks\" (2 July 1994).\n\nThe first computer magazine in German radio was \"Bit, byte, gebissen\" (BR, 1985). Today well-known programs are the \"Chaosradio\" (RBB) and \"Matrix\" (ORF).\n\n\n"}
{"id": "28244703", "url": "https://en.wikipedia.org/wiki?curid=28244703", "title": "Cnidariologist", "text": "Cnidariologist\n\nA cnidariologist is a zoologist specializing in Cnidaria, a group of freshwater and marine aquatic animals that include the sea anemones, corals, and jellyfish.\n"}
{"id": "51911983", "url": "https://en.wikipedia.org/wiki?curid=51911983", "title": "Cognitive Theory of Inquiry Teaching", "text": "Cognitive Theory of Inquiry Teaching\n\nThe Cognitive Theory of Inquiry Teaching, also referred to as the Cognitive Theory of Interactive Teaching, was developed by Allan Collins and Albert L. Stevens (Collins & Stevens, 1981). Allan Collins was a chief scientist at Bolt Beranek and Newman Inc., a research firm in Cambridge Massachusetts. He is also a specialist in the field of cognitive science and human semantic processing. Albert L. Stevens was a senior scientist at Bolt Beranek and Newman Inc. He was also director of the company's Artificial Intelligence, Education Technology and Training Systems Division. He is also a specialist in cognitive science. (Reigeluth, 1983)\nThe Cognitive Theory of Inquiry Teaching according to Collins and Stevens (1981) requires the learner to construct theories and principles through dialogue, the teaching of self-questioning techniques and the teaching of metacognitive or self-monitoring skills, all with the intent of clarifying misconceptions so the theory or principle is well articulated and developed. The essence of the cognitive theory of Inquiry teaching is that of developing students' metacognitive skills. Inquiry teaching deliberately attempts to develop these stills through instruction.\n\nThe theory is a prescriptive model rooted in the discovery tradition and cognitive sciences. It was derived form an analysis of the transcripts of teachers, described as interactive teachers, using a variety of teaching strategies. These strategies were in some way related to one of the following methodology: the inquiry method of the teaching, discovery method of teaching and Socratic method of teaching. The transcripts studied represent a variety of topics taught by teachers across different subject areas (Reigeluth, 1983). Collins and Stevens believed that their Cognitive Theory of Inquiry Teaching is domain independent or that it can be applied across subject areas or the curriculum.\n\nInquiry teaching is rooted in the didactic methodologies of the ancient Greek. According Mayer and Alexander (2011), inquiry teaching is rooted in the didactic methodology of the ancient Greek, where the teacher poses a problem and assists the student in solving that problem by asking a series of question. They have also pointed out that this method of instruction can be seen in the works of Plato and Socrates.\nBransford, Franks and Sherwood (1989), as cited by Mayer and Alexander (2011), have indicated that it was not until the 1960s that the role of teachers started to change from where the teacher provided the student with the questions as well as the answers. After the 1960s, instruction provided greater opportunity for students to engage in creating their own answers to questions posed (Mayer & Alexander, 2011).\n\nThe purpose of the Cognitive Theory of Inquiry Teaching according to Collins (1986) is to provide learners with the opportunity to “actively engage in articulating theories and principles that are critical to deep understanding of a domain” (p.1). Collins (1986) believes that the knowledge attained will allow the learner to actively and meaningfully engage in solving problems and making predictions. He also indicated that the knowledge acquire is not simply content knowledge. Figure 1 summarizes Collins and Stevens Theory of Inquiry Teaching.\n\nThe Cognitive Theory of Inquiry (Interactive) Teaching according to the Reigeluth (1983) consists of three parts: \n\nThere are two main goals teachers using the inquiry method seek to achieve, according to Collin and Stevens (1981): \n\nThese two main goals are associated with several sub-goals. The list below identifies the two main goals and their associated sub-goal as posited by Collins and Stevens (1981):\n\nCollins and Stevens (1981) have highlighted ten teaching strategies used by teachers using inquiry teaching. They believed that these ten teaching strategies, listed below, are the most important ones identified in their investigation. These ten teaching strategies identified by Collin and Stevens (1981, p. 18) are:\nColling and Stevens (1981) indicated that the time allocated between the teachers' goals and sub-goals is critical to the effectiveness of inquiry teaching. Collins and Stevens (1977), as cited by Collins and Stevens (1981) had made an attempt at developing a theory of controlling structures after getting feedback from several tutors. This controlling structure theory consisted of four parts. The four parts this theory includes the following: \nCollins and Stevens (1981) have suggested that base on the goals of the teacher, cases are selected to ensure that student develop mastery of the principles or theories being taught. They have also indicated that teachers appear to develop some strategies for selecting cases to develop students' mastery of the principles or theories. These are some of the strategies used by teachers for selecting cases as mentioned by Collins and Stevens (1981):\nAccording to Collins and Stevens the controlling structure used to govern the dialogue in the Cognitive Theory of Inquiry Teaching is demonstrated in the following manner:\nCollins and Stevens (1977), as cited in Collins and Stevens (1981), have identified how priorities have been set by teachers. Priorities have been set based on the following:\nCriticism of the Cognitive Theory of Inquiry Teaching\nReigeluth (1983) has put forward the following criticisms about Cognitive Theory of Inquiry:\n\nThe Collins and Stevens Cognitive Theory of Inquiry Teaching, despite the short coming identified by Reigeluth (1983), provides some strategies for teaching high-order thinking skills. Reigeluth (1983) also points out that the Collins and Stevens Cognitive Theory of Teaching provides some strategies for instruction that other theories have overlooked. Strategies such as considering alternative prediction, entrapping students, tracing consequences to a contradiction and questioning authority are invaluable skill in critical thinking. \nThe focus of education today is to develop learners who are independent critical thinkers. There was once the belief that critical thinking skills could not be taught and that these skills develop naturally independent of instruction. The Collins and Stevens Cognitive Theory of Inquiry of Teaching indicate that learners can be taught strategies to develop and apply critical thinking skills.\nDewey (1938), as cited by McGregor (2007), pointed out that if students are provided with factual information without the cognitive skills that would enable them to understand, appreciate, transfer and connect ideas, the information gained would become meaningless in the future. It implies therefore, that instruction should seek to nurture the development of critical thinking skill.\nDewey (1910) as cited by McGregor (2007), has also indicated that there is a connection between thinking and learning. He posited that \"thoughts involved in developing ideas and understandings are often assumed and implicated. To develop thoughtful learning these processes need to be more explicit and connected to the processes of coming to know and understand\" (p. 47).\n\n\n\n"}
{"id": "5236980", "url": "https://en.wikipedia.org/wiki?curid=5236980", "title": "Criminal investigation", "text": "Criminal investigation\n\nCriminal investigation is an applied science that involves the study of facts, used to identify, locate and prove the guilt of an accused criminal. A complete criminal investigation can include searching, interviews, interrogations, evidence collection and preservation and various methods of investigation. Modern-day criminal investigations commonly employ many modern scientific techniques known collectively as forensic science.\n\nCriminal investigation is an ancient science that may have roots as far back as c. 1700 BCE in the writings of the Code of Hammurabi. In the code it is suggested that both the accuser and the accused had the right to present evidence they collected. In the modern era criminal investigations are most often done by government police forces. Private investigators are also commonly hired to complete or assist in criminal investigations.\n\nAn early recorded professional criminal investigator was the English constable. Around 1250 CE it was recorded that the constable was to \"...record...matters of fact, not matters of judgment and law.\"\n\nAfter observing recent changes in the demographic composition of particular crimes during higher priority of their investigation, such as the increase of the percentage of women convicted for joining and fighting for terrorist organizations from a very low percentage (similar to those of murder and rape) to almost as many women as men (women in the 40-50 percent range in some jurisdictions) as the priority of investigating terror crimes increased, certain criminologists are expressing the notion that there may be more crimes that would change their demographics if they got higher priority. These criminologists theorize that in the case of limited budgets, criminal investigators rely on profiled and statistical likelihood of particular groups of people being convicted for the type of crimes that are being investigated, and ignore complaints that are filed about people that they consider less likely to commit the crimes or give the tracking or individual matching to the evidence lower priority. According to the hypotheses, even a minimal or nonexistent difference in the likelihood of committing crimes can be hidden behind a difference of a factor by many multiples in the likelihood of being convicted due to self-fulfilling prophecies in the statistics. These criminologists feel that criminals who are not getting caught due to being profiled as unlikely offenders are a major problem. Some of these criminologists propose an increased number of police officers. Others argue that investigations of the evidence are more expensive than police patrols and that not all crimes can be investigated, suggesting that profiling of criminal psychology should be replaced with randomized priorities of individual suspects within similar types of crime. The latter criminologists also argue that such randomization would not only fight hidden crimes by exposing the currently unsuspected criminals to the risk of being punished, but also that the abolition of profiling by forensic psychology and forensic psychiatry would be a monetary saving that could be used for investigation of technical evidence, tracking of criminals who are hiding and other investigation work that can reduce the need to ignore complaints for budget reasons.\n"}
{"id": "50740871", "url": "https://en.wikipedia.org/wiki?curid=50740871", "title": "David Leigh Clark", "text": "David Leigh Clark\n\nDavid L. Clark is a paleontologist. He was the W.H. Twenhofel Professor of Geology and Geophysics at the University of Wisconsin–Madison, Madison, Wisconsin.\n\nIn 1972, he described the conodont genus \"Neostreptognathodus\".\n\nIn 2001, he received the Raymond C. Moore Medal which is awarded by the Society for Sedimentary Geology to persons who have made significant contributions in the field which have promoted the science of stratigraphy by research in paleontology.\n\nThe conodont genus name \"Clarkina\" and species name \"Streptognathodus clarki\" are tributes to David Leigh Clark.\n\n\n\n"}
{"id": "22454273", "url": "https://en.wikipedia.org/wiki?curid=22454273", "title": "De Natura Fossilium", "text": "De Natura Fossilium\n\nDe Natura Fossilium is a scientific text written by Georg Bauer also known as Georgius Agricola, first published in 1546. The book represents the first scientific attempt to categorize minerals, rocks and sediments since the publication of Pliny's \"Natural History\". This text along with Agricola's other works including \"De Re Metallica\" compose the earliest comprehensive \"scientific\" approach to mineralogy, mining, and geological science.\n\n\n"}
{"id": "42362243", "url": "https://en.wikipedia.org/wiki?curid=42362243", "title": "Duncan G. Steel", "text": "Duncan G. Steel\n\nDuncan G. Steel (born 1951) is an American experimental physicist, researcher and professor in quantum optics in condensed matter physics. He is the Robert J. Hiller Professor of Electrical Engineering, Professor of Physics, Professor of Biophysics, and Research Professor in the Institute of Gerontology at the University of Michigan. Steel is also a Guggenheim Scholar and a Fellow of American Physical Society, the Optical Society of America, and the Institute of Electrical and Electronics Engineers. He coedited the five-volume series on the Encyclopedia of Modern Optics.\n\nSteel graduated from the University of North Carolina, Chapel Hill with an A.B. in 1972. Graduated from the University of Michigan with a Ph.D. in 1976.\nPrior to joining the faculty at Michigan, he was a Member of the Technical Staff and Senior Staff Physicist for the Hughes Aircraft Company at the Hughes Research Labs (HRL) in Malibu. There he worked on optical phase conjugation and real time holography. Working with Richard Lind, they demonstrated the first laser with a phase conjugate mirror using degenerate four-wave mixing.\n\nSteel works on nonlinear optical spectroscopy and coherent control of semiconductor heterostructures, for which he received 2010 Frank Isakson Prize for Optical Effects in Solids from the American Physical Society. His research focus is on using ultrafast optical techniques to manipulate electron spins embedded in semiconductor quantum dots to create quantum coherence as a new degree of freedom. Some of his publications describe the first demonstration of an optically driven CNOT gate in a solid state device and demonstration of entanglement in semiconductor system. In addition to semiconductor physics, he is also involved in the development and application of advanced laser spectroscopy methodology and other biophysical techniques for the study of protein folding and dynamics.\n\n"}
{"id": "33853457", "url": "https://en.wikipedia.org/wiki?curid=33853457", "title": "Earth system governance", "text": "Earth system governance\n\nEarth system governance is a recently developed paradigm that builds on earlier notions of environmental policy and nature conservation, but puts these into the broader context of human-induced transformations of the entire earth system. It conceptualizes the system of formal and informal rules, rule-making mechanisms and actor-networks at all levels of human society (from local to global) that are set up to steer societies towards preventing, mitigating, and adapting to global and local environmental change and earth system transformation, within the normative context of sustainable development.\n\nThe notion of governance refers to forms of steering that are less hierarchical than traditional governmental policy-making (even though most modern governance arrangements will also include some degree of hierarchy), rather decentralized, open to self-organization, and inclusive of non-state actors that range from industry and non-governmental organizations to scientists, indigenous communities, city governments and international organizations.\n\nThe integrative new paradigm of earth system governance has evolved into an active research area that brings together a variety of social science disciplines including political science, sociology, economics, ecology, policy studies, geography, sustainability science, and law.\n\nMajor international conferences on ‘Earth System Governance’ have been held in Amsterdam (2007, 2009), Berlin (2008, 2010), Colorado (2011), Lund (2012, 2017), Tokyo (2013), Norwich (2014), Canberra (2015) and Nairobi (2016). In 2017, the 8th Annual Earth System Governance Conference took place in Lund, Sweden. This conference was co-hosted by Lund University during its 350 year celebration. In 2018 it is due to be held in Utrecht, The Netherlands.\n\nOn 16–19 May 2011, more than twenty Nobel Laureates, several leading policy-makers and some of the world’s most renowned thinkers and experts on global sustainability met for the Third Nobel Laureate Symposium on Global Sustainability at the Royal Swedish Academy of Sciences in Stockholm. The Nobel Laureate Symposium concluded with the Stockholm Memorandum, calling for \"strengthening of Earth System Governance\" as a priority for coherent global action. This memorandum has been submitted to the High-level Panel on Global Sustainability appointed by the UN Secretary General and fed into the preparations for the 2012 UN Conference on Sustainable Development (Rio+20).\n\nThe new paradigm of earth system governance was originally developed in the Netherlands by Professor Frank Biermann in his inaugural lecture at the VU University Amsterdam, which was published later in 2007 Based on this pioneering contribution, Biermann was invited by the International Human Dimensions Programme on Global Environmental Change to develop a long-term comprehensive international programme in this field, which became in 2009 the global Earth System Governance Project.\n\nKey researchers who have applied the earth system governance framework in their work include Michele Betsill, John Dryzek, Peter M. Haas, Norichika Kanie, Lennart Olsson, and Oran Young. In 2011, Lund University appointed Biermann as guest professor of Earth System Governance, making him the worldwide first chair holder in this rapidly developing field of research.\n\nIn 2009, the UN-sponsored global change research networks have set up a long-term research programme in earth system governance, the Earth System Governance Project.\nThe Earth System Governance Project currently consists of a network of ca. 300 active and about 2,300 indirectly involved scholars from all continents, and is the largest social science research network in the area of governance and global environmental change. The Earth System Governance Project is essentially a scientific effort, but also aims to assist policy responses to the pressing problems of global environmental change. The International Project Office of the Earth System Governance Project is based at Lund University, Sweden.\n\nResearch centres on ‘Earth System Governance’ have been set up or designated at VU University Amsterdam; the Australian National University; Chiang Mai University; Colorado State University; Lund University; University of East Anglia; University of Oldenburg; the Stockholm Resilience Centre; the University of Toronto; the Tokyo Institute of Technology and Yale University. In addition, strong networks on earth system governance research exist in China, Latin America, Central and Eastern Europe, and Russia.\n\nMIT Press launched in 2009 a new book series on Earth System Governance. Recent papers drawing on the paradigm of integrated earth system governance research have analysed issues as diverse as river basin management in Hungary, deforestation policies in the Amazon, and climate change adaptation in Australia.\n\nThe Earth System Governance Project builds on a conceptual framework that is organized into five analytical problems. The five analytical problems identified in the Science and Implementation Plan of the Earth System Governance Project are: \n\n\n\n\n"}
{"id": "56959327", "url": "https://en.wikipedia.org/wiki?curid=56959327", "title": "Elsasser number", "text": "Elsasser number\n\nThe Elsasser number, Λ, is a dimensionless number in magnetohydrodynamics that represents the ratio of magnetic forces to the Coriolis force.\n\nformula_1\n\nwhere σ is the conductivity of the fluid, \"B\" is the magnetic field, ρ is the density of the fluid, and Ω is the rate of rotation of the body.\n\n"}
{"id": "3962658", "url": "https://en.wikipedia.org/wiki?curid=3962658", "title": "Epsilon 15", "text": "Epsilon 15\n\nEpsilon 15 (or ε15) is a virus, specifically a bacteriophage, known to infect species of \"Salmonella\" bacteria including \"Salmonella anatum\".\n\nThe virus is a short, tailed phage with a double-stranded DNA genome of 39,671 base pairs and 49 open reading frames.\n"}
{"id": "47319644", "url": "https://en.wikipedia.org/wiki?curid=47319644", "title": "Error level analysis", "text": "Error level analysis\n\nError level analysis is the analysis of compression artifacts in digital data with lossy compression such as JPEG.\n\nWhen used, lossy compression is normally applied uniformly to a set of data, such as an image, resulting in a uniform level of compression artifacts.\n\nAlternatively, the data may consist of parts with different levels of compression artifacts. This difference may arise from the different parts having been repeatedly subjected to the same lossy compression a different number of times, or the different parts having been subjected to different kinds of lossy compression. A difference in the level of compression artifacts in different parts of the data may therefore indicate that the data has been edited.\n\nIn the case of JPEG, even a composite with parts subjected to matching compressions will have a difference in the compression artifacts.\n\nIn order to make the typically faint compression artifacts more readily visible, the data to be analyzed is subjected to an additional round of lossy compression, this time at a known, uniform level, and the result is subtracted from the original data under investigation. The resulting difference image is then inspected manually for any variation in the level of compression artifacts. In 2007, N. Krawetz denoted this method \"error level analysis\".\n\nAdditionally, digital data formats such as JPEG sometimes include metadata describing the specific lossy compression used. If in such data the observed compression artifacts differ from those expected from the given metadata description, then the metadata may not describe the actual compressed data, and thus indicate that the data have been edited.\n\nBy its nature, data without lossy compression, such as a PNG image, cannot be subjected to error level analysis. Consequently, since editing could have been performed on data without lossy compression with lossy compression applied uniformly to the edited, composite data, the presence of a uniform level of compression artifacts does not rule out editing of the data.\n\nAdditionally, any non-uniform compression artifacts in a composite may be removed by subjecting the composite to repeated, uniform lossy compression. Also, if the image color space is reduced to 256 colors or less, for example, by conversion to GIF, then error level analysis will generate useless results.\n\nMore significant, the actual interpretation of the level of compression artifacts in a given segment of the data is subjective, and the determination of whether editing has occurred is therefore not robust.\n\nIn May 2013, Dr Neal Krawetz used error level analysis on the 2012 World Press Photo of the Year and concluded on his Hacker Factor blog that it was \"a composite\" with modifications that \"fail to adhere to the acceptable journalism standards used by Reuters, Associated Press, Getty Images, National Press Photographer's Association, and other media outlets\". The World Press Photo organizers responded by letting two independent experts analyze the image files of the winning photographer and subsequently confirmed the integrity of the files. One of the experts, \nHany Farid, said about error level analysis that \"It incorrectly labels altered images as original and incorrectly labels original images as altered with the same likelihood\". Krawetz responded by clarifying that \"It is up to the user to interpret the results. Any errors in identification rest solely on the viewer\".\n\nIn May 2015, the citizen journalism team Bellingcat wrote that error level analysis revealed that the Russian Ministry of Defense had edited satellite images related to the Malaysia Airlines Flight 17 disaster. In a reaction to this, image forensics expert Jens Kriese said about error level analysis: \"The method is subjective and not based entirely on science\", and that it is \"a method used by hobbyists\". On his Hacker Factor Blog, the inventor of error level analysis Neal Krawetz criticized both Bellingcat's use of error level analysis as \"misinterpreting the results\" but also on several points Jens Kriese's \"ignorance\" regarding error level analysis.\n\n\n"}
{"id": "7402585", "url": "https://en.wikipedia.org/wiki?curid=7402585", "title": "Expo Science Park", "text": "Expo Science Park\n\nExpo Park is a science park in Yuseong-gu to the north of Daejeon, South Korea, built for Taejŏn Expo '93. Facilities at Expo Park included a garden, amusement park, and observation tower. While the tower and some buildings remain, most of the expo buildings have now been removed. As of 2018, the site is hosting the headquarters complex for the Institute for Basic Science.\n"}
{"id": "23100977", "url": "https://en.wikipedia.org/wiki?curid=23100977", "title": "Farinograph", "text": "Farinograph\n\nIn baking, a farinograph measures specific properties of flour. It was first developed and launched in 1928. The farinograph is a tool used for measuring the shear and viscosity of a mixture of flour and water. The primary units of the farinograph are Brabender Units, an arbitrary unit of measuring the viscosity of a fluid. The farinograph is a variation of the Brabender Plastograph that has been specialized for the baking industry, and it is used around the world for the objective measurement of a variety of flours.\n\nA baker can formulate end products by using the farinograph's results to determine the following:\n\nThe farinograph is drawn on a curved graph with the vertical axis labeled in Brabender Units (BU) and the horizontal axis labeled as time in minutes. The graph is generally hockey-stick shaped, with the curve being more or less acute depending on the strength of the gluten in the flour.\nThe points of interest on the graph are fivefold:\n\nThe Farinograph is used worldwide by bakers and food technicians in building bakery formulations. The farinograph gives bakers a good snapshot of the flour's properties and how the flour will react in different stages of baking, which helps them pick a certain flour for any given purpose. Millers use the Brabender Farinograph to access the properties of the flour, to ascertain whether changes need to be made in the mill. The miller also uses the farinograph to prepare dough for further testing for extensibility after a resting period (akin to proofing) with the Brabender Extensograph.\n\nThe industrial application of these five points is far reaching. A baker may use, for example, the arrival time as a bare minimum time when planning full product floor time for a batch of dough. A baker may also use MTI as guideline to judge the response of a dough to the addition of other ingredients. Peak time may be used as a target mix time for optimal gluten structure and resilience. Stability may be used as a method of determining desired cell structure before irreparable gluten breakdown occurs.\n\n\n"}
{"id": "25556210", "url": "https://en.wikipedia.org/wiki?curid=25556210", "title": "Frederik Johnstrup", "text": "Frederik Johnstrup\n\nProfessor Johannes Frederik Johnstrup (12 March 1818, in Christianshavn, Denmark – 31 December 1894) was a Danish geologist and paleontologist. He was the founder of Meddelelser om Grønland.\n\nJohnstrup received his Bachelor of Science in 1844. He became an associate professor of mineralogy and natural science at the Sorø Academy in 1846. When the academy closed in 1848, he became assistant lecturer in Kolding. Three years later, he taught in Sorø and in 1866, he became professor of mineralogy and geology at the University of Copenhagen and the Polytechnic Institution.\n\nIn 1876, he led an expedition to Iceland to study Askja and the volcanoes at Mývatn with Þorvaldur Thoroddsen as his guide.\n\nIn 1878, with Heinrich Rink, Johnstrup founded the governmental institution, Commission for the Direction of Geological and Geographical Investigations in Greenland. Ten years later, he led the Danish Geological Survey. He developed Denmark's Mineralogical Museum, and led the construction of a new building in 1893.\n\nJohnstrup authored a number of treatises by which he greatly promoted the understanding of Denmark's geological conditions, particularly the glacial phenomena. In 1894, he became an honorary doctorate at the University.\n"}
{"id": "13592918", "url": "https://en.wikipedia.org/wiki?curid=13592918", "title": "Functional prerequisites", "text": "Functional prerequisites\n\nIn sociological research, functional prerequisites are the basic needs (food, shelter, clothing, and money) that an individual requires to live above the poverty line. Functional prerequisites may also refer to the factors that allow a society to maintain social order.\n\nOn the other hand, Parsons argued any successful social system has four functional prerequisites:\n\nAdaptation – To survive, any society needs the basics of food and shelter. Having these gives any society control over its environment. A society needs a functioning economy to provide this.\n\nGoal attainment – all societies must provide collective goals of some sort for its members to aspire to. Governments set goals such as New Labour setting a target of 50% of school graduates to attend university. To facilitate meeting such goals, governments provide resources, laws, and other institutional mechanisms.\n\nIntegration – all societies need a legal system that mediates conflict and protects the social system from breaking down.\n\nPattern maintenance – Institutions like education and the family reaffirm essential values needed for society to function. (For Parsons the key institution in passing on such basic values is religion.)\n"}
{"id": "56807771", "url": "https://en.wikipedia.org/wiki?curid=56807771", "title": "Glossary of speciation", "text": "Glossary of speciation\n\nThis is a glossary of terms used in speciation research and related evolutionary disciplines. It is intended as introductory material and a structured organization of the often complex language used in the literature. Related glossaries in biology are: the glossary of biology, glossary of genetics, glossary of ecology, and the glossary of botany.\n"}
{"id": "56145541", "url": "https://en.wikipedia.org/wiki?curid=56145541", "title": "Hans Capel", "text": "Hans Capel\n\nHans Willem Capel (born 3 June 1936) is a Dutch theoretical physicist. He was a professor of theoretical physics at the University of Amsterdam between 1983 and 1998.\n\nCapel was born in Sceaux, France. He obtained his PhD in mathematics and physics at Leiden University in 1965 with a dissertation titled: \"The hole-equivalence principle, the Van Vleck relation and the application to the theory of d-ions in Ligand fields\". Capel subsequently was a lector of theoretical physics at same the university in 1970, and served as professor between 1979 and 1983.\n\nCapel was elected a member of the Royal Netherlands Academy of Arts and Sciences in 1990.\n\n"}
{"id": "1591646", "url": "https://en.wikipedia.org/wiki?curid=1591646", "title": "Huchra's lens", "text": "Huchra's lens\n\nHuchra's lens is the lensing galaxy of the Einstein Cross (\"Quasar 2237+30\"); it is also called \"ZW 2237+030\" or \"QSO 2237+0305 G\". It exhibits the phenomenon of gravitational lensing that was postulated by Albert Einstein when he realized that gravity would be able to bend light and thus could have lens-like effects. The galaxy is named for astronomer John Huchra.\n"}
{"id": "2093602", "url": "https://en.wikipedia.org/wiki?curid=2093602", "title": "Info Exame", "text": "Info Exame\n\nInfo Exame (or simply \"Info\", stylized as \"INFO\") was a high-popular Brazilian technology magazine. Its name was \"Exame Informática\" initially, a reference to its creation as the technology supplement for \"Exame\", a business magazine. It was popular with technology non-experts because of its simple-language approach to IT-related topics. The headquarters was in Sao Paulo.\n\nBesides the magazine, the INFO brand was a reference in digital tendencies, sciences, culture and entrepreneurism. It is the inspiration for those who want to understand – in an instigating and good-humored way – the impact innovations have on our lives and our business.\n\nThe magazine ceased its publication in August 2015, becoming again a supplement in the \"Exame\" magazine. Its website now redirects to Exame website technology section. \n\n\nIs a special magazine's team destined to test the last-generation products bought by the magazine before their market release.\nIt is a system used by the laboratory's team to classify the products reviewed. This grade system was announced in 2002 and it was the official grade system until the end of the magazine:\nOn December of every year the magazine makes a special edition with 30 pages or more only with reviews about the products that will be released in the following year.\nHere is a list of the worldly best-selling products that passed through \"Info Laboratory\" before getting huge fame:\n\n"}
{"id": "39647494", "url": "https://en.wikipedia.org/wiki?curid=39647494", "title": "International Society for Forensic Genetics", "text": "International Society for Forensic Genetics\n\nThe International Society for Forensic Genetics - ISFG is an international non-profit scientific society founded in 1968. The main goal of the society is to advance the field of forensic genetics, also termed DNA profiling, through dissemination of scientific results and opinions, communication amongst scientists and education. The bi-annual international ISFG congresses, international workshops and seminars, the society’s scientific journal (\"\"), and the scientific recommendations on current topics all work towards this goal. The society’s website contains up to date information on all activities.\n\nThe International Society for Forensic Genetics - ISFG – was founded in 1968 in Mainz, Germany, under the name \"Gesellschaft für forensische Blutgruppenkunde\" (Society for Forensic Haemogenetics). The society was founded as a non-profit organisation according to German civil law. The original aim of the society was to promote the science of genetic markers in human blood for use in forensic science. In 1989, the society was transformed to an international society (‘International Society for Forensic Haemogenetics - ISFH’). In 1991, based on the transition from traditional serological markers in blood to universal DNA polymorphisms the name was changed into ‘International Society for Forensic Genetics’. Currently the ISFG strives to support all research interests in forensic genetics, also including non-human DNA studies, RNA-based test systems, and large-scale sequencing technologies.\n\nIn May 2015, the ISFG had approximately 1,200 individual members from more than 50 countries. The members are typically in academic institutions, criminal justice and police organizations, as well as private companies. The members’ fields of expertise include forensic biology, molecular genetics, population genetics, blood group serology, forensic pathology, parentage testing, biostatistics, criminal law, medical ethics.\n\nThe ISFG organises biennial international congresses, with educational workshops preceding the congress. This is achieved with the help of local forensic institutes or universities that will form a local organizing committee and host the congress under the joint leadership of the ISFG board and the local congress president. From 1985-2005, conference volumes with short articles based on these presentations were originally published as books under the title \"Advances in Forensic Haemogenetics\" and later as \"Progress in Forensic Genetics\".\n\nSince 2007, the proceedings have been published electronically as part of the \"Forensic Science International: Genetics Supplement Series\" and can be accessed online. Past ISFG meetings have been held mainly in Europe but also in cities on other continents such as New Orleans and San Francisco in the US, Buenos Aires in Argentina, and Melbourne, Australia. The ISFG meetings often serve as a focal point for a dialogue of relevant scientific issues, as in the 2007 Single-nucleotide polymorphism (SNP) panel discussion, or the debate on the limits of low template DNA in forensic genetics.\n\nThe DNA Commission of the ISFG functions as an international DNA expert advisory group and is formed based on emerging needs when dealing with new DNA technologies. For each topic the ISFG board will invite scientists with specific expertise and form a commission to discuss open issues and formulate recommendations that provide guidance to forensic geneticists. While not binding, these recommendations are a first step to establishing standards for new genetic typing methods. Topics have included best practices for paternity and relationship testing. ISFG recommendations are a valuable tool for forensic geneticists and as such are highly cited by other scientists.\n\nShortly after the implementation of polymerase chain reaction (PCR) based short tandem repeat (STR) DNA testing, the ISFG was instrumental in the standardization of allele designations, which was a key component in inter-laboratory data comparability and the creation of national DNA databases. DNA commission topics also have included best practices for paternity and relationship testing now integrated into forensic genetics textbooks. All published recommendations of the DNA and the Paternity Testing Commissions can be accessed openly on the ISFG website.\n\nThe ISFG has language-based working groups for Chinese, English, French, German, Italian, Korean and Spanish-Portuguese members. They meet regularly and typically work on topics of regional or national interest. The English Speaking Working Group (ESWG) offers an annual exercise for paternity testing laboratories. The Relationship Testing Workshop is open to all members of the ESWG and each year, blood samples, a questionnaire and a paper challenge are sent to the participating laboratories.\n\nThe Spanish and Portuguese Speaking Working Group of the ISFG (GHEP-ISFG) ) was founded in 1989 and has been actively supporting and coordinating the development of forensic DNA typing in Spain, Portugal and Latin America. The GHEP focuses on quality and scientific methodology and has organised many collaborative exercises on a variety of topics and shared the results with the scientific community through several publications. GHEP has also established a proficiency testing program, which as of December 2014 has been accredited by ENAC under the standard ISO/IEC 17043: 2010.\n\nThe European DNA Profiling Group (EDNAP) was established in 1988 by scientists from European countries. The initial purpose of EDNAP was to harmonize DNA technologies for criminal investigations so that DNA results could be exchanged across the borders in Europe. In 1991, the group was included among the working groups of the ISFG. It consists of approximately 20 European laboratories and collaborates closely with the DNA Working Group of the European Network of Forensic Science Institutes (ENFSI).\n\nEDNAP organises collaborative exercises in order to explore the possibility of standardization of new forensic genetic methods. The results of the exercises are published in peer-reviewed journals. One valuable resource resulting from these activities is the \"EDNAP mitochondrial DNA Population Database\", short EMPOP, which is being maintained by the Institute for Legal Medicine of the Innsbruck Medical University, Austria. This database allows for the determination of the statistical weight of evidence for forensic mitochondrial DNA-typing results.\n\nThe general assembly of ISFG members has appointed a number of distinguished honorary members, among them Erik Essen-Möller, Alsbäck/Lysekil, Sweden; Ruth Sanger, London, UK; Otto Prokop, Berlin, Germany; Konrad Hummel, Freiburg, Germany; Sir Alec Jeffreys, Leicester, UK.\n\n"}
{"id": "44051448", "url": "https://en.wikipedia.org/wiki?curid=44051448", "title": "List of Martin Gardner Mathematical Games columns", "text": "List of Martin Gardner Mathematical Games columns\n\nOver a period of 24 years (January 1957 – December 1980), Martin Gardner wrote 288 consecutive \"Mathematical Games\" columns for \"Scientific American\" magazine. Subsequently, he alternated with other authors, producing 9 more columns under that title (February 1981 – June 1986), for a total of 297. These are listed in chronological order below.\n\nThe subjects of twelve of the columns provided the cover art for the magazine in the month they were published. These columns are indicated in the table below by the word [cover] and a link to the associated cover.\n\nGardner wrote 5 other articles for \"Scientific American\". His flexagon article in December 1956 was in all but name the first article in the series of \"Mathematical Games\" columns and led directly to the series which began the following month. These five articles are listed below.\n\n"}
{"id": "1166103", "url": "https://en.wikipedia.org/wiki?curid=1166103", "title": "List of Microsoft codenames", "text": "List of Microsoft codenames\n\nMicrosoft codenames are given by Microsoft to products it has in development, before these products are given the names by which they appear on store shelves. Many of these products (new versions of Windows in particular) are of major significance to the IT community, and so the terms are often widely used in discussions prior to the official release. Microsoft usually does not announce a final name until shortly before the product is publicly available. It is not uncommon for Microsoft to reuse codenames a few years after a previous usage has been abandoned.\n\nThere has been some suggestion that Microsoft may move towards defining the real name of their upcoming products earlier in the product development lifecycle so as to avoid needing product codenames.\n\n"}
{"id": "489397", "url": "https://en.wikipedia.org/wiki?curid=489397", "title": "List of U.S. state mammals", "text": "List of U.S. state mammals\n\nA state mammal is the official mammal of a U.S. state as designated by a state's legislature. Many states also have separately officially designated state animals, state birds, state fish, state butterflies, and state reptiles. States similarly have state flowers, state trees and state songs.\n\nKey: Years in parentheses denote the year of adoption by the state's legislature.\n\n"}
{"id": "4269016", "url": "https://en.wikipedia.org/wiki?curid=4269016", "title": "List of flag names", "text": "List of flag names\n\nThis is an incomplete list of the names and nicknames of flags, organized in alphabetical order by flag name. Very few flags have any truly official names, but some unofficial names are so widely used that they are accepted as a flag's universal name. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "47227785", "url": "https://en.wikipedia.org/wiki?curid=47227785", "title": "List of lakes in South Carolina", "text": "List of lakes in South Carolina\n\nThis is a list of lakes and reservoirs in the state of South Carolina in the United States. All major lakes in South Carolina are man-made.\n\n\n\n"}
{"id": "51019874", "url": "https://en.wikipedia.org/wiki?curid=51019874", "title": "List of open formats", "text": "List of open formats\n\nAn open format is a file format for storing digital data, defined by a published specification usually maintained by a standards organization, and which can be used and implemented by anyone. For example, an open format can be implemented by both proprietary and free and open source software, using the typical software licenses used by each. In contrast to open formats, closed formats are considered trade secrets. Open formats are also called free file formats if they are not encumbered by any copyrights, patents, trademarks or other restrictions (for example, if they are in the public domain) so that anyone may use them at no monetary cost for any desired purpose.\n\nOpen formats (in alphabetical order) include:\n\n\n\n\n\n\n\n\n"}
{"id": "52695063", "url": "https://en.wikipedia.org/wiki?curid=52695063", "title": "List of things named after Eduard Heine", "text": "List of things named after Eduard Heine\n\nEduard Heine (16 March 1821, Berlin – October 1881, Halle) was a German mathematician in Prussia. His name is given to several mathematical concepts that he was instrumental in developing:\n\n"}
{"id": "42517096", "url": "https://en.wikipedia.org/wiki?curid=42517096", "title": "List of things named after Lord Rayleigh", "text": "List of things named after Lord Rayleigh\n\nLord Rayleigh (John William Strutt, 3rd Baron Rayleigh, 12 November 1842 – 30 June 1919) was a British physicist and mathematician who is the source of the following terms:\n\n\n\n\n\n"}
{"id": "26833159", "url": "https://en.wikipedia.org/wiki?curid=26833159", "title": "Meker–Fisher burner", "text": "Meker–Fisher burner\n\nA Meker–Fisher burner, or Meker burner, is a laboratory burner that produces multiple open gas flames, used for heating, sterilization, and combustion. It is used when laboratory work requires a hotter flame than attainable using a Bunsen burner, or used when a larger-diameter flame is desired, such as with an inoculation loop or in some glassblowing operations. The burner was introduced by French chemist Georges Méker in an article published in 1905.\n\nThe Meker–Fisher burner heat output can be in excess of per hour (about 3.5 kW) using LP gas. Flame temperatures of up to are achievable. Compared with a Bunsen burner, the lower part of its tube has more openings with larger total cross-section, admitting more air and facilitating better mixing of air and gas. The tube is wider, and its top is covered with a plate mesh, which separates the flame into an array of smaller flames with a common external envelope, ensures uniform heating, and also preventing flashback to the bottom of the tube, which is a risk at high air-to-fuel ratios and limits the maximal rate of air intake in a Bunsen burner. The flame burns without noise, unlike the Bunsen or Teclu burners.\n\n"}
{"id": "56200257", "url": "https://en.wikipedia.org/wiki?curid=56200257", "title": "Michael C. Mitchell", "text": "Michael C. Mitchell\n\nMichael C. Mitchell (born January 4, 1946 in Portland, Oregon) is an American planner, designer, lecturer and environmentalist. His work focuses primarily on the planning and design of destinations, attractions, leisure and rural development.\n\nMCM Group is an international planning and design firm headquartered in Los Angeles. Founded in 1984 by Michael C. Mitchell after the close of the Los Angeles Olympic Games, where he served as the head of planning and operations, the firm has sought to expand those planning techniques as a model to address prominent social problems. MCM Group provides feasibility consulting, planning, architecture, landscape design and sustainable engineering services. Mitchell has developed offices in Tokyo, Moscow, Middle East offices in Doha, Qatar, an African base in Nairobi, Kenya and currently four offices in China, with its headquarters in Beijing.\n\nAt Portland State University, Mitchell became one of the organizers of the First Earth Day in 1970, coordinating universities throughout America's northwest states. After his work on the First Earth Day, he was one of ten university students selected from across the nation by President Richard Nixon's Administration to form a national Youth Advisory Board on environmental matters, S.C.O.P.E (Student Council on Pollution and the Environment) was assigned to the U.S. Department of Interior where Mitchell was a reviewer on the creation of the first Environmental Impact Statement (EIS).\n\nMitchell continued his work with what became the United States Environmental Protection Agency(EPA), writing an environmental education program for students\n\nIn the early 1980s Mitchell was recruited by the Los Angeles Olympic Organizing Committee, where he served as the Group Vice-President of Planning and Control (Finance). Among his duties included overseeing the planning of the Olympic venues and supervising the architectural department's venue planning. During the Olympics he was responsible for the Games Operations Center and oversaw the closeout of the Games after their completion.\n\nHe has since served as a senior planning consultant to six other Olympic Games and four World Fairs.\n\nAs head of the close-out operations after the completion of the Los Angeles 1984 Summer Olympics, Mitchell oversaw the creation of the LA84 Foundation which was formed out of the $225 million surplus from the operations of the Games. The Foundation is now a national leader in supporting youth programs, providing recreation and learning opportunities to disadvantaged youth, training youth coaches and convening national conferences on youth sports issues.\n\nIn the Spring of 1985, Mitchell was contacted by Bob Geldof, an English rock musician, that had been working on issues of drought and famine in Africa. Geldof asked Mitchell to produce a worldwide televised music show to raise funds to help alleviate the catastrophic consequences of the worst African famine in a century.\n\nMitchell became the Executive Producer of the worldwide Live Aid broadcast (under a newly formed venture Worldwide Sports and Entertainment) and President of the Live Aid Foundation in America.\n\nThe July 13, 1985 broadcast was the world's first large globally interactive show seen by 1.5 billion viewers in 150 countries. Whereas the 1984 Olympics utilized three satellites to beam from one location around the world, Live Aid utilized thirteen satellites sending and receiving concerts from seven locations from around the world and producing one international feed back to the 150 nations. Despite 1985 being at the height of the Cold War, Mitchell established a global broadcast with a live concert from the Soviet Union featuring Autograph, and a delayed Live Aid showing in China.\n\nPresident Ronald Reagan's Administration supported the Live Aid Foundation by providing wheat from America's reserves and awarded Mitchell a Presidential Citation for the Live Aid Foundation's contributions to humanity.\n\nMitchell continued his contributions to social and education programs by accepting an appointment to the Board of the National Education Association's Foundation for the Improvement of Education (NFIE), serving on the board from 1987 – 1997. Since its beginning in 1969, the Foundation has served as a laboratory of learning, offering funding and other resources to public school educators, their schools, and districts to solve complex teaching and learning challenges\n\nDuring the dissolution of the Soviet Union starting in 1990, Russia and Ukraine experienced a severe shortage of medical and food supplies. Working throughout both countries witnessing first-hand the growing crisis, Mitchell and his close friend, Yankel Ginzburg, an American artist and humanitarian, who had family in Tver, Russia, responded to requests by Russia's leadership for assistance, co-founding the \"Fund for Democracy and Development\" to provide aid to alleviate the crisis.\n\nMitchell served as the founding board chairman in 1991 and L. Ronald Scheman (co-founder of the Pan American Development Foundation, where his work included providing financial assistance to low-income rural communities), served as the first President. Past President Richard M. Nixon served as the honorary chairman of the Fund.\n\nFrom 1991 to 1994 the Fund is credited with channeling 240 million dollars worth of staples and food supplies to the former Soviet Union. As gratitude for the contributions of the Fund, the Russian government commissioned a monument park to reflect American goodwill.\n\nWith offices established in Moscow and St. Petersburg, Mitchell contributed to several rural development and environmental projects across the former Soviet Union. Mitchell's planning of development projects in rural Russia included work in Siberia on sustainable resource and forest management practices.\n\nWhile undertaking those projects in conjunction with local wildlife scientists Mitchell convinced the Prime Minister of Russia, Viktor Chernomyrdin, to establish the Amur Tiger Sanctuary in 1993, which was initially funded through the Global Survival Network (GSN), an environmental organization he co-founded with Steve Galster now of Freeland Foundation\n\nThe Sanctuary included introducing armed ranger patrols to stop the threat that poachers played in the region. The initial work that Mitchell and the Executive Director of GSN, Steve Galster, did to establish the sanctuary was soon funded by the World Wildlife Fund (WWF), now known as World Wide Fund for Nature, and is currently carried out with the support of the Ministry of Natural Resources and Environment (Russia). As a result of this work, the wild Siberian Tiger population has rebounded from their critical endangered level.\n\nIn order to strengthen the Sanctuary efforts to stop poaching, Mitchell worked with Steve Galster conducting undercover video interviews with the poachers. Through these undercover meetings, he and Galster discovered a link between animal poachers and human traffickers. What began as an effort to preserve habitat became an international exposé on trafficking. From 1995-1997 they undertook a two-year undercover investigation personally holding meetings with traffickers and trafficked women to expose the international relationship between animal and human trafficking.\n\nInformation and undercover video derived from their investigation were used to create a GSN written report, \"Crime & Servitude\" and a video documentary, \"Bought & Sold.\" The film was released in 1997 and received widespread media coverage in the US and abroad, including specials on ABC Primetime Live, CNN, and BBC.\n\nThe documentary also helped to catalyze legislative reform on trafficking as well as new financial resources to address the problem.\n\nGalster took what was learned during that undercover period and continues this work, founding the Freeland Foundation, which is the lead implementing partner of Asia's Regional Response to Endangered Species Trafficking (ARREST), a program sponsored by the U.S. government in partnership with ASEAN and over fifty governmental and non-governmental organizations.\n\nThe material that was collected during those two years is housed at the Human Rights Documentation Initiative (HRDI), The University of Texas at Austin\n\nBeginning in 1985, Mitchell began an association with Irving Sarnoff, the Executive Director of Friends of the United Nations (FOTUN), and his co-Founder, Dr. Noel Brown, Director of the United Nations Environmental Program (UNEP), North America. The Friends of the United Nations is an NGO dedicated to advocating support for programs of the United Nations.\n\nAs part of their work on international social issues Mitchell was asked to create a celebration for the United Nations International Day for Tolerance in 1999. The International Day for Tolerance is an annual observation declared by UNESCO in 1995 to generate public awareness of the dangers of intolerance.\n\nMitchell organized the 1999 event honoring Mikhail Gorbachev, former leader of the Soviet Union and Arnold Schwarzenegger, actor, politician and Chairman of the USC Schwarzenegger Institute of State and Global Policy. Keynote speakers included John Kerry, U.S. Senator and U.S. Secretary of State.\n\nOne of the first projects integrating agricultural development, sustainability, community and social values, and economic growth was in a region of Qingdao, China where his company, MCM Group, brought international blueberry agricultural experts to develop what is considered now one of the world's largest blueberry farms (The Qingdao Cangma Mountain Development). The project included hi-technology organic agriculture, agritourism, educational programs, local culture and residential development to provide the local rural community with a successful economic transition.\n\nInvited by universities in the U.S., China, South Korea and Japan, he has given lectures and planning studios, sharing his professional experience with students and faculty members.\n\n\nHe also initiated internship programs providing Chinese and African students with opportunities to receive training in MCM offices.\n\n\n\n\n"}
{"id": "29114566", "url": "https://en.wikipedia.org/wiki?curid=29114566", "title": "Mushroom Observer", "text": "Mushroom Observer\n\nMushroom Observer is a collaborative mycology website started by Nathan Wilson in 2006. Its purpose is to \"record observations about mushrooms, help people identify mushrooms they aren’t familiar with, and expand the community around the scientific exploration of mushrooms\".\n\nThe community of about 10,000 registered users collaborates on identifying the submitted mushroom images, assigning their scientific names by means of a weighted voting process.\n\nAll photographs are subject to a Creative Commons license that allows their reuse by others without the need for remuneration or special permission, subject to the terms of the license. The software is open source and hosted on GitHub.\n\nAs of 2018, the website contains about 311,000 user-submitted mushroom observations illustrated by 945,000 photographs. In 2010, the website contained about 53,000 user-submitted mushroom observations illustrated by 101,000 photographs; up from 7,250 observations and 12,800 photographs in 2008. In November 2018 Mushroom observer offers approx. 945,000 photos and 311,000 observations or 15,000 species of fungi.\n\n"}
{"id": "56617071", "url": "https://en.wikipedia.org/wiki?curid=56617071", "title": "Natural Resource Governance Institute", "text": "Natural Resource Governance Institute\n\nThe Natural Resource Governance Institute (NRGI) is an independent nonprofit organisation dedicated to improving countries' governance over their natural resources (in particular oil, gas and minerals) to promote sustainable and inclusive development. The headquarters of NRGI are based in New York.\n\nThe Natural Resource Governance Institute was established through the merger of the Revenue Watch Institute and the Natural Resource Charter in 2013. Originally based in New York, NRGI has opened offices in London, Accra, Lima, Washington, D.C., Jakarta and Dar Es Salaam. This partly reflects its focus on Colombia, Democratic Republic of Congo, Ghana, Guinea, Indonesia, Mexico Mongolia, Myanmar, Nigeria, Tanzania, and Tunisia as \"priority countries\".\n\nThe Natural Resource Governance Institute is led by a president, with Daniel Kaufmann currently serving in that role. Its activities are supervised by a board of directors, with Ernesto Zedillo as chair and Smita Singh as vice-chair. Finally, NRGI's leadership team and its board of directors benefit are supported by an Advisory Council co-chaired by Michael Spence and Joseph Bell. Other prominent figures affiliated with NRGI include Paul Collier, Ernest Aryeetey, Elena Panfilova, Alicia Bárcena Ibarra, Peter Eigen, Antonio La Viña, Ilgar Mammadov, José Antonio Ocampo, Anya Schiffrin, Andrés Velasco, and Tony Venables.\n\nIn line with its mission, NRGI supports civil society organisations, government institutions, private sector enterprises, and the media with technical advice, advocacy, applied research, policy analysis, and capacity development with regard to natural resource governance. Key tools developed in that context include for example the 2017 Resource Governance Index, a map of resource projects, the Natural Resource Charter Benchmarking Framework, or the 2010 Revenue Watch Index. Notably, research and resources from NRGI have been featured by international media such as \"The Atlantic\", \"Financial Times\" or \"Forbes\", as well as national media in the concerned countries.\n\nThe Natural Resource Charter is a document aimed at providing advice and policy options with regard to the management of resource wealth in order to help resource-rich countries use their natural resources for sustainable development. NRGI promotes the implementation of the Natural Resource Charter and provides policy advice with regard to this implementation process. The Natural Resource Charter consists of the following 12 \"Precepts\", which are organised into three parts based on the chain of decisions involved in natural resource management:\n\nThe Resource Governance Index, developed by NRGI, measures the quality of countries' resource governance and ranks them accordingly. The index is constructed by sending a 149-item questionnaire to 150 experts in 81 countries, who research the issues raised in the questionnaire, compile documentation and complete the questionnaire. The quality of the survey data is then assessed by NRGI and enriched by further data on countries \"enabling environments\". Finally, NRGI calculates the index as a composite score out of the:\nwith higher scores indicating a better resource governance process.\n\nWebsite of the Natural Resource Governance Institute\n"}
{"id": "34962698", "url": "https://en.wikipedia.org/wiki?curid=34962698", "title": "Racah Lectures in Physics", "text": "Racah Lectures in Physics\n\n\"The Racah Lecture\" is annual memorial lecture given at The Racah Institute of Physics of the Hebrew University of Jerusalem commemorating Prof. Giulio Racah.\nThe lecturers are selected from among the leading physists in the world.\n"}
{"id": "16948699", "url": "https://en.wikipedia.org/wiki?curid=16948699", "title": "Records management taxonomy", "text": "Records management taxonomy\n\nRecords management taxonomy is the representation of data, upon which the classification of unstructured content is based, within an organization. It may manifest itself as metadata in structured database fields or in folder structures represented to end users from a user interface within a system. It is created to facilitate the correct records management policies within the organization, fulfilment of regulatory compliance, integration to operational and knowledge management systems and the search for information within the organization. It can be applied to physical and or electronic records.\n\nDisciplines and or professions may create template taxonomies which are commonly used such as for Engineering and or Pharmaceutical Research. However, most organizations and or business functions within an organization may define taxonomies based on organizational requirements.\n"}
{"id": "20697143", "url": "https://en.wikipedia.org/wiki?curid=20697143", "title": "Regional Studies Association", "text": "Regional Studies Association\n\nThe Regional Studies Association (RSA) is an international learned society that is concerned with the analysis of regions and regional issues. The Regional Studies Association works with its international and interdisciplinary membership to facilitate the highest standards of theoretical development, empirical analysis and policy debate of issues at this sub-national scale, incorporating both the urban and rural and different conceptions of space such as city regions and interstitial spaces. We are, for example, interested in issues of economic development and growth, conceptions of territory and its governance and in thorny problems of equity and injustice.\n\nIt runs a programme of international conferences and events based on excellence that has been growing steadily over the years and plays a major role in the Association’s aim in supporting its members disseminate their research. These events also create debates and offer opportunities for those who wish to be part of a community where high quality events are also the place to network within a friendly atmosphere, present their work to international audiences and strengthen their careers. In 2017, the RSA organised events in Ireland, Brazil, China, Romania, Belgium, Australia, UK & Russia. The RSA was a key partner in the European Week of Regions and Cities and we were coordinating the #EURegionsWeek 2017 University sessions and EU Cohesion Policy Master Class. \n\nThe association publishes 5 journals with the publisher Taylor and Francis: \"Regional Studies\", \"Spatial Economic Analysis\", and \"Territory, Politics, Governance\", the open access journal \"Regional Studies, Regional Science\", and \"Area Development and Policy\", that engages with the transformation of the modern world as a result of development of the global South/Greater BRICS. The \"Regions and Cities\" Book Series is published by Routledge in association with the RSA. The Book Series Editor-in-Chief is Joan Fitzgerald (Northeastern University, USA). \"Regions\" magazine is the membership magazine and the voice of the RSA membership.\n\nIt has allocated financial resources to support its members and offer a range of funding opportunities to suit different career stages. These opportunities provide members with the chance to apply for financial help to support their research, run networking events, receive awards for excellence and help towards the costs of travel to attend non-RSA events and present their work to international audiences. The RSA offers the following funding opportunities: Travel Grant up to £500, Policy Expo up to £15,000, Membership Research Grant (MeRSA) up to £5,000, Fellowship Research Grant (FeRSA) up to £7,500, Early Career Research Grant up to £10,000 and Research Network Grant Scheme up to £10,000. The RSA's Research Networks are formed by RSA members to organise a series of events to examine an issue of collective interest. \n\nThere is an extensive international network of around 70 Ambassadors, five Divisions (China, Latin America, Northern Europe, India and Russia), five country Sections (Hungary, Poland, Scotland, Ireland and Wales) and six UK regional Branches which support members in their own locales. We also have a non-profit Foundation in Belgium (RSA Europe). These international networks work together with the Association to expend the opportunities and benefits offered to its members but also to those who do not yet know the Association. These networks are set up to organise a range of activities that are open to all to attend and they have the role of informing and disseminating information on regional studies at a regional and/or national level. They also act as an invaluable link to the Association for those who work in a language other than English but also for those who cannot attend all the major events directly organised by the Association.\n\nThe RSA's Chair is Professor Mark Tewdwr-Jones, Director of Newcastle City Futures and Chair of Town Planning, Newcastle University, UK. The RSA’s previous Chair (2012-2017) was Professor Andrew Beer, Dean, Research & Innovation at UniSA Business School, Australia. He succeeded Professor David Bailey who served as Chair over 2006–12. Sally Hardy is Chief Executive for the past 30 years. The RSA President is Ron Martin, University of Cambridge, UK. The previous RSA President was Sir Peter Hall until 2014.\n\nSir Peter Hall was awarded the RSA's Prize for Outstanding Contribution to the field of Regional Studies at the RSA's Presidential Lunch held at the House of Commons in September 2008. In subsequent years the association has awarded an annual Sir Peter Hall Prize for outstanding contribution to the field of Regional Studies, with recipients including Ann Markusen, John Bachtler, John Goddard, Michael Storper, Allen J. Scott, Ray Hudson, Brian Robson, Susan Christopherson and Saskia Sassen in 2017. The RSA also awards excellence in the field with its Best Book Award, Nathaniel Lichfield Award, RSA and Routledge Early Career Award, as well as its Best Paper and Best Referee Awards for all its academic journals (\"Regional Studies; Spatial Economic Analysis; Territory, Politics, Governance; Regional Studies, Regional Science; Area Development and Policy\").\n\nThe RSA is a member society of the Academy of Social Sciences, previously known as the Academy of Learned Societies in the Social Sciences. The association is also a recognised United Nations NGO with Consultative Status (ECOSOC).\n\nTerritory, Politics, Governance is a peer-reviewed academic journal covering the development of theory and research in territorial politics and the governance of space. It is an official journal of the Regional Studies Association and is published by Routledge. The editor-in-chief is John Agnew (University of California, Los Angeles) and the Co-Editors are Martin Jones, Michael Keating, Walter Nicholls, Bae-Gyoon Park, and Ananya Roy. Articles published in \"Territory, Politics, Governance\" have included papers by Saskia Sassen, Jamie Peck, and Gerard Toal.\n\nRegional Studies, Regional Science is a peer-reviewed Open access journal publishing articles on regional issues in geography, economics, planning, and political science. It is an official journal of the Regional Studies Association and is published by Routledge. The editors-in-chief are Alasdair Rae (University of Sheffield) and Alex Singleton (University of Liverpool), who are supported by over 30 Associate Editors and an International Editorial Advisory Board with over 60 members. \"Regional Studies, Regional Science\" publishes a range of article types; including full length research articles of 8,000 words, shorter policy briefings of around 3,000 words and a 'regional graphics' section, featuring data visualisation in the form of maps, charts and other graphics. The journal also offers a mentored submission route for early career academics.\n\n\n"}
{"id": "838544", "url": "https://en.wikipedia.org/wiki?curid=838544", "title": "Robert Methven Petrie", "text": "Robert Methven Petrie\n\nRobert Methven Petrie (May 15, 1906 – April 8, 1966) was a Canadian astronomer.\n\nHe was born in Scotland but emigrated to Canada at the age of five. He grew up in Victoria, British Columbia and studied physics and mathematics at the University of British Columbia. He began working summer jobs at the Dominion Astrophysical Observatory and became fascinated with astronomy.\n\nHe obtained his Ph.D. at the University of Michigan in 1932. He taught there until 1935, when he joined the staff of the Dominion Astrophysical Observatory. In 1951 he became its director.\n\nHe extensively studied spectroscopic binaries.\n\nThe crater Petrie on the Moon is named after him. The Canadian Astronomical Society established the R. M. Petrie Prize Lecture to honor his astrophysical research.\n\n"}
{"id": "22908095", "url": "https://en.wikipedia.org/wiki?curid=22908095", "title": "SOPHIE échelle spectrograph", "text": "SOPHIE échelle spectrograph\n\nThe SOPHIE (Spectrographe pour l’Observation des Phénomènes des Intérieurs stellaires et des Exoplanètes, literally meaning \"Spectrograph for the observation of the phenomena of the stellar interiors and of the exoplanets\") échelle spectrograph is a high-resolution echelle spectrograph installed on the 1.93m reflector telescope at the Haute-Provence Observatory located in south-eastern France. The purpose of this instrument is asteroseismology and extrasolar planet detection by the radial velocity method. It builds upon and replaces the older ELODIE spectrograph. This instrument was made available for use by the general astronomical community October 2006.\n\nThe electromagnetic spectrum wavelength range is from 387.2 to 694.3 nanometers. The spectrograph is fed from the Cassegrain focus through either one of two separate optical fiber sets, yielding two different spectral resolutions (HE and HR modes). The instrument is entirely computer-controlled. A standard data reduction pipeline automatically processes the data upon every CCD readout cycle.\n\n\"HR mode\" is the high resolution mode. This mode incorporates a 40 micrometre exit slit to achieve high spectral resolution of R = 75000.\n\"HE mode\" is the high efficiency mode. This mode is used when a higher throughput is desired particularly in the case of faint objects spectral resolution is set to R = 40000.\n\nThe R2 échelle diffraction grating has 52.65 grooves per millimeter and was manufactured by Richardson Gratings. It is blazed at 65° its size is 20.4 cm x 40.8 cm. It is mounted in a fixed configuration. The spectrum is projected onto the E2V Technologies type 44-82 CCD detector of 4096 x 2048 pixels kept at a constant temperature of –100 °C. This grating yields 41 spectral orders, of which 39 are currently extracted, to obtain wavelengths between 387.2 nm and 694.3 nm.\n\nIn \"HE mode\", a signal-to-noise ratio (per pixel) of 27 was reached in 90 min for an object of magnitude 14.5 in the V band. \n\nThe stability of the instrument can be described by the lowest dispersion possible for radial velocity observations, in m/s. In \"HR mode\" the short term stability has been measured to be 1.3 m/s, while it is 2 m/s for longer timescales.\n\n\n"}
{"id": "1208872", "url": "https://en.wikipedia.org/wiki?curid=1208872", "title": "Shannon's source coding theorem", "text": "Shannon's source coding theorem\n\nIn information theory, Shannon's source coding theorem (or noiseless coding theorem) establishes the limits to possible data compression, and the operational meaning of the Shannon entropy.\n\nThe source coding theorem shows that (in the limit, as the length of a stream of independent and identically-distributed random variable (i.i.d.) data tends to infinity) it is impossible to compress the data such that the code rate (average number of bits per symbol) is less than the Shannon entropy of the source, without it being virtually certain that information will be lost. However it is possible to get the code rate arbitrarily close to the Shannon entropy, with negligible probability of loss.\n\nThe source coding theorem for symbol codes places an upper and a lower bound on the minimal possible expected length of codewords as a function of the entropy of the input word (which is viewed as a random variable) and of the size of the target alphabet.\n\n\"Source coding\" is a mapping from (a sequence of) symbols from an information source to a sequence of alphabet symbols (usually bits) such that the source symbols can be exactly recovered from the binary bits (lossless source coding) or recovered within some distortion (lossy source coding). This is the concept behind data compression.\n\nIn information theory, the source coding theorem (Shannon 1948) informally states that (MacKay 2003, pg. 81, Cover 2006, Chapter 5):\n\nLet denote two finite alphabets and let and denote the set of all finite words from those alphabets (respectively).\n\nSuppose that is a random variable taking values in and let be a uniquely decodable code from to where . Let denote the random variable given by the length of codeword .\n\nIf is optimal in the sense that it has the minimal expected word length for , then (Shannon 1948):\n\nGiven is an i.i.d. source, its time series is i.i.d. with entropy in the discrete-valued case and differential entropy in the continuous-valued case. The Source coding theorem states that for any , i.e. for any rate larger than the entropy of the source, there is large enough and an encoder that takes i.i.d. repetition of the source, , and maps it to binary bits such that the source symbols are recoverable from the binary bits with probability at least .\n\nProof of Achievability. Fix some , and let\n\nThe typical set, , is defined as follows:\n\nThe Asymptotic Equipartition Property (AEP) shows that for large enough , the probability that a sequence generated by the source lies in the typical set, , as defined approaches one. In particular, for sufficiently large , formula_4 can be made arbitrarily close to 1, and specifically, greater than formula_5 (See \nAEP for a proof).\n\nThe definition of typical sets implies that those sequences that lie in the typical set satisfy:\n\nNote that:\n\n\nSince formula_12 bits are enough to point to any string in this set.\n\nThe encoding algorithm: The encoder checks if the input sequence lies within the typical set; if yes, it outputs the index of the input sequence within the typical set; if not, the encoder outputs an arbitrary digit number. As long as the input sequence lies within the typical set (with probability at least ), the encoder doesn't make any error. So, the probability of error of the encoder is bounded above by .\n\nProof of Converse. The converse is proved by showing that any set of size smaller than (in the sense of exponent) would cover a set of probability bounded away from .\n\nFor let denote the word length of each possible . Define formula_13, where is chosen so that . Then\n\nwhere the second line follows from Gibbs' inequality and the fifth line follows from Kraft's inequality:\n\nso .\n\nFor the second inequality we may set\n\nso that\n\nand so\n\nand\n\nand so by Kraft's inequality there exists a prefix-free code having those word lengths. Thus the minimal satisfies\n\nDefine typical set as:\n\nThen, for given , for large enough, . Now we just encode the sequences in the typical set, and usual methods in source coding show that the cardinality of this set is smaller than formula_22. Thus, on an average, bits suffice for encoding with probability greater than , where and can be made arbitrarily small, by making larger.\n\n"}
{"id": "12162850", "url": "https://en.wikipedia.org/wiki?curid=12162850", "title": "Soyuz 7K-OKS", "text": "Soyuz 7K-OKS\n\nSoyuz 7K-OKS (also known as Soyuz 7KT-OK) is a version of the Soyuz spacecraft and was the first spacecraft designed for space station flights. Its only manned flights were conducted in 1971, with Soyuz 10 and Soyuz 11. \n\nThe two craft of the Soyuz 7K-OKS generation were modified from the original Soyuz 7K-OK. The new \"probe and drogue\" docking mechanism, which was first used by these two missions, featured an internal docking hatch that allowed for the first time internal transfer between Soviet spacecraft. This \"probe and drogue\" docking mechanism introduced with Soyuz 7K-OKS is still in use today at the ISS. The external toroidal fuel tank, a holdover from the original lunar mission models of the Soyuz, was dropped from the 7K-OKS since it was unneeded for Earth orbital flights.\n\nThe Soyuz 7K-OKS flew only twice, Soyuz 10 and Soyuz 11.\n\nOn its maiden flight, the 7KT-OK successfully launched into earth orbit, but failed to dock completely with the Salyut 1 space station. Upon reentry, the spacecraft encountered problems with toxic fumes.\n\nThis generation of Soyuz spacecraft is notable for the first successful manning of the first space station Salyut 1 by Soyuz 11 – this success was however overshadowed by the death of the crew, who were killed when the capsule depressurized during the re-entry phase.\n\n\n"}
{"id": "1439193", "url": "https://en.wikipedia.org/wiki?curid=1439193", "title": "Technoscience", "text": "Technoscience\n\nIn common usage, technoscience refers to the entire long-standing global human activity of technology combined with the relatively recent scientific method that occurred primarily in Europe during the 17th and 18th centuries. Technoscience thus comprises the history of human application of technology and modern scientific methods, ranging from the early development of basic technologies for hunting, agriculture, or husbandry (e.g. the well, the bow, the plow, the harness) and all the way through atomic applications, biotechnology, robotics, and computer sciences. This more common and comprehensive usage of the term \"technoscience\" can be found in general textbooks and lectures concerning the history of science.\n\nAn alternate, more narrow usage occurs in some philosophic science and technology studies. In this usage, technoscience refers specifically to the technological and social context of science. Technoscience recognises that scientific knowledge is not only socially coded and historically situated but sustained and made durable by material (non-human) networks. Technoscience states that the fields of science and technology are linked and grow together, and scientific knowledge requires an infrastructure of technology in order to remain stationary or move forward.\n\nThe latter, philosophic use of the term technoscience was popularized by French philosopher Gaston Bachelard in 1953. It was popularized in the French-speaking world by Belgian philosopher Gilbert Hottois in the late 1970s/early 1980s, and entered English academic usage in 1987 with Bruno Latour's \"Science in Action\".\n\nIn translating the concept to English, Bruno Latour also combined several arguments about \"Technoscience\" that had circulated separately within Science and Technology Studies(STS) before into a comprehensive framework:\n\n\nWe look at the concept of technoscience by considering three levels: a descriptive-analytic level, a deconstructivist level, and a visionary level.\n\nOn a descriptive-analytic level, technoscientific studies examine the decisive role of science and technology in how knowledge is being developed. What is the role played by large research labs in which experiments on organisms are undertaken, when it comes to a certain way of looking at the things surrounding us? To what extent do such investigations, experiments and insights shape the view on ‘nature’, and on ‘our’ bodies? How do these insights link to the concept of living organisms as biofacts? To what extent do such insights inform technological innovation? Can the laboratory be understood as a metaphor for social structures in their entirety?\n\nOn a deconstructive level, theoretical work is being undertaken on technoscience to address scientific practices critically, e.g. by Bruno Latour (sociology), by Donna Haraway (history of science), and by Karen Barad (theoretical physics). It is pointed out that scientific descriptions may be only allegedly objective; that descriptions are of a performative character, and that there are ways to de-mystify them. Likewise, new forms of representing those involved in research are being sought.\n\nOn a visionary level, the concept of technoscience comprises a number of social, literary, artistic and material technologies from western cultures in the third millennium. This is undertaken in order to focus on the interplay of hitherto separated areas and to question traditional boundary-drawing: this concerns the boundaries drawn between scientific disciplines as well as those commonly upheld for instance between research, technology, the arts and politics. One aim is to broaden the term ‘technology’ (which by the Greek etymology of ‘techné’ connotes all of the following: arts, handicraft, and skill) so as to negotiate possibilities of participation in the production of knowledge and to reflect on strategic alliances. Technoscience can be juxtaposed with a number of other innovative interdisciplinary areas of scholarship which have surfaced in these recent years such as technoetic, technoethics and technocriticism.\n\nAs with any subject, technoscience exists within a broader social context that must be considered. Science & Technology Studies researcher Sergio Sismondo argues, “Neither the technical vision nor the social vision will come into being without the other, though with enough Concerted Effort both may be brought into being together\". Despite the frequent separation between innovators and the consumers, Sismondo argues that development of technologies, though stimulated by a technoscientific themes, is an inherently social process.\n\nTechnoscience is so deeply embedded in our everyday lives that its developments exist outside a space for critical thought and evaluation, argues Daniel Lee Kleinman (2005). Those who do attempt to question the perception of progress as being only a matter of more technology are often seen as “champions of technological stagnation. The exception to this mentality is when a development is seen as threatening to human or environmental well-being. This holds true with the popular opposition of GMO crops, where the questioning of the validity of monopolized farming and patented genetics was simply not enough to rouse awareness.\n\nScience and technology are tools that continually change social structures and behaviors. Technoscience can be viewed as a form of government or having the power of government because of its impact on society. The impact extends to public health, safety, the environment, and beyond. Innovations create fundamental changes and drastically change the way people live. For example, C-SPAN and social media give American voters a near real-time view of Congress. This has allowed journalists and the people to hold their elected officials accountable in new ways.\n\nChlorine chemists and their scientific knowledge helped set the agenda for many environmental problems: PCBs in the Hudson River are polychlorinated biphenols; DDT, dieldrin, and aldrin are chlorinated pesticides; CFCs that deplete the ozone layer are chlorofluorocarbons. Industry actually manufactured the chemicals and consumers purchased them. Therefore, one can determine that chemists are not the sole cause for these issues, but they are not blameless.\n\n\n\n"}
{"id": "18253454", "url": "https://en.wikipedia.org/wiki?curid=18253454", "title": "Theory of conjoint measurement", "text": "Theory of conjoint measurement\n\nThe theory of conjoint measurement (also known as conjoint measurement or additive conjoint measurement) is a general, formal theory of continuous quantity. It was independently discovered by the French economist Gérard Debreu (1960) and by the American mathematical psychologist R. Duncan Luce and statistician John Tukey .\n\nThe theory concerns the situation where at least two natural attributes, \"A\" and \"X\", non-interactively relate to a third attribute, \"P\". It is not required that \"A\", \"X\" or \"P\" are known to be quantities. Via specific relations between the levels of \"P\", it can be established that \"P\", \"A\" and \"X\" are continuous quantities. Hence the theory of conjoint measurement can be used to quantify attributes in empirical circumstances where it is not possible to combine the levels of the attributes using a side-by-side operation or concatenation. The quantification of psychological attributes such as attitudes, cognitive abilities and utility is therefore logically plausible. This means that the scientific measurement of psychological attributes is possible. That is, like physical quantities, a magnitude of a psychological quantity may possibly be expressed as the product of a real number and a unit magnitude.\n\nApplication of the theory of conjoint measurement in psychology, however, has been limited. It has been argued that this is due to the high level of formal mathematics involved (e.g., ) and that the theory cannot account for the \"noisy\" data typically discovered in psychological research (e.g., ). It has been argued that the Rasch model is a stochastic variant of the theory of conjoint measurement (e.g., ; ; ; ; ; ), however, this has been disputed (e.g., Karabatsos, 2001; Kyngdon, 2008). Order restricted methods for conducting probabilistic tests of the cancellation axioms of conjoint measurement have been developed in the past decade (e.g., Karabatsos, 2001; Davis-Stober, 2009).\n\nThe theory of conjoint measurement is (different but) related to conjoint analysis, which is a statistical-experiments methodology employed in marketing to estimate the parameters of additive utility functions. Different multi-attribute stimuli are presented to respondents, and different methods are used to measure their preferences about the presented stimuli. The coefficients of the utility function are estimated using alternative regression-based tools.\n\nIn the 1930s, the British Association for the Advancement of Science established the Ferguson Committee to investigate the possibility of psychological attributes being measured scientifically. The British physicist and measurement theorist Norman Robert Campbell was an influential member of the committee. In its Final Report (Ferguson, \"et al.\", 1940), Campbell and the Committee concluded that because psychological attributes were not capable of sustaining concatenation operations, such attributes could not be continuous quantities. Therefore, they could not be measured scientifically. This had important ramifications for psychology, the most significant of these being the creation in 1946 of the \"operational theory of measurement\" by Harvard psychologist Stanley Smith Stevens. Stevens' non-scientific theory of measurement is widely held as definitive in psychology and the behavioural sciences generally .\n\nWhilst the German mathematician Otto Hölder (1901) anticipated features of the theory of conjoint measurement, it was not until the publication of Luce & Tukey's seminal 1964 paper that the theory received its first complete exposition. Luce & Tukey's presentation was algebraic and is therefore considered more general than Debreu's (1960) topological work, the latter being a special case of the former . In the first article of the inaugural issue of the \"Journal of Mathematical Psychology\", proved that via the theory of conjoint measurement, attributes not capable of concatenation could be quantified. N.R. Campbell and the Ferguson Committee were thus proven wrong. That a given psychological attribute is a continuous quantity is a logically coherent and empirically testable hypothesis.\n\nAppearing in the next issue of the same journal were important papers by Dana Scott (1964), who proposed a hierarchy of cancellation conditions for the indirect testing of the solvability and Archimedean axioms, and David Krantz (1964) who connected the Luce & Tukey work to that of Hölder (1901).\n\nWork soon focused on extending the theory of conjoint measurement to involve more than just two attributes. and Amos Tversky (1967) developed what became known as polynomial conjoint measurement, with providing a schema with which to construct conjoint measurement structures of three or more attributes. Later, the theory of conjoint measurement (in its two variable, polynomial and \"n\"-component forms) received a thorough and highly technical treatment with the publication of the first volume of \"Foundations of Measurement\", which Krantz, Luce, Tversky and philosopher Patrick Suppes cowrote .\n\nShortly after the publication of Krantz, et al., (1971), work focused upon developing an \"error theory\" for the theory of conjoint measurement. Studies were conducted into the number of conjoint arrays that supported only single cancellation and both single and double cancellation (; ). Later enumeration studies focused on polynomial conjoint measurement (; ). These studies found that it is highly unlikely that the axioms of the theory of conjoint measurement are satisfied at random, provided that more than three levels of at least one of the component attributes has been identified.\n\nJoel Michell (1988) later identified that the \"no test\" class of tests of the double cancellation axiom was empty. Any instance of double cancellation is thus either an acceptance or a rejection of the axiom. Michell also wrote at this time a non-technical introduction to the theory of conjoint measurement which also contained a schema for deriving higher order cancellation conditions based upon Scott's (1964) work. Using Michell's schema, Ben Richards (Kyngdon & Richards, 2007) discovered that some instances of the triple cancellation axiom are \"incoherent\" as they contradict the single cancellation axiom. Moreover, he identified many instances of the triple cancellation which are trivially true if double cancellation is supported.\n\nThe axioms of the theory of conjoint measurement are not stochastic; and given the ordinal constraints placed on data by the cancellation axioms, order restricted inference methodology must be used . George Karabatsos and his associates (Karabatsos, 2001; ) developed a Bayesian Markov chain Monte Carlo methodology for psychometric applications. Karabatsos & Ullrich 2002 demonstrated how this framework could be extended to polynomial conjoint structures. Karabatsos (2005) generalised this work with his multinomial Dirichlet framework, which enabled the probabilistic testing of many non-stochastic theories of mathematical psychology. More recently, Clintin Davis-Stober (2009) developed a frequentist framework for order restricted inference that can also be used to test the cancellation axioms.\n\nPerhaps the most notable (Kyngdon, 2011) use of the theory of conjoint measurement was in the prospect theory proposed by the Israeli – American psychologists Daniel Kahneman and Amos Tversky (Kahneman & Tversky, 1979). Prospect theory was a theory of decision making under risk and uncertainty which accounted for choice behaviour such as the Allais Paradox. David Krantz wrote the formal proof to prospect theory using the theory of conjoint measurement. In 2002, Kahneman received the Nobel Memorial Prize in Economics for prospect theory (Birnbaum, 2008).\n\nIn physics and metrology, the standard definition of measurement is the estimation of the ratio between a magnitude of a continuous quantity and a unit magnitude of the same kind (de Boer, 1994/95; Emerson, 2008). For example, the statement \"Peter's hallway is 4m long\" expresses a measurement of an hitherto unknown length magnitude (the hallway's length) as the ratio of the unit (the metre in this case) to the length of the hallway. The real number \"4\" is a real number in the strict mathematical sense of this term.\n\nFor some other quantities, it is easier or has been convention to estimate ratios between attribute \"differences\". Consider temperature, for example. In the familiar everyday instances, temperature is measured using instruments calibrated in either the Fahrenheit or Celsius scales. What are really being measured with such instruments are the magnitudes of temperature differences. For example, Anders Celsius defined the unit of the Celsius scale to be 1/100th of the difference in temperature between the freezing and boiling points of water at sea level. A midday temperature measurement of 20 degrees Celsius is simply the ratio of the Celsius unit to the midday temperature.\n\nFormally expressed, a scientific measurement is:\n\nformula_1\n\nwhere \"Q\" is the magnitude of the quantity, \"r\" is a real number and [\"Q\"] is a unit magnitude of the same kind.\n\nLength is a quantity for which natural concatenation operations exist. That is, we can combine in a side-by-side fashion lengths of rigid steel rods, for example, such that the additive relations between lengths is readily observed. If we have four 1 m lengths of such rods, we can place them end to end to produce a length of 4 m. Quantities capable of concatenation are known as \"extensive quantities\" and include mass, time, electrical resistance and plane angle. These are known as \"base\" quantities in physics and metrology.\n\nTemperature is a quantity for which there is an absence of concatenation operations. We cannot pour a volume of water of temperature 40 °C into another bucket of water at 20 °C and expect to have a volume of water with a temperature of 60 °C. Temperature is therefore an \"intensive\" quantity.\n\nPsychological attributes, like temperature, are considered to be intensive as no way of concatenating such attributes has been found. But this is not to say that such attributes are not quantifiable. The theory of conjoint measurement provides a theoretical means of doing this.\n\nConsider two natural attributes \"A\", and \"X\". It is not known that either \"A\" or \"X\" is a continuous quantity, or that both of them are. Let \"a\", \"b\", and \"c\" represent three independent, identifiable levels of \"A\"; and let \"x\", \"y\" and \"z\" represent three independent, identifiable levels of \"X\". A third attribute, \"P\", consists of the nine ordered pairs of levels of \"A\" and \"X\". That is, (\"a\", \"x\"), (\"b\", \"y\")..., (\"c\", \"z\") (see Figure 1). The quantification of \"A\", \"X\" and \"P\" depends upon the behaviour of the relation holding upon the levels of \"P\". These relations are presented as axioms in the theory of conjoint measurement.\n\nThe single cancellation axiom is as follows. The relation upon \"P\" satisfies \"single cancellation\" if and only if for all \"a\" and \"b\" in \"A\", and \"x\" in \"X\", (\"a\", \"x\") > (\"b\", \"x\") is implied for every \"w\" in \"X\" such that (\"a\", \"w\") > (\"b\", \"w\"). Similarly, for all \"x\" and \"y\" in \"X\" and \"a\" in \"A\", (\"a\", \"x\") > (\"a\", \"y\") is implied for every \"d\" in \"A\" such that (\"d\", \"x\") > (\"d\", \"y\"). What this means is that if any two levels, \"a\", \"b\", are ordered, then this order holds irrespective of each and every level of \"X\". The same holds for any two levels, \"x\" and \"y\" of \"X\" with respect to each and every level of \"A\".\n\nSingle cancellation is so-called because a single common factor of two levels of \"P\" cancel out to leave the same ordinal relationship holding on the remaining elements. For example, \"a\" cancels out of the inequality (\"a\", \"x\") > (\"a\", \"y\") as it is common to both sides, leaving \"x\" > \"y\". Krantz, et al., (1971) originally called this axiom \"independence\", as the ordinal relation between two levels of an attribute is independent of any and all levels of the other attribute. However, given that the term \"independence\" causes confusion with statistical concepts of independence, single cancellation is the preferable term. Figure One is a graphical representation of one instance of single cancellation.\n\nSatisfaction of the single cancellation axiom is necessary, but not sufficient, for the quantification of attributes \"A\" and \"X\". It only demonstrates that the levels of \"A\", \"X\" and \"P\" are ordered. Informally, single cancellation does not sufficiently constrain the order upon the levels of \"P\" to quantify \"A\" and \"X\". For example, consider the ordered pairs (\"a\", \"x\"), (\"b\", \"x\") and (\"b\", \"y\"). If single cancellation holds then (\"a\", \"x\") > (\"b\", \"x\") and (\"b\", \"x\") > (\"b\", \"y\"). Hence via transitivity (\"a\", \"x\") > (\"b\", \"y\"). The relation between these latter two ordered pairs, informally a \"left-leaning diagonal\", is determined by the satisfaction of the single cancellation axiom, as are all the \"left leaning diagonal\" relations upon \"P\".\n\nSingle cancellation does not determine the order of the \"right-leaning diagonal\" relations upon \"P\". Even though by transitivity and single cancellation it was established that (\"a\", \"x\") > (\"b\", \"y\"), the relationship between (\"a\", \"y\") and (\"b\", \"x\") remains undetermined. It could be that either (\"b\", \"x\") > (\"a\", \"y\") or (\"a\", \"y\") > (\"b\", \"x\") and such ambiguity cannot remain unresolved.\n\nThe double cancellation axiom concerns a class of such relations upon \"P\" in which the common terms of two antecedent inequalities cancel out to produce a third inequality. Consider the instance of double cancellation graphically represented by Figure Two. The antecedent inequalities of this particular instance of double cancellation are:\n\nformula_2\n\nand\n\nformula_3.\n\nGiven that:\n\nformula_4\n\nis true if and only if formula_5; and\n\nformula_6\n\nis true if and only if formula_7, it follows that:\n\nformula_8.\n\nCancelling the common terms results in:\n\nformula_9.\n\nHence double cancellation can only obtain when \"A\" and \"X\" are quantities.\n\nDouble cancellation is satisfied if and only if the consequent inequality does not contradict the antecedent inequalities. For example, if the consequent inequality above was:\n\nformula_10, or alternatively,\n\nformula_11,\n\nthen double cancellation would be violated and it could not be concluded that \"A\" and \"X\" are quantities.\n\nDouble cancellation concerns the behaviour of the \"right leaning diagonal\" relations on \"P\" as these are not logically entailed by single cancellation. discovered that when the levels of \"A\" and \"X\" approach infinity, then the number of right leaning diagonal relations is half of the number of total relations upon \"P\". Hence if \"A\" and \"X\" are quantities, half of the number of relations upon \"P\" are due to ordinal relations upon \"A\" and \"X\" and half are due to additive relations upon \"A\" and \"X\" .\n\nThe number of instances of double cancellation is contingent upon the number of levels identified for both \"A\" and \"X\". If there are \"n\" levels of \"A\" and \"m\" of \"X\", then the number of instances of double cancellation is \"n\"! × \"m\"!. Therefore, if \"n\" = \"m\" = 3, then 3! × 3! = 6 × 6 = 36 instances in total of double cancellation. However, all but 6 of these instances are trivially true if single cancellation is true, and if any one of these 6 instances is true, then all of them are true. One such instance is that shown in Figure Two. calls this a \"Luce–Tukey\" instance of double cancellation.\n\nIf single cancellation has been tested upon a set of data first and is established, then only the Luce–Tukey instances of double cancellation need to be tested. For \"n\" levels of \"A\" and \"m\" of \"X\", the number of Luce–Tukey double cancellation instances is formula_12formula_13. For example, if \"n\" = \"m\" = 4, then there are 16 such instances. If \"n\" = \"m\" = 5 then there are 100. The greater the number of levels in both \"A\" and \"X\", the less probable it is that the cancellation axioms are satisfied at random (; ) and the more stringent test of quantity the application of conjoint measurement becomes.\n\nThe single and double cancellation axioms by themselves are not sufficient to establish continuous quantity. Other conditions must also be introduced to ensure continuity. These are the \"solvability\" and \"Archimedean\" conditions.\n\n\"Solvability\" means that for any three elements of \"a\", \"b\", \"x\" and \"y\", the fourth exists such that the equation \"a\" \"x\" = \"b\" \"y\" is solved, hence the name of the condition. Solvability essentially is the requirement that each level \"P\" has an element in \"A\" and an element in \"X\". Solvability reveals something about the levels of \"A\" and \"X\" — they are either dense like the real numbers or equally spaced like the integers .\n\nThe \"Archimedean condition\" is as follows. Let \"I\" be a set of consecutive integers, either finite or infinite, positive or negative. The levels of \"A\" form a \"standard sequence\" if and only if there exists \"x\" and \"y\" in \"X\" where \"x\" ≠ \"y\" and for all integers \"i\" and \"i\" + 1 in \"I\":\n\nformula_14.\n\nWhat this basically means is that if \"x\" is greater than \"y\", for example, there are levels of \"A\" which can be found which makes two relevant ordered pairs, the levels of \"P\", equal.\n\nThe Archimedean condition argues that there is no infinitely greatest level of \"P\" and so hence there is no greatest level of either \"A\" or \"X\". This condition is a definition of continuity given by the ancient Greek mathematician Archimedes whom wrote that \"Further, of unequal lines, unequal surfaces, and unequal solids, the greater exceeds the less by such a magnitude as, when added to itself, can be made to exceed any assigned magnitude among those which are comparable with one another \" (\"On the Sphere and Cylinder\", Book I, Assumption 5). Archimedes recognised that for any two magnitudes of a continuous quantity, one being lesser than the other, the lesser could be multiplied by a whole number such that it equalled the greater magnitude. Euclid stated the Archimedean condition as an axiom in Book V of the Elements, in which Euclid presented his theory of continuous quantity and measurement.\n\nAs they involve infinitistic concepts, the solvability and Archimedean axioms are not amenable to direct testing in any finite empirical situation. But this does not entail that these axioms cannot be empirically tested at all. Scott's (1964) finite set of cancellation conditions can be used to indirectly test these axioms; the extent of such testing being empirically determined. For example, if both \"A\" and \"X\" possess three levels, the highest order cancellation axiom within Scott's (1964) hierarchy that indirectly tests solvability and Archimedeaness is double cancellation. With four levels it is triple cancellation (Figure 3). If such tests are satisfied, the construction of standard sequences in differences upon \"A\" and \"X\" are possible. Hence these attributes may be dense as per the real numbers or equally spaced as per the integers . In other words, \"A\" and \"X\" are continuous quantities.\n\nSatisfaction of the conditions of conjoint measurement means that measurements of the levels of \"A\" and \"X\" can be expressed as either ratios between magnitudes or ratios between magnitude differences. It is most commonly interpreted as the latter, given that most behavioural scientists consider that their tests and surveys \"measure\" attributes on so-called \"interval scales\" . That is, they believe tests do not identify absolute zero levels of psychological attributes.\n\nFormally, if \"P\", \"A\" and \"X\" form an \"additive conjoint structure\", then there exist functions from \"A\" and \"X\" into the real numbers such that for \"a\" and \"b\" in \"A\" and \"x\" and \"y\" in \"X\":\n\nformula_15.\n\nIf formula_16 and formula_17 are two other real valued functions satisfying the above expression, there exist formula_18 and formula_19 real valued constants satisfying:\n\nformula_20 and formula_21.\n\nThat is, formula_22 and formula_23 are measurements of \"A\" and \"X\" unique up to affine transformation (i.e. each is an interval scale in Stevens’ (1946) parlance). The mathematical proof of this result is given in .\n\nThis means that the levels of \"A\" and \"X\" are magnitude differences measured relative to some kind of unit difference. Each level of \"P\" is a difference between the levels of \"A\" and \"X\". However, it is not clear from the literature as to how a unit could be defined within an additive conjoint context. proposed a scaling method for conjoint structures but he also did not discuss the unit.\n\nThe theory of conjoint measurement, however, is not restricted to the quantification of differences. If each level of \"P\" is a product of a level of \"A\" and a level of \"X\", then \"P\" is another different quantity whose measurement is expressed as a magnitude of \"A\" per unit magnitude of \"X\". For example, \"A\" consists of masses and \"X\" consists of volumes, then \"P\" consists of densities measured as mass per unit of volume. In such cases, it would appear that one level of \"A\" and one level of \"X\" must be identified as a tentative unit prior to the application of conjoint measurement.\n\nIf each level of \"P\" is the sum of a level of \"A\" and a level of \"X\", then \"P\" is the same quantity as \"A\" and\"X\". For example, \"A\" and \"X\" are lengths so hence must be \"P\". All three must therefore be expressed in the same unit. In such cases, it would appear that a level of either \"A\" or \"X\" must be tentatively identified as the unit. Hence it would seem that application of conjoint measurement requires some prior descriptive theory of the relevant natural system.\n\nEmpirical applications of the theory of conjoint measurement have been sparse (; ).\n\nSeveral empirical evaluations of the double cancellation have been conducted. Among these, evaluated the axiom to the psychophysics of binaural loudness. They found the double cancellation axiom was rejected. conducted a similar investigation and replicated Levelt, \"et al.\"' (1972) findings. observed that the evaluation of double\ncancellation involves considerable redundancy that complicates its empirical testing. Therefore, evaluated instead the equivalent Thomsen Condition axiom, which avoids this redundancy, and found the property supported in binaural loudness. , summarized the literature to that date, including the observation that the evaluation of the Thomsen Condition also involves an empirical challenge that they find remedied by the Conjoint Commutativity axiom, which they show to be equivalent to the Thomsen Condition. found conjoint commutativity supported for binaural loudness and brightness.\n\n, Kyngdon (2006), Michell (1994) and tested the cancellation axioms of upon the interstimulus midpoint orders obtained by the use of Coombs' (1964) theory of unidimensional unfolding. Coombs' theory in all three studies was applied to a set of six statements. These authors found that the axioms were satisfied, however, these were applications biased towards a positive result. With six stimuli, the probability of an interstimulus midpoint order satisfying the double cancellation axioms at random is .5874 (Michell, 1994). This is not an unlikely event. Kyngdon & Richards (2007) employed eight statements and found the interstimulus midpoint orders rejected the double cancellation condition.\n\nKyngdon (2011) used Karabatsos's (2001) order-restricted inference framework to test a conjoint matrix of reading item response proportions (\"P\") where the examinee reading ability comprised the rows of the conjoint array (\"A\") and the difficulty of the reading items formed the columns of the array (\"X\"). The levels of reading ability were identified via raw total test score and the levels of reading item difficulty were identified by the Lexile Framework for Reading . Kyngdon found that satisfaction of the cancellation axioms was obtained only through permutation of the matrix in a manner inconsistent with the putative Lexile measures of item difficulty. Kyngdon also tested simulated ability test response data using polynomial conjoint measurement. The data were generated using Humphry's extended frame of reference Rasch model . He found support of distributive, single and double cancellation consistent with a distributive polynomial conjoint structure in three variables .\n\n\n\n"}
{"id": "9904712", "url": "https://en.wikipedia.org/wiki?curid=9904712", "title": "Walter Lewin Lectures on Physics", "text": "Walter Lewin Lectures on Physics\n\nThe Walter Lewin Lectures on Physics are a set of three courses including video lectures on physics by former MIT Physics Professor Walter Lewin. He explains the basics of classical mechanics, electricity, magnetism, vibrations, waves and introductory topics on astrophysics. It was prepared in the Massachusetts Institute of Technology and has been used by millions of students, teachers and self-learners around the world.\n\nIn late 2014, MIT indefinitely removed Lewin's lectures from OpenCourseWare and MITx due to a sexual harassment case. His lectures are still featured on other independent educational websites under the Creative Commons license.\n\nMany of Walter Lewin's lectures have been shown for over six years on UWTV in Seattle, reaching an audience of over four million people. For fifteen years he was on MIT Cable TV helping freshmen with their weekly homework assignments. His programs, which were aired 24 hours per day, were also frequently watched by upper-class students. His special lectures given at MIT for science teachers and for middle school students can be viewed on Lewin's YouTube channel. The three sets of video lectures are separated into classical mechanics, electricity and magnetism and vibrations and waves. All of the video lectures were available for free from MIT OpenCourseWare.\n\nOn December 8, 2014 all the material from his courses including the video lectures were pulled from OpenCourseWare after MIT announced that it determined that Lewin had engaged in sexual harassment of an MITx learner. However, the video lectures continue to be available on Lewin's YouTube channel.\n\n8.01 is a first-semester freshman physics class in Newtonian mechanics, fluid mechanics, and kinetic gas theory at MIT. In addition to the basic concepts of Newtonian mechanics, fluid mechanics, and kinetic gas theory, a variety of interesting topics are covered in this course: binary stars, neutron stars, black holes, resonance phenomena, musical instruments, stellar collapse, supernovae, Astronomical observations from very high flying balloons (lecture 35), and a peek into the intriguing quantum world.\n\n8.02 is a second-semester freshman physics class in electromagnetism at MIT. The 36 video lectures on electricity and magnetism, by Professor Lewin, were recorded on the MIT campus during the Spring of 2002. Prof. Lewin is well-known at MIT and beyond for his dynamic and engaging lecture style.\n\nThis class includes many topics, specially the classical theory of electromagnetism. In addition to the basic concepts of electromagnetism, a vast variety of interesting topics are covered in this course: lightning, pacemakers, electric shock treatment, electrocardiograms, metal detectors, musical instruments, magnetic levitation, bullet trains, electric motors, radios, TV, car coils, superconductivity, aurora borealis, rainbows, radio telescopes, interferometers, particle accelerators (a.k.a. atom smashers or colliders), mass spectrometers, red sunsets, blue skies, haloes around Sun and Moon, color perception, Doppler effect, Big-Bang cosmology.\n\n8.03 deals with lots of topics in physics dealing with vibrations and waves. Topics include: mechanical vibrations and waves; simple harmonic motion, superposition, forced vibrations and resonance, coupled oscillations and normal modes; vibrations of continuous systems; reflection and refraction; phase and group velocity. Optics; wave solutions to Maxwell's equations; polarization; Snell's Law, interference, Huygens' principle, Fraunhofer diffraction and gratings.\n\nIn addition to the traditional topics of mechanical vibrations and waves, coupled oscillators, and electro-magnetic radiation, students also learn about musical instruments, red sunsets, glories, coronae, rainbows, haloes, X-ray binaries, neutron stars, black holes and Big-Bang cosmology.\n\n"}
{"id": "56694560", "url": "https://en.wikipedia.org/wiki?curid=56694560", "title": "Weapon systems engineering", "text": "Weapon systems engineering\n\nWeapon systems engineering involves using engineering tools in technology to create and guarantee the safety and performance of weapons. It is currently being used by the military and the government to create new weapons to protect the United States. It is used to make nuclear and non-nuclear weapons and ensure their safety throughout their lifespan. \n\nMany companies help our government and military to manufacture new weapons and strategies. One is Parsons. The Missile Defense Agency, MDA, is a research agency that develops and tests the missile defense agency to defend the United States and its allies. Parsons helps with this through the MDA missile contract. They provide missile system support including tests and evaluating the performance. \n\nThe U.S. Navy awarded multi-million-dollar contracts with Tekla Research and Avian-Precise Co. to support the \"Naval Air Systems Command's Systems Engineering Department\". Tekla is to help NAVAIR assess technology, cost, and design. Avian-Precise is to help the command sustain a weapons system related to all U.S. naval platforms and systems. \n\nThe electromagnetic railgun launcher is a new long-range weapon using electricity instead of chemicals to launch projectiles. Projectiles can be launched at approximately 4,500 miles per hour using magnetic fields. These new weapons are allowing the military to eliminate explosives where possible. It is being used by the Navy as well for its ability to be effective and affordable. The Navy is working on modernizing their weapons, including nuclear weapons. Two weapons on the list to work on creating are a \"low-yield warhead for submarine-launched ballistic missiles\" and a \"nuclear capable submarine-launched cruise missile\". \n\nThe U.S. Army is using Weapons Systems Engineering to help protect our soldiers. They have designed a Humvee to be equipped to contain a gun with an automated tracking system. This would be controlled by another soldier or a computer program. This along with other new systems, vehicles, etc. is all possible because of the new advancements in Weapon Systems Engineering. In May the Army will test out automated Humvee's to certify them as combat ready vehicles. According to the Army Tank Automotive Research, Development and Engineering Center (TARDEC) this is the first step toward weaponized robotics. The Army is not looking to make killing machines, but machines controlled by humans. \n\nThe US Air Force Research Lab (AFRL) assigned Lockheed Martain the job of creating \"an aircraft-mounted high-power fibre laser\". They must make a weapon that is small enough and light enough, but also can be effective. The laser will be mounted on jet fighters with the ability to disable enemies targeting systems. \n"}
{"id": "45339895", "url": "https://en.wikipedia.org/wiki?curid=45339895", "title": "Zinaida Aksentyeva", "text": "Zinaida Aksentyeva\n\nZinaïda Mikolaïevna Aksentieva (June 25, 1900 – April 8, 1969) was a Ukrainian/Soviet astronomer.\n\nAksentieva or Aksentyeva was born in Odessa in 1900. She graduated from Odessa University in 1924. She worked on mapping gravity and her observatory was one of the first to be able to accurately find the centre of the earth. She worked in Poltava Observatory. She became observatory director in 1951. Her areas of study were tidal deformation of the earth and gravimeter Earth profiles.\n\nShe has a crater on Venus that is named in her honour.\n\nAksentieva died in 1969 in Poltava where her observatory was.\n"}
