{"id": "49448381", "url": "https://en.wikipedia.org/wiki?curid=49448381", "title": "A.G. Huntsman Award for Excellence in the Marine Sciences", "text": "A.G. Huntsman Award for Excellence in the Marine Sciences\n\nThe A.G. Huntsman Award for Excellence in the Marine Sciences was established in 1980 by the Canadian marine science community to recognize excellence of research and outstanding contributions to marine sciences. It is presented by the Royal Society of Canada. The award honours marine scientists of any nationality who have had and continue to have a significant influence on the course of marine scientific thought. It is named in honour of Archibald Gowanlock Huntsman (1883– 1973), a pioneer Canadian oceanographer and fishery biologist\n\nThe Award consists of a specially designed fine silver medal showing the CSS \"Hudson\"\nThe A.G. Huntsman Award is administered by the A.G. Huntsman Foundation, based at the Bedford Institute of Oceanography, Nova Scotia. The Foundation is organized as an independent, charitable, tax free foundation. Business of the Foundation is conducted by a Board of Directors and Executive Officers. The Lieutenant Governor of Nova Scotia serves as Honorary Patron of the Huntsman Foundation.\nThe Award is presented annually by the Royal Society of Canada. The annual process of selection is conducted by a separate Selection Committee of Canadian marine scientists. The award ceremony takes place in the late fall at the Bedford Institute of Oceanography.\n\nThe Award was created in 1980 under the leadership of scientists at the Bedford Institute of Oceanography. The award is now recognized as a major international prize. It is funded principally by interest earned on financial contributions originally received from Fisheries and Oceans Canada, Natural Resources Canada, the Province of Nova Scotia, and the Canadian Association of Petroleum Producers. Additional endowment was later granted from the LiFT Family Fund through Gift Funds Canada.\n\nThe A.G. Huntsman Medal is awarded to those men and women, of any nationality, who have had and still have a significant influence on the course of marine scientific thought; for unequalled excellence in their respective fields; for the influence of their work on the course of scientific thought in their respective fields; and for their continuing and current activities at the forefront of their respective fields.\n\nThe A.G. Huntsman Award reflects the multi-faceted nature of research in the world's oceans. From 1980 to 2013, the Award was presented annually in one of three categories – Marine Geoscience, Physical/Chemical Oceanography, and Biological Oceanography and Fisheries Science – except in its inaugural year when recipients were honoured in all three. To mark its 25th Anniversary in 2005, the Award was again presented in all three of the above categories, as well as in the category of Interdisciplinary Marine Science. Since 2014, the category distinctions have been dropped in recognition that many facets of marine science are multi-disciplinary or interdisciplinary in character.\n\n"}
{"id": "42564035", "url": "https://en.wikipedia.org/wiki?curid=42564035", "title": "Albert Heber Longman", "text": "Albert Heber Longman\n\nAlbert Heber Longman (24 June 1880 – 16 February 1954), also often referred to as Heber Longman or Heber Albert Longman, was an Australian newspaper publisher and museum director of British origin.\n\nLongman was born at Heytesbury in Wiltshire, England, and educated at Emwell House School at Warminster. Because of a chest weakness, in 1902 he emigrated to Australia and settled in Toowoomba, Queensland. There he, with support from local businesses, established a newspaper first called the \"Rag\", and later the \"Citizen\". In 1904 he married Irene Maud Bayley, who was to be the first woman elected to the Queensland Parliament. Interested in the natural history of the area, Longman collected botanical specimens and helped establish the local field naturalists club. In 1911 he published a book – \"The Religion of a Naturalist\" – expressing his philosophical position as an agnostic.\n\nIn 1911 Longman moved to Brisbane to take up a position as a member of the staff of the Queensland Museum, rising to become Acting Director in 1917 and Director in 1918. There the main focus of his interests turned from botany to zoology, especially vertebrate paleontology, describing new genera of fish, marine reptiles, dinosaurs and a marsupial. He wished to make the Museum more of an educational institution, rather than a repository of fossils. He acquired for the Museum several dinosaur skeletons, including the Rhoesaurus Brownei.\n\nHe published approximately 70 papers which appeared in the \"Memoirs of the Queensland Museum\". He also wrote a popular column – \"Nature’s ways\" – in the Brisbane \"Courier-Mail\". He retired from the museum in 1945 and died at his home in Brisbane in 1954. He was survived by his wife Irene Longman.\n\n\n \n"}
{"id": "41691948", "url": "https://en.wikipedia.org/wiki?curid=41691948", "title": "Antoine-Charles Vauthier", "text": "Antoine-Charles Vauthier\n\nAntoine-Charles Vauthier (1790 Paris - 1879 Paris) was a French entomologist, botanist, author and natural history illustrator. His brother Jules-Antoine Vauthier was also a prominent artist of his day. Their father was Michel Vauthier, a miniaturist at Versailles and later a publisher and art dealer in Paris.\n\nVauthier was a collector of plants and insects in Brazil, arriving in Rio de Janeiro in December 1831 and returning to France, arriving at the port of Toulon on 21 May 1833.\n\nHe wrote a number of books, including \"Description of a new species of arachnid of the genus Epeira, of M. Walckenaer\" (1825). Vauthier illustrated several books written by his brother-in-law Achille Richard. Vauthier also depicted many animals and is commemorated in some 40 species names.\n\n"}
{"id": "2517809", "url": "https://en.wikipedia.org/wiki?curid=2517809", "title": "Arsenide mineral", "text": "Arsenide mineral\n\nAn arsenide mineral is a mineral that contains arsenide as its main anion. Arsenides are grouped with the sulfides in both the Dana and Strunz mineral classification systems.\n\n"}
{"id": "27635655", "url": "https://en.wikipedia.org/wiki?curid=27635655", "title": "Auricle (botany)", "text": "Auricle (botany)\n\nIn botany, an auricle is a small ear-like projection from the base of a leaf or petal.\n\n"}
{"id": "1956063", "url": "https://en.wikipedia.org/wiki?curid=1956063", "title": "Aurorae Sinus", "text": "Aurorae Sinus\n\nAurorae Sinus is a dark feature in the southern hemisphere of the planet Mars. Together with albedo features contributed by Aonius Sinus and Solis Lacus, it is part of a feature known as the \"eye of Mars\".\n"}
{"id": "42766846", "url": "https://en.wikipedia.org/wiki?curid=42766846", "title": "Bacteriologist (Professional)", "text": "Bacteriologist (Professional)\n\nA bacteriologist is a professional trained in bacteriology a subdivision of microbiology. The duties of a bacteriologist include prevention, diagnosis and prognosis of diseases, as well as health care, and they may carry out various functions such as epidemiological surveillance, quality auditing with biotechnology development, basic research, management and teaching related to the career, scientist management, laboratory coordination and blood banks.\n\nThe name can vary depending on the official laws of each country and how they are established in the society and therefore they would have a different official name. Whether they have different official name among the countries, the background almost is similar.\nCalled Medical Laboratory Scientist or Bachelor of science in the United States depending on their training, bacteriologists to homologate their title in the United States are called Bachelor of Bacteriology.\n\nThe main aim of a bacteriologist is prognosticate, diagnose and the disease surveillance by a huge range of laboratory test, all of them in the context of health care. Hence, a bacteriologist plays a role in research, management of the health care, promoting health and disease prevention.\n\nThere are plenty of methods and procedures used by the bacteriologist in the clinical laboratory whose purpose is the diagnosis of various diseases, some of these techniques are:\n\nTechniques in Clinical Chemistry: used for determination of various analyses in various biological fluids, different reagents are used to play colorimetric techniques and enzyme kinetics, among others. The result of these reactions were measured by spectrometer or other similar physical techniques to quantitatively determine the value of an analyte, so as measured quantitatively analytes such as creation, BUN, lipid profile, total protein, glucose, direct bilirubin and overall, among others.\n\nTechniques in Immunology: many of the immunological techniques used in clinical laboratory are based on the ability of antibodies to bind to specific antigens either in vitro or vivo, is how this feature that is exploited by the professional laboratory diagnose various diseases and / or semi-quantitative or quantitative determination of various analytes, among the various techniques used in immunology section include: immunology Test as ELISA (Enzyme-linked immunodeficient assay) for its acronym in English, with agglutination techniques latex particles coated with antibody or antigen, agglutination techniques, articulation chromatography techniques, microorganisms stereotyping techniques, fluorescence, among many others. Some of the tests performed in this section mentioned techniques are: rheumatoid factor, antinuclear antibodies by agglutination with latex particles, c-reactive protein, chromatography for determination of Human Chorionic Gonadotropin, HIV 1 and 2, hepatitis B virus, ELISA for determination of hormones and the presence of other microorganisms causing diseases among other tests.\n\nTechniques in Microbiology: mainly used to determine infectious diseases the causative agent of the disease, regardless of whether it is caused by a bacterium, virus, fungi or parasites, bacteriologist through various coloration's, microscopic observation, specific crops for growth of each organism, biochemical tests whether manual, semi-automated or automated used to determine the genus and species of the organism causing the disease. a bacteriologist should be able to identify and name any parasitic observe microscopic structure, recently antimicrobial susceptibility tests have become very important for the emergence of multi-resistant bacterial strains in clinical microbiology section of the laboratory tests performed bacteriologists antimicrobial susceptibility to determine which antibiotics should treat various infectious diseases.\n\nTechniques in Hematology: Section of Hematology various examinations are performed as: Table He mic, FSP (peripheral blood smear), reticulated count, observation and identification of parasites, determining blood type and RH, erythrocyte sedimentation rate, among other . Bacteriologists in specialized clinical laboratory hematology specialists perform more specific tests for diagnosis of hematologic diseases such as leukemia, anemia, regardless of etiology or other genetic alterations by various techniques such wherein phenotype cell markers studies performed by flow optometry, molecular biology techniques molecular as electroscopes, PCR and other tests such as: osmotic fragility, analysis of bone marrow aspirates, among others.\n\nTechniques in Molecular Biology and Genetics: in specialized clinical laboratory services are included with molecular biology and genetics where the bacteriologist makes techniques such as chain reaction (PCR), electroscopes, Southern blot, Western blot, and DNA sequencing techniques genetically among others, which can be quite useful for diagnosis of various diseases.\n\nTechniques in Blood Bank and Transfusion Unit: Section blood bank and / or transfusion unit performs professional laboratory testing donated blood units to ensure they are in good condition to be transfused to a patient, tests are done as presence or absence of infectious agents that produce diseases such as HIV, Sagas disease, Hepatitis, HTLV I, syphilis and others. Equally prepared and necessary processes are separated by centrifugal units of RBCs, precipitate, and platelet units reductionist procedures among others. Different unit processes in transfusion procedures are performed to measure the compatibility of the donor - recipient, the bacteriologist must verify the absence of antibodies that produce severe thrombolytic reactions at the time of transfusion between these procedures include: cross match minor, major crosspatch, determining blood type and RH, direct Combs, Combos indirect detection of antibodies and antibodies.\n\nAll manual or automated procedures require the strictest quality control. Checks should be made internal and external quality, make the appropriate calibrations, maintenance of equipment when necessary and properly validate every bacteriologist should follow the protocols are conducted in the clinical laboratory for quality control concerns, besides being constantly updated theoretical and academic in this way make a correct clinical correlation and ensure the accuracy of the results reported to the doctor.\n\nOne of the most important properties that should be a bacteriologist is his biosecurity. A bacteriologist should work with the right equipment depending on the level of the laboratory where the work and the danger of working with samples, this equipment includes: gown, masks, goggles biosecurity, latex gloves, biohazard cabinet, if it is a laboratory where highly dangerous microorganisms work bacteriologist must use special biosafety suits.\n\nSome of the samples analyzed by bacteriologists are:\n\n\nMany medical diagnoses are based on the various examinations conducted by the bacteriologist in the clinical laboratory\n\nA bacteriologist has general knowledge in bacteriology, hematology, parasitology, mycology, virology, immunology, genetics, cytogenetics, molecular biology, quality control, biotechnology, among other disciplines. The bacteriologist can reinforce the knowledge acquired through specialization, diploma, masters and doctoral degrees recognized by official institutions.\n\nA bacteriologist performs in different fields as either professional or coordinator of clinical laboratory and blood banks, scientific research groups, microbiological quality control, quality control in clinical laboratory equipment epidemiology and public health, clinical laboratory veterinarian forensic clinical laboratory teaching in the fields of competence, among others. Besides this is the duty of bacteriologist transmit their knowledge to practitioners and bacteriologists future clinical laboratory professionals.\n\n\nA bacteriologist have the obligation of have the following attitudes;\n\nIn Colombia is regulated by the Bacteriologist National School \"“Colegio Nacional de Bacteriólogo (CNB)”\" which determine whether the professional is ready or not to perform the professional career by the obtaining of a professional card. that qualify to the professional to perform the professional career, it is regulated by the law (Article 2 of Act 1193 of 2008 which amends Act 841 of 2003), which determines;\n\"The National Association of Bacteriologists assumes the functions of issuing of the professional card referred in Article 5 of Act, the Professional Cards, meanwhile inscriptions or records Bacteriologists will be issued by the Ministries of Health of the different departments\"\n\n\"“While the National Association of Bacteriologists acts as the issue of the professional card referred to in Article 5 of this Law, the Professional Cards, inscriptions or records Bacteriologists will be issued by the Ministries of Health of the different departments”\"\n\nIn Bioethical see National Court and Ethics Bacteriology know will handle disciplinary proceedings on appeal - Bioethics - ethics - which identify professionals who practice in the profession of bacteriology in Colombia for petty offenses established in the existing laws matter. under Article 8 of Law 1193 of 2008.\n\nCurrently, to carry out the career in a different country of the initial studies, it is necessary to do a type approval or validation. As long as the professional be interested to work in the destiny country.\n\nIt is not necessary the approval or validate, when the professionals pretend to improve the knowledge in a Hague member country (Hague Convention), As long as the interested provided the correct documents legalization performed by the apostille convention, thus as ask for the postgraduates certificate access in accordance with paragraph c of Article 14 of Act 30 of 1992 (within Colombia), It is necessary determine whether the destiny country is on the haya member list.\n\nIn Colombia, 28 April is celebrated as Bacteriologist's day. However, in other countries this does not exist or is not celebrated. An international day is not present.\n\nThis are some of the laws/acts that regulated to bacteriologist; (Spanish)\n\nFurther information is at the web site of “CNB”, Colombian universities or well you can check each of the Laws/Acts given above.\n\n"}
{"id": "342851", "url": "https://en.wikipedia.org/wiki?curid=342851", "title": "Bilayer", "text": "Bilayer\n\nA bilayer is a double layer of closely packed atoms or molecules.\n\nThe properties of bilayers are often studied in condensed matter physics, particularly in the context of semiconductor devices, where two distinct materials are united to form junctions (such as p-n junctions, Schottky junctions, etc.).\n\nLayered materials, such as graphene, boron nitride, or transition metal dichalchogenides, have unique electronic properties as a bilayer system and are an active area of current research.\n\nIn biology a common example is the Lipid bilayer, which describes the structure of multiple organic structures, such as the membrane of a cell.\n\n"}
{"id": "151452", "url": "https://en.wikipedia.org/wiki?curid=151452", "title": "Burnside problem", "text": "Burnside problem\n\nThe Burnside problem, posed by William Burnside in 1902 and one of the oldest and most influential questions in group theory, asks whether a finitely generated group in which every element has finite order must necessarily be a finite group. Evgeny Golod and Igor Shafarevich provided a counter-example in 1964. The problem has many variants (see bounded and restricted below) that differ in the additional conditions imposed on the orders of the group elements.\n\nInitial work pointed towards the affirmative answer. For example, if a group \"G\" is generated by \"m\" elements and the order of each element of \"G\" is a divisor of 4, then \"G\" is finite. Moreover, A. I. Kostrikin was able to prove in 1958 that among the finite groups with a given number of generators and a given prime exponent, there exists a largest one. This provides a solution for the restricted Burnside problem for the case of prime exponent. (Later, in 1989, Efim Zelmanov was able to solve the restricted Burnside problem for an arbitrary exponent.) Issai Schur had showed in 1911 that any finitely generated periodic group that was a subgroup of the group of invertible \"n\" × \"n\" complex matrices was finite; he used this theorem to prove the Jordan–Schur theorem.\n\nNevertheless, the general answer to Burnside problem turned out to be negative. In 1964, Golod and Shafarevich constructed an infinite group of Burnside type without assuming that all elements have uniformly bounded order. In 1968, Pyotr Novikov and Sergei Adian's supplied a negative solution to the bounded exponent problem for all odd exponents larger than 4381. In 1982, A. Yu. Ol'shanskii found some striking counterexamples for sufficiently large odd exponents (greater than 10), and supplied a considerably simpler proof based on geometric ideas.\n\nThe case of even exponents turned out to be much harder to settle. In 1992, S. V. Ivanov announced the negative solution for sufficiently large even exponents divisible by a large power of 2 (detailed proofs were published in 1994 and occupied some 300 pages). Later joint work of Ol'shanskii and Ivanov established a negative solution to an analogue of Burnside problem for hyperbolic groups, provided the exponent is sufficiently large. By contrast, when the exponent is small and different from 2,3,4 and 6, very little is known.\n\nA group \"G\" is called periodic if every element has finite order; in other words, for each \"g\" in \"G\", there exists some positive integer \"n\" such that \"g\" = 1. Clearly, every finite group is periodic. There exist easily defined groups such as the \"p\"-group which are infinite periodic groups; but the latter group cannot be finitely generated.\n\nGeneral Burnside problem. If \"G\" is a finitely generated, periodic group, then is \"G\" necessarily finite?\n\nThis question was answered in the negative in 1964 by Evgeny Golod and Igor Shafarevich, who gave an example of an infinite \"p\"-group that is finitely generated (see Golod–Shafarevich theorem). However, the orders of the elements of this group are not \"a priori\" bounded by a single constant.\n\nPart of the difficulty with the general Burnside problem is that the requirements of being finitely generated and periodic give very little information about the possible structure of a group. Therefore, we pose more requirements on \"G\". Consider a periodic group \"G\" with the additional property that there exists a least integer \"n\" such that for all \"g\" in \"G\", \"g\" = 1. A group with this property is said to be \"periodic with bounded exponent\" \"n\", or just a \"group with exponent\" \"n\". Burnside problem for groups with bounded exponent asks:\n\nBurnside problem. If \"G\" is a finitely generated group with exponent \"n\", is \"G\" necessarily finite?\n\nIt turns out that this problem can be restated as a question about the finiteness of groups in a particular family. The free Burnside group of rank \"m\" and exponent \"n\", denoted B(\"m\", \"n\"), is a group with \"m\" distinguished generators \"x\", ..., \"x\" in which the identity \"x\" = 1 holds for all elements \"x\", and which is the \"largest\" group satisfying these requirements. More precisely, the characteristic property of B(\"m\", \"n\") is that, given any group \"G\" with \"m\" generators \"g\", ..., \"g\" and of exponent \"n\", there is a unique homomorphism from B(\"m\", \"n\") to \"G\" that maps the \"i\"th generator \"x\" of B(\"m\", \"n\") into the \"i\"th generator \"g\" of \"G\". In the language of group presentations, free Burnside group B(\"m\", \"n\") has \"m\" generators \"x\", ..., \"x\" and the relations \"x\" = 1 for each word \"x\" in \"x\", ..., \"x\", and any group \"G\" with \"m\" generators of exponent \"n\" is obtained from it by imposing additional relations. The existence of the free Burnside group and its uniqueness up to an isomorphism are established by standard techniques of group theory. Thus if \"G\" is any finitely generated group of exponent \"n\", then \"G\" is a homomorphic image of B(\"m\", \"n\"), where \"m\" is the number of generators of \"G\". Burnside problem can now be restated as follows:\n\nBurnside problem II. For which positive integers \"m\", \"n\" is the free Burnside group B(\"m\", \"n\") finite?\n\nThe full solution to Burnside problem in this form is not known. Burnside considered some easy cases in his original paper:\n\n\nThe following additional results are known (Burnside, Sanov, M. Hall):\n\n\nThe particular case of B(2, 5) remains open: it was not known whether this group is finite.\n\nThe breakthrough in Burnside problem was achieved by Pyotr Novikov and Sergei Adian in 1968. Using a complicated combinatorial argument, they demonstrated that for every odd number \"n\" with \"n\" > 4381, there exist infinite, finitely generated groups of exponent \"n\". Adian later improved the bound on the odd exponent to 665. The case of even exponent turned out to be considerably more difficult. It was only in 1994 that Sergei Vasilievich Ivanov was able to prove an analogue of Novikov–Adian theorem: for any \"m\" > 1 and an even \"n\" ≥ 2, \"n\" divisible by 2, the group B(\"m\", \"n\") is infinite; together with the Novikov–Adian theorem, this implies infiniteness for all \"m\" > 1 and \"n\" ≥ 2. This was improved in 1996 by I. G. Lysënok to \"m\" > 1 and \"n\" ≥ 8000. Novikov–Adian, Ivanov and Lysënok established considerably more precise results on the structure of the free Burnside groups. In the case of the odd exponent, all finite subgroups of the free Burnside groups were shown to be cyclic groups. In the even exponent case, each finite subgroup is contained in a product of two dihedral groups, and there exist non-cyclic finite subgroups. Moreover, the word and conjugacy problems were shown to be effectively solvable in B(\"m\", \"n\") both for the cases of odd and even exponents \"n\".\n\nA famous class of counterexamples to Burnside problem is formed by finitely generated non-cyclic infinite groups in which every nontrivial proper subgroup is a finite cyclic group, the so-called Tarski Monsters. First examples of such groups were constructed by A. Yu. Ol'shanskii in 1979 using geometric methods, thus affirmatively solving O. Yu. Schmidt's problem. In 1982 Ol'shanskii was able to strengthen his results to establish existence, for any sufficiently large prime number \"p\" (one can take \"p\" > 10) of a finitely generated infinite group in which every nontrivial proper subgroup is a cyclic group of order \"p\". In a paper published in 1996, Ivanov and Ol'shanskii solved an analogue of Burnside problem in an arbitrary hyperbolic group for sufficiently large exponents.\n\nFormulated in the 1930s, it asks another, related, question:\n\nRestricted Burnside problem. If it is known that a group \"G\" with \"m\" generators and exponent \"n\" is finite, can one conclude that the order of \"G\" is bounded by some constant depending only on \"m\" and \"n\"? Equivalently, are there only finitely many \"finite\" groups with \"m\" generators of exponent \"n\", up to isomorphism?\n\nThis variant of the Burnside problem can also be stated in terms of certain universal groups with \"m\" generators and exponent \"n\". By basic results of group theory, the intersection of two subgroups of finite index in any group is itself a subgroup of finite index. Let \"M\" be the intersection of all subgroups of the free Burnside group B(\"m\", \"n\") which have finite index, then \"M\" is a normal subgroup of B(\"m\", \"n\") (otherwise, there exists a subgroup \"g\"\"Mg\" with finite index containing elements not in \"M\"). One can therefore define a group B(\"m\", \"n\") to be the factor group B(\"m\", \"n\")/\"M\". Every finite group of exponent \"n\" with \"m\" generators is a homomorphic image of B(\"m\", \"n\").\nThe restricted Burnside problem then asks whether B(\"m\", \"n\") is a finite group.\n\nIn the case of the prime exponent \"p\", this problem was extensively studied by A. I. Kostrikin during the 1950s, prior to the negative solution of the general Burnside problem. His solution, establishing the finiteness of B(\"m\", \"p\"), used a relation with deep questions about identities in Lie algebras in finite characteristic. The case of arbitrary exponent has been completely settled in the affirmative by Efim Zelmanov, who was awarded the Fields Medal in 1994 for his work.\n\n\n"}
{"id": "49862973", "url": "https://en.wikipedia.org/wiki?curid=49862973", "title": "Coalition to Diversify Computing", "text": "Coalition to Diversify Computing\n\nThe Coalition to Diversify Computing (CDC) is a joint organization of the Association for Computing Machinery (ACM) and the Computing Research Association (CRA). CDC emphasizes recruiting minority undergraduates to MS/PhD programs, retaining minority graduate students enrolled in MS/PhD programs, and transitioning minority MS/PhD graduates into academia, industry, and government careers.\n\n\n"}
{"id": "12148336", "url": "https://en.wikipedia.org/wiki?curid=12148336", "title": "Control premium", "text": "Control premium\n\nA control premium is an amount that a buyer is sometimes willing to pay over the current market price of a publicly traded company in order to acquire a controlling share in that company. \n\nIf the market perceives that a public company's profit and cash flow is not being maximized, capital structure is not optimal, or other factors that can be changed are impacting the company's share price, an acquirer may be willing to offer a premium over the price currently established by other market participants. A discount for lack of control, sometimes referred to as a minority discount, reflects the reduction in value from a firm's perceived optimal or intrinsic value when cash flow or other factors prevent optimal value from being reached.\n\nTransactions involving small blocks of shares in public companies occur regularly and serve to establish the market price per share of company stock. Acquiring a controlling number of shares sometimes requires offering a premium over the current market price per share in order to induce existing shareholders to sell. It is made through a tender offer with specific terms, including the price.\n\nThe amount of control is the acquirer's decision and is based on its belief that the target company's share price is not optimized. An acquirer would not be making a prudent investment decision if a tender offer made is higher than the future benefit of the acquisition.\n\nIn general, the maximum value that an acquirer firm would be willing to pay should equal the sum of the target firm's intrinsic value, synergies that the acquiring firm can expect to achieve between the two firms, and the opportunity cost of not acquiring the target firm (i.e. loss to the acquirer if a rival firm acquires the target firm instead). A premium paid, if any, will be specific to the acquirer and the target; actual premiums paid have varied widely.\n\nCompany XYZ has an EBITDA of $1,500,000 and its shares are currently trading at an EV/EBITDA multiple of 5x. This results in a valuation of XYZ of $7,500,000 (=$1,500,000 * 5) on a EV basis. A potential buyer may believe that EBITDA can be improved to $2,000,000 by eliminating the CEO, who would become redundant after the transaction. Thus, the buyer could potentially value the target at $10,000,000 since the value expected to be achieved by replacing the CEO is the accretive $500,000 (=$2,000,000–$1,500,000) in EBITDA, which in turn translates to $2,500,000 (=$500,000 * 5 or =$10,000,000–$7,500,000) premium over the pre-transaction value of the target.\n\n\n"}
{"id": "9858191", "url": "https://en.wikipedia.org/wiki?curid=9858191", "title": "Cydia pomonella granulovirus", "text": "Cydia pomonella granulovirus\n\nCydia pomonella granulovirus (CpGV) is a granulovirus belonging to the family \"Baculoviridae\". It has a double-stranded DNA genome that is 123,500 base pairs in length with 143 ORFs. The virus forms small bodies called granules containing a single virion. CpGV is a virus of invertebrates – specifically \"Cydia pomonella\", commonly known as the Codling moth. CpGV is highly pathogenic, it is known as a fast GV – that is, one that will kill its host in the same instar as infection; thus, it is frequently used as a biological pesticide.\n\n\"C. pomonella\" has proved to be a problematic pest on several fruit trees, including apples and pears. The caterpillars burrow into the fruit, rendering it unfit for sale. Traditional insecticides are of limited use, as some strains have acquired resistance to several insecticides.\n\nCpGV has been shown to kill many of the larvae of \"C. pomonella\" in trials without having adverse effects on humans or other animals, thanks to the specific nature of the virus. Also, no development of resistance was observed.\n\nThe first CpGV strain, isolated in Mexico, has been commercially formulated into biological pesticides such as Madex (Andermatt Biocontrol AG), Carpovirusine (NPP/Arysta LifeScience) and Cyd-X (Certis). Due to the continued use of CpGV in the field, populations resistant to the Mexican strain of \"C. pomonella\" have been identified. New strains which are able to manage resistant codling moth populations have been developed. These strains are commercialized by NPP/Arysta LifeScience into Carpovirusine Evo 2 and by Andermatt Biocontrol AG into Madex Max and Madex Plus.\n\n\n"}
{"id": "13524258", "url": "https://en.wikipedia.org/wiki?curid=13524258", "title": "Dieter Korn", "text": "Dieter Korn\n\nDr. Dieter Korn (born 1958) is a German scientist and paleontologist specializing in research on ammonites and goniatites. He received his Ph.D. in 1996 from the University of Tübingen and is employed by the Museum für Naturkunde in Berlin, Germany, in the Leibniz Institute for Research on Evolution and Biodiversity (Humboldt University). Dr Korn has published or coauthored over 100 papers since 1979, including the description of numerous new species of cephalopods.\n\n"}
{"id": "14408410", "url": "https://en.wikipedia.org/wiki?curid=14408410", "title": "Floral Genome Project", "text": "Floral Genome Project\n\nThe Floral Genome Project is a collaborative research cooperation primarily between Penn State University, University of Florida, and Cornell University. The initial funding came from a grant of $7.4 million from the National Science Foundation. The Floral Genome Project was initiated to bridge the genomic gap between the most broadly studied plant model systems. According to the website, the following are the aims of the project:\n\n"}
{"id": "4576465", "url": "https://en.wikipedia.org/wiki?curid=4576465", "title": "Flower", "text": "Flower\n\nA flower, sometimes known as a bloom or blossom, is the reproductive structure found in flowering plants (plants of the division Magnoliophyta, also called angiosperms). The biological function of a flower is to effect reproduction, usually by providing a mechanism for the union of sperm with eggs. Flowers may facilitate outcrossing (fusion of sperm and eggs from different individuals in a population) or allow selfing (fusion of sperm and egg from the same flower). Some flowers produce diaspores without fertilization (parthenocarpy). Flowers contain sporangia and are the site where gametophytes develop. Many flowers have evolved to be attractive to animals, so as to cause them to be vectors for the transfer of pollen. After fertilization, the ovary of the flower develops into fruit containing seeds.\n\nIn addition to facilitating the reproduction of flowering plants, flowers have long been admired and used by humans to bring beauty to their environment, and also as objects of romance, ritual, religion, medicine and as a source of food.\n\nThe essential parts of a flower can be considered in two parts: the vegetative part, consisting of petals and associated structures in the perianth, and the reproductive or sexual parts. A stereotypical flower consists of four kinds of structures attached to the tip of a short stalk. Each of these kinds of parts is arranged in a whorl on the receptacle. The four main whorls (starting from the base of the flower or lowest node and working upwards) are as follows:\n\nCollectively the calyx and corolla form the perianth (see diagram).\n\n\nAlthough the arrangement described above is considered \"typical\", plant species show a wide variation in floral structure. These modifications have significance in the evolution of flowering plants and are used extensively by botanists to establish relationships among plant species.\n\nThe four main parts of a flower are generally defined by their positions on the receptacle and not by their function. Many flowers lack some parts or parts may be modified into other functions and/or look like what is typically another part. In some families, like Ranunculaceae, the petals are greatly reduced and in many species the sepals are colorful and petal-like. Other flowers have modified stamens that are petal-like; the double flowers of Peonies and Roses are mostly petaloid stamens. Flowers show great variation and plant scientists describe this variation in a systematic way to identify and distinguish species.\n\nSpecific terminology is used to describe flowers and their parts. Many flower parts are fused together; fused parts originating from the same whorl are connate, while fused parts originating from different whorls are adnate; parts that are not fused are free. When petals are fused into a tube or ring that falls away as a single unit, they are sympetalous (also called gamopetalous). Connate petals may have distinctive regions: the cylindrical base is the tube, the expanding region is the throat and the flaring outer region is the limb. A sympetalous flower, with bilateral symmetry with an upper and lower lip, is bilabiate. Flowers with connate petals or sepals may have various shaped corolla or calyx, including campanulate, funnelform, tubular, urceolate, salverform or rotate.\n\nReferring to \"fusion,\" as it is commonly done, appears questionable because at least some of the processes involved may be non-fusion processes. For example, the addition of intercalary growth at or below the base of the primordia of floral appendages such as sepals, petals, stamens and carpels may lead to a common base that is not the result of fusion.\n\nMany flowers have a symmetry. When the perianth is bisected through the central axis from any point and symmetrical halves are produced, the flower is said to be actinomorphic or regular, e.g. rose or trillium. This is an example of radial symmetry. When flowers are bisected and produce only one line that produces symmetrical halves, the flower is said to be irregular or zygomorphic, e.g. snapdragon or most orchids.\n\nFlowers may be directly attached to the plant at their base (sessile—the supporting stalk or stem is highly reduced or absent). The stem or stalk subtending a flower is called a peduncle. If a peduncle supports more than one flower, the stems connecting each flower to the main axis are called pedicels. The apex of a flowering stem forms a terminal swelling which is called the \"torus\" or receptacle.\n\nIn those species that have more than one flower on an axis, the collective cluster of flowers is termed an \"inflorescence\". Some inflorescences are composed of many small flowers arranged in a formation that resembles a single flower. The common example of this is most members of the very large composite (Asteraceae) group. A single daisy or sunflower, for example, is not a flower but a flower \"head\"—an inflorescence composed of numerous flowers (or florets). An inflorescence may include specialized stems and modified leaves known as bracts.\n\nA \"floral formula\" is a way to represent the structure of a flower using specific letters, numbers and symbols, presenting substantial information about the flower in a compact form. It can represent a taxon, usually giving ranges of the numbers of different organs, or particular species. Floral formulae have been developed in the early 19th century and their use has declined since. Prenner \"et al.\" (2010) devised an extension of the existing model to broaden the descriptive capability of the formula. The format of floral formulae differs in different parts of the world, yet they convey the same information.\n\nThe structure of a flower can also be expressed by the means of \"floral diagrams\". The use of schematic diagrams can replace long descriptions or complicated drawings as a tool for understanding both floral structure and evolution. Such diagrams may show important features of flowers, including the relative positions of the various organs, including the presence of fusion and symmetry, as well as structural details.\n\nA flower develops on a modified shoot or \"axis\" from a determinate apical meristem (\"determinate\" meaning the axis grows to a set size). It has compressed internodes, bearing structures that in classical plant morphology are interpreted as highly modified leaves. Detailed developmental studies, however, have shown that stamens are often initiated more or less like modified stems (caulomes) that in some cases may even resemble branchlets. Taking into account the whole diversity in the development of the androecium of flowering plants, we find a continuum between modified leaves (phyllomes), modified stems (caulomes), and modified branchlets (shoots).\n\nThe transition to flowering is one of the major phase changes that a plant makes during its life cycle. The transition must take place at a time that is favorable for fertilization and the formation of seeds, hence ensuring maximal reproductive success. To meet these needs a plant is able to interpret important endogenous and environmental cues such as changes in levels of plant hormones and seasonable temperature and photoperiod changes. Many perennial and most biennial plants require vernalization to flower. The molecular interpretation of these signals is through the transmission of a complex signal known as florigen, which involves a variety of genes, including CONSTANS, FLOWERING LOCUS C and FLOWERING LOCUS T. Florigen is produced in the leaves in reproductively favorable conditions and acts in buds and growing tips to induce a number of different physiological and morphological changes.\n\nThe first step of the transition is the transformation of the vegetative stem primordia into floral primordia. This occurs as biochemical changes take place to change cellular differentiation of leaf, bud and stem tissues into tissue that will grow into the reproductive organs. Growth of the central part of the stem tip stops or flattens out and the sides develop protuberances in a whorled or spiral fashion around the outside of the stem end. These protuberances develop into the sepals, petals, stamens, and carpels. Once this process begins, in most plants, it cannot be reversed and the stems develop flowers, even if the initial start of the flower formation event was dependent of some environmental cue. Once the process begins, even if that cue is removed the stem will continue to develop a flower.\n\nYvonne Aitken has shown that flowering transition depends on a number of factors, and that plants flowering earliest under given conditions had the least dependence on climate whereas later-flowering varieties reacted strongly to the climate setup.\n\nThe molecular control of floral organ identity determination appears to be fairly well understood in some species. In a simple model, three gene activities interact in a combinatorial manner to determine the developmental identities of the organ primordia within the floral meristem. These gene functions are called A, B and C-gene functions. In the first floral whorl only A-genes are expressed, leading to the formation of sepals. In the second whorl both A- and B-genes are expressed, leading to the formation of petals. In the third whorl, B and C genes interact to form stamens and in the center of the flower C-genes alone give rise to carpels. The model is based upon studies of mutants in \"Arabidopsis thaliana\" and snapdragon, \"Antirrhinum majus\". For example, when there is a loss of B-gene function, mutant flowers are produced with sepals in the first whorl as usual, but also in the second whorl instead of the normal petal formation. In the third whorl the lack of B function but presence of C-function mimics the fourth whorl, leading to the formation of carpels also in the third whorl.\n\nMost genes central in this model belong to the MADS-box genes and are transcription factors that regulate the expression of the genes specific for each floral organ.\n\nThe principal purpose of a flower is the reproduction of the individual and the species. All flowering plants are \"heterosporous\", producing two types of spores. Microspores are produced by meiosis inside anthers while megaspores are produced inside ovules, inside an ovary. In fact, anthers typically consist of four microsporangia and an ovule is an integumented megasporangium. Both types of spores develop into gametophytes inside sporangia. As with all heterosporous plants, the gametophytes also develop inside the spores (are endosporic).\n\nIn the majority of species, individual flowers have both functional carpels and stamens. Botanists describe these flowers as being \"perfect\" or \"bisexual\" and the species as \"hermaphroditic\". Some flowers lack one or the other reproductive organ and called \"imperfect\" or \"unisexual\". If unisex flowers are found on the same individual plant but in different locations, the species is said to be \"monoecious\". If each type of unisex flower is found only on separate individuals, the plant is \"dioecious\".\n\nFlowering plants usually face selective pressure to optimize the transfer of their pollen, and this is typically reflected in the morphology of the flowers and the behaviour of the plants. Pollen may be transferred between plants via a number of 'vectors'. Some plants make use of abiotic vectors — namely wind (anemophily) or, much less commonly, water (hydrophily). Others use biotic vectors including insects (entomophily), birds (ornithophily), bats (chiropterophily) or other animals. Some plants make use of multiple vectors, but many are highly specialised.\n\nCleistogamous flowers are self-pollinated, after which they may or may not open. Many Viola and some Salvia species are known to have these types of flowers.\n\nThe flowers of plants that make use of biotic pollen vectors commonly have glands called nectaries that act as an incentive for animals to visit the flower. Some flowers have patterns, called nectar guides, that show pollinators where to look for nectar. Flowers also attract pollinators by scent and color. Still other flowers use mimicry to attract pollinators. Some species of orchids, for example, produce flowers resembling female bees in color, shape, and scent. Flowers are also specialized in shape and have an arrangement of the stamens that ensures that pollen grains are transferred to the bodies of the pollinator when it lands in search of its attractant (such as nectar, pollen, or a mate). In pursuing this attractant from many flowers of the same species, the pollinator transfers pollen to the stigmas—arranged with equally pointed precision—of all of the flowers it visits.\n\nAnemophilous flowers use the wind to move pollen from one flower to the next. Examples include grasses, birch trees, ragweed and maples. They have no need to attract pollinators and therefore tend not to be \"showy\" flowers. Male and female reproductive organs are generally found in separate flowers, the male flowers having a number of long filaments terminating in exposed stamens, and the female flowers having long, feather-like stigmas. Whereas the pollen of animal-pollinated flowers tends to be large-grained, sticky, and rich in protein (another \"reward\" for pollinators), anemophilous flower pollen is usually small-grained, very light, and of little nutritional value to animals.\n\nThe primary purpose of a flower is reproduction. Since the flowers are the reproductive organs of plant, they mediate the joining of the sperm, contained within pollen, to the ovules — contained in the ovary. Pollination is the movement of pollen from the anthers to the stigma. The joining of the sperm to the ovules is called fertilization. Normally pollen is moved from one plant to another, but many plants are able to self pollinate. The fertilized ovules produce seeds that are the next generation. Sexual reproduction produces genetically unique offspring, allowing for adaptation. Flowers have specific designs which encourages the transfer of pollen from one plant to another of the same species. Many plants are dependent upon external factors for pollination, including: wind and animals, and especially insects. Even large animals such as birds, bats, and pygmy possums can be employed. The period of time during which this process can take place (the flower is fully expanded and functional) is called \"anthesis\". The study of pollination by insects is called \"anthecology\".\n\nThe pollination mechanism employed by a plant depends on what method of pollination is utilized.\n\nMost flowers can be divided between two broad groups of pollination methods:\n\n\"Entomophilous\": flowers attract and use insects, bats, birds or other animals to transfer pollen from one flower to the next. Often they are specialized in shape and have an arrangement of the stamens that ensures that pollen grains are transferred to the bodies of the pollinator when it lands in search of its attractant (such as nectar, pollen, or a mate). In pursuing this attractant from many flowers of the same species, the pollinator transfers pollen to the stigmas—arranged with equally pointed precision—of all of the flowers it visits. Many flowers rely on simple proximity between flower parts to ensure pollination. Others, such as the \"Sarracenia\" or lady-slipper orchids, have elaborate designs to ensure pollination while preventing self-pollination.\n\n\"Anemophilous\": flowers use the wind to move pollen from one flower to the next, examples include the grasses, Birch trees, Ragweed and Maples. They have no need to attract pollinators and therefore tend not to grow large blossoms. Whereas the pollen of entomophilous flowers tends to be large-grained, sticky, and rich in protein (another \"reward\" for pollinators), anemophilous flower pollen is usually small-grained, very light, and of little nutritional value to insects, though it may still be gathered in times of dearth. Honeybees and bumblebees actively gather anemophilous corn (maize) pollen, though it is of little value to them.\n\nSome flowers with both stamens and a pistil are capable of self-fertilization, which does increase the chance of producing seeds but limits genetic variation. The extreme case of self-fertilization occurs in flowers that always self-fertilize, such as many dandelions. Some flowers are self-pollinated and use flowers that never open or are self-pollinated before the flowers open, these flowers are called cleistogamous. Many Viola species and some Salvia have these types of flowers. Conversely, many species of plants have ways of preventing self-fertilization. Unisexual male and female flowers on the same plant may not appear or mature at the same time, or pollen from the same plant may be incapable of fertilizing its ovules. The latter flower types, which have chemical barriers to their own pollen, are referred to as self-sterile or self-incompatible.\n\nPlants cannot move from one location to another, thus many flowers have evolved to attract animals to transfer pollen between individuals in dispersed populations. Flowers that are insect-pollinated are called \"entomophilous\"; literally \"insect-loving\" in Greek. They can be highly modified along with the pollinating insects by co-evolution. Flowers commonly have glands called \"nectaries\" on various parts that attract animals looking for nutritious nectar. Birds and bees have color vision, enabling them to seek out \"colorful\" flowers.\n\nSome flowers have patterns, called nectar guides, that show pollinators where to look for nectar; they may be visible only under ultraviolet light, which is visible to bees and some other insects. Flowers also attract pollinators by scent and some of those scents are pleasant to our sense of smell. Not all flower scents are appealing to humans; a number of flowers are pollinated by insects that are attracted to rotten flesh and have flowers that smell like dead animals, often called Carrion flowers, including \"Rafflesia\", the titan arum, and the North American pawpaw (\"Asimina triloba\"). Flowers pollinated by night visitors, including bats and moths, are likely to concentrate on scent to attract pollinators and most such flowers are white.\n\nOther flowers use mimicry to attract pollinators. Some species of orchids, for example, produce flowers resembling female bees in color, shape, and scent. Male bees move from one such flower to another in search of a mate.\n\nMany flowers have close relationships with one or a few specific pollinating organisms. Many flowers, for example, attract only one specific species of insect, and therefore rely on that insect for successful reproduction. This close relationship is often given as an example of coevolution, as the flower and pollinator are thought to have developed together over a long period of time to match each other's needs.\n\nThis close relationship compounds the negative effects of extinction. The extinction of either member in such a relationship would mean almost certain extinction of the other member as well. Some endangered plant species are so because of shrinking pollinator populations.\n\nThere is much confusion about the role of flowers in allergies. For example, the showy and entomophilous goldenrod (\"Solidago\") is frequently blamed for respiratory allergies, of which it is innocent, since its pollen cannot be airborne. The types of pollen that most commonly cause allergic reactions are produced by the plain-looking plants (trees, grasses, and weeds) that do not have showy flowers. These plants make small, light, dry pollen grains that are custom-made for wind transport.\n\nThe type of allergens in the pollen is the main factor that determines whether the pollen is likely to cause hay fever. For example, pine tree pollen is produced in large amounts by a common tree, which would make it a good candidate for causing allergy. It is, however, a relatively rare cause of allergy because the types of allergens in pine pollen appear to make it less allergenic. Instead the allergen is usually the pollen of the contemporary bloom of anemophilous ragweed (\"Ambrosia\"), which can drift for many miles. Scientists have collected samples of ragweed pollen 400 miles out at sea and 2 miles high in the air. A single ragweed plant can generate a million grains of pollen per day.\n\nAmong North American plants, weeds are the most prolific producers of allergenic pollen. Ragweed is the major culprit, but other important sources are sagebrush, redroot pigweed, lamb's quarters, Russian thistle (tumbleweed), and English plantain.\n\nIt is common to hear people say they are allergic to colorful or scented flowers like roses. In fact, only florists, gardeners, and others who have prolonged, close contact with flowers are likely to be sensitive to pollen from these plants. Most people have little contact with the large, heavy, waxy pollen grains of such flowering plants because this type of pollen is not carried by wind but by insects such as butterflies and bees.\n\nWhile land plants have existed for about 425 million years, the first ones reproduced by a simple adaptation of their aquatic counterparts: spores. In the sea, plants—and some animals—can simply scatter out genetic clones of themselves to float away and grow elsewhere. This is how early plants reproduced. But plants soon evolved methods of protecting these copies to deal with drying out and other damage which is even more likely on land than in the sea. The protection became the seed, though it had not yet evolved the flower. Early seed-bearing plants include the ginkgo and conifers.\n\nSeveral groups of extinct gymnosperms, particularly seed ferns, have been proposed as the ancestors of flowering plants but there is no continuous fossil evidence showing exactly how flowers evolved. The apparently sudden appearance of relatively modern flowers in the fossil record posed such a problem for the theory of evolution that it was called an \"abominable mystery\" by Charles Darwin. Recently discovered angiosperm fossils such as \"Archaefructus\", along with further discoveries of fossil gymnosperms, suggest how angiosperm characteristics may have been acquired in a series of steps. An early fossil of a flowering plant, \"Archaefructus liaoningensis\" from China, is dated about 125 million years old. Even earlier from China is the 125–130 million years old \"Archaefructus sinensis\". Now, another plant (130 million-year-old \"Montsechia vidalii\", discovered in Spain) takes the title of world's oldest flower from \"Archaefructus sinensis\".\nRecent DNA analysis (molecular systematics) shows that \"Amborella trichopoda\", found on the Pacific island of New Caledonia, is the only species in the sister group to the rest of the flowering plants, and morphological studies suggest that it has features which may have been characteristic of the earliest flowering plants.\n\nWhile there is only hard proof of such flowers about 140 million years ago, there is some circumstantial evidence of flowers as much as 250 million years ago. A chemical used by plants to defend their flowers, oleanane, has been detected in fossil plants that old, including gigantopterids, which evolved at that time and bear many of the traits of modern, flowering plants, though they are not known to be flowering plants themselves, because only their stems and prickles have been found preserved in detail; one of the earliest examples of petrification.\n\nThe similarity in leaf and stem structure can be very important, because flowers are genetically just an adaptation of normal leaf and stem components on plants, a combination of genes normally responsible for forming new shoots. The most primitive flowers are thought to have had a variable number of flower parts, often separate from (but in contact with) each other. The flowers would have tended to grow in a spiral pattern, to be bisexual (in plants, this means both male and female parts on the same flower), and to be dominated by the ovary (female part). As flowers grew more advanced, some variations developed parts fused together, with a much more specific number and design, and with either specific sexes per flower or plant, or at least \"ovary inferior\".\n\nThe general assumption is that the function of flowers, from the start, was to involve animals in the reproduction process. Pollen can be scattered without bright colors and obvious shapes, which would therefore be a liability, using the plant's resources, unless they provide some other benefit. One proposed reason for the sudden, fully developed appearance of flowers is that they evolved in an isolated setting like an island, or chain of islands, where the plants bearing them were able to develop a highly specialized relationship with some specific animal (a wasp, for example), the way many island species develop today. This symbiotic relationship, with a hypothetical wasp bearing pollen from one plant to another much the way fig wasps do today, could have eventually resulted in both the plant(s) and their partners developing a high degree of specialization. Island genetics is believed to be a common source of speciation, especially when it comes to radical adaptations which seem to have required inferior transitional forms. Note that the wasp example is not incidental; bees, apparently evolved specifically for symbiotic plant relationships, are descended from wasps.\n\nLikewise, most fruit used in plant reproduction comes from the enlargement of parts of the flower. This fruit is frequently a tool which depends upon animals wishing to eat it, and thus scattering the seeds it contains.\n\nWhile many such symbiotic relationships remain too fragile to survive competition with mainland organisms, flowers proved to be an unusually effective means of production, spreading (whatever their actual origin) to become the dominant form of land plant life.\n\nFlower evolution continues to the present day; modern flowers have been so profoundly influenced by humans that many of them cannot be pollinated in nature. Many modern, domesticated flowers used to be simple weeds, which only sprouted when the ground was disturbed. Some of them tended to grow with human crops, and the prettiest did not get plucked because of their beauty, developing a dependence upon and special adaptation to human affection.\nMany flowering plants reflect as much light as possible within the range of visible wavelengths of the pollinator the plant intends to attract. Flowers that reflect the full range of visible light are generally perceived as \"white\" by a human observer. An important feature of white flowers is that they reflect equally across the visible spectrum. While many flowering plants use white to attract pollinators, the use of color is also widespread (even within the same species). Color allows a flowering plant to be more specific about the pollinator it seeks to attract. The color model used by human color reproduction technology (CMYK) relies on the modulation of pigments that divide the spectrum into broad areas of absorption. Flowering plants by contrast are able to shift the transition point wavelength between absorption and reflection. If it is assumed that the visual systems of most pollinators view the visible spectrum as circular then it may be said that flowering plants produce color by absorbing the light in one region of the spectrum and reflecting the light in the other region. With CMYK, color is produced as a function of the amplitude of the broad regions of absorption. Flowering plants by contrast produce color by modifying the frequency (or rather wavelength) of the light reflected. Most flowers absorb light in the blue to yellow region of the spectrum and reflect light from the green to red region of the spectrum. For many species of flowering plant, it is the transition point that characterizes the color that they produce. Color may be modulated by shifting the transition point between absorption and reflection and in this way a flowering plant may specify which pollinator it seeks to attract.\nSome flowering plants also have a limited ability to modulate areas of absorption. This is typically not as precise as control over wavelength. Humans observers will perceive this as degrees of saturation (the amount of \"white\" in the color).\n\nMany flowers have important symbolic meanings in Western culture. The practice of assigning meanings to flowers is known as floriography. Some of the more common examples include:\nBecause of their varied and colorful appearance, flowers have long been a favorite subject of visual artists as well. Some of the most celebrated paintings from well-known painters are of flowers, such as Van Gogh's sunflowers series or Monet's water lilies. Flowers are also dried, freeze dried and pressed in order to create permanent, three-dimensional pieces of flower art.\n\nFlowers within art are also representative of the female genitalia, as seen in the works of artists such as Georgia O'Keeffe, Imogen Cunningham, Veronica Ruiz de Velasco, and Judy Chicago, and in fact in Asian and western classical art. Many cultures around the world have a marked tendency to associate flowers with femininity.\n\nThe great variety of delicate and beautiful flowers has inspired the works of numerous poets, especially from the 18th–19th century Romantic era. Famous examples include William Wordsworth's \"I Wandered Lonely as a Cloud\" and William Blake's \"Ah! Sun-Flower\".\n\nTheir symbolism in dreams has also been discussed, with possible interpretations including \"blossoming potential\".\n\nThe Roman goddess of flowers, gardens, and the season of Spring is Flora. The Greek goddess of spring, flowers and nature is Chloris.\n\nIn Hindu mythology, flowers have a significant status. Vishnu, one of the three major gods in the Hindu system, is often depicted standing straight on a lotus flower. Apart from the association with Vishnu, the Hindu tradition also considers the lotus to have spiritual significance. For example, it figures in the Hindu stories of creation.\n\nIn modern times, people have sought ways to cultivate, buy, wear, or otherwise be around flowers and blooming plants, partly because of their agreeable appearance and smell. Around the world, people use flowers to mark important events in their lives:\n\n\nPeople therefore grow flowers around their homes, dedicate parts of their living space to flower gardens, pick wildflowers, or buy commercially-grown flowers from florists.\n\nFlowers provide less food than other major plant parts (seeds, fruits, roots, stems and leaves), but still provide several important vegetables and spices. Flower vegetables include broccoli, cauliflower and artichoke. The most expensive spice, saffron, consists of dried stigmas of a crocus. Other flower spices are cloves and capers. Hops flowers are used to flavor beer. Marigold flowers are fed to chickens to give their egg yolks a golden yellow color, which consumers find more desirable; dried and ground marigold flowers are also used as a spice and colouring agent in Georgian cuisine. Flowers of the dandelion and elder are often made into wine. Bee pollen, pollen collected from bees, is considered a health food by some people. Honey consists of bee-processed flower nectar and is often named for the type of flower, e.g. orange blossom honey, clover honey and tupelo honey.\n\nHundreds of fresh flowers are edible, but only few are widely marketed as food. They are often added to salads as garnishes. Squash blossoms are dipped in breadcrumbs and fried. Some edible flowers include nasturtium, chrysanthemum, carnation, cattail, Japanese honeysuckle, chicory, cornflower, canna, and sunflower. Edible flowers such as daisy, rose, and violet are sometimes candied.\n\nFlowers such as chrysanthemum, rose, jasmine, Japanese honeysuckle, and chamomile, chosen for their fragrance and medicinal properties, are used as tisanes, either mixed with tea or on their own.\n\nFlowers have been used since prehistoric times in funeral rituals: traces of pollen have been found on a woman's tomb in the El Miron Cave in Spain. Many cultures draw a connection between flowers and life and death, and because of their seasonal return flowers also suggest rebirth, which may explain why many people place flowers upon graves. The ancient Greeks, as recorded in Euripides's play \"The Phoenician Women\", placed a crown of flowers on the head of the deceased; they also covered tombs with wreaths and flower petals. Flowers were widely used in ancient Egyptian burials, and the Mexicans to this day use flowers prominently in their Day of the Dead celebrations in the same way that their Aztec ancestors did.\n\n\n\n"}
{"id": "5214209", "url": "https://en.wikipedia.org/wiki?curid=5214209", "title": "Great Diamond", "text": "Great Diamond\n\nThe Great Diamond is an asterism. Astronomy popularizer Hans A. Rey called it the Virgin's Diamond. It is composed of the following stars:\n\n\nThe Great Diamond is somewhat larger than the Big Dipper. The three southernmost stars are sometimes regarded as being their own asterism, the Spring Triangle.\n\nLying within the Great Diamond is the set of stars traditionally assigned to Coma Berenices. Many nearby galaxies, including galaxies within the Virgo Cluster, are located within this asterism, and some of these galaxies can easily be observed with amateur telescopes.\n\n"}
{"id": "8312093", "url": "https://en.wikipedia.org/wiki?curid=8312093", "title": "History of wind power", "text": "History of wind power\n\nWind power has been used as long as humans have put sails into the wind. For more than two millennia wind-powered machines have ground grain and pumped water. Wind power was widely available and not confined to the banks of fast-flowing streams, or later, requiring sources of fuel. Wind-powered pumps drained the polders of the Netherlands, and in arid regions such as the American mid-west or the Australian outback, wind pumps provided water for livestock and steam engines.\n\nWith the development of electric power, wind power found new applications in lighting buildings remote from centrally-generated power. Throughout the 20th century parallel paths developed small wind plants suitable for farms or residences, and larger utility-scale wind generators that could be connected to electricity grids for remote use of power. Today wind powered generators operate in every size range between tiny plants for battery charging at isolated residences, up to near-gigawatt sized offshore wind farms that provide electricity to national electrical networks.\n\nBy 2014, over 240,000 commercial-sized wind turbines were operating in the world, producing 4% of the world's electricity.\n\nSailboats and sailing ships have been using wind power for at least 5,500 years, and architects have used wind-driven natural ventilation in buildings since similarly ancient times. The use of wind to provide mechanical power came somewhat later in antiquity.\n\nThe Babylonian emperor Hammurabi planned to use wind power for his ambitious irrigation project in the 17th century BC.\n\nThe windwheel of the engineer Heron of Alexandria in 1st-century AD Roman Egypt is the earliest known instance of using a wind-driven wheel to power a machine. Another early example of a wind-driven wheel was the prayer wheel, which has been used in ancient India, Tibet, and China since the 4th century.\n\nThe first practical windmills were in use in Sistan, a region in Iran and bordering Afghanistan, at least by the 9th century and possibly as early as the 7th century. These \"Panemone windmills\" were horizontal windmills, which had long vertical driveshafts with six to twelve rectangular sails covered in reed matting or cloth. These windmills were used to pump water, and in the gristmilling and sugarcane industries. The use of windmills became widespread across the Middle East and Central Asia, and later spread to China and India. Vertical windmills were later used extensively in Northwestern Europe to grind flour beginning in the 1180s, and many examples still exist. By 1000 AD, windmills were used to pump seawater for salt-making in China and Sicily.\n\nWind-powered automata are known from the mid-8th century: wind-powered statues that \"turned with the wind over the domes of the four gates and the palace complex of the Round City of Baghdad\". The \"Green Dome of the palace was surmounted by the statue of a horseman carrying a lance that was believed to point toward the enemy. This public spectacle of wind-powered statues had its private counterpart in the 'Abbasid palaces where automata of various types were predominantly displayed.\"\n\nThe first windmills in Europe appear in sources dating to the twelfth century. These early European windmills were sunk post mills. The earliest certain reference to a windmill dates from 1185, in Weedley, Yorkshire, although a number of earlier but less certainly dated twelfth-century European sources referring to windmills have also been adduced. While it is sometimes argued that crusaders may have been inspired by windmills in the Middle East, this is unlikely since the European vertical windmills were of significantly different design than the horizontal windmills of Afghanistan. Lynn White Jr., a specialist in medieval European technology, asserts that the European windmill was an \"independent invention;\" he argues that it is unlikely that the Afghanistan-style horizontal windmill had spread as far west as the Levant during the Crusader period. In medieval England rights to waterpower sites were often confined to nobility and clergy, so wind power was an important resource to a new middle class. In addition, windmills, unlike water mills, were not rendered inoperable by the freezing of water in the winter.\n\nBy the 14th century Dutch windmills were in use to drain areas of the Rhine River delta.\n\nWindmills were used to pump water for salt making on the island of Bermuda, and on Cape Cod during the American revolution.\nIn Mykonos and in other islands of Greece windmills were used to mill flour and remained in use until the early 20th century. Many of them are now refurbished to be inhabited.\n\nThe first wind turbine used for the production of electricity was built in Scotland in July 1887 by Prof James Blyth of Anderson's College, Glasgow (the precursor of Strathclyde University). Blyth's 10 m high, cloth-sailed wind turbine was installed in the garden of his holiday cottage at Marykirk in Kincardineshire and was used to charge accumulators developed by the Frenchman Camille Alphonse Faure, to power the lighting in the cottage, thus making it the first house in the world to have its electricity supplied by wind power. Blyth offered the surplus electricity to the people of Marykirk for lighting the main street, however, they turned down the offer as they thought electricity was \"the work of the devil.\" Although he later built a wind turbine to supply emergency power to the local Lunatic Asylum, Infirmary and Dispensary of Montrose, the invention never really caught on as the technology was not considered to be economically viable.\n\nAcross the Atlantic, in Cleveland, Ohio a larger and heavily engineered machine was designed and constructed in the winter of 1887-1888 by Charles F. Brush, this was built by his engineering company at his home and operated from 1886 until 1900. The Brush wind turbine had a rotor 17 m (56 foot) in diameter and was mounted on an 18 m (60 foot) tower. Although large by today's standards, the machine was only rated at 12 kW; it turned relatively slowly since it had 144 blades. The connected dynamo was used either to charge a bank of batteries or to operate up to 100 incandescent light bulbs, three arc lamps, and various motors in Brush's laboratory. The machine fell into disuse after 1900 when electricity became available from Cleveland's central stations, and was abandoned in 1908.\n\nIn 1891 Danish scientist, Poul la Cour, constructed a wind turbine to generate electricity, which was used to produce hydrogen by electrolysis to be stored for use in experiments and to light the Askov High school. He later solved the problem of producing a steady supply of power by inventing a regulator, the Kratostate, and in 1895 converted his windmill into a prototype electrical power plant that was used to light the village of Askov.\n\nIn Denmark there were about 2,500 windmills by 1900, used for mechanical loads such as pumps and mills, producing an estimated combined peak power of about 30 MW.\n\nIn the American midwest between 1850 and 1900, a large number of small windmills, perhaps six million, were installed on farms to operate irrigation pumps. Firms such as Star, Eclipse, Fairbanks-Morse, and Aeromotor became famed suppliers in North and South America.\n\nDevelopment in the 20th century might be usefully divided into the periods:\n\nIn Denmark wind power was an important part of a decentralized electrification in the first quarter of the 20th century, partly because of Poul la Cour from his first practical development in 1891 at Askov. By 1908 there were 72 wind-driven electric generators from 5 kW to 25 kW. The largest machines were on 24 m (79 ft) towers with four-bladed 23 m (75 ft) diameter rotors. In 1957 Johannes Juul installed a 24 m diameter wind turbine at Gedser, which ran from 1957 until 1967. This was a three-bladed, horizontal-axis, upwind, stall-regulated turbine similar to those now used for commercial wind power development.\n\nIn 1927 the brothers Joe Jacobs and Marcellus Jacobs opened a factory, Jacobs Wind in Minneapolis to produce wind turbine generators for farm use. These would typically be used for lighting or battery charging, on farms out of reach of central-station electricity and distribution lines. In 30 years the firm produced about 30,000 small wind turbines, some of which ran for many years in remote locations in Africa and on the Richard Evelyn Byrd expedition to Antarctica. Many other manufacturers produced small wind turbine sets for the same market, including companies called Wincharger, Miller Airlite, Universal Aeroelectric, Paris-Dunn, Airline and Winpower.\n\nIn 1931 the Darrieus wind turbine was invented, with its vertical axis providing a different mix of design tradeoffs from the conventional horizontal-axis wind turbine. The vertical orientation accepts wind from any direction with no need for adjustments, and the heavy generator and gearbox equipment can rest on the ground instead of atop a tower.\n\nBy the 1930s windmills were widely used to generate electricity on farms in the United States where distribution systems had not yet been installed. Used to replenish battery storage banks, these machines typically had generating capacities of a few hundred watts to several kilowatts. Beside providing farm power, they were also used for isolated applications such as electrifying bridge structures to prevent corrosion. In this period, high tensile steel was cheap, and windmills were placed atop prefabricated open steel lattice towers.\n\nThe most widely used small wind generator produced for American farms in the 1930s was a two-bladed horizontal-axis machine manufactured by the Wincharger Corporation. It had a peak output of 200 watts. Blade speed was regulated by curved air brakes near the hub that deployed at excessive rotational velocities. These machines were still being manufactured in the United States during the 1980s. In 1936, the U.S. started a rural electrification project that killed the natural market for wind-generated power, since network power distribution provided a farm with more dependable usable energy for a given amount of capital investment.\n\nIn Australia, the Dunlite Corporation built hundreds of small wind generators to provide power at isolated postal service stations and farms. These machines were manufactured from 1936 until 1970.\n\nA forerunner of modern horizontal-axis utility-scale wind generators was the WIME D-30 in service in Balaklava, near Yalta, USSR from 1931 until 1942. This was a 100 kW generator on a 30 m (100 ft) tower, connected to the local 6.3 kV distribution system. It had a three-bladed 30 metre rotor on a steel lattice tower. It was reported to have an annual load factor of 32 per cent, not much different from current wind machines.\n\nIn 1941 the world's first megawatt-size wind turbine was connected to the local electrical distribution system on the mountain known as Grandpa's Knob in Castleton, Vermont, United States. It was designed by Palmer Cosslett Putnam and manufactured by the S. Morgan Smith Company. This 1.25 MW Smith-Putnam turbine operated for 1100 hours before a blade failed at a known weak point, which had not been reinforced due to war-time material shortages. No similar-sized unit was to repeat this \"bold experiment\" for about forty years.\n\nDuring the Second World War, small wind generators were used on German U-boats to recharge submarine batteries as a fuel-conserving measure. In 1946 the lighthouse and residences on the island of Neuwerk were partly powered by an 18 kW wind turbine 15 metres in diameter, to economize on diesel fuel. This installation ran for around 20 years before being replaced by a submarine cable to the mainland.\n\nThe Station d'Etude de l'Energie du Vent at Nogent-le-Roi in France operated an experimental 800 KVA wind turbine from 1956 to 1966.\n\nFrom 1974 through the mid-1980s the United States government worked with industry to advance the technology and enable large commercial wind turbines. The NASA wind turbines were developed under a program to create a utility-scale wind turbine industry in the U.S. With funding from the National Science Foundation and later the United States Department of Energy (DOE), a total of 13 experimental wind turbines were put into operation, in four major wind turbine designs. This research and development program pioneered many of the multi-megawatt turbine technologies in use today, including: steel tube towers, variable-speed generators, composite blade materials, partial-span pitch control, as well as aerodynamic, structural, and acoustic engineering design capabilities. The large wind turbines developed under this effort set several world records for diameter and power output. The MOD-2 wind turbine cluster of three turbines produced 7.5 megawatts of power in 1981. In 1987, the MOD-5B was the largest single wind turbine operating in the world with a rotor diameter of nearly 100 meters and a rated power of 3.2 megawatts. It demonstrated an availability of 95 percent, an unparalleled level for a new first-unit wind turbine. The MOD-5B had the first large-scale variable speed drive train and a sectioned, two-blade rotor that enabled easy transport of the blades. The 4 megawatt WTS-4 held the world record for power output for over 20 years. Although the later units were sold commercially, none of these two-bladed machines were ever put into mass production. When oil prices declined by a factor of three from 1980 through the early 1990s, many turbine manufacturers, both large and small, left the business. The commercial sales of the NASA/Boeing Mod-5B, for example, came to an end in 1987 when Boeing Engineering and Construction announced they were \"planning to leave the market because low oil prices are keeping windmills for electricity generation uneconomical.\"\n\nLater, in the 1980s, California provided tax rebates for wind power. These rebates funded the first major use of wind power for utility electricity. These machines, gathered in large wind parks such as at Altamont Pass would be considered small and un-economic by modern wind power development standards.\n\nA giant change took place in 1978 when the world's was constructed. It pioneered many technologies used in modern wind turbines and allowed Vestas, Siemens and others to get the parts they needed. Especially important was the novel wing construction using help from German aeronautics specialists. The power plant was capable of delivering 2MW, had a tubular tower, pitch controlled wings and three blades. It was built by the teachers and students of the Tvind school. Before completion these \"amateurs\" were much ridiculed. The turbine still runs today and looks almost identical to the newest most modern mills.\n\nDanish commercial wind power development stressed incremental improvements in capacity and efficiency based on extensive serial production of turbines, in contrast with development models requiring extensive steps in unit size based primarily on theoretical extrapolation. A practical consequence is that all commercial wind turbines resemble the \"Danish model\", a light-weight three-blade upwind design.\n\nAll major horizontal axis turbines today rotate the same way (clockwise) to present a coherent view. However, early turbines rotated counter-clockwise like the old windmills, but a shift occurred from 1978 and on. The individualist-minded blade supplier Økær made the decision to change direction in order to be distinguished from the collective Tvind and their small wind turbines. Some of the blade customers were companies that later evolved into Vestas, Siemens, Enercon and Nordex. Public demand required that all turbines rotate the same way, and the success of these companies made clockwise the new standard.\n\nIn the 1970s many people began to desire a self-sufficient life-style. Solar cells were too expensive for small-scale electrical generation, so some turned to windmills. At first they built ad-hoc designs using wood and automobile parts. Most people discovered that a reliable wind generator is a moderately complex engineering project, well beyond the ability of most amateurs. Some began to search for and rebuild farm wind generators from the 1930s, of which Jacobs Wind Electric Company machines were especially sought after. Hundreds of Jacobs machines were reconditioned and sold during the 1970s.\n\nFollowing experience with reconditioned 1930s wind turbines, a new generation of American manufacturers started building and selling small wind turbines not only for battery-charging but also for interconnection to electricity networks. An early example would be Enertech Corporation of Norwich, Vermont, which began building 1.8 kW models in the early 1980s.\n\nIn the 1990s, as aesthetics and durability became more important, turbines were placed atop tubular steel or reinforced concrete towers. Small generators are connected to the tower on the ground, then the tower is raised into position. Larger generators are hoisted into position atop the tower and there is a ladder or staircase inside the tower to allow technicians to reach and maintain the generator, while protected from the weather.\n\nAs the 21st century began, fossil fuel was still relatively cheap, but rising concerns over energy security, global warming, and eventual fossil fuel depletion led to an expansion of interest in all available forms of renewable energy. The fledgling commercial wind power industry began expanding at a robust growth rate of about 25% per year, driven by the ready availability of large wind resources, and falling costs due to improved technology and wind farm management.\n\nThe steady run-up in oil prices after 2003 led to increasing fears that peak oil was imminent, further increasing interest in commercial wind power. Even though wind power generates electricity rather than liquid fuels, and thus is not an immediate substitute for petroleum in most applications (especially transport), fears over petroleum shortages only added to the urgency to expand wind power. Earlier oil crises had already caused many utility and industrial users of petroleum to shift to coal or natural gas. Wind power showed potential for replacing natural gas in electricity generation on a cost basis.\n\nTechnological innovations, enabled by advances in computer aided engineering, continue to drive new developments in the application of wind power. By 2015, the largest wind turbine were 8MW capacity Vestas V164 for offshore use. By 2014, over 240,000 commercial-sized wind turbines were operating in the world, producing 4% of the world's electricity. Total installed capacity exceeded 336GW in 2014 with China, the U.S., Germany, Spain and Italy leading in installations.\n\nOffshore wind power began to expand beyond fixed-bottom, shallow-water turbines beginning late in the first decade of the 2000s. The world's first operational deep-water \"large-capacity\" floating wind turbine, Hywind, became operational in the North Sea off Norway in late 2009\n\nat a cost of some 400 million kroner (around US$62 million) to build and deploy.\n\nThese floating turbines are a very different construction technology—closer to floating oil rigs rather—than traditional fixed-bottom, shallow-water monopile foundations that are used in the other large offshore wind farms to date.\n\nBy late 2011, Japan announced plans to build a multiple-unit floating wind farm, with six 2-megawatt turbines, off the Fukushima coast of northeast Japan where the 2011 tsunami and nuclear disaster has created a scarcity of electric power. \nAfter the evaluation phase is complete in 2016, \"Japan plans to build as many as 80 floating wind turbines off Fukushima by 2020\" at a cost of some 10-20 billion Yen.\n\nAirborne wind energy systems use airfoils or turbines supported in the air by buoyancy or by aerodynamic lift. The purpose is to eliminate the expense of tower construction, and allow extraction of wind energy from steadier, faster, winds higher in the atmosphere. As yet no grid-scale plants have been constructed. Many design concepts have been demonstrated.\n\n\n\n"}
{"id": "51025077", "url": "https://en.wikipedia.org/wiki?curid=51025077", "title": "Human uses of animals", "text": "Human uses of animals\n\nHuman uses of animals include both practical uses, such as the production of food and clothing, and symbolic uses, such as in art, literature, mythology, and religion. Animals used in these ways include fish, crustaceans, insects, molluscs, mammals and birds, as do other living things. \n\nEconomically, animals provide much of the meat eaten by the human population, whether farmed or hunted, and until the arrival of mechanised transport, terrestrial mammals provided a large part of the power used for work and transport. Animals serve as models in biological research, such as in genetics, and in drug testing.\n\nMany species are kept as pets, the most popular being mammals, especially dogs and cats. These are often anthropomorphised.\n\nAnimals such as horses and deer are among the earliest subjects of art, being found in the Upper Paleolithic cave paintings such as at Lascaux. Major artists such as Albrecht Dürer, George Stubbs and Edwin Landseer are known for their portraits of animals. Animals further play a wide variety of roles in literature, film, mythology, and religion.\n\nCulture consists of the social behaviour and norms found in human societies and transmitted through social learning. Cultural universals in all human societies include expressive forms like art, music, dance, ritual, religion, and technologies like tool usage, cooking, shelter, and clothing. The concept of material culture covers physical expressions such as technology, architecture and art, whereas immaterial culture includes principles of social organization, mythology, philosophy, literature, and science. \nAnthropology has traditionally studied the roles of animals in human culture in two opposed ways: as physical resources that humans used; and as symbols or concepts through totemism and animism. More recently, anthropologists have also seen animals as participants in human social interactions.\nThis article describes the roles played by animals in human culture, so defined, both practical and symbolic.\n\nThe human population exploits a large number of animal species for food, both of domesticated livestock species in animal husbandry and, mainly at sea, by hunting wild species.\n\nMarine fish of many species, such as herring, cod, tuna, mackerel and anchovy, are caught commercially, forming an important part of the diet, including protein and fatty acids, of much of the world's population. A smaller number of species are farmed commercially, including salmon and carp.\n\nInvertebrates including cephalopods like squid and octopus; crustaceans such as prawns, crabs, and lobsters; and bivalve or gastropod molluscs such as clams, oysters, cockles, and whelks are all hunted or farmed for food.\n\nMammals form a large part of the livestock raised for meat across the world. They include (2011) around 1.4 billion cattle, 1.2 billion sheep, 1 billion domestic pigs, and (1985) over 700 million rabbits.\n\nTextiles from the most utilitarian to the most luxurious are made from animal fibres such as wool, camel hair, angora, cashmere, and mohair. Hunter-gatherers have used animal sinews as lashings and bindings. Leather from cattle, pigs and other species is widely used to make shoes, handbags, belts and many other items. Animals have been hunted and farmed for their fur, to make items such as coats and hats, again ranging from simply warm and practical to the most elegant and expensive.\n\nDyestuffs including carmine (cochineal), shellac, and kermes have been made from the bodies of insects. In classical times, Tyrian purple was extracted from sea snails such as \"Stramonita haemastoma\" (Muricidae) for the clothing of royalty, as recorded by Aristotle and Pliny the Elder.\n\nWorking domestic animals including cattle, horses, yaks, camels, and elephants have been used for work and transport from the origins of agriculture, their numbers declining with the arrival of mechanised transport and agricultural machinery. In 2004 they still provided some 80% of the power for the mainly small farms in the third world, and some 20% of the world's transport, again mainly in rural areas. In mountainous regions unsuitable for wheeled vehicles, pack animals continue to transport goods.\n\nAnimals such as the fruit fly \"Drosophila melanogaster\", the zebrafish, the chicken and the house mouse, serve a major role in science as experimental models, both in fundamental biological research, such as in genetics, and in the development of new medicines, which must be tested exhaustively to demonstrate their safety. Millions of mammals, especially mice and rats, are used in experiments each year.\n\nA knockout mouse is a genetically modified mouse with an inactivated gene, replaced or disrupted with an artificial piece of DNA. They enable the study of sequenced genes whose functions are unknown.\n\nVaccines have been made using animals since their discovery by Edward Jenner in the 18th century. He noted that inoculation with live cowpox afforded protection against the more dangerous smallpox. In the 19th century, Louis Pasteur developed an attenuated (weakened) vaccine for rabies. In the 20th century, vaccines for the viral diseases mumps and polio were developed using animal cells grown in vitro.\n\nAn increasing variety of drugs are based on toxins and other molecules of animal origin. The cancer drug Yondelis was isolated from the tunicate \"Ecteinascidia turbinata\". One of dozens of toxins made by the deadly cone snail \"Conus geographus\" is used as Prialt in pain relief.\n\nAnimals, and products made from them, are used to assist in hunting. People have used hunting dogs to help chase down animals such as deer, wolves, and foxes; birds of prey from eagles to small falcons are used in falconry, hunting birds or mammals; and tethered cormorants have been used to catch fish.\n\nDendrobatid poison dart frogs, especially those in the genus \"Phyllobates\", secrete toxins such as Pumiliotoxin 251D and Allopumiliotoxin 267A powerful enough to be used to poison the tips of blowpipe darts.\n\nA wide variety of animals are kept as pets, from invertebrates such as tarantulas and octopuses, insects including praying mantises, reptiles such as snakes and chameleons, and birds including canaries, parakeets and parrots all finding a place. \nAnthropomorphism is the innate tendency to attribute human traits, emotions, and intentions to animals, and it is an important aspect of the way that people relate to animals such as pets.\n\nHowever, mammals are the most popular pets in the Western world, with the most kept species being dogs, cats, and rabbits. For example, in America in 2012 there were some 78 million dogs, 86 million cats, and 3.5 million rabbits. There is a tension between the role of animals as companions to humans, and their existence as individuals with rights of their own.\n\nA wide variety of both terrestrial and aquatic animals are hunted for sport.\n\nThe aquatic animals most often hunted for sport are fish, including many species from large marine predators such as sharks and tuna, to freshwater fish such as trout and carp.\n\nBirds such as partridges, pheasants and ducks, and mammals such as deer and wild boar, are among the terrestrial game animals most often hunted for sport and for food.\n\nAnimals, often mammals but including fish and insects among other groups, have been the subjects of art from the earliest times, both historical, as in Ancient Egypt, and prehistoric, as in the cave paintings at Lascaux and other sites in the Dordogne, France and elsewhere. Major animal paintings include Albrecht Dürer's 1515 \"The Rhinoceros\", and George Stubbs's c. 1762 horse portrait \"Whistlejacket\".\n\nAnimals as varied as bees, beetles, mice, foxes, crocodiles and elephants play a wide variety of roles in literature and film, from \"Aesop's Fables\" of the classical era to Rudyard Kipling's \"Just So Stories\" and Beatrix Potter's \"little books\" starting with the 1901 \"Tale of Peter Rabbit\".\n\nA genre of films has been based on oversized insects, including the pioneering 1954 \"Them!\", featuring giant ants mutated by radiation, and the 1957 \"The Deadly Mantis\".\n\nBirds have occasionally featured in film, as in Alfred Hitchcock's 1963 \"The Birds\", loosely based on Daphne du Maurier's story of the same name, which tells the tale of sudden attacks on people by violent flocks of birds. Ken Loach's admired 1969 \"Kes\", based on Barry Hines's 1968 novel \"A Kestrel for a Knave\", tells a story of a boy coming of age by training a kestrel.\n\nAnimals including many insects and mammals feature in mythology and religion.\n\nAmong the insects, in both Japan and Europe, as far back as ancient Greece and Rome, a butterfly was seen as the personification of a person's soul, both while they were alive and after their death. The scarab beetle was sacred in ancient Egypt, while the praying mantis was considered a god in southern African Khoi and San tradition for its praying posture.\n\nAmong the mammals, cattle, deer, horses, lions, bats and wolves (including werewolves), are the subjects of myths and worship. Reptiles too, such as the crocodile, have been worshipped as gods in cultures including ancient Egypt and Hinduism.\n\nOf the twelve signs of the Western zodiac, six, namely Aries (ram), Taurus (bull), Cancer (crab), Leo (lion), Scorpio (scorpion), and Pisces (fish) are animals, while two others, Sagittarius (horse/man) and Capricorn (fish/goat) are hybrid animals; the name zodiac indeed means a circle of animals. All twelve signs of the Chinese zodiac are animals.\n"}
{"id": "40397328", "url": "https://en.wikipedia.org/wiki?curid=40397328", "title": "IUPAC Inorganic Chemistry Division", "text": "IUPAC Inorganic Chemistry Division\n\nThe Inorganic Chemistry Division of the International Union of Pure and Applied Chemistry (IUPAC), also known as Division II, deals with all aspects of inorganic chemistry, including materials and bioinorganic chemistry, and also with isotopes, atomic weights and the periodic table. It furthermore advises the Chemical Nomenclature and Structure Representation Division (Division VIII) on issues dealing with inorganic compounds and materials.\nFor the general public, the most visible result of the division's work is that it evaluates and advises the IUPAC on names and symbols proposed for new elements that have been approved for addition to the periodic table. For the scientific end educational community the work on isotopic abundances and atomic weights is of fundamental importance as these numbers are continuously checked and updated.\n\nThe division has the following subcommittees and commissions:\n\nList of Running Projects of IUPAC Division II\n\nThe Inorganic Chemistry Division was a partner in the 2011 Global Chemistry Experiment “Water: A Chemical Solution” that took part during the International Year of Chemistry.\n\n\n"}
{"id": "185945", "url": "https://en.wikipedia.org/wiki?curid=185945", "title": "Information architecture", "text": "Information architecture\n\nInformation architecture (IA) is the structural design of shared information environments; the art and science of organizing and labelling websites, intranets, online communities and software to support usability and findability; and an emerging community of practice focused on bringing principles of design and architecture to the digital landscape. Typically, it involves a model or concept of information that is used and applied to activities which require explicit details of complex information systems. These activities include library systems and database development.\n\nInformation architecture is considered to have been founded by Richard Saul Wurman. Today there is a growing network of active IA specialists who constitute the Information Architecture Institute.\n\n\"Information architecture\" has somewhat different meanings in different branches of Information systems or Information technology:\n\nThe difficulty in establishing a common definition for \"information architecture\" arises partly from the term's existence in multiple fields. In the field of systems design, for example, information architecture is a component of enterprise architecture that deals with the information component when describing the structure of an enterprise.\n\nWhile the definition of information architecture is relatively well-established in the field of systems design, it is much more debatable within the context of online information systems (i.e., websites). Andrew Dillon refers to the latter as the \"big IA–little IA debate\". In the little IA view, information architecture is essentially the application of information science to web design which considers, for example, issues of classification and information retrieval. In the big IA view, information architecture involves more than just the organization of a website; it also factors in user experience, thereby considering usability issues of information design.\n\nAbout the term information architect Richard Saul Wurman wrote: \"I mean architect as used in the words \"architect of foreign policy\". I mean architect as in the creating of systemic, structural, and orderly principles to make something work — the thoughtful making of either artifact, or idea, or policy that informs because it is clear.\"\n\n\n\n\n\n\n"}
{"id": "31833521", "url": "https://en.wikipedia.org/wiki?curid=31833521", "title": "Information deficit model", "text": "Information deficit model\n\nIn studies of the public understanding of science, the information deficit model (or simply deficit model) or science literacy/knowledge deficit model attributes public scepticism or hostility to science and technology to a lack of understanding, resulting from a lack of information. It is associated with a division between experts who have the information and non-experts who do not. The model implies that communication should focus on improving the transfer of information from experts to non-experts.\n\nThe original term ‘deficit model’ was coined in the 1980s by social scientists studying the public communication of science. The purpose of the phrase was not to introduce a new mode of science communication but rather it was to characterise a widely held belief that underlies much of what is carried out in the name of such activity. \n\nThere are two aspects to this belief. The first is the idea that public uncertainty and scepticism towards modern science including environmental issues and technology is caused primarily by a lack of sufficient knowledge about science and the relevant subjects. The second aspect relates to the idea that by providing the adequate information to overcome this lack of knowledge, also known as a ‘knowledge deficit’, the general public opinion will change and decide that the information provided on the environment and science as a whole is reliable and accurate.\n\nScientists are often heard to complain that the general public does not understand science, and that the public needs to be educated. In the deficit model scientists assume that there is a knowledge deficit that can be ‘fixed’ by giving the public more information: scientists often assume that “given the facts (whatever they are), the public will happily support new technologies.” \n\nThe deficit model, however, has been discredited by a wealth of literature that shows that simply giving more information to people does not necessarily change their views. This is partly because people want to feel that they have had their say (and have been heard) in any decision-making process, and partly because people make decisions based on a host of factors as well as the scientific ‘facts’. These factors include ethical and religious beliefs, in addition to culture, history and personal experience. This amounts to a kind of gut feeling, which scientific facts are unlikely to change. Put another way, people’s sense of risk extends beyond the purely scientific considerations of conventional risk analysis, and the deficit model marginalises these ‘externalities’. It is now widely accepted that the best alternative to deficit model thinking is to genuinely engage with the public and take these externalities into account.\n\nThe deficit model sees the general population as the receiver of information and scientific knowledge. The information they receive, through whatever medium, has been prearranged to inform them of information that the distributors believe to be in the public’s interest. Due to the recent growth of scientific research and subsequent discoveries, the deficit model suggests that this has led to a decrease in interest surrounding certain areas of science. This can be down to the public feeling overwhelmed with information and becomes uninterested, as it appears too much to take in. \n\nThe deficit model of scientific understanding makes assumptions about the public’s knowledge. The model perceives them to be “blank slates” where their knowledge of scientific discourse and research is almost non-existent. Again, this is the knowledge deficit that needs to be informed by a reliable, knowledgeable and hierarchical scientific community in the form of simple commands and generic instructions. But the increase in new information systems such as the Internet and their ease of accessibility has led to a greater knowledge of scientific research and this is evident as the public’s understanding can be seen to be growing. This is a good thing in terms of the members of the public that can actively increase their own knowledge base, decrease the knowledge deficit and assess the truth and validity of what mass media outlets and governments are telling them. This should enhance and increase the relationship between the passive “blank slates” of the public, with the minority of the population who hold the ‘knowledge surplus’.\n\nA 420 meta-analysis of 193 studies sought to interpret the link between science knowledge and attitude towards science. The studies included were taken using nonuniform methods across the world between 1989 and 2004 to provide a cross-cultural analysis. Broad and specific science knowledge and attitude categories were correlated. General science and general biology knowledge was gauged using questions similar to those by the National Science Foundation used to capture \"civil scientific literacy\". Data on general science and biology knowledge was then compared with attitudes towards general science, nuclear power, genetic medicine, genetically modified food, and environmental science. From the raw data, it was found that a small positive correlation exists between general science knowledge and attitude towards science, indicating that increased scientific knowledge is related to a favorable attitude towards a science topic, and that this was not related to socioeconomic or technologic status of a country, but rather the number of individuals enrolled in tertiary education. However, some studies have found that high levels of science knowledge may indicate highly positive and highly negative attitudes towards specific topics such as agriculture biotechnology. Thus knowledge may be a predictor of the attitude strength and not necessarily if the attitude is positive or negative.\n\nMass media representations, ranging from news to entertainment, are critical links between the everyday realities of how people experience certain issues and the ways in which these are discussed at a distance between science, policy and public actors. Numerous studies show that the public frequently learn about science and more specifically issues such as climate change from the mass media. \n\nThere is perceived to be a trend within much of the world’s media that a traditional commitment to report the full facts is and has given way to a more obvious, less reliable tendency to concentrate coverage on interpretations of the facts. This so-called ‘spin’ is reported by the world’s press under a combination of commercial and political pressure. This can be dangerous as it ‘fills’ the knowledge deficit and the unsuspecting public with sometimes unreliable, agenda promoting information. The subjects of anthropogenic global warming and climate change are at the forefront of this. However, in all cases it is becoming increasingly difficult to separate out the factual basis of what is being reported from the ‘spin’ that is exerted on the way a story is reported and presented.\n\nThe mass media is accessible to the vast numbers of the global population and ranging from entertainment, to news media, and spanning books, films, televisions, newspapers, radio, games and the Internet. More modern forms of communication and receiving of information have given the public a much wider and accessible format in which to gain knowledge themselves. \n\nThe actual processes behind the communication and dissemination of information from the experts to the public may be far more complex and deep running than the deficit model suggests.\n\nThe knowledge deficit model is important for science communicators to know about. This is particularly important with respect to the concept of framing when communicating information. Framing can be used to reduce the complexity of an issue, or to persuade audiences, and can play into the underlying religious beliefs, moral values, prior knowledge, and even trust in scientists or political individuals. Further, the transmission of scientific ideas and technological adoption may be strongly linked to the passage of information between easily influenced individuals, versus the widely accepted \"two-step flow\" theory where a few opinion leaders acted as intermediaries between mass media and the general public. Decreasing the knowledge deficit is a complicated task, but if we know how the general public thinks, or how they go about learning and interpreting new information, we can better communicate our message to them in the most unbiased, objective way possible.\n\nIn contrast to the knowledge-deficit model is the low-information rationality model that states humans minimize costs associated with making decisions and forming attitudes, thereby avoiding developing in-depth understandings. \n\n"}
{"id": "391726", "url": "https://en.wikipedia.org/wiki?curid=391726", "title": "International Plant Names Index", "text": "International Plant Names Index\n\nThe International Plant Names Index (IPNI) describes itself as \"a database of the names and associated basic bibliographical details of seed plants, ferns and lycophytes.\" Coverage of plant names is best at the rank of species and genus. It includes basic bibliographical details associated with the names. Its goals include eliminating the need for repeated reference to primary sources for basic bibliographic information about plant names.\n\nThe IPNI also maintains a list of standardized author abbreviations. These were initially based on Brummitt & Powell (1992), but new names and abbreviations are continually added.\n\nIPNI is the product of a collaboration between The Royal Botanic Gardens, Kew (Index Kewensis), The Harvard University Herbaria (Gray Herbarium Index), and the Australian National Herbarium (APNI). The IPNI database is a collection of the names registered by the three cooperating institutions and they work towards standardizing the information. The standard of author abbreviations recommended by the \"International Code of Nomenclature for algae, fungi, and plants\" is Brummitt and Powell’s \"Authors of Plant Names\". A digital and continually updated list of authors and abbreviations can be consulted online at IPNI.\n\nThe IPNI provides names that have appeared in scholarly publications, with the objective of providing an index of published names rather than prescribing the accepted botanical nomenclature.\n\n\n\n \n"}
{"id": "5230911", "url": "https://en.wikipedia.org/wiki?curid=5230911", "title": "John Billingham", "text": "John Billingham\n\nDr. John Billingham, BM BCh, (March 18, 1930 – August 4, 2013) was a British Physician and later director of the SETI Program Office and Director of the Life Sciences Division at the NASA Ames Research Center in the USA. After retiring from NASA he became a Trustee of the SETI Institute Board of Directors.\n\nHe was born in Worcester, England in 1930 and educated at the Royal Grammar School Worcester. From there he went on to University College, Oxford to study physiology. He gained a BM BCh degree from Oxford and Guy's Hospital, London (which is equivalent to an M.D. in the US). He served as a medical officer with the Royal Air Force (RAF) for seven years, rising to the rank of Squadron Leader (equivalent to Major in the USAF). In 1963, he was invited to join NASA’s Lyndon B. Johnson Space Center in Houston, Texas, where he headed the Environmental Physiology Branch, and worked on the Mercury, Gemini and Apollo programs.\n\nIn 1965 he moved to the NASA Ames Research Center in California, where he headed up the Biotechnology Division, then the Extraterrestrial Research Division, and later the Life Science Division. In 1977 he appeared in the television documentary \"Mysteries of the Gods\" hosted by William Shatner to outline the projected search for extraterrestrial life that would later become Project Cyclops.\n\nIn 2009 he was inducted into the NASA Ames Hall of Fame where he was recognized for his efforts as the Father of SETI in NASA. After retiring from NASA he joined the SETI Institute as Senior Scientist, and in 1995 he became a Member of the SETI Institute's Board of Trustees, serving a term as Vice-Chair. He was also one of the people behind Project Cyclops.\n\nHe died at the age of 83 in Grass Valley, California in August 2013.\n\n"}
{"id": "51153525", "url": "https://en.wikipedia.org/wiki?curid=51153525", "title": "Lean Out: The Struggle for Gender Equality in Tech and Start-up Culture", "text": "Lean Out: The Struggle for Gender Equality in Tech and Start-up Culture\n\nLean Out: The Struggle for Gender Equality in Tech and Start-Up Culture is a 2015 book written by Elissa Shevinsky, an information security entrepreneur and feminist. Shevinsky wrote it in response to Sheryl Sandberg's Lean In, a book often criticized by feminists as being unrealistic in its expectations on women.\n\nNewspaper reviews have been nearly universal in their approbation of Shevinsky's work, with the Los Angeles Times saying \"the book is not just directed at women who might want to opt out of the rat race and start their own thing. This book is packed with stories — and statistics — that should give anyone in tech management pause.\"\n\nAt least one review notes that \"the essays end up varying widely in quality, radical message, level of editing, and scope. Among incisive critiques of nerd sociology and important stories of micro and macro aggressions, there are also far too many unnecessarily long resumes and capitalist self-help manuals in the book.\" \n"}
{"id": "46532634", "url": "https://en.wikipedia.org/wiki?curid=46532634", "title": "Learning Lab", "text": "Learning Lab\n\nLearning Lab (LL) is a systemic transformation methodology developed by Aydin Bal in 2011. The Learning Lab methodology provides research-based guidelines for local stakeholders to develop productive family-school-community partnerships and design behavioral support systems that are culturally responsive to diverse needs, strengths, practices, and goals of all stakeholders within a local school community. The Learning Lab builds organizational capacity in schools and school districts by forming an inclusive problem solving team of multiple local stakeholders (teachers, education leaders, families, students, and local community representatives). In Learning Labs, local stakeholders, specifically those who are historically marginalized from schools’ decision-making activities, collectively examine disparities in behavioral outcomes in their local schools and develop solutions through systemic transformation.\nThe Learning Lab methodology was adapted from the change laboratory methodology and is grounded in Cultural Historical Activity Theory. The moral purpose of the Learning Lab is participatory social justice. Participatory social justice is about non-dominant communities’ equal participation and influence on decision-making activities. The goal of the Learning Lab methodology is to facilitate collective agency among local stakeholders who develop locally meaningful, socially just, and sustainable systemic solutions to educational equity issues such as racial disproportionality in exclusionary and punitive school disciplinary actions (e.g., detention, suspension, and expulsion).\n\nIn the Culturally Responsive Positive Behavioral Intervention and Supports Project, the Learning Lab has been implemented at urban pre K-12 schools in the United States. The Learning Lab was found to successfully facilitate and sustain authentic partnerships among local stakeholders that renovated their schoolwide behavioral support systems to be positive, inclusive and culturally responsive.\n"}
{"id": "3364578", "url": "https://en.wikipedia.org/wiki?curid=3364578", "title": "List of Crayola crayon colors", "text": "List of Crayola crayon colors\n\nSince the introduction of Crayola drawing crayons by Binney & Smith in 1903, more than two hundred distinctive colors have been produced in a wide variety of assortments. The table below represents all of the colors found in regular Crayola assortments from 1903 to the present. Since the introduction of fluorescent crayons in the 1970s, the standard colors have been complemented by a number of specialty crayon assortments, represented in subsequent tables.\n\nAlong with the regular packs of crayons, there have been many specialty sets, including Silver Swirls, Gem Tones, Pearl Brite, Metallic FX, Magic Scent, Silly Scents, and more.\n\nIn 1972, Binney & Smith introduced eight Crayola fluorescent crayons, designed to fluoresce under black light. The following year, they were added to the 72-count box, which had previously contained two of the eight most-used colors, in place of the duplicate crayons. These crayons remained steady until 1990, when all eight were renamed, and eight more were added, for a total of sixteen fluorescent crayons. One of the new colors, Hot Magenta, shared a name with one of the original colors, now Razzle Dazzle Rose. For some reason, two of the original eight fluorescent crayons have the same color as two of the newer crayons. In 1992, the fluorescent colors were added to the new No. 96 box, becoming part of the standard lineup. When four new crayons were added to the No. 96 assortment in 2003, four existing colors were discontinued, including two of the fluorescents. Also beginning in 1993, packs of fluorescent crayons were regularly labeled \"neon\" or \"neons\".\n\nIn 1976, Crayola released a pack of 8 Fabric crayons. Each crayon was named after a standard color. In 1980, \"Light Blue\" was discontinued and replaced with Black. The colors' hexadecimal values are currently unknown. The names of the colors are listed below:\n\nIn 1987, Crayola released a pack of 16 metallic crayons in Canada. 4 of the colors are named after 4 of the standard colors. Also, one of the colors is named before a Metallic FX color. The colors' hexadecimal values are currently unknown. The names of the colors are listed below:\n\nIn 1990, Crayola released the Silver Swirls, a pack of 24 silvery colors. The colors' hexadecimal values are approximated below.\n\nIn 1992, Crayola released a set of eight \"multicultural\" crayons which \"come in an assortment of skin hues that give a child a realistic palette for coloring their world.\" The eight colors used came from their standard list of colors (none of these colors are exclusive to this set), and the set was, for the most part, well received, though there has also been some criticism.\n\nIn 1994, Crayola produced a 16-pack of crayons that released fragrances when used. In 1995, Crayola changed some of the scents because of complaints received from parents that some of the crayons smelled good enough to eat, like the Cherry, Chocolate, & Blueberry scented crayons. Crayons with food scents were retired in favor of non-food scents. The thirty crayons all consisted of regular Crayola colors.\n\nIn 1994, Crayola released the Gem Tones, a pack of 16 crayons modeled after precious stones. The colors' hexadecimal values are approximated below:\n\nIn 1994, Crayola released the Glow in the Dark crayons, a pack of eight crayons. However, it did not contain any color names in North America. Only four of the colors were available in the UK.\n\nThe Crayola Changeables crayons were introduced in 1995. The chart includes the color changer, an off-white crayon that goes on clear and initiates the color changes in the other crayons from the \"From color\" to the \"To color\".\n\nFollowing previous issues with scented crayons in 1994 and 1995, Binney & Smith released a new line, known as \"Magic Scent\" crayons in 1997. None of the crayons were named after or given the scent of foods. The sixteen crayons all consisted of regular Crayola colors.\n\nIn 1997, Crayola released a 16-pack of Star Brite crayons. However, it did not contain any color names. The hex triplets below are representative of the colors produced by the named crayons.\n\nIn 1997, Crayola released a 16-pack of crayons, each of which contains a solid color with flecks of two other colors in it. Colors in chart below are approximated. The hex RGB values are in the order of the predominant color and then the flecks. Colors for crayons other than Mixed Veggies and Star Spangled Banner come from information on the crayon wrapper.\n\nIn 1997, Crayola released a 16-pack of Pearl Brite crayons: These were designed to give soft pearlescent colors. These had a new wrapper design, black with a white oval Crayola logo and white text.\n\nIn 1997, Crayola released Crayons with Glitter as part of a Special Effects crayons package. Starting as late as 1999, their crayon names don't appear on the crayon wrappers. In the below list, the background represents crayon color, and the highlighted \"square of glitter\" around text represents glitter color.\n\nIn 1998, Crayola Introduced Construction Paper Crayons, The specialty line remained one of the longest running specialty lines they ever put out. The hex triplets below are representative of the colors produced by the named crayons.\n\nIn 2001, Crayola produced the Metallic FX crayons, a set of 16 metallic crayons whose names were chosen through a contest open to residents of the U.S. and Canada. The hex triplets below are representative of the colors produced by the named crayons. All colors are included in the special 152-count Ultimate Crayon Collection pack alongside 120 standard and 16 glitter crayons. Four of the colors are included in the regular 96-count crayon box.\n\nIn 2001, Crayola produced the Gel FX crayons. However, it didn't contain any color names. Four of the colors are randomly included in the 96-count crayon box alongside four Metallic FX colors and is not included in the 152-count Ultimate Crayon Collection set. The hex triplets below are representative of the colors produced by the named crayons.\n\nThe Silly Scents are produced by Crayola in a 16-pack. The sixteen crayons all consisted of regular Crayola colors.\n\nThe eight Heads 'n Tails crayons are double-sided and encased in plastic tubes that function much like the ones on Crayola Twistables. Each crayon has two shades of color, for a total of 16 colors, which are approximated by the background colors and hex RGB values below.\n\nIn 2004, Crayola released a set of 24 mini twistable crayons. They are nearly half the size of large twistable crayons. The colors' hexadecimal values are shown below. The colors are from the standard list of crayon colors.\n\nIn 2004, Crayola released a 24 pack of Fun Effects mini twistable crayons. It contains 8 eXtreme colors, 8 metallic colors, and 8 rainbow colors.\n\nIn 2007, Crayola released the set of eight True to Life crayons. Each crayon is extra-long and contained within a plastic casing similar to that of Crayola Twistables crayons. In the table, the background approximates the primary color and the text is in the two supporting colors. The approximate RGB hex values for each are given as well.\n\nIn 2009, Crayola released eight crayons in long twistable barrels. Although the names of these crayons were new, all but one of the colors were borrowed from Crayola's fluorescent crayons. Atomic Tangerine became \"Sizzling Sunset\", Blizzard Blue became \"Absolute Zero\", Hot Magenta became \"Winter Sky\", Laser Lemon became \"Lemon Glacier\", Razzle Dazzle Rose became \"Fiery Rose\", Screamin' Green became \"Spring Frost\", Vivid Violet became \"Frostbite\", and Wild Watermelon became \"Heat Wave\". Frostbite was the only color not originally part of the fluorescent line.\n"}
{"id": "13547568", "url": "https://en.wikipedia.org/wiki?curid=13547568", "title": "List of Schedule III drugs (US)", "text": "List of Schedule III drugs (US)\n\nThis is the list of Schedule III drugs as defined by the United States Controlled Substances Act at and , with modifications through August 22, 2014 (). The following findings are required for drugs to be placed in this schedule:\n\nThe complete list of Schedule III drugs follows. The Administrative Controlled Substances Code Number for each drug is included.\n"}
{"id": "4650840", "url": "https://en.wikipedia.org/wiki?curid=4650840", "title": "List of computer systems from Slovenia", "text": "List of computer systems from Slovenia\n\nThis is the list of computer systems from Slovenia. See History of computer hardware in Slovenia for more information.\n\n"}
{"id": "54186755", "url": "https://en.wikipedia.org/wiki?curid=54186755", "title": "List of investigational analgesics", "text": "List of investigational analgesics\n\nThis is a list of investigational analgesics, or analgesics that are currently under development for clinical use but are not yet approved. \"Chemical/generic names are listed first, with developmental code names, synonyms, and brand names in parentheses.\"\n\n\n\n\n\n\n\n\n\n"}
{"id": "7120141", "url": "https://en.wikipedia.org/wiki?curid=7120141", "title": "List of volcanoes in the Pacific Ocean", "text": "List of volcanoes in the Pacific Ocean\n\nA list of active and extinct volcanoes in the Pacific Ocean. \n\n"}
{"id": "962300", "url": "https://en.wikipedia.org/wiki?curid=962300", "title": "Lovell Telescope", "text": "Lovell Telescope\n\nThe Lovell Telescope is a radio telescope at Jodrell Bank Observatory, near Goostrey, Cheshire in the north-west of England. When construction was finished in 1957, the telescope was the largest steerable dish radio telescope in the world at 76.2 m (250 ft) in diameter;\nit is now the third-largest, after the Green Bank telescope in West Virginia, United States, and the Effelsberg telescope in Germany.\nIt was originally known as the \"250 ft telescope\" or the Radio Telescope at Jodrell Bank, before becoming the Mark I telescope around 1961 when future telescopes (the Mark II, III, and IV) were being discussed. It was renamed to the \"Lovell Telescope\" in 1987 after Sir Bernard Lovell, and became a Grade I listed building in 1988. The telescope forms part of the MERLIN and European VLBI Network arrays of radio telescopes.\n\nBoth Bernard Lovell and Charles Husband were knighted for their roles in creating the telescope. In September 2006, the telescope won the BBC's online competition to find the UK's greatest \"Unsung Landmark\". 2007 marked the 50th anniversary of the telescope.\n\nIf the air is clear enough, the Mark I telescope can be seen from high-rise buildings in Manchester such as the Beetham Tower, and from as far away as the Pennines, Winter Hill in Lancashire, Snowdonia, Beeston Castle in Cheshire, and the Peak District. It can also be seen from south-facing windows of the Terminal 1 restaurant area and departure lounges of Manchester Airport.\n\nBernard Lovell built the Transit Telescope at Jodrell Bank in the late 1940s. This was a -diameter radio telescope that could only point directly upwards; the next logical step was to build a telescope that could look at all parts of the sky so that more sources could be observed, as well as for longer integration times. Although the Transit Telescope had been designed and constructed by the astronomers that used it, a fully steerable telescope would need to be professionally designed and constructed; the first challenge was to find an engineer willing to do the job. This turned out to be Charles Husband, whom Lovell first met on 8 September 1949.\nTwo bearing assemblies from 15-inch (38-cm) gun turrets were bought cheaply in 1950; these came from the World War I battleships HMS \"Revenge\" and \"Royal Sovereign\", which were being broken up at the time. The bearings became the two main altitude rotator bearings of the telescope, with the appropriate parts of the telescope being designed around them. Husband presented the first drawings of the proposed giant, fully steerable radio telescope in 1950. After refinements, these plans were detailed in a \"Blue Book\", which was presented to the DSIR on 20 March 1951; the proposal was approved in March 1952.\n\nConstruction began on 3 September 1952. The foundations for the telescope were completed on 21 May 1953 after being sunk into the ground. it then took until Mid-March 1954 to get the double railway lines completed due to their required accuracy. The central pivot was delivered to the site on 11 May 1954, and the final bogie in mid-April 1955.\nThe telescope bowl was originally going to have a wire mesh surface to observe at wavelengths between 1 and 10 meters (3.2 and 32 feet), so frequencies between 30 and 300 MHz; this was changed to a steel surface so that the telescope could observe at the 21 cm (8 in) hydrogen line, which was discovered in 1951. Also, in February 1954 Lovell and the Air Ministry met to see if funding could be made available for improving the accuracy of the dish so that it could be used on centimetre wavelengths, for research at these wavelengths for the Ministry as well as \"other purposes\". Although the funding was not ultimately made available from the Air Ministry, the planning process had already progressed too far and so this improvement was made anyway.\n\nThe telescope was constructed so that the bowl could be completely inverted. Originally, it was intended to use a movable tower at the base of the telescope to change the receivers at the focus. However, the movable tower was never built, due jointly to funding constraints and the fact that much of the receiver equipment was placed at the base of the telescope rather than at the focus. Instead, receivers were mounted on 50-foot (15-m) long steel tubes, which were then inserted by a winch into the top of the aerial tower while the bowl was inverted. The cables from the receivers then ran down the inside of this tube, which could then be connected when the telescope was pointed at the zenith. Associated receiver equipment could then be placed either in the small, swinging laboratory directly underneath the surface; in rooms at the tops of the two towers; at the base girders, or in the control building.\n\nThe telescope moved for the first time on 3 February 1957: by an inch. It was first moved azimuthally under power on 12 June 1957; the bowl was tilted under power for the first time on 20 June 1957. By the end of July the dish surface was completed, and first light was on 2 August 1957; the telescope did a drift scan across the Milky Way at 160 MHz, with the bowl at the zenith. The telescope was first controlled from the control room on 9 October 1957, by a purpose-built analogue computer.\n\nThere were large cost overruns with the telescope's construction, mainly due to the steeply rising cost of steel at the time the telescope was constructed. The original grant for the telescope's construction came jointly from the Nuffield Foundation and the government; this amounted to £335,000. The government increased its share of the funding several times as the cost of the telescope rose; other money came from private donations. The final part of the debt from the construction of the telescope, £50,000, was paid off by Lord Nuffield and the Nuffield Foundation on 25 May 1960 (partly due to the telescope's early, very public role in space probe tracking; see below), and Jodrell Bank observatory was renamed to the Nuffield Radio Astronomy Laboratories. The final total cost for the telescope was £700,000.\n\nShortly after the telescope was originally completed, Lovell and Husband started contemplating the idea of upgrading the telescope so that it had a more accurate surface, and was controlled by a digital computer. Plans for this upgrade were created by Husband and Co., and were presented to Lovell in April 1964. Their plans became more urgent when fatigue cracks were discovered in the elevation drive system in September 1967. The telescope was only expected to have an operational lifespan of 10 years, and Husband had been warning about the decay of the telescope since 1963. The appearance of fatigue cracks was the first of these problems that threatened to stop the telescope working; had they not been put right the elevation system could have failed and perhaps jammed. The telescope was therefore repaired and upgraded to become the Mark IA; the £400,000 of funding to do this was announced on 8 July 1968 by the SRC. The upgrade was carried out in three phases, phase 1 lasting between September 1968 and February 1969, phase 2 between September and November 1969 and phase 3 between August 1970 and November 1971.\n\nThe first phase saw the addition of an inner railway track, which was designed to take a third of the weight of the telescope. The outer railway track, which had been decaying and sinking over the previous years, was relaid in the second phase. Four bogies and their steelwork were added on the inner track, and the existing bogies on the outer track were overhauled.\n\nThe third phase saw the biggest changes; a new, more accurate bowl surface was constructed in front of the old surface, meaning that the telescope could be used on wavelengths as small as 6 cm (5 GHz), and the central \"bicycle wheel\" support was added. A new computer control system was also installed (reusing the Ferranti Argus 104 computer from the Mark II); fatigue cracks in the cones connecting the bowl to the towers were repaired, and the central antenna was lengthened and strengthened. Tragically, in January 1972 the hoist carrying two engineers to the central antenna broke, gravely injuring two engineers and causing the death of one of them.\n\nThe Mark IA upgrade was formally completed on 16 July 1974, when the telescope was handed back to the University of Manchester. Due to increases in the cost of steel during the upgrade, the final amount for the upgrade was £664,793.07.\n\nThe Gale of January 1976 on 2 January, brought winds of around 90 mph (140 km/h) which almost destroyed the telescope. The towers bowed, and one of the bearings connecting the dish to the towers slipped. After an expensive repair, diagonal bracing girders were added to the towers to prevent this happening again.\n\nBy the 1990s, the telescope surface was becoming badly corroded. In 2001–2003, the telescope was resurfaced, increasing its sensitivity at 5 GHz by a factor of five. A holographic profiling technique was used on the surface, meaning that the surface works optimally at wavelengths of 5 cm (compared to 18 cm on the old surface). A new drive system was installed, which provides a much higher pointing accuracy. The outer track was relaid, and the focal tower was strengthened so that it could support heavier receivers.\n\nIn 2007 the telescope needed a new drive wheel, as one of the 64 original wheels had cracked; in 2008 another new steel tyre was needed after a second wheel cracked. These are the only two wheel changes needed since the telescope started operation in 1957.\n\nThe presence (as at 2010) of two breeding pairs of wild peregrine falcons (nesting one in each of the telescope's two support towers) prevents the nuisance of pigeon infestation (by droppings fouling, and their body heat affecting sensitive instrument readings) that some other radio telescopes suffer from.\n\nThe telescope became operational in the summer of 1957, just in time for the launch of Sputnik 1, the world's first artificial satellite. While the transmissions from Sputnik itself could easily be picked up by a household radio, the Lovell Telescope was the only telescope capable of tracking Sputnik's booster rocket by radar; it first located it just before midnight on 12 October 1957. It also located Sputnik 2's carrier rocket at just after midnight on 16 November 1957.\n\nThe telescope also took part in some of the early work on satellite communication. In February and March 1963, the telescope transmitted signals via the moon and Echo II, a NASA balloon satellite at 750 km (466 mi) altitude, to the Zimenki Observatory in the USSR. Some signals were also relayed from the USA to the USSR via Jodrell Bank.\n\nThe Lovell Telescope was used to track both Soviet and American probes aimed at the Moon in the late 1950s and early 1960s. In terms of American space probes, the telescope tracked Pioneer 1 from 11 to 13 November 1958, Pioneer 3 in December 1958, and Pioneer 4 in March 1959. The telescope tracked Pioneer 5 between 11 March and 26 June 1960, and was also used to send commands to the probe, including the one to separate the probe from its carrier rocket and the ones to turn on the more powerful transmitter when the probe was 8 million miles (12.9 million km) away. It also received data from Pioneer 5, and was the only telescope in the world capable of doing so at the time. The last signal was picked up from the probe at a distance of 36.2 million kilometers on the 26 June 1960.\n\nThe telescope also tracked the Soviet moon probes, including Lunik II from 13 to 14 September 1959 as it hit the moon; this was proven by the telescope by measuring the effect of the moon's gravity on the probe, and Luna 3 around 4 October 1959. Also, the telescope tracked Luna 9 in February 1966, the first spacecraft to make a soft landing on the Moon. The telescope listened in on its facsimile transmission of photographs from the moon's surface. The photos were sent to the British press – the probe transmitted, likely intentionally to increase chances of reception, in the international format for image transmission by newswire – and published before the Soviets themselves had made the photos public.\n\nThe telescope tracked Luna 10, a Russian satellite put into orbit around the Moon, in April 1966, and Zond 5 in September 1968, a Russian probe that was launched at the moon, around which it sling-shotted before returning to Earth. The telescope did not track Apollo 11, as it was tracking Luna 15 in July 1969. However, a telescope at Jodrell Bank was used at the same time to track Apollo 11.\n\nThe telescope possibly detected signals from Venera 1, a Russian satellite en route to Venus, in 19–20 May 1961. However, it was not possible to confirm the origin of the signals. A few years later, in December 1962, the telescope tracked and received data from Mariner 2. On 18 October 1967, the telescope received signals from, and tracked, Venera 4, a Russian probe to Venus.\n\nThe telescope tracked Mars 1 in 1962–3, and Mars 2 and Mars 3 in 1971 (amidst the upgrade of the telescope to the Mark IA). In more recent years, it has also searched for several lost Mars spacecraft, including NASA's Mars Observer spacecraft in 1993, Mars Polar Lander in 2000,\nand the Beagle 2 lander on Mars in 2003. However, it did not succeed in locating any of them.\n\nAs a stopgap measure while RAF Fylingdales was being built, the telescope was on standby for \"Project Verify\" (also known by the codewords \"Lothario\" and \"Changlin\") between April 1962 and September 1963. During strategic alerts, a 'pulse transmitter, receiver and display equipment' could be connected to the telescope to scan known Russian launch sites for indications of launches of ICBMs and/or IRBMs. During the Cuban Missile Crisis in October 1962, the telescope was discreetly turned towards the Iron Curtain to provide a few minutes' warning of any missiles that might have been launched.\n\nWhen the telescope was proposed, a series of objectives for the telescope's observations were set out. These included:\n\nHowever, the actual observations made with the telescope differ from these original objectives, and are outlined in the following sections.\n\nIn Autumn 1958, the telescope was used to bounce \"Hellos\" off the Moon for a demonstration in Lovell's third Reith Lecture. The telescope was also used to receive messages bounced off the Moon (a \"moonbounce\") as part of the 50th anniversary First Move festival. In April 1961, a radar echo from Venus was achieved using the telescope while the planet was at a close approach, confirming measurements of the distance of the planet made by American telescopes.\n\nThe 21 cm hydrogen line was discovered during the telescope's construction; the telescope was subsequently redesigned so that it could observe at that frequency. Using this line emission, hydrogen clouds both in the Milky Way galaxy and in other galaxies can be observed; for example, the telescope discovered a large cloud around the M81 and M82 galaxies. The motion of these clouds either towards or away from us either redshifts or blueshifts the line, allowing the velocity to the cloud to be measured. This provides a probe of the internal dynamics of galaxies, and can also provide a measurement of the rate of expansion of the universe.\n\nIn 1963, the telescope discovered OH emissions from star-forming regions and giant stars; the first astronomical masers. OH masers emit on four frequencies around 18 cm (7 in), which are easily observable on the telescope. As part of MERLIN, the telescope is regularly used to construct maps of maser regions.\n\nIn 1968, the telescope observed the coordinates of the recently discovered pulsar, confirming its existence and investigating the dispersion measure. It was also used to make the first detection of polarization of the pulsar's radiation. This marked the start of a substantial amount of work investigating pulsars at Jodrell, which is still ongoing. In the 30 years following the discovery of pulsars, the telescope discovered over 100 new pulsars (and astronomers at Jodrell Bank discovered around 2/3 of the total number using the Lovell and other telescopes). 300 pulsars are regularly observed using either the Lovell, or a nearby 42-foot (13-m) dish.\n\nThe telescope was involved in the discovery of millisecond pulsars, and also discovered the first pulsar in a globular cluster in 1986—a millisecond pulsar in the Messier 28 globular cluster. In September 2006, the results of three years of observing a double pulsar, PSR J0737-3039, with the Lovell telescope, as well as with the Parkes and Green Bank Telescopes, were announced — confirming that the general theory of relativity is accurate to 99.5%.\n\nBetween 1972 and 1973, the telescope was used for \"a detailed survey of the radio sources in a limited area of the sky … up to the sensitivity limit of the instrument\". Among the objects catalogued was the first gravitational lens, which was confirmed optically in 1979 after its position was found to coincide with a pair of faint blue stars by using the Mark I as an interferometer with the Mark II. The telescope was also involved in the detection of the first Einstein ring in 1998, in conjunction with observations made with the Hubble Space Telescope.\n\nThe early investigation into the size and nature of quasars drove the development of interferometry techniques in the 1950s; the Lovell telescope had an advantage due to its large collecting area, meaning that high sensitivity interferometer measurements can be made relatively quickly using it. As a result, the telescope featured heavily in the discovery of quasars.\nInterferometry at Jodrell Bank started before the Lovell telescope was constructed, using the Transit Telescope with a 35 square meter broadside array to determine the size of radio-loud nebulae. Once construction of the Lovell telescope was complete, the broadside array was put on a steerable mount and the pair were used as a tracking radio interferometer. This was then used to determine the 2D shape of quasars on the sky. In the summer of 1961, a 25-foot (8-m) diameter paraboloid telescope was constructed (it was made of aluminium tubing and was mounted on the rotating structure of an old defence radar). This was then used as a steerable interferometer with the Mark I, with a resolution of 0.3 arcseconds, to determine the sizes of some high-redshift (z~0.86) quasars.\n\nThe Mark II telescope once constructed was also used as an interferometer with the Lovell telescope. This has a baseline of 425 m (1,394 ft) (meaning that it can synthesize a telescope with 425 m diameter), giving it a resolution of around 0.5 arcminutes. This telescope pair has been used to carry out survey work, and to determine the positions of faint radio objects. Also, one of the drivers behind the construction of the Mark III was to use it as an interferometer with the Mark I to carry out a survey of radio sources.\n\nThe telescope took part in the first transatlantic interferometer experiment in 1968, with other telescopes being those at Algonquin and Penticton in Canada. It was first used as an interferometer with the Arecibo radio telescope in 1969.\n\nIn 1980, it was used as part of the new MERLIN array with a series of smaller radio telescopes controlled from Jodrell Bank. With baselines of up to 217 km (135 mi), this gave a resolution around 0.05 arcminutes. An upgraded version of this became a national facility in 1992. It has also been used in Very Long Baseline Interferometry, with telescopes across Europe (the European VLBI Network), giving a resolution of around 0.001 arcseconds. Around half of the telescope's observing time is now spent doing interferometry with other telescopes. It is planned that the telescope will work as part of an interferometer with the Radioastron (Russian) and VLBI Space Observatory Programme (Japanese) orbital radio satellites, providing yet larger baselines and higher resolutions. \n\nThe telescope was used as a follow-up instrument for possible SETI detections made at Arecibo between 1998 and the end of 2003. No signals were detected. In February 2005, astronomers using the Lovell Telescope discovered the galaxy VIRGOHI21 that appears to be made almost entirely of dark matter.\n\n\nIn an August 1981 episode of Coronation Street the telescope was seen. Len and Rita Fairclough brought the boy they were fostering to see the telescope.\n\n\n\n\n"}
{"id": "56613244", "url": "https://en.wikipedia.org/wiki?curid=56613244", "title": "Magnus Nordborg", "text": "Magnus Nordborg\n\nMagnus Nordborg is a biologist specialising in population genetics. He is the scientific director of the Gregor Mendel Institute of the Austrian Academy of Sciences, located at the Vienna Biocenter.\n\nIn 2003, Nordborg received the Sloan Research Fellowship.\n\nHe was elected a fellow of the American Association for the Advancement of Science in 2010. In 2015, he was elected a member of the European Molecular Biology Organization. \n\nSince 2013, he is a corresponding member of the Austrian Academy of Sciences.\n"}
{"id": "58026100", "url": "https://en.wikipedia.org/wiki?curid=58026100", "title": "Meredith Perry (inventor)", "text": "Meredith Perry (inventor)\n\nMeredith Perry (1991−) is an American entrepreneur and the founder of uBeam.\n\nPerry attended the University of Pennsylvania (2007–2011) and graduated with a degree in Paleobiology, Geology, Astrobiology. She presented the uBeam at the universities \"PennVention\", an invention competition, and the uBeam won. Later, she presented her design at the All Things Digital conference in 2011. Perry was a student ambassador at NASA Ames Research Center where she led a zero gravity experiment, worked on technology to discover life on Mars, and researched astrobiology and medicine.\n\nThe uBeam device emits a high frequency sounds (inaudible to human ears) and transmits power directly to receivers, thus enabling users to walk around the room, laptop in hand without needing to be physically stationed to a charging location (like WIFI for charging). The effectiveness and safety of the technology have been questioned.\n\nuBeam has received over $26 million in investments from Mark Cuban, Marissa Mayer, Andreessen Horowitz, Shawn Fanning, Zappos’s CEO Tony Hsieh, Peter Thiel's Founder's Fund, among others.\n"}
{"id": "46204126", "url": "https://en.wikipedia.org/wiki?curid=46204126", "title": "Metatranscriptomics", "text": "Metatranscriptomics\n\nMetatranscriptomics is the science that studies gene expression of microbes within natural environments. It also allows to obtain whole gene expression profiling of complex microbial communities.\n\nWhile metagenomics focuses on studying the genomic content and on identifying which microbes are present within a community, metatranscriptomics can be used to study the diversity of the active genes within such community, to quantify their expression levels and to monitor how these levels change in different conditions (e.g., physiological vs. pathological conditions in an organism). The advantage of metatranscriptomics is that it can provide information about differences in the active functions of microbial communities which appear to be the same in terms of microbe composition.\n\nThe microbiome has been defined as a microbial community occupying a well-defined habitat. They are ubiquitous and extremely relevant for the maintenance of” the characteristic of the environment in which they reside and an imbalance in these communities can affect negatively the activity of the setting in which they reside. \nTo study these communities, and to then determine their impact and correlation with their niche, different omics- approaches have been used. While metagenomics allows to obtain a taxonomic profile of the sample, metatrascriptomics provides a functional profile by analysing which genes are expressed by the community. \nIt is possible to infer what genes are expressed under specific conditions, and this can be done using functional annotations of expressed genes.\n\nSince metatranscriptomics focuses on what genes are expressed, it allows to understand the active functional profile of the entire microbial community.\nThe overview of the gene expression in a given sample is obtained by capturing the total mRNA of the microbiome and by performing a whole metatranscriptomics shotgun sequencing.\n\nAlthough microarrays can be exploited to determine the gene expression profiles of some model organisms, NGS is the preferred technique in metatranscriptomics. The protocol that is used to perform a metatranscriptome analysis may vary depending on the type of sample that needs to be analysed. Indeed, many different protocols have been developed for studying the metatranscriptome of microbial samples. Generally, the steps include sample harvesting, RNA extraction (different extraction methods for different kinds of samples have been reported in the literature), mRNA enrichment, cDNA synthesis and preparation of metatranscriptomic libraries, sequencing and data processing and analysis.\nmRNA enrichment is one of the trickiest parts. Different strategies have been proposed:\nThe last two strategies are not recommended as they have been reported to be highly biased.\nA typical metatranscriptom¬e analysis pipeline:\nThe first strategy maps reads to reference genomes in databases, to collect information that is useful to deduce the relative expression of the single genes.\nMetatran¬scriptomic reads are mapped against databases using alignment tools, such as Bowtie2, BWA, and BLAST. Then, the results are annotated using resources, such as GO, KEGG, COG, and Swiss-Prot. The final analysis of the results is carried out depending on the aim of the study.\nOne of the latest metatranscriptomics techniques is stable isotope probing (SIP), which has been used to retrieve specific targeted transcriptomes of aerobic microbes in lake sediment.\nThe limitation of this strategy is its reliance on the information of reference genomes in databases.\nThe second strategy retrieves the abundance in the expression of the different genes by assembling metatranscrip¬tomic reads into longer fragments called contigs using different softwares. So, its limits depend on the software that is used for the assembly.\nThe Trinity software for RNA-seq, in comparison with other de novo transcriptome assemblers, was reported to recover more full-length transcripts over a broad range of expression levels, with a sensitivity similar to methods that rely on genome alignments. This is particularly important in the absence of a reference genome.\nA quantitative pipeline for transcriptomic analysis was developed by Li and Dewey and called RSEM (RNA-Seq by Expectation Maximization). It can work as stand-alone software or as a plug-in for Trinity. \nRSEM starts with a reference transcriptome or assembly along with RNA-Seq reads generated from the sample and calculates normalized transcript abundance (meaning the number of RNA-Seq reads cor-responding to each reference transcriptome or assembly).\nAlthough both Trinity and RSEM were designed for tran¬scriptomic datasets (i.e., obtained from a single organism), it may be possible to apply them to metatranscriptomic data (i.e., obtained from a whole microbial community). \nGiven the huge amount of data obtained from metagenomic and metatranscriptomic analysis, the use of bioinformatic tools have become of greater importance in the last decades. In order to achieve so, many different bioinformatic pipelines have been developed, often as open source platforms, such as HUMAnN and the most recent HUMAnN2, MetaTrans, SAMSA and Leimena-2013 \n\nHUMAnN2 is a bioinformatic pipeline designed from the latter HUMAnN developed during the Human Microbiome Project (HMP), implementing a “tiered search” approach. In the first tier, HUMAnN2 screens DNA or RNA reads with MetaPhlAn2 in order to identify already known microbes and constructing a sample-specific database by merging pangenomes of annotated species; in the second tier the algorithm performs a mapping of the reads against the assembled pangenome database; in the third tier, non-aligned reads are used for a translated search against a protein database \n\nMetaTrans is a pipeline that exploits multi-threading computers to improve metagenomic and metatranscriptomic analysis. Data is obtained from paired-end RNA-Seq, mainly from 16S RNA for taxonomy and mRNA for gene expression levels. The pipeline is divided in 4 major steps. Firstly, paired-end reads are filtered for quality control purposes, to be thereafter sorted for taxonomic analysis (by removal of tRNA sequences) or functional analysis (by removal of both tRNA and rRNA sequencing). For the taxonomic analysis, sequences are mapped against 16S rRNA Greengenes v13.5 database using SOAP2, while for functional analysis sequences are mapped against a functional database such as MetaHIT-2014 always by using SOAP2 tool. This pipeline is highly flexible, since it offers the possibility to use third-party tools and improve single modules as long as the general structure is preserved.\n\nThis pipeline is designed specifically for metatranscriptomics data analysis, by working in conjunction with the MG-RAST server for metagenomics. This pipeline is simple to use, requires low technical preparation and computational power and can be applied to a wide range of microbes. The algorithm is divided in 4 steps. At first, sequences from raw sequencing data are selected on quality basis and are then submitted to MG-RAST (which foresee different steps such as quality control check, gene calling, clustering of amino acid sequences and use of sBLAT on each cluster to detect the best matches). Matches are then aggregated for taxonomic and functional analysis purposes, that usually follow up as last steps of the process.\n\nThis pipeline actually does not have a name so that it is usually reckoned with the first name of the author of the article in which it is described. This algorithm foresees the implementation of alignment tools such as BLAST and MegaBLAST. Reads, usually obtained by Illumina sequencing, are clustered in identical-reads clusters and are then processed for in-silico removal of t-RNA and r-RNA sequences. Remaining reads are then mapped on NCBI databases by using BLAST and MegaBLAST tools and classified by their bitscore. Higher bitscore sequences are thereby interpreted to predict phylogenetic origin and function. Lower score reads instead are aligned with BLASTX (higher sensitivity) and eventually can be aligned in protein databases so that their function can be characterized.\n\nAnother method that can be exploited for metatranscriptomic purposes is Tiling Microarrays. \nIn particular, microarrays have been used to measure microbial transcription levels, to detect new transcripts and to obtain information about the structure of mRNAs (for instance, the UTR boundaries). Recently, it has also been used to find new regulatory ncRNA. \nHowever, microarrays are affected by some pitfalls:\nRNA-Seq can overcome these limitations: it does not require any previous knowledge about the genomes that have to be analysed and it provides high throughput validation of genes prediction, structure, expression. Thus, by combining the two approaches it is possible to have a more complete representation of bacterial transcriptome.\n\n\nThe gut microbiome has emerged in recent years as an important player in human health. Its prevalent functions are related to the fermentation of indigestible food components, competitions with pathogen, strengthening of the intestinal barrier, stimulation and regulation of the immune system.\nAlthough much has been learnt about the microbiome community in the last years, the wide diversity of microorganisms and molecules in the gut requires new tools to enable new discoveries. \nBy focusing on the changes in the expression of the genes, metatrascriptomics allows to take a more dynamic picture of the state and activity of the microbiome than metagenomics. It has been observed that metatranscriptomic functional profiles are more variable than what might have been reckoned only by metagenomic information. This suggests that non-housekeeping genes are not stably expressed in situ\nOne example of metatranscriptomic application is in the study of the gut microbiome in inflammatory bowel disease. Inflammatory bowel disease (IBD) is a group of chronic diseases of the digestive tract that affects millions of people worldwide.\nSeveral human genetic mutations have been linked to an increased susceptibility to IBD, but additional factors are needed for the full development of the disease.\nRegarding the relationship between IBD and gut microbiome, it is known that there is a dysbiosis in patients with IBD but microbial taxonomic profiles can be highly different among patients, making it difficult to implicate specific microbial species or strains in disease onset and progression. In addition, the gut microbiome composition presents a high variability over time among people, with more pronounced variations in patient with IBD.\nThe functional potential of an organism, meaning the genes and pathways encoded in its genome, provides only indirect information about the level or extent of activation of such functions. So, the measurement of functional activity (gene expression) is critical to understand the mechanism of the gut microbiome dysbiosis.\nAlterations in transcriptional activity in IBD, established on the rRNA expression, indicate that some bacterial populations are active in patients with IBD, while other groups are inactive or latent.\nA metatranscriptomics analysis measuring the functional activity of the gut microbiome reveals insights only partially observable in metagenomic functional potential, including disease-linked observations for IBD. It has been reported that many IBD-specific signals are either more pronounced or only detectable on the RNA level.\nThese altered expression profiles are potentially the result of changes in the gut environment in patients with IBD, which include increased levels of inflammation, higher concentrations of oxygen and a diminished mucous layer.\nMetatranscriptomics has the advantage of allowing to skip the assaying of biochemical products in situ (like mucus or oxygen) and allows to study the effects of environmental changes on microbial expression patterns in vivo for large human populations.\nIn addition, it can be coupled with longitudinal sampling to associate modulation of activity with the disease progression. Indeed, it has been shown that while a particular path may remain stable over time at the genomic level, the corresponding expression varies with the disease severity. This suggests that microbial dysbiosis affect the gut health through changing in the transcriptional programmes in a stable community. In this way, metatracriptomic profiling emerges as an important tool for understanding the mechanisms of that relationship.\nSome technical limitations of the RNA measurements in stool are related to the fact that the extracted RNA can be degraded and, if not, it still represents only the organisms presents in the stool sample.\nOther applications of metagenomics:\nExamples of techniques applied:\nMicroarrays: allow the monitoring of changes in the expression levels of many genes in parallel for both host and pathogen. First microarray approaches have shown the first global analysis of gene expression changes in pathogens such as Vibrio cholerae, Borrelia burgdorferi, Chlamydia trachomatis, Chlamydia pneumoniae and Salmonella enterica, revealing the strategies that are used by these microorganisms to adapt to the host.\nIn addition, microarrays only provide the first global insights about the host innate immune response to PAMPs, as the effects of bacterial infection on the expression of various host factor.\nAnyway, the detection through microarrays of both organisms at the same time could be problematic.\nProblems:\n\nDual RNA-Seq: this technique allows the simultaneous study of both host and pathogen transcriptomes as well. It is possible to monitor the expression of genes at different time points of the infection process; in this way could it be possible to study the changes in cellular networks in both organisms starting from the initial contact until the manipulation of the host (interplay host-patogen). \nMoreover, RNA-Seq is an important approach for identifying coregulated genes, enabling the organization of pathogen genomes into operons. Indeed, genome annotation has been done for some eukaryotic pathogens, such as Candida albicans, Trypanosoma brucei and Plasmodium falciparum.\nDespite the increasing sensitivity and depth of sequencing now available, there are still few published RNA-Seq studies concerning the response of the mammalian host cell to the infection.\n"}
{"id": "57753288", "url": "https://en.wikipedia.org/wiki?curid=57753288", "title": "Michael Johnson (scientist)", "text": "Michael Johnson (scientist)\n\nMichael Johnson is an Assistant Professor of Immunobiology and Science Communicator at the University of Arizona. He created the popular science blog \"Black Science Blog\" and the podcast \"Science Sound Bites.\"\n\nJohnson was born and raised on the south side of Chicago by a single parent. He earned a Bachelors in music at Duke University in 2004. He then studied Biochemistry and Biophysics at University of North Carolina at Chapel Hill. For his graduate studies he looked at the ways bacteria move and attach to cause infection under the supervision of Matthew Redinbo.\n\nIn 2012 Johnson was awarded a research fellowship in infectious diseases at St. Jude Children's Research Hospital. He studies how bacteria respond to stress and the mechanisms used by the immune system to remove infection and metal export in streptococcus pneumoniae. In 2016 Johnson was appointed to the University of Arizona College of Medicine, where his lab research how bacteria maintain homeostasis in metal environments.\n\n\"Black Science Blog\" was featured as a Scientific American \"You Should Know\" blog to follow. He created the podcast \"Science Sound Bites\" to help school teachers and students better understand scientists and their careers.\n"}
{"id": "2726726", "url": "https://en.wikipedia.org/wiki?curid=2726726", "title": "Military logistics", "text": "Military logistics\n\nMilitary logistics is the discipline of planning and carrying out the movement and maintenance of military forces. In its most comprehensive sense, it is those aspects or military operations that deal with:\n\n\nThe word \"logistics\" is derived from the Greek adjective \"logistikos\" meaning \"skilled in calculating\". The first administrative use of the word was in Roman and Byzantine times when there was a military administrative official with the title \"Logista\". At that time, the word apparently implied a skill involved in numerical computations.\n\nHistorically supplies for an army were first acquired by foraging or looting, especially in the case of food and fodder, although if traveling through a desolated region or staying in one place for too long resources could quickly be exhausted. A second method was for the army to bring along what was needed, whether by ships, pack animals, wagons or carried on the backs of the soldiers themselves. This allowed the army some measure of self-sufficiency, and up through to the 19th century most of the ammunition a soldier needed for an entire campaign could be carried on their person. However, this method led to an extensive baggage train which could slow down the army's advance and the development of faster-firing weapons soon outpaced an army's ability to supply itself. Starting with the Industrial Revolution new technological, technical and administrative advances led to a third method, that of maintaining supplies in a rear area and transporting them to the front. This led to a \"logistical revolution\" which began in the 20th century and drastically improved the capabilities of modern armies while making them highly dependent on this new system.\n\nThrough the medieval period (the 5th to 15th century in Europe), soldiers were responsible for supplying themselves, either through foraging, looting, or purchases. Even so, military commanders often provided their troops with food and supplies, but this would be provided in lieu of the soldiers' wages, or soldiers would be expected to pay for it from their wages, either at cost or even with a profit.\n\nIn 1294, the same year John II de Balliol of Scotland refused to support Edward I of England's planned invasion of France, Edward I implemented a system in Wales and Scotland where sheriffs would acquire foodstuffs, horses and carts from merchants with compulsory sales at prices fixed below typical market prices under the Crown's rights of prise and purveyance. These goods would then be transported to Royal Magazines in southern Scotland and along the Scottish border where English conscripts under his command could purchase them. This continued during the First War of Scottish Independence which began in 1296, though the system was unpopular and was ended with Edward I's death in 1307.\n\nStarting under the rule of Edward II in 1307 and ending under the rule of Edward III in 1337, the English instead used a system where merchants would be asked to meet armies with supplies for the conscripts to purchase. This led to discontent as the merchants saw an opportunity to profiteer, forcing conscripts to pay well above normal market prices for food.\n\nAs Edward III went to war with France in the Hundred Years' War (starting in 1337), the English returned to a practice of foraging and looting to meet their logistical needs. This practice lasted throughout the course of war, extending through the remainder of Edward III's reign into the reign of Henry VI.\n\nStarting in the late sixteenth century armies in Europe greatly increased in size, upwards of 100,000 or more in some cases. This increase in size came not just in the number of actual soldiers but also camp followers—anywhere from half to one and a half the size of the army itself—and the size of the baggage train—averaging one wagon for every fifteen men. However, very little state support was provided to these massive armies, the vast majority of which consisted of mercenaries. Beyond being paid for their service by the state—an act which bankrupted even the Spanish Empire on several occasions—these soldiers and their commanders were forced to provide everything for themselves. If permanently assigned to a town or city with a working marketplace, or traveling along a well-established military route, supplies could be easily bought locally with intendants overseeing the exchanges. In other cases an army traveling in friendly territory could expect to be followed by sutlers—although their supply stocks were small and subject to price gouging—or a commissioner could be sent ahead to a town to make arraignments, including quartering if necessary.\n\nWhen operating in enemy territory an army was forced to plunder the local countryside for supplies, a historical tradition meant to allow war to be conducted at the enemy's expense. However, with the increase in army sizes this reliance on plunder became a major problem, as many decisions regarding where an army could move or fight were made based not on strategic objectives but whether a given area was capable of supporting the soldiers' needs. Sieges in particular were affected by this, both for any army attempting to lay siege to a location or coming to its relief. Unless a military commander was able to implement some sort of regular resupply, a fortress or town with a devastated countryside could be effectively immune to either operation.\n\nConversely, armies of this time had little need to maintain lines of communication while on the move, except insofar as it was necessary to recruit more soldiers, and thus could not be cut off from non-existent supply bases. Although this theoretically granted armies freedom of movement, the need for plunder prevented any sort of sustained, purposeful advance. Many armies were further restricted to following waterways due to the fact that what supplies they were forced to carry could be more easily transported by boat. Artillery in particular was reliant of this method of travel, since even a modest number of cannons of the period required hundreds of horses to pull overland and traveled at half the speed of the rest of the army.\n\nBy the seventeenth century, the French under Secretary of State for War Michel Le Tellier began a series of military reforms to address some of the issues which had plagued armies in the previous century. Besides ensuring that soldiers were more regularly paid and combating the corruption and inefficiencies of private contractors, Le Tellier devised formulas to calculate the exact amount of supplies necessary for a given campaign, created standardized contracts for dealing with commercial suppliers, and formed a permanent vehicle-park manned by army specialists whose job was to carry a few days' worth of supplies while accompanying the army during campaigns. With these arrangements there was a gradual increase in the use of magazines which could provide a more regular flow of supply via convoys. While the concepts of magazines and convoys was not new at this time, prior to the increase in army sizes there had rarely been cause to implement them.\n\nDespite these changes, French armies still relied on plunder for a majority of their needs while on the move. Magazines were created for specific campaigns and any surplus was immediately sold for both monetary gain and to lessen the tax burden. The vehicles used to form convoys were contracted out from commercial interests or requisitioned from local stockpiles. In addition, given warfare of this era's focus on fortified towns and an inability to establish front lines or exert a stabilizing control over large areas, these convoys often needed armies of their own to provide escort. The primary benefits of these reforms was to supply an army during a siege. This was borne out in the successful campaign of 1658 when the French army at no point was forced to end a siege on account of supplies, including the Siege of Dunkirk.\n\nLe Tellier's son Louvois would continue his father's reforms after assuming his position. The most important of these was to guarantee free daily rations for the soldiers, amounting to two pounds of bread or hardtack a day. These rations were supplemented as circumstances allowed by a source of protein such as meat or beans; soldiers were still responsible for purchasing these items out-of-pocket but they were often available at below-market prices or even free at the expense of the state. He also made permanent a system of magazines which were overseen by local governors to ensure they were fully stocked. Some of these magazines were dedicated to providing frontier towns and fortresses several months' worth of supplies in the event of a siege, while the rest were dedicated to supporting French armies operating in the field.\n\nWith these reforms French armies enjoyed one of the best logistical systems in Europe, however there were still severe restrictions on its capabilities. Only a fraction of an army's supply needs could be met by the magazines, requiring that it continue to use plunder. In particular this was true for perishable goods or those too bulky to store and transport such as fodder. The administration and transportation of supplies remained inadequate and subject to the deprivations of private contractors. The primary aim of this system was still to keep an army supplied while conducting a siege, a task for which it succeeded, rather than increase its freedom of movement.\n\nThe British were seriously handicapped in the American Revolutionary War by the need to ship all supplies across the Atlantic, since the Americans prevented most local purchases. The British found a solution after the war by creating the infrastructure and the experience needed to manage an empire. London reorganized the management of the supply of military food and transport that was completed in 1793–94 when the naval Victualling and Transport Boards undertook those responsibilities. It built upon experience learned from the supply of the very-long-distance Falklands garrison (1767–72) to systematize needed shipments to distant places such as Australia, Nova Scotia, and Sierra Leone. This new infrastructure allowed Britain to launch large expeditions to the Continent during the French Revolutionary War and to develop a global network of colonial garrisons.\n\nBefore the Napoleonic wars, military supply was based on contracts with private companies, looting and requisition (legal taking of whatever the army needed, with minimal compensation). Napoleon made logistical operations a major part of French strategy. During the Ulm Campaign in 1805, the French army of 200,000 men had no need for time-consuming efforts to scour the countryside for supplies and live off the land, as it was well provided for by France's German allies. France's ally, the Electorate of Bavaria, turned the city of Augsburg into a gigantic supply center, allowing the Grande Armée, generously replenished with food, shoes and ammunition, to quickly invade Austria after the decisive French victory at Ulm. Napoleon left nothing to chance, requesting the Bavarians to prepare in advance a specified amount of food at certain cities such as Würzburg and Ulm, for which the French reimbursed them. When French demands proved excessive for the German principalities, the French army used a system of vouchers to requisition supplies and keep the rapid French advance going. The agreements with French allies permitted the French to obtain huge quantities of supplies within a few days' notice. Napoleon built up a major supply magazine at Passau, with barges transporting supplies down the Danube to Vienna to maintain the French army prior to the Battle of Austerlitz in combat readiness. In 1807, Napoleon created the first \"military train\" regiments — units entirely dedicated to the supply and the transport of equipment.\n\nThe French system fared poorly in the face of a guerrilla warfare that targeted supply lines during the Peninsular War in Spain, and the British blockade of Spanish ports. The need to supply a besieged Barcelona made it impossible to control the province and ended French plans to incorporate Catalonia into Napoleon's Empire.\n\nThe first theoretical analysis of this was by the Swiss writer, Antoine-Henri Jomini, who studied the Napoleonic wars. In 1838, he devised a theory of war based on the trinity of \"strategy\", \"tactics\", and \"logistics\".\n\nRailways and steamboats revolutionized logistics by the mid-19th century.\n\nIn the American Civil War (1861–65), both armies used railways extensively, for transport of personnel, supplies, horses and mules, and heavy field pieces. Both tried to disrupt the enemy's logistics by destroying trackage and bridges. Military railways were built specifically for supporting armies in the field.\n\nDuring the Seven Weeks War of 1866, railways enabled the swift mobilization of the Prussian Army, but the problem of moving supplies from the end of rail lines to units at the front resulted in nearly 18,000 tons trapped on trains unable to be unloaded to ground transport. The Prussian use of railways during the Franco-Prussian War is often cited as a prime example of logistic modernizations, but the advantages of maneuver were often gained by abandoning supply lines that became hopelessly congested with rear-area traffic.\n\nWith the expansion of military conscription and reserve systems in the decades leading up to the 20th century, the potential size of armies increased substantially, while the industrialization of firepower (bolt-action rifles with higher rate-of-fire, larger and more artillery, plus machine guns) was starting to multiply the potential amount munitions each required. Military logistical systems, however, continued to rely on 19th century technology.\n\nWhen World War I started, the capabilities of rail and horse-drawn supply were stretched to their limits. Where the stalemate of trench warfare took hold, special narrow gauge trench railways were built to extend the rail network to the front lines. The great size of the German Army proved too much for its railways to support except while immobile. Tactical successes like Operation Michael devolved into operational failures where logistics failed to keep up with the army's advance over shell-torn ground. \n\nOn the seas, the British blockade of Germany kept a stranglehold on raw materials, goods, and food needed to support Germany's war efforts, and is considered one of the key elements in the eventual Allied victory in the war. At the same time, Germany's unrestricted submarine warfare showed the vulnerability of shipping lanes despite Allied naval superiority.\n\nThe mechanization of warfare, starting at the tail end of World War I, added increasing ammo, fuel, and maintenance needs of tanks and other combat vehicles to the burden on military logistics. The growing needs of more powerful and numerous military ships and aircraft increased this burden even further. On the other hand, mechanization also brought trucks to logistics; though they generally require better roads and bridges, trucks are much faster and far more efficient than fodder-bound horse-drawn transport. While many nations, including Germany, continued to rely on wagons to some extent, the US and UK readily switched to trucks wherever possible.\n\nMilitary logistics played a significant role in many World War II operations, especially ones far from industrial centers, from the Finnish Lapland to the Burma Campaign, limiting the size and movement of any military forces. In the North African Campaign, with a lack of rail, few roads, and hot-dry climate, attacks and advances were timed as much by logistics as enemy actions. Poor logistics, in the form of Russia's vast distances and its state of road and rail networks, contributed to the fate of Germany's invasion of the USSR: despite many battlefield victories, the campaign lost momentum before the gates of Moscow.\n\nBreaking the logistics supply line became a major target for airpower; a single fighter aircraft could attack dozens of supply vehicles at a time by strafing down a road, many miles behind the front line. Air superiority became critical for almost any major offensive in good weather. Allied air forces took out German-controlled bridges and rail infrastructure throughout northern France to help ensure the success of the Normandy landings, but after the breakout from Normandy, this now limited the Allies' own logistics. In response, the Red Ball Express was organized — a massive truck convoy system to supply the advance towards Germany. During the Battle of Stalingrad, Supplying by air, called an airbridge, was attempted by Germany to keep its surrounded 6th Army supplied, but they lacked sufficient air transport. Allied airbridges were more successful, in the Burma Campaign, and in \"The Hump\" to resupply the Chinese war effort. (A few years after the war, the Berlin Air Lift was successful in supplying the whole non-Soviet half of the city.)\n\nAt sea, the Battle of the Atlantic began in the first days of the war and continued to the end. German surface raiders and U-boats targeted vital Allied cargo ship convoys supplying English, US, and Russian forces, and became more effective than in World War I. Technological improvements in both U-boats and anti-submarine warfare raced to out-do each other for years, with the Allies eventually keeping losses to U-boats in check.\n\nLogistics was a major challenge for the American war effort, since wartime material had to be supplied across either the Atlantic or the even wider Pacific Ocean. Germany undertook an aggressive U-boat campaign against American logistics on the Atlantic, but the Japanese neglected to attack shipping in the Pacific, using their submarines to fight alongside the surface Navy in large-scale battles. \n\nLong logistical distances dominated the Pacific War. For the attack on Pearl Harbor, the Japanese required numerous oiler ships to refuel the attacking fleet at sea on-route. Massive numbers of transports, including thousands of US Liberty ships, were required to sustain the Allied forces fighting back towards the Japanese homeland. As in the Atlantic, submarine warfare accounted for more losses than naval battles, with over 1,200 merchant ships sank.\n\nLogistics, occasionally referred to as \"combat service support\", must address highly uncertain conditions. While perfect forecasts are rarely possible, forecast models can reduce uncertainty about what supplies or services will be needed, where and when they will be needed, or the best way to provide them.\n\nUltimately, responsible officials must make judgments on these matters, sometimes using intuition and scientifically weighing alternatives as the situation requires and permits. Their judgments must be based not only upon professional knowledge of the numerous aspects of logistics itself but also upon an understanding of the interplay of closely related military considerations such as strategy, tactics, intelligence, training, personnel, and finance.\n\nHowever, case studies have shown that more quantitative, statistical analysis are often a significant improvement on human judgment. One such recent example is the use of Applied Information Economics by the Office of Naval Research and the Marine Corps for forecasting bulk fuel requirements for the battlefield.\n\nIn major military conflicts, logistics matters are often crucial in deciding the overall outcome of wars. For instance, tonnage war—the bulk sinking of cargo ships—was a crucial factor in World War II. The successful Allied anti-submarine campaign and the failure of the German Navy to sink enough cargo in the Battle of the Atlantic allowed Britain to stay in the war and the ability to maintain a Mediterranean supply chain allowed the maintenance of the second front against the Nazis in North Africa; by contrast, the successful U.S. submarine campaign against Japanese maritime shipping across Asian waters effectively crippled its economy and its military production capabilities and the Axis were unable to consistently maintain a supply chain to their North African forces with on average 25% fewer supplies than required being landed and critical fuel shortages dictating strategic decisions. In a tactical scale, in the Battle of Ilomantsi, the Soviets had an overwhelming numerical superiority in guns and men, but managed to fire only 10,000 shells against the Finnish 36,000 shells, eventually being forced to abandon their heavy equipment and flee the battlefield, resulting in a Finnish victory. One reason for this was the successful Finnish harassment of Soviet supply lines.\n\nMore generally, protecting one's own supply lines and attacking those of an enemy is a fundamental military strategy; an example of this as a purely logistical campaign for the military means of implementing strategic policy was the Berlin Airlift.\n\nMilitary logistics has pioneered a number of techniques that have since become widely deployed in the commercial world. Operations research grew out of WWII military logistics efforts. Likewise, military logistics borrows from methods first introduced to the commercial world.\n\nThe Kargil Conflict in 1999 between India and Pakistan also referred to as Operation Vijay (Victory in Hindi) is one of the most recent examples of high altitude warfare in mountainous terrain that posed significant logistical problems for the combating sides. The Stallion which forms the bulk of the Indian Army's logistical vehicles proved its reliability and serviceability with 95% operational availability during the operation.\n\nGeographic \"distance\" is a key factor in military affairs. The shorter the distance, the greater the ease with which force can be brought to bear upon an opponent. This is because it is easier to undertake the supply of logistics to a force on the ground as well as engage in bombardment. The importance of distance is demonstrated by the Loss of Strength Gradient devised by Kenneth Boulding. This shows the advantage of supply that is forward based.\n\nThe United States Military logistics support is grouped into 10 classes of supply:\nSupply chain management in military logistics often deals with a number of variables in predicting cost, deterioration, consumption, and future demand. The US Military's categorical supply classification was developed in such a way that categories of supply with similar consumption variables are grouped together for planning purposes. For instance peacetime consumption of ammunition and fuel will be considerably less than wartime consumption of these items, whereas other classes of supply such as subsistence and clothing have a relatively consistent consumption rate regardless of war or peace. Troops will always require uniform and food. More troops will require equally more uniforms and food.\n\nIn the table above, each class of supply has a consumer. Some classes of supply have a linear demand relationship—as more troops are added more supply items are needed—as more equipment is used more fuel and ammo is consumed. Other classes of supply must consider a third variable besides usage and quantity: time. As equipment ages more and more repair parts are needed over time, even when usage and quantity stays consistent. By recording and analyzing these trends over time and applying to future scenarios, the US military can accurately supply troops with the items necessary at the precise moment they are needed. History has shown that good logistical planning creates a lean and efficient fighting force. Lack thereof can lead to a clunky, slow, and ill-equipped force with too much or too little supply.\n\n\nNotes\nBibliography\n\n"}
{"id": "27142273", "url": "https://en.wikipedia.org/wiki?curid=27142273", "title": "Monoisotopic element", "text": "Monoisotopic element\n\nA monoisotopic element is one of 26 chemical elements which have only a single stable isotope (nuclide). A list is given in a following section.\n\nStability is experimentally defined for chemical elements, as there are a number of stable nuclides with atomic numbers over ~ 40 which are theoretically unstable, but apparently have half-lives so long that they have not been observed directly or indirectly (from measurement of products) to decay.\n\nMonoisotopic elements are characterized, except in a single case, by odd numbers of protons (odd \"Z\"), and even numbers of neutrons. Because of the energy gain from nuclear pairing effects, the odd number of protons imparts instability to isotopes of an odd \"Z\", which in heavier elements requires a completely paired set of neutrons to offset this tendency into stability. (The four stable nuclides with odd \"Z\" and odd neutron numbers are hydrogen-2, lithium-6, boron-10, and nitrogen-14.)\n\nThe single mononuclidic exception to the odd \"Z\" rule is beryllium, which has 4 protons and 5 neutrons. This element is prevented from having equal numbers of neutrons and protons (4 of each) by the instability toward alpha decay, which is favored due to the extremely tight binding of helium-4 nuclei. It is prevented from having a stable isotope with 4 protons and 6 neutrons by the very large mismatch in proton/neutron ratio for such a light element. (Nevertheless, beryllium-10 has a half-life of 1.36 million years, which is too short to be primordial, but still indicates unusual stability for a light isotope with such an imbalance.)\n\nThe set of monoisotopic elements overlap but are not the same as the set of 22 mononuclidic elements, which are characterized as having essentially only one isotope (nuclide) found in nature. The reason for this is the occurrence of certain long-lived radioactive primordial nuclides in nature, which may form admixtures with the monoisotopics, and thus prevent them from being naturally mononuclidic. This happens in the cases of 7 () of the monoisotopic elements. These isotopes are monoisotopic, but due to the presence of the long lived radioactive primordial nuclide, are not mononuclidic. These elements are vanadium, rubidium, indium, lanthanum, europium, rhenium and lutetium. See the list below; in two noted cases (indium and rhenium), the long-lived radionuclide is actually the most abundant isotope in nature, and the stable isotope is less abundant. \n\nIn 3 additional cases (bismuth, thorium, and protactinium), mononuclidic elements occur primordially which are not monoisotopic because the naturally occurring nuclide is radioactive, and thus the element has no stable isotopes at all. For an element to be monoisotopic, it must have one stable nuclide.\n\nNon-mononuclidic elements are marked with an asterisk, and the long-lived primordial radioisotope given. In two notable cases (indium and rhenium), the highest abundance naturally occurring isotope is the mildly radioactive one, and in the case of europium, nearly half of it is.\n\n"}
{"id": "361950", "url": "https://en.wikipedia.org/wiki?curid=361950", "title": "Network Control Program", "text": "Network Control Program\n\nThe Network Control Program (NCP) provided the middle layers of the protocol stack running on host computers of the ARPANET, the predecessor to the modern Internet.\n\nNCP preceded the Transmission Control Protocol (TCP) as a transport layer protocol used during the early ARPANET. NCP was a simplex protocol that utilized two port addresses, establishing two connections, for two-way communications. An odd and an even port were reserved for each application layer application or protocol. The standardization of TCP and UDP reduced the need for the use of two simplex ports for each application down to one duplex port.\n\nNCP provided connections and flow control between processes running on different ARPANET host computers. Application services, such as email and file transfer, were built on top of NCP, using it to handle connections to other host computers.\n\nOn the ARPANET, the protocols in the Physical Layer, the Data Link Layer, and the Network Layer used within the network were implemented on separate Interface Message Processors (IMPs). The host usually connected to an IMP using another kind of interface, with different physical, data link and network layer specifications. The IMP's capabilities were specified by the Host/IMP Protocol in BBN Report 1822.\n\nSince lower protocol layers were provided by the IMP-host interface, NCP essentially provided a Transport Layer consisting of the \"ARPANET Host-to-Host Protocol\" (AHHP) and the \"Initial Connection Protocol\" (ICP). AHHP defined procedures to transmit a unidirectional, flow-controlled data stream between two hosts. The ICP defined the procedure for establishing a bidirectional pair of such streams between a pair of host processes. Application protocols (e.g., FTP) accessed network services through an interface to the top layer of the NCP, a forerunner to the Berkeley sockets interface.\n\nOn January 1, 1983, known as flag day, NCP was officially rendered obsolete when the ARPANET changed its core networking protocols from NCP to the more flexible and powerful TCP/IP protocol suite, marking the start of the modern Internet.\n\n\n"}
{"id": "21933", "url": "https://en.wikipedia.org/wiki?curid=21933", "title": "Neutron activation analysis", "text": "Neutron activation analysis\n\nNeutron activation analysis (NAA) is a nuclear process used for determining the concentrations of elements in a vast amount of materials. NAA allows discrete sampling of elements as it disregards the chemical form of a sample, and focuses solely on its nucleus. The method is based on neutron activation and therefore requires a source of neutrons. The sample is bombarded with neutrons, causing the elements to form radioactive isotopes. The radioactive emissions and radioactive decay paths for each element are well known. Using this information, it is possible to study spectra of the emissions of the radioactive sample, and determine the concentrations of the elements within it. A particular advantage of this technique is that it does not destroy the sample, and thus has been used for analysis of works of art and historical artifacts. NAA can also be used to determine the activity of a radioactive sample.\n\nIf NAA is conducted directly on irradiated samples it is termed Instrumental Neutron Activation Analysis (INAA). In some cases irradiated samples are subjected to chemical separation to remove interfering species or to concentrate the radioisotope of interest, this technique is known as Radiochemical Neutron Activation Analysis (RNAA).\n\nNAA can perform non-destructive analyses on solids, liquids, suspensions, slurries, and gases with no or minimal preparation. Due to the penetrating nature of incident neutrons and resultant gamma rays, the technique provides a true bulk analysis. As different radioisotopes have different half-lives, counting can be delayed to allow interfering species to decay eliminating interference. Until the introduction of ICP-AES and PIXE, NAA was the standard analytical method for performing multi-element analyses with minimum detection limits in the sub-ppm range. Accuracy of NAA is in the region of 5%, and relative precision is often better than 0.1%. There are two noteworthy drawbacks to the use of NAA; even though the technique is essentially non-destructive, the irradiated sample will remain radioactive for many years after the initial analysis, requiring handling and disposal protocols for low-level to medium-level radioactive material; also, the number of suitable activation nuclear reactors is declining; with a lack of irradiation facilities, the technique has declined in popularity and become more expensive.\n\nNeutron activation analysis is a sensitive multi-element analytical technique used for both qualitative and quantitative analysis of major, minor, trace and rare elements. NAA was discovered in 1936 by Hevesy and Levi, who found that samples containing certain rare earth elements became highly radioactive after exposure to a source of neutrons. This observation led to the use of induced radioactivity for the identification of elements. NAA is significantly different from other spectroscopic analytical techniques in that it is based not on electronic transitions but on nuclear transitions. To carry out an NAA analysis, the specimen is placed into a suitable irradiation facility and bombarded with neutrons. This creates artificial radioisotopes of the elements present. Following irradiation, the artificial radioisotopes decay with emission of particles or, more importantly gamma rays, which are characteristic of the element from which they were emitted.\n\nFor the NAA procedure to be successful, the specimen or sample must be selected carefully. In many cases small objects can be irradiated and analysed intact without the need of sampling. But, more commonly, a small sample is taken, usually by drilling in an inconspicuous place. About 50 mg (one-twentieth of a gram) is a sufficient sample, so damage to the object is minimised. It is often good practice to remove two samples using two different drill bits made of different materials. This will reveal any contamination of the sample from the drill bit material itself. The sample is then encapsulated in a vial made of either high purity linear polyethylene or quartz. These sample vials come in many shapes and sizes to accommodate many specimen types. The sample and a standard are then packaged and irradiated in a suitable reactor at a constant, known neutron flux. A typical reactor used for activation uses uranium fission, providing a high neutron flux and the highest available sensitivities for most elements. The neutron flux from such a reactor is in the order of 10 neutrons cm s. The type of neutrons generated are of relatively low kinetic energy (KE), typically less than 0.5 eV. These neutrons are termed thermal neutrons. Upon irradiation, a thermal neutron interacts with the target nucleus via a non-elastic collision, causing neutron capture. This collision forms a compound nucleus which is in an excited state. The excitation energy within the compound nucleus is formed from the binding energy of the thermal neutron with the target nucleus. This excited state is unfavourable and the compound nucleus will almost instantaneously de-excite (transmutate) into a more stable configuration through the emission of a prompt particle and one or more characteristic prompt gamma photons. In most cases, this more stable configuration yields a radioactive nucleus. The newly formed radioactive nucleus now decays by the emission of both particles and one or more characteristic delayed gamma photons. This decay process is at a much slower rate than the initial de-excitation and is dependent on the unique half-life of the radioactive nucleus. These unique half-lives are dependent upon the particular radioactive species and can range from fractions of a second to several years. Once irradiated, the sample is left for a specific decay period, then placed into a detector, which will measure the nuclear decay according to either the emitted particles, or more commonly, the emitted gamma rays.\n\nNAA can vary according to a number of experimental parameters. The kinetic energy of the neutrons used for irradiation will be a major experimental parameter. The above description is of activation by slow neutrons, slow neutrons are fully moderated within the reactor and have KE <0.5 eV. Medium KE neutrons may also be used for activation, these neutrons have been only partially moderated and have KE of 0.5 eV to 0.5 MeV, and are termed epithermal neutrons. Activation with epithermal neutrons is known as Epithermal NAA (ENAA). High KE neutrons are sometimes used for activation, these neutrons are unmoderated and consist of primary fission neutrons. High KE or fast neutrons have a KE >0.5 MeV. Activation with fast neutrons is termed Fast NAA (FNAA).\nAnother major experimental parameter is whether nuclear decay products (gamma rays or particles) are measured during neutron irradiation (prompt gamma), or at some time after irradiation (delayed gamma, DGNAA). PGNAA is generally performed by using a neutron stream tapped off the nuclear reactor via a beam port. Neutron fluxes from beam ports are the order of 10 times weaker than inside a reactor. This is somewhat compensated for by placing the detector very close to the sample reducing the loss in sensitivity due to low flux. PGNAA is generally applied to elements with extremely high neutron capture cross-sections; elements which decay too rapidly to be measured by DGNAA; elements that produce only stable isotopes; or elements with weak decay gamma ray intensities. PGNAA is characterised by short irradiation times and short decay times, often in the order of seconds and minutes.\nDGNAA is applicable to the vast majority of elements that form artificial radioisotopes. DG analyses are often performed over days, weeks or even months. This improves sensitivity for long-lived radionuclides as it allows short-lived radionuclide to decay, effectively eliminating interference. DGNAA is characterised by long irradiation times and long decay times, often in the order of hours, weeks or longer.\n\na range of different sources can be used:\n\nSome reactors are used for the neutron irradiation of samples for radioisotope production for a range of purposes. The sample can be placed in an irradiation container which is then placed in the reactor; if epithermal neutrons are required for the irradiation then cadmium can be used to filter out the thermal neutrons.\n\nA relatively simple Farnsworth–Hirsch fusor can be used to generate neutrons for NAA experiments. The advantages of this kind of apparatus is that it is compact, often benchtop-sized, and that it can simply be turned off and on. A disadvantage is that this type of source will not produce the neutron flux that can be obtained using a reactor.\n\nFor many workers in the field a reactor is an item which is too expensive, instead it is common to use a neutron source which uses a combination of an alpha emitter and beryllium. These sources tend to be much weaker than reactors.\n\nThese can be used to create pulses of neutrons, they have been used for some activation work where the decay of the target isotope is very rapid. For instance in oil wells.\n\nThere are a number of detector types and configurations used in NAA. Most are designed to detect the emitted gamma radiation. The most common types of gamma detectors encountered in NAA are the gas ionisation type, scintillation type and the semiconductor type. Of these the scintillation and semiconductor type are the most widely employed. There are two detector configurations utilised, they are the planar detector, used for PGNAA and the well detector, used for DGNAA. The planar detector has a flat, large collection surface area and can be placed close to the sample. The well detector ‘surrounds’ the sample with a large collection surface area.\n\nScintillation-type detectors use a radiation-sensitive crystal, most commonly thallium-doped sodium iodide (NaI(Tl)), which emits light when struck by gamma photons. These detectors have excellent sensitivity and stability, and a reasonable resolution.\n\nSemiconductor detectors utilise the semiconducting element germanium. The germanium is processed to form a p-i-n (positive-intrinsic-negative) diode, and when cooled to ~77 K by liquid nitrogen to reduce dark current and detector noise, produces a signal which is proportional to the photon energy of the incoming radiation. There are two types of germanium detector, the lithium-drifted germanium or Ge(Li) (pronounced ‘jelly’), and the high-purity germanium or HPGe.\nThe semiconducting element silicon may also be used but germanium is preferred, as its higher atomic number makes it more efficient at stopping and detecting high energy gamma rays. Both Ge(Li) and HPGe detectors have excellent sensitivity and resolution, but Ge(Li) detectors are unstable at room temperature, with the lithium drifting into the intrinsic region ruining the detector. The development of undrifted high purity germanium has overcome this problem.\n\nParticle detectors can also be used to detect the emission of alpha (α) and beta (β) particles which often accompany the emission of a gamma photon but are less favourable, as these particles are only emitted from the surface of the sample and are often absorbed or attenuated by atmospheric gases requiring expensive vacuum conditions to be effectively detected. Gamma rays, however, are not absorbed or attenuated by atmospheric gases, and can also escape from deep within the sample with minimal absorption.\n\nNAA can detect up to 74 elements depending upon the experimental procedure, with minimum detection limits ranging from 0.1 to 1x10 ng g depending on element under investigation. Heavier elements have larger nuclei, therefore they have a larger neutron capture cross-section and are more likely to be activated. Some nuclei can capture a number of neutrons and remain relatively stable, not undergoing transmutation or decay for many months or even years. Other nuclei decay instantaneously or form only stable isotopes and can only be identified by PGNAA.\n\nNeutron Activation Analysis has a wide variety of applications including within the fields of archaeology, soil science, geology, forensics, and the semiconductor industry. Forensically, hairs subjected to a detailed forensic neutron analysis to determine whether they had sourced from the same individuals was first used in the trial of John Norman Collins.\n\nArchaeologists use NAA in order to determine the elements that comprise certain artifacts. This technique is used because it is nondestructive and it can relate an artifact to its source by its chemical signature. This method has proven to be very successful at determining trade routes, particularly for obsidian, with the ability of NAA to distinguish between chemical compositions. In agricultural processes, the movement of fertilizers and pesticides is influenced by surface and subsurface movement as it infiltrates the water supplies. In order to track the distribution of the fertilizers and pesticides, bromide ions in various forms are used as tracers that move freely with the flow of water while having minimal interaction with the soil. Neutron activation analysis is used to measure bromide so that extraction is not necessary for analysis. NAA is used in geology to aid in researching the processes that formed the rocks through the analysis of the rare earth elements and trace elements. It also assists in locating ore deposits and tracking certain elements. Neutron activation analysis is also used to create standards in the semiconductor industry. Semiconductors require a high level of purity, with contamination significantly reducing the quality of the semiconductor. NAA is used to detect trace impurities and establish contamination standards, because it involves limited sample handling and high sensitivity.\n"}
{"id": "23042988", "url": "https://en.wikipedia.org/wiki?curid=23042988", "title": "Oxford Leadership Academy", "text": "Oxford Leadership Academy\n\nThe Oxford Leadership Academy (OLA) is an international leadership consultancy based in Oxford, United Kingdom. The firm specialises in values-driven leadership development, strategy execution and culture change in complex global organizations. The company aims to improve the social responsibility and ethical commitments of business leaders. It has 200,000 alumni and 215 people in 28 countries.\n\nThe company supports the corporate social responsibility (CSR) movement, a form of corporate self-regulation integrated into a business model. Brian Bacon, OLA's President says: The company’s consulting processes and approach to leadership development are influenced by the work of Willis Harman, Ken Wilber, Peter Senge, Joseph Jaworski and Margaret Wheatley.\n\nThe main activities of the company are consulting in strategy execution, leadership development and culture change for multinational corporations. It also undertakes pro-bono work in support of non-profit organisations, including Girl Scouts USA and the British Red Cross.\n\nThe company had its origins in Australia in 1983 when Brian Bacon formed the management consulting arm of global advertising agency DDB Needham. The company supported the implementation of global marketing strategies for the Ford Motor Company, McDonald’s, P&G and Glaxo from 1983 to 1989. In 1990, the company separated from DDB Needham, and became International Pacific Consulting (IPC).\n\nFrom 1990 to 2000 the company worked with the micro-economic reform agenda of the Australian Federal Government, mainly on organisational reform in federal government, the national power industry and telecom sector as well as for global clients such as Volvo, Ericsson, Scandia, Pharmacia and Levi’s; and international agencies including the ILO and the United Nations in Geneva and New York.\n\nThe CEO, Brian Bacon, served on the WBA Board as Regional Vice-Chair for Europe from 1988-1996. He considers the thought leadership of World Business Academy co-founder Willis Harman, to be a major influence in the work of OLA. \nIn 1996, IPC Worldwide was engaged by Karen Jespersen, then the Danish Minister of Social Affairs, to engage prominent business executives to contribute to the debate on the social responsibility of enterprises and to encourage businesses to take initiatives which are designed to increase the social well-being both of their own staff and the local community in which they are located. More than 250 delegates from international companies and organisations, along with ministers and representatives of 17 governments were invited to discuss new partnerships for social cohesion at a conference on 16–18 October 1997 . This initiative, known as ‘New Partnerships for Social Cohesion’ involved representatives of international business in a \"think-tank’ in conjunction with members of the European Commission, the International Institute for Labour Studies (ILO), the United Nations Industrial Development Organization (UNIDO), the United Nations Development Programme (UNDP), the Japan Institute of Labour and representatives of Danish labour market organisations. This effort resulted in the formation of The Copenhagen Centre, which later became the Danish Centre for CSR in the Danish Commerce and Companies Agency.\n\nSeeking to continue this work further, in 1999 IPC Worldwide joined with Oxford Research in Denmark (representative of Oxford Analytica in the Nordic countries ) to form Oxford IPC Worldwide, a strategic management consultancy firm. In 2000 the company entered into a new phase of growth with major strategy execution consulting assignments for McDonald’s (2002–present) and for the Mexican Government. Brian Bacon was a special advisor to Mexico President Vicente Fox from 2000- 2006. On behalf of the Mexican Government Office of Innovation and Quality he led the leadership development and strategic focusing of several Government Ministries and the Cabinet of the President. During this time, the company also supported the EU with the Balkans Stability Pact in conjunction with Swedish Government Agency, International IDEA International Institute for Democracy and Electoral Assistance, working closely with Foreign Minister Anna Lind.\n\nThe company relocated its headquarters, UK to Oxford in 2005 and became the Oxford Leadership Academy. The founding directors of OLA were Dr Kim Moller, Dr Ron Nahser, Alan Johnson and Brian Bacon. Oxford Leadership Academy opened its offices in UK, Denmark and Hong Kong in 2005; Stockholm, Sweden in 2009; San Francisco, USA in 2010 and Madrid, Spain in 2011.\n\nOLA is engaged in the leadership consulting and training of top management teams for multinational clients including Unilever, Telefónica, AkzoNobel, BASF in 28 countries. it also provides pro-bono leadership training for non-profit organisations including the Girl Scouts USA and the British Red Cross.\n\nCyril Legrand is Chief Executive Officer since May 2011. He was previously a senior executive with French multinational Pernod Ricard in Asia before joining Oxford Leadership as Director of Client service in 2009. He is an alumnus of HEC in Paris and Oxford Said Business School.\n\nDr. Kim Moller is co-founder and President of Oxford Leadership in Nordic Region and Chairman of the Oxford Research, a sister company of Oxford Leadership. He is a strategist and researcher, formerly with Copenhagen Business School.\n\nBrian Bacon is Chairman and Founder of Oxford Leadership Academy. He was previously a Founding Trustee and President of World Business Academy. He was a mentor and strategy advisor to McDonald’s CEO Charlie Bell from 2002 to 2005; a special advisor to Mexican president Vicente Fox and the Office of Government Innovation from 2002 to 2006. He was a special advisor to the Swedish Government in the modernization of the Foreign Ministry from 2004 to 2006. He is a former visiting scholar at the International Institute of Labour Studies in Geneva and has been an advisor to the governments of Australia, Brazil, Chile, Mexico, Thailand and Sweden.\n\nThe Supervisory Board of Oxford Leadership Academy comprises Brian Bacon (Chairman) UK, Andre Bischoff (Commercial Director) UK, Cyril Legrand (CEO) France, Thomas Hurley (President USA), Juan Carlos Murillo (President Mexico), Ken O'Donnell (Vice Chair Latin America) Brazil, Juan Rovira (Vice Chair Iberia) Spain, Dr Kim Moller (President Nordic) Denmark, Lasse Wrennmark (Head of Leadership Training)Sweden, Kazemaru Yukawa-Bacon(Zenergy Academy Division) USA, Dr Erika Kleestorfer (Senior Fellow) Austria.\n\nIn 1997, Brian Bacon was awarded the Willis Harman Award by the World Business Academy, inspired by the work of Willis Harman, a social scientist who articulated the possibility for humankind to transcend the limits of outmoded thinking.\n\nIn 2006, Brian Bacon received the INNOVA Award from the Mexican president Vicente Fox, one of only 10 commendations given by President Fox to individuals and institutions who made a special contribution to the Republic of Mexico during the Presidential term of Vicente Fox.\n\n"}
{"id": "57418849", "url": "https://en.wikipedia.org/wiki?curid=57418849", "title": "Probe tip", "text": "Probe tip\n\nA probe tip in scanning microscopy literally is a very sharp piece of metal or non-metal, like a sewing needle with a point at one end with nano or sub-nanometer order of dimension. It can interact with up to one molecule or atom of a given surface of a sample that can reveal authentic properties of the surface such as morphology, topography, mapping and electrical properties of a single atom or molecule on the surface of the sample.\n\nThe proliferation of the thrust in probe-based tools began after the invention of Scanning tunneling microscope (STM) and Atomic force microscopy (AFM) (collectively called Scanning probe microscopy-SPM) by Gerd Binnig and Heinrich Rohrer at the IBM Zurich research laboratory in 1982. It opened a new era for probing nanoscale world of individual atoms and molecules as well as surface science due to their unprecedented capability of characterizing a wide range of unique properties such as mechanical, chemical, magnetic and optical functionalities of various samples at nanometer-scale resolution in vacuum, ambient and fluid environment. The utilization of sharp probe tips enabled us to see the inside the microscopic world from within the macroscopic world. The increasing demand for sub-nanometer probe tips attributes to their robustness and versatile applicability because of their direct application to the numerous fields of science that includes nanolithography, nanoelectronics, biosensor, Electrochemistry, Semiconductor, Micromachining and biological cells studies. The significant number of applications for the topographic surface characterization of the materials and biological specimen in various fields of science made researchers and scientists that it is imperative for reproducible mass production of probe tip with sharp apex.\n\nProbe tip size and shape in microscopy are important parameters providing direct connection between resolution and imaging quality. Resolution and imaging mechanism may depend on geometry (length, width, shape, aspect ratio, and tip apex radius) and composition (material properties) of tip and surface being probed. Tip size, shape and reproducibility are extremely important to monitor and detect the interaction between the surfaces.\n\nHere, we describe fabrication, characterization and application of sharp tips. A wide range of tip fabrication techniques including cutting, grinding, pulling, beam deposition, ion milling, controlled crashing, field emission, field evaporation, fracture and electrochemical etching/polishing are discussed. Both limitations and advantages are also provided for various tip fabrication method. We also describe history and development, working principles, characterization and applications with recent advancement of sharp tips.\n\nThe discovery of a sharp probe tip has always been of significant interest among the researchers considering its importance in the material, life and biological sciences for mapping the surface structure and material properties at molecular or atomic dimension. The history of tip can be tracked long back in nineteenth century during the invention of Phonautograph in 1859. Phonautograph is the predecessor of modern gramophone. It was invented by Scott and Koenig. It consisted of a parchment diaphragm with an attached stylus (sort of a pen-holder), along with a Hog’s hair, which was used to trace a wavy line on a lamp-blacked surface [Musical mirror by Bryceson, Richard, Jan. 1929-Dec.1930]. In the later development gramophone came out where along with other replacements, the Hog’s hair was replaced by a needle to reproduce sound [Musical mirror by Bryceson, Richard, Jan. 1929-Dec.1930]. In 1940, a pantograph was built utilizing a shielded probe and adjustable tip. A stylus was free to slide vertically to be in contact with the paper. In 1948, a tip was employed in probe circuit to measure peak voltage. The fabrication of electrochemically etched sharp tungsten, copper, nickel and molybdenum tip was reported by Muller in 1937. The revolution for sharp tips occurred in producing a good and various kind of tip with different shape, size, aspect ratio composed of tungsten wire, silicon, diamond, Carbon nanotube etc. occurred with Si-based circuit technologies. This allowed tips for numerous applications in the broad spectrum of nanotechnological fields. Following STM invention came Atomic force microscopy (AFM) by Gerd Binnig, Calvin F. Quate, and Christoph Gerber in 1986. In their instrument they used diamond broken piece as tip sticking it to a hand-cut gold foil cantilever. Focused ion and electron beam technique for the fabrication of strong, stable, reproducible SiN pyramidal tip with 1.0 μm length and 0.1 μm diameter was reported by Russell in 1992. Ground breaking advancement came through the introduction of microfabrication methods for the fabrication of precised conical or pyramidal silicon and silicon nitride tip. Later, numerous research experiments were explored to fabricate comparatively less expensive and robust tungsten tip production and characterization technique to attain less than 50 nm radius of curvature.\n\nThe new horizon in the field of fabrication of probe tip was revealed to the experts when carbon nanotube which is basically about 1 nm cylindrical shells of graphene was introduced. The use of single wall Carbon nanotubes is less vulnerable to breaking or crushing during imaging due to their flexibility. Probe tip made up of Carbon nanotubes can be efficiently used to get high-resolution images of both soft and weakly absorbed Biomolecules like DNA on surface at molecular resolution.\n\nMultifunctional hydrogel nano-probe technique uncovered new scope initiating a completely new concept to the fabrication of tip and their extended ease of applicability to inorganic and biological samples in both air and liquid. The biggest advantage of this mechanical method is that the tip can be made in different shape e.g. hemispherical, embedded spherical, pyramidal, and distorted pyramidal etc. with diameter ranging from 10 nm – 1000 nm for applications including Topography or functional imaging, force spectroscopy on soft matter, biological, chemical and physical Sensors. Table 1. Summarizes various fabrication, material and application of tips.\n\nThe tip itself does not have any working principle for imaging, but depending on the instrumentation, mode of application, and the nature of the sample under investigation, the probe tip may follow different principle to image the surface of the sample. For example, when a tip is integrated with STM, it measures the tunneling current that arises from the interaction between the sample and the tip. In AFM, short-ranged force deflection during the raster scan by tip across the surface is measured. A conductive tip is essential for the STM instrumentation whereas AFM can use conductive and non-conductive prob tip. Although probe tip is used in various techniques with different principle, for STM and AFM coupled with probe tip is discussed in detail.\n\nSomewhat, the name implies STM utilizes tunneling charge transfer principle from tip to surface or vice versa thereby recording the current response. This concept originates from particle in a box concept, that is, if potential energy for a particle is small, electron may be found outside of potential well which is a classically forbidden region. This phenomenon is called tunneling.\n\nExpression derived from Schrodinger equation for transmission charge transfer probability is as follow:\n\nformula_1\n\nwhere\n\nNon-conductive nano-scale tips are widely used for AFM measurements. For non-conducting tip, surface forces acting on the tip/cantilever, are responsible for deflection or attraction of tip. These attractive or repulsive forces are used for surface topology, chemical specifications, magnetic and electronic properties. The distance dependent forces between substrate surface and tip are responsible for imaging in AFM. These interactions include are van der Waals forces, capillary forces, electrostatic forces, Casimir forces and solvation forces. One unique repulsion force is Pauli Exclusion repulsive force is responsible for single atom imaging as in references and Figures 10 & 11 (contact region in Fig. 1).\nTip fabrication techniques fall generally in any two classifications: mechanical and physicochemical. In the early stage of development of probe tips, mechanical procedures were popular because of ease of fabrication. One of mechanical method for the fabrication of hydrogel tip will be discussed in detail.\n\nA few reported mechanical methods to fabricate tips include cutting grinding and pulling. For example, cutting a wire at certain angles with razor blade or wire cutter or scissor. Another mechanical method for tip preparation is fragmentation of bulk pieces into small pointy pieces. Grinding a metal wire/rod into a sharp tip was also a method used. These mechanical procedures usually leave rugged surface with many tiny asperities protruding from the apex which lead to atomic resolution on flat surface. However, irregular shape and large macroscopic radius of curvature result in poor reproducibility and decreased stability especially for probing rough surfaces. Another main disadvantage of making probe by this method is it yields many mini tips which lead to many different signals yielding error in imaging. Cutting, grinding and pulling procedures can only be adapted for metallic tips like W, Ag, Pt, Ir, Pt-Ir and gold. Non-metallic tips cannot be fabricated by these methods.\n\nOn other hand a sophisticated mechanical method for tip fabrication is based on hydro-gel method. This method is based on bottom-up strategy to make probe tips by molecular self-assembly process. First, cantilever is formed in mold by curing pre-polymer solution then it is brought into contact with mold of tip which contains pre-polymer solution. The polymer is cured with ultraviolet light which helps firmly attachment of cantilever with probe. This fabrication method is shown in Fig. 2.\n\nPhysiochemical procedures are fabrication methods of choice these days which yield extremely sharp and symmetric tips with more reproducibility compared to mechanical fabrication-based tips. Among physicochemical method, the electrochemical etching method is one of the most popular method. Etching is two or more steps procedure. The “zone electropolishing” is the second step which further sharpens the tip in a very controlled manner. Other physicochemical methods include chemical vapor deposition, electron beam deposition onto pre-existing tips. Other tip fabrication methods include field ion microscopy and ion milling. In field ion microscopy techniques, consecutive field evaporation of single atoms yields specific atomic configuration at probe tip which yields very high resolution.\n\nElectrochemical etching is one of the easiest, inexpensive, most practical, reliable and most widely accepted metallic probe tip fabrication method with desired quality and reproducibility. Three commonly used electrochemical etching for tungsten tip fabrication are: single lamella drop-off methods, double lamella drop-off method and submerged method. Various cone shape tips can be fabricated by this method by minor changes in the experimental setup. An DC potential between tip and metallic electrode (usually W wire) immersed in solution (Figure 3 a-c). Electrochemical reactions at cathode and anode in basic solutions (2M KOH or 2M NaOH) are usually used. The overall etching process involved is written here:\n\nAnode;\n\n<chem>W (s) + 8OH- -> WO4 + 4H2O + 6e- (E= 1.05V)</chem>\n\nCathode:\n\n<chem>6H2O + 6e- -> 3H2 + 6OH- (E=-2.48V)</chem>\n\nOverall:\n\n<chem>W (s) + 2OH- -> WO4^2- + 2H2O (l) + 6e- + 3H2 (g) (E= -1.43V)</chem>\n\nHere, all the potentials are reported vs. SHE.\n\nSchematics of fabrication method of probe tip through electrochemical etching method is shown in Fig. 3.\n\nExperimental setup for Electrochemical etching is shown in Fig. 3a.\n\nDifferent tips fabricated from etching method with different tip radius and angle is illustrated in Fig. 3(b-e).\n\nIn electrochemical etching process, W is etched at liquid, solid and air interface (due to surface tension), as shown in Fig. 3. Etching is called static if W wire is kept stationary. Once the tip is etched, lower part falls due to the lower tensile strength than the weight of lower part of wire. The irregular shape is produced by the shifting of meniscus. However, slow etching rate can produce regular tips which controlling current slowly through electrochemical cell. The dynamic etching involves slowly pulling up the wire from solution or sometimes it is moved up and down (oscillating wire) producing smooth tips.\n\nIn this method a metal wire is vertically etched reducing the diameter from 0.25 mm ~ 20 nm. Schematic diagram for probe tip fabrication with submerged electrochemical etching method is illustrated in Fig 4. These tips can be used for high quality STM images.\nIn double lamella method the lower part of metal is etched away, and upper part of tip was not etched further. Further etching of the upper part of wire is prevented by covering it with a polymer coating. This method is usually limited to laboratory fabrication. Double lamella method schematic is shown in Fig. 5.\nTransitions metals like Cu, Au and Ag adsorbs single molecules linearly on their surface due to weak van der Waals forces. This linear projection of single molecule allows interaction of terminal atom of tip with atom of substrate resulting in Pauli repulsion for single molecule or atom mapping studies. Gases deposition on tip is carried out in an ultrahigh vacuum (5 x 10 mbar) chamber at a low temperature (10K). Deposition of Xe, Kr, NO, CH or CO on tip have been successfully prepared and used for imaging studies. However, these tips preparation rely on attachment of single atom or molecule on tip and resulting atomic structure of tip is not known exactly. Probability of attachment of simple molecule on metals surface is very tedious and required great skills. Therefore, this method is not ubiquitous and not many laws are able to perform these experiments.\n\nSharp tips used in SPM are fragile and prone to damage and wear and tear easily under high working load. Diamond is considered the best option to address this issue. Diamond tips for SPM application are fabricated by fracture of bulk diamond, grinding and polishing diamond. But, these methods result in considerable loss of diamond. Other strategy to prevent this loss is coating of Silicone tips with thin diamond film. These thin films are usually deposited by CVD. In CVD, diamond is deposited directly on silicon or W cantilever. A schematic diagram for chemical vapor deposition set up is shown in Fig. 6. In this method, flow of methane and hydrogen gas is maintained in such a way that pressure inside chamber is maintained at 40Torr. CH and H are dissociated at elevated temperature of 2100 °C with the help of Ta filament. Nucleation sites are created on the tip of cantilever. Once CVD is complete, CH flow is stopped and chamber is cooled under flow of H. Schematics of CVD set up for diamond tip fabrication for AFM application is in Fig. 6.\n\nIn RIE method, first a grove or structure is made on a substrate followed by deposition of a desired material in that template. Once tip is formed, templating structure is etched off leaving tip and cantilever. A schematic for diamond tip fabrication on silicon wafers through this method has been described in Fig. 7\n\nFocused ion beam milling milling is a sharpening method for probe tips in SPM. In this method, first a blunt tip is fabricated by other methods, for example, pyramid mold can be used to fabricate pyramidal tip, CVD method or any other etching method. Then, this tip is sharpened by FIB milling as shown in Fig. 8. The focused ion beam diameter is controlled through a programmable aperture which directly correlates with tip diameter. \nThis method is used to attach carbon nanotubes on cantilever or blunt tip. A strong adhesive (such as soft acrylic glue) is used to bind CNT with silicon cantilever. CNT are robust, stiff and increase durability of probe tip and can be used for both contact and tapping mode.\n\nElectrochemically etched tips are usually covered with contaminant on surface which cannot be removed simply by rinsing in water, acetone or ethanol. Some oxides layers on metallic tips, especially on tungsten, need to be removed by post fabrication treatment.\n\nTo clean W sharp tips, it is highly desirable to remove contaminant and oxide layer. In this method a tip is heated in UHV chamber at elevated temperature which desorb contaminated layer. Reaction detail is shown below.\n\n2WO + W → 3WO ↑\n\nWO → W (sublimation at formula_51075K)\n\nAs at elevated temperature, trioxides of W are converted to WO which sublimates around 1075K and cleaned metallic W surface left behind. Additional advantage provided by annealing is healing of crystallographic defects produced by fabrication and it also smoothens the tip surface.\n\nIn HF cleaning method, freshly prepared tip is dipped in 15% concentrated hydrofluoric acid which dissolves oxides of W for 10~30 s.\n\nIn this method, argon ions are directed to the tip surface to remove contaminant layer by sputtering. Tip is either rotated in a flux of argon ions at certain angle in a way that this beam hits on the apex. The bombardment of ions on the tip depletes the contaminants and also results in reduction of radius of tip. Bombardment time needs to be finely tuned with respect to shape of tip. Sometimes, a short annealing is required after ion milling.\n\nThis method is very similar to ion milling but, in this procedure, UHV chamber is filled with neon at a pressure of 10 mbar. When negative voltage is applied on the tip, strong electric field (produced by tip under negative potential) will ionize neon gas and these positively charged ions are accelerated back to tip and causing sputtering at the tip. The sputtering removes contaminants and some atoms from tip which, like ion milling reduces apex radius. By, changing the field strength, one can tune radius of tip to 20 nm.\n\nThe surface of the Silicon based tips cannot be easily controlled because they usually carry silanol group. Si surface is hydrophilic and can be contaminated easily by environmental. Another disadvantage of Si tips is wear and tear of tip. It is important to coat Si tip to prevent tip deteriorations. The tip coating may also enhance image quality. Following types of coatings are employed for Si tips. First an adhesive layer is pasted (usually chromium layer on 5 nm thick titanium) and then gold is deposited by vapor deposition (40-100 nm or less). Sometimes, the coating layering reduces tunneling current detection capability of probe tips.\n\nThe most important aspect of a probe tip is imaging the surfaces efficiently at nanometer. Some concerns involving credibility of the imaging or measurement of sample arises when the shape of tip is not determined accurately. For example, when an unknown tip is used to measure linewidth pattern or other high aspect ratio feature of a surface. There may remain some confusion for the determination of the contribution of tip and the sample in the acquired image. Consequently, it is important to fully and accurately characterize the tips. Probe tips can be characterized for their shape, size, sharpness, bluntness, aspect ratio, radius of curvature, geometry and composition using many advanced instrumental techniques. For example, electron field emission measurement, scanning electron microscopy (SEM), transmission electron microscopy (TEM), scanning tunneling spectroscopy as well as more easily accessible optical microscope. In some cases, optical microscopy cannot provide exact measurements for small tips in nanoscale due to resolution limitation of the optical microscopy.\n\nIn electron field emission current measurement method, a high voltage is applied between tip and another electrode followed by measuring field emission current employing Fowler-Nordheim curves formula_6. Large fields-emission current measurements may indicate that the tip is sharp and low field-emission current indicates that the tip is blunt, molten or mechanically damaged. A minimum voltage is essential to facilitate the release of electrons from the surface of tip which in turn indirectly is used to obtain the tip curvature. The downside of this method to the several advantages is that the high electric field required for producing strong electric force that can melt the apex of the tip or might change crystallographic tip nature.\n\nThe size and shape of the tip can be obtained by scanning electron microscopy and transmission electron microscopy measurements. In addition, TEM images are helpful to detect any layer of insulating materials on the surface of the tip as well as to estimate the size of the layer. These oxides are formed gradually on the surface of tip the right after fabrication due to the oxidation of metallic tip by reacting with the O present in the surrounding atmosphere. SEM has a resolution limitation of below 4 nm, TEM may be needed to observe even a single atom theoretically and practically. Tip grain down to 1-3 nm or thin polycrystalline oxides or carbon or graphite layers at the tip apex are routinely measured using TEM. The orientation of tip crystal i.e. the angle between the tip plane in the single-crystal and the tip normal can be estimated.\n\nIn the past, optical microscope has been only used to investigate if the tip is bent microscale imaging at many microscales. This is because the resolution limitation of an optical microscope is about 200 nm. Imaging software including ImageJ allows determination of the curvature, and aspect ratio of the tip. One drawback of this method is that it renders an image of tip which is a object due to the uncertainty in the nanoscale dimension. This problem can be resolved taking images of tip multiple times followed by putting them together into image by confocal microscope with some fluorescent material coating on the tip. Also, it is a time-consuming process considering the necessity of monitoring the wear or damage or degradation of tip by the collision with the surface during scanning the surface after each scan.\n\nThe scanning tunneling spectroscopy (STS) is spectroscopic form of STM in which spectroscopic data based on curve is obtained to analyze the existence of any oxides or impurities on the tip by monitoring the linearity of the curve which represents metallic tunnel junction. Generally, cure is non-linear and hence, the tip has a gap like shape around zero bias voltage for oxidized or impure tip whereas the opposite is observed for sharp pure un-oxidized tip.\n\nIn Auger electron spectroscopy (AES), any oxides present on the tip surface is sputtered out during in-depth analysis with argon ion beam generated by differentially pumped ion pump followed by comparing the sputtering rate of the oxide with experimental sputtering yields. These Auger measurements may estimate the nature of oxides because of the surface contamination and/or composition can be revealed and in some cases thickness of the oxide layer down to 1-3 nm can be estimated. X-ray photoelectron spectroscopy also performs similar characterization for the chemical and surface composition by providing information on the binding energy of the surface elements.\n\nOverall, the aforementioned characterization methods of tips can be categorized in three major classes. They are:\n\n\nProbes tips have a wide variety of applications in different fields of science and technology. One of the major areas where probe tips are used is for application in SPM i.e., STM and AFM. For example, carbon nanotube tips in conjunction with AFM provides an excellent tool for surface characterization in the nanometer realm. CNT tips are also used in tapping-mode Scanning Force Microscopy (SFM), which is a technique where a tip taps a surface by a cantilever driven near resonant frequency of the cantilever. The CNT probe tips fabricated using CVD technique can be used for imaging of biological macromolecules, semiconductor and chemical structure. For example, it is possible to obtain intermittent AFM contact image of IgM macromolecules with excellent resolution using single CNT tip. Individual CNT tips can be used for high resolution imaging of protein molecules.\n\nIn another work, multiwall carbon nanotube (MWCNT) and Single wall carbon nanotube (SWCNT) tips were used to image amyloid β (1-40) derived protofibrils and fibrils by tapping mode AFM. Functionalized probes can be used in Chemical Force Microscopy (CFM) to measure intermolecular forces and map chemical functionality. Functionalized SWCNT probes can be used for chemically sensitive imaging with high lateral resolution and to study binding energy in chemical and biological system. Probe tips that have been functionalized with either hydrophobic or hydrophilic molecules can be used to measure the adhesive interaction between hydrophobic-hydrophobic, hydrophobic-hydrophilic, and hydrophilic-hydrophilic molecules. From these adhesive interactions the friction image of patterned sample surface can be found. Probe tips used in force microscopy can provide imaging of structure and dynamics of adsorbate at the nanometer scale. Self-Assembled Functionalized Organic Thiols onto the surface of Au coated SiN probe tips has been used to study the interaction between molecular groups. Again, carbon nanotube probe tips in conjunction with AFM can be used for probing crevices that occur in microelectronic circuits with improved lateral resolution. Functionality modified probe tips has been to measure the binding force between single protein-ligand pairs. Probe tips has been in used in tapping mode technique to provide information about the elastic properties of materials. Probe tips are also used in mass spectrometer. Enzymatically active probe tips have been used for the enzymatic degradation of analyte. They have also been used as devices to introduce sample into the mass spectrophotometer. For example, trypsin-activated gold (Au/trypsin) probe tips can be used for the peptide mapping of the hen egg lysozyme.\n\nAtomically sharp probe tips can be used for imaging a single atom in molecule. An example of visualizing single atoms in water cluster can be seen in Fig. 10. By visualizing single atoms in molecules present on a surface, the scientists can determine bond length, bond order and discrepancies, if any, in conjugation which was supposed to be impossible by experimental work. Fig. 9 shows experimentally determined bond order in poly aromatic compound which was thought to be very hard in past.\n"}
{"id": "24006408", "url": "https://en.wikipedia.org/wiki?curid=24006408", "title": "Psychological behaviorism", "text": "Psychological behaviorism\n\nPsychological behaviorism is a form of behaviorism — a major theory within psychology which holds that generally human behaviors are learned — proposed by Arthur W. Staats. The theory is constructed to advance from basic animal learning principles to deal with all types of human behavior, including personality, culture, and human evolution. Behaviorism was first developed by John B. Watson (1912), who coined the term \"behaviorism,\" and then B. F. Skinner who developed what is known as \"radical behaviorism.\" Watson and Skinner rejected the idea that psychological data could be obtained through introspection or by an attempt to describe consciousness; all psychological data, in their view, was to be derived from the observation of outward behavior. The strategy of these behaviorists was that the animal learning principles should then be used to explain human behavior. Thus, their behaviorisms were based upon research with animals.\n\nStaats' program takes the animal learning principles, in the form in which he presents them, to be basic. But, also on the basis of his study of human behaviors, adds human learning principles. These principles are unique, not evident in any other species. Holth also critically reviews psychological behaviorism as a \"path to the grand reunification of psychology and behavior analysis\".\n\nThe preceding behaviorisms of Ivan P. Pavlov, Edward L. Thorndike, John B. Watson, B. F. Skinner, and Clark L. Hull studied the basic principles of conditioning with animals. These behaviorists were animal researchers. Their basic approach was that those basic animal principles were to be applied to the explanation of human behavior. They did not have programs for the study of human behavior broadly, and deeply. \n\nStaats was the first to do his research with human subjects. His study ranged from research on basic principles to research and theory analysis of a wide variety of human behaviors, real life human behaviors. That is why Warren Tryon (2004) suggested that Staats change the name of his approach to psychological behaviorism, because Staats behaviorism is based upon human research and unifies aspects of traditional study with his behaviorism.\nThat includes his study of the basic principles. For example, the original behaviorists treated the two types of conditioning in different ways. The most generally used way by B. F. Skinner constructively considered classical conditioning and operant conditioning to be separate and independent principles. In classical conditioning, if a piece of food is provided to a dog shortly after a buzzer is sounded, for a number of times, the buzzer will come to elicit salivation, part of an emotional response. In operant conditioning, if a piece of food is presented to a dog after the dog makes a particular motor response, the dog will come to make that motor response more frequently. \n\nFor Staats, these two types of conditioning are not separate, they interact. A piece of food elicits an emotional response. A piece of food presented after the dog has made a motor response will have the effect of strengthening that motor response so that it occurs more frequently in the future.\n\nStaats sees the piece of food to have two functions: one function is that of eliciting an emotional response, the other function is that of strengthening the motor behavior the precedes the presenting of food. So classical conditioning and operant conditioning are very much related.\n\nPositive emotion stimuli will serve as positive reinforcers. Negative emotion stimuli will serve as punishers. As a consequence of humans’ inevitable learning positive emotion stimuli will serve as positive discriminative stimuli, incentives. Negative emotion stimuli will serve as negative discriminative stimuli, disincentives. So, emotion stimuli also have reinforcing value and discriminative stimulus value. Unlike Skinner’s basic principles, emotion and classical conditioning are central causes of behavior.\n\nUnlike the other behaviorisms, Staats’ considers human learning principles. He states that humans learn complex repertoires of behavior like language, values, and athletic skills –– that is cognitive, emotional, and sensory motor repertoires. When such a repertoire has been learned, they change the individual’s learning ability. A child who has learned language, a basic repertoire, can learn to read. A person who has learned a value system, such as a system of beliefs in human freedom, can learn to value different forms of government. An individual who has learned to be a track athlete, can learn to move more quickly as a football player. This introduces a basic principle of psychological behaviorism, that human behavior is learned cumulatively. Learning one repertoire enables the individual to learn other repertoires that enable the individual to learn additional repertoires, and on and on. Cumulative learning is a unique human characteristic. It has taken humans from chipping hand axes to flying to the moon, learned repertoires that enable the learning of new repertoires that enable the learning of new repertoires in an endless fashion of achievement.\n\nThat theory development enables psychological behaviorism to deal with types of human behavior. Out of the reach of radical behaviorism, for example, personality.\n\nStaats proposes that radical behaviorism is insufficient, because in his view psychology needs to unify traditional knowledge of human behavior with behaviorism. He has called that behaviorizing psychology in a way that enables psychological behaviorism to deal with topics not usually dealt with in behaviorism, such as personality. According to this theory, personality consists of three huge and complex behavioral repertoires:\n\nThe infant begins life without the basic behavioral repertoires. They are acquired through complex learning, and as this occurs, the child becomes able to respond appropriately to various situations.\n\nWhereas at the beginning learning involves only basic conditioning, as repertories are acquired the child's learning improves, being aided by the repertoires that are already functional. The way a person experiences the world depends on his/her repertoires. The individual's environment to the present results in learning a basic behavioral repertoire (BBR). The individual's behavior is function of the life situation and the individual's BBR. The BBRs are both a dependent and an independent variable, as they result from learning and cause behavior, constituting the individual's personality. According to this theory, biological conditions of learning are essential. Biology provides the mechanisms for learning and performance of behavior. For example, a severely brain-damaged child will not learn BBRs in a normal manner.\n\nAccording to Staats, the biological organism is the mechanism by which the environment produces learning that results in basic behavioral repertoires which constitute personality. In turn, these repertoires, once acquired, are modifying the brain's biology, through the creation of new neural connections. Organic conditions affect behavior through affecting learning, basic repertoires, and sensory processes. The effect of environment on behavior can be proximal, here-and-now, or distal, through memory and personality. Thus, biology provides the mechanism, learning and environment provide the content of behavior and personality. Creative behavior is explained by novel combinations of behaviors elicited by new, complex environmental situations. The self is the individual's perception of his/her behavior, situation, and organism. Personality, situation, and the interaction between them are the three main forces explaining behavior. The world acts upon the person, but the person also acts both on the world, and on him/herself.\n\nThe methodology of psychological behavioral theory contains techniques of assessment and therapy specially designed for the three behavioral repertoires:\n\nWatson named the approach \"behaviorism\" as a form of revolution against the then prevalent use of introspection to study the mind. Introspection was subjective and variable, not a source of objective evidence, and the mind consisted of an inferred entity that could never be observed. He insisted psychology had to be based on objective observation of behavior and the objective observation of the environmental events that cause behavior. Skinner’s radical behaviorism also has not established a systematic relationship to traditional psychology knowledge.\n\nPsychological behaviorism—while bolstering Watson’s rejection of inferring the existence of internal entities such as mind, personality, maturation stages, and free will—considers important knowledge produced by non-behavioral psychology that can be objectified by analysis in learning-behavioral terms. As one example, the concept of intelligence is inferred, not observed, and thus intelligence and intelligence tests are not considered systematically in behaviorism. However, PB considers IQ tests measure important behaviors that predict later school performance and intelligence is composed of learned repertoires of such behaviors. Joining the knowledge of behaviorism and intelligence testing yields concepts and research concerning what intelligence is behaviorally, what causes intelligence, as well as how intelligence can be increased. It is thus a behaviorism that systematically incorporates and explains, behaviorally, empirical parts of psychology.\n\nThe different behaviourisms also differ with respect to basic principles. Skinner contributed greatly in separating Pavlov’s classical conditioning of emotion responses and operant conditioning of motor behaviors. Staats, however, notes that food was used by Pavlov to elicit a positive emotional response in his classical conditioning and Thorndike Edward Thorndike used food as the reward (reinforcer) that strengthened a motor response in what came to be called operant conditioning, thus emotion-eliciting stimuli are also reinforcing stimuli. Watson, although the father of behaviorism, did not develop and research a basic theory of the principles of conditioning. The behaviorists whose work centered on that development treated differently the relationship of the two types of conditioning. Skinner’s basic theory was advanced in recognizing two different types of conditioning, but he didn’t recognize their interrelatedness, or the importance of classical conditioning, both very central for explaining human behavior and human nature.\n\nStaats’ basic theory specifies the two types of conditioning and the principles of their relationship. Since Pavlov used a food stimulus to elicit an emotional response and Thorndike used food as a reward (reinforcer) to strengthen a particular motor response, whenever food is used both types of conditioning thus take place. That means that food both elicits a positive emotion and food will serve as a positive reinforcer (reward). It also means that any stimulus that is paired with food will come to have those two functions. Psychological behaviorism and Skinner’s behaviorism both consider operant conditioning a central explanation of human behavior, but PB additionally concerns emotion and classical conditioning.\n\nThis difference between the two behaviorisms can be seen clearly in their theories of language. Staats, extending prior theory indicates that a large number of words elicit either a positive or negative emotional response because of prior classical conditioning. As such they should transfer their emotional response to anything with which they are paired. PB provides evidence this is the case. PB’s basic learning theory also states that emotional words have two additional functions. They will serve as rewards and punishments in learning other behaviors, and they also serve to elicit either approach or avoidance behavior. Thus, (1) hearing that people of an ethnic group are dishonest will condition a negative emotion to the name of that group as well as to members of that group, (2) complimenting (saying positive emotional words to) a person for a performance will increase the likelihood the person will perform that action later on, and (3) seeing the sign RESTAURANT will elicit a positive emotion in a hungry driver and thus instigate turning into the restaurant’s parking lot. Each case depends upon words eliciting an emotional response.\n\nPB treats various aspects of language, from its original development in children to its role in intelligence and in abnormal behavior, and backs this up with basic and applied study. His theory paper in the journal Behavior Therapy helped introduce cognitive (language) behavior therapy to the behavioral field.\n\nMuch of the research on which PB is based has concerned children’s learning. For example, there is a series of studies of the first learning of reading with preschoolers and also a series studying and training dyslexic adolescent children. The psychological behaviorism (PB) position became that the norms of child development—the ages when important behaviors appear—are due to learning, not biological maturation.\n\nStaats began studies to analyze cases of important human behaviors in basic and applied ways in 1954. In 1958 he analyzed dyslexia and introduced his token reinforcer system (later called the token economy) along with his teaching method and materials for treating the disorder. When his daughter Jenny was born in 1960 he began to study and to produce her language, emotional, and sensory-motor development. When she was a year and a half old he began teaching her number concepts, and then reading six months later, using his token reinforcer system, as he recorded on audiotape. Films were made in 1966 of Staats being interviewed about his conception of how variations in children’s home learning variously prepared them for school on the first of three Arthur Staats YouTube videos. Following that the second Staats YouTube video records him beginning teaching his three-year-old son with the reading learning (and counting) method he developed in 1962 with his daughter. This film also shows a graduate assistant working with a culturally deprived four-year-old learning reading and writing numbers and counting, participating voluntarily. The Staats YouTube video number 3 has additional cases of these usually delayed children voluntarily learning much ahead of time these cognitive repertoires that prepare them for school. This group of 11 children gained an average of 11 points in IQ and advanced significantly on a child development measure as they also learned to like the learning situation. Staats published the first study in this series in 1962 and describes his later studies and his more general conception in his 1963 book. This research, that included work with his own children from birth on, was the basis for Staats’ books specifying the importance of the parents' early training of the child in language and other cognitive repertoires. He shows they are the foundations for being intelligent and doing well on entering school. There are new studies showing that parents who talk to their children more have children with advanced language development, school success, and intelligence measures. These statistical studies should be joined with Staats’ work with individual children that shows the specifics of the learning involved and how to best produce it. The two together show powerfully the importance of early child learning.\n\nStaats also applied his approach in fathering his own children and employed his findings in constructing conception of human behavior and human nature. He deals with many aspects of child development, from babbling to walking to discipline and time-out, and he considers parents one of his audiences. In the last of his books he summarizes his theory of child development. His position is that children are the young of the human species that has a body that can make an infinity of different behaviors. The human species also has a nervous system and brain of 100 billion neurons that can learn in marvelous complexity. The child’s development consists of the learning of repertoires, extraordinarily complex, like a language-cognitive repertoire, an emotional-motivational repertoire, and a sensory-motor repertoire, each including sub-repertoires of various kinds. The child’s behavior, in the various life situations encountered, depend upon the repertoires that have been learned. The child’s ability to learn in the variety of situations encountered also depends on the repertoires that have been learned. This conception makes parenting central in the child's development, supported by many studies in behavior analysis, and offers knowledge to parents in raising their children.\n\nStaats describes humans great variability in behavior, across different people. Those individual differences are consistent in different life situations and typify people. Those differences also tend to run in families. Such phenomena have led to the concept of personality as some internal trait that is inherited that strongly determines individuals’ characteristic ways of behaving. Personality conceived in that way remains an inference, based on how people behave, but with no evidence of what personality is.\n\nMore successful has been the measurement of personality. There are tests of intelligence for example. No internal organ of intelligence has been found, and no genes either. But intelligence tests have been constructed that predict (helpfully but not perfectly) the performance of children in school. Children who have the behaviors measured on the tests display better learning behaviors in the classroom. Although such tests have been widely applied radical behaviorism has not invested in the study of personality or personality testing.\n\nPsychological behaviorism (e.g.) however considers it important to study what personality is, how personality determines behavior, what causes personality, as well as what personality tests measure. Tests (including intelligence tests) are considered to measure different repertoires of behavior that individuals have learned. The individual in life situations also displays behaviors that have been learned. That is why personality tests can predict how people will behave. That means also that tests can be used to identify important human behaviors, and the learning that produces those behaviors can be studied. Gaining that knowledge will make it possible to develop environmental experiences that produce or prevent types of personality from developing. A study has shown, for example, that in learning to write letters of the alphabet children learn repertoires that make them more intelligent.\n\nPsychological behaviorism’s theory of abnormal personality rejects the concept of mental illness. Rather behavior disorders are composed of learned repertoires of abnormal behavior. Behavior disorders also involve not having learned basic repertoires that are needed in adjusting to life's demands. Severe autism can involve not having learned a language repertoire as well as having learned tantrums and other abnormal repertoires.\n\nPB’s theories of various behavior disorders employ the Diagnostic and Statistical Manual of Mental Disorders (DSM) descriptions of both abnormal repertoires and the absence of normal repertoires. Psychological behaviorism provides the framework for an approach to clinical treatment of behavior disorders, as shown in the field of behavior analysis. PB theory also indicates how behavior disorders can be prevented by preventing the abnormal learning conditions that produce them.\n\nThe PB theory is that child development, besides its physical growth, consists of the learning of repertoires some of which are basic in the sense they provide the behaviors for many life situations and also they determine what and how well the individual can learn. That theory states that humans are unique in having a building type of learning, cumulative learning, in which basic repertoires enable the child to learn other repertoires that enable the learning of other repertoires. Learning language, for example, enables the child to learn various other repertoires, like reading, number concepts, and grammar. Those repertoires provide the bases for learning other repertoires. For example, reading ability, opens the possibilities for an individual to do things and learn things that a non-reader cannot.\n\nWith that theory, and with its empirical methodology, PB applies to education. For example, it has a theory of reading that explains children’s differences, from dyslexia to advanced reading ability. PB also suggests how to treat dyslexic children and those with other learning disabilities. Psychological behaviorism's approach has been supported and advanced in the field of behavior analysis.\n\nHuman origin is generally explained by Darwin’s natural selection; However, while Darwin gathered imposing evidence showing the evolution of physical characteristics of species his view that behavioral characteristics (such as human intelligence) also evolved was pure assumption with no evidentiary support PB presents a different theory, that the cumulative learning of pre-human hominins drove human evolution. That explains the consistent increase in brain size over the course of human evolution. That occurred because the members of the evolving hominin species were continually learning new language, emotion-motivation, and sensory-motor repertoires. That meant the new generations had to learn those ever more complex repertoires. It was cumulative learning that consistently created the selection device for the members of those generations that had the larger brains and were the better learners.\n\nThat theory makes learning ability central in human origin, selecting who would survive and reproduce, until the advent of Homo sapiens where all individuals (except if damaged) have full brains and full learning ability.\n\nPsychological behaviorism is set forth as an overarching theory, constructed of multiple theories in various areas. Staats considers it a unified theory. The areas are related, their principles consistent, and they are advanced consistently, composing levels from basic to increasingly advanced. Its most basic level calls for a systematic study of the biology of the learning “organs” and their evolutionary development, from species like amoeba that have no learning ability to humans that have the most. The basic learning principles constitute another level of theory, as do the human learning principles that specify cumulative learning. How the principles work—in areas like child development, personality, abnormal personality, clinical treatment, education, and human evolution—compose additional levels of study. Staats sees the overarching theory of PB as basic for additional levels that compose the social sciences of sociology, linguistics, political science, anthrology, and paleoanthropology. He criticizes the disunification of the sciences that study human behavior and human nature. Because they are disconnected, they do not build a related, simpler and more understandable conception and scientific endeavor as, for example, the biological sciences do. This philosophy of science of unification is at one with Staats’ attempt to construct his unified psychological behaviorism.\n\nPsychological behaviorism’s works project new basic and applied science at its various theory levels. The basic principles level, as one example, needs to study systematically the relationship of the classical conditioning of emotional responses and the operant conditioning of motor responses. As another projection, the field of child development should focus on the study of the learning of the basic repertoires. One essential is the systematic detailed study of the learning experiences of children in the home from birth on. He says such research could be accomplished by installing cameras in the homes of volunteering, remunerated families. This research should also be done to discover how such learning produces both normal and abnormal personality development. As another example, PB also calls for educational research into how school learning could be advanced using its methods and theories. Also, Staats' theory of human evolution is seen to call for research and theory developments.\n\n"}
{"id": "1532331", "url": "https://en.wikipedia.org/wiki?curid=1532331", "title": "Ramsden surveying instruments", "text": "Ramsden surveying instruments\n\nThe Ramsden surveying instruments are those constructed by\nJesse Ramsden and used in high precision geodetic surveys carried out in the period 1784 to 1853. This includes the five great theodolites—great in name, great in size and great in accuracy—used in surveys of Britain and other parts of the world. Ramsden also provided the equipment used in the measurement of the many base lines of these surveys and also the zenith telescope used in latitude determinations.\n\nA total of eight such instruments were manufactured by Ramsden and others for use in Britain, India and Switzerland.\nRamsden himself constructed three theodolites and a further two were completed to his design by Mathew Berge, his son-in-law and business successor, after Ramsden's death in 1805. Of the other instruments one was constructed by William Cary and the other two by the firm of Troughton and Simms.\n\nIn 1783 the Royal Society of London reacted to French criticism of Greenwich Observatory by seeking Royal assent to undertake a high precision geodetic survey, the Anglo-French Survey (1784–1790), between Greenwich and the established French survey stations on the other side of the English Channel. Approval having been granted, General William Roy agreed to undertake the work and he immediately approached Ramsden to commission new instruments. Three years later the \"great\" theodolite was delivered after a delay attributable to Ramsden's tardiness, workshop accidents and his prediliction for continuous refinement—\"this won't do, we must have at it again\". The instrument was paid for by the Crown and the King immediately presented it to the Royal Society; for this reason the theodolite is designated as the Royal Society theodelite, or Ramsden RS in short.\n\nThere is a complete description of this theodolite in the final report of the Anglo-French Survey (1784–1790). The instrument was large, 36 inches (914 mm) across and it was normally mounted on a stand which placed the sighting telescope between 5 and 6 ft. high. It weighed about 200 lb (90 kg) and the accessories and cases weighed as much again. It travelled around Britain for over sixty years, in its own sprung carriage, to locations where it was hauled up mountains, church towers and even scaffolded steeples. \n\nThe horizontal circular scale was divided very accurately with divisions at 15 minute (of arc) intervals using one of Ramden's own dividing engines; the marks on the 36 inch diameter scale would be about 1/6 inch (4 mm) apart. The position of the telescope could therefore be read to the nearest quarter of a degree by eye but the exact position between the divisions was read with the aid of micrometer microscopes fitted with adjustable cross wires in the focal plane, as shown. The threads of the screws were such that fifteen full turns moved from one scale mark to the next, \"i.e.\" 15 minutes, and since the scale on the adjusting knob allowed one sixtieth of a turn to be measured the resulting accuracy was within one arc second.\n\nThe instrument is also fitted with a vertical semi-circular scale to measure the elevations of distant stations and therefore a height difference. Cross wires similar to those used in the microscopes are fitted into the eyepiece; they are adjustable by a screw thread which allowed angles to be measured to within five arc seconds.\n\nTypical distances in the Anglo-French survey were less than 20 miles (32 km): at that distance one second of arc corresponds to lateral or vertical displacements at the target station of approximately 7 inches (17 cm). No other theodolite could match this precision at that time. It was the first instrument to be able to measure the spherical excess of large survey triangles.\n\nAfter completion of the Anglo-French survey this instrument was stored at the Royal Society but in 1799 the Board of Ordnance requested its use for the Principal Triangulation of Great Britain. On completion of the Survey the theodolite was stored in the headquarters of the Ordnance Survey at Southampton where it was destroyed in the bombing raids of 1941.\n\n In his report to the Royal Society in 1775 William Roy had noted the suitability of India as a location for both meridian arc and parallel arc measurements. To his delight the East India Company were willing to undertake such a venture and ordered a second great theodolite from Ramsden. It was ready in 1791 but Ramsden felt obliged to increase the price because of problems in its manufacture. To his surprise the company rejected his price and refused to purchase the instrument. It was bought by the Duke of Richmond who, as Master of the Board of Ordnance, had provided most of the finance for Roy's Anglo-French Survey (1784–1790). The theodolite is designated as the Board of Ordnance theodolite, or Ramsden BO in short. Richmond's intention was to use the new theodolite on the extension of Roy's survey to the southern counties of Britain. The instrument was basically the same as the first with added refinements, mainly to the number and placement of the microscopes with their precision micrometer stages. It was in use until the completion of the Principal Triangulation of Great Britain in 1853 and it is now in the Science Museum in London.\n\nThere is a description of the improvements made to this theodolite in the account of the Trigonometrical Survey for the years 1791–1794 by Mudge, Williams and Dalby.\n\nRamsden made at least one other 3ft. theodolite of which parts were discovered in Switzerland. After his death his firm was inherited by Mathew Berge who is known to have constructed two more large instruments to Ramsden's design. Ramsden made many theodolites including an eighteen inch instrument of the same general design as the great theodolites. This instrument and his two great theodolites are described in the account of the Principal Triangulation by .\n\nAlthough the East India company turned down Ramsden's second theodolite they commissioned a similar design from another London instrument maker, William Cary. This theodolite was in use in India from 1802 although there was a slight hiatus in 1808 when it was damaged when being hauled to the top of a building. It was repaired and in use for sixty years. A new theodolite to an improved design was made for the Indian Survey by Troughton and Sims in 1830 and the two together saw service until 1874 when the Cary instrument was replaced by another by Troughton and Sims. This last instrument was a monster weighing 1455 lbs when in its travelling cases: it was no surprise that it was deemed too heavy for transport up mountains and it passed to South Africa in 1882. No more great instruments were made after 1874 for continuing advances in precision instruments eclipsed their performance: by the end of the nineteenth century an accuracy of 1 arc second could be obtained with a 12 inch instrument.\n\nEighteenth-century surveyors used Gunter's chains which were 22 yards long (one chain with 100 links of 7.92 inches). Their accuracy was adequate for cadastral surveying but they were deemed insufficiently accurate for the Anglo-French Survey (1784–1790), Britain's first high-precision survey. Roy asked Ramsden to prepare a new chain of 100 links, each one foot in length. He also asked Ramsden to prepare three precisely-calibrated wooden rods 20 feet long. These proved to be a failure because of fluctuations in length due to varying humidity; instead three calibrated glass tubes were used. The chain proved to be as accurate as the glass tubes, and it was in use for baseline surveys over the next thirty years. Later, even more precise measurements showed that the accuracy of Ramsden's 100-foot (30.48 m) chain was within about 3 inches in 5 miles. In actual use the chain was supported throughout its length by wooden trestles, and tensioned with a known constant weight. Its coefficient of thermal expansion was carefully measured so that temperature fluctuations could be taken into account. Full details (with plates) are given in Roy's account of the measurement of the Hounslow Heath baseline.\n\nAmerican surveyors sometimes also used a chain of 100 feet, also with 100 links, known as the \"engineer's chain\". The term \"chain\" in both cases usually refers to the measuring instrument rather than a unit of length, and distances measured are normally given in feet and decimal fractions of a foot (not inches). \n\nDespite Ramsden's chain originating in the UK, Gunter's predominated there: \"When a chain is spoken of without qualification, Gunter's chain is meant\", noted Macquorn Rankine's mid-Victorian \"A Manual of Civil Engineering\".\n\nThe zenith telescope constructed by Jesse Ramsden in 1802 which was used to determine the latitude of many stations of the Principal Triangulation of Great Britain. This portable instrument was designed to bring observatory precision to fieldwork. The outer frame stood about 12 ft. high and the telescope mounted on an inner frame was 8 ft. long. The telescope was restricted to observations within a few degrees of the zenith in order to prevent errors due to refraction. A complete description of the instrument is given by Pearson.\n\n\n"}
{"id": "24714755", "url": "https://en.wikipedia.org/wiki?curid=24714755", "title": "Research Diagnostic Criteria", "text": "Research Diagnostic Criteria\n\nThe Research Diagnostic Criteria (RDC) are a collection of influential psychiatric diagnostic criteria published in late 1970s. As psychiatric diagnoses widely varied especially between the USA and Europe, the purpose of the criteria was to allow diagnoses to be consistent in psychiatric research. \n\nSome of the criteria were based on the earlier Feighner Criteria, although many new disorders were included; \"The historical record shows that the small group of individuals who created the Feighner criteria instigated a paradigm shift that has had profound effects on the course of American and, ultimately, world psychiatry.\"\n\nThe RDC is important in the history of psychiatric diagnostic criteria as the DSM-III was based on many of the RDC descriptions.\n\n"}
{"id": "45671841", "url": "https://en.wikipedia.org/wiki?curid=45671841", "title": "Riccardo Levi-Setti", "text": "Riccardo Levi-Setti\n\nRiccardo Levi-Setti (July 11, 1927 – November 8, 2018) was a Milanese-born emeritus professor of physics and paleontology at the University of Chicago. Prior to joining the University of Chicago in 1956, he worked at the Enrico Fermi Institute, of which he became director in 1992. In 2011 he, along with Roger Hildebrand, participated in the ceremony of the opening of the Enrico Fermi time capsule. \n\nIn 1975 he wrote a book called \"Trilobites\", which was reissued in 1993. In 2014 his third book, called \"The Trilobite Book: A Visual Journey\", was published.\n"}
{"id": "21567720", "url": "https://en.wikipedia.org/wiki?curid=21567720", "title": "Shadow life", "text": "Shadow life\n\nShadow life is a hypothesis proposed by cosmologist Paul Davies, chair of the SETI: Post-Detection Science and Technology Taskgroup of the International Academy of Astronautics. The theory suggests that if life has evolved on Earth more than once, microorganisms may exist on Earth which have no evolutionary connection with any other known form of life. He thinks that if scientists discover an alternate form of microbial life on Earth, the odds are good that life is also common elsewhere in the universe. He suggests that possible indicators could be alternate biochemistries such as right-handed amino acids, or a different genetic code, or even another kind of chemical for its genetic material that are not nucleic acids (DNA nor RNA) chains or biopolymers. The hypothesis speculates that the descendants of any \"second genesis\" may have survived until today in a shadow biosphere.\n\n"}
{"id": "9261359", "url": "https://en.wikipedia.org/wiki?curid=9261359", "title": "Species problem", "text": "Species problem\n\nThe species problem is the set of questions that arises when biologists attempt to define what a species is. Such a definition is called a species concept; there are at least 26 recognized species concepts. A species concept that works well for sexually reproducing organisms such as birds is useless for species that reproduce asexually, such as bacteria. The scientific study of the species problem has been called microtaxonomy.\n\nOne common, but sometimes difficult, question is how best to decide which species an organism belongs to, because reproductively isolated groups may not be readily recognizable, and cryptic species may be present. There is a continuum from \"reproductive isolation\" with no interbreeding, to \"panmixis\", unlimited interbreeding. Populations can move forward or backwards along this continuum, at any point meeting the criteria for one or another species concept, and failing others.\n\nMany of the debates on species touch on philosophical issues, such as nominalism and realism, and on issues of language and cognition.\n\nThe current meaning of the phrase \"species problem\" is quite different from what Charles Darwin and others meant by it during the 19th and early 20th centuries. For Darwin, the species problem was the question of how new species arose. Darwin was however one of the first people to question how well-defined species are, given that they constantly change.\n\nThe idea that one organism reproduces by giving birth to a similar organism, or producing seeds that grow to a similar organism, goes back to the earliest days of farming. While people tended to think of this as a relatively stable process, many thought that change was possible. The term \"species\" was just used as a term for a sort or kind of organism, until in 1686 John Ray introduced the biological concept that species were distinguished by always producing the same species, and this was fixed and permanent, though considerable variation was possible within a species. Carolus Linnaeus (1707–1778) formalized the taxonomic rank of species, and devised the two part naming system of binomial nomenclature that we use today. However, this did not prevent disagreements on the best way to identify species.\n\nThe history of definitions of the term \"species\" reveal that the seeds of the modern species debate were alive and growing long before Darwin.\n\n\"The traditional view, which was developed by Cain, Mayr and Hull in the mid-twentieth century, claims that until the ‘Origin of species’ by Charles Darwin both philosophy and biology considered species as invariable natural kinds with essential features. This ‘essentialism story’ was adopted by many authors, but questioned from the beginning by a minority … when Aristotle and the early naturalists wrote about the essences of species, they meant essential ‘functions’, not essential ‘properties’. Richards pointed out [Richard A. Richards, The Species Problem: A Philosophical Analysis, Cambridge University Press, 2010] that Linnaeus saw species as eternally fixed in his very first publication from 1735, but only a few years later he discovered hybridization as a modus for speciation.\n\nCharles Darwin's famous book \"On the Origin of Species\" (1859) offered an explanation as to how species evolve, given enough time. Although Darwin did not provide details on how species can split into two, he viewed speciation as a gradual process. If Darwin was correct, then, when new \"incipient species\" are forming, there must be a period of time when they are not yet distinct enough to be recognized as species. Darwin's theory suggested that there was often not going to be an objective fact of the matter, on whether there were one or two species.\n\nDarwin's book triggered a crisis of uncertainty for some biologists over the objectivity of species, and some came to wonder whether individual species could be objectively real — i.e. have an existence that is independent of the human observer.\n\nIn the 1920s and 1930s, Mendel's theory of inheritance and Darwin's theory of evolution by natural selection were joined in what was called the modern synthesis. This conjunction of theories also had a large impact on how biologists think about species. Edward Poulton anticipated many ideas on species that today are well accepted, and that were later more fully developed by Theodosius Dobzhansky and Ernst Mayr, two of the architects of the modern synthesis. Dobzhansky's 1937 book articulated the genetic processes that occur when incipient species are beginning to diverge. In particular, Dobzhansky described the critical role, for the formation of new species, of the evolution of reproductive isolation.\n\nErnst Mayr's 1942 book was a turning point for the species problem. In it, he wrote about how different investigators approach species identification, and he characterized their approaches as species concepts. He argued for what came to be called the \"Biological Species Concept\" (BSC), that a species consists of populations of organisms that can reproduce with one another and that are reproductively isolated from other populations, though he was not the first to define \"species\" on the basis of reproductive compatibility. For example, Mayr discusses how Buffon proposed this kind of definition of \"species\" in 1753.\nTheodosius Dobzhansky was a contemporary of Mayr and the author of a classic book about the evolutionary origins of reproductive barriers between species, published a few years before Mayr's. Many biologists credit Dobzhansky and Mayr jointly for emphasizing reproductive isolation.\n\nAfter Mayr's book, some two dozen species concepts were introduced. Some, such as the Phylogenetic Species Concept (PSC), were designed to be more useful than the BSC for describing species. Many authors have professed to \"solve\" or \"dissolve\" the species problem. Some have argued that the species problem is too multidimensional to be \"solved\" by any one concept. Since the 1990s, others have argued that concepts intended to help describe species have not helped to resolve the species problem. Although Mayr promoted the BSC for use in systematics, some systematists have criticized it as not operational. For others, the BSC is the preferred definition of species. Many geneticists who work on speciation prefer the BSC because it emphasizes the role of reproductive isolation. It has been argued that the BSC is a natural consequence of the effect of sexual reproduction on the dynamics of natural selection.\n\nRealism, in the context of the species problem, is the philosophical position that species are real mind-independent entities, natural kinds. Mayr, a proponent of realism, attempted to demonstrate species exist as natural, extra-mental categories. He showed for example that the New Guinean tribesman classify 136 species of birds, which Western ornithologists came to independently recognize:\n\nMayr's argument however has been criticized:\n\nAnother position of realism is that natural kinds are demarcated by the world itself by having a unique property that is shared by all the members of a species, and none outside the group. In other words, a natural kind possesses an essential or intrinsic feature (“essence”) that is self-individuating and non-arbitrary. This notion has been heavily criticized as essentialist, but modern realists have argued that while biological natural kinds have essences, these need not be fixed and are prone to change through speciation. According to Mayr (1957) reproductive isolation or interbreeding \"supplies an objective yardstick, a completely non-arbitrary criterion” and \"describing a presence or absence relationship makes this species concept non-arbitrary\". The BSC defines species as \"groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups\". From this perspective, each species is based on a property (reproductive isolation) that is shared by all the organisms in the species that objectively distinguishes them.\n\nSome philosophical variants of nominalism propose that species are just names that people have assigned to groups of creatures but where the lines between species get drawn does not reflect any fundamental underlying biological cut-off point. In this view, the kinds of things that people have given names to, do not reflect any underlying reality. It then follows that species do not exist outside the mind, because species are just named abstractions. If species are not real, then it would not be sensible to talk about \"the origin of a species\" or the \"evolution of a species\". As recently at least as the 1950s, some authors adopted this view and wrote of species as not being real.\n\nA counterpoint to the nominalist views in regard to species, was raised by Michael Ghiselin who argued that an individual species is not a type, but rather an actual individual, an actual entity. This idea comes from thinking of a species as an evolving dynamic population. If viewed as an entity, a species would exist regardless of whether or not people have observed it and whether or not it has been given a name.\n\nA popular alternative view, pragmatism, espoused by philosophers such as Philip Kitcher and John Dupre states while species do not exist in the sense of natural kinds, they are \"conceptually\" real and exist for convenience and for practical applications. For example, regardless of which definition of species one uses, one can still quantitatively compare species diversity across regions or decades, as long as the definition is held constant within a study. This has practical importance in advancing biodiversity science and environmental science.\n\nThe nominalist critique of the view that kinds of things exist, raises for consideration the role that humans play in the species problem. For example, Haldane suggested that species are just mental abstractions.\n\nSeveral authors have noted the similarity between \"species\", as a word of ambiguous meaning, and points made by Wittgenstein on family resemblance concepts and the indeterminacy of language.\n\nJody Hey described the species problem as a result of two conflicting motivations by biologists:\n\nUnder the first view, species appear to us as typical natural kinds, but when biologists turn to understand species evolutionarily they are revealed as changeable and without sharp boundaries. Hey argued that it is unrealistic to expect that one definition of \"species\" is going to serve the need for categorization and still reflect the changeable realities of evolving species.\n\nMany approaches to the species problem have attempted to develop one single common conception of what species are and of how they should be identified. It is thought that, if such a monistic description of species could be developed and agreed upon, then the species problem would be solved. In contrast, some authors have argued for pluralism, claiming that biologists cannot have just one shared concept of species, and that they should accept multiple, seemingly incompatible ideas about species. David Hull however argued that pluralist proposals were unlikely to actually solve the species problem.\n\n\"No term is more difficult to define than \"species,\" and on no point are zoologists more divided than as to what should be understood by this word.\" Nicholson (1872).\n\n\"Of late, the futility of attempts to find a universally valid criterion for distinguishing species has come to be fairly generally, if reluctantly, recognized\" Dobzhansky (1937).\n\n\"The concept of a species is a concession to our linguistic habits and neurological mechanisms\" Haldane (1956).\n\n\"The species problem is the long-standing failure of biologists to agree on how we should identify species and how we should define the word 'species'.\" Hey (2001).\n\n\"First, the species problem is not primarily an empirical one, but it is rather fraught with philosophical questions that require — but cannot be settled by — empirical evidence.\" Pigliucci (2003).\n\n\"An important aspect of any species definition whether in neontology or palaeontology is that any statement that particular individuals (or fragmentary specimens) belong to a certain species is an hypothesis (not a fact)\" Bonde (1977).\n\n\"We show that although discrete phenotypic clusters exist in most <nowiki>[plant]</nowiki> genera (> 80%), the correspondence of taxonomic species to these clusters is poor (< 60%) and no different between plants and animals. ... Contrary to conventional wisdom, plant species are more likely than animal species to represent reproductively independent lineages.\" Rieseberg et al. (2006).\n\n\n"}
{"id": "27579", "url": "https://en.wikipedia.org/wiki?curid=27579", "title": "Statistical theory", "text": "Statistical theory\n\nThe theory of statistics provides a basis for the whole range of techniques, in both study design and data analysis, that are used within applications of statistics. The theory covers approaches to statistical-decision problems and to statistical inference, and the actions and deductions that satisfy the basic principles stated for these different approaches. Within a given approach, statistical theory gives ways of comparing statistical procedures; it can find a best possible procedure within a given context for given statistical problems, or can provide guidance on the choice between alternative procedures.\n\nApart from philosophical considerations about how to make statistical inferences and decisions, much of statistical theory consists of mathematical statistics, and is closely linked to probability theory, to utility theory, and to optimization.\n\nStatistical theory provides an underlying rationale and provides a consistent basis for the choice of methodology used in applied statistics.\n\nStatistical models describe the sources of data and can have different types of formulation corresponding to these sources and to the problem being studied. Such problems can be of various kinds:\nStatistical models, once specified, can be tested to see whether they provide useful inferences for new data sets. Testing a hypothesis using the data that was used to specify the model is a fallacy, according to the natural science of Bacon and the scientific method of Peirce.\n\nStatistical theory provides a guide to comparing methods of data collection, where the problem is to generate informative data using optimization and randomization while measuring and controlling for observational error. Optimization of data collection reduces the cost of data while satisfying statistical goals, while randomization allows reliable inferences. Statistical theory provides a basis for good data collection and the structuring of investigations in the topics of:\n\nThe task of summarising statistical data in conventional forms (also known as descriptive statistics) is considered in theoretical statistics as a problem of defining what aspects of statistical samples need to be described and how well they can be described from a typically limited sample of data. Thus the problems theoretical statistics considers include:\n\nBesides the philosophy underlying statistical inference, statistical theory has the task of considering the types of questions that data analysts might want to ask about the problems they are studying and of providing data analytic techniques for answering them. Some of these tasks are:\nWhen a statistical procedure has been specified in the study protocol, then statistical theory provides well-defined probability statements for the method when applied to all populations that could have arisen from the randomization used to generate the data. This provides an objective way of estimating parameters, estimating confidence intervals, testing hypotheses, and selecting the best. Even for observational data, statistical theory provides a way of calculating a value that can be used to interpret a sample of data from a population, it can provide a means of indicating how well that value is determined by the sample, and thus a means of saying corresponding values derived for different populations are as different as they might seem; however, the reliability of inferences from post-hoc observational data is often worse than for planned randomized generation of data.\n\nStatistical theory provides the basis for a number of data analytic methods that are common across scientific and social research. Some of these are: \nInterpreting data is an important objective of statistical research:\nMany of the standard methods for these tasks rely on certain statistical assumptions (made in the derivation of the methodology) actually holding in practice. Statistical theory studies the consequences of departures from these assumptions. In addition it provides a range of robust statistical techniques that are less dependent on assumptions, and it provides methods checking whether particular assumptions are reasonable for a give data-set.\n\n\n\n"}
{"id": "24365117", "url": "https://en.wikipedia.org/wiki?curid=24365117", "title": "Thermodynamic square", "text": "Thermodynamic square\n\nThe thermodynamic square (also known as the thermodynamic wheel, Guggenheim scheme or Born square) is a mnemonic diagram attributed to Max Born and used to help determine thermodynamic relations. Born presented the thermodynamic square in a 1929 lecture. The symmetry of thermodynamics appears in a paper by F.O. Koenig. The corners represent common conjugate variables while the sides represent thermodynamic potentials. The placement and relation among the variables serves as a key to recall the relations they constitute.\n\nA mnemonic used by students to remember the Maxwell relations (in thermodynamics) is \"Good Physicists Have Studied Under Very Fine Teachers\", which helps them remember the order of the variables in the square, in clockwise direction. Another mnemonic used here is \"Valid Facts and Theoretical Understanding Generate Solutions to Hard Problems\", which gives the letter in the normal left-to-right writing direction. Both times A has to be identified with F, another common symbol for Helmholtz' Free Energy. To prevent the need for this switch the following mnemonic is also widely used:\"Good Physicists Have Studied Under Very Ambitious Teachers\"; another amusing one is Good Physicists Have SUVAT, in reference to the equations of motion. One other useful variation of the mnemonic when the symbol E is used for internal energy instead of U is the following: \"Some Hard Problems Go To Finish Very Easy\".\n\nThe thermodynamic square is mostly used to compute the derivative of any thermodynamic potential of interest. Suppose for example one desires to compute the derivative of the internal energy formula_1. The following procedure should be considered:\nThe Gibbs–Duhem equation can be derived by using this technique. Notice though that the final addition of the differential of the chemical potential has to be generalized.\n\nThe thermodynamic square can also be used to find the Maxwell relations. Looking at the four corners of the square and making a formula_16 shape, one can find \nformula_17.\nBy rotating the formula_16 shape (randomly, for example by 90 degrees counterclockwise into a formula_19 shape) other relations such as:\nformula_20\ncan be found.\n\nFinally, the potential at the center of each side is a natural function of the variables at the corner of that side. So, G is a natural function of p and T, and U is a natural function of S and V.\n\n"}
{"id": "27805701", "url": "https://en.wikipedia.org/wiki?curid=27805701", "title": "Zooniverse", "text": "Zooniverse\n\nZooniverse is a citizen science web portal owned and operated by the Citizen Science Alliance. It is home to some of the internet's largest, most popular and most successful citizen science projects. The organization grew from the original Galaxy Zoo project and now hosts dozens of projects which allow volunteers to participate in crowdsourced scientific research. It has headquarters at Oxford University and the Adler Planetarium. Unlike many early internet-based citizen science projects (such as SETI@home) which used spare computer processing power to analyse data, known as volunteer computing, Zooniverse projects require the active participation of human volunteers to complete research tasks. Projects have been drawn from disciplines including astronomy, ecology, cell biology, humanities, and climate science.\n\n, the Zooniverse community consisted of more than 1 million registered volunteers. The volunteers are often collectively referred to as \"Zooites\". The data collected from the various projects has led to the publication of more than 100 scientific papers. A daily news website called 'The Daily Zooniverse' provides information on the different projects under the Zooniverse umbrella, and has a presence on social media.\n\nThe Zooniverse is hosted by the Citizen Science Alliance, which is governed by a board of directors from seven institutions in the United Kingdom and the United States. The partners are the Adler Planetarium, Johns Hopkins University, University of Minnesota, National Maritime Museum, University of Nottingham, Oxford University and Vizzuality.\n\n"}
