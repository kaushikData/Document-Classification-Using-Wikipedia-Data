{"id": "4674162", "url": "https://en.wikipedia.org/wiki?curid=4674162", "title": "Addams (crater)", "text": "Addams (crater)\n\nAddams is a crater on Venus. It was named after Jane Addams.\n"}
{"id": "2904678", "url": "https://en.wikipedia.org/wiki?curid=2904678", "title": "Ammoniacum", "text": "Ammoniacum\n\nAmmoniacum, or gum ammoniac, is a gum-resin exuded from the stem of the perennial herb \"Dorema ammoniacum\" of the umbel family (Apiaceae). The plant grows to the height of 2½ or 3 meters (8 or 9 ft.) and its whole stem is pervaded with a milky juice, which oozes out on an incision being made at any part. This juice quickly hardens into round tears, forming the \"tear ammoniacum\" of commerce. \"Lump ammoniacum\", the other form of the substance, consists of aggregations of tears, frequently incorporating fragments of the plant itself, as well as other foreign bodies.\n\nAmmoniacum has a faintly fetid, unpleasant odor, which becomes more distinct on heating; externally, it possesses a reddish-yellow appearance, and when the tears or lumps are freshly fractured they exhibit a waxy luster. It is chiefly collected in central Persia, and comes to the European market by way of Bombay.\n\nAmmoniacum is closely related to asafoetida and galbanum (from which, however, it differs in yielding no umbelliferone) both in regard to the plant which yields it and its putative therapeutical effects. Internally it is used in conjunction with squills in bronchial affections. In asthma and chronic colds it is found useful, but it has no advantages over a number of other substances of more constant and active properties (Sir Thomas Richard Fraser). Only the \"tear ammoniacum\" is official.\n\nAfrican ammoniacum is the product of a plant said to be \"Ferula tingitana\", which grows in North Africa; it is a dark colored gum-resin, possessed of a very weak odor and a persistent acrid taste.\n\n"}
{"id": "670398", "url": "https://en.wikipedia.org/wiki?curid=670398", "title": "Ancillary statistic", "text": "Ancillary statistic\n\nIn statistics, an ancillary statistic is a statistic whose sampling distribution does not depend on the parameters of the model. An ancillary statistic is a pivotal quantity that is also a statistic. Ancillary statistics can be used to construct prediction intervals.\n\nThis concept was introduced by the statistical geneticist Sir Ronald Fisher.\n\nSuppose \"X\", ..., \"X\" are independent and identically distributed, and are normally distributed with unknown expected value \"μ\" and known variance 1. Let\n\nbe the sample mean.\n\nThe following statistical measures of dispersion of the sample\nare all \"ancillary statistics\", because their sampling distributions do not change as \"μ\" changes. Computationally, this is because in the formulas, the \"μ\" terms cancel – adding a constant number to a distribution (and all samples) changes its sample maximum and minimum by the same amount, so it does not change their difference, and likewise for others: these measures of dispersion do not depend on location.\n\nConversely, given i.i.d. normal variables with known mean 1 and unknown variance \"σ\", the sample mean formula_3 is \"not\" an ancillary statistic of the variance, as the sampling distribution of the sample mean is \"N\"(1, \"σ\"/\"n\"), which does depend on \"σ\" – this measure of location (specifically, its standard error) depends on dispersion.\n\nGiven a statistic \"T\" that is not sufficient, an ancillary complement is a statistic \"U\" that is ancillary and such that (\"T\", \"U\") is sufficient. Intuitively, an ancillary complement \"adds the missing information\" (without duplicating any).\n\nThe statistic is particularly useful if one takes \"T\" to be a maximum likelihood estimator, which in general will not be sufficient; then one can ask for an ancillary complement. In this case, Fisher argues that one must condition on an ancillary complement to determine information content: one should consider the Fisher information content of \"T\" to not be the marginal of \"T\", but the conditional distribution of \"T\", given \"U\": how much information does \"T\" \"add\"? This is not possible in general, as no ancillary complement need exist, and if one exists, it need not be unique, nor does a maximum ancillary complement exist.\n\nIn baseball, suppose a scout observes a batter in \"N\" at-bats. Suppose (unrealistically) that the number \"N\" is chosen by some random process that is independent of the batter's ability – say a coin is tossed after each at-bat and the result determines whether the scout will stay to watch the batter's next at-bat. The eventual data are the number \"N\" of at-bats and the number \"X\" of hits: the data (\"X\", \"N\") are a sufficient statistic. The observed batting average \"X\"/\"N\" fails to convey all of the information available in the data because it fails to report the number \"N\" of at-bats (e.g., a batting average of 0.400, which is very high, based on only five at-bats does not inspire anywhere near as much confidence in the player's ability than a 0.400 average based on 100 at-bats). The number \"N\" of at-bats is an ancillary statistic because\nThis ancillary statistic is an ancillary complement to the observed batting average \"X\"/\"N\", i.e., the batting average \"X\"/\"N\" is not a sufficient statistic, in that it conveys less than all of the relevant information in the data, but conjoined with \"N\", it becomes sufficient.\n\n"}
{"id": "34636170", "url": "https://en.wikipedia.org/wiki?curid=34636170", "title": "Angelicall Stone", "text": "Angelicall Stone\n\nThe Angelicall Stone is a concept in alchemy. According to Elias Ashmole the stone was the goal above all goals for the alchemist. In his prologue to the \"Theatrum Chemicum Britannicum\", he states:\n\n"}
{"id": "1441730", "url": "https://en.wikipedia.org/wiki?curid=1441730", "title": "Argo (crater)", "text": "Argo (crater)\n\nArgo is a crater in the Meridiani Planum on Mars, which was visited by the \"Opportunity\" rover on approximately its 365th Martian sol. The crater is about south of the heat shield and Heat Shield Rock.\n\n"}
{"id": "34599886", "url": "https://en.wikipedia.org/wiki?curid=34599886", "title": "Aurora Max", "text": "Aurora Max\n\nThe AuroraMax project is an outreach and education initiative carried out by the combined effort of The Canadian Space Agency, Astronomy North, University of Calgary and The city of Yellowknife in Yellowknife Northwest Territories. The AuroraMax provides a place to explore and discover the wonder and science behind phenomena such as sunspots, Earth's magnetic field, space weather and the magnificent Northern lights of Canada. Auroras are natural exhibition of lights in the sky that can be seen by the naked eyes. Auroras happen when protons and electrons or charged particles and gases in the Earth's atmosphere crash into each other, creating tiny flares of light that decorate the sky with colorful light. Billions of these tiny flares occur in sequence causing the lights to seem like they are dancing or moving. The lights in the Northern hemisphere are called Aurora Borealis or simply The Northern Lights while the lights in the Southern hemisphere are called Aurora Australis or simply The Southern Lights.\n\nThe purpose of the project is to monitor the frequency and intensity of the Aurora Borealis (Northern lights) in the years that led up to the Solar Maximum that was scheduled to occur in 2013-14. Scientists have noticed that the sun's activity follows a regular cycle and the peak (Solar maximum) is reached every 11 years. The main purpose of this project is to observe the activities and their developments leading up to the occurrence of the solar maximum, so that the pattern can be established and understood. The AuroraMax, also, has a website where everyone and anyone can view, admire the lights and the other wonders explored in the project in real time, as well as follow up on solar activities.\n\nAuroras are a product of the sun's activity; therefore the beautiful colorful lights seen at night are often caused by activities by the sun 150 million kilometers away. The sun sends out a constant flow of charged particles known as solar wind. When these winds or eruptions are pointed or directed at the Earth, they can spark up sun-induced auroras. The greater the activity of the sun, the brighter the auroras we see.\nThe colors of the aurora are determined by three elements which are: the density of the atmosphere, the composition of gases in the Earth's atmosphere, the altitude where aurora occurs and the level of energy involved. The most common color seen is green, which is caused when the charged particles collide with oxygen at lower altitudes between 100-300 kilometers. Occasionally, the ends or tips of the aurora have a pink or crimson fringe, this is caused by the presence of nitrogen molecules. \nIn the upper atmosphere between 300-400 kilometers, red instead of green is caused by the collision with atomic oxygen. It takes longer for the lights to be produced at higher altitudes because the atmosphere is less dense so it takes more energy and more time for the red lights to be produced. Blue and purple lights are also produced by the collision with hydrogen and helium but they are difficult for the eyes to see against the night sky.\n"}
{"id": "2196176", "url": "https://en.wikipedia.org/wiki?curid=2196176", "title": "Castle Union", "text": "Castle Union\n\nCastle Union was the code name given to one of the tests in the Operation Castle series of United States nuclear tests. It was the first test of the TX-14 thermonuclear weapon (initially the \"emergency capability\" EC-14), one of the first deployed U.S. thermonuclear bombs.\n\nAn \"Alarm Clock\" device is a \"dry\" fusion bomb, using lithium deuteride fuel for the fusion stage of a \"staged\" fusion bomb, unlike the cryogenic liquid deuterium of the first-generation Ivy Mike fusion device.\n\nIt differed from the Castle Romeo \"Runt\" device, tested shortly before, in using highly enriched lithium (approximately 95% lithium-6; natural lithium is a mixture of lithium-6 and lithium-7 isotopes). The \"Runt\" device had 7.5% lithium-6 in the fusion fuel.\n\nThe test took place on April 26, 1954 at Bikini atoll of the Marshall Islands, on a barge moored in the lagoon, off Yurochi island. The yield of 6.9 megatons of TNT was somewhat higher than the predicted 3-4 megatons. Although the barge had been moored in over of water, the test left a crater in diameter and deep in the bottom of the lagoon.\n\nLike the Ivy Mike, Castle Bravo, and Castle Romeo tests, a large percentage of the yield was produced by fast fission of the natural uranium tamper, which contributed to the extensive fallout caused by these tests.\n\nAs the highly enriched lithium was both expensive and scarce at the time, it limited the number of these weapons that could be produced. The \"Runt\" design tested in Castle Romeo and Castle Yankee was preferred for deployment.\n\n\n"}
{"id": "902164", "url": "https://en.wikipedia.org/wiki?curid=902164", "title": "Centaurus Cluster", "text": "Centaurus Cluster\n\nThe Centaurus Cluster (A3526) is a cluster of hundreds of galaxies, located approximately 170 million light years away in the Centaurus constellation. The brightest member galaxy is the elliptical galaxy NGC 4696 (~11m). The Centaurus cluster shares its supercluster, the Hydra-Centaurus Supercluster, with IC4329 Cluster and Hydra Cluster.\n\nThe cluster consists of two different sub-groups of galaxies with different velocities. Cen 30 is the main subgroup containing NGC 4696. Cen 45 which is centered on NGC 4709, is moving at 1500 km/s relative to Cen 30, and is believed to be merging with the main cluster.\n\n"}
{"id": "2783196", "url": "https://en.wikipedia.org/wiki?curid=2783196", "title": "Corpuscular theory of light", "text": "Corpuscular theory of light\n\nIn optics, the corpuscular theory of light, arguably set forward by Descartes (1637) states that light is made up of small discrete particles called \"corpuscles\" (little particles) which travel in a straight line with a finite velocity and possess impetus. This was based on an alternate description of atomism of the time period. This theory cannot explain refraction, diffraction, interference and polarization.\n\nIsaac Newton was a pioneer of this theory, in 1672.\n\nIn the early 17th century, natural philosophers were seeking new information to replace Aristotelianism that had been popular for centuries. Various European philosophers adopted what came to be known as mechanical philosophy sometime between around 1610 to 1650, which described the universe and its contents as a kind of large-scale mechanism, a philosophy that explained the universe is made with matter and motion. This mechanical philosophy was based on Epicureanism, and the work of Leucippus and his pupil Democritus and their atomism, in which everything in the universe, including a person's body, mind, soul and even thoughts, was made of atoms; very small particles of moving matter. During the early part of the 17th century, the atomistic portion of mechanical philosophy was largely developed by Gassendi, René Descartes and other atomists.\n\nThe core of Pierre Gassendi's philosophy is his atomist matter theory. In his great work, \"Syntagma Philosophicum\", (\"Philosophical Treatise\"), published posthumously in 1658, Gassendi tried to explain aspects of matter and natural phenomena of the world in terms of atoms and the void. He took Epicurean atomism and modified it to be compatible with Christian theology, by suggesting several key changes to it:\n\n\nCorpuscular theories, or corpuscularianism, are similar to the theories of atomism, except that in atomism the atoms were supposed to be indivisible, whereas corpuscles could in principle be divided. Corpuscles are single, infinitesimally small, particles which have shape, size, color, and other physical properties which alter their functions and effects in phenomena in the mechanical and biological sciences. This later led to the modern idea that compounds have secondary properties different from the elements of those compounds. Gassendi asserts that corpuscles are particles that carry other substance or substances and are of different types. These corpuscles are also emissions from various sources such as solar entities, animals or plants. Robert Boyle was a strong proponent of corpuscularianism and used the theory to exemplify the differences between a vacuum and a plenum, by which he aimed to further support his mechanical philosophy and overall atomist theory. About a half-century after Gassendi, Isaac Newton used existing corpuscular theories to develop his particle theory of the physics of light.\n\nIsaac Newton argued that the geometric nature of reflection and refraction of light could only be explained if light was made of particles, referred to as corpuscles, because waves do not tend to travel in straight lines. Newton sought to disprove Christiaan Huygens' theory that light was made of waves. In his 44th trial in a series of experiments concerning physics of light, he concluded that light is made of particles and not waves by having passed a beam of white light through two prisms which were held at such an angle that the light split into a spectrum after passing through the first prism and then was recomposed, back into white light, by the second prism.\n\nThe corpuscular theory was largely developed by Isaac Newton. Newton's theory was predominant for more than 100 years and took precedence over Huygens' wave front theory, partly because of Newton's great prestige. When the corpuscular theory failed to adequately explain the diffraction, interference and polarization of light it was abandoned in favour of Huygens' wave theory. To some extent, Newton's corpuscular (particle) theory of light re-emerged in the 20th century, as light phenomenon is currently explained as particle and wave.\n\nNewton's corpuscular theory was an elaboration of his view of reality as interactions of material points through forces. Note Albert Einstein's description of Newton's conception of physical reality:\n\n[Newton's] physical reality is characterised by concepts of space, time, the material point and force (interaction between material points). Physical events are to be thought of as movements according to law of material points in space. The material point is the only representative of reality in so far as it is subject to change. The concept of the material point is obviously due to observable bodies; one conceived of the material point on the analogy of movable bodies by omitting characteristics of extension, form, spatial locality, and all their 'inner' qualities, retaining only inertia, translation, and the additional concept of force.\n\n\n\n"}
{"id": "15631864", "url": "https://en.wikipedia.org/wiki?curid=15631864", "title": "Dark current spectroscopy", "text": "Dark current spectroscopy\n\nDark Current Spectroscopy is a technique that is used to determine contaminants in silicon.\n"}
{"id": "21486509", "url": "https://en.wikipedia.org/wiki?curid=21486509", "title": "Differential stress", "text": "Differential stress\n\nDifferential stress is the difference between the greatest and the least compressive stress experienced by an object. For both the geological and civil engineering convention formula_1 is the greatest compressive stress and formula_2 is the weakest,\n\nformula_3.\n\nIn other engineering fields and in physics, formula_2 is the greatest compressive stress and formula_1 is the weakest, so\n\nformula_6.\n\nThese conventions originated because geologists and civil engineers (especially soil mechanicians) are often concerned with failure in compression, while many other engineers are concerned with failure in tension. A further reason for the second convention is that it allows a positive stress to cause a compressible object to increase in size, making the sign convention self-consistent.\n\nIn structural geology, differential stress is used to assess whether tensile or shear failure will occur when a Mohr circle (plotted using formula_1 and formula_2) touches the failure envelope of the rocks. If the differential stress is less than four times the tensile strength of the rock, then extensional failure will occur. If the differential stress is more than four times the tensile strength of the rock, then shear failure will occur.\n"}
{"id": "13902244", "url": "https://en.wikipedia.org/wiki?curid=13902244", "title": "Echelon above corps", "text": "Echelon above corps\n\nEchelons above corps (EAC), in US and NATO practice, refer to higher headquarters, of purpose-built organization, which involve a greater number of troops than would be in an army corps. They may be standing organizations with a regional responsibility, or may be established for a particular operational purpose. While EAC most commonly refer to ground combat forces, they may refer to joint commands. They may also be administrative headquarters with responsibility for preparing combat forces.\n\nWhile there were a significant number of EAC in World War II, with increasing power of smaller organizations, it may not be required to have a ground force of the size of:\n\nEven in World War II, while the Western Allies used these terms, they were not universal. A Soviet army was roughly equivalent to a US or Commonwealth corps, with a front roughly equivalent to an army group. Japanese armies were also equivalent to US or Commonwealth corps, an area army to a western field army, and a general army to a theater.\n\nThe US Army divides types of troops into combat arms (infantry, armor, artillery, aviation, special forces), combat support (intelligence, communications, engineer, military police) and combat service support (supply, maintenance, medical, transportation, chaplain, finance and administration).\n\nMilitary police, in the current environment, often are in a combat role. Doctrine is catching up with reality, as in Panama, and most recently in Iraq, with the Raven 42 patrol led by Staff Sergeant Timothy Nein and Sergeant Leigh Ann Hester, where a woman, for the first time, received the Silver Star medal, and qualified for the Combat Action Badge. Raven 42, a unit of the 617th Military Police Company of the Kentucky National Guard, was on a convoy escort mission where combat was reasonable to expect, as opposed to the rear area MP units guarding prisons and POWs.\n\nThomas J. Newman, a Quartermaster Corps major in 1993, analyzes the roles of combat service support at EAC. It must be remembered that US Army doctrine was in flux as his monograph was being developed, analyzing the lessons learned from Operation Desert Storm, but the Army not yet gone to the major restructuring into units of action/brigade combat teams and units of employment.\n[from the abstract] Army concepts for supporting operations involving multiple corps were called into question by actions taken during Operations Desert Shield and Desert Storm. Non-doctrinal organizations provided support to U.S. forces deployed on these operations, despite the fact that doctrinally correct organizations existed. The monograph examines existing doctrine for support of multi-corps operations, and also doctrine for Army theater command and control architecture. It then summarizes support operations during the Gulf\nWar Emerging logistics doctrine is then compared with both previous doctrine and with lessons learned in the Gulf. The monograph concludes that existing doctrine for support at echelons above corps requires revision, that-emerging doctrine is on the right track, and that a key requirement will be for the Army to identify a way to test new doctrine in a realistic manner.\n\nStill in use in US forces are Unified Combatant Commands, roughly equivalent to a theater. Multinational Force Iraq (MNF-I) is a level of command comparable to a reinforced field army.\n"}
{"id": "474245", "url": "https://en.wikipedia.org/wiki?curid=474245", "title": "Einstein (unit)", "text": "Einstein (unit)\n\nThe einstein is a unit defined as one mole () of photons. As such, photosynthetically active radiation (PAR) was formerly often reported in microeinsteins per second per square meter (μE m s). This usage is also not part of the International System of Units and when used this way it is redundant with the mole.\n\nSince the unit does not have a standard definition and is not part of the SI system, it is usually better to avoid its use. The same information about photosynthetically active radiation can be conveyed using the SI convention by stating something such as, \"The photon flux was 1500 μmol m s\".\n\nThis unit was named after physicist Albert Einstein.\n"}
{"id": "3807786", "url": "https://en.wikipedia.org/wiki?curid=3807786", "title": "Experimental Advanced Superconducting Tokamak", "text": "Experimental Advanced Superconducting Tokamak\n\nThe Experimental Advanced Superconducting Tokamak (EAST), internal designation HT-7U, is an experimental superconducting tokamak magnetic fusion energy reactor in Hefei, China. The Hefei-based Institute of Plasma Physics is conducting the experiment for the Chinese Academy of Sciences. It has operated since 2006. It was later put under control of Hefei Institutes of Physical Science.\n\nIt is the first tokamak to employ superconducting toroidal and poloidal magnets. It aims for plasma pulses of up to 1000 seconds.\n\nThe project was proposed in 1996 and approved in 1998. According to a 2003 schedule, buildings and site facilities were to be constructed by 2003. Tokamak assembly was to take place from 2003 through 2005.\n\nConstruction was completed in March 2006 and on September 28, 2006, \"first plasma\" was achieved.\n\nThe reactor is an improvement over China's first superconducting tokamak device, dubbed HT-7, built by the Institute of Plasma Physics in partnership with Russia in the early 1990s.\n\nAccording to official reports, the project's budget is CNY ¥300 million (approximately US$37 million), some 1/15 to 1/20 the cost of a comparable reactor built in other countries.\n\nOn September 28, 2006, first plasma was achieved--the first test lasted nearly three seconds, and generated an electrical current of 200 kiloamperes. \n\nBy Jan 2007 \"the reactor created a plasma lasting nearly five seconds and generating an electrical current of 500 kilo amperes\".\n\nOn November 7, 2010, EAST achieved its first H-mode plasma by LHW alone. \n\nIn May 2011, EAST became the first tokamak to successfully sustain H-Mode plasma for over 30 seconds at ~50 million Kelvin.\n\nOn November 29, 2011, The ribbon-cutting ceremony for EAST auxiliary heating system project was held, signifying EAST's entering of “Phase-II”.\n\nOn May 19, 2014, after nearly 20-month-long upgrading break since September 2012, EAST was ready for the first round of experiment campaign in 2014. \n\nBy May 2015, EAST was reporting 1 MA currents, and H-mode for 6.4 seconds.\n\nIn February, 2016, a plasma pulse was maintained for a record 102 seconds at ~50 million Kelvin. Plasma current of 400kA and a density of about 2.4 x 10/m with slowly increasing temperature.\n\nOn November 2, 2016, EAST became the first tokamak to successfully sustain H-Mode plasma for over a minute at ~50 million °C.\n\nOn July 3, 2017, EAST became the first tokamak to successfully sustain H-Mode plasma for over 100 seconds at ~50 million degree Celsius.\n\nOn November 12, 2018, EAST reached a milestone of 100 million degree Celsius.\n\nChina is a member of the ITER consortium, and EAST is a testbed for ITER technologies.\n\nEAST was designed to test:\n\n\n\n"}
{"id": "45295473", "url": "https://en.wikipedia.org/wiki?curid=45295473", "title": "Fanny Hesse", "text": "Fanny Hesse\n\nFanny Hesse (born Angelina Fanny Elishemius, June 22, 1850 – December 1, 1934) is best known for her work in microbiology alongside her husband, Walther Hesse. Together they were instrumental in developing agar as a medium for culturing microorganisms.\n\nHesse was born in 1850 in New York City to Gottfried Elishemius, a wealthy import merchant, and his wife, Ceclie Elise. She met her husband and research partner Walther Hesse in 1872 while in Germany. They were engaged in 1873, and married in 1874 in Geneva.\n\nHesse worked in an unpaid capacity to assist her husband through preparing bacterial growth media, cleaning equipment and producing illustrations for publications.\n\nIn 1881, while her husband was working in the laboratory of German physician and microbiologist Robert Koch, Hesse suggested that agar was preferable to gelatin for cultivating bacteria. She was aware of the properties of agar as a gelling agent that maintained its gel properties at warm temperatures through using it at home to make puddings and jellies. A neighbour who had lived in Java had introduced agar to her when she lived in America. \n\nThis led to Koch using agar to cultivate the bacteria that cause tuberculosis.\n\nAlthough Koch, in an 1882 paper on tuberculosis bacilli, mentioned he used agar instead of gelatin, he did not credit either Hesse, or mention why he made the switch. Hesse's suggestion never resulted in financial benefit for the Hesse family.\n\n\n"}
{"id": "57058572", "url": "https://en.wikipedia.org/wiki?curid=57058572", "title": "Frederick Debell Bennett", "text": "Frederick Debell Bennett\n\nFrederick Debell Bennett was a ship surgeon, a Fellow of the Royal College of Surgeons a member of the Royal Geographical Society and a biologist. Born to a family of means in Devon, England in 1806, he obtained his Licentiate of the Society of Apothecaries (L.S.A.) in 1828, and his membership of the Royal College of Surgeons in 1829. Bennett first served as Assistant Surgeon on the hospital ship Grampus, which was moored on the Thames. Then in 1833, he joined the London whaleship Tuscan. From 1833-1836 he sailed round the globe on board the 'Tuscan'. The task of this journey was to study whales, lands and nature. He described several species, for example Whalesucker (\"Remora australis\"), blue noddy and \"Cheilopogon nigricans\". After his return he practiced medicine in Southark where he died in 1859 at the age of fifty-three.\n\n\n"}
{"id": "57275063", "url": "https://en.wikipedia.org/wiki?curid=57275063", "title": "Frederick William Rudler", "text": "Frederick William Rudler\n\nFrederick William Rudler FGS (8 July 1840, London – 23 January 1915. Tatsfield, Surrey) was an English mineralogist, geologist, anthropologist, and natural scientist.\n\nAfter education at the Regent Street Royal Polytechnic Institution, Rudler was appointed in 1861 an assistant curator at the Museum of Practical Geology in Jermyn Street, London and remained in that post until 1876. From 1876 to 1879 he was a lecturer at the University College of Wales at Aberystwyth. From 1879 until his retirement in 1902, he was the curator and librarian of the Museum of Practical Geology.\n\nRudler was elected a Fellow of the Geological Society in 1870 and received the Society's Lyell Medal in 1903. He was president in 1880 of the anthropological department of the British Association, in 1887–1889 of the Geologists' Association, and in 1898–1899 of the Royal Anthropological Institute. He contributed numerous articles to Ure's \"Dictionary of Arts, Manufactures, and Mines\" (1875), Thorpe's \"Dictionary of Applied Chemistry\", Muir's \"Dictionary of Chemistry\", the \"Encyclopædia Britannica\" (1911), and prestigious journals.\n\n"}
{"id": "5628883", "url": "https://en.wikipedia.org/wiki?curid=5628883", "title": "Gautama Siddha", "text": "Gautama Siddha\n\nGautama Siddha, (fl. 8th century) astronomer, astrologer and compiler of Indian descent, known for leading the compilation of the \"Treatise on Astrology of the Kaiyuan Era\" during the Tang Dynasty. He was born in Chang'an, and his family was originally from India, according to a tomb stele uncovered in 1977 in Xi'an. The Gautama family had probably settled in China over many generations, and might have been present in China prior even to the foundation of the Tang Dynasty. He was most notable for his translation of Navagraha calendar into Chinese. He also introduced Indian numerals with zero (〇) in 718 in China as a replacement of counting rods.\n\n"}
{"id": "970716", "url": "https://en.wikipedia.org/wiki?curid=970716", "title": "Henry Charles Gordon", "text": "Henry Charles Gordon\n\nHenry Charles Gordon (December 23, 1925 – September 24, 1996), (Col, USAF), was an American aeronautical engineer, U.S. Air Force officer, test pilot, and astronaut in the X-20 Dyna-Soar program.\n\nGordon was born in Valparaiso, Indiana, on December 23, 1925. In 1950 he earned his Bachelor of Science degree in Aeronautical Engineering from Purdue University, and in 1966 he earned his Master of Business Administration degree from the University of Southern California. He is married and has four children.\n\nGordon was in the Air Force, and flew combat missions in the Korean and Vietnam wars. He was selected as an astronaut in the X-20 Dyna-Soar program in April 1962 and began training at the Air Force Flight Test Center, Edwards Air Force Base, California.\n\nHe retired as an astronaut when the Dyna-Soar program was cancelled on December 10, 1963, having never flown in space. He remained in the U.S. Air Force after the Dyna-Soar program was cancelled and retired from the Air Force with the rank of Colonel.\n\nGordon died in Peoria, Arizona on September 24, 1996, age 70.\n\n"}
{"id": "45042712", "url": "https://en.wikipedia.org/wiki?curid=45042712", "title": "Hussey (crater)", "text": "Hussey (crater)\n\nHussey Crater is an impact crater in the Phaethontis quadrangle on Mars at 53.8° S and 126.7° W. and is 99.0 km in diameter. Its name was approved in 1973, and it was named after William Hussey (astronomer).\n\nThe crater and the small vicinity was first imaged by Mariner 4 in 1965 as its 17th image.\n\nNearby notable craters include Brashear to the east, that crater being in the Thaumasia quadrangle, Dokuchaev to the south and Clark not far to the west-southwest. Northwest and further north of Hussey are a series of rifts known as Icaria Fossae.\n\nDunes are visible in the northern part of the crater floor. Also gullies are dominant in parts of its crater rims.\n\nPictures below show the dunes from a wide view and then eventually show greatly enlarged view with HiRISE.\n"}
{"id": "17727869", "url": "https://en.wikipedia.org/wiki?curid=17727869", "title": "ITools Resourceome", "text": "ITools Resourceome\n\niTools is a distributed infrastructure for managing, discovery, comparison and integration of computational biology resources. iTools employs Biositemap technology to retrieve and service meta-data about diverse bioinformatics data services, tools, and web-services. iTools is developed by the National Centers for Biomedical Computing as part of the NIH Road Map Initiative.\n\n\n"}
{"id": "44733635", "url": "https://en.wikipedia.org/wiki?curid=44733635", "title": "I Zw 36", "text": "I Zw 36\n\nI Zwicky 36, often abbreviated to I Zw 36, is a blue compact dwarf galaxy which is in the constellation Canes Venatici. The dominant population of stars in I Zw 36 is young in stellar terms, with ages of under 3 million years.\n"}
{"id": "26255748", "url": "https://en.wikipedia.org/wiki?curid=26255748", "title": "Josef Finger", "text": "Josef Finger\n\nJosef Finger (1 January 1841 – 6 May 1925) was an Austrian physicist and mathematician.\n\nJoseph Finger was born the son of a baker in Pilsen. He attended high school in Pilsen. He studied mathematics and physics at Charles University in Prague from 1859 to 1862. In 1865 and for financial reason he acquired the qualification to teach mathematics and physics at secondary schools and went into the teaching profession. On 17 March 1875 Finger received his doctorate at the University of Vienna, in 1876 he was qualified for the subject of analytical mechanics. 1897 Finger publishes \"On the internal virial of an elastic body\". Finger was from 1888 to 1890 the Dean of the Chemical School, and from 1890 to 1891 he was rector of the Technische Hochschule in Vienna. In 1916 Finger was awarded an honorary Doctorate of Technical Sciences. Finger is considered a pioneer of continuum mechanics.\n\n"}
{"id": "312408", "url": "https://en.wikipedia.org/wiki?curid=312408", "title": "Law of total variance", "text": "Law of total variance\n\nIn probability theory, the law of total variance or variance decomposition formula or conditional variance formulas or Law of Iterated Variances also known as Eve's law, states that if \"X\" and \"Y\" are random variables on the same probability space, and the variance of \"Y\" is finite, then\n\nIn language perhaps better known to statisticians than to probabilists, the two terms are the \"unexplained\" and the \"explained\" components of the variance respectively (cf. fraction of variance unexplained, explained variation). In actuarial science, specifically credibility theory, the first component is called the expected value of the process variance (EVPV) and the second is called the variance of the hypothetical means (VHM).\n\nThere is a general variance decomposition formula for \"c\" ≥ 2 components (see below). For example, with two conditioning random variables:\n\nwhich follows from the law of total conditional variance:\n\nNote that the conditional expected value is a random variable in its own right, whose value depends on the value of \"X\". Notice that the conditional expected value of \"Y\" given the \"event\" \"X\" = \"x\" is a function of \"x\" (this is where adherence to the conventional and rigidly case-sensitive notation of probability theory becomes important!). If we write E( \"Y\" | \"X\" = \"x\" ) = \"g\"(\"x\") then the random variable is just \"g\"(\"X\"). Similar comments apply to the conditional variance.\n\nOne special case, (similar to the law of total expectation) states that if formula_4 is a partition of the whole outcome space, i.e. these events are mutually exclusive and exhaustive, then\n\nIn this formula, the first component is the expectation of the conditional variance; the other two rows are the variance of the conditional expectation.\n\nThe law of total variance can be proved using the law of total expectation. First,\n\nfrom the definition of variance. Then we apply the law of total expectation to each term by conditioning on the random variable \"X\":\n\nNow we rewrite the conditional second moment of Y in terms of its variance and first moment:\n\nSince the expectation of a sum is the sum of expectations, the terms can now be regrouped:\n\nFinally, we recognize the terms in parentheses as the variance of the conditional expectation E[\"Y\" | \"X\"]:\n\nThe following formula shows how to apply the general, measure theoretic variance decomposition formula to stochastic dynamic systems. Let \"Y\"(\"t\") be the value of a system variable at time \"t\". Suppose we have the internal histories (natural filtrations) formula_11, each one corresponding to the history (trajectory) of a different collection of system variables. The collections need not be disjoint. The variance of \"Y\"(\"t\") can be decomposed, for all times \"t\", into \"c\" ≥ 2 components as follows:\n\nThe decomposition is not unique. It depends on the order of the conditioning in the sequential decomposition.\n\nIn cases where (\"Y\", \"X\") are such that the conditional expected value is linear; i.e., in cases where\n\nit follows from the bilinearity of covariance that \n\nand\n\nand the explained component of the variance divided by the total variance is just the square of the correlation between \"Y\" and \"X\"; i.e., in such cases,\n\nOne example of this situation is when (\"X\", \"Y\") have a bivariate normal (Gaussian) distribution.\n\nMore generally, when the conditional expectation is a non-linear function of \"X\"\n\nwhich can be estimated as the \"R\" squared from a non-linear regression of \"Y\" on \"X\", using data drawn from the joint distribution of (\"X\",\"Y\"). When has a Gaussian distribution (and is an invertible function of \"X\"), or \"Y\" itself has a (marginal) Gaussian distribution, this explained component of variation sets a lower bound on the mutual information:\n\nA similar law for the third central moment \"μ\" says\n\nFor higher cumulants, a simple and elegant generalization exists. See law of total cumulance.\n\n\n"}
{"id": "1336522", "url": "https://en.wikipedia.org/wiki?curid=1336522", "title": "List of distributed computing projects", "text": "List of distributed computing projects\n\nThis is a list of distributed computing and grid computing projects. For each project, donors volunteer computing time from personal computers to a specific cause. The donated computing power comes typically from CPUs and GPUs, but can also come from home video game systems. Each project seeks to solve a problem which is difficult or infeasible to tackle using other methods.\n\n(Work on these projects was either finished, or the project was discontinued).\nWhile distributed computing functions by dividing a complex problem among diverse and independent computer systems and then combine the result, grid computing works by utilizing a network of large pools of high-powered computing resources. These are typically \"umbrella\" projects that have a number of sub-projects underneath them, with multiple research areas.\n\n\nThese projects attempt to make large physical computation infrastructures available for researchers to use:\n\n"}
{"id": "13724299", "url": "https://en.wikipedia.org/wiki?curid=13724299", "title": "List of flags of Israel", "text": "List of flags of Israel\n\nThe following is a list of flags of Israel.\n\n"}
{"id": "52638411", "url": "https://en.wikipedia.org/wiki?curid=52638411", "title": "List of things named after Christiaan Huygens", "text": "List of things named after Christiaan Huygens\n\n\n\n\n\n"}
{"id": "36978429", "url": "https://en.wikipedia.org/wiki?curid=36978429", "title": "List of xanthoma variants associated with hyperlipoproteinemia subtypes", "text": "List of xanthoma variants associated with hyperlipoproteinemia subtypes\n\nWhen the cholesterol levels in the body rise above the normal level a number of skin lesions can occur. Xanthomas are one of types of skin lesions that may occur in this situation. \n\nOther systemic conditions may also occur with increased levels of cholesterol in the blood.\n\n\n"}
{"id": "57014144", "url": "https://en.wikipedia.org/wiki?curid=57014144", "title": "MACS J1149 Lensed Star 1", "text": "MACS J1149 Lensed Star 1\n\nMACS J1149 Lensed Star 1, also known as Icarus, is a blue supergiant star observed through a gravitational lens. It is the most distant individual star detected, at approximately 14 billion light-years from Earth (redshift z=1.49; comoving distance of 14.4 billion light-years; lookback time of 9.34 billion years), as of April 2018. Light from the star was emitted 4.4 billion years after the Big Bang. According to co-discoverer Patrick Kelly, the star is at least a hundred times more distant than the next-farthest non-supernova star observed, SDSS J1229+1122, and is the first magnified individual star seen.\n\nIn April and May 2016, the blue supergiant star was found in the course of studying the supernova SN Refsdal with the Hubble Space Telescope. Astronomer Patrick Kelly of the University of Minnesota is the lead author of the finding, published in the journal \"Nature Astronomy\".\n\nWhile astronomers had been collecting images of this supernova from 2004 onward, they recently discovered a point source that had appeared in their 2013 images, and become much brighter by 2016. They determined that the point source was a solitary star being magnified more than 2,000 times by gravitational lensing. The light from LS1 was magnified not only by the huge total mass of the galaxy cluster MACS J1149+2223—located 5 billion light-years away—but also transiently by another compact object of about three solar masses within the galaxy cluster itself that passed through the line of sight, an effect known as gravitational microlensing. The galaxy cluster magnification is probably by a factor of ≈600, while the microlensing event, which peaked in May 2016, brightened the image by an additional factor of ≈4. There was a second peak near the brightness curve maximum, which may indicate the star was binary. The microlensing body may have been a star or a black hole in the cluster. Continuous monitoring of the star \"Icarus\" may one day rule out the possibility that primordial black holes constitute a sizable fraction of dark matter. Normally, the only astronomical objects that can be detected at this range would be either whole galaxies, quasars, or supernovas, but the light from the star was magnified by the lensing effect. They determined the light was from a stable star, and not a supernova as its temperature did not fluctuate; the temperature also allowed them to catalog the star as a blue supergiant.\n\nThe light observed from the star was emitted when the universe was about 30% of its current age of 13.8 billion years. Kelly suggested that similar microlensing discoveries could help them identify the earliest stars in the universe. The star no longer exists as a blue supergiant, given the known lifetime of such stars.\n\nThe formal name MACS J1149 is a reference to MAssive Cluster Survey and the star's coordinates in the J2000 astronomical epoch.\n\nWhile Kelly had wanted to name the star \"Warhol\", alluding to Andy Warhol's notion of having 15 minutes of fame, the team ended up naming the star \"Icarus\" based on the Greek mythological figure.\n\nThe discovery shows that astronomers can study the oldest stars in background galaxies of the early universe by combining the strong gravitational lensing effect from galaxy clusters with gravitational microlensing events caused by compact objects in these galaxy clusters. By using these events, astronomers can study and test some models about dark matter in galaxy clusters and observe high energy events (supernovae, variable stars) in young galaxies.\n\n\n"}
{"id": "58096358", "url": "https://en.wikipedia.org/wiki?curid=58096358", "title": "Meltdown (Clearfield and Tilcsik book)", "text": "Meltdown (Clearfield and Tilcsik book)\n\nMeltdown: Why Our Systems Fail and What We Can Do About It is a non-fiction book by Chris Clearfield and András Tilcsik, published in March 2018 by Penguin Press. It explores how complexity causes failure in modern systems and how individuals, organizations, and societies can prevent or mitigate the resulting meltdowns. The \"Financial Times\" selected \"Meltdown\" as one of the books of year in 2018. \n\nThe book is the result of a collaboration between Chris Clearfield, a former derivatives trader, and András Tilcsik, a professor at the University of Toronto's Rotman School of Management. Clearfield and Tilcsik took top honors in the 2015 competition for \"The Financial Times\" and McKinsey Bracken Bower Prize with their proposal for a book about how to understand systemic risk and prevent catastrophic failures in business and beyond. The following year, they signed with Penguin Press to publish \"Meltdown\".\n\nThe first three chapters of the book extend Charles Perrow’s \"normal accident theory\"—originally developed in the 1980s—to recent trends, such as the rise of social media, the increasing reliance of organizations on computer-based systems, and the growing complexity of modern financial, transportation, and communication systems. The authors warn that companies, governments, and even individuals have become dangerously reliant on complex, tightly coupled systems and are ignoring simple fixes that could avert both accidents and intentional wrongdoing.\n\nEach of the remaining chapters focuses on a different set of solutions: designing more transparent and loosely coupled systems (Chapter 4); using structured decision tools (Chapter 5); learning from near misses and other warning signs (Chapter 6); encouraging dissent and skepticism (Chapter 7); building diverse teams (Chapter 8); learning from outsiders (Chapter 9); and preparing for and managing crises more effectively (Chapter 10).\n\nIn the epilogue, the authors state that humanity is in “the golden age of meltdowns”: though modern systems give us tremendous capabilities, they also make us vulnerable to unexpected system failures. Clearfield and Tilcsik, however, express optimism about the future and argue that the solutions are within reach. At the same time, they caution that putting the necessary solutions into practice is difficult because doing so often goes against our natural instincts and violates persistent organizational and cultural norms.\n\nThe book combines analyses of case studies with summaries of social science research, drawing on academic studies, accident reports, and original interviews. The case studies include, for example, a failed social media campaign at Starbucks, a Thanksgiving dinner gone awry, the Three Mile Island nuclear accident, the Flint water crisis, Nasdaq’s botched handling of the Facebook IPO, the accidents of the space shuttles \"Challenger\" and \"Columbia\", the electronic trading meltdown of Knight Capital, the crashes of flights ValuJet 592 and Air France 447, and the June 2009 Washington Metro train collision.The academic research covered in the book comes from sociology, psychology, behavioural economics, organizational theory, and neuroscience, including studies by Charles Perrow, Diane Vaughan, Karl Weick, Amy Edmondson, Dacher Keltner, Gary Klein, Gregory Berns, Daniel Kahneman, and Georg Simmel, among others.\n\nThe \"Financial Times\" called \"Meltdown\" \"an exciting and insightful analysis of why things go wrong and how to avoid catastrophe\" and selected it as one of the books of 2018. David A. Shaywitz, reviewing the book in \"The Wall Street Journal\", concluded that \"\"Meltdown\" effectively conveys why addressing systemic failures is both difficult and essential: difficult because it’s so much more comfortable to rely on gut instinct and trust familiar colleagues than to insist on structured approaches and solicit the views of others; essential because we are moving into the danger zone and need all the help we can get.\" Shaywitz called the book’s examples \"illuminating\" and noted that \"where \"Meltdown\" really hits its stride is in taking on the factors that promote groupthink and discourage dissent.\"\n\nLeigh Buchanan, editor-at-large at \"Inc.\" magazine, named \"Meltdown\" as one of the \"eight books you need to read in 2018\" and described it as \"admirably evidence-based.\" Writing for Forbes.com, Brook Manville called \"Meltdown\" \"a thought-provoking new book\" and wrote that the authors \"colorfully explain why your job, like everyone else’s in today’s global economy, is becoming part of bigger networks of co-dependent systems, laden with unforeseeable risks and unimaginable outcomes.\" Kirkus Reviews described \"Meltdown\" as \"a useful, thought-provoking book\" and \"a cautionary study in how complex systems can easily go awry\". \"CBS This Morning\" co-host John Dickerson called the book \"really interesting\" and \"a great new book.\" In a book review for \"Strategy+Business\", Theodore Kinni wrote that \"\"Meltdown\" is something of a rarity: an enlightening and entertaining business book. It synthesizes the work of experts in high-risk systems . . . and builds upon it in an accessible and practical way.\" \n"}
{"id": "57748243", "url": "https://en.wikipedia.org/wiki?curid=57748243", "title": "Minerva (Springer journal)", "text": "Minerva (Springer journal)\n\nMinerva: A Review of Science, Learning and Policy is a quarterly peer-reviewed academic journal covering the sociological study of scientific knowledge and research. It was established in 1962, replacing a series of bulletins that had been published by the Congress for Cultural Freedom's Committee on Science and Freedom beginning in 1954. It is published by Springer Science+Business Media and the editor-in-chief is Peter Weingart (Bielefeld University). Since 2013, the journal's home institution has been the Institute for Interdisciplinary Studies of Science (I²SOS) at Bielefeld University.\n\nPast editors-in-chief of \"Minerva\" are:\n"}
{"id": "622545", "url": "https://en.wikipedia.org/wiki?curid=622545", "title": "Need", "text": "Need\n\nA need is something that is necessary for an organism to live a healthy life. Needs are distinguished from wants in that, in the case of a need, a deficiency causes a clear adverse outcome: a dysfunction or death. In other words, a need is something required for a safe, stable and healthy life (e.g. food, water, shelter) while a want is a desire, wish or aspiration. When needs or wants are backed by purchasing power, they have the potential to become economic demands. \n\nBasic needs such as water, air, food and protection from environmental dangers are necessary for an organism to live. In addition to basic needs, humans also have needs of a social or societal nature such as the human need to socialise of belong to a family unit or group. Needs can be objective and physical, such as the need for food, or psychological and subjective, such as the need for self-esteem.\n\nNeeds and wants are a matter of interest in, and form a common substrate for, the fields of philosophy, biology, psychology, social science, economics, marketing and politics.\n\nTo most psychologists, need is a psychological feature that arouses an organism to action toward a goal, giving purpose and direction to behavior.\n\nThe most widely known academic model of needs was proposed by psychologist, Abraham Maslow, in 1943. His theory proposed that people have a hierarchy of psychological needs, which range from basic physiological or lower order needs such as food, water and safety (e.g. shelter) through to the higher order needs such as self-actualization. People tend to spend most of their resources (time, energy and finances) attempting to satisfy these basic before the higher order needs of belonging, esteem and self-actualization become meaningful. Maslow's approach is a generalised model for understanding human motivations in a wide variety of contexts, but must be adapted for specific contexts. While intuitively appealing, Maslow's model has been difficult to operationalize experimentally. It was developed further by Clayton Alderfer.\n\nThe academic study of needs, which was at its zenith in the 1950s, receives less attention among psychologists today. One exception involves Richard Sennett's work on the importance of respect.\n\nOne difficulty with a psychological theory of needs is that conceptions of \"need\" may vary radically among different cultures or among different parts of the same society. For a psychological theory of human need, one found compatible with the Doyal/Gough Theory, see self-determination theory.\nany need is therefore met\n\nA second view of need is presented in the work of political economy professor Ian Gough, who has published on the subject of human needs in the context of social assistance provided by the welfare state. Together with medical ethics professor Len Doyal, he has also published \"A Theory of Human Need\".\n\nTheir view goes beyond the emphasis on psychology: it might be said that an individual's needs represent \"the costs of being human\" within society. A person who does not have his needs fulfilled—i.e., a \"needy\" person—will function poorly in society.\n\nIn the view of Gough and Doyal, every person has an objective interest in avoiding serious harm that prevents that person from endeavoring to attain his vision of what is good, regardless of what exactly that may be. That endeavor requires a capacity to \"participate\" in the societal setting in which the individual lives. More specifically, every person needs to possess both physical health and personal autonomy. The latter involves the capacity to make informed choices about what should be done and how to implement it. This requires mental health, cognitive skills, and opportunities to participate in society's activities and collective decision-making.\n\nHow are such needs satisfied? Doyal and Gough point to twelve broad categories of \"intermediate needs\" that define how the needs for physical health and personal autonomy are fulfilled: \n\nHow are the details of needs satisfaction determined? The authors point to rational identification of needs, using up-to-date scientific knowledge; consideration of the actual experiences of individuals in their everyday lives; and democratic decision-making. The satisfaction of human needs cannot be imposed \"from above\".\n\nThis theory may be compared to the capability approach developed by Amartya Sen and Martha Nussbaum. Individuals with more internal \"assets\" or \"capacities\" (e.g., education, mental health, physical strength, etc.) have more capabilities (i.e., more available choices, more positive freedom). They are thus more able to escape or avoid poverty. Those individuals who possess more capabilities fulfill more of their needs.\n\nPending publication in 2015 in the Cambridge Journal of Economics of the final version of this work, Gough discussed the Doyal/Gough theory in a working paper available online.\n\nThe concept of intellectual need has been studied in education, as well as in social work, where an Oxford Bibliographies Online: Social Work entry on Human Need reviewed the literature as of 2008 on human need from a variety of disciplines. Also see the 2008 and pending 2015 entries on Human Needs: Overview in the Encyclopedia of Social Work.\n\nIn his 1844 \"Paris Manuscripts\", Karl Marx famously defined humans as \"creatures of need\" or \"needy creatures\" who experienced suffering in the process of learning and working to meet their needs. These needs were both physical needs as well as moral, emotional and intellectual needs. According to Marx, human development is characterized by the fact that in the process of meeting their needs, humans develop new needs, implying that at least to some extent they make and remake their own nature. This idea is discussed in more detail by the Hungarian philosopher Ágnes Heller in \"A Theory of Need in Marx\" (London: Allison and Busby, 1976). Political economy professor Michael Lebowitz has developed the Marxian interpretation of needs further in two editions of his book \"Beyond Capital\".\n\nProfessor György Márkus systematized Marx's ideas about needs as follows: humans are different from other animals because their vital activity, work, is mediated to the satisfaction of needs (an animal who manufactures tools to produce other tools or his/her satisfactors), which makes a human being a universal natural being capable to turn the whole nature into the subject of his/her needs and his/her activity, and develops his/her needs and abilities (essential human forces) and develops himself/herself, a historical-universal being. Work generates the breach of the animal subject-object fusion, thus generating the possibility of human conscience and self-conscience, which tend to universality (the universal conscious being). A human being's conditions as a social being are given by work, but not only by work as it is not possible to live like a human being without a relationship with others: work is social because human beings work for each other with means and abilities produced by prior generations. Human beings are also free entities able to accomplish, during their lifetime, the objective possibilities generated by social evolution, on the basis of their conscious decisions. Freedom should be understood both in a negative (freedom to decide and to establish relationships) and a positive sense (dominion over natural forces and development of human creativity) of the essential human forces. To sum up, the essential interrelated traits of human beings are: a) work is their vital activity; b) human beings are conscious beings; c) human beings are social beings; d) human beings tend to universality, which manifests in the three previous traits and make human beings natural-historical-universal, social-universal and universal conscious entities, and e) human beings are free.\n\nIn his texts about what he calls \"moral economics\", professor Julio Boltvinik Kalinka asserts that the ideas exposed by David Wiggins about needs are correct but insufficient: needs are of a normative nature but they are also factual. These \"gross ethical concepts\" (as stated by Hilary Putnam) should also include an evaluation: Ross Fitzgerald's criticism of Maslow's ideas rejects the concept of objective human needs and uses instead the concept of preferences.\n\nMarshall Rosenberg's model of Compassionate Communication, also known as Nonviolent Communication (NVC) makes the distinction between universal human needs (what sustains and motivates human life) and specific strategies used to meet these needs. Feelings are seen as neither good nor bad, right nor wrong, but as indicators of when human needs are met or unmet. Life-sustaining and life-denying needs are especially highlighted. In contrast to Maslow, Rosenberg's model does not place needs in a hierarchy.\n\nRosenberg's model supports people developing awareness of feelings as indicators, of what needs are alive within them and others, moment by moment; to forefront needs, to make it more likely and possible for two or more people, to arrive at mutually agreed upon strategies to meet the needs of all parties. Rosenberg diagrams this sequence in part like this: Feelings > Needs > Requests where identifying needs is most significant to the process.\n\nPeople also talk about the needs of a community or organization. Such needs might include demand for a particular type of business, for a certain government program or entity, or for individuals with particular skills. This is an example of metonymy in language and presents with the logical problem of reification.\n\nMedical needs. \nIn clinical medical practice, it may be difficult to distinguish between treatment a patient needs; treatment that may be desirable;and treatment that could be deemed frivolous. At one end of this spectrum, any practising clinician would accept that a child with fulminating meningococcal meningitis - say - \"NEEDS\" rapid access to medical care, including resuscitation and intravenous antimicrobials. At the other, rarely could a young healthy woman be deemed to \"need\" breast augmentation. Numerous surgical procedures fall into this spectrum: particularly, this is so in our ageing Western population, where there is an ever-increasing prevalence of painful, but not life-threatening disorders: typified by the ageing spine.\n\n"}
{"id": "7637545", "url": "https://en.wikipedia.org/wiki?curid=7637545", "title": "Non-exact solutions in general relativity", "text": "Non-exact solutions in general relativity\n\nNon-exact solutions in general relativity are solutions of Albert Einstein's field equations of general relativity which hold only approximately. These solutions are typically found by treating the gravitational field, formula_1, as a background space-time, formula_2, (which is usually an exact solution) plus some small perturbation, formula_3. Then one is able to solve the Einstein field equations as a series in formula_3, dropping higher order terms for simplicity.\n\nA common example of this method results in the linearised Einstein field equations. In this case we expand the full space-time metric about the flat Minkowski metric, formula_5:\n\nand dropping all terms which are of second or higher order in formula_3.\n\n"}
{"id": "54194344", "url": "https://en.wikipedia.org/wiki?curid=54194344", "title": "Non-motile bacteria", "text": "Non-motile bacteria\n\nNon-motile bacteria are those bacterial species that lack the ability and structures that would allow them to propel themselves, under their own power, through their environment. When non-motile bacteria are cultured in a stab tube, they only grow along the stab line. If the bacteria are mobile, the line will appear diffuse and extend into the medium. The cell structures that provide the ability for locomotion are the cilia and flagella. Coliform and Streptococci are examples of non-motile bacteria as are \"Klebsiella pneumoniae\", and \"Yersinia pestis\". Motility is one characteristic used in the identification of bacteria and evidence of possessing structures: patrichous flagella, polar flagella and/or a combination of both.\n\nThough the lack of motility might be regarded a disadvantage, some non-motile bacteria possess structures that allow their attachment to eukaryotic cells, like GI mucousal cells.\n\nVancomycin resistant Enterococcus spp. are non-motile while vancomycin susceptible Enterococcus spp. Some genera have been divided based upon the presence or absence of motility. Motility is determined by using a motility medium. The ingredients include motility test medium, nutrient broth powder, NaCl and distilled water. An inoculating needle (not a loop) is used to insert the bacterial sample. The needle is inserted through the medium for a length of one inch. The media tube incubated at 38C. Bacteria that are motile grow away from the stab, and toward the sides and downward toward the bottom of the tube. Growth should be observed in 24 to 48 hours. With some species, the bacterium is inconsistent related to its motility.\n"}
{"id": "21535761", "url": "https://en.wikipedia.org/wiki?curid=21535761", "title": "Olav Holt", "text": "Olav Holt\n\nOlav Holt (born 7 January 1935) is a Norwegian physicist.\n\nHolt was born in Hedrum, and took his dr.philos. degree in 1963. Holt is a specialist in the upper atmosphere physics and radio wave propagation in the ionosphere. \n\nLater, Holt was hired as professor at the University of Tromsø in 1969, where he served as rector from 1973 to 1977. He is a fellow of the Norwegian Academy of Technological Sciences. In 2003 he was proclaimed Knight, First Class of the Royal Norwegian Order of St. Olav.\n\nHis wife is Tordis Holt and he is the father of the Norwegian bestselling author and former Minister of Justice Anne Holt, and the Cardiologist Dr.med. Even Holt.\n"}
{"id": "33229131", "url": "https://en.wikipedia.org/wiki?curid=33229131", "title": "Program in Placebo Studies", "text": "Program in Placebo Studies\n\nThe Program in Placebo Studies and the Therapeutic Encounter (PiPS) was founded in July 2011, at Beth Israel Deaconess Medical Center and the Harvard Medical School. Its purpose is to bring together researchers who are examining the placebo response and the impact of medical ritual, the patient-physician relationship and the power of imagination, hope, trust, persuasion, compassion and empathic witnessing in the healing process. PiPS research is translational, spanning molecular biology, neuroscience and clinical care, as well as interdisciplinary, ranging from the basic sciences to psychology to the history of medicine. The program hopes to soon expand its efforts to include health policy, with the ultimate goal of improving health care systems by elucidating, quantifying and reaffirming the more intangible and humanistic aspects of medical care. \n\nPiPS researchers include many of the founding members of the field of placebo studies. They are drawn from Harvard University’s teaching hospitals, as well as its Faculty of Arts and Sciences. Articles written by PiPS researchers have been published in top-tier peer-reviewed journals, including the New England Journal of Medicine, The Lancet, British Medical Journal, Annals of Internal Medicine, PLoS Medicine, Archives of General Psychiatry, American Psychologist, Psychological Bulletin, Psychological Science, Journal of Neuroscience, Social Science & Medicine, Bulletin of the History of Medicine, and Psychosomatic Medicine. The team's research has also been featured on ABC, CBS, CNN, NBC, HBO, NPR, BBC television and radio, Forbes, Slate Magazine, New York Times, Wall Street Journal, The New Yorker, New York Review of Books, Newsweek and Time Magazine.\n\nPiPS is led by the following members of the Harvard faculty: \n\n"}
{"id": "17248833", "url": "https://en.wikipedia.org/wiki?curid=17248833", "title": "Research Council of Norway", "text": "Research Council of Norway\n\nThe Research Council of Norway () is a Norwegian government agency responsible for awarding grants for research as well as promoting research and science. It also advises the Government in matters related to research, and is subordinate to the Norwegian Ministry of Education and Research. The council's total budget in 2009 amounted to NOK 6 165 million.\n\nThere were five predecessors of the council, each established as independent councils related to their own areas of interest: science and technology (1946), social sciences (1949), agriculture (1949), fisheries (1972) and applied social sciences (1987). The five were merged in 1993 to form the current council.\n\nThe Research Council has some 400 employees. The Research Council of Norway is led by the Director General Arvid Hallen (2014) with an executive staff organised directly under the Director General. The Director General’s executive staff is responsible for coordinating activities relating to budget planning, annual reports, statistics, strategic initiatives, international cooperation and media contact. \nThe Research Council’s highest authority is the Executive Board, which consists of seven permanent members and two deputies. Three of the Executive Board members also serve as the chairs of the respective Division Research Boards. The other members of the Division Research Boards are appointed by the Executive Board.\nThe Research Council of Norway comprises four research divisions and one division for administrative affairs:: \nIt has local representatives in nine different regions of Norway. Since 23 June 2014, the Research Council's main office is located just outside Oslo at Drammensveien 288 in Lysaker.\n\n\nThe Notur project provides the national infrastructure for high-performance computing in Norway. The project serves the Norwegian computational science community by providing the infrastructure to individuals or groups involved in\n\n\nThe Notur project aims to provide a powerful and cost-effective infrastructure for computational science and enable its efficient utilization. In addition, the project shall contribute to the development of a national grid infrastructure, be proactive in international collaboration on infrastructure and computational science, and disseminate computational science as an important discipline in Norway.\n\nThe Notur project is funded by the Research Council of Norway and the university partners. The Research Council of Norway entered into a 10-year agreement (2005-2014) with UNINETT Sigma. UNINETT Sigma is the coordinator of the project and entered into agreements with consortium partners\n\n\nSince January 2006, the Notur project placed under the eVITA programme from the Research Council of Norway.\n\nThe National Grid Initiative of Norway is NorGrid. The NorGrid initiative aims to establish and maintain a national grid infrastructure in Norway.\n\nThe NorGrid project was initially established as a subactivity of the Norwegian infrastructure project Notur for high-performance computing. In March 2007, NorGrid was recognized by the Research Council of Norway as a separate initiative with its own funding.\n\nA detailed description of the mission, organization and status of NorGrid can be found on the home pages for NorGrid.\n\nThe coordinating legal entity of the NorGrid initiative is UNINETT Sigma. NorGrid includes the national Norwegian NREN (UNINETT), the University of Bergen (UiB) and its affiliated research organization Unifob, the University of Oslo (UiO) and the University of Tromsø (UiT).\n\nNorGrid collaborates with the Nordic Data Grid Facility (NDGF), a collaboration between the Nordic countries (Denmark, Finland, Norway, Sweden), on the operations and support for the Nordic Tier-1 center that is part of the WLCG collaboration. NDGF is hosted by NORDUnet.\n\nNorGrid is funded in part by the Research Council of Norway through the eVITA programme on e-Science.\n\nThe objective of the NorStore project is to provide infrastructure in Norway for the curation of digital scientific data. The infrastructure must provide services for easy and secure access to distributed storage resources, facilitate the creation and use of digital scientific repositories, provide large aggregate capacities for storage and data transfer, and optimize the utilization of the overall storage capacity.\n\nLong-term objectives of the NorStore project include:\n\n\nThe project is funded in part by the Research Council of Norway through the eVITA programme on e-Science.\n\n\n"}
{"id": "21813898", "url": "https://en.wikipedia.org/wiki?curid=21813898", "title": "Saint Louis University School of Social Work", "text": "Saint Louis University School of Social Work\n\nThe Saint Louis University School of Social Work is established in 1930 and continually accredited since 1933, the school offers students and faculty an opportunity to follow the Jesuit tradition of being \"women and men for others.\"\n\nSLU's School of Social Work is fully accredited by the Council on Social Work Education (CSWE) – a recognition it has held since 1952.\n\n\"US News & World Report\" ranks the School among the top 25% in the nation.\n\nSLU's School of Social Work offers two undergraduate degrees, three master's, six dual-degree master's, one PhD, and two certificates.\n\nSaint Louis University's School of Social Work is located in Tegeler Hall, on the main Frost Campus in midtown St. Louis, Missouri.\n"}
{"id": "2264342", "url": "https://en.wikipedia.org/wiki?curid=2264342", "title": "Science Centre Singapore", "text": "Science Centre Singapore\n\nThe Science Centre Singapore (SCS, previously known as Singapore Science Centre) is a scientific institution in Jurong East, Singapore, specialising in the promotion of scientific and technological education for the general public. With over 850 exhibits spread over eight exhibition galleries, it sees over a million visitors a year, and over 25 million visitors up to the year 2003 when it celebrated its silver jubilee.\n\nThe Science Centre was carved out of the National Museum of Singapore as a separate institution so that the latter could focus on its artistic and historical collections. This idea was first mooted in 1969 by the Science Council of Singapore, and was subsequently approved by the Government which was keen to promote scientific education in the rapidly modernising country so as to tap into the technological sector.\nThe SCS building's architecture was decided by an architectural design competition organised by the Science Centre Board. Raymond Woo's entry was selected, and he was thus commissioned as the architect for the project. Built at a cost of S$12 million on a site in Jurong East, it was officially opened on 10 December 1977 by Dr. Toh Chin Chye, who was the Minister-in-charge of the Science Centre Board.\n\nIn 1987, the centre saw a significant expansion with the opening of Singapore's first and only OMNIMAX theatre, the Singapore Omni-Theatre. Costing $18 million, it has a 276-seat theatre underneath a tilted dome.\n\nIn 1999, a $38 million renovation expanded the centre's exhibition space, and created a new entrance as well as open-air exhibition areas and a direct connection to the separate Omni-Theatre building. In 2000, Snow City, a recreation of a environment in tropical Singapore, was set up beside the Omni-Theatre.\n\nOn 7 December 2007 in the year of its 30th anniversary, the Science Centre rebranded itself as the Science Centre Singapore (SCS).\n\nThe Science Centre Observatory is situated at , and is above mean sea level. It is one of the few observatories in the world located next to the equator. Its unique position allows constellations in both the northern and southern celestial hemispheres to be observed and thus opens up more vistas in the sky for observers. The Observatory is endowed with a range of sophisticated facilities as well as a classroom for astronomy lessons, slide shows and public talks.\n\nThe main telescope of the Observatory is a Cassegrain reflector of a combined focal length of . The sub-telescope is a apochromatic Kepler refractor with a focal length of . The equatorial mount for the telescopes was designed for Singapore's unique location; the accompanying English yoke provides the stability needed for the drive and tracking mechanisms. The stainless steel dome can be made to swivel in any direction and its shutter can be made to slide open for the telescope to be focused on to interesting objects in the sky.\n\nThe Observatory has been open to the public for stargazing sessions every Friday night since June 2006. The opening hours are from 7:50 to 10:00 pm. The Observatory can comfortably accommodate 50 visitors per session. It is important to note that stargazing through the observatory telescope is only possible when the sky is clear. However, regardless of weather conditions, the staff will be present.\n\nOn 4 April 2008, the Urban Redevelopment Authority announced plans to relocate the Science Centre next to Chinese Garden MRT Station in ten to 15 years. During his speech at the National Day Rally 2014, the prime minister of Singapore, Lee Hsien Loong said that by 2020, the Science Centre will be relocated to the north shore of Jurong Lake, beside Chinese Garden MRT Station.\n\n\n"}
{"id": "20685207", "url": "https://en.wikipedia.org/wiki?curid=20685207", "title": "Science Under Siege", "text": "Science Under Siege\n\nScience Under Siege: The Politicians' War on Nature and Truth is a 1998 book by journalist Todd Wilkinson. Wilkinson describes the careers of a variety of publicly employed scientists who, in the course of their work for government agencies, found habitat degradation, threatened species, or other decline in availability of a natural resource. When they expressed their views that certain activities must be scaled back or areas protected, they met with poor job performance ratings, hostility from their supervisors, transfers out of the region, and in many cases a severely damaged career. Science is \"under siege\" in these cases because many of the researchers were told to modify their scientific reports so that commercial uses or environmentally destructive activities could continue.\n\n"}
{"id": "8701228", "url": "https://en.wikipedia.org/wiki?curid=8701228", "title": "Shuttle Mission Simulator", "text": "Shuttle Mission Simulator\n\nThe Shuttle Mission Simulator (SMS) consisted of two simulators in Building 5 and one simulator in Building 35 of Johnson Space Center. The \"fixed-base simulators\" included high-fidelity mockups of the flight deck of a Space Shuttle, as well as a low-fidelity mockup of the middeck. The facility in Building 5 was known as the Fixed Base Simulator (FBS), while the facility in Building 35 was known as the GNS (an acronym for the original name, Guidance and Navigation Simulator). The \"motion-base simulator\" consisted of the forward part of the flight deck of the Space Shuttle. It utilized a six-axis hexapod motion system with an additional extended pitch axis to provide motion cuing for all phases of flight.\n\nThe Motion Base Simulator (MBS) provided crews with computer generated visual scenes out of the forward windows only, while the fixed-base simulators supplied forward, aft, and overhead window views. Simulation software modeled all Space Shuttle systems including many pre-programmed malfunctions, response to cockpit controls, and interactions between systems. Before a flight, astronauts logged many hours in these simulators. Instructor stations in the complex allowed simulator instructors to monitor and control student progress in the simulations, including the insertion of malfunctions. A central simulation control office monitored the health of the facility, scheduled its use, and responded to maintenance requests.\n\nDepending on training requirements, simulations were conducted with varying levels of interconnection with other simulators or control centers, each of which had a unique identifier used internally within the training and flight control divisions.\nThe less complex standalone sim was controlled by the instructors in the simulator instructor station, who also portrayed the flight controllers. A dedicated console area in the Mission Control Center, called the Simulation Control Area (or SCA), controlled simulation conduct during integrated activities while the instructors operated the simulator itself.\n\nAs the Space Shuttle Program ended in July 2011, all the simulators in the SMS complex were mothballed and prepared for removal and transport as excess NASA inventory throughout 2012. \n\n"}
{"id": "7327367", "url": "https://en.wikipedia.org/wiki?curid=7327367", "title": "Simon Plössl", "text": "Simon Plössl\n\nSimon Plössl (September 19, 1794, Vienna – January 29, 1868, Vienna) was an Austrian optical instrument maker. Initially trained at the Voigtländer company, he set up his own workshop in 1823. His major achievement at the time was the improvement of the achromatic microscope objective. Today he is best known for the eponymous Plössl telescope eyepiece, which follows his 1860 design, and is extensively used by amateur astronomers since the 1980s.\n\n\n"}
{"id": "4003020", "url": "https://en.wikipedia.org/wiki?curid=4003020", "title": "Sun-1", "text": "Sun-1\n\nSun-1 was the first generation of UNIX computer workstations and servers produced by Sun Microsystems, launched in May 1982. These were based on a CPU board designed by Andy Bechtolsheim while he was a graduate student at Stanford University and funded by DARPA. The Sun-1 systems ran SunOS 0.9, a port of UniSoft's UniPlus V7 port of Seventh Edition UNIX to the Motorola 68000 microprocessor, with no window system. Early Sun-1 workstations and servers used the original Sun logo, a series of red \"U\"s laid out in a square, rather than the more familiar purple diamond shape used later.\n\nThe first Sun-1 workstation was sold to Solo Systems in May 1982. The Sun-1/100 was used in the original Lucasfilm EditDroid non-linear editing system.\n\nThe Sun 1 workstation was based on the Stanford University SUN workstation designed by Andy Bechtolsheim (advised by Vaughan Pratt and Forrest Baskett), a graduate student and co-founder of Sun Microsystems. At the heart of this design were the Multibus CPU, memory, and video display cards. The cards used in the Sun-1 workstation were a second-generation design with a private memory bus allowing memory to be expanded to 2 MB without performance degradation.\n\nThe Sun 68000 board introduced in 1982 was a powerful single-board computer. It combined a 10 MHz Motorola 68000 microprocessor, a Sun designed memory management unit (MMU), 256 KB of zero wait state memory with parity, up to 32 KB of EPROM memory, two serial ports, a 16-bit parallel port and an Intel Multibus (IEEE 796 bus) interface in a single , Multibus form factor.\n\nBy using the Motorola 68000 processor tightly coupled with the Sun-1 MMU the Sun 68000 CPU board was able to support a multi-tasking operating system such as UNIX. It included an advanced Sun designed multi-process two-level memory management unit with facilities for memory protection, code sharing and demand paging of memory. The Sun-1 MMU was necessary because the Motorola 68451 MMU did not always work correctly with the 68000 and could not always restore the processor state after a page fault.\n\nThe CPU board included 256 KB of memory which could be replaced or augmented with two additional memory cards for a total of 2 MB. Although the memory cards used the Multibus form factor, they only used the Multibus interface for power; all memory access was via the smaller private \"P2\" bus. This was a synchronous private memory bus which allowed for simultaneous memory input/output transfers. It also allowed for full performance zero wait state operation of the memory. When installing the first 1 MB expansion board either the 256 Kb of memory on the CPU board or the first 256 KB on the expansion board had to be disabled.\n\nOn-board I/O included a dual serial port UART and a 16-bit parallel port. The serial ports were implemented with an Intel 8274 UART and later with a NEC D7201C UART. Serial port A was wired as a Data Communications Equipment (DCE) port and had full modem control. It was also the console port if no graphical display was installed in the system. Serial port B was wired as a Data Terminal Equipment (DTE) port and had no modem control. Both serial ports could also be used as terminal ports and quite often were allowing 3 people to use one workstation, although two did not have graphical displays. The 16-bit parallel port was a special purpose port for connecting 8-bit parallel port keyboard and 8-bit parallel port optical mouse for workstations with graphical displays. The parallel port was never used as a general purpose parallel printer port.\n\nThe CPU board included a fully compatible Multibus (IEEE 796 bus). It was an asynchronous bus that accommodated devices with various transfer rates while maintaining maximum throughput. It had 20 address lines so it could address up to 1 MB of Multibus memory and 1 MB of I/O locations although most I/O devices only decoded the first 64 KB of address space. The Sun CPU board fully supported multi-master functionality that allowed it to share the Multibus with other DMA devices.\n\nThe keyboard was a Micro Switch 103SD30-2, or a KeyTronic P2441 for the German market. The memory-mapped, bit-mapped frame buffer (graphics) board had a resolution of 1024×1024 pixels, but only 1024×800 was displayed on the monitor. The graphics board included hardware to accelerate raster operations. A Ball model HD17H 17-inch video display monitor was used. An Ethernet board was available, originally implementing the 3 Mbit/s Xerox PARC Ethernet specification, which was later upgraded to the 3Com 10 Mbit/s version. An Interphase SMD 2180 disk controller could be installed to connect up to four Fujitsu 84 MB M2313K or CDC 16.7 MB (8.35 MB fixed, 8.35 MB removable) 9455 Lark drives. All of the boards were installed in a 6 or 7-slot Multibus card cage.\n\nLater documentation shows that a 13- or 19-inch color display was available. The color frame buffer had a resolution of 640×512 pixels, with 640×480 displayed on the monitor. The board could display 256 colors from a palette of 16 million. ½-inch 9-track reel-to-reel tape drives and QIC-02 ¼-inch cartridge tape drives were also added to the offering.\n\nThere was also a second generation Sun-1 CPU board referred to as the Sun-1.5 CPU board.\n\nSun-1 systems upgraded with Sun-2 Multibus CPU boards were identified with a \"U\" suffix to their model number.\n\n\n"}
{"id": "2949345", "url": "https://en.wikipedia.org/wiki?curid=2949345", "title": "Sutton's law", "text": "Sutton's law\n\nSutton's law states that when diagnosing, one should first consider the obvious. It suggests that one should first conduct those tests which could confirm (or rule out) the most likely diagnosis. It is taught in medical schools to suggest to medical students that they might best order tests in that sequence which is most likely to result in a quick diagnosis, hence treatment, while minimizing unnecessary costs. It is also applied in pharmacology, when choosing a drug to treat a specific disease you want the drug to reach the disease. It is applicable to any process of diagnosis, e.g. debugging computer programs. Computer-aided diagnosis provides a statistical and quantitative approach. \n\nA more thorough analysis will consider the false positive rate of the test and the possibility that a less likely diagnosis might have more serious consequences. A competing principle is the idea of performing simple tests before more complex and expensive tests, moving from bedside tests to blood results and simple imaging such as ultrasound and then more complex such as MRI then specialty imaging. The law can also be applied in prioritizing tests when resources are limited, so a test for a treatable condition should be performed before an equally probable but less treatable condition.\n\nThe law is named after the bank robber Willie Sutton, who reputedly replied to a reporter's inquiry as to why he robbed banks by saying \"because that's where the money is.\" In Sutton's 1976 book \"Where the Money Was\", Sutton denies having said this. \n\nA similar idea is contained in the physician's adage, \"When you hear hoofbeats, think horses, not zebras.\"\n\n\n"}
{"id": "50767680", "url": "https://en.wikipedia.org/wiki?curid=50767680", "title": "The Know-How of Face Transplantation", "text": "The Know-How of Face Transplantation\n\n'The Know-How of Face Transplantation' is a college-level textbook edited by Maria Siemionow that is 493 pages long. It was published in 2011 by Springer Publishing. Siemionow performed the first near-full face transplant in the United States, which is still extremely rare. Siemionow says that \"30 have been done around the world.\"\n\nThe textbook \"The Know-How of Face Transplantation\" was written and published in \"2011\". The book is divided into six sections:\n\n\"The Know-How of Face Transplantation\" has 71 contributors from the medical, dental and other scientific fields. These contributors include:\n"}
{"id": "20074706", "url": "https://en.wikipedia.org/wiki?curid=20074706", "title": "The Trinity Paradox", "text": "The Trinity Paradox\n\nThe Trinity Paradox is a time travel novel by Kevin J. Anderson and Doug Beason, exploring the premise of an anti-nuclear activist from 1990s being transported back in time to the Manhattan Project, giving her the potential to sabotage the project in an attempt to prevent the development of nuclear weapons altogether.\n\nHer attempt to do so, however, has far-reaching and unpredictable results, changing the outcome of the Second World War and the face of the post-war world.\n\nDonald Erbschloe in his review for \"Physics Today\" said that \"the setting and the people of Los Alamos come alive\", \"the cast is impressive\" and \"the climax is thrilling and, as one might suspect, explosive\".\nJames P. Hogan noted that it \"soberingly shows the perils of the sheep solemnly pledging themselves to vegetarianism while the wolves remain unconverted\".\n\n"}
{"id": "214513", "url": "https://en.wikipedia.org/wiki?curid=214513", "title": "Transmission electron microscopy", "text": "Transmission electron microscopy\n\nTransmission electron microscopy (TEM, also sometimes conventional transmission electron microscopy or CTEM) is a microscopy technique in which a beam of electrons is transmitted through a specimen to form an image. The specimen is most often an ultrathin section less than 100 nm thick or a suspension on a grid. An image is formed from the interaction of the electrons with the sample as the beam is transmitted through the specimen. The image is then magnified and focused onto an imaging device, such as a fluorescent screen, a layer of photographic film, or a sensor such as a charge-coupled device.\n\nTransmission electron microscopes are capable of imaging at a significantly higher resolution than light microscopes, owing to the smaller de Broglie wavelength of electrons. This enables the instrument to capture fine detail—even as small as a single column of atoms, which is thousands of times smaller than a resolvable object seen in a light microscope. Transmission electron microscopy is a major analytical method in the physical, chemical and biological sciences. TEMs find application in cancer research, virology, and materials science as well as pollution, nanotechnology and semiconductor research.\n\nAt lower magnifications TEM image contrast is due to differential absorption of electrons by the material due to differences in composition or thickness of the material. At higher magnifications complex wave interactions modulate the intensity of the image, requiring expert analysis of observed images. Alternate modes of use allow for the TEM to observe modulations in chemical identity, crystal orientation, electronic structure and sample induced electron phase shift as well as the regular absorption based imaging.\n\nThe first TEM was demonstrated by Max Knoll and Ernst Ruska in 1931, with this group developing the first TEM with resolution greater than that of light in 1933 and the first commercial TEM in 1939. In 1986, Ruska was awarded the Nobel Prize in physics for the development of transmission electron microscopy.\n\nIn 1873, Ernst Abbe proposed that the ability to resolve detail in an object was limited approximately by the wavelength of the light used in imaging or a few hundred nanometers for visible light microscopes. Developments in ultraviolet (UV) microscopes, led by Köhler and Rohr, increased resolving power by a factor of two. However this required expensive quartz optics, due to the absorption of UV by glass. It was believed that obtaining an image with sub-micrometer information was not possible due to this wavelength constraint.\n\nIn 1858 Plücker observed the deflection of \"cathode rays\" (electrons) with the use of magnetic fields. This effect was used by Ferdinand Braun in 1897 to build simple cathode ray oscilloscopes (CROs) measuring devices. In 1891 Riecke noticed that the cathode rays could be focused by magnetic fields, allowing for simple electromagnetic lens designs. In 1926 Hans Busch published work extending this theory and showed that the lens maker's equation could, with appropriate assumptions, be applied to electrons.\n\nIn 1928, at the Technical University of Berlin, Adolf Matthias, Professor of High voltage Technology and Electrical Installations, appointed Max Knoll to lead a team of researchers to advance the CRO design. The team consisted of several PhD students including Ernst Ruska and Bodo von Borries. The research team worked on lens design and CRO column placement, to optimize parameters to construct better CROs, and make electron optical components to generate low magnification (nearly 1:1) images. In 1931 the group successfully generated magnified images of mesh grids placed over the anode aperture. The device used two magnetic lenses to achieve higher magnifications, arguably creating the first electron microscope. In that same year, Reinhold Rudenberg, the scientific director of the Siemens company, patented an electrostatic lens electron microscope.\n\nAt the time, electrons were understood to be charged particles of matter; the wave nature of electrons was not fully realized until the publication of the De Broglie hypothesis in 1927. The research group was unaware of this publication until 1932, when they quickly realized that the De Broglie wavelength of electrons was many orders of magnitude smaller than that for light, theoretically allowing for imaging at atomic scales. In April 1932, Ruska suggested the construction of a new electron microscope for direct imaging of specimens inserted into the microscope, rather than simple mesh grids or images of apertures. With this device successful diffraction and normal imaging of an aluminium sheet was achieved. However the magnification achievable was lower than with light microscopy. Magnifications higher than those available with a light microscope were achieved in September 1933 with images of cotton fibers quickly acquired before being damaged by the electron beam.\nAt this time, interest in the electron microscope had increased, with other groups, such as Paul Anderson and Kenneth Fitzsimmons of Washington State University, and Albert Prebus and James Hillier at the University of Toronto, who constructed the first TEMs in North America in 1935 and 1938, respectively, continually advancing TEM design.\n\nResearch continued on the electron microscope at Siemens in 1936, where the aim of the research was the development and improvement of TEM imaging properties, particularly with regard to biological specimens. At this time electron microscopes were being fabricated for specific groups, such as the \"EM1\" device used at the UK National Physical Laboratory. In 1939 the first commercial electron microscope, pictured, was installed in the Physics department of IG Farben-Werke. Further work on the electron microscope was hampered by the destruction of a new laboratory constructed at Siemens by an air-raid, as well as the death of two of the researchers, Heinz Müller and Friedrick Krause during World War II.\n\nAfter World War II, Ruska resumed work at Siemens, where he continued to develop the electron microscope, producing the first microscope with 100k magnification. The fundamental structure of this microscope design, with multi-stage beam preparation optics, is still used in modern microscopes. The worldwide electron microscopy community advanced with electron microscopes being manufactured in Manchester UK, the USA (RCA), Germany (Siemens) and Japan (JEOL). The first international conference in electron microscopy was in Delft in 1949, with more than one hundred attendees. Later conferences included the \"First\" international conference in Paris, 1950 and then in London in 1954.\n\nWith the development of TEM, the associated technique of scanning transmission electron microscopy (STEM) was re-investigated and did not become developed until the 1970s, with Albert Crewe at the University of Chicago developing the field emission gun and adding a high quality objective lens to create the modern STEM. Using this design, Crewe demonstrated the ability to image atoms using annular dark-field imaging. Crewe and coworkers at the University of Chicago developed the cold field electron emission source and built a STEM able to visualize single heavy atoms on thin carbon substrates. In 2008, Jannick Meyer et al. described the direct visualization of light atoms such as carbon and even hydrogen using TEM and a clean single-layer graphene substrate.\n\nTheoretically, the maximum resolution, \"d\", that one can obtain with a light microscope has been limited by the wavelength of the photons that are being used to probe the sample, λ, and the numerical aperture of the system, \"NA\".\n\nwhere n is the index of refraction of the medium in which the lens is working and α is the maximum half-angle of the cone of light that can enter the lens (see numerical aperture). Early twentieth century scientists theorized ways of getting around the limitations of the relatively large wavelength of visible light (wavelengths of 400–700 nanometers) by using electrons. Like all matter, electrons have both wave and particle properties (as theorized by Louis-Victor de Broglie), and their wave-like properties mean that a beam of electrons can be made to behave like a beam of electromagnetic radiation. The wavelength of electrons is related to their kinetic energy via the de Broglie equation. An additional correction must be made to account for relativistic effects, as in a TEM an electron's velocity approaches the speed of light, \"c\".\n\nwhere, \"h\" is Planck's constant, \"m\" is the rest mass of an electron and \"E\" is the energy of the accelerated electron. Electrons are usually generated in an electron microscope by a process known as thermionic emission from a filament, usually tungsten, in the same manner as a light bulb, or alternatively by field electron emission. The electrons are then accelerated by an electric potential (measured in volts) and focused by electrostatic and electromagnetic lenses onto the sample. The transmitted beam contains information about electron density, phase and periodicity; this beam is used to form an image.\n\nFrom the top down, the TEM consists of an emission source, which may be a tungsten filament or needle, or a lanthanum hexaboride (LaB) single crystal source. The gun is connected to a high voltage source (typically ~100–300 kV) and, given sufficient current, the gun will begin to emit electrons either by thermionic or field electron emission into the vacuum. The electron source is typically mounted in a Wehnelt cylinder to provide preliminary focus of the emitted electrons into a beam. The upper lenses of the TEM then further focus the electron beam to the desired size and location.\n\nManipulation of the electron beam is performed using two physical effects. The interaction of electrons with a magnetic field will cause electrons to move according to the left hand rule, thus allowing for electromagnets to manipulate the electron beam. The use of magnetic fields allows for the formation of a magnetic lens of variable focusing power, the lens shape originating due to the distribution of magnetic flux. Additionally, electrostatic fields can cause the electrons to be deflected through a constant angle. Coupling of two deflections in opposing directions with a small intermediate gap allows for the formation of a shift in the beam path, allowing for beam shifting in TEM, which is important for STEM. From these two effects, as well as the use of an electron imaging system, sufficient control over the beam path is possible for TEM operation. The optical configuration of a TEM can be rapidly changed, unlike that for an optical microscope, as lenses in the beam path can be enabled, have their strength changed, or be disabled entirely simply via rapid electrical switching, the speed of which is limited by effects such as the magnetic hysteresis of the lenses.\n\nThe lenses of a TEM allow for beam convergence, with the angle of convergence as a variable parameter, giving the TEM the ability to change magnification simply by modifying the amount of current that flows through the coil, quadrupole or hexapole lenses. The quadrupole lens is an arrangement of electromagnetic coils at the vertices of the square, enabling the generation of a lensing magnetic fields, the hexapole configuration simply enhances the lens symmetry by using six, rather than four coils.\n\nTypically a TEM consists of three stages of lensing. The stages are the condenser lenses, the objective lenses, and the projector lenses. The condenser lenses are responsible for primary beam formation, while the objective lenses focus the beam that comes through the sample itself (in STEM scanning mode, there are also objective lenses above the sample to make the incident electron beam convergent). The projector lenses are used to expand the beam onto the phosphor screen or other imaging device, such as film. The magnification of the TEM is due to the ratio of the distances between the specimen and the objective lens' image plane. Additional stigmators allow for the correction of asymmetrical beam distortions, known as astigmatism. It is noted that TEM optical configurations differ significantly with implementation, with manufacturers using custom lens configurations, such as in spherical aberration corrected instruments, or TEMs using energy filtering to correct electron chromatic aberration.\n\nImaging systems in a TEM consist of a phosphor screen, which may be made of fine (10–100 μm) particulate zinc sulfide, for direct observation by the operator, and, optionally, an image recording system such as film based or doped YAG screen coupled CCDs. Typically these devices can be removed or inserted into the beam path by the operator as required.\n\nA TEM is composed of several components, which include a vacuum system in which the electrons travel, an electron emission source for generation of the electron stream, a series of electromagnetic lenses, as well as electrostatic plates. The latter two allow the operator to guide and manipulate the beam as required. Also required is a device to allow the insertion into, motion within, and removal of specimens from the beam path. Imaging devices are subsequently used to create an image from the electrons that exit the system.\n\nTo increase the mean free path of the electron gas interaction, a standard TEM is evacuated to low pressures, typically on the order of 10 Pa. The need for this is twofold: first the allowance for the voltage difference between the cathode and the ground without generating an arc, and secondly to reduce the collision frequency of electrons with gas atoms to negligible levels—this effect is characterized by the mean free path. TEM components such as specimen holders and film cartridges must be routinely inserted or replaced requiring a system with the ability to re-evacuate on a regular basis. As such, TEMs are equipped with multiple pumping systems and airlocks and are not permanently vacuum sealed.\n\nThe vacuum system for evacuating a TEM to an operating pressure level consists of several stages. Initially, a low or roughing vacuum is achieved with either a rotary vane pump or diaphragm pumps setting a sufficiently low pressure to allow the operation of a turbo-molecular or diffusion pump establishing high vacuum level necessary for operations. To allow for the low vacuum pump to not require continuous operation, while continually operating the turbo-molecular pumps, the vacuum side of a low-pressure pump may be connected to chambers which accommodate the exhaust gases from the turbo-molecular pump. Sections of the TEM may be isolated by the use of pressure-limiting apertures to allow for different vacuum levels in specific areas such as a higher vacuum of 10 to 10 Pa or higher in the electron gun in high-resolution or field-emission TEMs.\n\nHigh-voltage TEMs require ultra-high vacuums on the range of 10 to 10 Pa to prevent the generation of an electrical arc, particularly at the TEM cathode. As such for higher voltage TEMs a third vacuum system may operate, with the gun isolated from the main chamber either by gate valves or a differential pumping aperture – a small hole that prevents the diffusion of gas molecules into the higher vacuum gun area faster than they can be pumped out. For these very low pressures, either an ion pump or a getter material is used.\n\nPoor vacuum in a TEM can cause several problems ranging from the deposition of gas inside the TEM onto the specimen while viewed in a process known as electron beam induced deposition to more severe cathode damages caused by electrical discharge. The use of a cold trap to adsorb sublimated gases in the vicinity of the specimen largely eliminates vacuum problems that are caused by specimen sublimation.\n\nTEM specimen stage designs include airlocks to allow for insertion of the specimen holder into the vacuum with minimal loss of vacuum in other areas of the microscope. The specimen holders hold a standard size of sample grid or self-supporting specimen. Standard TEM grid sizes are 3.05 mm diameter, with a thickness and mesh size ranging from a few to 100 μm. The sample is placed onto the meshed area having a diameter of approximately 2.5 mm. Usual grid materials are copper, molybdenum, gold or platinum. This grid is placed into the sample holder, which is paired with the specimen stage. A wide variety of designs of stages and holders exist, depending upon the type of experiment being performed. In addition to 3.05 mm grids, 2.3 mm grids are sometimes, if rarely, used. These grids were particularly used in the mineral sciences where a large degree of tilt can be required and where specimen material may be extremely rare. Electron transparent specimens have a thickness usually less than 100 nm, but this value depends on the accelerating voltage.\n\nOnce inserted into a TEM, the sample has to be manipulated to locate the region of interest to the beam, such as in single grain diffraction, in a specific orientation. To accommodate this, the TEM stage allows movement of the sample in the XY plane, Z height adjustment, and commonly a single tilt direction parallel to the axis of side entry bolders. Sample rotation may be available on specialized diffraction holders and stages. Some modern TEMs provide the ability for two orthogonal tilt angles of movement with specialized holder designs called double-tilt sample holders. Some stage designs, such as top-entry or vertical insertion stages once common for high resolution TEM studies, may simply only have X-Y translation available. The design criteria of TEM stages are complex, owing to the simultaneous requirements of mechanical and electron-optical constraints and specialized models are available for different methods.\n\nA TEM stage is required to have the ability to hold a specimen and be manipulated to bring the region of interest into the path of the electron beam. As the TEM can operate over a wide range of magnifications, the stage must simultaneously be highly resistant to mechanical drift, with drift requirements as low as a few nm/minute while being able to move several μm/minute, with repositioning accuracy on the order of nanometers. Earlier designs of TEM accomplished this with a complex set of mechanical downgearing devices, allowing the operator to finely control the motion of the stage by several rotating rods. Modern devices may use electrical stage designs, using screw gearing in concert with stepper motors, providing the operator with a computer-based stage input, such as a joystick or trackball.\n\nTwo main designs for stages in a TEM exist, the side-entry and top entry version. Each design must accommodate the matching holder to allow for specimen insertion without either damaging delicate TEM optics or allowing gas into TEM systems under vacuum.\n\nInsertion procedures for side-entry TEM holders typically involve the rotation of the sample to trigger micro switches that initiate evacuation of the airlock before the sample is inserted into the TEM column.\n\nThe second design is the top-entry holder consists of a cartridge that is several cm long with a bore drilled down the cartridge axis. The specimen is loaded into the bore, possibly using a small screw ring to hold the sample in place. This cartridge is inserted into an airlock with the bore perpendicular to the TEM optic axis. When sealed, the airlock is manipulated to push the cartridge such that the cartridge falls into place, where the bore hole becomes aligned with the beam axis, such that the beam travels down the cartridge bore and into the specimen. Such designs are typically unable to be tilted without blocking the beam path or interfering with the objective lens.\n\nThe electron gun is formed from several components: the filament, a biasing circuit, a Wehnelt cap, and an extraction anode. By connecting the filament to the negative component power supply, electrons can be \"pumped\" from the electron gun to the anode plate, and the TEM column, thus completing the circuit. The gun is designed to create a beam of electrons exiting from the assembly at some given angle, known as the gun divergence semi-angle, α. By constructing the Wehnelt cylinder such that it has a higher negative charge than the filament itself, electrons that exit the filament in a diverging manner are, under proper operation, forced into a converging pattern the minimum size of which is the gun crossover diameter.\n\nThe thermionic emission current density, \"J\", can be related to the work function of the emitting material via Richardson's law\nwhere \"A\" is the Richardson's constant, Φ is the work function and T is the temperature of the material.\n\nThis equation shows that in order to achieve sufficient current density it is necessary to heat the emitter, taking care not to cause damage by application of excessive heat. For this reason materials with either a high melting point, such as tungsten, or those with a low work function (LaB) are required for the gun filament. Furthermore, both lanthanum hexaboride and tungsten thermionic sources must be heated in order to achieve thermionic emission, this can be achieved by the use of a small resistive strip. To prevent thermal shock, there is often a delay enforced in the application of current to the tip, to prevent thermal gradients from damaging the filament, the delay is usually a few seconds for LaB, and significantly lower for tungsten.\n\nElectron lenses are designed to act in a manner emulating that of an optical lens, by focusing parallel electrons at some constant focal distance. Electron lenses may operate electrostatically or magnetically. The majority of electron lenses for TEM use electromagnetic coils to generate a convex lens. The field produced for the lens must be radially symmetrical, as deviation from the radial symmetry of the magnetic lens causes aberrations such as astigmatism, and worsens spherical and chromatic aberration. Electron lenses are manufactured from iron, iron-cobalt or nickel cobalt alloys, such as permalloy. These are selected for their magnetic properties, such as magnetic saturation, hysteresis and permeability.\n\nThe components include the yoke, the magnetic coil, the poles, the polepiece, and the external control circuitry. The pole piece must be manufactured in a very symmetrical manner, as this provides the boundary conditions for the magnetic field that forms the lens. Imperfections in the manufacture of the pole piece can induce severe distortions in the magnetic field symmetry, which induce distortions that will ultimately limit the lenses' ability to reproduce the object plane. The exact dimensions of the gap, pole piece internal diameter and taper, as well as the overall design of the lens is often performed by finite element analysis of the magnetic field, whilst considering the thermal and electrical constraints of the design.\n\nThe coils which produce the magnetic field are located within the lens yoke. The coils can contain a variable current, but typically use high voltages, and therefore require significant insulation in order to prevent short-circuiting the lens components. Thermal distributors are placed to ensure the extraction of the heat generated by the energy lost to resistance of the coil windings. The windings may be water-cooled, using a chilled water supply in order to facilitate the removal of the high thermal duty.\n\nApertures are annular metallic plates, through which electrons that are further than a fixed distance from the optic axis may be excluded. These consist of a small metallic disc that is sufficiently thick to prevent electrons from passing through the disc, whilst permitting axial electrons. This permission of central electrons in a TEM causes two effects simultaneously: firstly, apertures decrease the beam intensity as electrons are filtered from the beam, which may be desired in the case of beam sensitive samples. Secondly, this filtering removes electrons that are scattered to high angles, which may be due to unwanted processes such as spherical or chromatic aberration, or due to diffraction from interaction within the sample.\n\nApertures are either a fixed aperture within the column, such as at the condenser lens, or are a movable aperture, which can be inserted or withdrawn from the beam path, or moved in the plane perpendicular to the beam path. Aperture assemblies are mechanical devices which allow for the selection of different aperture sizes, which may be used by the operator to trade off intensity and the filtering effect of the aperture. Aperture assemblies are often equipped with micrometers to move the aperture, required during optical calibration.\n\nImaging methods in TEM use the information contained in the electron waves exiting from the sample to form an image. The projector lenses allow for the correct positioning of this electron wave distribution onto the viewing system. The observed intensity, \"I\", of the image, assuming sufficiently high quality of imaging device, can be approximated as proportional to the time-averaged amplitude of the electron wavefunctions, where the wave that forms the exit beam is denoted by Ψ.\n\nDifferent imaging methods therefore attempt to modify the electron waves exiting the sample in a way that provides information about the sample, or the beam itself. From the previous equation, it can be deduced that the observed image depends not only on the amplitude of beam, but also on the phase of the electrons, although phase effects may often be ignored at lower magnifications. Higher resolution imaging requires thinner samples and higher energies of incident electrons, which means that the sample can no longer be considered to be absorbing electrons (i.e., via a Beer's law effect). Instead, the sample can be modeled as an object that does not change the amplitude of the incoming electron wave function, but instead modifies the phase of the incoming wave; in this model, the sample is known as a pure phase object. For sufficiently thin specimens, phase effects dominate the image, complicating analysis of the observed intensities. To improve the contrast in the image, the TEM may be operated at a slight defocus to enhance contrast, owing to convolution by the contrast transfer function of the TEM, which would normally decrease contrast if the sample was not a weak phase object.\nFigure on the right shows the two basic operation modes of TEM – imaging and diffraction modes. In both cases specimen is illuminated with the parallel beam, formed by electron beam shaping with the system of Condenser lenses and Condenser aperture. After interaction with the sample, on the exit surface of the specimen two types of electrons exist – unscattered (which will correspond to the bright central beam on the diffraction pattern) and scattered electrons (which change their trajectories due to interaction with the material).\n\nIn Imaging mode, Objective aperture is inserted in a back focal plane (BFP) of Objective lens (where diffraction spots are formed). If using the Objective aperture, the central beam is selected (the rest signal is blocked) and the bright field image (BF image) is obtained. If we allow the signal from the diffracted beam, the dark field image (DF image) is received. Then selected signal is magnified and projected on a screen (or on a camera) with the help of Intermediate and Projector lenses. Image of the sample is received.\n\nIn Diffraction mode, Selected area aperture is used to determine the specimen area from which the signal will be displayed. By changing the strength of Intermediate lens, the Diffraction pattern is projected on a screen. Diffraction is a very powerful tool for doing a cell reconstruction and crystal orientation determination.\n\nThe contrast between two adjacent areas in a TEM image can be defined as the difference in the electron densities in image plane. Due to the scattering of the incident beam by the sample, the amplitude and phase of the electron wave change, which results in amplitude contrast and phase contrast, correspondingly. Most of images have both contrast components.\n\nAmplitude–contrast is obtained due to removal of some electrons before the image plane. During their interaction with the specimen some of electrons will be lost due to absorption, or due to scattering at very high angles beyond the physical limitation of microscope or are blocked by the objective aperture. While the first two losses are due to the specimen and microscope construction, the objective aperture can be used by operator to enhance the contrast.\n\nFigure on the right shows a TEM image (a) and the corresponding diffraction pattern (b) of Pt polycrystalline film taken without an objective aperture. In order to enhance the contrast in the TEM image the number of scattered beams as visible in the diffraction pattern should be reduced. This can be done by selecting a certain area in the diffraction plane like only the central beam or a diffracted beam, or combinations of beams with objective aperture to form BF (c) in case the central beam is included or DF (d-e) images in case the central beam is blocked. DF images (d-e) are obtained using the diffracted beams indicated in diffraction pattern with circles (b). Grains from which electrons are scattered into these diffraction spots appear brighter. More details about diffraction contrast formation are given further.\n\nThere are two types of amplitude contrast – mass–thickness and diffraction contrast. First, let’s consider mass–thickness contrast. When the beam illuminates two neighbouring areas with low mass (or thickness) and high mass (or thickness), the heavier region scatters electrons at bigger angles. These strongly scattered electrons are blocked in BF TEM mode by objective aperture. As a result, heavier regions appear darker in BF images (have low intensity). It should be noted, that mass–thickness contrast is most important for non–crystalline, amorphous materials.\n\nDiffraction contrast occurs due to a specific crystallographic orientation of a grain. In such a case the crystal is in a so-called Bragg condition, whereby atomic planes are oriented in a way that there is a high probability of scattering. Thus diffraction contrast provides information on the orientation of the crystals in a polycrystalline sample. Note that in case diffraction contrast exists, the contrast cannot be interpreted as due to mass or thickness variations.\n\nSamples can exhibit diffraction contrast, whereby the electron beam undergoes Bragg scattering, which in the case of a crystalline sample, disperses electrons into discrete locations in the back focal plane. By the placement of apertures in the back focal plane, i.e. the objective aperture, the desired Bragg reflections can be selected (or excluded), thus only parts of the sample that are causing the electrons to scatter to the selected reflections will end up projected onto the imaging apparatus.\n\nIf the reflections that are selected do not include the unscattered beam (which will appear up at the focal point of the lens), then the image will appear dark wherever no sample scattering to the selected peak is present, as such a region without a specimen will appear dark. This is known as a dark-field image.\n\nModern TEMs are often equipped with specimen holders that allow the user to tilt the specimen to a range of angles in order to obtain specific diffraction conditions, and apertures placed above the specimen allow the user to select electrons that would otherwise be diffracted in a particular direction from entering the specimen.\n\nApplications for this method include the identification of lattice defects in crystals. By carefully selecting the orientation of the sample, it is possible not just to determine the position of defects but also to determine the type of defect present. If the sample is oriented so that one particular plane is only slightly tilted away from the strongest diffracting angle (known as the Bragg Angle), any distortion of the crystal plane that locally tilts the plane to the Bragg angle will produce particularly strong contrast variations. However, defects that produce only displacement of atoms that do not tilt the crystal to the Bragg angle (i. e. displacements parallel to the crystal plane) will not produce strong contrast.\n\nCrystal structure can also be investigated by high-resolution transmission electron microscopy (HRTEM), also known as phase contrast. When using a field emission source and a specimen of uniform thickness, the images are formed due to differences in phase of electron waves, which is caused by specimen interaction. Image formation is given by the complex modulus of the incoming electron beams. As such, the image is not only dependent on the number of electrons hitting the screen, making direct interpretation of phase contrast images more complex. However this effect can be used to an advantage, as it can be manipulated to provide more information about the sample, such as in complex phase retrieval techniques.\n\nAs previously stated, by adjusting the magnetic lenses such that the back focal plane of the lens rather than the imaging plane is placed on the imaging apparatus a diffraction pattern can be generated. For thin crystalline samples, this produces an image that consists of a pattern of dots in the case of a single crystal, or a series of rings in the case of a polycrystalline or amorphous solid material. For the single crystal case the diffraction pattern is dependent upon the orientation of the specimen and the structure of the sample illuminated by the electron beam. This image provides the investigator with information about the space group symmetries in the crystal and the crystal's orientation to the beam path. This is typically done without using any information but the position at which the diffraction spots appear and the observed image symmetries.\n\nDiffraction patterns can have a large dynamic range, and for crystalline samples, may have intensities greater than those recordable by CCD. As such, TEMs may still be equipped with film cartridges for the purpose of obtaining these images, as the film is a single use detector.\nAnalysis of diffraction patterns beyond point-position can be complex, as the image is sensitive to a number of factors such as specimen thickness and orientation, objective lens defocus, spherical and chromatic aberration. Although quantitative interpretation of the contrast shown in lattice images is possible, it is inherently complicated and can require extensive computer simulation and analysis, such as electron multislice analysis.\n\nMore complex behaviour in the diffraction plane is also possible, with phenomena such as Kikuchi lines arising from multiple diffraction within the crystalline lattice. In convergent beam electron diffraction (CBED) where a non-parallel, i.e. converging, electron wavefront is produced by concentrating the electron beam into a fine probe at the sample surface, the interaction of the convergent beam can provide information beyond structural data such as sample thickness.\n\nUsing the advanced technique of electron energy loss spectroscopy (EELS), for TEMs appropriately equipped, electrons can be separated into a spectrum based upon their velocity (which is closely related to their kinetic energy, and thus energy loss from the beam energy), using magnetic sector based devices known as EEL spectrometers. These devices allow for the selection of particular energy values, which can be associated with the way the electron has interacted with the sample. For example, different elements in a sample result in different electron energies in the beam after the sample. This normally results in chromatic aberration – however this effect can, for example, be used to generate an image which provides information on elemental composition, based upon the atomic transition during electron-electron interaction.\n\nEELS spectrometers can often be operated in both spectroscopic and imaging modes, allowing for isolation or rejection of elastically scattered beams. As for many images inelastic scattering will include information that may not be of interest to the investigator thus reducing observable signals of interest, EELS imaging can be used to enhance contrast in observed images, including both bright field and diffraction, by rejecting unwanted components.\n\nAs TEM specimen holders typically allow for the rotation of a sample by a desired angle, multiple views of the same specimen can be obtained by rotating the angle of the sample along an axis perpendicular to the beam. By taking multiple images of a single TEM sample at differing angles, typically in 1° increments, a set of images known as a \"tilt series\" can be collected. This methodology was proposed in the 1970s by Walter Hoppe. Under purely absorption contrast conditions, this set of images can be used to construct a three-dimensional representation of the sample.\n\nThe reconstruction is accomplished by a two-step process, first images are aligned to account for errors in the positioning of a sample; such errors can occur due to vibration or mechanical drift. Alignment methods use image registration algorithms, such as autocorrelation methods to correct these errors. Secondly, using a reconstruction algorithm, such as filtered back projection, the aligned image slices can be transformed from a set of two-dimensional images, \"I\"(\"x\", \"y\"), to a single three-dimensional image, \"I\"<nowiki>'</nowiki>(\"x\", \"y\", \"z\"). This three-dimensional image is of particular interest when morphological information is required, further study can be undertaken using computer algorithms, such as isosurfaces and data slicing to analyse the data.\n\nAs TEM samples cannot typically be viewed at a full 180° rotation, the observed images typically suffer from a \"missing wedge\" of data, which when using Fourier-based back projection methods decreases the range of resolvable frequencies in the three-dimensional reconstruction. Mechanical refinements, such as multi-axis tilting (two tilt series of the same specimen made at orthogonal directions) and conical tomography (where the specimen is first tilted to a given fixed angle and then imaged at equal angular rotational increments through one complete rotation in the plane of the specimen grid) can be used to limit the impact of the missing data on the observed specimen morphology. Using focused ion beam milling, a new technique has been proposed which uses pillar-shaped specimen and a dedicated on-axis tomography holder to perform 180° rotation of the sample inside the pole piece of the objective lens in TEM. Using such arrangements, quantitative electron tomography without the missing wedge is possible. In addition, numerical techniques exist which can improve the collected data.\n\nAll the above-mentioned methods involve recording tilt series of a given specimen field. This inevitably results in the summation of a high dose of reactive electrons through the sample and the accompanying destruction of fine detail during recording. The technique of low-dose (minimal-dose) imaging is therefore regularly applied to mitigate this effect. Low-dose imaging is performed by deflecting illumination and imaging regions simultaneously away from the optical axis to image an adjacent region to the area to be recorded (the high-dose region). This area is maintained centered during tilting and refocused before recording. During recording the deflections are removed so that the area of interest is exposed to the electron beam only for the duration required for imaging. An improvement of this technique (for objects resting on a sloping substrate film) is to have two symmetrical off-axis regions for focusing followed by setting focus to the average of the two high-dose focus values before recording the low-dose area of interest.\n\nNon-tomographic variants on this method, referred to as single particle analysis, use images of multiple (hopefully) identical objects at different orientations to produce the image data required for three-dimensional reconstruction. If the objects do not have significant preferred orientations, this method does not suffer from the missing data wedge (or cone) which accompany tomographic methods nor does it incur excessive radiation dosage, however it assumes that the different objects imaged can be treated as if the 3D data generated from them arose from a single stable object.\n\nSample preparation in TEM can be a complex procedure. TEM specimens should be less than 100 nanometers thick for a conventional TEM. Unlike neutron or X-Ray radiation the electrons in the beam interact readily with the sample, an effect that increases roughly with atomic number squared (Z). High quality samples will have a thickness that is comparable to the mean free path of the electrons that travel through the samples, which may be only a few tens of nanometers. Preparation of TEM specimens is specific to the material under analysis and the type of information to be obtained from the specimen.\n\nMaterials that have dimensions small enough to be electron transparent, such as powdered substances, small organisms, viruses, or nanotubes, can be quickly prepared by the deposition of a dilute sample containing the specimen onto films on support grids. Biological specimens may be embedded in resin to withstand the high vacuum in the sample chamber and to enable cutting tissue into electron transparent thin sections. The biological sample can be stained using either a negative staining material such as uranyl acetate for bacteria and viruses, or, in the case of embedded sections, the specimen may be stained with heavy metals, including osmium tetroxide. Alternately samples may be held at liquid nitrogen temperatures after embedding in vitreous ice. In material science and metallurgy the specimens can usually withstand the high vacuum, but still must be prepared as a thin foil, or etched so some portion of the specimen is thin enough for the beam to penetrate. Constraints on the thickness of the material may be limited by the scattering cross-section of the atoms from which the material is comprised.\n\nBiological tissue is often embedded in a resin block then thinned to less than 100 nm on an ultramicrotome. The resin block is fractured as it passes over a glass or diamond knife edge. This method is used to obtain thin, minimally deformed samples that allow for the observation of tissue ultrastructure. Inorganic samples, such as aluminium, may also be embedded in resins and ultrathin sectioned in this way, using either coated glass, sapphire or larger angle diamond knives. To prevent charge build-up at the sample surface when viewing in the TEM, tissue samples need to be coated with a thin layer of conducting material, such as carbon.\n\nTEM samples of biological tissues need high atomic number stains to enhance contrast. The stain absorbs the beam electrons or scatters part of the electron beam which otherwise is projected onto the imaging system. Compounds of heavy metals such as osmium, lead, uranium or gold (in immunogold labelling) may be used prior to TEM observation to selectively deposit electron dense atoms in or on the sample in desired cellular or protein region. This process requires an understanding of how heavy metals bind to specific biological tissues and cellular structures.\n\nMechanical polishing is also used to prepare samples for imaging on the TEM. Polishing needs to be done to a high quality, to ensure constant sample thickness across the region of interest. A diamond, or cubic boron nitride polishing compound may be used in the final stages of polishing to remove any scratches that may cause contrast fluctuations due to varying sample thickness. Even after careful mechanical milling, additional fine methods such as ion etching may be required to perform final stage thinning.\n\nCertain samples may be prepared by chemical etching, particularly metallic specimens. These samples are thinned using a chemical etchant, such as an acid, to prepare the sample for TEM observation. Devices to control the thinning process may allow the operator to control either the voltage or current passing through the specimen, and may include systems to detect when the sample has been thinned to a sufficient level of optical transparency.\n\nIon etching is a sputtering process that can remove very fine quantities of material. This is used to perform a finishing polish of specimens polished by other means. Ion etching uses an inert gas passed through an electric field to generate a plasma stream that is directed to the sample surface. Acceleration energies for gases such as argon are typically a few kilovolts. The sample may be rotated to promote even polishing of the sample surface. The sputtering rate of such methods is on the order of tens of micrometers per hour, limiting the method to only extremely fine polishing.\n\nIon etching by argon gas has been recently shown to be able to file down MTJ stack structures to a specific layer which has then been atomically resolved. The TEM images taken in plan view rather than cross-section reveal that the MgO layer within MTJs contains a large number of grain boundaries that may be diminishing the properties of devices.\n\nMore recently focused ion beam methods have been used to prepare samples. FIB is a relatively new technique to prepare thin samples for TEM examination from larger specimens. Because FIB can be used to micro-machine samples very precisely, it is possible to mill very thin membranes from a specific area of interest in a sample, such as a semiconductor or metal. Unlike inert gas ion sputtering, FIB makes use of significantly more energetic gallium ions and may alter the composition or structure of the material through gallium implantation.\n\nSamples may also be replicated using cellulose acetate film, the film subsequently coated with a heavy metal such as platinum, the original film dissolved away, and the replica imaged on the TEM. Variations of the replica technique are used for both materials and biological samples. In materials science a common use is for examining the fresh fracture surface of metal alloys.\n\nThe capabilities of the TEM can be further extended by additional stages and detectors, sometimes incorporated on the same microscope.\n\nA TEM can be modified into a scanning transmission electron microscope (STEM) by the addition of a system that rasters the beam across the sample to form the image, combined with suitable detectors. Scanning coils are used to deflect the beam, such as by an electrostatic shift of the beam, where the beam is then collected using a current detector such as a Faraday cup, which acts as a direct electron counter. By correlating the electron count to the position of the scanning beam (known as the \"probe\"), the transmitted component of the beam may be measured. The non-transmitted components may be obtained either by beam tilting or by the use of annular dark field detectors.\n\nA low-voltage electron microscope (LVEM) is operated at relatively low electron accelerating voltage between 5–25 kV. Some of these can be a combination of SEM, TEM and STEM in a single compact instrument. Low voltage increases image contrast which is especially important for biological specimens. This increase in contrast significantly reduces, or even eliminates the need to stain. Resolutions of a few nm are possible in TEM, SEM and STEM modes. The low energy of the electron beam means that permanent magnets can be used as lenses and thus a miniature column that does not require cooling can be used.\n\nMain article: Transmission electron cryomicroscopy\n\nCryogenic transmission electron microscopy (Cryo-TEM) uses a TEM with a specimen holder capable of maintaining the specimen at liquid nitrogen or liquid helium temperatures. This allows imaging specimens prepared in vitreous ice, the preferred preparation technique for imaging individual molecules or macromolecular assemblies, imaging of vitrified solid-electrolye interfaces, and imaging of materials that are volatile in high vacuum at room temperature, such as sulfur.\n\nIn-situ experiments may also be conducted in TEM using differentially pumped sample chambers, or specialized holders. Types of in-situ experiments include studying nanomaterials, biological specimens, and chemical reactions using liquid-phase electron microscopy, and material deformation testing.\n\nModern research TEMs may include aberration correctors, to reduce the amount of distortion in the image. Incident beam monochromators may also be used which reduce the energy spread of the incident electron beam to less than 0.15 eV. Major aberration corrected TEM manufacturers include JEOL, Hitachi High-technologies, FEI Company, and NION.\n\nThere are a number of drawbacks to the TEM technique. Many materials require extensive sample preparation to produce a sample thin enough to be electron transparent, which makes TEM analysis a relatively time consuming process with a low throughput of samples. The structure of the sample may also be changed during the preparation process. Also the field of view is relatively small, raising the possibility that the region analyzed may not be characteristic of the whole sample. There is potential that the sample may be damaged by the electron beam, particularly in the case of biological materials.\n\nThe limit of resolution obtainable in a TEM may be described in several ways, and is typically referred to as the information limit of the microscope. One commonly used value is a cut-off value of the contrast transfer function, a function that is usually quoted in the frequency domain to define the reproduction of spatial frequencies of objects in the object plane by the microscope optics. A cut-off frequency, \"q\", for the transfer function may be approximated with the following equation, where C is the spherical aberration coefficient and λ is the electron wavelength:\n\nFor a 200 kV microscope, with partly corrected spherical aberrations (\"to the third order\") and a C value of 1 µm, a theoretical cut-off value might be 1/\"q\" = 42 pm. The same microscope without a corrector would have C = 0.5 mm and thus a 200-pm cut-off. The spherical aberrations are suppressed to the third or fifth order in the \"aberration-corrected\" microscopes. Their resolution is however limited by electron source geometry and brightness and chromatic aberrations in the objective lens system.\n\nThe frequency domain representation of the contrast transfer function may often have an oscillatory nature, which can be tuned by adjusting the focal value of the objective lens. This oscillatory nature implies that some spatial frequencies are faithfully imaged by the microscope, whilst others are suppressed. By combining multiple images with different spatial frequencies, the use of techniques such as focal series reconstruction can be used to improve the resolution of the TEM in a limited manner. The contrast transfer function can, to some extent, be experimentally approximated through techniques such as Fourier transforming images of amorphous material, such as amorphous carbon.\n\nMore recently, advances in aberration corrector design have been able to reduce spherical aberrations and to achieve resolution below 0.5 Ångströms (50 pm) at magnifications above 50 million times. Improved resolution allows for the imaging of lighter atoms that scatter electrons less efficiently, such as lithium atoms in lithium battery materials. The ability to determine the position of atoms within materials has made the HRTEM an indispensable tool for nanotechnology research and development in many fields, including heterogeneous catalysis and the development of semiconductor devices for electronics and photonics.\n\n"}
{"id": "1252386", "url": "https://en.wikipedia.org/wiki?curid=1252386", "title": "Transmitter Hamburg-Billstedt", "text": "Transmitter Hamburg-Billstedt\n\nThe Transmitter Hamburg-Billstedt is a broadcasting facility in Hamburg-Billstedt, established in 1934. It is owned and operated by the Norddeutscher Rundfunk public broadcasting service, but open to competitors, too.\n\nFrom 1934 to 1949 it used as transmission aerial a wire hung up in a tower of wood. This tower had until 1941 a height of 145 metres. In 1941 its height was reduced to 84.5 metres and in 1949 it was demolished.\n\nIn 1940 a second aerial in form of a triangle area aerial was built. This aerial allowing transmitting on a wide frequency range was demolished in the Fifties.\nIn 1949/50 a 198 metre high guyed steelframework mast with a cage aerial and a transmission aerial for FM and TV on its top was erected. From this mast, which was partly destroyed by a storm during its erection in December 1949, between 1953 and 1962 the programme of the \"Deutschen Langwellensender\" (German longwave transmitter) was broadcast.\nThis programme was transmitted in a special modulation mode, the compatible single sideband modulation, allowing smaller bandwidth and the possibility of reception with conventional AM receivers.\nBecause this mast was under high voltage the aerials for FM and TV on its top were fed via a Goubau line.\n\nIn the first half of the 1960s this aerial mast was demounted and the current installation built. It consists of:\n\n\nSince 1967, the University of Hamburg has been using the 304 m-mast as a six-level meteorological measurement platform, with thermometers, hygrometers, and anemometers mounted at various heights up to 280 m above ground. The atmospheric variables are sampled at a high temporal resolution to allow computation of boundary layer turbulent fluxes of heat and momentum. Live data and time series are also made available via the World Wide Web. \n\nIn 1934, shortly after the inauguration of the facility, some owners of recreational gardens discovered that a light bulb connected to ground and a highly spun wire made the bulb light up brightly enough to illuminate a small house. Later many other did so. Transmission energy is taken from the transmitter, and induces electrical power in the wire. This effect was not discovered immediately. Later, technicians of the transmitter noticed, that in the houses of nearby gardens, lights went on and off depending on whether the transmitter was switched on or off. This phenomenon got the nickname \"Hamburger Lichtwunder\" \"(German: \"Hamburg's light miracle\")\". After this was discovered, use of transmitting energy of radio transmitters for other purposes than radio reception was prohibited in Germany by law.\n\ntechnic3d.com, Literature \"Wunder der Wellen\", Author: Eduard Rhein\n\n\n\n"}
{"id": "55052879", "url": "https://en.wikipedia.org/wiki?curid=55052879", "title": "Vitaly Napadow", "text": "Vitaly Napadow\n\nVitaly Napadow is a Ukrainian-born American neuroscientist and acupuncturist. He is associate professor at Harvard Medical School and the Martinos Center for Biomedical Imaging. He is also the director of the Center for Integrative Pain NeuroImaging and the co-president of the Society for Acupuncture Research. He is known for researching acupuncture and its effects on the brain. He has also researched the effects of nausea on brain activation, and differences in resting state brain connectivity associated with the intensity of spontaneous fibromyalgia pain.\n\nNapadow received his master's degree in acupuncture from the New England School of Acupuncture in 2002 and his Ph.D. from the Harvard–MIT Program of Health Sciences and Technology in 2001. He joined the faculty of Harvard Medical School in 2004 as an instructor in radiology, where he became an assistant professor of anesthesiology in 2010 and an associate professor of radiology in 2014.\n\n"}
