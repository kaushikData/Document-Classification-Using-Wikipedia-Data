{"id": "5259052", "url": "https://en.wikipedia.org/wiki?curid=5259052", "title": "A Briefer History of Time (Schulman book)", "text": "A Briefer History of Time (Schulman book)\n\nA Briefer History of Time is a science humor book by the American astronomer Eric Schulman. In this book, Schulman presents humorous summaries of what he claims are the fifty-three most important events since the beginning of time.\nThe title and cover are a parody of Stephen Hawking's book \"A Brief History of Time\". Coincidentally, Hawking would later write a \"sequel\" entitled \"A Briefer History of Time\". Hawking's publisher Bantam Books was aware the title had already been used in a popular science book, but went ahead since \"The other book was published six years ago, and Professor Hawking is an international figure.\"\nIn 2004 the author released the book under a creative commons license, CC BY-NC-ND 1.0, as free download on his website.\n\nLaughing while learning is the intent of Schulman's book. The book shows why, even though the Universe is expanding, it doesn't get any easier to find a parking space. Furthermore, there is the pulp version of the origin of life (\"It was a dark and stormy night. In the shallow tide pool, a nucleic acid base collided with a sugar molecule. An amino acid sank beneath the murky depths . . . .\").\n"}
{"id": "15541504", "url": "https://en.wikipedia.org/wiki?curid=15541504", "title": "Aleksandr Arbuzov", "text": "Aleksandr Arbuzov\n\nAleksandr Erminingeldovich Arbuzov (12 October 1877 – 22 January 1968) was a Russian Empire and Soviet chemist who discovered the Michaelis–Arbuzov reaction.\n\nA native of Bilyarsk, Arbuzov studied in the Kazan University under Alexander Mikhaylovich Zaytsev. He graduated in 1900 and became professor at the same university in 1911. After World War II he was put in charge of the Soviet Institute of Organic Chemistry.\n\nArbuzov was awarded the Stalin Prize in 1943.\n\nIn addition to his scientific research, Arbuzov also wrote \"A Brief Sktech of the Development of Organic Chemistry in Russian\" (1948).\n\n"}
{"id": "28119069", "url": "https://en.wikipedia.org/wiki?curid=28119069", "title": "Alexander Strauch", "text": "Alexander Strauch\n\nAlexander Strauch (born 1 March 1832 in Saint Petersburg – died 14 August 1893 in Wiesbaden, Germany) was a Russian naturalist, most notably an herpetologist.\n\nIn 1861 he started work as a curator of the zoological museum at the Imperial Academy of Sciences in St. Petersburg. From 1879 to 1890 he was director of the museum. He is credited with establishing St. Petersburg as a major world center in regard to herpetology.\n\nTaxa with the specific epithet of \"strauchi\" or \"strauchii\" commemorate his name, five examples being:\n\n\"Nota bene\": A Taxon author in parentheses indicates that the species was originally described in a different genus.\n\nHis zoologist author abbreviation is Strauch.\n\n"}
{"id": "2741589", "url": "https://en.wikipedia.org/wiki?curid=2741589", "title": "Algodonite", "text": "Algodonite\n\nAlgodonite is a copper arsenide mineral with formula: CuAs. It is a gray white metallic mineral crystallizing in the hexagonal system. It has a Mohs hardness of 4 and a specific gravity of 8.38 - 8.72.\n\nIt was first described in 1857 from the Algodones silver mine, Coquimbo, Chile.\n"}
{"id": "58833146", "url": "https://en.wikipedia.org/wiki?curid=58833146", "title": "Alice (spacecraft instrument)", "text": "Alice (spacecraft instrument)\n\nAlice is an ultraviolet imaging spectrometer for spacecraft, with one used on the \"New Horizons\" spacecraft, and another on the \"Rosetta\" spacecraft. Alice is a small telescope with spectrograph and a special detector with 32 pixels each with 1024 spectral channels detecting ultraviolet light.\n\nAlice uses an array of potassium bromide and caesium iodide type photocathodes. It detects in the extreme and far ultraviolet spectrum, from wavelengths of light. \n\nAlice is intended, among its capabilities, to detect ultraviolet signatures of noble (aka inert) gases including helium, neon, argon, and krypton. Alice should also be able to detect water, carbon monoxide, and carbon dioxide in the ultraviolet. \n\nALICE was built and operated by the Southwest Research Institute for NASA's Jet Propulsion Laboratory.\n\nIn August 2018, NASA confirmed, based on results by \"Alice\" on the \"New Horizons\" spacecraft, the detection of a \"hydrogen wall\" at the outer edges of the Solar System that was first detected in 1992 by the two Voyager spacecraft which have detected a surplus of ultraviolet light determined to be coming from hydrogen.\n\nThe \"New Horizons\" version of Alice uses an average power of 4.4 watts and weighs 4.5 kg (9.9 pounds).\n\nOn \"Rosetta\", a mission to a comet, ALICE performed ultraviolet spectroscopy to search and quantify the noble gas content in the comet nucleus. \n\nOn \"Rosetta\" it is an instrument which uses 2.9 watts. \n\n"}
{"id": "42681375", "url": "https://en.wikipedia.org/wiki?curid=42681375", "title": "Anaerolinea thermolimosa", "text": "Anaerolinea thermolimosa\n\nAnaerolinea thermolimosa is a thermophilic, non-spore-forming, non-motile, Gram-negative, filamentous bacteria with type strain IMO-1 (=JCM 12577 =DSM 16554).\n\n\n"}
{"id": "2839", "url": "https://en.wikipedia.org/wiki?curid=2839", "title": "Angular momentum", "text": "Angular momentum\n\nIn physics, angular momentum (rarely, moment of momentum or rotational momentum) is the rotational equivalent of linear momentum. It is an important quantity in physics because it is a conserved quantity—the total angular momentum of a system remains constant unless acted on by an external torque.\n\nIn three dimensions, the angular momentum for a point particle is a pseudovector r × p, the cross product of the particle's position vector r (relative to some origin) and its momentum vector p = \"m\"v. This definition can be applied to each point in continua like solids or fluids, or physical fields. Unlike momentum, angular momentum does depend on where the origin is chosen, since the particle's position is measured from it. The angular momentum vector of a point particle is parallel and directly proportional to the angular velocity vector ω of the particle (how fast its angular position changes), where the constant of proportionality depends on both the mass of the particle and its distance from origin. For continuous rigid bodies, though, the spin angular velocity ω is proportional but not always parallel to the spin angular momentum of the object, making the constant of proportionality I (called the moment of inertia) a second-rank tensor rather than a scalar.\n\nAngular momentum is additive; the total angular momentum of a system is the (pseudo)vector sum of the angular momenta. For continua or fields one uses integration. The total angular momentum of any rigid body can be split into the sum of two main components: the angular momentum of the centre of mass (with a mass equal to the total mass) about the origin, plus the spin angular momentum of the object about the centre of mass.\n\nTorque can be defined as the rate of change of angular momentum, analogous to force. The conservation of angular momentum helps explain many observed phenomena, for example the increase in rotational speed of a spinning figure skater as the skater's arms are contracted, the high rotational rates of neutron stars, the Coriolis effect, and precession of tops and gyroscopes. Applications include the gyrocompass, control moment gyroscope, inertial guidance systems, reaction wheels, flying discs or Frisbees, and Earth's rotation to name a few. In general, conservation does limit the possible motion of a system, but does not uniquely determine what the exact motion is.\n\nIn quantum mechanics, angular momentum is an operator with quantized eigenvalues. Angular momentum is subject to the Heisenberg uncertainty principle, meaning that at any time, only one component can be measured with definite precision; the other two cannot. Also, the \"spin\" of elementary particles does not correspond to literal spinning motion.\n\nAngular momentum is a vector quantity (more precisely, a pseudovector) that represents the product of a body's rotational inertia and rotational velocity about a particular axis. However, if the particle's trajectory lies in a single plane, it is sufficient to discard the vector nature of angular momentum, and treat it as a scalar (more precisely, a pseudoscalar). Angular momentum can be considered a rotational analog of linear momentum. Thus, where linear momentum formula_1 is proportional to mass formula_2 and linear speed \n\nangular momentum formula_4 is proportional to moment of inertia formula_5 and angular speed formula_6,\n\nUnlike mass, which depends only on amount of matter, moment of inertia is also dependent on the position of the axis of rotation and the shape of the matter. Unlike linear speed, which occurs in a straight line, angular speed occurs about a center of rotation. Therefore, strictly speaking, formula_4 should be referred to as the angular momentum \"relative to that center\".\n\nBecause formula_9 for a single particle and formula_10 for circular motion, angular momentum can be expanded, formula_11 and reduced to,\n\nthe product of the radius of rotation formula_13 and the linear momentum of the particle formula_14, where formula_15 in this case is the equivalent linear (tangential) speed at the radius (formula_16).\n\nThis simple analysis can also apply to non-circular motion if only the component of the motion which is perpendicular to the radius vector is considered. In that case,\n\nwhere formula_18 is the perpendicular component of the motion. Expanding, formula_19 rearranging, formula_20 and reducing, angular momentum can also be expressed,\n\nwhere formula_22 is the length of the \"moment arm\", a line dropped perpendicularly from the origin onto the path of the particle. It is this definition, to which the term \"moment of momentum\" refers.\n\nAnother approach is to define angular momentum as the conjugate momentum (also called canonical momentum) of the angular coordinate formula_23 expressed in the Lagrangian of the mechanical system. Consider a mechanical system with a mass formula_2 constrained to move in a circle of radius formula_25 in the absence of any external force field. The kinetic energy of the system is\n\nAnd the potential energy is\n\nThen the Lagrangian is\n\nThe \"generalized momentum\" \"canonically conjugate to\" the coordinate formula_23 is defined by\n\nTo completely define angular momentum in three dimensions, it is required to know the angle swept out in unit time, the direction perpendicular to the instantaneous plane of angular displacement, and the sense (right- or left-handed) of the angular velocity, as well as the mass involved. By retaining this vector nature of angular momentum, the general nature of the equations is also retained, and can describe any sort of three-dimensional motion about the center of rotation – circular, linear, or otherwise. In vector notation, the angular momentum of a point particle in motion about the origin is defined as:\n\nwhere\n\nThis can be expanded, reduced, and by the rules of vector algebra, rearranged:\n\nwhich is the cross product of the position vector formula_34 and the linear momentum formula_40 of the particle. By the definition of the cross product, the formula_41 vector is perpendicular to both formula_34 and formula_43. It is directed perpendicular to the plane of angular displacement, as indicated by the right-hand rule – so that the angular velocity is seen as counter-clockwise from the head of the vector. Conversely, the formula_41 vector defines the plane in which formula_34 and formula_43 lie.\n\nBy defining a unit vector formula_47 perpendicular to the plane of angular displacement, a scalar angular speed formula_6 results, where\n\nThe two-dimensional scalar equations of the previous section can thus be given direction:\n\nand formula_53 for circular motion, where all of the motion is perpendicular to the radius formula_13.\n\nAngular momentum can be described as the rotational analog of linear momentum. Like linear momentum it involves elements of mass and displacement. Unlike linear momentum it also involves elements of position and shape.\n\nMany problems in physics involve matter in motion about some certain point in space, be it in actual rotation about it, or simply moving past it, where it is desired to know what effect the moving matter has on the point — can it exert energy upon it or perform work about it? Energy, the ability to do work, can be stored in matter by setting it in motion — a combination of its inertia and its displacement. Inertia is measured by its mass, and displacement by its velocity. Their product,\n\nis the matter's momentum. Referring this momentum to a central point introduces a complication: the momentum is not applied to the point directly. For instance, a particle of matter at the outer edge of a wheel is, in effect, at the end of a lever of the same length as the wheel's radius, its momentum turning the lever about the center point. This imaginary lever is known as the \"moment arm\". It has the effect of multiplying the momentum's effort in proportion to its length, an effect known as a \"moment\". Hence, the particle's momentum referred to a particular point,\n\nis the \"angular momentum\", sometimes called, as here, the \"moment of momentum\" of the particle versus that particular center point. The equation formula_57 combines a moment (a mass formula_2 turning moment arm formula_13) with a linear (straight-line equivalent) speed formula_15. Linear speed referred to the central point is simply the product of the distance formula_13 and the angular speed formula_6 versus the point: formula_63 another moment. Hence, angular momentum contains a double moment: formula_64 Simplifying slightly, formula_65 the quantity formula_66 is the particle's moment of inertia, sometimes called the second moment of mass. It is a measure of rotational inertia.\n\nBecause rotational inertia is a part of angular momentum, it necessarily includes all of the complications of moment of inertia, which is calculated by multiplying elementary bits of the mass by the squares of their distances from the center of rotation. Therefore, the total moment of inertia, and the angular momentum, is a complex function of the configuration of the matter about the center of rotation and the orientation of the rotation for the various bits.\n\nFor a rigid body, for instance a wheel or an asteroid, the orientation of rotation is simply the position of the rotation axis versus the matter of the body. It may or may not pass through the center of mass, or it may lie completely outside of the body. For the same body, angular momentum may take a different value for every possible axis about which rotation may take place. It reaches a minimum when the axis passes through the center of mass.\n\nFor a collection of objects revolving about a center, for instance all of the bodies of the Solar System, the orientations may be somewhat organized, as is the Solar System, with most of the bodies' axes lying close to the system's axis. Their orientations may also be completely random.\n\nIn brief, the more mass and the farther it is from the center of rotation (the longer the moment arm), the greater the moment of inertia, and therefore the greater the angular momentum for a given angular velocity. In many cases the moment of inertia, and hence the angular momentum, can be simplified by,\nSimilarly, for a point mass formula_2 the moment of inertia is defined as,\nand for any collection of particles formula_73 as the sum,\n\nAngular momentum's dependence on position and shape is reflected in its units versus linear momentum: kg·m/s, N·m·s or J·s for angular momentum versus kg·m/s or N·s for linear momentum. Angular momentum's units can be interpreted as torque·seconds, work·seconds, or energy·seconds. An object with angular momentum of can be reduced to zero rotation (all of the energy can be transferred out of it) by an angular impulse of or equivalently, by torque or work of for one second, or energy of for one second.\n\nThe plane perpendicular to the axis of angular momentum and passing through the center of mass is sometimes called the \"invariable plane\", because the direction of the axis remains fixed if only the interactions of the bodies within the system, free from outside influences, are considered. One such plane is the invariable plane of the Solar System.\n\nNewton's second law of motion can be expressed mathematically,\nor force = mass × acceleration. The rotational equivalent for point particles is\nBecause angular acceleration is the time derivative of angular velocity, and because the moment of inertia is formula_77 for point particles, the above formula is equivalent to formula_78 Rearranging into a form suitable for integration, formula_79 and formula_80 and integrating with respect to time,\nTherefore, a torque acting over time is equivalent to a change in angular momentum, known as \"angular impulse\", by analogy with impulse, which is defined as the change in translational momentum. The constant can be interpreted as the initial angular momentum of the body, before the torque began to act. In particular, if torque formula_82 then angular momentum formula_83 That is, if no torque acts upon a body, then its angular momentum remains constant. Conversely,\nor Angular momentum = moment of inertia × angular velocity, and its time derivative is\n\nBecause the moment of inertia is formula_77, it follows that formula_87, and formula_88 which, as above, reduces to\n\nTherefore, the time rate of change of angular momentum about a particular center of rotation is equivalent to applied torque about that center. If angular momentum is constant, formula_90 and no torque is applied.\n\nA rotational analog of Newton's third law of motion might be written, \"In a closed system, no torque can be exerted on any matter without the exertion on some other matter of an equal and opposite torque.\" Hence, \"angular momentum can be exchanged between objects in a closed system, but total angular momentum before and after an exchange remains constant (is conserved).\"\n\nSimilarly, a rotational analog of Newton's second law of motion might be, \"A change in angular momentum is proportional to the applied torque and occurs about the same axis as that torque.\" Since a torque applied over time is equivalent to a change in angular momentum, then if torque is zero, angular momentum is constant. As above, a system with constant angular momentum is a closed system. Therefore, \"requiring the system to be closed is equivalent to requiring that no external influence, in the form of a torque, acts upon it.\"\n\nA rotational analog of Newton's first law of motion might be written, \"A body continues in a state of rest or of uniform rotation unless acted by an external torque.\" Thus \"with no external influence to act upon it, the original angular momentum of the system is conserved\".\n\nThe conservation of angular momentum is used in analyzing \"central force motion\". If the net force on some body is directed always toward some point, the \"center\", then there is no torque on the body with respect to the center, as all of the force is directed along the radius vector, and none is perpendicular to the radius. Mathematically, torque formula_91 because in this case formula_34 and formula_93 are parallel vectors. Therefore, the angular momentum of the body about the center is constant. This is the case with gravitational attraction in the orbits of planets and satellites, where the gravitational force is always directed toward the primary body and orbiting bodies conserve angular momentum by exchanging distance and velocity as they move about the primary. Central force motion is also used in the analysis of the Bohr model of the atom.\n\nFor a planet, angular momentum is distributed between the spin of the planet and its revolution in its orbit, and these are often exchanged by various mechanisms. The conservation of angular momentum in the Earth–Moon system results in the transfer of angular momentum from Earth to Moon, due to tidal torque the Moon exerts on the Earth. This in turn results in the slowing down of the rotation rate of Earth, at about 65.7 nanoseconds per day, and in gradual increase of the radius of Moon's orbit, at about 3.82 centimeters per year.\n\nThe conservation of angular momentum explains the angular acceleration of an ice skater as she brings her arms and legs close to the vertical axis of rotation. By bringing part of the mass of her body closer to the axis she decreases her body's moment of inertia. Because angular momentum is the product of moment of inertia and angular velocity, if the angular momentum remains constant (is conserved), then the angular velocity (rotational speed) of the skater must increase.\n\nThe same phenomenon results in extremely fast spin of compact stars (like white dwarfs, neutron stars and black holes) when they are formed out of much larger and slower rotating stars. Decrease in the size of an object \"n\" times results in increase of its angular velocity by the factor of \"n\".\n\nConservation is not always a full explanation for the dynamics of a system but is a key constraint. For example, a spinning top is subject to gravitational torque making it lean over and change the angular momentum about the nutation axis, but neglecting friction at the point of spinning contact, it has a conserved angular momentum about its spinning axis, and another about its precession axis. Also, in any planetary system, the planets, star(s), comets, and asteroids can all move in numerous complicated ways, but only so that the angular momentum of the system is conserved.\n\nNoether's theorem states that every conservation law is associated with a symmetry (invariant) of the underlying physics. The symmetry associated with conservation of angular momentum is rotational invariance. The fact that the physics of a system is unchanged if it is rotated by any angle about an axis implies that angular momentum is conserved.\n\nIn astrodynamics and celestial mechanics, a \"massless\" (or \"per unit mass\") angular momentum is defined\ncalled \"specific angular momentum\". Note that formula_95 Mass is often unimportant in orbital mechanics calculations, because motion is defined by gravity. The primary body of the system is often so much larger than any bodies in motion about it that the smaller bodies have a negligible gravitational effect on it; it is, in effect, stationary. All bodies are apparently attracted by its gravity in the same way, regardless of mass, and therefore all move approximately the same way under the same conditions.\n\nFor a continuous mass distribution with density function \"ρ\"(r), a differential volume element \"dV\" with position vector r within the mass has a mass element \"dm\" = \"ρ\"(r)\"dV\". Therefore, the infinitesimal angular momentum of this element is:\n\nand integrating this differential over the volume of the entire mass gives its total angular momentum:\n\nIn the derivation which follows, integrals similar to this can replace the sums for the case of continuous mass.\n\nFor a collection of particles in motion about an arbitrary origin, it is informative to develop the equation of angular momentum by resolving their motion into components about their own center of mass and about the origin. Given,\n\nThe total mass of the particles is simply their sum,\n\nThe position vector of the center of mass is defined by,\n\nBy inspection,\n\nThe total angular momentum of the collection of particles is the sum of the angular momentum of each particle,\nExpanding formula_100,\n\nExpanding formula_102,\n\nIt can be shown that (see sidebar),\ntherefore the second and third terms vanish,\n\nThe first term can be rearranged,\n\nand total angular momentum for the collection of particles is finally,\nThe first term is the angular momentum of the center of mass relative to the origin. Similar to Single particle, below, it is the angular momentum of one particle of mass \"M\" at the center of mass moving with velocity V. The second term is the angular momentum of the particles moving relative to the center of mass, similar to Fixed center of mass, below. The result is general — the motion of the particles is not restricted to rotation or revolution about the origin or center of mass. The particles need not be individual masses, but can be elements of a continuous distribution, such as a solid body.\n\nRearranging equation () by vector identities, multiplying both terms by \"one\", and grouping appropriately,\n\ngives the total angular momentum of the system of particles in terms of moment of inertia formula_5 and angular velocity formula_124,\n\nIn the case of a single particle moving about the arbitrary origin,\n\nFor the case of the center of mass fixed in space with respect to the origin,\n\nIn modern (20th century) theoretical physics, angular momentum (not including any intrinsic angular momentum – see below) is described using a different formalism, instead of a classical pseudovector. In this formalism, angular momentum is the 2-form Noether charge associated with rotational invariance. As a result, angular momentum is not conserved for general curved spacetimes, unless it happens to be asymptotically rotationally invariant.\n\nIn classical mechanics, the angular momentum of a particle can be reinterpreted as a plane element:\n\nin which the exterior product ∧ replaces the cross product × (these products have similar characteristics but are nonequivalent). This has the advantage of a clearer geometric interpretation as a plane element, defined from the x and p vectors, and the expression is true in any number of dimensions (two or higher). In Cartesian coordinates:\n\nor more compactly in index notation:\n\nThe angular velocity can also be defined as an antisymmetric second order tensor, with components \"ω\". The relation between the two antisymmetric tensors is given by the moment of inertia which must now be a fourth order tensor:\n\nAgain, this equation in L and ω as tensors is true in any number of dimensions. This equation also appears in the geometric algebra formalism, in which L and ω are bivectors, and the moment of inertia is a mapping between them.\n\nIn relativistic mechanics, the relativistic angular momentum of a particle is expressed as an antisymmetric tensor of second order:\n\nin the language of four-vectors, namely the four position \"X\" and the four momentum \"P\", and absorbs the above L together with the motion of the centre of mass of the particle.\n\nIn each of the above cases, for a system of particles, the total angular momentum is just the sum of the individual particle angular momenta, and the centre of mass is for the system.\n\nAngular momentum in quantum mechanics differs in many profound respects from angular momentum in classical mechanics. In relativistic quantum mechanics, it differs even more, in which the above relativistic definition becomes a tensorial operator.\n\nThe classical definition of angular momentum as formula_141 can be carried over to quantum mechanics, by reinterpreting r as the quantum position operator and p as the quantum momentum operator. L is then an operator, specifically called the \"orbital angular momentum operator\". The components of the angular momentum operator satisfy the commutation relations of the Lie algebra so(3). Indeed, these operators are precisely the infinitesimal action of the rotation group on the quantum Hilbert space. (See also the discussion below of the angular momentum operators as the generators of rotations.)\n\nHowever, in quantum physics, there is another type of angular momentum, called \"spin angular momentum\", represented by the spin operator S. Almost all elementary particles have spin. Spin is often depicted as a particle literally spinning around an axis, but this is a misleading and inaccurate picture: spin is an intrinsic property of a particle, unrelated to any sort of motion in space and fundamentally different from orbital angular momentum. All elementary particles have a characteristic spin, for example electrons have \"spin 1/2\" (this actually means \"spin ħ/2\") while photons have \"spin 1\" (this actually means \"spin ħ\").\n\nFinally, there is total angular momentum J, which combines both the spin and orbital angular momentum of all particles and fields. (For one particle, J = L + S.) Conservation of angular momentum applies to J, but not to L or S; for example, the spin–orbit interaction allows angular momentum to transfer back and forth between L and S, with the total remaining constant. Electrons and photons need not have integer-based values for total angular momentum, but can also have fractional values.\n\nIn quantum mechanics, angular momentum is quantized – that is, it cannot vary continuously, but only in \"quantum leaps\" between certain allowed values. For any system, the following restrictions on measurement results apply, where formula_142 is the reduced Planck constant and formula_143 is any Euclidean vector such as x, y, or z:\n\nThe reduced Planck constant formula_142 is tiny by everyday standards, about 10 J s, and therefore this quantization does not noticeably affect the angular momentum of macroscopic objects. However, it is very important in the microscopic world. For example, the structure of electron shells and subshells in chemistry is significantly affected by the quantization of angular momentum.\n\nQuantization of angular momentum was first postulated by Niels Bohr in his Bohr model of the atom and was later predicted by Erwin Schrödinger in his Schrödinger equation.\n\nIn the definition formula_145, six operators are involved: The position operators formula_146, formula_147, formula_148, and the momentum operators formula_149, formula_150, formula_151. However, the Heisenberg uncertainty principle tells us that it is not possible for all six of these quantities to be known simultaneously with arbitrary precision. Therefore, there are limits to what can be known or measured about a particle's angular momentum. It turns out that the best that one can do is to simultaneously measure both the angular momentum vector's magnitude and its component along one axis.\n\nThe uncertainty is closely related to the fact that different components of an angular momentum operator do not commute, for example formula_152. (For the precise commutation relations, see angular momentum operator.)\n\nAs mentioned above, orbital angular momentum L is defined as in classical mechanics: formula_145, but \"total\" angular momentum J is defined in a different, more basic way: J is defined as the \"generator of rotations\". More specifically, J is defined so that the operator\nis the rotation operator that takes any system and rotates it by angle formula_23 about the axis formula_156. (The \"exp\" in the formula refers to operator exponential) To put this the other way around, whatever our quantum Hilbert space is, we expect that the rotation group SO(3) will act on it. There is then an associated action of the Lie algebra so(3) of SO(3); the operators describing the action of so(3) on our Hilbert space are the (total) angular momentum operators.\n\nThe relationship between the angular momentum operator and the rotation operators is the same as the relationship between Lie algebras and Lie groups in mathematics. The close relationship between angular momentum and rotations is reflected in Noether's theorem that proves that angular momentum is conserved whenever the laws of physics are rotationally invariant.\n\nWhen describing the motion of a charged particle in an electromagnetic field, the canonical momentum P (derived from the Lagrangian for this system) is not gauge invariant. As a consequence, the canonical angular momentum L = r × P is not gauge invariant either. Instead, the momentum that is physical, the so-called \"kinetic momentum\" (used throughout this article), is (in SI units)\n\nwhere \"e\" is the electric charge of the particle and A the magnetic vector potential of the electromagnetic field. The gauge-invariant angular momentum, that is \"kinetic angular momentum\", is given by\n\nThe interplay with quantum mechanics is discussed further in the article on canonical commutation relations.\n\nIn \"classical Maxwell electrodynamics\" the Poynting vector\nis a linear momentum density of electromagnetic field.\n\nThe angular momentum density vector formula_160 is given by a vector product\nas in classical mechanics:\n\nThe above identities are valid \"locally\", i.e. in each space point formula_34 in a given moment formula_163.\n\nNewton, in the \"Principia\", hinted at angular momentum in his examples of the First Law of Motion,\n\nHe did not further investigate angular momentum directly in the \"Principia\",\nHowever, his geometric proof of the law of areas is an outstanding example of Newton's genius, and indirectly proves angular momentum conservation in the case of a central force.\n\nAs a planet orbits the Sun, the line between the Sun and the planet sweeps out equal areas in equal intervals of time. This had been known since Kepler expounded his second law of planetary motion. Newton derived a unique geometric proof, and went on to show that the attractive force of the Sun's gravity was the cause of all of Kepler's laws.\n\nDuring the first interval of time, an object is in motion from point A to point B. Undisturbed, it would continue to point c during the second interval. When the object arrives at B, it receives an impulse directed toward point S. The impulse gives it a small added velocity toward S, such that if this were its only velocity, it would move from B to V during the second interval. By the rules of velocity composition, these two velocities add, and point C is found by construction of parallelogram BcCV. Thus the object's path is deflected by the impulse so that it arrives at point C at the end of the second interval. Because the triangles SBc and SBC have the same base SB and the same height Bc or VC, they have the same area. By symmetry, triangle SBc also has the same area as triangle SAB, therefore the object has swept out equal areas SAB and SBC in equal times.\n\nAt point C, the object receives another impulse toward S, again deflecting its path during the third interval from d to D. Thus it continues to E and beyond, the triangles SAB, SBc, SBC, SCd, SCD, SDe, SDE all having the same area. Allowing the time intervals to become ever smaller, the path ABCDE approaches indefinitely close to a continuous curve.\n\nNote that because this derivation is geometric, and no specific force is applied, it proves a more general law than Kepler's second law of planetary motion. It shows that the Law of Areas applies to any central force, attractive or repulsive, continuous or non-continuous, or zero.\n\nThe proportionality of angular momentum to the area swept out by a moving object can be understood by realizing that the bases of the triangles, that is, the lines from S to the object, are equivalent to the radius, and that the heights of the triangles are proportional to the perpendicular component of velocity. Hence, if the area swept per unit time is constant, then by the triangular area formula , the product and therefore the product are constant: if and the base length are decreased, and height must increase proportionally. Mass is constant, therefore angular momentum is conserved by this exchange of distance and velocity.\n\nIn the case of triangle SBC, area is equal to (SB)(VC). Wherever C is eventually located due to the impulse applied at B, the product (SB)(VC), and therefore remain constant. Similarly so for each of the triangles.\n\nLeonhard Euler, Daniel Bernoulli, and Patrick d'Arcy all understood angular momentum in terms of conservation of areal velocity, a result of their analysis of Kepler's second law of planetary motion. It is unlikely that they realized the implications for ordinary rotating matter.\n\nIn 1736 Euler, like Newton, touched on some of the equations of angular momentum in his \"Mechanica\" without further developing them.\n\nBernoulli wrote in a 1744 letter of a \"moment of rotational motion\", possibly the first conception of angular momentum as we now understand it.\n\nIn 1799, Pierre-Simon Laplace first realized that a fixed plane was associated with rotation — his \"invariable plane\".\n\nLouis Poinsot in 1803 began representing rotations as a line segment perpendicular to the rotation, and elaborated on the \"conservation of moments\".\n\nIn 1852 Léon Foucault used a gyroscope in an experiment to display the Earth's rotation.\n\nWilliam J. M. Rankine's 1858 \"Manual of Applied Mechanics\" defined angular momentum in the modern sense for the first time:\n\nIn an 1872 edition of the same book, Rankine stated that \"The term \"angular momentum\" was introduced by Mr. Hayward,\" probably referring to R.B. Hayward's article \"On a Direct Method of estimating Velocities, Accelerations, and all similar Quantities with respect to Axes moveable in any manner in Space with Applications,\" which was introduced in 1856, and published in 1864. Rankine was mistaken, as numerous publications feature the term starting in the late 18th to early 19th centuries. However, Hayward's article apparently was the first use of the term and the concept seen by much of the English-speaking world. Before this, angular momentum was typically referred to as \"momentum of rotation\" in English.\n\n\n"}
{"id": "2465964", "url": "https://en.wikipedia.org/wiki?curid=2465964", "title": "Bibliogram", "text": "Bibliogram\n\nA bibliogram is a verbal construct made when noun phrases from extended stretches of text are ranked high to low by their frequency of co-occurrence with one or more user-supplied seed terms. Each bibliogram has three components:\n\n\nThe term was introduced in 2005 by Howard D. White to name the linguistic object studied, but not previously named, in informetrics, scientometrics and bibliometrics. The noun phrases in the ranking may be authors, journals, subject headings, or other indexing terms. The \"stretches of text” may be a book, a set of related articles, a subject bibliography, a set of Web pages, and so on. Bibliograms are always generated from writings, usually from scholarly or scientific literature.\n\nAs a family of term-frequency distributions, the bibliogram has frequently been written about under descriptions such as:\n\n\nIt is sometimes called a \"core and scatter\" distribution. The \"core\" consists of relatively few top-ranked terms that account for a disproportionately large share of co-occurrences overall.\n\nThe \"scatter” consists of relatively many lower-ranked terms that account for the remaining share of co-occurrences. Usually the top-ranked terms are not tied in frequency, but identical frequencies and tied ranks become more common as the frequencies get smaller. At the bottom of the distribution, a long tail of terms are tied in rank because each co-occurs with the seed term only once.\n\nIn most cases bibliograms can be described by power laws such as Zipf's law and Bradford's law. In this regard, they have long been studied by mathematicians and statisticians in information science. However, these treatments typically ignore the qualitative meanings of the ranked terms themselves, which are often of interest in their own right. For example, the following bibliogram was made with an author's name as seed and shows the descriptors that co-occur with her name in the ERIC database. The descriptors are ranked by how many of her articles they were used to index:\n\nThis author is a researcher in education, and it will be seen that the terms profile her intellectual interests over the years. In general, bibliograms can be used to:\n\n\nBibliograms can be created with the RANK command on Dialog (other vendors have similar commands), ranking options within WorldCat, HistCite, Google Scholar, and inexpensive content analysis software.\n\nWhite suggests that bibliograms have a parallel construct in what he calls \"associograms\". These are the rank-ordered lists of word association norms studied in psycholinguistics. They are similar to bibliograms in statistical structure but are not generated from writings. Rather, they are generated by presenting panels of people with a stimulus term (which functions like a seed term) and tabulating the words they associate with the seed by frequency of co-occurrence. They are currently of interest to information scientists as a nonstandard way of creating thesauri for document retrieval.\n\nOther examples of bibliograms are the ordered set of an author's co-authors or the list of authors that are published in a specific journal together with their number of articles. A popular example is the list of additional titles to consider for purchase that you get when you search an item in Amazon. These suggested titles are the top terms in the \"core\" of a bibliogram formed with your search term as seed. The frequencies are counts of the times they have been co-purchased with the seed.\n\nExamples of associagrams may be found in the Edinburgh Associative Thesaurus.\n\nSimilar but different methods are used in data clustering and data mining. Google Sets does also create list of associated terms to a given set of terms.\n\n\n"}
{"id": "29071169", "url": "https://en.wikipedia.org/wiki?curid=29071169", "title": "Blair Glacier", "text": "Blair Glacier\n\nBlair Glacier () is a glacier draining northward to the western corner of Maury Bay. It was delineated from aerial photographs taken by U.S. Navy Operation Highjump (1946–47), and named by the Advisory Committee on Antarctic Names for James L. Blair, Midshipman on the sloop \"Peacock\" during the United States Exploring Expedition (1838–42) under Lieutenant Charles Wilkes.\n\n"}
{"id": "1266561", "url": "https://en.wikipedia.org/wiki?curid=1266561", "title": "Cold frame", "text": "Cold frame\n\nIn agriculture and gardening, a cold frame is a transparent-roofed enclosure, built low to the ground, used to protect plants from adverse weather, primarily excessive cold or wet. The transparent top admits sunlight and prevents heat escape via convection that would otherwise occur, particularly at night. Essentially, a cold frame functions as a miniature greenhouse to extend the growing season. \n\nHistorically, cold frames were built to be used in addition to a heated greenhouse. The name itself exemplifies the distinction between the warm greenhouse and the unheated cold frame. They were frequently built as part of the greenhouse's foundation brickwork along the southern wall (in northern latitudes). This allowed seeds to be germinated in the greenhouse and then easily moved to the attached cold frame to be \"hardened-off\" before final planting outside. Cold frames are similar to some enclosed hotbeds, also called hotboxes. The difference is in the amount of heat generated inside. This is parallel to the way that some greenhouses are called \"hothouses\" to emphasize their higher temperature, achieved either by the solar effects alone or by auxiliary heating via a heater or HVAC system of some kind. \n\nCold frames are found in home gardens and in vegetable farming. They create microclimates that provide several degrees of air and soil temperature insulation, and shelter from wind. In cold-winter regions, these characteristics allow plants to be started earlier in the spring, and to survive longer into the fall and winter. They are most often used for growing seedlings that are later transplanted into open ground, and can also be a permanent home to cold-hardy vegetables grown for autumn and winter harvest.\n\nCold frame construction is a common home or farm building project, although kits and commercial systems are available. A traditional plan makes use of old glass windows: a wooden frame is built, about one to two feet tall, and the window placed on top. The roof is often sloped towards the winter sun to capture more light, and to improve runoff of water, and hinged for easy access. Clear plastic, rigid or sheeting, can be used in place of glass. An electric heating cable, available for this purpose, can be placed in the soil to provide additional heat.\n\nCold frames can be used to extend the growing season for many food and ornamental crops, primarily by providing increased warmth in early spring. This means that it's possible to harvest vegetable crops ahead of their normal season when they are extremely expensive to buy. Some crops suitable for growing in a cold frame include lettuces, parsley, salad onions, spinach, radishes and turnips etc. One vegetable crop can occupy the whole of a cold frame or a combination of crops can be grown so that they mature in rotation in order to get a wide range of different vegetables throughout the year from a single cold frame.\n\nA \"bulb frame\" is a specialized kind of cold frame, designed for growing hardy or almost hardy ornamental bulbous plants, particularly in climates with wet winters. Typically it is raised further above ground level than a normal cold frame, so that the plants can be seen better when in flower. They are often used for the cultivation of winter-growing bulbs which flower in the autumn or spring. The covers are used in winter to provide some protection from very bad weather, while allowing good ventilation. Then in the summer, the covers provide dry, warm conditions which many such bulbs need.\n\n"}
{"id": "405493", "url": "https://en.wikipedia.org/wiki?curid=405493", "title": "Digital physics", "text": "Digital physics\n\nIn physics and cosmology, digital physics is a collection of theoretical perspectives based on the premise that the universe is describable by information. It is a form of digital ontology about the physical reality. According to this theory, the universe can be conceived of as either the output of a deterministic or probabilistic computer program, a vast, digital computation device, or mathematically isomorphic to such a device.\n\nThe operations of computers must be compatible with the principles of information theory, statistical thermodynamics, and quantum mechanics. In 1957, a link among these fields was proposed by Edwin Jaynes. He elaborated an interpretation of probability theory as generalized Aristotelian logic, a view linking fundamental physics with digital computers, because these are designed to implement the operations of classical logic and, equivalently, of Boolean algebra.\n\nThe hypothesis that the universe is a digital computer was proposed by Konrad Zuse in his book \"Rechnender Raum\" (translated into English as \"Calculating Space\"). The term \"digital physics\" was employed by Edward Fredkin, who later came to prefer the term \"digital philosophy\". Others who have modeled the universe as a giant computer include Stephen Wolfram, Juergen Schmidhuber, and Nobel laureate Gerard 't Hooft. These authors hold that the probabilistic nature of quantum physics is not necessarily incompatible with the notion of computability. Quantum versions of digital physics have recently been proposed by Seth Lloyd and Paola Zizzi.\n\nRelated ideas include Carl Friedrich von Weizsäcker's binary theory of ur-alternatives, pancomputationalism, computational universe theory, John Archibald Wheeler's \"It from bit\", and Max Tegmark's ultimate ensemble.\n\nDigital physics suggests that there exists, at least in principle, a program for a universal computer that computes the evolution of the universe. The computer could be, for example, a huge cellular automaton (Zuse 1967), or a universal Turing machine, as suggested by Schmidhuber (1997), who pointed out that there exists a short program that can compute all possible computable universes in an asymptotically optimal way.\n\nLoop quantum gravity could lend support to digital physics, in that it assumes space-time is quantized. Paola Zizzi has formulated a realization of this concept in what has come to be called \"computational loop quantum gravity\", or CLQG. Other theories that combine aspects of digital physics with loop quantum gravity are those of Marzuoli and Rasetti and Girelli and Livine.\n\nPhysicist Carl Friedrich von Weizsäcker's theory of ur-alternatives (theory of archetypal objects), first publicized in his book \"The Unity of Nature\" (1971), further developed through the 1990s, is a kind of digital physics as it axiomatically constructs quantum physics from the distinction between empirically observable, binary alternatives. Weizsäcker used his theory to derive the 3-dimensionality of space and to estimate the entropy of a proton. In 1988 Görnitz has shown that Weizsäcker's assumption can be connected with the Bekenstein-Hawking Entropy.\n\nPancomputationalism (also known as naturalist computationalism) is a view that the universe is a computational machine, or rather a network of computational processes which, following fundamental physical laws, computes (dynamically develops) its own next state from the current one.\n\nA computational universe is proposed by Jürgen Schmidhuber in a paper based on Zuse's 1967 thesis. He pointed out that a simple explanation of the universe would be a Turing machine programmed to execute all possible programs computing all possible histories for all types of computable physical laws. He also pointed out that there is an optimally efficient way of computing all computable universes based on Leonid Levin's universal search algorithm (published in 1973). In 2000, he expanded this work by combining Ray Solomonoff's theory of inductive inference with the assumption that quickly computable universes are more likely than others. This work on digital physics also led to limit-computable generalizations of algorithmic information or Kolmogorov complexity and the concept of Super Omegas, which are limit-computable numbers that are even more random (in a certain sense) than Gregory Chaitin's number of wisdom Omega.\n\nFollowing Jaynes and Weizsäcker, the physicist John Archibald Wheeler proposed an \"it from bit\" doctrine: information sits at the core of physics, and every \"it\", whether a particle or field, derives its existence from observations.\n\nThe toughest nut to crack in Wheeler's research program of a digital dissolution of physical being in a unified physics, Wheeler says, is time. In a 1986 eulogy to the mathematician Hermann Weyl, Wheeler proclaimed: \"Time, among all concepts in the world of physics, puts up the greatest resistance to being dethroned from ideal continuum to the world of the discrete, of information, of bits. ... Of all obstacles to a thoroughly penetrating account of existence, none looms up more dismayingly than 'time.' Explain time? Not without explaining existence. Explain existence? Not without explaining time. To uncover the deep and hidden connection between time and existence ... is a task for the future.\"\n\nNot every informational approach to physics (or ontology) is necessarily digital. According to Luciano Floridi, \"informational structural realism\" is a variant of structural realism that supports an ontological commitment to a world consisting of the totality of informational objects dynamically interacting with each other. Such informational objects are to be understood as constraining affordances.\n\nPancomputationalists like Lloyd (2006), who models the universe as a quantum computer, can still maintain an analogue or hybrid ontology; and informational ontologists like Kenneth Sayre and Floridi embrace neither a digital ontology nor a pancomputationalist position.\n\nThe classic Church–Turing thesis claims that any computer as powerful as a Turing machine can, in principle, calculate anything that a human can calculate, given enough time. Turing moreover showed that there exist universal Turing machines which can compute anything any other Turing machine can compute—that they are generalizable Turing machines. But the limits of practical computation are set by physics, not by theoretical computer science:\n\"Turing did not show that his machines can solve any problem that can be solved 'by instructions, explicitly stated rules, or procedures', nor did he prove that the universal Turing machine 'can compute any function that any computer, with any architecture, can compute'. He proved that his universal machine can compute any function that any Turing machine can compute; and he put forward, and advanced philosophical arguments in support of, the thesis here called Turing's thesis. But a thesis concerning the extent of effective methods—which is to say, concerning the extent of procedures of a certain sort that a human being unaided by machinery is capable of carrying out—carries no implication concerning the extent of the procedures that machines are capable of carrying out, even machines acting in accordance with 'explicitly stated rules.' For among a machine's repertoire of atomic operations there may be those that no human being unaided by machinery can perform.\"\n\nOn the other hand, a modification of Turing's assumptions \"does\" bring practical computation within Turing's limits; as David Deutsch puts it:\n\"I can now state the physical version of the Church–Turing principle: 'Every \"finitely\" realizable physical system can be perfectly simulated by a universal model computing machine operating by \"finite\" means.' This formulation is both better defined and more physical than Turing's own way of expressing it.\" \nThis compound conjecture is sometimes called the \"strong Church–Turing thesis\" or the Church–Turing–Deutsch principle. It is stronger because a human or Turing machine computing with pencil and paper (under Turing's conditions) is a finitely realizable physical system.\n\nSo far there is no experimental confirmation of either binary or quantized nature of the universe, which are basic for digital physics.\nThe few attempts made in this direction would include the experiment with holometer designed by Craig Hogan, which among others would detect a bit structure of space-time.\nThe experiment started collecting data in August 2014.\n\nA new result of the experiment released on December 3, 2015, after a year of data collection, has ruled out Hogan's theory of a pixelated universe to a high degree of statistical significance (4.6 sigma). The study found that space-time is not quantized at the scale being measured.\n\nOne objection is that extant models of digital physics are incompatible with the existence of several continuous characters of physical symmetries, e.g., rotational symmetry, translational symmetry, Lorentz symmetry, and the Lie group gauge invariance of Yang-Mills theories, all central to current physical theory.\n\nProponents of digital physics claim that such continuous symmetries are only convenient (and very good) approximations of a discrete reality. For example, the reasoning leading to systems of natural units and the conclusion that the Planck length is a minimum meaningful unit of distance suggests that at some level, space itself is quantized.\n\nMoreover, computers can manipulate and solve formulas describing real numbers using symbolic computation, thus avoiding the need to approximate real numbers by using an infinite number of digits.\n\nA number—in particular a real number, one with an infinite number of digits—was defined by Turing to be computable if a Turing machine will continue to spit out digits endlessly. In other words, there is no \"last digit\". But this sits uncomfortably with any proposal that the universe is the output of a virtual-reality exercise carried out in real time (or any plausible kind of time). Known physical laws (including quantum mechanics and its continuous spectra) are very much infused with real numbers and the mathematics of the continuum.\n\"So ordinary computational descriptions do not have a cardinality of states and state space trajectories that is sufficient for them to map onto ordinary mathematical descriptions of natural systems. Thus, from the point of view of strict mathematical description, the thesis that everything is a computing system in this second sense cannot be supported\".\n\nFor his part, David Deutsch generally takes a \"multiverse\" view to the question of continuous vs. discrete. In short, he thinks that “within each universe all observable quantities are discrete, but the multiverse as a whole is a continuum. When the equations of quantum theory describe a continuous but not-directly-observable transition between two values of a discrete quantity, what they are telling us is that the transition does not take place entirely within one universe. So perhaps the price of continuous motion is not an infinity of consecutive actions, but an infinity of concurrent actions taking place across the multiverse.” January, 2001 The Discrete and the Continuous, an abridged version of which appeared in The Times Higher Education Supplement.\n\nSome argue that extant models of digital physics violate various postulates of quantum physics. For example, if these models are not grounded in Hilbert spaces and probabilities, they belong to the class of theories with local hidden variables that have so far been ruled out experimentally using Bell's theorem. This criticism has two possible answers. First, any notion of locality in the digital model does not necessarily have to correspond to locality formulated in the usual way in the emergent spacetime. A concrete example of this case was given by Lee Smolin. Another possibility is a well-known loophole in Bell's theorem known as superdeterminism (sometimes referred to as predeterminism). In a completely deterministic model, the experimenter's decision to measure certain components of the spins is predetermined. Thus, the assumption that the experimenter could have decided to measure different components of the spins than he actually did is, strictly speaking, not true.\n\n\n\n\n"}
{"id": "2087110", "url": "https://en.wikipedia.org/wiki?curid=2087110", "title": "Dunyazad (crater)", "text": "Dunyazad (crater)\n\nDunyazad is a large crater on Saturn's moon Enceladus first discovered by the \"Voyager 2\" spacecraft. It is named after Dunyazad, the sister of Scheherazade in \"The Book of One Thousand and One Nights\".\n\nDunyazad is located at and is approximately 31 kilometers across, making it one of the largest craters on Enceladus. It is the southernmost crater of a prominent crater triplet on Enceladus' anti-Saturnian hemisphere (there is no evidence that the impacts are related or were formed from break-up of a single body, like Shoemaker-Levy 9). The craters to its north are Shahrazad, and Al-Haddar. \"Voyager 2\" discovery images of this crater revealed an up-domed floor at Dunyazad, suggesting that the crater had been modified by viscous relaxation. Higher resolution views of Dunyazad taken by the \"Cassini\" Spacecraft during a close flyby on March 9, 2005 reveal not only an up-domed floor, but numerous tectonic fractures as well, particularly within the dome and northeastern crater rim.\n"}
{"id": "7991854", "url": "https://en.wikipedia.org/wiki?curid=7991854", "title": "Economic and Social Research Council", "text": "Economic and Social Research Council\n\nThe Economic and Social Research Council (ESRC) is one of the seven Research Councils in the United Kingdom. It receives most of its funding from the Department for Business, Energy and Industrial Strategy, and provides funding and support for research and training work in the social sciences and economics, such as postgraduate degrees.\n\nThe ESRC is based at Polaris House in Swindon, UK which is also the location of the head offices of several other UK Research Councils and Innovate UK. ESRC is part of United Kingdom Research and Innovation (UKRI), an organisation that brings together the UK’s research councils, Innovate UK and Research England. UKRI is a non-departmental public body funded by a grant-in-aid from the UK government. \n\nThe ESRC's mission, according to its website, is to:\n\n\nThe ESRC was founded in 1965 as the \"Social Science Research Council\" (SSRC - not to be confused with the Social Science Research Council in the United States). The establishment of a state funding body for the social sciences in the United Kingdom, had been under discussion since the Second World War; however, it was not until the 1964 election of Prime Minister Harold Wilson that the political climate for the creation of the SSRC became sufficiently favourable.\n\nThe first chief executive of the SSRC was Michael Young (later Baron Young of Dartington). Subsequent holders of the post have included Michael Posner, later Secretary General of the European Science Foundation. The current Chief Executive of the ESRC is Professor Jennifer Rubin who took over from Professor Jane Elliott in October 2017.\nFollowing the election of Prime Minister Margaret Thatcher in the 1979 general election, the Government expressed reservations about the value of research in the social sciences, and the extent to which it should be publicly funded. In 1981, the Education Secretary Sir Keith Joseph asked Lord Rothschild to lead a review into the future of the SSRC.\n\nIt was ultimately decided (due in no small part to the efforts of Michael Posner, chief executive of the SSRC at the time) that the Council should remain, but that its remit should be expanded beyond the social sciences, to include more 'empirical' research and research of 'more public concern'. To reflect this, in 1983 the SSRC was renamed the Economic and Social Research Council.\n\nChairman:\n\nChief Executive:\n\nExecutive Chair:\n\n"}
{"id": "32706452", "url": "https://en.wikipedia.org/wiki?curid=32706452", "title": "EteRNA", "text": "EteRNA\n\nEteRNA is a browser-based \"game with a purpose\", developed by scientists at Carnegie Mellon University and Stanford University, that engages users to solve puzzles related to the folding of RNA molecules. The project is supported by the Bill and Melinda Gates Foundation, Stanford University, and the National Institutes of Health. Prior funders include the National Science Foundation.\n\nSimilar to Foldit—created by some of the same researchers that developed EteRNA—the puzzles take advantage of human problem-solving capabilities to solve puzzles that are computationally laborious for current computer models. The researchers hope to capitalize on \"crowdsourcing\" and the collective intelligence of EteRNA players to answer fundamental questions about RNA folding mechanics. The top voted designs are synthesized in a Stanford biochemistry lab to evaluate the folding patterns of the RNA molecules to compare directly with the computer predictions, ultimately improving the computer models.\n\nUltimately, EteRNA researchers hope to determine a \"complete and repeatable set of rules\" to allow the synthesis of RNAs that consistently fold in expected shapes. EteRNA project leaders hope that determining these basic principles may facilitate the design of RNA-based nanomachines and switches. EteRNA creators have been pleasantly surprised by the solutions of EteRNA players, particularly those of non-researchers whose \"creativity isn't constrained by what they think a correct answer should look like\".\n\nAs of 2016, EteRNA has about 250,000 registered players.\n\nPlayers are presented with a given target shape into which an RNA strand must fold. The player can change the sequence by placing any of the four RNA nucleotides (adenine, cytosine, guanosine and uracil) at various positions; this can alter the free energy of the system and dramatically affect the RNA strand's folding dynamics. In EteRNA, different restrictions, such as those on the number of certain bases and the number of the three base pair types, as well as locked bases, are sometimes imposed. A molecule is occasionally also included, which binds with the RNA and has critical effects on the free energy of the system. In some more advanced puzzles, players may be presented with two or three different target shapes at the same time; the single sequence the player produces must fold in the respective shapes under different conditions (presence or absence of a binding molecule).\n\nEteRNA puzzles are roughly classified into three types: Challenges, Player Puzzles, and Cloud Lab. Challenges are the puzzles prepared by the game-makers to introduce players to the workings of EteRNA as well as to provide series of pre-set puzzles for players to attempt. Player puzzles are generated by players, and Cloud Lab is where the active, proposed and archived laboratory projects are presented for players to review, vote or attempt.\n\nNew players are guided through an initial puzzle progression which introduces the basic concepts of RNA structure and design. As players proceed through puzzles of increasing complexity, additional game interface elements are unlocked. After earning 10 badges representing the 10 stages of the progression, players gain access to the Cloud Lab where they can participate in laboratory research. Once players have completed a sufficient number of RNA puzzles, they unlock the chance to generate puzzles for other players. These puzzles can be selected as future synthesis candidates if they fit certain rules and prove interesting for research.\n\nIn 2016, EteRNA launched its first biomedical challenge called OpenTB, an initiative to develop a new diagnostic device for tuberculosis. The project uses a gene expression \"signature\" discovered by Stanford researchers using public data, and aims to create an open source, paper-based diagnostic kit that can be easily deployed in clinics around the world. The development of the open source kit is a collaboration with MIT's Little Devices Lab. Players successfully designed RNAs to detect the gene signature by round 2 of the challenge, and as of February 2018 testing continues with real patient samples.\n\nFollowing the success of OpenTB, EteRNA launched OpenCRISPR in August 2017, which challenges players to design single guide RNAs (sgRNAs) used in CRISPR gene editing. The goal of the project is to create a new class of sgRNAs that can be modulated by another small molecule (such as theophylline), allowing gene editing in the body to be turned on or off as needed. At the conclusion of round 1 in November 2017, players had submitted over 90,000 RNA designs for synthesis, the largest set of submissions to date.\n\n\n\n"}
{"id": "10122624", "url": "https://en.wikipedia.org/wiki?curid=10122624", "title": "Factor 10", "text": "Factor 10\n\nFactor Ten is a social and economic policy program developed by the Factor Ten institute with the stated goal of reducing human resource turnover by 90% on a global scale within the next 30 to 50 years.\n\nFriedrich Schmidt-Bleek, from the Wuppertal Institute for Climate, Environment and Energy, first proposed the Factor 10 and dematerialization concepts in the early 1990s. He concluded in his studies that 80% of the world’s resources are distributed among First World nations, which contribute 20% of the global population, so those nations are promoting an unsustainable system of development. The goal of Factor 10 is to assure that nations do not exceed the planet’s carrying capacity but leave sufficient resources for future generations.\n\nFactor 10 evolved from the less dramatic Factor 4 was originally proposed by L. Hunter Lovins and Amory Lovins of the Rocky Mountain Institute and Ernst von Weizsäcker, the founder of the Wuppertal Institute for Climate, Environment & Energy. Their book \"Factor 4\" explains how simple it is for nations to achieve Factor 4 results with existing technologies. The concept attempts to reduce resource and energy use by 75 % by doubling output and halving input of production.\n\nFactor 10 requires the creation of new technologies, policies, and manufacturing processes along with sociocultural change to create a global economy that is sustainable for a long period of time. The long-term goal of Factor 10, many governments and firms aspiring toward shortIterm relief have difficulty achieving the massive reductions proposed by factor 10. Eco-efficiency, environmental purchasing design for environment, policies and environmental taxes have already been used by business and governments implementing the Factor 10 theory.\n\nFactor 10 goes farther as a response to the United Nations Environment Programme call for a tenfold reduction in resource consumption in industrialised countries as a necessary long-term target if adequate resources are to be released for the needs of the developing countries. With the predicted rise in population and economic growth to maintain the level of pollution we have today, we need to be able to produce the same output for 10% of the impact.\n\nFactor X concept is the direct way of using metric and various activities that can reduce the throughput of resources and energy in the given process. The essential question is by what factor can or should certain flows be reduced. It is a useful tool to monitor the performance of business in terms of dematerialization.\n"}
{"id": "16508375", "url": "https://en.wikipedia.org/wiki?curid=16508375", "title": "Fuel temperature coefficient of reactivity", "text": "Fuel temperature coefficient of reactivity\n\nFuel temperature coefficient of reactivity is the change in reactivity of the nuclear fuel per degree change in the fuel temperature. The coefficient quantifies the amount of neutrons that the nuclear fuel (such as uranium-238) absorbs from the fission process as the fuel temperature increases. It is a measure of the stability of the reactor operations. This coefficient is also known as the Doppler coefficient.\n\n\n"}
{"id": "30578763", "url": "https://en.wikipedia.org/wiki?curid=30578763", "title": "Functional Mock-up Interface", "text": "Functional Mock-up Interface\n\nThe Functional Mock-up Interface (or FMI) defines a standardized interface to be used in computer simulations to develop complex cyber-physical systems.\n\nThe vision of FMI is to support this approach: if the real product is to be assembled from a wide range of parts interacting in complex ways, each controlled by a complex set of physical laws, then it should be possible to create a virtual product that can be assembled from a set of models that each represent a combination of parts, each a model of the physical laws as well as a model of the control systems (using electronics, hydraulics, digital software, ..) assembled digitally.\nThe FMI standard thus provides the means for model based development of systems and is used for example for designing functions that are driven by electronic devices inside vehicles (e.g. ESP controllers, active safety systems, combustion controllers).\nActivities from systems modelling, simulation, validation and test can be covered with the FMI based approach. \nTo create the FMI standard, a large number of software companies and research centers have worked in a cooperation project established through a European consortium that has been conducted by Dassault Systèmes under the name of MODELISAR.\nThe MODELISAR project started in 2008 to define the FMI specifications, deliver technology studies, prove the FMI concepts through Use Cases elaborated by the consortium partners and enable tool vendors to build advanced prototypes or in some cases even products.\n\nThe development of the FMI specifications was coordinated by Daimler AG.\n\nAfter the end of the MODELISAR project in 2011, FMI is managed and developed as a Modelica Association Project (MAP).\n\nThe four required FMI aspects of creating models capable of being assembled have been covered in Modelisar project: \n\nIn practice, the FMI implementation by a software modelling tool enables the creation of a simulation model that can be interconnected or the creation of a software library called FMU (Functional Mock-up Unit).\n\nThe typical FMI approach is described in the following stages:\n\nThe FMI specifications are distributed under open source licenses:\n\nEach FMU (Functional Mock-up Unit) is distributed in a zip file with the extension \".fmu\" which contains:\n\nBelow is an example of an FMI model description issued from Modelica.\nFMI is often compared to Simulink S-Functions since both technologies can be used to integrate third-party tools together. S-Functions are used to specify a computer language description of a dynamic system. They are compiled as MEX-files that are dynamically linked into MATLAB when needed. S-Functions use a calling syntax that interacts with Simulink’s equation solvers. This interaction is similar to the interaction that takes place between built-in Simulink blocks and the solvers.\n\nFMI proponents explain that FMI models have several advantages over Simulink S-Functions:\n\nThere are also several limitations cited when using FMI/FMU:\n\nAs of November 2011, FMI is supported on the following simulation frameworks:\nSee full, up-to-date list and details in FMI web pages.\n\nIn May 2014, the Project Group Smart Systems Engineering (SmartSE) of the ProSTEP iViP Association published its Recommendation PSI 11 for the cross-company behavior model exchange. FMI thereby is the technological basis. The PSI 11 specifies interaction scenarios, use cases, a reference process and templates, which thereby could ease the industrial application. End of 2016 the group published a movie, which should highlight the industrial benefits.\n\n\n"}
{"id": "68466", "url": "https://en.wikipedia.org/wiki?curid=68466", "title": "Gamma correction", "text": "Gamma correction\n\nGamma correction, or often simply gamma, is a nonlinear operation used to encode and decode luminance or tristimulus values in video or still image systems. Gamma correction is, in the simplest cases, defined by the following power-law expression:\nwhere the non-negative real input value formula_2 is raised to the power formula_3 and multiplied by the constant \"A,\" to get the output value formula_4. In the common case of , inputs and outputs are typically in the range 0–1.\n\nA gamma value formula_5 is sometimes called an \"encoding gamma\", and the process of encoding with this compressive power-law nonlinearity is called gamma compression; conversely a gamma value formula_6 is called a \"decoding gamma\" and the application of the expansive power-law nonlinearity is called gamma expansion.\n\nGamma encoding of images is used to optimize the usage of bits when encoding an image, or bandwidth used to transport an image, by taking advantage of the non-linear manner in which humans perceive light and color. The human perception of brightness, under common illumination conditions (not pitch black nor blindingly bright), follows an approximate power function (note: no relation to the gamma function), with greater sensitivity to relative differences between darker tones than between lighter ones, consistent with the Stevens' power law for brightness perception. If images are not gamma-encoded, they allocate too many bits or too much bandwidth to highlights that humans cannot differentiate, and too few bits or too little bandwidth to shadow values that humans are sensitive to and would require more bits/bandwidth to maintain the same visual quality. Gamma encoding of floating-point images is not required (and may be counterproductive), because the floating-point format already provides a piecewise linear approximation of a logarithmic curve.\n\nAlthough gamma encoding was developed originally to compensate for the input–output characteristic of cathode ray tube (CRT) displays, that is not its main purpose or advantage in modern systems. In CRT displays, the light intensity varies nonlinearly with the electron-gun voltage. Altering the input signal by gamma compression can cancel this nonlinearity, such that the output picture has the intended luminance. However, the gamma characteristics of the display device do not play a factor in the gamma encoding of images and video—they need gamma encoding to maximize the visual quality of the signal, regardless of the gamma characteristics of the display device. The similarity of CRT physics to the inverse of gamma encoding needed for video transmission was a combination of coincidence and engineering, which simplified the electronics in early television sets.\n\nThe concept of gamma can be applied to any nonlinear relationship. For the power-law relationship formula_7, the curve on a log–log plot is a straight line, with slope everywhere equal to gamma (slope is represented here by the derivative operator):\n\nThat is, gamma can be visualized as the slope of the input–output curve when plotted on logarithmic axes. For a power-law curve, this slope is constant, but the idea can be extended to any type of curve, in which case gamma (strictly speaking, \"point gamma\") is defined as the slope of the curve in any particular region.\n\nWhen a photographic film is exposed to light, the result of the exposure can be represented on a graph showing log of exposure on the horizontal axis, and density, or log of transmittance, on the vertical axis. For a given film formulation and processing method, this curve is its characteristic or Hurter–Driffield curve. Since both axes use logarithmic units, the slope of the linear section of the curve is called the gamma of the film. Negative film typically has a gamma less than 1; positive film (slide film, reversal film) typically has a gamma greater than 1.\n\nPhotographic film has a much greater ability to record fine differences in shade than can be reproduced on photographic paper. Similarly, most video screens are not capable of displaying the range of brightnesses (dynamic range) that can be captured by typical electronic cameras.\nFor this reason, considerable artistic effort is invested in choosing the reduced form in which the original image should be presented. The gamma correction, or contrast selection, is part of the photographic repertoire used to adjust the reproduced image.\n\nAnalogously, digital cameras record light using electronic sensors that usually respond linearly. In the process of rendering linear raw data to conventional RGB data (e.g. for storage into JPEG image format), color space transformations and rendering transformations will be performed. In particular, almost all standard RGB color spaces and file formats use a non-linear encoding (a gamma compression) of the intended intensities of the primary colors of the photographic reproduction; in addition, the intended reproduction is almost always nonlinearly related to the measured scene intensities, via a tone reproduction nonlinearity.\n\nIn most computer display systems, images are encoded with a gamma of about 0.45 and decoded with the reciprocal gamma of 2.2. A notable exception, until the release of Mac OS X 10.6 (Snow Leopard) in September 2009, were Macintosh computers, which encoded with a gamma of 0.55 and decoded with a gamma of 1.8. In any case, binary data in still image files (such as JPEG) are explicitly encoded (that is, they carry gamma-encoded values, not linear intensities), as are motion picture files (such as MPEG). The system can optionally further manage both cases, through color management, if a better match to the output device gamma is required.\n\nThe sRGB color space standard used with most cameras, PCs, and printers does not use a simple power-law nonlinearity as above, but has a decoding gamma value near 2.2 over much of its range, as shown in the plot to the right. Below a compressed value of 0.04045 or a linear intensity of 0.00313, the curve is linear (encoded value proportional to intensity), so . The dashed black curve behind the red curve is a standard power-law curve, for comparison.\n\nOutput to CRT-based television receivers and monitors does not usually require further gamma correction, since the standard video signals that are transmitted or stored in image files incorporate gamma compression that provides a pleasant image after the gamma expansion of the CRT (it is not the exact inverse). For television signals, the actual gamma values are defined by the video standards (NTSC, PAL or SECAM), and are always fixed and well-known values.\n\nGamma correction in computers is used, for example, to display a gamma = 1.8 Apple picture correctly on a gamma = 2.2 PC monitor by changing the image gamma. Another usage is equalizing of the individual color-channel gammas to correct for monitor discrepancies.\n\nThe \"best\" system gamma for display depends on the ambient light. Some picture formats include gamma meta information to allow the display system an automatic gamma correction. Some web browser use this meta information, others do not. The PNG format has the gAMA meta information, the Exif format has the Gamma tag. The W3 PNG gamma test images show that the Mozilla Firefox browser supports PNG gAMA and the Microsoft Edge browser does not.\n\nA \"gamma characteristic\" is a power-law relationship that approximates the relationship between the encoded luma in a television system and the actual desired image luminance.\n\nWith this nonlinear relationship, equal steps in encoded luminance correspond roughly to subjectively equal steps in brightness. Ebner and Fairchild used an exponent of 0.43 to convert linear intensity into lightness (luma) for neutrals; the reciprocal, approximately 2.33 (quite close to the 2.2 figure cited for a typical display subsystem), was found to provide approximately optimal perceptual encoding of grays. \n\nThe following illustration shows the difference between a scale with linearly-increasing encoded luminance signal (linear gamma-compressed luma input) and a scale with linearly-increasing intensity scale (linear luminance output).\n\nOn most displays (those with gamma of about 2.2), one can observe that the linear-intensity scale has a large jump in perceived brightness between the intensity values 0.0 and 0.1, while the steps at the higher end of the scale are hardly perceptible. The gamma-encoded scale, which has a nonlinearly-increasing intensity, will show much more even steps in perceived brightness.\n\nA cathode ray tube (CRT), for example, converts a video signal to light in a nonlinear way, because the electron gun's intensity (brightness) as a function of applied video voltage is nonlinear. The light intensity \"I\" is related to the source voltage \"V\" according to\nwhere \"γ\" is the Greek letter gamma. For a CRT, the gamma that relates brightness to voltage is usually in the range 2.35 to 2.55; video look-up tables in computers usually adjust the system gamma to the range 1.8 to 2.2, which is in the region that makes a uniform encoding difference give approximately uniform perceptual brightness difference, as illustrated in the diagram at the top of this section.\n\nFor simplicity, consider the example of a monochrome CRT. In this case, when a video signal of 0.5 (representing a mid-gray) is fed to the display, the intensity or brightness is about 0.22 (resulting in a mid-gray, about 22% the intensity of white). Pure black (0.0) and pure white (1.0) are the only shades that are unaffected by gamma.\n\nTo compensate for this effect, the inverse transfer function (gamma correction) is sometimes applied to the video signal so that the end-to-end response is linear. In other words, the transmitted signal is deliberately distorted so that, after it has been distorted again by the display device, the viewer sees the correct brightness. The inverse of the function above is:\nwhere \"V\" is the corrected voltage and \"V\" is the source voltage, for example from an image sensor that converts photocharge linearly to a voltage. In our CRT example 1/\"γ\" is 1/2.2 or 0.45.\n\nA color CRT receives three video signals (red, green, and blue) and in general each color has its own value of gamma, denoted \"γ\", \"γ\" or \"γ\". However, in simple display systems, a single value of \"γ\" is used for all three colors.\n\nOther display devices have different values of gamma: for example, a Game Boy Advance display has a gamma between 3 and 4 depending on lighting conditions. In LCDs such as those on laptop computers, the relation between the signal voltage \"V\" and the intensity \"I\" is very nonlinear and cannot be described with gamma value. However, such displays apply a correction onto the signal voltage in order to approximately get a standard behavior. In NTSC television recording, .\n\nThe power-law function, or its inverse, has a slope of infinity at zero. This leads to problems in converting from and to a gamma colorspace. For this reason most formally defined colorspaces such as sRGB will define a straight-line segment near zero and add raising (where \"K\" is a constant) to a power so the curve has continuous slope. This straight line does not represent what the CRT does, but does make the rest of the curve more closely match the effect of ambient light on the CRT. In such expressions the exponent is \"not\" the gamma; for instance, the sRGB function uses a power of 2.4 in it, but more closely resembles a power-law function with an exponent of 2.2, without a linear portion.\n\nUp to four elements can be manipulated in order to achieve gamma encoding to correct the image to be shown on a typical 2.2- or 1.8-gamma computer display:\n\nIn a correctly calibrated system, each component will have a specified gamma for its input and/or output encodings. Stages may change the gamma to correct for different requirements, and finally the output device will do gamma decoding or correction as needed, to get to a linear intensity domain. All the encoding and correction methods can be arbitrarily superimposed, without mutual knowledge of this fact among the different elements; if done incorrectly, these conversions can lead to highly distorted results, but if done correctly as dictated by standards and conventions will lead to a properly functioning system.\n\nIn a typical system, for example from camera through JPEG file to display, the role of gamma correction will involve several cooperating parts. The camera encodes its rendered image into the JPEG file using one of the standard gamma values such as 2.2, for storage and transmission. The display computer may use a color management engine to convert to a different color space (such as older Macintosh's color space) before putting pixel values into its video memory. The monitor may do its own gamma correction to match the CRT gamma to that used by the video system. Coordinating the components via standard interfaces with default standard gamma values makes it possible to get such system properly configured.\n\nThis procedure is not suitable for calibrating or print-proofing a monitor. It can be useful for making a monitor display sRGB images approximately correctly, on systems in which profiles are not used (for example, the Firefox browser prior to version 3.0 and many others) or in systems that assume untagged source images are in the sRGB colorspace.\n\nNormally a graphics card has contrast and brightness control and a transmissive LCD monitor has contrast, brightness and backlight control. Only backlight is a \"real\" brightness control. Graphics card and monitor contrast and brightness have a (negative) influence on gamma correction and should not be changed after gamma correction is completed. The correct brightness and contrast is often found with minimum setting of contrast and medium setting of brightness. The test image shows on top two grey gradients and below three areas of primary colors. If the observers eye sees only a small white area in the middle of the first grey area and only a small black area in the middle of the second grey area, then the brightness and contrast settings are correct.\n\nIf the observer sees the same brightness in the lined part and in the homogeneous part of every colored area, for the desired gamma, then the gamma correction is approximately correct. In many cases the gamma correction values for the primary colors are slightly different. The test image display has to be pixel accurate; normally this is given at a web browser zoom value of 100%.\n\nSetting the white point is the next step in monitor adjustment. Gamma correction and color balance are different, but both use a 3x3 matrix for transformation. The color temperature for a monitor in a bright environment, typical for type writing, is a color temperature between 5000K and 6500K or the Illuminant D65.\n\nBefore gamma correction the color temperature of the monitor should be set to off or to 5500K. Using the controls for gamma, contrast and brightness, the gamma correction on a LCD can only be done for one specific vertical viewing angle, which implies one specific horizontal line on the monitor, at one specific brightness and contrast level. The quality (and price) of the monitor determines how much deviation of this operating point still gives a satisfactory gamma correction. Twisted nematic (TN) displays have lowest quality with 6-bit color depth per primary color. In-plane switching (IPS) displays with typically 8-bit color depth. Some monitors have hardware color management and allow hardware calibration with a tristimulus colorimeter and have 10-bit color depth. The 24-bit and 32-bit color depth formats have 8 bits per primary color.\n\nWith Microsoft Windows 7 and above the user should set the gamma correction through the display color calibration tool dccw.exe. Increase the gamma slider in the dccw program until the last colored area, often the green color, has the same brightness in checkered and non-checkered area. The brightness controls for individual colors in the graphics card allow gamma correction of the two other colors. \n\nOn some operating systems running the X Window System, one can set the gamma correction factor (applied to the existing gamma value) by issuing the command codice_1 for setting gamma correction factor to 0.9, and codice_2 for querying current value of that factor (the default is 1.0). In OS X systems, the gamma and other related screen calibrations are made through the System Preferences.\n\nThe term intensity refers strictly to the amount of light that is emitted per unit of time and per unit of surface, in units of lux. Note, however, that in many fields of science this quantity is called luminous exitance, as opposed to luminous intensity, which is a different quantity. These distinctions, however, are largely irrelevant to gamma compression, which is applicable to any sort of normalized linear intensity-like scale.\n\n\"Luminance\" can mean several things even within the context of video and imaging:\n\nOne contrasts relative luminance in the sense of color (no gamma compression) with luma in the sense of video (with gamma compression), and denote relative luminance by \"Y\" and luma by \"Y\"′, the prime symbol (′) denoting gamma compression.\nNote that luma is not directly calculated from luminance, it is the (somewhat arbitrary) weighted sum of gamma compressed RGB components.\n\nLikewise, \"brightness\" is sometimes applied to various measures, including light levels, though it more properly applies to a subjective visual attribute.\n\nGamma correction is a type of power law function whose exponent is the Greek letter gamma (\"γ\"). It should not be confused with the mathematical Gamma function. The lower case gamma, \"γ\", is a parameter of the former; the upper case letter, Γ, is the name of (and symbol used for) the latter (as in Γ(\"x\")). To use the word \"function\" in conjunction with gamma correction, one may avoid confusion by saying \"generalized power law function\".\n\nWithout context, a value labeled gamma might be either the encoding or the decoding value. Caution must be taken to correctly interpret the value as that to be applied-to-compensate or to be compensated-by-applying its inverse. In common parlance, in many occasions the decoding value (as 2.2) is employed as if it were the encoding value, instead of its inverse (1/2.2 in this case), which is the \"real\" value that must be applied to encode gamma.\n\n\n"}
{"id": "34603052", "url": "https://en.wikipedia.org/wiki?curid=34603052", "title": "Geoffrey Michael William Hodgkins", "text": "Geoffrey Michael William Hodgkins\n\nGeoffrey Michael William Hodgkins (1 May 1902–27 October 1965) was a New Zealand naturalist and character. He was born in Dunedin, New Zealand on 1 May 1902.\n"}
{"id": "5029556", "url": "https://en.wikipedia.org/wiki?curid=5029556", "title": "Harbi al-Himyari", "text": "Harbi al-Himyari\n\nHarbi al-Himyari ( ), was an Arab scholar from Yemen, who lived between the 7th and 8th century AD. He is the mentor for teaching Koran and mathematics to Jābir ibn Hayyān. According to Holmyard nothing else is known about him.\n\n"}
{"id": "12711369", "url": "https://en.wikipedia.org/wiki?curid=12711369", "title": "Hibernaculum (botany)", "text": "Hibernaculum (botany)\n\nHibernaculum (plural \"hibernacula\") is the term often applied to a winter bud of certain aquatic plants, such as the bladderworts (\"Utricularia\"). The buds are heavier than water, and, being developed at the approach of cold weather, they become detached, sink to the bottom of the pond, and thus survive the winter. In the spring, they enlarge, developing air spaces, rise to the surface, and reproduce their species.\n\nCertain terrestrial plants also form hibernacula. These include some temperate sundews (\"Drosera\") such as \"D. anglica\", \"D. filiformis\", \"D. intermedia\", \"D. rotundifolia\"; and some temperate butterworts (\"Pinguicula\") such as \"P. balcanica\", \"P. grandiflora\", \"P. longifolia\", and \"P. vulgaris\".\n\n"}
{"id": "205464", "url": "https://en.wikipedia.org/wiki?curid=205464", "title": "Human microbiota", "text": "Human microbiota\n\nThe human microbiota is the aggregate of microorganisms that resides on or within any of a number of human tissues and biofluids, including the skin, mammary glands, placenta, seminal fluid, uterus, ovarian follicles, lung, saliva, oral mucosa, conjunctiva, biliary and gastrointestinal tracts. They include bacteria, archaea, fungi, protists and viruses. Though micro-animals can also live on the human body, they are typically excluded from this definition. The human microbiome refers specifically to the collective genomes of resident microorganisms.\n\nHumans are colonized by many microorganisms; the traditional estimate is that the average human body is inhabited by ten times as many non-human cells as human cells, but more recent estimates have lowered that ratio to 3:1 or even to approximately the same number. Some microorganisms that colonize humans are commensal, meaning they co-exist without harming humans; others have a mutualistic relationship with their human hosts. Conversely, some non-pathogenic microorganisms can harm human hosts via the metabolites they produce, like trimethylamine, which the human body converts to trimethylamine N-oxide via FMO3-mediated oxidation. Certain microorganisms perform tasks that are known to be useful to the human host but the role of most of them is not well understood. Those that are expected to be present, and that under normal circumstances do not cause disease, are sometimes deemed \"normal flora\" or \"normal microbiota\".\n\nThe Human Microbiome Project took on the project of sequencing the genome of the human microbiota, focusing particularly on the microbiota that normally inhabit the skin, mouth, nose, digestive tract, and vagina. It reached a milestone in 2012 when it published its initial results.\n\nThough widely known as \"flora or\" \"microflora\", this is a misnomer in technical terms, since the word root \"flora\" pertains to plants, and \"biota\" refers to the total collection of organisms in a particular ecosystem. Recently, the more appropriate term \"microbiota\" is applied, though its use has not eclipsed the entrenched use and recognition of \"flora\" with regard to bacteria and other microorganisms. Both terms are being used in different literature.\n\nAs of 2014, it was often reported in popular media and in the scientific literature that there are about 10 times as many microbial cells in the human body as there are human cells; this figure was based on estimates that the human microbiome includes around 100 trillion bacterial cells and that an adult human typically has around 10 trillion human cells. In 2014, the American Academy of Microbiology published a FAQ that emphasized that the number of microbial cells and the number of human cells are both estimates, and noted that recent research had arrived at a new estimate of the number of human cellsapproximately 37.2 trillion, meaning that the ratio of microbial-to-human cells, if the original estimate of 100 trillion bacterial cells is correct, is closer to 3:1. In 2016, another group published a new estimate of the ratio being roughly 1:1 (1.3:1, with \"an uncertainty of 25% and a variation of 53% over the population of standard 70-kg males\").\n\nThe problem of elucidating the human microbiome is essentially identifying the members of a microbial community which includes bacteria, eukaryotes, and viruses. This is done primarily using DNA-based studies, though RNA, protein and metabolite based studies are also performed. DNA-based microbiome studies typically can be categorized as either targeted amplicon studies or more recently shotgun metagenomic studies. The former focuses on specific known marker genes and is primarily informative taxonomically, while the latter is an entire metagenomic approach which can also be used to study the functional potential of the community. One of the challenges that is present in human microbiome studies, but not in other metagenomic studies is to avoid including the host DNA in the study.\n\nAside from simply elucidating the composition of the human microbiome, one of the major questions involving the human microbiome is whether there is a \"core\", that is, whether there is a subset of the community that is shared among most humans. If there is a core, then it would be possible to associate certain community compositions with disease states, which is one of the goals of the Human Microbiome Project. It is known that the human microbiome (such as the gut microbiota) is highly variable both within a single subject and among different individuals, a phenomenon which is also observed in mice. \nOn 13 June 2012, a major milestone of the Human Microbiome Project (HMP) was announced by the NIH director Francis Collins. The announcement was accompanied with a series of coordinated articles published in Nature and several journals in the Public Library of Science (PLoS) on the same day. By mapping the normal microbial make-up of healthy humans using genome sequencing techniques, the researchers of the HMP have created a reference database and the boundaries of normal microbial variation in humans. From 242 healthy U.S. volunteers, more than 5,000 samples were collected from tissues from 15 (men) to 18 (women) body sites such as mouth, nose, skin, lower intestine (stool), and vagina. All the DNA, human and microbial, were analyzed with DNA sequencing machines. The microbial genome data were extracted by identifying the bacterial specific ribosomal RNA, 16S rRNA. The researchers calculated that more than 10,000 microbial species occupy the human ecosystem and they have identified 81 – 99% of the genera.\n\nIt is frequently difficult to culture in laboratory communities of bacteria, archaea and viruses, therefore sequencing technologies can be exploited in metagenomics, too. Indeed, the complete knowledge of the functions and the characterization of specific microbial strains offer a great potentiality in therapeutic discovery and human health.\nThe main point is to collect an amount microbial biomass that is sufficient to perform the sequencing and to minimize the sample contamination; for this reason, enrichment techniques can be used. In particular, the DNA extraction method must be good for every bacterial strain, not to have the genomes of the ones that are easy to lyse. Mechanical lysis is usually preferred rather than chemical lysis, and bead beating may result in DNA loss when preparing the library.\nThe most used platforms are Illumina, Ion Torrent, Oxford Nanopore MinION and Pacific Bioscience Sequel, although the Illumina platform is considered the most appealing option due to its wide availability, high output and accuracy. There are no indications regarding the correct amount of sample to use.\nThe de novo approach is exploited; however, it presents some difficulties to be overcome. The coverage depends on each genome abundance in its specific community; low-abundance genomes may undergo fragmentation if the sequencing depth is not sufficient enough to avoid the formation of gaps. Luckily, there are metagenome-specific assemblers to help, since, if hundreds of strains are present, the sequencing depth needs to be increased to its maximum.\nNeither from which genome every contig derives, nor the number of genomes present in the sample are known \"a priori\"; the aim of this step is to divide the contigs into species. The methods to perform such analysis can be either supervised (database with known sequences) or unsupervised (direct search for contig groups in the collected data). However, both methods require a kind of metric to define a score for the similarity between a specific contig and the group in which it must be put, and algorithms to convert the similarities into allocations in the groups.\nThe statistical analysis is essential to validate the obtained results (ANOVA can be used to size the differences between the groups); if it is paired with graphical tools, the outcome is easily visualized and understood.\n\nOnce a metagenome is assembled, it is possible to infer the functional potential of the microbiome. The computational challenges for this type of analysis are greater than for single genomes, due the fact that usually metagenomes assemblers have poorer quality, and many recovered genes are non-complete or fragmented. After the gene identification step, the data can be used to carry out a functional annotation by means of multiple alignment of the target genes against orthologs databases.\n\nIt is a technique that exploits primers to target a specific genetic region and enables to determine the microbial phylogenies. The genetic region is characterized by a highly variable region which can confer detailed identification; it is delimited by conserved regions, which function as binding sites for primers used in PCR. The main gene used to characterize bacteria and archaea is 16S rRNA gene, while fungi identification is based on Internal Transcribed Spacer (ITS). The technique is fast and not so expensive and enables to obtain a low-resolution classification of a microbial sample; it is optimal for samples that may be contaminated by host DNA. Primer affinity varies among all DNA sequences, which may result in biases during the amplification reaction; indeed, low-abundance samples are susceptible to overamplification errors, since the other contaminating microorganisms result to be over-represented in case of increasing the PCR cycles. Therefore, the optimization of primer selection can help to decrease such errors, although it requires complete knowledge of the microorganisms present in the sample, and their relative abundances.\n\nMarker gene analysis can be influenced by the primer choice; in this kind of analysis it's desirable to use a well-validated protocol (such as the one used in the Earth Microbiome Project). The first thing to do in a marker gene amplicon analysis is to remove sequencing errors; a lot of sequencing platforms are very reliable, but most of the apparent sequence diversity is still due to errors during the sequencing process. To reduce this phenomenon a first approach is to cluster sequences into Operational taxonomic unit (OTUs): this process consolidates similar sequences (a 97% similarity threshold is usually adopted) into a single feature that can be used in further analysis steps; this method however would discard SNPs because they would get clustered into a single OTU. Another approach is Oligotyping, which includes position-specific information from 16s rRNA sequencing to detect small nucleotide variations and from discriminating between closely related distinct taxa. These methods give as an output a table of DNA sequences and counts of the different sequences per sample rather than OTU.\n\nAnother important step in the analysis is to assign a taxonomic name to microbial sequences in the data. This can be done using machine learning approaches that can reach an accuracy at genus-level of about 80%. Other popular analysis packages provide support for taxonomic classification using exact matches to reference databases and should provide greater specificity, but poor sensitivity. Unclassified microorganism should be further checked for organelle sequences.\n\nMany methods that exploit phylogenetic inference use the 16SRNA gene for Archea and Bacteria and the 18SRNA gene for Eukariotes. Phylogenetic comparative methods (PCS) are based on the comparison of multiple traits among microorganisms; the principle is: the closely they are related, the higher number of traits they share. Usually PCS are coupled with phylogenetic generalized least square (PGLS) or other statistical analysis to get more significant results. Ancestral state reconstruction is used in microbiome studies to impute trait values for taxa whose traits are unknown. This is commonly performed with PICRUSt, which relies on avaible databases. Phylogenetic variables are chosen by researchers according to the type of study: through the selection of some variables with significant biological informations, it is possible to reduce the dimension of the data to analyse.\n\nPhylogenetic aware distance is usually performed with UniFrac or similar tools, such as Soresen's index or Rao's D, to quantify the differences between the different communities. All this methods are negatively affected by horizontal gene trasmission (HGT), since it can generate errors and lead to the correlation of distant species. There are different ways to reduce the negative impact of HGT: the use of multiple genes or computational tools to assess the probability of putative HGT events.\n\nPopulations of microbes (such as bacteria and yeasts) inhabit the skin and mucosal surfaces in various parts of the body. Their role forms part of normal, healthy human physiology, however if microbe numbers grow beyond their typical ranges (often due to a compromised immune system) or if microbes populate (such as through poor hygiene or injury) areas of the body normally not colonized or sterile (such as the blood, or the lower respiratory tract, or the abdominal cavity), disease can result (causing, respectively, bacteremia/sepsis, pneumonia, and peritonitis).\n\nThe Human Microbiome Project found that individuals host thousands of bacterial types, different body sites having their own distinctive communities. Skin and vaginal sites showed smaller diversity than the mouth and gut, these showing the greatest richness. The bacterial makeup for a given site on a body varies from person to person, not only in type, but also in abundance. Bacteria of the same species found throughout the mouth are of multiple subtypes, preferring to inhabit distinctly different locations in the mouth. Even the enterotypes in the human gut, previously thought to be well understood, are from a broad spectrum of communities with blurred taxon boundaries.\n\nIt is estimated that 500 to 1,000 species of bacteria live in the human gut but belong to just a few phyla: Firmicutes and Bacteroidetes dominate but there are also Proteobacteria, Verrumicrobia, Actinobacteria, Fusobacteria and Cyanobacteria.\n\nA number of types of bacteria, such as \"Actinomyces viscosus\" and \"A. naeslundii\", live in the mouth, where they are part of a sticky substance called plaque. If this is not removed by brushing, it hardens into calculus (also called tartar). The same bacteria also secrete acids that dissolve tooth enamel, causing tooth decay.\n\nThe vaginal microflora consist mostly of various lactobacillus species. It was long thought that the most common of these species was \"Lactobacillus acidophilus\", but it has later been shown that \"L. iners\" is in fact most common, followed by \"L. crispatus\". Other lactobacilli found in the vagina are \"L. jensenii, L. delbruekii\" and \"L. gasseri\". Disturbance of the vaginal flora can lead to infections such as bacterial vaginosis or candidiasis (\"yeast infection\").\n\nArchaea are present in the human gut, but, in contrast to the enormous variety of bacteria in this organ, the numbers of archaeal species are much more limited. The dominant group are the methanogens, particularly \"Methanobrevibacter smithii\" and \"Methanosphaera stadtmanae\". However, colonization by methanogens is variable, and only about 50% of humans have easily detectable populations of these organisms.\n\nAs of 2007, no clear examples of archaeal pathogens were known, although a relationship has been proposed between the presence of some methanogens and human periodontal disease.\n\nFungi, in particular yeasts, are present in the human gut. The best-studied of these are \"Candida\" species due to their ability to become pathogenic in immunocompromised and even in healthy hosts. Yeasts are also present on the skin, such as \"Malassezia\" species, where they consume oils secreted from the sebaceous glands.\n\nViruses, especially bacterial viruses (bacteriophages), colonize various body sites. These colonized sites include the skin, gut, lungs, and oral cavity. Virus communities have been associated with some diseases, and do not simply reflect the bacterial communities.\n\nA study of twenty skin sites on each of ten healthy humans found 205 identified genera in nineteen bacterial phyla, with most sequences assigned to four phyla: Actinobacteria (51.8%), Firmicutes (24.4%), Proteobacteria (16.5%), and Bacteroidetes (6.3%). A large number of fungal genera are present on healthy human skin, with some variability by region of the body; however, during pathological conditions, certain genera tend to dominate in the affected region. For example, \"Malassezia\" is dominant in atopic dermatitis and \"Acremonium\" is dominant on dandruff-afflicted scalps.\n\nThe skin acts as a barrier to deter the invasion of pathogenic microbes. The human skin contains microbes that reside either in or on the skin and can be residential or transient. Resident microorganism types vary in relation to skin type on the human body. A majority of microbes reside on superficial cells on the skin or prefer to associate with glands. These glands such as oil or sweat glands provide the microbes with water, amino acids, and fatty acids. In addition, resident bacteria that associated with oil glands are often Gram-positive and can be pathogenic.\n\nA small number of bacteria and fungi are normally present in the conjunctiva. Classes of bacteria include Gram-positive cocci (e.g., \"Staphylococcus\" and Streptococcus) and Gram-negative rods and cocci (e.g., \"Haemophilus\" and \"Neisseria\") are present. Fungal genera include \"Candida\", \"Aspergillus\", and \"Penicillium\". The lachrymal glands continuously secrete, keeping the conjunctiva moist, while intermittent blinking lubricates the conjunctiva and washes away foreign material. Tears contain bactericides such as lysozyme, so that microorganisms have difficulty in surviving the lysozyme and settling on the epithelial surfaces.\n\nIn humans the composition of gut flora is established during birth. Birth by Cesarean section or vaginal delivery also influences the gut's microbial composition. Babies born through the vaginal canal have non-pathogenic, beneficial gut microbiota similar to those found in the mother. However, the gut microbiota of babies delivered by C-section harbors more pathogenic bacteria such as Escherichia coli and Staphylococcus and it takes longer to develop non-pathogenic, beneficial gut microbiota.\n\nThe relationship between some gut flora and humans is not merely commensal (a non-harmful coexistence), but rather a mutualistic relationship. Some human gut microorganisms benefit the host by fermentating dietary fiber into short-chain fatty acids (SCFAs), such as acetic acid and butyric acid, which are then absorbed by the host. Intestinal bacteria also play a role in synthesizing vitamin B and vitamin K as well as metabolizing bile acids, sterols, and xenobiotics. The systemic importance of the SCFAs and other compounds they produce are like hormones and the gut flora itself appears to function like an endocrine organ, and dysregulation of the gut flora has been correlated with a host of inflammatory and autoimmune conditions.\n\nThe composition of human gut flora changes over time, when the diet changes, and as overall health changes. A systematic review of 15 human randomized controlled trials from July 2016 found that certain commercially available strains of probiotic bacteria from the \"Bifidobacterium\" and \"Lactobacillus\" genera (\"B. longum\", \"B. breve\", \"B. infantis\", \"L. helveticus\", \"L. rhamnosus\", \"L. plantarum\", and \"L. casei\"), when taken by mouth in daily doses of 10–10 colony forming units (CFU) for 1–2 months, possess treatment efficacy (i.e., improves behavioral outcomes) in certain central nervous system disorders – including anxiety, depression, autism spectrum disorder, and obsessive–compulsive disorder – and improves certain aspects of memory. However, changes in the composition of gut microbiota has also been found to be correlated with harmful effects on health. In an article published by Musso et al., it was found that the gut microbiota of obese invidividuals had more Firmicutes and less Bacteroidetes than healthy individuals. Furthermore, a study done by Gordon et al., confirmed that it was the composition of the microbiota that causes obesity rather than the other way around. This was done by transplanting the gut microbiota from diet-induced obese(DIO) mice or lean control mice into lean germ-free mice that do not have a microbiome. They found that the mice transplanted with DIO mouse gut microbiota had significantly higher total body fat than the mice transplanted with lean mouse microbiota when fed with the same diet.\n\nThe genitourinary system appears to have a microbiota, which is an unexpected finding in light of the long-standing use of standard clinical microbiological culture methods to detect bacteria in urine when people show signs of a urinary tract infection; it is common for these tests to show no bacteria present. It appears that common culture methods do not detect many kinds of bacteria and other microorganisms that are normally present. As of 2017, sequencing methods were used to identify these microorganisms to determine if there are differences in microbiota between people with urinary tract problems and those who are healthy.\n\nVaginal microbiota refers to those species and genera that colonize the vagina. These organisms play an important role in protecting against infections and maintaining vaginal health. The most abundant vaginal microorganisms found in premenopausal women are from the genus \"Lactobacillus\", which suppress pathogens by producing hydrogen peroxide and lactic acid. Bacterial species composition and ratios vary depending on the stage of the menstrual cycle. Ethnicity also influences vaginal flora. The occurrence of hydrogen peroxide-producing lactobacilli is lower in African American women and vaginal pH is higher. Other influential factors such as sexual intercourse and antibiotics have been linked to the loss of lactobacilli. Moreover, studies have found that sexual intercourse with a condom does appear to change lactobacilli levels, and does increase the level of \"Escherichia coli\" within the vaginal flora. Changes in the normal, healthy vaginal microbiota is an indication of infections, such as candidiasis or bacterial vaginosis. \"Candida albicans\" inhibits the growth of \"Lactobacillus\" species, while \"Lactobacillus\" species which produce hydrogen peroxide inhibit the growth and virulence of \"Candida albicans\" in both the vagina and the gut.\n\nFungal genera that have been detected in the vagina include \"Candida\", \"Pichia\", \"Eurotium\", \"Alternaria\", \"Rhodotorula\", and \"Cladosporium\", among others.\n\nUntil recently the placenta was considered to be a sterile organ but commensal, nonpathogenic bacterial species and genera have been identified that reside in the placental tissue.\n\nUntil recently, the upper reproductive tract of women was considered to be a sterile environment. A variety of microorganisms inhabit the uterus of healthy, asymptomatic women of reproductive age. The microbiome of the uterus differs significantly from that of the vagina and gastrointestinal tract.\n\nThe environment present in the human mouth allows the growth of characteristic microorganisms found there. It provides a source of water and nutrients, as well as a moderate temperature. Resident microbes of the mouth adhere to the teeth and gums to resist mechanical flushing from the mouth to stomach where acid-sensitive microbes are destroyed by hydrochloric acid.\n\nAnaerobic bacteria in the oral cavity include: \"Actinomyces\", \"Arachnia\", \"Bacteroides\", \"Bifidobacterium\", \"Eubacterium\", \"Fusobacterium\", \"Lactobacillus\", \"Leptotrichia\", \"Peptococcus\", \"Peptostreptococcus\", \"Propionibacterium\", \"Selenomonas\", \"Treponema\", and \"Veillonella\". Genera of fungi that are frequently found in the mouth include \"Candida\", \"Cladosporium\", \"Aspergillus\", \"Fusarium\", \"Glomus\", \"Alternaria\", \"Penicillium\", and \"Cryptococcus\", among others.\n\nBacteria accumulate on both the hard and soft oral tissues in biofilm allowing them to adhere and strive in the oral environment while protected from the environmental factors and antimicrobial agents. Saliva plays a key biofilm homeostatic role allowing recolonization of bacteria for formation and controlling growth by detaching biofilm buildup. It also provides a means of nutrients and temperature regulation. The location of the biofilm determines the type of exposed nutrients it receives.\n\nOral bacteria have evolved mechanisms to sense their environment and evade or modify the host. However, a highly efficient innate host defense system constantly monitors the bacterial colonization and prevents bacterial invasion of local tissues. A dynamic equilibrium exists between dental plaque bacteria and the innate host defense system.\n\nThis dynamic between host oral cavity and oral microbes plays a key role in health and disease as it provides entry into the body.\nA healthy equilibrium presents a symbiotic relationship where oral microbes limit growth and adherence of pathogens while the host provides an environment for them to flourish. Ecological changes such as change of immune status, shift of resident microbes and nutrient availability shift from a mutual to parasitic relationship resulting in the host being prone to oral and systemic disease. Systemic diseases such as diabetes and cardiovascular diseases has been correlated to poor oral health. Of particular interest is the role of oral microorganisms in the two major dental diseases: dental caries and periodontal disease. Pathogen colonization at the periodontium cause an excessive immune response resulting in a periodontal pocket- a deepened space between the tooth and gingiva. This acts as a protected blood-rich reservoir with nutrients for anaerobic pathogens. Systemic disease at various sites of the body can result from oral microbes entering the blood bypassing periodontal pockets and oral membranes.\n\nPersistent proper oral hygiene is the primary method for preventing oral and systemic disease. It reduces the density of biofilm and overgrowth of potential pathogenic bacteria resulting in disease. However, proper oral hygiene may not be enough as the oral microbiome, genetics, and changes to immune response play a factor in developing chronic infections. Use of antibiotics could treat already spreading infection but ineffective against bacteria within biofilms.\n\nMuch like the oral cavity, the upper and lower respiratory system possess mechanical deterrents to remove microbes. Goblet cells produce mucous which traps microbes and moves them out of the respiratory system via continuously moving ciliated epithelial cells. In addition, a bactericidal effect is generated by nasal mucus which contains the enzyme lysozyme. The upper and lower respiratory tract appears to have its own set of microbiota. Pulmonary bacterial microbiota belong to 9 major bacterial genera: \"Prevotella\", \"Sphingomonas\", \"Pseudomonas\", \"Acinetobacter\", \"Fusobacterium\", \"Megasphaera\", \"Veillonella\", \"Staphylococcus\", and \"Streptococcus\". Some of the bacteria considered \"normal biota\" in the respiratory tract can cause serious disease especially in immunocompromised individuals; these include \"Streptococcus pyogenes\", \"Haemophilus influenzae\", \"Streptococcus pneumoniae\", \"Neisseria meningitidis\", and \"Staphylococcus aureus\". Fungal genera that compose the pulmonary mycobiome include \"Candida\", \"Malassezia\", \"Neosartorya\", \"Saccharomyces\", and \"Aspergillus\", among others.\n\nUnusual distributions of bacterial and fungal genera in the respiratory tract is observed in people with cystic fibrosis. Their bacterial flora often contains antibiotic-resistant and slow-growing bacteria, and the frequency of these pathogens changes in relation to age.\n\nTraditionally the biliary tract has been considered to be normally sterile, and the presence of microorganisms in bile is a marker of pathological process. This assumption was confirmed by failure in allocation of bacterial strains from the normal bile duct. Papers began emerging in 2013 showing that the normal biliary microbiota is a separate functional layer which protects a biliary tract from colonization by exogenous microorganisms.\n\nA symbiotic relationship between the gut microbiota and different bacteria may influence an individual's immune response.\n\nAlthough cancer is generally a disease of host genetics and environmental factors, microorganisms are implicated in some 20% of human cancers. Particularly for potential factors in colon cancer, bacterial density is one million times higher than in the small intestine, and approximately 12-fold more cancers occur in the colon compared to the small intestine, possibly establishing a pathogenic role for microbiota in colon and rectal cancers. Microbial density may be used as a prognostic tool in assessment of colorectal cancers.\n\nThe microbiota may affect carcinogenesis in three broad ways: (i) altering the balance of tumor cell proliferation and death, (ii) regulating immune system function, and (iii) influencing metabolism of host-produced factors, foods and pharmaceuticals. Tumors arising at boundary surfaces, such as the skin, oropharynx and respiratory, digestive and urogenital tracts, harbor a microbiota. Substantial microbe presence at a tumor site does not establish association or causal links. Instead, microbes may find tumor oxygen tension or nutrient profile supportive. Decreased populations of specific microbes or induced oxidative stress may also increase risks. Of the around 10 microbes on earth, ten are designated by the International Agency for Research on Cancer as human carcinogens. Microbes may secrete proteins or other factors directly drive cell proliferation in the host, or may up- or down-regulate the host immune system including driving acute or chronic inflammation in ways that contribute to carcinogenesis.\n\nConcerning the relationship of immune function and development of inflammation, mucosal surface barriers are subject to environmental risks and must rapidly repair to maintain homeostasis. Compromised host or microbiota resiliency also reduce resistance to malignancy, possibly inducing inflammation and cancer. Once barriers are breached, microbes can elicit proinflammatory or immunosuppressive programs through various pathways. For example, cancer-associated microbes appear to activate NF-κΒ signaling within the tumor microenviroment. Other pattern recognition receptors, such as nucleotide-binding oligomerization domain–like receptor (NLR) family members \"NOD-2\", \"NLRP3\", \"NLRP6\" and \"NLRP12\", may play a role in mediating colorectal cancer. Likewise \"Helicobacter pylori\" appears to increase the risk of gastric cancer, due to its driving a chronic inflammatory response in the stomach.\n\nInflammatory bowel disease consists of two different diseases: ulcerative colitis and Crohn's disease and both of these diseases present with disruptions in the gut microbiota (also known as dysbiosis). This dysbiosis presents itself in the form of decreased microbial diversity in the gut, and is correlated to defects in host genes that changes the innate immune response in individuals.\n\nThe HIV disease progression influences the composition and function of the gut microbiota, with notable differences between HIV-negative, HIV-positive, and post-ART HIV-positive populations. HIV decreases the integrity of the gut epithelial barrier function by affecting tight junctions. This breakdown allows for translocation across the gut epithelium, which is thought to contribute to increases in inflammation seen in people with HIV.\n\nVaginal microbiota plays a role in the infectivity of HIV, with an increased risk of infection and transmission when the woman has bacterial vaginosis, a condition characterized by an abnormal balance of vaginal bacteria. The enhanced infectivity is seen with the increase in pro-inflammatory cytokines and CCR5 + CD4+ cells in the vagina. However, a decrease in infectivity is seen with increased levels of vaginal \"Lactobacillus,\" which promotes an anti-inflammatory condition.\n\nWith death the microbiome of the living body collapses and a different microbiome named necrobiome establishes itself. Its predictable changes over time are thought to be useful to help determine the time of death.\n\nStudies in 2009 questioned whether the decline in biota (including microfauna) as a result of human intervention might impede human health, hospital safety procedures, food product design, and treatments of disease.\n\nPreliminary research indicates that immediate changes in the microbiota may occur when a person migrates from one country to another, such as when Thai immigrants settled in the United States. Losses of microbiota diversity were greater in obese individuals and children of immigrants.\n\n\n\n"}
{"id": "5636629", "url": "https://en.wikipedia.org/wiki?curid=5636629", "title": "Ignacio Bolívar", "text": "Ignacio Bolívar\n\nIgnacio Bolívar y Urrutia (9 November 1850, Madrid – 19 November 1944, Mexico) was a Spanish naturalist and entomologist, and one of the founding fathers of Spanish entomology. He helped found the \"Real Sociedad Española de Historia Natural\" (Royal Spanish Natural History Society) in 1871, and was the author of several books and of over 1000 species.\n\nAfter the Spanish Civil War he was exiled to Mexico when the nationalist government harshly repressed Republican militants and sympathisers, as retaliation for the equally harsh repression of clergy and nationalist militants on the opposite side. Here he was made \"Doctor honoris\" of the National Autonomous University of Mexico.\nIn Mexico he was devoted mainly to entomology. In this field he wrote more than 300 books and monographs and discovered more than thousand new species and about 200 genera. \n\nHe also encouraged other naturalists to study entomology, José María de la Fuente being one example.\n\nBolívar founded the journal \"Ciencia\" (Science). His more important works include:-\n\"Ortópteros de España nuevos o poco conocidos\" (1873) and \"Catálogo sinóptico de los ortópteros de la fauna ibérica\" (1900).\n"}
{"id": "16781685", "url": "https://en.wikipedia.org/wiki?curid=16781685", "title": "Informal social control", "text": "Informal social control\n\nInformal social control, or the reactions of individuals and groups that bring about conformity to norms and laws, includes peer and community pressure, bystander intervention in a crime, and collective responses such as citizen patrol groups. The agents of the criminal justice system exercise more control when informal social control is weaker (Black, 1976). It is people who know each other informally controlling each other in subtle ways subconsciously.\n\n"}
{"id": "8580973", "url": "https://en.wikipedia.org/wiki?curid=8580973", "title": "Jeremy S. Heyl", "text": "Jeremy S. Heyl\n\nJeremy Samuel Heyl is an astronomer and a Professor at the University of British Columbia's Department of Physics and Astronomy, in Vancouver, British Columbia. He holds a Canada Research Chair in Black Holes and Neutron Stars. In the past he was a Goldwater Scholar, a Marshall Scholar and a Chandra Fellow.\n\nHeyl is best known for his work in the physics of neutron stars especially the importance of quantum electrodynamics in radiative transfer, non-radial oscillations during Type-I X-ray bursts and the cooling of magnetars. He has also made important contributions to our understanding of galaxy formation, evolution and mergers.\n\n"}
{"id": "39241070", "url": "https://en.wikipedia.org/wiki?curid=39241070", "title": "Kilden – Information Centre for Gender Research", "text": "Kilden – Information Centre for Gender Research\n\nKilden – Information Centre for Gender Research (\"Kilden – informasjonssenter for kjønnsforskning\") is a gender research center established in 1998 by Research Council of Norway.\n\nThe centre is a subdivision of Research Council of Norway, with a board of directors appointed by the council's CEO. It reports to the council's Science Division.\n\n"}
{"id": "56351815", "url": "https://en.wikipedia.org/wiki?curid=56351815", "title": "Lisa Hardaway", "text": "Lisa Hardaway\n\nLisa Hardaway (1967–2017) was an American aerospace engineer. She was named Engineer of the Year for 2015–2016 by Colorado American Institute of Aeronautics and Astronautics.\n\nShe graduated from Massachusetts Institute of Technology, Stanford University, and University of Colorado. She worked for Ball Aerospace. She was program manager for RALPH, on the \"New Horizons\" mission.\n\n\n"}
{"id": "2055634", "url": "https://en.wikipedia.org/wiki?curid=2055634", "title": "List of Microsoft Windows application programming interfaces and frameworks", "text": "List of Microsoft Windows application programming interfaces and frameworks\n\nThe following is a list of Microsoft APIs and frameworks.\n\n\n\n\n\n"}
{"id": "1048723", "url": "https://en.wikipedia.org/wiki?curid=1048723", "title": "List of Russian philosophers", "text": "List of Russian philosophers\n\nRussian philosophy includes a variety of philosophical movements. Authors who developed them are listed below sorted by movement.\n\nWhile most authors listed below are primarily philosophers, also included here are some Russian fiction writers, such as Tolstoy and Dostoyevsky, who are also known as philosophers.\n\nRussian philosophy as a separate entity started its development in the 19th century, defined initially by the opposition of Westernizers, advocating Russia's following the Western political and economical models, and Slavophiles, insisting on developing Russia as a unique civilization. The latter group included Nikolai Danilevsky and Konstantin Leontiev, the early founders of eurasianism. The discussion of Russia's place in the world has since become the most characteristic feature of Russian philosophy.\n\nIn its further development, Russian philosophy was also marked by deep connection to literature and interest in creativity, society, politics and nationalism; cosmos and religion were other notable subjects.\n\nNotable philosophers of the late 19th and early 20th centuries include Vladimir Solovyev, Vasily Rozanov, Lev Shestov, Leo Tolstoy, Sergei Bulgakov, Pavel Florensky, Nikolai Berdyaev, Pitirim Sorokin, and Vladimir Vernadsky.\n\nFrom the early 1920s to late 1980s, Russian philosophy was dominated by Marxism presented as dogma and not grounds for discussion. Stalin's purges, culminating in 1937, delivered a deadly blow to the development of philosophy.\n\nA handful of dissident philosophers survived through the Soviet period, among them Aleksei Losev. Stalin's death in 1953 gave way for new schools of thought to spring up, among them Moscow Logic Circle, and Tartu-Moscow Semiotic School.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPre-Solovyov\n\n\n\n\n\n\n\n\n\n"}
{"id": "42504960", "url": "https://en.wikipedia.org/wiki?curid=42504960", "title": "List of Spanish inventions and discoveries", "text": "List of Spanish inventions and discoveries\n\nThe following list is composed of items, techniques and processes that were invented by or discovered by people from Spain.\n\nSpain was an important center of knowledge during the medieval era. While most of western and southern Europe suffered from the collapse of the Roman Empire, although declining, some regions of the former empire, Hispania (the Iberian Peninsula), southern Italy, and the remainder of the Eastern Roman Empire or Byzantine Empire, did not to suffer from the full impact of the so-called Dark Ages when education collapsed with the collapse of the empire and most knowledge was lost. The Islamic conquests of places such as Egypt, which was a major part of the Byzantine Empire, and other places which were centers of knowledge in earlier times, gave the Muslims access to knowledge from many cultures which they translated into Arabic and recorded in books for the use of their own educated elites, who flourished in this period, and took with them to the Hispania after it fell under Muslim control. Much of this knowledge was later translated by Christian and Jewish scholars in the Christian kingdoms of the Reconquista from Arabic into Latin, and from there it spread through Europe.\n\n\n\n\n\n\n\nSpain has been a center of fashion since medieval times. Barcelona and Madrid have both been named as fashion capitals of the world, with Barcelona being the fifth most important fashion capital in the world back in 2015. Spain is the home country of the largest fashion retail store and textiles designer in the world, Zara and its parent Inditex, making their CEO main shareholder, Amancio Ortega Gaona, the second wealthiest man in the world in 2015.\n\nInventions:\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "38940908", "url": "https://en.wikipedia.org/wiki?curid=38940908", "title": "List of botanists by author abbreviation (T–V)", "text": "List of botanists by author abbreviation (T–V)\n\nTo find entries for A–S, use the table of contents above.\n\n\n\n\nTo find entries for W–Z, use the table of contents above.\n"}
{"id": "2679924", "url": "https://en.wikipedia.org/wiki?curid=2679924", "title": "List of compounds with carbon number 11", "text": "List of compounds with carbon number 11\n\nThis is a partial list of molecules that contain 11 carbon atoms.\n\n"}
{"id": "4109538", "url": "https://en.wikipedia.org/wiki?curid=4109538", "title": "List of political science journals", "text": "List of political science journals\n\nThis is a list of political science journals presenting representative academic journals in the field of political science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "56842637", "url": "https://en.wikipedia.org/wiki?curid=56842637", "title": "Lists of molecules", "text": "Lists of molecules\n\nThis is an index of lists of molecules (i.e. by year, number of atoms, etc.). Millions of molecules have existed in the universe before the formation of Earth, elements have being mixed and formed molecules for millions of years, three of them, carbon dioxide, water and oxygen were necessary for the growth of life, even thought, we were able to see these substances we did not know what was their components.\n\nAmedeo Avogadro created the word \"molecule\". His 1811 paper \"Essay on Determining the Relative Masses of the Elementary Molecules of Bodies\", he essentially states, i.e. according to Partington's \"A Short History of Chemistry\", that:\n\nThe following is an index of list of molecules organized by time of discovery of their molecular formula or their specific molecule in case of isomers:\n\n\n\n"}
{"id": "58980298", "url": "https://en.wikipedia.org/wiki?curid=58980298", "title": "Marc Grégoire (Tefal)", "text": "Marc Grégoire (Tefal)\n\nMarc Grégoire (Tefal) was the inventor of nonstick pans. When his wife learned of an ONERA engineer using Teflon to assist in the removal of glass fibre moulds, she challenged him to create some non-stick cookware. He attached Teflon to the base of an aluminum frying pan. It was Grégoire who found a way to bond PTFE to aluminum creating the first ever nonstick cookware (in 1954) which later became the world leader in nonstick pans. In 1956 Grégoire and his wife launched the Tefal Corporation developing the slogan: \"La Poêle Tefal, la poêle qui n’attache vraimant pas (The Tefal saucepan, the saucepan pan that really doesn’t stick.\") In 2018 it is still regarded as one of the best non-stick pans. \n"}
{"id": "3306344", "url": "https://en.wikipedia.org/wiki?curid=3306344", "title": "Natterer compressor", "text": "Natterer compressor\n\nA Natterer compressor was a type of air compression machine which was used in early experiments in making liquid oxygen (LOX) in the 1870s. A manually operated screw jack was utilized to compress air or other gases up to ~200 atm (~3000 psi).\n\nThe device was created by Johann Natterer, a student of Adolf Martin Pleischl, for experiments creating liquid carbonic acid.\n"}
{"id": "37354359", "url": "https://en.wikipedia.org/wiki?curid=37354359", "title": "Nature Arabic Edition", "text": "Nature Arabic Edition\n\nNature Arabic Edition is an online publication by Nature Publishing Group (NPG) in partnership with the King Abdulaziz City for Science and Technology. The magazine was started in 2012. It contains high-quality science news from the original \"Nature\" journal. The content of this journal is available online for free, and the print issues are also available to qualified subscribers for free.\n\n\n"}
{"id": "1674369", "url": "https://en.wikipedia.org/wiki?curid=1674369", "title": "Patrick J. Keeling", "text": "Patrick J. Keeling\n\nPatrick John Keeling is a biologist and professor in the Department of Botany at the University of British Columbia. His research investigates the phylogeny, genomics and molecular evolution of protists and his work has led to numerous advances in assembling the eukaryotic tree of life. He has also identified several cases of horizontal gene transfer.\n"}
{"id": "1776140", "url": "https://en.wikipedia.org/wiki?curid=1776140", "title": "Power law of practice", "text": "Power law of practice\n\nThe power law of practice states that the logarithm of the reaction time for a particular task decreases linearly with the logarithm of the number of practice trials taken. It is an example of the learning curve effect on performance. It was first proposed as a psychological law by Snoddy (1928), used by Crossman (1959) in his study of a cigar roller in Cuba, and played an important part in the development of Cognitive Engineering by Card, Moran, & Newell (1983). Mechanisms that would explain the power law were popularized by Fitts and Posner (1967), Newell and Rosenbloom (1981), and Anderson (1982).\n\nHowever, subsequent research by Heathcote, Brown, and Mewhort suggests that the power function observed in learning curves that are averaged across participants is an artifact of aggregation. Heathcote et al. suggest that individual-level data is better fit by an exponential function and the authors demonstrate that the multiple exponential curves will average to produce a curve that is misleadingly well fit by a power function. \n\nThe power function is based on the idea that something is slowing down the learning process; at least, this is what the function suggests. Our learning does not occur at a constant rate according to the this function; our learning is hindered. The exponential function shows that learning increases at a constant rate in relationship to what is left to be learned. If you know absolutely nothing about a topic, you can learn 50% of the information quickly, but when you have 50% less to learn, it takes more time to learn that final 50%.\n\nResearch by Logan suggests that the instance theory of automaticity can be used to explain why the power law is deemed an accurate portrayal of reaction time learning curves. A skill is automatic when there is one step from stimulus to retrieval. For many problem solving tasks (see table below), reaction time is related to how long it takes to discover an answer, but as time goes on, certain answers are stored within the individual's memory and they have to simply recall the information, thus reducing reaction time. This is the first theory that addresses the why of the power law of practice. \n\nPower function:\n\nExponential function:\n\nWhere\n\nPractice effects are also influenced by latency. Anderson, Fincham, and Douglass looked at the relationship between practice and latency and people's ability to retain what they learned. As the time between trials increases, there is greater decay. The latency function relates to the forgetting curve.\n\nLatency function:\n\nlatency = A + B*T\n\nWhere\n\nA = asymptotic latency\nB = latency that varies\nT = time between introduction and testing\nd = decay rate\n\n"}
{"id": "323779", "url": "https://en.wikipedia.org/wiki?curid=323779", "title": "Structural functionalism", "text": "Structural functionalism\n\nStructural functionalism, or simply functionalism, is \"a framework for building theory that sees society as a complex system whose parts work together to promote solidarity and stability\". This approach looks at society through a macro-level orientation, which is a broad focus on the social structures that shape society as a whole, and believes that society has evolved like organisms. This approach looks at both social structure and social functions. Functionalism addresses society as a whole in terms of the function of its constituent elements; namely norms, customs, traditions, and institutions.\n\nA common analogy, popularized by Herbert Spencer, presents these parts of society as \"organs\" that work toward the proper functioning of the \"body\" as a whole. In the most basic terms, it simply emphasizes \"the effort to impute, as rigorously as possible, to each feature, custom, or practice, its effect on the functioning of a supposedly stable, cohesive system\". For Talcott Parsons, \"structural-functionalism\" came to describe a particular stage in the methodological development of social science, rather than a specific school of thought.\n\nClassical theories are defined by a tendency towards biological analogy and notions of social evolutionism:\n\nWhile one may regard functionalism as a logical extension of the organic analogies for societies presented by political philosophers such as Rousseau, sociology draws firmer attention to those institutions unique to industrialized capitalist society (or \"modernity\"). Functionalism also has an anthropological basis in the work of theorists such as Marcel Mauss, Bronisław Malinowski and Radcliffe-Brown. It is in Radcliffe-Brown's specific usage that the prefix 'structural' emerged. Radcliffe-Brown proposed that most stateless, \"primitive\" societies, lacking strong centralized institutions, are based on an association of corporate-descent groups. Structural functionalism also took on Malinowski's argument that the basic building block of society is the nuclear family, and that the clan is an outgrowth, not \"vice versa\".\n\nÉmile Durkheim was concerned with the question of how certain societies maintain internal stability and survive over time. He proposed that such societies tend to be segmented, with equivalent parts held together by shared values, common symbols or, as his nephew Marcel Mauss held, systems of exchanges. Durkheim used the term \"mechanical solidarity\" to refer to these types of \"social bonds, based on common sentiments and shared moral values, that are strong among members of pre-industrial societies\". In modern, complex societies, members perform very different tasks, resulting in a strong interdependence. Based on the metaphor above of an organism in which many parts function together to sustain the whole, Durkheim argued that complex societies are held together by \"organic\" solidarity, i.e. \"social bonds, based on specialization and interdependence, that are strong among members of industrial societies\".\n\nThese views were upheld by Durkheim, who, following Auguste Comte, believed that society constitutes a separate \"level\" of reality, distinct from both biological and inorganic matter. Explanations of social phenomena had therefore to be constructed within this level, individuals being merely transient occupants of comparatively stable social roles. The central concern of structural functionalism is a continuation of the Durkheimian task of explaining the apparent stability and internal cohesion needed by societies to endure over time. Societies are seen as coherent, bounded and fundamentally relational constructs that function like organisms, with their various (or social institutions) working together in an unconscious, quasi-automatic fashion toward achieving an overall social equilibrium. All social and cultural phenomena are therefore seen as functional in the sense of working together, and are effectively deemed to have \"lives\" of their own. They are primarily analyzed in terms of this function. The individual is significant not in and of himself, but rather in terms of his status, his position in patterns of social relations, and the behaviours associated with his status. Therefore, the social structure is the network of statuses connected by associated roles.\n\nIt is simplistic to equate the perspective directly with political conservatism. The tendency to emphasize \"cohesive systems\", however, leads functionalist theories to be contrasted with \"conflict theories\" which instead emphasize social problems and inequalities.\n\nAuguste Comte, the \"Father of Positivism\", pointed out the need to keep society unified as many traditions were diminishing. He was the first person to coin the term sociology. Comte suggests that sociology is the product of a three-stage development:\n\n\nHerbert Spencer (1820–1903) was a British philosopher famous for applying the theory of natural selection to society. He was in many ways the first true sociological functionalist. In fact, while Durkheim is widely considered the most important functionalist among positivist theorists, it is known that much of his analysis was culled from reading Spencer's work, especially his \"Principles of Sociology\" (1874–96). In describing society, Spencer alludes to the analogy of a human body. Just as the structural parts of the human body — the skeleton, muscles, and various internal organs — function independently to help the entire organism survive, social structures work together to preserve society.\n\nWhile reading Spencer's massive volumes can be tedious (long passages explicating the organic analogy, with reference to cells, simple organisms, animals, humans and society), there are some important insights that have quietly influenced many contemporary theorists, including Talcott Parsons, in his early work \"The Structure of Social Action\" (1937). Cultural anthropology also consistently uses functionalism.\n\nThis evolutionary model, unlike most 19th century evolutionary theories, is cyclical, beginning with the differentiation and increasing complication of an organic or \"super-organic\" (Spencer's term for a social system) body, followed by a fluctuating state of equilibrium and disequilibrium (or a state of adjustment and adaptation), and, finally, the stage of disintegration or dissolution. Following Thomas Malthus' population principles, Spencer concluded that society is constantly facing selection pressures (internal and external) that force it to adapt its internal structure through differentiation.\n\nEvery solution, however, causes a new set of selection pressures that threaten society's viability. It should be noted that Spencer was not a determinist in the sense that he never said that\n\n\nIn fact, he was in many ways a political sociologist, and recognized that the degree of centralized and consolidated authority in a given polity could make or break its ability to adapt. In other words, he saw a general trend towards the centralization of power as leading to stagnation and ultimately, pressures to decentralize.\n\nMore specifically, Spencer recognized three functional needs or prerequisites that produce selection pressures: they are regulatory, operative (production) and distributive. He argued that all societies need to solve problems of control and coordination, production of goods, services and ideas, and, finally, to find ways of distributing these resources.\n\nInitially, in tribal societies, these three needs are inseparable, and the kinship system is the dominant structure that satisfies them. As many scholars have noted, all institutions are subsumed under kinship organization, but, with increasing population (both in terms of sheer numbers and density), problems emerge with regard to feeding individuals, creating new forms of organization—consider the emergent division of labour—coordinating and controlling various differentiated social units, and developing systems of resource distribution.\n\nThe solution, as Spencer sees it, is to differentiate structures to fulfill more specialized functions; thus a chief or \"big man\" emerges, soon followed by a group of lieutenants, and later kings and administrators. The structural parts of society (e.g. families, work) function interdependently to help society function. Therefore, social structures work together to preserve society.\n\nPerhaps Spencer's greatest obstacle that is being widely discussed in modern sociology is the fact that much of his social philosophy is rooted in the social and historical context of ancient Egypt. He coined the term \"survival of the fittest\" in discussing the simple fact that small tribes or societies tend to be defeated or conquered by larger ones. Of course, many sociologists still use his ideas (knowingly or otherwise) in their analyses, especially due to the recent re-emergence of evolutionary theory.\n\nTalcott Parsons began writing in the 1930s and contributed to sociology, political science, anthropology, and psychology. Structural functionalism and Parsons have received a lot of criticism. Numerous critics have pointed out Parsons' underemphasis of political and monetary struggle, the basics of social change, and the by and large \"manipulative\" conduct unregulated by qualities and standards. Structural functionalism, and a large portion of Parsons' works, appear to be insufficient in their definitions concerning the connections amongst institutionalized and non-institutionalized conduct, and the procedures by which institutionalization happens.\n\nParsons was heavily influenced by Durkheim and Max Weber, synthesizing much of their work into his action theory, which he based on the system-theoretical concept and the methodological principle of voluntary action. He held that \"the social system is made up of the actions of individuals.\" His starting point, accordingly, is the interaction between two individuals faced with a variety of choices about how they might act, choices that are influenced and constrained by a number of physical and social factors.\n\nParsons determined that each individual has expectations of the other's action and reaction to his own behavior, and that these expectations would (if successful) be \"derived\" from the accepted norms and values of the society they inhabit. As Parsons himself emphasized, in a general context there would never exist any perfect \"fit\" between behaviors and norms, so such a relation is never complete or \"perfect\".\n\nSocial norms were always problematic for Parsons, who never claimed (as has often been alleged) that social norms were generally accepted and agreed upon, should this prevent some kind of universal law. Whether social norms were accepted or not was for Parsons simply a historical question.\n\nAs behaviors are repeated in more interactions, and these expectations are entrenched or institutionalized, a role is created. Parsons defines a \"role\" as the normatively-regulated participation \"of a person in a concrete process of social interaction with specific, concrete role-partners.\" Although any individual, theoretically, can fulfill any role, the individual is expected to conform to the norms governing the nature of the role they fulfill.\n\nFurthermore, one person can and does fulfill many different roles at the same time. In one sense, an individual can be seen to be a \"composition\" of the roles he inhabits. Certainly, today, when asked to describe themselves, most people would answer with reference to their societal roles.\n\nParsons later developed the idea of roles into collectivities of roles that complement each other in fulfilling functions for society. Some roles are bound up in institutions and social structures (economic, educational, legal and even gender-based). These are functional in the sense that they assist society in operating and fulfilling its functional needs so that society runs smoothly.\n\nContrary to prevailing myth, Parsons never spoke about a society where there was no conflict or some kind of \"perfect\" equilibrium. A society's cultural value-system was in the typical case never completely integrated, never static and most of the time, like in the case of the American society, in a complex state of transformation relative to its historical point of departure. To reach a \"perfect\" equilibrium was not any serious theoretical question in Parsons analysis of social systems, indeed, the most dynamic societies had generally cultural systems with important inner tensions like the US and India. These tensions were a source of their strength according to Parsons rather than the opposite. Parsons never thought about system-institutionalization and the level of strains (tensions, conflict) in the system as opposite forces per se.\n\nThe key processes for Parsons for system reproduction are socialization and social control. Socialization is important because it is the mechanism for transferring the accepted norms and values of society to the individuals within the system. Parsons never spoke about \"perfect socialization\"—in any society socialization was only partial and \"incomplete\" from an integral point of view.\n\nParsons states that \"this point [...] is independent of the sense in which [the] individual is concretely autonomous or creative rather than 'passive' or 'conforming', for individuality and creativity, are to a considerable extent, phenomena of the institutionalization of expectations\"; they are culturally constructed.\n\nSocialization is supported by the positive and negative sanctioning of role behaviours that do or do not meet these expectations. A punishment could be informal, like a snigger or gossip, or more formalized, through institutions such as prisons and mental homes. If these two processes were perfect, society would become static and unchanging, but in reality this is unlikely to occur for long.\n\nParsons recognizes this, stating that he treats \"the structure of the system as problematic and subject to change,\" and that his concept of the tendency towards equilibrium \"does not imply the empirical dominance of stability over change.\" He does, however, believe that these changes occur in a relatively smooth way.\n\nIndividuals in interaction with changing situations adapt through a process of \"role bargaining\". Once the roles are established, they create norms that guide further action and are thus institutionalized, creating stability across social interactions. Where the adaptation process cannot adjust, due to sharp shocks or immediate radical change, structural dissolution occurs and either new structures (or therefore a new system) are formed, or society dies. This model of social change has been described as a \"moving equilibrium\", and emphasizes a desire for social order.\n\nKingsley Davis and Wilbert E. Moore (1945) gave an argument for social stratification based on the idea of \"functional necessity\" (also known as the Davis-Moore hypothesis). They argue that the most difficult jobs in any society have the highest incomes in order to motivate individuals to fill the roles needed by the division of labour. Thus inequality serves social stability.\n\nThis argument has been criticized as fallacious from a number of different angles: the argument is both that the individuals who are the most deserving are the highest rewarded, and that \"a system of unequal rewards\" is necessary, otherwise no individuals would perform as needed for the society to function. The problem is that these rewards are supposed to be based upon objective merit, rather than subjective \"motivations.\" The argument also does not clearly establish why some positions are worth more than others, even when they benefit more people in society, e.g., teachers compared to athletes and movie stars. Critics have suggested that structural inequality (inherited wealth, family power, etc.) is itself a cause of individual success or failure, not a consequence of it.\n\nRobert K. Merton made important refinements to functionalist thought. He fundamentally agreed with Parsons' theory. However, he acknowledged Parsons' theory problematic, believing that it was over generalized. Merton tended to emphasize middle range theory rather than a grand theory, meaning that he was able to deal specifically with some of the limitations in Parsons' theory. Merton believed that any social structure probably has many functions, some more obvious than others. He identified 3 main limitations: functional unity, universal functionalism and indispensability. He also developed the concept of deviance and made the distinction between manifest and latent functions. Manifest functions referred to the recognized and intended consequences of any social pattern. Latent functions referred to unrecognized and unintended consequences of any social pattern.\n\nMerton criticized functional unity, saying that not all parts of a modern complex society work for the functional unity of society. Consequently, there is a social dysfunction referred to as any social pattern that may disrupt the operation of society. Some institutions and structures may have other functions, and some may even be generally dysfunctional, or be functional for some while being dysfunctional for others. This is because not all structures are functional for society as a whole. Some practices are only functional for a dominant individual or a group. There are two types of functions that Merton discusses the \"manifest functions\" in that a social pattern can trigger a recognized and intended consequence. The manifest function of education includes preparing for a career by getting good grades, graduation and finding good job. The second type of function is \"latent functions\", where a social pattern results in an unrecognized or unintended consequence. The latent functions of education include meeting new people, extra-curricular activities, school trips. Another type of social function is \"social dysfunction\" which is any undesirable consequences that disrupts the operation of society. The social dysfunction of education includes not getting good grades, a job. Merton states that by recognizing and examining the dysfunctional aspects of society we can explain the development and persistence of alternatives. Thus, as Holmwood states, \"Merton explicitly made power and conflict central issues for research within a functionalist paradigm.\"\n\nMerton also noted that there may be functional alternatives to the institutions and structures currently fulfilling the functions of society. This means that the institutions that currently exist are not indispensable to society. Merton states \"just as the same item may have multiple functions, so may the same function be diversely fulfilled by alternative items.\" This notion of functional alternatives is important because it reduces the tendency of functionalism to imply approval of the status quo.\n\nMerton's theory of deviance is derived from Durkheim's idea of anomie. It is central in explaining how internal changes can occur in a system. For Merton, anomie means a discontinuity between cultural goals and the accepted methods available for reaching them.\n\nMerton believes that there are 5 situations facing an actor.\nThus it can be seen that change can occur internally in society through either innovation or rebellion. It is true that society will attempt to control these individuals and negate the changes, but as the innovation or rebellion builds momentum, society will eventually adapt or face dissolution.\n\nIn the 1970s, political scientists Gabriel Almond and Bingham Powell introduced a structural-functionalist approach to comparing political systems. They argued that, in order to understand a political system, it is necessary to understand not only its institutions (or structures) but also their respective functions. They also insisted that these institutions, to be properly understood, must be placed in a meaningful and dynamic historical context.\n\nThis idea stood in marked contrast to prevalent approaches in the field of comparative politics—the state-society theory and the dependency theory. These were the descendants of David Easton's system theory in international relations, a mechanistic view that saw all political systems as essentially the same, subject to the same laws of \"stimulus and response\"—or inputs and outputs—while paying little attention to unique characteristics. The structural-functional approach is based on the view that a political system is made up of several key components, including interest groups, political parties and branches of government.\n\nIn addition to structures, Almond and Powell showed that a political system consists of various functions, chief among them political socialization, recruitment and communication: socialization refers to the way in which societies pass along their values and beliefs to succeeding generations, and in political terms describe the process by which a society inculcates civic virtues, or the habits of effective citizenship; recruitment denotes the process by which a political system generates interest, engagement and participation from citizens; and communication refers to the way that a system promulgates its values and information.\n\nIn their attempt to explain the social stability of African \"primitive\" stateless societies where they undertook their fieldwork, Evans-Pritchard (1940) and Meyer Fortes (1945) argued that the Tallensi and the Nuer were primarily organized around unilineal descent groups. Such groups are characterized by common purposes, such as administering property or defending against attacks; they form a permanent social structure that persists well beyond the lifespan of their members. In the case of the Tallensi and the Nuer, these corporate groups were based on kinship which in turn fitted into the larger structures of unilineal descent; consequently Evans-Pritchard's and Fortes' model is called \"descent theory\". Moreover, in this African context territorial divisions were aligned with lineages; descent theory therefore synthesized both blood and soil as the same. Affinal ties with the parent through whom descent is not reckoned, however, are considered to be merely complementary or secondary (Fortes created the concept of \"complementary filiation\"), with the reckoning of kinship through descent being considered the primary organizing force of social systems. Because of its strong emphasis on unilineal descent, this new kinship theory came to be called \"descent theory\".\n\nWith no delay, descent theory had found its critics. Many African tribal societies seemed to fit this neat model rather well, although Africanists, such as Paul Richards, also argued that Fortes and Evans-Pritchard had deliberately downplayed internal contradictions and overemphasized the stability of the local lineage systems and their significance for the organization of society. However, in many Asian settings the problems were even more obvious. In Papua New Guinea, the local patrilineal descent groups were fragmented and contained large amounts of non-agnates. Status distinctions did not depend on descent, and genealogies were too short to account for social solidarity through identification with a common ancestor. In particular, the phenomenon of cognatic (or bilateral) kinship posed a serious problem to the proposition that descent groups are the primary element behind the social structures of \"primitive\" societies.\n\nLeach's (1966) critique came in the form of the classical Malinowskian argument, pointing out that \"in Evans-Pritchard's studies of the Nuer and also in Fortes's studies of the Tallensi unilineal descent turns out to be largely an ideal concept to which the empirical facts are only adapted by means of fictions.\" People's self-interest, manoeuvring, manipulation and competition had been ignored. Moreover, descent theory neglected the significance of marriage and affinal ties, which were emphasized by Levi-Strauss' structural anthropology, at the expense of overemphasizing the role of descent. To quote Leach: \"The evident importance attached to matrilateral and affinal kinship connections is not so much explained as explained away.\"\n\nStructural functionalism reached the peak of its influence in the 1940s and 1950s, and by the 1960s was in rapid decline. By the 1980s, its place was taken in Europe by more conflict-oriented approaches, and more recently by structuralism. While some of the critical approaches also gained popularity in the United States, the mainstream of the discipline has instead shifted to a myriad of empirically-oriented middle-range theories with no overarching theoretical orientation. To most sociologists, functionalism is now \"as dead as a dodo\".\n\nAs the influence of functionalism in the 1960s began to wane, the linguistic and cultural turns led to a myriad of new movements in the social sciences: \"According to Giddens, the orthodox consensus terminated in the late 1960s and 1970s as the middle ground shared by otherwise competing perspectives gave way and was replaced by a baffling variety of competing perspectives. This third generation of social theory includes phenomenologically inspired approaches, critical theory, ethnomethodology, symbolic interactionism, structuralism, post-structuralism, and theories written in the tradition of hermeneutics and ordinary language philosophy.\"\n\nWhile absent from empirical sociology, functionalist themes remained detectable in sociological theory, most notably in the works of Luhmann and Giddens. There are, however, signs of an incipient revival, as functionalist claims have recently been bolstered by developments in multilevel selection theory and in empirical research on how groups solve social dilemmas. Recent developments in evolutionary theory—especially by biologist David Sloan Wilson and anthropologists Robert Boyd and Peter Richerson—have provided strong support for structural functionalism in the form of multilevel selection theory. In this theory, culture and social structure are seen as a Darwinian (biological or cultural) adaptation at the group level.\n\nIn the 1960s, functionalism was criticized for being unable to account for social change, or for structural contradictions and conflict (and thus was often called \"consensus theory\"). Also, it ignores inequalities including race, gender, class, which cause tension and conflict. The refutation of the second criticism of functionalism, that it is static and has no concept of change, has already been articulated above, concluding that while Parsons' theory allows for change, it is an orderly process of change [Parsons, 1961:38], a moving equilibrium. Therefore, referring to Parsons' theory of society as static is inaccurate. It is true that it does place emphasis on equilibrium and the maintenance or quick return to social order, but this is a product of the time in which Parsons was writing (post-World War II, and the start of the cold war). Society was in upheaval and fear abounded. At the time social order was crucial, and this is reflected in Parsons' tendency to promote equilibrium and social order rather than social change.\n\nFurthermore, Durkheim favoured a radical form of guild socialism along with functionalist explanations. Also, Marxism, while acknowledging social contradictions, still uses functionalist explanations. Parsons' evolutionary theory describes the differentiation and reintegration systems and subsystems and thus at least temporary conflict before reintegration (\"ibid\"). \"The fact that functional analysis can be seen by some as inherently conservative and by others as inherently radical suggests that it may be \"inherently\" neither one nor the other.\"\n\nStronger criticisms include the epistemological argument that functionalism is tautologous, that is it attempts to account for the development of social institutions solely through recourse to the effects that are attributed to them and thereby explains the two circularly. However, Parsons drew directly on many of Durkheim's concepts in creating his theory. Certainly Durkheim was one of the first theorists to explain a phenomenon with reference to the function it served for society. He said, \"the determination of function is…necessary for the complete explanation of the phenomena.\" However Durkheim made a clear distinction between historical and functional analysis, saying, \"When ... the explanation of a social phenomenon is undertaken, we must seek separately the efficient cause which produces it and the function it fulfills.\" If Durkheim made this distinction, then it is unlikely that Parsons did not. However Merton does explicitly state that functional analysis does not seek to explain why the action happened in the first instance, but why it continues or is reproduced. By this particular logic, it can be argued that functionalists do not necessarily explain the original cause of a phenomenon with reference to its effect. Yet the logic stated in reverse, that social phenomena are (re)produced because they serve ends, is unoriginal to functionalist thought. Thus functionalism is either undefinable or it can be defined by the teleological arguments which functionalist theorists normatively produced before Merton.\n\nAnother criticism describes the ontological argument that society cannot have \"needs\" as a human being does, and even if society does have needs they need not be met. Anthony Giddens argues that functionalist explanations may all be rewritten as historical accounts of individual human actions and consequences (see Structuration).\n\nA further criticism directed at functionalism is that it contains no sense of agency, that individuals are seen as puppets, acting as their role requires. Yet Holmwood states that the most sophisticated forms of functionalism are based on \"a highly developed concept of action,\" and as was explained above, Parsons took as his starting point the individual and their actions. His theory did not however articulate how these actors exercise their agency in opposition to the socialization and inculcation of accepted norms. As has been shown above, Merton addressed this limitation through his concept of deviance, and so it can be seen that functionalism allows for agency. It cannot, however, explain why individuals choose to accept or reject the accepted norms, why and in what circumstances they choose to exercise their agency, and this does remain a considerable limitation of the theory.\n\nFurther criticisms have been levelled at functionalism by proponents of other social theories, particularly conflict theorists, Marxists, feminists and postmodernists. Conflict theorists criticized functionalism's concept of systems as giving far too much weight to integration and consensus, and neglecting independence and conflict. Lockwood, in line with conflict theory, suggested that Parsons' theory missed the concept of system contradiction. He did not account for those parts of the system that might have tendencies to mal-integration. According to Lockwood, it was these tendencies that come to the surface as opposition and conflict among actors. However Parsons thought that the issues of conflict and cooperation were very much intertwined and sought to account for both in his model. In this however he was limited by his analysis of an ‘ideal type' of society which was characterized by consensus. Merton, through his critique of functional unity, introduced into functionalism an explicit analysis of tension and conflict. Yet Merton's functionalist explanations of social phenomena continued to rest on the idea that society is primarily co-operative rather than conflicted, which differentiates Merton from conflict theorists.\n\nMarxism, which was revived soon after the emergence of conflict theory, criticized professional sociology (functionalism and conflict theory alike) for being partisan to advanced welfare capitalism. Gouldner thought that Parsons' theory specifically was an expression of the dominant interests of welfare capitalism, that it justified institutions with reference to the function they fulfill for society. It may be that Parsons' work implied or articulated that certain institutions were necessary to fulfill the functional prerequisites of society, but whether or not this is the case, Merton explicitly states that institutions are not indispensable and that there are functional alternatives. That he does not identify any alternatives to the current institutions does reflect a conservative bias, which as has been stated before is a product of the specific time that he was writing in.\n\nAs functionalism's prominence was ending, feminism was on the rise, and it attempted a radical criticism of functionalism. It believed that functionalism neglected the suppression of women within the family structure. Holmwood shows, however, that Parsons did in fact describe the situations where tensions and conflict existed or were about to take place, even if he did not articulate those conflicts. Some feminists agree, suggesting that Parsons' provided accurate descriptions of these situations. On the other hand, Parsons recognized that he had oversimplified his functional analysis of women in relation to work and the family, and focused on the positive functions of the family for society and not on its dysfunctions for women. Merton, too, although addressing situations where function and dysfunction occurred simultaneously, lacked a \"feminist sensibility.\"\n\nJeffrey Alexander (1985) sees functionalism as a broad school rather than a specific method or system, such as Parsons, who is capable of taking equilibrium (stability) as a reference-point rather than assumption and treats structural differentiation as a major form of social change. The name 'functionalism' implies a difference of method or interpretation that does not exist. This removes the determinism criticized above. Cohen argues that rather than needs a society has dispositional facts: features of the social environment that support the existence of particular social institutions but do not cause them.\n\n\n"}
{"id": "3747720", "url": "https://en.wikipedia.org/wiki?curid=3747720", "title": "The Evolution of the Conservation Movement, 1850–1920", "text": "The Evolution of the Conservation Movement, 1850–1920\n\nThe Evolution of the Conservation Movement, 1850–1920 is an online exhibition from the Library of Congress' American Memory series. It documents the historical formation and cultural foundations of the movement to conserve and protect America's natural heritage, through books, pamphlets, government documents, manuscripts, prints, photographs, and motion picture footage drawn from the collections of the Library of Congress.\n\nThe collection consists of 62 books and pamphlets, 140 Federal statutes and Congressional resolutions, 34 additional legislative documents, excerpts from the Congressional Globe and the Congressional Record, 360 Presidential proclamations, 170 prints and photographs, 2 historic manuscripts, and 2 motion pictures.\n\n\n"}
{"id": "3753135", "url": "https://en.wikipedia.org/wiki?curid=3753135", "title": "The Making of the Representative for Planet 8", "text": "The Making of the Representative for Planet 8\n\nThe Making of the Representative for Planet 8 is a 1982 science fiction novel by Doris Lessing. It is the fourth book in her five-book \"Canopus in Argos\" series and relates the fate of a planet, under the care of the benevolent galactic empire Canopus, that is plunged into an ice age. It was first published in the United States in January 1982 by Alfred A. Knopf, and in the United Kingdom in March 1982 by Jonathan Cape.\n\nChristopher Lehmann-Haupt of \"The New York Times\" wrote in a review of this book that \"the effect of the story is powerful and immediate – with all the drama of good polar-exploration literature, and the eloquence, at its best, of the King James Bible.\" However, John Leonard, also of The New York Times, was critical of Lessing's switch to science fiction and in a review of this book, complained that \"Mrs. Lessing is no longer very interested in people. She has come to feel that individuality is a 'degenerative disease'... She seems ... to be in the process of junking not only traditional narrative and conventional characters but the details of feeling as well...\"\n\n\"The Making of the Representative for Planet 8\" can be read as a stand-alone book, although it does make reference to the planet \"Shikasta\", introduced in the first book of the \"Canopus\" series.\n\nPlanet 8 is a small world that was colonised by the benevolent galactic empire Canopus and populated with a new species created from the stock of four different species originating on several other Canopean planets. Planet 8 has a warm temperate climate and, under Canopus's skilled guidance, the inhabitants live comfortably and at peace with themselves and their world.\n\nOne day Canopus instructs them to build a huge wall, to exact Canopean specifications, right around the girth of the planet. The construction takes the inhabitants years to complete, and when it is finished, Canopus tells the planet's representatives, leaders of each of the planet's main disciplines, to relocate all settlements north of the wall to the south. Canopus informs everyone that unfortunate interstellar \"re-alignments\" have taken place and that Planet 8 will soon experience an ice age. After a while temperatures start to drop and the climate begins to change. Glaciers form in the north and slowly advance towards the wall. Canopus, however, assures Planet 8 that Canopus has a new home for them, a peaceful and prosperous world called Rohanda (the subject of the first book in this series, \"Shikasta\"), and that when it has reached a certain level of development, Canopus will space-lift the inhabitants of Planet 8 to Rohanda. This fills the people of Planet 8 with hope as they are forced to adapt their lifestyles to cope with this new and unfamiliar climate.\n\nBy the time the glaciers reach the wall, much of the vegetation in the south has been destroyed by snow and ice and conditions grow worse. Conflict breaks out amongst the erstwhile peaceful villagers as food becomes scarce. But the wall holds the glaciers back and the people still remain resolute in their faith that Canopus will rescue them. Then Canopean agent Johor (first introduced in \"Shikasta\") arrives on Planet 8 with the devastating news that disaster has struck Rohanda: it has been renamed Shikasta (the stricken) and is no longer available for re-settlement. But Johor does not leave Planet 8. He remains to endure the hardships with the villagers and does what he can to help them face their inevitable demise.\n\nIn time, when the population is now faced with starvation, the wall, which was only a temporary barrier, gives way and the glaciers start over-running settlements in the south. The senior representatives, at a loss as to what to do, head north over the wall and onto the glacier. Johor travels with them as they try to reach the pole, but they soon all succumb to cold and hunger. Their physical bodies perish, but their \"beings\" rise and merge into a single consciousness that becomes the Representative for Planet 8 and all its memories. After watching Planet 8 freeze over completely, the Representative departs for a place \"where Canopus tends and guards and instructs.\"\n\n\nThe book contains a 30-page afterword in which Lessing states that \"The Making of the Representative for Planet 8\" and the previous book in this series, \"The Sirian Experiments\" were inspired by her 50-year fascination in the ill-fated 1910–13 Antarctic expedition of Robert Falcon Scott, though not, she points out, by the element of ice and polar conditions in itself.\n\nLessing expresses a concern that Scott and his explorers, heroes in their day, were vilified by recent generations. She analyses the disastrous expedition and the changes as to how it, and Scott himself, have been viewed by posterity, and discusses these changes in the popular views of the journey in connection with the changing fashions of thought and the decline of imperialist ideas. The efforts of Scott and his men, she concludes, should be understood as a deliberate wager with destiny rather than a scientific project: they tried to transcend themselves by taking on responsibilities they knew they might not be fully equipped for or experienced enough to handle, which ultimately cost many of them their lives. Lessing explicitly likens this view of the pride in sacrifice and self-transcending to the expectations and propaganda imagery of the First World War soon after.\n\n\n\"The Making of the Representative for Planet 8\" was adapted for the opera in 1986 by composer Philip Glass with story-libretto by Doris Lessing. The three-act opera, for orchestra, small chorus and soloists was commissioned by the Houston Grand Opera, the English National Opera and the Landestheater Kiel in Germany. It first premiered on 8 July 1988 at the Houston Grand Opera in Houston, Texas. The British premiere was on 9 November 1988 by the English National Opera at the London Coliseum. The premiere received a lukewarm review by \"New York Times\" music critic John Rockwell.\n\nIn 1997, Glass adapted another of Lessing's books from this series for the opera, \"The Marriages Between Zones Three, Four and Five\".\n\n\n"}
{"id": "43516985", "url": "https://en.wikipedia.org/wiki?curid=43516985", "title": "The Rational Optimist", "text": "The Rational Optimist\n\nThe Rational Optimist is a 2010 popular science book by Matt Ridley, author of \"\". The book primarily focuses on the benefits of the innate human tendency to trade goods and services. Ridley argues that this trait, together with the specialization linked to it, is the source of modern human civilization, and that, as people increasingly specialize in their skill sets, we will have increased trade and more prosperity.\n\nBill Gates praised the book for critiquing opposition to international aid, but criticised the book for under-representing global catastrophic risks. Ricardo Salinas Pliego praised the book as a defence of free trade and globalisation. Michael Shermer gave the book positive reviews in \"Nature\" and \"Scientific American\" before going on to present similar ideas in conference talks, and writing \"The Moral Arc\" partly in response. David Papineau praised the book for refuting \"doomsayers who insist that everything is going from bad to worse\".\n\nGeorge Monbiot criticised the book in his \"Guardian\" column.\n\n\n"}
{"id": "27461561", "url": "https://en.wikipedia.org/wiki?curid=27461561", "title": "Theories of cloaking", "text": "Theories of cloaking\n\nTheories of cloaking discusses various theories based on science and research, for producing an electromagnetic cloaking device. Theories presented employ transformation optics, event cloaking, dipolar scattering cancellation, tunneling light transmittance, sensors and active sources, and acoustic cloaking.\n\nA cloaking device is one where the purpose of the transformation is to hide something, so that a defined region of space is invisibly isolated from passing electromagnetic fields (see Metamaterial cloaking) or sound waves. Objects in the defined location are still present, but incident waves are guided around them without being affected by the object itself. Along with this basic \"cloaking device\", other related concepts have been proposed in peer reviewed, scientific articles, and are discussed here. Naturally, some of the theories discussed here also employ metamaterials, either electromagnetic or acoustic, although often in a different manner than the original demonstration and its successor, the \"broad-band cloak\".\n\nThe first electromagnetic cloaking device was produced in 2006, using gradient-index metamaterials. This has led to the burgeoning field of transformation optics (and now transformation acoustics), where the propagation of waves is precisely manipulated by controlling the behaviour of the material through which the light (sound) is travelling.\n\nWaves and the host material in which they propagate have a symbiotic relationship: both act on each other. A simple spatial cloak relies on fine tuning the properties of the propagation medium in order to direct the flow smoothly around an object, like water flowing past a rock in a stream, but without reflection, or without creating turbulence. Another analogy is that of a flow of cars passing a symmetrical traffic island – the cars are temporarily diverted, but can later reassemble themselves into a smooth flow that holds no information about whether the traffic island was small or large, or whether flowers or a large advertising billboard might have been planted on it.\n\nAlthough both analogies given above have an implied direction (that of the water flow, or of the road orientation), cloaks are often designed so as to be isotropic, i.e. to work equally well for all orientations. However, they do not need to be so general, and might only work in two dimensions, as in the original electromagnetic demonstration, or only from one side, as for the so-called carpet cloak.\n\nSpatial cloaks have other characteristics: whatever they contain can (in principle) be kept invisible forever, since an object inside the cloak may simply remain there. Signals emitted by the objects inside the cloak that are not absorbed can likewise be trapped forever by its internal structure. If a spatial cloak could be turned off and on again at will, the objects inside would then appear and disappear accordingly.\n\nThe event cloak is a means of manipulating electromagnetic radiation in space and time in such a way that a certain collection of happenings, or events, is concealed from distant observers. Conceptually, a safecracker can enter a scene, steal the cash and exit, whilst a surveillance camera records the safe door locked and undisturbed all the time. The concept utilizes the science of metamaterials in which light can be made to behave in ways that are not found in naturally occurring materials.\n\nThe event cloak works by designing a medium in which different parts of the light illuminating a certain region can be either slowed or accelerated. A leading portion of the light is accelerated so that it arrives before the events occur, whilst a trailing part is slowed and arrives too late. After their occurrence, the light is reformed by slowing the leading part and accelerating the trailing part. The distant observer only sees a continuous illumination, whilst the events that occurred during the dark period of the cloak's operation remain undetected. The concept can be related to traffic flowing along a highway: at a certain point some cars are accelerated up, whilst the ones behind are slowed. The result is a temporary gap in the traffic allowing a pedestrian to cross. After this, the process can be reversed so that the traffic resumes its continuous flow without a gap. Regarding the cars as light particles (photons), the act of the pedestrian crossing the road is never suspected by the observer down the highway, who sees an uninterrupted and unperturbed flow of cars.\n\nFor absolute concealment, the events must be non-radiating. If they do emit light during their occurrence (e.g. by fluorescence), then this light is received by the distant observer as a single flash.\n\nApplications of the Event Cloak include the possibility to achieve `interrupt-without-interrupt' in data channels that converge at a node. A primary calculation can be temporarily suspended to process priority information from another channel. Afterwards the suspended channel can be resumed in such a way as to appear as though it was never interrupted.\n\nThe idea of the event cloak was first proposed by a team of researchers at Imperial College London (UK) in 2010, and published in the Journal of Optics. An experimental demonstration of the basic concept using nonlinear optical technology has been presented in a preprint on the Cornell physics arXiv. This uses time lenses to slow down and speed up the light, and thereby improves on the original proposal from McCall et al which instead relied on the nonlinear refractive index of optical fibres. The experiment claims a cloaked time interval of about 10 picoseconds, but that extension into the nanosecond and microsecond regimes should be possible.\n\nIn 2006, the same year as the first metamaterial cloak, another type of cloak was proposed. This type of cloaking exploits resonance of light waves while matching the resonance of another object. In particular a particle placed near a superlens would appear to disappear as the light surrounding the particle resonates as the same frequency as the superlens. The resonance would effectively cancel out the light reflecting from the particle, rendering the particle electromagnetically invisible.\n\nIn 2009, a passive cloaking device was designed to be an 'external invisibility device' that leaves the concealed object out in the open so that it can ‘see’ its surroundings. This is based on the premise that cloaking research has not adequately provided a solution to an inherent problem; because no electromagnetic radiation can enter or leave the cloaked space, this leaves the concealed object of the cloak without ability to detect visually, or otherwise, anything outside the cloaked space.\n\nSuch a cloaking device is also capable of ‘cloaking’ only parts of an object, such as opening a virtual peep hole on a wall so as to see the other side.\n\nThe traffic analogy used above for the spatial cloak can be adapted (albeit imperfectly) to describe this process. Imagine that a car has broken down in the vicinity of the roundabout, and is disrupting the traffic flow, causing cars to take different routes or creating a traffic jam. This exterior cloak corresponds to a carefully misshapen roundabout which manages to cancel or counteract the effect of the broken down car – so that as the traffic flow departs, there is again no evidence in it of either the roundabout or of the broken down car.\n\nThe \"plasmonic cover\", mentioned alongside \"metamaterial covers\" (see plasmonic metamaterials), theoretically utilizes plasmonic resonance effects to reduce the total scattering cross section of spherical and cylindrical objects. These are lossless metamaterial covers near their plasma resonance which could possibly induce a dramatic drop in the scattering cross section, making these objects nearly “invisible” or “transparent” to an outside observer. Low loss, even no-loss, passive covers might be utilized that do not require high dissipation, but rely on a completely different mechanism.\n\nMaterials with either negative or low value constitutive parameters, are required for this effect. Certain metals near their plasma frequency, or metamaterials with negative parameters could fill this need. For example, several noble metals achieve this requirement because of their electrical permittivity at the infra-red or visible wavelengths with relatively low loss.\n\nCurrently only microscopically small objects could possibly appear transparent.\n\nThese materials are further described as a homogeneous, isotropic, metamaterial covers near plasma frequency dramatically reducing the fields scattered by a given object. Furthermore, These do not require any absorptive process, any anisotropy or inhomogeneity, and nor any interference cancellation.\n\nThe \"classical theory\" of \"metamaterial covers\" works with light of only one specific frequency.\nA new research, of Kort-Kamp \"et al\", who won the prize “School on Nonlinear Optics and Nanophotonics” of 2013, shows that is possible to tune the metamaterial to different light frequencies.\n\nAs implied in the nomenclature, this is a type of light transmission. Transmission of light (EM radiation) through an object such as metallic film occurs with an assist of tunnelling between resonating inclusions. This effect can be created by embedding a periodic configuration of dielectrics in a metal, for example. By creating and observing transmission peaks interactions between the dielectrics and interference effects cause mixing and splitting of resonances. With an effective permittivity close to unity, the results can be used to propose a method for turning the resulting materials invisible.\n\nThere are other proposals for use of the cloaking technology.\n\nIn 2007 cloaking with metamaterials is reviewed and deficienies are presented. At the same time, theoretical solutions are presented that could improve the capability to cloak objects. Later in 2007, a mathematical improvement in the cylindrical shielding to produce an electromagnetic \"wormhole\", is analyzed in three dimensions. Electromagnetic wormholes, as an optical device (not gravitational) are derived from cloaking theories has potential applications for advancing some current technology.\n\nOther advances may be realized with an acoustic superlens. In addition, acoustic metamaterials have realized negative refraction for sound waves. Possible advances could be enhanced ultrasound scans, sharpening sonic medical scans, seismic maps with more detail, and buildings no longer susceptible to earthquakes. Underground imaging may be improved with finer details. The acoustic superlens, acousitc cloaking, and acoustic metamaterials translates into novel applications for focusing, or steering, sonic waves.\n\nAcoustic cloaking technology could be used to stop a sonar-using observer from detecting the presence of an object that would normally be detectable as it reflects or scatters sound waves. Ideally, the technology would encompass a broad spectrum of vibrations on a variety of scales. The range might be from miniature electronic or mechanical components up to large earthquakes. Although most progress has been made on mathematical and theoretical solutions, a laboratory metamaterial device for evading sonar has been recently demonstrated. It can be applied to sound wavelengths from 40 to 80 kHz.\n\nWaves also apply to bodies of water. A theory has been developed for a cloak that could \"hide\", or protect, man-made platforms, ships, and natural coastlines from destructive ocean waves, including tsunamis.\n\n"}
{"id": "853744", "url": "https://en.wikipedia.org/wiki?curid=853744", "title": "Trailer (promotion)", "text": "Trailer (promotion)\n\nA trailer (also known as a preview or coming attraction) is a commercial advertisement for a feature film that will be exhibited in the future at a cinema, the result of creative and technical work. The term \"trailer\" comes from their having originally been shown at the end of a feature film screening. That practice did not last long, because patrons tended to leave the theater after the films ended, but the name has stuck. Trailers are now shown before the film begins.\n\nMovie trailers have now become popular on DVDs and Blu-ray discs, as well as on the Internet and mobile devices. Of some 10 billion videos watched online annually, film trailers rank third, after news and user-created video. The trailer format has also been adopted as a promotional tool for television shows, video games, books, and theatrical events/concerts.\n\nThe first trailer shown in an American film theater was in November 1913, when Nils Granlund, the advertising manager for the Marcus Loew theater chain, produced a short promotional film for the musical \"The Pleasure Seekers\", opening at the Winter Garden Theatre on Broadway. As reported in a wire service story carried by the Lincoln, Nebraska \"Daily Star\", the practice which Loew adopted was described as \"an entirely new and unique stunt\", and that \"moving pictures of the rehearsals and other incidents connected with the production will be sent out in advance of the show, to be presented to the Loew’s picture houses and will take the place of much of the bill board advertising\". Granlund was also first to introduce trailer material for an upcoming motion picture, using a slide technique to promote an upcoming film featuring Charlie Chaplin at Loew's Seventh Avenue Theatre in Harlem in 1914.\n\nTrailers were initially shown after, or \"trailing\", the feature film, and this led to their being called \"trailers\". The practice was found to be somewhat ineffective, often ignored by audiences who left immediately after the feature. Later, exhibitors changed their practice so that trailers were only one part of the film program, which included cartoon shorts, newsreels, and serial adventure episodes. Today, more elaborate trailers and commercial advertisements have largely replaced other forms of pre-feature entertainment, and in major multiplex chains, about the first 20 minutes after the posted showtime is devoted to trailers.\n\nUp until the late 1950s, trailers were mostly created by National Screen Service and consisted of various key scenes from the film being advertised, often augmented with large, descriptive text describing the story, and an underscore generally pulled from studio music libraries. Most trailers had some form of narration, and those that did featured stentorian voices.\n\nIn the early 1960s, the face of motion picture trailers changed. Textless, montage trailers and quick-editing became popular, largely due to the arrival of the \"new Hollywood\" and techniques that were becoming increasingly popular in television. Among the trend setters were Stanley Kubrick with his montage trailers for \"Lolita\" (1962), \"Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb\" (1964), and \"\" (1968). Kubrick's main inspiration for the \"Dr. Strangelove\" trailer was the short film \"Very Nice, Very Nice\" (1961) by Canadian film visionary Arthur Lipsett. Pablo Ferro, who pioneered the techniques Kubrick required as necessary elements for the success of his campaign, created the \"Dr. Strangelove\" trailer, as well as the award-winning trailer for \"A Clockwork Orange\" (1971).\n\nMany home videos contain trailers for other movies produced by the same company scheduled to be available shortly after the legal release of the video, so as not to spend money advertising the videos on TV. Most VHS tapes would play them at the beginning of the tape, but some VHS tapes contained previews at the end of the film or at both ends of the tape. VHS tapes that contained trailers at the end usually reminded the viewer to \"Stay tuned after the feature for more previews.\" With DVDs and Blu-rays, trailers can operate as a bonus feature instead of having to watch through the trailers before the film.\n\nTrailers consist of a series selected shots from the film being advertised. Since the purpose of the trailer is to attract an audience to the film, these excerpts are usually drawn from the most exciting, funny, or otherwise noteworthy parts of the film but in abbreviated form and usually without producing spoilers. For this purpose the scenes are not necessarily in the order in which they appear in the film. A trailer has to achieve that in less than 2 minutes and 30 seconds, the maximum length allowed by the MPAA. Each studio or distributor is allowed to exceed this time limit once a year, if they feel it is necessary for a particular film.\n\nIn January 2014, the movie theater trade group \"National Association of Theatre Owners\" issued an industry guideline asking that film distributors supply trailers that run no longer than 2 minutes, which is 30 second shorter than the prior norm. The guideline is not mandatory, and also allows for limited exceptions of a select few movies having longer trailers. Film distributors reacted coolly to the announcement. There had been no visible disputes on trailer running time prior to the guideline, which surprised many.\n\nSome trailers use \"special shoot\" footage, which is material that has been created specifically for advertising purposes and does not appear in the actual film. The most notable film to use this technique was \"\", whose trailer featured an elaborate special effect scene of a T-800 Terminator being assembled in a factory that was never intended to be in the film itself. Dimension Films also shot extra scenes for their 2006 horror remake, \"Black Christmas\" - these scenes were used in promotional footage for the film, but are similarly absent from the theatrical release. A trailer for the 2002 blockbuster \"Spider-Man\" had an entire action sequence especially constructed that involved escaping bank robbers in a helicopter getting caught in a giant web between the World Trade Center's two towers. However, after the September 11 attacks the studio pulled it from theaters.\n\nOne of the most famous \"special shoot\" trailers is that used for the 1960s thriller \"Psycho\", which featured director Alfred Hitchcock giving viewers a guided tour of the Bates Motel, eventually arriving at the infamous shower. At this point, the soft-spoken Hitchcock suddenly throws the shower curtain back to reveal Vera Miles with a blood-curdling scream. As the trailer, in fact, was made after completion of the film when Janet Leigh was no longer available for filming, Hitchcock had Miles don a blonde wig for the fleeting sequence. Since the title, \"Psycho\", instantly covers most of the screen, the switch went unnoticed by audiences for years until freeze-frame analysis clearly revealed that it was Vera Miles and not Janet Leigh in the shower during the trailer.\n\nThere are dozens of companies that specialize in the creation of film trailers in Los Angeles and New York. The trailer may be created at agencies (such as The Cimarron Group, MOJO, The Ant Farm, Ben Cain, Aspect Ratio, Flyer Entertainment, Trailer Park, Buddha Jones) while the film itself is being cut together at the studio. Since the edited film does not exist at this point, the trailer editors work from rushes or dailies. Thus, the trailer may contain footage that is not in the final movie, or the trailer editor and the film editor may use different takes of a particular shot. Another common technique is including music on the trailer which does not appear on the movie's soundtrack. This is nearly always a requirement, as trailers and teasers are created long before the composer has even been hired for the film score—sometimes as much as a year ahead of the movie's release date—while composers are usually the last creative people to work on the film.\n\nSome trailers that incorporate material not in the film are particularly coveted by collectors, especially trailers for classic films. For example, in a trailer for \"Casablanca\" the character Rick Blaine says, \"OK, you asked for it!\" before shooting Major Strasser; this line of dialogue is not spoken in the final film.\n\nOver the years, there have been many instances where trailers give misleading representations of their films. They may give the impression that a celebrity who only has a minor part in the film is one of the main cast members, or advertising a film as being more action-packed than it is. These tricks are usually done to draw in a larger audience. Sometimes the trailers include footage not from the film itself. This could be an artistic choice, or because the trailer was put together before the film's final cut, but at other times it is to give the audience a different impression of the movie. Then trailers could be misleading in a 'for the audience's own good' kind of way, in that a general audience would not usually see such a film due to preconceptions, and by bait and switching, they can allow the audience to have a great viewing experience that they would not ordinarily have. However, the opposite is true too, with the promise of great trailers being let down by mediocre films. An American woman sued the makers of \"Drive\" because their film \"failed to live up to its promo's promise\", although her lawsuit was dismissed. In August 2016, an American lawyer attempted to sue \"Suicide Squad\" for false advertising over lack of scenes including Joker.\n\nTrailers tell the story of a film in a highly condensed fashion to have maximum appeal. In the decades since film marketing has become a large industry, trailers have become highly polished pieces of advertising, able to present even poor movies in an attractive light. Some of the elements common to many trailers are listed below. Trailers are typically made up of scenes from the film they are promoting, but sometimes contain deleted scenes from the film.\n\nThe key ambition in trailer-making is to impart an intriguing story that gets film audiences emotionally involved.\n\nMost trailers have a three-act structure similar to a feature-length film. They start with a beginning (act 1) that lays out the premise of the story. The middle (act 2) drives the story further and usually ends with a dramatic climax. Act 3 usually features a strong piece of \"signature music\" (either a recognizable song or a powerful, sweeping orchestral piece). This last act often consists of a visual montage of powerful and emotional moments of the film and may also contain a cast run if there are noteworthy stars that could help sell the movie.\n\nVoice-over narration is sometimes used to briefly set up the premise of the film and provide explanation when necessary, although this practice has declined in the years after the passing of voice-over artist Don LaFontaine. Since the trailer is a highly condensed format, voice-over is a useful tool to enhance the audience's understanding of the plot. Some of the best-known, modern-day trailer voice-over artists have been the aforementioned LaFontaine, Hal Douglas, Mark Elliott, John Leader, Corey Burton, George DelHoyo, Peter Cullen, Morgan Freeman, Ashton Smith, Jim Cummings, John Garry, Tom Kane, Nick Schatzki, Ben Patrick Johnson, Tony Rodgers, Beau Weaver, and Brian Cummings. Classic voice-over artists in film trailers of the 1940s, 1950s and 1960s included Howard Strickling (for MGM), Lou Marcelle (for Warner Bros.), Art Gilmore, Knox Manning, Reed Hadley, Les Tremayne (for MGM), Fred Foy (for MGM), Karl Weber (for MGM) and Bob Marcato. Hollywood trailers of the classic film era were renowned for clichés such as \"Colossal!\", \"Stupendous!\", etc. Some trailers have used voice over clichés for satirical effect. This can be seen in trailers for films such as Jerry Seinfeld's Comedian and Tenacious D in The Pick of Destiny.\n\nMusic helps set the tone and mood of the trailer. Usually the music used in the trailer is not from the film itself (the film score may not have been composed yet). The music used in the trailer may be:\n\n\nA \"cast run\" is a list of the stars that appear in the movie. If the director or producer is well-known or has made other popular movies, they often warrant a mention as well. Most trailers conclude with a billing block, which is a list of the principal cast and crew. It is the same list that appears on posters and print publicity materials, and also usually appears on-screen at the beginning (or end) of the movie. Studio production logos are usually featured near the beginning of the trailer. Until the late 1970s, they were put only at the end of the trailer or not used at all; however, Paramount Pictures was the first studio to use its actual studio logo at the beginning of its trailers in the 1940s. Often there will be logos for both the production company and distributor of the film.\n\nMany trailers are mixed in Dolby Digital or any other multichannel sound mix. Scenes including sound effects and music that are enhanced by stereophonic sound are therefore the focus point of many modern trailers.\n\nTrailers preceding feature films are generally presented in the same format as the feature, being either 35 mm film or a digital format. High bandwidth internet connections allow for trailers to be distributed online at any resolution. Since the advent of Digital 3‑D, it has become common for a 3‑D feature film to be preceded by one or more trailers that are also presented in 3‑D.\n\nNational Screen Service contracts required that trailers be returned (at the cinema's expense) or destroyed, however it required no proof of destruction and depositing them in a waste bin counted. A market for trailers evolved as it became clear that some had a commercial value to collectors. Many of the trailers for films like the \"Star Wars\" series reported as 'destroyed' were taken back out of the bin and sold by cinema staff. As they cost about $60 each to make (1981 estimate) and were hired to the cinema for $10, such losses led to NSS increasing its rental charges, which led to a decrease in the number of trailers rented and shown to audiences.\n\nSome cinemas also began to show \"trailer trash\" programs of trailers without a main feature. Similarly, several DVDs containing nothing but trailers for films, typically from exploitation film genres, have been produced for sale.\n\nBeginning in the late 1990s to early 2000s, and along with the development of the Internet and sites such as YouTube as well as animation techniques, more types of trailers began to be created due to easier and cheaper costs to produce and show trailers.\n\nBeginning in the late 1990s to early 2000s, video game trailers began to be produced as they became more mainstream. Used to entice viewers to go out and play the game, game trailers are very useful. The content and production process is similar to that for movies, complicated by the need to convey the way the game plays. The trailer for Aliens: Colonial Marines, for example, featured graphics that were of a higher standard than the game that was eventually sold. Hideo Kojima, a game creator strongly influenced by Hollywood movies, edits the elaborate trailers for his own games from a special studio in his office.\n\nTV spots are trailers for movies shown on television that are often shortened to 30–60 seconds. These trailers are similar to green band trailers and have content \"appropriate\" for the channel.\n\nTV show trailers are trailers advertising a new TV series, episode, event or marathon premiering on television. Trailers for the next episode of a TV series are often shown during or following the closing credits of the show.\n\nA book trailer is a video advertisement for a book which employs techniques similar to those of movie trailers to promote books and encourage readers. These trailers can also be referred to as \"video-podcasts\", with higher quality trailers being called \"cinematic book trailers\". They are circulated on television and online in most common digital video formats. Common formats of book trailers include actors performing scenes from the book akin to a movie trailer, full production trailers, flash videos, animation or simple still photos set to music with text conveying the story. This differs from author readings and interviews, which consist of video footage of the author narrating a portion of their writing or being interviewed. Early book trailers consisted mostly of still images of the book, with some videos incorporating actors, with John Farris's book trailer for his 1986 novel \"Wildwood\" incorporating images from the book cover along with actors such as John Zacherle.\n\nIn September 2007, the \"School Library Journal\" established the Trailie Award for the best book trailers. There are three categories: author/publisher created, student created and librarian/adult created. The award was announced at the \"School Library Journal\" Leadership Summit on the Future of Reading on October 22, 2010 in Chicago.\n\nIn 2014, Dan Rosen and CV Herst established BookReels, a website dedicated to allowing publishers and authors to post book trailers and other multimedia, culminating in the annual BookReels Awards. BookReels lets readers browse and rate trailers, post comments and reviews, join discussion groups, and share BookReel discoveries.\n\nFor popular movies, fans often make trailers on their own. These are unofficial videos by fans utilizing audio or video of a movie, studio trailer, animation techniques or fan-acted scenes replacing the video of the official trailer.\n\nThe Motion Picture Association of America (MPAA) mandates that theatrical trailers be no longer than two minutes and thirty seconds. Each major studio is given one exception to this rule per year. Internet or home-video trailers have no time restrictions. \"Rating cards\" appear at the head of trailers in the United States which indicate how closely the trailer adheres to the MPAA's standards.\n\nA \"green band\" is an all-green graphic at the beginning of the trailer. Until April 2009, these cards indicated that they had been approved for \"all audiences\" and often included the movie's MPAA rating. This signified that the trailer adheres to the standards for motion picture advertising outlined by the MPAA, which include limitations on foul language and violent, sexual, or otherwise objectionable imagery. In April 2009, the MPAA began to permit the green band language to say that a trailer had been approved for \"appropriate\" audiences, meaning that the material would be appropriate for audiences in theaters, based on the content of the film they had come to see. In May 2013, the MPAA changed the trailer approval band from \"for appropriate audiences\" to \"to accompany this feature\", but only when accompanying a feature film; for bands not accompanying a feature film, the text of the band remained the same. The font and style of the text on the graphic bands (green and red) was also changed at the time the green band was revised in 2013.\n\nTrailers which do not adhere to these guidelines may be issued a \"red band\", which indicates approval for only \"restricted\" or \"mature\" audiences. These trailers may only be shown theatrically before R-rated, NC-17-rated, or unrated movies (only films that are released in theaters rated R and not in theaters rated PG-13). These trailers may include nudity, profanity, or other material deemed inappropriate for children.\n\nAdditionally, \"yellow band\" trailers were introduced around 2007 to indicate restricted content, only for distribution on the Internet. Although official, this practice appears to have never been widespread (although occasional yellow band trailers are created). A notable example is the yellow band trailer for Rob Zombie's \"Halloween\" (2007).\n\nEvery year there are two main events that give awards to outstanding film trailers: The Key Art Awards, presented by \"The Hollywood Reporter\", and the Golden Trailer Awards. The Golden Trailer Awards and the Key Art Awards pick winners in all creative parts of film advertising, from trailers and TV spots to posters and print ads. The Golden Trailer Awards are currently expanding to add a sister event, The World Trailer Awards, to be a kickoff to the Cannes Film Festival in France, 2013. The yearly Key Art Awards ceremony is often held at the Dolby Theater in Hollywood. \"The Film Informant\" also recognizes movie marketing media and held the first annual TFI Awards in early January 2012. The site is the first to officially start recognizing and rating movie marketing media on a daily basis.\n\n\n"}
{"id": "47424555", "url": "https://en.wikipedia.org/wiki?curid=47424555", "title": "Two for Physics", "text": "Two for Physics\n\nTwo for Physics is a Canadian science television series which aired on CBC Television in 1959.\n\nThis Toronto-produced series on scientific subjects concerned the realm of physics. It was hosted by Patterson Hume and Donald Ivey, professors with the University of Toronto who were previously featured on the local CBLT series \"Live And Learn\".\n\nThis half-hour series was broadcast on Tuesdays at 10:30 p.m. from 7 July to 29 September 1959.\n"}
{"id": "154877", "url": "https://en.wikipedia.org/wiki?curid=154877", "title": "Weather balloon", "text": "Weather balloon\n\nA weather or sounding balloon is a balloon (specifically a type of high-altitude balloon) that carries instruments aloft to send back information on atmospheric pressure, temperature, humidity and wind speed by means of a small, expendable measuring device called a radiosonde. To obtain wind data, they can be tracked by radar, radio direction finding, or navigation systems (such as the satellite-based Global Positioning System, GPS). Balloons meant to stay at a constant altitude for long periods of time are known as transosondes. Weather balloons that do not carry an instrument pack are used to determine upper-level winds and the height of cloud layers. For such balloons, a theodolite or total station is used to track the balloon's azimuth and elevation, which are then converted to estimated wind speed and direction and/or cloud height, as applicable.\n\nOne of the first people to use weather balloons was Léon Teisserenc de Bort, the French meteorologist. Starting in 1896 he launched hundreds of weather balloons from his observatory in Trappes, France. These experiments led to his discovery of the tropopause and stratosphere. Transosondes, weather balloons with instrumentation meant to stay at a constant altitude for long periods of time to help diagnose radioactive debris from atomic fallout, were experimented with in 1958.\n\nThe balloon itself produces the lift, and is usually made of a highly flexible latex material, though Chloroprene may also be used. The unit that performs the actual measurements and radio transmissions hangs at the lower end of the string, and is called a radiosonde. Specialized radiosondes are used for measuring particular parameters, such as determining the ozone concentration. \n\nThe balloon is usually filled with hydrogen due to lower cost, though helium can also be used. The ascent rate can be controlled by the amount of gas with which the balloon is filled. Weather balloons may reach altitudes of 40 km (25 miles) or more, limited by diminishing pressures causing the balloon to expand to such a degree (typically by a 100:1 factor) that it disintegrates. In this instance the instrument package is usually lost. Above that altitude sounding rockets are used, and for even higher altitudes satellites are used.\n\nWeather balloons are launched around the world for observations used to diagnose current conditions as well as by human forecasters and computer models for weather forecasting. About 800 locations around the globe do routine releases, twice daily, usually at 0000 UTC and 1200 UTC. Some facilities will also do occasional supplementary \"special\" releases when meteorologists determine there is a need for additional data between the 12-hour routine launches in which time much can change in the atmosphere. Military and civilian government meteorological agencies such as the National Weather Service in the US typically launch balloons, and by international agreements almost all the data are shared with all nations.\n\nSpecialized uses also exist, such as for aviation interests, pollution monitoring, photography or videography and research. Examples include pilot balloons (Pibal). Field research programs often use mobile launchers from land vehicles as well as ships and aircraft (usually dropsondes in this case). In recent years weather balloons have also been used for scattering human ashes at high-altitude by companies such as Stardust Ashes, founded by Chester Mojay-Sinclare. \n\nThe Weather balloon was also used to create the fictional entity 'Rover' during production of the 1960s TV series 'The Prisoner' in Portmeirion, Gwynedd, North Wales, UK in September 1966. This was retained in further scenes shot at MGM Borehamwood UK during 1966-67.\n\n\n"}
{"id": "53578608", "url": "https://en.wikipedia.org/wiki?curid=53578608", "title": "Widom-Larsen theory", "text": "Widom-Larsen theory\n\nThe Widom-Larsen theory is a proposed explanation for supposed Low Energy Nuclear Reactions (LENR) developed in 2005 by Allan Widom and Lewis Larsen. In the paper describing the idea, they claim that ultra low momentum neutrons are produced in the cold fusion apparatuses during weak interactions when protons capture \"heavy\" electrons from metallic hydride surfaces. The theory has been criticized as being \"based on a number of fallacies and an obscuring way of handling the equations.\"\n\nThe idea was expanded by Yogendra Srivastava together with Widom and Larsen in 2014, who went on to propose that it could be an explanation for neutrons observed in exploding wire experiments, solar corona and flares, and neutron production in thunderstorms. However, unrealistic concentrations of free electrons are needed for the neutron yield to be a significant component of thunderstorm neutrons, discounting the explanation.\n"}
{"id": "998826", "url": "https://en.wikipedia.org/wiki?curid=998826", "title": "Wikispecies", "text": "Wikispecies\n\nWikispecies is a wiki-based online project supported by the Wikimedia Foundation. Its aim is to create a comprehensive free content catalogue of all species; the project is directed at scientists, rather than at the general public. Jimmy Wales stated that editors are not required to fax in their degrees, but that submissions will have to pass muster with a technical audience. Wikispecies is available under the GNU Free Documentation License and CC BY-SA 3.0.\n\nStarted in September 2004, with biologists across the world invited to contribute, the project had grown a framework encompassing the Linnaean taxonomy with links to Wikipedia articles on individual species by April 2005.\n\n co-ordinated the efforts of several people who are interested in getting involved with the project and contacted potential supporters in early summer 2004. Databases were evaluated and the administrators contacted, some of them have agreed on providing their data for Wikispecies. Mandl defined two major tasks:\n\nAdvantages and disadvantages were widely discussed by the wikimedia-I mailing list. The board of directors of the Wikimedia Foundation voted by 4 to 0 in favor of the establishment of a Wikispecies. The project was launched in August 2004 and is hosted at species.wikimedia.org. It was officially merged to a sister project of Wikimedia Foundation on September 14, 2004.\n\nWikispecies comprises taxon pages, and additionally pages about synonyms, taxon authorities, taxonomical publications, and institutions or repositories holding type specimen.\n\nWikispecies has disabled local upload and asks users to use images from . Wikispecies does not allow the use of content that does not conform to a free license.\n\n\n"}
