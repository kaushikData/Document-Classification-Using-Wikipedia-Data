{"id": "5619266", "url": "https://en.wikipedia.org/wiki?curid=5619266", "title": "132524 APL", "text": "132524 APL\n\n132524 APL, provisional designation , is a stony background asteroid in the intermediate asteroid belt, approximately 2.3 kilometers across. It was imaged by the \"New Horizons\" space probe on its flyby in 2006 when it was passing though the asteroid belt. At its closest is was about 1/4 of a lunar distance away from the asteroid, which was able to be used for various tests. This flyby was incidental, and not all the instruments were online at this time, which were still being turned online during 2006.\n\nIt was discovered on 9 May 2002 by astronomers of the Lincoln Near-Earth Asteroid Research (LINEAR) at the Lincoln Laboratory's ETS near Socorro, New Mexico, United States. The asteroid orbits the Sun in a somewhat eccentric orbit at a distance of 1.9–3.3 AU once every 4.2 years. Its orbit is tilted off the ecliptic by 4 degrees.\n\nThe \"New Horizons\" probe flew by it at a distance of approximately 102,000 kilometers on 13 June 2006. The spectra obtained by \"New Horizons\" show that \"APL\" is a stony S-type asteroid.\n\n\"New Horizons\" was not intended to fly by \"APL\", and the flyby was just a coincidence. Alan Stern, principal investigator for \"New Horizons\", named the asteroid in reference to the Johns Hopkins University Applied Physics Laboratory (APL), which runs the mission.\n\nThe asteroid was imaged with Ralph, but not the designed reconnaissance imager LORRI because it was not turned on yet. LORRI was not activated until August 29, 2006 when its cover was opened and its first light image would be Messier 7. It was in general possible to capitalize on the target of opportunity, and the asteroid was tracked for several days in June 2006 in addition to the other tests. In March, New Horizons had passed the orbit of Mars, and the spacecraft was undergoing various course correction maneuvers and tests through out this time; as mentioned LORRI was not activated for another couple months. New Horizons passed through asteroid belt during the summer of 2006, and the test helped prepare the team and spacecraft for the future flybys of Jupiter and Pluto in the coming years. The asteroid belt is a feature of the solar system, consisting of a large number asteroids that orbit the sun primarily between 2.2 and 3.2 AU (Earth-Sun distance) which is between the orbits of planets Mars and Jupiter.\n\nCrossing the asteroid belt is possible, because although there is over a million asteroids larger than 1 km the distance between them is so large spacecraft pass through empty space. This was established in the early 1970s when Pioneer 10 and Pioneer 11 traversed the belt for the first time. There is some increased probability of encountering dust, but otherwise it takes special planning to actually pass very close to an asteroid as was done with Galileo spacecraft. When it passed through the belt on its way to orbit Jupiter in the 1990s, it did a flyby of asteroid 243 Ida.\n\n\n"}
{"id": "19499175", "url": "https://en.wikipedia.org/wiki?curid=19499175", "title": "An Appeal to Reason", "text": "An Appeal to Reason\n\nThis book is an expansion on Lawson’s 2006 lecture to the Centre for Policy Studies, called \"The Economics and Politics of Climate Change. An Appeal to Reason\" As explained in the introduction, the aim of the book is to examine each of the dimensions of the global warming issue, including the science, the economics, both from the perspective of long-term forecasting and cost-effectiveness analysis, the politics, and the ethical aspect.\nThe book begins by arguing that \"the science of global warming is far from settled.\" Although Lawson accepts that warming is real, he questions the validity of global climate models. Specifically, he highlights the lack of falsifiability of their predictions and the fact that all models failed to predict that there has been no further warming between 2001 and 2007. He also questions if indeed the sole cause of the warming is man-made CO and how great that contribution is. Lawson raises several issues regarding the IPCC process and its findings, including the Hockey stick controversy, and criticizes the Stern Report. After the introduction, the remainder of the book proceeds under the assumption that the IPCC majority view is correct.\n\nLawson then examines how much warming will occur and what are the practical consequences over the next hundred years, based on the 2007 IPCC Report (AR4) scenarios and policy recommendations. The next issue analyzed is the importance of adaptation, what he claims is the IPPC’s most serious flaw regarding the impact of global warming, as there is a \"systematic underestimation of the benefits of adaptation\" and \"the most cost-effective way of addressing the likely consequences\" as opposed to reducing CO emissions. His next criticism regards the Stern Review, which he claims is \"at the extreme end of the alarmist camp\". He also critiques the Kyoto Protocol and the practical difficulties of reaching a global agreement. Next, he presents an analysis of the different technologies and market alternatives being implemented and available to reduce emissions, concluding with his proposal of a carbon tax across the board, together with the reduction of other taxes to compensate for the extra revenues. The book closes with a discussion about the discount rates used by the IPCC and the Stern Review in their economic analysis, with a more detailed discussion on the latter. The final chapter summarizes the book, ending with a warning about the dangers of the environmental movement, calling it \"the new religion of eco-fundamentalism\" and claiming that \"we appear to have entered a new age of unreason.\"\n\nAs at 23 May 2010, the British Amazon store ranks \"An Appeal to Reason\" overall sales as 93,317, and as 30th in the following categories: Earth Sciences & Geography > Meteorology > Global Warming; Environment & Ecology > Pollution > Greenhouse Effect; and Environment & Ecology > Global Warming. and the American Amazon store ranks the book sales as 523,609 overall.\n\nThe book has been reviewed by, amongst others, Nature Reports Climate Change, Literary Review, The Guardian, The Spectator, The Daily Telegraph, and the Daily Mail,\n\nJames Dent reviewed the book in the Royal Meteorological Society's journal \"Weather\". Robert Watson, the former head of the IPCC and now chief scientist to the Department for the Environment, Food and Rural Affairs, accused Lawson of selective quotation and not understanding \"the current scientific and economic debate\". He also wrote in a letter to a newspaper: \"Lord Lawson's perspective that the UK and Europe are over-reacting to the threat of human-induced climate change is substantially wrong and ignores a significant body of scientific, technological and economic evidence.\"\n\nSir John Houghton reported that \"Although Lawson makes some worthwhile critiques of energy policy... his book is largely one of misleading messages.\" Lawson ignores or misunderstands the science, brushes off economic analysis by the International Energy Agency, and lumps respected scientists with purveyors of nonsense. Jean Palutikof, one of the authors of a new IPCC report, said: \"By the time you get past 2050 the winners become fewer and fewer. By 2100, we will be losing almost everywhere.\" She also said that Lawson's view was \"very wrong\" when it came to the availability of water.\n\nScientists from the Met Office's Hadley Centre responded to Lord Lawson's contention that there has been no global warming since 2000, saying this was due to the La Niña cooling event of early 2007.\n\n"}
{"id": "20483665", "url": "https://en.wikipedia.org/wiki?curid=20483665", "title": "Bangkok Planetarium", "text": "Bangkok Planetarium\n\nThe Bangkok Planetarium is the oldest planetarium in Thailand and Southeast Asia. It is located on Sukhumvit Road in Bangkok as part of the Science Centre for Education, which is operated by the Department of Non-Formal Education of the Ministry of Education.\n\nConstruction of the planetarium began in 1962 with a budget of twelve million baht and it opened on 18 August 1964. The planetarium dome is 20.60 metres in diameter and 13 metres high, and holds 450 seats. The planetarium uses a Mark IV Zeiss projector, which was the first installation of a large planetarium projector in Southeast Asia. Apart from the theatre itself, the building also features permanent exhibitions on astronomy, aimed at young audiences.\n\nThe planetarium underwent extensive renovations in 2015, including the installation of two new Christie Boxer 4K30 projectors alongside the old Mark IV, which helped reignite interest in the previously ailing museum.\n"}
{"id": "3102025", "url": "https://en.wikipedia.org/wiki?curid=3102025", "title": "Blue Gemini", "text": "Blue Gemini\n\nBlue Gemini was a United States Air Force (USAF) project first proposed in August 1962 for a series of seven flights of Gemini spacecraft to enable the Air Force to gain manned spaceflight experience prior to the launch of the Manned Orbital Development System, or MODS. The plan was to use off-the-shelf Gemini spacecraft.\n\nBlue Gemini would consist of two National Aeronautics and Space Administration (NASA) missions which would include a USAF co-pilot and would accomplish NASA objectives. These would be followed by two more NASA missions that would have USAF crews. Those missions would be devoted to NASA goals, but would include USAF experiments if possible. The final phase of Blue Gemini would consist of three dedicated USAF missions. One of these would be an Agena Target Vehicle rendezvous mission and it was possible that some of these later missions would carry only a single crew member, the other seat being occupied by experimental equipment. Possible payloads included a Manned Maneuvering Unit that would allow an astronaut to maneuver around the spacecraft, an advanced navigation system, an erectable structure, and a large ground mapping radar. The plan was to end Blue Gemini missions approximately four months before the debut of the MODS space station.\n\nBlue Gemini was canceled in January 1963 by Secretary of Defense Robert McNamara after he decided that military experiments could be carried aboard some NASA missions, \"i.e.,\" Project Gemini. McNamara also canceled MODS at the same time.\n\nIn December 1963 McNamara likewise approved the development of a Manned Orbital Laboratory (MOL) which was essentially a revived MODS. Blue Gemini should not be confused with the Gemini B spacecraft that was developed for MOL. Gemini B included a tunnel through its heat shield to enable the astronauts to reach the MOL spacecraft.\n\nBecause Blue Gemini was only a paper project that was canceled before NASA started any Gemini flights, no Blue Gemini hardware was constructed. A test article on display at the National Museum of the United States Air Force at Wright-Patterson AFB, Ohio is the Gemini B spacecraft, recognized by its distinctive \"US Air Force\" written on the side, and the circular hatch cut through the heat shield.\n\nThe \"Blue Gemini\" trilogy of novels (\"Blue Gemini,\" \"Blue Darker Than Black,\" and \"Pale Blue\") by Mike Jenne describe a fictional \"Aerospace Support Project\" which used a modified version of the Gemini spacecraft to execute military IIK (Intercept-Inspect-Kill) missions against Soviet satellites suspected of carrying nuclear weapons.\n\n"}
{"id": "51122593", "url": "https://en.wikipedia.org/wiki?curid=51122593", "title": "Bump-in-the-wire", "text": "Bump-in-the-wire\n\nThe term bump-in-the-wire (BITW) refers to a communications device which can be inserted into existing (legacy) systems to enhance the integrity, confidentiality, or reliability of communications across an existing logical link without altering the communications endpoints. The term was originally used to indicate that the device should introduce only a relatively small increased latency in communications compared to the original, unsecured, approach.\n\nAn example of such a device might be a security appliance which applies IPsec protection to communications between existing devices which themselves lack IPsec implementation protocol stack. Such a device might also be called a security gateway or could be implemented as part of a network firewall to implement a tunneling protocol.\n"}
{"id": "54198505", "url": "https://en.wikipedia.org/wiki?curid=54198505", "title": "Canadian Journal of Communication", "text": "Canadian Journal of Communication\n\nThe Canadian Journal of Communication publishes Canadian research and scholarship in the field of communication studies. \"In pursuing this objective, particular attention is paid to research that has a distinctive Canadian flavour by virtue of choice of topic or by drawing on the legacy of Canadian theory and research. The purview of the journal is the entire field of communication studies as practised in Canada or with relevance to Canada.\" It is an independent journal and is owned by its subscribers.\n"}
{"id": "13195996", "url": "https://en.wikipedia.org/wiki?curid=13195996", "title": "Carlo Emery", "text": "Carlo Emery\n\nCarlo Emery (25 October 1848, Naples – 11 May 1925) was an Italian entomologist. He is remembered for Emery's rule, which states that insect social parasites are often closely related to their hosts.\n\nEarly in his career Carlo Emery pursued a course in general medicine, and in 1872 narrowed his interests to ophthalmology. In 1878 he was appointed Professor of Zoology at the University of Cagliari, remaining there for several years until 1881 when he took up an appointment at the University of Bologna as Professor of Zoology, remaining there for thirty-five years until his death. Emery specialised in Hymenoptera, but his early work was on Coleoptera. Prior to 1869, his earliest works were a textbook of general zoology and papers on fishes and molluscs. From 1869 to 1925 he devoted himself almost entirely to the study of ants. \n\nEmery published extensively between 1869 and 1926 describing 130 genera and 1057 species mainly in Wytsman's \"Genera Insectorum\" series. \n\nEmery’s collections of Hymenoptera are in Museo Civico di Storia Naturale di Genova. His Coleoptera are in Museo Civico di Zoologia di Roma. He died at Bologna in 1925.\n\n\n\n"}
{"id": "21518183", "url": "https://en.wikipedia.org/wiki?curid=21518183", "title": "EJB QL", "text": "EJB QL\n\nEJB QL or EJB-QL is a portable database query language for Enterprise Java Beans. It was used in Java EE applications. Compared to SQL, however, it is less complex but less powerful as well.\n\nThe language has been inspired, especially EJB3-QL, by the native Hibernate Query Language HQL.\n\nIn EJB3 It has been mostly replaced by the Java Persistence Query Language.\n\nEJB QL is a database query language similar to SQL. The used queries are somewhat different from relational SQL, as it uses a so-called \"abstract schema\" of the enterprise beans instead of the relational model. In other words, EJB QL queries do not use tables and their components, but enterprise beans, their persistent state, and their relationships. The result of an SQL query is a set of rows with a fixed number of columns. The result of an EJB QL query is either a single object, a collection of entity objects of a given type, or a collection of values retrieved from CMP fields. One has to understand the data model of enterprise beans in order to write effective queries.\n\n"}
{"id": "33685545", "url": "https://en.wikipedia.org/wiki?curid=33685545", "title": "ESPRIT project", "text": "ESPRIT project\n\nESPRIT, or the Elite Sport Performance Research in Training is a UK EPSRC and UK Sport funded research project aiming to develop pervasive sensing technologies for better the understanding of the physiology and biomechanics of athletes in training, and apply the technologies to enhance the well being and healthcare of general public.\n\n\n\n\nA number of sports exemplars have been selected in the ESPRIT Programme to demonstrate and validate the application of pervasive sensing technology in elite sport performance monitoring\nOne of the main objectives of the ESPRIT project is to extend the developed sensing technology for wellbeing and healthcare applications. To demonstrate the application of the technology, a number of healthcare exemplars have been selected.\n\n"}
{"id": "35228384", "url": "https://en.wikipedia.org/wiki?curid=35228384", "title": "Eyewire", "text": "Eyewire\n\nEyewire is a game to map the brain from Sebastian Seung's Lab at Princeton University. This citizen science human-based computation game challenges players to map retinal neurons. Eyewire launched on December 10, 2012. Over five years, 250,000 people from 150 countries have signed up. The game utilizes data generated by the Max Planck Institute for Medical Research.\n\nEyewire gameplay advances neuroscience by enabling the reconstruction of morphological neuron data, which helps researchers model information processing circuits. Anyone with an internet connection can participate by solving 2D puzzles to fit together segmentation produced by artificial intelligence used to map the connectome. In Eyewire, players reconstruct 3D models of neurons from electron microscope images by solving puzzles. Playing requires no scientific background.\n\nEyewire challenges players, \"Eyewirers\", to map neurons in 3D. Upon registering, players are directed through a tutorial that explains the game. Supplementary video tutorials are available on the Eyewire Blog.\n\nIn Eyewire, the player is given a cube with a partially reconstructed neuron branch stretching through it. On the right side of the screen is a grayscale image of the cross sections of neurons. The player learns to \"color\" inside a gray outline of a single neuron branch, which usually extends from one side of the cube to another. As a player colors, segmentations that were generated by AI are added to the 3D section on the left of the screen. Reconstructions are compared across players as each cube is submitted, yielding a consensus reconstruction that is later checked by expert players of rank Scout and Scythe. These players have the power to extend branches, remove erroneous segments (nicknamed \"mergers\"), and flag cubes for further review. This end result is volumetric reconstructions of complete neurons.\nEach volume is presented to two to five different players. Generally, the trace chosen by the majority of accurate players is accepted. Players win points based on whether their tracing matches the majority of other players' tracings, time spent on the cube, and the new amount of neural volume found.\n\nThe goal of Eyewire is to identify and classify specific cell types as well as potentially expand the known broad classes of retinal cells. Eyewire aims to advance the use of artificial intelligence in neuronal reconstruction. The project aims to help determine how mammals see directional motion. \n\nOver 1,000 neurons mapped by Eyewirers may be explored on the Eyewire Museum, a browser-based visualization tool that pairs anatomical and functional data. \n\nThe activity of each neuron in a 350×300×60 μm portion of a retina was determined by two-photon microscopy. Using serial block-face scanning electron microscopy, the same volume was stained to bring out the contrast of the plasma membranes, sliced into layers by a microtome, and imaged using an electron microscope.\n\nA neuron is selected by the researchers. The program chooses a cubic volume associated with that neuron for the player, along with an artificial intelligence's best guess for tracing the neuron through the two-dimensional images.\n\n\n\nEyewire has been featured by WIRED, Nature, Forbes, Scientific American, NPR and more.\n\n"}
{"id": "17549646", "url": "https://en.wikipedia.org/wiki?curid=17549646", "title": "Flipism", "text": "Flipism\n\nFlipism, sometimes written as \"Flippism,\" is a pseudophilosophy under which all decisions are made by flipping a coin. It originally appeared in the Disney comic \"Flip Decision\" by Carl Barks, published in 1953. Barks called a practitioner of \"Flipism\" a \"Flippist\" (with two P's).\n\nAn actual coin is not necessary: dice or another random generator may be used for decision making.\n\nFlipism can be seen as a normative decision theory, although it does not fulfill the criteria of rationality.\n\nIn the 1952 comic book, Donald Duck meets Professor Batty, who persuades Donald to make decisions based on flipping a coin at every crossroad of life. \"Life is but a gamble! Let flipism chart your ramble!\" Donald soon gets into trouble when following this advice. He drives a one way road in the wrong direction and is fined $50. The reason for the fine is not the bad driving but letting the coin do the thinking. Indeed, there are those who view the resort to Flipism to be a disavowal of responsibility for making personal and societal decisions based upon rationality. However, in the end, flipism shows surprising efficiency in guiding some decisions.\n\nFlipism is a normative decision theory in a sense that it prescribes how decisions should be made. In the comic, flipism shows remarkable ability to make right conclusions without any information – but only once in a while. Of course, in real life flipping a coin would only lead to random decisions. However, there is an article about benefits of some randomness in the decision-making process in certain conditions. It notes:\n\nThough the author himself may have intended this as a rejection of the idea that rationality (in the standard sense) has some special claim to superiority as a basis for making decisions, what he may really have discovered are the potential benefits of strategic commitment to randomization.\n\nCommitment to a non-trivial mixed strategy can be beneficial for the informed party in a potential conflict under asymmetric information, as it allows the player to manipulate his opponent’s beliefs in an optimal fashion. Such a strategy also makes the player less inclined to enter into conflict when it is avoidable. Coins and \"flipism\" have been used to suggest mathematical outcomes to a variation of the Prisoners Dilemma.\n\nAnother way of seeing the utility of flipism in decision-making can be called revealed preferences. In the traditional form, revealed preferences mean that the preferences of consumers can be revealed by their purchasing habits. With flipism, the preferences can be revealed to the decision-maker themselves. Decisions with conflicting preferences are especially difficult even in situations where there is only one decision-maker and no uncertainty. The decision options may be either all appealing or all unpleasant, and therefore the decision-maker is unable to choose. Flipism, \"i.e.\", flipping a coin can be used to find a solution. However, the decision-maker should not decide based on the coin but instead observe his or her own feelings about the outcome; whether it was relieving or agonizing. In this way, flipism removes the mental block related to the act of decision-making, and the post-decision preferences can be revealed before the decision is actually made. An example of revealed preferences is embodied in the Old Testament story, the Judgment of Solomon, wherein King Solomon offered to resolve a child custody dispute by ordering the baby cut in two, and upon seeing the reactions made an award.\n\nStill a third approach is to look at flipism as the endpoint of a continuum bounded on the other side by perfectly rational decision-making. Flipism requires the minimum possible cognitive overhead to make decisions, at the price of making sub-optimal choices. Truly rational decision-making requires a tremendous investment in information and cognition to arrive at an optimal decision. However, the expected marginal value of information gathered (discounted for risk and uncertainty) is often lower than the marginal cost of the information or processing itself. The concept of bounded rationality posits that people employ cognitive parsimony, gathering only what they expect to be sufficient information to arrive at a satisficing (or \"good enough\") solution. Flipism is therefore a rational strategy to employ when the cost of information is very high relative to its expected value, and using it is an example of motivated tactical thinking.\n\nThis is a commonly recognized decision making technique used in everyday life. Other similar methods include:\n\nThese forms are in contradistinction to Analytics, a commonly used method of data-based decision making.\n\nAccording to Kevin Durand and Mary Leigh, Flipism is “a psychological tool, and not an agent of fate.” It is neither a revelation of the wishes of the head of state (\"e.g.\", Julius Caesar, whose head was on the coin, \"ergo\", heads showed 'Caesar's will') nor the divination of a deity's will.\n\nIn game theory, negotiations, nuclear deterrence, diplomacy and other Conflict theory – rationality, realpolitik or realism can themselves limit strategies and results. They can limit the ability of a player to make demands or get its own way through bluff, bully, instill fear, cause apprehension, or psychologically manipulate or send a heeded warning—and therefore can increase the likelihood that an opposing party may engage in objectionable or unwelcome behavior. If one knows the lines and can predict the response, then predictability and proportionality become a restraint, not a virtue. Consequently, 'taunting a junkyard dog is OK, if you know you are beyond the reach of its tether.' Thus irrationality (real or perceived) can be an important countervailing tool or strategy, particularly as a deterrent and if it engenders hesitation, fear, negotiation and resolution, or change of course. On the other hand, alternate strategies such as honesty, building a climate of trust, respect, using intermediaries, mediation or other forms of conflict resolution, sanctions, patience, process, data and reasoning might still be available, as might strategies like so-called win/win bargaining (also called \"interest-based\" bargaining) – which tries to reach an accord based on interests, not necessarily on positions, power, rights or distribution.\nFlipism is a film trope that is used to argue for “the supremacy of free will in a chaotic world.” It is a recurrent theme, often seen on television and other media, and even in real life.\n\n\n\n\n\n"}
{"id": "51144140", "url": "https://en.wikipedia.org/wiki?curid=51144140", "title": "Forces of Nature (book)", "text": "Forces of Nature (book)\n\nForces of Nature is a 2016 book by Professor Brian Cox and Andrew Cohen. The book accompanies the BBC One TV series of the same name, Forces of Nature.\n\nThe book attempts to provide deep answers to simple questions, ranging from the nature of motion to the uniqueness of a snowflake. It uncovers how some of our planet's beautiful sights and events are forged by just a handful of natural forces.\n"}
{"id": "2112153", "url": "https://en.wikipedia.org/wiki?curid=2112153", "title": "Frederick Schram", "text": "Frederick Schram\n\nFrederick Robert Schram (born August 11, 1943 in Chicago, Illinois) is an American palaeontologist and carcinologist. He received his B.S. in biology from Loyola University Chicago in 1965, and a Ph.D. on palaeozoology from the University of Chicago in 1968 .\n\nHe has written over 200 papers on various aspects of crustacean biology, taxonomy and systematics, as well as several books, including the standard text \"Crustacea\" . In 1983, he founded the journal \"Crustacean Issues\", which he continued to edit for over twenty years. Much of his career has been spent at the Universiteit van Amsterdam, the Netherlands, from which he retired in 2005. In July 2005, he became the Editor of the \"Journal of Crustacean Biology\" .\n"}
{"id": "57855242", "url": "https://en.wikipedia.org/wiki?curid=57855242", "title": "Fundación Española para la Ciencia y la Tecnología", "text": "Fundación Española para la Ciencia y la Tecnología\n\nThe Fundación Española para la Ciencia y la Tecnología (FECYT) (English: Spanish Foundation for Science and Technology) is an independent nonprofit organization in Spain that promotes Spanish science and technology. It formed in 2001, and receives funding from the national Ministerio de Ciencia, Innovación y Universidades (former Ministerio de Educación y Ciencia).\n\n"}
{"id": "9274203", "url": "https://en.wikipedia.org/wiki?curid=9274203", "title": "GRAIL", "text": "GRAIL\n\nThe Gravity Recovery and Interior Laboratory (GRAIL) was an American lunar science mission in NASA's Discovery Program which used high-quality gravitational field mapping of the Moon to determine its interior structure. The two small spacecraft GRAIL A (Ebb) and GRAIL B (Flow) were launched on 10 September 2011 aboard a single launch vehicle: the most-powerful configuration of a Delta II, the 7920H-10. GRAIL A separated from the rocket about nine minutes after launch, GRAIL B followed about eight minutes later. They arrived at their orbits around the Moon 25 hours apart. The first probe entered orbit on 31 December 2011 and the second followed on 1 January 2012. The two spacecraft impacted the Lunar surface on December 17, 2012.\n\nMaria Zuber of the Massachusetts Institute of Technology is GRAIL's principal investigator. NASA's Jet Propulsion Laboratory manages the project. , the program has cost US$496 million. Upon launch the spacecraft were named GRAIL A and GRAIL B and a contest was opened to school children to select names. Nearly 900 classrooms from 45 states, Puerto Rico and the District of Columbia, participated in the contest. The winning names, Ebb and Flow, were suggested by 4th grade students at Emily Dickinson Elementary School in Bozeman, Montana.\n\nEach spacecraft transmitted and received telemetry from the other spacecraft and Earth-based facilities. By measuring the change in distance between the two spacecraft, the gravity field and geological structure of the Moon was obtained. The two spacecraft were able to detect very small changes in the distance between one another. Changes in distance as small as one micron were detectable and measurable. The gravitational field of the Moon was mapped in unprecedented detail.\n\n\nThe data collection phase of the mission lasted from 7 Mar 2012 to 29 May 2012, for a total of 88 days. A second phase, at a lower altitude, of data collection began 31 Aug 2012, and was followed by 12 months of data analysis. On 5 Dec 2012 NASA released a gravity map of the Moon made from GRAIL data. The knowledge acquired will aid understanding of the evolutionary history of the terrestrial planets and computations of lunar orbits.\n\n\nThrusters aboard each spacecraft were capable of producing . \nEach spacecraft was fueled with of hydrazine to be used by the thrusters and main engine to enable the spacecraft to enter lunar orbit and transition to the science phase of its mission. The propulsion subsystem consisted of a main fuel tank and a Re-repressurization system which were activated shortly after lunar orbit insertion.\n\nAll times are in EDT (UTC-4).\n\nUnlike the Apollo program missions, which took three days to reach the Moon, GRAIL made use of a three- to four-month low-energy trans-lunar cruise well outside the Moon's orbit and passing near the Sun-Earth Lagrange point L1 before looping back to rendezvous with the Moon. This extended and circuitous trajectory enabled the mission to reduce fuel requirements, protect instruments and reduce the velocity of the two spacecraft at lunar arrival to help achieve the extremely low orbits with separation between the spacecraft (arriving 25 hours apart) of . The very tight tolerances in the flight plan left little room for error correction leading to a launch window lasting one second and providing only two launch opportunities per day.\n\nThe primary science phase of GRAIL lasted for 88 days, from 7 Mar 2012 to 29 May 2012. It was followed by a second science phase starting on 8 Aug.\n\nThe gravity mapping technique was similar to that used by Gravity Recovery and Climate Experiment (GRACE), and the spacecraft design was based on XSS-11.\n\nThe orbital insertion dates were (for GRAIL-A) and (for GRAIL-B).\n\nThe spacecraft were operated over the 88-day acquisition phase, divided into three 27.3 day long nadir-pointed mapping cycles. Twice each day there was an 8-hour pass in view of the Deep Space Network for transmission of science and \"E/PO MoonKam\" data.\n\nAt the end of the science phase and a mission extension, the spacecrafts were powered down and decommissioned over a five-day period. The spacecraft impacted the lunar surface on December 17, 2012. Both spacecraft impacted an unnamed lunar mountain between Philolaus and Mouchez at . Ebb, the lead spacecraft in formation, impacted first. Flow impacted moments later. Each spacecraft was traveling at . A final experiment was conducted during the final days of the mission. Main engines aboard the spacecraft were fired, depleting remaining fuel. Data from that effort will be used by mission planners to validate fuel consumption computer models to improve predictions of fuel needs for future missions. NASA has announced that the crash site will be named after GRAIL collaborator and first American woman in space, Sally Ride.\n"}
{"id": "30950706", "url": "https://en.wikipedia.org/wiki?curid=30950706", "title": "GTRI Office of Policy Analysis and Research", "text": "GTRI Office of Policy Analysis and Research\n\nThe GTRI Office of Policy Analysis and Research is a division of the Georgia Tech Research Institute that focuses on policy analysis, particularly in fields where GTRI has science and technology experience. OPAR assists the Georgia General Assembly and publishes briefs on relevant issues, including how other states treat various issues.\n\nIn particular, OPAR hosts an annual \"Legislative Roundtable\" that brings together interested state representatives, prominent members of Georgia industry, and Georgia Tech students and faculty.\n\n"}
{"id": "826216", "url": "https://en.wikipedia.org/wiki?curid=826216", "title": "Galaxy morphological classification", "text": "Galaxy morphological classification\n\nGalaxy morphological classification is a system used by astronomers to divide galaxies into groups based on their visual appearance. There are several schemes in use by which galaxies can be classified according to their morphologies, the most famous being the Hubble sequence, devised by Edwin Hubble and later expanded by Gérard de Vaucouleurs and Allan Sandage.\n\nThe Hubble sequence is a morphological classification scheme for galaxies invented by Edwin Hubble in 1926.\nIt is often known colloquially as the “Hubble tuning-fork” because of the shape in which it is traditionally represented. Hubble’s scheme divides galaxies into three broad classes based on their visual appearance (originally on photographic plates):\n\nThese broad classes can be extended to enable finer distinctions of appearance and to encompass other types of galaxies, such as irregular galaxies, which have no obvious regular structure (either disk-like or ellipsoidal).\n\nThe Hubble sequence is often represented in the form of a two-pronged fork, with the ellipticals on the left (with the degree of ellipticity increasing from left to right) and the barred and unbarred spirals forming the two parallel prongs of the fork. Lenticular galaxies are placed between the ellipticals and the spirals, at the point where the two prongs meet the “handle”.\n\nTo this day, the Hubble sequence is the most commonly used system for classifying galaxies, both in professional astronomical research and in amateur astronomy.\n\nThe de Vaucouleurs system for classifying galaxies is a widely used extension to the Hubble sequence, first described by Gérard de Vaucouleurs in 1959. De Vaucouleurs argued that Hubble's two-dimensional classification of spiral galaxies—based on the tightness of the spiral arms and the presence or absence of a bar—did not adequately describe the full range of observed galaxy morphologies. In particular, he argued that rings and lenses are important structural components of spiral galaxies.\n\nThe de Vaucouleurs system retains Hubble’s basic division of galaxies into ellipticals, lenticulars, spirals and irregulars. To complement Hubble’s scheme, de Vaucouleurs introduced a more elaborate classification system for spiral galaxies, based on three morphological characteristics:\n\nThe different elements of the classification scheme are combined — in the order in which they are listed — to give the complete classification of a galaxy. For example, a weakly barred spiral galaxy with loosely wound arms and a ring is denoted SAB(r)c.\n\nVisually, the de Vaucouleurs system can be represented as a three-dimensional version of Hubble’s tuning fork, with stage (spiralness) on the \"x\"-axis, family (barredness) on the \"y\"-axis, and variety (ringedness) on the \"z\"-axis.\n\nDe Vaucouleurs also assigned numerical values to each class of galaxy in his scheme. Values of the numerical Hubble stage \"T\" run from −6 to +10, with negative numbers corresponding to early-type galaxies (ellipticals and lenticulars) and positive numbers to late types (spirals and irregulars). Elliptical galaxies are divided into three 'stages': compact ellipticals (cE), normal ellipticals (E) and late types (E). Lenticulars are similarly subdivided into early (S), intermediate (S) and late (S) types. Irregular galaxies can be of type magellanic irregulars (\"T\" = 10) or 'compact' (\"T\" = 11).\nThe use of numerical stages allows for more quantitative studies of galaxy morphology.\n\nCreated by American astronomer William Wilson Morgan. Together with Philip Keenan, Morgan developed the MK system for the classification of stars through their spectra. The Yerkes scheme uses the spectra of stars in the galaxy; the shape, real and apparent; and the degree of the central concentration to classify galaxies.\n\nThus, for example, the Andromeda Galaxy is classified as kS5.\n\n\n"}
{"id": "51568271", "url": "https://en.wikipedia.org/wiki?curid=51568271", "title": "George Kuo", "text": "George Kuo\n\nGeorge Ching-Hung Kuo is a scientist, who along with Michael Houghton, Qui-Lim Choo and Daniel W. Bradley, co-discovered and cloned Hepatitis C in 1989. The discovery of Hepatitis C led to the rapid development of diagnostic reagents to detect HCV in blood supplies which has reduced the risk of acquiring HCV through blood transfusion from one in three to about one in two million. It is estimated that antibody testing has prevented at least 40,000 new infections per year in the US alone and many more worldwide.\n\nHe graduated from the National Taiwan University in 1961 and completed his PhD in molecular biology at the Albert Einstein College of Medicine in 1972.\n\nHe was awarded the Karl Landsteiner Memorial Award (1992) and Dale A. Smith Memorial Award (2005) of the American Association of Blood Banks, and the William Beaumont Prize of the American Gastroenterological Association in 1994.\n"}
{"id": "11415165", "url": "https://en.wikipedia.org/wiki?curid=11415165", "title": "Goodman fatigue equation", "text": "Goodman fatigue equation\n\nThe Goodman fatigue equation is used by engineers for fatigue analysis. The equation is used to make correlations of experimental fatigue data of metals and other materials.\n\nThe equation is also used to determine the failure mechanisms of sucker rods in pumping oil wells worldwide, and to help design new sucker rod strings for rod-pumped oil wells.\n\nFrom similar triangles the Goodman fatigue equation is :\n\n( 4a) τm /Sus + τa /Ses = 1/n in which the stress components are given by \n\n"}
{"id": "51024444", "url": "https://en.wikipedia.org/wiki?curid=51024444", "title": "Hannah Fry", "text": "Hannah Fry\n\nHannah Fry (born 21 February 1984) is a British mathematician, lecturer on the Mathematics of Cities, television presenter and public speaker. Her work includes studying the patterns of human behaviour, such as relationships and dating and how mathematics can apply to them.\n\nFry attended Presdales School in Ware, Hertfordshire. She studied mathematics at University College London and stayed there to obtain a doctorate in fluid dynamics. In 2011 she qualified for the doctorate with a thesis entitled \"A Study of Droplet Deformation\".\n\nFry regularly appears on radio and television in the UK, including in \"Computing Britain\", \"The Curious Cases of Rutherford & Fry\" (with Adam Rutherford) and \"Music By Numbers\". In the BBC 2 series \"City in the Sky\". Fry studies the logistics of aviation. She has expressed concern about the future of the aviation industry, speculating that switching to electric planes or downsizing aircraft size may be a suitable long-term alternative.\n\nOn 30 March 2014, Fry gave a TED talk at TEDxBinghamtonUniversity titled \"The Mathematics of Love,\" which has attracted over 4.3 million views. Following the TED talk she published a book on the topic, \"The Mathematics of Love: Patterns, Proofs, and the Search for the Ultimate Equation,\" in which she applies statistical and data-scientific models to dating, sex and marriage.\n\nOn 17 September 2015 Fry presented a film biography of Ada Lovelace for BBC television.\n\nIn 2016, Fry co-presented the television programme \"Trainspotting Live\" with Peter Snow, a three-part series about trains and trainspotting, for BBC 4. She also hosted \"The Joy of Data,\" which examines the history of data and how they affect us today. A further credit for the year was her co-hosting an episode of UK TV series \"Horizon\" with Dr Xand van Tulleken titled \"\".\n\nIn 2017, Fry presented an episode of \"Horizon\" titled \"10 Things You Need to Know About the Future\". The following year, she presented \"Contagion! The BBC Four Pandemic\", a programme about the possible impact of a flu pandemic.\n\nFry has featured in several videos for a YouTube mathematics channel, Numberphile, run by Brady Haran.\n\nHannah Fry hosted a one-off 90-minute special of the BBC science programme \"Tomorrow's World\" along with four presenters from the show’s original run, Maggie Philbin, Howard Stableford, Judith Hann and Peter Snow. The show was broadcast at 9pm on BBC 4 on 22 November 2018.\n\nFry has published three books. The first, \"The Mathematics of Love\", includes the \"37% rule\", a form of the secretary problem according to which roughly the first third of any potential partners should be rejected. The second, \"The Indisputable Existence of Santa Claus\" (co-authored with fellow mathematician, Thomas Oléron Evans), discusses various Christmas-related topics and how mathematics can be involved in them, including a fair Secret Santa, decoration of Christmas trees, winning at \"Monopoly\", and comparing the vocabulary of the Queen's Christmas message to Snoop Dogg.\n\nFry lives in Lewisham, southeast London with her husband and one child.\n\nOn 8 August 2018, the Institute of Mathematics and its Applications and London Mathematical Society announced that Fry had won the 2018 Christopher Zeeman Medal \"for her contributions to the public understanding of the mathematical sciences\".\n\n\n"}
{"id": "2020038", "url": "https://en.wikipedia.org/wiki?curid=2020038", "title": "How now brown cow", "text": "How now brown cow\n\n\"How now brown cow\" () is a phrase used in elocution teaching to demonstrate rounded vowel sounds. Each \"ow\" sound in the phrase represents the diphthong /aʊ/. Although orthographies for each of the four words in this utterance is represented by the English spelling \"ow\", the articulation required to create this same diphthong represented by the International Phonetic Association's phonetic alphabet as /aʊ/ is also represented by the spelling \"ou\". Some examples of these homophonic /aʊ/'s are the English words \"house\", \"blouse\", \"noun\", and \"cloud\". The use of the phrase \"how now brown cow\" in teaching elocution can be dated back to at least 1926.\n\nAlthough not in use today, the phrase \"how now\" is a greeting, short for \"how say you now\", and can be found in archaic literature, such as the plays of William Shakespeare.\n"}
{"id": "2227640", "url": "https://en.wikipedia.org/wiki?curid=2227640", "title": "Image-forming optical system", "text": "Image-forming optical system\n\nIn optics, an image-forming optical system is a system capable of being used for imaging. The diameter of the aperture of the main objective is a common criterion for comparison among optical systems, such as large telescopes.\n\nThe two traditional systems are mirror-systems (catoptrics) and lens-systems (dioptrics), although in the late twentieth century, optical fiber was introduced. Catoptrics and dioptrics have a focal point, while optical fiber transfers an image from one plane to another without an optical focus.\n\nIsaac Newton is reported to have designed what he called a \"catadioptrical phantasmagoria\", which can be interpreted to mean an elaborate structure of both mirrors and lenses.\n\nCatoptrics and optical fiber have no chromatic aberration, while dioptrics need to have this error corrected. Newton believed that such correction was impossible, because he thought the path of the light depended only on its color. In 1757 John Dollond was able to create an achromatised dioptric, which was the forerunner of the lenses used in all popular photographic equipment today.\n\nLower-energy X-Rays are the highest energy electromagnetic radiation that can be formed into an image, using a Wolter telescope. There are three types of Wolter telescopes Near infrared is typically the longest wavelength that are handled optically, such as in some large telescopes.\n"}
{"id": "1121041", "url": "https://en.wikipedia.org/wiki?curid=1121041", "title": "International Union of Biological Sciences", "text": "International Union of Biological Sciences\n\nThe International Union of Biological Sciences (IUBS) is a non-profit organization and non-governmental organization, founded in 1919, that promotes the biological sciences internationally. As a scientific umbrella organization it was a founding member of the International Council for Science (ICSU).\n\nThe union has several key objectives:\n\nThe Union was a founding member of the ICSU Scientific Committee and works closely with UNESCO. It also maintains relations with the World Health Organization (WHO), the Food and Agriculture Organization (FAO) and the United Nations Environment Programme (UNEP). It cooperates with the European Commission and numerous other organizations, agencies and foundations.\n\nThe Union currently consists of:\n\nThe national and the academic members identify promising areas of biological science and bring them to the attention of the Union and in the reverse, promote the programs of the Union in their own country to stimulate research projects. The Union reviews the member's suggestions, checks them against the international academic and scientific-political background and develops the programs, if they have sufficient support. Approval is given in the General Assembly and the project progresses through international conferences for accreditation and then implementation through national or international funding agencies.\n\nThe Executive Committee consists of: the President, the former President, two Vice-Presidents, the Secretary, the Treasurer and other members of the Extended Board. The Board meets annually. The Secretariat, with its Executive Director coordinates the programs and activities.\n\nIn the General Assembly, each national member has one vote. The scientific members are invited to send one representative each to the talks and make programmatic proposals. The General Assembly elects the Executive Board for the proposed projects, selects the scientific programs of the Union, reviews the progress of scientific programs, collaborates with other international organizations and decides on the allocation of funding. The General Assembly takes place in parallel to a scientific conference, organized in cooperation with the National Union Committee of the host country.\n\nThe scientific programmes of the General Assembly are approved in accordance with the Statutes of the Union. Where necessary and possible, the EU has granted start-up funding for individual programs, that supports the further funding from national or international donors or by negotiated agreements with the Union.\n\nDiversitas, Human Dimensions of Biodiversity, Climate Change Integrative Biology (ICCB), Systematics Agenda, Biological Education (BioED), IUBS Bioethics Ethics Committee earlier, Bionomenclature, Biology and Traditional Knowledge, Biological Consequences of Global Change (BCGC) Darwin200, Biosystematics, Species 2000, Genomics and Evolution, Modernizing the codes to meet future needs of scientific communities (Biocode), Biology Research and Education Resources in Africa, Reproductive Biology, Aquaculture, Bio-Energy and Towards an Integrative Biology (TAIB).\n\nThe institution publishes four times a year \"Biology International\" and other publications such as \"IUBS Monograph Series\", \"Methodology Manual Series\" and the \"Proceedings of the IUBS General Assemblies\".\n\nThe International Union of Biological Sciences is non-profit and does not pursue economic goals. It is funded from the following sources:\n\nThe activities of the Union are within an annual budget of about 340,000, - € (2006). Of these, the salaries of a director and a secretary are paid, who run the Union's office in Paris. All other offices (President, Secretary General, Treasurer, etc.) are honorary offices for which only the direct expenses will be paid for.\n\nThe Union was founded in 1919 following the work of the \"Conference of Allied Academies of Sciences\" held in Brussels. Originally the 'S' was not for science but for Societies. \nAfter defining its constitution and organization in 1925, the IUBS adheres to the International Research Council (International Council for Science), which is now known by the acronym ICSU (International Council of Scientific Unions).\nFrom 1925 to 1939, the IUBS worked on two main themes: information science and the environment. This second project resulted in the creation of the \"World Conservation Union\" or IUCN.\nAfter being dormant for 1935 to 1949, the IUCB developed a highly original program of international research:\n\n\n\n"}
{"id": "21465194", "url": "https://en.wikipedia.org/wiki?curid=21465194", "title": "Karl-Theodor zu Guttenberg", "text": "Karl-Theodor zu Guttenberg\n\nKarl-Theodor Maria Nikolaus Johann Jacob Philipp Franz Joseph Sylvester Buhl-Freiherr von und zu Guttenberg (born 5 December 1971) is a German businessman and politician of the Christian Social Union (CSU). He served as a member of the Bundestag from 2002 to 2011, as Secretary-General of the CSU from 2008 to 2009, as Federal Minister for Economics and Technology in 2009 and as Federal Minister of Defence from 2009 to 2011. \n\nAfter the discovery of plagiarism in his doctoral dissertation and the decision of the University of Bayreuth to revoke his doctorate, an affair known as Guttenberg plagiarism scandal, he resigned from all political posts in March 2011.\n\nIn 2011, Guttenberg joined the Center for Strategic and International Studies (CSIS). Guttenberg is the chairman and a founder of \"Spitzberg Partners\", an advisory and investment firm based in New York City. He returned to German politics during the 2017 federal election by making a speech which was widely described as a comeback.\nIn 1991, after finishing high school (Gymnasium) in Rosenheim, Guttenberg completed his mandatory military service reaching the rank of Sergeant. Guttenberg studied law at the University of Bayreuth, where he passed the first legal state examination (said to be the equivalent of a master's degree) in 1999. Guttenberg chose not to pursue the second state examination (the equivalent of a bar exam), and focused on running the Munich-based \"Guttenberg GmbH\" holding where, along with a few employees, he managed his family's significant assets and various participations. Due to the holding's low turnover and small number of employees, it was said that Guttenberg had exaggerated his business experience.\n\nGuttenberg studied political science at the University of Bayreuth. He successfully defended his thesis \"Verfassung und Verfassungsvertrag. Konstitutionelle Entwicklungsstufen in den USA und der EU\" (\"Constitution and Constitutional Treaty. Stages of Constitutional Development in the USA and EU\"), and was awarded the academic title Doctor of Law (\"Doktor der Rechte\") in 2007. Following accusations of plagiarism in Guttenberg's thesis, the University of Bayreuth conducted an investigation, culminating on 23 February 2011 with the revocation of Guttenberg's doctorate.\n\nGuttenberg is a member of the Christian Social Union of Bavaria (CSU) and held different positions within the party, including that of secretary general.\n\nIn 2002, Guttenberg was elected to the Bundestag as the representative of Kulmbach. He was reelected in 2005, winning 60.0% of the votes in his constituency. In 2009, he was reelected again with 68.1% of the votes in his district, obtaining the highest percentage of votes of all elected representatives in Germany for that election cycle.\n\nFrom October 2005 to November 2008, Guttenberg served as chairman of the CDU/CSU parliamentary group in the Bundestag's Foreign Affairs Committee and as spokesman of the CDU/CSU parliamentary group in the Bundestag's Committee on disarmament, non-proliferation and arms control. He also chaired the CSU Foreign Policy Expert Committee and the German-British parliamentary group during that time.\n\nIn early 2004, Guttenberg introduced the concept of a Privileged Partnership between Turkey and the European Union as a viable alternative to accession of Turkey to the European Union into the German political discourse. Guttenberg based his opposition to full Turkish membership in the EU on the country's insufficient fulfillment of relevant accession criteria, for example with regard to the Cyprus dispute. At the same time, he stressed the necessity of maintaining good relations with Turkey and was therefore critical of a French initiative to criminalize the denial of the Armenian Genocide.\n\nGuttenberg also repeatedly warned of the looming threat posed to German and European security by Iran's nuclear and ballistic missile programs. However, he rejected taking rash military action against Iran and instead called for an international diplomatic effort to deal with Tehran's nuclear program. As a Member of Parliament, he was a strong critic of the far-left party Die Linke, which he accused of links to terrorists.\n\nIn September 2008, the CSU suffered heavy losses in the Bavaria state election and lost its absolute majority in the Bavarian Landtag - for the first time in 46 years. As a result of this political defeat the CSU party leadership stepped down and Horst Seehofer, the new CSU chairman and minister-president of Bavaria, appointed Guttenberg as secretary general of the CSU in November 2008.\n\nAs secretary-general, Guttenberg called for tax cuts, an increase in family benefits and structural reforms within the CSU to foster more direct political participation of the party base. In addition to domestic policy he also emphasized his focus on international affairs.\n\nAfter the resignation of Michael Glos on 9 February 2009 Guttenberg became Federal Minister of Economics and Technology in the first Merkel cabinet.\nGuttenberg, the youngest economics minister in the German post-war era, came to office in the midst of a deep global financial crisis and recession.\n\nIn the wake of the global financial crisis, several major German banks were near failure, including Hypo Real Estate, which received €102 billion of credit and guarantees from Germany's bank rescue fund. In this case, Guttenberg opposed an overly hasty nationalization of Hypo Real Estate, which he considered only as \"\"ultissima ratio\", a measure of the very last resort\". A few months later he drafted a legislative proposal to minimize the financial risks of failing banks, which caused controversy but later became the foundation of the German bank restructuring bill.\n\nIn the case of troubled German companies asking for state aid, including automaker Opel and now-defunct mail-order service Arcandor/Quelle, Guttenberg was reluctant to commit government resources. He insisted on strict conditionality, including restructuring, and limited support to only those companies which were otherwise competitive but were temporarily affected by the crisis.\n\nIn November 2008, Opel had appealed for governmental assistance because of severe financial problems facing its American parent, General Motors (GM). In 2009, Opel employed 25,000 workers in Germany and indirectly supported 50,000 additional jobs through its supplier network. In March 2009, Guttenberg made his first visit to the US as economics minister, focusing his trip on the future of Opel. In his talks with U.S. Treasury Secretary Timothy Geithner, the director of the United States National Economic Council, Lawrence Summers, and Rick Wagoner, then CEO of General Motors, Guttenberg demanded that GM provide a viable restructuring plan for Opel as a precondition for receiving financial assistance from the German state. Guttenberg and Wagoner agreed on the necessity of a private investor for Opel.\n\nAfter Guttenberg's visit to the US, negotiations between the German government, GM, and potential Opel investors, including Fiat and Canada's Magna International, were stalled by GM and the U.S. Treasury. In contrast to Merkel and other German political leaders, Guttenberg preferred insolvency for Opel rather than the infusion of unconditional financial assistance from the German state. Because of the resultant financial risks to the German state, Guttenberg opposed the sale of Opel to Magna International, favoured by Chancellor Merkel, and—according to media coverage—even offered his resignation over the controversy. The Opel-Magna deal later failed, and Opel remained a subsidiary of GM, who had to reimburse financial assistance to Germany. In the summer of 2009, he surpassed Angela Merkel as the most popular politician in Germany.\n\nThe 2009 Bundestag elections led to a change in government, as the incumbent grand coalition of CDU/CSU and SPD was replaced by a center-right coalition of CDU/CSU and FDP.\n\nAccording to German press reports, Chancellor Merkel offered Guttenberg the choice between the interior and the defence ministries while negotiating the distribution of ministerial posts within the new coalition government. Guttenberg decided to opt for the defence portfolio and took the oath of office on 28 October 2009 as part of the Second Cabinet Merkel. He was the youngest-ever German defence minister in the post-war era.\n\nThe first political challenge facing defence minister Guttenberg was dealing with the Kunduz airstrike of 4 September 2009. Initially, he adopted the position of his predecessor Jung and defended the air strike as \"militarily appropriate\". However, in contrast to Jung, Guttenberg conceded that the strike had also caused civilian casualties. After Guttenberg had received additional information and investigative reports dating back to the tenure of his predecessor Jung, Guttenberg changed his position concerning the \"Kunduz airstrike\" and dismissed Bundeswehr Chief of Staff Schneiderhahn and Parliamentary State Secretary of Defence Wichert on 26 November 2009.\n\nJung, who in the meantime had assumed the position of labor minister in the second Merkel cabinet, took full political responsibility for the delay in sharing relevant Kunduz air strike information and resigned the following day.\n\nAt the demand of the opposition parties, the Bundestag subsequently established a special investigative committee to shed light on the defense ministry's communications policy in connection with the Kunduz air strike.\nThe final report of the Bundestag's special investigative committee cleared Guttenberg from the accusation that he had been responsible for the defence ministry's inadequate communications policy following the Kunduz strike. The findings were supported by members from the ruling CDU/CSU-FDP coalition while the opposition parties criticized the special investigative committee's report and later published their own account of the investigation.\n\nShortly after taking office, Guttenberg publicly compared the situation faced by Bundeswehr soldiers in Afghanistan to \"war\". In doing so, Guttenberg broke a major political taboo since up until then Germany's political leadership – including the Chancellor and previous defense ministers – had only referred to the Afghanistan intervention as a \"stabilization deployment\".\nThe new classification of the Bundeswehr's Afghanistan deployment as \"war\" improved the legal status of German soldiers operating under international law.\nGuttenberg attempted to elevate public perception of Germany's Afghan mission by personally participating - sometimes along with the Chancellor - at funeral services held for fallen Bundeswehr soldiers. In November 2010, Guttenberg established the Combat Action Medal of the Bundeswehr, which is awarded for bravery in combat and to soldiers who were the targets of terrorist or military attacks.\n\nAt the political level, Guttenberg spoke out strongly against a military withdrawal from Afghanistan. He specifically warned against imposing fixed troop withdrawal timetables that do not take into account the security situation on the ground. Furthermore, Guttenberg also demanded a stronger involvement of key neighboring states such as Russia, India, and China in the resolution of the Afghan conflict.\nIn light of the traditionally challenging security situation in Afghanistan, Guttenberg called for the development of an internationally coordinated long-term security strategy – based on the use of special forces and close intelligence cooperation within the coalition – to stabilize the country even after the eventual withdrawal of all foreign troops.\n\nDuring his tenure as defense minister, Guttenberg made nine visits to Afghanistan and the German soldiers deployed there. To gain a first-hand understanding of the situation on the ground and the military risks of the Bundeswehr's mission, Guttenberg went several times to the frontlines of the Afghan conflict. He also invited journalists to accompany him on these trips in an effort to educate the wider German public about the nature of the Bundeswehr deployment in Afghanistan. In December 2010, Guttenberg traveled to Afghanistan along with his wife Stephanie to visit with the troops before the Christmas holidays. In addition, he was also accompanied by German TV moderator Johannes B. Kerner, who hosted his prime-time talk show at the Bundeswehr camp in Mazar-i-Sharif featuring the Guttenbergs and German soldiers deployed there. While other German media and the opposition parties sharply criticized Guttenberg for allowing Kerner to host his show in Afghanistan, the reaction by the German troops and the general public was predominantly positive.\n\nIn early 2010, Guttenberg decided to push for fundamental Bundeswehr reforms in an effort to address the structural deficits within the German armed forces and to deal with declining defense budgets. The overall goal was to boost the Bundeswehr's expeditionary capabilities while, at the same time, achieving cost reductions. To accomplish these reforms, Guttenberg proposed to reduce the armed forces to 165,000 active duty soldiers and to suspend the draft, resulting in the most comprehensive restructuring of the Bundeswehr since its founding in 1955. Guttenberg's reform plans were supported by a blue-ribbon \"Bundeswehr Structural Commission\" that the minister created in April 2010. The proposals triggered a major debate about the country's draft system and were met with significant political opposition, not least in Guttenberg's own political party. In the end, Guttenberg's view won out and on 29 October 2010, the CSU general party convention approved the minister's motion to suspend the draft by a large majority. Several weeks later, Chancellor Merkel's CDU held its own party convention and also voted in favor of suspending the draft. In November 2010, a United States diplomatic cables leak revealed that American diplomats viewed Guttenberg positively, with one cable describing him as a \"foreign policy expert, a transatlanticist and a close and well-known friend to the United States\".\n\nIn 2011, Guttenberg resigned amid controversy over his dissertation. The first accusations of plagiarism in Guttenberg's dissertation were made public in February 2011. Guttenberg's doctoral dissertation, \"Verfassung und Verfassungsvertrag\" (\"Constitution and Constitutional Treaty\"), had been the basis of his 2007 Doctorate from the University of Bayreuth. Guttenberg at first denied intentional plagiarism, calling the accusations \"absurd,\" but acknowledged that he may have made errors in his footnotes. In addition, it emerged that Guttenberg had requested a report from the Bundestag's research department, which he had then inserted into his thesis without attribution. On 23 February 2011, Guttenberg apologized in parliament for flaws in his thesis, but denied intentional deception and denied the use of a ghostwriter.\n\nOn 23 February 2011, the University of Bayreuth withdrew Guttenberg's doctorate. In part due to the expressions of confidence by Angela Merkel, the scandal continued to evoke heavy criticism from prominent academics, legal scholars (who accused Guttenberg of intentional plagiarism), and politicians both in the opposition and in the governing coalition. On 1 March 2011, Guttenberg announced his resignation as Minister of Defense, from his seat in the Bundestag, and from all other political offices.\n\nIn May 2011, a University of Bayreuth commission tasked with investigating Guttenberg's dissertation came to the conclusion that Guttenberg had engaged in intentional deception in the writing of his dissertation, and had violated standards of good academic practice. The commission found that he had included borrowed passages throughout his thesis, without citation, and had modified those passages in order to conceal their origin.\n\nIn November 2011, the prosecution in Hof discontinued the criminal proceedings for copyright violations against Guttenberg on condition of Guttenberg paying €20,000 to a charity. The prosecutor found 23 prosecutable copyright violations in Guttenberg's dissertation, but estimated that the material damage suffered by the authors of those texts was marginal.\n\nThe doctorate affair also led to scrutiny of other politicians' doctorates, notably that of his colleague in the federal cabinet Annette Schavan and Hungarian President Pál Schmitt, who lost their doctorates for similar reasons.\n\nIn September 2011, Guttenberg joined the Washington-based Center for Strategic and International Studies (CSIS). At the Halifax International Security Forum in November 2011, Guttenberg made his first public appearance since joining CSIS. During a plenary session on the economic and financial crisis he voiced pessimism about the current state of the EU and decried a severe \"crisis of political leadership\".\n\nIn November 2011, Guttenberg published the book \"Vorerst gescheitert\" (\"Failed for Now\"). The publication is based on a series of conversations with the editor-in-chief of Die Zeit, Giovanni di Lorenzo, in which Guttenberg talks extensively for the first time about his political career, the plagiarism scandal and his resignation, as well as his plans for the future. Guttenberg's criticism in the book of the direction which the CSU is headed sparked some controversy within his party.\n\nGuttenberg is a strong critic of Russian President Vladimir Putin's expansionist foreign policy. In Spring 2014, he decried an \"astonishing leadership vacuum in the world\" and demanded that the West, and especially Europe, respond to Moscow's aggressions with strong political action. He praised Chancellor Merkel for her firm stance against Putin. Guttenberg appeared together with Henry Kissinger during a CNN interview about the Ukraine crisis and explained the significant domestic political resistance that Merkel's Russia policy faced in Germany. Several months later Guttenberg accused the EU of inaction and a lack of capabilities given the crises in Ukraine and the Middle East.\n\nIn spring 2014, Guttenberg formed Spitzberg Partners LLC, a consulting and investment company. In September 2014, Spitzberg Partners and Canadian Acasta Capital founded \"Atlantic Advisory Partners\" (AAP), a partnership to promote business and trade between Canada and the European Union in connection with the Comprehensive Economic and Trade Agreement (CETA).\n\nAs a part of diplomatic outreach, Guttenberg and Karin Olofsdotter were part of a town hall style meeting in Peters Township, south of Pittsburgh, Pennsylvania.\n\nGuttenberg returned to German politics by making a speech during the German federal election, 2017 in which he hailed Chancellor Angela Merkel. The speech was widely reported on in German media and described as marking his comeback in German politics. Guttenberg also said Germany should preserve its relationship with the United States in order to renew the ties after the end of the Trump era, noting that \"they won’t forget it if, at a time when they were being mocked by the whole world, we maintained contacts with them and said this relationship is important now and will remain so in future.\"\n\nGuttenberg is a member of the House of Guttenberg, first documented in 1158, and conferred the rank of baron by the Holy Roman Emperor in 1700. Since the adoption of Germany's 1919 Weimar Constitution, which abolished the nobility's privileges, \"noble titles form part of the name only\".\n\nHis grandfather, Karl Theodor Freiherr von und zu Guttenberg (1921–1972), was a CSU politician and hard-line conservative during the Cold War, noted for his opposition to the Ostpolitik. During the Second World War, he narrowly escaped execution after refusing to kill Jews, stating that he would rather shoot SS members. Several other members of Guttenberg's family also offered resistance to the Nazi regime, among them his great-grandfather Georg Enoch Freiherr von und zu Guttenberg, and Karl Ludwig Freiherr von und zu Guttenberg, a great-great-uncle of Guttenberg. Karl Ludwig was a Catholic monarchist, who prior to the Second World War published the \"Weiße Blätter\" (White Papers), an important publication of the conservative opposition to the Nazi regime. He belonged to the circle of anti-Hitler conspirators around Hans von Dohnányi, Justus Delbrück, and Hans Oster. After the failure of the 20 July plot he was arrested and later executed. His grandmother, Rosa Sophie Prinzessin von Arenberg (1922-2012), was a member of the House of Arenberg.\nGuttenberg was born in Munich. He lived at his family castle in Guttenberg, Bavaria (district of Kulmbach), a village whose history is closely associated with the House of Guttenberg, and in a mansion in a refined part of Berlin, Berlin-Westend. The castle has been in the possession of the Guttenberg family since 1482. \n\nGuttenberg's father was Enoch zu Guttenberg, a conductor, who has been decorated with the Order of Merit of the Federal Republic of Germany (Officer's Cross) as well as the Bavarian Order of Merit. He has received many awards for his contributions to classical music, including the German Cultural Award and the Echo Klassik award.<br>\nPreviously, Enoch zu Guttenberg also owned the winery estate Weingut Reichsrat von Buhl.\n\nGuttenberg's mother, Christiane zu Eltz is a member of the Eltz family, which has strong ties with Croatia. She is the daughter of Ludwine, Countess Pejacsevich de Verocze. Her father was Jakob von und zu Eltz, a former President of the Association of Winemakers in Rheingau who became active in Croatian politics after Croatian independence. She divorced Enoch zu Guttenberg in 1977, and Karl-Theodor grew up with his father. His mother married secondly Adolf Richard Barthold von Ribbentrop, owner of an Eltville art gallery and son of Joachim von Ribbentrop, in 1985, and has two children from her second marriage. Guttenberg has a younger brother, Philipp Franz zu Guttenberg (born 1973), who married a daughter of Godfrey James Macdonald, the 8th Baron Macdonald.\n\nIn February 2000, Guttenberg married Stephanie Gräfin von Bismarck-Schönhausen (born 1976), a great-great-granddaughter of the first Chancellor of Germany Otto von Bismarck. They have two daughters. Guttenberg is Catholic, while his wife is Protestant (Lutheran). In September 2011, it became known that Guttenberg had bought a house for his family in Greenwich (Connecticut), close to New York City.\n\nGuttenberg was awarded the \"Politikaward\" in 2009, which is a German \"Politician of the Year\" award. It was awarded by \"politik & kommunikation\", a German periodical for political communications.\n\nIn 2010, the German news magazine \"Focus\" named him \"Man of the Year\". \n\nIn 2011, the Carneval Association of Aachen awarded him the \"Order Against Dead Seriousness\" (\"Orden wider den tierischen Ernst\"), although he did not attend the ceremony in person, sending instead his younger brother.\n\n\n\n"}
{"id": "1659766", "url": "https://en.wikipedia.org/wiki?curid=1659766", "title": "Karl Johann Bernhard Karsten", "text": "Karl Johann Bernhard Karsten\n\nKarl Johann Bernhard Karsten (26 November 1782 – 22 August 1853) was a German mineralogist known for contributions made to the German metallurgy industry.\n\nHe was born at Bützow in Mecklenburg-Schwerin and initially studied law in Rostock. From 1801 he devoted his time to mining and metallurgy. In 1819 he was named mining councilor to the Ministry of the Interior in Berlin. He was a major factor in the emergence of the zinc industry in Silesia.\n\nHe was author of several comprehensive works, including:\n\nHe was well known as editor of the \"Archiv für Bergbau und Hüttenwesen\" (Archive for mining and metallurgy; 20 volumes, 1818-1831); and (with Ernst Heinrich Karl von Dechen) of the \"Archiv für Mineralogie, Geognosie, Bergbau und Hüttenkunde\" (26 volumes, 1829-1854).\n\nHe died at Berlin in 1853. His son, Dr. Hermann Karsten (1809-1877), was a professor of mathematics and physics at the University of Rostock.\n"}
{"id": "5785677", "url": "https://en.wikipedia.org/wiki?curid=5785677", "title": "Landau's problems", "text": "Landau's problems\n\nAt the 1912 International Congress of Mathematicians, Edmund Landau listed four basic problems about prime numbers. These problems were characterised in his speech as \"unattackable at the present state of mathematics\" and are now known as Landau's problems. They are as follows:\n\n, all four problems are unresolved.\n\nVinogradov's theorem proves Goldbach's weak conjecture for sufficiently large \"n\". In 2013 Harald Helfgott proved the weak conjecture for all odd numbers greater than 5. Unlike Goldbach's conjecture, Goldbach's weak conjecture states that every odd number greater than 5 can be expressed as the sum of three primes. Although Goldbach's strong conjecture has not been proven or disproven, its proof would imply the proof of Goldbach's weak conjecture.\n\nChen's theorem proves that for all sufficiently large \"n\", formula_1 where \"p\" is prime and \"q\" is either prime or semiprime. Montgomery and Vaughan showed that the exceptional set (even numbers not expressible as the sum of two primes) was of density zero.\n\nIn 2015 Tomohiro Yamada proved an explicit version of Chen's theorem: every even number greater than formula_2 is the sum of a prime and a product of at most two primes.\n\nYitang Zhang showed that there are infinitely many prime pairs with gap bounded by 70 million, and this result has been improved to gaps of length 246 by a collaborative effort of the Polymath Project. Under the generalized Elliott–Halberstam conjecture this was improved to 6, extending earlier work by Maynard and Goldston, Pintz & Yıldırım.\n\nChen showed that there are infinitely many primes \"p\" (later called Chen primes) such that \"p\"+2 is either a prime or a semiprime.\n\nIt suffices to check that each prime gap starting at \"p\" is smaller than formula_3. A table of maximal prime gaps shows that the conjecture holds to 4×10. A counterexample near 10 would require a prime gap fifty million times the size of the average gap. Matomäki shows that there are at most formula_4 exceptional primes followed by gaps larger than formula_5; in particular,\n\nA result due to Ingham shows that there is a prime between formula_7 and formula_8 for every large enough \"n\".\n\nLandau's fourth problem asked whether there are infinitely many primes which are of the form formula_9 for integer \"n\". (The list of such primes is .) The existence of infinitely many such primes would follow as a consequence of other number-theoretic conjectures such as the Bunyakovsky conjecture and Bateman–Horn conjecture. , this problem is open. \n\nOne example of near-square primes are Fermat primes. Henryk Iwaniec showed that there are infinitely many numbers of the form formula_10 with at most two prime factors. Nesmith Ankeny proved that, assuming the extended Riemann hypothesis for L-functions on Hecke characters, there are infinitely many primes of the form formula_11 with formula_12. Landau's conjecture is for the stronger formula_13.\n\nDeshouillers and Iwaniec, improving on Hooley and Todd, showed that there are infinitely many numbers of the form formula_10 with greatest prime factor at least formula_15. Replacing the exponent with 2 would yield Landau's conjecture.\n\nThe Brun sieve establishes an upper bound on the density of primes having the form formula_9: there are formula_17 such primes up to formula_18. It then follows that almost all numbers of the form n+1 are composite.\n"}
{"id": "44437774", "url": "https://en.wikipedia.org/wiki?curid=44437774", "title": "Lens adapter", "text": "Lens adapter\n\nIn photography and videography, a lens adapter is a device that enables the use of camera and lens combinations from otherwise incompatible systems. The most simple lens adapter designs, passive lens adapters provide a secure physical connection between the camera and the lens. Some passive adapters may include a mechanism for manual iris control. So called, active lens adapters will include electronic connections, enabling communication between the lens and the camera.\n\nSome lens adapters include a special optical element called a telecompressor, focal reducer, or more recently a \"speed booster\", a genericised trademark that refers to the Metabones Speed Booster line of lens adapters. This type of adapter is designed to reduce focal length, increase lens speed, and in some instances improve MTF performance. Another innovator in the field is KIPON, a German/Chinese co-operation between Kipon and IB/E Optics.\n\nAnother branch of lens adapters include an ND-filter to simplify changing lenses, which HolyManta introduced in 2013.\n\nThe depth-of-field adapter (also called a DOF adapter or 35 mm adapter) is a largely obsolete device that uses a ground glass focusing screen to enable the use of interchangeable lenses on a fixed lens camcorder. There are also lens adapters made for other optical systems, including microscopes and telescopes.\n\nMost lens adapters feature a male fitting and a female fitting. The male fitting attaches to the camera, and the female fitting attaches to the lens. There is also an opening in the center to allow the light to pass through. Lens adapters can be shorter or longer, based on the respective flange focal distance of the lens and camera being adapted. Flange focal distance is the key specification in determining whether or not a particular type of lens can be adapted to any given camera. Generally speaking, cameras with a shorter flange focal distance can be adapted to a greater diversity of lenses. This is because there must be enough room to fit an adapter between the lens and the camera.\n\nIt is possible to physically mount a lens with a short flange focal distance on a camera with a long flange focal distance, but this will make it impossible to achieve infinity focus. The effect will be similar to that of an extension tube, because the lens will be mounted further from the image plane than intended. Some lens adapters utilize an optical element to compensate for this, effectively acting as a teleconverter. This introduces the typical side effects of a teleconverter, namely decreasing the amount of light that reaches the sensor, and adding a crop factor to the lens.\n\nAdditionally, there are several secondary factors to consider. Lenses designed for a smaller image sensor may not be adaptable to camera bodies with larger sensors as the image circle may not be large enough to cover the entire sensor, and will therefore exhibit vignetting. The age of the camera body may limit the availability of adapters; as camera bodies typically reach obsolescence before lenses. Finally, some adapters may include electronic or mechanical provisions in order to control the lens aperture and autofocus, to record the correct Exif data, or to report focus confirmation.\n\nThe following table shows commonly available adapters that don't require an optical element.\n\n†Image may exhibit vignetting due to sensor size.\n\n‡Theoretically possible, but may be difficult to find.\n\n∗Although the camera's flange distance is shorter than the lens', there is not enough distance between the flange and the lens to integrate the bayonet.\n\n"}
{"id": "65663", "url": "https://en.wikipedia.org/wiki?curid=65663", "title": "List of geological phenomena", "text": "List of geological phenomena\n\nA geological phenomenon is a phenomenon which is explained by or sheds light on the science of geology.\n\nExamples of geological phenomena are:\n"}
{"id": "43315", "url": "https://en.wikipedia.org/wiki?curid=43315", "title": "List of mail server software", "text": "List of mail server software\n\nThis is a list of mail server software: mail transfer agents, mail delivery agents, and other computer software which provide e-mail within Internet message handling services (MHS).\nAccording to one survey, sendmail, Microsoft Exchange Server, Postfix, and Exim together control over 90% of market share for SMTP service in 2014.\n\nAnother survey suggests a more balanced distribution, though it included hosted e-mail services such as Postini.\n\nThese surveys are rather difficult to produce, hence it is no surprise if they disagree.\n\n"}
{"id": "5976378", "url": "https://en.wikipedia.org/wiki?curid=5976378", "title": "Man and Power", "text": "Man and Power\n\nMan and Power: the Story of Power from the Pyramids to the Atomic Age is a science book for children by L. Sprague de Camp, illustrated with documents and photographs, and with paintings by Alton S. Tobey, first published in hardcover by Golden Press in 1961.\n\nAs stated on the cover, the work is a survey of \"the story of power from the pyramids to the atomic age.\" It traces the \"progression of man's discovery and utilization of power ... in chapters dealing consecutively with the different sources of power--animal, wind, water, steam, internal combustion, chemical, electrical, and nuclear power, and possible future sources.\"\n\n\nThomas Goonan, writing for \"Library Journal\", rated the book \"[r]ecommended,\" praising its \"[e]xcellent illustrations\" that \"elucidate the text\" and \"[g]ood index. Comparing it to Edward Stoddard's \"The Story of Power\", he judged de Camp's work \"[m]ore comprehensive and \ndetailed.\"\n\n\"The Science News-Letter\", in its September 23, 1961 issue, listed the book among its \"Books of the Week,\" describing the work as a \"[c]olorful panorama depicting and describing man's development of sources of energy to help him build, move around and produce.\"\n\n\"The Booklist\" considered the subject \"effectively presented in well-written text and a multitude of supplementary [illustrative materials], all captioned and most of them in color. Its review repeated Goonan's judgment of the work as \"more comprehensive\" than Stoddard's. In appraising the work for older children, \"The Booklist\" noted it was \"[j]uvenile in approach but may be useful in high schools, particularly for its illustrations.\n\nIsaac Asimov, writing for \"The Horn Book Magazine\", called it \"an exciting book written with great authority and illustrated lavishly,\" noting that \"[f]or young people interested in mechanics and machinery this book is a complete feast.\" He finds that \"[t]he human mind is the hero throughout,\" with \"[t]he personalities of scientists interest[ing] Mr. de Camp only as they affect the scientists as conveyors of new thoughts.\"\n\nClaire Huchet Bishop in \"The Commonweal\" praised the book's \"[e]xcellent approach which makes less of the machines than it does of the minds that created them.\"\n\nHenry W. Hubbard in \"The New York Times\" wrote that \"Mr. de Camp has filled his book with accurate information and absorbing history,\" but noted that while \"[t]he writing is usually good, ... the first chapter, on manpower, suffers from jarring transitions, and the detailed explanations of steam engines and such are occasionally hard to follow.\" He finds the illustrations \"colorful and skillfully chosen. They are, in fact, the high point of the book.\" Summing up, he states that \"[b]y virtue of its thoroughness, and its informative illustrations, \"Man and Power\" should be especially useful in libraries and schools.\"\n"}
{"id": "25683745", "url": "https://en.wikipedia.org/wiki?curid=25683745", "title": "Meat &amp; Livestock Australia", "text": "Meat &amp; Livestock Australia\n\nMeat & Livestock Australia (MLA) is an exempt public authority which provides research for the Australian red meat and livestock industry and promotes beef, lamb and goat meat in Australia and international markets.\n\nIts marketing activities include appearances by \"Lambassador\" Sam Kekovich.\nIn May 2011, an exposé on the ABCTV current affairs program \"Four Corners\" revealed poor processing techniques in certain Indonesian abattoirs. Facilities included in the program had received training and equipment from Australia's live export industry, including MLA, and Government through a co-funded animal welfare program.\n\nFrom mid-2011, the Australian Government has required the adoption of a new regulatory framework for Indonesian supply chains, including independent auditing, to provide greater oversight and accountability of in-market operations.\n\nThe Eastern Young Cattle Indicator (EYCI), an indicator of general cattle markets in Australia, is produced daily by MLA's National Livestock Reporting Service (NLRS).\n"}
{"id": "52380256", "url": "https://en.wikipedia.org/wiki?curid=52380256", "title": "Mediation-driven attachment model", "text": "Mediation-driven attachment model\n\nIn the scale-free network theory (mathematical theory of networks or graph theory), a mediation-driven attachment (MDA) model appears to embody a preferential attachment rule tacitly rather than explicitly. According to MDA rule, a new node first picks a node from the existing network at random and connect itself not with that but with one of the neighbors also picked at random.\n\nBarabasi and Albert in 1999 noted through their seminal paper noted that (i) most natural and man-made networks are not static, rather they grow with time and (ii) new nodes do not connect with an already connected one randomly rather preferentially with respect to their degrees. The later mechanism is called preferential attachment (PA) rule which embodies the rich get richer phenomena in economics. In their first model, known as the Barabási–Albert model, Barabási and Albert (BA model) choose\n\nwhere, formula_2 is the probability that the new node picks a node formula_3 from the labelled nodes of the existing network. It directly embodies the rich get richer mechanism.\n\nRecently, Hassan \"et al.\" proposed a mediation-driven attachment model which appears to embody the PA rule but not directly rather in disguise. In the MDA model, an incoming node choose an existing node to connect by first picking one of the existing nodes at random which is regarded as mediator. The new node then connect with one of the neighbors of the mediator which is also picked at random. Now the question is: What is the probability formula_2 that an already existing node formula_3 is finally picked to connect it with the new node? Say, the node formula_3 has degree formula_7 and hence it has formula_7 neighbors. Consider that the neighbors of formula_3 are labeled formula_10 which have degrees formula_11 respectively. One can reach the node formula_3 from each of these formula_7 nodes with probabilities inverse of their respective degrees, and each of the formula_7 nodes are likely to be picked at random with probability formula_15. Thus the probability formula_2 of the MDA model is:\n\nIt can be re-written as\n\nwhere the factor formula_19 is the inverse of the harmonic mean (IHM) of degrees of the formula_7 neighbors of the node formula_3. Extensive numerical simulation suggest that for small formula_22 the IHM value of each node fluctuate so wildly that the mean of the IHM values over the entire network bears no meaning. However, for large formula_22 (specially formula_22 approximately greater than 14) the distribution of IHM value of the entire network become left skewed Gaussian type and mean starts to have a meaning which becomes a constant value in the large formula_22 limit. In this limit one finds that formula_26 which is exactly the PA rule. It implies that the higher the links (degree) a node has, the higher its chance of gaining more links since they can be reached in a larger number of ways through mediators which essentially embodies the intuitive idea of rich get richer mechanism. Therefore, the MDA network can be seen to follow the PA rule but in disguise. Moreover, for small formula_22 the MFA is no longer valid rather the attachment probability formula_2 becomes super-preferential in character.\n\nThe idea of MDA rule can be found in the growth process of the weighted planar stochastic lattice (WPSL). An existing node (the center of each block of the WPSL is regarded as nodes and the common border between blocks as the links between the corresponding nodes) during the process gain links only if one of its neighbor is picked not itself. It implies that the higher the links (or degree) a node has, the higher its chance of gaining more links since they can be reached in a larger number of ways. It essentially embodies the intuitive idea of PA rule. Therefore, the dual of the WPSL is a network which can be seen to follow preferential attachment rule but in disguise. Indeed, its degree distribution is found to exhibit power-law as underlined by Barabasi and Albert as one of the essential ingredients.\n\nimplies that one can apply the mean-field approximation (MFA). That is, within this approximation one can replace the true IHM value formula_30 of each node by their mean, where the factor formula_22 that the number of edges the new nodes come with is introduced for latter convenience. The rate equation to solve then becomes exactly like that of the BA model and hence the network that emerges following MDA rule is also scale-free in nature. The only difference is that the exponent formula_32 depends on formula_22 where as in the BA model formula_34 independent of formula_22.\n\nIn the growing network not all nodes are equally important. The extent of their importance is measured by the value of their degree formula_36. Nodes which are linked to an unusually large number of other nodes, i.e. nodes with exceptionally high formula_36 value, are known as hubs. They are special because their existence make the mean distance, measured in units of the number of links, between nodes incredibly small thereby playing the key role in spreading rumors, opinions, diseases, computer viruses etc. It is, therefore, important to know the properties of the largest hub, which we regard as the leader. Like in society, the leadership in a growing network is not permanent. That is, once a node becomes the leader, it does not mean that it remains the leader \"ad infinitum\". An interesting question is: how long does the leader retain this leadership property as the network evolves? To find an answer to this question, we define the leadership persistence probability formula_38 that aleader retains its leadership for at least up to time formula_39. Persistence probability has been of interest in many different systems ranging from coarsening dynamics to fluctuating interfaces or polymer chains.\n\nThe basic idea of the MDA rule is, however not completely new as either this or models similar to this can be found in a few earlier works, albeit their approach, ensuing analysis and their results are different from ours. For instance, Saramaki and Kaski presented a random-walk based model. Another model proposed by Boccaletti \"et al.\" may appear similar to ours, but it markedly differs on closer look. Recently, Yang {\\it et al.} too gave a form for formula_2 and resorted to mean-field approximation. However, the nature of their expressions are significantly different from the one studied by Hassan \"et al.\". Yet another closely related model is the Growing Network with Redirection (GNR) model presented by Gabel, Krapivsky and Redner where at each time step a new node either attaches to a randomly chosen target node with probability formula_41, or to the parent of the target with probability formula_42. The GNR model with formula_42 may appear similar to the MDA model. However, it should be noted that unlike the GNR model, the MDA model is for undirected networks, and that the new link can connect with any neighbor of the mediator-parent or not. One more difference is that, in the MDA model new node may join the existing network with formula_22 edges and in the GNR model it is considered formula_45 case only.\n"}
{"id": "38928903", "url": "https://en.wikipedia.org/wiki?curid=38928903", "title": "Meiothermus", "text": "Meiothermus\n\nMeiothermus is a genus of Deinococcus–Thermus bacteria. \nThe phylogeny is based on 16S rRNA-based LTP release 132 by 'The All-Species Living Tree' Project.\n\nThe currently accepted taxonomy is based on the List of Prokaryotic names with Standing in Nomenclature (LSPN) and the National Center for Biotechnology Information.\n\n\nNote:\n♠ Strain found at the National Center for Biotechnology Information (NCBI) but not listed in the List of Prokaryotic names with Standing in Nomenclature (LPSN)\n\nList of bacterial orders\n"}
{"id": "2626472", "url": "https://en.wikipedia.org/wiki?curid=2626472", "title": "Michel Callon", "text": "Michel Callon\n\nMichel Callon (born 1945) is a professor of sociology at the École des mines de Paris and member of the Centre de sociologie de l'innovation. He is an influential author in the field of Science and Technology Studies and one of the leading proponents of actor–network theory (ANT) with Bruno Latour.\n\nIn recent years (since the late 1990s), Michel Callon has spearheaded the movement of applying ANT approaches to study economic life (notably economic markets). This body of work interrogates the interrelation between the economy and economics, highlighting the ways in which economics (and economics-inspired disciplines such as marketing) shapes the economy (see Callon, 1998 and 2005).\n\n\n\n\n"}
{"id": "1161816", "url": "https://en.wikipedia.org/wiki?curid=1161816", "title": "Microwave scanning beam landing system", "text": "Microwave scanning beam landing system\n\nThe microwave scanning beam landing system (MSBLS) was a K band approach and landing navigation aid used by NASA's space shuttle. It provided precise elevation, directional and distance data which was used to guide the orbiter for the last two minutes of flight until touchdown. The signal was typically usable from a horizontal distance of approximately 28 km and from an altitude of approximately 5 km (18,000 feet).\n\nMSBLS installations used by NASA had to be certified every two years for accuracy. From 2004, the Federal Aviation Administration worked with NASA to execute this verification; previously, only NASA aircraft and equipment were used. Testing of the Kennedy Space Center's MSBLS in 2004 revealed an accuracy of 5 centimeters.\n\nThe shuttle landing approach started with a glide slope of 19 degrees, which is over six times steeper than the typical 3-degree slope of commercial jet airliners.\n\n\n"}
{"id": "21591425", "url": "https://en.wikipedia.org/wiki?curid=21591425", "title": "Modified Newtonian dynamics", "text": "Modified Newtonian dynamics\n\nModified Newtonian dynamics (MOND) is a theory that proposes a modification of Newton's laws to account for observed properties of galaxies. It is an alternative to the theory of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.\n\nCreated in 1982 and first published in 1983 by Israeli physicist Mordehai Milgrom, the theory's original motivation was to explain that the velocities of stars in galaxies were observed to be larger than expected based on Newtonian mechanics. Milgrom noted that this discrepancy could be resolved if the gravitational force experienced by a star in the outer regions of a galaxy was proportional to the square of its centripetal acceleration (as opposed to the centripetal acceleration itself, as in Newton's second law), or alternatively if gravitational force came to vary inversely with radius (as opposed to the inverse square of the radius, as in Newton's law of gravity). In MOND, violation of Newton's laws occurs at extremely small accelerations, characteristic of galaxies yet far below anything typically encountered in the Solar System or on Earth.\n\nMOND is an example of a class of theories known as modified gravity, and is an alternative to the hypothesis that the dynamics of galaxies are determined by massive, invisible dark matter halos. Since Milgrom's original proposal, MOND has successfully predicted a variety of galactic phenomena that are difficult to understand from a dark matter perspective. However, MOND and its generalisations do not adequately account for observed properties of galaxy clusters, and no satisfactory cosmological model has been constructed from the theory.\n\nThe accurate measurement of the speed of gravitational waves compared to the speed of light in 2017 ruled out many theories which used modified gravity to explain dark matter.\nHowever, both Milgrom’s bi-metric formulation of MOND and nonlocal MOND are not ruled out according to the same study.\n\nSeveral independent observations point to the fact that the visible mass in galaxies and galaxy clusters is insufficient to account for their dynamics, when analysed using Newton's laws. This discrepancy – known as the \"missing mass problem\" – was first identified for clusters by Swiss astronomer Fritz Zwicky in 1933 (who studied the Coma cluster), and subsequently extended to include spiral galaxies by the 1939 work of Horace Babcock on Andromeda. These early studies were augmented and brought to the attention of the astronomical community in the 1960s and 1970s by the work of Vera Rubin at the Carnegie Institute in Washington, who mapped in detail the rotation velocities of stars in a large sample of spirals. While Newton's Laws predict that stellar rotation velocities should decrease with distance from the galactic centre, Rubin and collaborators found instead that they remain almost constant – the rotation curves are said to be \"flat\". This observation necessitates at least one of the following: 1) There exists in galaxies large quantities of unseen matter which boosts the stars' velocities beyond what would be expected on the basis of the visible mass alone, or 2) Newton's Laws do not apply to galaxies. The former leads to the dark matter hypothesis; the latter leads to MOND.\n\nThe basic premise of MOND is that while Newton's laws have been extensively tested in high-acceleration environments (in the Solar System and on Earth), they have not been verified for objects with extremely low acceleration, such as stars in the outer parts of galaxies. This led Milgrom to postulate a new effective gravitational force law (sometimes referred to as \"Milgrom's law\") that relates the true acceleration of an object to the acceleration that would be predicted for it on the basis of Newtonian mechanics. This law, the keystone of MOND, is chosen to reduce to the Newtonian result at high acceleration but lead to different (\"deep-MOND\") behaviour at low acceleration:\n\nHere \"F\" is the Newtonian force, \"m\" is the object's (gravitational) mass, \"a\" is its acceleration, \"μ\"(\"x\") is an as-yet unspecified function (known as the \"interpolating function\"), and \"a\" is a new fundamental constant which marks the transition between the Newtonian and deep-MOND regimes. Agreement with Newtonian mechanics requires \n\nand consistency with astronomical observations requires\n\nBeyond these limits, the interpolating function is not specified by the theory, although it is possible to weakly constrain it empirically. Two common choices are the \"simple interpolating function\":\n\nand the \"standard interpolating function\":\n\nThus, in the deep-MOND regime (\"a\" ≪ \"a\"):\n\nApplying this to an object of mass \"m\" in circular orbit around a point mass \"M\" (a crude approximation for a star in the outer regions of a galaxy), we find:\n\nthat is, the star's rotation velocity is independent of \"r\", its distance from the centre of the galaxy – the rotation curve is flat, as required. By fitting his law to rotation curve data, Milgrom found formula_6 to be optimal. This simple law is sufficient to make predictions for a broad range of galactic phenomena.\n\nMilgrom's law can be interpreted in two different ways. One possibility is to treat it as a modification to the classical law of inertia (Newton's second law), so that the force on an object is not proportional to the particle's acceleration \"a\" but rather to \"μ\"(\"a\"/\"a\")\"a\". In this case, the modified dynamics would apply not only to gravitational phenomena, but also those generated by other forces, for example electromagnetism. Alternatively, Milgrom's law can be viewed as leaving Newton's Second Law intact and instead modifying the inverse-square law of gravity, so that the true gravitational force on an object of mass \"m\" due to another of mass \"M\" is roughly of the form\n\nIn this interpretation, Milgrom's modification would apply exclusively to gravitational phenomena.\n\nBy itself, Milgrom's law is not a complete and self-contained physical theory, but rather an ad-hoc empirically motivated variant of one of the several equations that constitute classical mechanics. Its status within a coherent non-relativistic theory of MOND is akin to Kepler's Third Law within Newtonian mechanics; it provides a succinct description of observational facts, but must itself be explained by more fundamental concepts situated within the underlying theory. Several complete classical theories have been proposed (typically along \"modified gravity\" as opposed to \"modified inertia\" lines), which generally yield Milgrom's law exactly in situations of high symmetry and otherwise deviate from it slightly. A subset of these non-relativistic theories have been further embedded within relativistic theories, which are capable of making contact with non-classical phenomena (e.g., gravitational lensing) and cosmology. Distinguishing both theoretically and observationally between these alternatives is a subject of current research.\n\nThe majority of astronomers, astrophysicists and cosmologists accept ΛCDM (based on general relativity, and hence Newtonian mechanics), and are committed to a dark matter solution of the missing-mass problem. MOND, by contrast, is actively studied by only a handful of researchers. The primary difference between supporters of ΛCDM and MOND is in the observations for which they demand a robust, quantitative explanation and those for which they are satisfied with a qualitative account, or are prepared to leave for future work. Proponents of MOND emphasize predictions made on galaxy scales (where MOND enjoys its most notable successes) and believe that a cosmological model consistent with galaxy dynamics has yet to be discovered; proponents of ΛCDM require high levels of cosmological accuracy (which concordance cosmology provides) and argue that a resolution of galaxy-scale issues will follow from a better understanding of the complicated baryonic astrophysics underlying galaxy formation.\n\nSince MOND was specifically designed to produce flat rotation curves, these do not constitute evidence for the theory, but every matching observation adds to support of the empirical law. Nevertheless, a broad range of astrophysical phenomena are neatly accounted for within the MOND framework. Many of these came to light after the publication of Milgrom's original papers and are difficult to explain using the alternative dark matter hypothesis. The most prominent are the following:\n\n\nMilgrom's law requires incorporation into a complete theory if it is to satisfy conservation laws and provide a unique solution for the time evolution of any physical system. Each of the theories described here reduce to Milgrom's law in situations of high symmetry (and thus enjoy the successes described above), but produce different behaviour in detail.\n\nThe first complete theory of MOND (dubbed AQUAL) was constructed in 1984 by Milgrom and Jacob Bekenstein. AQUAL generates MONDian behaviour by modifying the gravitational term in the classical Lagrangian from being quadratic in the gradient of the Newtonian potential to a more general function. (AQUAL is an acronym for AQUAdratic Lagrangian.) In formulae:\n\nwhere formula_9 is the standard Newtonian gravitational potential and \"F\" is a new dimensionless function. Applying the Euler–Lagrange equations in the standard way then leads to a non-linear generalisation of the Newton–Poisson equation:\n\nThis can be solved given suitable boundary conditions and choice of F to yield Milgrom's law (up to a curl field correction which vanishes in situations of high symmetry).\n\nAn alternative way to modify the gravitational term in the lagrangian is to introduce a distinction between the true (MONDian) acceleration field a and the Newtonian acceleration field a. The Lagrangian may be constructed so that a satisfies the usual Newton-Poisson equation, and is then used to find a via an additional algebraic but non-linear step, which is chosen to satisfy Milgrom's law. This is called the \"quasi-linear formulation of MOND\", or QUMOND, and is particularly useful for calculating the distribution of \"phantom\" dark matter that would be inferred from a Newtonian analysis of a given physical situation.\n\nBoth AQUAL and QUMOND propose changes to the gravitational part of the classical matter action, and hence interpret Milgrom's law as a modification of Newtonian gravity as opposed to Newton's second law. The alternative is to turn the kinetic term of the action into a functional depending on the trajectory of the particle. Such \"modified inertia\" theories, however, are difficult to use because they are time-nonlocal, require energy and momentum to be non-trivially redefined to be conserved, and have predictions that depend on the entirety of a particle's orbit.\n\nIn 2004, Jacob Bekenstein formulated TeVeS, the first complete relativistic theory with MONDian behaviour. TeVeS is constructed from a local Lagrangian (and hence respects conservation laws), and employs a unit vector field, a dynamical and non-dynamical scalar field, a free function and a non-Einsteinian metric in order to yield AQUAL in the non-relativistic limit (low speeds and weak gravity). TeVeS has enjoyed some success in making contact with gravitational lensing and structure formation observations, but faces problems when confronted with data on the anisotropy of the cosmic microwave background, the lifetime of compact objects, and the relationship between the lensing and matter overdensity potentials.\n\nSeveral alternative relativistic generalisations of MOND exist, including BIMOND and generalised Einstein-Aether theories. There is also a relativistic generalisation of MOND that assumes a Lorentz-type invariance as the physical basis of MOND phenomenology.\n\nIn Newtonian mechanics, an object's acceleration can be found as the vector sum of the acceleration due to each of the individual forces acting on it. This means that a subsystem can be decoupled from the larger system in which it is embedded simply by referring the motion of its constituent particles to their centre of mass; in other words, the influence of the larger system is irrelevant for the internal dynamics of the subsystem. Since Milgrom's law is non-linear in acceleration, MONDian subsystems cannot be decoupled from their environment in this way, and in certain situations this leads to behaviour with no Newtonian parallel. This is known as the \"external field effect\" (EFE).\n\nThe external field effect is best described by classifying physical systems according to their relative values of \"a\" (the characteristic acceleration of one object within a subsystem due to the influence of another), \"a\" (the acceleration of the entire subsystem due to forces exerted by objects outside of it), and \"a\":\n\n\n\n\n\nThe external field effect implies a fundamental break with the strong equivalence principle (but not necessarily the weak equivalence principle). The effect was postulated by Milgrom in the first of his 1983 papers to explain why some open clusters were observed to have no mass discrepancy even though their internal accelerations were below a. It has since come to be recognised as a crucial element of the MOND paradigm.\n\nThe dependence in MOND of the internal dynamics of a system on its external environment (in principle, the rest of the universe) is strongly reminiscent of Mach's principle, and may hint towards a more fundamental structure underlying Milgrom's law. In this regard, Milgrom has commented:\n\nIt has been long suspected that local dynamics is strongly influenced by the universe at large, a-la Mach's principle, but MOND seems to be the first to supply concrete evidence for such a connection. This may turn out to be the most fundamental implication of MOND, beyond its implied modification of Newtonian dynamics and general relativity, and beyond the elimination of dark matter.\n\nIndeed, the potential link between MONDian dynamics and the universe as a whole (that is, cosmology) is augmented by the observation that the value of \"a\" (determined by fits to internal properties of galaxies) is within an order of magnitude of \"cH\", where \"c\" is the speed of light and \"H\" is the Hubble constant (a measure of the present-day expansion rate of the universe). It is also close to the acceleration rate of the universe, and hence the cosmological constant. However, as yet no full theory has been constructed which manifests these connections in a natural way.\n\nWhile acknowledging that Milgrom's law provides a succinct and accurate description of a range of galactic phenomena, many physicists reject the idea that classical dynamics itself needs to be modified and attempt instead to explain the law's success by reference to the behaviour of dark matter. Some effort has gone towards establishing the presence of a characteristic acceleration scale as a natural consequence of the behaviour of cold dark matter halos, although Milgrom has argued that such arguments explain only a small subset of MOND phenomena. An alternative proposal is to modify the properties of dark matter (e.g., to make it interact strongly with itself or baryons) in order to induce the tight coupling between the baryonic and dark matter mass that the observations point to. Finally, some researchers suggest that explaining the empirical success of Milgrom's law requires a more radical break with conventional assumptions about the nature of dark matter. One idea (dubbed \"dipolar dark matter\") is to make dark matter gravitationally polarisable by ordinary matter and have this polarisation enhance the gravitational attraction between baryons.\n\nThe most serious problem facing Milgrom's law is that it cannot completely eliminate the need for dark matter in all astrophysical systems: galaxy clusters show a residual mass discrepancy even when analysed using MOND. The fact that some form of unseen mass must exist in these systems detracts from the elegance of MOND as a solution to the missing mass problem, although the amount of extra mass required is 5 times less than in a Newtonian analysis, and there is no requirement that the missing mass be non-baryonic. It has been speculated that 2 eV neutrinos could account for the cluster observations in MOND while preserving the theory's successes at the galaxy scale. Indeed, analysis of sharp lensing data for the galaxy cluster Abell 1689 shows that MOND only becomes distinctive at Mpc distance from the center, so that Zwicky's conundrum remains , and 1.8 eV neutrinos are needed in clusters. \n\nThe 2006 observation of a pair of colliding galaxy clusters known as the \"Bullet Cluster\", poses a significant challenge for all theories proposing a modified gravity solution to the missing mass problem, including MOND. Astronomers measured the distribution of stellar and gas mass in the clusters using visible and X-ray light, respectively, and in addition mapped the inferred dark matter density using gravitational lensing. In MOND, one would expect the missing mass (which is only apparent since it results from using Newtonian as opposed to MONDian dynamics) to be centred on the visible mass. In ΛCDM, on the other hand, one would expect the dark matter to be significantly offset from the visible mass because the halos of the two colliding clusters would pass through each other (assuming, as is conventional, that dark matter is collisionless), whilst the cluster gas would interact and end up at the centre. An offset is clearly seen in the observations. It has been suggested, however, that MOND-based models may be able to generate such an offset in strongly non-spherically-symmetric systems, such as the Bullet Cluster.\n\nSeveral other studies have noted observational difficulties with MOND. For example, it has been claimed that MOND offers a poor fit to the velocity dispersion profile of globular clusters and the temperature profile of galaxy clusters, that different values of a are required for agreement with different galaxies' rotation curves, and that MOND is naturally unsuited to forming the basis of a theory of cosmology. Furthermore, many versions of MOND predict that the speed of light be different from the speed of gravity, but in 2017 the speed of gravitational waves was measured to be equal to the speed of light.\n\nBesides these observational issues, MOND and its generalisations are plagued by theoretical difficulties. Several ad-hoc and inelegant additions to general relativity are required to create a theory with a non-Newtonian non-relativistic limit, the plethora of different versions of the theory offer diverging predictions in simple physical situations and thus make it difficult to test the framework conclusively, and some formulations (most prominently those based on modified inertia) have long suffered from poor compatibility with cherished physical principles such as conservation laws.\n\nSeveral observational and experimental tests have been proposed to help distinguish between MOND and dark matter-based models:\n\n\nTechnical:\n\nPopular:\n\n"}
{"id": "591348", "url": "https://en.wikipedia.org/wiki?curid=591348", "title": "National Science Bowl", "text": "National Science Bowl\n\nThe National Science Bowl (NSB) is a high school and middle school science knowledge competition using a quiz bowl format held in the United States. A buzzer system similar to those seen on popular television game shows is used to signal an answer. The competition has been organized and sponsored by the United States Department of Energy since its inception in 1991.\n\nDuring the competition, each question has a category and the high school and middle schools have a different set of categories for their questions. The 2017 high school categories are Chemistry, Biology, Physics, Mathematics, Energy, and Earth and Space Science. The 2017 middle school categories are Physical Science, Life Science, Mathematics, Energy, and Earth and Space Science. \n\nSeveral categories have been added, dropped, or merged throughout the years. Computer Science was dropped from the list in late 2002. Current Events was in the 2005 competition, but did not make a return. General Science was dropped for the high school competition and Astronomy was merged with Earth Science to create Earth and Space in 2011. General Science was also dropped for the middle school competition in 2015.\n\nThe winning team of each regional Science Bowl competition is invited to participate in the National Science Bowl all expenses paid. There are a number of regional competitions across the United States. In the early years, the exact number changed from year to year. For example, in 2006 there were 65 regionals while in 2004 there were 64 regionals, and in 2003 there were 66 regionals. As of 2018, however, there are 65 high school regionals and 48 middle school regionals. These figures include the two \"super regional\" sites that are permitted to send two teams to the national competition. The two super regionals are the Kansas/Missouri Regional High School Science Bowl and the Connecticut/Northeast Regional High School Science Bowl (The Northeast Regional includes Rhode Island, Massachusetts, New Hampshire, Vermont, and parts of New York).\n\nTypically, any school that meets the eligibility requirements of the National Science Bowl competition is permitted to register for the regional competition according to its geographic location. No school may compete in multiple regionals. In addition, most regional competitions permit schools to register up to three teams. Beginning in 2017, club teams will no longer be able to compete.\n\nThis section is concerned with the rules of the national competition. The rules of regional competitions vary greatly. There are very few prescribed rules for regional competitions. Some regionals are run nearly identically to the national competition, while others use variations of the rules or different methods of scoring.\n\nA team consists of 4 or 5 students from a single school. Only 4 students play at any one time, while the 5th is designated as the alternate. Substitutions and switching captains may occur at halftime and between rounds.\n\nTwo teams compete against each other in each match. Each team member is given a number A1, A Captain, A2, A3, B1, B Captain, B2, B3, according to the position each student sits in. In regional competitions, each round consists of 23 questions (that is, 23 toss-ups and 23 corresponding bonuses). At the National Finals, each round consists of 25 questions. The match is over when all the toss-up questions have been read (and any bonuses related to correctly answered toss-ups), or after two halves have elapsed, whichever occurs first. The team with the most points at this time is the winner. At the regional level, all matches consist of two 8-minute halves, separated by a 2-minute break. At the national level for middle schools, all matches consist of two 10-minute halves. For high schools, all round robin and some double elimination matches consist of two 10-minute halves, with the final rounds consisting of two 12-minute halves to accommodate the longer visual bonus questions.\n\nEvery match begins with a toss-up question. The moderator announces the subject of the question (see \"Subject Areas\" above), as well as its type (Multiple Choice or Short Answer). Once the moderator completes the reading of the question, students have 5 seconds to buzz in and give an answer. Students may buzz in at any time after the category has been read—there is no need to wait for the moderator to finish. However, there is a penalty for interrupting the moderator and giving an incorrect answer. Once a student from a team has buzzed in, that team may not buzz in again on that question. Conferring between members of a team is not allowed on toss-up questions; if conferring occurs on a question, the team is disallowed from answering that question. The rules regarding conferring are typically very strict: excessive noise, eye contact, or even noticeable shifts in position can be considered conferring, as they convey information to teammates.\n\nAn answer given by a student is ruled correct or incorrect by the moderator. On short answer questions, if the answer given differs from the official one, the moderator uses his or her judgment to make a ruling (which is subject to a challenge by the competitors). On multiple choice questions, the answer given by the student is only correct if it matches the official answer \"exactly\". Alternatively, the student may give the letter choice that corresponds to the correct answer. Although A, B, C, and D were once used as answer choice letters, W, X, Y, and Z are now favored due to a lower chance of confusion. On short answers, the answer does not have to exactly match the official answer, so any given answer that roughly means the same as the official answer is accepted.\n\nIf a student answers a toss-up question correctly, that student's team receives a bonus question. The bonus question is always in the same category as the corresponding toss-up question, though it may not always relate to the toss-up question. Since only that team has the opportunity to answer the bonus question, there is no need to buzz in to answer it. After the moderator finishes reading the question, the team has 20 seconds to answer. The timekeeper will give a 5-second warning when 5 seconds remain. Conferring between team members is permitted, but the team captain must give the team's final answer.\n\nVisual bonuses were introduced in 2003. They are only included in the final elimination rounds. The team has 30 seconds to answer a question with the aid of a visual displayed on a 19-inch monitor (for the final matches) or on a distributed worksheet (for earlier elimination matches).\n\nThe same rules apply to the judging of responses to bonus questions as apply to responses to toss-up questions. Once the team's answer has been ruled right or wrong, the moderator proceeds to the next toss-up question.\n\nIf neither team answers the toss-up question correctly, the bonus question is not read, and the moderator proceeds to the next toss-up question.\n\nCorrect responses to toss-up questions are worth 4 points each. If a student buzzes in on a toss-up question before the moderator has completely read the question (i.e., interrupts the moderator) and responds \"incorrectly\", 4 points are awarded to the opposing team, and the question is re-read in its entirety so that the opposing team has an opportunity to buzz in.\n\nA correct response on a bonus question earns 10 points, making the total possible score on a single question 18 points (4 for a correct answer, 4 penalty points, and 10 for the bonus), and a perfect score 450 points. Against a team which never buzzes in (often the better approximation), the maximum (perfect) score is 350 points. This is assuming that only the regular 25 questions have been answered. Earning more than 200 points in one game is very impressive and earning more than 300 points is extremely rare.\n\nThere are various different types of penalties given. If a player interrupts and answers incorrectly, four penalty points are awarded to the opposing team. If a player answers without being recognized (saying his number out loud), four penalty points are also given to the other team. \n\nCommunication among team members during a toss-up incurs no point penalty, but disqualifies that team from answering the question. However, if the team that is no longer able to answer the toss-up engages in distracting behavior, then the opposing team is awarded four points, the opportunity to answer the corresponding bonus, and the option of running 20 additional seconds off the game clock. Similarly, if the team not playing the bonus engages in distracting behavior, then the opposing team is awarded ten points and the option of running 40 additional seconds off the game clock.\n\nChallenges must be made before the moderator begins reading the next question, or 3 seconds after the last question of the half or game. The question officially begins after its subject area has been read. Only the 4 actively competing members may challenge. The fifth team member, coach, and others associated with a team may not become involved in challenges or their discussion.\n\nChallenges may be made either to scientific content or the administration of rules. They may not be made to judgment calls by the officials, such as whether a buzz was an interrupt, whether 20 seconds have passed before beginning to answer a bonus, or whether a stall or blurt has happened. Challenges to scientific content is limited to 2 unsuccessful challenges per round. Successful challenges do not count against this limit. Challenges to rules may be made at any time.\n\nThis section is concerned with the format of the national competition only. As is the case with competition rules, the competition format varies greatly among the different regional competitions.\n\nRegionals typically use round robin, single-elimination, double-elimination, or any combination of these formats.\n\nThe national competition always consists of two stages: round-robin and double-elimination.\n\nAll competing teams are randomly arranged (each team captain randomly picks a division and position on the first day of the National Finals) into several round-robin groups of eight or nine teams each. Every team plays every other team in its group once, receiving 2 points for a win, 1 point for a tie, or 0 points for a loss. If a team's opponent has not arrived, that team can practice instead. The rules still apply, though any win or loss is not counted. The top two teams from each group advance to the double-elimination stage.\n\nIn the event that two or more teams are tied for one of the top spots in a division, the result of the Division Team Challenge (DTC) is used as a tiebreak. This method is only used for high schools.\n\nIn previous rules, there were several tiebreak procedures, applied in the following order:\n\nIn years before that, there was no DTC, so the following procedure was used in its place:\n\nIf a tie still existed after this procedure, it is reapplied until the tie is resolved. These last rules are still used for the Middle School competition.\n\nTo determine which middle schools that are tied in third place in each division advance to double-elimination, several procedures are used:\n\nApproximately 16 teams advance from the round-robin (depending on the number of round robin groups). In 2004 and 2007, exactly 16 teams advanced, while in 2003 and 2002, 18 teams advanced. In 2006, the teams were seeded into a single-elimination tournament based on their preliminary round-robin results. In previous years, a team's position in the double-elimination tournament was determined by random draw; teams were not seeded in any way. The competition then proceeded (in 2006) like a typical single-elimination tournament. Seeding continued in the 2007 tournament: teams that won their pool were paired against teams that placed second in theirs. Unlike in the round-robin, a match in double-elimination cannot be tied. If a match is tied at the end of regulation, overtime periods of five toss-ups each are played until the tie is broken.\n\nThe elimination tournament produces a first-place, second-place, third-place, and fourth-place team. Except for the 2006 tournament, a double-elimination tournament format has been used, allowing a fifth place to be added. The tournament reverted to a double-elimination format for the 2007 tournament, without a fifth-place match.\n\nThe top two high school teams receive trips to one of the National Parks, all-expenses paid.\n\nThe top three middle and high school teams receive a trophy, individual medals, and photographs with officials of the Department of Energy.\n\nThe top 16 middle and high schools earn a $1,000 check for their school's science departments.\n\nEach team with the best Division Team Challenge result in their division earns a $500 check for their school's science department.\n\nFor the middle school teams, the DOE also sponsors a car competition challenging competitors to construct a car capable of attaining high speeds. They are powered through alternative energy sources such as hydrogen fuel cells and solar panels. The winners of the car competition are awarded with $500 for their school.\n\nSeveral companies and organizations sponsor the National Science Bowl competition, the most prominent being the United States Department of Energy. General Motors is also a regular sponsor of the event, and has in recent years sponsored the Hydrogen Fuel Cell Car competition held at NSB, where 16 teams compete to build the fastest or most powerful fuel cell-powered miniature car. IBM and Bechtel also sponsor the NSB.\n\nThe winning teams from the years 1991-2001 were\n\n\n\n"}
{"id": "20989387", "url": "https://en.wikipedia.org/wiki?curid=20989387", "title": "Netprimer", "text": "Netprimer\n\nNetPrimer is a gratis web-based tool used for analysing primers used in PCR to amplify a DNA sequence. The software predicts the melting temperature of the primers using the nearest neighbor thermodynamic algorithm. The accurate prediction of the melting temperature (Tm) is one of the most important factors that governs the success of a PCR reaction.\n\nNetPrimer also analyzes the thermodynamically important secondary structures such as hairpins, self and cross dimers, runs and repeats. These structures significantly affect the primer efficiency and therefore the success of a PCR reaction.\n\nNetPrimer can be used to determine the best primer pairs for a given set of experimental conditions. The program assigns a rating to each primer analyzed. The rating is based on the proximity of the thermodynamic parameters to their ideal scores.\n\nIn addition to the primer quality, its molecular weight and optical activity (both in nmol/A260 & µg/A260) are also presented for quantitation. Primers are analyzed for their GC% (Guanine-Cytosine content). This important parameter determines their annealing strength.\n\nAlthough Netprimer is provided without charge, it is not free software. Users must register for access and thereby receive advertising of Premier Biosoft's other products.\n\n"}
{"id": "25110709", "url": "https://en.wikipedia.org/wiki?curid=25110709", "title": "Nuclear magnetic resonance", "text": "Nuclear magnetic resonance\n\nNuclear magnetic resonance (NMR) is a physical phenomenon in which nuclei in a strong static magnetic field are perturbed by a weak oscillating magnetic field (in the near field and therefore not involving electromagnetic waves) and respond by producing an electromagnetic signal with a frequency characteristic of the magnetic field at the nucleus. This process occurs near resonance, when the oscillation frequency matches the intrinsic frequency of the nuclei, which depends on the strength of the static magnetic field, the chemical environment, and the magnetic properties of the isotope involved; in practical applications with static magnetic fields up to ca. 20 tesla, the frequency is similar to VHF and UHF television broadcasts (60–1000 MHz). NMR results from specific magnetic properties of certain atomic nuclei. Nuclear magnetic resonance spectroscopy is widely used to determine the structure of organic molecules in solution and study molecular physics, crystals as well as non-crystalline materials. NMR is also routinely used in advanced medical imaging techniques, such as in magnetic resonance imaging (MRI).\n\nAll isotopes that contain an odd number of protons and/or neutrons (see Isotope) have an intrinsic nuclear magnetic moment and angular momentum, in other words a nonzero nuclear spin, while all nuclides with even numbers of both have a total spin of zero. The most commonly used nuclei are and , although isotopes of many other elements (e.g. , , , , , , , , , , , , , , ) have been studied by high-field NMR spectroscopy as well.\n\nA key feature of NMR is that the resonance frequency of a particular simple substance is usually directly proportional to the strength of the applied magnetic field. It is this feature that is exploited in imaging techniques; if a sample is placed in a non-uniform magnetic field then the resonance frequencies of the sample's nuclei depend on where in the field they are located. Since the resolution of the imaging technique depends on the magnitude of the magnetic field gradient, many efforts are made to develop increased gradient field strength. \n\nThe principle of NMR usually involves three sequential steps:\n\nThe two magnetic fields are usually chosen to be perpendicular to each other as this maximizes the NMR signal strength. The frequencies of the time-signal response by the total magnetization (M) of the nuclear spins are analyzed in NMR spectroscopy and magnetic resonance imaging. Both use applied magnetic fields (B) of great strength, often produced by large currents in superconducting coils, in order to achieve dispersion of response frequencies and of very high homogeneity and stability in order to deliver spectral resolution, the details of which are described by chemical shifts, the Zeeman effect, and Knight shifts (in metals). The information provided by NMR can also be increased using hyperpolarization, and/or using two-dimensional, three-dimensional and higher-dimensional techniques.\n\nNMR phenomena are also utilized in low-field NMR, NMR spectroscopy and MRI in the Earth's magnetic field (referred to as Earth's field NMR), and in several types of magnetometers.\n\nNuclear magnetic resonance was first described and measured in molecular beams by Isidor Rabi in 1938, by extending the Stern–Gerlach experiment, and in 1944, Rabi was awarded the Nobel Prize in Physics for this work. In 1946, Felix Bloch and Edward Mills Purcell expanded the technique for use on liquids and solids, for which they shared the Nobel Prize in Physics in 1952.\n\nYevgeny Zavoisky likely observed nuclear magnetic resonance in 1941, well before Felix Bloch and Edward Mills Purcell, but dismissed the results as not reproducible.\n\nRussell H. Varian filed the \"Method and means for correlating nuclear properties of atoms and magnetic fields\", on July 24, 1951. Varian Associates developed the first NMR unit called NMR HR-30 in 1952.\n\nPurcell had worked on the development of radar during World War II at the Massachusetts Institute of Technology's Radiation Laboratory. His work during that project on the production and detection of radio frequency power and on the absorption of such RF power by matter laid the foundation for his discovery of NMR in bulk matter.\n\nRabi, Bloch, and Purcell observed that magnetic nuclei, like and , could absorb RF energy when placed in a magnetic field and when the RF was of a frequency specific to the identity of the nuclei. When this absorption occurs, the nucleus is described as being \"in resonance\". Different atomic nuclei within a molecule resonate at different (radio) frequencies for the same magnetic field strength. The observation of such magnetic resonance frequencies of the nuclei present in a molecule allows any trained user to discover essential chemical and structural information about the molecule.\n\nThe development of NMR as a technique in analytical chemistry and biochemistry parallels the development of electromagnetic technology and advanced electronics and their introduction into civilian use.\n\nAll nucleons, that is neutrons and protons, composing any atomic nucleus, have the intrinsic quantum property of spin, an intrinsic angular momentum analogous to the classical angular momentum of a spinning sphere. The overall spin of the nucleus is determined by the spin quantum number \"S\". If the numbers of both the protons and neutrons in a given nuclide are even then , i.e. there is no overall spin. Then, just as electrons pair up in nondegenerate atomic orbitals, so do even numbers of protons or even numbers of neutrons (both of which are also spin formula_1 particles and hence fermions), giving zero overall spin.\n\nHowever, a proton and will have lower energy when their spins are parallel, not anti-parallel. This parallel spin alignment of distinguishable particles does not violate the Pauli exclusion principle. The lowering of energy for parallel spins has to do with the quark structure of these two nucleons. As a result, the spin ground state for the deuteron (the nucleus of deuterium, the H isotope of hydrogen), which has only a proton and a neutron, corresponds to a spin value of 1, \"not of zero\". On the other hand, because of the Pauli exclusion principle, the tritium isotope of hydrogen must have a pair of anti-parallel spin neutrons (of total spin zero for the neutron-spin pair), plus a proton of spin . Therefore, the tritium total nuclear spin value is again , just like for the simpler, abundant hydrogen isotope, H nucleus (the \"proton\"). The NMR absorption frequency for tritium is also similar to that of H. In many other cases of \"non-radioactive\" nuclei, the overall spin is also non-zero. For example, the nucleus has an overall spin value .\n\nA non-zero spin formula_2 is always associated with a non-zero magnetic dipole moment, formula_3, via the relation\n\nformula_4\n\nwhere \"γ\" is the gyromagnetic ratio. Classically, this corresponds to the proportionality between the angular momentum and the magnetic dipole moment of a spinning charged sphere, both of which are vectors parallel to the rotation axis whose length increases proportional to the spinning frequency. It is the magnetic moment and its interaction with magnetic fields that allows the observation of NMR signal associated with transitions between nuclear spin levels during resonant RF irradiation or caused by Larmor precession of the average magnetic moment after resonant irradiation. Nuclides with even numbers of both protons and neutrons have zero nuclear magnetic dipole moment and hence do not exhibit NMR signal. For instance, is an example of a nuclide that produces no NMR signal, whereas , , and are nuclides that do exhibit NMR spectra. The last two nuclei have spin \"S\" > 1/2 and are therefore quadrupolar nuclei.\n\nElectron spin resonance (ESR) is a related technique in which transitions between electronic rather than nuclear spin levels are detected. The basic principles are similar but the instrumentation, data analysis, and detailed theory are significantly different. Moreover, there is a much smaller number of molecules and materials with unpaired electron spins that exhibit ESR (or electron paramagnetic resonance (EPR)) absorption than those that have NMR absorption spectra. On the other hand, ESR has much higher signal per spin than NMR does.\n\nNuclear spin is an intrinsic angular momentum that is quantized. This means that the magnitude of this angular momentum is quantized (i.e. \"S\" can only take on a restricted range of values), and also that the x, y, and z-components of the angular momentum are quantized, being restricted to integer or half-integer multiples of \"ħ\". The integer or half-integer quantum number associated with the spin component along the z-axis or the applied magnetic field is known as the magnetic quantum number, \"m\", and can take values from +\"S\" to −\"S\", in integer steps. Hence for any given nucleus, there are a total of angular momentum states.\n\nThe \"z\"-component of the angular momentum vector (formula_2) is therefore , where \"ħ\" is the reduced Planck constant. The \"z\"-component of the magnetic moment is simply:\n\nConsider nuclei with a spin of one-half, like , or . Each nucleus has two linearly independent spin states, with \"m\" = or \"m\" = − (also referred to as spin-up and spin-down, or sometimes α and β spin states, respectively) for the z-component of spin. In the absence of a magnetic field, these states are degenerate; that is, they have the same energy. Hence the number of nuclei in these two states will be essentially equal at thermal equilibrium.\n\nIf a nucleus is placed in a magnetic field, however, the two states no longer have the same energy as a result of the interaction between the nuclear magnetic dipole moment and the external magnetic field. The energy of a magnetic dipole moment formula_3 in a magnetic field B is given by:\n\nUsually the \"z\"-axis is chosen to be along B, and the above expression reduces to:\nor alternatively:\n\nAs a result, the different nuclear spin states have different energies in a non-zero magnetic field. In less formal language, we can talk about the two spin states of a spin as being \"aligned\" either with or against the magnetic field. If \"γ\" is positive (true for most isotopes used in NMR) then is the lower energy state.\n\nThe energy difference between the two states is:\n\nand this results in a small population bias favoring the lower energy state in thermal equilibrium. With more spins pointing up than down, a net spin magnetization along the magnetic field B results.\n\nA central concept in NMR is the precession of the spin magnetization around the magnetic field at the nucleus, with the angular frequency\n\nformula_12\n\nwhere formula_13 relates to the oscillation frequency formula_14 and \"B\" is the magnitude of the field. This means that the spin magnetization, which is proportional to the sum of the spin vectors of nuclei in magnetically equivalent sites (the expectation value of the spin vector in quantum mechanics), moves on a cone around the B field. This is analogous to the precessional motion of the axis of a tilted spinning top around the gravitational field. In quantum mechanics, formula_15 is the \"Bohr frequency\" formula_16 of the formula_17 and formula_18 expectation values. Precession of non-equilibrium magnetization in the applied magnetic field B occurs with the Larmor frequency\n\nformula_19,\n\nwithout change in the populations of the energy levels because energy is constant (time-independent Hamiltonian).\n\nA perturbation of nuclear spin orientations from equilibrium will occur only when an oscillating magnetic field is applied whose frequency \"ν\" sufficiently closely matches the Larmor precession frequency \"ν\" of the nuclear magnetization. The populations of the spin-up and -down energy levels then undergo Rabi oscillations, which are analyzed most easily in terms of precession of the spin magnetization around the effective magnetic field in a reference frame rotating with the frequency \"ν\". The stronger the oscillating field, the faster the Rabi oscillations or the precession around the effective field in the rotating frame. After a certain time on the order of 2-1000 microseconds, a resonant RF pulse flips the spin magnetization to the transverse plane, i.e. it makes an angle of 90 with the constant magnetic field B (\"90 pulse\"), while after a twice longer time, the initial magnetization has been inverted (\"180 pulse\"). It is the transverse magnetization generated by a resonant oscillating field which is usually detected in NMR, during application of the relatively weak RF field in old-fashioned continuous-wave NMR, or after the relatively strong RF pulse in modern pulsed NMR. \n\nIt might appear from the above that all nuclei of the same nuclide (and hence the same \"γ\") would resonate at exactly the same frequency. This is not the case. The most important perturbation of the NMR frequency for applications of NMR is the \"shielding\" effect of the surrounding shells of electrons. Electrons, similar to the nucleus, are also charged and rotate with a spin to produce a magnetic field opposite to the applied magnetic field. In general, this electronic shielding reduces the magnetic field \"at the nucleus\" (which is what determines the NMR frequency). As a result, the frequency required to achieve resonance is also reduced. This shift in the NMR frequency due to the electronic molecular orbital coupling to the external magnetic field is called chemical shift, and it explains why NMR is able to probe the chemical structure of molecules, which depends on the electron density distribution in the corresponding molecular orbitals. If a nucleus in a specific chemical group is shielded to a higher degree by a higher electron density of its surrounding molecular orbital, then its NMR frequency will be shifted \"upfield\" (that is, a lower chemical shift), whereas if it is less shielded by such surrounding electron density, then its NMR frequency will be shifted \"downfield\" (that is, a higher chemical shift).\n\nUnless the local symmetry of such molecular orbitals is very high (leading to \"isotropic\" shift), the shielding effect will depend on the orientation of the molecule with respect to the external field (B). In solid-state NMR spectroscopy, magic angle spinning is required to average out this orientation dependence in order to obtain frequency values at the average or isotropic chemical shifts. This is unnecessary in conventional NMR investigations of molecules in solution, since rapid \"molecular tumbling\" averages out the chemical shift anisotropy (CSA). In this case, the \"average\" chemical shift (ACS) or isotropic chemical shift is often simply referred to as the chemical shift.\n\nThe process of population relaxation refers to nuclear spins that return to thermodynamic equilibrium in the magnet. This process is also called \"T\", \"spin-lattice\" or \"longitudinal magnetic\" relaxation, where \"T\" refers to the mean time for an individual nucleus to return to its thermal equilibrium state of the spins. After the nuclear spin population has relaxed, it can be probed again, since it is in the initial, equilibrium (mixed) state.\n\nThe precessing nuclei can also fall out of alignment with each other and gradually stop producing a signal. This is called \"T\" or \"transverse relaxation\". Because of the difference in the actual relaxation mechanisms involved (for example, intermolecular versus intramolecular magnetic dipole-dipole interactions ), \"T\" is usually (except in rare cases) longer than \"T\" (that is, slower spin-lattice relaxation, for example because of smaller dipole-dipole interaction effects). In practice, the value of \"T\"* which is the actually observed decay time of the observed NMR signal, or free induction decay (to of the initial amplitude immediately after the resonant RF pulse), also depends on the static magnetic field inhomogeneity, which is quite significant. (There is also a smaller but significant contribution to the observed FID shortening from the RF inhomogeneity of the resonant pulse). In the corresponding FT-NMR spectrum—meaning the Fourier transform of the free induction decay—the \"T\"* time is inversely related to the width of the NMR signal in frequency units. Thus, a nucleus with a long \"T\" relaxation time gives rise to a very sharp NMR peak in the FT-NMR spectrum for a very homogeneous (\"well-shimmed\") static magnetic field, whereas nuclei with shorter \"T\" values give rise to broad FT-NMR peaks even when the magnet is shimmed well. Both \"T\" and \"T\" depend on the rate of molecular motions as well as the gyromagnetic ratios of both the resonating and their strongly interacting, next-neighbor nuclei that are not at resonance.\n\nA Hahn echo decay experiment can be used to measure the dephasing time, as shown in the animation below. The size of the echo is recorded for different spacings of the two pulses. This reveals the decoherence which is not refocused by the 180° pulse. In simple cases, an exponential decay is measured which is described by the \"T\" time.\n\nNMR spectroscopy is one of the principal techniques used to obtain physical, chemical, electronic and structural information about molecules due to the chemical shift of the resonance frequencies of the nuclear spins in the sample. Peak splittings due to J- or dipolar couplings between nuclei are also useful. NMR spectroscopy can provide detailed and quantitative information on the functional groups, topology, dynamics and three-dimensional structure of molecules in solution and the solid state. Since the area under an NMR peak is usually proportional to the number of spins involved, peak integrals can be used to determine composition quantitatively. \n\nStructure and molecular dynamics can be studied (with or without \"magic angle\" spinning (MAS)) by NMR of quadrupolar nuclei (that is, with spin ) even in the presence of magnetic \"dipole-dipole\" interaction broadening (or simply, dipolar broadening) which is always much smaller than the quadrupolar interaction strength because it is a magnetic vs. an electric interaction effect.\n\nAdditional structural and chemical information may be obtained by performing double-quantum NMR experiments for pairs of spins or quadrupolar nuclei such as . Furthermore, nuclear magnetic resonance is one of the techniques that has been used to design quantum automata, and also build elementary quantum computers.\n\nIn the first few decades of nuclear magnetic resonance, spectrometers used a technique known as continuous-wave (CW) spectroscopy, where the transverse spin magnetization generated by a weak oscillating magnetic field is recorded as a function of the oscillation frequency or static field strength \"B\". When the oscillation frequency matches the nuclear resonance frequency, the transverse magnetization is maximized and a peak is observed in the spectrum. Although NMR spectra could be, and have been, obtained using a fixed constant magnetic field and sweeping the frequency of the oscillating magnetic field, it was more convenient to use a fixed frequency source and vary the current (and hence magnetic field) in an electromagnet to observe the resonant absorption signals. This is the origin of the counterintuitive, but still common, \"high field\" and \"low field\" terminology for low frequency and high frequency regions, respectively, of the NMR spectrum.\n\nAs of 1996, CW instruments were still used for routine work because the older instruments were cheaper to maintain and operate, often operating at 60 MHz with correspondingly weaker (non-superconducting) electromagnets cooled with water rather than liquid helium. One radio coil operated continuously, sweeping through a range of frequencies, while another orthogonal coil, designed not to receive radiation from the transmitter, received signals from nuclei that reoriented in solution. As of 2014, low-end refurbished 60 MHz and 90 MHz systems were sold as FT-NMR instruments, and in 2010 the \"average workhorse\" NMR instrument was configured for 300 MHz.\n\nCW spectroscopy is inefficient in comparison with Fourier analysis techniques (see below) since it probes the NMR response at individual frequencies or field strengths in succession. Since the NMR signal is intrinsically weak, the observed spectrum suffers from a poor signal-to-noise ratio. This can be mitigated by signal averaging, i.e. adding the spectra from repeated measurements. While the NMR signal is the same in each scan and so adds linearly, the random noise adds more slowly – proportional to the square root of the number of spectra (see random walk). Hence the overall signal-to-noise ratio increases as the square-root of the number of spectra measured.\n\nMost applications of NMR involve full NMR spectra, that is, the intensity of the NMR signal as a function of frequency. Early attempts to acquire the NMR spectrum more efficiently than simple CW methods involved illuminating the target simultaneously with more than one frequency. A revolution in NMR occurred when short radio-frequency pulses began to be used, with a frequency centered at the middle of the NMR spectrum. In simple terms, a short pulse of a given \"carrier\" frequency \"contains\" a range of frequencies centered about the carrier frequency, with the range of excitation (bandwidth) being inversely proportional to the pulse duration, i.e. the Fourier transform of a short pulse contains contributions from all the frequencies in the neighborhood of the principal frequency. The restricted range of the NMR frequencies made it relatively easy to use short (1 - 100 microsecond) radio frequency pulses to excite the entire NMR spectrum.\n\nApplying such a pulse to a set of nuclear spins simultaneously excites all the single-quantum NMR transitions. In terms of the net magnetization vector, this corresponds to tilting the magnetization vector away from its equilibrium position (aligned along the external magnetic field). The out-of-equilibrium magnetization vector then precesses about the external magnetic field vector at the NMR frequency of the spins. This oscillating magnetization vector induces a voltage in a nearby pickup coil, creating an electrical signal oscillating at the NMR frequency. This signal is known as the free induction decay (FID), and it contains the sum of the NMR responses from all the excited spins. In order to obtain the frequency-domain NMR spectrum (NMR absorption intensity vs. NMR frequency) this time-domain signal (intensity vs. time) must be Fourier transformed. Fortunately, the development of Fourier transform (FT) NMR coincided with the development of digital computers and the digital Fast Fourier Transform. Fourier methods can be applied to many types of spectroscopy. (See the full article on Fourier transform spectroscopy.)\n\nRichard R. Ernst was one of the pioneers of pulsed NMR and won a Nobel Prize in chemistry in 1991 for his work on Fourier Transform NMR and his development of multi-dimensional NMR spectroscopy.\n\nThe use of pulses of different durations, frequencies, or shapes in specifically designed patterns or \"pulse sequences\" allows the NMR spectroscopist to extract many different types of information about the molecules in the sample. In multi-dimensional nuclear magnetic resonance spectroscopy, there are at least two pulses and, as the experiment is repeated, the pulse timings are systematically varied and the oscillations of the spin system are probed point by point in the time domain. Multidimensional Fourier transformation of the multidimensional time signal yields the multidimensional spectrum. In two-dimensional nuclear magnetic resonance there will be one systematically varied time period in the sequence of pulses, which will modulate the intensity or phase of the detected signals. In 3D NMR, two time periods will be varied independently, and in 4D NMR, three will be varied. The remaining \"dimension\" is always provided by the directly detected signal.\n\nThere are many such experiments. In some, fixed time intervals allow (among other things) magnetization transfer between nuclei and, therefore, the detection of the kinds of nuclear-nuclear interactions that allowed for the magnetization transfer. Interactions that can be detected are usually classified into two kinds. There are \"through-bond\" and \"through-space\" interactions, the latter being a consequence of dipolar couplings in solid-state NMR and of the nuclear Overhauser effect in solution NMR. Experiments of the nuclear Overhauser variety may be employed to establish distances between atoms, as for example by 2D-FT NMR of molecules in solution.\n\nAlthough the fundamental concept of 2D-FT NMR was proposed by Jean Jeener from the Free University of Brussels at an international conference, this idea was largely developed by Richard Ernst who won the 1991 Nobel prize in Chemistry for his work in FT NMR, including multi-dimensional FT NMR, and especially 2D-FT NMR of small molecules. Multi-dimensional FT NMR experiments were then further developed into powerful methodologies for studying molecules in solution, in particular for the determination of the structure of biopolymers such as proteins or even small nucleic acids.\n\nIn 2002 Kurt Wüthrich shared the Nobel Prize in Chemistry (with John Bennett Fenn and Koichi Tanaka) for his work with protein FT NMR in solution.\n\nThis technique complements X-ray crystallography in that it is frequently applicable to molecules in an amorphous or liquid-crystalline state, whereas crystallography, as the name implies, is performed on molecules in a crystalline phase. In electronically conductive materials, the Knight shift of the resonance frequency can provide information on the mobile charge carriers. Though nuclear magnetic resonance is used to study the structure of solids, extensive atomic-level structural detail is more challenging to obtain in the solid state. Due to broadening by chemical shift anisotropy (CSA) and dipolar couplings to other nuclear spins, without special techniques such as MAS or dipolar decoupling by RF pulses, the observed spectrum is often only a broad Gaussian band for non-quadrupolar spins in a solid.\n\nProfessor Raymond Andrew at the University of Nottingham in the UK pioneered the development of high-resolution solid-state nuclear magnetic resonance. He was the first to report the introduction of the MAS (magic angle sample spinning; MASS) technique that allowed him to achieve spectral resolution in solids sufficient to distinguish between chemical groups with either different chemical shifts or distinct Knight shifts. In MASS, the sample is spun at several kilohertz around an axis that makes the so-called magic angle \"θ\" (which is ~54.74°, where 3cos\"θ\"-1 = 0) with respect to the direction of the static magnetic field B; as a result of such magic angle sample spinning, the broad chemical shift anisotropy bands are averaged to their corresponding average (isotropic) chemical shift values. Correct alignment of the sample rotation axis as close as possible to \"θ\" is essential for cancelling out the chemical-shift anisotropy broadening. There are different angles for the sample spinning relative to the applied field for the averaging of electric quadrupole interactions and paramagnetic interactions, correspondingly ~30.6° and ~70.1°. In amorphous materials, residual line broadening remains since each segment is in a slightly different environment, therefore exhibiting a slightly different NMR frequency.\n\nDipolar and J-couplings to nearby H nuclei are usually removed by radio-frequency pulses applied at the H frequency during signal detection. The concept of cross polarization developed by Sven Hartmann and Erwin Hahn was utilized in transferring magnetization from protons to less sensitive nuclei by M.G. Gibby, Alex Pines and John S. Waugh. Then, Jake Schaefer and Ed Stejskal demonstrated the powerful use of cross polarization under MAS conditions (CP-MAS) and proton decoupling, which is now routinely employed to measure high resolution spectra of low-abundance and low-sensitivity nuclei, such as carbon-13, silicon-29, or nitrogen-15, in solids. Significant further signal enhancement can be achieved by dynamic nuclear polarization from unpaired electrons to the nuclei, usually at temperatures near 110 K.\n\nBecause the intensity of nuclear magnetic resonance signals and, hence, the sensitivity of the technique depends on the strength of the magnetic field the technique has also advanced over the decades with the development of more powerful magnets. Advances made in audio-visual technology have also improved the signal-generation and processing capabilities of newer instruments.\n\nAs noted above, the sensitivity of nuclear magnetic resonance signals is also dependent on the presence of a magnetically susceptible nuclide and, therefore, either on the natural abundance of such nuclides or on the ability of the experimentalist to artificially enrich the molecules, under study, with such nuclides. The most abundant naturally occurring isotopes of hydrogen and phosphorus (for example) are both magnetically susceptible and readily useful for nuclear magnetic resonance spectroscopy. In contrast, carbon and nitrogen have useful isotopes but which occur only in very low natural abundance.\n\nOther limitations on sensitivity arise from the quantum-mechanical nature of the phenomenon. For quantum states separated by energy equivalent to radio frequencies, thermal energy from the environment causes the populations of the states to be close to equal. Since incoming radiation is equally likely to cause stimulated emission (a transition from the upper to the lower state) as absorption, the NMR effect depends on an excess of nuclei in the lower states. Several factors can reduce sensitivity, including:\n\nMany isotopes of chemical elements can be used for NMR analysis.\n\nCommonly used nuclei:\n\nOther nuclei (usually used in the studies of their complexes and chemical bonding, or to detect presence of the element):\n\nNMR is extensively used in medicine in the form of magnetic resonance imaging. NMR is used industrially mainly for routine analysis of chemicals. The technique is also used, for example, to measure the ratio between water and fat in foods, monitor the flow of corrosive fluids in pipes, or to study molecular structures such as catalysts.\n\nThe application of nuclear magnetic resonance best known to the general public is magnetic resonance imaging for medical diagnosis and magnetic resonance microscopy in research settings. However, it is also widely used in biochemical studies, notably in NMR spectroscopy such as proton NMR, carbon-13 NMR, deuterium NMR and phosphorus-31 NMR. Biochemical information can also be obtained from living tissue (e.g. human brain tumors) with the technique known as in vivo magnetic resonance spectroscopy or chemical shift NMR microscopy.\n\nThese spectroscopic studies are possible because nuclei are surrounded by orbiting electrons, which are charged particles that generate small, local magnetic fields that add to or subtract from the external magnetic field, and so will partially shield the nuclei. The amount of shielding depends on the exact local environment. For example, a hydrogen bonded to an oxygen will be shielded differently from a hydrogen bonded to a carbon atom. In addition, two hydrogen nuclei can interact via a process known as spin-spin coupling, if they are on the same molecule, which will split the lines of the spectra in a recognizable way.\n\nAs one of the two major spectroscopic techniques used in metabolomics, NMR is used to generate metabolic fingerprints from biological fluids to obtain information about disease states or toxic insults.\n\nBy studying the peaks of nuclear magnetic resonance spectra, chemists can determine the structure of many compounds. It can be a very selective technique, distinguishing among many atoms within a molecule or collection of molecules of the same type but which differ only in terms of their local chemical environment. NMR spectroscopy is used to unambiguously identify known and novel compounds, and as such, is usually required by scientific journals for identity confirmation of synthesized new compounds. See the articles on carbon-13 NMR and proton NMR for detailed discussions.\n\nA chemist can determine the identity of a compound by comparing the observed nuclear precession frequencies to known frequencies. Further structural data can be elucidated by observing \"spin-spin coupling\", a process by which the precession frequency of a nucleus can be influenced by the spin orientation of a chemically bonded nucleus. Spin-spin coupling is easily observed in NMR of hydrogen-1 ( NMR) since its natural abundance is nearly 100%.\n\nBecause the nuclear magnetic resonance \"timescale\" is rather slow, compared to other spectroscopic methods, changing the temperature of a \"T\"* experiment can also give information about fast reactions, such as the Cope rearrangement or about structural dynamics, such as ring-flipping in cyclohexane. At low enough temperatures, a distinction can be made between the axial and equatorial hydrogens in cyclohexane.\n\nAn example of nuclear magnetic resonance being used in the determination of a structure is that of buckminsterfullerene (often called \"buckyballs\", composition C). This now famous form of carbon has 60 carbon atoms forming a sphere. The carbon atoms are all in identical environments and so should see the same internal H field. Unfortunately, buckminsterfullerene contains no hydrogen and so nuclear magnetic resonance has to be used. spectra require longer acquisition times since carbon-13 is not the common isotope of carbon (unlike hydrogen, where is the common isotope). However, in 1990 the spectrum was obtained by R. Taylor and co-workers at the University of Sussex and was found to contain a single peak, confirming the unusual structure of buckminsterfullerene.\n\nWhile NMR is primarily used for structural determination, it can also be used for purity determination, provided that the structure and molecular weight of the compound is known. This technique requires the use of an internal standard of known purity. Typically this standard will have a high molecular weight to facilitate accurate weighing, but relatively few protons so as to give a clear peak for later integration e.g. 1,2,4,5-tetrachloro-3-nitrobenzene. Accurately weighed portions of the standard and sample are combined and analysed by NMR. Suitable peaks from both compounds are selected and the purity of the sample is determined via the following equation.\n\nWhere:\n\nNuclear magnetic resonance is extremely useful for analyzing samples non-destructively. Radio-frequency magnetic fields easily penetrate many types of matter and anything that is not highly conductive or inherently ferromagnetic. For example, various expensive biological samples, such as nucleic acids, including RNA and DNA, or proteins, can be studied using nuclear magnetic resonance for weeks or months before using destructive biochemical experiments. This also makes nuclear magnetic resonance a good choice for analyzing dangerous samples. \n\nIn addition to providing static information on molecules by determining their 3D structures, one of the remarkable advantages of NMR over X-ray crystallography is that it can be used to obtain important dynamic information. This is due to the orientation dependence of the chemical-shift, dipole-coupling, or electric-quadrupole-coupling contributions to the instantaneous NMR frequency in an anisotropic molecular environment. When the molecule or segment containing the NMR-observed nucleus changes its orientation relative to the external field, the NMR frequency changes, which can result in changes in one- or two-dimensional spectra or in the relaxation times, depending of the correlation time and amplitude of the motion. \n\nAnother use for nuclear magnetic resonance is data acquisition in the petroleum industry for petroleum and natural gas exploration and recovery. Initial research in this domain began in the 1950s, however, the first commercial instruments were not released until the early 1990s. A borehole is drilled into rock and sedimentary strata into which nuclear magnetic resonance logging equipment is lowered. Nuclear magnetic resonance analysis of these boreholes is used to measure rock porosity, estimate permeability from pore size distribution and identify pore fluids (water, oil and gas). These instruments are typically low field NMR spectrometers.\n\nNMR logging, a subcategory of electromagnetic logging, measures the induced magnet moment of hydrogen nuclei (protons) contained within the fluid-filled pore space of porous media (reservoir rocks). Unlike conventional logging measurements (e.g., acoustic, density, neutron, and resistivity), which respond to both the rock matrix and fluid properties and are strongly dependent on mineralogy, NMR-logging measurements respond to the presence of hydrogen. Because hydrogen atoms primarily occur in pore fluids, NMR effectively responds to the volume, composition, viscosity, and distribution of these fluids, for example oil, gas or water. NMR logs provide information about the quantities of fluids present, the properties of these fluids, and the sizes of the pores containing these fluids. From this information, it is possible to infer or estimate:\n\n\nThe basic core and log measurement is the \"T\" decay, presented as a distribution of \"T\" amplitudes versus time at each sample depth, typically from 0.3 ms to 3 s. The \"T\" decay is further processed to give the total pore volume (the total porosity) and pore volumes within different ranges of \"T\". The most common volumes are the bound fluid and free fluid. A permeability estimate is made using a transform such as the Timur-Coates or SDR permeability transforms. By running the log with different acquisition parameters, direct hydrocarbon typing and enhanced diffusion are possible.\n\nRecently, real-time applications of NMR in liquid media have been developed using specifically designed flow probes (flow cell assemblies) which can replace standard tube probes. This has enabled techniques that can incorporate the use of high performance liquid chromatography (HPLC) or other continuous flow sample introduction devices.\n\nNMR has now entered the arena of real-time process control and process optimization in oil refineries and petrochemical plants. Two different types of NMR analysis are utilized to provide real time analysis of feeds and products in order to control and optimize unit operations. Time-domain NMR (TD-NMR) spectrometers operating at low field (2–20 MHz for ) yield free induction decay data that can be used to determine absolute hydrogen content values, rheological information, and component composition. These spectrometers are used in mining, polymer production, cosmetics and food manufacturing as well as coal analysis. High resolution FT-NMR spectrometers operating in the 60 MHz range with shielded permanent magnet systems yield high resolution NMR spectra of refinery and petrochemical streams. The variation observed in these spectra with changing physical and chemical properties is modeled using chemometrics to yield predictions on unknown samples. The prediction results are provided to control systems via analogue or digital outputs from the spectrometer.\n\nIn the Earth's magnetic field, NMR frequencies are in the audio frequency range, or the very low frequency and ultra low frequency bands of the radio frequency spectrum. Earth's field NMR (EFNMR) is typically stimulated by applying a relatively strong dc magnetic field pulse to the sample and, after the end of the pulse, analyzing the resulting low frequency alternating magnetic field that occurs in the Earth's magnetic field due to free induction decay (FID). These effects are exploited in some types of magnetometers, EFNMR spectrometers, and MRI imagers. Their inexpensive portable nature makes these instruments valuable for field use and for teaching the principles of NMR and MRI.\n\nAn important feature of EFNMR spectrometry compared with high-field NMR is that some aspects of molecular structure can be observed more clearly at low fields and low frequencies, whereas other aspects observable at high fields are not observable at low fields. This is because:\n\nIn zero field NMR all magnetic fields are shielded such that magnetic fields below 1 nT (nanotesla) are achieved and the nuclear precession frequencies of all nuclei are close to zero and indistinguishable. Under those circumstances the observed spectra are no-longer dictated by chemical shifts but primarily by \"J\"-coupling interactions which are independent of the external magnetic field. Since inductive detection schemes are not sensitive at very low frequencies, on the order of the \"J\"-couplings (typically between 0 and 1000 Hz), alternative detection schemes are used. Specifically, sensitive magnetometers turn out to be good detectors for zero field NMR. A zero magnetic field environment does not provide any polarization hence it is the combination of zero field NMR with hyperpolarization schemes that makes zero field NMR attractive.\n\nNMR quantum computing uses the spin states of nuclei within molecules as qubits. NMR differs from other implementations of quantum computers in that it uses an ensemble of systems, in this case molecules.\n\nVarious magnetometers use NMR effects to measure magnetic fields, including proton precession magnetometers (PPM) (also known as proton magnetometers), and Overhauser magnetometers. See also Earth's field NMR.\n\nSurface magnetic resonance (or magnetic resonance sounding) is based on the principle of Nuclear magnetic resonance (NMR) and measurements can be used to indirectly estimate the water content of saturated and unsaturated zones in the earth's subsurface. SNMR is used to estimate aquifer properties, including quantity of water contained in the aquifer, Porosity,and Hydraulic conductivity .\n\nMajor NMR instrument makers include Thermo Fisher Scientific, Magritek, Oxford Instruments, Bruker, Spinlock SRL, General Electric, JEOL, Kimble Chase, Philips, Siemens AG, and formerly Agilent Technologies, Inc. (who own Varian, Inc.).\n\n\n\n\n\n"}
{"id": "24951890", "url": "https://en.wikipedia.org/wiki?curid=24951890", "title": "Oceanic zone", "text": "Oceanic zone\n\nThe oceanic zone is typically defined as the area of the ocean lying beyond the continental shelf, but operationally is often referred to as beginning where the water depths drop to below 200 meters (656  feet), seaward from the coast to the open ocean.\n\nIt is the region of open sea beyond the edge of the continental shelf and includes 65% of the ocean’s completely open water. The oceanic zone has a wide array of undersea terrain, including crevices that are often deeper than Mount Everest is tall, as well as deep-sea volcanoes and ocean basins. While it is often difficult for life to sustain itself in this type of environment, some species to thrive in the oceanic zone.\n\nThe oceanic zone is subdivided into the epipelagic, mesopelagic, and bathypelagic zones.\n\nThe mesopelagic (disphotic) zone, where only small amounts of light penetrate, lies below the epipelagic zone. This zone is often referred to as the Twilight Zone due to its scarce amount of light. Temperatures in the mesopelagic zone range from . The pressure is higher here, it can be up to and increases with depth.\n\n54% of the ocean lies in the bathypelagic (aphotic) zone into which no light penetrates. This is also called the midnight zone and the deep ocean. Due to the complete lack of sunlight, photosynthesis cannot occur and the only light source is bioluminescence. Water pressure is very intense and the temperatures are near freezing (range ).\n\nOceanographers have divided the ocean into zones based on how far light reaches. All of the light zones can be found in the oceanic zone. The epipelagic zone is the one closest to the surface and is the best lit. It extends to 200 meters and contains both phytoplankton and zooplankton that can support larger organisms like marine mammals and some types of fish. Past 200 meters, not enough light penetrates the water to support life, and no plant life exists.\n\nThere are creatures, however, which thrive around hydrothermal vents, or geysers located on the ocean floor that expel superheated water that is rich in minerals. These organisms feed off of chemosynthetic bacteria, which use the superheated water and chemicals from the hydrothermal vents to create energy in place of photosynthesis. The existence of these bacteria allow creatures like squids, hatchet fish, octopuses, tube worms, giant clams, spider crabs and other organisms to survive.\n\nDue to the total darkness in the zones past the epipelagic zone, many organisms that survive in the deep oceans do not have eyes, and other organisms make their own light with bioluminescence. Often the light is blue-green in color, because many marine organisms are sensitive to blue light. Two chemicals, luciferin, and luciferase that react with one another to create a soft glow. The process by which bioluminescence is created is very similar to what happens when a glow stick is broken. Deep-sea organisms use bioluminescence for everything from luring prey to navigation.\n\nAnimals such as fish, whales, and sharks are found in the oceanic zone.\n"}
{"id": "51521629", "url": "https://en.wikipedia.org/wiki?curid=51521629", "title": "Paolo Panceri", "text": "Paolo Panceri\n\nPaolo Panceri (Milan, 1833 – Naples, 1877) was an Italian naturalist.\nPanceri graduated in medicine at the University of Pavia where he began his research. In 1861 he took\nthe Chair of Comparative anatomy at the University of Naples, where he directed the Zoology Museum.\nPanceri was cautious about the scientific validity of evolutionary theories but was instrumental in the foundation of the Stazione Zoologica Anton Dohrn (Dohrn was a Darwinian).His findings on the bioluminescence of marine invertebrates and studies of \"Amphioxus\" led to fame in Italy and abroad. In 1874 he sold his books and scientific papers to Biblioteca Universitaria di Napoli to pay for an expedition to Egypt. They constitute an example of a nineteenth-century library specializing in the natural sciences and comparative anatomy.\nHis students in Naples include Carlo Emery, Leopoldo Maggi and Antonio della Valle.He died aged 44.\n\n\n\n"}
{"id": "1038273", "url": "https://en.wikipedia.org/wiki?curid=1038273", "title": "Planetary engineering", "text": "Planetary engineering\n\nPlanetary engineering is the application of technology for the purpose of influencing the global environments of a planet. Its objectives usually involve increasing the habitability of other worlds or mitigating decreases in habitability to Earth.\n\nPerhaps the best-known type of planetary engineering is terraforming, by which a planet's surface conditions are altered to be more like those of Earth. Planetary engineering is largely the realm of science fiction at present, although recent climate change on Earth shows that human technology can cause change on a global scale.\n\nTerraforming is the hypothetical process of deliberately modifying the atmosphere, temperature, or environment of a planet, moon, or other body to be similar to those of Earth in order to make it habitable by humans.\n\nSeeding is a term used for the process of introducing microbial or algae species on a planet or moon already offering habitable regions. Jupiter's moon Europa is a good example as it has a well protected ocean under a thick ice crust. Living on the surface of the ice crust is not possible, but some microorganisms from Earth could possibly survive in the oxygen rich environment of Europa's subsurface ocean. In the case of Europa it might be possible to incrementally seed with the species in their trophic (see foodchain) context and to build up an aquatic ecosystem.\n\nGeoengineering is the application of planetary engineering techniques to Earth. Recent geoengineering proposals have principally been methods to tackle human-induced climate change by either removing carbon dioxide from the atmosphere (e.g. using ocean iron fertilization) or by managing solar radiation (e.g. by using mirrors in space) in order to negate the net warming effect of climate change. Future geoengineering projects may preserve the habitability of Earth through the sun's life cycle by moving the Earth to keep it constantly within the habitable zone.\n\nPlanetary engineering can also be applied to keep habitability of Earth in the far future. The rate of weathering of silicate minerals will increase as rising temperatures speed up chemical processes. This in turn will decrease the level of carbon dioxide in the atmosphere, as these weathering processes convert carbon dioxide gas into solid carbonates. Within the next 600 million years from the present, the concentration of CO2 will fall below the critical threshold needed to sustain C3 photosynthesis: about 50 parts per million. At this point, trees and forests in their current forms will no longer be able to survive. Unlike other animal species, humankind has the potential ability to prolong the habitability of the planet Earth by extracting and re-injecting inert carbon trapped underground back into the biosphere.\n\n\n"}
{"id": "26667808", "url": "https://en.wikipedia.org/wiki?curid=26667808", "title": "Principle of least astonishment", "text": "Principle of least astonishment\n\nThe principle of least astonishment (POLA) (and variations of \"\"principle\"/\"law\"/\"rule\" of least \"astonishment\"/\"surprise\"\") applies to user interface and software design. A typical formulation of the principle, from 1984, is: \"If a necessary feature has a high astonishment factor, it may be necessary to redesign the feature.\"\n\nMore generally, the principle means that a component of a system should behave in a way that most users will expect it to behave; the behavior should not astonish or surprise users.\n\nA textbook formulation is: \"People are part of the system. The design should match the user's experience, expectations, and mental models.\"\n\nThe choice of \"least surprising\" behavior can depend on the expected audience (for example, end users, programmers, or system administrators).\n\nIn more practical terms, the principle aims to leverage the pre-existing knowledge of users to minimize the learning curve, for instance by designing interfaces that borrow heavily from \"functionally similar or analogous programs with which your users are likely to be familiar\". User expectations in this respect may be closely related to a particular computing platform or tradition. For example, Unix command line programs are expected to follow certain conventions with respect to switches, and widgets of Microsoft Windows programs are expected to follow certain conventions with respect to keyboard shortcuts. In more abstract settings like an API, the expectation that function or method names intuitively match their behavior is another example. This practice also involves the application of sensible defaults.\n\nWhen two elements of an interface conflict, or are ambiguous, the behavior should be that which will least surprise the user; in particular a programmer should try to think of the behavior that will least surprise someone who uses the program, rather than that behavior that is natural from knowing the inner workings of the program.\n\nA web site could declare an input that should autofocus when the page is loaded, such as a search field (e.g., Google.com), or the \"username\" field of a login form. Sites offering keyboard shortcuts often allow pressing to see the available shortcuts. Examples include Gmail and Jira.\n\nThe Function key in Windows operating systems is almost always for opening a help program associated with an application, and similarly for some of the Linux desktop environments. The corresponding key combination in Mac OS X is . Users expect a help screen or similar help services popup when they press this key. Software binding this key to some other feature is likely to cause astonishment if no help appears. Malicious programs are known to exploit users' familiarity with regular shortcut keys.\n\nIn programming, a good example of this principle is the common codice_1 function which exists in most languages and is used to convert a string to an integer value. The radix is usually an optional argument and assumed to be 10 (representing base 10). Other bases are usually supported (like binary or octal) but only when specified explicitly; when the radix argument is not specified, base 10 is assumed. Notably JavaScript did not originally adopt this behavior, which resulted in developer confusion and software bugs.\n\n\n"}
{"id": "30979854", "url": "https://en.wikipedia.org/wiki?curid=30979854", "title": "Pseudospectral knotting method", "text": "Pseudospectral knotting method\n\nIn applied mathematics, the pseudospectral knotting method is a generalization and enhancement of a standard pseudospectral method for optimal control. The concept was introduced by I. Michael Ross and F. Fahroo in 2004, and forms part of the collection of the Ross–Fahroo pseudospectral methods.\n\nAccording to Ross and Fahroo a pseudospectral (PS) knot is a double Lobatto point; i.e. two boundary points on top of one another. At this point, information (such as discontinuities, jumps, dimension changes etc.) is exchanged between two standard PS methods. This information exchange is used to solve some of the most difficult problems in optimal control known as hybrid optimal control problems.\n\nIn a hybrid optimal control problem, an optimal control problem is intertwined with a graph problem. A standard pseudospectral optimal control method is incapable of solving such problems; however, through the use of pseudospectral knots, the information of the graph can be encoded at the double Lobatto points thereby allowing a hybrid optimal control problem to be discretized and solved using powerful software such as DIDO.\n\nPS knots have found applications in aerospace problems such as the ascent guidance of a launch vehicles, and advancing the Aldrin Cycler through the use of solar sails. \n\nPS knots have also been used for anti-aliasing of PS optimal control solutions and for capturing critical information in switches in solving bang-bang-type optimal control problems.\n\nThe PS knotting method was first implemented in the MATLAB optimal control software package, DIDO.\n\n"}
{"id": "27942343", "url": "https://en.wikipedia.org/wiki?curid=27942343", "title": "Psychological effects of Internet use", "text": "Psychological effects of Internet use\n\nVarious researchers have undertaken efforts to examine the psychological effects of Internet use. Some research employs studying brain functions in Internet users. Some studies assert that these changes are harmful, while others argue that asserted changes are beneficial.\n\nAmerican writer Nicholas Carr asserts that Internet use reduces the deep thinking that leads to true creativity. He also says that hyperlinks and overstimulation means that the brain must give most of its attention to short-term decisions. Carr also states that the vast availability of information on the World Wide Web overwhelms the brain and hurts long-term memory. He says that the availability of stimuli leads to a very large cognitive load, which makes it difficult to remember anything.\nComputer scientist Ramesh Sitaraman has asserted that Internet users are impatient and are likely to get more impatient with time. In a large-scale research study that completed in 2012 involving millions of users watching videos on the Internet, Krishnan and Sitaraman show that users start to abandon online videos if they do not start playing within two seconds. In addition, users with faster Internet connections (such as FTTH) showed less patience and abandoned videos at a faster rate than users with slower Internet connections. Many commentators have since argued that these results provide a glimpse into the future: as Internet services become faster and provide more instant gratification, people become less patient and less able to delay gratification and work towards longer-term rewards.\n\nPsychologist Steven Pinker, however, argues that people have control over what they do, and that research and reasoning never came naturally to people. He says that \"experience does not revamp the basic information-processing capacities of the brain\" and asserts that the Internet is actually making people smarter.\n\nThe BBC describes the research published in the peer-reviewed science journal \"PLoS ONE\":\n\nSpecialised MRI brain scans showed changes in the white matter of the brain—the part that contains nerve fibres—in those classed as being web addicts, compared with non-addicts. Furthermore, the study says, \"We provided evidences demonstrating the multiple structural changes of the brain in IAD subjects. VBM results indicated the decreased gray matter volume in the bilateral dorsolateral prefrontal cortex (DLPFC), the supplementary motor area (SMA), the orbitofrontal cortex (OFC), the cerebellum and the left rostral ACC (rACC).\"\nUCLA professor of psychiatry Gary Small studied brain activity in experienced web surfers versus casual web surfers. He used MRI scans on both groups to evaluate brain activity. The study showed that when Internet surfing, the brain activity of the experienced Internet users was far more extensive than that of the novices, particularly in areas of the prefrontal cortex associated with problem-solving and decision making. However, the two groups had no significant differences in brain activity when reading blocks of text. This evidence suggested that the distinctive neural pathways of experienced Web users had developed because of their Web use. Dr. Small concluded that “The current explosion of digital technology not only is changing the way we live and communicate, but is rapidly and profoundly altering our brains.” \n\nIn an August 2008 article in \"The Atlantic\" (\"Is Google Making Us Stupid?\"), Nicholas Carr experientially asserts that using the Internet can lead to lower attention span and make it more difficult to read in the traditional sense (that is, read a book at length without mental interruptions). He says that he and his friends have found it more difficult to concentrate and read whole books, even though they read a great deal when they were younger (that is, when they did not have access to the Internet). This assertion is based on anecdotal evidence, not controlled research.\n\nResearchers from the University College London have done a 5-year study on Internet habits, and have found that people using the sites exhibited “a form of skimming activity,” hopping from one source to another and rarely returning to any source they’d already visited. The 2008 report says, \"It is clear that users are not reading online in the traditional sense; indeed there are signs that new forms of “reading” are emerging as users “power browse” horizontally through titles, contents pages and abstracts going for quick wins. It almost seems that they go online to avoid reading in the traditional sense.\"\n\nResearch suggests that using the Internet helps boost brain power for middle-aged and older people (research on younger people has not been done). The study compares brain activity when the subjects were reading and when the subjects were surfing the Internet. It found that Internet surfing uses much more brain activity than reading does. Lead researcher Professor Gary Small said: \"The study results are encouraging, that emerging computerized technologies may have physiological effects and potential benefits for middle-aged and older adults. Internet searching engages complicated brain activity, which may help exercise and improve brain function.\"\n\nOne of the most widely debated effects of social networking has been its influence on productivity. In many schools and workplaces, social media sites are blocked because employers believe their employees will be distracted and unfocused on the sites. It seems, at least from one study, that employers do, indeed, have reason to be concerned. A survey from Hearst Communications found that productivity levels of people that used social networking sites were 1.5% lower than those that did not. Logically, people cannot get work done when they are performing other tasks. If the employees suffer from degrading self-control, it will be even harder for them to get back to work and maintain productivity.\n\nEvgeny Morozov has said that social networking could be potentially harmful to people. He writes that they can destroy privacy, and notes that \"Insurance companies have accessed their patients’ Facebook accounts to try to disprove they have hard-to-verify health problems like depression; employers have checked social networking sites to vet future employees; university authorities have searched the web for photos of their students’ drinking or smoking pot.\" He also said that the Internet also makes people more complacent and risk averse. He said that because much of the ubiquity of modern technology—cameras, recorders, and such—people may not want to act in unusual ways for fear of getting a bad name. People can see pictures and videos of you on the Internet, and this may make you act differently.\n\nAccording to the New York Times, many scientists say that \"people's ability to focus is being undermined by bursts of information\".\n\nFrom 53,573 page views taken from various users, 17% of the views lasted less than 4 seconds while 4% lasted more than 10 minutes. In regards to page content, users will only read 49% of a site that contains 111 words or fewer while users will opt to read 28% of an average website (approximately 593 words). For each additional 100 words on a site, users will spend 4.4 seconds longer on the site.\n\nIt is found that those who read articles online go through the article more thoroughly than those who read from print-based materials. Upon choosing their reading material, online readers read 77% of the content, which can be compared to broadsheet newspaper where the corresponding number is 62%.\n\nInteracting on the Internet mostly does not involve \"physical\" interactions with another person (i.e. face-to-face conversation), and therefore easily leads to a person feeling free to act differently online, as well as unrestraint in civility and minimization of authority, etc.\n\nPeople who are socially anxious are more likely to use electronic communication as their only means of communication. This, in turn, makes them more likely to disclose personal information to strangers online that they normally wouldn't give out face-to-face. The phenomenon is a likely cause for the prevalence of cyberbullying, especially for children who do not understand \"social networking etiquette.\"\n\nInternet anonymity can lead to online disinhibition, in which people do and say things online that they normally wouldn't do or say in person. Psychology researcher John Suler differentiates between \"benign disinhibition\" in which people can grow psychologically by revealing secret emotions, fears, and wishes and showing unusual acts of kindness and generosity and \"toxic disinhibition\", in which people use rude language, harsh criticisms, anger, hatred and threats or visit pornographic or violent sites that they wouldn't in the 'real world.' \n\nPeople become addicted or dependent on the Internet through excessive computer use that interferes with daily life. Kimberly S. Young links internet addiction disorder with existing mental health issues, most commonly depression. Young states that the disorder has significant effects socially, psychologically and occupationally.\n\n\"Aric Sigman’s presentation to members of the Royal College of Paediatrics and Child Health outlined the parallels between screen dependency and alcohol and drug addiction: the instant stimulation provided by all those flickering graphics leads to the release of dopamine, a chemical that’s central to the brain’s reward system\".\n\nA 2009 study suggested that brain structural changes were present in those classified by the researchers as Internet addicted, similar to those classified as chemically addicted.\n\nIn one study, the researchers selected seventeen subjects with online gaming addiction and another seventeen naive internet users who rarely used the internet. Using a magnetic resonance imaging scanner, they performed a scan to \"acquire 3-dimensional T1-weighted images\" of the subject's brain. The results of the scan revealed that online gaming addiction \"impairs gray and white matter integrity in the orbitofrontal cortex of the prefrontal regions of the brain\". According to Keath Low, psychotherapist, the orbitofrontal cortex \"has a major impact on our ability to perform such tasks as planning, prioritizing, paying attention to and remembering details, and controlling our attention\". As a result, Keith Low believes that these online gaming addicts are incapable of prioritizing their life or setting a goal and accomplishing it because of the impairment of their orbitofrontal cortex.\n\nEase of access to the Internet can increase escapism in which a user uses the Internet as an \"escape\" from the perceived unpleasant or banal aspects of daily/real life. Because the internet and virtual realities easily satisfy social needs and drives, according to Jim Blascovich and Jeremy Bailensen, \"sometimes [they are] so satisfying that addicted users will withdraw physically from society.” Stanford psychiatrist Dr. Elias Aboujaoude believes that advances in virtual reality and immersive 3-D have led us to “where we can have a ‘full life’ [online] that can be quite removed from our own.” Eventually, virtual reality may drastically change a person’s social and emotional needs. “We may stop ‘needing’ or craving real social interactions because they may become foreign to us,” Aboujaoude says.\n\nInternet has its impact on all age groups from elders to children. According to the article 'Digital power: exploring the effects of social media on children’s spirituality', children consider the Internet as their third place after home and school.\n\nOne of the main effects social media has had on children is the effect of cyber bullying. A study carried out by 177 students in Canada found that “15% of the students admitted that they cyberbullied others” while “40% of the cyber victims had no idea who the bullies were”. The psychological harm cyber bullying can cause is reflected in low self-esteem, depression and anxiety. It also opens up avenues for manipulation and control. Cyber bullying has ultimately led to depression, anxiety and in severe cases suicide. Suicide is the third leading cause of death for youth between the ages of 10 and 24. Cyber bullying is rapidly increasing. Some writers have suggested monitoring and educating children from a young age about the risks associated with cyber bullying. Not only does cyberbullying effect children but a huge issue is insomnia. This is because of the amount of time spent using the internet. children use on average 27 hours of internet a week and it is on the increase. \n\n\"A psychologist, Aric Sigman, warned of the perils of “passive parenting” and “benign neglect” caused by parent's reliance on gadgets\". In some cases, parents' internet addictions can have drastic effects on their children. In 2009, a three-year-old girl from New Mexico died of malnutrition and dehydration on the same day that her mother was said to have spent 15 hours playing World of Warcraft online. In another case in 2014, a Korean couple became so immersed in a video game that allowed them to raise a virtual child online that they let their real baby die.\nThe effects of the Internet on parenting can be observed a how parents utilize the Internet, the response to their child's Internet consumption, as well as the effects and influences that the Internet has on the relationship between parent and child.\n\nOverall, parents are seen to do simple tasks such as sending e-mails and keep up with current events whereas social networking sites are less frequented. In regards to researching parental material, a study conducted in January 2012 by the University of Minnesota found that 75% of questioned parents have stated that the Internet improves their method of obtaining parenting related information, 19.7% found parenting websites too complex to navigate, and 13.1% of the group did not find any useful parenting information on any website.\n\nMany studies have shown that parents view the Internet as a hub of information especially in their children's education. They feel that it is a valuable commodity that can enhance their learning experience and when used in this manner it does not contribute to any family tension or conflicts. However, when the Internet is used as a social medium (either online gaming or social networking sites) there is a positive correlation between the use of the Internet and family conflicts. In conjunction with using the Internet for social means, there is a risk of exposing familial information to strangers, which is perceived to parents as a threat and can ultimately weaken family boundaries.\n\nA report released in October 2012 by Ofcom focused on the amount of online consumption done by children aged 5–15 and how the parents react to their child’s consumption. Of the parents interviewed, 85% use a form of online mediation ranging from face-to-face talks with their children about online surfing to cellphone browser filters. The remaining 15% of parents do not take active measures to adequately inform their children of safe Internet browsing; these parents have either spoken only briefly to their children about cautious surfing or do not do anything at all.\n\nParents are active in monitoring their child’s online use by using methods such as investigating the browsing history and by regulating Internet usage. However, since parents are less versed in Internet usage than their children they are more concerned with the Internet interfering with family life than online matters such as child grooming or cyber-bullying.\n\nWhen addressing those with lack of parental control over the Internet, parents state that their child is rarely alone (defined for children from 5–11 years old) or that they trust their children when they are online (for children 12–15 years old). Approximately 80% of parents ensure that their child has been taught Internet safety from school and 70% of parents feel that the benefits of using the Internet are greater than the risks that come along with it.\n\nConversely an American study, conducted by PewInternet released on November 20, 2012, reveal that parents are highly concerned about the problems the Internet can impose on their teenage children. 47% of parents are tend to worry about their children being exposed to inappropriate material on the Internet and 45% of the parents are concerned about their children’s behaviour towards each other both online offline. Only 31% of parents showed concern about the Internet taking away social time from the family.\n\nResearcher Sanford Grossbart and others explores the relationship between the mother and child and how Internet use affects this relationship. This study forms its basis around Marvin Sussman and Suzanne Steinmetz’s idea that the relationship between parent and child is highly influenced by the changing experiences and events of each generation. “Parental warmth” is a factor in how receptive a parent is to being taught the nuances of the Internet by their child versus the traditional method of the parent influencing the child. If the parent displayed “warm” tendencies she was more open to learning how to use the Internet from their child even if the parent happened to be more knowledgeable on the subject. This fosters teaching in a positive environment, which sustains a strong relationship between mother and child, encourages education, and promotes mature behaviour. “Cooler” mothers only allowed themselves to be taught if they thought that their child held the same amount of knowledge or greater and would dismiss the teaching otherwise suggesting a relationship that stems from the majority of influence coming from the parent.\n\nHowever, despite \"warm\" and \"cool\" parenting methods, parents who encounter a language barrier rely more heavily on their children to utilize the Internet. Vikki Katz of Rutgers University has studied the interaction between immigrant parents and children and how they use technology. Katz notes that the majority resources that immigrants find helpful are located online, however the search algorithms currently in place do not direct languages other than English appropriately. Because of this shortcoming, parents strongly encourage their bilingual children to bridge the gap between the Internet and language.\n\nThe Internet is increasingly being used as a virtual babysitter when parents actively download applications specifically for their children with intentions to keep them calm. A survey conducted by Ipsos has found that half of the interviewed parents believe children ages 8–13 are old enough to own or carry smartphones thus increasing online content consumption in younger generations.\n\n\n"}
{"id": "6730317", "url": "https://en.wikipedia.org/wiki?curid=6730317", "title": "RTD info", "text": "RTD info\n\nRTD info was a quarterly magazine published by the European Commission presenting a mix of research results and debate on scientific subjects of interest to a wide, non-specialised readership. In 2007 the decision was made to change the name to research*eu. It lasted 63 issues, which are still available as pdf files. The common theme was Europe.\n\nRTD info was published by the Communication Unit of the Research DG on paper in English, French \"(RDT info)\" and German \"(FTE info)\", and on the internet in these languages plus Spanish \"(I+DT info)\". When the name was changed to research*eu, a Spanish translation was also printed. The magazine had its headquarters in Brussels. Michel Claessens served as the editor-in-chief of the magazine.\n"}
{"id": "325496", "url": "https://en.wikipedia.org/wiki?curid=325496", "title": "Servomechanism", "text": "Servomechanism\n\nIn control engineering a servomechanism, sometimes shortened to servo, is an automatic device that uses error-sensing negative feedback to correct the action of a mechanism. It usually includes a built-in encoder or other position feedback mechanism to ensure the output is achieving the desired effect. \n\nThe term correctly applies only to systems where the feedback or error-correction signals help control mechanical position, speed or other parameters. For example, an automotive power window control is not a servomechanism, as there is no automatic feedback that controls position—the operator does this by observation. By contrast a car's cruise control uses closed-loop feedback, which classifies it as a servomechanism.\n\nA common type of servo provides \"position control\". Commonly, servos are electrical, hydraulic or pneumatic. They operate on the principle of negative feedback, where the control input is compared to the actual position of the mechanical system as measured by some type of transducer at the output. Any difference between the actual and wanted values (an \"error signal\") is amplified (and converted) and used to drive the system in the direction necessary to reduce or eliminate the error. This procedure is one widely used application of control theory. Typical servos can give a rotary (angular) or linear output.\n\nSpeed control via a governor is another type of servomechanism. The steam engine uses mechanical governors; another early application was to govern the speed of water wheels. Prior to World War II the constant speed propeller was developed to control engine speed for maneuvering aircraft. Fuel controls for gas turbine engines employ either hydromechanical or electronic governing.\n\nPositioning servomechanisms were first used in military fire-control and marine navigation equipment. Today servomechanisms are used in automatic machine tools, satellite-tracking antennas, remote control airplanes, automatic navigation systems on boats and planes, and antiaircraft-gun control systems. Other examples are fly-by-wire systems in aircraft which use servos to actuate the aircraft's control surfaces, and radio-controlled models which use RC servos for the same purpose. Many autofocus cameras also use a servomechanism to accurately move the lens. A hard disk drive has a magnetic servo system with sub-micrometre positioning accuracy. In industrial machines, servos are used to perform complex motion, in many applications.\n\nA \"servomotor\" is a specific type of motor that is combined with a rotary encoder or a potentiometer to form a servomechanism. This assembly may in turn form part of another servomechanism. A potentiometer provides a simple analog signal to indicate position, while an encoder provides position and usually speed feedback, which by the use of a PID controller allow more precise control of position and thus faster achievement of a stable position (for a given motor power). Potentiometers are subject to drift when the temperature changes whereas encoders are more stable and accurate.\n\nServomotors are used for both high-end and low-end applications. On the high end are precision industrial components that use a rotary encoder. On the low end are inexpensive radio control servos (RC servos) used in radio-controlled models which use a free-running motor and a simple potentiometer position sensor with an embedded controller. The term \"servomotor\" generally refers to a high-end industrial component while the term \"servo\" is most often used to describe the inexpensive devices that employ a potentiometer. Stepper motors are not considered to be servomotors, although they too are used to construct larger servomechanisms. Stepper motors have inherent angular positioning, owing to their construction, and this is generally used in an open-loop manner without feedback. They are generally used for medium-precision applications.\n\nRC servos are used to provide actuation for various mechanical systems such as the steering of a car, the control surfaces on a plane, or the rudder of a boat. Due to their affordability, reliability, and simplicity of control by microprocessors, they are often used in small-scale robotics applications. A standard RC receiver (or a microcontroller) sends pulse-width modulation (PWM) signals to the servo. The electronics inside the servo translate the width of the pulse into a position. When the servo is commanded to rotate, the motor is powered until the potentiometer reaches the value corresponding to the commanded position.\n\nJames Watt's steam engine governor is generally considered the first powered feedback system. The windmill fantail is an earlier example of automatic control, but since it does not have an amplifier or gain, it is not usually considered a servomechanism.\n\nThe first feedback position control device was the ship steering engine, used to position the rudder of large ships based on the position of the ship's wheel. \nJohn McFarlane Gray was a pioneer. His patented design was used on the SS Great Eastern in 1866.\nJoseph Farcot may deserve equal credit for the feedback concept, with several patents between 1862 and 1868.\n\nThe telemotor was invented around 1872 by Andrew Betts Brown, allowing elaborate mechanisms between the control room and the engine to be greatly simplified. Steam steering engines had the characteristics of a modern servomechanism: an input, an output, an error signal, and a means for amplifying the error signal used for negative feedback to drive the error towards zero. The Ragonnet power reverse mechanism was a general purpose air or steam-powered servo amplifier for linear motion patented in 1909.\n\nElectrical servomechanisms were used as early as 1888 in Elisha Gray's Telautograph.\n\nElectrical servomechanisms require a power amplifier. World War II saw the development of electrical fire-control servomechanisms, using an amplidyne as the power amplifier. Vacuum tube amplifiers were used in the UNISERVO tape drive for the UNIVAC I computer. The Royal Navy began experimenting with Remote Power Control (RPC) on HMS Champion in 1928 and began using RPC to control searchlights in the early 1930s. During WW2 RPC was used to control gun mounts and gun directors.\n\nModern servomechanisms use solid state power amplifiers, usually built from MOSFET or thyristor devices. Small servos may use power transistors.\n\nThe origin of the word is believed to come from the French \"Le Servomoteur\" or the slavemotor, first used by J. J. L. Farcot in 1868 to describe hydraulic and steam engines for use in ship steering.\n\nThe simplest kind of servos use bang–bang control. More complex control systems use proportional control, PID control, and state space control, which are studied in modern control theory.\n\nServos can be classified by means of their feedback control systems:\n\n\nThe servo bandwidth indicates the capability of the servo to follow rapid changes in the commanded input.\n\n\n\n"}
{"id": "2225599", "url": "https://en.wikipedia.org/wiki?curid=2225599", "title": "Shuttle Training Aircraft", "text": "Shuttle Training Aircraft\n\nThe Shuttle Training Aircraft (STA) was a NASA training vehicle that duplicated the Space Shuttle's approach profile and handling qualities, allowing Space Shuttle pilots to simulate Shuttle landings under controlled conditions before attempting the task on board the orbiter.\n\nThe aircraft's exterior was modified to withstand the high aerodynamic forces incurred during training sorties. A redesigned cockpit provided a high-fidelity simulation of the Shuttle Orbiter's controls and pilot vantage point; even the seats were fitted in the same position as those in the Space Shuttle.\n\nThe four STAs were normally located at the NASA Forward Operating Location in El Paso, Texas and rotated through Ellington Field (Houston, Texas) for maintenance. The STA was also used at Kennedy Space Center in Florida. It was primarily flown by astronauts practicing landings at the Shuttle Landing Facility and White Sands Space Harbor as well as to assess weather conditions prior to Space Shuttle launches and landings.\n\nOn December 3, 2003, a NASA Gulfstream II Shuttle Training Aircraft (STA) was flying a series of simulated shuttle landings to the Kennedy Space Center shuttle landing facility. On board the aircraft was an unidentified NASA astronaut pilot and two training personnel. The aircraft was on final approach at 13,000 feet when onboard instruments indicated a malfunction on one of the jet engine thrust reversers. The aircraft landed safely. A post-landing inspection showed that one of the 585-pound, 4-foot-wide, 5-foot-long thrust reversers had fallen off the aircraft. Divers later found the thrust reverser on the bottom of the nearby Banana River. An investigation showed that a bolt failed, releasing the part from the aircraft.\n\nThe STA was particularly critical for Shuttle pilots in training because the Orbiter lacked atmospheric engines that would allow the craft to \"go around\" after a poor approach. After re-entry, the Shuttle was a very heavy glider (it was sometimes referred to as a 'flying brick') and as such had only one chance to land successfully.\n\nTo match the descent rate and drag profile of the real Shuttle at , the main landing gear of the C-11A was lowered (the nose gear stayed retracted due to wind load constraints) and engine thrust was reversed. Its flaps could deflect upwards to decrease lift as well as downwards to increase lift.\n\nCovers were placed on the left hand cockpit windows to provide the same view as from a Shuttle cockpit, and the left-hand pilot's seat was fitted with the same controls as a Shuttle. The STA's normal flight controls were moved to the right, where the instructor sat. Both seat positions had a head-up display (HUD).\n\nIn a normal exercise, the pilot descended to at an airspeed of , from the landing target. The pilot then rolled the STA at , from landing. The nose of the aircraft was then dropped to increase speed to , descending at a 20-degree angle on the outer glide slope (OGS). The outer glide slope aiming point was short of the runway threshold, and used PAPIs for visual guidance in addition to the MLS system. At the guidance system changed to pre-flare and shortly after, at , the pilot started the flare maneuver to gradually reduce the descent angle and transition to the inner glide slope (IGS) which was 1.5 degrees from onwards, using a \"ball-bar\" system for visual guidance. The shuttle landing gear release was simulated at above the ground, since the STA main gear remained down for the whole simulation. The nose gear of the STA was lowered at AGL in case of an inadvertent touchdown with the runway surface.\n\nIf the speed was correct, a green light on the instrument panel simulated shuttle landing when the pilot's eyes were above the runway. This was the exact position that the pilot's head would be in during an actual landing. In the exercise, the STA was still flying above the ground. The instructor pilot deselected the simulation mode, stowed the thrust reversers, and the instructor executed a \"go-around\", never actually landing the aircraft (on training approaches).\n\nA sophisticated computer system installed on board the STA simulated the flight dynamics of the orbiter with nearly perfect accuracy. The STA's highly realistic simulation of the orbiter was not limited to handling characteristics, but also implemented the shuttle control interfaces for the pilot.\n\nAn onboard computer called the Advanced Digital Avionics System (ADAS) controlled the Direct Lift Control (DLC) and the in-flight reverse thrust during Simulation Mode.\n\nEvery shuttle commander practiced at least 1,000 landings in this manner, as had each mission's shuttle pilot.\n\nFour Gulfstream II aircraft constituted the now retired STA fleet, although other Gulfstream II aircraft, lacking STA capabilities, are still used by NASA for personnel transport purposes. Although the majority of the fleet had markings similar to those pictured above, paint schemes do vary slightly across aircraft. The STA tail numbers were:\n\n\nOn August 22, 2011, NASA announced that all four Shuttle Training Aircraft would be retired at various NASA facilities around the country, with N944 retiring at the Dryden Flight Research Center.\n\nIn the event NASA's T-38 Talons were not available, the STAs were used for transporting shuttle crew members between major sites, namely from Johnson Space Center in Houston to Kennedy Space Center in Cape Canaveral, Florida.\n\nDuring the early phases of the Shuttle program, NASA originally considered using the Boeing 737 airliner as the basis for the STA, but rejected it due to cost and opted for the less-expensive Gulfstream II.\n\n"}
{"id": "2238325", "url": "https://en.wikipedia.org/wiki?curid=2238325", "title": "Take-the-best heuristic", "text": "Take-the-best heuristic\n\nThe take-the-best heuristic estimates which of two alternatives has a higher value on a criterion by choosing the alternative based on the first cue that discriminates between the alternatives, where cues are ordered by cue validity (highest to lowest). In the original formulation, the cues were assumed to have binary values (yes or no) or have an unknown value.\n\nGerd Gigerenzer and Daniel Goldstein discovered that the heuristic did surprisingly well at making accurate inferences in real-world environments, such as inferring which of two cities is larger. The heuristic has since been modified and applied to domains from medicine, artificial intelligence, and political forecasting.\n\n"}
{"id": "11807146", "url": "https://en.wikipedia.org/wiki?curid=11807146", "title": "The Tennis Partner", "text": "The Tennis Partner\n\nThe Tennis Partner is the second of Abraham Verghese's books. Published in 1999, when he was a physician practicing internal medicine in El Paso, Texas, this is an autobiographical memoir, and Abraham Verghese writes of his experience moving to El Paso in the midst of an unraveling marriage. Once there, he meets and eventually becomes a mentor to David Smith, a medical resident at the hospital where Verghese worked and a brilliant tennis player recovering from drug addiction.\n\nBecause of his own love for the game and as part of his effort to reach out to the troubled resident, Verghese begins to play singles tennis regularly during their free time outside the hospital. What starts as a casual game between the two men eventually develops into a complex ritual that allows them to develop a deep friendship and understanding of the pressures they each face. In the hospital, Verghese is the teacher and Smith the student. On the court, however, Smith, the one-time professional player, becomes the teacher. The story tells of their all too brief friendship as Smith battles and eventually succumbs to his disease, and Verghese's helpless attempts to intervene.\n\nWhile cited as fiction, \"The Tennis Partner\" is heavily autobiographical.\n"}
