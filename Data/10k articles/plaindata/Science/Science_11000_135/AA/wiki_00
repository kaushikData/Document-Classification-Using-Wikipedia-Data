{"id": "52481827", "url": "https://en.wikipedia.org/wiki?curid=52481827", "title": "50 Things That Made the Modern Economy", "text": "50 Things That Made the Modern Economy\n\n50 Things That Made the Modern Economy began as a weekly economic history documentary radio series on the BBC World Service presented by economist and journalist Tim Harford. The first episode was broadcast on Saturday 5 November 2016, and all episodes are available as podcasts.\n\nHarford explained his motivation \"to paint a picture of economic change by telling the stories of the ideas, people, and tools that had far-reaching consequences\". He was fascinated by the many unexpected outcomes, such as \"the impact of the fridge on global politics, or of the gramophone on income inequality.\"\n\nThe series was subsequently released by Harford as a book titled Fifty Things That Made The Modern Economy in the UK published by Little, Brown Books, and as Fifty Inventions That Shaped The Modern Economy in the US published by Riverhead Books. The hardcover and eBook editions were released on 6 July 2017 in the UK and the US edition was released on 29 August 2017. A paperback edition has been announced for May 2018.\n\nA public call was made for suggestions for a 51st thing in August 2017. Harford chose his six favourite submissions for an online vote from late September to 6 October 2017. The winning item was announced as the Credit Card in an episode released on 28 October 2017.\n\nEach of the nine-minute long programmes introduces the story of fifty products or inventions that have revolutionised the modern world.\n\nEach episode was originally broadcast on BBC World Service, with a subsequent broadcast on BBC Radio 4 and distribution as a BBC podcast.\n\nThe shortlist selected by Tim Harford for public vote as the 51st thing was:\n\nThe item selected by public vote was the Credit Card, announced by Tim Harford in the episode entitled “Number 51”\n\nEach of the short chapters describes fifty products or inventions that have revolutionised the modern world.\n\nThe book title uses the word Fifty whereas the radio programme uses the number 50. The chapter order is different to the radio broadcast and podcast order, some book chapters have modified titles, and the chapters are grouped into sections in the book.\n\n\n"}
{"id": "55722689", "url": "https://en.wikipedia.org/wiki?curid=55722689", "title": "A1689B11", "text": "A1689B11\n\nA1689B11 is an extremely old spiral galaxy located in the Abell 1689 galaxy cluster in the Virgo constellation. The disk of A1689B11 is cool and thin, yet it produced stars at thirty times the rate of the Milky Way. A1689B11 is 11 billion light years from the Earth, forming 2.6 billion years after the Big Bang. It is the most distant known and earliest spiral galaxy .\n"}
{"id": "6859511", "url": "https://en.wikipedia.org/wiki?curid=6859511", "title": "A307 road", "text": "A307 road\n\nThe A307 road runs through SW London and NW Surrey. It is primary at the north-east end; the remainder is non-primary, generally superseded in the mid-twentieth century in two stages by newer alignments of the Portsmouth Road, the Kingston bypass and Esher bypass of the A3 road which runs along a slightly oblique axis.\n\nIt begins at the junction with the A205 South Circular Road beside Kew Green , where it is named Kew Road. It then runs towards Richmond upon Thames or Richmond, London, through the west of Kew. At the junction with the A316 in Richmond it becomes a non-primary A-road through the town centre then heads through Petersham where for fewer than 100 metres it kinks west and then travels south through Ham. A B-class road, the B353, leaves the A307 in Kew and runs around the town centre and up Richmond Hill and by-passing Richmond, before rejoining the A307 at Petersham.\n\nIt bisects the north of the town before becoming the western half of the one-way system in Kingston upon Thames. Here it is briefly merged with the A308. It leads south to the northern end of the A240, for 200m travels west to the River Thames, and resuming south becomes at last the old version of the Portsmouth Road (which is also its name here). It runs next to the River Thames, heading through Surbiton. It passes a junction with the A243, shortly before exiting the borough at Seething Wells there next to Long Ditton.\n\nThe Thames which is curved alongside Thames Ditton veers away from its now almost straight south-west course; the road passes through Hinchley Wood, going through the Scilly Isles roundabout with the A309. It forms the High Street of Esher, crosses the A3 (new Portsmouth Road) by way of a bridge north of Cobham, before terminating near a junction of the A3 in Cobham which is generally also known as Portsmouth Road.\n\n\nThe A307 follows the old route of the Portsmouth Road, particularly the section south of the junction with the A308. Since two major projects of the 1930s and 1960s respectively the Portsmouth Road, the A3 (Portsmouth Road) of today, has been diverted away from towns/villages instead through buffer land or more from urban centres and is a tripled or dualled (duplicated as described at the time) in each direction.\n\nRobert Clive (\"Clive of India\") diverted it slightly believing it ran too close to his house at Claremont, the Landscape Garden of which remains and which it still borders.\n\nA watchman's box that also served as a village lock-up, dating from 1787 is next to the 'Fox & Duck' in Petersham.\n\nResponsibility for the north section, Kew Road and Richmond Road, passed from the crown to the Commissioner of Works under the Crown Lands Act 1851.\n\nThe A307 was closed during 1979 and 1980 for a total of almost 18 months by the repeated collapse of a sewer and fresh water culvert in the road's narrowest section which is in Petersham, an ordeal referred to as the Petersham Hole. The route has sometimes been used from a few hundred metres south of the Richmond Gate of Richmond Park to Kingston as part of the London-Surrey cycle classic events routes, depending on the availability of Park Road, Kingston which avoids the hairpin and sharper descent at Richmond Gate.\n"}
{"id": "17406353", "url": "https://en.wikipedia.org/wiki?curid=17406353", "title": "AUFS", "text": "AUFS\n\nAbsorbance Units Full Scale (AUFS) is a ubiquitous unit of UV absorbance intensity.\n\nAUFS is an arbitrary but ubiquitous unit of UV absorbance intensity. It can be used in chemical analysis to quantify components in a mixture, as each components' integrated peak area correspond to their relative abundance. \n\n\n\n"}
{"id": "23462587", "url": "https://en.wikipedia.org/wiki?curid=23462587", "title": "Advanced Gemini", "text": "Advanced Gemini\n\nAdvanced Gemini is a number of proposals that would have extended the Gemini program by the addition of various missions, including manned low Earth orbit, circumlunar and lunar landing missions. Gemini was the second manned spaceflight program operated by NASA, and consisted of a two-seat spacecraft capable of maneuvering in orbit, docking with unmanned spacecraft such as Agena Target Vehicles, and allowing the crew to perform tethered extra-vehicular activities.\n\nA range of applications were considered for Advanced Gemini missions, including military flights, space station crew and logistics delivery, and lunar flights. The Lunar proposals ranged from reusing the docking systems developed for the Agena target vehicle on more powerful upper stages such as the Centaur, which could propel the spacecraft to the Moon, to complete modifications of the Gemini to enable it to land on the Lunar surface. Its applications would have ranged from manned lunar flybys before Apollo was ready, to providing emergency shelters or rescue for stranded Apollo crews, or even replacing the Apollo program.\n\nSome of the Advanced Gemini proposals used \"off-the-shelf\" Gemini spacecraft, unmodified from the original program, while others featured modifications to allow the spacecraft to carry more crew, dock with space stations, visit the Moon, and perform other mission objectives. Other modifications considered included the addition of wings or a parasail to the spacecraft, in order to enable it to make a horizontal landing.\n\nGemini was the second American manned orbital spaceflight program, after Mercury. It was intended to demonstrate technologies and techniques required for the Apollo program, such as extra-vehicular activities, rendezvous and docking, maneuvering in orbit and long duration flight.\n\nThe Gemini spacecraft, which was built by McDonnell Aircraft, was derived from the earlier Mercury spacecraft, but modified to accommodate two astronauts. It was also equipped with a larger equipment module, allowing it to support longer missions, and maneuver in orbit. It was launched by the Titan II rocket flying from Launch Complex 19 at the Cape Canaveral Air Force Station. In total, twelve missions were launched, ten of which were manned. Following two unmanned test flights, the first manned flight, Gemini 3, was launched on 23 March 1965. The program concluded on 15 November 1966, with the successful recovery of Gemini 12.\n\nMany other applications were envisaged for the Gemini spacecraft at various stages before, during, and after the two years in which it was used by NASA for manned spaceflight. Although none of these proposals ever made it into operation, many were considered seriously, and in some cases flight hardware was constructed prior to cancellation. In the case of the Manned Orbital Laboratory, a Gemini spacecraft was launched on a suborbital demonstration flight in support of the program. In some cases technology developed in the Advanced Gemini program has been reintegrated into other programs, such as components from the Titan IIIM, which was to have launched MOL, being used to upgrade other Titan rockets.\n\nThe United States Air Force intended to use the Gemini spacecraft to transport astronauts to its proposed space stations, the Manned Orbital Development System and later the Manned Orbital Laboratory (MOL). These stations would have been launched by Titan IIIM rockets, with a Gemini spacecraft atop, eliminating the need for rendezvous and docking maneuvers. For this purpose, several modifications were made to the Gemini capsule, including the installation of a hatch in the heat shield to allow access to the space station.\nIn order to give its astronauts experience before these programs started, the Blue Gemini program was proposed, which would have seen USAF astronauts fly on NASA missions in order to practice various techniques required for their own missions. This would have first seen cooperative missions between NASA and the US Air Force, with two missions flying with crews composed of one astronaut from NASA, and one from the USAF, followed by two missions with all-USAF crews, but performing missions for NASA. After these flights, the US Air Force would have flown a number of missions of its own. Firstly, it would have flown a two-man Agena rendezvous and docking mission, followed by two one-man scientific or technology research missions. Other proposed missions included tests of the Astronaut Mobility Unit which was designed to assist with EVAs, inertial navigation systems, and flying a radar imaging system.\nMOL Launches would have been conducted from Launch Complex 40 at the Cape Canaveral Air Force Station, and Space Launch Complex 6 at Vandenberg Air Force Base. In 1966, a test flight was launched from LC-40, using a Titan IIIC. It consisted of the Gemini B spacecraft, built from the spacecraft used for the Gemini 2 test flight, atop OPS 0855, a boilerplate MOL space station. Gemini B was released on a suborbital trajectory, and descended to Earth to test modifications made to the heat shield, and ensure that the access hatch would not affect its performance. OPS 0855 continued on to orbit. Early MOL stations would have only been manned by a single crew, launched with the station. Later stations would have been designed to be resupplied, and support multiple crews, delivered by additional Gemini spacecraft, or derivatives.\n\nThe MOL program was cancelled on 10 June 1969, in favor of unmanned reconnaissance satellites. Some systems developed for the program were later used on unmanned missions, while the space suits which were under development were transferred to NASA. The Titan IIIM rocket which was to launch MOL never flew, however some of the upgrades that were built into it were later used to upgrade other Titan rockets — the stretched first and second stages became the Titan 34, which was used as the core of some later Titan IIIB flights, and on the Titan 34D. The seven-segment solid rocket boosters were later introduced on the Titan IVA.\n\nSeveral Gemini Ferry spacecraft were proposed to provide transportation of crews and cargo to NASA and USAF space stations in low Earth orbit. NASA contracted McDonnell to conduct a study into what modifications would be needed to allow the Gemini spacecraft to support this. Three spacecraft were envisioned; a manned spacecraft to transport crew to the stations, a manned spacecraft with a cargo module for both crew and cargo delivery, and a dedicated unmanned spacecraft to resupply the station every three or four months.\n\nThe studies looked at minimizing required modifications to the Gemini spacecraft. Three docking methods were considered. The first was use of the existing docking system used on Gemini-Agena missions. This would have allowed the mission to be accomplished with little modification to the Gemini spacecraft needed, however crew transfer could only have been accomplished by means of an Extra-vehicular activity (EVA). Changes that would have been required included strengthening the nose, installing two solid rockets to be used for a separation burn, adding the necessary equipment to perform the transfer EVA, and providing provisions for flight to and from the station. The number of retro-rockets would have been increased from four to six. A second method would have seen the spacecraft dock in the same way, but after docking, the spacecraft would be swung round and attached to the side of the space station. A tunnel would then have been placed over the Gemini's hatches, allowing the crew to transfer to the station without performing an EVA. Some modifications to the hatches would have been required. The final proposed docking method was to use a port mounted on the rear of the equipment module, which would have allowed the crew to transfer directly between the spacecraft and space station, through the docking port.\n\nA modified version of the spacecraft was proposed, which would have included a cargo module attached to the back of a modified equipment module. The spacecraft would have approached the station, and docked backwards using a port on the rear of the cargo module. If one of the forward docking configurations had been used for the Gemini itself, the docking would have been controlled remotely from the station, with the Gemini then separating from the cargo module and flying around the station to dock normally on a different port. The rear-docking Gemini would have simply remained attached to the cargo module, with the crew boarding the station through it. Its docking would have been controlled by its own crew, from a station at the back of the cargo module.\n\nTwo Gemini-derived spacecraft were considered for unmanned resupply flights. The first of these would have involved a Gemini spacecraft, with all systems for manned flight, re-entry and landing removed. The spacecraft would have docked using a port at the front of the spacecraft. Cargo would have been transferred through the nose of the spacecraft, where the re-entry attitude control system was located on the manned spacecraft. The spacecraft was equipped with a liquid propellant engine to perform rendezvous, and to reboost the space station. The other proposal was for a new spacecraft to be built for unmanned missions, but re-using as many Gemini systems as possible. It would have had a higher cargo capacity than the stripped-down version of the Gemini spacecraft.\n\nCrew-only or cargo-only supply missions would have been launched aboard a Titan II, and the Saturn I or Saturn IB would have been used for the combined crew and cargo spacecraft. Because of the increased power of the Saturn I, the Gemini spacecraft's ejection seats would not have been able to propel the crew far enough in the event of an explosion, so a launch escape tower was proposed, based on the one used on the Mercury spacecraft. The Titan IIIM was also considered to launch the heavier spacecraft.\n\nBig Gemini, or Big G, grew out of a 1963 proposal called Gemini Transport, to develop an enlarged Gemini spacecraft with docking capability, which would take advantage of the increased capacity offered by the Saturn IB and Titan IIIM rockets. It was designed to transport between nine and twelve astronauts into space, and to be able to dock with space stations, in support of Apollo Applications and MOL missions. It would have been launched by a Heavy lift launch vehicle; either a Titan IIIG or Saturn INT-20, the former being intended for use on US Air Force missions and the latter being intended for NASA missions. The Titan IIIM was also considered, which would have launched a smaller version of the spacecraft due to its lower capacity, to resupply MOL space stations later in the program. NASA also proposed several Saturn IB derivatives with solid first stages as alternatives to the INT-20, offering similar payload capacity.\n\nThe shape and mass of the spacecraft would have varied depending on the rocket that was to launch it. The Saturn-launched version had a short, conical cargo module, and a total mass of whereas the Titan IIIG-launched version featured a longer, cylindrical module, with a total mass of . The Titan IIIM version would have been much shorter and lighter, with a mass of , as that rocket had less payload capacity than the Titan IIIG or Saturn. The IIIG variant would have carried twelve crew, while the others had a maximum capacity of nine. Use of the NASA variant with the Apollo Service Module was also considered.\n\nOnce in orbit, Big G would have docked with space stations using an Apollo docking probe mounted on the rear of the cargo module, which was in turn mounted on the rear of the re-entry module. The re-entry module itself would have been enlarged to accommodate the larger crew, and the modifications made to the spacecraft for the MOL program would have been incorporated, including the hatch in the heat shield, which would have been used to provide access to the cargo module. In the event of a launch failure, the launch escape system developed for the Apollo spacecraft would have been used to propel the re-entry module clear of the rocket.\n\nBig G would have made landings on land, using a parasail or paraglider to guide it onto a runway or dry lake, such as the one at Edwards Air Force Base. It would have landed using skids derived from the North American X-15.\n\nA number of studies investigated sending a Gemini spacecraft onto a circumlunar trajectory. Many of the proposals made for this involved a double-launch architecture, with the Gemini spacecraft rendezvousing with an upper stage in orbit. Upper stages that were considered included the Transtage, the second stage of a Titan II, four different types of Centaur, including the S-V variant developed for the Saturn I, the Agena-D, an enlarged Agena, and two Agenas burning in parallel. Either a Titan or Saturn IB would have been used to launch the upper stage, while the Gemini would launch on the Titan II, as it had in the Gemini program.\n\nOther proposals involved launching the Gemini spacecraft on a Titan IIIC, and refueling in low Earth orbit before proceeding to the Moon, and a single launch architecture using a three-stage variant of the Saturn IB.\n\nThe Gemini-Centaur proposal was predicted to have been able to achieve a 72-hour circumlunar flight. The Centaur would have performed trans-lunar injection, before separating from the Gemini spacecraft.\n\nSome concerns were raised that the Gemini spacecraft's heat shield would not have been able to protect it during the higher speed ballistic reentry associated with the trajectory that would have been required. NASA proposed using a thicker heat shield and more insulation to protect the spacecraft. This and several other modifications made the spacecraft too heavy to be launched by the Titan II rocket which was used for the original twelve Gemini missions, so several solid rocket motors would have been added to allow this additional mass to be flown.\n\nThe Gemini spacecraft would have rendezvoused with stacked Centaur and Agena upper stages in low Earth orbit. The Centaur would have placed the Gemini and Agena onto a circumlunar trajectory, along which they would coast until they reached the Moon. The Agena would then have been used to perform Lunar orbit insertion. Following the completion of activities in Lunar orbit, the Agena would have been fired again for trans-Earth injection.\n\nThis architecture would have used a Titan II to launch the Gemini spacecraft, with a Saturn IB launching the upper stages.\n\nUsing the Gemini spacecraft for a manned Lunar landing was considered as early as the original Mercury Mark II proposal which led to the Gemini program. The initial proposal was for a Lunar orbit rendezvous mission, using a Gemini spacecraft and a lightweight, open cockpit lander, launched by a Saturn C-3 rocket. It was the first time that Lunar orbit rendezvous was proposed as part of a lunar landing concept. The spacecraft would have been tested in Low Earth orbit before the Lunar missions, using two Titan II launches. The lander, which was designed by NASA's Langley Research Center, would have had a mass of no more than . Some of the proposals had a mass as low as , with cryogenic propellants being used in place of heavier hypergolic propellant. The proposal was intended to provide a faster and lower-cost alternative to the Apollo program, which was at that time proposing a direct ascent landing.\n\nAnother proposal would have used a Saturn V to achieve a direct ascent mission profile. The spacecraft consisted of four modules. The Retrograde Module would have been powered by an RL10 engine, and used to propel the spacecraft during the trans-lunar coast. During landing, it would be used for the initial phases of powered descent. At an altitude of above the Lunar surface, the RM would have been jettisoned, and a second module, the Terminal Landing Module, would have performed the final descent. After landing, the spacecraft would have stayed on the Moon for a day, before it was propelled back to Earth. Launch from the Lunar Surface and trans-Earth injection would have been performed by the Service Module, which would also have contained components of the life support system, and other systems which were located in the Equipment Module of the Earth orbit Gemini spacecraft. The Reentry Module was based on the Gemini capsule, but would have been modified to allow it to reenter the atmosphere at the higher velocity that would have been required by a Lunar flight. It was seen as the last effort by NASA managers and engineers who still advocated the direct ascent mission profile, and was intended to be cheaper, faster and safer than the Apollo lunar orbit rendezvous technique.\n\nDue to the risks associated with the Lunar landing, a number of rescue spacecraft were proposed, to be used to allow the crew of an Apollo mission to return to Earth safely in the event of a problem. Many of these designs were based around the Gemini spacecraft.\n\nOne of these proposals was the Gemini Lunar Orbit Rescue Vehicle, which was designed to retrieve the crew of an Apollo spacecraft stranded in orbit around the Moon. It was to have used a stretched reentry module to accommodate the three astronauts who would have been aboard the Apollo. This would have been attached to a modified Equipment Module. The Equipment Module contained engines which would be used for Lunar orbit insertion and trans-Earth injection, as well as life support equipment to keep the crew alive until they returned to Earth. Launch and trans-Lunar injection would have been performed by a Saturn V. Once the spacecraft was in Lunar orbit, the Apollo crew would have boarded it by means of an EVA.\n\nAnother proposed spacecraft, the Gemini Lunar Surface Survival Shelter, was designed to be sent to the Moon ahead of an Apollo mission. It would have landed close to the planned Apollo landing site, and if the Apollo Lunar Module's ascent stage failed to ignite, the crew would have performed an EVA to transfer to the LSSS. The shelter was not designed to take off again after landing, so an LSRS or another Apollo mission would then be sent to collect the crew, while the Command Module Pilot of the original Apollo mission would have returned to Earth alone aboard his spacecraft. It consisted of a Gemini Reentry Module, which would have housed the astronauts while they awaited rescue, and a descent stage containing life support systems, consumables, and the engine and propellant used to land the spacecraft.\n\nThe Gemini Lunar Surface Rescue Spacecraft was intended to fly a direct ascent mission, launched by a Saturn V. Descent stages, built from either the descent stage of the Apollo Lunar Module, or from the Apollo Service Module, would have reduced the spacecraft's velocity as it approached the Moon. One configuration used two Service Modules and one LEM descent stage, with the LEM descent stage performing the final landing, and then being reused for ascent from the Lunar surface and trans-Earth injection. The other configuration used three LEM descent stages, with the second being used for landing and the third for ascent and TEI. The LSRS would have landed close to the Apollo Lunar Module on the Moon, and the Apollo crew would have transferred to it by EVA.\n\nFollowing the Apollo 1 fire in January 1967, NASA conducted a safety review of the Apollo program. In response to this review, McDonnell proposed the Universal Lunar Rescue Vehicle, a repackaged version of the Lunar Surface Rescue Spacecraft with an enlarged capsule to make room for the three astronauts who were being rescued. It was intended to rescue an Apollo crew at almost any point while they were at the Moon, should an anomaly occur. Some ULRV designs included five seats, with two astronauts piloting it to the Moon. The proposal was considered, but rejected due to lack of funds.\n\nA number of other applications were considered for the Gemini spacecraft.\n\nThe Manned Orbital Telescope was a proposal for a manned spacecraft for astronomical or solar observation. It would have used the larger reentry module which was also proposed for the Big Gemini spacecraft, and would have been launched on a Saturn IB. The enlarged reentry module was also considered for a spacecraft proposed at the same time, which would have been used to rescue the crews of manned spacecraft that were stranded in low Earth orbit. It would have launched atop a Titan IIIC, once in orbit it would have maneuvered by means of a Transtage integrated into the equipment module. The larger capsule would have been used to accommodate the crew of the spacecraft which required rescuing.\n\nAnother proposed mission would have seen a Gemini spacecraft rendezvous with a Pegasus satellite in low Earth orbit. The spacecraft would have either been launched directly into an orbit to rendezvous with the Pegasus, or into a lower orbit, subsequently docking with an Agena, and using that to raise its orbit. The Gemini would have flown around the Pegasus, and then one of the crew would have performed an EVA to recover a piece of one of the spacecraft's micrometeoroid detection panels. This mission would have been used to prove that rendezvous could be accomplished with any spacecraft, to collect data on the satellite's exposure to micrometeoroids — supporting data that the satellite had returned itself, and to demonstrate technology for military flights to inspect hostile satellites.\n\nSeveral missions were proposed to demonstrate methods of landing the Gemini spacecraft on land. The spacecraft had originally been designed to land using a flexible Rogallo wing and a set of skis or wheels, however this was abandoned in favor of splashdowns under parachutes due to delays in development and failures during testing. As the proposed Big Gemini spacecraft would have landed this way, McDonnell Aircraft asked NASA to consider flying standard Gemini spacecraft with the paraglider in order to test the system before it would be required operationally.\n\nAnother alternative landing concept was the US Air Force's proposal to attach wings to the spacecraft. This proposal arose soon after the cancellation of the X-20 Dyna-Soar, and would have seen a Gemini spacecraft attached to a set of wings developed during the ASSET program. This would have been launched by a Titan II rocket, and would have been unable to maneuver in orbit. Another proposal saw the spacecraft being launched by a Titan IIIA or IIIC, using the Transtage for maneuvering. Once the mission was complete, the spacecraft would have been deorbited by means of five solid rocket motors.\n\nThere were also proposals to use the Gemini spacecraft to conduct long-duration missions to small, purpose-built space stations in low-Earth orbit. One proposal saw a space station based on the Agena, which would have been used to provide propulsion and attitude control for the station. A pressurized module between the Agena and the docking adapter would have been used by the crew for accommodation and to conduct experiments. The crew would have boarded the Agena by means of an inflatable tunnel between the hatches and its airlock. The Pecan spacecraft was a similar proposal.\n\n"}
{"id": "16909668", "url": "https://en.wikipedia.org/wiki?curid=16909668", "title": "An Elementary Treatise on Electricity", "text": "An Elementary Treatise on Electricity\n\nAn Elementary Treatise on Electricity is a book by James Clerk Maxwell. The origin of the book are lecture notes Clerk Maxwell gave to members of the Cavendish Laboratory, which he founded.\n\n"}
{"id": "36681811", "url": "https://en.wikipedia.org/wiki?curid=36681811", "title": "Anticipatory Systems; Philosophical, Mathematical, and Methodological Foundations", "text": "Anticipatory Systems; Philosophical, Mathematical, and Methodological Foundations\n\nAnticipatory Systems is a book by Robert Rosen, conceived in the 1970s and published for the first time in 1985. The book describes the way that biological systems anticipate the environment. The book draws from mathematics, in particular category theory, in describing the way systems can anticipate.\n\nThe first five chapters of the book are about modeling: Rosen shows that natural systems, physical things in the world, are modeled by formal systems, which are at their heart mathematical. These formal systems simulate the natural systems. But in order to provide anticipatory knowledge, they must produce predictions ahead of the predicted phenomena.\n\nThe first edition of \"Anticipatory Systems\" was published in 1985 by Pergamon Press. The second edition of \"Anticipatory Systems\" was published in 2012 by Springer.\n\nThe second edition includes a fifty-page prolegomena by Mihai Nadin, as well as contributions by Judith Rosen and John J. Kineman. The book is the 27th volume in the International Federation for Systems Research International Series on Systems Science and Engineering, a series edited by George Klir.\n\nA review by Eric Minch of the first edition called the book \"radical and profound\".\n\nRosen followed this book with a text called \"Life Itself\" that discussed the mathematical, scientific, and ethical issues related to generating artificial life.\n\nA. H. Louie, one of Rosen's students, published a tutorial article on the book, explaining, among other things, the mathematical concepts behind the book \n\nConjunction and disjunction terms are reversed in the text, pages 60–68, but the mathematical expressions are correct.\n\nFootnote 8 missing from page 384: probably the reference on 382 should be to footnote 7.\n\nPages 327 and 328 are swapped.\n"}
{"id": "901697", "url": "https://en.wikipedia.org/wiki?curid=901697", "title": "Blindness (novel)", "text": "Blindness (novel)\n\nBlindness (, meaning \"Essay on Blindness\") is a novel by Portuguese author José Saramago. It is one of his most famous novels, along with \"The Gospel According to Jesus Christ\" and \"Baltasar and Blimunda\". In 1998, Saramago received the Nobel Prize for Literature, and \"Blindness\" was one of his works noted by the committee when announcing the award.\n\n\"Blindness\" is the story of an unexplained mass epidemic of blindness afflicting nearly everyone in an unnamed city, and the social breakdown that swiftly follows. The novel follows the misfortunes of a handful of characters who are among the first to be stricken and centers on \"the doctor's wife,\" her husband, several of his patients, and assorted others, who are thrown together by chance. After lengthy and traumatic quarantine in an asylum, the group bands together in a family-like unit to survive by their wits and by the unexplained good fortune that the doctor’s wife has escaped the blindness. The sudden onset and unexplained origin and nature of the blindness cause widespread panic, and the social order rapidly unravels as the government attempts to contain the apparent contagion and keep order via increasingly repressive and inept measures.\n\nThe first part of the novel follows the experiences of the central characters in the filthy, overcrowded asylum where they and other blind people have been quarantined. Hygiene, living conditions, and morale degrade horrifically in a very short period, mirroring the society outside.\n\nAnxiety over the availability of food, caused by delivery irregularities, acts to undermine solidarity; and lack of organization prevents the internees from fairly distributing food or chores. Soldiers assigned to guard the asylum and look after the well-being of the internees become increasingly antipathetic as one soldier after another becomes infected. The military refuse to allow in basic medicines, so that a simple infection becomes deadly. Fearing a break out, soldiers shoot down a crowd of internees waiting upon food delivery.\n\nConditions degenerate further as an armed clique gains control over food deliveries, subjugating their fellow internees and exposing them to rape and deprivation. Faced with starvation, internees battle each other and burn down the asylum, only to discover that the army has abandoned the asylum, after which the protagonists join the throngs of nearly helpless blind people outside who wander the devastated city and fight one another to survive.\n\nThe story then follows the doctor's wife, her husband, and their impromptu “family” as they attempt to survive outside, cared for largely by the doctor’s wife, who can still see (though she must hide this fact at first). The breakdown of society is near total. Law and order, social services, government, schools, etc., no longer function. Families have been separated and cannot find each other. People squat in abandoned buildings and scrounge for food. Violence, disease, and despair threaten to overwhelm human coping. The doctor and his wife and their new “family” eventually make a permanent home in the doctor's house and are establishing a new order to their lives when the blindness lifts from the city en masse just as suddenly and inexplicably as it struck.\n\nThe doctor's wife is the only character in the entire novel who does not lose her sight. This phenomenon remains unexplained in the novel. Unwilling to leave her husband to be interned, she lies to the doctors and claims to be blind. At this point she is interned with the rest of the afflicted. Once inside, she attempts to help the compound organize, but she is increasingly unable to hold back the animality of the compound. When one ward begins withholding food and demanding that the women of other wards submit to being raped in return for food, she kills the leader of their ward. Once they escape the compound, she helps her group survive in the city. The doctor's wife is the de facto leader of their small group, although in the end she often serves their disabled needs.\n\nThe doctor is an ophthalmologist stricken blind after treating a patient with what will come to be called \"the white sickness\". He is among the first to be quarantined along with his wife. Due to his medical expertise he has a certain authority among those quarantined. Much of this really comes from his wife not having gone blind; she is able to see what is going on on the ward and relay this to her husband. When the group from his ward finally escapes they end up travelling to and staying in the doctor and his wife's apartment. Several of the other main characters had been visiting the doctor's office when the epidemic begins to spread.\n\nThe girl with the dark glasses is a former part-time prostitute who is struck blind while with a customer. She seemingly contracted the “white-blindness” while visiting the doctor due to conjunctivitis (hence the dark glasses). She is unceremoniously removed from the hotel and taken to the quarantine. Once inside, she joins the small group of people who were contaminated at the doctor's office. When the car thief gropes her on the way to the lavatory, she kicks him – giving him a wound from which he will eventually die. While inside, she also takes care of the boy with the squint, whose mother is nowhere to be found. At the end of the story, she and the old man with the black eye patch become lovers.\n\nThe old man with the black eye patch is the last person to join the first ward. He brings with him a portable transistor radio that allows the internees to listen to the news. He is also the main architect of the failed attack on the ward of hoodlums hoarding the food rations. Once the group escapes the quarantine, the old man becomes the lover of the girl with the dark glasses.\n\nThe dog of tears is a dog that joins the small group of blind when they leave the quarantine. While he is mostly loyal to the doctor's wife, he helps the whole group by protecting them all from packs of dogs who are becoming more feral by the day. He is called the dog of tears because he became attached to the group when he licked the tears off the face of the doctor's wife.\n\nThe boy with the squint was a patient of the doctor's, which is most likely how he became infected. He is brought to the quarantine without his mother and soon falls in with the group in the first ward. The girl with the dark glasses feeds him and takes care of him like a mother.\n\nAfter the first blind man was struck blind in traffic, a car thief brought him home and, subsequently stole his car. Soon after he went blind, the car thief and the first blind man re-encounter one another in the quarantine, where they soon come to blows. They have no time to resolve their issues, though, since the car thief is the first internee killed by the guards. He is gunned down while trying to ask the guards for medication for his infected leg.\n\nThe first man to go blind is struck blind in the middle of traffic, waiting at a stoplight. He is immediately taken home and then to the doctor's office, where he infects all of the other patients. He is one of the principal members of the first ward - the ward with all of the original internees. He is also the first to regain his sight, when the epidemic is finally over.\n\nThe wife of the first blind man goes blind soon after helping her husband to the quarantine. They are reunited by pure chance in the quarantine. Once inside, she also joins the first ward with the doctor and the doctor's wife. When the ward of hoodlums begins to demand that the women sleep with them in order to be fed, the first blind man's wife volunteers to go, in solidarity with the others.\n\nThe man with the gun is the leader of the ward of hoodlums that seizes control of the food supply in the quarantine. He and his ward take the rations by force and threaten to shoot anyone who doesn't comply. This ward extorts valuables from the other internees in exchange for food and, when the bracelets and watches run out, they begin to rape the women. He is later stabbed to death by the doctor's wife.\n\nThis man is not one of those afflicted by the \"white sickness\"—rather he has been blind since birth. He is the only one in the ward who can read and write braille and who knows how to use a walking stick. Additionally, he is the second in command to the man with the gun in the ward of hoodlums. When the doctor's wife kills the man with the gun, the blind accountant takes the gun and tries to seize control but he is unable to rally support. He dies when one of the rape victims sets fire to the ward.\n\nLike most works by Saramago, the novel contains many long, breathless sentences in which commas take the place of periods, quotation marks, semicolons and colons. The lack of quotation marks around dialogue means that the speakers' identities (or the fact that dialogue is occurring) may not be immediately apparent to the reader. The lack of proper character names in \"Blindness\" is typical of many of Saramago's novels (e.g. \"All the Names\"). The characters are instead referred to by descriptive appellations such as \"the doctor's wife\", \"the car thief\", or \"the first blind man\". Given the characters' blindness, some of these names seem ironic (\"the boy with the squint\" or \"the girl with the dark glasses\").\n\nThe city afflicted by the blindness is never named, nor the country specified. Few definite identifiers of culture are given, which contributes an element of timelessness and universality to the novel. Some signs hint that the country is Saramago's homeland of Portugal: the main character is shown eating chouriço, a spicy sausage, and some dialogue in the original Portuguese employs the familiar \"tu\" second-person singular verb form (a distinction absent in most of Brazil). The church, with all its saintly images, is likely of the Catholic variety.\n\nSaramago wrote a sequel to \"Blindness\" in 2004, titled \"Seeing\" (\"Ensaio sobre a lucidez\", literal English translation \"Essay on lucidity\"), which has also been translated into English. The new novel takes place in the same country featured in \"Blindness\" and features several of the same characters.\n\nAn English-language film adaptation of \"Blindness\" was directed by Fernando Meirelles. Filming began in July 2007 and stars Mark Ruffalo as the doctor and Julianne Moore as the doctor's wife. The film opened the 2008 Cannes Film Festival.\n\nIn 2007 the Drama Desk Award Winning Godlight Theatre Company staged the New York City theatrical premiere of Blindness at 59E59 Theaters. This stage version was adapted and directed by Joe Tantalo. The First Blind Man was played by Mike Roche.\n\nAn outdoor performance adaptation by the Polish group Teatr KTO, was first presented in June 2010. It has since been performed at a number of venues, including the Old College Quad of Edinburgh University during the 2012 Edinburgh Festival Fringe.\n\nShortly before his death, Saramago gave German composer Anno Schreier the rights to compose an opera based on the novel. The libretto is written in German by Kerstin Maria Pöhler. Like the German translation of the novel, the opera's title is \"Die Stadt der Blinden\". It saw its first performance on November 12, 2011 at the Zurich Opera House.\n\n"}
{"id": "3604433", "url": "https://en.wikipedia.org/wiki?curid=3604433", "title": "Cannon–Bard theory", "text": "Cannon–Bard theory\n\nThe main concepts of the Cannon–Bard theory are that emotional expression results from the function of hypothalamic structures, and emotional feeling results from stimulations of the dorsal thalamus. The physiological changes and subjective feeling of an emotion in response to a stimulus are separate and independent; arousal does not have to occur before the emotion. Thus, the thalamic region is attributed a major role in this theory of emotion. The theory is therefore also referred to as the thalamic theory of emotion.\n\nWalter Bradford Cannon (1871–1945) was a physiologist at Harvard University, who is perhaps best known for his classic treatise on homeostasis. Philip Bard (1898–1977) was a doctoral student of Cannon's, and together they developed a model of emotion called the Cannon–Bard Theory. Cannon was an experimenter who relied on studies of animal physiology. Through these studies, Cannon and Bard highlighted the role of the brain in generating physiological responses and feelings; a role that is important in their explanation of emotion experience and production.\n\nA dominant theory of emotion of Cannon's time was the James–Lange theory of emotion, and Cannon recognized that to test this theory, an examination of emotional expression with no visceral afferent feedback was required. This was necessary because the link between visceral changes and the feedback required to stimulate cerebral manifestations of an emotion would no longer be present. To do so, Cannon experimented with severing afferent nerves of the sympathetic branch of the autonomic nervous system in cats. Cannon compiled his experimental results in 1915, then refined and expanded them, and finally proposed his model of emotion as a challenge and alternative to the James–Lange theory of emotion.\n\nThe James–Lange theory relies on the backflow of impulses from the periphery to account for unique emotional experiences; impulses that William James assumed to come from all parts of the organism, including the muscles, skin, and the viscera. The viscera were attributed a major role by James. The viscera are composed of smooth muscle and glands. Cannon identified and outlined five issues with the James–Lange theory's notion of the vasomotor center as the explanation of emotional experience.\n\n\nWilliam James argued that there were either special centers for cerebral processes that accompany emotion, or they occurred in the ordinary motor and sensory centers of the cortex. Cannon responded by positing that there may not be one or the other, that there may be cortical processes and special centers that accompany emotional responses. He outlined two ideas regarding the existence of two sources of cerebral processes of emotions.\n\n\nCannon summarized research done by Bechterev regarding emotional expression. In this research, it was argued that emotional expression must be independent of the cortex because the expression of emotions cannot always be inhibited or controlled (e.g. laughing from being tickled) because visceral changes occur independent of our control, and because these responses, which cannot be inhibited, are seen soon after birth before cortical management is developed. Furthermore, after cerebral hemispheres were removed from animal test subjects, correct affective responses could be elicited by appropriate stimulations. These emotional effects were no longer present when the optic thalamus was removed from the animals; thus, it was concluded that this region plays a significant role in the expression of emotions.\n\nTo further support the assertion that emotional expression results from action of subcortical centers, Cannon and Britton performed further experimental research with cats. Cats were decorticated, and after a period of recovery they spontaneously displayed the behaviours characteristic of intense fury. This response, referred to as sham rage, continued to be displayed after ablation of all brain regions anterior to the diencephalon. However, once the lower posterior portion of the thalamic region was removed, the display of sham rage by the cats subsided. Based on this finding, it was concluded that the thalamus was a region from which, in the absence of cortical control, impulses are discharged which evoke an extreme degree of \"emotional\" activity, both muscular and visceral.\nBased on these findings and observations, Cannon asserts that the optic thalamus is a region in the brain responsible for the neural organization for the different emotional expressions.\n\n\nThere are numerous reported and cited cases of patients with unilateral lesions in the thalamus region who have a tendency to react excessively to affective stimuli. For example, pin pricks, painful pressure, and excessive heat or cold all cause more distress on the damaged side of the body as compared to the normal side. Similar results can be observed from agreeable stimuli: warmth stimuli may cause intense pleasure, demonstrated by facial expressions of enjoyment and exclamations of delight by the individual. The increased influence of stimuli resulting in excessive responses was attributed to the release of the thalamus from cortical inhibition. When the thalamus is released from cortical control, the affective states and responses are increased; thus, it was concluded that the thalamic region is occupied with the affective component of sensation.\n\nBased on the previous discussion of the purported faults regarding the James–Lange theory of emotion's explanation, Cannon put forward a theory of emotion based on thalamic processes.\n\nAccording to Cannon, an external stimulus activates receptors and this excitation starts impulses toward the cortex. Upon arriving in the cortex, the impulses are associated with conditioned processes that determine the direction of the subsequent response. It is this response that stimulates the thalamic processes. Once the thalamic processes are activated, they are ready to discharge. The thalamic neurons fire in a special combination in a given emotional expression. These neurons then discharge precipitately and intensely. Cannon wrote that within and near the thalamus, the neurons responsible for an emotional expression lie close to the relay in the sensory path from the periphery to the cortex, and when these neurons fire in a particular combination they innervate muscles and viscera and excite afferent paths to the cortex by direct connection or irradiation.\n\nThe key component of the Cannon–Bard theory of emotion is that when the thalamic discharge occurs, the bodily changes occur almost simultaneously with the emotional experience. The bodily changes and emotional experience occur separately and independently of one another; physiological arousal does not have to precede emotional expression or experience. The theory asserts that the thalamic region is the brain area responsible for emotional responses to experienced stimuli.\n\nCannon summarises the observations that serve as the basis for his theory of emotion which claims the thalamic region is the coordinating center for emotional reactions. First, after the removal of the cerebrum anterior to the thalamus in animal test subjects, the animals continue to display rage-like emotional responses. These reactions cease when the thalamus is then removed. Secondly, a tumor on one side of the thalamus can result in unilateral laughter or grimace under the appropriate conditions, although cortical and voluntary control of the same muscles is bilateral. Lastly, temporary impairment of cortical control of lower centers from light amnesia or permanent impairment by disease (e.g. tumor or lesion) can cause uncontrollable and prolonged weeping or laughing.\n\nThe Cannon–Bard theory of emotion was formulated as a challenge and alternative to James–Lange theory. The Papez-Maclean theory is another influential theory of emotion that differs from the Cannon–Bard theory in terms of the area that is considered to be responsible for emotion expression. James Papez initially suggested that the interconnections among structures of the limbic system were ideally constituted to handle the long-lasting, intense aspects of experience that are typically associated with emotion.\nThe circuit originally proposed by Papez consisted of the hippocampus, the ipsilateral mammillary body, the anterior nucleus of the thalamus, the cingulate cortex, the parahippocampal gyrus, and the entorhinal cortex, returning to the hippocampus. MacLean elaborated on Papez's earlier work, adding the prefrontal cortex, the septum, and the amygdala, and named this group of structures the limbic system.\n\nThere is also the two-factor theory of emotion, as proposed by Stanley Schachter and Jerome E. Singer.\n"}
{"id": "1783086", "url": "https://en.wikipedia.org/wiki?curid=1783086", "title": "Characteristic time", "text": "Characteristic time\n\nThe characteristic time is an estimate of the order of magnitude of the reaction time scale of a system. It can loosely be defined as the inverse of the reaction rate. In chemistry, the characteristic time is used to determine whether the problem needs to be solved as an equilibrium problem or a kinetic problem.\n\n\n"}
{"id": "49793844", "url": "https://en.wikipedia.org/wiki?curid=49793844", "title": "Citizen Design Science", "text": "Citizen Design Science\n\nCitizen Design Science is the combination of Citizen Science and Design Science. Design Science is known since the early 1960s when Buckminster Fuller defined it as systematic designing, whereas half a decade later Herbert A. Simon saw it more as the study of the design process. The term Citizen Science emerged in the 1990s as work that citizens of all ages and backgrounds conduct under the guidance of scientists, collecting data or inventing processes or artefacts. A contemporary example is the Zooniverse platform that enables people to “contribute to real discoveries in fields ranging from astronomy to zoology”.\nCitizen Design Science is a combination of both concepts for urban systems: it adds the combination of human observation, cognition, experience and local knowledge into a scientific framework that improves the planning, design, management and transformation of buildings and cities. Design Science contributes the methods and instruments, Citizen Science adds data and inventions. Thus, Citizen Design Science bundles individual observations and inventions into a bottom up flow of data and information to improve the planning and functioning of a city. As such this concept is taught at ETH Zurich and online\n"}
{"id": "2820241", "url": "https://en.wikipedia.org/wiki?curid=2820241", "title": "Coccobacillus", "text": "Coccobacillus\n\nA coccobacillus (plural coccobacilli) is a type of bacterium with a shape intermediate between cocci (spherical bacteria) and bacilli (rod-shaped bacteria). Coccobacilli, then, are very short rods which may be mistaken for cocci.\n\n\"Haemophilus influenzae\", \"Gardnerella vaginalis\", and \"Chlamydia trachomatis\" are coccobacilli. \"Aggregatibacter actinomycetemcomitans\" is a Gram-negative coccobacillus prevalent in subgingival plaques. \"Acinetobacter\" strains may grow on solid media as coccobacilli. \"Bordetella pertussis\" is a Gram-negative coccobacillus responsible for causing whooping cough. \"Yersinia pestis\", the bacteria that causes plague, is also coccobacillus.\n\n\"Coxiella burnetti\" is also a coccobacillus. Bacteria from the genus \"Brucella\" are medically important coccobacilli that cause brucellosis. \"Haemophilus ducreyi\", another medically important Gram-negative coccobacillus, is observed in sexually transmitted disease, chancroid, of Third World countries.\n"}
{"id": "1186904", "url": "https://en.wikipedia.org/wiki?curid=1186904", "title": "Confocal microscopy", "text": "Confocal microscopy\n\nConfocal microscopy, most frequently confocal laser scanning microscopy (CLSM) or laser confocal scanning microscopy (LCSM), is an optical imaging technique for increasing optical resolution and contrast of a micrograph by means of using a spatial pinhole to block out-of-focus light in image formation. Capturing multiple two-dimensional images at different depths in a sample enables the reconstruction of three-dimensional structures (a process known as optical sectioning) within an object. This technique is used extensively in the scientific and industrial communities and typical applications are in life sciences, semiconductor inspection and materials science.\n\nLight travels through the sample under a conventional microscope as far into the specimen as it can penetrate, while a confocal microscope only focuses a smaller beam of light at one narrow depth level at a time. The CLSM achieves a controlled and highly limited depth of focus.\n\nThe principle of confocal imaging was patented in 1957 by Marvin Minsky and aims to overcome some limitations of traditional wide-field fluorescence microscopes. In a conventional (i.e., wide-field) fluorescence microscope, the entire specimen is flooded evenly in light from a light source. All parts of the specimen in the optical path are excited at the same time and the resulting fluorescence is detected by the microscope's photodetector or camera including a large unfocused background part. In contrast, a confocal microscope uses point illumination (see Point Spread Function) and a pinhole in an optically conjugate plane in front of the detector to eliminate out-of-focus signal – the name \"confocal\" stems from this configuration. As only light produced by fluorescence very close to the focal plane can be detected, the image's optical resolution, particularly in the sample depth direction, is much better than that of wide-field microscopes. However, as much of the light from sample fluorescence is blocked at the pinhole, this increased resolution is at the cost of decreased signal intensity – so long exposures are often required. To offset this drop in signal after the \"pinhole\", the light intensity is detected by a sensitive detector, usually a photomultiplier tube (PMT) or avalanche photodiode, transforming the light signal into an electrical one that is recorded by a computer.\n\nAs only one point in the sample is illuminated at a time, 2D or 3D imaging requires scanning over a regular raster (i.e., a rectangular pattern of parallel scanning lines) in the specimen. The beam is scanned across the sample in the horizontal plane by using one or more (servo controlled) oscillating mirrors. This scanning method usually has a low reaction latency and the scan speed can be varied. Slower scans provide a better signal-to-noise ratio, resulting in better contrast and higher resolution.\n\nThe achievable thickness of the focal plane is defined mostly by the wavelength of the used light divided by the numerical aperture of the objective lens, but also by the optical properties of the specimen. The thin optical sectioning possible makes these types of microscopes particularly good at 3D imaging and surface profiling of samples.\n\nSuccessive slices make up a 'z-stack' which can either be processed by certain software to create a 3D image, or it is merged into a 2D stack (predominately the maximum pixel intensity is taken, other common methods include using the standard deviation or summing the pixels).\n\nConfocal microscopy provides the capacity for direct, noninvasive, serial optical sectioning of intact, thick, living specimens with a minimum of sample preparation as well as a marginal improvement in lateral resolution. Biological samples are often treated with fluorescent dyes to make selected objects visible. However, the actual dye concentration can be low to minimize the disturbance of biological systems: some instruments can track single fluorescent molecules. Also, transgenic techniques can create organisms that produce their own fluorescent chimeric molecules (such as a fusion of GFP, green fluorescent protein with the protein of interest). Confocal microscopes work on the principle of point excitation in the specimen (diffraction limited spot) and point detection of the resulting fluorescent signal. A pinhole at the detector provides a physical barrier that blocks out-of-focus fluorescence. Only the in-focus, or central spot of the airy disk, is recorded. Raster scanning the specimen one point at a time permits thin optical sections to be collected by simply changing the z-focus. The resulting images can be stacked to produce a 3D image of the specimen.\n\nFour types of confocal microscopes are commercially available:\n\nConfocal laser scanning microscopes use multiple mirrors (typically 2 or 3 scanning linearly along the x- and the y- axes) to scan the laser across the sample and \"descan\" the image across a fixed pinhole and detector.\n\nSpinning-disk (Nipkow disk) confocal microscopes use a series of moving pinholes on a disc to scan spots of light. Since a series of pinholes scans an area in parallel, each pinhole is allowed to hover over a specific area for a longer amount of time thereby reducing the excitation energy needed to illuminate a sample when compared to laser scanning microscopes. Decreased excitation energy reduces phototoxicity and photobleaching of a sample often making it the preferred system for imaging live cells or organisms.\n\nMicrolens enhanced or dual spinning-disk confocal microscopes work under the same principles as spinning-disk confocal microscopes except a second spinning-disk containing micro-lenses is placed before the spinning-disk containing the pinholes. Every pinhole has an associated microlens. The micro-lenses act to capture a broad band of light and focus it into each pinhole significantly increasing the amount of light directed into each pinhole and reducing the amount of light blocked by the spinning-disk. Microlens enhanced confocal microscopes are therefore significantly more sensitive than standard spinning-disk systems. Yokogawa Electric invented this technology in 1992.\n\nProgrammable array microscopes (PAM) use an electronically controlled spatial light modulator (SLM) that produces a set of moving pinholes. The SLM is a device containing an array of pixels with some property (opacity, reflectivity or optical rotation) of the individual pixels that can be adjusted electronically. The SLM contains microelectromechanical mirrors or liquid crystal components. The image is usually acquired by a charge coupled device (CCD) camera.\n\nEach of these classes of confocal microscope have particular advantages and disadvantages. Most systems are either optimized for recording speed (i.e. video capture) or high spatial resolution. Confocal laser scanning microscopes can have a programmable sampling density and very high resolutions while Nipkow and PAM use a fixed sampling density defined by the camera's resolution. Imaging frame rates are typically slower for single point laser scanning systems than spinning-disk or PAM systems. Commercial spinning-disk confocal microscopes achieve frame rates of over 50 per second – a desirable feature for dynamic observations such as live cell imaging.\n\nIn practice, Nipkow and PAM allow multiple pinholes scanning the same area in parallel as long as the pinholes are sufficiently far apart.\n\nCutting-edge development of confocal laser scanning microscopy now allows better than standard video rate (60 frames per second) imaging by using multiple microelectromechanical scanning mirrors.\n\nConfocal X-ray fluorescence imaging is a newer technique that allows control over depth, in addition to horizontal and vertical aiming, for example, when analyzing buried layers in a painting.\n\nCLSM is a scanning imaging technique in which the resolution obtained is best explained by comparing it with another scanning technique like that of the scanning electron microscope (SEM). CLSM has the advantage of not requiring a probe to be suspended nanometers from the surface, as in an AFM or STM, for example, where the image is obtained by scanning with a fine tip over a surface. The distance from the objective lens to the surface (called the \"working distance\") is typically comparable to that of a conventional optical microscope. It varies with the system optical design, but working distances from hundreds of micrometres to several millimeters are typical.\n\nIn CLSM a specimen is illuminated by a point laser source, and each volume element is associated with a discrete scattering or fluorescence intensity. Here, the size of the scanning volume is determined by the spot size (close to diffraction limit) of the optical system because the image of the scanning laser is not an infinitely small point but a three-dimensional diffraction pattern. The size of this diffraction pattern and the focal volume it defines is controlled by the numerical aperture of the system's objective lens and the wavelength of the laser used. This can be seen as the classical resolution limit of conventional optical microscopes using wide-field illumination. However, with confocal microscopy it is even possible to improve on the resolution limit of wide-field illumination techniques because the confocal aperture can be closed down to eliminate higher orders of the diffraction pattern. For example, if the pinhole diameter is set to 1 Airy unit then only the first order of the diffraction pattern makes it through the aperture to the detector while the higher orders are blocked, thus improving resolution at the cost of a slight decrease in brightness. In fluorescence observations, the resolution limit of confocal microscopy is often limited by the signal to noise ratio caused by the small number of photons typically available in fluorescence microscopy. One can compensate for this effect by using more sensitive photodetectors or by increasing the intensity of the illuminating laser point source. Increasing the intensity of illumination laser risks excessive bleaching or other damage to the specimen of interest, especially for experiments in which comparison of fluorescence brightness is required. When imaging tissues which are differentially refractive, such as the spongy mesophyll of plant leaves or other air-space containing tissues, spherical aberrations that impair confocal image quality are often pronounced. Such aberrations however, can be significantly reduced by mounting samples in optically transparent, non-toxic perfluorocarbons such as perfluorodecalin, which readily infiltrates tissues and has a refractive index almost identical to that of water .\n\nCLSM is widely used in numerous biological science disciplines, from cell biology and genetics to microbiology and developmental biology. It is also used in quantum optics and nano-crystal imaging and spectroscopy.\n\nClinically, CLSM is used in the evaluation of various eye diseases, and is particularly useful for imaging, qualitative analysis, and quantification of endothelial cells of the cornea. It is used for localizing and identifying the presence of filamentary fungal elements in the corneal stroma in cases of keratomycosis, enabling rapid diagnosis and thereby early institution of definitive therapy. Research into CLSM techniques for endoscopic procedures (endomicroscopy) is also showing promise. In the pharmaceutical industry, it was recommended to follow the manufacturing process of thin film pharmaceutical forms, to control the quality and uniformity of the drug distribution.\n\nCLSM is used as the data retrieval mechanism in some 3D optical data storage systems and has helped determine the age of the Magdalen papyrus.\n\nThe point spread function of the pinhole is an ellipsoid, several times as long as it is wide. This limits the axial resolution of the microscope. One technique of overcoming this is 4 microscopy where incident and or emitted light are allowed to interfere from both above and below the sample to reduce the volume of the ellipsoid. An alternative technique is confocal theta microscopy. In this technique the cone of illuminating light and detected light are at an angle to each other (best results when they are perpendicular). The intersection of the two point spread functions gives a much smaller effective sample volume. From this evolved the single plane illumination microscope. Additionally deconvolution may be employed using an experimentally derived point spread function to remove the out of focus light, improving contrast in both the axial and lateral planes.\n\nThere are confocal variants that achieve resolution below the diffraction limit such as stimulated emission depletion microscopy (STED).\nBesides this technique a broad variety of other (not confocal based) super-resolution techniques are available like PALM, (d)STORM, SIM, and so on. They all have their own advantages such as ease of use, resolution, and the need for special equipment, buffers, or fluorophores.\n\nTo image samples at low temperatures, two main approaches have been used, both based on the laser scanning confocal microscopy architecture. One approach is to use a continuous flow cryostat: only the sample is at low temperature and it is optically addressed through a transparent window. Another possible approach is to have part of the optics (especially the microscope objective) in a cryogenic storage dewar. This second approach, although more cumbersome, guarantees better mechanical stability and avoids the losses due to the window.\n\nIn 1940 Hans Goldmann, ophthalmologist in Bern, Switzerland, developed a slit lamp system to document eye examinations. This system is considered by some later authors as the first confocal optical system.\n\nIn 1943 Zyun Koana published a confocal system.\n\nIn 1951 Hiroto Naora, a colleague of Koana, described a confocal microscope in the journal Science for spectrophotometry.\n\nThe first confocal \"scanning\" microscope was built by Marvin Minsky in 1955 and a patent was filed in 1957. The scanning of the illumination point in the focal plane was achieved by moving the stage. No scientific publication was submitted and no images made with it were preserved.\n\nIn the 1960s, the Czechoslovak Mojmír Petráň from the Medical Faculty of the Charles University in Plzeň developed the Tandem-Scanning-Microscope, the first commercialized confocal microscope. It was sold by a small company in Czechoslovakia and in the United States by Tracor-Northern (later Noran) and used a rotating Nipkow disk to generate multiple excitation and emission pinholes.\n\nThe Czechoslovak patent was filed 1966 by Petráň and Milan Hadravský, a Czechoslovak coworker. A first scientific publication with data and images generated with this microscope was published in the journal Science in 1967, authored by M. David Egger from Yale University and Petráň.\nAs a footnote to this paper it is mentioned that Petráň designed the microscope and supervised its construction and that he was, in part, a \"research associate\" at Yale. A second publication from 1968 described the theory and the technical details of the instrument and had Hadravský and Robert Galambos, the head of the group at Yale, as additional authors.\nIn 1970 the US patent was granted. It was filed in 1967.\n\nIn 1969 and 1971, M. David Egger and Paul Davidovits from Yale University, published two papers describing the first confocal \"laser\" scanning microscope.\nIt was a point scanner, meaning just one illumination spot was generated. It used epi-Illumination-reflection microscopy for the observation of nerve tissue. A 5 mW Helium-Neon-Laser with 633 nm light was reflected by a semi-transparent mirror towards the objective. The objective was a simple lens with a focal length of 8.5 mm. As opposed to all earlier and most later systems, the sample was scanned by movement of this lens (objective scanning), leading to a movement of the focal point. Reflected light came back to the semitransparent mirror, the transmitted part was focused by another lens on the detection pinhole behind which a photomultiplier tube was placed. The signal was visualized by a CRT of an oscilloscope, the cathode ray was moved simultaneously with the objective. A special device allowed to make Polaroid photos, three of which were shown in the 1971 publication.\n\nThe authors speculate about fluorescent dyes for in vivo investigations. They cite Minsky’s patent, thank Steve Baer, at the time a doctoral student at the Albert Einstein School of Medicine in New York City where he developed a confocal line scanning microscope, for suggesting to use a laser with ‘Minsky’s microscope’ and thank Galambos, Hadravsky and Petráň for discussions leading to the development of their microscope. The motivation for their development was that in the Tandem-Scanning-Microscope only a fraction of 10 of the illumination light participates in generating the image in the eye piece. Thus, image quality was not sufficient for most biological investigations.\n\nIn 1977 Colin J. R. Sheppard and Amarjyoti Choudhury, Oxford, UK, published a theoretical analysis of confocal and laser-scanning microscopes.<ref name=\"DOI10.1080/713819421\">C.J.R. Sheppard, A. Choudhury: \"Image Formation in the Scanning Microscope.\" In: \"Optica Acta: International Journal of Optics.\" 24, 1977, S. 1051–1073, .</ref> It is probably the first publication using the term \"confocal microscope.\"\n\nIn 1978, the brothers Christoph Cremer and Thomas Cremer published a design for a confocal laser-scanning-microscope using fluorescent excitation with electronic autofocus. They also suggested a laser point illumination by using a „4π-point-hologramme“. This CLSM design combined the laser scanning method with the 3D detection of biological objects labeled with fluorescent markers for the first time.\n\nIn 1978 and 1980, the Oxford-group around Colin Sheppard and Tony Wilson described a confocal with epi-laser-illumination, stage scanning and photomultiplier tubes as detectors. The stage could move along the optical axis (z-axis), allowing optical serial sections.\n\nIn 1979 Fred Brakenhoff and coworkers demonstrated that the theoretical advantages of optical sectioning and resolution improvement are indeed achievable in practice. In 1985 this group became the first to publish convincing images taken on a confocal microscope that were able to answer biological questions. Shortly after many more groups started using confocal microscopy to answer scientific questions that until now had remained a mystery due to technological limitations.\n\nIn 1983 I. J. Cox und C. Sheppard from Oxford published the first work whereby a confocal microscope was controlled by a computer.\nThe first commercial laser scanning microscope, the stage-scanner SOM-25 was offered by Oxford Optoelectronics (after several take-overs acquired by BioRad) starting in 1982. It was based on the design of the Oxford group.\n\nIn the mid-1980s, William Bradshaw Amos and John Graham White and colleagues working at the Laboratory of Molecular Biology in Cambridge built the first confocal beam scanning microscope.\nHugely magnified intermediate images, due to a 1-2 meter long beam path, allowed the use of a conventional iris diaphragm as a ‘pinhole’, with diameters ~1 mm. First micrographs were taken with long-term exposure on film before a digital camera was added. A further improvement allowed zooming into the preparation for the first time. Zeiss, Leitz and Cambridge Instruments had no interest in a commercial production. The Medical Research Council (MRC) finally sponsored development of a prototype. The design was acquired by Bio-Rad, amended with computer control and commercialized as ‘MRC 500’. The successor MRC 600 was later the basis for the development of the first two-photon-fluorescent microscope developed 1990 at Cornell University.\n\nDevelopments at the University of Stockholm around the same time led to a commercial CLSM distributed by the Swedish company Sarastro. The venture was acquired in 1990 by Molecular Dynamics, but the CLSM was eventually discontinued. In Germany, Heidelberg Instruments, founded in 1984, developed a CLSM which was initially meant for industrial applications, rather than Biology. This instrument was taken over in 1990 by Leica Lasertechnik. Zeiss already had a non-confocal flying-spot laser scanning microscope on the market which was upgraded to a confocal. A report from 1990, mentioning “some” manufacturers of confocals lists: Sarastro, Technical Instrument, Meridian Instruments, Bio-Rad, Leica, Tracor-Northern and Zeiss.\n\nIn 1989, Fritz Karl Preikschat, with son Ekhard Preikschat, invented the scanning laser diode microscope for particle-size analysis. He and Ekhard Preikschat co-founded Lasentec to commercialize it. In 2001, Lasentec was acquired by Mettler Toledo (NYSE: MTD). About ten thousand systems have been installed globally, mostly in the pharmaceutical industry to provide in-situ control of the crystallization process in large purification systems.\n\n\n\n"}
{"id": "20412506", "url": "https://en.wikipedia.org/wiki?curid=20412506", "title": "Cooperative web", "text": "Cooperative web\n\nThe Cooperative Web or Co-Web refers to a browser-based platform that promises to replicate the power of face-to-face communications via web-touch without sacrificing the quality of human interactions. A Co-Web enabled\nsituational application exploits direct high-definition video mixed with web based telepresence to further increase conversational productivity. The objective of the Cooperative Web is to enrich collaborative web meetings with a browser metaphor that supports simultaneous interactions between meeting participants.\n\nA convergence is occurring between various technologies associated with the notion of live web meetings. The phrase Web conferencing has been used to describe group discussions over the internet. These discussions are often implemented using Synchronous conferencing protocols and are commonly used for webinars, where one meeting participant lectures to other participants while presenting some information that is rendered to all participants by a common client application (web or fat client). The term Telepresence refers to a set of technologies which allow a person to\nfeel as if they were present, to give the appearance that they were present, or to have an effect, at a location other than their true location. Telepresence requires that the senses of the user, or users, are provided with such\nstimuli as to give the feeling of being in that other location. Additionally, the user(s) may be given the ability to affect the remote location. As the analysts at IDC describe, the goal is to create a sensory experience that communicates the full range of human interactions in a live meeting. \n\nWhile many Telepresence solutions have focused on the ambiance aspects of remote meeting environments and while\nmost web conferencing solutions have focused on the integration of webinar and telephony features into collaboration software offerings/services, little has been done to simulate or reflect the asynchronous or simultaneous aspects of \nlive meetings over the internet. Ideally what is required is a set of technologies that enrich communications with sensory elements that provide a \"just-like-being-there\" experience for live meetings. The sensory experience should \ninclude a range of sight, sound, and touch interactions. However, the current state of web conferencing and \ntelepresence solutions focus mainly on the sight and sound aspects of a meeting and fall short on the interaction capabilities of participants. Moreover, web conferencing and telepresence solutions are typically not vendor neutral and tend to be pricey.\n\nUsers have the ability to incorporate webinar capabilities into an Immersive or Adaptive Telepresence solution to extend meeting attendance. Yet this injection of Telepresence Lite capabilities all along the Telepresence solution spectrum still does not address support for simultaneous interactions with the material being presented in a meeting. \n\nOrthogonal to this convergence in the technologies that support live web meetings, the web browser platform has evolved to a point whereby the mediation of user interactions amongst meeting attendees is possible. The common browser has raised the bar of expectation by users. Regardless of your browser of choice, your ability to access web applications simply and efficiently has become the norm. The overall browser experience has improved due in part by the broad adoption of web standards by browser providers and by the sheer economics and reach of the web browser platform. Essentially, the browser has evolved into the ubiquitous application container for the web.\n\nPeople meet, gather, huddle for business and personal endeavors. The majority of the discussions associated with these conferencing activities tend to be associated with decisions. Typically, the decision making process incorporates the analysis of one or more visualizations of data. The Cooperative Web pertains to a set of technologies and associated architectures that promise to empower decision agility with respect to information available for evaluation\nvia web-based applications.\n\nWhile Cooperative Web solutions can incorporate digital components (audio, video) to replicate the face-face meeting experience with the human sensory elements of sight and sound, the interactive sensory element is a differentiating factor. A core value proposition of this technology is that it provides all meeting participants with the ability to manipulate data and drive web centric applications that are used in the decision making process.\n\nIn consideration to IDC's call for the ability to develop a low-cost plug-and-play Telepresence solution that can be easily adopted by medium-sized companies to extend the reach and promise of Telepresence, the\nCooperative Web offers solution vendors and composite web application developers the ability to develop standards based web applications that can be used in collaborative communication-oriented meetings.\n\n\n\nThe maturity of service-oriented-architectures has fueled an ecosystem of ajax-based gadgets (widgets) that encapsulate content services (REST, WSDL). These gadgets, standalone fragments of a web page, make up the information rich web palette that can be assembled, wired and shared in composite web applications.\n\nLets assume that the gadgets used in a given web page all publish the events associated with user-interactions to a predefined mediation server whereby all participants in a managed web meeting were notified of the interactions. Now,\neach instance of the gadgets in the web page (running in each meeting participants browser) would receive these remote interaction events as if they were locally triggered. The result would be that each meeting participant would broadcast his/her web page interactions and also subscribe to the results associated with interactions of other meeting participants.\n\nThe following attributes most common associated with Telepresence solutions are\n\nthe reality is that many of top selling solutions focus more on the features associated with sight and sound and complementary ambiance aspects like high-end furniture. However, few focus on extending the Telepresence experience with interactive presentation capabilities. \"Cooperative Web\" enabled applications are complementary to the spectrum of Telepresence solutions as they extend the scope of the browser-based platform to allow all meeting participants to share browser applications in a live online meeting.\n\nAs described by Gartner, the spectrum of Telepresence solutions can range from Lite to Adaptive to Immersive. Cooperative Web enabled applications can be leveraged along all points on the solution spectrum to help enrich the \"just-like-being-there\" aspects of the Telepresence experience.\n\nTypically, sophisticated technologies are required for a user to be given a convincing telepresence experience.However, one of the benefits of Cooperative Web solutions is the ubiquity of the technology dependencies. The cost of entry for adoption has been reduced to a common browser, a few JavaScript libraries and some browser plug-ins. The net result is that the Cooperative Web not only extends the reach of Telepresence solutions, this technology also improves the overall experience of remote communications. \n\nHowever, in its raw state, a Co-Web enabled application does not require a Telepresence environment nor a hosted Web Conferencing solution. The minimum requirement is a mediation server and a co-web enabled application.\n\n\n"}
{"id": "28017162", "url": "https://en.wikipedia.org/wiki?curid=28017162", "title": "Derek Lowe (chemist)", "text": "Derek Lowe (chemist)\n\nDerek Lowe is a medicinal chemist working on preclinical drug discovery in the pharmaceutical industry, and has written a blog about his field called \"In the Pipeline\" since 2002.\n\nLowe (born in Harrisburg, Arkansas) got his BA from Hendrix College and his PhD in organic chemistry from Duke University on synthesis of natural products, before spending time in Germany on a Humboldt Fellowship.\n\nLowe was the one of the first people to blog from inside the pharmaceutical industry (he talked to his supervisor and the company legal department before starting), and one of the first science bloggers. By 2006, his blog had between 3,000 and 4,000 visitors per day during the workweek; he covered business matters, trends and issues in medicinal chemistry, and legal matters like patent law and regulation. At that time he was working at a pharmaceutical chemistry doing hit to lead medical chemistry work. his blog received between 15,000 and 20,000 page views on a typical weekday. His response to a 2013 article in Buzzfeed that propagated chemophobia was widely cited.\n\nHe serves on the editorial board of \"ACS Medicinal Chemistry Letters\" and on the advisory board of \"Chemical & Engineering News\".\n\n\n\n"}
{"id": "35049423", "url": "https://en.wikipedia.org/wiki?curid=35049423", "title": "Epidemiology of representations", "text": "Epidemiology of representations\n\nEpidemiology of representations, or cultural epidemiology, provides a conceptual framework for explaining cultural phenomena by how mental representations get distributed within a population. The theory appeals to an analogy with medical epidemiology; because \"...macro-phenomena such as endemic and epidemic diseases are unpacked in terms of patterns of micro-phenomena of individual pathology and inter-individual transmission\".\nRepresentations get transferred via so called cognitive causal chains (cf. Table 1). The stability of public productions and mental representations (constituting a cultural phenomenon) is explained via ecological and psychological factors. The latter include properties of the human mind and cultural epidemiologists have emphasized the significance of evolved properties: the existence of naïve theories, domain-specific abilities, principles of relevance.\n\nThe theory has been formulated mainly by the French social and cognitive scientist Dan Sperber for the study of society and culture, by taking into account evidence from anthropology and cognitive science.\n\nA cognitive causal chain (CCC) links a mental representation (e.g. satisfaction, justification, truth-value or similarity of content) with individual behaviors and mental processes (e.g. perception, inference, remembering, and the carrying out of an intention). More generally, it links something that can be perceived with the evolved and domain-specific process that makes it perceivable; for example, a visual perception of a stimulus that leads to a mental representation of the stimulus that triggered it.\n\nHere is an example of a CCC:\n\nSocial cognitive causal chains (SCCCs) are inter-individual CCCs. A SCCC always implies individual CCCs but a CCC just leads to a SCCC if it involves an inter-individual act of communication or other effective forms of non-communicative interaction.\n\nHere is an example of a SCCC involving an act of communication (ringing a doorbell):\n\nHuman interaction involves many cases of ad-hoc cultural cognitive causal chains (SCCCs) that do not follow a significant long-term pattern over many people. Yet, other SCCCs can be long-lasting, systematical, and across a large population; for example, the Halloween custom mentioned above. The latter kinds of SCCCs arguably stabilize mental representations intra- and inter-individually to an extent that they can be considered as cultural if their behavior (practices and resulting artifacts) significantly represent their population. A CCCC therefore always implies SCCCs but a SCCC just leads to a CCCC if it involves one or more SCCCs that significantly indicate mental representations or public productions.\n\nTable 1: Overview of cognitive causal chains constituting the epidemiology of representations (from ).\n\nEpidemiology of representations suggests that both cultural diversity and stability (macro-level) together can be explained by the massive modularity of the human brain and mind (micro-level) and SCCCs. This means that the manifold of human cultural behavior is ultimately explained by the manifold of domain-specific human cognitive abilities (mental representations) and respective SCCC. This claim would have broad impact, when applicable. It is discussed in further detail by Sperber and Hirschfeld for the cases of folkbiology, folksociology, and supernaturalism.\nHere is an example: Think about a human cognitive sub-system that must have been very important for human cognitive evolution (i.e. a module with an innate basis); like the ability that allows humans to recognize and interpret visual patterns as faces. One can call this cognitive sub-system the human \"face recognition module\". It was most likely built by evolution to recognize and interpret animal faces via decoding facial expressions produced by a complex system of facial muscles. Humans with certain types of brain damage lose this ability (c.f. prosopagnosia, Social-Emotional Agnosia). Yet, the module also processes visual input that is relatively similar to patterns of natural faces. Such can be cultural artifacts like portraits, caricatures, masks, and made-up faces.\n\nAccording to the epidemiology of representation the effectiveness (defined by its relevance) of a public production depends on the extent to which it exploits a human cognitive module. Cultural artifacts \"...rely on and exploit a natural disposition. Often, they exaggerate crucial features, as in caricature or in make up, and constitute what ethologists call 'superstimuli.'\" Two domains of cognitive modules can therefore be distinguished: the proper, natural domain and the actual, cultural domain. In the above example, the first relates to natural faces, the second to portraits, caricatures, masks, and made-up faces. Those categories can intersect, like shown in Figure 2. Since made-up faces literally overlap the proper with the actual domain, they are the most effective and relevant public product in the example. Therefore, they lead to the most stable CCCC, if they significantly reflect the population's behavior. The other stimuli in the cultural domain will theoretically be as efficient and stable depending on the extent that they exploit the \"face perception module\".\n\nEpidemiology of representations states that there must be a SCCC, the mechanism inter-linking a mental representation with an individual behavior, for the latter and explains its stability over time and space by the relevance theoretical status of the underlying behavior. There are three minimal conditions for an inter-individual replication that ensures transfer stability.\n\nFor \"b\" to be an actual replication of \"a\",\n\nHere is an example:\n\nImagine one (A) produces a line-drawing (a), see Figure 3, and then shows it to a friend (B) for ten seconds. An asks the friend ten minutes later to reproduce it as exactly as possible with another line-drawing (b). After that, a second person is shown for ten seconds the figure drawn by your friend and presented with the same task. This is iterated with nine further participants. Now, theoretically, it is most likely that each drawing will differ from its model (a) and that the more distant two drawings are in the chain, the more they are likely to differ. Imagine further, you conduct exactly the same little experiment, but with the line-drawing in Figure 4. The result, you theoretically get this time, may be such that \"...the distance in the chain of two drawings on the one hand, and their degree of difference on the other hand should be two variables independent of one another.\"; meaning that it was a chain of stable replications.\n\nTheoretically, this is because Figure 4 looks like a five-branched star, drawn without lifting the pencil, whereas Figure 3 has no perceivable meaning (at least in our western culture), the second chain is a SCCC but not so the first. Arguably and on the one hand, the second causal chain was driven by perceiving the shared meaning of the stimulus by inferring the underlying mental representation and a sequential reproduction of by new behavior. On the other hand, the first causal chain was driven by mere imitation that does not crucially depends on recognition of the underlying meaning. There are forms of inter-individual transfer of behavior that blend reproduction and imitation to different extends. However, the more the meaning of a stimulus is actually reproduced rather than imitated by a subject, the more stable the transfer of the underlying mental representation is supposed to be over time.\n\nLike cognitive science, the epidemiology of representations is also based on the assumption that domain specificity characterizes cognitive abilities or mechanisms. Epidemiology of representations assumes that human animals are cognitively predefined by their evolved neurophysiology (i.e. their cognition is massively modular). However, it also acknowledges that cognitive development plays a functional role for the formation of mental representations, concepts, intuitive theories, and the like.\nAccordingly, theories in cognitive science argue that humans are evolutionarily equipped with a certain brain-body setup (c.f. common coding theory) that allows them to encode and decode specific kinds of information to their memory via interacting with their environment. This is sensory-specific (i.e. visual-, acoustic-, tactile-, and olfactory perception etc.) and task or reasoning specific (i.e. formulation of intuitive theories). Hence, by these theories, humans are assumed to have (1) an innate cognitive potential that (2) is realized during a natural cognitive development.\nThe reason for the first argument comes from fields like evolutionary anthropology and evolutionary psychology, stating that evolution has been selecting merely those animals that have evolved adaptive mental and neural mechanisms to efficiently cope with specific challenges regarding their environment (e.g. getting food, shelter, mates, etc.). The reason for the second argument comes from cognitive development, stating that animals (especially humans) in their infancy are highly sensitive to input patterns, since their cognitive system automatically and rapidly adapts their environment.\n\nSince the human brain is organized into areas that focus on the processing of distinct sensory input and output and also interact with one another, humans are assumed to learn and perform best in processing those patterns of information for which their neurophysiological system has been evolved. Mental representations manifest, for example, in human long-term memory (Figure 1). Other evidence for massive modularity is that human cognitive performance for respective domains correlates with the degree of damage to the corresponding cortical areas.\n\nThe cognitive approach in the epidemiology of representations differs from other philosophical theories with evolutionary orientation, such as memetics, formulated by the British ethologist and evolutionary biologist Richard Dawkins (cf.). Roughly speaking, the three crucial differences between the two approaches are the following:\n\nWith the epidemiology of representations, Sperber has argued that the notion of meaning is disregarded in memetics and that this is questionable since the study of society and culture without an explanation of how meaning is perceived and reproduced is contradictory. If memetics, nevertheless, attempts to explain culture based on evolutionary biology, it will need to present empirical evidence for the transfer of memes, that is \"...showing that elements of culture inherit all or nearly all their relevant properties from other elements of culture that they replicate \".\n"}
{"id": "39271235", "url": "https://en.wikipedia.org/wiki?curid=39271235", "title": "Far Ultraviolet Camera/Spectrograph", "text": "Far Ultraviolet Camera/Spectrograph\n\nThe Far Ultraviolet Camera/Spectrograph (UVC) was one of the experiments deployed on the lunar surface by the Apollo 16 astronauts. It consisted of a telescope and camera that obtained astronomical images and spectra in the far ultraviolet region of the electromagnetic spectrum.\n\nThe Far Ultraviolet Camera/Spectrograph was a tripod mounted, f/1.0, 75 mm electronographic Schmidt camera weighing 22 kg. It had a 20° field of view in the imaging mode, and 0.5x20° field in the spectrographic mode. Spectroscopic data were provided from 300 to 1350 Ångström, with 30 Å resolution, and images were provided in two passbands ranges, 1050-1260 Å and 1200-1550 Å. There were two corrector plates made of lithium fluoride (LiF) or calcium fluoride (CaF), which could be selected for different bands of UV. The camera contained a cesium iodide (CsI) photocathode and used a film cartridge which was recovered and returned to earth for processing.\n\nThe experiment was placed on the Descartes Highlands region of lunar surface where Apollo 16 astronauts John Young and Charles Duke landed in April 1972. To keep it cool and eliminate solar glare, it was placed in the shadow of the lunar module. It was manually aimed by the astronauts, who would re-point the telescope at targets throughout the lunar stay.\n\nThe goals of the Far Ultraviolet Camera/Spectrograph spanned across several disciplines of astronomy. Earth studies were made by studying the Earth's upper atmosphere's composition and structure, the ionosphere, the geocorona, day and night airglow, and aurorae. Heliophysics studies were made by obtaining spectra and images of the solar wind, the solar bow cloud, and other gas clouds in the solar system. Astronomical studies by obtaining direct evidence of intergalactic hydrogen, and spectra of distant galaxy clusters and within the Milky Way. Lunar studies were conducted by detecting gasses in the lunar atmosphere, and searching for possible volcanic gasses. There were also considerations to evaluate the lunar surface as a site for future astronomical observatories.\n\nThe film cartridge was removed during the third and final extravehicular activity, and returned to earth. The rest of the instrument package was left on the lunar surface. A total of 178 frames of film were obtained of 11 different targets including: the Earth's upper atmosphere and aurora, various nebulae and star clusters, and the Large Magellanic Cloud.\n\nThe film was digitally scanned and saved on tape. Files from these tapes can be requested from NASA. Most of the Apollo 16 and Skylab photos have been converted to JPGs by a third party enthusiast and can be viewed here:\n\nApollo 16: https://archive.org/details/AS16-123-JPG\n\nSkylab: https://archive.org/details/Skylab-S019-JPG\n\nUnconverted files have been uploaded as well.\n\nThe principal investigator and chief engineer of the Far Ultraviolet Camera/Spectrograph was Dr. George Robert Carruthers, who was working at the US Naval Research Lab. In 1969, Dr. Carruthers was given a patent for \"Image Converter for Detecting Electromagnetic Radiation Especially in Short Wave Lengths\". For this and his further work, he received the 2012 National Medal of Technology and Innovation.\n\nA second spare telescope was slightly modified and later flown on Skylab 4. It was given an aluminum (Al) and magnesium fluoride (MgF) mirror rather than rhenium. It was mounted on Skylab's Apollo Telescope Mount for usage in orbit. Among the many images and spectra that it took, it was used to study ultraviolet emission from Comet Kohoutek.\n\n\n"}
{"id": "524600", "url": "https://en.wikipedia.org/wiki?curid=524600", "title": "Greater Yellowstone Ecosystem", "text": "Greater Yellowstone Ecosystem\n\nThe Greater Yellowstone Ecosystem (GYE) is one of the last remaining large, nearly intact ecosystems in the northern temperate zone of the Earth. It is located within the northern Rocky Mountains, in areas of northwestern Wyoming, southwestern Montana, and eastern Idaho, and is about 18 million acres. Yellowstone National Park and the Yellowstone Caldera 'hotspot' are within it.\n\nConflict over ecological and resource management has been controversial, and the area is a flagship site among conservation groups that promote ecosystem management. The Greater Yellowstone Ecosystem (GYE) is one of the world's foremost natural laboratories in landscape ecology and Holocene geology, and is a world-renowned recreational destination. It is also home to the diverse native plants and animals of Yellowstone.\n\nYellowstone National Park boundaries were drawn in 1872 to include all the known geothermal basins in the region. No other landscape ecology considerations were incorporated into boundary decisions. By the 1970s, however, the grizzly bear's (\"Ursus arctos\") range in and near the park became the first informal minimum boundary of a theoretical \"Greater Yellowstone Ecosystem\" that included at least . Since then, definitions of the greater ecosystem's size have steadily grown larger. A 1994 study listed the size as , while a 1994 speech by a Greater Yellowstone Coalition leader enlarged that to .\n\nIn 1985 the United States House of Representatives Subcommittees on Public Lands and National Parks and Recreation held a joint subcommittee hearing on Greater Yellowstone, resulting in a 1986 report by the Congressional Research Service outlining shortcomings in inter-agency coordination and concluding that the area's essential values were at risk.\n\nFederally managed areas within the GYE include: \n\nTen distinct National Wilderness Areas have been established within the GYE's National Forests since 1966, mandating a higher level of habitat protection than the USFS otherwise uses.\n\nThe GYE also encompasses some privately held and state lands surrounding those managed by the U.S. Government.\n\nThe Trust for Public Land has protected 67,000 acres over about 40 projects in the Greater Yellowstone Ecosystem.\n\nThe great ecosystem concept has been most often advanced through concerns over individual species rather than over broader ecological principles. Though 20 or 30 or even 50 years of information on a population may be considered long-term by some, one of the important lessons of Greater Yellowstone management is that even half a century is not long enough to give a full idea of how a species may vary in its occupation of a wild ecosystem.\n\nThe Yellowstone hot springs are important for their diversity of thermophilic bacteria. These bacteria have been useful in studies of the evolution of photosynthesis and as sources of thermostable enzymes for molecular biology. Although the smell of sulfur is common and there are some sulfur fixing cyanobacteria, it has been found that hydrogen is being used as an energy source by extremophile microbes.\n\nAmong native plants of the GYE, whitebark pine (\"Pinus albicaulis\") is a species of special interest, in large part because of its seasonal importance to grizzly bears, but also because its distribution could be dramatically reduced by relatively minor global warming. In this case, researchers do not have a good long-term data set on the species, but they understand its ecology well enough to project declining future conservation status. A more immediate and serious threat to whitebark pines is an introduced fungal rust disease, White Pine Blister Rust (\"Cronartium ribicola\"), which is causing heavy mortality in the species. Occasional resistant individuals occur, but in the short to medium term, a severe population decline is expected.\n\nEstimates of the decline of quaking aspen (\"Populus tremuloides\") on the park's northern range since 1872 range from 50% to 95%. Perhaps no conservation controversy underway in Greater Yellowstone more clearly reveals the need for comprehensive interdisciplinary research.<br>\nSeveral factors are suspected in the quaking aspen's changing status, including: \n\nAnecdotal information on grizzly bear abundance dates to the mid-19th century, and administrators have made informal population estimates for more than 70 years. From these sources, ecologists know the species was common in Greater Yellowstone when Europeans arrived and that the population was not isolated before the 1930s, but is now. Researchers do not know if bears were more or less common than now.\n\nA 1959-1970 bear study suggested a grizzly bear population size of about 176, later revised to about 229. Later estimates have ranged as low as 136 and as high as 540; the most recent is a minimum estimate of 236, but biologists think there may be as many as 1,000 bears in the ecosystem. Although the Greater Yellowstone population is relatively close to recovery goals, the plan's definition of recovery is controversial. Thus, even though the population may be stable or possibly increasing in the short term, in the longer term, continued habitat loss, climate change, and increasing human activities may well reverse the trend.\n\nYellowstone cutthroat trout (\"Oncorhynchus clarki bouvieri\") have suffered considerable declines since European settlement, but recently began flourishing in some areas. Especially in Yellowstone Lake itself, long-term records indicate an almost remarkable restoration of robust populations from only three decades ago when the numbers of this fish were depleted because of excessive harvest. Its current recovery, though a significant management achievement, does not begin to restore the species' historical abundance. Also, they declined because of invasive lake trout. An aggressive lake trout removal program has caused the cutthroats to rebound.\n\nEarly accounts of pronghorn (\"Antilocapra americana\") in Greater Yellowstone described herds of hundreds seen ranging through most major river valleys. These populations were decimated by 1900, and declines continued among remaining herds. On the park's northern range, pronghorn declined from 500-700 in the 1930s to about 122 in 1968. By 1992 the herd had increased to 536.\n\nThe park is a commonly cited example of apex predators affecting an ecosystem through a trophic cascade. After the reintroduction of the gray wolf in 1995, researchers noticed drastic changes occurring. Elk, the primary prey of the gray wolf, became less abundant and changed their behavior, freeing riparian zones from constant grazing. The respite allowed willows and aspens to grow, creating habitat for beaver, moose, and scores of other species. In addition to the effects on prey species, the gray wolf's presence also affected the park's grizzly bear population. The bears, emerging from hibernation, chose to scavenge off wolf kills to gain needed energy and fatten up after fasting for months. Dozens of other species have been documented scavenging from wolf kills.\n\n\n"}
{"id": "6156068", "url": "https://en.wikipedia.org/wiki?curid=6156068", "title": "Heather Reid", "text": "Heather Reid\n\nDr Heather Margaret Murray Reid OBE CPhys FInstP FRMetS (born c. 1969), also known as \"Heather the Weather\", is a Scottish meteorologist, physicist, science communicator and educator. She was formerly a broadcaster and weather presenter for BBC Scotland. \n\nReid was born in Paisley. She graduated from the University of Edinburgh with an honours degree in physics followed by a master's degree in satellite image processing from the University's Meteorology Department. In 1993 Reid took a position at the UK Met Office working in satellite research. From 1994 to 2009 she worked as a weather presenter for BBC Scotland, presenting on Reporting Scotland. She became BBC Scotland's senior weather forecaster, and gave her final broadcast for \"Reporting Scotland\" on 22 December 2009. Reid now works as a science education consultant, and in the public promotion of science.\n\nFrom 1999 to 2001 Reid was Chairperson of the Institute of Physics in Scotland, she has also served as a Council member and non-executive director of the Institute in London.\n\nIn the summer of 2006 Reid became a member of the Board of Trustees of Glasgow Science Centre, maintaining a connection that has seen her involved in various stages with the Centre, notably developing educational weather shows and workshops.\n\nReid is a member of the Science and Engineering Education Advisory Group set up by the Scottish Government.\n\nReid is a Fellow of the Royal Meteorological Society, and has an honorary lectureship in the Physics and Astronomy Department at the University of Glasgow. In 2001 she was awarded the Public Awareness of Physics prize from the Institute of Physics for her science communication work, and in 2004 received the Institute's prestigious Kelvin Medal. In 2003 Reid received an honorary doctorate from Paisley University, and on 16 June 2010 she was awarded an honorary doctorate by the University of Glasgow.\n\nReid was awarded an Order of the British Empire (OBE) for services to physics in the 2007 New Year Honours list, announced on 30 December 2006. \n\nEdinburgh band Randan Discothèque's 2010 single \"Heather the Weather\" is a homage to Heather Reid.\n\nReid resides in Glasgow with her husband Miles Padgett, a professor of physics, and their daughter.\n\n\n \n"}
{"id": "26391771", "url": "https://en.wikipedia.org/wiki?curid=26391771", "title": "Henry Stommel Research Award", "text": "Henry Stommel Research Award\n\nThe Henry Stommel Research Award is awarded by the American Meteorological Society to researchers in recognition of outstanding contributions to the advancement of the understanding of the dynamics and physics of the ocean. The award is in the form of a medallion and was named for Henry Stommel.\n\nA. The information in the table is according to the \"Past winners\" web page at the official website of the American Meteorological Society, unless otherwise specified by additional citations. (Enter award name only and click\nsubmit)\n\n"}
{"id": "1676370", "url": "https://en.wikipedia.org/wiki?curid=1676370", "title": "History of political science", "text": "History of political science\n\nPolitical science as a separate field is a rather late arrival in terms of social sciences. However, the term \"political science\" was not always distinguished from political philosophy, and the modern discipline has a clear set of antecedents including also moral philosophy, political economy, political theology, history, and other fields concerned with normative determinations of what ought to be and with deducing the characteristics and functions of the ideal state.\n\nThe antecedents of Western politics can be traced back to the Socratic political philosophers, and Aristotle (\"The Father of Political Science\") (384–322 BC). These authors, in such works as \"The Republic\" and \"Laws\" by Plato, and \"The Politics\" and \"Nicomachean Ethics\" by Aristotle, analyzed political systems philosophically, going beyond earlier Greek poetic and historical reflections which can be found in the works of epic poets like Homer and Hesiod, historians like Herodotus and Thucydides, and dramatists such as Sophocles, Aristophanes, and Euripides.\n\nDuring the height of the Roman Empire, famous historians such as Polybius, Livy and Plutarch documented the rise of the Roman Republic, and the organization and histories of other nations, while statesmen like Julius Caesar, Cicero and others provided us with examples of the politics of the republic and Rome's empire and wars. The study of politics during this age was oriented toward understanding history, understanding methods of governing, and describing the operation of governments. Nearly a thousand years elapsed, from the foundation of the city of Rome in 753 BC to the fall of the Western Roman Empire or the beginning of the Middle Ages. In the interim, there is a manifest translation of Hellenic culture into the Roman sphere. The Greek gods become Romans and Greek philosophy in one way or another turns into Roman law e.g. Stoicism. The Stoic was committed to preserving proper hierarchical roles and duties in the state so that the state as a whole would remain stable. Among the best known Roman Stoics were philosopher Seneca and the emperor Max Aurelius. Seneca, a wealthy Roman patrician, is often criticized by some modern commentators/historians for failing to adequately live by his own precepts. The Meditations of Marcus Aurelius, on the other hand, can be best thought of as the philosophical reflections of an emperor divided between his philosophical aspirations and the duty he felt to defend the Roman Empire from its external enemies through his various military campaigns. According to Polybius, Roman institutions were the backbone of the empire but Goldman Law is the medulla.\n\nWith the fall of the Western Roman Empire, there arose a more diffuse arena for political studies. The rise of monotheism and, particularly for the Western tradition, Christianity, brought to light a new space for politics and political action. Works such as Augustine of Hippo's \"The City of God\" synthesized current philosophies and political traditions with those of Christianity, redefining the borders between what was religious and what was political. During the Middle Ages, the study of politics was widespread in the churches and courts. Most of the political questions surrounding the relationship between church and state were clarified and contested in this period.\n\nThe Arabs lost sight of Aristotle's political science but continued to study Plato's \"Republic\" which became the basic text of Judeo-Islamic political philosophy as in the works of Alfarabi and Averroes; this did not happen in the Christian world, where Aristotle's \"Politics\" was translated in the 13th century and became the basic text as in the works of Saint Thomas Aquinas.\n\nDuring the Italian Renaissance, Niccolò Machiavelli established the emphasis of modern political science on direct empirical observation of political institutions and actors. In the Prince Machiavelli posits a realist viewpoint, arguing that even evil means should be considered if they help to create and preserve a desired regime. Machiavelli therefore also argues against the use of idealistic models in politics, and has been described as the father of the \"politics model\" of political science. Machiavelli takes a different tone in his lesser known work, the Discourses of Livy. In this work he expounds on the virtues of republicanism and what it means to be a good citizen. Later, the expansion of the scientific paradigm during the Enlightenment further pushed the study of politics beyond normative determinations.\n\nThe works of the French philosophers Voltaire, Rousseau, Diderot to name a few are paragon for political analysis, social science, social and political critic. Their influence leading to the French revolution has been enormous in the development of modern democracy throughout the world.\n\nLike Machiavelli, Thomas Hobbes, well known for his theory of the social contract, believed that a strong central power, such as a monarchy, was necessary to rule the innate selfishness of the individual but neither of them believed in the divine right of kings. John Locke, on the other hand, who gave us Two Treatises of Government and who did not believe in the divine right of kings either, sided with Aquinas and stood against both Machiavelli and Hobbes by accepting Aristotle's dictum that man seeks to be happy in a state of social harmony as a social animal. Unlike Aquinas' preponderant view on the salvation of the soul from original sin, Locke believed man comes into this world with a mind that is basically a tabula rasa. According to Locke, an absolute ruler as proposed by Hobbes is unnecessary, for natural law is based on reason and equality, seeking peace and survival for man.\n\nThe new Western philosophical foundations that emerged from the pursuit of reason during the Enlightenment era helped pave the way for policies that emphasized a need for a separation of church and state. Principles similar to those that dominated the material sciences could be applied to society as a whole, originating the social sciences. Politics could be studied in a laboratory as it were, the social milieu. In 1787, Alexander Hamilton wrote: \"...The science of politics like most other sciences has received great improvement.\" (\"The Federalist Papers\" Number 9 and 51). Both the marquis d'Argenson and the abbé de Saint-Pierre described politics as a science; d'Argenson was a philosopher and de Saint-Pierre an allied reformer of the enlightenment.\n\nOther important figures in American politics who participated in the Enlightenment were Benjamin Franklin and Thomas Jefferson.\n\nThe Darwinian models of evolution and natural selection exerted considerable influence in the late 19th century. Society seemed to be evolving ever upward, a belief that was shattered by World War I.\n\n\"History is past politics and politics present history\" was the motto of the first generation of American political scientists, 1882-1900. The motto had been coined by the Oxford professor Edward Augustus Freeman, and was enshrined on the wall of the seminar room at Johns Hopkins University where the first large-scale training of America and political scientists began. The founding professors of the field included Herbert Baxter Adams at Johns Hopkins, John Burgess and William Dunning at Columbia, Woodrow Wilson at Princeton, and Albert Bushnell Hart at Harvard. Their graduate seminars had a thick historical cast, which typically reflected their experience in German University seminars. However, succeeding generations of scholars progressively cut back on the history and deliberate fashion. The second generation wanted to model itself on the physical sciences.\n\nIn the Progressive Era in the United States (1890s-1920s), political science became not only a prestigious university curriculum but also an applied science that was welcomed as a way to apply expertise to the problems of governance. Among the most prominent applied political scientists were Woodrow Wilson, Charles A. Beard, and Charles E. Merriam. Many cities and states set up research bureaus to apply the latest results.\n\nThe American Political Science Association, established in 1903, is the largest professional association of political scientists.\n\nBehavioralism (Behaviouralism) is an empirical approach which emerged in the 1930s in the United States. It emphasized an objective, quantified approach to explain and predict political behavior. Guy says \"Behaviouralism emphasized the systematic understanding of all identifiable manifestations of political behaviour. But it also meant the application of rigorous scientific and statistical methods to standardize testing and to attempt value free inquiry of the world of politics... For the behaviouralist, the role of political science is primarily to gather and analyze facts as rigorously and objectively as possible. Petro p 6 says \"Behavioralists generally felt that politics should be studied much in the same way hard sciences are studied.\" It is associated with the rise of the behavioral sciences, modeled after the natural sciences. As Guy notes, quote=The term behaviouralism was recognized as part of a larger scientific movement occurring simultaneously in \"all\" of the social sciences, now referred to as the behavioural sciences.\" This means that behavioralism tries to explain behavior with an unbiased, neutral point of view.\n\nBehavioralism seeks to examine the behavior, actions, and acts of \"individuals\" – rather than the characteristics of institutions such as legislatures, executives, and judiciaries and groups in different social settings and explain this behavior as it relates to the political.\n\nGunnell argues that since the 1950s the concept of system was the most important theoretical concept used by American political scientists. The idea appeared in sociology and other social sciences but David Easton specified how it could be best applied to behavioral research on politics.\n\nCanadian universities until the 1950s were led by British trained scholars for whom political science was not a high priority. Canadians favoured the study of political economy. After 1950 younger scholars increasingly took American PhDs and Canadian departments promoted behavioralism and quantification.\n\nPolitical science operates on a smaller scale in European universities compared to American ones. Traditionally political studies were handled by law professors or professors of philosophy. American impulses toward behavioralism have made the \"European Consortium for Political Research\" (ECPR) is a unifying force. It sponsors several scholarly journals including \"European Political Science\" (EPS) (since 2001), \"European Journal of Political Research\" (EJPR) and \"European Political Science Review\" (EPSR).\n\nIn ancient India, the antecedents of politics can be traced back to the \"Rig-Veda\", \"Samhitas\", \"Brahmanas\", the Mahabharata and Buddhist \"Pali Canon\". Chanakya (c. 350–275 BC) was a political thinker in Takshashila. Chanakya wrote the \"Arthashastra\", a treatise on political thought, economics and social order. It discusses monetary and fiscal policies, welfare, international relations, and war strategies in detail, among other topics. The Manusmriti, dated to about two centuries after the time of Chanakya is another important Indian political treatise.\n\nAncient China was home to several competing schools of political thought, most of which arose in the Spring and Autumn period. These included Mohism (a utilitarian philosophy), Taoism, Legalism (a school of thought based on the supremacy of the state), and Confucianism. Eventually, a modified form of Confucianism (heavily infused with elements of Legalism) became the dominant political philosophy in China during the Imperial Period. This form of Confucianism also deeply influenced and were expounded upon by scholars in Korea and Japan.\n\nModern scholarship has rapidly developed in the 21st century. Since 1995 \"The Journal of Chinese Political Science\" (JCPS) has been a refereed academic journal that publishes theoretical, policy, and empirical research articles on Chinese topics.\n\nIn medieval Persia, works such as the Rubaiyat of Omar Khayyam and Epic of Kings by Ferdowsi provided evidence of political analysis, while the Middle Eastern Aristotelians such as Avicenna and later Maimonides and Averroes, continued Aristotle's tradition of analysis and empiricism, writing commentaries on Aristotle's works. Averroe did not have at hand a text of Aristotle's \"Politics\", so he wrote a commentary on Plato's \"Republic\" instead.\n\n\n"}
{"id": "41627505", "url": "https://en.wikipedia.org/wiki?curid=41627505", "title": "Imago Universi", "text": "Imago Universi\n\nAndreas Cellarius, German mathematician and cartographer (1596–1665), conceived an Atlas of the Universe, published in 1660, under the title of \"Harmonia Macrocosmica\". Numerous illustrations of the solar system appear in this atlas by different authors known at that time. Referring to Ptolemy, Cellarius called the representation of this Ptolemaic conception of heaven as \"Imago universi secundum Ptolaeum\"\n\n\"Imago\" is a word in Latin which means\" 'image\"' or even \"representation\". Therefore, the title expresses the \"Picture of the Universe according to Ptolemy.\" The Latin expression was used in the Middle Ages to express the representation and size of the known world at that time.\n\n\"Imago Universi\" is also the title, in Latin, of a cosmographic treatise, written in 2013 by the Spanish scientist Gabriel Barceló.\n\nAfter analyzing the history of cosmology, the treatise delves into the prevailing scientific lack of explanation of the rotation of the heavenly bodies in the laws of dynamic behaviour of the sidereal system. The author proposes the application of the Theory of Dynamic Interactions (TID) to astrophysics, in particular, the dynamics of stellar systems and galaxies. This theory allows new comprehension of the dynamics of nature and understands the dynamic equilibrium of the universe, always subjected to rotational accelerations, but repetitive and persistent. The author also highlights that the orbiting always coincides with the intrinsic rotation of celestial bodies. Paradox incorporating the book, noting that this had not been found to date.\n\n\n1. Einstein, Albert: The Origins of the General Theory of Relativity, lecture given at the George A. Foundation Gibson, University of Glasgow, 20 June 1933. Published by Jackson, Wylie and co, Glasgow, 1933.\n\n"}
{"id": "1558261", "url": "https://en.wikipedia.org/wiki?curid=1558261", "title": "Inorganic compounds by element", "text": "Inorganic compounds by element\n\nThis is a list of common inorganic and organometallic compounds of each element. Compounds are listed alphabetically by their chemical element name rather than by symbol, as in list of inorganic compounds.\n\nAcCl,\nAcF,\nAcO\n\nAlH,\nAlBr,\nAlBr,\nAlCl,\nAlCl(OH),\nAlF,\nAlGaAs,\nAlGaN,\nAl(OH),\nAlI,\nAlI,\nAl(\"i\"-PrO),\nAl(NO),\nAlN,\nAlO,\nAl(SO),\nAlS,\n\"i\"-BuAlH (DIBAL-H),\nEtAlCl,\nLiAlH,\n(−Al(CH)O−),\nEtAl,\nMeAl\n\nAmCl,\nAmCl,\nAmF,\nAmO\n\nSbF,\nSbF,\nSbCl,\nSbCl,\nSbO,\nSbO,\nSbS,\nSbS,\nInSb,\nCHKOSb,\nKSbF,\nNaSbF,\nSbH,\n(CH)Sb\n\nHArF\n\nAsBr,\nAsCl,\nAsF,\nAsF,\nAsO,\nAsO,\nAsS,\nAsH,\n(CH)AsOOH,\nGaAs,\nKAsF,\nKHAsO,\nNaAsO,\nNaHAsO,\nNaAsO,\n(CH)As,\n(CH)As\n\nLiAt, NaAt, KAt, RbAt, CsAt, FeAt, FeAt, PbAt, HAt, CuAt\n\nBaBr,\nBaCO,\nBaCl,\nBa(ClO),\nBaCuO,\nBa(OH),\nBaF,\nBaI,\nBa(NO),\nBaO,\nBaO,\nBaSO,\nBaS,\nBaTiO,\nYBaCuO or YBCO\n\nBkBr,\nBk(CO),\nBkCl,\nBkF,\nBkI,\nBk(NO),\nBkO,\nBkPO,\nBk(SO),\nBkS\n\nBeBr,\nBeCO,\nBeCl,\nBeF,\nBeH,\nBe(OH),\nBeI,\nBe(NO),\nBeN,\nBeO,\nBeSO,\nBeSO,\nBe(BH),\nBeTe\n\nBiCl,\nBiF,\nBi(NO).5HO,\nBiO,\nBiTe,\nBiOCl,\nBiSrCaCuO,\n(BiO)CO,\nBiO(OH)(NO),\nBi(OSOCF),\n\nBh,\nadd Bohrium compounds here\n\nNHBF,\nCHB, \nBH.THF,\nNaBO·10HO, \nBNH, \nHBO, \nBC, \nBN, \nBO,\nBO, \nBBr, \nBCl, \nBF, \nHCBH, \nBH, \nBH, \nLaB,\nLiBO,\nLiBH,\nLiBF,\nLi(\"sec\"-CH)BH, \nLiBH(CH),\nNaBH, \nNaBF,\nNaBHCN,\nNaB(CH), \nBH, \nHBF, \nTiB, \nB(OCH), \n(CHBO), \nWB\n\nAlBr, \nNHBr, \nBBr, \nHBrO, \nBrCl, \nBrO, \nBrF, \nBrF, \nBrF,\nCaBr,\nCBr, \nCuBr, \nCuBr,\nHBr(aq), \nHBr(g),\nHOBr,\nIBr, \nFeBr,\nFeBr, \nPbBr, \nLiBr, \nMgBr, \nHgBr, \nHgBr, \nNOBr,\nPBr, \nPBr, \nKBr, \nKBrO,\nKBrO,\nHSiBr,\nSiBr,\nAgBr, \nNaBr, \nNaBrO,\nNaBrO,\nSOBr, \nSnBr, \nZnBr\n\nCdAs,\nCdBr,\nCdCO,\nCdF, \nCdCl, \nCdI, \nCd(OH),\nCdO, \nCdSe,\nCdSO, \nCdS,\nCdTe\n\nCsCO, \nCsCl, \nCsF,\nCsOH,\nCsI,\nCsSO,\nCsO\n\nCa(HCO),\nCaB,\nCaBr,\nCaC,\nCaCO,\nCaCl,\nCaNCN,\nCaF,\nCaPOF,\nCa(HOCH(CHOH)CO),\nCaH,\nCaH,\nCaHPO,\nCa(OH),\nCa(OCl),\nCaI,\nCaMoO,\nCa(NO),\nCaCO,\nCaO,\nCaO,\nCa(PO),\nCaSiO,\nCaSO,\nCaS,\nCaTiO,\nCaWO,\nCaZrO\n\nCfBr,\nCf(CO),\nCfCl,\nCfF,\nCfI,\nCf(NO),\nCfO,\nCfPO,\nCf(SO),\nCfS,\nCfOF,\nCfOCl\n\n\"This list focuses only on inorganic carbon compounds – some of these such as urea may be \"borderline\" but have been included for completeness. If you cannot find a compound here, please try checking under another element in the compound, or also in list of organic compounds.\"\n\nNHHCO,\nNHCOONH,\n(NH)CO,\nNHSCN,\nBaCO,\nBC,\nCdCO,\nCsCO,\nCa(HCO),\nCaC\nCaCO,\nCaNCN,\nCO,\nCS,\nHCO,\nCO,\nβ−CN,\nCO,\nCBr,\nCCl,\nCF,\nCOF,\nCOS,\nHCBH,\nCe(CO),\nCr(CO),\nCoCO,\nCo(CO),\nCo(SCN),\nCuCO,\nCuCN,\n(HCNO),\nNHCN,\nHCNO,\n(CN),\nBrCN,\nClCN,\nHCN,\nFeCO,\nFe(CO),\nPbCO,\nLa(CO),\nLiCO,\nMgCO,\nMnCO,\nMn(CO),\nMo(CO),\nNiCO,\nNi(CO),\nCOCl,\nKHCO,\nKCO,\nKOCN,\nKCN,\nKFe(CN),\nKFe(CN),\nKSCN,\nFe(Fe(CN)),\nSiC,\nAgCO,\nAgCN,\nNaHCO,\nNaCO,\nNaOCN,\nNaCN,\nNaFe(CN)NO,\nNaSCN,\nSrCO,\nTaC,\n(SCN),\nCSCl,\nTiC,\nWC,\nW(CO),\nCO(NH),\nZnCO\n\n(NH)Ce(NO) (CAN),\n(NH)Ce(SO),\nCe(CO),\nCeCl,\nCeF,\nCeF,\nCeO,\nCePO,\nCe(SO),\nCe(OSOCF),\nCe(OSOCF)\n\nAcCl,\nAlCl,\nAmCl,\nNHCl,\nSbCl,\nSbCl,\nAsCl,\nBaCl,\nBeCl,\nBiCl,\nBCl, \nBrCl,\nCdCl, \nCdCl, \nCsCl, \nCaCl,\nCa(OCl),\nCCl,\nCeCl, \nNHCl,\nClO,\nClO,\nClF,\nHPtCl,\nClSOH,\nClSONCO,\nCrCl,\nCrCl,\nCrOCl,\nCoCl,\nCuCl,\nCuCl,\nCmCl,\nClCN,\nClO,\nClO,\nSCl,\nDyCl,\nErCl,\nEuCl,\nEuCl,\nGdCl,\nGaCl,\nGeCl,\nGeCl,\nAuCl,\nAuCl,\nHfCl,\nHPtCl,\nHoCl,\nHCl,\nHCl(aq),\nHOCl,\nInCl,\nInCl,\nICl,\nIrCl,\nFeCl,\nFeCl,\nLaCl,\nPbCl,\nLiCl,\nLiClO,\nLuCl,\nMgCl,\nMg(ClO),\nMnCl,\nHgCl,\nHgCl,\nHg(ClO),\nMoCl,\nMoCl,\nNdCl, \nNpCl,\nNiCl,\nNbCl,\nNbCl,\nNbOCl,\nNCl,\nNOCl,\nNOCl,\nOsCl,\nPdCl,\nCOCl,\n(PNCl),\nPOCl,\nPCl,\nPCl,\nPtCl,\nPtCl,\nPuCl,\nKCl,\nKClO,\nKClO,\nPrCl,\nPaCl,\nRaCl,\nReCl,\nReCl,\nRhCl,\nRbCl,\nRuCl,\nSmCl,\nScCl,\nSeCl,\nSeCl,\nSiCl,\nAgCl,\nAgClO,\nNaClO,\nNaClO,\nNaCl,\nNaOCl,\nNaClO,\nSrCl,\nSCl,\nSOCl,\nTaCl,\nTaCl,\nTaCl,\nTeCl,\nTbCl,\nTlCl,\nTlCl,\nSOCl,\nCSCl,\nThCl,\nTmCl,\nSnCl,\nSnCl,\nTiCl,\nTiCl,\nHSiCl,\nWCl,\nWCl,\nWCl,\nUCl,\nUCl,\nUCl,\nUCl,\nUOCl,\nVCl,\nVCl,\nVCl,\nVOCl,\nYbCl,\nYCl,\nZnCl,\nZrCl\n\n(NH)CrO,\nNH[Cr(NCS)(NH)],\nBaCrO,\nCr(CO),\nCrCl,\nCrCl,\nCrCl,\nCrF,\nCrO,\nCrO,\nCrO,\nCr(NO),\nKCr(SO),\nCr(SO),\nCrS,\nCrN,\nCrOCl,\nCuCrO,\nPbCrO,\nKCrO,\nKCrO,\nNaCrO,\n\nCoBr,\nCoCO,\nCoCl,\nCoF,\nCoF,\nCo(OH),\nCo(NO),\nCoO,\nCoO, \nCoSO,\nCoS,\nCo(SCN),\nCo(CO),\nHexol,\nNaCo(NO),\nCo(CO),\n\nCn, add Copernicium compounds here\n\nCuCO·Cu(OH),\nCuBr,\nCuBr,\nCuCO,\nCuCl,\nCuCl,\nCuCrO,\nCuCN,\nCuF,\nCu(OH),\nCuI,\nCu(NO),\nCuO,\nCuO,\nCuI,\nCuSO,\nCuS,\nCuS,\nRCuLi,\nLiCuCl,\nYBCO\n\nCmCl,\nCmO,\nCmO,\nCm(OH)\n\nDs, add Darmstadtium compounds here\n\nDb,\nadd Dubnium compounds here\n\nDy(CO),\nDyCl,\nDyF,\nDy(NO),\nDyO,\nDyPO,\nDy(SO),\nDy(OSOCF)\n\nEr(CO),\nErCl,\nErF,\nEr(NO),\nErO,\nErPO,\nEr(SO),\n\nEsBr,\nEs(CO),\nEsCl,\nEsF,\nEsI,\nEs(NO),\nEsO,\nEsPO,\nEs(SO),\nEsS\n\nEu(CO),\nEuCl,\nEuCl,\nEuF,\nEu(fod),\nEu(hfc),\nEuI,\nEu(NO),\nEuO,\nEuSO,\nEuPO,\nEu(SO),\nEu(tmhd),\nEu(NTf)\n\nFmBr,\nFm(CO),\nFmCl,\nFmF,\nFmI,\nFm(NO),\nFmO,\nFmPO,\nFm(SO),\nFmS\n\nAlF,\nAmF,\nNHF,\nNHHF,\nNHBF,\nSbF, \nSbF,\nAsF, \nAsF,\nBaF,\nBeF,\nBiF,\nFSOOSF,\nBF, \nBrF,\nBrF,\nBrF,\nCdF, \nCsF, \nCaF,\nCF,\nCOF,\nCeF,\nCeF,\nClF,\nClF,\nClF,\nCrF,\nCrF,\nCrOF,\nCoF,\nCoF,\nCuF,\nCuF,\nCmF,\nNF,\nNF,\nOF,\nPF,\nSF,\nDyF,\nErF,\nEuF,\nHBF,\nFN,\nFOSOF,\nFNO,\nFSOH,\nGdF,\nGaF,\nGeF,\nAuF,\nHfF,\nHSbF,\nHPF,\nHSiF,\nHTiF,\nHF,\nHF(aq),\nHFO,\nInF,\nIF,\nIF,\nIF,\nIrF,\nIrF,\nFeF,\nFeF,\nKrF,\nLaF,\nPbF,\nPbF,\nLiF,\nMgF,\nMnF,\nMnF,\nMnF,\nHgF, \nHgF,\nMoF,\nMoF,\nMoF,\nNbF,\nNbF,\nNdF,\nNiF,\nNpF,\nNpF,\nNpF,\nONF,\nNF,\nNOBF,\nNOBF,\nNOF,\nNOF,\nOsF,\nOsF,\nOsF,\nOF,\nPdF,\nPdF,\nFSOOOSOF,\nPOF,\nPF,\nPF,\nPtF,\nPtF,\nPtF,\nPuF,\nPuF,\nPuF,\nKF,\nKPF,\nKBF,\nPrF,\nPaF,\nRaF,\nRnF,\nReF,\nReF,\nReF,\nRhF,\nRbF,\nRuF,\nRuF,\nRuF,\nSmF,\nScF,\nSeF,\nSeF,\nSiF,\nAgF,\nAgF,\nAgBF,\nNaF,\nNaFSO,\nNaAlF,\nNaSbF,\nNaPF,\nNaSiF,\nNaTiF,\nNaBF,\nSrF,\nSF,\nSF,\nSF,\nSOF,\nTaF,\nTcF,\nTeF,\nTeF,\nTlF,\nTlF,\nSOF,\nThF,\nSnF,\nSnF,\nTiF,\nTiF,\nHSiF,\nWF,\nUF,\nUF,\nUF,\nUOF,\nVF,\nVF,\nVF,\nXeF,\nXeOF,\nXeF,\nXePtF,\nXeF,\nYbF,\nYF,\nZnF,\nZrF\n\nFrO,\nFrF,\nFrCl,\nFrBr,\nFrI,\nFrCO, \nFrOH,\nFrSO\n\nGd(CO),\nGdCl,\nGdF,\nGdGaO,\nGd(NO),\nGdO,\nGdPO,\nGd(SO)\n\nAlGaAs,\nGdGaO, GaH,\nGaBr,\nGaCl,\nGaF,\nGa(OH),\nGa(NO),\nGaN,\nGaO,\nGaP,\nGa(SO),\nGaS,\nGa(CH)\n\nGeH,\nGeCl,\nGeI,\nGeO,\nGeSe,\nGeS,\nGeS,\nGeCl,\nGeF,\nGe(CH),\n(CH)GeCl\n\nHAuCl,\n(PhP)AuCl,\nAuBr,\nAuCl,\nAuCl,\nAuO,\nAuS,\nKAu(CN)\n\nHfC,\nHfO,\nHfCl,\nCpHfCl\n\nHs,\nHsONa<nowiki>[</nowiki>HsO(OH)<nowiki>]</nowiki>\n\nHe, HeH\nNo neutral compounds known.\n\nHo(CO),\nHoCl,\nHoF,\nHo(NO),\nHoO,\nHoPO,\nHo(SO),\n\n\"Only simple and common hydrides are listed here. For hydroxides of metals, see the appropriate metal. For ammonium compounds, see #Nitrogen.\"\n\nAlH,\nNH,\nAsH,\nBeH,\nBiH,\nHBO,\nHBrO,\nCaH,\nHCBH, HClO,\nHClO,\nBH,\nNH,\nPH,\nHBF,\nGaH,\nGeH,\nHSbF,\nHPF,\nHSiF,\nHTiF,\nNH,\nHN,\nHBr(aq),\nHCl(aq),\nHCN,\nHF(aq)\nHBr,\nHCl,\nHF,\nHI,\nHO,\nHSe,\nHS,\nHTe,\nHI(aq),\nHOBr(aq),\nHOCl(aq), \nHPO,\nLiAlH,\nLiBH,\nLiH,\nHNO,\nHNO,\nBH,\nBH,\nHClO,\nHSO,\nHSO,\nHPO,\nHPO,\nKH,\nHPO,\nHSeO,\nSiH,\nHSiO,\nNaBH,\nNaH,\nSnH,\nSbH,\nHSO,\nHSO,\nHTeO,\nBH,\nZn(BH)\n\nInSb,\nInAs,\nInBr,\nInCl,\nInF,\nInI,\nIn(NO),\nInN,\nInO,\nInO,\nInP,\nInSe,\nIn(SO),\nInS,\nIn(CH)\n\nAlI, \nNHI, \nBaI,\nBI,\nCsI,\nCaI,\nCI, \nCuI,\nPI, \nGeI,\nHI(aq), \nHI(g),\nInI,\nHIO, \nIBr, \nICl, \nIO,\nIF,\nIF, \nIF, \nFeI,\nPbI, \nLiI, \nMgI, \nHgI, \nHgI, \nPI, \nKI, \nKIO,\nKIO, \nSmI, \nAgI, \nNaI, \nNaIO,\nNaIO,\nTlI, \nSnI, \nZnI\n\nIr(CO),\nIrCl,\nIrCl,\nIrO,\nNaIrCl\n\n(NH)Fe(SO), \nFeBrñ,\nFeBr,\nFeCl,\nFeCl,\nFeS,\nFe(CO),\nFeF,\nFeI,\nFe(ONap),\nFe(NO),\nFe(CO),\nFeCO,\nFeO,\nFeO,\nFeO,\nFe(CO),\nFe(ClO),\nFePO,\n(NHSO)Fe,\nFeSO,\nFe(SO),\nFeS\n\nKr,\nKrF,\nadd Krypton compounds here\n\nLaB,\nLa(CO),\nLaCl,\nLaF,\nLaO,\nLa(NO),\nLaPO,\nLa(SO)\n\nLr,\nadd Lawrencium compounds here\n\nPbCO,\nPbCl,\nPbCl,\nPbO,\nPbF,\nPbI,\nPb(CH),\nPb(CH),\nPb(NO),\nPbO,\nPbSO,\nPbS,\nPbSe,\nPbTe.\n\nLiH,\nLiN,\nLiNH,\nLiO,\nLiOH,\nLiF,\nLiCl,\nLiBr,\nLiI,\nLiCO, \nLiSO\n\nLuBr,\nLu(CO),\nLuCl,\nLuF,\nLuI,\nLu(NO),\nLuO,\nLuPO,\nLu(SO),\nLuS\n\nMg(HCO),\nMgB,\nMgBr,\nMgC,\nMgCO,\nMgCl,\nMagnesium citrate,\nMgNCN,\nMgF,\nMgPOF,\nMg(HOCH(CHOH)CO),\nMgH,\nMgHPO,\nMg(OH),\nMg(OCl),\nMgI,\nMgMoO,\nMg(NO),\nMgCO,\nMgO,\nMgO,\nMg(PO),\nMgSiO,\nMgSO,\nMgS,\nMgTiO,\nMgWO,\nMgZrO\n\nMn,\nMnO,\nKMnO,\nMnPc\n\nMt,\nadd Meitnerium compounds here\n\nMd,\nadd Mendelevium compounds here\n\nHgO,\nHgS,\nHgSe,\nHgTe,\nHgCdTe,\nHgF,\nHgCl,\nHgCl,\nHgBr,\nHgI,\nHg(OH),\nHgSO,\nHgSO,\n(CH)Hg\n\nMoCl,\nMo(CO),\nMoO,\nMoS,\n\nNd,\nNdFeB,\nNdF,\nNdCl,\nNdBr,\nNdI\n\nNe,\nNo Neon compounds are known.\n\nNpCl,\nNpCl,\nNpCl,\nNpCl,\nNpF,\nNpF,\nNpF,\nNpO\n\nNi, NiF, NiCl, Ni(NO), NiSO\nadd Nickel compounds here\n\nNb, NbO\n\nN, NH, NH, HNO, HNO, NO, NHOH, HN\nadd Nitrogen compounds here\n\nNo,\nadd Nobelium compounds here\n\nOs, OsO,\nadd Osmium compounds here\n\n\"This section lists only simple oxides, oxyhalides, and related compounds, not hydroxides, carbonates, acids, or other compounds.\"\n\nAlO,\nAmO,\nAmO,\nSbO,\nSbO,\nAsO,\nAsO,\nBaO,\nBeO,\nBiO,\nBiOCl,\nBO, \nBrO,\nCO,\nCO,\nCeO,\nClO,\nClO,\nClO,\nClO,\nCrO,\nCrO,\nCrO,\nCoO,\nCuO,\nCuO,\nCmO,\nCmO,\nDyO,\nErO,\nEuO,\nOF,\nOF,\nFrO,\nGdO,\nGaO,\nGeO,\nAuO,\nHfO,\nHoO,\nInO,\nInO,\nIO,\nIrO,\nFeO,\nFeO,\nFeO,\nLaO,\nPbO,\nPbO,\nLiO,\nMgO,\nKO,\nRbO,\nNaO,\nSrO,\nTeO,\nUO\n\nPdCl\n\nPBr,\nPCl,\nPF,\nPO,\nPO,\nPS,\nPS,\nPH,\nHPO,\nHPO,\nKPF,\nKPO,\nKHPO,\nKHPO,\n(CH)P,\n(CH)P\n\nPt, \n\nPu,\nPuO\n\nPo, PoO, PoO, PoO,\nadd Polonium compounds here\n\nKO,\nKF,\nKCl,\nKBr,\nKI,\nKCO, \nKOH,\nKSO,\nKMnO,\nKFeO,\nKAl(SO),\nKHCO,\nKNO,\nKVO,\nKCrO,\nKCrO,\nKS,\nKSb(OH),\nK[Fe(CN)],\nK[Fe(CN)],\nKPtCl\n\nPrBr,\nPr(CO),\nPrCl,\nPrF,\nPrI,\nPr(NO),\nPrO,\nPrPO,\nPr(SO),\nPrS\n\nPmBr,\nPm(CO),\nPmCl,\nPmF,\nPmI,\nPm(NO),\nPmO,\nPmPO,\nPm(SO),\nPmS\n\nPaCl,\nPaCl,\nPaCl,\nPaCl,\nPaF,\nPaF,\nPaF,\nPaO\n\nRa(HCO),\nRaB,\nRaBeF4,\nRaBr,\nRaC,\nRaCO,\nRaCl,\nRaCrO,\nRaNCN,\nRaF,\nRaPOF,\nRa(HOCH(CHOH)CO),\nRaH,\nRaHPO,\nRa(OH),\nRa(OCl),\nRaI,\nRaMoO,\nRa(NO),\nRaCO,\nRaO,\nRaO,\nRa(PO),\nRaSiO,\nRaSO,\nRaSO,\nRaS,\nRaTiO,\nRaWO,\nRaZrO\n\nRnF\n\nRhB,\n\nRh,\nadd Rhodium compounds here\n\nRg,\nadd Roentgenium compounds here\n\nRbH,\nRbO,\nRbF,\nRbCl,\nRbBr,\nRbI,\nRbTe, \nRbCO, \nRbOH,\nRbSO,\nRbClO.\n\nRu,\nRuCl,\nRuO,\nRuO\n\nRf,\nadd Rutherfordium compounds here\n\nSmBr,\nSm(CO),\nSmCl,\nSmF,\nSmI,\nSm(NO),\nSmO,\nSmPO,\nSm(SO),\nSmS\n\nScBr,\nSc(CO),\nScCl,\nScF,\nScI,\nSc(NO),\nScO,\nScPO,\nSc(SO),\nScS\n\nSg,\nSg(CO)\n\nCdSe,\nHSe,\nHgSe,\nSeO, \nHSeO,\nHSeO, \nNaSeO, \nZnSe,\nSeCNOH\n\nSi,\nSiC,\nSi3N4,\nSiO2,\nSiS2,\nSiH4 (silanes),\nSiCl4,\nNa2Si etc. (silicides),\nH2SiO3 and other Silicic acids,\nand their silicates, orthosilicates, disilicates and pyrosilicates,\nH2SiF6 and hexafluorosilicates etc.\n\nAgF,\nAgF,\nAgBr,\nAgCl,\nAgI,\nAgCN,\nAgONC,\nAgNO,\nAgClO,\nAgS,\nAgTe,\nAgO,\nAgO,\nAgBF.\n\nNaHCO, \nNaBr,\nNaCO, \nNaCl,\nNaFe(CN),\nNaF,\nNaOH,\nNaI,\nNaO,\nNaSO\n\nSr(HCO),\nSrB,\nSrBr,\nSrC,\nSrCO,\nSrCl,\nSrNCN,\nSrF,\nSrPOF,\nSr(HOCH(CHOH)CO),\nSrH,\nSrHPO,\nSr(OH),\nSr(OCl),\nSrI,\nSrMoO,\nSr(NO),\nSrCO,\nSrO,\nSrO,\nSr(PO),\nSrSiO,\nSrSO,\nSrS,\nSrTiO,\nSrWO,\nSrZrO\n\nS, NaSH, HNSO, HS, SO, HSO, HSO, SOCl, CdS, HgS, ZnS, SCl, SCl, SF, SOCl, SOBr, CS, CSCl, MoS, (NH)S, (NH)SO, FeS\n\nTa,\nadd Tantalum compounds here\n\nTc,\nNHTcO,\nNaTcO\n\nBeTe, BiTe, CdTe, (Cd,Zn)Te, (CH)Te, (Hg,Cd)Te, PbTe, HgTe, (Hg,Zn)Te, AgTe, SnTe, ZnTe, HOTeF, HTeO, NaTeO, TeO, TeF, TeF.TeCl\n\nTbBr,\nTb(CO),\nTbCl,\nTbF,\nTbI,\nTb(NO),\nTbO,\nTbPO,\nTb(SO),\nTbS\n\nTlSb,\nTlAs,\nTlBr,\nTlCl,\nTlF,\nTlI,\nTl(NO),\nTlO,\nTlO,\nTlP,\nTlSe,\nTl(SO),\nTlS,\nTl(CH),\nTlOH\n\nThCl,\nThCl,\nThCl,\nThCl,\nThF,\nThF,\nThF,\nThO\n\nTmBr,\nTm(CO),\nTmCl,\nTmF,\nTmI,\nTm(NO),\nTmO,\nTmPO,\nTm(SO),\nTmS\n\nSnF,\nSnCl,\nSnCl,\nSnO,\nSnO,\nSn(CH),\nSn(CH),\nSnS,\nSnSe,\nSnTe.\n\nTiC, \nTiN, \nTiF, \nTiCl, \nTiCl,\nTiO,\nTiBr,\nTiCl,\nTiI,\nCp2TiCl,\nMgTiO,\nCaTiO,\nSrTiO\n\nWC, \nWCl, \nWF, \nWO\n\nUCl,\nUCl,\nUCl,\nUCl,\nUF,\nUF,\nUF,\nUOF,\nUO,\nUO\n\nVC,\nVBr,\nVCl, \nVCl,\nVCl,\nVOCl,\nVO,\nCpVCl\n\nXeF,\nXePtF,\nXeF,\nXeF,\nXeO\n\nYbBr,\nYb(CO),\nYbCl,\nYbF,\nYbI,\nYb(NO),\nYbO,\nYbPO,\nYb(SO),\nYbS\n\nYBr,\nY(CO),\nYCl,\nYF,\nYI,\nY(NO),\nYO,\nYPO,\nY(SO),\nYS,\nYOF,\nYP\n\nZnO,\nZnS,\nZnSe,\nZnTe,\nZnF,\nZnCl,\nZnBr,\nZnI,\nZn(OH),\nZnSO\n\nZrC,\nZrO,\nZrN,\nZrCl,\nZrS,\nZrSi,\nZrSiO,\nZrF,\nZrBr,\nZrI,\nZr(OH),\nCHClZr,\nZr(CHCHCOO),\nZr(WO),\nZrH,\nPb(ZrTi)O\n\n"}
{"id": "1763912", "url": "https://en.wikipedia.org/wiki?curid=1763912", "title": "John Arnold (watchmaker)", "text": "John Arnold (watchmaker)\n\nJohn Arnold (1736 – 11 August 1799) was an English watchmaker and inventor.\n\nJohn Arnold was the first to design a watch that was both practical and accurate, and also brought the term \"chronometer\" into use in its modern sense, meaning a precision timekeeper. His technical advances enabled the quantity production of marine chronometers for use on board ships from around 1782. The basic design of these has remained, with a few modifications unchanged until the late twentieth century. With regard to his legacy, one can say that both he and Abraham-Louis Breguet largely invented the modern mechanical watch. Certainly one of his most important inventions, the overcoil balance spring is still to be found in most mechanical wristwatches to this day.\n\nIt was from around 1770 that Arnold developed the portable precision timekeeper, almost from the point where John Harrison ended his work in this field. But, compared to Harrison's complicated and expensive watch, Arnold's basic design was simple whilst consistently accurate and mechanically reliable. Importantly, the relatively simple and conventional design of his movement facilitated its production in quantity at a reasonable price whilst also enabling easier maintenance and adjustment.\n\nBut three elements were necessary for this achievement:\n\nJohn Arnold was apprenticed to his father, also a clockmaker, in Bodmin. He probably also worked with his uncle, a gunsmith. Around 1755, when he was 19, he left England and worked as a watchmaker in the Hague, Holland, returning to England around 1757.\n\nIn 1762, whilst at St Albans, Hertfordshire, he encountered William McGuire for whom he repaired a repeating watch. Arnold made a sufficient impression so that McGuire gave him a loan, enabling him to set up in business as a watchmaker at Devereux Court, Strand, London. In 1764, Arnold obtained permission to present to King George III an exceptionally small half quarter repeating watch cylinder escapement watch mounted in a ring. A similar repeating watch by Arnold has survived; it is of interest that the basic movement is Swiss in origin but finished in London. The escapement of this watch was later fitted with one of the first jewelled cylinders made of ruby.\n\nArnold made another watch for the King around 1768, which was a gold and enamel pair cased watch with a movement that had every refinement, including minute repetition and centre seconds motion. In addition, Arnold fitted bi-metallic temperature compensation, and not only was every pivot hole jewelled but the escapement also had a stone cylinder made of ruby or sapphire. Arnold designated this watch \"Number 1\", as he did with all watches he made that he regarded as significant, these numbering twenty in all.\n\nOther early productions by Arnold from 1768 to 1770 display both originality and ingenuity; this includes a centre seconds watch wound up by depressing the pendant once a day. The movement of this watch was also fully jewelled with a temperature compensation device and a ruby stone cylinder escapement.\n\nThese watches were made as demonstrations of Arnold's talent and, in terms of style and substance, were similar to other \"conversation pieces\" being made at the same time as those being produced for James Cox and made primarily for export to the East.\n\nArnold's facility and ingenuity, coupled with his undoubted charm brought him to the attention of the Astronomer Royal Nevil Maskelyne, who at this time was seeking a watchmaker skilled enough to make a copy of John Harrison's successful marine timekeeper. A full and detailed description of this watch was published by the Board of Longitude in 1767, entitled \"The Principles of Mr. Harrison's Timekeeper\", the intention clearly being for it to act as a blueprint for future quantity production. In fact, it was a highly complex and technically very advanced piece of micro engineering, and capable of being reproduced by less than a handful of watchmakers. However, the challenge was taken up by Larcum Kendall, who spent two years making a near identical copy (now known as \"K1\") that cost £450, a huge sum at the time. \nAlthough successful as a precision timekeeper, the Admiralty for obvious reasons wanted a timekeeper on every major ship, and Kendall's was too expensive and took too long to make. Kendall made a simplified version (K2) in 1771, leaving out the complicated remontoir system. But the result was still too costly and, moreover, not as accurate as the original.\n\nIn retrospect therefore, it was a significant occasion when in 1767, Nevil Maskelyne presented John Arnold with a copy of the \"Principles of Mr. Harrison's Timekeeper\" as soon as it was published, evidently with a view to encourage him to make a precision timekeeper of the same kind. \nMaskelyne subsequently encouraged Arnold by employing him on several occasions, mostly in connection with watch and clock jewelling. In 1769, Arnold modified Maskelyne's centre seconds watch by John Ellicott, changing the cylinder escapement from steel to one made of sapphire. He lent this watch to the Astronomer William Wales for use in assessing the practicability of Maskelyne's Lunar distance method for finding the ship's longitude during the voyage of the Transit of Venus expedition to the West Indies in 1769. Around this time, Arnold also seems to have started to think about making an accurate timekeeper to find the longitude.\n\nArnold's approach to precision timekeeping was completely different from that of Harrison, whose technical ethos was rooted in seventeenth and early eighteenth century theory and practice.\nArnold knew that as the balance and balance spring control the timekeeping in a portable watch, he only needed to find a way of giving the balance a consistent impulse with minimal interference from the wheel work, together with an effective temperature compensation. After making some experimental machines, he produced what could be regarded as a production model to the Board of Longitude in March 1771.\n\nThis machine was completely different from Harrison's watch. It was a mahogany box of approximately that housed a movement that, though relatively simple, was close to the same size as Harrison's, with a balance of a similar diameter. The radical difference, however, was a newly-designed escapement that featured a horizontally-placed pivoted detent that allowed the balance to vibrate freely, except when impulsed by the escape wheel. The spiral balance spring also had a temperature compensation device similar to those in Arnold's watches, and based on Harrison's bimetallic strip of brass and steel. Arnold proposed manufacture of these timekeepers at 60 guineas each.\n\nThree of these timekeepers travelled with the explorers James Cook and Captain Furneaux during their second voyage to the southern Pacific Ocean in 1772-1775. Captain Cook also had Kendall's first timekeeper on board as well as one of Arnold's. Whereas Kendall's performed very well and kept excellent time during the voyage, only one of Arnold's was still running on their return to England in 1775. The performance of these clocks was recorded in the logbooks of astronomers William Wales and William Bayly who were assessing their suitability for measuring longitude.\n\nDuring this period, Arnold also made at least one precision pocket watch, a miniature version of the larger marine timekeepers.\n\nThis surviving watch dates from around 1769-1770, and is signed \"Arnold No. 1 Invenit et Fecit\". The movement, which indicates centre seconds, has a steel balance with a bimetallic temperature compensation strip that acts on the flat balance spring. Though now altered, the original escapement was Arnold's horizontal pivoted detent as fitted to the larger timekeepers, which was it seems not entirely successful and needed improvements.\n\nAround 1772, Arnold modified this escapement so that it now was pivoted vertically and acted on by a spring. This was a much more successful arrangement, and it is known that in 1772 at least two pocket timekeepers with this escapement were supplied to Joseph Banks at a cost of £100 (Arnold No.5), and also Banks' fellow Etonian Captain Constantine John Phipps, 2nd Baron Mulgrave. In a long and detailed article on this matter published in Australiana November 2014 Vol 36 No. 4, John Hawkins details the association between these two men and the probability that this instrument is one and the same, the world's first pocket chronometer originally destined for Cook's second voyage, purchased by Banks and lent to Phipps.\n\nIn 1773, Captain Phipps made a voyage to the North Pole, taking with him not only his Arnold pocket timekeeper and an Arnold box timekeeper in gimbals, but also Kendall's \"K2\" timekeeper. From Phipps's account, it appears that the pocket watch performed very well indeed and was a convenient instrument for ascertaining the longitude.\n\nIt seems likely that before 1775, Arnold's earliest pocket chronometers, such as those supplied to Phipps and Banks, were plain watches with centre seconds motion, largely resembling Maskelyne's cylinder watch by Ellicott. Certainly, those few surviving examples are of this caliper such as No. 3.\n\nBy 1772, Arnold had finalized the design of his pocket timekeepers and started series production with a standardized movement caliper, this being around 50 mm in diameter, larger than a conventional watch of the period, and showing seconds with a pivoted detent escapement and spiral compensation curb. However, the latter appears to have proved ineffective, which seems to have substantially slowed the rate of production.\n\nEven though he produced a number of pocket timekeepers, from around 1772-1778, Arnold was still experimenting with different types of compensation balance and methods of balance spring adjustment. The most difficult problem to surmount was the problem of making an effective and continuously adjustable temperature compensation device. For technical reasons, the temperature compensation for the balance spring had to somehow be incorporated into the balance itself and not act on the balance spring directly as had been done previously by Arnold and others.\n\nIn 1775, Arnold took out a patent for a new form of compensation balance with a bimetallic spiral at the centre. This spiral actuated two weighted arms, making them move in and out from the centre, changing the radius of gyration, and thus the period of oscillation.\nIn the same patent, he included a new helical balance spring. This shape reduced lateral thrust on the balance pivots as they rotated, and reduced random errors from the \"point of attachment\" effect, which any balance with a flat spring suffers from. As Arnold stated rather succinctly in a 1782 letter to the Board of Longitude, \"...the power in all parts of the spring is uniform.\"\n\nThe fact that Arnold had recognized the technical advantages of a balance spring of this form clearly demonstrates a high degree of insight. The balance that was the subject of the patent appears to have been an unsuccessful design. Certainly, some marine chronometers used this balance, but none have survived. Pearson records a balance of this kind in his possession that was 2.4 inches in diameter.\n\nFrom 1772 to 1775, Arnold also made about thirty five pocket timekeepers. Not many, about ten of these, survive and none in their original form, as Arnold was constantly upgrading their specification. They appear to have originally had a pivoted detent escapement, with a steel balance and a helical balance spring. A spiral bimetallic curb acting on this spring was intended to provide the temperature compensation, but this system evidently did not work, as every watch was subsequently altered and improved by Arnold shortly afterwards. Surviving chronometers from this series include Numbers 3,29 and 28.\n\nFurther experimentation and invention by Arnold led to a breakthrough in the late 1770s. He redesigned the compensation balance and developed two designs that showed promise. Known as the \"T\" and \"S\" balances, and marked as such in Arnold's 1782 patent (probably because of their appearance), both employed bimetallic strips of brass and steel with weights attached, which changed the radius of gyration with change in temperature. Although these probably needed a lot of adjustment, they appear to have worked well compared to his previous attempts at a compensation balance.\n\nAround 1777, Arnold redesigned his chronometer, making it larger in order to accommodate the new \"T\" balance that worked with his pivoted detent escapement and patented helical spring. The first chronometer of this pattern was signed \"Invenit et Fecit\" and given the fractional number 1 over 36, as it was the first of this new design.\n\nIt is generally known as \"Arnold 36\" and was, in fact, the first watch that Arnold called a \"chronometer\", a term that subsequently came into general use and still means any highly accurate watch. The Royal Observatory, Greenwich tested Arnold 36 for thirteen months, from 1 February 1779 to 6 July 1780. The testers placed it in several positions during the trial, and even wore it and carried it around. The watch exceeded all expectations, as it demonstrated great accuracy. The timekeeping error was 2 minutes 32.2 seconds, but the error in the last nine months amounted to just one minute. The greatest error in any 24 hours was only four seconds, or one nautical mile of longitude at the equator.\n\nSubsequently, Arnold produced a pamphlet that detailed the trial and results, with attestations of veracity from all those concerned with the tests. Maskelyne's assistant, the Rev. John Hellins, was in charge of the pamphlet. The astonishing performance of this watch caused controversy, because many thought the result was either a fluke or a \"fix\" of some kind, particularly as Maskelyne was, effectively, one of Arnold's patrons.\n\nFrom a technical point of view, however, the design was entirely sound and highly accurate over long periods. Arnold evidently learned the lessons that Harrison had learned before him—using a large, quickly oscillating balance (18,000 beats per hour) with small pivots. Arnold's detent escapement provided minimal interference with the controlling helical balance spring, since the temperature compensation was in the balance itself. Harrison had suggested this as a prerequisite, though he never developed the idea. \nArnold's pivoted detent escapement did not need oil on acting surfaces, with the advantage that the rate of action did not deteriorate, and remained stable for long periods. At the time, only vegetable oil was available, which degraded quickly compared to modern lubricants.\n\nThis chronometer, 60mm in diameter, is housed in a gold case, and miraculously has survived in perfect and original condition. It can be seen in the collections of the National Maritime Museum, Greenwich, London, having been saved for the nation in 1993.\n\nIn Britain, prior to Harrison's marine watch, a small, or very small watch (Such as Arnold's ring watch) was generally regarded as the ultimate test of watchmaking skill, especially with regard to complex and accurate watches. Both Harrison and Arnold however, demonstrated that an accurate watch had to be of large diameter, so by the end of the 18th century, a watch of large size was considered the primary characteristic of a well made and superior watch.\n\nIn 1782, Arnold took out another patent to protect the latest and most important inventions, which were potentially lucrative. Several other watchmakers, most notably Thomas Earnshaw, had started to copy Arnold's work. Around 1780, Earnshaw modified his detent escapement by mounting the detent on a spring to create the spring detent escapement.\n\nDuring the same period, between 1779 and 1782, Arnold finalized the form of his chronometer watches. Through continuous experimentation, he worked out a way to make an effective but simple form of compensation balance. At the same time, he discovered a simple modification to his helical balance spring that let develop concentrically and, also, confer the property of isochronism on the oscillating balance. Not only this, but adjustments to the compensation balance and the balance spring could be carried out in a simple, calculated way. These were the main subjects of the patent, which he took out in 1782.\n\nThe balance consisted of a circular steel balance wheel with two bimetallic strips attached diametrically. Each bimetallic strip terminated with a screw thread mounted with a weight or balance nut. The further along the strip this nut was screwed, the greater the compensating effect. Another part of the patent concerned an addition to the form of the balance spring—a coil of smaller radius at each end of the helical spring, which offered increasing resistance to the rotating balance as it turned towards the end of each vibration. This was an important invention, as it largely eliminated the problem of the positional adjustment of balance controlled watches. This device, known as the \"Overcoil balance spring\" is still used today in most precision mechanical watches.\n\nAnother part of the patent concerned the escapement—a modification of Arnold's pivoted detent escapement that essentially mounted the detent on a spring. The specification only shows the part of this escapement that is the method of impulse on the impulse roller.\n\nThe fact that Arnold had gained great success by modifying the technology of the timekeeper by means of simple yet effective mechanical techniques, also meant that other watchmakers could copy these methods and use them without permission. This is why Arnold took out his patents.\n\nTwo other makers also made precision watches with the detached escapement: Josiah Emery and John Brockbank. Both were friends of Arnold, and both employed the highly skilled workman and escapement maker Thomas Earnshaw. Josiah Emery used, with Arnold's permission, an earlier form of his compensation balance and helical balance spring, in conjunction with the detached lever escapement of Thomas Mudge. John Brockbank employed Earnshaw to make his pattern of chronometer, but with Brockbank's design of compensation balance.\n\nIn 1780, while making these chronometers for Brockbank, Earnshaw modified the pivoted detent by mounting the locking piece on a spring, thus dispensing with the pivots. Arnold managed to see this new idea and promptly took out the 1782 patent for his own design of spring detent, but it is not known whether this preceded Earnshaw's own idea.\n\nTherefore, there has been a great deal of debate over who invented the spring detent escapement, Arnold or Earnshaw. This argument, first initiated by Earnshaw, has been continued by horological historians (such as Rupert Gould) to present day. However, the argument is irrelevant. \nIn recent years, research has established that Arnold's success was not due to the form of detent escapement, but to his original methods of adjusting the balance spring for positional errors by manipulating the overcoil terminal curve. For obvious reasons, Arnold tried to keep these methods secret, certainly it is recorded that he clearly expressed his concerns about possible plagiarism to Earnshaw, warning him in no uncertain terms not to use his Helical balance spring.\n\nNevertheless, a year later, in 1783, Earnshaw—through another watchmaker, Thomas Wright—took out a patent that included Earnshaw's pattern of integral compensation balance and spring detent escapement in the multiple specification. However, both of these were undeveloped and compared to Arnold's were of little use, the balance especially having to be redesigned.\n\nEventually, after much argument, the Board of Longitude granted Earnshaw and Arnold awards for their improvements to chronometers. Earnshaw received £2500 and John Arnold's son, John Roger Arnold, received £1672. The bimetallic compensation balance and the spring detent escapement in the forms designed by Earnshaw have been used essentially universally in marine chronometers since then. For this reason, Earnshaw is also generally regarded as one of the pioneers of chronometer development.\n\nHowever, because Arnold's balance spring patents were in force (each for 14 years), Earnshaw could not use the helical balance spring until the 1775 patent lapsed in 1789, and, in the case of the 1782 patent, 1796. Until around 1796, Earnshaw made watches with flat balance springs only, but post 1800, practically every marine chronometer, including those by Earnshaw, had a helical spring with terminal overcoils.\n\nArnold was the first to produce marine and pocket chronometers in significant quantities at his factory at Well Hall, Eltham from around 1783. During the next 14 or 15 years, he produced hundreds before he had any kind of commercial competition. The facts prove that authors such as Gould and Sobel are quite incorrect in their assertion that there was commercial rivalry between Arnold Sr. and Earnshaw.\n\nThe important French watchmaker Abraham-Louis Breguet became a great friend of Arnold. In 1792, the Duke of Orleans met Arnold in London and showed him one of Breguet's clocks. Arnold was so impressed that he immediately travelled to Paris and sought permission for Breguet to take on his son as his apprentice. Arnold appears to have given Breguet \"carte blanche\" to incorporate or develop any of Arnold's inventions and techniques into his own watches.\n\nThese included his balance designs, helical springs made of steel or gold, the spring detent escapement, the overcoil balance spring, and even the layout of an Arnold dial design that Breguet incorporated into his own. These were made from engine-turned gold or silver—a pattern that became the classic and distinctive \"Breguet dial\". Arnold's pattern first appeared in 1783, on the enamel dials Arnold designed for his small chronometers, and the proportions and layout of their figuring is identical to that of the classic \"Breguet\" type of engine turned metal dials which appeared around 1800, and which were quite unlike anything else made in France or Switzerland at the time.\n\nArnold also appears to have been the first to think of the concept of the Tourbillon; this must have derived from his known work on the recognition and elimination of positional errors. In the Tourbillon device, the balance and escapement is continuously rotated and virtually eliminates errors arising from the balance wheel not being perfectly balanced whilst in vertical positions. Arnold appears to have experimented with this idea but died in 1799, before he could develop it further. \nIt is known that Breguet made a successful and practical Tourbillon mechanism around 1795 but, nevertheless, he acknowledged Arnold as the inventor by presenting his first Tourbillon in 1808 to Arnold's son John Roger. As a tribute to his friend Arnold Sr., he incorporated his first Tourbillon mechanism into one of Arnold's early pocket chronometers, Arnold No.11. An engraved commemorative inscription on this watch reads:\n\nThis important and significant watch is now in the British Museum's collection of clocks and watches.\nBy the time of Arnold's death in 1799, he was the most famous watchmaker in the world, recognized for his preeminence as the inventor of the precision chronometer.\n\nArnold's son John Roger Arnold was born in 1769 and served an apprenticeship with both his father and the eminent French watchmaker Abraham Louis Breguet. He became Master of the Worshipful Company of Clockmakers in 1817. From 1787, he and his father founded the company Arnold & Son. After his father's death in 1799, John Roger continued the business, taking John Dent into partnership between 1830 and 1840. After his death in 1843, the company was bought by Charles Frodsham.\n\n"}
{"id": "3812997", "url": "https://en.wikipedia.org/wiki?curid=3812997", "title": "John Gater", "text": "John Gater\n\nJohn Gater is a British archaeological geophysicist, who has regularly featured on \"Time Team\" – the Channel 4 archaeological television series – since 1993.\n\nHe was educated at the University of Bradford and graduated with a BSc Archaeological Sciences in 1979. He worked with British Gas (for five years), the Ancient Monuments Laboratory (English Heritage) and Bradford University Research.\n\nIn 1986 he set up Geophysical Surveys of Bradford (GSB), an independent consultancy in geophysics for archaeology. In 1983 he became a member of the Institute of Field Archaeologists and is now also an associate editor for the \"Journal of Archaeological Prospection\".\n\nOn 21 July 2006, Gater was awarded an honorary Doctor of Science by the University of Bradford for his \"distinguished contributions to the field of Archaeological Geophysics\".\n\nOn the DVD set \"The Very Best Time Team Digs\", John states that, as of the time of the DVD's production, his favourite dig was Turkdean (the only site where the quality of the archaeological data was so good that \"Time Team\" returned at a later date to dig it again).\n\n"}
{"id": "48217770", "url": "https://en.wikipedia.org/wiki?curid=48217770", "title": "KIC 8462852", "text": "KIC 8462852\n\nKIC 8462852 (also Tabby's Star or Boyajian's Star) is an F-type main-sequence star located in the constellation Cygnus approximately from Earth. Unusual light fluctuations of the star, including up to a 22% dimming in brightness, were discovered by citizen scientists as part of the Planet Hunters project. In September 2015, astronomers and citizen scientists associated with the project posted a preprint of an article describing the data and possible interpretations. The discovery was made from data collected by the \"Kepler\" space telescope, which observed changes in the brightness of distant stars to detect exoplanets.\n\nSeveral hypotheses have been proposed to explain the star's large irregular changes in brightness as measured by its light curve, but none to date fully explain all aspects of the curve. One explanation is that an \"uneven ring of dust\" orbits KIC 8462852. In another explanation, the star's luminosity is modulated by changes in the efficiency of heat transport to its photosphere, so no external obscuration is required. A third hypothesis, based on a lack of observed infrared light, posits a swarm of cold, dusty comet fragments in a highly eccentric orbit, however, the notion that disturbed comets from such a cloud could exist in high enough numbers to obscure 22% of the star's observed luminosity has been doubted. Another hypothesis is that a large number of small masses in \"tight formation\" are orbiting the star. Furthermore, spectroscopic study of the system has found no evidence for coalescing material or hot close-in dust or circumstellar matter from an evaporating or exploding planet within a few astronomical units of the mature central star. It has also been hypothesized that the changes in brightness could be signs of activity associated with intelligent extraterrestrial life constructing a Dyson swarm; however, further analysis based on data till the end of 2017 showed wavelength-dependent dimming consistent with dust but not an opaque object such as an alien megastructure which would block all wavelengths of light equally.\n\nKIC 8462852 is not the only star that has large irregular dimmings, but all other such stars are young stellar objects called YSO dippers, which have different dimming patterns. An example of such an object is EPIC 204278916.\n\nNew light fluctuation events of KIC 8462852 began in the middle of May 2017. Except for a period between late-December 2017 and mid-February 2018 when the star was obscured by the Sun, the fluctuations have been observed to have continued as of July 2018.\n\nKIC is an acronym for the Kepler Input Catalog, 8462852 being the star's catalog number. Colloquially the names \"Tabby's Star\" and \"Boyajian's Star\" refer to the initial study's lead author, Tabetha S. Boyajian; KIC 8462852 is sometimes called the \"WTF Star\", after the study's subtitle \"Where's The Flux?\" (a joking reference to the colloquial expression of disbelief \"WTF\").\n\nKIC 8462852 in the constellation Cygnus is located roughly halfway between the bright stars Deneb and Delta Cygni as part of the Northern Cross. KIC 8462852 is situated south of 31 Cygni, and northeast of the star cluster NGC 6866. While only a few arcminutes away from the cluster, it is unrelated and closer to the Sun than it is to the star cluster.\n\nWith an apparent magnitude of 11.7, the star cannot be seen by the naked eye, but is visible with a telescope in a dark sky with little light pollution.\n\nKIC 8462852 was observed as early as the year 1890. The star was cataloged in the Tycho, 2MASS, UCAC4, and WISE astronomical catalogs (published in 1997, 2003, 2009, and 2012, respectively).\n\nThe main source of information about the luminosity fluctuations of KIC 8462852 is the \"Kepler\" space observatory. During its primary and extended mission from 2009 to 2013 it continuously monitored the light curves of over 100,000 stars in a patch of sky in the constellations Cygnus and Lyra.\n\nObservations of the luminosity of the star by the \"Kepler\" space telescope show small, frequent, non-periodic dips in brightness, along with two large recorded dips in brightness roughly 750 days apart. The amplitude of the changes in the star's brightness, and the aperiodicity of the changes, mean that this star is of particular interest for astronomers. The star's changes in brightness are consistent with many small masses orbiting the star in \"tight formation\".\n\nThe first major dip, on 5 March 2011, reduced the star's brightness by up to 15%, and the next 726 days later (on 28 February 2013) by up to 22%. (A third dimming, around 8%, occurred 48 days later.) In comparison, a planet the size of Jupiter would only obscure a star of this size by 1%, indicating that whatever is blocking light during the star's major dips is not a planet, but rather something covering up to half the width of the star. Due to the failure of two of \"Kepler\" reaction wheels, the star's predicted 750-day dip around February 2015 was not recorded. The light dips do not exhibit an obvious pattern.\n\nIn addition to the day-long dimmings, a study of a century's worth of photographic plates suggests that the star has gradually faded from 1890 to 1989 by about 20%, which would be unprecedented for any F-type main-sequence star. Teasing accurate magnitudes from long-term photographic archives is a complex procedure, however, requiring adjustment for equipment changes, and is strongly dependent on the choice of comparison stars. Another study, examining the same photographic plates, concluded that the possible century-long dimming was likely a data artifact, and not a real astrophysical event. Another study from plates between 1895 and 1995 found strong evidence that the star has not dimmed, but kept a constant flux within a few percent, except a 8% dip on 24 October 1978, resulting in a period of the putative occulter of 738 days.\n\nA third study, using light measurements by the \"Kepler\" observatory over a four-year period, determined that KIC 8462852 dimmed at about 0.34% per year before dimming more rapidly by about 2.5% in 200 days. It then returned to its previous slow fade rate. The same technique was used to study 193 stars in its vicinity and 355 stars similar in size and composition to KIC 8462852. None of these stars exhibited such dimming.\n\nIn 2018, a possible periodicity in dimming of the star was reported.\n\nOriginally it was erroneously thought that, based on spectrum and stellar type, its changes in brightness could not be attributed to intrinsic variability. Consequently, a few hypotheses have been proposed involving material orbiting the star and blocking its light, although none of these fully fit the observed data.\n\nSome of the proposed explanations involve interstellar dust, a series of giant planets with very large ring structures, a recently captured asteroid field, the system undergoing Late Heavy Bombardment, and an artificial megastructure orbiting the star.\n\nAs of 2018, the leading hypothesis suggests that the \"missing\" heat flux involved in the star's dimming can be stored within the star's interior. Such variations in luminosity might arise from a number of mechanisms affecting the efficiency of heat transport inside the star.\n\nMeng et al. (2017) suggested that, based on observational data of KIC 8462852 from the Swift Gamma-Ray Burst Mission, Spitzer Space Telescope and Belgian AstroLAB IRIS Observatory, only \"microscopic fine-dust screens\", originating from \"circumstellar material\", are able to disperse the starlight in the way detected in their measurements. Based on these studies, on 4 October 2017, NASA reported that the unusual dimming events of KIC 8462852 are due to an \"uneven ring of dust\" orbiting the star. Although the explanation of a significant amount of small particles orbiting the star regards \"long-term fading\" as noted by Meng, the explanation also seems consistent with the week-long fadings found by amateur astronomer Bruce L. Gary and the Tabby Team, coordinated by astronomer Tabetha S. Boyajian, in more recent dimming events. A related, but more sophisticated, explanation of dimming events, involving a transiting \"brown dwarf\" in a 1600-day eccentric orbit near KIC 8462852, a \"drop feature\" in dimness and predicted intervals of \"brightening\", has been proposed. Dimming and brightening events of KIC 8462852 continue to be monitored; related light curves are currently updated and released frequently.\n\nNonetheless, data similar to that observed for KIC 8462852, along with supporting data from the Chandra X-ray Observatory, were found with dust debris orbiting WD 1145+017, a white dwarf that also has unusual light curve fluctuations. Further, the highly variable star RZ Piscium, which brightens and dims erratically, has been found to emit excessive infrared radiation, suggesting that the star is surrounded by large amounts of gas and dust, possibly resulting from the destruction of local planets.\n\nOne proposed explanation for the reduction in light is that it is due to a cloud of disintegrating comets orbiting the star elliptically. This scenario would assume that planetary system has something similar to the Oort cloud and that gravity from a nearby star caused comets from said cloud to fall closer into the system, thereby obstructing spectra. Evidence supporting this hypothesis includes an M-type red dwarf within of . The notion that disturbed comets from such a cloud could exist in high enough numbers to obscure 22% of the star's observed luminosity has been doubted.\n\nSubmillimetre-wavelength observations searching for farther-out cold dust in an asteroid belt akin to the Sun's Kuiper Belt suggest that a distant \"catastrophic\" planetary disruption explanation is unlikely; the possibility of a disrupted asteroid belt scattering comets into the inner system is still to be determined.\n\nAstronomer Jason T. Wright and others who have studied KIC 8462852 have suggested that if the star is younger than its position and speed would suggest, then it may still have coalescing material around it.\n\nA 0.8–4.2-micrometer spectroscopic study of the system using the NASA Infrared Telescope Facility (NASA IRTF) found no evidence for coalescing material within a few astronomical units of the mature central star.\n\nHigh-resolution spectroscopy and imaging observations have also been made, as well as spectral energy distribution analyses using the Nordic Optical Telescope in Spain. A massive collision scenario would create warm dust that glows in infrared wavelengths, but there is no observed excess infrared energy, ruling out massive planetary collision debris. Other researchers think the planetary debris field explanation is unlikely, given the very low probability that \"Kepler\" would ever witness such an event due to the rarity of collisions of such size.\n\nAs with the possibility of coalescing material around the star, spectroscopic studies using the NASA IRTF found no evidence for hot close-in dust or circumstellar matter from an evaporating or exploding planet within a few astronomical units of the central star. Similarly, a study of past infrared data from NASA's Spitzer Space Telescope and Wide-field Infrared Survey Explorer found no evidence for an excess of infrared emission from the star, which would have been an indicator of warm dust grains that could have come from catastrophic collisions of meteors or planets in the system. This absence of emission supports the hypothesis that a swarm of cold comets on an unusually eccentric orbit could be responsible for the star's unique light curve, but more studies are needed.\nIn December 2016, a team of researchers proposed that KIC 8462852 swallowed a planet, causing a temporary and unobserved increase in brightness due to the release of gravitational energy. As the planet fell into its star, it could have been ripped apart or had its moons stripped away, leaving clouds of debris orbiting the star in eccentric orbits. Planetary debris still in orbit around the star would then explain its observed drops in intensity. Additionally, the researchers suggest that the consumed planet could have caused the star to increase in brightness up to 10,000 years ago, and its stellar flux is now returning to the normal state.\nSucerquia et al. (2017) suggested that a large planet with oscillating rings may help explain the unusual dimmings associated with KIC 8462852.\nBallesteros et al. (2017) proposed a large, ringed planet trailed by a swarm of Trojan asteroids in its L5 Lagrangian point, and estimated an orbit that predicts another event in early 2021 due to the leading Trojans followed by another transit of the hypothetical planet in 2023. The model suggests a planet with a radius of 4.7 Jupiter radii, large for a planet (unless very young). An early red dwarf of about would be easily seen in infrared. The current radial velocity observations available (four runs at σ ≈ 400 m/s) hardly constrain the model, but new radial velocity measurements would greatly reduce the uncertainty. The model predicts a discrete and short-lived event for the May 2017 dimming episode, corresponding to the secondary eclipse of the planet passing behind KIC 8246852, with about a 3% decrease in the stellar flux with a transit time of about 2 days. If this is the cause of the May 2017 event, the planet's orbital period is more precisely estimated as 12.41 years with a semi-major axis of 5.9 AU.\n\nThe reddening observed during the deep dimming events of KIC 8462852 is consistent with cooling of its photosphere. It does not require obscuration by dust. Such cooling could be produced by a decreased efficiency of heat transport caused e.g. by decreased effectiveness of convection due to the star's strong differential rotation, or by changes in its modes of heat transport if it is located near the transition between radiative and convective heat transport. The \"missing\" heat flux is stored as a small increase of internal and potential energy.\n\nThe possible location of this early F star near the boundary between radiative and convective transport seems to be supported by the finding\nthat KIC 8462852's observed brightness variations appear to fit the \"avalanche statistics\" known to occur in a system close to a phase-transition. \"Avalanche statistics\" with a self-similar or power-law spectrum are a universal property of complex dynamical systems operating close to a phase transition or bifurcation point between two different types of dynamical behavior. Such close-to-critical systems are often observed to exhibit behavior that is intermediate between \"order\" and \"chaos\". Three other stars in the Kepler Input Catalog likewise exhibit similar \"avalanche statistics\" in their brightness variations, and all three are known to be magnetically active. It has been conjectured that stellar magnetism may be involved in KIC 8462852.\nSome astronomers have speculated that the objects eclipsing KIC 8462852 could be parts of a megastructure made by an alien civilization, such as a Dyson swarm, a hypothetical structure that an advanced civilization might build around a star to intercept some of its light for their energy needs. According to Steinn Sigurðsson, the megastructure hypothesis is implausible and disfavored by Occam's razor and fails to sufficiently explain the dimming. He says that it remains a valid subject for scientific investigation, however, because it is a falsifiable hypothesis. Due to extensive media coverage on this matter, has been compared by Kepler's Steve Howell to , another star with an odd light curve that was shown, after years of research, to be a part of a five-star system. The likelihood of extraterrestrial intelligence being the cause of the dimming is very low; however, the star remains an outstanding SETI target because natural explanations have yet to fully explain the dimming phenomenon. The latest results have ruled out explanations involving only opaque objects such as stars, planets, swarms of asteroids, or alien megastructures.\n\nNumerous optical telescopes continually monitor in anticipation of another multi-day dimming event, with planned follow-up observations of a dimming event using large telescopes equipped with spectrographs to determine if the eclipsing mass is a solid object, or composed of dust or gas. Additional follow-up observations may involve the ground-based Green Bank Telescope, the Very Large Array Radio Telescope, and future orbital telescopes dedicated to exoplanetology such as WFIRST, TESS, and PLATO.\n\nA Kickstarter fund-raising campaign was led by Tabetha Boyajian, the lead author of the initial study on anomalous light curve. The project proposes to use the Las Cumbres Observatory Global Telescope Network for continuous monitoring of the star. The campaign raised over , enough for one year of telescope time. Furthermore, more than fifty amateur astronomers working under the aegis of the American Association of Variable Star Observers have been providing effectively full coverage since AAVSO's alert about the star in October 2015, namely a nearly continuous photometric record. In a study published in January 2018, Boyajian et al. reported that whatever is blocking KIC 8462852 filters different wavelengths of light differently, so it cannot be an opaque object. They concluded that it is most likely space dust.\n\nIn October 2015, the SETI Institute used the Allen Telescope Array to look for radio emissions from possible intelligent extraterrestrial life in the vicinity of the star. After an initial two-week survey, the SETI Institute reported that it found no evidence of technology-related radio signals from the star system. No narrowband radio signals were found at a level of 180–300 Jy in a 1 Hz channel, or medium-band signals above 10 Jy in a 100 kHz channel.\n\nIn 2016, the VERITAS gamma-ray observatory was used to search for ultra-fast optical transients from astronomical objects, with astronomers developing an efficient method sensitive to nanosecond pulses with fluxes as low as about one photon per square meter. This technique was applied on archival observations of from 2009 to 2015, but no emissions were detected.\n\nIn May 2017, a related search, based on laser light emissions, was reported, with no evidence found for technology-related signals from KIC 8462852.\n\nIn September 2017, some SETI@Home workunits were created based on a previous RF survey of the region around this star. This was coupled with a doubling in the size of SETI@Home workunits, so the workunits related to this region will probably be the first workunits to have less issues with quantization noise.\n\nA star called EPIC 204278916, as well as some other young stellar objects, have been observed to exhibit dips similar to those observed in . They differ in several aspects, however. shows much deeper dips than , and they are grouped over a shorter period, whereas the dips at are spread out over several years. Furthermore, is surrounded by a proto-stellar disc, whereas appears to be a normal F-type star displaying no evidence of a disc.\n\nOn 20 May 2017, Boyajian and her colleagues reported, via The Astronomer's Telegram, on an ongoing dimming event (named \"Elsie\") which possibly began on 14 May 2017. It was detected by the Las Cumbres Observatory Global Telescope Network, specifically by its telescope located in Maui (LCO Maui). This was verified by the Fairborn Observatory (part of the N2K Consortium) in Southern Arizona (and later by LCO Canary Islands). Further optical and infrared spectroscopy and photometry were urgently requested, given the short duration, measured in days or weeks, of these events. Observations from multiple observers globally were coordinated, including polarimetry. Furthermore, the independent SETI projects Breakthrough Listen and Near-InfraRed Optical SETI (NIROSETI), both at Lick Observatory, continue to monitor the star. By the end of the three-day dimming event, a dozen observatories had taken spectra, with some astronomers having dropped their own projects to provide telescope time and resources. More generally the astronomical community was described as having gone \"mildly bananas\" over the opportunity to collect data in real-time on the unique star. The 2% dip event was named \"Elsie\" (in reference to Las Cumbres and light curve).<ref name=\"Dip update 6/n\"></ref>\n\nInitial spectra with FRODOSpec at the two-meter Liverpool Telescope showed no changes visible between a reference spectrum and this dip. Several observatories, however, including the twin Keck telescopes (HIRES) and numerous citizen science observatories, acquired spectra of the star, showing a dimming dip that had a complex shape, and initially had a pattern similar to the one at 759.75 days from the Kepler event 2, epoch 2 data. Observations were taken across the electromagnetic spectrum.\n\nEvidence of a second dimming event (named \"Celeste\") was observed on 13–14 June 2017, and which possibly began 11 June, by amateur astronomer Bruce L. Gary. While the light curve on 14 and 15 June indicated a possible recovery from the dimming event, the dimming continued to increase afterwards, and on 16 June, Boyajian wrote that the event was approaching a 2% dip in brightness.\n\nA third prominent 1% dimming event (named \"Skara Brae\") was detected beginning 2 August 2017, and which recovered by 17 August.\n\nA fourth prominent dimming event (named \"Angkor\") began 5 September 2017, and is, as of 16 September 2017, between 2.3% and 3% dimming event, making it the \"deepest dip this year\".\n\nAnother dimming event, amounting to a 0.3% dip, began around 21 September 2017 and completely recovered by 4 October 2017.\n\nOn 10 October 2017, an increasing brightening, lasting about two weeks, of the starlight from KIC 8462852 was noted by Bruce L. Gary of the Hereford Arizona Observatory and Boyajian. A possible explanation, involving a transiting \"brown dwarf\" in a 1600-day eccentric orbit near KIC 8462852, a \"drop feature\" in dimness and predicted intervals of \"brightening\", to account for the unusual fluctuating starlight events of KIC 8462852, has been proposed.\n\nOn 20 November 2017 (est), a fifth prominent dimming event began and had deepened to a 0.44% depth; as of 16 December 2017, the event recovered, leveled off at dip bottom for 11 days, faded again, to a current total dimming depth of 1.25%, and is now recovering again.\n\nDimming and brightening events of the star continue to be monitored; related light curves are currently updated and released frequently.\nThe star was too close to the Sun's position in the sky from late December, 2017 to mid February, 2018 to be seen. Observations resumed in late February. A new series of dips began on 16 March 2018. By 18 March 2018 the star was down by more than 1% in g-band, according to Bruce L. Gary, and about 5% in r-band, making it the deepest dip observed since the Kepler Mission in 2013, according to Tabetha S. Boyajian. A second even deeper dip with a depth of 5+% started on 24 March 2018, as confirmed by AAVSO observer John Hall.<ref name=\"dip update 6/n\"></ref><ref name=\"dip update 7/n\"></ref> As of 27 March 2018, that second dip is recovering.<ref name=\"dip update 8/n\"></ref>\n\n"}
{"id": "50636950", "url": "https://en.wikipedia.org/wiki?curid=50636950", "title": "Khamidbi M. Beshtoev", "text": "Khamidbi M. Beshtoev\n\nKhamidbi M. Beshtoev (5 May 1943 – 13 May 2016) was a Russian physicist.\n\nReferences\n"}
{"id": "1719638", "url": "https://en.wikipedia.org/wiki?curid=1719638", "title": "Law of specific nerve energies", "text": "Law of specific nerve energies\n\nThe law of specific nerve energies, first proposed by Johannes Peter Müller in 1835, is that the nature of perception is defined by the pathway over which the sensory information is carried. Hence, the origin of the sensation is not important. Therefore, the difference in perception of seeing, hearing, and touch are not caused by differences in the stimuli themselves but by the different nervous structures that these stimuli excite. For example, pressing on the eye elicits sensations of flashes of light because the neurons in the retina send a signal to the occipital lobe. Despite the sensory input's being mechanical, the experience is visual.\n\nHere is Müller's statement of the law, from \"Handbuch der Physiologie des Menschen für Vorlesungen\", 2nd Ed., translated by Edwin Clarke and Charles Donald O'Malley:\n\nAs the above quotation shows, Müller's law seems to differ from the modern statement of the law in one key way. Müller attributed the quality of an experience to some specific quality of the energy in the nerves. For example, the visual experience from light shining into the eye, or from a poke in the eye, arises from some special quality of the energy carried by optic nerve, and the auditory experience from sound coming into the ear, or from electrical stimulation of the cochlea, arises from some different, special quality of the energy carried by the auditory nerve. In 1912, Lord Edgar Douglas Adrian showed that all neurons carry the same energy, electrical energy in the form of action potentials. That means that the quality of an experience depends on the part of the brain to which nerves deliver their action potentials (e.g., light from nerves arriving at the visual cortex and sound from nerves arriving at the auditory cortex).\n\nIn 1945, Roger Sperry showed that it is the location in the brain to which nerves attach that determines experience. He studied amphibians whose optic nerves cross completely, so that the left eye connects to the right side of the brain and the right eye connects to the left side of the brain. He was able to cut the optic nerves and cause them to regrow on the opposite side of the brain so that the left eye now connected to the left side of the brain and the right eye connected to the right side of the brain. He then showed that these animals made the opposite movements from the ones they would have made before the operation. For example, before the operation, the animal would move to the left to get away from a large object approaching from the right. After the operation, the animal would move to the right in response to the same large object approaching from the right. Sperry showed similar results in other animals including mammals (rats), this work contributing to his Nobel Prize in 1981.\n\n"}
{"id": "14405683", "url": "https://en.wikipedia.org/wiki?curid=14405683", "title": "List of Pennsylvania state symbols", "text": "List of Pennsylvania state symbols\n\nThe U.S. state of Pennsylvania has 21 official emblems, as designated by the Pennsylvania General Assembly and signed into law by the Governor of Pennsylvania.\n\n"}
{"id": "5786489", "url": "https://en.wikipedia.org/wiki?curid=5786489", "title": "List of Plan 9 programs", "text": "List of Plan 9 programs\n\nThis is a list of Plan 9 programs. Many of these programs are very similar to the UNIX programs with the same name, other are to be found only on Plan 9. Others again share only the name, but have a different behaviour.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "46576802", "url": "https://en.wikipedia.org/wiki?curid=46576802", "title": "List of aftershocks of April 2015 Nepal earthquake", "text": "List of aftershocks of April 2015 Nepal earthquake\n\nThe following is a list of aftershocks that occurred after the earthquake in Nepal on 25 April 2015. There was some seismic activity in the Jumla district before the main shock. However, they are not foreshocks to the main shock. , over 304 aftershocks have occurred. The large number of aftershocks after the earthquake is considered normal by seismologists.\n\nAccording to seismologist Roger Musson, the standard pattern for aftershocks is that the biggest aftershock will be one day after, and one magnitude less. Thus, the 6.7 magnitude aftershock on 26 April 2015 following the 7.9 magnitude main shock would fit this pattern. It is highly unlikely that the earthquake was a foreshock preceding an even larger earthquake. Therefore, an earthquake larger than 7.9 in the near future after 25 April 2015 is not expected.\n\nThe list below is incomplete and maybe inaccurate for some aftershocks.\nNote: The list below shows bigger earthquakes only.\n\n"}
{"id": "14485544", "url": "https://en.wikipedia.org/wiki?curid=14485544", "title": "List of members of the National Academy of Sciences (Applied physical sciences)", "text": "List of members of the National Academy of Sciences (Applied physical sciences)\n"}
{"id": "40201784", "url": "https://en.wikipedia.org/wiki?curid=40201784", "title": "List of things named after James Clerk Maxwell", "text": "List of things named after James Clerk Maxwell\n\nThis is a list of things named for James Clerk Maxwell.\n\n\n\n\n\n\n\n\n\nMaxwellian\n"}
{"id": "30552634", "url": "https://en.wikipedia.org/wiki?curid=30552634", "title": "MIMOS II", "text": "MIMOS II\n\nMIMOS II is the miniaturised Mössbauer spectrometer, developed by Dr. Göstar Klingelhöfer at the Johannes Gutenberg University in Mainz, Germany, that is used on the Mars Exploration Rovers \"Spirit\" and \"Opportunity\" for close-up investigations on the Martian surface of the mineralogy of iron-bearing rocks and soils.\n\nMIMOS II uses a Cobalt-57 gamma ray source of about 300 mCi at launch which gave a 6-12 hr time for acquisition of a standard MB spectrum during the primary mission on Mars, depending on total Fe content and which Fe-bearing phases are present. \nCobalt-57 has a half-life of only 271.8 days (hence the extended measuring times now on Mars after over a decade).\n\nThe MIMOS II sensorheads used on Mars are approx 9 cm x 5 cm x 4 cm and weigh about 400g\n\nThe MIMOS II system also includes a circuit board of about 100g.\n\n"}
{"id": "2091420", "url": "https://en.wikipedia.org/wiki?curid=2091420", "title": "Maucherite", "text": "Maucherite\n\nMaucherite is a grey to reddish silver white nickel arsenide mineral. It crystallizes in the tetragonal crystal system. It occurs in hydrothermal veins alongside other nickel arsenide and sulfide minerals. It is metallic and opaque with a hardness of 5 and a specific gravity of 7.83. It is also known as placodine and Temiskamite. The unit cell is of symmetry group \"P4\"22 or \"P4\"22. \n\nIt has the chemical formula: NiAs and commonly contains copper, iron, cobalt, antimony, and sulfur as impurities.\n\nIt was discovered in 1913 in Eisleben, Germany and was named after Wilhelm Maucher (1879–1930), a German mineral collector.\n\n"}
{"id": "164597", "url": "https://en.wikipedia.org/wiki?curid=164597", "title": "Mean flow", "text": "Mean flow\n\nIn fluid dynamics, the fluid flow is often decomposed into a mean flow – and deviations from the mean. The averaging can be done either in space or in time, or by ensemble averaging.\n\nCalculation of the mean flow may often be as simple as the mathematical mean: simply add up the given flow rates and then divide the final figure by the number of initial readings.\n\nFor example, given two discharges (\"Q\") of 3 m³/s and 5 m³/s, we can use these flow rates \"Q\" to calculate the mean flow rate \"Q\". Which in this case is \"Q\" = 4 m³/s.\n\n\n"}
{"id": "22448503", "url": "https://en.wikipedia.org/wiki?curid=22448503", "title": "Mercury-Redstone Launch Vehicle", "text": "Mercury-Redstone Launch Vehicle\n\nThe Mercury-Redstone Launch Vehicle, designed for NASA's Project Mercury, was the first American manned space booster. It was used for six sub-orbital Mercury flights from 1960–61; culminating with the launch of the first, and 11 weeks later, the second American (and the second and third humans) in space. The four subsequent Mercury human spaceflights used the more powerful Atlas booster to enter low Earth orbit.\n\nA member of the Redstone rocket family, it was derived from the U.S. Army's Redstone ballistic missile and the first stage of the related Jupiter-C launch vehicle; but to human-rate it, the structure and systems were modified to improve safety and reliability.\n\nNASA chose the U.S. Army's Redstone liquid-fueled ballistic missile for its suborbital flights as it was the oldest one in the US fleet, having been active since 1953 and had many successful test flights.\n\nThe standard military Redstone lacked sufficient thrust to lift a Mercury capsule into the ballistic suborbital trajectory needed for the project; however, the first stage of the Jupiter-C, which was a modified Redstone with lengthened tanks, could carry enough propellant to reach the desired trajectory. Therefore, this Jupiter-C first stage was used as the starting point for the Mercury-Redstone design. The Jupiter-C's engine, however, was being phased out by the Army, so to avoid potential complications such as parts shortages or design revisions, the Mercury-Redstone designers chose the Rocketdyne A-7 engine used on the latest military Redstones. Hans Paul and William Davidson, propulsion engineers at the Army Ballistic Missile Agency (ABMA), were assigned the task of modifying the A-7 to be safe and reliable for manned flights.\n\nDuring 1959, most of ABMA were preoccupied with the Saturn project, but those engineers who could find enough free time in their schedule were invited to work on man-rating the Jupiter-C. As a starting point, the most obvious step was getting rid of its staging capability as the Mercury-Redstone would not utilize upper stages. Many of the more advanced Jupiter-C components were also removed for reliability reasons or because they were not necessary for Project Mercury.\n\nThe standard Redstone was fueled with a 75 percent ethyl alcohol 25 percent water solution, essentially the same propellants as the V-2, but the Jupiter-C first stage had used hydyne fuel, a blend of 60 percent unsymmetrical dimethylhydrazine (UDMH) and 40 percent diethylenetriamine (DETA). This was a more powerful fuel than ethyl alcohol, but it was also more toxic, which could be hazardous for an astronaut in a launch pad emergency. Furthermore, hydyne had never been used with the new A-7 engine. The Mercury-Redstone designers rejected hydyne and returned to the standard ethyl alcohol fuel. The lengthened propellant tanks were thus also necessary in lieu of using more powerful fuel.\n\nUse of alcohol created a problem with the Mercury-Redstone in that the graphite thrust vector vanes could be eroded due to the significantly longer burn time, hence NASA put out a requirement that the launch vehicles needed high quality vanes.\n\nBecause Mercury-Redstone had larger propellant tanks than the Redstone missile, an additional nitrogen bottle was added for tank pressurization, and an extra hydrogen peroxide tank for powering the turbopump due to the longer burn time.\n\nThe most important change in making the Mercury-Redstone a suitable vehicle for an astronaut was the addition of an automatic in-flight abort sensing system. In an emergency where the rocket was about to suffer a catastrophic failure, an abort would activate the launch escape system attached to the Mercury capsule, which would rapidly eject it from the booster. Either the astronaut or the ground controllers could initiate an abort manually, but some potential failures during flight might lead to disaster before an abort could be manually triggered.\n\nThe Mercury-Redstone's automatic in-flight abort sensing system solved this problem by monitoring the rocket's performance during flight. If it detected an anomaly which might threaten the astronaut, such as loss of flight control, engine thrust, or electrical power, it would automatically abort, shutting down the engine and activating the capsule's escape system. The abort system could not shut off the engine until at least 30 seconds after liftoff in order to prevent a malfunctioning launch vehicle from coming down on or near the pad; during the initial 30 seconds, only the Range Safety Officer could terminate the flight. Review of flight data from the more than 60 Redstone and Jupiter C launches since 1953 was used to analyze the most likely failure modes of this launch vehicle family. In the interest of simplicity, the abort sensing system had to be kept as simple as possible and only monitor parameters that were vital to booster operation. An automatic abort could be triggered by any of the following conditions, all of which could be indicative of a catastrophic launch vehicle malfunction:\n\n\nInstant abort capability was important because certain failure modes such as loss of thrust upon liftoff (for example the third Redstone test flight in May 1954) could result in an immediate catastrophic situation. Other failure modes such as deviation from the proper flight path or a drop in engine chamber pressure during ascent did not necessarily present an immediate risk to the astronaut's safety and he could either initial a manual abort by pulling a lever in the capsule to activate the Launch Escape System or ground control could send a command to activate it.\n\nThe range safety system was modified slightly in that a three-second delay would take place between engine cutoff and missile destruct so as to give the escape tower enough time to pull the capsule away.\n\nThe most visible difference between the Jupiter-C first stage and the Mercury-Redstone was in the section just below the Mercury capsule and above the propellant tanks. This section was known as the \"aft section\", a term which was inherited from the military Redstone. (The actual rear end of the rocket was called the \"tail section\".) The aft section held most of the Mercury-Redstone's electronics and instrumentation, including the guidance system, as well as the adapter for the Mercury capsule. In the military Redstone and the Jupiter-C first stage, when the rocket had burned out, its lower portion, containing the rocket engine and propellant tanks, would separate from the aft section and be discarded, and the aft section, with its guidance system, would direct the top half of the rocket during its unpowered ballistic flight. However, in the Mercury-Redstone, the aft section was permanently attached to the lower portion of the rocket. When the rocket had shut down, the Mercury capsule would separate from the aft section and would rely on its own guidance.\n\nOther changes were made to improve the Mercury-Redstone's reliability. The standard Redstone's ST-80 inertial guidance system was replaced in the Mercury-Redstone with the simpler LEV-3 autopilot. The LEV-3, whose design dated back to the German V-2 missile, was not as sophisticated or as precise as the ST-80, but it was accurate enough for the Mercury mission and its simplicity made it more reliable. A special instrument compartment was built in the \"aft section\" to hold the most important instrumentation and electronics, including the guidance system, the abort and destruct systems, the telemetry instrumentation, and the electrical power supplies. To reduce the chance of failure in this equipment, this compartment was cooled before launch and kept pressurized during flight.\n\nThe fuel prevalves were deleted from the Mercury-Redstone in the interest of improved reliability, since if they closed during a launch, an abort condition could be triggered. On the three unmanned flights, it was discovered that the Mercury-Redstone exhibited a roll transient of 8° per second versus 4° for the Redstone missile. Although this was below the 12° per second roll transient required to trigger an abort, the roll rate sensor was removed from the two manned flights to reduce the chances of an accidental abort (the booster still retained the roll attitude angle sensor which would be triggered at 10°).\n\nMercury-Redstone 1A and Mercury-Redstone 2 both experienced overacceleration in flight, the former due to a problem with an accelerometer, the latter due to a problem with the LOX regulator which oversupplied the engine with oxidizer and caused thrust termination to occur 1.2 seconds early. The ASIS system activated and the escape tower yanked the capsule away, subjecting its chimpanzee passenger to high G loads. The third flight, Mercury-Redstone BD, was designed as an engineering test to correct these problems before the booster could be considered man-rated.\n\nThe space between the pressurized instrument compartment and the capsule was originally intended to hold a parachute recovery system for the rocket, but it had been left empty after this system was abandoned. The three unmanned Mercury-Redstone flights exhibited high vibration levels and structural bending in the adapter area, so Alan Shepard's flight included 340 pounds of lead-infused plastic in the adapter section along with additional bracing and stiffeners. After Shepard still reported noticeable vibration during launch, Gus Grissom's booster included even more ballast. The Atlas booster used for orbital Mercury flights had also experienced this issue, but with more catastrophic results as Mercury-Atlas 1 was destroyed in-flight due to structural failure caused by excessive flexing at the point where the booster mated with the capsule adapter.\n\nIn total, some 800 modifications were made to the Redstone design in the process of adapting it for the Mercury program. The process of man-rating Redstone was so extensive that NASA quickly found themselves not using an off-the-shelf rocket, but what was in effect a completely new one and thus negating all of the hardware and flight test data from previous Redstone and Jupiter-C launches. This created a series of disputes between Von Braun's team at ABMA and NASA, as the former preferred simply making the abort system as foolproof as possible so as to guarantee that the astronaut would be bailed out of a malfunctioning launch vehicle, while the latter favored maximum booster reliability to minimize the chance of aborts happening at all.\n\nThe Mercury-Redstone designers originally planned for the rocket to be recovered by parachute after its separation from the Mercury capsule. This was the first significant effort to develop a recoverable launch vehicle and the first to reach the testing phase.\n\nThe recovery system, at the top of the rocket, would have used two stages of parachutes. In the first stage, a single parachute, in diameter, would stabilize the rocket's fall and slow its descent. This parachute would then draw out a set of three main parachutes, each across. The rocket would come down in the Atlantic Ocean, to be recovered by ship.\n\nTo determine the feasibility of this system, several tests were performed on full-sized Redstones, including water impact and flotation tests, and an exercise at sea in which a floating Redstone was picked up by a Navy recovery ship. All these tests showed recovery of the rocket to be workable. Further development was halted, however, due to lack of funding, so the parachute system was not tested.\n\nMercury-Redstone flights were designated with the prefix \"MR-\". Confusingly, the Mercury-Redstone boosters used for these flights were designated in the same way, usually with different numbers. (In photographs, this designation can sometimes be seen on the rocket's tail end.) Two rockets, MR-4 and MR-6, were never flown. Although there had been rumors that NASA in the very beginning of Project Mercury had intended to launch each astronaut on a suborbital mission before beginning orbital Atlas flights, they only purchased eight Mercury-Redstone boosters, one of which was damaged in the unsuccessful MR-1 launch and not reused, and another used for the MR-BD flight (the original schedule was for one unmanned Mercury-Redstone flight, one chimpanzee flight, and six manned flights). Since Alan Shepard and Gus Grissom's flights were successful and since the Soviet Union had flown two orbital manned space flights by the late summer of 1961, there was no need to continue with Redstone missions.\n\n"}
{"id": "18328126", "url": "https://en.wikipedia.org/wiki?curid=18328126", "title": "Michał Abramowicz", "text": "Michał Abramowicz\n\nMichał Abramowicz (1884 – 1965) was an Azerbaijani-Soviet geologist of Polish origin. He was born and died in Baku, the capital of Azerbaijan and was a member of the Academy of Sciences of Azerbaijan.\n\n"}
{"id": "29482487", "url": "https://en.wikipedia.org/wiki?curid=29482487", "title": "Mission: Earth, Voyage to the Home Planet", "text": "Mission: Earth, Voyage to the Home Planet\n\nMission: Earth, Voyage to the Home Planet is a children's literature book by science writer June A. English and astronaut Thomas David Jones that was published in 1996 by Scholastic. Jones was among the crew members of the Space Shuttle \"Endeavour\" during an eleven-day mission in space, which was launched in April 1994 to study the ecological well-being of Earth using specialized radar technology. The book, which is illustrated with radar images and picturesque photographs, chronicles the mission and Jones' experiences of it.\n\n\"Mission: Earth, Voyage to the Home Planet\" received a generally favorable reception in media coverage and book reviewers. The \"Pittsburgh Post-Gazette\" said, \"The authors convey the awe and wonderment of seeing Earth from space and the intricate delicacy of the Earth's ecology\". \"The Dallas Morning News\" said, \"The astronaut's descriptions are vivid\". A review in \"Booklist\" was more critical; it said, \"The authors try to cover too much in so few pages, and the narrative, with several focal points, becomes simplified at times\". \"School Library Journal\" wrote, \"It provides a unique look at a new method of research and an opportunity for youngsters to read one scientist's account of what it's like to engage in this exciting field of endeavor\". The book was selected for inclusion in books \"Best Books for Children\", and \"Adventuring With Books\", or educating youths about history by using children's literature works.\n\nJune A. English is an author who specializes in science writing. She has published works within the genre of children's literature.\n\nThomas David Jones was a member of the United States Air Force prior to working for NASA. As Jones was a United States federal employee at the time of the book's publication, he did not receive profits from sales of the work. English and Jones later produced another collaborative work, \"Scholastic Encyclopedia of the United States at War\", which was published in 1998. Jones wrote another book about his experiences in space, \"Sky Walking: An Astronaut's Memoir\", which was published in 2006.\n\nIn \"Mission: Earth, Voyage to the Home Planet\", astronaut Thomas David Jones recounts his experiences studying the Earth during his first mission with the United States space agency NASA. Jones' space mission lasted eleven days. Jones was a member of the crew of the Space Shuttle \"Endeavour\", which left Earth in April 1994. The mission was intended to provide the Space Radar Lab with a view of the planet. By using three separate radar echoes, scientists were able to analyze a full image of the planet. The radar technology could penetrate obstructions, including clouds and thick outgrowth of trees. Another purpose of the mission was to analyze the ecological state of the planet and to take measurements of pollution caused by carbon monoxide.\n\nThe book is based upon journal entries written by Jones during the mission. He presents a typical NASA mission member's day-to-day experiences while living on the space shuttle. Jones illustrates his account with photographs from his mission and pictures from the radar capture. He describes his observations while looking at the Earth from space, for example he says, \"Maybe more than any other sight from space, lightning gives the decided impression that the Earth is alive, a living organism\". He described aurora phenomena as, \"shimmering fingers of green light reaching up from the dark ocean, an arc of light around the South Magnetic Pole\". June English provides contextual information about Jones' mission.\n\n\"Mission: Earth, Voyage to the Home Planet\" received a positive review from journalist Rebecca O'Connell, who wrote in the \"Pittsburgh Post-Gazette\", \"The authors convey the awe and wonderment of seeing Earth from space and the intricate delicacy of the Earth's ecology\". She said of the descriptive nature of the text, \"Their descriptions of scientific phenomena read like revelations of mystic wonders. But this is not to say it is ponderous, not at all.\" O'Connell's review concluded, \"The text is packed with fascinating information and humorous touches. Even children who are not ordinarily interested in space or ecology will be interested in this book, and aficionados will go ga-ga.\" Leigh Fenly of \"The San Diego Union-Tribune\" recommended the book, and said, \"Science writer June English tracked the launch and adds perspective on how the mission will help scientists understand Earth, including its atmosphere, geology, ecologyand future\". Nita Thurman wrote for \"The Dallas Morning News\", \"\"Mission: Earth, A Journey to the Home Planet\" has dozens of color photos taken inside the shuttle and through its windows. The astronaut's descriptions are vivid.\" \"Publishers Weekly\" wrote favorably of the book, saying, \"Astronaut Jones's enthusiasm for his work is contagious; it shines through the text, making for a roundly enjoyable and informative read\". \"Publishers Weekly\" recommended the book and said it is, \"[w]ell-conceived and thoughtfully executed, this book deserves a wide audience.\"\n\nA review of the book by Ilana Steinhorn in \"Booklist\" was more critical. Steinhorn said, \"The authors try to cover too much in so few pages, and the narrative, with several focal points, becomes simplified at times ... Still, the account is interesting, and the many high-quality, often breathtaking color photographs and radar images complement the text nicely\". Elaine Fort Weischedel reviewed the book for \"School Library Journal\", and recommended it for teaching specific subject matter. She wrote, \"Pollution patterns, shifting ocean currents, destruction of rain forests, and other environmental changes were the heart of Jones's project, and this book might be used more successfully to supplement the curriculum in those areas than in units on space flight per se\". Weischedel said the book \"provides a unique look at a new method of research and an opportunity for youngsters to read one scientist's account of what it's like to engage in this exciting field of endeavor\". \"Horn Book Review\" called the book \"a uniquely personal account\", and said it \"features a crowded design, but the many color photographs throughout the text are fascinating\".\n\n\"Mission: Earth, Voyage to the Home Planet\" is listed in the book \"Teaching U.S. history through children's literature: post-World War II\" by Wanda J. Miller as a resource for educating youth about history by using children's literature works. It is used in other such books, including \"Strategies That Work: Teaching Comprehension for Understanding and Engagement\" by Stephanie Harvey and Anne Goudvis, \"Reading Comprehension: Books and Strategies for the Elementary Curriculum\" by Kathryn K. Matthew and Kimberly Kimbell-Lopez, and \"An Integrated Language Perspective in the Elementary School: An Action Approach\" by Christine Pappas, Barbara Zulandt Kiefer, and Linda S. Levstik. It was selected for inclusion in \"Best Books for Children\" by John Thomas Gillespie, and \"Adventuring With Books\" by Kathryn Mitchell Pierce.\n\n\n\n"}
{"id": "14529483", "url": "https://en.wikipedia.org/wiki?curid=14529483", "title": "Otto Tetens", "text": "Otto Tetens\n\nOtto Tetens (26 September 1865, Rendsburg, Germany – 15 February 1945, Teplitz-Schönau) was a German natural scientist with an astronomer background.\n\nTetens was the son of a high ranked police officer in Schleswig, Northern Germany. He went to several universities for different natural science topics in Tübingen, Munich, Berlin and Kiel.\nAfter this period, he worked at the private astronomy observatory of Miklos Konkly-Thege in Ogyalla in Hungary.\nIn 1891 he got his Doctor degree of Natural Science. He then worked at different institutes in astronomy such as the Deutsche Seewarte, Hamburg, and the Observatory of Strasbourg.\n\nFrom 1902 to 1905 Otto Tetens worked for the Royal Society of Science of Göttingen on a climate project in Samoa, then a German colony. He founded the Apia Observatory in June 1902 at Mulinuu near the main town of Apia. After his time in Samoa, he went back to Germany and Göttingen to work on his reports for the previous three years and then took an occupation at the Royal Astronomy Observatory in Kiel, Germany.\n\nFrom 1909 to 1931 Tetens worked at the meteorological observatory in , Germany (south-east of Berlin) as chief-observer. After retirement he moved to Bad Saarow, some 10 km away from Lindenberg and lived at his house in Seestrasse] at the Lake Scharmützelsee.\n\nHe died in 1945 while staying at a sanatorium in Teplitz-Schönau. His wife Dorothee Heimrod, daughter of the Council of the United States in Samoa and later Switzerland, moved back to the USA in 1947 and died 1962 in New York City.\n\nDuring his time in Samoa, Otto Tetens took a great number of valuable and interesting photos of his time and the people of Samoa. These photos were exhibited in Germany and Samoa in 2004 and 2005.\n\nThe photo with the paramount chief Josefo Mataafa shows the way Otto Tetens was living in Samoa: as a partner on the same level, with strong adoption of Samoan culture and habits. His houses for work and living at the Mulinuu observatory were built as Samoan fales, not as German wood houses. The photo was taken in 1904 by Otto Tetens.\n\nChristiane Niggemann (Hrsg.): Samoa 1904. Menschen, Landschaft und Kultur im Süd-Pazifik vor Hundert Jahren. Fotos von Otto Tetens in Samoa 1902–1905. Arkana-Verlag, Göttingen 2004 (Germany), \n\nFelix Lühning. \"...Eine ausnehmende Zierde und Vortheil\". Geschichte der Kieler Universitätssternwarte und ihrer Vorgängerinnen 1770–1950. Zwei Jahrhunderte Arbeit und Forschung zwischen Grenzen und Möglichkeiten. Neumünster: Wachholtz, 2007 (Habilitationsschrift, Fachbereich Mathematik der Universität Hamburg 2002).\n\nHans Steinhagen: Lindenberger Himmelsjäger – Miniaturen rund um das Observatorium. Arkana-Verlag, Göttingen 2011 (Germany), S. 70 ff\n\n"}
{"id": "26348335", "url": "https://en.wikipedia.org/wiki?curid=26348335", "title": "Physical test", "text": "Physical test\n\nA physical test is a qualitative or quantitative procedure that consists of determination of one or more characteristics of a given product, process or service according to a specified procedure. Often this is part of an experiment.\n\nPhysical testing is common in physics, engineering, and quality assurance.\n\nPhysical testing might have a variety of purposes, such as:\n\nSome physical testing is \"performance testing\" which covers a wide range of engineering or functional evaluations where a material, product, or system is not specified by detailed material or component specifications. Rather, emphasis is on the final measurable performance characteristics. Testing can be a qualitative or quantitative procedure.\nMany Acceptance testing protocols employ performance testing e.g. In the stress testing of a new design of chair.\n\n\n\n"}
{"id": "12522048", "url": "https://en.wikipedia.org/wiki?curid=12522048", "title": "Puerto Rican hutia", "text": "Puerto Rican hutia\n\nThe Puerto Rican hutia (\"Isolobodon portoricensis\") is an extinct species of rodent in the family Capromyidae. It was found in the Dominican Republic, Haiti, and Puerto Rico.\n\nThe Puerto Rican hutia was a vital food source for the Amerindians for many years. With being hunted by Arawak Indians, they continued to survive until the arrival of early European explorers. Christopher Columbus and his crew are believed to have eaten the species upon their arrival. The species declined following European colonization of the West Indies. It is unclear whether it survived after facing threats from the early introduction of black rats (\"Rattus rattus\") by the first European settlers around 1500, although it may have been finally wiped out by introduced mongooses in the nineteenth or early 20th century. Although commonly regarded as extinct, some researchers hold out hopes that the species still survives in undisturbed refuges.\n"}
{"id": "31477756", "url": "https://en.wikipedia.org/wiki?curid=31477756", "title": "Quantum capacity", "text": "Quantum capacity\n\nIn the theory of quantum communication, the quantum capacity is the highest rate at which quantum information can be communicated over many independent uses of a noisy quantum channel from a sender to a receiver. It is also equal to the highest rate at which entanglement can be generated over the channel, and forward classical communication cannot improve it. The quantum capacity theorem is important for the theory of quantum error correction, and more broadly for the theory of quantum computation. The theorem giving a lower bound on the quantum capacity of any channel is colloquially known as the LSD theorem, after the authors Lloyd, Shor, and Devetak who proved it with increasing standards of rigor.\n\nThe LSD theorem states that the coherent information of a quantum channel is an achievable rate for reliable quantum communication. For a Pauli channel, the coherent information has a simple form and the proof that it is achievable is particularly simple as well. We prove the theorem for this special case by exploiting random stabilizer codes and correcting only the likely errors that the channel produces.\n\nTheorem (hashing bound). There exists a stabilizer quantum error-correcting code that achieves the hashing limit formula_1 for a Pauli channel of the following form:\nwhere formula_3 and formula_4 is the entropy of this probability vector.\n\nProof. Consider correcting only the typical errors. That is, consider defining the\ntypical set of errors as follows:\nwhere formula_6 is some sequence consisting of the letters formula_7 and formula_8 is the probability\nthat an IID Pauli channel issues some tensor-product error formula_9. This typical set consists of the likely errors in the sense that\nfor all formula_11 and sufficiently large formula_12. The error-correcting\nconditions for a stabilizer code formula_13 in this case are that formula_14 is a correctable set of errors if\nfor all error pairs formula_16 and formula_17 such that formula_18 where formula_19 is the normalizer of formula_13. Also, we consider the expectation of the error probability under a random choice of a stabilizer code.\n\nProceed as follows:\nThe first equality follows by definition—formula_22 is an indicator function equal to one if formula_16 is uncorrectable under formula_13 and equal to zero otherwise. The first inequality follows, since we correct only the typical errors because the atypical error set has negligible probability mass. The second equality follows by exchanging the expectation and the sum. The third equality follows because the expectation of an indicator function is the probability that the event it selects occurs. Continuing, we have\nThe first equality follows from the error-correcting conditions for a quantum stabilizer code, where formula_32 is the normalizer of\nformula_13. The first inequality follows by ignoring any potential degeneracy in the code—we consider an error uncorrectable if it lies in the normalizer formula_32 and the probability can only be larger because formula_35. The second equality follows by realizing that the probabilities for the existence criterion and the union of events are equivalent. The second inequality follows by applying the union bound. The third inequality follows from the fact that the probability for a fixed operator formula_36 not equal to the identity commuting with\nthe stabilizer operators of a random stabilizer can be upper bounded as follows:\nformula_37\nThe reasoning here is that the random choice of a stabilizer code is equivalent to\nfixing operators formula_38, ..., formula_39 and performing a uniformly random\nClifford unitary. The probability that a fixed operator commutes with\nformula_40, ..., formula_41 is then just the number of\nnon-identity operators in the normalizer (formula_42) divided by the total number of non-identity operators (formula_43). After applying the above bound, we then exploit the following typicality bounds:\nWe conclude that as long as the rate formula_46, the expectation of the error probability becomes arbitrarily small, so that there exists at least one choice of a stabilizer code with the same bound on the error probability.\n\n"}
{"id": "17718357", "url": "https://en.wikipedia.org/wiki?curid=17718357", "title": "SAT Subject Test in Physics", "text": "SAT Subject Test in Physics\n\nThe SAT Subject Test in Physics, Physics SAT II, or simply the Physics SAT, is a one-hour multiple choice test on physics administered by The College Board in the United States. A high school student generally chooses to take the test to fulfill college entrance requirements for the schools at which the student is planning to apply. Until 1994, the SAT Subject Tests were known as Achievement Tests; until January 2005, they were known as SAT IIs; they are still well known by this name.\n\nThe material tested on the Physics SAT is supposed to be equivalent to that taught in a junior- or senior-level high school physics class. It requires critical thinking and test-taking strategies, at which high school freshmen or sophomores may be inexperienced. The Physics SAT tests more than what normal state requirements are; therefore, many students prepare for the Physics SAT using a preparatory book or by taking an AP course in Physics.\n\nThe SAT Subject Test in Physics has 75 questions and consists of two parts: Part A and Part B.\n\nPart A: \n\nPart B:\n\nThe test has 75 multiple choice questions that are to be answered in one hour. All questions have five answer choices. Students receive 1 point for every correct answer, lose ¼ of a point for each incorrect answer, and receive 0 points for questions left blank. This score is then converted to a scaled score of 200–800. The mean score for the 2006–07 test administrations was 643 with a standard deviation of 107. Sample percentile ranks for the 2008 administrations are available from the College Board.\n\nThe College Board's recommended preparation is a one-year college preparatory course in physics, a one-year course in algebra and trigonometry, and experience in the laboratory.\n\nStudents taking the SAT Subject Test in Physics are prohibited from using any resources during the test, including textbooks, notes, or formula sheets. Although there are mathematics questions including trigonometry, the use of a calculator is not allowed. All scratch work must be done directly in the test booklet.\n\n\n"}
{"id": "17720432", "url": "https://en.wikipedia.org/wiki?curid=17720432", "title": "Series (stratigraphy)", "text": "Series (stratigraphy)\n\nSeries are subdivisions of rock layers based on the age of the rock and formally defined by international conventions of the geological timescale. A series is therefore a sequence of strata defining a chronostratigraphic unit. Series are subdivisions of systems and are themselves divided into stages.\n\nSeries is a term defining a unit of rock layers formed during a certain interval of time (a chronostratigraphic unit); it is equivalent (but not synonymous) to the term \"geological epoch\" (see epoch criteria) which defines the interval of time itself, although the two words are sometimes confused in informal literature.\n\nThe geological timescale has all systems in the Phanerozoic eonothem subdivided into series. Some of these have their own names, in other cases a system is simply divided into a Lower, Middle and Upper series. The Cretaceous system is for example divided into the Upper Cretaceous and Lower Cretaceous series; while the Carboniferous system is divided in the Pennsylvanian and Mississippian series. In 2008, the International Commission on Stratigraphy had not yet named all four series of the Cambrian. Currently series are limited to the Phanerozoic, but the ICS has stated its intention of subdividing the three systems of the Neoproterozoic (Ediacaran, Cryogenian and Tonian) into stages too.\n\nSystems can include many lithostratigraphic units (for example formations, beds, members, etc.) of differing rock types that were being laid down in different environments at the same time. In the same way, a lithostratigraphic unit can include a number of systems or parts of them.\n\n"}
{"id": "1855958", "url": "https://en.wikipedia.org/wiki?curid=1855958", "title": "Shuttle Avionics Integration Laboratory", "text": "Shuttle Avionics Integration Laboratory\n\nThe Shuttle Avionics Integration Laboratory (SAIL) was a facility at Lyndon B. Johnson Space Center in Houston, Texas.\n\nIt was the only facility in the Space Shuttle Program where actual orbiter hardware and flight software can be integrated and tested in a simulated flight environment. It supported the entire Space Shuttle program to perform integrated verification tests. It also contained Firing Room Launch Equipment identical to that used at KSC. Thus complete ground verifications as well as countdown and abort operations could be tested and simulated.\n\nThe testing process is extensive and rigorous; the software on the Shuttle is often considered to be among the most bug-free of operational systems.\n\nThe laboratory contains a complete avionics mock-up of a Shuttle, designated OV-095. While only a skeleton of an orbiter, the electronics are identical in position and type to those used on the Shuttle; it is a sufficiently faithful replica that crews sometimes prefer to use it to train on, rather than the training simulators.\n\nNASA personnel who have been assigned to SAIL testing include Charlie Bolden (former NASA Administrator), Michael Coats (former Director at JSC NASA), Brewster Shaw (Boeing Vice President of Space Exploration Division) and Al Crews (selected as an astronaut for the X-20 Dyna-Soar).\n\nThe first SAIL commander was James E. Westom of Rockwell International, retired Major USAF. He flew the Approach and Landing phase in SAIL before Space Shuttle Enterprise was launched off the top of the NASA C-747 airplane to prove it could fly on its own in the atmosphere.\n\nThe SAIL facility will be renovated and recreated as a stop on the Space Center Houston Level 9 Tour, a separate add-on to the visitor's center admission in which tourists are given entrance to buildings normally off limits to visitors.\n"}
{"id": "44904451", "url": "https://en.wikipedia.org/wiki?curid=44904451", "title": "Solar weather", "text": "Solar weather\n\nSolar weather is branch of meteorology, which studies the behavior of the sun and how those behaviors can be predicted.\nSolar weather research involves developing and running simulations of solar behavior, as well as observing the sun using observation equipment.\n\nDue to the enormous size and mass of the sun, solar weather predictions aren’t as precise as earth-weather predictions. \n\nSolar weather is closely related to space weather and often included in same category.\n"}
{"id": "22612651", "url": "https://en.wikipedia.org/wiki?curid=22612651", "title": "Stockpiling antiviral medications for pandemic influenza", "text": "Stockpiling antiviral medications for pandemic influenza\n\nAn antiviral stockpile is a reserve supply of essential antiviral medications in case of shortage. Many countries have chosen to stockpile antiviral medications against pandemic influenza. Because of the time required to prepare and distribute an influenza vaccine, these stockpiles are the only medical defense against widespread infection for the first six months. The stockpiles may be in the form of capsules or simply as the active pharmaceutical ingredient, which is stored in sealed drums and, when needed, dissolved in water to make a bitter-tasting, clear liquid.\n\nThere are no evidence-based guidelines to guide the use of these stockpiled drugs, and plans are based on assumed similarities to seasonal influenza. The most common antivirals are neuraminidase inhibitors, which, if begun during the first 48 hours after symptoms appear, will reduce the duration of seasonal influenza by about one day. Taken before symptoms appear, it may prevent disease in about three-quarters of people treated prophylactically. Currently, this is recommended in institutionalized elderly people and other high-risk groups as a form of post-exposure prophylaxis during seasonal influenza outbreaks. However, since pandemic influenza differs somewhat from normal seasonal influenza, it is not clear that these drugs will prove either safe or effective for their intended purpose.\n\nFor a person that has very recently been exposed to seasonal influenza, effective post-exposure prophylaxis generally requires taking a drug like oseltamivir for seven to ten days, at half the daily dose needed for treatment. A person that is repeatedly exposed, such as hospital staff members, may require continuous treatment throughout the duration of the outbreak in a community. Based on experience with seasonal influenza in nursing homes, control of influenza requires full treatment of any ill persons and prophylactic treatment of all their contacts. In a pandemic situation, before a vaccine becomes available, this level of treatment and medical prevention may require providing drugs to 80% of the people in an affected community. Consequently, very large supplies of the drugs must be made available — much larger supplies than could be produced on demand. Stockpiles are generally arranged in advance by government health authorities, due to fear of shortages and an awareness of manufacturing limitations during an outbreak.\n\nList of available treatments of antiviral per country.\n"}
{"id": "426205", "url": "https://en.wikipedia.org/wiki?curid=426205", "title": "Storm chasing", "text": "Storm chasing\n\nStorm chasing is broadly defined as the pursuit of any severe weather condition, regardless of motive, which can be curiosity, adventure, scientific investigation, or for news or media coverage.\n\nA person who chases storms is known as a storm chaser, or simply a chaser. While witnessing a tornado is the single biggest objective for most chasers, many chase thunderstorms and delight in viewing cumulonimbus and related cloud structures, watching a barrage of hail and lightning, and seeing what skyscapes unfold. There are also a smaller number of storm chasers who intercept tropical cyclones and waterspouts.\n\nStorm chasing is chiefly a recreational endeavor, with motives usually given toward photographing or videoing the storm and for multivariate personal reasons. These can include the beauty of views afforded by the sky and land, the mystery of not knowing precisely what will unfold and the quest to undetermined destination on the open road, intangible experiences such as feeling one with a much larger and powerful natural world, the challenge of correctly forecasting and intercepting storms with the optimal vantage points, and pure thrill seeking. Pecuniary interests and competition may also be components; in contrast, camaraderie is common.\n\nAlthough scientific work is sometimes cited as a goal, direct participation in such work is almost always impractical except for those collaborating in an organized university or government project. Many chasers also are storm spotters, reporting their observations of hazardous weather to relevant authorities. These reports greatly benefit real-time warnings with ground truth information as well as science by increasing the reliability of severe storm databases used in climatology and other research (which ultimately boosts forecast and warning skill). Additionally, many recreational chasers submit photos and videos to researchers as well as to the National Weather Service (NWS) for spotter training.\n\nStorm chasers are not generally paid to chase, with the exception of television media crews in certain television market areas, video stringers and photographers (freelancers mostly, but some staff), and researchers such as graduate meteorologists and professors. An increasing number sell storm videos and pictures and manage to make a profit. A few operate \"chase tour\" services, making storm chasing a recently developed niche tourism. Financial returns usually are relatively meager given the expenses with most chasers spending more than they take in and very few making a living solely from chasing.\n\nNo degree or certification is required to be a storm chaser. Local National Weather Service offices do hold storm spotter training classes, usually early in the spring. Some offices collaborate to produce severe weather workshops oriented toward operational meteorologists.\n\nStorm chasers come from a wide variety of occupational and socioeconomic backgrounds. A fair number are meteorologists, however, most storm chasers may be from any number of occupational fields that have little or nothing to do with meteorology. A large majority of chasers are male and white but a minority are female or of Asian or other descents. The average age is probably around 35 but chasers could theoretically be of any age. A relatively high proportion possess college degrees and a large number live in the central and southern U.S. Many are lovers of nature with interests that also include flora, fauna, volcanoes, aurora, meteors, eclipses, and astronomy.\n\nThe first recognized storm chaser is David Hoadley (1938– ), who began chasing North Dakota storms in 1956, systematically using data from area weather offices and airports. He is widely considered the pioneer storm chaser and was the founder of \"Storm Track\" magazine.\n\nBringing research chasing to the forefront was Neil B. Ward (1914–1972) who in the 1950s and 1960s enlisted the help of Oklahoma Highway Patrol to study storms. His work pioneered modern storm spotting and made institutional chasing a reality.\n\nThe first coordinated storm chasing activity sponsored by institutions was undertaken as part of the Alberta Hail Studies project beginning in 1969. Vehicles were outfitted with various meteorological instrumentation and hail catching apparatus. They were directed into suspected thunderstorm hail regions by a controller at a radar site. The controller communicated with the vehicles by radio. \n\nIn 1972 the University of Oklahoma (OU) in cooperation with the National Severe Storms Laboratory (NSSL) began the Tornado Intercept Project, with the first outing taking place on 19 April of that year. This was the first large-scale tornado chasing activity sponsored by an institution. It culminated in a brilliant success in 1973 with the Union City, Oklahoma tornado providing a foundation for tornado and supercell morphology that proved the efficacy of storm chasing field research. The project produced the first legion of veteran storm chasers, with Hoadley's \"Storm Track\" magazine bringing the community together in 1977.\n\nStorm chasing then reached popular culture in three major spurts: in 1978 with the broadcast of an episode of the television program \"In Search of...\"; in 1985 with a documentary on the PBS series \"Nova\"; and in May 1996 with the theatrical release of \"Twister\" which provided an action-packed but distorted glimpse at the hobby. Further early exposure to storm chasing encouraging some in the weather community resulted from several articles beginning in the late 1970s in \"Weatherwise\" magazine.\n\nVarious television programs and increased coverage of severe weather by the media, especially since the initial video revolution in which VHS ownership became widespread by the early 1990s, substantially elevated awareness of and interest in storms and storm chasing. The advent of the Internet, in particular, contributed to a significant growth in number of storm chasers since the mid-late 1990s. A sharp increase in the general public impulsively wandering in their local area searching for tornadoes similarly is largely attributable to these factors. The 2007-2011 Discovery Channel reality series \"Storm Chasers\" produced another surge in activity. Over the years the nature of chasing and the characteristics of chasers shifted.\n\nFrom their advent in the 1970s until the mid-1990s, scientific field projects were occasionally conducted during spring in the Great Plains. Then, the first of the seminal VORTEX projects occurred in 1994-95 and this was soon followed by various field experiments each spring, with another large project, VORTEX2, in 2009-10. Since the mid-1990s, most storm chasing science, with the notable exception of large field projects, consists of mobile Doppler radar intercepts.\n\nChasing often involves driving thousands of miles in order to witness the relatively short window of time of active severe thunderstorms. It is not uncommon for a chaser to end up empty handed on any particular day. Storm chasers' degrees of involvement, competencies, philosophies, and techniques vary widely, but many chasers spend a significant amount of time forecasting, both before going on the road as well as during the chase, utilizing various sources for weather data. Most storm chasers are not meteorologists, and many chasers expend significant time and effort in learning meteorology and the intricacies of severe convective storm prediction through both study and experience.\n\nBesides the copious driving to, from, and during chases, storm chasing is punctuated with contrasting periods of long waiting and ceaseless action. Downtime can consist of sitting under sun-baked skies for hours, playing pickup sports, evaluating data, or visiting landmarks whilst awaiting convective initiation. During an inactive pattern, this down time can persist for days. When storms are going there is often little or no time to eat or relieve oneself and finding fuel can cause frustrating delays and detours. Navigating obstacles such as rivers and areas with inadequate road networks is a paramount concern. Only a handful of chasers decide to chase Dixie Alley, an area of the Southern United States in which trees and road networks heavily obscure the often large tornadoes. The combination of driving and waiting has been likened to \"extreme sitting\". A \"bust\" occurs when storms do not fire, sometimes referred to as \"severe clear\", when storms fire but are missed, when storms fire but are meager, or when storms fire after dusk.\n\nMost chasing is accomplished by driving, however, a few individuals occasionally fly planes and television stations in some markets use helicopters. Research projects sometimes employ aircraft, as well.\n\nStorm chasers are most active in the spring and early summer, particularly May and June, across the Great Plains of the United States (extending into Canada) in an area colloquially known as Tornado Alley, with many hundred individuals active on some days during this period. This coincides with the most consistent tornado days in the most desirable topography of the Great Plains. Not only are the most intense supercells common here, but due to the moisture profile of the atmosphere the storms tend to be more visible than locations farther east where there are also frequent severe thunderstorms. There is a tendency for chases earlier in the year to be farther south, shifting farther north with the jet stream as the season progresses. Storms occurring later in the year tend to be more isolated and slower moving, both of which are also desirable to chasers.\n\nChasers may operate whenever significant thunderstorm activity is occurring, whatever the date. This most commonly includes more sporadic activity occurring in warmer months of the year bounding the spring maximum, such as the active month of April and to a lesser extent March. The focus in the summer months is the Central or Northern Plains states and the Prairie Provinces, the Upper Midwest, or on to just east of the Colorado Front Range. An annually inconsistent and substantially smaller peak of severe thunderstorm and tornado activity also arises in the transitional months of autumn, particularly October and November. This follows a pattern somewhat the reverse of the spring pattern with the focus beginning in the north then dropping south and with an overall eastward shift. In the area with the most consistent significant tornado activity, the Southern Plains, the tornado season is intense but is relatively brief whereas central to northern and eastern areas experience less intense and consistent activity that is diffused over a longer span of the year.\n\nAdvancing technology since the mid-2000s led to chasers more commonly targeting less amenable areas (i.e. hilly or forested) that were previously eschewed when continuous wide visibility was critical. These advancements, particularly in-vehicle weather data such as radar, also led to an increase in chasing after nightfall. Most chasing remains during daylight hours with active storm intercepting peaking from mid-late afternoon through early-to-mid evening. This is dictated by a chaser's schedule (availability to chase) and by when storms form, which usually is around peak heating during the mid-to-late afternoon but on some days occurs in early afternoon or even in the morning. An additional advantage of later season storms is that days are considerably longer than in early spring. Morning or early afternoon storms tend to be associated with stronger wind shear and thus most often happen earlier in the spring season or later during the fall season.\n\nSome organized chasing efforts have also begun in the Top End of the Northern Territory and in southeastern Australia, with the biggest successes in November and December. A handful of individuals are also known to be chasing in other countries, including the United Kingdom, Israel, Italy, Spain, France, Belgium, the Netherlands, Finland, Germany, Austria, Switzerland, Poland, Bulgaria, Slovenia, Hungary, the Czech Republic, Slovakia, Estonia, Argentina, South Africa, Bangladesh, and New Zealand; although many people trek to the Great Plains of North America from these and other countries around the world (especially from the UK). The number of chasers and number countries where chasers are active expanded at an accelerating pace in Europe from the 1990s-2010s.\n\nThere are inherent dangers involved in pursuing hazardous weather. These range from lightning, tornadoes, large hail, flooding, hazardous road conditions (rain or hail-covered roadways), animals on the roadway, downed power lines (and occasionally other debris), reduced visibility from heavy rain (often wind blown), blowing dust, and hail fog. Most directly weather-related hazards such as from a tornado are minimized if the storm chaser is knowledgeable and cautious. In some situations severe downburst winds may push automobiles around, especially high-profile vehicles. Tornadoes affect a relatively small area and are predictable enough to be avoided if sustaining situational awareness and following strategies including always having an open escape route, maintaining a safe distance, and avoiding placement in the direction of travel of a tornado (in most cases in the Northern Hemisphere this is to the north and to the east of a tornado). Lightning, however, is an unavoidable hazard. \"Core punching\", storm chaser slang for driving through a heavy precipitation core to intercept the area of interest within a storm, is recognized as hazardous due to reduced visibility and because many tornadoes are rain-wrapped. The \"bear's cage\" refers to the area under a rotating wall cloud (and any attendant tornadoes), which is the \"bear\", and to the blinding precipitation (which can include window-shatteringly large hail) surrounding some or all sides of a tornado, which is the \"cage\". Similarly, chasing at night heightens risk due to darkness.\n\nIn reality, the most significant hazard is driving, which is made more dangerous by the severe weather. Adding still more to this hazard are the multiple distractions which can compete for a chaser's attention, such as driving, communicating with chase partners and others with a phone and/or radio, navigating, watching the sky, checking weather data, and shooting photos or video. Again here, prudence is key to minimizing the risk. Chasers ideally work to prevent the driver from multitasking either by chase partners covering the other aspects or by the driver pulling over to do these other things if he or she is chasing alone. Falling asleep while driving is a chase hazard, especially on long trips back. This also is exacerbated by nocturnal darkness and by the defatigating demands of driving through precipitation and on slick roads.\n\nFor nearly 60 years, the only known chaser deaths were driving-related. The first was Christopher Phillips, a University of Oklahoma undergraduate student, killed in a hydroplaning accident when swerving to miss a rabbit in 1984. Three other incidents occurred when Jeff Wear was driving home from a hurricane chase in 2005, when Fabian Guerra swerved to miss a deer while driving to a chase in 2009, and when a wrong-way driver resulted in a head-on collision that killed Andy Gabrielson returning from a chase in 2012. On 31 May 2013, an extreme event led to the first known chaser deaths inflicted directly by weather when the widest tornado ever recorded struck near El Reno, Oklahoma. Engineer Tim Samaras, his photographer son Paul, and meteorologist Carl Young were killed doing in situ probe and infrasonic field research by an exceptional combination of events in which an already large and rain-obscured tornado swelled within less than a minute to wide simultaneously as it changed direction and accelerated. Several other chasers were also struck and some injured by this tornado and its parent supercell's rear flank downdraft. Three chasers were killed, two in one vehicle and one in another, when their vehicles collided in West Texas in 2017, bringing the total number of known traffic related fatalities to 7. There are other incidents in which chasers were injured by automobile accidents, lightning strikes, and tornado impacts. While chasing a tornado outbreak on 13 March 1990, KWTV television photographer Bill Merickel was shot and injured near Lindsay, Oklahoma.\n\nStorm chasers vary with regards to the amount of equipment used, some prefer a minimalist approach; for example, where only basic photographic equipment is taken on a chase, while others use everything from satellite-based tracking systems and live data feeds to vehicle-mounted weather stations and hail guards.\n\nHistorically, storm chasing relied on either in-field analysis or in some cases nowcasts from trained observers and forecasters. The first in-field technology consisted of radio gear for communication. Much of this equipment could also be adapted to receive radiofax data which was useful for receiving basic observational and analysis data. The primary users of such technology were university or government research groups who often had larger budgets than individual chasers.\n\nRadio scanners were also heavily used to listen in on emergency services and storm spotters so as to determine where the most active or dangerous weather was located. A number of chasers were also radio amateurs, and used mobile (or portable) amateur radio to communicate directly with spotters and other chasers, allowing them to keep abreast of what they could not themselves see.\n\nIt was not until the mid- to late 1980s that the evolution of the laptop computer would begin to revolutionize storm chasing. Early on, some chasers carried acoustic couplers to download batches of raw surface and upper air data from payphones. The technology was too slow for graphical imagery such as radar and satellite data; and during the first years this wasn't available on any connection over telephone lines, anyway. Some raw data could be downloaded and plotted by software, such as surface weather observations using \"WeatherGraphix\" (predecessor to \"Digital Atmosphere\") and similar software or for upper air soundings using \"SHARP\", \"RAOB\", and similar software.\n\nMost meteorological data was acquired all at once early in the morning, and the rest of day's chasing was based on analysis and forecast gleaned from this; as well as on visual clues that presented themselves in the field throughout the day. Plotted weather maps were often analyzed by hand for manual diagnosis of meteorological patterns. Occasionally chasers would make stops at rural airstrips or NWS offices for an update on weather conditions. NOAA Weather Radio (NWR) could provide information in the vehicle, without stopping, such as weather watches and warnings, surface weather conditions, convective outlooks, and NWS radar summaries. Nowadays, storm chasers may use high-speed Internet access available in any library, even in small towns in the US. This data is available throughout the day, but one must find and stop at a location offering Internet access.\n\nWith the development of the mobile computers, the first computer mapping software became feasible, at about the same time as the popular adoption of the VHS camcorder began a rapid growth phase. Prior to the mid to late 1980s most motion picture equipment consisted of 8 mm film cameras. While the quality of the first VHS consumer cameras was quite poor (and the size somewhat cumbersome) when compared to traditional film formats, the amount of video which could be shot with a minimal amount of resources was much greater than any film format at the time.\n\nIn the 1980s and 1990s The Weather Channel (TWC) and \"A.M. Weather\" were popular with chasers, in the morning preceding a chase for the latter and both before and during a chase for the former. Commercial radio sometimes also provides weather and damage information. The 1990s brought technological leaps and bounds. With the swift development of solid state technology, television sets for example could be installed with ease in most vehicles allowing storm chasers to actively view local TV stations. Mobile phones became popular making group coordination easier when traditional radio communications methods were not ideal or for those possessing radios. The development of the World Wide Web (WWW) in 1993 hastened adoption of the Internet and led to FTP access to some of the first university weather sites.\n\nThe mid-1990s marked the development of smaller more efficient marine radars. While such marine radars are illegal if used in land-mobile situations, a number of chasers were quick to adopt them in an effort to have mobile radar. These radars have been found to interfere with research radars, such as the Doppler on Wheels (DOW) utilized in field projects. The first personal lightning detection and mapping devices also became available and the first online radar data was offered by private corporations or, at first with delays, with free services. A popular data vendor by the end of the 1990s was WeatherTAP.\n\nChasers used paper maps for navigation and some of those now using GPS still use these as a backup or for strategizing with other chasers. Foldable state maps can be used but are cumbersome due to the multitude of states needed and only show major roads. National atlases allow more detail and all states are contained in a single book, with AAA favored and Rand McNally followed by Michelin also used. The preferred atlases due to great detail in rural areas are the \"Roads of...\" series originally by Shearer Publishing, which first included Texas but expanded to other states such as Oklahoma and Colorado. Covering every state of the union are the DeLorme \"Atlas and Gazetteer\" series. DeLorme also produced early GPS receivers that connected to laptops and for years was one of two major mapping software creators. \"DeLorme Street Atlas USA\" or \"Microsoft Streets & Trips\" were used by most chasers until their discontinuations in 2013. Chasers now use Google Maps or other web mapping as no suitable alternative mapping software emerged. GPS receivers may still be used with other software, such as for displaying radar data.\n\nA major turning point was the advent of civilian GPS in 1996. At first, GPS units were very costly and only offered basic functions, but that would soon change. Towards the late 1990s the Internet was awash in weather data and free weather software, the first true cellular Internet modems for consumer use also emerged providing chasers access to data in the field without having to rely on a nowcaster. The NWS also released the first free, up-to-date NEXRAD Level 3 radar data. In conjunction with all of this, GPS units now had the ability to connect with computers, granting greater ease when navigating.\n\n2001 marked the next great technological leap for storm chasers as the first Wi-Fi units began to emerge offering wireless broadband service in many cases for free. Some places (restaurants, motels, libraries, etc.) were known to reliably offer wireless access and wardriving located other availabilities. In 2002 the first Windows-based package to combine GPS positioning and Doppler radar appeared called SWIFT WX. SWIFT WX allowed storm chasers to seamlessly position themselves accurately relative to tornadic storms.\n\nIn 2004 two more storm chaser tools emerged. The first, WxWorx, was a new XM Satellite Radio based system utilizing a special receiver and Baron Services weather software. Unlike preexisting cellular based services there was no risk of dead spots, and that meant that even in the most remote areas storm chasers still had a live data feed. The second tool was a new piece of software called GRLevel3. GRLevel3 utilized both free and subscription based raw radar files, displaying the data in a true vector format with GIS layering abilities. Since 2006 a growing number of chasers are using Spotter Network (SN), which uses GPS data to plot real time position of participating spotters and chasers, and allows observers to report significant weather as well as GIS layering for navigation maps, weather products, and the like.\n\nThe most common chaser communications device is the cellular phone. These are used for both voice and data connections. External antennas and amplifiers may be used to boost signal transception. It is not uncommon that chasers travel in small groups of cars, and they may use CB radio (declining in use) or inexpensive GMRS / FRS hand-held transceivers for inter-vehicle communication. More commonly, many chasers are also ham radio operators and use the 2 meters VHF and, less often, 70 cm UHF bands to communicate between vehicles or with Skywarn / Canwarn spotter networks. Scanners are often used to monitor spotter, sometimes public safety communications, and can double as weather radios. Since the mid-2000s social networking services may also be used, with Twitter most used for ongoing events, Facebook for sharing images and discussing chase reports, and Instagram trailing in adoption. Social networking services largely (but not completely) replace forums and email lists, which complemented and eventually supplanted \"Stormtrack\" magazine, for conversing about storms.\n\nIn-field environmental data is still popular among some storm chasers, especially temperature, moisture, and wind speed and direction data. Many have chosen to mount weather stations atop their vehicles. Others use handheld anemometers. Rulers or baseballs may be brought along for measuring hail and for showing as a comparison object. Vehicle mounted cameras, such as on the roof or more commonly on the dash, provide continuous visual recording capability.\n\nChasers heavily utilize still photography since the beginning. Videography gained prominence by the 1990s into the early 2000s but a resurgence of photography occurred with the advent of affordable and versatile digital SLR (DSLR) cameras. Prior to this, 35 mm SLR print and slide film formats were mostly used, along with some medium format cameras. In the late 2000s, mobile phone 3G data networks became fast enough to allow live streaming video from chasers using webcams. This live imagery is frequently used by the media, as well as NWS meteorologists, emergency managers, and the general public for direct ground truth information, and it promotes video sales opportunities for chasers. Also by this time, camcorders using memory cards to record video began to be adopted. Digital video had been around for years but was recorded on tape, whereas solid-state is random access rather than sequential access (linear) and has no moving parts. Late in the 2000s HD video began to overtake SD (which had been NTSC in North America) in usage as prices came down and performance increased (initially there were low-light and sporadic aliasing problems due to chip and sensor limitations). By the mid-2010s 4K cameras were increasingly in use. Tripods are used by those seeking crisp professional photo and video imagery and also enable chasers to tend to other activities. Other accessories include cable/remote shutter releases, lightning triggers, and lens filters. Windshield mounted cameras or dome enclosed cameras atop vehicle roofs may also be used, and a few chasers use UAVs (\"drones\").\n\nLate in the 2000s smartphones increased in usage, with radar viewing applications frequently used. Particularly, \"RadarScope\" on the iOS and Android platforms is favored. \"Pkyl3\" was a dominant early choice on Android devices which discontinued development in August 2018. Other apps may be used as are browsers for viewing meteorological data and accessing social networking services. Some handsets can be used as WiFi hotspots and wireless cards may also be used to avoid committing a handset to tethering or operating as a hotspot. Some hotspots operate as mobile broadband MNVO devices using any radio spectrum that is both available and is in contract with a service provider. Such devices may expand mobile data range beyond a single carrier's service area and typically can work on month-to-month contracts. Adoption of tablet computers expanded by the early 2010s. 4G LTE has been adopted when available and can be especially useful for uploading HD video. A gradual uptick of those selecting mirrorless interchangeable-lens cameras (MILCs) began in the mid-2010s. Usage of DSLR for video capture, called HDSLR, is common, although HD camcorders remain popular due to their greater functionality (many chasers still shoot both).\n\nChasers also carry common travel articles and vehicle maintenance items, and sometimes first aid kits. Full sized spare tires are strongly preferable to \"donut\" emergency replacement tires. Power inverters (often with surged protected power strips) power devices that require AC (indoor/wall outlet) power, although some devices may be powered directly with DC (battery power) from the vehicle electric system. Water repellent products, such as Rain-X or Aquapel, are frequently applied to windshields to dispel water when driving as well as mud and small detritus, which boosts visibility and image clarity on photographs and videos shot through glass (which is particularly problematic if autofocus is on). Binoculars and sunglasses are commonly employed.\n\nA growing number of experienced storm chasers advocate the adoption of a code of ethics in storm chasing featuring safety, courtesy, and objectivity as the backbone. Storm chasing is a highly visible recreational activity (which is also associated with science) that is vulnerable to sensationalist media promotion. Veteran storm chasers Chuck Doswell and Roger Edwards have deemed reckless storm chasers as \"yahoos\". Doswell and Edwards believe poor chasing ethics at TV news stations add to the growth of \"yahoo\" storm chasing. Edwards and Rich Thompson, among others, also expressed concern about pernicious effects of media profiteering with Matt Crowther, among others, agreeing in principle but viewing sales as not inherently corrupting. Self-policing is seen as the means to mold the hobby. There is occasional discussion among chasers that at some point government regulation may be imposed due to increasing numbers of chasers and because of poor behavior by some individuals; however, many chasers do not expect this eventuality and almost all oppose regulations -—as do some formal studies of dangerous leisure activities which advocate deliberative self-policing.\n\nAs there is for storm chaser conduct, there is concern about chaser responsibility. Since some chasers are trained in first aid and even first responder procedures, it is not uncommon for tornado chasers to be first on a scene and tending to storm victims or treating injuries at the site of a disaster in advance of emergency personnel and other outside aid.\n\nAside from questions concerning their ethical values and conduct, many have been accredited for giving back to the community in several ways. Just before the Joplin tornado a storm chaser provided advanced warning to local law enforcement, prompting them to activate the emergency sirens. Though lives were lost, many who survived accredited their survival to the siren. After a storm has passed storm chasers are often the first to arrive on the scene to help assist in the aftermath. An unexpected and yet increasingly more common result of storm chasers is the data they provide to storm research from their videos, social video posts and documentation of storms they encounter. After the El Reno tornado in 2013 portals were created for chasers to submit their information to help in the research of the deadly storm.\n\n\n\n\n"}
{"id": "3059114", "url": "https://en.wikipedia.org/wiki?curid=3059114", "title": "Urban studies", "text": "Urban studies\n\nUrban studies is based on the study of the urban development of cities. This includes studying the history of city development from an architectural point of view, to the impact of urban design on community development efforts. Urban studies helps with the understanding of human values, development, and the interactions they have with their physical environment. The field originated primarily from the United Kingdom and the United States, and has spread to research how international cities apply this research.\n\nToday Urban Studies is a large and diverse area of academic research. In recognition of this, organizations like the Urban Studies Foundation (in connection with the Urban Studies Journal), provide grant funding for research explicitly aimed at better understanding the complex reality of cities and urban life.\n\nThe study of cities has changed dramatically from the 1800s over time, with new frames of analysis being applied to the development of urban areas. The first college programs were created to observe how cities were developed based off anthropological research of ghetto communities. In the mid-1900s, urban study programs expanded beyond just looking at the current and historical impacts of city design and began studying how those designs impacted the future interactions of people and how to improve city development through architecture, open spaces, the interactions of people, and different types of capital that forms a community.\n\nUrban history plays an important role in this field of study because it reveals how cities have developed previously. History plays a large role in determining how cities will change in the future. Such areas change continuously as part of larger processes and create new histories that researchers study on both large-scale and individual levels.\n\nOverall, three different themes have influenced how researchers have and will continue to study urban areas:\n\nScholars have also researched how cities outside of the United Kingdom and the United States have developed, but only to a limited degree. Urban history previously focused mostly on how European and American cities developed over time, instead of focusing on how non-European cities developed. Additional geographic areas researched in this field include South Africa, Australia, Latin America, and India.\n\nRacial history in the United States\n\nThe racial segregation of urban residents in the United States has played an important role in developing this field. One program founded to research African-American urban residents, the Harvard-MIT Joint Center for Urban Studies, was founded in 1959 to study residential segregation and to support affected communities. More recently, studies related to race and urban life started to focus on ethnographic methods to study how individuals lived in relation to the city and their respective systems as a whole.\n\nIsrael Zangwill wrote one of the first books on the Ghetto's of Europe and how they impacted the Jewish children that were descendants of the original residents, \"Children of the Ghetto\" \"(1892),\" he also wrote two other books about the European Ghettos. Louis Wirth was the next scholar to write about the Ghettos, he wrote about them from a sociological perspective. Louis Wirth and Roberts Ezra Park also became the first sociologists to publish about the immigrant neighborhoods in America with suggestions on their future design. Roberts Ezra Park was the student of George Zimmel who studied the \"Black Belt\" in Chicago. Other famous scholars that studied segregation, American Ghettos, and impoverished neighborhoods include Du Bois (1903), Haynes (1913), Johnson (1943), Horace Cayton (1944), Kenneth Clark (1965), William Julius Wilson (1987).\n\nThis field is transdisciplinary because it uses theories from a variety of academic fields and places them within an urban context. A wide variety of academic fields refers to the urban environment as a location studied, such as Environmental Studies, Economics, Geography, Public Health, and Sociology. However, scholars in this field research how specific elements contribute to how the city operates, such as how housing and transportation will change. In addition, researchers also study how residents interact within the city, such as how race and gender differences lead to social inequalities, or concentrated disadvantage in urban areas.\n\nIn the United States, race has heavily impacted where African-Americans live. Black Power movements, particularly the Student Nonviolent Coordinating Committee, have criticized how the Harvard-MIT Joint Center for Urban Studies researched the African-American urban population but did not understand the community's needs.\n\nResearchers also struggle with how terms are created and used both inside and outside the field. Researchers even struggle how to define basic terms precisely, such as how a city is defined, due to how the roles of cities change. Researchers must be careful in how they describe urban areas, as their work can be manipulated as positive elements for city boosters wanting to promote a specific city.\n"}
