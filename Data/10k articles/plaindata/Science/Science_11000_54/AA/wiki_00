{"id": "28253247", "url": "https://en.wikipedia.org/wiki?curid=28253247", "title": "13 Things That Don't Make Sense", "text": "13 Things That Don't Make Sense\n\n13 Things That Don't Make Sense is a non-fiction book by the British writer Michael Brooks, published in both the UK and the US during 2008. It became a best-selling paperback in 2010.\n\nThe British subtitle is \"The Most \"Intriguing\" Scientific Mysteries of Our Time\" while the American is \"The Most \"Baffling\" ...\" (see image). \n\nBased on an article Brooks wrote for \"New Scientist\" in March 2005, the book, aimed at the general reader rather than the science community, contains discussion and description of a number of unresolved issues in science. It is a literary effort to discuss some of the inexplicable anomalies that after centuries science still cannot completely comprehend.\n\nThe Missing Universe. This deals with astronomy and theoretical physics and the ultimate fate of the universe, in particular the search for understanding of dark matter and dark energy and includes discussion of: the work of astronomers Vesto Slipher and then Edwin Hubble in demonstrating the universe is expanding; Vera Rubin's investigation of galaxy rotation curves that suggest something other than gravity is preventing galaxies from spinning apart, which led to the revival of unobserved \"dark matter\" theory; experimental efforts to discover dark matter, including the search for the hypothetical neutralino and other weakly interacting massive particles); the study of supernovae at Lawrence Berkeley National Laboratory and Harvard University (under Robert Kirshner) that point to an accelerating universe powered by \"dark energy\" possibly Vacuum energy; and finally the assertion that the proposed modified Newtonian dynamics hypothesis and the accelerating universe disproves the dark matter theory.\n\nThe Pioneer Anomaly. This discusses the \"Pioneer 10\" and \"Pioneer 11\" space probes, which appear to be veering off course and drifting towards the sun. There is growing speculation as to whether this phenomenon can be explained by a yet-undetermined fault in the rockets' systems or whether this obliges us to rethink theories of physics such as gravity. The lead investigator into the progress of the rockets is physicist Slava Turyshev of the NASA Jet Propulsion Laboratory in California who is analysing the data of the rockets' launch and progress and \"reflying\" the missions as computer simulations to try to find a solution to the mystery.\n\nHowever, in 2012, after the book was published, Turyshev was able to give an to the Pioneer Anomaly.\n\nVarying Constants. This chapter discusses the reliability of some physical constants, quantities or values that are held to be always fixed. One of these, the Fine-structure constant, which calculates the behaviour and amount of energy transmitted in subatomic interactions from light reflection and refraction to nuclear fusion, has been called into question by physicist John Webb of the University of New South Wales who may have identified differences in the behaviour of light from quasars and light sources today. According to Webb's observations quasar light appears to refract different shades of colour from light waves emitted today. Brooks also discusses the Oklo natural nuclear fission reactor, in which the natural conditions in caves in Gabon 2 billion years ago caused the uranium there to react. It may be that the amount of energy released was different from today. Both sets of data are subject to ongoing investigation and debate but, Brooks suggests, may indicate that the behaviour of matter and energy can vary radically and essentially as the conditions of the universe changes through time.\n\nCold Fusion. A review of efforts to create nuclear energy at room temperature using hydrogen that is embedded in a metal crystal lattice. Theoretically, this should not happen, because nuclear fusion requires a huge activation energy to get it started. The effect was first reported by chemists Martin Fleischmann and Stanley Pons in 1989, but attempts to reproduce it over the ensuing months were mostly unsuccessful. Cold fusion research was discredited, and articles on the subject became difficult to publish. But according to the book, a scattering of scientists around the world continue to report positive results, with multiple, independent verifications, making the evidence difficult to deny.\n\nLife. This chapters describes efforts to define life and how it emerged from inanimate matter (Abiogenesis) and even recreate Artificial life including: the Miller–Urey experiment by chemists Stanley Miller and Harold Urey at the University of Chicago in 1953 to spark life into a mixture of chemicals by using an electrical charge; Steen Rasmussen's work at the Los Alamos National Laboratory to implant primitive DNA, Peptide nucleic acid, into soap molecules and heat them up; and the work of the Institute for Complex Adaptive Matter at the University of California.\n\nViking. A discussion of the experiments by engineer Gilbert Levin to search for life on Mars in the 1970s as part of the Viking program. Levin's Labeled Release experiment appeared to conclusively show that life does exist on Mars, but as his results were not supported by the other three Viking biological experiments, they were called into question and eventually not accepted by NASA, which instead hypothesized that the gases observed being generated may not have been a product of living metabolism but of a chemical reaction of hydrogen peroxide. Brooks goes into detail on some of Levin's other experiments and also describes how NASA's subsequent missions to Mars have focused on the geology and climate of the planet rather than looking for life on the planet. (Several missions are searching for water and geological conditions which could support life on Mars currently or in the past.)\n\nThe Wow! Signal. Brooks discusses whether or not the signal spotted by astronomer Jerry R. Ehman at the \"Big Ear\" radio telescope of Ohio State University in 1977 was a genuine indication of intelligent life in outer space. This was a remarkably clear signal and Big Ear was the largest and longest running SETI (Search for Extra-Terrestrial Intelligence) radio-telescope project in the world. Brooks goes on to discuss the abandonment of NASA's Microwave Observing Program after government funding was stopped by the efforts of senator Richard Bryan of Nevada. There is no public funding for similar observations today while the SETI Institute, which continues NASA's work is funded by private donation, as are a number of other initiatives (see SETI).\n\nA Giant Virus. Brooks describes the huge and highly resistant Mimivirus found in Bradford, England in 1992 and whether this challenges the traditional view of viruses being inanimate chemicals rather than living things. Mimivirus is not only much larger than most viruses but it also has a much more complex genetic structure. The discovery of Mimivirus has given weight to the theories of microbiologist Philip Bell and others that viral infection was indeed the reason for the emergence from primitive life forms of complex cell structures based on a cell nucleus. (See viral eukaryogenesis.) Study of the behaviour and structure of viruses is ongoing.\n\nDeath. Beginning with the example of Blanding's turtle and certain species of fish, amphibians and reptiles that do not age as they grow older, Brooks discusses theories and research into the evolution of ageing. These include the studies of Peter Medawar and George C. Williams in the 1950s and Thomas Johnson, David Friedman and Cynthia Kenyon in the 1980s claiming that ageing is a genetic process that has evolved as organism select genes that help them to grow and reproduce over ones that help them to thrive in later life. Brooks also talks about Leonard Hayflick, as well as others, who have observed that cells in culture will at a fixed point in time stop reproducing and die as their DNA eventually becomes corrupted by continuous division, a mechanical process at cell level rather than part of a creature's genetic code.\n\nSex. This chapter is a discussion of theories of the evolution of sexual reproduction. The common-sense explanation is that although asexual reproduction is much easier and more efficient for an organism it is less common than sexual reproduction because having two parents allows species to adapt and evolve more easily to survive in changing environments. Brooks discusses efforts to prove this by laboratory experiment and goes on to discuss alternative theories including the work of Joan Roughgarden of Stanford University who proposes that sexual reproduction, rather than being driven by Charles Darwin's sexual selection in individuals is a mechanism for the survival of social groupings, which most higher species depend on for survival.\n\nFree Will. Discusses the experimental investigations into the Neuroscience of free will by Benjamin Libet of the University of California, San Francisco and others, which show that the brain seems to commit to certain decisions before the person becomes aware of having made them and discusses the implications of these findings on our conception of free will.\n\nThe Placebo Effect. This is a discussion of the role of the placebo in modern medicine, including examples such as Diazepam, which, Brooks claims, in some situations appears to work only if the patient knows they are taking it. Brooks describes research into prescription behaviour which appears to show that use of placebos is commonplace. He describes the paper by Asbjørn Hrobjartsson and Peter C. Gøtzsche in the New England Journal of Medicine that challenges use of placebos entirely, and the work of others towards an understanding of the mechanism of the effect.\n\nHomeopathy. Brooks discusses the work of researcher Madeleine Ennis involving a homeopathic solution which once contained histamine but was diluted to the point where no histamine remained. Brooks conjectures that the results might be explained by some previously unknown property of water. Brooks supports the investigation of documented anomalies even though he is critical of the practice of homeopathy in general, as are many of the scientists he cites, such as Martin Chaplin of South Bank University.\n\nChapter 1\n\n"}
{"id": "2427945", "url": "https://en.wikipedia.org/wiki?curid=2427945", "title": "2.02 (Buran-class spacecraft)", "text": "2.02 (Buran-class spacecraft)\n\n2.02 is the designation of the fourth built Soviet/Russian space shuttle orbiter to be produced as part of the Buran program. It carried the GRAU index serial number \"11F35 K4\" and is (depending on the source) also known as \"OK-2K2\", \"Orbiter K4\", \"OK 2.02\" or \"Shuttle 2.02\". It was never officially named.\n\nIn 1993, when the Buran program was stopped, orbiter 2.02 was in an early stage of construction (10-20 percent). The incomplete 2.02 was later partially dismantled at its construction site and moved to the outside of the Tushino Machine Building Plant, near Moscow.\n\nSome of the tiles from orbiter 2.02 were sold and auctioned on the Internet.\n\n\n"}
{"id": "42786676", "url": "https://en.wikipedia.org/wiki?curid=42786676", "title": "Acado virus", "text": "Acado virus\n\nThe Acado virus (ACDV) is a serotype of Corriparta virus in the genus Orbivirus in the Corriparta serogroup. Isolated from Culex antennatus and C. univittatus neavi in Ethiopia. Not reported to cause disease in humans.\n"}
{"id": "853888", "url": "https://en.wikipedia.org/wiki?curid=853888", "title": "Astronomical algorithm", "text": "Astronomical algorithm\n\nAstronomical algorithms are the algorithms used to calculate ephemerides, calendars, and positions (as in celestial navigation or satellite navigation). Examples of large and complex astronomical algorithms are those used to calculate the position of the Moon. A simple example is the calculation of the Julian day.\n\nNumerical model of solar system discusses a generalized approach to local astronomical modeling. The \"variations séculaires des orbites planétaires\" describes an often used model.\n\n"}
{"id": "5258856", "url": "https://en.wikipedia.org/wiki?curid=5258856", "title": "Astrovirtel", "text": "Astrovirtel\n\nAstrovirtel (Accessing Astronomical Archives as Virtual Telescopes, obs. code: I03) is a data archive used as virtual astronomical observatory. The project was funded from 2000 until 2003 and supported by the European Commission's Access to Research Infrastructures action of the Improving Human Potential Programme and managed by the Space Telescope European Coordinating Facility (ST-ECF) on behalf of European Space Agency and European Southern Observatory (ESO). \n\nIts aim was to enhance the scientific return of the ST-ECF/ESO Archive, allowing European users to exploit the archive as a virtual telescope, retrieving and analyzing large quantities of data with the assistance of the archive operators and personnel. The Astrovirtel Selection Panel selected up to six proposals per year. \n\nAstrovirtel was an initial step towards a proper virtual observatory, such as the European Virtual Observatory.\n\n"}
{"id": "15566007", "url": "https://en.wikipedia.org/wiki?curid=15566007", "title": "Auguste Faguet", "text": "Auguste Faguet\n\nAuguste Faguet (1841–1886) was a noted 19th-century French botanical illustrator, well known for his contributions to major botanical works.\n\nAuguste Faguet produced fine botanical wood engravings to illustrate the works of Henri Ernest Baillon. For the \"Dictionnaire de botanique\", which appeared in 34 fascicles between 1876 and 1892, he produced 32 chromolithographed plates, published with each fascicle.\n\nFaguet's drawings involve accurate perspective, his skill creating persuasive depth. Faguet's other work for Baillon included \"Recherches des coniferes\" of 1860, 1186 woodcuts in \"Traite de botanique medicale phanerogamique\" 1883-1884, 370 woodcuts in \"Traite de botanique medicale cryptogamique\" of 1889, \"Loganiacees\" of 1856 and \"Bignoniacees\" of 1864. Henri Faguet also did illustrations for Edouard Bureau's \"Monographie des bignoniacee\" of 1864, Alfred Grandidier's \"Histoire physique, naturelle et politique de Madagascar\" of 1875 and his \"Histoire naturelle des plantes\" that appeared between 1886 and 1903. Faguet also collaborated on a periodical, \"L'Horticulteur Francais\", between 1851 and 1872. His excellent woodcuts were overtaken by the renewed use of metal printing plates for botanical illustrations. Thiebault and Faguet illustrated Henri Baillon's \"Histoire des plantes\" (1866-1895). Faguet supplied engraved text figures for Georges Octave Dujardin-Beaumetz (1833-1895) & Egasse's \"Les plantes medicinales indigenes ex exotique\" of 1889, and contributed to \"The Floral Register\", a periodical which appeared between 1825 and 1851.\n\n\n"}
{"id": "13569791", "url": "https://en.wikipedia.org/wiki?curid=13569791", "title": "Autonomous telepresence", "text": "Autonomous telepresence\n\nAutonomous Telepresence (or AT) is a term coined by Marque Cornblatt to describe an emerging field of study that combines robotics, social networking and human interaction. These elements together enable an entirely new and unique form of two-way communication - remotely operated face-to-face video chat. Using off-the-shelf components, Gomi and found objects, AT was first developed and built by Cornblatt in 1993 in the Sparky series of rovers (also known as the Self Portrait Artifact - Roving Chassis series).\n\nCombining the real-time interaction commonly associated with web-based video chat with the independence of a remotely operated vehicle, AT has become a novel method for human interaction, helping connect people in schools, museums, corporate environment and events.\n\nBy the late 1990s the Sparky series had grown to include a wide range of technologies to enable face-to-face interaction, These include the development of unique software and code, as well as the inclusion of Internet, Wi-Fi, and other technologies. AT was featured at the 2007 Interactive Television of Tomorrow show, as well as the American Film Institute Digital Content Festival the same year. AT and Sparky were recently showcased on The History Channel's Modern Marvels program as the future of the telephone.\n\n\n"}
{"id": "1251862", "url": "https://en.wikipedia.org/wiki?curid=1251862", "title": "Biological Physics", "text": "Biological Physics\n\nBiological Physics: Energy, Information, Life: With new art by David Goodsell is a book by Philip Nelson, illustrated by David Goodsell. The fifth printing was published by W. H. Freeman in late 2013. It is a work on biology with an emphasis on the application of physical principles.\n\n"}
{"id": "14719776", "url": "https://en.wikipedia.org/wiki?curid=14719776", "title": "Copper sweetening", "text": "Copper sweetening\n\nCopper sweetening is a petroleum refining process using a slurry of clay and cupric chloride to oxidize mercaptans. The resulting disulfides are less odorous and usually very viscous, and are usually removed from the lower-boiling fractions and left in the heavy fuel oil fraction. \n\nCopper sweetening introduces trace amount of copper into the resulting products, which tends to have detrimental effects as it leads to formation of gummy residues. Other sources of copper include contact with refinery parts made of copper and copper alloys. Copper is one of the most active instability promoters, and concentrations as low as 0.1 ppm can have marked negative effect. To combat these, metal deactivators are added to some fuels.\n\n"}
{"id": "723777", "url": "https://en.wikipedia.org/wiki?curid=723777", "title": "Cornelis Drebbel", "text": "Cornelis Drebbel\n\nCornelis Jacobszoon Drebbel (; 1572 – 7 November 1633) was a Dutch engineer and inventor. He was the builder of the first navigable submarine in 1620 and an innovator who contributed to the development of measurement and control systems, optics and chemistry.\n\nCornelis Drebbel was born in Alkmaar, Holland in an Anabaptist family in 1572. After some years at the Latin school in Alkmaar, around 1587, he attended the Academy in Haarlem, also located in North-Holland. Teachers at the Academy were Hendrik Goltzius, engraver, painter, alchemist and humanist, Karel van Mander, painter, writer, humanist and Cornelis Corneliszoon of Haarlem. Drebbel became a skilled engraver on copperplate and also took an interest in alchemy.\n\nIn 1595 he married Sophia Jansdochter Goltzius, younger sister of Hendrick, and settled at Alkmaar. They had at least six children, of whom four survived. Drebbel worked initially as a painter, engraver and cartographer. But he was in constant need of money because of the prodigal lifestyle of his wife. In 1598 he obtained a patent for a water-supply system and a sort of perpetual clockwork. In 1600, Drebbel was in Middelburg where he built a fountain at the Noorderpoort. In that spectacle making center he may have picked up knowledge in the art of lens grinding and later would construct a magic lantern and a camera obscura.\n\nAround 1604 the Drebbel family moved to England, probably at the invitation of the new king, James I of England (VI of Scotland). He was accommodated at Eltham Palace. Drebbel worked there at the masques, that were performed by and for the court. He was attached to the court of young Renaissance crown-prince Henry. He astonished the court with his inventions (a perpetuum mobile, automatic and hydraulic organs) and his optical instruments.\n\nHis fame circulated through the courts of Europe. In October 1610 Drebbel and his family moved to Prague on invitation of Emperor Rudolf II, who was preoccupied with the arts, alchemy and occult sciences. Here again Drebbel demonstrated his inventions. When in 1611 Rudolf II was stripped of all effective power by his younger brother Archduke Matthias, Drebbel was imprisoned for about a year. After Rudolf's death in 1612, Drebbel was set free and went back to London. Unfortunately his patron prince Henry had also died and Drebbel was in financial trouble.\n\nWith his glass-grinding machine he manufactured optical instruments and compound microscopes with two convex lenses, for which there was a constant demand. In 1622 Constantijn Huygens stayed as a diplomat for more than one year in England. It is quite possible that he learned the art of glass grinding at this time from Drebbel, and that he passed this knowledge to his second son Christiaan Huygens, who became a prominent Dutch mathematician and scientist. The English natural philosopher Robert Hooke may have learned the art of glass grinding from his acquaintance Johannes Sibertus Kuffler, the son-in-law of Drebbel.\n\nTowards the end of his life, in 1633, Drebbel was involved in a plan to drain the Fens around Cambridge, while living in near-poverty running an ale house in England. He died in London.\n\nIn keeping with traditional Mennonite practice, Drebbel's estate was split between his four living children at the time of his death.\n\nThe Edison of his era, Drebbel was an empirical researcher and innovator. His constructions and innovations cover measurement and control technology, pneumatics, optics, chemistry, hydraulics and pyrotechnics. Along with Staten General he registered several patents. He also wrote essays about his experiments with air pressure and made beautiful engravings; including The Seven Liberal Arts on a map of the city of Alkmaar. He was involved in making theater props, moving statues and in plans to build a new theater in London. He worked on producing torpedoes, naval mines, detonators with that used glass Batavian tears, and worked on fulminating gold (\"aurum fulminans\") as an explosive.\n\nHe was known for his Perpetuum Mobile, built an incubator for eggs and a portable stove/oven with an optimal use of fuel, able to keep the heat on a constant temperature by means of a regulator/thermostat. He designed a solar energy system for London (perpetual fire), demonstrated air-conditioning, made lightning and thunder ‘on command’, and developed fountains and a fresh water supply for the city of Middelburg. He was involved in the draining of the moors around Cambridge (the Fens), developed a predecessors of the barometer and thermometer, and a harpsichords that played on solar energy.\n\nDrebbel's most famous written work was \"Een kort Tractaet van de Natuere der Elementen\" (A short treatise of the nature of the elements) (Haarlem, 1621). He was also involved in the invention of mercury fulminate. He also discovered that mixtures of “spiritus vini” with mercury and silver in “aqua fortis” could explode.\n\nDrebbel invented a chicken incubator and a mercury thermostat which automatically kept it stable at a constant temperature; one of the first recorded feedback-controlled devices. He also developed and demonstrated a working air conditioning system. The invention of a working thermometer is also credited to Drebbel.\n\nThe story goes that, while making a coloured liquid for a thermometer Cornelis dropped a flask of aqua regia on a tin window sill, and discovered that stannous chloride makes the colour of carmine much brighter and more durable. Although Cornelis did not make much money from his work, his daughters Anna and Catharina and his sons-in-law Abraham and Johannes Sibertus Kuffler set up a very successful dye works.\nOne was set up in 1643 in Bow, London, and the resulting colour was called bow dye.\nThe recipe for \"colour Kufflerianus\" was kept a family secret, and the new bright red colour was very popular in Europe.\n\nDevelops an automatic precision lens-grinding machine, builds improved telescopes, constructs the first microscope ('lunette de Dreubells'), camera obscura, laterna magica, manufactures Dutch or Batavian tears\n\nOne of the optical devices some historians believe Drebbel invented when he was working for the Duke of Buckingham was the compound microscope. The device appeared in Europe around 1620 with the earliest account being Dutch Ambassador Willem Boreel's 1619 visit to London where he saw a compound microscope in Drebbel's possession, described as an instrument about eighteen inches long, two inches in diameter, and supported on 3 brass dolphins. In 1621 Drebbel had a compound microscope with two convex lenses. Several of his contemporaries, including Christiaan Huygens, credited the invention of the compound microscope to Drebbel. The invention has many counter claims including Dutch spectacle-maker Johannes Zachariassen's claim that Drebbel stole the idea from him and his father, Zacharias Jansen, and claims that Galileo Galilei used his telescope after 1610 as a type of compound microscope. In 1624 Galileo saw Drebbel's design for a microscope in Rome and created an improved version of it to send to Federico Cesi, founder of the Accademia dei Lincei, who used it to illustrate \"Apiarum\", his book about bees.\n\nHe also built the first navigable submarine in 1620 while working for the English Royal Navy. He manufactured a steerable submarine with a leather-covered wooden frame. Between 1620 and 1624 Drebbel successfully built and tested two more submarines, each one bigger than the last. The final (third) model had 6 oars and could carry 16 passengers. This model was demonstrated to King James I in person and several thousand Londoners. The submarine stayed submerged for three hours and could travel from Westminster to Greenwich and back, cruising at a depth between 12 and 15 feet (4 to 5 metres). Drebbel even took James in this submarine on a test dive beneath the Thames, making James I the first monarch to travel underwater. This submarine was tested many times in the Thames, but it couldn't attract enough enthusiasm from the Admiralty and was never used in combat.\n\nMore recently it has been suggested that the contemporary accounts of the craft contained significant elements of exaggeration and it was at most a semi-submersible which was able to travel down the Thames by the force of the current.\n\nCornelis Drebbel has been honoured on postage stamps issued by the postal services of both Mali and the Netherlands in 2010.\n\nA portrayal of Cornelis Drebbel and his submarine can be briefly seen in the film \"The Four Musketeers\" (1974). A small leatherclad submersible surfaces off the coast of England, and the top opens clamshell-wise revealing Cornelis Drebbel and the Duke of Buckingham.\n\nDrebbel was honoured in an episode of the cartoon \"Sealab 2021\" during a submarine rescue of workers on a research station in the Arctic. A German U-boat captain fired a pistol in celebration at the mention of Drebbel, to shouts of, \"Sieg Heil! Cornelis Drebbel!\" Also, on the Sealab 2021 Season 3 DVD, Cornelis Drebbel has two DVD commentaries devoted to the story of his life.\n\nIn the Dutch Eighty Years' War comic Gilles de Geus, Drebbel is a supporting character to the warhero Gilles. He is drawn as a crazy inventor, similar to Q in the James Bond series. His submarine plays a role in the comic.\n\nRichard SantaColoma has speculated that the Voynich Manuscript may be connected to Drebbel, initially suggesting it was Drebbel's cipher notebook on microscopy and alchemy, and then later hypothesising it is a fictional \"tie in\" to Francis Bacon's utopian novel \"New Atlantis\" in which some Drebbel-related items (submarine, perpetual clock) are said to appear.\n\nA small lunar crater has been named after him. The street \"Cornelis Drebbelweg\" in Delft, the Netherlands has been named after him.\n\n\n"}
{"id": "38890725", "url": "https://en.wikipedia.org/wiki?curid=38890725", "title": "Database System Concepts", "text": "Database System Concepts\n\nDatabase System Concepts, by Abraham Silberschatz and Hank Korth,\nis a classic textbook on database systems.\nIt is often called the sailboat book, because its first edition had on its cover a number of\nsailboats, labelled with the names of various database models.\nThe boats are sailing from a desert island towards a tropical island,\nwhile the wind pushes them away and prevents their arrival.\n\nThe book is currently in its sixth edition.\nThe third edition added S. Sudrashan as an author.\n\nOfficial website\n"}
{"id": "40421924", "url": "https://en.wikipedia.org/wiki?curid=40421924", "title": "Deinococcus murrayi", "text": "Deinococcus murrayi\n\nDeinococcus murrayi is a bacterium. It produces orange-pigmented colonies and has an optimum growth temperature of about to . It is extremely gamma radiation-resistant. Its type strain is ALT-1b (= DSM 11303).\n\n"}
{"id": "47814230", "url": "https://en.wikipedia.org/wiki?curid=47814230", "title": "Delta Flume", "text": "Delta Flume\n\nDelta Flume is a 300 meter long man-made flume with a wave generator that is capable of producing waves as tall as five meters, the world's largest artificial waves. It is located at the Deltares Research Institute outside the city of Delft, Netherlands. It is used to simulate forces generated by natural waves in order to test materials used in the construction of dykes.\n\nO. H. Hinsdale Wave Research Laboratory\n"}
{"id": "11607980", "url": "https://en.wikipedia.org/wiki?curid=11607980", "title": "Demographic marketer", "text": "Demographic marketer\n\nDemographic marketers use demographics in marketing research, and the assessment of the changing trends of consumer behavior. Demographics can be called a science and demographic marketers can be called scientists. A demographic is used to describe individuals who are from a particular area. It can also be used to describe individuals who would rely on purchasing a particular product or service. Using demographics, a marketing manager can try to grasp what certain people think and what they are willing to buy.\n\nBy understanding how various characteristics of the population reflect their tastes, demographic marketers get an idea of the probability of the sales returns of a launched product in a given area. For any type of business, knowing who the customers are most likely to be through demographic analysis will make it easier to market effectively.\n\n"}
{"id": "37940531", "url": "https://en.wikipedia.org/wiki?curid=37940531", "title": "Echelon cracks", "text": "Echelon cracks\n\nEchelon cracks are a related series of cracks in a planar structure and are a response to shearing forces in the plane of the surface. Such cracks are typically found in asphalt roadways due to aseismic creep and in other planar structures such as walls and building facades due to non-uniform settlement into soft soil.\n\nSuch cracks in a uniform surface will form at a uniform angle to the general direction of shear and will progress with more-or-less uniform spacing, length, and offset, thus forming the echelon.\n\nOn a larger scale, a zone between offset strike slip faults may have minor \"en echelon\" fault strands that accommodate the shearing motion induced in the region. On rock undergoing these shearing stresses en echelon veins may occur as fracture fillings.\n"}
{"id": "183435", "url": "https://en.wikipedia.org/wiki?curid=183435", "title": "Epistemic community", "text": "Epistemic community\n\nAn epistemic community is a transnational network of knowledge-based experts who help decision-makers to define the problems they face, identify various policy solutions and assess the policy outcomes. The definitive conceptual framework of an epistemic community is widely accepted as that of Peter M. Haas. He describes them as \n\"...a network of professionals with recognised expertise and competence in a particular domain and an authoritative claim to policy relevant knowledge within that domain or issue-area.\"\n\nAlthough the members of an epistemic community may originate from a variety of academic or professional backgrounds, they are linked by a set of unifying characteristics for the promotion of collective amelioration and not collective gain. This is termed their \"normative component\". In the big picture, epistemic communities are socio-psychological entities that create and justify knowledge. Such communities can constitute of only two persons and yet gain an important role in building knowledge on any specific subject. Miika Vähämaa has recently suggested that epistemic communities consist of persons being able to understand, discuss and gain self-esteem concerning the matters being discussed.\n\nSome theorists argue that an epistemic community may consist of those who accept one version of a story, or one version of validating a story. Michel Foucault referred more elaborately to mathesis as a rigorous episteme suitable for enabling cohesion of a discourse and thus uniting a community of its followers. In philosophy of science and systems science the process of forming a self-maintaining epistemic community is sometimes called a mindset. In politics, a tendency or faction is usually described in very similar terms.\n\nMost researchers carefully distinguish between epistemic forms of community and \"real\" or \"bodily\" community which consists of people sharing risk, especially bodily risk.\n\nIt is also problematic to draw the line between modern ideas and more ancient ones, for example, Joseph Campbell's concept of myth from cultural anthropology, and Carl Jung's concept of archetype in psychology. Some consider forming an epistemic community a deep human need, and ultimately a mythical or even religious obligation. Among these very notably are E. O. Wilson, as well as Ellen Dissanayake, an American historian of aesthetics who famously argued that almost all of our broadly shared conceptual metaphors centre on one basic idea of safety: that of \"home\".\n\nFrom this view, an epistemic community may be seen as a group of people who do not have any specific history together, but search for a common idea of home as if forming an intentional community. For example, an epistemic community can be found in a network of professionals from a wide variety of disciplines and backgrounds.\n\nAs discussed in Peter M. Haas's definitive text, an epistemic community is made up of a diverse range of academic and professional experts, who are allied on the basis of four unifying characteristics:\n\n\nThus, when viewed as an epistemic community, the overall enterprise of the expert members emerges as the product of a combination of shared beliefs and more subtle conformity pressures, rather than a direct drive for concurrence (Michael J. Mazarr). Epistemic communities also have a \"normative component\" meaning the end goal is always for the betterment of society, rather than self gain of the community itself (Peter M. Haas).\n\nIn international relations and political science, an epistemic community can also be referred to as a global network of knowledge-based professionals in scientific and technological areas that often affect policy decisions.\n\nThe global environmental agenda is increasing in complexity and interconnectedness. Often environmental policymakers do not understand the technical aspects of the issues they are regulating. This affects their ability to define state interests and develop suitable solutions within cross-boundary environmental regulation.\n\nAs a result, conditions of uncertainty are produced which stimulate a demand for new information. Environmental crises play a significant role in exacerbating conditions of uncertainty for decision-makers. Political elites seek expert knowledge and advice to reduce this technical uncertainty, on issues including:\n\nTherefore, epistemic communities can frame environmental problems as they see fit, and environmental decision-makers begin to make policy-shaping decisions based on these specific depictions.\n\nThe initial identification and bounding of environmental issues by epistemic community members is very influential. They can limit what would be preferable in terms of national interests, frame what issues are available for collective debate, and delimit the policy alternatives deemed possible. The political effects are not easily reversible. The epistemic community vision is institutionalised as a collective set of understandings reflected in any subsequent policy choices.\n\nThis is a key point of power. Policy actors are persuaded to conform to the community’s consensual, knowledge-driven ideas without the epistemic community requiring a more material form of power. Members of successful communities can become strong actors at the national and international level as decision-makers attach responsibility to their advice.\n\nAs a result, epistemic communities have a direct input on how international cooperation may develop in the long term. Transboundary environmental problems require a unified response rather than patchwork policy efforts, but this is problematic due to enduring differences of state interest and concerns over reciprocity. The transnational nature of epistemic communities means numerous states may absorb new patterns of logic and behaviour, leading to the adoption of concordant state policies. Therefore, the likelihood of convergent state behaviour and associated international coordination is increased.\n\nInternational cooperation is further facilitated if powerful states are involved, as a quasi-structure is created containing the reasons, expectations and arguments for coordination. Also, if epistemic community members have developed authoritative bureaucratic reputations in various countries, they are likely to participate in the creation and running of national and international institutions that directly pursue international policy coordination, for example, a regulatory agency, think tank or governmental research body.\n\nAs a result, epistemic community members in a number of different countries can become connected through intergovernmental channels, as well as existing community channels, producing a transnational governance network, and facilitating the promotion of international policy coordination. An example of a scientific epistemic community in action is the 1975 collectively negotiated Mediterranean Action Plan (MAP), a marine pollution control regime for the Mediterranean Sea developed by the United Nations Environment Programme.\n\n\n\n"}
{"id": "39579033", "url": "https://en.wikipedia.org/wiki?curid=39579033", "title": "Epstein–Barr virus viral-capsid antigen", "text": "Epstein–Barr virus viral-capsid antigen\n\nThe Epstein–Barr virus viral-capsid antigen (EBV-VCA) is the viral protein that forms the viral capsid of the Epstein–Barr virus. It is the antigen targeted by \"anti-VCA antibodies\". Such antibodies can be used in serology to diagnose infectious mononucleosis. In cases with primary infection, the sensitivity of IgG (immunoglobulin) and IgM anti-VCA testing has been estimated to be 100% reliable.\n\nElevated Anti-VCA IgM indicates acute infection.\n\nElevated Anti-VCA IgG indicates prior infection.\n"}
{"id": "4995149", "url": "https://en.wikipedia.org/wiki?curid=4995149", "title": "Forensic electrical engineering", "text": "Forensic electrical engineering\n\nForensic electrical engineering is a branch of forensic engineering, and is concerned with investigating electrical failures and accidents in a legal context. Many forensic electrical engineering investigations apply to fires suspected to be caused by electrical failures. Forensic electrical engineers are most commonly retained by insurance companies or attorneys representing insurance companies, or by manufacturers or contractors defending themselves against subrogation by insurance companies. Other areas of investigation include accident investigation involving electrocution, and intellectual property disputes such as patent actions. Additionally, since electrical fires are most often cited as the cause for \"suspect\" fires an electrical engineer is often employed to evaluate the electrical equipment and systems to determine whether the cause of the fire was electrical in nature.\n\nThe ultimate goal of these investigations is often to determine the legal liability for a fire or other accident for purposes of insurance subrogation or an injury lawsuit. Some examples include:\n\nForensic electrical engineers are also involved in some arson and set-fire investigations; while it is not common for arsonists to cause electrical failures to ignite fires, the presence of electrical appliances and systems in many fires scenes often requires them to be evaluated as possible accidental causes of the fire. Some electrical means of ignition, when discovered, are fairly obvious to an origin and cause investigator and most likely do not require consulting with a forensic electrical engineer. (Note that \"arson\" refers specifically to a criminal act, subject to criminal prosecution; a more general term is a \"set fire\". A homeowner setting a fire deliberately in order to defraud an insurance company might be prosecuted for arson by a government body; however, the insurance company would concern itself only with denying the insurance claim, possibly leading to a civil lawsuit.)\n\nPatent disputes may also require the expert opinion of an electrical engineer to advise a court. Issues in conflict may include the precise meaning of technical terms, especially in the patent claims, the prior art in a particular product field and the obviousness of various patents.\n\nMost states have a statute of ultimate repose (similar to, but not to be confused with, a statute of limitations) that limits the length of time after which a party can legally be held liable for their negligent act or defective product. Many states have a \"useful life\" statute of ultimate repose. Therefore, a determination of the length of time the product would normally be expected to be used before wearing out needs to be made. For example, a refrigerator might have a longer \"useful life\" than an electric fan; an airplane might have a longer useful life than a car. Some states pick an arbitrary number of years for the statute of ultimate repose. It may be short (six or seven years) or longer 15 or 25 years. If a coffee maker starts on fire after the statute of ultimate repose has expired, the manufacturer can no longer be held liable for manufacturing or design defects. The statute of ultimate repose is different from the statute of limitations. In a state with a short statute of ultimate repose, it is common that a person's right to bring a claim in court expires before their injury ever occurs. Thus, if a defective product (for example a car) caused a collision when the steering failed, but the collision occurred after the expiration of a statute of ultimate repose, no claim could be brought against the manufacturer for selling a defective product. The right to bring the claim expired before the claim even occurred.\n\n"}
{"id": "55620628", "url": "https://en.wikipedia.org/wiki?curid=55620628", "title": "Gabinete de Urbanização Colonial", "text": "Gabinete de Urbanização Colonial\n\nThe Gabinete de Urbanização Colonial (1944-1974) of Portugal was a government office responsible for urban planning in Portuguese colonies in Africa and Asia. It began operating in 1945. In 1951 it became the Gabinete de Urbanização do Ultramar, and in 1957 the Direcção de Serviços de Urbanismo e Habitação of the Overseas Ministry public works department. Staff included Joao A. de Aguiar, Rogerio Cavaca, and Pinto Coelho.\n\n\n"}
{"id": "3362809", "url": "https://en.wikipedia.org/wiki?curid=3362809", "title": "Galileo affair", "text": "Galileo affair\n\nThe Galileo affair () was a sequence of events, beginning around 1610, culminating with the trial and condemnation of Galileo Galilei by the Roman Catholic Inquisition in 1633 for his support of heliocentrism.\n\nIn 1610, Galileo published his \"Sidereus Nuncius\" (\"Starry Messenger\"), describing the surprising observations that he had made with the new telescope, namely the phases of Venus and the Galilean moons of Jupiter. With these observations he promoted the heliocentric theory of Nicolaus Copernicus (published in \"De revolutionibus orbium coelestium\" in 1543). Galileo's initial discoveries were met with opposition within the Catholic Church, and in 1616 the Inquisition declared heliocentrism to be formally heretical. Heliocentric books were banned and Galileo was ordered to refrain from holding, teaching or defending heliocentric ideas.\n\nGalileo went on to propose a theory of tides in 1616, and of comets in 1619; he argued that the tides were evidence for the motion of the earth. In 1632 Galileo, now an old man, published his \"Dialogue Concerning the Two Chief World Systems\", which implicitly defended heliocentrism, and was immensely popular. Responding to mounting controversy over theology, astronomy and philosophy, the Roman Inquisition tried Galileo in 1633 and found him \"vehemently suspect of heresy\", sentencing him to indefinite imprisonment. Galileo was kept under house arrest until his death in 1642.\n\nGalileo began his telescopic observations in the later part of 1609, and by March 1610 was able to publish a small book, \"The Starry Messenger\" (\"Sidereus Nuncius\"), describing some of his discoveries: mountains on the Moon, lesser moons in orbit around Jupiter, and the resolution of what had been thought to be very cloudy masses in the sky (nebulae) into collections of stars too faint to see individually without a telescope. Other observations followed, including the phases of Venus and the existence of sunspots.\nGalileo's contributions caused difficulties for theologians and natural philosophers of the time, as they contradicted scientific and philosophical ideas based on those of Aristotle and Ptolemy and closely associated with the Catholic Church. In particular, Galileo's observations of the phases of Venus, which showed it to circle the sun, and the observation of moons orbiting Jupiter, contradicted the geocentric model of Ptolemy, which was backed and accepted by the Roman Catholic Church, and supported the Copernican model advanced by Galileo.\n\nJesuit astronomers, experts both in Church teachings, science, and in natural philosophy, were at first skeptical and hostile to the new ideas; however, within a year or two the availability of good telescopes enabled them to repeat the observations. In 1611, Galileo visited the Collegium Romanum in Rome, where the Jesuit astronomers by that time had repeated his observations. Christoph Grienberger, one of the Jesuit scholars on the faculty, sympathized with Galileo’s theories, but was asked to defend the Aristotelian viewpoint by Claudio Acquaviva, the Father General of the Jesuits. Not all of Galileo's claims were completely accepted: Christopher Clavius, the most distinguished astronomer of his age, never was reconciled to the idea of mountains on the Moon, and outside the collegium many still disputed the reality of the observations. In a letter to Kepler of August 1610, Galileo complained that some of the philosophers who opposed his discoveries had refused even to look through a telescope:\n\nGeocentrists who did verify and accept Galileo's findings had an alternative to Ptolemy's model in an alternative geocentric (or \"geo-heliocentric\") model proposed some decades earlier by Tycho Brahe – a model, in which, for example, Venus circled the sun. Brahe argued that the distance to the stars in the Copernican system would have to be 700 times greater than the distance from the sun to Saturn. Moreover, the only way the stars could be so distant and still appear the sizes they do in the sky would be if even average stars were gigantic – at least as big as the orbit of the earth, and of course vastly larger than the sun (refer to article on Tychonic System).\n\nGalileo became involved in a dispute over priority in the discovery of sunspots with Christoph Scheiner, a Jesuit. This became a bitter lifelong feud. Neither of them, however, was the first to recognise sunspots – the Chinese had already been familiar with them for centuries.\n\nAt this time, Galileo also engaged in a dispute over the reasons that objects float or sink in water, siding with Archimedes against Aristotle. The debate was unfriendly, and Galileo's blunt and sometimes sarcastic style, though not extraordinary in academic debates of the time, made him enemies. During this controversy one of Galileo's friends, the painter Lodovico Cardi da Cigoli, informed him that a group of malicious opponents, which Cigoli subsequently referred to derisively as \"the Pigeon league\", was plotting to cause him trouble over the motion of the earth, or anything else that would serve the purpose. According to Cigoli, one of the plotters asked a priest to denounce Galileo's views from the pulpit, but the latter refused. Nevertheless, three years later another priest, Tommaso Caccini, did in fact do precisely that, as described below.\n\nIn the Catholic world prior to Galileo's conflict with the Church, the majority of educated people subscribed to the Aristotelian geocentric view that the earth was the center of the universe and that all heavenly bodies revolved around the earth, though Copernican theories were used to reform the calendar in 1582.\nGeostaticism agreed with a literal interpretation of Scripture in several places, such as , , , , (but see varied interpretations of ). Heliocentrism, the theory that the earth was a planet, which along with all the others revolved around the sun, contradicted both geocentrism and the prevailing theological support of the theory.\n\nOne of the first suggestions of heresy that Galileo had to deal with came in 1613 from a professor of philosophy, poet and specialist in Greek literature, Cosimo Boscaglia. In conversation with Galileo's patron Cosimo II de' Medici and Cosimo's mother Christina of Lorraine, Boscaglia said that the telescopic discoveries were valid, but that the motion of the earth was obviously contrary to Scripture:\nDr. Boscaglia had talked to Madame [Christina] for a while, and though he conceded all the things you have discovered in the sky, he said that the motion of the earth was incredible and could not be, particularly since Holy Scripture obviously was contrary to such motion.\n\nGalileo was defended on the spot by his former student Benedetto Castelli, now a professor of mathematics and Benedictine abbot. The exchange having been reported to Galileo by Castelli, Galileo decided to write a letter to Castelli, expounding his views on what he considered the most appropriate way of treating scriptural passages which made assertions about natural phenomena. Later, in 1615, he expanded this into his much longer \"Letter to the Grand Duchess Christina\".\n\nIn late 1614 or early 1615, one of Caccini's fellow Dominicans, Niccolò Lorini, acquired a copy of Galileo's letter to Castelli. Lorini and other Dominicans at the Convent of San Marco considered the letter of doubtful orthodoxy, in part because it may have violated the decrees of the Council of Trent:\n\nLorini and his colleagues decided to bring Galileo's letter to the attention of the Inquisition. In February 1615 Lorini accordingly sent a copy to the Secretary of the Inquisition, Cardinal Paolo Emilio Sfondrati, with a covering letter critical of Galileo's supporters:\n\nOn March 19, Caccini arrived at the Inquisition's offices in Rome to denounce Galileo for his Copernicanism and various other alleged heresies supposedly being spread by his pupils.\n\nGalileo soon heard reports that Lorini had obtained a copy of his letter to Castelli and was claiming that it contained many heresies. He also heard that Caccini had gone to Rome and suspected him of trying to stir up trouble with Lorini's copy of the letter. As 1615 wore on he became more concerned, and eventually determined to go to Rome as soon as his health permitted, which it did at the end of the year. By presenting his case there, he hoped to clear his name of any suspicion of heresy, and to persuade the Church authorities not to suppress heliocentric ideas.\n\nIn going to Rome Galileo was acting against the advice of friends and allies, and of the Tuscan ambassador to Rome, Piero Guicciardini.\n\nCardinal Robert Bellarmine, one of the most respected Catholic theologians of the time, was called on to adjudicate the dispute between Galileo and his opponents. The question of heliocentrism had first been raised with Cardinal Bellarmine, in the case of Paolo Antonio Foscarini, a Carmelite father; Foscarini had published a book, \"Lettera ... sopra l'opinione ... del Copernico\", which attempted to reconcile Copernicus with the biblical passages that seemed to be in contradiction. Bellarmine at first expressed the opinion that Copernicus's book would not be banned, but would at most require some editing so as to present the theory purely as a calculating device for \"saving the appearances\" (i.e. preserving the observable evidence).\n\nFoscarini sent a copy of his book to Bellarmine, who replied in a letter of April 12, 1615. Galileo is mentioned by name in the letter, and a copy was soon sent to him. After some preliminary salutations and acknowledgements, Bellarmine begins by telling Foscarini that it is prudent for him and Galileo to limit themselves to treating heliocentrism as a merely hypothetical phenomenon and not a physically real one. Further on he says that interpreting heliocentrism as physically real would be \"a very dangerous thing, likely not only to irritate all scholastic philosophers and theologians, but also to harm the Holy Faith by rendering Holy Scripture as false.\" Moreover, while the topic was not inherently a matter of faith, the statements about it in Scripture \"were\" so by virtue of who said them – namely, the Holy Spirit. He conceded that if there were conclusive proof, \"then one would have to proceed with great care in explaining the Scriptures that appear contrary; and say rather that we do not understand them, than that what is demonstrated is false.\" However, demonstrating that heliocentrism merely \"saved the appearances\" could not be regarded as sufficient to establish that it was physically real. Although he believed that the former may well have been possible, he had \"very great doubts\" that the latter would be, and in case of doubt it was not permissible to depart from the traditional interpretation of Scriptures. His final argument was a rebuttal of an analogy that Foscarini had made between a moving earth and a ship on which the passengers perceive themselves as apparently stationary and the receding shore as apparently moving. Bellarmine replied that in the case of the ship the passengers know that their perceptions are erroneous and can mentally correct them, whereas the scientist on the earth clearly experiences that it is stationary and therefore the perception that the sun, moon and stars are moving is not in error and does not need to be corrected.\n\nBellarmine found no problem with heliocentrism so long as it was treated as a purely hypothetical calculating device and not as a physically real phenomenon, but he did not regard it as permissible to advocate the latter unless it could be conclusively proved through current scientific standards. This put Galileo in a difficult position, because he believed that the available evidence strongly favoured heliocentrism, and he wished to be able to publish his arguments.\n\nIn addition to Bellarmine, Monsignor Francesco Ingoli initiated a debate with Galileo, sending him in January 1616 an essay disputing the Copernican system. Galileo later stated that he believed this essay to have been instrumental in the action against Copernicanism that followed in February. According to Maurice Finocchiaro, Ingoli had probably been commissioned by the Inquisition to write an expert opinion on the controversy, and the essay provided the \"chief direct basis\" for the ban. The essay focused on eighteen physical and mathematical arguments against heliocentrism. It borrowed primarily from the arguments of Tycho Brahe, and it notedly mentioned Brahe's argument that heliocentrism required the stars to be much larger than the sun. Ingoli wrote that the great distance to the stars in the heliocentric theory \"clearly proves ... the fixed stars to be of such size, as they may surpass or equal the size of the orbit circle of the Earth itself.\" Ingoli included four theological arguments in the essay, but suggested to Galileo that he focus on the physical and mathematical arguments. Galileo did not write a response to Ingoli until 1624, in which, among other arguments and evidence, he listed the results of experiments such as dropping a rock from the mast of a moving ship.\n\nOn February 19, 1616, the Inquisition asked a commission of theologians, known as qualifiers, about the propositions of the heliocentric view of the universe. Historians of the Galileo affair have offered different accounts of why the matter was referred to the qualifiers at this time. Beretta points out that the Inquisition had taken a deposition from Gianozzi Attavanti in November 1615, as part of its investigation into the denunciations of Galileo by Lorini and Caccini. In this deposition, Attavanti confirmed that Galileo had advocated the Copernican doctrines of a stationary sun and a mobile earth, and as a consequence the Tribunal of the Inquisition would have eventually needed to determine the theological status of those doctrines. It is however possible, as surmised by the Tuscan ambassador, Piero Guiccardini, in a letter to the Grand Duke, that the actual referral may have been precipitated by Galileo's aggressive campaign to prevent the condemnation of Copernicanism.\n\nOn February 24 the Qualifiers delivered their unanimous report: the proposition that the sun is stationary at the centre of the universe is \"foolish and absurd in philosophy, and formally heretical since it explicitly contradicts in many places the sense of Holy Scripture\"; the proposition that the earth moves and is not at the centre of the universe \"receives the same judgement in philosophy; and ... in regard to theological truth it is at least erroneous in faith.\" The original report document was made widely available in 2014.\n\nAt a meeting of the cardinals of the Inquisition on the following day, Pope Paul V instructed Bellarmine to deliver this result to Galileo, and to order him to abandon the Copernican opinions; should Galileo resist the decree, stronger action would be taken. On February 26, Galileo was called to Bellarmine's residence and ordered,\n\nWith no attractive alternatives, Galileo accepted the orders delivered, even sterner than those recommended by the Pope. Galileo met again with Bellarmine, apparently on friendly terms; and on March 11 he met with the Pope, who assured him that he was safe from persecution so long as he, the Pope, should live. Nonetheless, Galileo's friends Sagredo and Castelli reported that there were rumors that Galileo had been forced to recant and do penance. To protect his good name, Galileo requested a letter from Bellarmine stating the truth of the matter. This letter assumed great importance in 1633, as did the question whether Galileo had been ordered not to \"hold or defend\" Copernican ideas (which would have allowed their hypothetical treatment) or not to teach them in any way. If the Inquisition had issued the order not to teach heliocentrism at all, it would have been ignoring Bellarmine's position.\n\nIn the end, Galileo did not persuade the Church to stay out of the controversy, but instead saw heliocentrism formally declared false. It was consequently termed heretical by the Qualifiers, since it contradicted the literal meaning of the Scriptures, though this position was not binding on the Church.\n\nFollowing the Inquisition's injunction against Galileo, the papal Master of the Sacred Palace ordered that Foscarini's \"Letter\" be banned, and Copernicus' \"De revolutionibus\" suspended until corrected. The papal Congregation of the Index preferred a stricter prohibition, and so with the Pope's approval, on March 5 the Congregation banned all books advocating the Copernican system, which it called \"the false Pythagorean doctrine, altogether contrary to Holy Scripture.\"\n\nFrancesco Ingoli, a consultor to the Holy Office, recommended that \"De revolutionibus\" be amended rather than banned due to its utility for calendrics. In 1618 the Congregation of the Index accepted his recommendation, and published their decision two years later, allowing a corrected version of Copernicus' book to be used. The uncorrected \"De revolutionibus\" remained on the Index of banned books until 1758.\n\nGalileo's works advocating Copernicanism were therefore banned, and his sentence prohibited him from \"teaching, defending… or discussing\" Copernicanism. In Germany, Kepler's works were also banned by the papal order.\n\nIn 1623, Pope Gregory XV died and was succeeded by Pope Urban VIII who showed greater favor to Galileo, particularly after Galileo traveled to Rome to congratulate the new Pontiff.\n\nGalileo's \"Dialogue Concerning the Two Chief World Systems\", which was published in 1632 to great popularity, was an account of conversations between a Copernican scientist, Salviati, an impartial and witty scholar named Sagredo, and a ponderous Aristotelian named Simplicio, who employed stock arguments in support of geocentricity, and was depicted in the book as being an intellectually inept fool. Simplicio's arguments are systematically refuted and ridiculed by the other two characters with what Youngson calls \"unassailable proof\" for the Copernican theory (at least versus the theory of Ptolemy – as Finocchiaro points out, \"the Copernican and Tychonic systems were observationally equivalent and the available evidence could be explained equally well by either\"), which reduces Simplicio to baffled rage, and makes the author's position unambiguous. Indeed, although Galileo states in the preface of his book that the character is named after a famous Aristotelian philosopher (Simplicius in Latin, Simplicio in Italian), the name \"Simplicio\" in Italian also had the connotation of \"simpleton.\" Authors Langford and Stillman Drake asserted that Simplicio was modeled on philosophers Lodovico delle Colombe and Cesare Cremonini. Pope Urban demanded that his own arguments be included in the book. which resulted in Galileo putting them in the mouth of Simplicio. Some months after the book's publication, Pope Urban VIII banned its sale and had its text submitted for examination by a special commission.\n\nWith the loss of many of his defenders in Rome because of \"Dialogue Concerning the Two Chief World Systems\", in 1633 Galileo was ordered to stand trial on suspicion of heresy \"for holding as true the false doctrine taught by some that the sun is the center of the world\" against the 1616 condemnation, since \"it was decided at the Holy Congregation [...] on 25 Feb 1616 that [...] the Holy Office would give you an injunction to abandon this doctrine, not to teach it to others, not to defend it, and not to treat of it; and that if you did not acquiesce in this injunction, you should be imprisoned\".\n\nGalileo was interrogated while threatened with physical torture. A panel of theologians, consisting of Melchior Inchofer, Agostino Oreggi and Zaccaria Pasqualigo, reported on the \"Dialogue\". Their opinions were strongly argued in favour of the view that the \"Dialogue\" taught the Copernican theory.\n\nGalileo was found guilty, and the sentence of the Inquisition, issued on 22 June 1633, was in three essential parts:\n\nAccording to popular legend, after his abjuration Galileo allegedly muttered the rebellious phrase \"and yet it moves\" (Eppur si muove), but there is no evidence that he actually said this or anything similar. The first account of the legend dates to a century after his death. The phrase \"Eppur si muove\" does appear, however, in a painting of the 1640s by the Spanish painter Bartolomé Esteban Murillo or an artist of his school. The painting depicts an imprisoned Galileo apparently pointing to a copy of the phrase written on the wall of his dungeon.\nAfter a period with the friendly Archbishop Piccolomini in Siena, Galileo was allowed to return to his villa at Arcetri near Florence, where he spent the rest of his life under house arrest. He continued his work on mechanics, and in 1638 he published a scientific book in Holland. His standing would remain questioned at every turn. In March 1641, Vincentio Reinieri, a follower and pupil of Galileo, wrote him at Arcetri that an Inquisitor had recently compelled the author of a book printed at Florence to change the words \"most distinguished Galileo\" to \"Galileo, man of noted name\".\n\nHowever, partially in tribute to Galileo, at Arcetri the first academy devoted to the new experimental science, the Accademia del Cimento, was formed, which is where Francesco Redi performed controlled experiments, and many other important advancements were made which would eventually help usher in The Age of Enlightenment.\n\nThere is some evidence that enemies of Galileo persuaded Urban that Simplicio was intended to be a caricature of him. Modern historians have dismissed this as most unlikely.\n\nDava Sobel argues that during this time, Urban had fallen under the influence of court intrigue and problems of state. His friendship with Galileo began to take second place to his feelings of persecution and fear for his own life. The problem of Galileo was presented to the pope by court insiders and enemies of Galileo, following claims by a Spanish cardinal that Urban was a poor defender of the church. This situation did not bode well for Galileo's defense of his book.\n\nMultiple authors have argued that the Catholic Church, rather than Galileo, was scientifically justified in the dispute over the placement and rotation of the sun and earth, given available knowledge at the time. Referring to Bellarmine's letter to Foscarini, physicist Pierre Duhem \"suggests that in one respect, at least, Bellarmine had shown himself a better scientist than Galileo by disallowing the possibility of a 'strict proof of the earth's motion', on the grounds that an astronomical theory merely 'saves the appearances' without necessarily revealing what 'really happens'\".\n\nIn his 1998 book, \"Scientific Blunders\", Robert Youngson indicates that Galileo struggled for two years against the ecclesiastical censor to publish a book promoting heliocentrism. He claims the book passed only as a result of possible idleness or carelessness on the part of the censor, who was eventually dismissed. On the other hand, Jerome K. Langford and Raymond J. Seeger contend that Pope Urban and the Inquisition gave formal permission to publish the book, \"Dialogue Concerning the Two Chief World Systems, Ptolemaic & Copernican\". They claim Urban personally asked Galileo to give arguments for and against heliocentrism in the book, to include Urban's own arguments, and for Galileo not to advocate heliocentrism.\n\nSome historians emphasize Galileo's confrontation not only with the church, but also with Aristotelian philosophy, either secular or religious. \n\nAccording to a controversial alternative theory proposed by Pietro Redondi in 1983, the main reason for Galileo's condemnation in 1633 was his attack on the Aristotelian doctrine of matter rather than his defence of Copernicanism. An anonymous denunciation, labeled \"G3\", discovered by Redondi in the Vatican archives, had argued that the atomism espoused by Galileo in his previous work of 1623, \"The Assayer\", was incompatible with the doctrine of transubstantiation of the Eucharist. At the time, investigation of this complaint was apparently entrusted to a Father Giovanni di Guevara, who was well-disposed towards Galileo, and who cleared \"The Assayer\" of any taint of unorthodoxy. A similar attack against \"The Assayer\" on doctrinal grounds was penned by Jesuit Orazio Grassi in 1626 under the pseudonym \"Sarsi\". According to Redondi:\n\nRedondi's hypothesis concerning the hidden motives behind the 1633 trial has been criticized, and mainly rejected, by other Galileo scholars. However, it has been supported recently by novelist and science writer Michael White.\n\nIn 1758 the Catholic Church dropped the general prohibition of books advocating heliocentrism from the \"Index of Forbidden Books\". It did not, however, explicitly rescind the decisions issued by the Inquisition in its judgement of 1633 against Galileo, or lift the prohibition of uncensored versions of Copernicus's \"De Revolutionibus\" or Galileo's \"Dialogue\". The issue finally came to a head in 1820 when the Master of the Sacred Palace (the Church's chief censor), Filippo Anfossi, refused to license a book by a Catholic canon, Giuseppe Settele, because it openly treated heliocentrism as a physical fact. Settele appealed to pope Pius VII. After the matter had been reconsidered by the Congregation of the Index and the Holy Office, Anfossi's decision was overturned. Copernicus's \"De Revolutionibus\" and Galileo's \"Dialogue\" were then subsequently omitted from the next edition of the \"Index\" when it appeared in 1835.\n\nIn 1979, Pope John Paul II expressed the hope that \"theologians, scholars and historians, animated by a spirit of sincere collaboration, will study the Galileo case more deeply and in loyal recognition of wrongs, from whatever side they come.\" However, the Pontifical Interdisciplinary Study Commission constituted in 1981 to study the case did not reach any definitive result. Because of this, the Pope’s 1992 speech that closed the project was vague, and did not fulfill his intentions expressed in 1979.\n\nOn February 15, 1990, in a speech delivered at La Sapienza University in Rome, Cardinal Ratzinger (later Pope Benedict XVI) cited some current views on the Galileo affair as forming what he called \"a symptomatic case that illustrates the extent to which modernity’s doubts about itself have grown today in science and technology\". As evidence, he presented the views of a few prominent philosophers including Ernst Bloch and Carl Friedrich von Weizsäcker, as well as Paul Feyerabend, whom he quoted as saying:\n\nRatzinger did not directly say whether he agreed or disagreed with Feyerabend's assertions, but did say in this same context that \"It would be foolish to construct an impulsive apologetic on the basis of such views.\"\n\nIn 1992, it was reported that the Catholic Church had turned towards vindicating Galileo:\n\nIn January 2008, students and professors protested the planned visit of Pope Benedict XVI to La Sapienza University, stating in a letter that the pope's expressed views on Galileo \"offend and humiliate us as scientists who are loyal to reason and as teachers who have dedicated our lives to the advance and dissemination of knowledge\". In response the pope canceled his visit. The full text of the speech that would have been given was made available a few days following Pope Benedict's cancelled appearance at the university.\nLa Sapienza's rector, Renato Guarini, and former Italian Prime Minister Romano Prodi opposed the protest and supported the pope's right to speak.\nAlso notable were public counter-statements by La Sapienza professors Giorgio Israel and Bruno Dalla Piccola.\n\nIn addition to the large non-fiction literature and the many documentary films about Galileo and the Galileo affair, there have also been several treatments in historical plays and films. The Museo Galileo has posted a listing of several of the plays. A listing centered on the films was presented in a 2010 article by Cristina Olivotto and Antonella Testa.\n\n\n\n\n\n\n\n\n"}
{"id": "57255110", "url": "https://en.wikipedia.org/wiki?curid=57255110", "title": "George Alexander Louis Lebour", "text": "George Alexander Louis Lebour\n\nGeorge Alexander Louis Lebour, MA, DSc, FGS (1847 – 21 February 1918) was an English geologist.\n\nLebour was educated at the Royal School of Mines and was then a staff member of the Geological Survey from 1873 to 1876. At the Durham College of Science, he was a lecturer in geological surveying from 1876 to 1879 and then succeeded David Page as professor of geology upon the latter's death in 1879. Lebour held this professorial chair until his own death in 1918.\n\nHe wrote \"Handbook to the Geology and Natural History of Northumberland and Durham\", which went through three editions from 1879 to 1889. He wrote more than 100 papers on carboniferous geology, thermal conductivity of rocks, and underground temperature. He was awarded the Murchison Medal in 1904.\n\nUpon his death, Lebour was survived by his widow Emily and two daughters; another daughter predeceased him. His youngest daughter was the noted marine biologist Marie Lebour.\n"}
{"id": "1706838", "url": "https://en.wikipedia.org/wiki?curid=1706838", "title": "Hip fracture", "text": "Hip fracture\n\nA hip fracture is a break that occurs in the upper part of the femur (thigh bone). Symptoms may include pain around the hip particularly with movement and shortening of the leg. Usually the person cannot walk.\nThey most often occur as a result of a fall. Risk factors include osteoporosis, taking many medications, alcohol use, and metastatic cancer. Diagnosis is generally by X-rays. Magnetic resonance imaging, a CT scan, or a bone scan may occasionally be required to make the diagnosis.\nPain management may occur with opioids or a nerve block. If a person's health is sufficient, surgery is generally recommended within two days. Options for surgery may include a total hip replacement or screws. Efforts to prevent deep vein thrombosis following surgery are recommended.\nAbout 15% of women break their hip at some point in their life. Women are more often affected than men. Hip fractures become more common with age. The risk of death in the year following a fracture is about 20% in older people.\n\nThe classic clinical presentation of a hip fracture is an elderly patient who sustained a low-energy fall and now has groin pain and is unable to bear weight. Pain may be referred to the supracondylar knee. On examination, the affected extremity is often shortened and unnaturally, externally rotated compared to the unaffected leg.\n\nHip fracture following a fall is likely to be a pathological fracture. The most common causes of weakness in bone are:\n\nThe hip joint, an enarthrodial joint, can be described as a ball and socket joint. The femur connects at the acetabulum of the pelvis and projects laterally before angling medially and inferiorly to form the knee. Although this joint has three degrees of freedom, it is still stable due to the interaction of ligaments and cartilage. The labrum lines the circumference of the acetabulum to provide stability and shock absorption. Articular cartilage covers the concave area of acetabulum, providing more stability and shock absorption. Surrounding the entire joint itself is a capsule secured by the tendon of the psoas muscle and three ligaments. The iliofemoral, or Y, ligament is located anteriorly and serves to prevent hip hyperextension. The pubofemoral ligament is located anteriorly just underneath the iliofemoral ligament and serves primarily to resist abduction, extension, and some external rotation. Finally the ischiofemoral ligament on the posterior side of the capsule resists extension, adduction, and internal rotation. When considering the biomechanics of hip fractures, it is important to examine the mechanical loads the hip experiences during low energy falls.\n\nThe hip joint is unique in that it experiences combined mechanical loads. An axial load along the shaft of the femur results in compressive stress. Bending load at the neck of the femur causes tensile stress along the upper part of the neck and compressive stress along the lower part of the neck. While osteoarthritis and osteoporosis are associated with bone fracture as we age, these diseases are not the cause of the fracture alone. Low energy falls from standing are responsible for the majority of fractures in the elderly, but fall direction is also a key factor. Elderly tend to fall to the side as instead of forward, and the lateral hip and strikes the ground first. During a sideways fall, the chances of hip fracture see a 15-fold and 12-fold increase in elderly males and females, respectively.\n\nElderly individuals are also predisposed to hip fractures due to many factors that can compromise proprioception and balance, including medications, vertigo, stroke, and peripheral neuropathy.\n\nDisplaced fractures of the trochanter or femoral neck will typically cause external rotation and shortening of the leg when the patient is laying supine.\n\nTypically, radiographs are taken of the hip from the front (AP view), and side (lateral view). Frog leg views are to be avoided, as they may cause severe pain and further displace the fracture. In situations where a hip fracture is suspected but not obvious on x-ray, an MRI is the next test of choice. If an MRI is not available or the patient can not be placed into the scanner a CT may be used as a substitute. MRI sensitivity for radiographically occult fracture is greater than CT. Bone scan is another useful alternative however substantial drawbacks include decreased sensitivity, early false negative results, and decreased conspicuity of findings due to age related metabolic changes in the elderly.\n\nAs the patients most often require an operation, full pre-operative general investigation is required. This would normally include blood tests, ECG and chest x-ray.\n\nX-rays of the affected hip usually make the diagnosis obvious; AP (anteroposterior) and lateral views should be obtained.\n\nTrochanteric fractures are subdivided into either intertrochanteric (between the greater and lesser trochanter) or pertrochanteric (through the trochanters) by the Müller AO Classification of fractures. Practically, the difference between these types is minor. The terms are often used synonymously. An \"isolated trochanteric fracture\" involves one of the trochanters without going through the anatomical axis of the femur, and may occur in young individuals due to forceful muscle contraction. Yet, an \"isolated trochanteric fracture\" may not be regarded as a true hip fracture because it is not cross-sectional.\n\nThe majority of hip fractures are the result of a fall, particularly in the elderly. Therefore, identifying why the fall occurred, and implementing treatments or changes, is key to reducing the occurrence of hip fractures. Multiple contributing factors are often identified. These can include environmental factors and medical factors (such as postural hypotension or co-existing disabilities from disease such as Stroke or Parkinson's Disease which cause visual and/or balance impairments). A recent study has identified a high incidence of undiagnosed cervical spondylotic myelopathy (CSM) amongst patients with a hip fracture. This is relatively unrecognised consequent of CSM.\n\nAdditionally, there is some evidence to systems designed to offer protection in the case of a fall. Hip protectors, for example appear to decrease the number of hip fractures among the elderly. They; however, are not often used.\n\nMost hip fractures are treated surgically by implanting an orthosis. Surgical treatment outweighs the risks of nonsurgical treatment which requires extensive bedrest. Prolonged immobilization increases risk of thromboembolism, pneumonia, deconditioning, and decubitus ulcers. Regardless, the surgery is a major stress, particularly in the elderly. Pain is also significant, and can also result in immobilization, so patients are encouraged to become mobile as soon as possible, often with the assistance of physical therapy. Skeletal traction pending surgery is not supported by the evidence. Regional nerve blocks are useful for pain management in hip fractures. Surgery can be performed under general anaesthesia or with neuraxial techniques - choice is based on surgical and patient factors, as outcomes such as mortality and post-procedure complications including pneumonia, MI, stroke or confusion, are not affected by anaesthetic technique.\n\nRed blood cell transfusion is common for people undergoing hip fracture surgery due to the blood loss sustained during surgery and from the injury. Giving blood when the hemoglobin is less than 10 g/dL versus less than 8 g/dL was of unclear benefit per a 2015 Cochrane review. A review in 2018 however found that waiting until the hemoglobin was less than 8 g/dL or the person had symptoms increased the risk of heart problems.\n\nIf operative treatment is refused or the risks of surgery are considered to be too high the main emphasis of treatment is on pain relief. Skeletal traction may be considered for long term treatment. Aggressive chest physiotherapy is needed to reduce the risk of pneumonia and skilled rehabilitation and nursing to avoid pressure sores and DVT/pulmonary embolism Most people will be bedbound for several months. Non-operative treatment is now limited to only the most medically unstable or demented patients, or those who are nonambulatory at baseline with minimal pain during transfers.\n\nSurgery on the same day or day following the break is estimated to reduce postoperative mortality in people who are medically stable. \n\nFor low-grade fractures (Garden types 1 and 2), standard treatment is fixation of the fracture in situ with screws or a sliding screw/plate device. This treatment can also be offered for displaced fractures after the fracture has been reduced.\n\nFractures managed by closed reduction can possibly be treated by percutaneously inserted screws.\n\nIn elderly patients with displaced or intracapsular fractures many surgeons prefer to undertake a hemiarthroplasty, replacing the broken part of the bone with a metal implant. The advantage is that the patient can mobilize without having to wait for healing.\n\nIn elderly patients who are medically well and still active, a total hip replacement may be indicated.\n\nTraction is contraindicated in femoral neck fractures due to it affecting blood flow to the head of the femur.\n\nA trochanteric fracture, below the neck of the femur, has a good chance of healing.\n\nClosed reduction may not be satisfactory and open reduction then becomes necessary. The use of open reduction has been reported as 8-13% among pertrochanteric fractures, and 52% among intertrochanteric fractures. Both intertrochanteric and pertrochanteric fractures may be treated by a dynamic hip screw and plate, or an intramedullary rod.\n\nThe fracture typically takes 3–6 months to heal. As it is only common in elderly, removal of the dynamic hip screw is usually not recommended to avoid unnecessary risk of second operation and the increased risk of re-fracture after implant removal. The most common cause for hip fractures in the elderly is osteoporosis; if this is the case, treatment of the osteoporosis can well reduce the risk of further fracture. Only young patients tend to consider having it removed; the implant may function as a stress riser, increasing the risk of a break if another accident occurs.\n\nSubtrochanteric fractures may be treated with an intramedullary nail or a screw-plate construction and may require traction pre-operatively, though this practice is uncommon. It is unclear if any specific type of nail results in different outcomes than any other type of nail.\n\nRehabilitation has been proven to increase daily functional status. It is unclear if the use of anabolic steroids effects recovery.\n\nThere is not enough evidence to ascertain what are the best strategies to promote walking after hip fracture surgery.\n\nOral supplements with non-protein energy, protein, vitamins and minerals started before or early after surgery may prevent complications during the first year after hip fracture in aged adults; without seemingly effects on mortality.\n\nNonunion, failure of the fracture to heal, is common in fractures of the neck of the femur, but much more rare with other types of hip fracture. Avascular necrosis of the femoral head occurs frequently (20%) in intracapsular hip fractures, because the blood supply is interrupted.\n\nMalunion, healing of the fracture in a distorted position, is very common. The thigh muscles tend to pull on the bone fragments, causing them to overlap and reunite incorrectly. Shortening, varus deformity, valgus deformity, and rotational malunion all occur often because the fracture may be unstable and collapse before it heals. This may not be as much of a concern in patients with limited independence and mobility.\n\nHip fractures rarely result in neurological or vascular injury.\n\nDeep or superficial wound infection has an approximate incidence of 2%. It is a serious problem as superficial infection may lead to deep infection. This may cause infection of the healing bone and contamination of the implants. It is difficult to eliminate infection in the presence of metal foreign bodies such as implants. Bacteria inside the implants are inaccessible to the body's defence system and to antibiotics. The management is to attempt to suppress the infection with drainage and antibiotics until the bone is healed. Then the implant should be removed, following which the infection may clear up.\n\nImplant failure may occur; the metal screws and plate can break, back out, or cut out superiorly and enter the joint. This occurs either through inaccurate implant placement or if the fixation does not hold in weak and brittle bone. In the event of failure, the surgery may be redone, or changed to a total hip replacement.\n\nMal-positioning: The fracture can be fixed and subsequently heal in an incorrect position; especially rotation. This may not be a severe problem or may require subsequent osteotomy surgery for correction.\n\nMany people are unwell before breaking a hip; it is common for the break to have been caused by a fall due to some illness, especially in the elderly. Nevertheless, the stress of the injury, and a likely surgery, increases the risk of medical illness including heart attack, stroke, and chest infection.\n\nHip fracture patients are at considerable risk for thromboemoblism, blood clots that dislodge and travel in the bloodstream. Deep venous thrombosis (DVT) is when the blood in the leg veins clots and causes pain and swelling. This is very common after hip fracture as the circulation is stagnant and the blood is hypercoagulable as a response to injury. DVT can occur without causing symptoms. A pulmonary embolism (PE) occurs when clotted blood from a DVT comes loose from the leg veins and passes up to the lungs. Circulation to parts of the lungs are cut off which can be very dangerous. Fatal PE may have an incidence of 2% after hip fracture and may contribute to illness and mortality in other cases.\n\nMental confusion is extremely common following a hip fracture. It usually clears completely, but the disorienting experience of pain, immobility, loss of independence, moving to a strange place, surgery, and drugs combine to cause delirium or accentuate pre-existing dementia.\n\nUrinary tract infection (UTI) can occur. Patients are immobilized and in bed for many days; they are frequently catheterised, commonly causing infection.\n\nProlonged immobilization and difficulty moving make it hard to avoid pressure sores on the sacrum and heels of patients with hip fractures. Whenever possible, early mobilization is advocated; otherwise, alternating pressure mattresses should be used.\n\nHip fractures are very dangerous episodes especially for elderly and frail patients. The risk of dying from the stress of the surgery and the injury in the first thirty days is about 10%. At one year after fracture this may reach 30%. If the condition is untreated the pain and immobility imposed on the patient increase that risk. Problems such as pressure sores and chest infections are all increased by immobility. The prognosis of untreated hip fractures is very poor.\n\nAmong those affected over the age of 65, 40% are transferred directly to long-term care facilities, long-term rehabilitation facilities, or nursing homes; most of those affected require some sort of living assistance from family or home-care providers. 50% permanently require walkers, canes, or crutches for mobility; all require some sort of mobility assistance throughout the healing process. Most of the recovery of walking ability and activities of daily living occurs within 6 months of the fracture. After the fracture about half of older people recover their pre-fracture level of mobility and ability to perform instrumental activities of daily living, while 40–70 % regain their level of independence for basic activities of daily living.\n\nAmong those affected over the age of 50, approximately 25% die within the next year due to complications such as blood clots (deep venous thrombosis, pulmonary embolism), infections, and pneumonia.\n\nPatients with hip fractures are at high risk for future fractures including hip, wrist, shoulder, and spine. After treatment of the acute fracture, the risk of future fractures should be addressed. Currently, only 1 in 4 patients after a hip fracture receives treatment and work up for osteoporosis, the underlying cause of most of the fractures. Current treatment standards include the starting of a bisphosphonate to reduce future fracture risk by up to 50%.\n\nHip fractures are seen globally and are a serious concern at the individual and population level. By 2050 it is estimated that there will be 6 million cases of hip fractures worldwide. One study published in 2001 found that in the US alone, 310,000 individuals were hospitalized due to hip fractures, which can account for 30% of Americans who were hospitalized that year. Another study found that in 2011, femur neck fractures were among the most expensive conditions seen in US hospitals, with an aggregated cost of nearly $4.9 billion for 316,000 inpatient hospitalizations. Rates of hip fractures is declining in the United States, possibly due to increased use of bisphosphonates and risk management. Falling, poor vision, weight and height are all seen as risk factors. Falling is one of the most common risk factors for hip fractures. Approximately 90% of hip fractures are attributed to falls from standing height.\n\nGiven the high morbidity and mortality associated with hip fractures and the cost to the health system, in England and Wales, the National Hip Fracture Database is a mandatory nationwide audit of care and treatment of all hip fractures.\n\nAll populations experience hip fractures but numbers vary with race, gender, and age. Women suffer three times as many hip fractures as men. In a lifetime, men have an estimated 6% risk whereas postmenopausal women have an estimated 14% risk of suffering a hip fracture. These statistics provide insight over a lifespan and conclude that women are twice as likely to suffer a hip fracture. The overwhelming majority of hip fractures occur in white individuals while blacks and Hispanics have a lower rate of them. This may be due to their generally greater bone density and also because whites have longer overall lifespan and higher likelihood of reaching an advanced age where the risk of breaking a hip goes up. Deprivation is also a key factor: in England it has been found that people in the poorest parts of the country are more likely to fracture a hip and less likely to recover well than those in the least deprived areas.\n\nAge is the most dominant factor in hip fracture injuries, with most cases occurring in people over 75. The increase of age is related to the increase of the incidence of hip fracture, which is the most frequent cause of hospitalization in centenarians, overcoming congestive heart failure and respiratory infection. Falls are the most common cause of hip fractures, around 30-60% of older adults fall each year. This increases the risk for hip fracture and leads to the increase risk of death in older individuals, the rate of one year mortality is seen from 12-37%. For those remaining patients who do not suffer from mortality, half of them need assistance and cannot live independently. Also, older adults sustain hip fractures because of osteoporosis, which is a degenerative disease due to age and decrease in bone mass. The average age for suffering a hip fracture is 77 years old for women and 72 years old for men. This shows how closely age is related to hip fractures.\n\n"}
{"id": "47826292", "url": "https://en.wikipedia.org/wiki?curid=47826292", "title": "John James Hall", "text": "John James Hall\n\nJohn James Hall FRAS (11 December 1845 – 15 January 1941) was an eminent horologist and author who restored many early clocks.\n\nJohn James Hall spent his working life as an employee of the London and South Western Railway from 1865, but on retirement devoted his time to horology. He made over 200 contributions to \"The Horological Journal\", \"The English Mechanic and World of Science\", \"Watch and Clockmaker\" and other journals.\n\nHis best known accomplishments were the restoration of the fourteenth-century astronomical clocks in St Mary's Church, Ottery St Mary in 1907 and Exeter Cathedral in 1910.\n\nHe designed a new clock for Exeter Public Library which was set going in 1931. One of his major areas of study was the life and work of Jacob Lovelace of Exeter. He also published his complete articles in \"Fasces Exonienses\".\n\nHe was elected a Fellow of the Royal Astronomical Society on 10 February 1899. He was also a Fellow of the British Horological Institute, the Meteorological Society, the National Geographic Society of Washington, and the . \n\nHis ashes were interred in the Syke Chantry in the north transept of Exeter Cathedral, below the clock he had restored.\n"}
{"id": "12984160", "url": "https://en.wikipedia.org/wiki?curid=12984160", "title": "Joseph Finnegan (cryptographer)", "text": "Joseph Finnegan (cryptographer)\n\nJoseph Finnegan (April 12, 1905 – September 8, 1980) was an American linguist and cryptanalyst with Station Hypo during the Second World War.\n\nIn 2002, Tex Biard described Finnegan as \"intuitive\" and \"brilliant\", and second only to Station Hypo chief Joseph J. Rochefort, saying that Finnegan's survival of the bombing of the \"USS Tennessee\" was a \"fatal mistake\" on the part of the Japanese, and that Finnegan's survival \"cost (the Japanese) the war.\"\n\nEdwin T. Layton, in his book \"And I Was There: Pearl Harbor and Midway -- Breaking the Secrets\" (1985) recounts a tremendous effort by Finnegan on the Hypo team concerning the exact date on which the attack on Midway would occur. This involved the date-time groups in Japanese naval messages.\n\nLayton refers to the date-time data as being “superenciphered,” meaning that this data was preencoded even before it was added to the JN-25 cipher. When Hypo made their all-out effort to crack this, they started by searching the stacks of printouts and punched cards for five-digit number sequences. Those they found were in low grade codes, a poor starting point, but a starting point.\n\nLayton describes this method as \"involving a 12 x 31 (12 rows for months, 31 columns for day) garble check. The 31 kana [Japanese syllabic scripts] of the first row were A, I, U, E, O, KA, KI …………….HA, HI, HU, HE, HO. The second row was I, U, E, O ……………HE, HO, A; the third, U, E, O ……….HO, A, I, and so on, for 12 rows. At the left, representing the 12 months, was a column of 12 kana, different from those in the table – SA, AI, SU, SE, SO, TA, TI, TU, TE, TO, NA, NI (SA for January, NI for December). To encipher, for example 27 May, one picked the 5th line (May=SO), ran across to the twenty-seventh column, HA, and recorded the kana at that intersection, HO. The encipherment, then, was SO, HA, HO, the third kana providing the garble check.\" (Layton, pp. 427–428)\n\n"}
{"id": "4435012", "url": "https://en.wikipedia.org/wiki?curid=4435012", "title": "Karel Fortyn", "text": "Karel Fortyn\n\nKarel Fortýn (1930–2001) was a Czech (originally Czechoslovakian) physician who invented a breakthrough surgical method in healing cancer called devitalization.\n"}
{"id": "23276232", "url": "https://en.wikipedia.org/wiki?curid=23276232", "title": "Latvian Museum of Natural History", "text": "Latvian Museum of Natural History\n\nThe Natural History Museum of Latvia is a natural history museum in Riga, Latvia. It was founded in 1845 by the Riga Balto-German intelligentsia as part of the Riga Naturalist Society which also established a library. It contains the oldest collection of natural history in the Baltic States.\n\n"}
{"id": "5482979", "url": "https://en.wikipedia.org/wiki?curid=5482979", "title": "List of Abell clusters", "text": "List of Abell clusters\n\nThe Abell catalogue is a catalogue of approximately 4,000 galaxy clusters with at least 30 members, almost complete to a redshift of \"z\" = 0.2. It was originally compiled by the American astronomer George O. Abell in 1958 using plates from POSS, and extended to the southern hemisphere by Abell, Corwin and Olowin in 1987. The name \"Abell\" is also commonly used as a designation for objects he compiled in a catalogue of 86 planetary nebulae in 1966. The proper designation for the galaxy clusters is ACO, as in \"ACO 13\", while the planetary-nebula designation is the single letter A, as in \"A 39\".\n\n\n"}
{"id": "37001823", "url": "https://en.wikipedia.org/wiki?curid=37001823", "title": "List of astronomical instruments", "text": "List of astronomical instruments\n\nAstronomical instruments include:\n\n"}
{"id": "22148940", "url": "https://en.wikipedia.org/wiki?curid=22148940", "title": "List of eponymous fractures", "text": "List of eponymous fractures\n\nEponymous fractures and fracture-dislocations are most commonly named after the doctor who first described them. They may also be named after an activity with which they are associated.\n"}
{"id": "46987861", "url": "https://en.wikipedia.org/wiki?curid=46987861", "title": "List of fentanyl analogues", "text": "List of fentanyl analogues\n\nThis is a list of fentanyl analogues, including both compounds developed by pharmaceutical companies for legitimate medical use, and those which have been sold as designer drugs and reported to national drug control agencies such as the DEA, or transnational agencies such as the EMCDDA and UNODC. This is not a comprehensive listing of fentanyl analogues, as more than 1400 compounds from this family have been described in the scientific and patent literature, but it includes all notable compounds that have reached late-stage human clinical trials, or which have been identified as having been sold as designer drugs.\n\nIn the United States, the Drug Enforcement Agency placed the broadly defined class of \"Fentanyl-Related Substances\" on the list of Schedule I drugs in 2018, making it illegal to manufacture, distribute, or possess fentanyl analogs.\nSeveral jurisdictions have implemented analogue law controls of fentanyl analogues in an attempt to pre-emptively ban novel derivatives before they appear on the market. One representative example is the New Zealand provisions enacted in 1988 in response to the first wave of fentanyl derivatives. This bans a set of structures as follows;\n\n\"Fentanyl analogues, in which the N-[1-(2-phenethyl)-4-piperidyl]aniline nucleus has additional radicals, either alone or in combination, attached as follows:\n\n(a) an acetyl, propionyl, butenoyl or butanoyl radical, attached to the aniline nitrogen atom:\n\n(b) 1 or more alkyl radicals, with up to 10 carbon atoms in total, attached to the ethyl moeity:\n\n(c) any combination of up to 5 alkyl radicals and/or alkoxy radicals (each with up to 6 carbon atoms, including cyclic radicals) and/or halogen radicals, attached to each of the benzene rings.\"\n\nA more recent and somewhat broader example was introduced into US Federal legislation in 2018, covering the following structures;\n\n\"...fentanyl-related substances includes any substance not otherwise controlled in any schedule...that is structurally related to fentanyl by one or more of the following modifications:\n\n\n\nA more extensive list can be found here.\n"}
{"id": "25453138", "url": "https://en.wikipedia.org/wiki?curid=25453138", "title": "List of uncertainty propagation software", "text": "List of uncertainty propagation software\n\nList of uncertainty propagation software used to perform propagation of uncertainty calculations:\n\n\n\n"}
{"id": "13710682", "url": "https://en.wikipedia.org/wiki?curid=13710682", "title": "Lunar Landing Research Facility", "text": "Lunar Landing Research Facility\n\nThe Lunar Landing Research Facility was an area at NASA's Langley Research Center in Hampton, Virginia that was used to simulate Apollo Moon landings with a mock Lunar Module powered by a small rocket motor suspended from a crane over a simulated lunar landscape.\n\nCompleted in 1965 at a cost of $3.5 million, the facility was used by 24 astronauts, including Neil Armstrong and Buzz Aldrin, to practice solving piloting problems they would encounter in the last 150 feet of descent to the surface of the Moon.\n\nThe structure was used to facilitate \"flying\" a full-scale Lunar Excursion Module Simulator (LEMS). The LEMS was suspended from a -tall, -long A-frame gantry by an overhead bridge crane. The LEMS is now on display at the Virginia Air and Space Center.\n\nRe-designated the Impact Dynamics Research Facility (IDRF) in 1974, the site was used for research on aircraft crashes until 2003. With limited funding for maintenance, NASA then closed the facility and it was listed for demolition.\n\nIn 2004, NASA determined that the IDRF could be adapted to support the Constellation program. It was re-opened in 2005 to conduct landing tests associated with the development of the Crew Exploration Vehicle (CEV) \"Orion\". The facility was renamed the Landing and Impact Research Facility (LandIR) and minor modifications were made, including a new parallel winch system to support full-scale \"Orion\" testing and a new hydro-impact basin (splashdown pool) below the gantry. Construction of the basin was completed in 2011. After Constellation was cancelled, the LandIR continued performing impact testing since the CEV will be used to service the International Space Station.\n\nThe facility was designated a National Historic Landmark in 1985 for its role in the space program.\n\n\n"}
{"id": "142142", "url": "https://en.wikipedia.org/wiki?curid=142142", "title": "Motion picture content rating system", "text": "Motion picture content rating system\n\nA motion picture content rating system is designated to classify films with regard to suitability for audiences in terms of issues such as sex, violence, substance abuse, profanity, impudence or other types of mature content. A particular issued rating can be called a certification, classification, certificate or rating. Ratings typically carry age recommendations in an advisory or restrictive capacity, and are often given in lieu of censorship. In some jurisdictions the legal obligation of administering the rating may be imposed on movie theaters.\n\nIn countries such as Australia and Singapore, an official government body decides on ratings; in other countries, such as the United States, it is done by industry committees with little if any official government status. In most countries, however, films that are considered morally offensive have been censored, restricted, or banned. Even if the film rating system has no legal consequences, and a film has not explicitly been restricted or banned, there are usually laws forbidding certain films, or forbidding minors to view them.\n\nThe influence of specific factors in deciding a rating varies from country to country. In countries such as the United States, films with strong sexual content tend to be restricted to older viewers, though those same films are very often considered suitable for all ages in countries such as France and Germany. In contrast, films with violent content which would be rated leniently in the United States and Australia are often subject to high ratings and sometimes even censorship in countries such as Germany and Finland.\n\nOther factors may or may not influence the classification process, such as being set within a non-fictional historical context, whether the film glorifies violence or drug use, whether said violence or drug use is carried out by the protagonist, with whom the viewer should empathize, or by the antagonist. In Germany, for example, films depicting explicit war violence in a real war context (such as the Second World War) are handled more leniently than films with purely fictional settings.\n\nA film may be produced with a particular rating in mind. It may be re-edited if the desired rating is not obtained, especially to avoid a higher rating than intended. A film may also be re-edited to produce an alternate version for other countries.\n\nA comparison of current film rating systems, showing age on the horizontal axis. Note however that the specific \"criteria\" used in assigning a classification can vary widely from one country to another. Thus a color code or age range cannot be directly compared from one country to another.\n\nKey:\n\n\nThrough its Advisory Commission of Cinematographic Exhibition (\"Comisión Asesora de Exhibición Cinematográfica\") the National Institute of Cinema and Audiovisual Arts (INCAA) issues ratings for films based on the following categories:\n\n\nThe Classification Board and Classification Review Board are government-funded organizations which classify all films that are released for public exhibition.\n\nFilms intended to inform, educate or instruct or concerned with sport, religion or music are exempt from classification provided they do not contain material that would result in an \"M\" rating or higher if submitted for classification.\n\nMotion pictures are rated by the Austrian Board of Media Classification (ABMC) for the Federal Ministry of Education, Arts and Culture (Bundesministerium für Unterricht, Kunst und Kultur). The recommendations made by the ABMC are generally not legally binding and there are nine sets of state laws on the cinema sector with different age provisions. The only exception is in the case of \"16\" rated films, since under Austrian law there is a legal age restriction on certain types of content i.e. discrimination, sexual abuse, glorification of violence etc. In addition to the ABMC's age recommendations, in the state of Vienna children under the age of 6 are only permitted to attend public film performances if they are accompanied.\n\nThe AMBC issues age recommendation from the following categories:\n\nThere are only two classifications for films publicly exhibited in Belgium issued by the Inter-Community Commission for Film Rating (; ). Films are prohibited to minors under the age of 16 unless passed by the commission. There is no mandatory rating system for video formats but 90% of video distribution abides by the voluntary Belgium Video Federation. It is basically the same as the system for theatrical exhibition, but also provides a \"12\" rating.\n\n\nAll films that are exhibited in public or released on a home video format in Brazil must be submitted for classification to the advisory rating (\"Classificação Indicativa\", abbreviated ClassInd), which is run by the Brazilian Ministry of Justice (\"Ministério da Justiça\"). Anyone below the film's minimum age can watch it if accompanied by the parent or guardian who is at least 18 years old, except for those rated \"Not recommended for ages under 18\", which, by law, are strictly prohibited from viewing by people under 18. Unlike many countries, the ClassInd does not have any legal right to ban, demand cuts or refuse to rate any movie.\n\nThe ClassInd uses the following system:\n\nThere are also operational descriptions of attenuating and aggravating elements that can interfere on the final rating.\n\nThe Bulgarian film rating system is defined in the Film Industry Act of 2003 and administered by the National Film Rating Committee.\n\n\nFilm ratings in Canada are a provincial responsibility, and each province has its own legislation, rules and regulations regarding rating, exhibition and admission. Ratings are required for theatrical exhibition, but not all provinces require classification for home video. In the past there was a wide range of rating categories and practices in the various provinces; however, the seven rating systems—with the exception of Quebec—now all use categories and logos derived from the Canadian Home Video Rating System (CHVRS).\n\nThe categories are mostly identical to the CHVRS with a few minor variations. In the provinces that require classification of video formats, supply of 14A and 18A films is restricted to customers above those ages. In the case of theater exhibition, children are admitted to 14A and 18A films in the Manitoba and Maritime provinces if accompanied by an adult, although admittance is restricted to children over the age of 14 in the case of 18A films. Likewise, British Columbia, Saskatchewan (administered by the British Columbia Film Classification Office), Alberta and Ontario also admit children to 14A and 18A films if accompanied, but do not impose an age restriction on 18A films. The Maritimes and British Columbia (along with Saskatchewan) also provide an \"A\" classification for adult content. Some provinces, such as Nova Scotia, reserve the right to prohibit films altogether.\n\nIn general, the categories are:\n\nIn Quebec, the Régie du cinéma rates all films and videos. The Régie is a governmental agency overseen by the Quebec Ministry of Culture and Communications; its purview devolves from the \"Cinema Act\" (chapter C-18.1). In some cases the Régie du cinéma may refuse to provide a classification, effectively banning the film. Educational and sports films are exempt from classification.\n\n\nFilms are classified by the Council of Cinematographic Classification (\"Consejo de Calificación Cinematográfica\") which is a central agency under the Ministry of Education. In 2002 legislation was enacted which reversed the ban on all 1,090 films that had previously been banned in Chile.\n\nThe age ratings are:\n\n\nThe age ratings may also be supplemented by the following content categories:\n\n\nPornographic films may only be exhibited at venues licensed for that purpose. Minors are not admitted to films with pornographic or excessively violent content.\n\nChina does not have a rating system. Only films that are passed as \"suitable for all ages\" are released although some exhibitors have introduced informal ratings. A March 2017 effective law on film does require non-violations of the lawful rights and interests of minors (Chinese: 未成年人) or harming the physical and psychological health of minors. However, in an interview with China Central Television in the same month, the State Administration of Press, Publication, Radio, Film and Television's film chief Mr. Zhang Hongsen said it was inaccurate for the media to label the guideline for minors as manual/euphemistic classification and it was a misinterpretation or over-interpretation of the new law.\n\nAs of June 22, 2005, the Ministry of Culture issued its new rating system. The classifications are:\n\n\nIn Denmark, the Media Council for Children and Young People currently rates films. Films do not have to be submitted for a rating and in such instances must be labelled a \"15\" (restricted to people aged 15 and above). Children aged 7 and above may attend any performance—including those restricted to older audiences—if they are accompanied by an adult.\n\nFilm classification in Estonia is regulated by the Child Welfare Act.\n\n\nFilms in Finland are classified by the National Audiovisual Institute. A minor up to 3 years younger than the age limit is permitted to see a film in a cinema when accompanied by an adult, except for 18-rated films. Films with an age rating may contain an additional marker for violence, sex, fear, or substance abuse. The ratings are as follows:\n\n\nPrior to showing in theaters, a distribution certificate must be obtained from the Ministry of Culture. The Minister will decide which certificate to issue based on a recommendation by the National Center of Cinematography and the moving image (CNC) classification. In some cases films may be classified as \"pornographic films or those containing an incitement to violence\" or completely prohibited from screening. A certificate will be granted from the following:\n\n\nThe Freiwillige Selbstkontrolle der Filmwirtschaft (Voluntary Self-Regulation of the Film Industry, FSK) has a film ratings system under which films are classified. All the ratings contain the phrase \"gemäß §14 JuSchG\" (in accordance with §14 of the Youth Protection Law), signifying that they are legally binding for minors. Cinemas may legally exhibit films without a classification but minors are prohibited from such screenings.\n\n\nThe FSK rating also limits the time of the day in which the movie may be aired on free-to-air TV stations to a time frame between 22:00 (FSK 16) or 23:00 (FSK 18) and 6:00. Stations are permitted to broadcast films not approved for audiences under 12 at their own discretion.\n\nAll publicly released films must be submitted to the Youth Committee for classification. There are four categories:\n\n\nFilms intended for public exhibition have to be submitted to the Director of Film, Newspaper and Article Administration, who is the Film Censorship Authority (FCA) under the Ordinance, for approval. Films approved for public exhibition are then either classified or exempted from classification.\n\n\nOf the four levels, Levels I, II, and II are unrestricted. Only Level III is a restricted category and regulated by the Government.\n\nHungarian ratings are decided by the National Media and Infocommunications Authority (NMHH):\n\n\nThe current one is the third motion picture rating system in Hungary. The first system existed between 1965 and 2004, and was administered by the Ministry for National Cultural Heritage and its predecessors. Its categories were \"Without age restriction\", \"Not recommended below age of 14\", \"Above age of 16 only\", and \"Above age of 18 only\". A second system was introduced in 2004 which was overhauled in 2011 in favour of the current system. Its categories—given by the National Film Office—were \"Without age restriction\", \"Parental guidance suggested below age of 12\", \"Not recommended below age of 16\", \"Not recommended below age of 18\", and \"For adults only\".\n\nSince July 1, 2006, FRÍSK (short for Félag rétthafa í sjónvarps- og kvikmyndaiðnaði) has replaced the Kvikmyndaskoðun system in Iceland. In October 2013, FRÍSK announced that it was adopting a new system similar to the Netherlands' Kijkwijzer at least through 2016. The Icelandic ratings system also provides an \"18\" rating in addition to the Kijkwijzer ratings. Under Icelandic law, minors aged 14-years-old and over may be admitted to a film carrying a higher age rating if accompanied by an adult.\n\n\nIn India, Central Board of Film Certification (CBFC) is responsible for certifying films meant for public exhibition.\n\n\nMotion pictures shown in Indonesia must undergo reviewing by the Indonesian Film Censorship Board. Other than issuing certificates, the LSF/IFCB also reviews and issues permits for film-related advertising, such as movie trailers and posters. LSF has the authority to cut scenes from films. Films passed for exhibition are awarded one of the following classifications:\n\nAll films that are exhibited in public or released on a home video format must be submitted for classification to the Irish Film Classification Office (IFCO).\n\n\nAll films aimed to be shown in Italy are classified by the Committee for the Theatrical Review of the Italian Ministry of Cultural Heritage and Activities into one of the following categories:\n\n\nFilm classification in Jamaica is a requirement of the Cinematograph Act of 1913, which also established the Cinematograph Authority.\n\n\nA Japanese film rating regulator known as [full-name: ] has a film classification system under which films are classified into one of four categories. The categories have been in use since 1 May 1998.\n\nIn Kazakhstan, films are rated by the Committee for Culture of the Ministry for Culture and Information.\n\n\nIn Latvia it is the duty of the producer of a film or distributor to assign a rating according to a pre-determined set of criteria. All publicly exhibited films, visual recordings and films broadcast over television and electronic networks must be classified.\n\n\nHistorically, film censorship in Malaysia was carried out by police under the Theatre Ordinance 1908. In 1954 the Film Censorship Board (LPF) was created to censor films distributed across Malaysia in accordance with the Cinematograph Films Act 1952, and later the Film Censorship Act 2002. Malaysia's motion picture rating system was introduced in 1953, initially classifying films either for General Audiences (\"Tontonan Umum\") or For Adults Only (\"Untuk Orang Dewasa Sahaja\"), and in 1996 these classifications were changed to U and four different 18 categories. In mid-April 2010, the four 18 categories were deprecated, and was simplified to just 18. In late 2008, the PG13 classification was introduced, which was changed to P13 in 2012.\n\nUpon viewing the board will assign one of three categories to the film:\n\n\nShould a film be approved, the Board then assigns the film a classification. As of 2012 the ratings are:\n\n\nFilm in the Maldives are classified by the National Bureau of Classification. Certificates issued are based on the following categories:\n\nAs of 2012, films in Malta are classified by the Film Board in accordance with the Malta Council for Culture and the Arts Act. As part of an overhaul in 2013 the \"14\" and \"16\" age classifications were replaced by \"12A\" and \"15\"; the \"PG\" rating was redefined while \"U\", \"12\" and \"18\" were retained in their existing form.\n\nIf the film is deemed \"fit for exhibition\" it will be awarded one of the following classifications:\n\n\nThe General Directorate of Radio, Television and Cinematography (in Spanish, \"Dirección General de Radio, Televisión y Cinematografía\") is the issuer of ratings for motion pictures. The RTC is an agency of the Department of State (\"Secretaría de Gobernación\"). It has its own classification system, as follows:\n\n\nIn the Netherlands, the Kijkwijzer system is used, which is executed by the Netherlands Institute for the Classification of Audiovisual Media (NICAM). Under Dutch law children are admitted to films carrying an age rating if accompanied by an adult except in the case of \"16\" rated films.\n\n\nMostly, these icons are used along with other symbols, displaying if a movie contains violence, sexual content, frightening scenes, drug or alcohol abuse, discrimination, or coarse language. These symbols are also used for TV-programs in the Netherlands.\n\nThe \"Films, Videos, and Publications Classification Act 1993\" gives the Office of Film and Literature Classification the power to classify publications into three categories: unrestricted, restricted, or \"objectionable\" (banned). With a few exceptions, films, videos, DVDs and restricted computer games must carry a label before being offered for supply or exhibited to the public.\n\nIn 2017 the Office of Film and Literature Classification created a special RP18 rating for online content in response to the Netflix television series, \"13 Reasons Why\". The new classification reflects concerns raised with 17 and 18 year olds in New Zealand being at a higher risk of suicide. The current ratings are:\n\n\nThe National Film and Video Censors Board classifies films, videos, DVDs, and VCDs. Classifications carrying an age rating are legally restricted, although the \"15\" and \"18\" classifications do not apply to people below 2 years of age. The categories are:\n\nThe Norwegian Media Authority (\"Medietilsynet\") sets the age limits on films to be exhibited in Norway. Films not submitted to the Media Authority for classification carry a mandatory age rating of \"18\".\n\nThe following age limits apply to films to be shown in cinemas:\n\n\nThe Media Authority has no power to ban films but must not classify films which they consider contravene Norwegian criminal law.\n\nIn the Philippines, motion pictures, along with television programs, are rated by the Movie and Television Review and Classification Board, a special agency of the Office of the President. As of 2012, the Board uses six classification ratings.\n\n\nRatings in Poland are not set by any board or advisory body. Prior to 1989 the applicable age ratings were \"no age limit\", \"over 7\", \"over 12\", \"over 15\" and \"over 18\" and were set by The General Committee of Cinematography. Since 1989 there is no official classification system, with age ratings being self-prescriptive and set by the distributors. In case of television, the supervisory body – Krajowa Rada Radiofonii i Telewizji (KRRiT, The National Council of Radio Broadcasting and Television) can impose fines upon those responsible for improper rating of a broadcast, or lack of it.\n\nMovies are rated in Portugal by the Comissão de Classificação de Espectáculos of the Ministry of Culture. In cinemas the ratings are mandatory (subject to parental guidance) whereas for video releases they are merely advisory, except in the case of pornographic content. Children under the age of 3 were previously prohibited from public film performances, but a special category was introduced for this age group when the classification system was overhauled in 2014. A category for 14-year-olds was also introduced, and the lowest age rating was dropped from 4 years of age to 3. The categories are the following:\n\n\nRatings in Romania are set by the National Center of Cinematography () (CNC).\n\n\nSince 2012 the rating appears inside circles, which indicate age restrictions followed by a plus(+), and appears in most shows, including TV and Internet shows in Russian.\n\nThe indication shown:\n\nFilm classification in Singapore was introduced on 1 July 1991 and comes under the jurisdiction of the Board of Film Censors (BFC), currently part of the Info-communications Media Development Authority (IMDA). There were three ratings originally: G (General), PG (Parental Guidance) and R (Restricted to 18 years and above). Prior to then films were either approved or effectively banned. Since then, there have been several alterations to the ratings over the years. In September 1991, a Restricted (Artistic) (R(A)) rating was introduced to replaced the previous R-rating so as to allow the screening of certain art-house films which would otherwise have been banned without said rating, with an increased age restriction set at 21 years of age. The R(A) rating has since been replaced by NC16 (No Children under 16), M18 (Mature 18) and R21 (Restricted 21). A PG13 (Parental Guidance 13) rating, introduced in 2011, is the latest rating to be introduced. The G, PG and PG13 ratings are advisory while NC16, M18 and R21 carry age restrictions. Video ratings are mostly the same as the cinema ratings, except only go up to M18. Some titles, such as documentaries, children's programmes and sports programmes may be exempt from classification on video, but all titles must be classified for public theatrical exhibition. \n\nThe categories are:\n\nIn South Africa, films are classified by the Film and Publication Board. Distributors and exhibitors are legally compelled to comply with the age ratings. All broadcasters, cinemas and distributors of DVD/video and computer games must comply with the following:\n\n\nThere are also sub-descriptors used with some of the ratings:\n\n\nThe Korea Media Rating Board (영상물등급위원회) in Seoul divides licensed films into the following categories:\n\nAll films to be commercially released in Spain in any medium must be submitted to the ICAA (Instituto de Cinematografía y Artes Audiovisuales - Cinematography and Audiovisual Arts Institute). Classifications are advisory except for X-rated films, which are restricted to specially licensed venues. A supplementary classification, \"Especialmente Recomendada para la Infancia\" (Especially recommended for children), is sometimes appended to the lowest two classifications. Another supplementary classification, \"Especialmente recomendada para el fomento de la igualdad de género\" (Especially recommended for the promotion of gender equality), is sometimes appended to any of the classifications except the last one.\n\n\nThe Swedish Media Council (\"Statens medieråd\") is a government agency with the aims to reduce the risk of harmful media influences among minors and to empower minors as conscious media users. The classification bestowed on a film should not be viewed as recommendations on the suitability for children, as the law the council operates under (SFS 2010:1882) only mandates them to assess the relative risk to children's well-being. It is not a legal requirement to submit a film to the Media Council. The councils classification only applies to public exhibition, and the law does not require classification of home media.\n\nThe following categories are used:\n\n\nSwitzerland has adopted Germany's Freiwillige Selbstkontrolle der Filmwirtschaft (Voluntary Self-Regulation of the Film Industry, FSK). Under Swiss law, however, children up to two years younger than the age recommendations will be admitted if accompanied by a person invested with parental authority.\n\nFrom 1994 until 2015, the Government Information Office (GIO) classified films into four categories (General Audience/Protected/Parental Guidance/Restricted) pursuant to its issued ( in traditional Chinese): The \"Parental Guidance\" rating previously prohibited viewing by children under the age of 12 and required adolescents aged 12–17 to be accompanied by an adult. In 2015, the \"Parental Guidance\" rating was further divided into two categories: one that prohibits children under the age of 12 and one that prohibits adolescents under the age of 15.\n\nA motion picture rating system was proposed in the Film and Video Act of 2007, and was passed on December 20, 2007 by the Thai military-appointed National Legislative Assembly, replacing laws which had been in place since 1930. The draft law was met with resistance from the film industry and independent filmmakers. Activists had hoped for a less-restrictive approach; however, films are still subject to censorship, or can be banned from release altogether if the film is deemed to \"undermine or disrupt social order and moral decency, or might impact national security or the pride of the nation\".\n\nThe ratings were put into effect in August 2009. They are as follows:\n\n\nIn Turkey, movies to be shown in cinemas are rated by the Evaluation and Classification Board of the Ministry of Culture and Tourism. All films to be made commercially available must be classified, except in the case of educational films which are labeled as \"for educational purposes\" instead. The board also has the power to refuse classification in extreme cases (producers and distributors can submit an edited version of a movie to the board but edited versions may also be rejected if still deemed inappropriate); in this case, the movie will be banned with the exception of special artistic activities like fairs, festivals, feasts and carnivals.\n\n\nThe Ministry of Information of the United Arab Emirates classifies all films, which cinemas must abide by.\n\n\nThe British Board of Film Classification (BBFC) classifies films to be publicly exhibited in the United Kingdom, although statutory powers remain with local councils which can overrule any of the BBFC's decisions. Since 1984, the BBFC also classifies films made commercially available though a home video format. If the BBFC refuses a classification this effectively amounts to a ban (although local councils retain the legal right to overturn it in the case of cinema exhibition). The BBFC's regulatory powers do not extend to the Internet, so a film they have banned on physical media can still be made available via streaming media/video on demand. Videos designed to inform, educate or instruct or concerned with sport, religion or music are exempt from classification; exempt films may be marked as \"E\", but this is not an official label.\n\nThe current BBFC system is:\n\nIn the United States of America, film classification is a voluntary process with the ratings issued by the Motion Picture Association of America (MPAA) via the Classification and Rating Administration (CARA). The system was established in 1968, but the version listed below is the most recent revision, having been in effect since 1990. An unrated film is often informally denoted by \"NR\" in newspapers and so forth.\n\n\nAge ratings are divided into several categories. The age that corresponds to the category and the level of enforcement is defined by municipality ordinances.\n\nIn the San Cristóbal municipality the following ratings apply:\n\n\nIn the Baruta municipality the following ratings apply:\n\n\nIn the Maracaibo municipality children under the age of two are not admitted to performances and the ratings are enforced:\n\n\nAll theatrical releases are screened by the Cinema Department of the Ministry of Culture, Sport and Travel of Vietnam to ensure suitability for public viewing. Regardless of the rating, some scenes may be altered or removed to comply with regulations. The classification was revised in January 2017, replacing the previous rating system.\n\n\nUnlike the previous rating system, the current rating system does not have parental guidance and ratings other than P are considered to be restricted.\n\n\n"}
{"id": "908385", "url": "https://en.wikipedia.org/wiki?curid=908385", "title": "Nanoelectromechanical systems", "text": "Nanoelectromechanical systems\n\nNanoelectromechanical systems (NEMS) are a class of devices integrating electrical and mechanical functionality on the nanoscale. NEMS form the logical next miniaturization step from so-called microelectromechanical systems, or MEMS devices. NEMS typically integrate transistor-like nanoelectronics with mechanical actuators, pumps, or motors, and may thereby form physical, biological, and chemical sensors. The name derives from typical device dimensions in the nanometer range, leading to low mass, high mechanical resonance frequencies, potentially large quantum mechanical effects such as zero point motion, and a high surface-to-volume ratio useful for surface-based sensing mechanisms. Uses include accelerometers, or detectors of chemical substances in the air.\n\nAs noted by Richard Feynman in his famous talk in 1959, \"There's Plenty of Room at the Bottom,\" there are many potential applications of machines at smaller and smaller sizes; by building and controlling devices at smaller scales, all technology benefits. Among the expected benefits include greater efficiencies and reduced size, decreased power consumption and lower costs of production in electromechanical systems.\n\nIn 2000, the first very-large-scale integration (VLSI) NEMS device was demonstrated by researchers at IBM. Its premise was an array of AFM tips which can heat/sense a deformable substrate in order to function as a memory device. Further devices have been described by Stefan de Haan. In 2007, the International Technical Roadmap for Semiconductors (ITRS) contains NEMS Memory as a new entry for the Emerging Research Devices section.\n\nA key application of NEMS is atomic force microscope tips. The increased sensitivity achieved by NEMS leads to smaller and more efficient sensors to detect stresses, vibrations, forces at the atomic level, and chemical signals. AFM tips and other detection at the nanoscale rely heavily on NEMS.\n\nTwo complementary approaches to fabrication of NEMS can be found. The top-down approach uses the traditional microfabrication methods, i.e. optical, electron beam lithography and thermal treatments, to manufacture devices. While being limited by the resolution of these methods, it allows a large degree of control over the resulting structures. In this manner devices such as nanowires, nanorods, and patterned nanostructures are fabricated from metallic thin films or etched semiconductor layers.\n\nBottom-up approaches, in contrast, use the chemical properties of single molecules to cause single-molecule components to self-organize or self-assemble into some useful conformation, or rely on positional assembly. These approaches utilize the concepts of molecular self-assembly and/or molecular recognition. This allows fabrication of much smaller structures, albeit often at the cost of limited control of the fabrication process.\n\nA combination of these approaches may also be used, in which nanoscale molecules are integrated into a top-down framework. One such example is the carbon nanotube nanomotor.\n\nMany of the commonly used materials for NEMS technology have been carbon based, specifically diamond, carbon nanotubes and graphene. This is mainly because of the useful properties of carbon based materials which directly meet the needs of NEMS. The mechanical properties of carbon (such as large Young's modulus) are fundamental to the stability of NEMS while the metallic and semiconductor conductivities of carbon based materials allow them to function as transistors.\n\nBoth graphene and diamond exhibit high Young's modulus, low density, low friction, exceedingly low mechanical dissipation, and large surface area. The low friction of CNTs, allow practically frictionless bearings and has thus been a huge motivation towards practical applications of CNTs as constitutive elements in NEMS, such as nanomotors, switches, and high-frequency oscillators. Carbon nanotubes and graphene's physical strength allows carbon based materials to meet higher stress demands, when common materials would normally fail and thus further support their use as a major materials in NEMS technological development.\n\nAlong with the mechanical benefits of carbon based materials, the electrical properties of carbon nanotubes and graphene allow it to be used in many electrical components of NEMS. Nanotransistors have been developed for both carbon nanotubes as well as graphene. Transistors are one of the basic building blocks for all electronic devices, so by effectively developing usable transistors, carbon nanotubes and graphene are both very crucial to NEMS.\n\nCarbon nanotubes (CNTs) are allotropes of carbon with a cylindrical nanostructure. They can be considered a rolled up graphene. When rolled at specific and discrete (\"chiral\") angles, and the combination of the rolling angle and radius decides whether the nanotube has a bandgap (semiconducting) or no bandgap (metallic).\nMetallic carbon nanotubes have also been proposed for nanoelectronic interconnects since they can carry high current densities. This is a useful property as wires to transfer current are another basic building block of any electrical system. Carbon nanotubes have specifically found so much use in NEMS that methods have already been discovered to connect suspended carbon nanotubes to other nanostructures. This allows carbon nanotubes to form complicated nanoelectric systems. Because carbon based products can be properly controlled and act as interconnects as well as transistors, they serve as a fundamental material in the electrical components of NEMS.\n\nDespite all of the useful properties of carbon nanotubes and graphene for NEMS technology, both of these products face several hindrances to their implementation. One of the main problems is carbon’s response to real life environments. Carbon nanotubes exhibit a large change in electronic properties when exposed to oxygen. Similarly, other changes to the electronic and mechanical attributes of carbon based materials must fully be explored before their implementation, especially because of their high surface area which can easily react with surrounding environments. Carbon nanotubes were also found to have varying conductivities, being either metallic or semiconducting depending on their helicity when processed. Because of this, special treatment must be given to the nanotubes during processing to assure that all of the nanotubes have appropriate conductivities. Graphene also has complicated electric conductivity properties compared to traditional semiconductors because it lacks an energy band gap and essentially changes all the rules for how electrons move through a graphene based device. This means that traditional constructions of electronic devices will likely not work and completely new architectures must be designed for these new electronic devices.\n\nThe emerging field of bio-hybrid systems combines biological and synthetic structural elements for biomedical or robotic applications. The constituting elements of bio-nanoelectromechanical systems (BioNEMS) are of nanoscale size, for example DNA, proteins or nanostructured mechanical parts. Examples include the facile top-down nanostructuring of thiol-ene polymers to create cross-linked and mechanically robust nanostructures that are subsequently functionalized with proteins.\n\nComputer simulations have long been important counterparts to experimental studies of NEMS devices. Through continuum mechanics and molecular dynamics (MD), important behaviors of NEMS devices can be predicted via computational modeling before engaging in experiments. Additionally, combining continuum and MD techniques enables engineers to efficiently analyze the stability of NEMS devices without resorting to ultra-fine meshes and time-intensive simulations. Simulations have other advantages as well: they do not require the time and expertise associated with fabricating NEMS devices; they can effectively predict the interrelated roles of various electromechanical effects; and parametric studies can be conducted fairly readily as compared with experimental approaches. For example, computational studies have predicted the charge distributions and “pull-in” electromechanical responses of NEMS devices. Using simulations to predict mechanical and electrical behavior of these devices can help optimize NEMS device design parameters.\n\nKey hurdles currently preventing the commercial application of many NEMS devices include low-yields and high device quality variability. Before NEMS devices can actually be implemented, reasonable integrations of carbon based products must be created. A recent step in that direction has been demonstrated for diamond, achieving a processing level comparable to that of silicon. The focus is currently shifting from experimental work towards practical applications and device structures that will implement and profit from such novel devices. The next challenge to overcome involves understanding all of the properties of these carbon-based tools, and using the properties to make efficient and durable NEMS with low failure rates.\n\nCarbon-based materials have served as prime materials for NEMS use, because of their exceptional mechanical and electrical properties.\n\nThe global market of NEMS is projected to reach $108.88 million by 2022.\n\n"}
{"id": "52505750", "url": "https://en.wikipedia.org/wiki?curid=52505750", "title": "Negative testing", "text": "Negative testing\n\nNegative testing is a method of testing an application or system that ensures that the plot of the application is according to the requirements and can handle the unwanted input and user behavior. Invalid data is inserted to compare the output against the given input. Negative testing is also known as failure testing or error path testing. When performing negative testing exceptions are expected. This shows that the application is able to handle improper user behavior. Users input values that do not work in the system to test it's ability to handle incorrect values or system failure.\n\n\nNegative testing is done to check that product deals properly with the circumstance for which it is not programmed. The fundamental aim of this testing is check that how bad data is taken care of by the systems, and appropriate errors are shown to client when bad data is entered. Both positive and negative testing play important role. Positive testing ensures that application does what it is implied for and perform each function as expected. Negative testing is opposite of positive testing. Negative testing discovers diverse approaches to make the application crash and handle the crash effortlessly.\n\nExample\n\nThere are two basic techniques that help to write the sufficient test cases to cover the most of the functionalities of the system. Both these techniques are used in positive testing as well.\nThe two parameters are:\nBoundary indicates limit to something. In this parameter, test scenarios are designed in such a way that it covers the boundary values and validate how application behaves on these boundary values.\nExample\nIf there is an application that accepts Ids ranging from 0-255.Hence in this scenario, 0,255 will form the boundary values. The values within the range of 0-255 will constitute the positive testing. Any inputs going below from 0 and input going above from 255 will be considered invalid and will constitute negative testing.\n"}
{"id": "49392871", "url": "https://en.wikipedia.org/wiki?curid=49392871", "title": "Nima (device)", "text": "Nima (device)\n\nNima is a portable connected food sensor. The first product, a gluten sensor was released in Fall 2016. The second product, a peanut sensor is scheduled to be released in the second half of 2018.\n\nA food sample is placed in a one-time use capsule, which is inserted into the Nima sensor. Once the test begins, the food is mixed with proprietary antibodies and analyzed by the sensor for approximately two to three minutes. If the sample contains less than twenty parts per million of gluten (the same limit required by the U.S. Food and Drug Administration for gluten-free labeling), or less than ten parts per million of peanut, Nima displays a smiley face; otherwise a \"gluten found\" or \"peanut found\" icon is displayed. The device has not been approved by the FDA, although the developers contest that such approval is not necessary because they \"are not using it to diagnose or manage disease.\"\n\nNima has also developed a companion application for iOS and Android, connected through Bluetooth, that allows users to connect with other Nima owners and find allergen information for specific restaurants and packaged foods. The company plans to eventually expand its technology to detect other allergens, such as dairy and tree nuts. They are also exploring detection methods for additives, GMOs, preservatives and possible causes of food poisoning.\n\nNima was listed among \"Time\" magazine's best inventions of 2015.\n\nIt has also received positive reviews in \"Popular Science\" and \"The Medical Futurist\" \n"}
{"id": "50642320", "url": "https://en.wikipedia.org/wiki?curid=50642320", "title": "Nominal power", "text": "Nominal power\n\nNominal power is a power capacity in engineering.\n\nNominal power is a measurement of a mediumwave radio station's output used in the United States.\n\nNominal power is the nameplate capacity of photovoltaic (PV) devices, such as solar cells, panels and systems, and is determined by measuring the electric current and voltage in a circuit, while varying the resistance under precisely defined conditions.\n\n"}
{"id": "3566883", "url": "https://en.wikipedia.org/wiki?curid=3566883", "title": "Particle physics and representation theory", "text": "Particle physics and representation theory\n\nThere is a natural connection between particle physics and representation theory, as first noted in the 1930s by Eugene Wigner. It links the properties of elementary particles to the structure of Lie groups and Lie algebras. According to this connection, the different quantum states of an elementary particle give rise to an irreducible representation of the Poincaré group. Moreover, the properties of the various particles, including their spectra, can be related to representations of Lie algebras, corresponding to \"approximate symmetries\" of the universe.\n\nIn quantum mechanics, any particular one-particle state is represented as a vector in a Hilbert space formula_1. To help understand what types of particles can exist, it is important to classify the possibilities for formula_1 allowed by symmetries, and their properties. In a relativistic quantum system, for example, formula_3 might be the Poincaré group, while for the hydrogen atom, formula_3 might be the rotation group SO(3). The particle state is more precisely characterized by the associated projective Hilbert space formula_5, also called ray space, since two vectors that differ by a nonzero scalar factor correspond to the same physical quantum state represented by a \"ray\" in Hilbert space, which is an equivalence class in formula_1 and, under the natural projection map formula_7, an element of formula_5.\n\nLet formula_1 be a Hilbert space describing a particular quantum system and let formula_3 be a group of symmetries of the quantum system. By definition of a symmetry of a quantum system, there is a group action on formula_5. For each formula_12, there is a corresponding transformation formula_13 of formula_5. More specifically, if formula_15 is some symmetry of the system (say, rotation about the x-axis by 12°), then the corresponding transformation formula_13 of formula_5 is a map on ray space. For example, when rotating a \"stationary\" (zero momentum) spin-5 particle about its center, formula_15 is a rotation in 3D space (an element of formula_19), while formula_13 is an operator whose domain and range are each the space of possible quantum states of this particle, in this example the projective space formula_5 associated with an 11-dimensional complex Hilbert space formula_1.\n\nEach map formula_13 preserves, by definition of symmetry, the ray product on formula_5 induced by the inner product on formula_1; according to Wigner's theorem, this transformation of formula_5 comes from a unitary or anti-unitary transformation formula_27 of formula_1. Note, however, that the formula_27 associated to a given formula_13 is not unique, but only unique \"up to a phase factor\". The composition of the operators formula_27 should, therefore, reflect the composition law in formula_3, but only up to a phase factor:\n\nwhere formula_34 will depend on formula_15 and formula_36. Thus, the map sending formula_15 to formula_27 is a \"projective unitary representation\" of formula_3, or possibly a mixture of unitary and anti-unitary, if formula_3 is disconnected. In practice, anti-unitary operators are always associated with time-reversal symmetry.\n\nIt is important physically that in general formula_41 does not have to be an ordinary representation of formula_3; it may not be possible to choose the phase factors in the definition of formula_27 to eliminate the phase factors in their composition law. An electron, for example, is a spin-one-half particle; its Hilbert space consists of wave functions on formula_44 with values in a two-dimensional spinor space. The action of formula_19 on the spinor space is only projective: It does not come from an ordinary representation of formula_19. There is, however, an associated ordinary representation of the universal cover formula_47 of formula_19 on spinor space. \n\nFor many interesting classes of groups formula_3, tells us that every projective unitary representation of formula_3 comes from an ordinary representation of the universal cover formula_51 of formula_3. Actually, if formula_1 is finite dimensional, then regardless of the group formula_3, every projective unitary representation of formula_3 comes from an ordinary unitary representation of formula_51. If formula_1 is infinite dimensional, then to obtain the desired conclusion, some algebraic assumptions must be made on formula_3 (see below). In this setting the result is a theorem of Bargmann. Fortunately, the crucial case of the Poincaré group, Bargmann's theorem applies. (See Wigner's classification of the representations of the universal cover of the Poincaré group.)\n\nThe requirement referred to above is that the Lie algebra formula_59 does not admit a nontrivial one-dimensional central extension. This is the case if and only if the second cohomology group of formula_59 is trivial. In this case, it may still be true that the group admits a central extension by a \"discrete\" group. But extensions of formula_3 by a discrete group are nothing but a covers of formula_3. For instance, the universal cover formula_63 is related to formula_3 through the quotient formula_65 with the central subgroup formula_66 being the center of formula_63 itself, isomorphic to the fundamental group of the covered group.\n\nThus, in favorable cases, the quantum system will carry a unitary representation of the universal cover formula_51 of the symmetry group formula_3. This is desirable because formula_1 is much easier to work with than the non-vector space formula_5. If the representations of formula_51 can be classified, much more information about the possibilities and properties of formula_1 are available.\n\nAn example in which Bargmann's theorem does not apply comes from a quantum particle moving in formula_74. The group of translational symmetries of the associated phase space, formula_75, is the commutative group formula_75. In the usual quantum mechanical picture, the formula_75 symmetry is not implement by a unitary representation of formula_75. After all, in the quantum setting, translations in position space and translations in momentum space do not commute. This failure to commute reflects the failure of the position and momentum operators—which are the infinitesimal generators of translations in momentum space and position space, respectively—to commute. Nevertheless, translations in position space and translations in momentum space \"do\" commute up to a phase factor. Thus, we have a well-defined projective representation of formula_75, but it does not come from an ordinary representation of formula_75, even though formula_75 is simply connected. \n\nIn this case, to obtain an ordinary representation, one has to pass to the Heisenberg group, which is a nontrivial one-dimensional central extension of formula_75.\n\nThe group of translations and Lorentz transformations form the Poincaré group, and this group should be a symmetry of a relativistic quantum system (neglecting general relativity effects, or in other words, in flat space). Representations of the Poincaré group are in many cases characterized by a nonnegative mass and a half-integer spin (see Wigner's classification); this can be thought of as the reason that particles have quantized spin. (Note that there are in fact other possible representations, such as tachyons, infraparticles, etc., which in some cases do not have quantized spin or fixed mass.)\n\nWhile the spacetime symmetries in the Poincaré group are particularly easy to visualize and believe, there are also other types of symmetries, called internal symmetries. One example is color SU(3), an exact symmetry corresponding to the continuous interchange of the three quark colors.\n\nMany (but not all) symmetries or approximate symmetries form Lie groups. Rather than study the representation theory of these Lie groups, it is often preferable to study the closely related representation theory of the corresponding Lie algebras, which are usually simpler to compute.\n\nNow, representations of the Lie algebra correspond to representations of the universal cover of the original group. In the finite-dimensional case—and the infinite-dimensional case, provided that applies—irreducible projective representations of the original group correspond to ordinary unitary representations of the universal cover. In those cases, computing at the Lie algebra level is appropriate. This is the case, notably, for studying the irreducible projective representations of the rotation group SO(3). These are in one-to-one correspondence with the ordinary representations of the universal cover SU(2) of SO(3). The representations of the SU(2) are then in one-to-one correspondence with the representations of its Lie algebra su(2), which is isomorphic to the Lie algebra so(3) of SO(3). \n\nThus, to summarize, the irreducible projective representations of SO(3) are in one-to-one correspondence with the irreducible ordinary representations of its Lie algebra so(3). The two-dimensional \"spin 1/2\" representation of the Lie algebra so(3), for example, does not correspond to an ordinary (single-valued) representation of the group SO(3). (This fact is the origin of statements to the effect that \"if you rotate the wave function of an electron by 360 degrees, you get the negative of the original wave function.\") Nevertheless, the spin 1/2 representation does give rise to a well-defined \"projective\" representation of SO(3), which is all that is required physically.\n\nAlthough the above symmetries are believed to be exact, other symmetries are only approximate.\n\nAs an example of what an approximate symmetry means, suppose an experimentalist lived inside an infinite ferromagnet, with magnetization in some particular direction. The experimentalist in this situation would find not one but two distinct types of electrons: one with spin along the direction of the magnetization, with a slightly lower energy (and consequently, a lower mass), and one with spin anti-aligned, with a higher mass. Our usual SO(3) rotational symmetry, which ordinarily connects the spin-up electron with the spin-down electron, has in this hypothetical case become only an \"approximate\" symmetry, relating \"different types of particles\" to each other.\n\nIn general, an approximate symmetry arises when there are very strong interactions that obey that symmetry, along with weaker interactions that do not. In the electron example above, the two \"types\" of electrons behave identically under the strong and weak forces, but differently under the electromagnetic force.\n\nAn example from the real world is isospin symmetry, an SU(2) group corresponding to the similarity between up quarks and down quarks. This is an approximate symmetry: While up and down quarks are identical in how they interact under the strong force, they have different masses and different electroweak interactions. Mathematically, there is an abstract two-dimensional vector space\nand the laws of physics are \"approximately\" invariant under applying a determinant-1 unitary transformation to this space:\nFor example, formula_85 would turn all up quarks in the universe into down quarks and vice versa. Some examples help clarify the possible effects of these transformations:\n\nIn general, particles form isospin multiplets, which correspond to irreducible representations of the Lie algebra SU(2). Particles in an isospin multiplet have very similar but not identical masses, because the up and down quarks are very similar but not identical.\n\nIsospin symmetry can be generalized to flavour symmetry, an SU(3) group corresponding to the similarity between up quarks, down quarks, and strange quarks. This is, again, an approximate symmetry, violated by quark mass differences and electroweak interactions—in fact, it is a poorer approximation than isospin, because of the strange quark's noticeably higher mass.\n\nNevertheless, particles can indeed be neatly divided into groups that form irreducible representations of the Lie algebra SU(3), as first noted by Murray Gell-Mann and independently by Yuval Ne'eman.\n\n\n"}
{"id": "24503102", "url": "https://en.wikipedia.org/wiki?curid=24503102", "title": "Propagation of light in non-inertial reference frames", "text": "Propagation of light in non-inertial reference frames\n\nThe description of motion in relativity requires more than one concept of speed. Coordinate speed is the coordinate distance measured by the observer divided by the coordinate time of the observer. Proper speed is the local proper distance divided by the local proper time. For example, at the event horizon of a black hole the coordinate speed of light is zero, while the proper speed is c. The coordinate speed of light (both instantaneous and average) is slowed in the presence of gravitational fields. The local instantaneous proper speed of light is always c.\n<br>\nIn an inertial frame an observer cannot detect their motion via light signals as the speed of light in a vacuum is constant. This means an observer can detect when their motion is accelerated by studying light signals.\n\n\n\n<br>\n"}
{"id": "28034", "url": "https://en.wikipedia.org/wiki?curid=28034", "title": "Scanning electron microscope", "text": "Scanning electron microscope\n\nA scanning electron microscope (SEM) is a type of electron microscope that produces images of a sample by scanning the surface with a focused beam of electrons. The electrons interact with atoms in the sample, producing various signals that contain information about the surface topography and composition of the sample. The electron beam is scanned in a raster scan pattern, and the position of the beam is combined with the detected signal to produce an image. SEM can achieve resolution better than 1 nanometer. Specimens are observed in high vacuum in conventional SEM, or in low vacuum or wet conditions in variable pressure or environmental SEM, and at a wide range of cryogenic or elevated temperatures with specialized instruments.\n\nThe most common SEM mode is the detection of secondary electrons emitted by atoms excited by the electron beam. The number of secondary electrons that can be detected depends, among other things, on specimen topography. By scanning the sample and collecting the secondary electrons that are emitted using a special detector, an image displaying the topography of the surface is created.\n\nAn account of the early history of SEM has been presented by McMullan. Although Max Knoll produced a photo with a 50 mm object-field-width showing channeling contrast by the use of an electron beam scanner, it was Manfred von Ardenne who in 1937 invented a true microscope with high magnification by scanning a very small raster with a demagnified and finely focused electron beam. Ardenne applied the scanning principle not only to achieve magnification but also to purposefully eliminate the chromatic aberration otherwise inherent in the electron microscope. He further discussed the various detection modes, possibilities and theory of SEM, together with the construction of the . Further work was reported by Zworykin's group, followed by the Cambridge groups in the 1950s and early 1960s headed by Charles Oatley, all of which finally led to the marketing of the first commercial instrument by Cambridge Scientific Instrument Company as the \"Stereoscan\" in 1965, which was delivered to DuPont.\n\nThe signals used by a scanning electron microscope to produce an image result from interactions of the electron beam with atoms at various depths within the sample. Various types of signals are produced including secondary electrons (SE), reflected or back-scattered electrons (BSE), characteristic X-rays and light (cathodoluminescence) (CL), absorbed current (specimen current) and transmitted electrons. Secondary electron detectors are standard equipment in all SEMs, but it is rare for a single machine to have detectors for all other possible signals.\n\nIn secondary electron imaging (SEI), the secondary electrons are emitted from very close to the specimen surface. Consequently, SEI can produce very high-resolution images of a sample surface, revealing details less than 1 nm in size. Back-scattered electrons (BSE) are beam electrons that are reflected from the sample by elastic scattering. They emerge from deeper locations within the specimen and, consequently, the resolution of BSE images is less than SE images. However, BSE are often used in analytical SEM, along with the spectra made from the characteristic X-rays, because the intensity of the BSE signal is strongly related to the atomic number (Z) of the specimen. BSE images can provide information about the distribution, but not the identity, of different elements in the sample. In samples predominantly composed of light elements, such as biological specimens, BSE imaging can image colloidal gold immuno-labels of 5 or 10 nm diameter, which would otherwise be difficult or impossible to detect in secondary electron images. Characteristic X-rays are emitted when the electron beam removes an inner shell electron from the sample, causing a higher-energy electron to fill the shell and release energy. The energy or wavelength of these characteristic X-rays can be measured by Energy-dispersive X-ray spectroscopy or Wavelength-dispersive X-ray spectroscopy and used to identify and measure the abundance of elements in the sample and map their distribution.\n\nDue to the very narrow electron beam, SEM micrographs have a large depth of field yielding a characteristic three-dimensional appearance useful for understanding the surface structure of a sample. This is exemplified by the micrograph of pollen shown above. A wide range of magnifications is possible, from about 10 times (about equivalent to that of a powerful hand-lens) to more than 500,000 times, about 250 times the magnification limit of the best light microscopes.\n\nSEM samples are prepared to withstand the vacuum conditions and the high energy beam of electrons, and have to be small enough to fit on the specimen stage. Samples are generally mounted rigidly to a specimen holder or stub using a conductive adhesive. SEM is used extensively for defect analysis of semiconductor wafers, and manufacturers make instruments that can examine any part of a 300 mm semiconductor wafer. Many instruments have chambers that can tilt an object of that size to 45° and provide continuous 360° rotation.\n\nNonconductive specimens collect charge when scanned by the electron beam, and especially in secondary electron imaging mode, this causes scanning faults and other image artifacts. For conventional imaging in the SEM, specimens must be electrically conductive, at least at the surface, and electrically grounded to prevent the accumulation of electrostatic charge. Metal objects require little special preparation for SEM except for cleaning and conductively mounting to a specimen stub. Non-conducting materials are usually coated with an ultrathin coating of electrically conducting material, deposited on the sample either by low-vacuum sputter coating or by high-vacuum evaporation. Conductive materials in current use for specimen coating include gold, gold/palladium alloy, platinum, iridium, tungsten, chromium, osmium, and graphite. Coating with heavy metals may increase signal/noise ratio for samples of low atomic number (Z). The improvement arises because secondary electron emission for high-Z materials is enhanced.\n\nAn alternative to coating for some biological samples is to increase the bulk conductivity of the material by impregnation with osmium using variants of the OTO staining method (O-osmium tetroxide, T-thiocarbohydrazide, O-osmium).\n\nNonconducting specimens may be imaged without coating using an environmental SEM (ESEM) or low-voltage mode of SEM operation. In ESEM instruments the specimen is placed in a relatively high-pressure chamber and the electron optical column is differentially pumped to keep vacuum adequately low at the electron gun. The high-pressure region around the sample in the ESEM neutralizes charge and provides an amplification of the secondary electron signal. Low-voltage SEM is typically conducted in an instrument with a field emission guns (FEG) which is capable of producing high primary electron brightness and small spot size even at low accelerating potentials. To prevent charging of non-conductive specimens, operating conditions must be adjusted such that the incoming beam current is equal to sum of outcoming secondary and backscattered electrons currents a condition that is more often met at accelerating voltages of 0.3–4 kV.\n\nSynthetic replicas can be made to avoid the use of original samples when they are not suitable or available for SEM examination due to methodological obstacles or legal issues. This technique is achieved in two steps: (1) a mold of the original surface is made using a silicone-based dental elastomer, and (2) a replica of the original surface is obtained by pouring a synthetic resin into the mold.\n\nEmbedding in a resin with further polishing to a mirror-like finish can be used for both biological and materials specimens when imaging in backscattered electrons or when doing quantitative X-ray microanalysis.\n\nThe main preparation techniques are not required in the environmental SEM outlined below, but some biological specimens can benefit from fixation.\n\nFor SEM, a specimen is normally required to be completely dry, since the specimen chamber is at high vacuum. Hard, dry materials such as wood, bone, feathers, dried insects, or shells (including egg shells) can be examined with little further treatment, but living cells and tissues and whole, soft-bodied organisms require chemical fixation to preserve and stabilize their structure.\n\nFixation is usually performed by incubation in a solution of a buffered chemical fixative, such as glutaraldehyde, sometimes in combination with formaldehyde and other fixatives, and optionally followed by postfixation with osmium tetroxide. The fixed tissue is then dehydrated. Because air-drying causes collapse and shrinkage, this is commonly achieved by replacement of water in the cells with organic solvents such as ethanol or acetone, and replacement of these solvents in turn with a transitional fluid such as liquid carbon dioxide by critical point drying. The carbon dioxide is finally removed while in a supercritical state, so that no gas–liquid interface is present within the sample during drying.\n\nThe dry specimen is usually mounted on a specimen stub using an adhesive such as epoxy resin or electrically conductive double-sided adhesive tape, and sputter-coated with gold or gold/palladium alloy before examination in the microscope. Samples may be sectioned (with a microtome) if information about the organism's internal ultrastructure is to be exposed for imaging.\n\nIf the SEM is equipped with a cold stage for cryo microscopy, cryofixation may be used and low-temperature scanning electron microscopy performed on the cryogenically fixed specimens. Cryo-fixed specimens may be cryo-fractured under vacuum in a special apparatus to reveal internal structure, sputter-coated and transferred onto the SEM cryo-stage while still frozen. Low-temperature scanning electron microscopy (LT-SEM) is also applicable to the imaging of temperature-sensitive materials such as ice and fats.\n\nFreeze-fracturing, freeze-etch or freeze-and-break is a preparation method particularly useful for examining lipid membranes and their incorporated proteins in \"face on\" view. The preparation method reveals the proteins embedded in the lipid bilayer.\n\nBack-scattered electron imaging, quantitative X-ray analysis, and X-ray mapping of specimens often requires grinding and polishing the surfaces to an ultra smooth surface. Specimens that undergo WDS or EDS analysis are often carbon-coated. In general, metals are not coated prior to imaging in the SEM because they are conductive and provide their own pathway to ground.\n\nFractography is the study of fractured surfaces that can be done on a light microscope or, commonly, on an SEM. The fractured surface is cut to a suitable size, cleaned of any organic residues, and mounted on a specimen holder for viewing in the SEM.\n\nIntegrated circuits may be cut with a focused ion beam (FIB) or other ion beam milling instrument for viewing in the SEM. The SEM in the first case may be incorporated into the FIB.\n\nMetals, geological specimens, and integrated circuits all may also be chemically polished for viewing in the SEM.\n\nSpecial high-resolution coating techniques are required for high-magnification imaging of inorganic thin films.\n\nIn a typical SEM, an electron beam is thermionically emitted from an electron gun fitted with a tungsten filament cathode. Tungsten is normally used in thermionic electron guns because it has the highest melting point and lowest vapor pressure of all metals, thereby allowing it to be electrically heated for electron emission, and because of its low cost. Other types of electron emitters include lanthanum hexaboride () cathodes, which can be used in a standard tungsten filament SEM if the vacuum system is upgraded or field emission guns (FEG), which may be of the cold-cathode type using tungsten single crystal emitters or the thermally assisted Schottky type, that use emitters of zirconium oxide.\n\nThe electron beam, which typically has an energy ranging from 0.2 keV to 40 keV, is focused by one or two condenser lenses to a spot about 0.4 nm to 5 nm in diameter. The beam passes through pairs of scanning coils or pairs of deflector plates in the electron column, typically in the final lens, which deflect the beam in the \"x\" and \"y\" axes so that it scans in a raster fashion over a rectangular area of the sample surface.\n\nWhen the primary electron beam interacts with the sample, the electrons lose energy by repeated random scattering and absorption within a teardrop-shaped volume of the specimen known as the interaction volume, which extends from less than 100 nm to approximately 5 µm into the surface. The size of the interaction volume depends on the electron's landing energy, the atomic number of the specimen and the specimen's density. The energy exchange between the electron beam and the sample results in the reflection of high-energy electrons by elastic scattering, emission of secondary electrons by inelastic scattering and the emission of electromagnetic radiation, each of which can be detected by specialized detectors. The beam current absorbed by the specimen can also be detected and used to create images of the distribution of specimen current. Electronic amplifiers of various types are used to amplify the signals, which are displayed as variations in brightness on a computer monitor (or, for vintage models, on a cathode ray tube). Each pixel of computer video memory is synchronized with the position of the beam on the specimen in the microscope, and the resulting image is, therefore, a distribution map of the intensity of the signal being emitted from the scanned area of the specimen. Older microscopes captured images on film, but most modern instrument collect digital images.\n\nMagnification in an SEM can be controlled over a range of about 6 orders of magnitude from about 10 to 500,000 times. Unlike optical and transmission electron microscopes, image magnification in an SEM is not a function of the power of the objective lens. SEMs may have condenser and objective lenses, but their function is to focus the beam to a spot, and not to image the specimen. Provided the electron gun can generate a beam with sufficiently small diameter, an SEM could in principle work entirely without condenser or objective lenses, although it might not be very versatile or achieve very high resolution. In an SEM, as in scanning probe microscopy, magnification results from the ratio of the dimensions of the raster on the specimen and the raster on the display device. Assuming that the display screen has a fixed size, higher magnification results from reducing the size of the raster on the specimen, and vice versa. Magnification is therefore controlled by the current supplied to the x, y scanning coils, or the voltage supplied to the x, y deflector plates, and not by objective lens power.\n\nThe most common imaging mode collects low-energy (<50 eV) secondary electrons that are ejected from the k-shell of the specimen atoms by inelastic scattering interactions with beam electrons. Due to their low energy, these electrons originate within a few nanometers from the sample surface. The electrons are detected by an Everhart-Thornley detector, which is a type of scintillator-photomultiplier system. The secondary electrons are first collected by attracting them towards an electrically biased grid at about +400 V, and then further accelerated towards a phosphor or scintillator positively biased to about +2,000 V. The accelerated secondary electrons are now sufficiently energetic to cause the scintillator to emit flashes of light (cathodoluminescence), which are conducted to a photomultiplier outside the SEM column via a light pipe and a window in the wall of the specimen chamber. The amplified electrical signal output by the photomultiplier is displayed as a two-dimensional intensity distribution that can be viewed and photographed on an analogue video display, or subjected to analog-to-digital conversion and displayed and saved as a digital image. This process relies on a raster-scanned primary beam. The brightness of the signal depends on the number of secondary electrons reaching the detector. If the beam enters the sample perpendicular to the surface, then the activated region is uniform about the axis of the beam and a certain number of electrons \"escape\" from within the sample. As the angle of incidence increases, the interaction volume increases and the \"escape\" distance of one side of the beam decreases, resulting in more secondary electrons being emitted from the sample. Thus steep surfaces and edges tend to be brighter than flat surfaces, which results in images with a well-defined, three-dimensional appearance. Using the signal of secondary electrons image resolution less than 0.5 nm is possible.\n\nBackscattered electrons (BSE) consist of high-energy electrons originating in the electron beam, that are reflected or back-scattered out of the specimen interaction volume by elastic scattering interactions with specimen atoms. Since heavy elements (high atomic number) backscatter electrons more strongly than light elements (low atomic number), and thus appear brighter in the image, BSEs are used to detect contrast between areas with different chemical compositions. The Everhart-Thornley detector, which is normally positioned to one side of the specimen, is inefficient for the detection of backscattered electrons because few such electrons are emitted in the solid angle subtended by the detector, and because the positively biased detection grid has little ability to attract the higher energy BSE. Dedicated backscattered electron detectors are positioned above the sample in a \"doughnut\" type arrangement, concentric with the electron beam, maximizing the solid angle of collection. BSE detectors are usually either of scintillator or of semiconductor types. When all parts of the detector are used to collect electrons symmetrically about the beam, atomic number contrast is produced. However, strong topographic contrast is produced by collecting back-scattered electrons from one side above the specimen using an asymmetrical, directional BSE detector; the resulting contrast appears as illumination of the topography from that side. Semiconductor detectors can be made in radial segments that can be switched in or out to control the type of contrast produced and its directionality.\n\nBackscattered electrons can also be used to form an electron backscatter diffraction (EBSD) image that can be used to determine the crystallographic structure of the specimen.\n\nThe nature of the SEM's probe, energetic electrons, makes it uniquely suited to examining the optical and electronic properties of semiconductor materials. The high-energy electrons from the SEM beam will inject charge carriers into the semiconductor. Thus, beam electrons lose energy by promoting electrons from the valence band into the conduction band, leaving behind holes.\n\nIn a direct bandgap material, recombination of these electron-hole pairs will result in cathodoluminescence; if the sample contains an internal electric field, such as is present at a p-n junction, the SEM beam injection of carriers will cause electron beam induced current (EBIC) to flow. Cathodoluminescence and EBIC are referred to as \"beam-injection\" techniques, and are very powerful probes of the optoelectronic behavior of semiconductors, in particular for studying nanoscale features and defects.\n\nCathodoluminescence, the emission of light when atoms excited by high-energy electrons return to their ground state, is analogous to UV-induced fluorescence, and some materials such as zinc sulfide and some fluorescent dyes, exhibit both phenomena. Over the last decades, cathodoluminescence was most commonly experienced as the light emission from the inner surface of the cathode ray tube in television sets and computer CRT monitors. In the SEM, CL detectors either collect all light emitted by the specimen or can analyse the wavelengths emitted by the specimen and display an emission spectrum or an image of the distribution of cathodoluminescence emitted by the specimen in real color.\n\nCharacteristic X-rays that are produced by the interaction of electrons with the sample may also be detected in an SEM equipped for energy-dispersive X-ray spectroscopy or wavelength dispersive X-ray spectroscopy. Analysis of the x-ray signals may be used to map the distribution and estimate the abundance of elements in the sample.\n\nSEM is not a camera and the detector is not continuously image-forming like a CCD array or film. Unlike in an optical system, the resolution is not limited by the diffraction limit, fineness of lenses or mirrors or detector array resolution. The focusing optics can be large and coarse, and the SE detector is fist-sized and simply detects current. Instead, the spatial resolution of the SEM depends on the size of the electron spot, which in turn depends on both the wavelength of the electrons and the electron-optical system that produces the scanning beam. The resolution is also limited by the size of the interaction volume, the volume of specimen material that interacts with the electron beam. The spot size and the interaction volume are both large compared to the distances between atoms, so the resolution of the SEM is not high enough to image individual atoms, as is possible with transmission electron microscope (TEM). The SEM has compensating advantages, though, including the ability to image a comparatively large area of the specimen; the ability to image bulk materials (not just thin films or foils); and the variety of analytical modes available for measuring the composition and properties of the specimen. Depending on the instrument, the resolution can fall somewhere between less than 1 nm and 20 nm. As of 2009, The world's highest resolution conventional (≤30 kV) SEM can reach a point resolution of 0.4 nm using a secondary electron detector.\n\nConventional SEM requires samples to be imaged under vacuum, because a gas atmosphere rapidly spreads and attenuates electron beams. As a consequence, samples that produce a significant amount of vapour, e.g. wet biological samples or oil-bearing rock, must be either dried or cryogenically frozen. Processes involving phase transitions, such as the drying of adhesives or melting of alloys, liquid transport, chemical reactions, and solid-air-gas systems, in general cannot be observed. Some observations of living insects have been possible however.\n\nThe first commercial development of the ESEM in the late 1980s allowed samples to be observed in low-pressure gaseous environments (e.g. 1–50 Torr or 0.1–6.7 kPa) and high relative humidity (up to 100%). This was made possible by the development of a secondary-electron detector capable of operating in the presence of water vapour and by the use of pressure-limiting apertures with differential pumping in the path of the electron beam to separate the vacuum region (around the gun and lenses) from the sample chamber.\n\nThe first commercial ESEMs were produced by the ElectroScan Corporation in USA in 1988. ElectroScan was taken over by Philips (who later sold their electron-optics division to FEI Company) in 1996.\n\nESEM is especially useful for non-metallic and biological materials because coating with carbon or gold is unnecessary. Uncoated Plastics and Elastomers can be routinely examined, as can uncoated biological samples. Coating can be difficult to reverse, may conceal small features on the surface of the sample and may reduce the value of the results obtained. X-ray analysis is difficult with a coating of a heavy metal, so carbon coatings are routinely used in conventional SEMs, but ESEM makes it possible to perform X-ray microanalysis on uncoated non-conductive specimens; however some specific for ESEM artifacts are introduced in X-ray analysis. ESEM may be the preferred for electron microscopy of unique samples from criminal or civil actions, where forensic analysis may need to be repeated by several different experts.\n\nIt is possible to study specimens in liquid with ESEM or with other liquid-phase electron microscopy methods.\n\nThe SEM can also be used in transmission mode by simply incorporating an appropriate detector below a thin specimen section. Both bright and dark field imaging has been reported in the generally low accelerating beam voltage range used in SEM, which increases the contrast of unstained biological specimens at high magnifications with a field emission electron gun. This mode of operation has been abbreviated by the acronym tSEM.\n\nElectron microscopes do not naturally produce color images, as an SEM produces a single value per pixel; this value corresponds to the number of electrons received by the detector during a small period of time of the scanning when the beam is targeted to the (x, y) pixel position.\n\nThis single number is usually represented, for each pixel, by a grey level, forming a \"black-and-white\" image. However, several ways have been used to get color electron microscopy images.\n\nThe easiest way to get color is to associate to this single number an arbitrary color, using a color look-up table (i.e. each grey level is replaced by a chosen color). This method is known as false color. On a BSE image, false color may be performed to better distinguish the various phases of the sample.\n\nAs an alternative to simply replacing each grey level by a color, a sample observed by an oblique beam allows researchers to create an approximative topography image (see further section \"Photometric 3D rendering from a single SEM image\"). Such topography can then be processed by 3D-rendering algorithms for a more natural rendering of the surface texture\nVery often, published SEM images are artificially colored. This may be done for aesthetic effect, to clarify structure or to add a realistic appearance to the sample and generally does not add information about the specimen.\n\nColoring may be performed manually with photo-editing software, or semi-automatically with dedicated software using feature-detection or object-oriented segmentation.\n\nIn some configurations more information is gathered per pixel, often by the use of multiple detectors.\n\nAs a common example, secondary electron and backscattered electron detectors are superimposed and a color is assigned to each of the images captured by each detector, with an end result of a combined color image where colors are related to the density of the components. This method is known as density-dependent color SEM (DDC-SEM). Micrographs produced by DDC-SEM retain topographical information, which is better captured by the secondary electrons detector and combine it to the information about density, obtained by the backscattered electron detector.\n\nMeasurement of the energy of photons emitted from the specimen is a common method to get analytical capabilities. Examples are the energy-dispersive X-ray spectroscopy (EDS) detectors used in elemental analysis and cathodoluminescence microscope (CL) systems that analyse the intensity and spectrum of electron-induced luminescence in (for example) geological specimens. In SEM systems using these detectors it is common to color code these extra signals and superimpose them in a single color image, so that differences in the distribution of the various components of the specimen can be seen clearly and compared. Optionally, the standard secondary electron image can be merged with the one or more compositional channels, so that the specimen's structure and composition can be compared. Such images can be made while maintaining the full integrity of the original signal data, which is not modified in any way.\n\nSEMs do not naturally provide 3D images contrary to SPMs. However 3D data can be obtained using an SEM with different methods as follows.\n\n\nThis method typically uses a four-quadrant BSE detector (alternatively for one manufacturer, a 3-segment detector). The microscope produces four images of the same specimen at the same time, so no tilt of the sample is required. The method gives metrological 3D dimensions as far as the slope of the specimen remains reasonable. Most SEM manufacturers now (2018) offer such built-in or optional four-quadrant BSE detector, together with proprietary software allowing to calculate a 3D image in real time.\nOther approaches use more sophisticated (and sometimes GPU-intensive) methods like the optimal estimation algorithm and offer much better results at the cost of high demands on computing power.\n\nIn all instances, this approach works by integration of the slope, so vertical slopes and overhangs are ignored; for instance, if an entire sphere lies on a flat, little more than the upper hemisphere is seen emerging above the flat, resulting in wrong altitude of the sphere apex. The prominence of this effect depends on the angle of the BSE detectors with respect to the sample, but these detectors are usually situated around (and close to) the electron beam, so this effect is very common.\n\nThis method requires an SEM image obtained in oblique low angle lighting. The grey-level is then interpreted as the slope, and the slope integrated to restore the specimen topography. This method is interesting for visual enhancement and the detection of the shape and position of objects ; however the vertical heights cannot usually be calibrated, contrary to other methods such as photogrammetry.\n\n\nOne possible application is measuring the roughness of ice crystals. This method can combine variable-pressure environmental SEM and the 3D capabilities of the SEM to measure roughness on individual ice crystal facets, convert it into a computer model and run further statistical analysis on the model. Other measurements include fractal dimension, examining fracture surface of metals, characterization of materials, corrosion measurement, and dimensional measurements at the nano scale (step height, volume, angle, flatness, bearing ratio, coplanarity, etc.).\n\nThe following are examples of images taken using an SEM.\n\n\n\n"}
{"id": "40626870", "url": "https://en.wikipedia.org/wiki?curid=40626870", "title": "Social determinism", "text": "Social determinism\n\nSocial determinism is the theory that social interactions and constructs alone determine individual behavior (as opposed to biological or objective factors).\n\nConsider certain human behaviors, such as committing murder, or writing poetry. A social determinist would look only at social phenomena, such as customs and expectations, education, and interpersonal interactions, to decide whether or not a given person would exhibit any of these behaviors. They would discount biological and other non-social factors, such as genetic makeup, the physical environment, etc. Ideas about nature and biology would be considered to be socially constructed.\n\nThe socially determined actions of an individual can be influenced by forces that control the flow of ideas. By creating an ideology within the society of the individual, the individual's actions and reactions to stimuli are predetermined to adhere to the social rules imposed on him/her. Ideologies can be created using social institutions such as schooling, which \"have become the terrain upon which contending forces express their social and political interest\" (Mayberry 3), or the mass media, which has \"significant power in shaping the social agenda and framing of public opinion to support that agenda\" (Colaguori 35).\n\nBy creating a social construction of reality, these forces directly create a hegemonic control over the future of individuals who feel like the social construct is natural and therefore unchangeable. Their actions become based in the context of their society so that, even if they possess an innate talent for a sport, if the social construction implies that their race is unathletic in general, or their nation or state does not produce athletes, they do not include the possibility of athleticism in their future. Their society has successfully determined their actions.\n\nSocial determinism can favor a political party's agenda by setting social rules so that the individual considers the party's agenda to be morally correct, an example being the 2010 G20 summit riots in Toronto. The media, controlled by corporations and the governments with agendas of their own, publicizes the riots as violent and dangerous, but the goal of the rioters, to rebel against those whose position in power enables them to abuse the system for personal gain, is lost because the focus is on the violence. The individuals' view on the subject are then directly influenced by the media and their reactions are predetermined by that social form of control. \"We have been taught to think that censorship is the main mechanism of how the media uses information as a form of social control, but in fact what \"is\" said, and how it is \"selectively presented\", is a far more powerful form of information control.\" (Colaguori 35).\n\nSocial determinism was first studied by Emile Durkheim (1858 - 1917), French philosopher considered as the father of the social science.\n\nSocial determinism is most commonly understood in opposition to biological determinism. Within the media studies discipline, however, social determinism is understood as the counterpart of technological determinism.\nTechnological determinism is the notion that technological change and development is inevitable, and that the characteristics of any given technology determine the way it is used by the society in which it is developed. The concept of technological determinism is dependent upon the premise that social changes come about as a result of the new capabilities that new technologies enable.\n\nThe notion of social determinism opposes this perspective. Social determinism perceives technology as a result of the society in which it is developed. A number of contemporary media theorists have provided persuasive accounts of social determinism, including Leila Green.\n\nIn her book \"Technoculture\" Leila Green examines in detail the workings of a social determinist perspective, and argues “social processes determine technology for social purposes”. She claims that every technological development throughout history was born of a social need, be this need economical, political or military. \n\nAccording to Green, technology is always developed with a particular purpose or objective in mind. As the development of technology is necessarily facilitated by financial funding, a social determinist perspective recognizes that technology is always developed to benefit those who are capable of funding its development.\n\nThus social determinists perceive that technological development is not only determined by the society in which it occurs, but that it is inevitably shaped by the power structures that exist in that society. \n\nScientific studies have shown that social behavior is partly inherited and can influence infants and also even influence foetuses. Wired to be social means that infants are not taught that they are social beings, but they are born as prepared social beings. The infants are born with an inherited social skill.\n\nSocial pre-wiring deals with the study of fetal social behavior and social interactions in a multi-fetal environment. Specifically, social pre-wiring refers to the ontogeny of social interaction. Also informally referred to as, \"wired to be social.\" The theory questions whether there is a propensity to socially oriented action already present \"before\" birth. Research in the theory concludes that newborns are born into the world with a unique genetic wiring to be social.\n\nCircumstantial evidence supporting the social pre-wiring hypothesis can be revealed when examining newborns' behavior. Newborns, not even hours after birth, have been found to display a preparedness for social interaction. This preparedness is expressed in ways such as their imitation of facial gestures. This observed behavior cannot be contributed to any current form of socialization or social construction. Rather, newborns most likely inherit to some extent social behavior and identity through genetics.\n\nPrincipal evidence of this theory is uncovered by examining Twin pregnancies. The main argument is, if there are social behaviors that are inherited and developed before birth, then one should expect twin foetuses to engage in some form of social interaction before they are born. Thus, ten foetuses were analyzed over a period of time using ultrasound techniques. Using kinematic analysis, the results of the experiment were that the twin foetuses would interact with each other for longer periods and more often as the pregnancies went on. Researchers were able to conclude that the performance of movements between the co-twins were not accidental but specifically aimed.\n\nThe social pre-wiring hypothesis was proved correct, \"The central advance of this study is the demonstration that 'social actions' are already performed in the second trimester of gestation. Starting from the 14th week of gestation twin foetuses plan and execute movements specifically aimed at the co-twin. These findings force us to predate the emergence of social behavior: when the context enables it, as in the case of twin foetuses, other-directed actions are not only possible but predominant over self-directed actions.\".\n\n\nColaguori, Claudio. Power and Society: Critical issues in Social Science. Toronto: York University, 2011. Print.\n\nMayberry, Maralee. Conflict and Social Determinism: The Reprivatization of Education. Chicago: Viewpoints, 1991. Print.\n\nStafford, Rebecca, Elaine Backman, and Pamela Dibona. \"The Division of Labour among Cohabiting and Married Couples.\" Journal of Marriage and Family 39.1 (1977): 43-57. Print.\n"}
{"id": "10669005", "url": "https://en.wikipedia.org/wiki?curid=10669005", "title": "Solar Anomalous and Magnetospheric Particle Explorer", "text": "Solar Anomalous and Magnetospheric Particle Explorer\n\nThe Solar Anomalous and Magnetospheric Particle Explorer (SAMPEX) was a NASA solar and magnetospheric observatory, and was the first spacecraft in the Small Explorer program. It was launched into low Earth orbit on July 3, 1992, from Vandenberg Air Force Base aboard a Scout G-1 rocket. SAMPEX was an international collaboration between NASA of the United States and the Max Planck Institute for Extraterrestrial Physics of Germany.\n\nThe spacecraft carried four instruments designed to measure the anomalous components of cosmic rays, emissions from solar energetic particles, and electron counts in Earth's magnetosphere. Built for a three-year mission, its science mission was ended on June 30, 2004. Mission control for SAMPEX was handled by the Goddard Space Flight Center until October 1997, after which it was turned over to the Bowie State University Satellite Operations Control Center (BSOCC). BSOCC, with funding assistance from The Aerospace Corporation, continued to operate the spacecraft after its science mission ended, using the spacecraft as an educational tool for its students while continuing to release science data to the public.\n\nBuilt for a three-year primary mission, the spacecraft continued to return science data until its reentry on November 13, 2012.\n\n\nSAMPEX collaborators included:\n\n"}
{"id": "12567092", "url": "https://en.wikipedia.org/wiki?curid=12567092", "title": "Stephen Robert Nockolds", "text": "Stephen Robert Nockolds\n\nStephen Robert Nockolds, FRS (10 May 1909 – 7 February 1990) was a geochemist, petrologist and winner of the Murchison Medal of the Geological Society of London.\n\nRobert Nockolds was born at St Columb Major, Cornwall, the son of Dr Stephen Nockolds, a surgeon of Brighton, and his wife Hilda Tomlinson. He was educated at Ascham St Vincent's School, Eastbourne and at Felsted School where his interest in rocks already manifested itself. He then went to Manchester University and then took a PhD at Trinity College, Cambridge. He became a Fellow of Trinity College, and lectured in petrology at Cambridge. In 1957 he became Reader in Geochemistry at Cambridge, and Emeritus Reader. In 1959 he became a Fellow of the Royal Society, and was also an Honorary Fellow of the Geological Society of India. In 1972 he retired from Cambridge and was awarded the Murchison Medal of the Geological Society. He published a leading work on petrology and a large number of papers in various journals as shown below.\n\nIn 1932 he married Hilda Jackson (1909–1976) and subsequently after she died in 1976 he married Patricia Horsley (1923 - 10 July 2013) He was known to his friends as Nocky or Bob and whilst he didn't have children of his own he was affectionately embraced by Patricia's large family. In his retirement he devoted himself to his immaculate garden.\n\n"}
{"id": "171526", "url": "https://en.wikipedia.org/wiki?curid=171526", "title": "Subaru Telescope", "text": "Subaru Telescope\n\nThe Subaru Telescope is a Ritchey-Chretien reflecting telescope. Instruments can be mounted at a Cassegrain focus below the primary mirror; at either of two Nasmyth focal points in enclosures on the sides of the telescope mount, to which light can be directed with a tertiary mirror; or at the prime focus in lieu of a secondary mirror, an arrangement rare on large telescopes, to provide a wide field of view suited to deep wide-field surveys.\n\nIn 1984, the University of Tokyo formed an engineering working group to develop and study the concept of a telescope. In 1985, the astronomy committee of Japan's science council gave top priority to the development of a \"Japan National Large Telescope\" (JNLT), and in 1986, the University of Tokyo signed an agreement with the University of Hawaii to build the telescope in Hawaii. In 1988, the National Astronomical Observatory of Japan was formed through a reorganization of the University's Tokyo Astronomical Observatory, to oversee the JNLT and other large national astronomy projects.\n\nConstruction of the Subaru telescope began in April 1991, and later that year, a public contest gave the telescope its official name, \"Subaru Telescope.\" Construction was completed in 1998, and the first scientific images were taken in January 1999. In September 1999, Princess Sayako of Japan dedicated the telescope.\n\nA number of state-of-the-art technologies were worked into the telescope design. For example, 261 computer-controlled actuators press the main mirror from underneath, which corrects for primary mirror distortion caused by changes in the telescope orientation. The telescope enclosure building is also shaped to improve the quality of astronomical images by minimizing the effects caused by atmospheric turbulence.\n\nSubaru is one of the few state-of-the-art telescopes to have ever been used with the naked eye. For the dedication, an eyepiece was constructed so that Princess Sayako could look through it directly. It was enjoyed by the staff for a few nights until it was replaced with the much more sensitive working instruments.\n\nSubaru is the primary tool in the search for Planet Nine. Its large field of view, 75 times that of the Keck telescopes, and strong light-gathering power are suited for deep wide-field sky surveys. The search, split between a research group led by Batygin and Brown and another led by Sheppard and Trujillo, is expected to take up to five years.\n\nTwo separate incidents claimed the lives of four workers during the construction of the telescope. On October 13, 1993, 42-year-old Paul F. Lawrence was fatally injured when a forklift tipped over onto him. On January 16, 1996, sparks from a welder ignited insulation which smoldered, generating noxious smoke that killed Marvin Arruda, 52, Ricky Del Rosario, 38, and Warren K. \"Kip\" Kaleo, 36, and sent twenty-six other workers to the hospital in Hilo. All four workers are memorialized by a plaque outside the base of the telescope dome and a sign posted temporarily each January along the Mauna Kea access road.\n\nOn July 2, 2011, the telescope operator in Hilo noted an anomaly from the top unit of the telescope. Upon further examination, coolant from the top unit was found to have leaked over the primary mirror and other parts of the telescope.\nObservation using Nasmyth foci resumed on July 22, and Cassegrain focus resumed on August 26.\n\nSeveral cameras and spectrographs can be mounted at Subaru Telescope's four focal points for observations in visible and infrared wavelengths.\n\n\n\n"}
{"id": "9683659", "url": "https://en.wikipedia.org/wiki?curid=9683659", "title": "Tanager Expedition", "text": "Tanager Expedition\n\nThe Tanager Expedition was a series of five biological surveys of the Northwestern Hawaiian Islands conducted in partnership between the Bureau of Biological Survey and the Bishop Museum, with the assistance of the U.S. Navy. Four expeditions occurred from April to August 1923, and a fifth in July 1924. Led by Lieutenant Commander Samuel Wilder King on the minesweeper , and Alexander Wetmore directing the team of scientists, the expedition studied the plant animal life, and geology of the central Pacific islands. Noted members of the team include archaeologist Kenneth Emory and herpetologist Chapman Grant.\n\nThe expedition began with the goal of exterminating domestic rabbits that had been introduced to Laysan island by the guano industry in 1902. Since that time, the rabbits had devoured Laysan's vegetation and led to the extinction of several endemic species. The rabbits were eventually eliminated on Laysan, and the crew witnessed the extinction of the Laysan honeycreeper (apapane). Throughout the expedition, new species were discovered and named, and unique specimens were captured and returned to laboratories for further study. Over 100 archaeological sites were found, including ancient religious sites and prehistoric settlements on Nihoa and Necker Island.\n\nThe first expedition departed Honolulu on April 4, 1923 and returned on May 4. The team visited the island of Laysan, Pearl and Hermes Atoll, Midway Atoll, and Kure Atoll. When they spent a month on Laysan studying the endemic Laysan honeycreeper, a violent and sudden storm ravaged the island. After the storm, the crew concluded that the last three specimens of the honeycreeper had been killed.\n\n\nThe second expedition departed Honolulu on May 10. The team visited the island of Laysan, the French Frigate Shoals and the Pearl and Hermes Atoll.\n\n\nThe third expedition departed Honolulu on June 9. The team visited the islands of Necker, Nihoa, and the French Frigate Shoals. An attempt was also made to visit Kaula. \"Tanager\" arrived at Nihoa on June 10 and dropped off scientists for a ten-day visit and moved on to Necker the following day to drop off a second team. Both teams used radio to keep in constant communication between the two islands. On Nihoa, botanist Edward Leonard Caum collected the first specimen of \"Amaranthus brownii\" and Alexander Wetmore discovered the Nihoa millerbird and named it \"Acrocephalus familiaris kingi\", in honor of Captain Samuel Wilder King. Evidence of an ancient settlement on Nihoa was discovered, along with platforms, terraces, and human remains.\n\nOn June 22, the \"Tanager\" arrived in the French Frigate Shoals and remained for six days, completing the first comprehensive survey of the atoll. The expedition returned to Honolulu on July 1.\n\nThe fourth expedition consisted of two teams, with the first departing Honolulu on July 7. Destinations included Johnston Atoll and Wake Island. The first team left on the \"Whippoorwill\" (AM-35), which made the first survey of Johnston Island in the 20th century. Aerial survey and mapping flights over Johnston were conducted with a Douglas DT-2 floatplane carried on her fantail, which was hoisted into the water for take off. Two destroyer convoys accompanied the expedition from Honolulu. The \"Tanager\" (AM-5) left Honolulu on July 16 and joined up with the \"Whippoorwill\" to complete the survey. From July 27 to August 5, the expedition surveyed Wake Island and named its islets: The southwest islet was named after Charles Wilkes who had led the United States Exploring Expedition in 1841 and determined the location of Wake Island. The northwest islet was named after Titian Peale, the chief naturalist for the 1841 expedition.\n\n\n\nThe fifth expedition visited Nihoa and Necker Island in 1924. Archaeologist Kenneth P. Emory of the Bishop Museum cleared out 60 sites on Nihoa and collected and cataloged artifacts. The expedition visited Necker from July 14–17.\n\n\"This list is incomplete\"\n\n\nIn 1990, the U.S. congress passed the Native American Graves Protection and Repatriation Act which requires federal agencies and institutions that receive federal funding to return Native American cultural items and human remains to their people. In the 1990s, Hui Mālama (Hui Mālama I Na Kūpuna O Hawaii Nei), a Native Hawaiian group, spent two years petitioning the United States Fish and Wildlife Service for the release of the bones (\"iwi\") from seven Hawaiian skeletons originally taken from Nihoa and Necker Island by the Tanager Expedition in 1924. Although the bones were owned by the USFWS, the Bishop Museum acted as custodian. The bones were finally released to the group, and in November, 1997, Hui Mālama chartered a yacht and travelled to Nihoa and Necker to rebury the remains.\n\n\n\n"}
{"id": "4840666", "url": "https://en.wikipedia.org/wiki?curid=4840666", "title": "The Mechanical Universe", "text": "The Mechanical Universe\n\nThe Mechanical Universe... \"And Beyond\" is a 52-part telecourse, filmed at the California Institute of Technology, that introduces university level physics, covering topics from Copernicus to quantum mechanics. The series was produced by Caltech and INTELECOM, a nonprofit consortium of California community colleges now known as Intelecom Learning, with financial support from Annenberg/CPB.\n\nProduced starting in 1982, the videos make heavy use of historical dramatizations and visual aids to explain physics concepts. The latter were state of the art at the time, incorporating almost 8 hours of computer animation created by computer graphics pioneer Jim Blinn. Each episode opens and closes with bookend segments in which Caltech professor David Goodstein, speaking in a lecture hall, delivers explanations \"that can't quite be put into the mouth of our affable, faceless narrator\". After more than a quarter century, the series is still often used as a supplemental teaching aid, for its clear explanation of fundamental concepts such as special relativity.\n\nThe bookend segments featuring Goodstein were specially staged versions of actual freshman physics lectures from Caltech's courses Physics 1a and 1b. The organization and the choice of topics to emphasize in the television show reflect a then-recent revision of Caltech's introductory physics curriculum, the first total overhaul since the one represented by \"The Feynman Lectures on Physics\" almost two decades earlier. While Feynman generally sought contemporary examples of topics, the later revision of the curriculum brought a more historical focus:In essence, the earlier Feynman course had sought to make physics exciting by relating each subject, wherever possible, to contemporary scientific problems. The new course took the opposite tack, of trying to recreate the historical excitement of the original discovery. For example, classical mechanics—a notoriously difficult and uninspiring subject for students—is treated as the discovery of \"our place in the universe\". Accordingly, celestial mechanics is the backbone of the subject and its climax is Newton's solution of the Kepler problem.\n\nThe room seen in the bookend segments is the Bridge lecture hall at Caltech. Many of the extras were students from other schools, and the front rows of the lecture hall were deliberately filled with more women than would have typically been seen at Caltech lectures. The TV production team added fake wood paneling to the lecture hall so that it would more closely resemble that seen in the show \"The Paper Chase\". Later, the Caltech physics department was sufficiently impressed by the result that panels were installed permanently. Many seats in the lecture hall had to be removed in order to make room for the camera track and studio lights. To cover this, additional reaction shots of a full lecture hall were filmed later, so that the illusion of a complete audience could be created in editing. For most of the footage of Goodstein himself, only two rows of students were present.\n\nMany other video segments were shot on location, for example at a Linde industrial plant that produced liquid air. Historical scenes were often made to be generic, in order to facilitate their reuse across multiple episodes: \"Young Newton strolls through an apple orchard, old Newton testily refuses a cup of tea from a servant, and so on\". Footage featuring historical reenactment of Johannes Kepler was purchased from the Carl Sagan television series \"\".\n\nThe series was originally planned to consist of 26 episodes. This was later expanded to 60 episodes, a number then cut back to the eventual total of 52 for budget and production-schedule reasons.\n\nThe show was intended not to require previous experience with calculus. Instead, the basics of differential and integral calculus would both be taught early in the series itself. Caltech mathematician Tom M. Apostol joined the \"Mechanical Universe\" production staff in order to ensure that the series did not compromise on the quality of the mathematics it presented. Seeing an example of Blinn's computer animation for the first time convinced Apostol that the series could bring mathematics \"to life in a way that cannot be done in a textbook or at the chalkboard\". When test screenings to humanities students revealed that their greatest difficulty learning calculus was a weak background in trigonometry, Apostol wrote a primer on the subject to be distributed with the telecourse. After advising the production of \"The Mechanical Universe\", Apostol decided that a similar series, geared to high-school mathematics, would be beneficial. This became the later Caltech series \"Project Mathematics!\", which also featured computer animation by Blinn. Some of Blinn's animations for \"The Mechanical Universe\" were reused in the new series, in order to illustrate applications of algebra, geometry, and trigonometry.\n\nThe science-fiction action movie \"Total Recall\" (1990) used portions of the \"Mechanical Universe\" title sequence, in a scene where the protagonist (Douglas Quaid, played by Arnold Schwarzenegger) is offered virtual vacations in locales around the Solar System. The animation was used without licensing, and consequently, Caltech and Intelecom sued Carolco Pictures for $3 million.\n\nIn order to present detailed mathematical equation derivations, the show employed a technique its creators called the \"algebraic ballet\". Computer animation presented derivations in step-by-step detail, but rapidly and with touches of whimsy, such as algebraic terms being canceled by a Monty Python-esque stomping foot or Michelangelo's Hand of God. Blinn felt that \"Cosmos\" had taken itself \"too seriously\", and so he aimed to include more humor in the \"Mechanical Universe\" animations. The goal was to avoid putting the viewers' \"brains into a 60-cycle hum\", without sacrificing rigor; the creators intended that students could learn the overall gist of each derivation from the animation, and then study the details using the accompanying textbook. Computer animation was also used to portray idealizations of physical systems, like simulated billiard balls illustrating Newton's laws of motion. (Blinn had used some of the same software earlier to visualize the interaction of DNA and DNA polymerase for \"Cosmos\".) One commenter deemed these animations \"particularly useful in providing students with subjective insights into dynamic three-dimensional phenomena such as magnetic fields\".\n\nCreating the computer graphics necessary to visualize physics concepts led Blinn to invent new techniques for simulating clouds, as well as the virtual \"blobby objects\" known as metaballs. Blinn used the vertex coordinates of regular icosahedra and dodecahedra to determine the placement of electric field lines radiating away from point charges.\n\nMost of the narration was voiced by actor Aaron Fletcher, who also played Galileo Galilei in the historical segments. Some portions, such as explanations of particular technical details, were narrated by Sally Beaty, the show's executive producer.\n\nShorter versions of \"Mechanical Universe\" episodes, 10 to 20 minutes in length, were created for use in high schools. This adaptation, for which a dozen high-school teachers and administrators were consultants, was supported by a $650,000 grant from the National Science Foundation. These videos were distributed alongside supplemental written material for teachers' benefit, and were intended to be employed in conjunction with existing textbooks. Yorkshire Television later produced a version repackaged for the United Kingdom audience, which was released in April 1991.\n\nPBS and The Learning Channel began broadcasting \"The Mechanical Universe\" in September 1985. During the fall of 1986, roughly 100 PBS stations carried \"The Mechanical Universe,\" and by the fall of 1987, over 600 higher-education institutions had purchased it or licensed the episodes for use. In 1992, Goodstein noted that the series had been broadcast, via PBS, by over 100 stations, \"usually at peculiar hours when innocent people were unlikely to tune in accidentally on a differential equation in the act of being solved\". He observed that detailed viewership figures were difficult to obtain, but when the show had been broadcast in Miami during Saturday mornings, the producers were able to obtain Nielsen ratings.In fact, it came in second in its time slot, beating the kiddie cartoons on two network stations. There were 18,000 faithful core households in Dade County alone, the median age of the viewers was 18, and half were female. However, we seldom get that kind of detailed information.Goodstein and assistant project director Richard Olenick noted,Anecdotal information in the form of letters and phone calls indicates very considerable enthusiasm among users at all levels from casual viewers to high-school students to research university professors, but there have also been a number of sharp disappointments, particularly when Instructional Television administrators have tried to handle TMU like a conventional telecourse.Similarly, a 1988 review in \"Physics Today\" suggested that the programs would not function well on their own as a telecourse, but would work much better as a supplement to a traditional classroom or a more standard distance-learning course such as Open University. The reviewers also found the \"algebraic ballet\" of computer-animated equations too fast to follow: \"After a short time, one yearns for a live professor filling the blackboard with equations\". Similarly, a review in the \"American Journal of Physics\", while praising the \"technical proficiency of the films\", wrote of the animated equation manipulations, \"As the MIT students say, this is like trying to take a drink of water out of a fire hose\". A considerably more enthusiastic evaluation came from physicist Charles H. Holbrow, who told Olenick, \"These materials will constitute the principal visual image of physics for decades\". Goodstein and Olenick reported that younger viewers tended to enjoy the \"algebraic ballet\" style \"much more than older viewers, who are made uncomfortable by the algebraic manipulations they cannot quite follow\".\n\nIn 1986, \"The Mechanical Universe\" was used as part of a summer program for gifted children, to overall success.\n\nA 1987 study at Indiana University Bloomington used 14 \"Mechanical Universe\" episodes as part of an introductory course on Newtonian mechanics, with generally positive results:[T]hese tapes were particularly effective in placing Newtonian mechanics in a historical perspective; dramatizing the historical overthrow of Aristotelian and medieval ideas; illustrating the diverse nature of scientists and the scientific endeavor; stimulating student interest and enthusiasm; and, through excellent animation, illustrating the time dimension of certain mechanics concepts. The companion text [...] was placed on library reserve for the course but was not extensively utilized by students.A follow-up study found that the videos could also be helpful explaining physics to professors in other fields. Negative reactions generally had less to do with the intrinsic perceived quality of the episodes than with the time the science-history material took away from content seen as \"critical exam-preparing instruction\". The investigator recalled,[S]ome students, thinking that the videotape material would not be covered on the tests, headed for the doors when the lights dimmed! To counter this tendency I started to use a few test questions based on historical or literary details discussed in the videotapes. Some students were outraged: \"What is this, a poetry class?\"\n\nClassroom use continued into the 1990s. A minority education program at the University of California, Berkeley employed \"Mechanical Universe\" episode segments (on LaserDisc) as part of group discussions. In a 1993 review of the series, a science historian stated that he had used episodes in his classes for several years, naming \"Kepler's Three Laws\" and \"The Michelson–Morley Experiment\" as his personal favorites.The highlight of the Kepler film is a segment in which we are shown an exquisite graphical realization of the way in which Kepler actually figured out that the orbits of the planets are elliptical rather than circular. The sheer difficulty of the problem he faced and the elegance of the method he applied to solve it are abundantly clear. I cannot imagine a better way to present this magnificent discovery, which can easily appear so trivial.A 2005 column in \"The Physics Teacher\" suggested \"The Mechanical Universe\" as preparatory viewing for instructors attempting to teach physics for the first time. \"The Physics Teacher\" has also recommended the series \"as enrichment or a makeup assignment for high-ability students\". Writing for \"Wired\" magazine's web site, Rhett Allain cited the series as an example of videos that could replace some functions of traditional lectures.\n\nIn 1987, \"The Lorentz Transformation\" (episode 42) was awarded the sixteenth annual Japan Prize for educational television. Other awards received by \"The Mechanical Universe\" include the 1986 Gold Award from the Birmingham International Film Festival, two \"Cindy\" awards from the International Association of Audio Visual Communicators (1987 and 1988), a Gold Award (1985) and a Silver Award (1987) from the International Film and TV Festival of New York, Silver (1986) and Gold Apple (1987) awards from the National Educational Film and Video Festival, and a Gold Plaque (1985) from the Chicago International Film Festival.\n\nGoodstein received the 1999 Oersted Medal for his work in physics education, including \"The Mechanical Universe\". For his contributions to the field of computer graphics, including his animations for \"Cosmos\", \"The Mechanical Universe\" and \"Project Mathematics!\", Blinn received a MacArthur fellowship in 1991, as well as the 1999 Steven A. Coons Award.\n\nLike many introductory physics texts, \"The Mechanical Universe\" cites the spectacular 1940 collapse of the Tacoma Narrows Bridge as an example of resonance, using footage of the disaster in the \"Resonance\" episode. However, as more-recent expositions have emphasized, the catastrophic oscillations that destroyed the bridge were not due to simple mechanical resonance, but to a more complicated interaction between the bridge and the winds passing through it—a phenomenon known as aeroelastic flutter. This phenomenon is a kind of \"self-sustaining vibration\" that lies beyond the regime of applicability of the linear theory of the externally-driven simple harmonic oscillator.\n\nThe opening sequence used for the first 26 episodes lists the show's title as \"The Mechanical Universe\", whereas the latter 26 episodes are titled \"The Mechanical Universe ...and Beyond\". The reason for the addition is explained by Goodstein in the closing lecture segment of the final episode: In the first scientific revolution, disputation over the interpretation of human or divine authority was replaced by observation, by measurement, by the testing of hypotheses, all of it with the powerful help of quantitative mathematical reasoning. And the result of all that was the mechanical universe, a universe that inexorably worked out its destiny according to precise, predictable, mechanical laws. Today, we no longer believe in that universe. If I know the precise position of some particle at some instant of time, I cannot have \"any idea\" of where it's going or how fast. And it doesn't make any difference at all if you say, \"All right, you don't \"know\" where it's going, but where is it \"really\" going?\" That is precisely the kind of question that is scientifically meaningless. That is the nature of the world we live in. That is the quantum mechanical universe.The series can be purchased from Caltech or streamed from online video sources, including Caltech's official YouTube channel. Caltech also posted on YouTube a series of short videos made by Blinn to demonstrate the show's computer animation at SIGGRAPH conferences.\n\nAnnenberg/CPB provided the funding for the production of \"The Mechanical Universe\". The show was one of the first twelve projects funded by the initial $90 million pledge the Annenberg Foundation gave to the Corporation for Public Broadcasting in the early 1980s. The total cost of the project was roughly $10 million.\n\nFunding to broadcast the show came from the following.\n\n\n"}
{"id": "41258992", "url": "https://en.wikipedia.org/wiki?curid=41258992", "title": "Urban planning in the United States", "text": "Urban planning in the United States\n\nUrban planning in the United States is practice of urban planning as it relates specifically to localities and urban centers in the United States.\n\nIn 1682, William Penn founded Philadelphia, Pennsylvania, planning it as a city to serve as a port on the Delaware River and as a place for government. Hoping that Philadelphia would become more like an English rural town instead of a city, Penn laid out roads on a grid plan to keep houses and businesses spread far apart, with areas for gardens and orchards. The city's inhabitants did not follow Penn's plans, as they crowded by the Delaware River, the port, and subdivided and resold their lots. Before Penn left Philadelphia for the last time, he issued the Charter of 1701 establishing it as a city. It became an important trading center, poor at first, but with tolerable living conditions by the 1750s. Benjamin Franklin, a leading citizen, helped improve city services and founded new ones, such as fire protection, a library, and one of the American colonies' first hospitals.\n\nPresident George Washington appointed Pierre L'Enfant in 1791 to design the new capital city (later named the City of Washington) under the supervision of three Commissioners, whom Washington had appointed to oversee the planning and development of the federal territory that would later become the District of Columbia. Thomas Jefferson, who worked alongside President Washington in overseeing the plans for the capital, sent L'Enfant a letter outlining his task, which was to provide a drawing of suitable sites for the federal city and the public buildings. Though Jefferson had modest ideas for the Capital, L'Enfant saw the task as far more grandiose, believing he was not only locating the capital, but also devising the city plan and designing the buildings. Dismissed by the Washington D.C. commission, L'Enfant's plan would not be revived and implemented until 1902, as part of the McMillan Commission's efforts to rebuild the city.\n\nIn 1811, land for New York's Central Park was acquired by the city through eminent domain and incorporated into the urban plan. Residents of Seneca Village, a settlement of free African Americans and German and Irish immigrants located on the future site of Central Park, were evicted by 1857. Frederick Law Olmsted, with the help of Calvert Vaux, developed a design for Central Park, which was entered into the open competition held by the city, and subsequently won. Their plan was then successfully implemented by the city, and was opened to the public in the winter of 1858.\n\nIn 1909, the first National Conference on City Planning was held in Washington D.C. Dominating the conference, Benjamin C. Marshall urged the government to conduct a 'civic census' to study the state of American cities, educate the public on its findings, and establish a national city-planning committee.\n\nDaniel Burnham was commissioned by the Chicago Merchants Association to lead a team to plan Chicago's World's Columbian Exposition of 1893.\n\nIn 1933, the National Planning Board was established.\n\nIn 1965, President Lyndon B. Johnson established the Department of Housing and Urban Development.\n\n\n"}
{"id": "39452826", "url": "https://en.wikipedia.org/wiki?curid=39452826", "title": "Vanishing dimensions theory", "text": "Vanishing dimensions theory\n\nVanishing-dimensions theory is a particle physics theory suggesting that systems with higher energy have a smaller number of dimensions.\n\nFor example, the theory implies that the Universe had fewer dimensions after the Big Bang when its energy was high. Then the number of dimensions may have increased as the system cooled and the Universe may gain more dimensions with time. There could have originally been only one spatial dimension, with two dimensions total — one time dimension and one space dimension. When there were only two dimensions, the Universe lacked gravitational degrees of freedom.\n\nThe theory is also tied to smaller amount of dimensions in smaller systems with the universe expansion being a suggested motivating phenomenon for growth of the number of dimensions with time, suggesting a larger number of dimensions in systems on larger scale.\n\nIn 2011, Dejan Stojkovic from the University at Buffalo and Jonas Mureika from the Loyola Marymount University described use of a Laser Interferometer Space Antenna system, intended to detect gravitational waves, to test the vanishing-dimension theory by detecting a maximum frequency after which gravitational waves can't be observed.\n\nThe vanishing-dimensions theory is seen as an explanation to cosmological constant problem: a fifth dimension would answer the question of energy density required to maintain the constant.\n"}
{"id": "394121", "url": "https://en.wikipedia.org/wiki?curid=394121", "title": "Voskhod (rocket)", "text": "Voskhod (rocket)\n\nThe Voskhod rocket (, \"ascent\", \"dawn\") was a derivative of the Soviet R-7 ICBM designed for the human spaceflight programme but later used for launching Zenit reconnaissance satellites. It consisted of the Molniya 8K78M third stage minus the Blok L. In 1966, all R-7 variants were equipped with the uprated core stage and strap-ons of the Soyuz 11A511. The Blok I stage in the Voskhod booster used the RD-107 engine rather than the RD-110 in the Soyuz, which was more powerful and also man-rated. The sole exception to this were the two manned Voskhod launches, which had RD-108 engines, a man-rated RD-107 but with the same performance.\n\nAll 11A57s launched after 1965 were functionally an 11A511 without the Soyuz's payload shroud and launch escape system (with the exception of the third stage propulsion system as noted above). Around 300 were flown from Baikonur and Plesetsk through 1976 (various payloads, but Zenith PHOTINT satellites were the most common). The newer 11A511U core had been introduced in 1973, but the existing stock of 11A57s took another three years to use up.\n\n\n"}
{"id": "35854962", "url": "https://en.wikipedia.org/wiki?curid=35854962", "title": "Zoo Parade", "text": "Zoo Parade\n\nZoo Parade is an American television program broadcast from 1950 to 1957 that featured animals from the Lincoln Park Zoo in Chicago. The program's host was Marlin Perkins, the Zoo's director. Perkins went on to host the program \"Wild Kingdom\". Jim Wehmeyer has described the show: \"A precursor of sorts to the regularly featured animal segments on \"The Tonight Show\" and other late-night talk shows, \"Zoo Parade\" was a location-bound production (filmed in the reptile house basement) during which Perkins would present and describe the life and peculiarities of Lincoln Park Zoo animals.\"\n\nMarcel LaFollette has written, \"Production approaches that are now standard practice on \"NOVA\" and the Discovery Channel derive, in fact, from experimentation by television pioneers like Lynn Poole and Don Herbert and such programs as \"Adventure\", \"Zoo Parade\", \"Science in Action\", and the Bell Telephone System’s science specials. These early efforts were also influenced by television’s love of the dramatic, refined during its first decade and continuing to shape news and public affairs programming, as well as fiction and fantasy, today.\"\n\nThe show won a Peabody Award in 1951, and was nominated for Emmy Awards four times.\n\n"}
