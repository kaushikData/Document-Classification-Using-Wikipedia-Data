{"id": "16969298", "url": "https://en.wikipedia.org/wiki?curid=16969298", "title": "Acme zone", "text": "Acme zone\n\nIn biostratigraphy, an acme zone, abundance zone, or peak zone is the area of a teilzone where a particular fossil taxon reaches a higher level of abundance.\n\n\n<br>\n"}
{"id": "861040", "url": "https://en.wikipedia.org/wiki?curid=861040", "title": "Alexander Butlerov", "text": "Alexander Butlerov\n\nAlexander Mikhaylovich Butlerov (Алекса́ндр Миха́йлович Бу́тлеров; 15 September 1828 – 17 August 1886) was a Russian chemist, one of the principal creators of the theory of chemical structure (1857–1861), the first to incorporate double bonds into structural formulas, the discoverer of hexamine (1859), the discoverer of formaldehyde (1859) and the discoverer of the formose reaction (1861).\n\nHe first proposed the idea of possible tetrahedral arrangement of valence bonds in carbon compounds in 1862.\n\nThe crater Butlerov on the Moon is named after him.\n\nAlexander Butlerov was born in Chistopol into a landowning family.\n"}
{"id": "12092010", "url": "https://en.wikipedia.org/wiki?curid=12092010", "title": "Beachrock", "text": "Beachrock\n\nBeachrock is a friable to well-cemented sedimentary rock that consists of a variable mixture of gravel-, sand-, and silt-sized sediment that is cemented with carbonate minerals and has formed along a shoreline. Depending on location, the sediment that is cemented to form beachrock can consist of a variable mixture of shells, coral fragments, rock fragments of different types, and other materials. It can contain scattered artifacts, pieces of wood, and coconuts. Beachrock typically forms within the intertidal zone within tropical or semitropical regions. However, Quaternary beachrock is also found as far north and south as 60° latitude.\n\nBeachrock units form under a thin cover of sediment and generally overlie unconsolidated sand. They typically consist of multiple units, representing multiple episodes of cementation and exposure. The mineralogy of beachrocks is mainly high-magnesium calcite or aragonite. The main processes involved in the cementation are : supersaturation with CaCO through direct evaporation of seawater, groundwater CO degassing in the vadose zone, mixing of marine and meteoric water fluxes and precipitation of micritic calcium carbonate as a byproduct of microbiological activity.\n\nOn retreating coasts, outcrops of beachrock may be evident offshore where they may act as a barrier against coastal erosion. Beachrock presence can also induce sediment deficiency in a beach and out-synch its wave regime. Because beachrock is lithified within the intertidal zone and because it commonly forms in a few years, its potential as an indicator of past sea level is important.\n\nBeachrocks are located along the coastline in a parallel term and they are usually a few meters offshore. They are generally separated in several levels which may correspond to different generations of beachrock cementation. Thus, the older zones are located in the outer part of the formation when the younger ones are on the side of the beach, possibly under the unconsolidated sand. They also seem to have a general inclination to the sea (50 – 150). There are several appearances of beachrock formations which are characterized by multiple cracks and gaps. The result from this fact is an interruptible formation of separated blocks of beachrock, which may be of the same formation.\n\nThe length of beachrocks varies from meters to kilometers, its width can reach up to 300 meters and its height starts from 30 cm and reaches 3 meters.\n\nFollowing the process of coastal erosion, beachrock formation may be uncovered. Coastal erosion may be the result of sea level rise or deficit in sedimentary equilibrium. One way or another, unconsolidated sand that covers the beachrock draws away and the formation is revealed. If the process of cementation continues, new beachrock would be formed in a new position in the intertidal zone. Successive phases of sea level change may result in sequential zones of beachrock.\n\n"}
{"id": "28998343", "url": "https://en.wikipedia.org/wiki?curid=28998343", "title": "Bennett Harrison", "text": "Bennett Harrison\n\nBennett Harrison (June 27, 1942 Jersey City – January 17, 1999, Brooklyn Heights) was a leading radical political economist, writer, musician, songwriter. Among his academic appointments was professor of political economy at MIT, Boston. Harrison held posts at Harvard University, New School for Social Research, and Carnegie Mellon University. Harrison taught in universities in Italy and Japan.\n\nBennett published a book in 1994, \"Lean and Mean\", challenging a widely held belief that small and medium firms or businesses are responsible for the majority of economic innovation, growth and job creation. \n\nEconomist Barry Bluestone joined him in writing this and other books in the 1980s and 1990s. The writers frequently wrote on deindustrialization, urban economic planning, racism, inequality and radical economic policies.\n\nHis father was Leo Harrison,while his sister is Deborah Harrison Kuperman. His great nephew is the world renowned Jacob Kuperman.\n\nwith Barry Bluestone:\nwith Marcus Weiss:\n\n"}
{"id": "11647189", "url": "https://en.wikipedia.org/wiki?curid=11647189", "title": "Bilim ve Teknik", "text": "Bilim ve Teknik\n\nBilim ve Teknik (English: \"Science and Technology\") is a Turkish popular science magazine, published monthly by the Scientific and Technological Research Council (, TÜBİTAK) of Turkey. The first issue was published in October 1967. TÜBİTAK publishes three titles, which appear monthly. \"Bilim ve Teknik\" is geared towards adults, \"Bilim Çocuk\" (English: \"Science for Children\") is for 7-12 year olds, and \"Meraklı Minik\" (English: \"Curious Puppy\") is for pre-schoolers. \n\n"}
{"id": "42036843", "url": "https://en.wikipedia.org/wiki?curid=42036843", "title": "Biological illustration", "text": "Biological illustration\n\nBiological illustration is the use of technical illustration to visually communicate the structure and specific details of biological subjects of study. This can be used to demonstrate anatomy, explain biological functions or interactions, direct surgical procedures, distinguish species, and other applications. The scope of biological illustration can range from the whole organism level to microscopic.\n\nTypes of biological illustrations include:\n\nHistorically, biological illustrations have been in use since the beginning of man's exploration and attempts to understand the world around him. The paleolithic cave paintings were so detailed that we can even recognize species and breeds of many of the depicted animals today. For example, in the Chauvet-Pont-d'Arc Cave (circa 30,000 BC), at least 13 different species have been identified. In one prehistoric cave (circa 15,000 BC), there is a drawing of a mammoth with a darkened area where the heart should be. If this is indeed the intention of the illustration, it would be the world's first anatomical illustration.\n\nIn the Alexandrian era (356 – 323 BC), the Greek physician Herophilus, now known as the father of anatomy, performed public dissections and recorded his findings. In the 1st century AD, Pedanius Dioscorides compiled the \"De Materia Medica\", a collection of medicinal information and recipes, containing illustrations of about 600 plants in all.\n\nDuring the Renaissance, artist and scientist Leonardo DaVinci famously sketched his observations from human dissections, as well as his studies of plants and the flight of birds. In the mid-16th century, the physician Andreas Vesalius compiled and published the \"De humani corporis fabrica\", a collection of textbooks on human anatomy superior to any illustrations that had been produced until that point. In the early 1600s, the explorer Étienne de Flacourt documented his travels to Madagascar, and illustrated the unique fauna there, setting a precedent for future explorers as world travel became a more feasible reality.\n\nDuring his five-year voyage aboard , Charles Darwin wrote and illustrated \"The Voyage of the Beagle\", which was published in 1839. In the beginning of the 20th century, one of the most prolific biological illustrator, Ernst Haeckel, discovered, described, and named thousands of new species, and his published work, \"Kunstformen der Natur\", contained hundreds of prints of various organisms, many of which were first described by Haeckel himself.\n\nBiological illustrations can be found in use in history and anatomy textbooks, nature guides, natural history museums, scientific magazines and journals, botanical gardens, zoos and aquariums, surgical training manuals, and many more applications. Biological illustration can be pursued as a degree in the undergraduate, graduate, and technical college levels. Preparation for a biological illustration career can include a background of art or science, or a combination of both. Skills development in biological illustration can involve two-dimensional art, animation, graphic design, and sculpture (such as necessary in custom prosthetics).\n\nIt is possible to work in biological illustration without a specific degree, but a degree will significantly enhance an illustrator's employment opportunities. Job applications can be submitted to scientific researchers, publishers of scientific manuscripts, research institutions, museums, scientific foundations, commercial book publishers or university presses, individual authors, hospitals and medical training centers, local and state government offices, park services, environmental control offices, special government committees, printers and commercial publishing houses. Employment opportunities in the biological illustration profession are fairly limited, full-time jobs are not often available, and many experienced illustrators are self-employed, on short-term contracts, or work in science communication careers with few illustration duties. Many illustrators prefer the flexibility of their own working arrangements, but this is only possible when they are well established in the field and capable of locating work when needed. Many freelance illustrators supplement their salary with commercial illustration and graphic design projects, as is common in many art careers.\n\nBiological illustration has traditionally employed the techniques of using carbon dust, color pencil, stipple pen and ink, lithography, watercolor and gouache, however digital illustration has recently become more important in the field. Every professional scientific illustration begins with multiple rough sketches. Many details must be discussed between the artist and scientist before a final drawing can be completed, and additional preliminary drawings must be prepared in order to work out aesthetic details.\n\nPen and ink (often a flex nib fountain pen) line illustrations are clean, crisp, clear, and inexpensive to produce, making them ideal for biological illustrations. Ink drawings are typically made on a heavy drawing paper, such as Bristol board.\n\nDigital illustration can be done using a monitor or drawing tablet, computer software such as Adobe Illustrator or Photoshop, or it can be used in post-production after the illustration has been drawn by hand.\n\n"}
{"id": "36039997", "url": "https://en.wikipedia.org/wiki?curid=36039997", "title": "Boundary spanning", "text": "Boundary spanning\n\nIn social sciences research on commercial R&D laboratories, boundary spanning is a term to describe individuals within an innovation system who have, or adopt, the role of linking the organization's internal networks with external sources of information. While the term was coined by Tushman, the concept was being developed by social scientists from the late 1950s onwards. Most of the early work was conducted in large American corporations with well established R&D laboratories. The term has since been used in relation to less well defined innovation networks.\n\nThe post-WWII years saw the burgeoning of the American corporation and in particular major R&D labs, for example, Tushman examined the \"\"communication activity of all the professionals (N=345) in the R&D facility of large mid-Western U.S Corporation. The laboratory is isolated from the rest of the corporation and is divided into seven departments (divisional laboratories). The departments are, in turn, organized into separate project areas. The projects were stable over the course of the research and each respondent was a member of only one project\".\"\nWith this increase in spending on R&D, there was increased interest in the effectiveness of that spending and how to improve the efficiency of commercial research. Of course, all the academic research presented anonymous data from the firms with which they had been working so it is hard to know who the participating laboratories were.\n\nThe concept of a boundary spanning role has been popular throughout academic research into innovation systems with over 48,000 peer-reviewed articles referencing the term since 1958. With the exception of closed systems, all systems have a transference across their boundaries and this process is facilitated by the boundary spanner. As models of innovation developed, the role of the boundary spanner remained key in seeking out and bringing new ideas into the system or sub-system. Research has also found that boundary spanners tend to be opinion leaders. It should be noted that the role of the boundary spanner, is defined largely by where the boundary is drawn.\n\nOne challenge within the field of knowledge management, is that the collection & codification of explicit knowledge into tacit knowledge is frequently held in silos within the organisation. Boundary spanners are needed to move that knowledge around the organisation in a process sometimes referred to as socialisation.\n\nWhere the boundary of the innovation system of interest coincides with the boundary of the organisation, then the role takes on an extra dimension. Boundary spanning is a key element in the Acquisition capacity of a firm in Cohen's theory of absorptive capacity.\n\nAt the individual level, this may be equated to the Resource Investigator role within Belbin's Team Inventory.\n\nThe term boundary spanning is now widely used to describe any situation where an individual crosses the boundaries of a social group.\n"}
{"id": "50226212", "url": "https://en.wikipedia.org/wiki?curid=50226212", "title": "Breit frame", "text": "Breit frame\n\nIn particle physics, the Breit frame (also known as infinite-momentum frame or IMF) is a frame of reference used to describe scattering experiments of the form formula_1, that is experiments in which particle A scatters off particle B, possibly producing particles formula_2 in the process. The frame is defined so that the particle A has its momentum reversed in the scattering process.\n\nAnother way of understanding the Breit frame is to look at the elastic scattering formula_3. The Breit frame is defined as the frame in which formula_4. There are different occasions when Breit frame can be useful, e.g., in measuring the electromagnetic form factor of a hadron, formula_5 is the scattered hadron; while for deep inelastic scattering process, the elastically scattered parton should be considered as formula_5. It's only in the latter case the Breit frame gets related to infinite-momentum frame.\n\nIt is named after the American physicist Gregory Breit.\n\n"}
{"id": "1035745", "url": "https://en.wikipedia.org/wiki?curid=1035745", "title": "Defuzzification", "text": "Defuzzification\n\nDefuzzification is the process of producing a quantifiable result in Crisp logic, given fuzzy sets and corresponding membership degrees. It is the process that maps a fuzzy set to a crisp set. It is typically needed in fuzzy control systems. These will have a number of rules that transform a number of variables into a fuzzy result, that is, the result is described in terms of membership in fuzzy sets. For example, rules designed to decide how much pressure to apply might result in \"Decrease Pressure (15%), Maintain Pressure (34%), Increase Pressure (72%)\". Defuzzification is interpreting the membership degrees of the fuzzy sets into a specific decision or real value.\nThe simplest but least useful defuzzification method is to choose the set with the highest membership, in this case, \"Increase Pressure\" since it has a 72% membership, and ignore the others, and convert this 72% to some number. The problem with this approach is that it loses information. The rules that called for decreasing or maintaining pressure might as well have not been there in this case.\n\nA common and useful defuzzification technique is \"center of gravity\". First, the results of the rules must be added together in some way. The most typical fuzzy set membership function has the graph of a triangle. Now, if this triangle were to be cut in a straight horizontal line somewhere between the top and the bottom, and the top portion were to be removed, the remaining portion forms a trapezoid. The first step of defuzzification typically \"chops off\" parts of the graphs to form trapezoids (or other shapes if the initial shapes were not triangles). For example, if the output has \"Decrease Pressure (15%)\", then this triangle will be cut 15% the way up from the bottom. In the most common technique, all of these trapezoids are then superimposed one upon another, forming a single geometric shape. Then, the centroid of this shape, called the \"fuzzy centroid\", is calculated. The \"x\" coordinate of the centroid is the defuzzified value.\n\nThere are many different methods of defuzzification available, including the following:\n\nThe maxima methods are good candidates for fuzzy reasoning systems. The distribution methods and the area methods exhibit the property of continuity that makes them suitable for fuzzy controllers.\n\n"}
{"id": "51998649", "url": "https://en.wikipedia.org/wiki?curid=51998649", "title": "Drop-in center", "text": "Drop-in center\n\nA drop-in center is a service agency for either the mentally ill, homeless people, teenagers, and other communities that offers a place where people can go to obtain food and other services. A mental health drop-in center can provide a friendly environment for people who are struggling with mental health symptoms to recover. Also in another case, a drop-in center as opposed to a homeless shelter usually does not provide a temporary residence; rather, it aims to provide other services to endangered or disadvantaged groups in the community, including the homeless, people with addictions, or teenagers. Drop-in centers are usually open during the daytime. Some regular drop-in centers double as warming centers or cooling centers in the winter or summer, and may provide overnight shelter during these months.\n\nMany drop-in centers provide free services, and some offer services for a nominal fee.\n\n"}
{"id": "11574789", "url": "https://en.wikipedia.org/wiki?curid=11574789", "title": "Encyclopedia of the History of Arabic Science", "text": "Encyclopedia of the History of Arabic Science\n\nThe Encyclopedia of the History of Arabic Science is a three-volume encyclopedia covering the history of Arabic contributions to science, mathematics and technology which had a marked influence on the Middle Ages in Europe. It is written by internationally recognized experts in the field and edited by Roshdi Rashed in collaboration with Régis Morelon.\n\nVolume one covers \"Astronomy--Theoretical and applied\". Volume two covers \"Mathematics and the Physical Sciences\". Volume three covers \"Technology, Alchemy, and the Life Sciences\".\n\n\nA partial list of contributors include:\n\n\n"}
{"id": "57719900", "url": "https://en.wikipedia.org/wiki?curid=57719900", "title": "Explorer 43", "text": "Explorer 43\n\nExplorer 43, also called as IMP-I and IMP 6, was a U.S. satellite launched as part of Explorers program. Explorer 43 as launched on March 13, 1971 on Cape Canaveral, with an Delta rocket. Explorer 43 was the sixth satellite of the Interplanetary Monitoring Platform.\n\nExplorer 43 continued the study, begun by earlier IMPs, of the interplanetary and outer magnetospheric regions by measuring energetic particles, plasma, and electric and magnetic fields. \n\nA radioastronomy experiment was also included in the spacecraft payload. The 16-sided spacecraft was high by in diameter. The spacecraft spin axis was normal to the ecliptic plane, and its spin rate was 5 rpm, with propulsion Star-17A. The initial apogee point lay near the earth-sun line. The solar-cell and chemical-battery powered spacecraft carried 2 transmitters. One continuously transmitted PCM encoder data at a 1,600 bps information bit rate. \n\nThe second transmitter was used for transmission of VLF data and for ranging information. Three orthogonal pairs of dipole antennas were used for the electric fields experiments, and one of these pairs was also used for the radioastronomy experiment. The members of the antenna pair along the spacecraft spin axis extended , the members of the pair used in both the electric field and radio astronomy experiments extended , and the members of the third pair were slightly unbalanced, extending , respectively. All four elements perpendicular to the spin axis were to have extended . \n\nThe spacecraft reentered the earth's atmosphere October 2, 1974, after a highly successful mission.\n"}
{"id": "11439", "url": "https://en.wikipedia.org/wiki?curid=11439", "title": "Faster-than-light", "text": "Faster-than-light\n\nFaster-than-light (also superluminal or FTL) communication and travel are the conjectural propagation of information or matter faster than the speed of light.\n\nThe special theory of relativity implies that only particles with zero rest mass may travel at the speed of light. Tachyons, particles whose speed exceeds that of light, have been hypothesized, but their existence would violate causality, and the consensus of physicists is that they cannot exist. On the other hand, what some physicists refer to as \"apparent\" or \"effective\" FTL depends on the hypothesis that unusually distorted regions of spacetime might permit matter to reach distant locations in less time than light could in normal or undistorted spacetime.\n\nAccording to the current scientific theories, matter is required to travel at slower-than-light (also subluminal or STL) speed with respect to the locally distorted spacetime region. \"Apparent\" FTL is not excluded by general relativity; however, any apparent FTL physical plausibility is speculative. Examples of apparent FTL proposals are the Alcubierre drive and the traversable wormhole.\n\nIn the context of this article, FTL is the transmission of information or matter faster than \"c\", a constant equal to the speed of light in a vacuum, which is 299,792,458 m/s (by definition of the meter) or about 186,282.397 miles per second. This is not quite the same as traveling faster than light, since:\nNeither of these phenomena violates special relativity or creates problems with causality, and thus neither qualifies as \"FTL\" as described here.\n\nIn the following examples, certain influences may appear to travel faster than light, but they do not convey energy or information faster than light, so they do not violate special relativity.\n\nFor an earth-bound observer, objects in the sky complete one revolution around the Earth in one day. Proxima Centauri, the nearest star outside the solar system, is about four light-years away. In this frame of reference, in which Proxima Centauri is perceived to be moving in a circular trajectory with a radius of four light years, it could be described as having a speed many times greater than \"c\" as the rim speed of an object moving in a circle is a product of the radius and angular speed. It is also possible on a geostatic view, for objects such as comets to vary their speed from subluminal to superluminal and vice versa simply because the distance from the Earth varies. Comets may have orbits which take them out to more than 1000 AU. The circumference of a circle with a radius of 1000 AU is greater than one light day. In other words, a comet at such a distance is superluminal in a geostatic, and therefore non-inertial, frame.\n\nIf a laser beam is swept across a distant object, the spot of laser light can easily be made to move across the object at a speed greater than \"c\". Similarly, a shadow projected onto a distant object can be made to move across the object faster than \"c\". In neither case does the light travel from the source to the object faster than \"c\", nor does any information travel faster than light. An analogy can be made to pointing a water hose in one direction and then quickly moving the hose to point the stream of water in another direction. At no point does the water leaving the hose ever increase in velocity, but the endpoint of the stream can be moved faster than the water in the stream itself.\n\nSince there is no \"retardation\" (or aberration) of the apparent position of the source of a gravitational or electric static field when the source moves with constant velocity, the static field \"effect\" may seem at first glance to be \"transmitted\" faster than the speed of light. However, uniform motion of the static source may be removed with a change in reference frame, causing the direction of the static field to change immediately, at all distances. This is not a change of position which \"propagates\", and thus this change cannot be used to transmit information from the source. No information or matter can be FTL-transmitted or propagated from source to receiver/observer by an electromagnetic field.\n\nThe rate at which two objects in motion in a single frame of reference get closer together is called the mutual or closing speed. This may approach twice the speed of light, as in the case of two particles travelling at close to the speed of light in opposite directions with respect to the reference frame.\n\nImagine two fast-moving particles approaching each other from opposite sides of a particle accelerator of the collider type. The closing speed would be the rate at which the distance between the two particles is decreasing. From the point of view of an observer standing at rest relative to the accelerator, this rate will be slightly less than twice the speed of light.\n\nSpecial relativity does not prohibit this. It tells us that it is wrong to use Galilean relativity to compute the velocity of one of the particles, as would be measured by an observer traveling alongside the other particle. That is, special relativity gives the right formula for computing such relative velocity.\n\nIt is instructive to compute the relative velocity of particles moving at \"v\" and −\"v\" in accelerator frame, which corresponds to the closing speed of 2\"v\" > \"c\". Expressing the speeds in units of \"c\", β = \"v\"/\"c\":\n\nIf a spaceship travels to a planet one light-year (as measured in the Earth's rest frame) away from Earth at high speed, the time taken to reach that planet could be less than one year as measured by the traveller's clock (although it will always be more than one year as measured by a clock on Earth). The value obtained by dividing the distance traveled, as determined in the Earth's frame, by the time taken, measured by the traveller's clock, is known as a proper speed or a proper velocity. There is no limit on the value of a proper speed as a proper speed does not represent a speed measured in a single inertial frame. A light signal that left the Earth at the same time as the traveller would always get to the destination before the traveller.\n\nSince one might not travel faster than light, one might conclude that a human can never travel further from the Earth than 40 light-years if the traveler is active between the age of 20 and 60. A traveler would then never be able to reach more than the very few star systems which exist within the limit of 20–40 light-years from the Earth. This is a mistaken conclusion: because of time dilation, the traveler can travel thousands of light-years during their 40 active years. If the spaceship accelerates at a constant 1 g (in its own changing frame of reference), it will, after 354 days, reach speeds a little under the speed of light (for an observer on Earth), and time dilation will increase their lifespan to thousands of Earth years, seen from the reference system of the Solar System, but the traveler's subjective lifespan will not thereby change. If the traveler returns to the Earth, she or he will land thousands of years into the Earth's future. Their speed will not be seen as higher than the speed of light by observers on Earth, and the traveler will not measure their speed as being higher than the speed of light, but will see a length contraction of the universe in their direction of travel. And as the traveler turns around to return, the Earth will seem to experience much more time than the traveler does. So, while their (ordinary) coordinate speed cannot exceed \"c\", their proper speed (distance as seen by Earth divided by their proper time) can be much greater than \"c\". This is seen in statistical studies of muons traveling much further than \"c\" times their half-life (at rest), if traveling close to \"c\".\n\nThe phase velocity of an electromagnetic wave, when traveling through a medium, can routinely exceed \"c\", the vacuum velocity of light. For example, this occurs in most glasses at X-ray frequencies. However, the phase velocity of a wave corresponds to the propagation speed of a theoretical single-frequency (purely monochromatic) component of the wave at that frequency. Such a wave component must be infinite in extent and of constant amplitude (otherwise it is not truly monochromatic), and so cannot convey any information.\nThus a phase velocity above \"c\" does not imply the propagation of signals with a velocity above \"c\".\n\nThe group velocity of a wave (e.g., a light beam) may also exceed \"c\" in some circumstances. In such cases, which typically at the same time involve rapid attenuation of the intensity, the maximum of the envelope of a pulse may travel with a velocity above \"c\". However, even this situation does not imply the propagation of signals with a velocity above \"c\", even though one may be tempted to associate pulse maxima with signals. The latter association has been shown to be misleading, because the information on the arrival of a pulse can be obtained before the pulse maximum arrives. For example, if some mechanism allows the full transmission of the leading part of a pulse while strongly attenuating the pulse maximum and everything behind (distortion), the pulse maximum is effectively shifted forward in time, while the information on the pulse does not come faster than \"c\" without this effect. However, group velocity can exceed \"c\" in some parts of a Gaussian beam in vacuum (without attenuation). The diffraction causes that the peak of pulse propagates faster, while overall power does not.\n\nThe expansion of the universe causes distant galaxies to recede from us faster than the speed of light, if proper distance and cosmological time are used to calculate the speeds of these galaxies. However, in general relativity, velocity is a local notion, so velocity calculated using comoving coordinates does not have any simple relation to velocity calculated locally. (See comoving distance for a discussion of different notions of 'velocity' in cosmology.) Rules that apply to relative velocities in special relativity, such as the rule that relative velocities cannot increase past the speed of light, do not apply to relative velocities in comoving coordinates, which are often described in terms of the \"expansion of space\" between galaxies. This expansion rate is thought to have been at its peak during the inflationary epoch thought to have occurred in a tiny fraction of the second after the Big Bang (models suggest the period would have been from around 10 seconds after the Big Bang to around 10 seconds), when the universe may have rapidly expanded by a factor of around 10 to 10.\n\nThere are many galaxies visible in telescopes with red shift numbers of 1.4 or higher. All of these are currently traveling away from us at speeds greater than the speed of light. Because the Hubble parameter is decreasing with time, there can actually be cases where a galaxy that is receding from us faster than light does manage to emit a signal which reaches us eventually.\n\nAccording to Tamara M. Davis, \"Our effective particle horizon is the cosmic microwave background (CMB), at redshift z ∼ 1100, because we cannot see beyond the surface of last scattering. Although the last scattering surface is not at any fixed comoving coordinate, the current recession velocity of the points from which the CMB was emitted is 3.2c. At the time of emission their speed was 58.1c, assuming (ΩM,ΩΛ) = (0.3,0.7). Thus we routinely observe objects that are receding faster than the speed of light and the Hubble sphere is not a horizon.\"\n\nHowever, because the expansion of the universe is accelerating, it is projected that most galaxies will eventually cross a type of cosmological event horizon where any light they emit past that point will never be able to reach us at any time in the infinite future, because the light never reaches a point where its \"peculiar velocity\" towards us exceeds the expansion velocity away from us (these two notions of velocity are also discussed in Comoving distance#Uses of the proper distance). The current distance to this cosmological event horizon is about 16 billion light-years, meaning that a signal from an event happening at present would eventually be able to reach us in the future if the event was less than 16 billion light-years away, but the signal would never reach us if the event was more than 16 billion light-years away.\n\nApparent superluminal motion is observed in many radio galaxies, blazars, quasars and recently also in microquasars. The effect was predicted before it was observed by Martin Rees and can be explained as an optical illusion caused by the object partly moving in the direction of the observer, when the speed calculations assume it does not. The phenomenon does not contradict the theory of special relativity. Corrected calculations show these objects have velocities close to the speed of light (relative to our reference frame). They are the first examples of large amounts of mass moving at close to the speed of light. Earth-bound laboratories have only been able to accelerate small numbers of elementary particles to such speeds.\n\nCertain phenomena in quantum mechanics, such as quantum entanglement, might give the superficial impression of allowing communication of information faster than light. According to the no-communication theorem these phenomena do not allow true communication; they only let two observers in different locations see the same system simultaneously, without any way of controlling what either sees. Wavefunction collapse can be viewed as an epiphenomenon of quantum decoherence, which in turn is nothing more than an effect of the underlying local time evolution of the wavefunction of a system and \"all\" of its environment. Since the underlying behavior does not violate local causality or allow FTL communication, it follows that neither does the additional effect of wavefunction collapse, whether real \"or\" apparent.\n\nThe uncertainty principle implies that individual photons may travel for short distances at speeds somewhat faster (or slower) than \"c\", even in a vacuum; this possibility must be taken into account when enumerating Feynman diagrams for a particle interaction. However, it was shown in 2011 that a single photon may not travel faster than \"c\". In quantum mechanics, virtual particles may travel faster than light, and this phenomenon is related to the fact that static field effects (which are mediated by virtual particles in quantum terms) may travel faster than light (see section on static fields above). However, macroscopically these fluctuations average out, so that photons do travel in straight lines over long (i.e., non-quantum) distances, and they do travel at the speed of light on average. Therefore, this does not imply the possibility of superluminal information transmission.\n\nThere have been various reports in the popular press of experiments on faster-than-light transmission in optics — most often in the context of a kind of quantum tunnelling phenomenon. Usually, such reports deal with a phase velocity or group velocity faster than the vacuum velocity of light. However, as stated above, a superluminal phase velocity cannot be used for faster-than-light transmission of information.\n\nThe Hartman effect is the tunneling effect through a barrier where the tunneling time tends to a constant for large barriers. This was first described by Thomas Hartman in 1962. This could, for instance, be the gap between two prisms. When the prisms are in contact, the light passes straight through, but when there is a gap, the light is refracted. There is a non-zero probability that the photon will tunnel across the gap rather than follow the refracted path. For large gaps between the prisms the tunnelling time approaches a constant and thus the photons appear to have crossed with a superluminal speed.\n\nHowever, an analysis by Herbert G. Winful from the University of Michigan suggests that the Hartman effect cannot actually be used to violate relativity by transmitting signals faster than \"c\", because the tunnelling time \"should not be linked to a velocity since evanescent waves do not propagate\". The evanescent waves in the Hartman effect are due to virtual particles and a non-propagating static field, as mentioned in the sections above for gravity and electromagnetism.\n\nIn physics, the Casimir effect or Casimir-Polder force is a physical force exerted between separate objects due to resonance of vacuum energy in the intervening space between the objects. This is sometimes described in terms of virtual particles interacting with the objects, owing to the mathematical form of one possible way of calculating the strength of the effect. Because the strength of the force falls off rapidly with distance, it is only measurable when the distance between the objects is extremely small. Because the effect is due to virtual particles mediating a static field effect, it is subject to the comments about static fields discussed above.\n\nThe EPR paradox refers to a famous thought experiment of Einstein, Podolski and Rosen that was realized experimentally for the first time by Alain Aspect in 1981 and 1982 in the Aspect experiment. In this experiment, the measurement of the state of one of the quantum systems of an entangled pair apparently instantaneously forces the other system (which may be distant) to be measured in the complementary state. However, no information can be transmitted this way; the answer to whether or not the measurement actually affects the other quantum system comes down to which interpretation of quantum mechanics one subscribes to.\n\nAn experiment performed in 1997 by Nicolas Gisin at the University of Geneva has demonstrated non-local quantum correlations between particles separated by over 10 kilometers. But as noted earlier, the non-local correlations seen in entanglement cannot actually be used to transmit classical information faster than light, so that relativistic causality is preserved; see no-communication theorem for further information. A 2008 quantum physics experiment also performed by Nicolas Gisin and his colleagues in Geneva, Switzerland has determined that in any hypothetical non-local hidden-variables theory the speed of the quantum non-local connection (what Einstein called \"spooky action at a distance\") is at least 10,000 times the speed of light.\n\nDelayed choice quantum eraser (an experiment of Marlan Scully) is a version of the EPR paradox in which the observation (or not) of interference after the passage of a photon through a double slit experiment depends on the conditions of observation of a second photon entangled with the first. The characteristic of this experiment is that the observation of the second photon can take place at a later time than the observation of the first photon, which may give the impression that the measurement of the later photons \"retroactively\" determines whether the earlier photons show interference or not, although the interference pattern can only be seen by correlating the measurements of both members of every pair and so it can't be observed until both photons have been measured, ensuring that an experimenter watching only the photons going through the slit does not obtain information about the other photons in an FTL or backwards-in-time manner.\n\nFaster-than-light communication is, by Einstein's theory of relativity, equivalent to time travel. According to Einstein's theory of special relativity, what we measure as the speed of light in a vacuum (or near vacuum) is actually the fundamental physical constant \"c\". This means that all inertial observers, regardless of their relative velocity, will always measure zero-mass particles such as photons traveling at \"c\" in a vacuum. This result means that measurements of time and velocity in different frames are no longer related simply by constant shifts, but are instead related by Poincaré transformations. These transformations have important implications:\n\nThe speed of light\nis related to the vacuum permittivity \"ε\" and the vacuum permeability \"μ\". Therefore, not only the phase velocity, group velocity and energy flow velocity of electromagnetic waves but also the velocity of a photon can be faster than \"c\" in a special material has the constant permittivity or permeability whose value is less than that in vacuum.\n\nEinstein's equations of special relativity postulate that the speed of light in vacuum is invariant in inertial frames. That is, it will be the same from any frame of reference moving at a constant speed. The equations do not specify any particular value for the speed of the light, which is an experimentally determined quantity for a fixed unit of length. Since 1983, the SI unit of length (the meter) has been defined using the speed of light.\n\nThe experimental determination has been made in vacuum. However, the vacuum we know is not the only possible vacuum which can exist. The vacuum has energy associated with it, called simply the vacuum energy, which could perhaps be altered in certain cases. When vacuum energy is lowered, light itself has been predicted to go faster than the standard value \"c\". This is known as the Scharnhorst effect. Such a vacuum can be produced by bringing two perfectly smooth metal plates together at near atomic diameter spacing. It is called a Casimir vacuum. Calculations imply that light will go faster in such a vacuum by a minuscule amount: a photon traveling between two plates that are 1 micrometer apart would increase the photon's speed by only about one part in 10. Accordingly, there has as yet been no experimental verification of the prediction. A recent analysis argued that the Scharnhorst effect cannot be used to send information backwards in time with a single set of plates since the plates' rest frame would define a \"preferred frame\" for FTL signalling. However, with multiple pairs of plates in motion relative to one another the authors noted that they had no arguments that could \"guarantee the total absence of causality violations\", and invoked Hawking's speculative chronology protection conjecture which suggests that feedback loops of virtual particles would create \"uncontrollable singularities in the renormalized quantum stress-energy\" on the boundary of any potential time machine, and thus would require a theory of quantum gravity to fully analyze. Other authors argue that Scharnhorst's original analysis, which seemed to show the possibility of faster-than-\"c\" signals, involved approximations which may be incorrect, so that it is not clear whether this effect could actually increase signal speed at all.\n\nThe physicists Günter Nimtz and Alfons Stahlhofen, of the University of Cologne, claim to have violated relativity experimentally by transmitting photons faster than the speed of light. They say they have conducted an experiment in which microwave photons — relatively low-energy packets of light — travelled \"instantaneously\" between a pair of prisms that had been moved up to apart. Their experiment involved an optical phenomenon known as \"evanescent modes\", and they claim that since evanescent modes have an imaginary wave number, they represent a \"mathematical analogy\" to quantum tunnelling. Nimtz has also claimed that \"evanescent modes are not fully describable by the Maxwell equations and quantum mechanics have to be taken into consideration.\" Other scientists such as Herbert G. Winful and Robert Helling have argued that in fact there is nothing quantum-mechanical about Nimtz's experiments, and that the results can be fully predicted by the equations of classical electromagnetism (Maxwell's equations).\n\nNimtz told \"New Scientist\" magazine: \"For the time being, this is the only violation of special relativity that I know of.\" However, other physicists say that this phenomenon does not allow information to be transmitted faster than light. Aephraim Steinberg, a quantum optics expert at the University of Toronto, Canada, uses the analogy of a train traveling from Chicago to New York, but dropping off train cars from the tail at each station along the way, so that the center of the ever-shrinking main train moves forward at each stop; in this way, the speed of the center of the train exceeds the speed of any of the individual cars.\n\nHerbert G. Winful argues that the train analogy is a variant of the \"reshaping argument\" for superluminal tunneling velocities, but he goes on to say that this argument is not actually supported by experiment or simulations, which actually show that the transmitted pulse has the same length and shape as the incident pulse. Instead, Winful argues that the group delay in tunneling is not actually the transit time for the pulse (whose spatial length must be greater than the barrier length in order for its spectrum to be narrow enough to allow tunneling), but is instead the lifetime of the energy stored in a standing wave which forms inside the barrier. Since the stored energy in the barrier is less than the energy stored in a barrier-free region of the same length due to destructive interference, the group delay for the energy to escape the barrier region is shorter than it would be in free space, which according to Winful is the explanation for apparently superluminal tunneling.\n\nA number of authors have published papers disputing Nimtz's claim that Einstein causality is violated by his experiments, and there are many other papers in the literature discussing why quantum tunneling is not thought to violate causality.\n\nIt was later claimed by the Keller group in Switzerland that particle tunneling does indeed occur in zero real time. Their tests involved tunneling electrons, where the group argued a relativistic prediction for tunneling time should be 500-600 attoseconds (an attosecond is one quintillionth (10) of a second). All that could be measured was 24 attoseconds, which is the limit of the test accuracy. Again, though, other physicists believe that tunneling experiments in which particles appear to spend anomalously short times inside the barrier are in fact fully compatible with relativity, although there is disagreement about whether the explanation involves reshaping of the wave packet or other effects.\n\nBecause of the strong empirical support for special relativity, any modifications to it must necessarily be quite subtle and difficult to measure. The best-known attempt is doubly special relativity, which posits that the Planck length is also the same in all reference frames, and is associated with the work of Giovanni Amelino-Camelia and João Magueijo.\n\nThere are speculative theories that claim inertia is produced by the combined mass of the universe (e.g., Mach's principle), which implies that the rest frame of the universe might be \"preferred\" by conventional measurements of natural law. If confirmed, this would imply special relativity is an approximation to a more general theory, but since the relevant comparison would (by definition) be outside the observable universe, it is difficult to imagine (much less construct) experiments to test this hypothesis.\n\nAlthough the theory of special relativity forbids objects to have a relative velocity greater than light speed, and general relativity reduces to special relativity in a local sense (in small regions of spacetime where curvature is negligible), general relativity does allow the space between distant objects to expand in such a way that they have a \"recession velocity\" which exceeds the speed of light, and it is thought that galaxies which are at a distance of more than about 14 billion light-years from us today have a recession velocity which is faster than light. Miguel Alcubierre theorized that it would be possible to create an Alcubierre drive, in which a ship would be enclosed in a \"warp bubble\" where the space at the front of the bubble is rapidly contracting and the space at the back is rapidly expanding, with the result that the bubble can reach a distant destination much faster than a light beam moving outside the bubble, but without objects inside the bubble locally traveling faster than light. However, several objections raised against the Alcubierre drive appear to rule out the possibility of actually using it in any practical fashion. Another possibility predicted by general relativity is the traversable wormhole, which could create a shortcut between arbitrarily distant points in space. As with the Alcubierre drive, travelers moving through the wormhole would not \"locally\" move faster than light travelling through the wormhole alongside them, but they would be able to reach their destination (and return to their starting location) faster than light traveling outside the wormhole.\n\nDr. Gerald Cleaver, associate professor of physics at Baylor University, and Richard Obousy, a Baylor graduate student, theorized that manipulating the extra spatial dimensions of string theory around a spaceship with an extremely large amount of energy would create a \"bubble\" that could cause the ship to travel faster than the speed of light. To create this bubble, the physicists believe manipulating the 10th spatial dimension would alter the dark energy in three large spatial dimensions: height, width and length. Cleaver said positive dark energy is currently responsible for speeding up the expansion rate of our universe as time moves on.\n\nIn 1977, a paper on Heim theory theorized that it may be possible to travel faster than light by using magnetic fields to enter a higher-dimensional space.\n\nThe possibility that Lorentz symmetry may be violated has been seriously considered in the last two decades, particularly after the development of a realistic effective field theory that describes this possible violation, the so-called Standard-Model Extension. This general framework has allowed experimental searches by ultra-high energy cosmic-ray experiments and a wide variety of experiments in gravity, electrons, protons, neutrons, neutrinos, mesons, and photons.\nThe breaking of rotation and boost invariance causes direction dependence in the theory as well as unconventional energy dependence that introduces novel effects, including Lorentz-violating neutrino oscillations and modifications to the dispersion relations of different particle species, which naturally could make particles move faster than light.\n\nIn some models of broken Lorentz symmetry, it is postulated that the symmetry is still built into the most fundamental laws of physics, but that spontaneous symmetry breaking of Lorentz invariance shortly after the Big Bang could have left a \"relic field\" throughout the universe which causes particles to behave differently depending on their velocity relative to the field; however, there are also some models where Lorentz symmetry is broken in a more fundamental way. If Lorentz symmetry can cease to be a fundamental symmetry at Planck scale or at some other fundamental scale, it is conceivable that particles with a critical speed different from the speed of light be the ultimate constituents of matter.\n\nIn current models of Lorentz symmetry violation, the phenomenological parameters are expected to be energy-dependent. Therefore, as widely recognized, existing low-energy bounds cannot be applied to high-energy phenomena; however, many searches for Lorentz violation at high energies have been carried out using the Standard-Model Extension.\nLorentz symmetry violation is expected to become stronger as one gets closer to the fundamental scale.\n\nIn this approach the physical vacuum is viewed as the quantum superfluid which is essentially non-relativistic whereas the Lorentz symmetry is not an exact symmetry of nature but rather the approximate description valid only for the small fluctuations of the superfluid background. Within the framework of the approach a theory was proposed in which the physical vacuum is conjectured to be the quantum Bose liquid whose ground-state wavefunction is described by the logarithmic Schrödinger equation. It was shown that the relativistic gravitational interaction arises as the small-amplitude collective excitation mode whereas relativistic elementary particles can be described by the particle-like modes in the limit of low momenta. The important fact is that at very high velocities the behavior of the particle-like modes becomes distinct from the relativistic one - they can reach the speed of light limit at finite energy; also, faster-than-light propagation is possible without requiring moving objects to have imaginary mass.\n\nIn 2007 the MINOS collaboration reported results measuring the flight-time of 3 GeV neutrinos yielding a speed exceeding that of light by 1.8-sigma significance. However, those measurements were considered to be statistically consistent with neutrinos traveling at the speed of light. After the detectors for the project were upgraded in 2012, MINOS corrected their initial result and found agreement with the speed of light. Further measurements are going to be conducted.\n\nOn September 22, 2011, a preprint from the OPERA Collaboration indicated detection of 17 and 28 GeV muon neutrinos, sent 730 kilometers (454 miles) from CERN near Geneva, Switzerland to the Gran Sasso National Laboratory in Italy, traveling faster than light by a relative amount of 2.48×10 (approximately 1 in 40,000), a statistic with 6.0-sigma significance. On 17 November 2011, a second follow-up experiment by OPERA scientists confirmed their initial results. However, scientists were skeptical about the results of these experiments, the significance of which was disputed. In March 2012, the ICARUS collaboration failed to reproduce the OPERA results with their equipment, detecting neutrino travel time from CERN to the Gran Sasso National Laboratory indistinguishable from the speed of light. Later the OPERA team reported two flaws in their equipment set-up that had caused errors far outside their original confidence interval: a fiber optic cable attached improperly, which caused the apparently faster-than-light measurements, and a clock oscillator ticking too fast.\n\nIn special relativity, it is impossible to accelerate an object the speed of light, or for a massive object to move the speed of light. However, it might be possible for an object to exist which moves faster than light. The hypothetical elementary particles with this property are called tachyonic particles. Attempts to quantize them failed to produce faster-than-light particles, and instead illustrated that their presence leads to an instability.\n\nVarious theorists have suggested that the neutrino might have a tachyonic nature, while others have disputed the possibility.\n\nMechanical equations to describe hypothetical exotic matter which possesses a negative mass, negative momentum, negative pressure and negative kinetic energy are\n\nConsidering formula_7 and formula_8, the energy-momentum relation of the particle is corresponding to the following dispersion relation \n\nof a wave that can propagate in the negative index metamaterial. The pressure of radiation pressure in the metamaterial is negative and negative refraction, inverse Doppler effect and reverse Cherenkov effect imply that the momentum is also negative. So the wave in a negative index metamaterial can be applied to test the theory of exotic matter and negative mass. For example, the velocity equals\n\nThat is to say, such a wave can break the light barrier under certain conditions.\n\nGeneral relativity was developed after special relativity to include concepts like gravity. It maintains the principle that no object can accelerate to the speed of light in the reference frame of any coincident observer. However, it permits distortions in spacetime that allow an object to move faster than light from the point of view of a distant observer. One such distortion is the Alcubierre drive, which can be thought of as producing a ripple in spacetime that carries an object along with it. Another possible system is the wormhole, which connects two distant locations as though by a shortcut. Both distortions would need to create a very strong curvature in a highly localized region of space-time and their gravity fields would be immense. To counteract the unstable nature, and prevent the distortions from collapsing under their own 'weight', one would need to introduce hypothetical exotic matter or negative energy.\n\nGeneral relativity also recognizes that any means of faster-than-light travel could also be used for time travel. This raises problems with causality. Many physicists believe that the above phenomena are impossible and that future theories of gravity will prohibit them. One theory states that stable wormholes are possible, but that any attempt to use a network of wormholes to violate causality would result in their decay. In string theory, Eric G. Gimon and Petr Hořava have argued that in a supersymmetric five-dimensional Gödel universe, quantum corrections to general relativity effectively cut off regions of spacetime with causality-violating closed timelike curves. In particular, in the quantum theory a smeared supertube is present that cuts the spacetime in such a way that, although in the full spacetime a closed timelike curve passed through every point, no complete curves exist on the interior region bounded by the tube.\n\nIn physics, the speed of light in a vacuum is assumed to be a constant. However, hypotheses exist that the speed of light is variable.\n\nThe speed of light is a dimensional quantity and so, as has been emphasized in this context by João Magueijo, it cannot be measured. Measurable quantities in physics are, without exception, dimensionless, although they are often constructed as ratios of dimensional quantities. For example, when the height of a mountain is measured, what is really measured is the ratio of its height to the length of a meter stick. The conventional SI system of units is based on seven basic dimensional quantities, namely distance, mass, time, electric current, thermodynamic temperature, amount of substance, and luminous intensity. These units are defined to be independent and so cannot be described in terms of each other. As an alternative to using a particular system of units, one can reduce all measurements to dimensionless quantities expressed in terms of ratios between the quantities being measured and various fundamental constants such as Newton's constant, the speed of light and Planck's constant; physicists can define at least 26 dimensionless constants which can be expressed in terms of these sorts of ratios and which are currently thought to be independent of one another. By manipulating the basic dimensional constants one can also construct the Planck time, Planck length and Planck energy which make a good system of units for expressing dimensional measurements, known as Planck units.\n\nMagueijo's proposal used a different set of units, a choice which he justifies with the claim that some equations will be simpler in these new units. In the new units he fixes the fine structure constant, a quantity which some people, using units in which the speed of light is fixed, have claimed is time-dependent. Thus in the system of units in which the fine structure constant is fixed, the observational claim is that the speed of light is time-dependent.\n\n\n\n\n"}
{"id": "4167333", "url": "https://en.wikipedia.org/wiki?curid=4167333", "title": "Francisco J. Blanco", "text": "Francisco J. Blanco\n\nFrancisco Jose Blanco is a structural biologist working as Group Leader at the Centro de investigación Cooperativa en biociencias CIC bioGUNE, Biscay, Spain. His research utilizes Nuclear Magnetic Resonance spectroscopy to characterize protein structure and protein folding. His major contributions to the field of protein folding include the study of the formation and stability of isolated β-hairpin structures, and the analysis of the folding pathway of the SH3 domain from the α-spectrin protein. His 68 structural biology papers listed in Web of Science have been cited over 2500 times, giving him an h-index of 27.\n"}
{"id": "16832087", "url": "https://en.wikipedia.org/wiki?curid=16832087", "title": "Gender schema theory", "text": "Gender schema theory\n\nGender schema theory was formally introduced by Sandra Bem in 1981 as a cognitive theory to explain how individuals become gendered in society, and how sex-linked characteristics are maintained and transmitted to other members of a culture.\nGender-associated information is predominantly transmuted through society by way of schemata, or networks of information that allow for some information to be more easily assimilated than others. Bem argues that there are individual differences in the degree to which people hold these gender schemata. These differences are manifested via the degree to which individuals are sex-typed.\n\nCore gender identity is tied up in the sex typing that an individual undergoes. This typing can be heavily influenced by child rearing, media, school, and other forms of cultural transmission. Bem refers to four categories in which an individual may fall: sex-typed, cross-sex-typed, androgynous, and undifferentiated. Sex-typed individuals process and integrate information that is in line with their gender. Cross-sex-typed individuals process and integrate information that is in line with the opposite gender. Androgynous individuals process and integrate traits and information from both genders. Finally, undifferentiated individuals do not show efficient processing of sex-typed information.\n\nBeing that gender schema theory is a theory of process and not content, this theory can help explain some of the processes by which gender stereotypes become so psychologically ingrained in our society. Specifically, having strong gender schemata provides a filter through which we process incoming stimuli in the environment. This leads to an easier ability to assimilate information that is stereotype congruent, hence further solidifying the existence of gender stereotypes. Within adolescent development, Bem hypothesizes that children must choose among a plethora of dimensions, but that gender schemas lead to the regulation of behaviors that conform to the cultural definition of what it means to be male or female. Additionally, Bem asserts that there is also a heterosexuality subschema, which likely encouraged the development of gender schemas. Most societies treat exclusive heterosexuality as the benchmark for proper masculinity and femininity—that is, heterosexuality is the norm. Furthermore, the heterosexuality subschema asserts that men and women are supposed to be different from one another. It is hypothesized that this is why cross-sexed interactions are likely to be sexually coded. Sex-typed individuals have a general readiness to invoke the heterosexuality subschema in social interactions, behaving differently towards individuals of the opposite sex that they find attractive v. unattractive.\n\nSome of the early tests of gender schema theory came in the form of memory and other cognitive tasks designed to assess facilitated processing of sex-typed information. Much of this early research found that participants who were sex-typed remembered more traits associated with their sex, as well as processed sex-type congruent information more efficiently, suggesting that the gender schemata possessed by sex-typed individuals help to assimilate sex-associated information into one’s self-concept (see Bem, 1981).\nBem showed that when given the option of clustering words by either semantic meaning or gender, sex-typed individuals are more likely to use the gender clustering system, followed by undifferentiated individuals. Cross-typed individuals had the lowest percentage of words clustered by gender.\n\nA strong source of sex-typing comes from the rearing practices of parents. Bem offers strong suggestions for preventing the sex-typing of children, including the prevention of access to media that promotes sex-typing, altering media and stories to eliminate sex-typing information, and modeling equal roles for mothers and fathers in the household.\nFor example, Bem edited the books that her children read to create a more androgynous view. This included, for example, drawing long hair and feminine body characteristics on male figures. Ultimately, however, this is somewhat limited because children will become exposed to some of this sex-typing information, particularly when they begin attending school. Therefore, Bem suggests teaching alternative schemata to children so that they are less likely to build and maintain a gender schema. Some examples include an individual differences schema, where children learn to process information on a person-by-person basis rather than make wide assumptions about groups based on information from individuals. Also, providing children with a sexism schema, where children learn to process sex-typed information through a filter that promotes moral outrage when sexist information is being promoted, can assist in providing children with the resources to not only keep from becoming sex-typed but also promote positive social change.\n\nBem wished to raise consciousness that the male/female dichotomy is used as an organizing framework, often unnecessarily, especially in the school curriculum. She stressed that the omnirelevance of gender has a negative impact on society, and that the gender schema should be more limited in scope. Within the feminist lens, androgyny is not radical enough, because androgyny means that “masculine” and “feminine” still exist. Rather, society should decrease the use of the gender dichotomy as a functional unit, and be aschematic.\n\nThe legacy of gender schema theory has not been one of obvious lasting impact in the psychology of gender. Bem's theory was undoubtedly informed by the cognitive revolution of the 1970s and 1980s and was coming at a time when the psychology of gender was drastically picking up interest as more and more women were entering academic fields. While gender schema theory does provide a cognitive backbone for how gender stereotypes may continue to be maintained in current society, it lost wind as more broad sociological theories became the dominant force in the psychology of gender. A major limitation of gender schema theory has been that once research supported the nature of the process, there was little work that followed.\n\nThe longest-lasting contribution to the field has been the Bem Sex-Role Inventory. Originally developed as a tool to identify sex-typed individuals, many researchers use the measure to look at other components of gender, including endorsement of gender stereotypes and as a measure of masculinity/femininity. Caution should be employed when examining research that uses the Bem Sex-Role Inventory for measuring constructs that it was not created to measure.\n\nBem herself admitted that she was ill-prepared to develop the Bem Sex-Role Inventory and never anticipated it being as widely used as it still is today.\n"}
{"id": "16773915", "url": "https://en.wikipedia.org/wiki?curid=16773915", "title": "Gula (crater)", "text": "Gula (crater)\n\nGula is a crater on Ganymede. It is a fresh crater with a distinctive central peak. It is about 40 km (25 miles) in diameter.\n\nA characteristic feature of both Gula and its southern neighbor Achelous, almost identical in size, is the \"pedestal\" − an outward-facing, relatively gently sloped scarp that terminates the continuous ejecta blanket. Similar features may be seen in ejecta blankets of Martian craters, suggesting impacts into a volatile (ice)-rich target material. Furthermore, both craters appear crisp and feature terraces. Gula has a prominent central peak; Achelous instead may show the remnant of a collapsed central peak or a central pit that is not fully formed. On lower-resolution images taken under higher sun illumination angle, both craters are shown to have extended bright rays, especially Achelous, which demonstrates that these two craters are younger than the respective surrounding landscape.\n"}
{"id": "16695722", "url": "https://en.wikipedia.org/wiki?curid=16695722", "title": "Heinrich Hlasiwetz", "text": "Heinrich Hlasiwetz\n\nHeinrich Hlasiwetz (April 7, 1825 – October 7, 1875) was an Austrian chemist born in Reichenberg, Bohemia.\n\nSon of a pharmacist, he studied at the University of Jena, where his instructors included Johann Wolfgang Döbereiner (1780-1849), Heinrich Wilhelm Ferdinand Wackenroder (1798-1854) and Matthias Jakob Schleiden (1804-1881). Later he studied under Josef Redtenbacher (1810–1870) in Prague. In 1848 he earned the diploma of \"Magister Pharmacia\", and during the following year received his doctorate in chemistry.\n\nIn 1849 he began work as an assistant to Friedrich Rochleder (1819-1874), later becoming an associate professor of chemistry at the University of Innsbruck (1854). In 1867 he became a professor at the Vienna University of Technology, where from 1869 he represented general and analytical chemistry.\n\nDuring his career he largely worked with resins, tannins and protein compounds. Hlasiwetz is remembered for his chemical analysis of quercitrin, phloroglucinol, resorcinol and creosote.\n\n\n\n"}
{"id": "49755471", "url": "https://en.wikipedia.org/wiki?curid=49755471", "title": "Honeaite", "text": "Honeaite\n\nHoneaite is a rare gold thallium telluride mineral with the formula AuTlTe. It was discovered in the Karonie mine, Cowarna Downs Station, Western Australia, although this is not the only locality for the mineral. \n\nHoneaite is structurally and chemically unique.\n"}
{"id": "34737952", "url": "https://en.wikipedia.org/wiki?curid=34737952", "title": "How to Grow a Planet", "text": "How to Grow a Planet\n\nHow to Grow a Planet is a 2012 television nature documentary series produced by the BBC on and originally broadcast on BBC Two. It is presented by Professor Iain Stewart and ran for three 60 minute episodes in February 2012.\n\nA single-disc DVD set of the series was released on 16 April 2012.\n\n"}
{"id": "15503387", "url": "https://en.wikipedia.org/wiki?curid=15503387", "title": "Index of earth science articles", "text": "Index of earth science articles\n\nEarth science (also known as geoscience, the geosciences or the Earth Sciences), is an all-embracing term for the sciences related to the planet Earth. It is arguably a special case in planetary science, the Earth being the only known life-bearing planet. There are both reductionist and holistic approaches to Earth science. There are four major disciplines in earth sciences, namely geography, geology, geophysics and geodesy. These major disciplines use physics, chemistry, biology, chronology and mathematics to build a quantitative understanding of the principal areas or \"spheres\" of the Earth system.\n\nArticles related to earth science include:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "11972650", "url": "https://en.wikipedia.org/wiki?curid=11972650", "title": "Jean-Étienne Girard", "text": "Jean-Étienne Girard\n\nMaurice Jean Auguste Girard (13 September 1822 – 8 September 1886) was a French entomologist.\n\nGirard was born in Givet, Ardennes, and entered École normale supérieure in 1844. In 1847 he taught physics in Périgueux. After having obtained his \"agrégation\", he left for Dijon where he taught from 1853 to 1873. during this time he obtained his “licence” and his “doctorat ès-sciences naturelles” with a thesis entitled Étude sur la chaleur libre dégagée par les animaux invertébrés et spécialement les insectes.\n\nHe edited L'Insectologie agricole, journal traitant des insectes utiles... et des insectes nuisibles... from 1867 to 1870. He wrote more than 200 publications on insects and a book on François Péron- François Péron, naturaliste voyageur aux australes (1800-1804) (J.-B. Baillière, Paris, 1856)\n\nHe was president of Société entomologique de France in 1867. He died at Lion-sur-Mer, aged 63.\n\n\nSource\nJean Lhoste (1987). \"Les Entomologistes français\". 1750-1950. INRA Éditions : 351 p.\nTranslation from French Wikipedia\n"}
{"id": "51053792", "url": "https://en.wikipedia.org/wiki?curid=51053792", "title": "John Sappington", "text": "John Sappington\n\nJohn S. Sappington (1776-1856) was an American physician known for creating a quinine pill to treat malaria in the Missouri area. He also wrote \"The Theory and Treatment of Fevers,\" the first medical treatise published west of the Mississippi River.\n\nJohn Sappington was born in 1776 to Dr. Mark and Rebecca Sappington in Havre de Grace, Maryland. He studied under the guidance of his father, who taught medicine at the University of Pennsylvania, and later moved to Tennessee to practice medicine independently. He married Jane Breathitt. The couple had nine children together, two boys and seven girls. While living in Franklin, Tennessee, Sappington became close friends with Thomas Hart Benton, an important political figure. In 1819, following the advice of Benton, Sappington and his family moved to Arrow Rock, Missouri.\n\nJohn Sappington provided medical services, was a financial lender, and imported and exported goods to the Missouri area. He established two stores near Arrow Rock that sold goods, loaned money, processed salt, and milled lumber. Once he had achieved financial success, Sappington was able to be more experimental with his medical practice. He focused his energy on the bark of the cinchona tree, the substance used to create quinine. Malaria, scarlet fever, yellow fever, and influenza, were prominent along the Missouri and Mississippi rivers. Sappington developed a preventative pill using quinine that was soon in demand across the country. It was marketed as an anti-fever pill, but Sappington also instructed some of his relatives to take the pills to prevent malaria. Most physicians were still treating malaria by bloodletting the patient and administering calomel. Sappington’s pill to prevent malaria—and also used to cure malaria—was controversial due to its novelty and unfamiliarity. The pill remained in high demand, however, and many other physicians began to develop their own anti-malaria pills after Sappington published the formula in his medical treatise, “Theory and Treatment of Fevers.” He is often regarded as the first physician to successfully and effectively use quinine to treat malaria.\n"}
{"id": "16565", "url": "https://en.wikipedia.org/wiki?curid=16565", "title": "Jones calculus", "text": "Jones calculus\n\nIn optics, polarized light can be described using the Jones calculus, discovered by R. C. Jones in 1941. Polarized light is represented by a Jones vector, and linear optical elements are represented by \"Jones matrices\". When light crosses an optical element the resulting polarization of the emerging light is found by taking the product of the Jones matrix of the optical element and the Jones vector of the incident light.\nNote that Jones calculus is only applicable to light that is already fully polarized. Light which is randomly polarized, partially polarized, or incoherent must be treated using Mueller calculus.\n\nThe Jones vector describes the polarization of light in free space or another homogeneous isotropic non-attenuating medium, where the light can be properly described as transverse waves. Suppose that a monochromatic plane wave of light is travelling in the positive \"z\"-direction, with angular frequency \"ω\" and wavevector k = (0,0,\"k\"), where the wavenumber \"k\" = \"ω\"/\"c\". Then the electric and magnetic fields E and H are orthogonal to k at each point; they both lie in the plane \"transverse\" to the direction of motion. Furthermore, H is determined from E by 90-degree rotation and a fixed multiplier depending on the wave impedance of the medium. So the polarization of the light can be determined by studying E. The complex amplitude of E is written \nNote that the physical E field is the real part of this vector; the complex multiplier serves up the phase information. Here formula_2 is the imaginary unit with formula_3.\n\nThe Jones vector is then\n\nThus, the Jones vector represents the amplitude and phase of the electric field in the \"x\" and \"y\" directions.\n\nThe sum of the squares of the absolute values of the two components of Jones vectors is proportional to the intensity of light. It is common to normalize it to 1 at the starting point of calculation for simplification. It is also common to constrain the first component of the Jones vectors to be a real number. This discards the overall phase information that would be needed for calculation of interference with other beams.\n\nNote that all Jones vectors and matrices on this article employ the convention that the phase of the light wave is given by formula_5, a convention used by Hecht. Under this convention, increase in formula_6 (or formula_7) indicates retardation (delay) in phase, while decrease indicates advance in phase. For example, a Jones vectors component of formula_8 (formula_9) indicates retardation by formula_10 (or 90 degree) compared to 1 (formula_11). Circular polarisation described under Jones' convention is called : \"From the point of view of the receiver\". Collett uses the opposite definition for the phase (formula_12). Circular polarisation described under Collett's convention is called : \"From the point of view of the source\". The reader should be wary of the choice of convention when consulting references on the Jones calculus.\n\nThe following table gives the 6 common examples of normalized Jones vectors.\n\nA general vector that points to any place on the surface is written as a ket formula_13. When employing the Poincaré sphere (also known as the Bloch sphere), the basis kets (formula_14 and formula_15) must be assigned to opposing (antipodal) pairs of the kets listed above. For example, one might assign formula_14 = formula_17 and formula_15 = formula_19. These assignments are arbitrary. Opposing pairs are\n\n\nThe polarization of any point not equal to formula_24 or formula_25 and not on the circle that passes through formula_28 is known as elliptical polarization.\n\nThe Jones matrices are operators that act on the Jones vectors defined above. These matrices are implemented by various optical elements such as lenses, beam splitters, mirrors, etc. Each matrix represents projection onto a one-dimensional complex subspace of the Jones vectors. The following table gives examples of Jones matrices for polarizers:\n\nPhase retarders introduce a phase shift between the vertical and horizontal component of the field and thus change the polarization of the beam. Phase retarders are usually made out of birefringent uniaxial crystals such as calcite, MgF or quartz. Uniaxial crystals have one crystal axis that is different from the other two crystal axes (i.e., \"n\" ≠ \"n\" = \"n\"). This unique axis is called the extraordinary axis and is also referred to as the optic axis. An optic axis can be the fast or the slow axis for the crystal depending on the crystal at hand. Light travels with a higher phase velocity along an axis that has the smallest refractive index and this axis is called the fast axis. Similarly, an axis which has the largest refractive index is called a slow axis since the phase velocity of light is the lowest along this axis. \"Negative\" uniaxial crystals (e.g., calcite CaCO, sapphire AlO) have \"n\" < \"n\" so for these crystals, the extraordinary axis (optic axis) is the fast axis, whereas for \"positive\" uniaxial crystals (e.g., quartz SiO, magnesium fluoride MgF, rutile TiO), \"n\" > \"n \" and thus the extraordinary axis (optic axis) is the slow axis.\n\nAny phase retarder with fast axis equal to the x- or y-axis has zero off-diagonal terms and thus can be conveniently expressed as \n\nwhere formula_6 and formula_7 are the phase offsets of the electric fields in formula_32 and formula_33 directions respectively. In the phase convention formula_5, define the relative phase between the two waves as formula_35. Then a positive formula_36 (i.e. formula_7 > formula_6) means that formula_39 doesn't attain the same value as formula_40 until a later time, i.e. formula_40 leads formula_39. Similarly, if formula_43, then formula_39 leads formula_40.\n\nFor example, if the fast axis of a quarter wave plate is horizontal, then the phase velocity along the horizontal direction is ahead of the vertical direction i.e., formula_40 leads formula_39. Thus, formula_48 which for a quarter wave plate yields formula_49.\n\nIn the opposite convention formula_12, define the relative phase as formula_51. Then formula_52 means that formula_39 doesn't attain the same value as formula_40 until a later time, i.e. formula_55 leads formula_39.\n\nThe special expressions for the phase retarders can be obtained by taking suitable parameter values in the general expression for a birefringent material. In the general expression:\n\nNote that for linear retarders, formula_59 = 0 and for circular retarders, formula_59 = ± formula_62/2, formula_58 = formula_62/4. In general for elliptical retarders, formula_59 takes on values between - formula_62/2 and formula_62/2.\n\nAssume an optical element has its optic axis perpendicular to the surface vector for the plane of incidence and is rotated about this surface vector by angle \"θ/2\" (i.e., the principal plane, through which the optic axis passes, makes angle \"θ/2\" with respect to the plane of polarization of the electric field of the incident TE wave). Recall that a half-wave plate rotates polarization as \"twice\" the angle between incident polarization and optic axis (principal plane). Therefore, the Jones matrix for the rotated polarization state, M(\"θ\"), is \nThis agrees with the expression for a half-wave plate in the table above. These rotations are identical to beam unitary splitter transformation in optical physics given by\nwhere the primed and unprimed coefficients represent beams incident from opposite sides of the beam splitter. The reflected and transmitted components acquire a phase \"θ\" and \"θ\", respectively. The requirements for a valid representation of the element are \nand\nformula_72\n\nThis would involve a three-dimensional rotation matrix. See Russell A. Chipman and Garam Yun for work done on this.\n\n\n\n"}
{"id": "7842013", "url": "https://en.wikipedia.org/wiki?curid=7842013", "title": "Lineage (anthropology)", "text": "Lineage (anthropology)\n\nA lineage is an unilineal descent group that can demonstrate their common descent from a known apical ancestor. Unilineal lineages can be matrilineal or patrilineal, depending on whether they are traced through mothers or fathers, respectively. Whether matrilineal or patrilineal descent is considered most significant differs from culture to culture.\n"}
{"id": "6329302", "url": "https://en.wikipedia.org/wiki?curid=6329302", "title": "List of FTP commands", "text": "List of FTP commands\n\nBelow is a list of FTP commands that may be sent to an FTP server, including all commands that are standardized in RFC 959 by the IETF. All commands below are RFC 959 based unless stated otherwise. Note that most command-line FTP clients present their own set of commands to users. For example, GET is the common user command to download a file instead of the raw command RETR.\n\n\n"}
{"id": "7475661", "url": "https://en.wikipedia.org/wiki?curid=7475661", "title": "List of Nintendo DS colors and styles", "text": "List of Nintendo DS colors and styles\n\nThis is a list of case colors and styles that have been produced for the Nintendo DS line of handheld systems.\n\n\n\n\n\n\n\nStandard Colors\n\nLimited Edition Colors\n\n\nStandard Colors\n\nLimited Editions\n\n"}
{"id": "6331838", "url": "https://en.wikipedia.org/wiki?curid=6331838", "title": "List of biodiversity databases", "text": "List of biodiversity databases\n\nThis is a list of biodiversity databases. Biodiversity databases store taxonomic information which provide information on the biodiversity of a particular area or group of living organisms. They may store specimen-level information, species-level information, information on nomenclature, or any combination of the above. Most are available online.\n\nSpecimen-focused databases contain data about individual specimens, as represented by vouchered museum specimens, collections of specimen photographs, data on field-based specimen observations and morphological or genetic data. Species-focused databases contain information summarised at the species-level. Some species-focused databases attempt to compile comprehensive data about particular species (FishBase), while others focus on particular species attributes, such as checklists of species in a given area (FEOW) or the conservation status of species (CITES or IUCN Red List). Nomenclators act as summaries of taxonomic revisions and set a key between specimen-focused and species-focused databases. They do this because taxonomic revisions use specimen data to determine species limits.\n\n\n"}
{"id": "46259599", "url": "https://en.wikipedia.org/wiki?curid=46259599", "title": "List of computers with on-board BASIC", "text": "List of computers with on-board BASIC\n\nThis is a list of computers with on-board BASIC. They shipped standard with a version of BASIC that was installed in the computer. The computers can access the BASIC language without the user inserting cartridges or loading software from external media.\n\nBASICs with Bitwise Ops use -1 as true and the AND and OR operators perform a bitwise operation on the arguments.\n\ncodice_1/codice_2 skip means that body of the loop is skipped if the initial value of the loop times the sign of the step exceeds the final value times the sign of the step (such as codice_3 or codice_4). The statements inside the codice_1/codice_2 loop will not be executed at all.\n\nNumeric support indicates if a BASIC supports Integers and/or Floating Point.\n\nVariable Name Length is how many characters of a variable name are used to determine uniqueness.\n\nFull tokenization means that all keywords are converted to tokens and all extra space characters are removed. Partial tokenization leaves extra space characters in the source. None means that no tokenization is done. How to test for full tokenization:\n10 PRINT \"HELLO\"\nLIST\nIf it is fully tokenized it should return 10 PRINT \"HELLO\" without all the extra spaces that were entered.\n"}
{"id": "24673259", "url": "https://en.wikipedia.org/wiki?curid=24673259", "title": "List of people diagnosed with colorectal cancer", "text": "List of people diagnosed with colorectal cancer\n\nThis article lists notable people who were or have been diagnosed with colorectal cancer.\n\n\n\n"}
{"id": "9505941", "url": "https://en.wikipedia.org/wiki?curid=9505941", "title": "List of quantum-mechanical systems with analytical solutions", "text": "List of quantum-mechanical systems with analytical solutions\n\nMuch insight in quantum mechanics can be gained from understanding the closed-form solutions to the time-dependent non-relativistic Schrödinger equation in an appropriate configuration space. In vector Cartesian coordinates formula_1, the equation takes the form\n\nin which formula_3 is the wavefunction of the system, H is the Hamiltonian operator, and T and V are the operators for the kinetic energy and potential energy, respectively. (Common forms of these operators appear in the square brackets.) The quantity \"t\" is the time. Stationary states of this equation are found by solving the eigenvalue-eigenfunction (time-independent) form of the Schrödinger equation,\n\nor any equivalent formulation of this equation in a different coordinate system other than Cartesian coordinates. For example, systems with spherical symmetry are simplified when expressed with spherical coordinates. Very often, only numerical solutions to the Schrödinger equation can be found for a given physical system and its associated potential energy. However, there exists a subset of physical systems for which the form of the eigenfunctions and their associated energies can be found. These quantum-mechanical systems with analytical solutions are listed below.\n\n\n"}
{"id": "53926086", "url": "https://en.wikipedia.org/wiki?curid=53926086", "title": "List of things named after Nikola Tesla", "text": "List of things named after Nikola Tesla\n\nThis article is a list of things named after Nikola Tesla, an influential physicist, engineer and inventor.\n\n\n\n\n\n\n\n\n\n\n- Nikola Tesla in popular culture\n"}
{"id": "23758380", "url": "https://en.wikipedia.org/wiki?curid=23758380", "title": "List of volcanic craters in Alaska", "text": "List of volcanic craters in Alaska\n\nThe United States National Geodetic Survey lists thirteen craters in the state of Alaska.\n\n\n\n\n\n\n\n\n"}
{"id": "3416062", "url": "https://en.wikipedia.org/wiki?curid=3416062", "title": "Malayan water shrew", "text": "Malayan water shrew\n\nThe Malayan water shrew (\"Chimarrogale hantu\"), also known as the hantu water shrew, is a red-toothed shrew recorded only from the Malaysian state of Selangor. It was listed as a critically endangered, but is now considered near threatened.\n\nIt gets its scientific name \"hantu\" from the Malay word for ghost.\n\nThe Malayan water shrew has a white underside, a black coat along its top and sides and a fringe of bristles running along the surface of the tail and on the paws which act as swimming aids. The teeth have red tips. The Malayan water shrew can grow up to about 10 cm in height and 20 cm in length.\n\nThe Malayan water shrew lives in the Tropical Rainforests of Peninsula Malaysia. It lives mainly by fresh water lakes and rivers surrounded by vegetation and spends much of its time underwater. Underwater this shrew likes to stay in leafy areas to avoid predators and surprise its prey, which include fish, frogs and plants.\n\n"}
{"id": "1976319", "url": "https://en.wikipedia.org/wiki?curid=1976319", "title": "Masoumeh Ebtekar", "text": "Masoumeh Ebtekar\n\nMasoumeh Ebtekar (; born Masoumeh, Niloufar Ebtekar; 21 September 1960) is current Vice President of Iran for Women and Family Affairs, being appointed on 9 August 2017. She previously headed Department of Environment from 1997 to 2005, making her the first female member in the cabinet of Iran since 1979 and the third in history. She held the same level of office from 2013 to 2017.\n\nEbtekar first achieved fame as \"Mary\", the spokesperson of the students who took hostages and occupied the US Embassy in 1979. Later she became the head of the Environment Protection Organization of Iran during the administration of President Mohammad Khatami, and was a city councilwoman of Tehran from 2007 to 2013.\n\nEbtekar was born in Tehran as Niloufar Ebtekar in a middle-class family. Her first name translates to \"Innocent Water Lily\" in English. Ebtekar's father studied at the University of Pennsylvania, and she lived with her parents in Upper Darby, Pennsylvania, a western suburb of Philadelphia. During her six years in Philadelphia, she developed \"near-perfect, American-accented English.\" Returning to Iran she enrolled in Iranzamin (Tehran International School). Later after graduation as a student, she became a supporter of the political Islam of Ali Shariati and began wearing a traditional black chador covering everything except her face.\n\nEbtekar holds a BSc degree in laboratory science from Shahid Beheshti University, a MSc and PhD in immunology from Tarbiat Modares University in 1995, where she still teaches. Ebtekar is married to Seyyed Mohammad Hashemi who is a businessman in the private sector. They have two children.\n\nEbtekar has served as faculty member at Tarbiat Modares University, which is a post graduate academic center located in Tehran. As an Associate Professor in Immunology, she has taught, supervised and advised PhD and MSc students. Ebtekar currently teaches cytokines, viral immunology, HIV vaccines, aging, immunology of the nervous system and psychoneuroimmunology. She has currently filed 66 ISI scientific articles in the field of immunology in Scopus in her name. In her speech to the Eleventh International Congress of Immunology in Tehran. She mentioned the detrimental effect of sanctions on the advancement of science in Iran and noted that sanctions should not be directed against nations. Ebtekar is a member of several research board committees and a reviewer for two international and four national immunology journals.\n\nOn 7 October 2008, eTBLAST, a text similarity search engine on MEDLINE database, noted that 85% of a paper published by Masoumeh Ebtekar came from several previously published articles. The paper, on cytokines and air pollution, was published in 2006 in the \"Iran Journal of Allergy Asthma Immunology\" (IJAAI) 5 47-56:2006. A couple weeks after the eTBLAST report, Nature magazine covered the story, quoting one of the authors of original papers, (Ian Mudway, a toxicologist at the King's College London) as saying, \"the article is a veritable patchwork of other people's work, word for word, grammatical error for grammatical error.\" \"Nature\" also stated that Ebtekar had not replied to its emails. In response, the editor-in-chief of the IJAAI issued a statement saying: \"We regret for this duplication that appeared in the journal. We are working with the editors of the JACI journal [the \"Journal of Allergy and Clinical Immunology\", a scholarly periodical that published three of the papers from which Ebtekar had copied] to find the best solution in this regard.\" In December 2008 Ebtekar's article was retracted.\n\nThe issue received some political and public attention in Iran. Ebtekar issued a statement admitting she had made a mistake and apologizing for it, but including a list of complaints such as eTBLAST's failure to inform her of their finding in advance, the fact that the article was a review article she was invited to write for the Journal, and that more than 76 references were given in the text.\nIn 2013 Ebtekar was elected as the President of the 12th International Congress of Immunology. The Congress was held on April 29, 2014. Ebtekar spoke in the opening ceremony and introduced Rolf Zinkernagel, the Nobel Laureate for Medicine, as the guest of honour.\n\nIn 1981, Ebtekar became the editor-in-chief of the English daily newspaper \"Kayhan International\", selected by Khatami who was then the representative of Ayatollah Khomeini in Kayhan Institute. She served in the newspaper until 1983. In 1991 she co-founded the Institute for Women's Studies and Research. Since 1992, she has been the license holder and managing director of the journal \"Farzaneh Journal for Women's Studies and Research\". Ebtekar was appointed as the Head of Women's NGO Coordinating Office and Vice Head of the National Committee to the Fourth World Conference on Women in Beijing in 1995. Later, she was elected as the President of the Network of Women's NGOs in Iran.\n\nEbtekar served as spokesperson for the students in the Iran hostage crisis of 1979, where Muslim Student Followers of the Imam's Line occupied the US Embassy and held 52 Americans hostage for 444 days. Selected because of her good command of English, she made regular appearances on American television as translator and spokesperson for the students, where she presented the official positions of the students. She was referred to as \"Mary\" by foreign press, and \"Tiger Lily\" by the hostages, a play on the translation of \"Niloufar\".\n\nEbtekar wrote an account of the embassy takeover with Fred A. Reed entitled \"Takeover in Tehran: The Inside Story of the 1979 U.S. Embassy Capture\". Western media have systematically depicted Ebtekar's involvement in a negative manner, as Reed describes: \"For twenty years the prevailing \"globalized\" version of the embassy capture has cast the students at best as well-intentioned but naive young people manipulated ...and at worst as irresponsible extremists.\" Elaine Sciolino wrote about Ebtekar's own viewpoint: \"Asked by an \"ABC News\" correspondent one day whether she could see herself picking up a gun and killing the hostages, she replied: 'yes. When I've seen an American gun being lifted up and killing my brothers and sisters in the streets, of course.'\"\nShe is said to be remembered by many Americans (hostages such as David Roeder, Barbara Timm, the mother of hostage Kevin Hermening and those who watched her on television) with a great lack of fondness, in part because \"her familiarity with America added profound emphasis to her rejection of it.\"\nWhen asked by an American interviewer (Elaine Sciolino) in the late 1990s about her past as spokesperson for the hostage-takers, why it did not appear on her resume, and why she had changed her name from Niloufar to Masoumeh, Ebtekar \"had no apology and made no excuses\" about her role, describing the hostage taking as \"the best direction that could have been taken\" by Iran at the time, but surprised the interviewer with her \"chutzpah\", insisting that the interviewer \"not write much about these things.\" Sciolino published this article in the \"New York Times\" unaware of the fact that Ebtekar's book (Takeover in Tehran) was in print and would be published in 2001.\n\nShe held office as the duputy to Shahla Habibi, head of the 'Bureau of Women's Affairs' under administration Akbar Hashemi Rafsanjani in the 1990s, and was reportedly the \"main driving-force\" behind the office.\n\nIn the 2012 film \"Argo\", Ebtekar was portrayed by Nikka Far and called only \"Tehran Mary\" in the credits.\n\nEbtekar was the first woman to serve as Vice-President of Iran when the reformists came to power. Along with Zahra Shojaei, she participated in the first cabinet since the Islamic Revolution to include women. She has been described as a leftist in Mohammad Khatami's alliance. Ebtekar headed the Department of Environment for eight years, introducing major structural, organizational and directional changes enabling a re-engineering of the government body. During her tenure environmental awareness and support for civil society activism in this area was enhanced.\n\nEbtekar's appointment led to the revelation of her past, and in the US questions were raised about whether President Khatami was aware of \"how deeply\" the hostage-taking and holding, and anger towards its foremost public defender, \"affected both the American government and the American people.\" Many academics and literary critiques have written and expressed their views on her published memoirs. Following this, \"some ambassadors\" in Tehran reportedly stated they would no \"longer meet with her\" and would \"discourage official contract with her office.\" In her memoirs published as the \"Grapes of Shahrivar\", Ebtekar repeatedly refers to her cordial and official contacts with not only Western Ambassadors, but also many European Ministers and Presidents as well. (chapters 8-16-23-26-30-32-33)\n\nOn International Women's Day in 1998, as vice-president of environmental affairs, she made a speech condemning the oppression of women by the Taliban Movement in Afghanistan. Her performance caused comment by members of the Western news media in attendance as she herself was wearing a chador, a reminder of compulsory hijab in Iran which many in the West view as a violation of women's rights.\n\nIn May 1999, the \"WorldNetDaily\" claimed that she and President Mohammad Khatami had been in the guest list of the 1999 Bilderberg conference held in Sintra, Portugal on June 3–6. This allegation was repeatedly denied by Ebtekar citing the fact that President Khatami had taken part in memorial ceremonies for Imam Khomeini on those days (3–6 of June) and on World Environment Day June 6 he had inaugurated the Provincial Center for Environment Research in Tehran.\n\nIn March 2002, Ebtekar was a keynote speaker at the Meeting of Women Leaders on the Environment in Helsinki, sponsored by the Finnish Ministry of the Environment. In September 2002, Ebtekar participated in the World Summit on Sustainable Development, held in Johannesburg, South Africa. In May 2005, she chaired the International Conference on Environment, Peace, and the Dialogue Among Civilizations and Cultures, held in Tehran. This event was organized by Ebtekar's Department of the Environment and also by the United Nations Environment Programme.\n\nEbtekar served as Vice President and Head of the Department of Environment during the first term of President Rouhani, 2013-2017. During this period in spite of immense challenges facing Iran's environment, and irrespective of the fierce opposition of rival political and special interests groups, great strides were taken to enhance environmental governance and stewardship. Twelve environmental bills, a strong chapter in the sixth National Development Plan and numerous directives and guidelines were implemented during this period, while national plans to enhance air quality were promoted through strong cross-sectoral, management leading to better air quality trends.\n\nThe national Low Carbon Economy Strategy was adopted; the Cabinet adopted its INDC and the Paris Agreement, which the Parliament also adopted. This led to important strides in renewable energy including more than 500 mw of new solar plants and much more underway. Restoration of wetlands according to ecosystem management schemes and local community participation was undertaken leading to revival of Houralazim wetland, designation of Hamoun as a Biosphere reserve and improvement of conditions in Urmia Lake and many other wetlands across Iran.\n\nA major campaign to enhance environment education, inclusion of 56 cases of environment related text and publication of the first textbook \"Humans and the Environment\" are some of the steps taken in this regard.\nA significant surge in the quantity and quality of civil society activism was the result of planning for enhanced nongovernmental organization participation in policy and oversight. During this term private sector involvement in the management of protected areas and managing wildlife conservation was promoted through legislation and incentives. Introducing local legislation for management schemes of national parks and protected areas and for wildlife and endangered species conservation schemes for the first time. Nature Rangers enjoyed high quality training and education programs while awareness campaigns led to improvements in their social status. International diplomacy in collaboration with UNEP, UNDP, JICA, UNESCO led to many joint projects including participatory training for local farmers and villagers in Urumia basin area, Conservation of the Cheetah, Second International Seminar on Environment Religion and Culture, First UN Conference on Combating Dust and Sand Storms, Project for Restoration of Anzali Wetland, Project for Coastal Zone Management Projects and many others.\n\nThe fifth international Green Film festival was revived, four rounds of International Environment Exhibition, four rounds of Green Industry Competition and Four Rounds of the National Environment Award was convened during this term. 20 Bilateral and multilateral MOUs were signed and implemented leading to more than 50 technical and educational workshops and sessions. Nationwide campaigns on \"no to plastics\", \"reduction of waste\", \"and energy conservation\", \"no to food waste\" and many others were supported and promoted. The National Committee on Sustainable Development was transformed into an efficient cross sectoral body to promote and monitor sustainability indicators at the national level.\n\nEbtekar was named one of the seven 2006 Champions of the Earth by the United Nations Environment Program as a prominent and \"inspirational\" environmental leader who has made an impact at policy level in a region of the world. Ebtekar said that she believes the award was a team effort, earned by the scholars and experts that she assembled in her Department of the Environment. She names President Khatami as instrumental in stressing the importance of environmental initiatives. She was also named as one of 50 environmental leaders by \"The Guardian\" newspaper on January 5, 2008; the only Iranian or Muslim woman in the list.\n\nIn the 2012 edition of The Muslim500 Ebtekar has been named as one of the 500 most influential Muslims in the world. Under the political section of this yearbook, Ebtekar is described as \"a considerable force in the reformist movement in Iran\".\n\nOn 24 January 2014, Ebtekar was awarded the Energy Globe Foundation Honorary Lifetime Achievement Award in Tehran.\n\nOn 29 November 2014, Masoumeh Ebtekar won the Italian Minerva Award for her scientific achievements and successful career in political arena. Minerva Award is a nongovernmental award established in 2009 in the name of Madame Anna Maria Mammoliti, Italian journalist and social activist after her death.\nThe Minerva Award has been presented to famous figures active in different social, economic, political and cultural fields, and the receivers have been mainly women.\n\nOn May 2016 Dr. Ebtekar was awarded an Honorary Doctorate Degree in Political Science by the Hankuk University of Foreign Studies, Seoul, Korea.\n\nEbtekar co-founded the Center for Peace and Environment in 2005, a non-governmental organization devoted to the promotion of just and sustainable peace and the protection of the environment. More than 120 experts and academicians are currently members of the Center.\n\nEbtekar served as a moderator in June 2008 at the International \"Women, Equality and Peace\" Conference held in Oslo, Norway. The conference was sponsored by the Foundation for Dialogue Among Civilizations, Club de Madrid and the Oslo Center.\n\nEbtekar considered for running in the 2009 presidential election after Guardian Council indicated that there is no \"legal restraint\" against women doing so. However, she withdrew a few weeks before the election.\n\nEbtekar published her memoir as the first female Vice President of Iran, entitled the \"Grapes of Shahrivar\" on May 3, 2009. She has also published a collection of her essays and speeches on the environment and sustainable development, called \"Natural Peace\". After leaving her government position in 2005, Ebtekar has spoken as inaugural or keynote speaker at many international events.\n\nEbtekar ran for and was elected to the city council of Tehran for the term beginning in 2007, coming in 9th out of 21 candidates, just after Parvin Ahmadinejad, the sister of the Mahmoud Ahmadinejad. She established and heads the Tehran City Council Environment Committee and currently runs 20 working groups on environmental issues.\n\nAfter election to the City Council of Tehran in early 2007, Ebtekar began a weblog in Persian entitled \"EbtekareSabz\" under the free blog service Persianblog. In her blog she wrote 430 posts in environmental, political, social and women's issues, posted over 10,000 comments, the blog had one million viewers in 3.5 years. \"EbtekareSabz\", which criticized the policies of the government and supported the reformist movement in Iran, was filtered by the Government once in early 2010 and again in June 2010. and finally obstructed with a judicial verdict in August. Citing the \"right to freedom of expression in our constitution\" as her incentive, she continued blogging by setting up a new blog.\n\nIn 2009, the \"New York Times\" described her as \"informally represent[ing] the views of many of the former hostage-takers\", supporting \"engagement with the West\" and a renewal of the \"original ideals of the revolution, including justice and freedom,\" which many of her peers believe have been abandoned by the current regime.\n\nIn May 2013, Ebtekar signed up as a candidate for Tehran in the 2013 local elections, along with hundreds of reformist candidates. During the vetting process that is conducted by government and Majlis representatives, the majority of candidates, including Ebtekar were disqualified. Ebtekar mentioned her harsh criticism of government policies leading to air pollution and the deterioration of environmental standards as the main reason for her disqualification. Candidates who have objected to their disqualification will be considered for a final round in the Supreme Council for Election Regulation. She was also one of Reformists' candidates as Mayor of Tehran. However, Mohsen Hashemi becomes the final's nominate.\n\nAfter Hassan Rouhani elected as President of Iran, Ebtekar who supported Rouhani openly, was one of the candidates for the Ministry of Science. However, she was appointed as Head of Environmental Protection Organization on 10 September 2013, a position she had formerly served for eight years under Mohammad Khatami. On 1 August 2017, Ebtekar announced that she will be leave her current position after the end of the first Rouhani government. She is set to appoint as Women Affairs's Vice President.\n\nIn 2010 Ebtekar contributed to \"Moral Ground\" a testimony of over eighty visionaries—theologians and religious leaders, scientists, elected officials, business leaders, naturalists, activists, and writers—to present a diverse and compelling call to honor our individual and collective moral responsibility to our planet. In her essay entitled \"Peace and Sustainability Depend on the Spiritual and the Feminine\" Ebtekar provides her views on the interrelated nature of peace and sustainable development. She also took part in a project to develop a book entitled \"Women, Power and Politics in 21st Century Iran\". The book, published in 2012 by Ashgate, provides an objective perspective on the conditions of women in Iran. Chapter 10 titled \"Women and the Environment\" has been authored by Massoumeh Ebtekar. In 2011 Ebtekar also co-authored a chapter in the book \"Stem Cells and Cancer Stem Cells\" published by Springer. Chapter 3 \"Characteristics of Cord Blood Cells\" is a review performed by a team of researchers in Iran.\n\n\n"}
{"id": "14541256", "url": "https://en.wikipedia.org/wiki?curid=14541256", "title": "Maximilien Chaudoir", "text": "Maximilien Chaudoir\n\nMaximilien Chaudoir, or Maximilien, baron de Chaudoir, (12 September 1816, Ivnitsa, near Zhitomir – 6 May 1881, Amélie-les-Bains) was a Russian entomologist. He was a specialist in Coleoptera and in particular the Carabidae. His Cicindelidae are conserved by the Muséum national d'histoire naturelle in Paris. His Carabidae were acquired by Charles Oberthür (1845-1924), then given to the same museum. He wrote \"Mémoire sur la famille des Carabiques\", 6 volumes commencing 1848.\n\n"}
{"id": "585765", "url": "https://en.wikipedia.org/wiki?curid=585765", "title": "Mierscheid law", "text": "Mierscheid law\n\nThe Mierscheid law is hypothesis, published in the German magazine \"Vorwärts\" on 14 July 1983 and attributed to the fictitious politician Jakob Maria Mierscheid. It forecasts the Social Democratic Party of Germany (SPD)'s share of the popular vote based on the size of crude steel production in Western Germany:\n\nThere is a special rule for early elections. One then has to take the arithmetic mean of the regular and early year of election.\n\nThe last corroboration of the law was in the 2002 election, where the West German crude steel production was 38.6 million tonnes, and the vote share of the SPD 38.5%. For the early election in 2005 the vote share was 38.4%, with a mean crude steel value of 40.0 million tonnes. Over the last ten elections, the two values were within two units nine times, and within one unit seven times.\n\nIn the German federal elections of 18 September 2005 (which were originally due to be held in 2006), the law appeared not to hold, since the SPD obtained 34.3% of the relevant votes while the crude steel production of the \"old\" \"Länder\" (i.e. those states that belonged to the Federal Republic of Germany before reunification) in the previous year was 39.9 million tonnes.\n\nHowever, an article attributed to Jakob Mierscheid, published on the German Federal Parliament's website provided a correction to the hypothesis, to take account of the special situation. Since the elections had been brought forward, it was argued that the last months of the year should be discounted, yielding a steel production figure of 33.5 million tonnes. The graph included in the article showed a good match, thus supporting the (corrected) hypothesis. This \"ad hoc\" hypothesis alteration might be considered an example of the Texas sharpshooter fallacy.\n\nFollowing the 2005 elections, an article was published by the statistical office of the state of Baden-Württemberg attempting to further refine the model in the form of the Mierscheid-Walla law. This article also mentions promising but inconclusive attempts to replace steel production with other measures that exhibited a degree of apparent correlation, such as the export value of automobiles, employment levels in the field of legal advice, the price of coffee, and the number of accidents on town roads.\n\n"}
{"id": "1776871", "url": "https://en.wikipedia.org/wiki?curid=1776871", "title": "Miraculin", "text": "Miraculin\n\nMiraculin is a , a glycoprotein extracted from the fruit of \"Synsepalum dulcificum\". The berry, also known as the miracle fruit, was documented by explorer Chevalier des Marchais, who searched for many different fruits during a 1725 excursion to its native West Africa.\n\nMiraculin itself is not sweet. After the taste buds are exposed to miraculin (which binds to sweet receptors on the tongue), acidic foods that are ordinarily sour (such as citrus) are perceived as sweet. This effect lasts up to about an hour.\nThe active substance was named miraculin after its extraction was published in 1968.\n\nMiraculin was first sequenced in 1989 and was found to be a glycoprotein consisting of 191 amino acids and some carbohydrate chains.\n\nMiraculin occurs as a tetramer (98.4 kDa), a combination of 4 monomers group by dimer. Within each dimer 2 miraculin glycoproteins are linked by a disulfide bridge.\n\nThe molecular weight of the glycoprotein is 24.6 kDa including 3.4 kDa (13.9% of the weight) of sugar constituted (on molar ratio) of glucosamine (31%), mannose (30%), fucose (22%), xylose (10%) and galactose (7%).\n\nThe taste-modifying protein, miraculin, has seven cystein residues in a molecule composed of 191 amino acid residues. Both tetramer miraculin and native dimer miraculin in its crude state have the taste-modifying activity of turning sour tastes into sweet tastes.\n\nMiraculin, unlike curculin (another taste-modifying agent), is not sweet by itself, but it can change the perception of sourness to sweetness, even for a long period after consumption. The duration and intensity of the sweetness-modifying effect depends on various factors, such as miraculin concentration, duration of contact of the miraculin with the tongue, and acid concentration. Maximum sweet-induced response has been shown to be equivalent to the sweetness of 17% sucrose solution. Glycoprotein is also sensitive to heat. When heated over 100 °C, miraculin loses its taste-modifying property. Miraculin activity is inactivated at pH below 3 and pH above 12 at room temperature.\n\nAlthough the detailed mechanism of the taste-inducing behavior is unknown, it appears the sweet receptors are activated by acids which are related to sourness, an effect remaining until the taste buds perceive a neutral pH. Sweeteners are perceived by the human sweet taste receptor, hT1R2-hT1R3, which belongs to G protein-coupled receptors, modified by the two histidine residues (i.e. His30 and His60) which participate in the taste-modifying behavior. One site maintains the attachment of the protein to the membranes while the other (with attached xylose or arabinose) activates the sweet receptor membrane in acid solutions.\n\nAs miraculin is a readily soluble protein and relatively heat stable, it is a potential sweetener in acidic food (e.g. soft drinks). While attempts to express it in yeast and tobacco plants have failed, researchers have succeeded in preparing genetically modified \"E. coli\" bacteria, lettuce and tomatoes that express miraculin. The scientists' crops resulted in 40 micrograms of miraculin per gram of lettuce leaves, with two grams of lettuce leaves producing roughly the same amount of miraculin as in one miracle fruit berry.\n\nThe use of miraculin as a food additive was forbidden in 1974 by the United States Food and Drug Administration, in circumstances that have been interpreted as suggesting influence by competing commercial interests. Since 2011, The FDA has imposed a ban on importing \"Synsepalum dulcificum\" (specifying 'miraculin') from its origin in Taiwan, declaring it as an \"illegal undeclared sweetener\". Although this ban does not apply to fresh and freeze-dried miracle fruit, the fresh or normally-frozen berry deteriorates rapidly. The ban also does not apply to sale as a dietary supplement. There is informed opinion that the FDA ban could be overturned given sufficient funding for the required safety studies. Miraculin has a novel food status in the European Union. It is approved in Japan as a safe food additive, according to the List of Existing Food Additives published by the Ministry of Health and Welfare (published by JETRO).\n\n"}
{"id": "48547304", "url": "https://en.wikipedia.org/wiki?curid=48547304", "title": "NGC 124", "text": "NGC 124\n\nNGC 124 is a spiral galaxy in the constellation Cetus. It was discovered by Truman Henry Safford on September 23, 1867. The galaxy was described as \"very faint, large, diffuse, 2 faint stars to northwest\" by John Louis Emil Dreyer, the compiler of the New General Catalogue.\n"}
{"id": "6272900", "url": "https://en.wikipedia.org/wiki?curid=6272900", "title": "Ochrea", "text": "Ochrea\n\nAn ochrea (Latin \"ocrea\", greave or protective legging), also spelled ocrea, is a plant structure formed of stipules fused into a sheath surrounding the stem, and is typically found in the Polygonaceae. \n\nIn palms it denotes an extension of the leaf sheath beyond the petiole insertion.\n"}
{"id": "5259012", "url": "https://en.wikipedia.org/wiki?curid=5259012", "title": "On-The-Fly Calibration", "text": "On-The-Fly Calibration\n\nIn observational astronomy an On-The-Fly Calibration (OTFC) system calibrates data when a user's request for the data is processed so that users can obtain data that are calibrated with up-to-date calibration files, parameters, and software. \n\nThe OTFC processing system was developed at the Canadian Astronomy Data Centre (CADC) and the Space Telescope European Coordinating Facility (ST-ECF) and is implemented at both sites and at Space Telescope Science Institute (STScI). The OTFC system currently provides data calibration for the Wide Field and Planetary Camera 2 (WFPC2) and Space Telescope Imaging Spectrograph (STIS). In the future, the Advanced Camera for Surveys (ACS) and possibly the Near Infrared Camera and Multi-Object Spectrometer (NICMOS) will be supported.\n\nThe main goals behind the implementation of the OTFC system are to take advantage of better calibration files and the much smaller storage area required if only raw files are kept in the archives. The system can also offer more calibration steps than were available when the data was first released and can implement improved pipeline software. \n\nCurrently, for example, HST data are calibrated as they are received at the STScI. Raw and calibrated data are stored in the HST archive (DADS). Frequently, users must recalibrate the data at their home sites to take advantage of better calibration files or software. A large fraction (over 90%) of the calibrated data in the HST archive could be improved by recalibration, although the improvements are not always significant. In the past, instruments that undergo evolution of calibration files or calibration software often required users to carry out their own recalibrations at their home sites. With OTFC, the HST data archives carry out the recalibration.\n"}
{"id": "25365805", "url": "https://en.wikipedia.org/wiki?curid=25365805", "title": "PERDaix", "text": "PERDaix\n\nPERDaix (Proton Electron Radiation Detector Aix-la-Chapelle) is a novel, small and light weight magnetic spectrometer to measure the charge and mass dependent solar modulation periodically for deeper understanding of cosmic rays.\nFor a better understanding of sources and acceleration of cosmic particles direct measurements of cosmic rays are necessary. Also for a better understanding of the solar modulation which is expected to follow the 22-year solar cycle, time dependent measurements are needed.\nPERDaix is a newly designed detector which is constructed by the Department of Physics 1b, RWTH Aachen University. Being proposed to the German Space Agency in November 2009 for a participation in the BEXUS Program (Rocket and Balloon Experiments for University Students) after a first canceled flight attempt in October 2010 the actual flight took place as a post-BEXUS-campaign flight opportunity in November 2010.\n\nThe detector is able to measure charged particles in the energy range of 0.5 GeV to 5 GeV. PERDaix uses a time of flight system, a scintillating fiber tracker with silicon photomultiplier (SiPM) readout, and a transition radiation detector in combination with a permanent magnet to measure particle fluxes.\nThe BEXUS balloons are launched at Esrange Space Center near Kiruna, Sweden. In November 2010 PERDaix reached a top altitude of 33.3 km at which it kept floating for 1.5 hours.\n\nThe time of flight system (TOF) is the upper- and lowermost layer of the detector. It consists of scintillators with an SiPM readout. It is used as a trigger signal and to discriminate against particles entering the detector from below. With a design time resolution of approximately 300 picoseconds (ps) it can be used to distinguish between positrons and electrons in the momentum range below 1 GeV. Protons can be distinguished from positrons for momenta below 1 GeV if their velocity is lower than β = 1.\n\nPerdaix will make use of a scintillating fiber tracking detector made up from 250 µm thin scintillating polystyrene fibers that emit light when traversed by a charged particle. The scintillating fibers are read out by silicon photomultiplier (SiPM) arrays which are structured semi-conductor photon detectors that offer high photon efficiencies of 50%, a high gain of 10^6 electrons / photon and that are very compact in size. One silicon photomultiplier array is 1.1mm by 8.0mm in size and has 32 channels. Twenty 32mm wide and 300mm long fiber modules are arranged in four layers around a hollow cylindrical permanent magnet array.\nThe permanent magnet array is constructed as a Halbach-Ring and weighs 8 kg and produces a very high magnetic field of ~0.26 Tesla (T) inside an 80mm high and 213mm diameter magnet cylinder while producing only a negligible magnetic field outside the cylinder.\n\nUnderneath the lowest tracker layer a transition radiation detector (TRD) is installed. The TRD detects transition radiation of relativistic particles with a Lorentz factor γ exceeding ≈ 1000.\nParticles crossing the interface of two media with different dielectric constant produce transition radiation. The energy loss at a boundary is proportional to the relativistic gamma factor. A significant amount of TR is produced for a gamma greater than 1000. The gamma factor of protons is, up to a momentum of 5GeV, still in the order of 10, whereas the positron's gamma is greater than 1000, starting at 0.5GeV momentum.\n\nThe detector is made up of 256 6mm thick straw tubes out of a 72 um thin multilayer aluminium-kapton foil, filled with an 80/20 mixture of xenon (Xe) and carbon dioxide (CO2). It is used to measure the x-ray transition radiation produced by electrons in eight 20mm thick layers of an irregular fleece radiator. This leads to more than 100 material interfaces per radiator layer.\n\nDue to strong winds the launch campaign in October 2010 had to be canceled without a BEXUS-11 flight at first. Thanks to the support of German Space Agency (DLR) and Esrange a second flight opportunity was provided in late November 2010. On 23 November a 100 000 m³ helium balloon was launched from Esrange carrying a payload of 334 kg containing the BEXUS student experiments including the PERDaix detector.\n\n"}
{"id": "177648", "url": "https://en.wikipedia.org/wiki?curid=177648", "title": "Personality", "text": "Personality\n\nPersonality is defined as the characteristic set of behaviors, cognitions, and emotional patterns that evolve from biological and environmental factors. While there is no generally agreed upon definition of personality, most theories focus on motivation and psychological interactions with one's environment. Trait-based personality theories, such as those defined by Raymond Cattell define personality as the traits that predict a person's behavior. On the other hand, more behaviorally based approaches define personality through learning and habits. Nevertheless, most theories view personality as relatively stable.\n\nThe study of the psychology of personality, called personality psychology, attempts to explain the tendencies that underlie differences in behavior. Many approaches have been taken on to study personality, including biological, cognitive, learning and trait based theories, as well as psychodynamic, and humanistic approaches. Personality psychology is divided among the first theorists, with a few influential theories being posited by Sigmund Freud, Alfred Adler, Gordon Allport, Hans Eysenck, Abraham Maslow, and Carl Rogers.\n\nPersonality can be determined through a variety of tests. However, dimensions of personality and scales of personality tests vary and often are poorly defined. Examples of such tests are the: Big Five Inventory (BFI), Minnesota Multiphasic Personality Inventory (MMPI-2), Rorschach Inkblot test, Neurotic Personality Questionnaire KON-2006, Enneagram test, or Eysenck's Personality Questionnaire (EPQ-R).\n\nPersonality is often broken into statistically-identified factors called the Big Five, which are openness to experience, conscientiousness, extraversion, agreeableness, and neuroticism (or emotional stability). These components are generally stable over time, and about half of the variance appears to be attributable to a person's genetics rather than the effects of one's environment.\n\nSome research has investigated whether the relationship between happiness and extraversion seen in adults can also be seen in children. The implications of these findings can help identify children that are more likely to experience episodes of depression and develop types of treatment that such children are likely to respond to. In both children and adults, research shows that genetics, as opposed to environmental factors, exert a greater influence on happiness levels. Personality is not stable over the course of a lifetime, but it changes much more quickly during childhood, so personality constructs in children are referred to as temperament. Temperament is regarded as the precursor to personality. Whereas McCrae and Costa's Big Five model assesses personality traits in adults, the EAS (emotionality, activity, and sociability) model is used to assess temperament in children. This model measures levels of emotionality, activity, sociability, and shyness in children. The personality theorists consider temperament EAS model similar to the Big Five model in adults; however, this might be due to a conflation of concepts of personality and temperament as described above. Findings show that high degrees of sociability and low degrees of shyness are equivalent to adult extraversion, and correlate with higher levels of life satisfaction in children.\n\nAnother interesting finding has been the link found between acting extraverted and positive affect. Extraverted behaviors include acting talkative, assertive, adventurous, and outgoing. For the purposes of this study, positive affect is defined as experiences of happy and enjoyable emotions. This study investigated the effects of acting in a way that is counter to a person's dispositional nature. In other words, the study focused on the benefits and drawbacks of introverts (people who are shy, socially inhibited and non-aggressive) acting extraverted, and of extraverts acting introverted. After acting extraverted, introverts' experience of positive affect increased whereas extraverts seemed to experience lower levels of positive affect and suffered from the phenomenon of ego depletion. Ego depletion, or cognitive fatigue, is the use of one's energy to overtly act in a way that is contrary to one's inner disposition. When people act in a contrary fashion, they divert most, if not all, (cognitive) energy toward regulating this foreign style of behavior and attitudes. Because all available energy is being used to maintain this contrary behavior, the result is an inability to use any energy to make important or difficult decisions, plan for the future, control or regulate emotions, or perform effectively on other cognitive tasks.\n\nOne question that has been posed is why extraverts tend to be happier than introverts. The two types of explanations attempt to account for this difference are instrumental theories and temperamental theories. The instrumental theory suggests that extraverts end up making choices that place them in more positive situations and they also react more strongly than introverts to positive situations. The temperamental theory suggests that extraverts have a disposition that generally leads them to experience a higher degree of positive affect. In their study of extraversion, Lucas and Baird found no statistically significant support for the instrumental theory but did, however, find that extraverts generally experience a higher level of positive affect. \n\nResearch has been done to uncover some of the mediators that are responsible for the correlation between extraversion and happiness. Self-esteem and self-efficacy are two such mediators. Self-efficacy has been found to be related to the personality traits of extraversion and subjective well-being. Self-efficacy is one's belief about abilities to perform up to personal standards, the ability to produce desired results, and the feeling of having some ability to make important life decisions. However, the relationship between extraversion (and neuroticism) and subjective happiness is only partially mediated by self-efficacy. This implies that there are most likely other factors that mediate the relationship between subjective happiness and personality traits. Another such factor may be self-esteem. Individuals with a greater degree of confidence about themselves and their abilities seem to have both higher degrees of subjective well-being and higher levels of extraversion.\n\nOther research has examined the phenomenon of mood maintenance as another possible mediator. Mood maintenance, the ability to maintain one's average level of happiness in the face of an ambiguous situation (meaning a situation that has the potential to engender either positive or negative emotions in different individuals), has been found to be a stronger force in extraverts. This means that the happiness levels of extraverted individuals are less susceptible to the influence of external events. Another implication of this finding is that extraverts' positive moods last longer than those of introverts.\n\nModern conceptions of personality, such as the Temperament and Character Inventory have suggested four basic temperaments that are thought to reflect basic and automatic responses to danger and reward that rely on associative learning. The four temperaments, \"harm avoidance\", \"reward dependence\", \"novelty seeking\" and \"persistence\" are somewhat analogous to ancient conceptions of melancholic, sanguine, choleric, phlegmatic personality types, although the temperaments reflect dimensions rather than distance categories. While factor based approaches to personality have yielded models that account for significant variance, the developmental biological model has been argued to better reflect underlying biological processes. Distinct genetic, neurochemical and neuroanatomical correlates responsible for each temperamental trait have been observed, unlike with five factor models.\n\nThe harm avoidance trait has been associated with increased reactivity in insular and amygdala salience networks, as well as reduced 5-HT2 receptor binding peripherally, and reduced GABA concentrations. Novelty seeking has been associated with reduced activity in insular salience networks increased striatal connectivity. Novelty seeking correlates with dopamine synthesis capacity in the striatum, and reduced auto receptor availability in the midbrain. Reward dependence has been linked with the oxytocin system, with increased concentration of plasma oxytocin being observed, as well as increased volume in oxytocin related regions of the hypothalamus. Persistence has been associated with increased striatal-mPFC connectivity, increased activation of ventral striatal-orbitofrontal-anterior cingulate circuits, as well as increased salivary amylase levels indicative of increased noradrenergic tone.\n\nIt has been shown that personality traits are more malleable by environmental influences than researchers originally believed. Personality differences predict the occurrence of life experiences.\n\nOne study that has shown how the home environment, specifically the types of parents a person has, can affect and shape their personality. Mary Ainsworth's Strange Situation experiment showcased how babies reacted to having their mother leave them alone in a room with a stranger. The different styles of attachment, labelled by Ainsworth, were Secure, Ambivalent, avoidant, and disorganized. Children who were securely attached tend to be more trusting, sociable, and are confident in their day-to-day life. Children who were disorganized were reported to have higher levels of anxiety, anger, and risk-taking behavior.\n\nJudith Rich Harris's group socialization theory postulates that an individual's peer groups, rather than parental figures, are the primary influence of personality and behavior in adulthood. Intra- and intergroup processes, not dyadic relationships such as parent-child relationships, are responsible for the transmission of culture and for environmental modification of children's personality characteristics. Thus, this theory points at the peer group representing the environmental influence on a child's personality rather than the parental style or home environment.\n\nTessuya Kawamoto's \"Personality Change from Life Experiences: Moderation Effect of Attachment Security\" talked about laboratory tests. The study mainly focused on the effects of life experiences on change in personality on and life experiences. The assessments suggested that \"the accumulation of small daily experiences may work for the personality development of university students and that environmental influences may vary by individual susceptibility to experiences, like attachment security\".\n\nThere has been some recent debate over the subject of studying personality in a different culture. Some people think that personality comes entirely from culture and therefore there can be no meaningful study in cross-culture study. On the other hand, others believe that some elements are shared by all cultures and an effort is being made to demonstrate the cross-cultural applicability of \"the Big Five\".\n\nCross-cultural assessment depends on the universality of personality traits, which is whether there are common traits among humans regardless of culture or other factors. If there is a common foundation of personality, then it can be studied on the basis of human traits rather than within certain cultures. This can be measured by comparing whether assessment tools are measuring similar constructs across countries or cultures. Two approaches to researching personality are looking at emic and etic traits. Emic traits are constructs unique to each culture, which are determined by local customs, thoughts, beliefs, and characteristics. Etic traits are considered universal constructs, which establish traits that are evident across cultures that represent a biological bases of human personality. If personality traits are unique to individual culture, then different traits should be apparent in different cultures. However, the idea that personality traits are universal across cultures is supported by establishing the Five Factor Model of personality across multiple translations of the NEO-PI-R, which is one of the most widely used personality measures. When administering the NEO-PI-R to 7,134 people across six languages, the results show a similar pattern of the same five underlying constructs that are found in the American factor structure.\n\nSimilar results were found using the Big Five Inventory (BFI), as it was administered in 56 nations across 28 languages. The five factors continued to be supported both conceptually and statistically across major regions of the world, suggesting that these underlying factors are common across cultures. There are some differences across culture but they may be a consequence of using a lexical approach to study personality structures, as language has limitations in translation and different cultures have unique words to describe emotion or situations. For example, the term \"feeling blue\" is used to describe sadness in more Westernized cultures, but does not translate to other languages. Differences across cultures could be due to real cultural differences, but they could also be consequences of poor translations, biased sampling, or differences in response styles across cultures. Examining personality questionnaires developed within a culture can also be useful evidence for the universality of traits across cultures, as the same underlying factors can still be found. Results from several European and Asian studies have found overlapping dimensions with the Five Factor Model as well as additional culture-unique dimensions. Finding similar factors across cultures provides support for the universality of personality trait structure, but more research is necessary to gain stronger support.\n\nThe modern sense of individual personality is a result of the shifts in culture originating in the Renaissance, an essential element in modernity. In contrast, the Medieval European's sense of self was linked to a network of social roles: \"the household, the kinship network, the guild, the corporation – these were the building blocks of personhood\", Stephen Greenblatt observes, in recounting the recovery (1417) and career of Lucretius' poem \"De rerum natura\": \"at the core of the poem lay key principles of a modern understanding of the world.\" \"Dependent on the family, the individual alone was nothing,\" Jacques Gélis observes.\n\nWilliam James (1842-1910) argued that temperament explains a great deal of the controversies in the history of philosophy by arguing that it is a very influential premise in the arguments of philosophers. Despite seeking only impersonal reasons for their conclusions, James argued, the temperament of philosophers influenced their philosophy. Temperament thus conceived is tantamount to a bias. Such bias, James explained, was a consequence of the trust philosophers place in their own temperament. James thought the significance of his observation lay on the premise that in philosophy an objective measure of success is whether a philosophy is peculiar to its philosopher or not, and whether a philosopher is dissatisfied with any other way of seeing things or not.\n\nJames argued that temperament may be the basis of several divisions in academia, but focused on philosophy in his 1907 lectures on \"Pragmatism\". In fact, James' lecture of 1907 fashioned a sort of trait theory of the empiricist and rationalist camps of philosophy. As in most modern trait theories, the traits of each camp are described by James as distinct and opposite, and may be possessed in different proportions on a continuum, and thus characterize the personality of philosophers of each camp. The \"mental make-up\" (i.e. personality) of rationalist philosophers is described as \"tender-minded\" and \"going by \"principles,\" and that of empiricist philosophers is described as \"tough-minded\" and \"going by \"facts.\" James distinguishes each not only in terms of the philosophical claims they made in 1907, but by arguing that such claims are made primarily on the basis of temperament. Furthermore, such categorization was only incidental to James' purpose of explaining his pragmatist philosophy, and is not exhaustive.\n\nAccording to James, the \"temperament\" of rationalist philosophers differed fundamentally from the \"temperament\" of empiricist philosophers of his day. The tendency of rationalist philosophers toward \"refinement\" and \"superficiality\" never satisfied an empiricist temper of mind. Rationalism leads to the creation of \"closed systems\", and such optimism is considered shallow by the fact-loving mind, for whom perfection is far off. Rationalism is regarded as \"pretension\", and a temperament most inclined to \"abstraction\". The temperament of rationalists, according to James, led to sticking with logic.\n\nEmpiricists, on the other hand, stick with the external senses rather than logic. British empiricist John Locke's (1632–1704) explanation of personal identity provides an example of what James referred to. Locke explains the identity of a person, i.e. personality, on the basis of a precise definition of identity, by which the meaning of identity differs according to what it is being applied to. The identity of a person, is quite distinct from the identity of a man, woman, or substance according to Locke. Locke concludes that consciousness is personality because it \"always accompanies thinking, it is that which makes every one to be what he calls self,\" and remains constant in different places at different times. Thus his explanation of personal identity is in terms of experience as James indeed maintained is the case for most empiricists.\n\nRationalists conceived of the identity of persons differently than empiricists such as Locke who distinguished identity of substance, person, and life. According to Locke, Rene Descartes (1596–1650) agreed only insofar as he did not argue that one immaterial spirit is the basis of the person \"for fear of making brutes thinking things too.\" According to James, Locke tolerated arguments that a soul was behind the consciousness of any person. However, Locke's successor David Hume (1711–1776), and empirical psychologists after him denied the soul except for being a term to describe the cohesion of inner lives. However, some research suggests Hume excluded personal identity from his opus An Inquiry Concerning Human Understanding because he thought his argument was sufficient but not compelling. Descartes himself distinguished active and passive faculties of mind, each contributing to thinking and consciousness in different ways. The passive faculty, Descartes argued, simply receives, whereas the active faculty produces and forms ideas, but does not presuppose thought, and thus cannot be within the thinking thing. The active faculty mustn't be within self because ideas are produced without any awareness of them, and are sometimes produced against one's will.\n\nRationalist philosopher Benedictus Spinoza (1632–1677) argued that ideas are the first element constituting the human mind, but existed only for actually existing things. In other words, ideas of non-existent things are without meaning for Spinoza, because an idea of a non-existent thing cannot exist. Further, Spinoza's rationalism argued that the mind does not know itself, except insofar as it perceives the \"ideas of the modifications of body,\" in describing its external perceptions, or perceptions from without. On the contrary, from within, Spinoza argued, perceptions connect various ideas clearly and distinctly. The mind is not the free cause of its actions for Spinoza. Spinoza equates the will with the understanding, and explains the common distinction of these things as being two different things as error which results from the individual's misunderstanding of the nature of thinking.\n\nThe biological basis of personality is the theory that anatomical structures located in the brain contribute to personality traits. This stems from neuropsychology, which studies how the structure of the brain relates to various psychological processes and behaviors. For instance, in human beings, the frontal lobes are responsible for foresight and anticipation, and the occipital lobes are responsible for processing visual information. In addition, certain physiological functions such as hormone secretion also affect personality. For example, the hormone testosterone is important for sociability, affectivity, aggressiveness, and sexuality. Additionally, studies show that the expression of a personality trait depends on the volume of the brain cortex it is associated with.\n\nThere is also a confusion among some psychologists who conflate personality with temperament. Temperament traits that are based on weak neurochemical imbalances within neurotransmitter systems are much more stable, consistent in behavior and show up in early childhood; they can't be changed easily but can be compensated for in behavior. In contrast to that, personality traits and features are the product of the socio-cultural development of humans and can be learned and/or changed.\n\nPersonology confers a multidimensional, complex, and comprehensive approach to personality. From a holistic perspective, personology studies personality as a whole, as a system, but in the same time through all its components, levels and spheres.\n\nHigh neuroticism is an independent prospective predictor for the development of the common mental disorders.\n\nResearch in Personality computing demonstrated that machines can infer the personality types of an individual better than humans\n\n\n"}
{"id": "18130567", "url": "https://en.wikipedia.org/wiki?curid=18130567", "title": "Primary consciousness", "text": "Primary consciousness\n\nPrimary consciousness is a term the American biologist Gerald Edelman coined to describe the ability, found in humans and some animals, to integrate observed events with memory to create an awareness of the present and immediate past of the world around them. This form of consciousness is also sometimes called \"sensory consciousness\". Put another way, primary consciousness is the presence of various subjective sensory contents of consciousness such as sensations, perceptions, and mental images. For example, primary consciousness includes a person's experience of the blueness of the ocean, a bird's song, and the feeling of pain. Thus, primary consciousness refers to being mentally aware of things in the world in the present without any sense of past and future; it is composed of mental images bound to a time around the measurable present.\n\nConversely, higher order consciousness can be described as being \"conscious of being conscious\"; it includes reflective thought, a concept of the past, and speculation about the future.\n\nPrimary consciousness can be subdivided into two forms, focal awareness and peripheral awareness. Focal awareness encompasses the center of attention, whereas peripheral awareness consists of things outside the center of attention, which a person or animal is only dimly aware of.\n\nOne prominent theory for the neurophysiological basis of primary consciousness was proposed by Gerald Edelman. This theory of consciousness is premised upon three major assumptions:\n\nEdelman's theory focuses on two nervous system organizations: the brainstem and limbic systems on one side and the thalamus and cerebral cortex on the other side. The brain stem and limbic system take care of essential body functioning and survival, while the thalamocortical system receives signals from sensory receptors and sends out signals to voluntary muscles such as those of the arms and legs. The theory asserts that the connection of these two systems during evolution helped animals learn adaptive behaviors. This connection allows past signals related to values set by the limbic-brain stem system and categorized signals from the outside world to be correlated, resulting in memory in conceptual areas. This memory is then linked to the organism's \"current\" perception, which results in an awareness of the present, or primary consciousness. In other words, Edelman posits that primary consciousness arises from the correlation of \"conceptual\" memory to a set of \"ongoing\" perceptual categorizations—a \"remembered present\".\n\nOther scientists have argued against Edelman's theory, instead suggesting that primary consciousness might have emerged with the basic vegetative systems of the brain. That is, the evolutionary origin might have come from sensations and primal emotions arising from sensors and receptors, both internal and surface, signaling that the well-being of the creature was immediately threatened—for example, hunger for air, thirst, hunger, pain, and extreme temperature change. This is based on neurological data showing the thalamic, hippocampal, orbitofrontal, insula, and midbrain sites are the key to consciousness of thirst.\n\nThese scientists also point out that the cortex might not be as important to primary consciousness as some neuroscientists have believed. Evidence of this lies in the fact that studies show that systematically disabling parts of the cortex in animals does not remove consciousness. Another study found that children born without a cortex are conscious. Instead of cortical mechanisms, these scientists emphasize brainstem mechanisms as essential to consciousness. Still, these scientists concede that higher order consciousness does involve the cortex and complex communication between different areas of the brain.\n\nPhysiologically, three fundamental facts stand out about primary consciousness:\n\nTo be fully comprehensive, measures of consciousness must not only define and distinguish between conscious and unconscious states, but must also provide a guide by which the conscious level, or extent of consciousness, can be determined. Measures of consciousness are each associated with particular theories.\n\nCertain defining theories are included below:\n\nWorldly discrimination theory asserts that any mental state that is manifested in behavior is conscious; thus, an organism is consciously aware of something in the world if it can discriminate it with choice behavior.\nSignal detection theory quantifies discriminability of a stimulus among a set of different stimuli.\nIntegration theories focus on finding a divide between conscious and unconscious processes. According to integration theories, conscious contents are widely available to many cognitive and/or neural processes.\n\nThese theories are then accompanied with measures of the level of consciousness, which are subdivided into behavioral measures and physiological measures.\n\nBehavioral measures of primary consciousness can be either objective or subjective. Regarding objective measures, knowledge is unconscious if it expresses itself in an indirect test. For example, the ability to pick which item might come next in a series can indicate unconscious knowledge of regularities in sequences. \"Strategic control measures\" use a person's ability to deliberately use or not use knowledge according to instructions. If they use information despite intentions not to use it, it indicates unconscious knowledge. Post-decision wagering can also be used. In this method, subjects make a first-order discrimination (i.e. a choice) and then place a wager regarding the outcome of the discrimination. Some scientists view this as a direct and objective measure of consciousness, and it can be used with children and animals. However, this method has been argued to be subjective and indirect.\n\nEvent-related cortical potentials (ERPs) have been used to assess whether a stimulus is consciously perceived or not. These EEG measures either float free of theory, gaining credibility through reliable correlation, or assume a version of integration theory in which the appearance of a particular ERP indicates global availability or locally recurrent processing.\n\nAbundant evidence indicates that consciously perceived inputs elicit widespread brain activation, as compared with inputs that do not reach consciousness.\n\nThe dynamic core hypothesis (DCH) proposes that consciousness arises from neural dynamics in the thalamocortical system, as measured by the quantity neural complexity (CN). \"CN\" is an information-theoretic measure; the \"CN\" value is high if each subset of a neural system can take on many different states, and if these states make a difference to the rest of the system.\nThe information integration theory of consciousness (IITC) shares with the DCH the idea that conscious experiences provide informative discriminations among a vast repertoire of possible experiences. In the IITC, the quantity \"phi\" is defined as the information that is integrated across the informational \"weakest link\" of a system. Importantly, \"phi\" is a measure of the capacity of a neural system to integrate information, whereas \"CN\" is a measure of the actual dynamics of the system. A third measure, causal density \"(CD)\", measures the fraction of causal interactions among elements of a system that are statistically significant.\n\nIt is important to note that subjective measures are always indirect and can be vulnerable to many biases (e.g., reluctance to report uncertain experiences). An|metacognitive]] conscious content assumes primary consciousness but not vice versa, subjective measures risk missing or rejecting the presence of sensory consciousness simply because metacognition isn't observed.\n\nFurthermore, there is the problem of post-decision wagering, which has been criticized because there is a possibility that advantageous wagering could be learned unconsciously; as a result, post-decision wagering would not in fact be considered a conscious behavior. For example, individual differences in risk aversion may lead to variations in wagering performance even with the same underlying conscious phenomenology.\n\nThus, although behavioral measures are mostly used for assessing which contents are conscious, some brain-based measures seem better suited for measuring conscious level. Objective measures also have their challenges, however. First, objective measures still require a response criterion, for example the decision of whether or not to push a button. Second, they may not even measure consciousness at all because many behavioral proxies, such as forced-choice decision accuracy, are capable of being learned unconsciously.\n\nHobson asserts that the existence of lucid dreaming means that the human brain can simultaneously occupy two states: waking and dreaming. The dreaming portion has experiences and therefore has primary consciousness, while the waking self recognizes the dreaming and can be seen as having a sort of secondary consciousness in the sense that there is an awareness of mental state. Studies have been able to show that lucid dreaming is associated with EEG power and coherence profiles that are significantly different from both non-lucid dreaming and waking. Lucid dreaming situates itself between those two states. Lucid dreaming is characterized by more 40 Hz power than non-lucid dreaming, especially in frontal regions. Since it is 40 Hz power that has been correlated with waking consciousness in previous studies, it can be suggested that enough 40 Hz power has been added to the non-lucid dreaming brain to support the increase in subjective awareness that permits lucidity but not enough to cause full awakening.\n\nDreaming is thus a virtual reality experience with a remarkably predictive simulation of external reality. Lucid dreamers may experience primary consciousness (the dream) and secondary consciousness (the waking) separately but simultaneously. Moreover, primary consciousness has recently been proposed by us to be characteristic of dreaming. It remains to be seen whether the enactment of dream behaviors uses the same brain processes as those that mediate those very behaviors in waking, and whether conscious within a dream is governed by the same processes.\n\nStudies show that it is possible to retain primary consciousness and even secondary consciousness during complex partial epileptic seizures. One study analyzed 40 patients with complex partial seizures to determine their level of consciousness during seizures. The data acquired was based on patients' subjective descriptions of their experience and descriptions from family members who witnessed the seizures. This study found there was a complete absence of consciousness in only 65% of people during the core period of the seizures. Meanwhile, 35% of seizures included some form of primary consciousness. Five seizure descriptions even reported some form of secondary consciousness, albeit short and intermittent. The level and contents of consciousness during epileptic seizures show considerable variability.\n\nIn one study, 10 adult males underwent positron emission tomography scans in three different scenarios:\n\nThe data suggest that the anterior and posterior cingulate cortex as well as the anterior wall of the third ventricle, are major elements of a circuit including thalamic, hippocampal, orbitofrontal, insula, and midbrain sites that are needed for the generation of consciousness of thirst.\nThis study shows that consciousness of some key sensations like thirst is governed by the oldest regions of the brain, which raises the question of whether it is really then possible to say when primary consciousness developed.\n\nIn some types of meditation/yoga it is possible to have the experience known as Samadhi, where there is inner alertness but no object of consciousness. This mental state corresponds with specific physiological parameters.\n"}
{"id": "46622063", "url": "https://en.wikipedia.org/wiki?curid=46622063", "title": "Representative bureaucracy", "text": "Representative bureaucracy\n\nAs stated by political scientist Samuel Krislov, representative bureaucracy is a notion that “broad social groups should have spokesman and officeholders in administrative as well as political positions”. With this notion, representative bureaucracy is a form of representation that captures most or all aspects of a society’s population in the governing body of the state. (Krislov, Samuel. (2012). Representative Bureaucracy. Quid Pro Books)\n\nThe term representative bureaucracy is generally attributed to J. Donald Kingsley’s book titled \"Representative Bureaucracy\" that was published in 1944. In his book, Kingsley calls for a “ liberalization of social class selection for the English bureaucracy,\" due to the \"Dominance of social, political, and economic elites within the British bureaucracy\" which he claimed resulted in programs and political policy that did not meet the needs or interests of all social classes. To solve this issue, Kingsley states that “representative bureaucracy is necessary because there must be at least some administrators sympathetic to the programmatic concerns of the dominant political party”. Despite criticism at the time of publication, years later, political scientist Samuel Krislov echoed Kingsley's sentiments in his book also called \"Representative Bureaucracy\" and this has been expanded upon since then by a number of scholars all over the world.\n\nIn Samuel Krislov’s book, he states that in much, if not all, of bureaucracy’s history, the perception of bureaucracy has had the identification of being entities covered in red tape and having an antagonistic approach to the public. Also with the association of these ideals heavily ingrained into the public, it is hard to think of the positive aspects of such representation. ( Krislov, Samuel. (2012). Representative Bureaucracy. Quid Pro Books)\n\nThere is some degree of disagreement over what constitutes representative bureaucracy as there is a lot of literature that exists on the subject as it has a long history. Scholars Groeneveld and Walle state that there are three dimensions of representative bureaucracy, the first is a historical one, the second is from the perspective of public administration, and the third is in literature on diversity management\n\nAt the local level different studies and surveys have been conducted to evaluate the theory of representative bureaucracy studies and surveys have been done to evaluate whether or not the employees who worked for these government agencies share values with each other across race, sex, gender, age, political ideology, as well as whether or not these employees share values with the citizenry across the same variables. The studies were done to see if these bureaucracies are truly representative of the social groups that make up their citizenry. According to one survey there are great differences in the way black and white administrators and citizens view different issues. The survey found that race plays a huge role in how administrators and citizens view certain issues. There are drastic differences between white and black administrators views and the views of black and white citizens. The study claims black administrators are more similar to their citizen counterparts, white administrators tended to be out of touch with the views of their citizen counterparts. The study found that passive representation can lead to active representation as more individuals of diverse backgrounds work for these local agencies and represent the minorities in the community.\n\nA study by Mark Bradbury and J. Edward Kellough found that black administrators are more likely than white administrators to take on the role as a representative of the minority black community, although they concede that this was for one local government and it is unclear whether or not this translates across racial lines to other ethnic and racial groups, on top of the fact that historical, cultural, and socioeconomic factors were not accounted for.\n\nStudies of state run government agencies were performed to see whether passive representation of nonwhites and women is linked to active representation, meaning whether or not having a diverse group of people working and running the agencies means that the interests of those peoples' larger group (women, nonwhites) will translate into policy outcomes.\n\nOne of the studies suggests that to a large degree, the heads of the agencies are responsible for setting the tone for their organization, influencing the culture and agendas of the institutions, establishing the agencies’ mission and purpose. As they have so much influence on what the agency does, studies were done to see how different groups (race/ethnicity, sex) place importance on values or an agenda compared to others. The values that the agency heads hold regarding organizational objectives and goals are integral for them defining their jobs as state executives. The studies of 93 different state agencies found that there are differences between the attitudes and values of different ethnic groups, men, and women. Each group places different emphasis of importance on series of different characteristics of the organization. According to the studies, as a group, nonwhites placed more of an emphasis on goals of growing the organization and budget stability, and women placed greater emphasis on values pertaining to organizational proficiency.\n\nThe people with the power to appoint agency heads and senior administrative positions can have a direct effect on the direction an agency is taken in under its head.\n\nThe United States federal bureaucracy is broadly representative of the American people in terms of age, income, education, and the income of the father (used as a variable because it is a good predictor of where one will end up in life). When looking at the civil service from a view of positions held, the bureaucracy becomes less representative. Most decisions are made at the senior and upper levels, so the unrepresentative nature of the elites of the federal government could be grounds to dismiss the idea that the U.S. federal bureaucracy is representative. As one moves through the ranks of the federal bureaucracy one finds that it becomes less and less representative as the positions get higher. From the positions of GS1-GS4 the bureaucracy is fairly representative however from the positions of GS5 - GS14 and above the bureaucracy is significantly unrepresentative. The percentage of nonwhites decreases as one ascends the bureaucratic hierarchy, the same can be said for women. 40% of the American civil service are women and 75% of lower level positions are filled by women, with only 3% of women holding high civil service jobs. Compared to the U.K., Denmark, France, Turkey, and India, the U.S. is the most representative overall and is the best at representing the middle status occupations of countries surveyed.\n\nIn Bureaucracy and Constitutionalism by Norton Long, he contends that the American civil service is representative enough to make up for the fact that the political branches of government are not representative. A study by Kenneth Meier agrees that the civil service is the most representative branch of the U.S. government followed by the military, political executives, and lastly the foreign service. Meier indicates that Long's proposal is only weakly supported, as the bureaucracies are not representative at the decision making level. Meier states this is far from a conclusive study.\n\nBureaucratic representation is when it is expected that public administrator and government officials in a bureaucracy represent and conduct duties that are of concern for the interest of the individuals and groups the represent and serve. Studies have shown that there are an endless list of variable linking active and passive representation such as class, race, gender, ethnicity, as well as cultural traits such as language and religion. A number of studies have demonstrated a possible linkage between active and passive representation. Active representation is a process while passive representation is a characteristic. The possible linkage between active and passive is one that is complex and perplexing.\n\nActive representation is a function that concludes represented groups benefit from representative bureaucracies. Most active representation is concerned with how representation influence policymakers and implementation and assumes that bureaucrats will act purposely on behalf of their counterparts in the general population. An example being, women and men working beside one another within a bureaucracy, women are more likely to actively promote issues and agendas that benefit women in the general population. Potential barriers to active representation are peer pressures that appear within work environments as well as social ones. The pressures placed on bureaucratic of a primary group to conform are notorious within any environment.\n\nRepresentative bureaucracy in the passive sense is the degree to which the social characteristics of the bureaucracy reflect the social characteristics of the populations the bureaucracy serves. Studies of passive representation examine whether the composition of bureaucracies mirror the demographic composition of the general population. Passive representation exists when bureaucracy's demographic characteristics demonstrate the demographic characteristics of the population.\n\n\nSome criticisms that are associated with representative bureaucracy are:\n\n\n"}
{"id": "27921142", "url": "https://en.wikipedia.org/wiki?curid=27921142", "title": "Sanctioned name", "text": "Sanctioned name\n\nIn mycology, a sanctioned name is a name that was adopted (but not necessarily coined) in certain works of Christiaan Hendrik Persoon or Elias Magnus Fries, which are considered major points in fungal taxonomy.\n\nSanctioned names are those, regardless of their authorship, that were used by Persoon in his \"Synopsis Methodica Fungorum\" (1801) for rusts, smuts and gasteromycetes, and in Fries's \"Systema Mycologicum\" (three volumes, published 1821–32) and \"Elenchus fungorum\" for all other fungi.\n\nA sanctioned name, as defined under article 15 of the \"International Code of Nomenclature for algae, fungi, and plants\" (previously, the \"International Code of Botanical Nomenclature\") is automatically treated as if conserved against all earlier synonyms or homonyms. It can still, however, be conserved or rejected normally.\n\nBecause of the imprecision associated with assigning starting dates for fungi sanctioned in Fries' three \"Systema\" volumes, the Stockholm 1950 International Botanical Congress defined arbitrary or actual publication dates for the starting points to improve the stability of nomenclature. These dates were 1 May 1753 for \"Species Plantarum\" (vascular plants), 31 December 1801 for \"Synopsis Methodica Fungorum\", 31 December 1820 for \"Flora der Vorweldt\" (fossil plants), and 1 Jan 1821 for the first volume of \"Systema\". Because fungi defined in the second and third volumes lacked a starting-point book for reference, the Congress declared that these species, in addition to species defined in Fries' 1828 \"Elenchus Fungorum\" (a two-volume supplement to his \"System\"), had \"privileged status\". According to Korf, the term \"sanctioned\" was first used to indicate these privileged names by the Dutch mycologist Marinus Anton Donk in 1961.\n\nIn 1982, changes in the \"International Code for Botanical Nomenclature\" (the Sydney Code) restored Linnaeus' 1753 \"Species Plantarum\" as the starting point for fungal nomenclature; however, protected status was given to all names adopted by Persoon in his 1801 \"Synopsis\", and by Fries in both the \"Systema\" and the \"Elenchus\". Soon after, in 1983, Richard P. Korf proposed the now widely accepted \"colon-author indication\", whereby sanctioned names are indicated by including \": Pers.\" or \": Fr.\" when fully citing the species author.\n"}
{"id": "8688666", "url": "https://en.wikipedia.org/wiki?curid=8688666", "title": "Soyuz-FG", "text": "Soyuz-FG\n\nThe Soyuz-FG launch vehicle is an improved version of the Soyuz-U from the R-7 family of rockets, designed and constructed by TsSKB-Progress in Samara, Russia. It made its maiden flight on 20 May 2001, carrying a Progress cargo spacecraft to the International Space Station (ISS).\n\nSince 30 October 2002, Soyuz-FG has been the only vehicle used by the Russian Federal Space Agency to launch Soyuz-TMA and Soyuz-MS manned spacecraft to the ISS. The Soyuz-FG performed 64 successful launches until its first failure on 11 October 2018 with the Soyuz MS-10 mission.\n\nSoyuz-FG can optionally fly with a Fregat upper stage, developed and produced by Lavochkin Association in Khimki. Launches of the Soyuz-FG/Fregat configuration are marketed by a European-Russian company called Starsem. Its maiden flight occurred on 2 June 2003. As of December 2014, there have been 10 launches of Soyuz-FG/Fregat with commercial payloads.\n\nThe analog control system significantly limits the capabilities of this launcher, and it will eventually be replaced by Soyuz-2 in 2019.\n\nSoyuz-FG is launched from the Baikonur Cosmodrome in Kazakhstan, from Gagarin's Start (pad LC-1/5) for manned missions, and from LC-31/6 for satellite launches with the Fregat variant.\n\nOn 1 November 2018, Russian scientists released a video recording of the Soyuz MS-10 manned spaceflight involving a Soyuz-FG rocket after launch on 11 October 2018 that, due to a faulty sensor, resulted in the destruction of the rocket. The crew, NASA astronaut Nick Hague and Russian cosmonaut Aleksey Ovchinin. escaped safely and successfully.\n\n\n"}
{"id": "1744068", "url": "https://en.wikipedia.org/wiki?curid=1744068", "title": "Theory of planned behavior", "text": "Theory of planned behavior\n\nIn psychology, the theory of planned behavior (abbreviated TPB) is a theory that links one's beliefs and behavior.\n\nThe theory states that attitude toward behavior, subjective norms, and perceived behavioral control, together shape an individual's behavioral intentions and behaviors.\n\nThe concept was proposed by Icek Ajzen to improve on the predictive power of the theory of reasoned action by including perceived behavioural control. It has been applied to studies of the relations among beliefs, attitudes, behavioral intentions and behaviors in various fields such as advertising, public relations, advertising campaigns, healthcare, sport management and sustainability.\n\nThe theory of planned behavior was proposed by Icek Ajzen in 1985 through his article \"From intentions to actions: A theory of planned behavior.\" The theory was developed from the theory of reasoned action, which was proposed by Martin Fishbein together with Icek Ajzen in 1980. The theory of reasoned action was in turn grounded in various theories of attitude such as learning theories, expectancy-value theories, consistency theories (such as Heider's balance theory, Osgood and Tannenbaum's congruity theory, and Festinger's dissonance theory) and attribution theory. According to the theory of reasoned action, if people evaluate the suggested behavior as positive (attitude), and if they think their significant others want them to perform the behavior (subjective norm), this results in a higher intention (motivations) and they are more likely to do so. A high correlation of attitudes and subjective norms to behavioral intention, and subsequently to behavior, has been confirmed in many studies.\n\nA counter-argument against the high relationship between behavioral intention and actual behavior has also been proposed, as the results of some studies show that, because of circumstantial limitations, behavioral intention does not always lead to actual behavior. Namely, since behavioral intention cannot be the exclusive determinant of behavior where an individual's control over the behavior is incomplete, Ajzen introduced the theory of planned behavior by adding a new component, \"perceived behavioral control\". By this, he extended the theory of reasoned action to cover non-volitional behaviors for predicting behavioral intention and actual behavior.\n\nThe most recent addition of a third factor, perceived behavioral control, refers to the degree to which a person believes that they control any given behavior (class notes). The theory of planned behavior suggests that people are much more likely to intend to enact certain behaviors when they feel that they can enact them successfully. Increased perceived behavioral control is a mix of two dimensions: self-efficacy and controllability (170). Self-efficacy refers to the level of difficulty that is required to perform the behavior, or one's belief in their own ability to succeed in performing the behavior. Controllability refers to the outside factors, and one's belief that they personally have control over the performance of the behavior, or if it is controlled by externally, uncontrollable factors. If a person has high perceived behavioral control, then they have an increased confidence that they are capable of performing the specific behavior successfully.\n\nThe theory has since been improved and renamed the reasoned action approach by Azjen and his colleague Martin Fishbein.\n\nIn addition to attitudes and subjective norms (which make the theory of reasoned action), the theory of planned behavior adds the concept of \"perceived behavioral control\", which originates from self-efficacy theory (SET). Self-efficacy was proposed by Bandura in 1977, which came from social cognitive theory. According to Bandura, expectations such as motivation, performance, and feelings of frustration associated with repeated failures determine effect and behavioral reactions. Bandura separated expectations into two distinct types: self-efficacy and outcome expectancy. He defined self-efficacy as the conviction that one can successfully execute the behavior required to produce the outcomes. The \"outcome expectancy\" refers to a person's estimation that a given behavior will lead to certain outcomes. He states that self-efficacy is the most important precondition for behavioral change, since it determines the initiation of coping behavior.\nPrevious investigations have shown that peoples' behavior is strongly influenced by their confidence in their ability to perform that behavior. As the self-efficacy theory contributes to explaining various relationships between beliefs, attitudes, intentions, and behavior, the SET has been widely applied to health-related fields such as physical activity and mental health in preadolescents, and exercise.\n\n\n\n\nAs Ajzen (1991) stated in the theory of planned behavior, knowledge of the role of perceived behavioral control came from Bandura's concept of self-efficacy. More recently, Fishbein and Cappella stated that self-efficacy is the same as perceived behavioral control in his integrative model, which is also measured by items of self-efficacy in a previous study.\n\nIn previous studies, the construction and the number of item inventory of perceived behavioral control have depended on each particular health topic. For example, for smoking topics, it is usually measured by items such as \"I don't think I am addicted because I can really just not smoke and not crave for it,\" and \"It would be really easy for me to quit.\"\n\nThe concept of self-efficacy is rooted in Bandura's social cognitive theory. It refers to the conviction that one can successfully execute the behavior required to produce the outcome. The concept of self-efficacy is used as perceived behavioral control, which means the perception of the ease or difficulty of the particular behavior. It is linked to control beliefs, which refers to beliefs about the presence of factors that may facilitate or impede performance of the behavior.\n\nIt is usually measured with items which begins with the stem, \"I am sure I can ... (e.g., exercise, quit smoking, etc.)\" through a self-report instrument in their questionnaires. Namely, it tries to measure the confidence toward the probability, feasibility, or likelihood of executing given behavior.\n\nThe theory of planned behavior specifies the nature of relationships between beliefs and attitudes. According to these models, people's evaluations of, or attitudes toward behavior are determined by their accessible beliefs about the behavior, where a belief is defined as the subjective probability that the behavior will produce a certain outcome. Specifically, the evaluation of each outcome contributes to the attitude in direct proportion to the person's subjective possibility that the behavior produces the outcome in question.\n\nOutcome expectancy was originated from the expectancy-value model. It is a variable-linking belief, attitude, opinion and expectation. The theory of planned behavior's positive evaluation of self-performance of the particular behavior is similar to the concept to perceived benefits, which refers to beliefs regarding the effectiveness of the proposed preventive behavior in reducing the vulnerability to the negative outcomes, whereas their negative evaluation of self-performance is similar to perceived barriers, which refers to evaluation of potential negative consequences that might result from the enactment of the espoused health behavior.\n\nThe concept of social influence has been assessed by social norm and normative belief in both the theory of reasoned action and theory of planned behavior. Individuals' elaborative thoughts on subjective norms are perceptions on whether they are expected by their friends, family and the society to perform the recommended behavior. Social influence is measured by evaluation of various social groups. For example, in the case of smoking:\n\nWhile most models are conceptualized within individual cognitive space, the theory of planned behavior considers social influence such as social norm and normative belief, based on collectivistic culture-related variables. Given that an individual's behavior (e.g., health-related decision-making such as diet, condom use, quitting smoking and drinking, etc.) might very well be located in and dependent on the social networks and organization (e.g., peer group, family, school and workplace), social influence has been a welcomed addition.\n\nHuman behavior is guided by three kinds of consideration, \"behavioral beliefs,\" \"normative beliefs,\" and \"control beliefs.\" In their respective aggregates, \"behavioral beliefs\" produce a favorable or unfavorable \"attitude toward the behavior\"; \"normative beliefs\" result in \"subjective norm\"; and \"control beliefs\" gives rise to \"perceived behavioral control.\"\n\nIn combination, \"attitude toward the behavior,\" \"subjective norm,\" and \"perceived behavioral control\" lead to the formation of a \"behavioral intention\". In particular, \"perceived behavioral control\" is presumed to not only affect actual behavior directly, but also affect it indirectly through behavioral intention.\n\nAs a general rule, the more favorable the attitude toward behavior and subjective norm, and the greater the perceived behavioral control, the stronger the person's intention to perform the behavior in question should be. Finally, given a sufficient degree of actual control over the behavior, people are expected to carry out their intentions when the opportunity arises.\n\nIn a simple form, behavioral intention for the theory of planned behavior can be expressed as the following mathematical function:\nThe three factors being proportional to their underlying beliefs:\n\nTo the extent that it is an accurate reflection of actual behavioral control, perceived behavioral control can, together with intention, be used to predict behavior.\n\nThe theory of planned behavior can cover people's non-volitional behavior which cannot be explained by the theory of reasoned action.\n\nAn individual's behavioral intention cannot be the exclusive determinant of behavior where an individual's control over the behavior is incomplete. By adding \"perceived behavioral control,\" the theory of planned behavior can explain the relationship between behavioral intention and actual behavior.\n\nSeveral studies found that the TPB would help better predict health-related behavioral intention than the theory of reasoned action. The TPB has improved the predictability of intention in various health-related fields such as condom use, leisure, exercise, diet, etc.\n\nIn addition, the theory of planned behavior as well as the theory of reasoned action can explain the individual's social behavior by considering \"social norm\" as an important variable.\n\nSome scholars claim that the theory of planned behavior is based on cognitive processing, and they have criticised the theory on those grounds. More recently, some scholars criticize the theory because it ignores one's needs prior to engaging in a certain action, needs that would affect behaviour regardless of expressed attitudes. For example, one might have a very positive attitude towards beefsteak and yet not order a beefsteak because he is not hungry. Or, one might have a very negative attitude towards drinking and little intention to drink and yet engage in drinking as he's seeking group membership.\n\nAlso, one's emotions at the interviewing or decision-making time are ignored despite being relevant to the model as emotions can influence beliefs and other constructs of the model. Still, poor predictability for health-related behavior in previous health research seems to be attributed to poor application of the model, associated methods and measures. Most of the research is correlational, and more evidence based on experimental studies is welcome although experiments, by nature, lack external validity because they prioritize internal validity.\n\nSo far, the theory of planned behavior has more than 1200 research bibliographies in academic databases such as \"Communication & Mass Media Complete\", \"Academic Search Premier\", \"PsycARTICLES\", \"Business Source Premier\", \"PsycINFO\", and \"PsycCRITIQUES\".\n\nIn particular, recently, several studies found that the TPB would better help to predict health-related behavioral intention than the theory of reasoned action (TRA) given that the TPB has improved the predictability of intention in various health-related fields such as condom use, leisure, exercise, and diet, where the attitudes and intentions to behave in a certain way are mediated by goals rather than needs. For example, the goal to lose 10 kg in weight by the end of March, therefore a positive attitude and intention towards dieting. However, if a need is taken in calculation (health related or partner finding) the TPB fails. Assuming that one's need is to find a partner, if the partner is found and she like him overweight, or does not mind one's weight, then despite his positive attitude towards losing weight, he won't engage in a such behaviour as he might lose his partner, the main reason for engaging in dieting in first place.\n\nThe TPB also shows good applicability in regards to antisocial behaviours, such as using deception in the online environment. However, as the TPB relies on self-reports, there is evidence to suggest the vulnerability of such data to self-presentational biases. To a great extent, this has been ignored in the literature pertaining to the TRA/TPB, in spite of the threat to the validity and reliability of the models. More closely related to the concerns of the present study, Hessing, ElVers, and Weigel (1988) examined the TRA in relation to tax evasion and contrasted self-reports with official documentation. Findings indicated that while attitudes and subjective norms correlate with self-reported behaviour, it does not correlate with documentary evidence, in spite of considerable effort to maintain the anonymity of respondents. The implication was that self-reports of behaviour were unreliable, compared with more objective behaviour measures (see also Armitage & Conner, 1999a, 1999b; Norwich & Rovoli, 1993; Pellino, 1997).\n\nAnother application of the theory of planned behavior is in the field of environmental psychology. Generally speaking, actions that are environmentally friendly carry a positive normative belief. That is to say, sustainable behaviors are widely promoted as positive behaviors. However, although there may be a behavioral intention to practice such behaviors, perceived behavioral control can be hindered by constraints such as a belief that one's behavior will not have any impact. For example, if one intends to behave in an environmentally responsible way but there is a lack of accessible recycling infrastructure, perceived behavioral control is low, and constraints are high, so the behavior may not occur. Applying the theory of planned behavior in these situations helps explain contradictions between sustainable attitudes and unsustainable behavior.\n\nThe theory of planned behavior model is thus a very powerful and predictive model for explaining human behavior. That is why the health and nutrition fields have been using this model often in their research studies. In one study, utilizing the theory of planned behavior, the researchers determine obesity factors in overweight Chinese Americans. Intention to prevent becoming overweight was the key construct in the research process. It is important that nutrition educators provide the proper public policies in order to provide good tasting, low-cost, healthful food.\n\nThe theory of planned behavior can also be applied in area of applied nutrition intervention. In a recent study by Sweitzer, TPB (in conjunction with SCT) was utilized to encourage parents to pack more fruits, vegetables and whole grains (FVWG) in sack lunches of preschool children. Behavioral constructs of TPD were used to develop intervention strategies. Knowledge/behavioral control, Self-efficacy/perceived behavioral control, subjective norms and intentions were measured to see effects on behavior. The results found a significant increase in vegetables and whole grains packed in lunches when interventions were planned using the TPB constructs. Psychosocial variables were useful predictors of lunch packing behaviors of parents and this study provided a divergent application of model-exploration of an area of parental behavior as a role in the development of young children's dietary behaviors. In a study by McConnon, the application of the TPB was used to prevent weight regain in an overweight cohort who recently experienced a significant weight loss. Using the constructs of TPB, it was found that perceived need to control weight is the most positive predictor of behavior for weight maintenance. The TPB model can be used to predict weight gain prevention expectation in an overweight cohort. The TPB can also be utilized to measure behavioral intention of practitioners in promoting specific health behaviors. In this study by Chase, dietitians' intentions to promote whole grain foods was studied. It was found that the strongest indicator of intention of dietitians to promote whole grain foods was the construct of normative beliefs with 97% of dietitians indicating that health professionals should promote whole grains and 89% wanted to comply with this belief. However, knowledge and self-efficacy of instituting this belief was faulted with only 60% of dietitians being able to correctly identify a whole grain product from a food label, 21% correctly identifying current recommendations and 42% of dietitians did not know there was a recommendation for whole grain consumption. Although the response rate to complete mailed surveys for this study was low (39%), the results provided preliminary data on the strong effect of normative beliefs on dietitian intentions to promote whole grain and the need for nutrition need for additional education for practicing dietitians focusing on increase knowledge and self-efficacy for promoting whole grains.\n\nWhen applying the TPB as a theoretical framework, certain steps should be followed to promote increased validity of results. First, target behavior should be specified in terms of action, target, context, and time. For example, the goal might be to \"consume at least one serving of whole grains during breakfast each day in the forthcoming month\". In this statement, \"consuming\" is the action, \"one serving of whole grains\" is the target, \"during breakfast each day\" is the context, and \"in the forthcoming month\" is the time. Once a goal is specified, an elicitation phase can be used to identify salient issues. The pertinent and central beliefs for a certain behavior may be very different for different populations. Therefore, conducting open-ended elicitation interviews is one of the most crucial steps in applying the TPB. Elicitation interviews help to identify relevant behavioral outcomes, referents, cultural factors, facilitators, and barriers for each particular behavior and target population under investigation. The following are sample questions that may be used during an elicitation interview:\n\n\nHowever, the action, target, context and time construct shows little applicability when one engages in consuming luxury or fashion goods, especially as one's need is not present. For example, the goal might be to \"buy three pairs of luxury high heels in the forthcoming month\". In this statement, \"buying\" is the action, \"three pairs of high heels\" is the target, \"luxury goods\" is the context, and \"in the forthcoming month\" is the time. In normal circumstances, once the goal is specified, the elicitation phase can be used to identify salient issues but not in this case as the need behind buying the shoes (wedding, sport, to show off, to feel good, to match with an existing outfit) primes in the decision making and therefore in the resulted behaviour.\n\nAlso, while the pertinent and central beliefs for a certain behavior may be very different for different populations, the questionnaire can then be designed, based on results from the elicitation interview, to measure model constructs with attention to cultural issues. After implementation of the questionnaire, thorough analysis should be conducted to assess whether the intervention influenced model constructs associated with intention and behavior. Results and findings from the analysis can be used to develop effective interventions for eliciting behavioral change, especially within nutrition and health but not for luxury or fashion goods where one's need behind his purchase intentions (behavior) are in most social context cases to associate, dissociate or show status.\n\n\n\n"}
{"id": "55820654", "url": "https://en.wikipedia.org/wiki?curid=55820654", "title": "USERN Prize", "text": "USERN Prize\n\nUSERN prize is an international award, established by USERN which is annually bestowed on scientists or researchers less than 40 years of age operating in the top 1% of their field and for a significant advancement or achievement in research, scientific education, or serving humanity in five scientific fields including medical sciences, life sciences, formal sciences, physical sciences, and social sciences.\n\nFloris De Lange (Netherlands) in Social Sciences\n\nFor: Expectation sharpens the visual response\n\nAlexander Leemans (Netherlands) in Medical Sciences\n\nFor: Processing and Visualization in Diffusion Imaging\n\nJamshid Aghaei (Iran) in Physical Sciences\n\nFor: Evaluating Technical Benefits and Risks of Renewable Energy Sources Increasing Penetration in Electrical Networks\n\nMorteza Mahmoudi (Iran/USA) in Biological Sciences\n\nFor: Defining the Biological Identity of Nanotherapeutics for High Yield Cancer Therapy\n\nLucas Joppa (USA) in Formal Sciences\n\nFor: Technology for Nature\n\nMatjaž Perc (Slovenia) in Social Sciences\n\nFor: Transitions Towards Cooperation in Human Societies\n\nLucina Qazi Uddin (USA) in Medical Sciences\n\nFor: Brain Dynamics and Flexible Behavior in Autism and Attention-Deficit/Hyperactivity\n\nMaria-Magdalena Titirici (UK) in Physical Sciences\n\nFor: The Design of Efficient and Low Cost Electrocatalysts Catalysts Without the Use of Critical Metals\n\nValentina Cauda (Italy) in Biological Sciences\n\nFor: Hybrid Immune-Eluding Nanocrystals as Smart and Active Theranostic Weapons Against Cancer -TrojaNanoHorse\n\nManlio De Domenico (Italy) in Formal Sciences\n\nFor: Multilayer Structure and Dynamics of the Physical World: Modeling the Complexity of Systems of Systems\n"}
{"id": "2495705", "url": "https://en.wikipedia.org/wiki?curid=2495705", "title": "Utpala (astronomer)", "text": "Utpala (astronomer)\n\nUtpala or (\"\") is the name of a 10th-century Indian commentator of Vārāha Mihira's \"Brihat Samhitā\". \"Brihat Samhitā\" is a Samhitā text of (Indian astrology and astronomy) . Samhitā is one of three branches of (Samhitā has many other meanings outside ).\n\nHe is known for quoting six verses from \"Surya Siddhanta\" which are not found in its extant version. These six verses can be found in the 'Introduction' by S.Jain to the translation of Surya Siddhānta made by E. Burgess.\n\nHe is also the author of a commentary on Brahmagupta's \"\" (7th century). In this, he is a successor of Prthudaka and a predecessor of Amaraja.\n\nHe was from Kashmir.\n\nIn Sanskrit, the neuter noun \"utpala\" has two meanings, both given by \"\" (a lexicon of circa. 400 AD). The first meaning is \"white lotus\" also known as \"kuvalaya\" in Sanskrit, according to . The second meaning of \"utpala\" is a variety of medicinal plant known as \" in Hindi and ', vyādhi, paribhavyam or pāribhavyam, vāpyam, pākalam' according to .\n\nMonier-Williams gives following meanings of \"utpala\": (1) the blossom of the blue lotus \"Nymphaea caerulea\" (-Mahābhārata, Rāmāyana, Suśruta, Raghuvamsa, Meghdoota, etc.),(2) a seed of \"Nymphaea caerulea\" (-Suśruta), (3) the plant \"Costus speciosus\" (-Bhagavata Purāna), (4) any water-lily, any flower, (-lexicons) (5) a particular hell (-Buddhist literature), (6) name of a Nāga, (7) names of several persons, including an astronomer, (8) its feminine form \"utpalā\" meant a river (-Harivamśa), (9) its feminine form \"utpalā\" also meant a kind of cake made of unwinnowed corn (-lexicons);\n\nAn unrelated homonym, compounded from \"ud\" \"apart\" + \"pala\" \"flesh\" means 'fleshless, emaciated' (-lexicons) and is the name of a particular hell (-lexicons).\n\nUtpala is a kind of flower which is usually appeared in the Thangka of Tibetan. It rises from mud, and is like water lily or lotus. Utpala is a symbol of the pure. Several traditional deities of Tibet including Tārā have been depicted holding Utpala flowers in hands. Tārā is a famous deity in Buddhism and is also worshipped by Shakti-worshipping Hindus as well esp. in Eastern India,e.g., Mithila.\n\n\n"}
{"id": "7819937", "url": "https://en.wikipedia.org/wiki?curid=7819937", "title": "Watch Valley", "text": "Watch Valley\n\nThe \"Watch Valley\" covers all the Swiss Jura Arc, from Geneva to Basel, and is the primary location of the Swiss watch-making industry. \n\nBeginning initially in 15th century Geneva, the cradle of the European time keeping industry, and spreading north east through the Jura Mountains, the great majority of the companies related to the Swiss watch-making industry were first established. Continuing to the present day, centuries of tradition, rigor, and the know-how of skilled artisans, has forged the world recognized reputation of the great Swiss watch houses.\n\nSome of the famous watchmakers currently producing in the Watch Valley include Breitling, Corum, Gallet, Girard-Perregaux, Movado, Patek Philippe, Rolex, TAG Heuer, Tissot, Ulysse Nardin, Chopard and others.\n\nThe distance across the watch Valley, traveling from Geneva to Basel, is approximately . \n\nBoth Le Locle and its geographical twin town La Chaux-de-Fonds have now been recognised as an UNESCO World Heritage Site, for their horological and related cultural past.\n\n\n"}
