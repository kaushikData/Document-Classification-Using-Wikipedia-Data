{"id": "8753482", "url": "https://en.wikipedia.org/wiki?curid=8753482", "title": "15810 Arawn", "text": "15810 Arawn\n\n15810 Arawn, provisional designation , is a trans-Neptunian object (TNO) from the inner regions of the Kuiper belt, approximately in diameter. It belongs to the plutinos, the largest class of resonant TNOs. It was named after Arawn, the ruler of the Celtic underworld, and discovered on 12 May 1994, by astronomers Michael Irwin and Anna Żytkow with the 2.5-metre Isaac Newton Telescope at La Palma Observatory in the Canary Islands, Spain.\n\nArawn is unique in that it has been observed at a much closer distance than most Kuiper belt objects, by the \"New Horizons\" spacecraft, which imaged it a distance of in April 2016; this and its other observations have allowed its rotation period to be determined.\n\nArawn is moving in a relatively eccentric orbit entirely beyond the orbit of Neptune. With a semi-major axis of 39.4 AU, it orbits the Sun once every 247 years and 6 months (90,409 days). Its orbit has perihelion (closest approach to the Sun) of 34.7 AU, an aphelion (farthest distance from the Sun) of 44.1 AU, an eccentricity of 0.12 and an inclination of 4° with respect to the ecliptic. It is a plutino, being trapped in a 2:3 mean motion resonance with Neptune, similarly to dwarf planet Pluto, the largest known plutino.\n\nIt measures approximately in diameter, based on an absolute magnitude of 7.6, and estimated albedo of 0.1. In April 2016, a rotation period of 5.47 hours was determined for this minor planet.\n\nIn 2012, Arawn was hypothesized to be in a quasi-satellite loop around Pluto, as part of a recurring pattern, becoming a Plutonian quasi-satellite every 2 Myr and remaining in that phase for nearly 350,000 years. Measurements made by the \"New Horizons\" probe in 2015 made it possible to calculate the motion of Arawn much more accurately. These calculations confirm the general dynamics described in the hypotheses. However, it is not agreed upon among astronomers whether Arawn should be classified as a quasi-satellite of Pluto based on this motion, since its orbit is primarily controlled by Neptune with only occasional smaller perturbations caused by Pluto.\n\nArawn is moving in a very stable orbit, likely as stable as Pluto's. This suggests that it may be a primordial plutino formed around the same time Pluto itself and Charon came into existence. It is unlikely to be relatively recent debris originated in collisions within Pluto's system or a captured object.\n\nArawn is currently relatively close to Pluto. In 2017, it will be only 2.7 AU from Pluto. Before was discovered in 2014, Arawn was the best known target for a flyby by the \"New Horizons\" spacecraft after its Pluto flyby in 2015.\n\nArawn was one of the first objects targeted for distant observations by \"New Horizons\", which were taken on 2 November 2015. More observations were made in April 2016.\n\nOn 2 November 2015, Arawn was imaged by the LORRI instrument aboard \"New Horizons\", making it the closest observation of a Kuiper belt object other than the Pluto–Charon system by a factor of 15.\n\nBetween 7–8 April 2016, \"New Horizons\" imaged Arawn from a new record distance of about 111 million kilometers, using the LORRI instrument. The new images allowed the science team at Southwest Research Institute (SwRI) in Boulder, Colorado, to further pinpoint the location of Arawn to within 1000 kilometers. The new data also made it possible for scientists to observe Arawn's rotation period, which was determined to be 5.47 hours.\n\n"}
{"id": "16524868", "url": "https://en.wikipedia.org/wiki?curid=16524868", "title": "Adams Glacier (Wilkes Land)", "text": "Adams Glacier (Wilkes Land)\n\nAdams Glacier (), also known as John Quincy Adams Glacier, is a broad channel glacier in Wilkes Land, Antarctica which is over long. It debouches into the head of Vincennes Bay, just east of Hatch Islands. It was first mapped in 1955 by G.D. Blodgett from aerial photographs taken by U.S. Navy Operation Highjump (1947), and it was named by the Advisory Committee on Antarctic Names (US-ACAN) for John Quincy Adams, sixth President of the United States. Adams was instrumental while later serving as U.S. representative from Massachusetts in gaining congressional authorization of the United States Exploring Expedition (1838–42) under Lieutenant Charles Wilkes, and perpetuating the compilation and publication of the large number of scientific reports based on the work of this expedition.\n\n"}
{"id": "2239611", "url": "https://en.wikipedia.org/wiki?curid=2239611", "title": "Alpharetrovirus", "text": "Alpharetrovirus\n\nAlpharetrovirus is a genus of the retroviridae family. It has type C morphology. Members can cause sarcomas, other tumors, and anaemia of wild and domestic birds and also affect rats.\n\nSpecies include the Rous sarcoma virus, avian leukosis virus, and avian myeloblastosis virus.\n\n"}
{"id": "2391570", "url": "https://en.wikipedia.org/wiki?curid=2391570", "title": "Anteclise", "text": "Anteclise\n\nAn anteclise is a large uplifted structure in a continental platform setting, with low-angle, divergent dips, formed by slow and steady central uplift.\n\nThe term is used mostly by Russian geologists. It is often synonymous with the more widespread terms \"high\", \"uplift\" and \"massif\".\n\n\n"}
{"id": "23871249", "url": "https://en.wikipedia.org/wiki?curid=23871249", "title": "Anti-sidereal time", "text": "Anti-sidereal time\n\nAnti-sidereal time and extended-sidereal time are artificial time standards used to analyze the daily variation in the number of cosmic rays received on Earth. Anti-sidereal time has about 364.25 days per year, one day less than the number of days in a year of solar time, 365.25. Thus each anti-sidereal day is longer than a solar day (24 hr) by about four minutes or 24 hr 4 min. Extended-sidereal time has about 367.25 days per year, one day more than the number of days in a year of sidereal time, 366.25. Thus each extended-sidereal day is shorter than a sidereal day (23 hr 56 min) by about four minutes or 23 hr 52 min. All years mentioned have the same length.\n\nCosmic rays received on Earth exhibit daily variations in amplitude in solar time due to the distribution of cosmic rays in the inner heliosphere and to the Compton-Getting effect caused by Earth's orbital velocity around the Sun. Other daily variations in amplitude in sidereal time are caused by the anisotropy in the direction from which cosmic rays are received relative to the plane of our galaxy, the Milky Way. Both are contaminated by an annual seasonal variation. The daily solar variation is amplitude modulated by the seasonal variation of , producing sidebands on either side of the solar frequency, about , of and . Similarly, an annual amplitude modulation of of the sidereal frequency, about , produces sidebands of and . The upper sideband of the solar frequency contaminates the amplitude of the sidereal frequency, while the lower sideband of the sidereal frequency contaminates the amplitude of the solar frequency. Because the magnitudes of the two sidebands produced by amplitude modulation of the solar frequency are the same and no known natural phenomenon recurs at , the spurious amplitude at the sidereal frequency can be corrected by subtracting any signal present at the anti-sidereal frequency. Similarly, the spurious amplitude in the solar frequency of per year can be corrected by subtracting any signal present at the extended-sidereal frequency of per year.\n\n"}
{"id": "10923676", "url": "https://en.wikipedia.org/wiki?curid=10923676", "title": "Apple Multimedia Lab", "text": "Apple Multimedia Lab\n\nThe Apple Multimedia Lab was a pioneering<ref name=\"NYT, 2/1997\"></ref> electronic media research group operated by Apple Computer. It was founded in 1987 by cognitive psychologist Kristina Hooper Woolsey and educational psychologist Sueann Ambron.\n"}
{"id": "4446697", "url": "https://en.wikipedia.org/wiki?curid=4446697", "title": "Asimov's Biographical Encyclopedia of Science and Technology", "text": "Asimov's Biographical Encyclopedia of Science and Technology\n\nAsimov's Biographical Encyclopedia of Science and Technology is a history of science by Isaac Asimov, written as the biographies of over 1500 scientists. Organized chronologically, beginning with Imhotep (entry \"[1]\") and concluding with Stephen Hawking (entry \"[1510]\"), each biographical entry is numbered, allowing for easy cross-referencing of one scientist with another. Nearly every biographical sketch contains links to other biographies. For example, the article about John Franklin Enders [1195] has the sentence \"Fleming's [1077] penicillin was available thanks to the work of Florey [1213] and Chain [1306] . . .\" This allows one to quickly refer to the articles about Fleming, Florey, and Chain. It includes scientists in all fields including biologists, chemists, astronomers, physicists, mathematicians, geologist, and explorers. The alphabetical list of biographical entries starts with ABBE, Cleveland [738] and ends with ZWORYKIN, Vladimir Kosma [1134]\n\nIn the Second Revised Edition Isaac Newton receives the greatest coverage, a biography of seven pages. Galileo, Faraday and Einstein tie, with five pages each, and Lavoisier and Darwin get four pages each. Dutch writer Gerrit Krol said about the book, \"One of the charms of this encyclopedia is that to each name he adds those with whom this scientist has been in contact.\" The book has been revised several times, by both Asimov himself, and most recently, by his daughter Robyn Asimov.\n"}
{"id": "12386424", "url": "https://en.wikipedia.org/wiki?curid=12386424", "title": "Barton's fracture", "text": "Barton's fracture\n\nA Barton's fracture is an intra-articular fracture of the distal radius with dislocation of the radiocarpal joint.\n\nThere exist two types of Barton's fracture – dorsal and palmar, the latter being more common. The Barton's fracture is caused by a fall on an extended and pronated wrist increasing carpal compression force on the dorsal rim. Intra-articular component distinguishes this fracture from a Smith's or a Colles' fracture. \nTreatment of this fracture is usually done by open reduction and internal fixation with a plate and screws, but occasionally the fracture can be treated conservatively.\nIt is named after John Rhea Barton (1794–1871), an American surgeon who first described this in 1838.\n\n"}
{"id": "9253306", "url": "https://en.wikipedia.org/wiki?curid=9253306", "title": "Blueprint for Disaster", "text": "Blueprint for Disaster\n\nBlueprint for Disaster is a Canadian documentary television series that premiered in 2004 on Discovery Channel Canada. Produced by Temple Street Productions, the program investigates why and how various disasters have happened. Toronto-based Voice Artist Adrian Bell provided the narration for the first series. The show also aired in the UK under the title Seconds from Disaster. , two seasons have been produced.\n\n\n"}
{"id": "5753127", "url": "https://en.wikipedia.org/wiki?curid=5753127", "title": "Botanical illustrator", "text": "Botanical illustrator\n\nA botanical illustrator is a person who paints, sketches or otherwise illustrates botanical subjects. Typical illustrations are in watercolour, but may also be in oils, ink or pencil, or a combination of these. The image may be life size or not, the scale is often shown, and may show the habit and habitat of the plant, the upper and reverse sides of leaves, and details of flowers, bud, seed and root system.\n\nBotanical illustration is sometimes used as a type for attribution of a botanical name to a taxon. The inability of botanists to conserve certain dried specimens, or restrictions on safe transport, have meant illustrations have been nominated as the type for some names. Many minute plants, which may only be viewed under a microscope, are often identified by an illustration to overcome the difficulties in using slide mounted specimens. The standards for this are by international agreement (Art 37.5 of the Vienna Code, 2006)...\nThe use of illustrations was frequently seen in the herbals, seed catalogues and popular works of natural history. The illustrations produced during the eighteenth and nineteenth century are regarded as both appealing and scientifically valid. The finer detail of the printing processes, greatly improving at this time, allowed artists such as Franz and Ferdinand Bauer to depict the minute aspects of the subject. The use of exploded details would further illustrate the description given in the accompanying text. These details allowed a non scientific audience to go some way in identifying the species, the widening interest in natural history and horticulture was an inducement to the production of many Floras and regular publications.\n\nMany books and publication continued to use the illustrators, even after printed matter began to incorporate photography. It would be many years before the colour printing would equal the illustrators plates. The accuracy and craft of the illustrators had developed in tandem with the botanists concerned, the work came to be accepted as important to the botanists and their institutions. The illustrated publication, Curtis's Botanical Magazine (1787), was to eventually appoint an official artist. The 230-year-old magazine, long associated with the Linnaean Society and Kew Gardens, is now primarily one of finer botanical illustration. A stream of the finest illustrators to appear in print have been featured in the magazine.\n\nThe contribution of botanical illustrators continues to be praised and sought, very fine examples continue to be produced. In the 1980s, Celia Rosser undertook to illustrate every \"Banksia\" species for the masterwork, \"The Banksias\". When another species was described after its publication, \"Banksia rosserae\", it was named to honour her mammoth accomplishment. Other illustrators, such as the profuse illustrator Matilda Smith, have been specifically honoured for this work. In 1972 the Smithsonian Institution hired its first botanical illustrator, Alice Tangerini.\n\nNotable botanical illustrators include:\n\n\nThe Linnaean Society of London awards the Jill Smythies Award for botanical illustration.\n\n\n"}
{"id": "4182553", "url": "https://en.wikipedia.org/wiki?curid=4182553", "title": "Carbon dioxide transmission rate", "text": "Carbon dioxide transmission rate\n\nCarbon dioxide transmission rate (COTR) is the measurement of the amount of carbon dioxide gas that passes through a substance over a given period. It is mostly carried out on non-porous materials, where the mode of transport is diffusion, but there are a growing number of applications where the transmission rate also depends on flow through apertures of some description.\n\n\n"}
{"id": "3300308", "url": "https://en.wikipedia.org/wiki?curid=3300308", "title": "Community land trust", "text": "Community land trust\n\nA community land trust (CLT) is a nonprofit corporation that develops and stewards affordable housing, community gardens, civic buildings, commercial spaces and other community assets on behalf of a community. “CLTs” balance the needs of individuals to access land and maintain security of tenure with a community’s need to maintain affordability, economic diversity and local access to essential services.\n\nThe community land trust (CLT) is a neutral and sustainable model of affordable housing and community development that has slowly spread throughout the United States, Canada, and the United Kingdom over the past 40 years. The model was originated in the United States by Ralph Borsodi and Robert Swann, drawing upon earlier examples of planned communities on leased land including the Garden city movement in the United Kingdom, single tax communities in the US, Gramdan villages in India, and moshav communities on lands owned by the Jewish National Fund in Israel. New Communities, Inc., the prototype for the modern-day community land trust, was formed in 1969 near Albany, Georgia, by leaders of the Civil Rights Movement who were seeking a new way to achieve secure access to land for African American farmers.\n\nAccording to the Schumacher Center for a New Economics website, \"Swann was inspired by Ralph Borsodi and by Borsodi's work with J. P. Narayan and Vinoba Bhave, both disciples of Gandhi. Vinoba walked from village to village in rural India in the 1950s and 1960s, gathering people together and asking those with more land than they needed to give a portion of it to their poorer sisters and brothers. The initiative was known as the Bhoodan or Land gift movement, and many of India's leaders participated in these walks.\n\nSome of the new landowners, however, became discouraged. Without tools to work the land and seeds to plant it, without an affordable credit system available to purchase these necessary things, the land was useless to them. They soon sold their deeds back to the large landowners and left for the cities. Seeing this, Vinoba altered the Boodan system to a Gramdan or Village gift system. All donated land was subsequently held by the village itself. The village would then lease the land to those capable of working it. The lease expired if the land was unused. The Gramdan movement inspired a series of regional village land trusts that anticipated Community Land Trusts in the United States.\n\nThe first organization to be labeled with the term 'community land trust' in the U.S., called New Communities, Inc., was founded with the purpose of helping African-American farmers in the rural South to gain access to farmland and to work it with security.\n\nA precursor to this was the Celo Community in North Carolina, which was founded in 1937 by Arthur Ernest Morgan.\n\nRobert Swann worked with Slater King, president of the Albany Movement and a cousin of Martin Luther King, Jr., Charles Sherrod, an organizer for the Student Nonviolent Coordinating Committee, his wife Shirley Sherrod, and individuals from other southern civil rights organizations in the South to develop New Communities, Inc., \"a nonprofit organization to hold land in perpetual trust for the permanent use of rural communities\".\n\nTheir vision for New Communities Inc. drew heavily on the example and experience of the Jewish National Fund (JNF) in making land available through 99-year ground leases for the development of planned communities and agricultural cooperatives. The JNF was founded in 1901 to buy and develop land in Ottoman Palestine (later Israel) for Jewish settlement. By 2007, the JNF owned 13% of all the land in Israel. It has a long and established legal history of leasing land to individuals, to cooperatives, and to intentional communities such as kibbutzim and moshavim. Swann, Slater King, Charles Sherrod, Faye Bennett, director of the National Sharecroppers Fund, and four other Southerners travelled to Israel in 1968 to learn more about ground leasing. They decided on a model that included individual leaseholds for homesteads and cooperative leases for farmland. New Communities Inc. purchased a farm near Albany, Georgia in 1970, developed a plan for the land, and farmed it for 20 years. The land was eventually lost as a result of USDA racial discrimination, but the example of New Communities inspired the formation of a dozen other rural community land trusts in the 1970s. It also inspired and informed the first book about community land trusts, produced by the International Independence Institute in 1972.\n\nRalph Borsodi, Robert Swann, and Erick Hansch founded the International Independence Institute in 1967 to provide training and technical assistance for rural development in the United States and other countries, drawing on the model of the Gramdan villages being developed in India. In 1972, Swann, Hansch, Shimon Gottschalk, and Ted Webster proposed a \"new model for land tenure in America\" in \"The Community Land Trust\", the first book to name and describe this new approach to the ownership of land, housing, and other buildings. One year later, they changed the name of the International Independence Institute to the Institute for Community Economics (ICE).\n\nIn the 1980s, ICE began popularizing a new notion of the CLT, applying the model for the first time to problems of affordable housing, gentrification, displacement, and neighborhood revitalization in urban areas. From 1980–1990, Chuck Matthei, an activist with roots in the Catholic Worker movement and the peace movement, served as Executive Director of ICE, then based in Greenfield, MA and now an affiliate of the National Housing Trust and located in Washington, DC. ICE pioneered the modern community land trust and community loan fund models. During Matthei's tenure, the number of community land trusts increased from a dozen to more than 100 groups in 23 states, creating many hundreds of permanently affordable housing units, as well as commercial and public service facilities. With colleagues Matthei guided the development of 25 regional loan funds and organized the National Association of Community Development Loan Funds, later known as the National Community Capital Association. From 1985–1990, Matthei served as a founding Chairman of the Association and from 1983–1988 he served as a founding board member of the Social Investment Forum, the national professional association in the field of socially responsible investment. Matthei also launched an effort in the early to mid-1980s to address many of the legal and operational questions about CLTs that were arising as banks, public officials and1 by an ecumenical association of churches and ministries created to prevent the displacement of low-income, African-American residents from their neighborhood. During the 1980s, the number of urban CLTs increased dramatically. They were sometimes formed, as in Cincinnati, in opposition to the plans and politics of municipal government. In other cities, like Burlington, Vermont and Syracuse, New York, community land trusts were formed in partnership with a local government. One of the most significant city-CLT partnerships was formed in 1989 when a CLT subsidiary of the Dudley Neighborhood Initiative was granted the power of eminent domain by the City of Boston. others encountered the growing effort to create such community-based organizations around the country.\n\nThe first urban CLT, the Community Land Cooperative of Cincinnati, was founded in 1981.\n\nThere are currently over 250 community land trusts in the United States. In 2006, a national association was established in the United States to provide assistance and support for CLTs: the National Community Land Trust Network. Also established that year to serve as the Network’s training and research arm was the National CLT Academy. Fledgling CLT movements are also underway in England, Canada, Australia, Belgium, Kenya and New Zealand. Similar nationwide networks for promoting and supporting CLTs have recently been formed in the United Kingdom and in Australia.\n\nSince 1992, the defining features of the CLT model in the United States have been enshrined in federal law (). There is considerable variation among the hundreds of organizations that call themselves a community land trust, but ten key features are to be found in most of them.\n\nA community land trust is an independent, nonprofit corporation that is legally chartered in the state in which it is located. Most CLTs are started from scratch, but some are grafted onto existing nonprofit corporations such as community development corporations. Most CLTs target their activities and resources toward charitable activities like providing housing for low-income people and redeveloping blighted neighborhoods, making them eligible to receive 501(c)(3) designation from the IRS.\n\nA nonprofit corporation, the CLT, acquires multiple parcels of land throughout a targeted geographic area with the intention of retaining ownership of the parcels forever. Any building already located on the land or later constructed on the land can be held by the CLT or sold off to an individual homeowner, a cooperative housing corporation, a nonprofit developer of rental housing, or some other nonprofit, governmental, or for-profit entity.\n\nAlthough CLTs intend never to resell their land, they can provide for the exclusive use of their land by the owners of any buildings located thereon. Exclusive use of parcels of land can be conveyed to individual homeowners or to the owners of other types of residential or commercial structures by long-term ground leases. The two-party contract between the landowner (the CLT) and a building's owner protects the owner's interests in security, privacy, legacy, and equity and enforces the CLT's interests in preserving the appropriate use, the structural integrity and the continuing affordability of any buildings on its land.\n\nThe CLT retains an option to repurchase any residential (or commercial) structures on its land if their owners ever choose to sell. The resale price is set by a formula contained in the ground lease that is designed to give present homeowners a fair return on their investment but giving future homebuyers fair access to housing at an affordable price. By design and by intent, the CLT is committed to preserving the affordability of housing (and other structures), one owner after another, one generation after another, in perpetuity.\n\nThe CLT does not disappear once a building is sold. As owner of the underlying land and as owner of an option to repurchase any buildings located on its land, the CLT has an abiding interest in what happens to the structures and to the people who occupy them. The ground lease requires owner-occupancy and responsible use of the premises. Should buildings become a hazard, the ground lease gives the CLT the right to step in and force repairs. Should property owners default on their mortgages, the ground lease gives the CLT the right to step in and cure the default, forestalling foreclosure. The CLT remains a party to the deal, safeguarding the structural integrity of the buildings and the residential security of the occupants.\n\nThe CLT operates within the physical boundaries of a targeted locality. It is guided by and accountable to the people who call the locale their home. Most commonly, any adult who resides on the CLT’s land and any adult who resides within the area deemed by the CLT to be its community can become a voting member of the CLT. The community may encompass a single neighborhood, multiple neighborhoods, or, in some cases, an entire town, city, or county.\n\nTypically, CLTs are run by a board of directors whose members include three groups of stakeholders: residents or leaseholders, people who reside within its targeted community but do not live on its land, and lastly the broader public interest. This third group is frequently represented by government officials, funders, housing agencies, and social service providers. Organization bylaws may designate each of these groups a specific and equal number of seats, and they may be elected separately by their constituent groups.\nControl of the CLT’s board is diffused and balanced to ensure that all interests are heard but that no interest predominates.\n\nCLTs are not focused on a single project located on a single parcel of land. They are committed to an active acquisition and development program aimed at expanding the CLT's holdings of land and increasing the supply of affordable housing (and other types of buildings) under the CLT's stewardship. A CLT's holdings are seldom concentrated in one corner of a community but tend to be scattered throughout its service area, indistinguishable from other owner-occupied housing in the same neighborhood.\n\nThere is enormous variability in the types of projects that CLTs pursue and in the roles they play in developing them. Many CLTs do development with their own staff. Others delegate development to nonprofit or for-profit partners, confining their own efforts to assembling land and preserving the affordability of any structures located upon it. Some CLTs focus on a single type and tenure of housing, like detached, owner-occupied houses. Others take full advantage of the model’s unique flexibility. They develop housing of many types and tenures or they focus more broadly on comprehensive community development, undertaking a diverse array of residential and commercial projects. CLTs around the country have constructed (or acquired, rehabilitated, and resold) single-family homes, duplexes, condos, co-ops, SROs, multi-unit apartment buildings, and mobile home parks. CLTs have created facilities for neighborhood businesses, nonprofit organizations, and social service agencies. CLTs have provided sites for community gardens. vest-pocket parks, and affordable working land for entry-level agriculturalists. Permanently affordable access to land is the common ingredient, linking them all. The CLT is the social thread, connecting them all.\n\nFledgling CLT movements are also underway in:\n Projects have been delivered in the United Kingdom as viable alternatives to \"council housing\" or social housing. One example of this is the Beer Community Land Trust in the village of Beer, Devon.\n\nIn the United States, Community Land Trusts may also be referred to as:\n\n\n\n\nAustralia. www.macll.org.au\n"}
{"id": "4864945", "url": "https://en.wikipedia.org/wiki?curid=4864945", "title": "Contact area", "text": "Contact area\n\nWhen two objects touch, a certain portion of their surface areas will be in contact with each other. Contact area is the fraction of this area that consists of the atoms of one object in contact with the atoms of the other object. Because objects are never perfectly flat due to asperities, the actual contact area (on a microscopic scale) is usually much less than the contact area apparent on a macroscopic scale. Contact area may depend on the normal force between the two objects due to deformation.\n\nThe contact area depends on the geometry of the contacting bodies, the load, and the material properties. The contact area between the two parallel cylinders is a narrow rectangle. Two, non-parallel cylinders have an elliptical contact area, unless the cylinders are crossed at 90 degrees, in which case they have a circular contact area. Two spheres also have a circular contact area.\n\nIt is an empirical fact for many materials that \"F\" = \"μN\", where \"F\" is the frictional force for sliding friction, \"μ\" is the coefficient of friction, and \"N\" is the normal force. There isn't a simple derivation for sliding friction's independence from area.\n\nOne way of determining the actual contact area is to determine it indirectly through a physical process that depends on contact area. For example, the resistance of a wire is dependent on the cross-sectional area, so one may find the contact area of a metal by measuring the current that flows through that area (through the surface of an electrode to another electrode, for example.)\n\n"}
{"id": "1364633", "url": "https://en.wikipedia.org/wiki?curid=1364633", "title": "Cosmic ray visual phenomena", "text": "Cosmic ray visual phenomena\n\nCosmic ray visual phenomena, or \"light flashes\" (LF), are spontaneous flashes of light visually perceived by some astronauts outside the magnetosphere of the Earth, such as during the Apollo program. While LF may be the result of actual photons of visible light being sensed by the retina, the LF discussed here could also pertain to phosphenes, which are sensations of light produced by the activation of neurons along the visual pathway.\n\nResearchers believe that the LF perceived specifically by astronauts in space are due to cosmic rays (high-energy charged particles from beyond the Earth's atmosphere), though the exact mechanism is unknown. Hypotheses include Cherenkov radiation created as the cosmic ray particles pass through the vitreous humour of the astronauts' eyes, direct interaction with the optic nerve, direct interaction with visual centres in the brain, retinal receptor stimulation, and a more general interaction of the retina with radiation.\n\nAstronauts who had recently returned from space missions to the Hubble Space Telescope, the International Space Station and Mir Space Station reported seeing the LF under different conditions. In order of decreasing frequency of reporting in a survey, they saw the LF in the dark, in dim light, in bright light and one reported that he saw them regardless of light level and light adaptation. They were seen mainly before sleeping.\n\nSome LF were reported to be clearly visible, while others were not. They manifested in different colors and shapes. How often each type was seen varied across astronauts' experiences, as evident in a survey of 59 astronauts.\n\nOn Lunar missions, astronauts almost always reported that the flashes were white, with one exception where the astronaut observed \"blue with a white cast, like a blue diamond.\" On other space missions, astronauts reported seeing other colors such as yellow and pale green, though rarely. Others instead reported that the flashes were predominantly yellow, while others reported colors such as orange and red, in addition to the most common colors of white and blue.\n\nThe main shapes seen are \"spots\" (or \"dots\"), \"stars\" (or \"supernovas\"), \"streaks\" (or \"stripes\"), \"blobs\" (or \"clouds\") and \"comets\". These shapes were seen at varying frequencies across astronauts. On the Moon flights, astronauts reported seeing the \"spots\" and \"stars\" 66% of the time, \"streaks\" 25% of the time, and \"clouds\" 8% of the time. Astronauts who went on other missions reported mainly \"elongated shapes\". About 40% of those surveyed reported a \"stripe\" or \"stripes\" and about 20% reported a \"comet\" or \"comets\". 17% of the reports mentioned a \"single dot\" and only a handful mentioned \"several dots\", \"blobs\" and a \"supernova\".\n\nA reporting of motion of the LF was common among astronauts who experienced the flashes. For example, Jerry Linenger reported that during a solar storm, they were directional and that they interfered with sleep since closing his eyes would not help. Linenger tried shielding himself behind the station's lead-filled batteries, but this was only partly effective.\n\nThe different types of directions that the LF have been reported to move in vary across reports. Some reported that the LF travel across the visual field, moving from the periphery of the visual field to where the person is fixating, while a couple of others reported motion in the opposite direction. Terms that have been used to describe the directions are \"sideways\", \"diagonal\", \"in-out\" and \"random\". In Fuglesang \"et al.\" (2006), it was pointed out that there were no reports of vertical motion.\n\nThere appear to be individual differences across astronauts in terms of whether they reported seeing the LF or not. While these LF were reported by many astronauts, not all astronauts have experienced them on their space missions, even if they have gone on multiple missions. For those who did report seeing these LF, how often they saw them varied across reports. On the Apollo 15 mission all three astronauts recorded the same LF, which James Irwin described as \"a brilliant streak across the retina\".\n\nOn Lunar missions, once their eyes became adapted to the dark, Apollo astronauts reported seeing this phenomenon once every 2.9 minutes on average.\n\nOn other space missions, astronauts reported perceiving the LF once every 6.8 minutes on average. The LF were reported to be seen primarily before the astronauts slept and in some cases disrupted sleep, as in the case of Linenger. Some astronauts pointed out that the LF were seemingly perceived more frequently as long as they were perceived at least once before and attention was directed to the perception of them. One astronaut, on his first flight, only took note of the LF after being told to look out for them. These reports are not surprising considering that the LF may not stand out clearly from the background.\n\nApollo astronauts reported that they observed the phenomenon more frequently during the transit to the Moon than during the return transit to Earth. Avdeev \"et al.\" (2002) suggested that this might be due to a decrease in sensitivity to the LF over time while in space. Astronauts on other missions reported a change in the rate of occurrence and intensity of the LF during the course of a mission. While some noted that the rate and intensity increased, others noted a decrease. These changes were said to take place during the first days of a mission. Other astronauts have reported changes in the rate of occurrence of the LF across missions, instead of during a mission. For example, Avdeev himself was on Mir for six months during one mission, six months during the second mission a few years later and twelve months during a third mission a couple of years after. He reported that the LF were seen less frequently with each subsequent flight.\n\nOrbital altitude and inclination have also correlated positively with rate of occurrence of the LF. Fuglesang \"et al.\" (2006) have suggested that this trend could be due to the increasing particles fluxes at increasing altitudes and inclinations.\n\nDuring the Apollo 16 and Apollo 17 transits, astronauts conducted the Apollo Light Flash Moving Emulsion Detector (ALFMED) experiment where an astronaut wore a helmet designed to capture the tracks of cosmic ray particles to determine if they coincided with the visual observation. Examination of the results showed that two of fifteen tracks coincided with observation of the flashes. These results in combination with considerations for geometry and Monte Carlo estimations led researchers to conclude that the visual phenomena were indeed caused by cosmic rays.\n\nThe SilEye-Alteino and Anomalous Long Term Effects in Astronauts' Central Nervous System (ALTEA) projects have investigated the phenomenon aboard the International Space Station, using helmets similar in nature to those in the ALFMED experiment. The SilEye project has also examined the phenomenon on Mir. The purpose of this study was to examine the particle tracks entering the eyes of the astronauts when the astronaut said they observed a LF. In examining the particles, the researchers hoped to gain a deeper understanding of what particles might be causing the LF. Astronauts wore the SilEye detector over numerous sessions while on Mir. During those sessions, when they detected a LF, they pressed a button on a joystick. After each session, they recorded down their comments about the experience. Particle tracks that hit the eye during the time when the astronauts indicated that they detected a LF would have had to pass through silicon layers, which were built to detect protons and nuclei and distinguish between them.\n\nThe findings show that \"a continuous line\" and \"a line with gaps\" was seen a majority of the time. With less frequency, a \"shapeless spot\", a \"spot with a bright nucleus\" and \"concentric circles\" were also reported. The data collected also suggested to the researchers that one's sensitivity to the LF tends to decrease during the first couple of weeks of a mission. With regards to the probable cause of the LF, the researchers concluded that nuclei are likely to be the main cause. They based this conclusion off of the finding that in comparison to an \"All time\" period, an \"In LF time window\" period saw the nucleus rate increase to about six to seven times larger, while the proton rate only increased by twice the amount when comparing the two time periods. Hence, the researchers ruled out the Cherenkov effect as a probable cause of the LF observed in space, at least in this case.\n\nExperiments conducted in the 1970s also studied the phenomenon. These experiments revealed that although several explanations for why the LF were observed by astronauts have been proposed, there may be other causes as well. Charman \"et al.\" (1971) asked whether the LF were the result of single cosmic-ray nuclei entering the eye and directly exciting the eyes of the astronauts, as opposed to the result of Cherenkov radiation within the retina. The researchers had observers view a neutron beam, composed of either 3 or 14 MeV monoenergetic neutrons, in several orientations, relative to their heads. The composition of these beams ensured that particles generated in the eye were below 500 MeV, which was considered the Cherenkov threshold, thereby allowing the researchers to separate one cause of the LF from the other. Observers viewed the neutron beam after being completely dark-adapted.\n\nThe 3 MeV neutron beam produced no reporting of LF whether it was exposed to the observers through the front exposure of one eye or through the back of the head. With the 14 MeV neutron beam, however, LF were reported. Lasting for short periods of time, \"streaks\" were reported when the beam entered one eye from the front. The \"streaks\" seen had varying lengths (a maximum of 2 degrees of visual angle), and were seen to either have a blueish-white color or be colorless. All but one observer reported seeing fainter but a higher number of \"points\" or short lines in the center of visual field. When the beam entered both eyes in a lateral orientation, the number of streaks reported increased. The orientation of the streaks corresponded to the orientation of the beam entering the eye. Unlike in the previous case, the streaks seen were more abundant in the periphery than the center of visual field. Lastly, when the beam entered the back of the head, only one person reported seeing the LF. From these results, the researchers concluded that at least for the LF seen in this case, the flashes could not be due to Cherenkov radiation effects in the eye itself (although they did not rule out the possibility that the Cherenkov radiation explanation was applicable to the case of the astronauts). They also suggested that because the number of LF observed decreased significantly when the beam entered the back of the head, the LF were likely not caused by the visual cortex being directly stimulated as this decrease suggested that the beam was weakened as it passed through the skull and brain before reaching the retina. The most probable explanation proposed was that the LF were a result of the receptors on the retina being directly stimulated and \"turned on\" by a particle in the beam.\n\nIn another experiment, Tobias \"et al.\" (1971) exposed two people to a beam composed of neutrons ranging from 20 to 640 MeV after they were fully dark-adapted. One observer, who was given four exposures ranging in duration from one to 3.5 seconds, observed \"pinpoint\" flashes. The observer described them as being similar to \"luminous balls seen in fireworks, with initial tails fuzzy and heads like tiny stars\". The other observer who was given one exposure lasting three seconds long, reported seeing 25 to 50 \"bright discrete light, he described as stars, blue-white in color, coming towards him\".\n\nBased on these results, the researchers, like in Charman \"et al.\" (1971), concluded that while the Cherenkov effect may be the plausible explanation for the LF experienced by astronauts, in this case, that effect cannot explain the LF seen by the observers. It is possible that the LF observed were the result of interaction of the retina with radiation. They also suggested that the tracks seen may point to tracks that are within the retina itself, with the earlier portions of the streak or track fading as it moves.\n\nConsidering the experiments conducted, at least in some cases the LF observed appear to be caused by activation of neurons along the visual pathway, resulting in phosphenes. However, because the researchers cannot definitively rule out the Cherenkov radiation effects as a probable cause of the LF experienced by astronauts, it seems likely that some LF may be the result of Cherenkov radiation effects in the eye itself, instead. The Cherenkov effect can cause Cherenkov light to be emitted in the vitreous body of the eye and thus allow the person to perceive the LF. Hence, it appears that the LF perceived by astronauts in space have different causes. Some may be the result of actual light stimulating the retina, while others may be the result of activity that occurs in neurons along the visual pathway, producing phosphenes.\n\n"}
{"id": "54051064", "url": "https://en.wikipedia.org/wiki?curid=54051064", "title": "Crown Hill Formation", "text": "Crown Hill Formation\n\nCrown Hill Formation is a late Ediacaran volcanic non-marine sedimentary formation in Newfoundland. It's topped off with a bright red conglomerate, with silt and arkose sands of similar hue too.\n\nIt's subdivided into nine facies, including (on Random Island) Brook Point, Duntara Harbour, Red Cliff (with Bluye Point Horizon subfacies') and Broad Head.\n(on Cape St Mary's) Cross Pt Member, Hurrican Brook Mmbr\n"}
{"id": "24169074", "url": "https://en.wikipedia.org/wiki?curid=24169074", "title": "Discrete dipole approximation codes", "text": "Discrete dipole approximation codes\n\nThis article contains list of discrete dipole approximation codes and their applications.\n\nThe discrete dipole approximation (DDA) is a flexible technique for computing scattering and absorption by targets of arbitrary geometry. Given a target of arbitrary geometry, one seeks to calculate its scattering and absorption properties. The DDA is an approximation of the continuum target by a finite array of polarizable points. The points acquire dipole moments in response to the local electric field. The dipoles of course interact with one another via their electric fields, so the DDA is also sometimes referred to as the coupled dipole approximation. It is closely related to method of moments, digitized Green's function, or volume integral equation method.\n\nThe compilation contains information about the discrete dipole approximation, relevant links, and their applications. There are reviews\nas well as published comparison of existing codes.\n\n\n"}
{"id": "38554875", "url": "https://en.wikipedia.org/wiki?curid=38554875", "title": "Erwin Janchen", "text": "Erwin Janchen\n\nEmil Erwin Alfred Ritter von Janchen-Michel (born 15 May 1882 in Vöcklabruck; died 10 July 1970 in Vienna) was an Austrian botanist.\n\nHe earned his doctorate in 1923 at the University of Vienna. He was scientifically active at the Botanical Institute of the University. He made several research trips. Among the plants first described by Janchen (with co-author Gustav Wendelberger) is the native Austrian wild form of \"Brassica rapa\" subsp. \"silvestris\".\n\n"}
{"id": "31541379", "url": "https://en.wikipedia.org/wiki?curid=31541379", "title": "Expected progeny difference", "text": "Expected progeny difference\n\nExpected progeny differences (EPD) are an evaluation of an animal’s genetic worth as a parent. They are based on animal models which combine all information known about an individual and its relatives to create a genetic profile of the animal’s merits. These profiles are then compared only to other individuals of the same breed.\n\nAn example of a set of EPDs looks like the following chart. Each set of letters stands for a specific measurement with an accuracy reading and percent rank below it. Each EPD is compared to the breed average of a given year. The number given by the EPD is the amount above or below this given average.\nGrowth EPDs measure the amount of weight a given offspring will gain due to the parent’s genetics.\n\nCalving ease predicts the level of difficulty first time heifers will have during birth. These are determined by the percentage of unassisted births for that particular animal.\n\nBirth weight measures how much above or below the breed’s average an offspring will gain due to the parent. It does not necessarily predict the exact weight of all offspring, but instead gives a general prediction of how much extra or less weight an offspring will weigh compared to if it had been sired by another bull. High birth weight is the biggest cause of difficulty in calving, so having a bull with a low birth weight EPD is high beneficial.\n\nWeaning and yearling weight measure the amount of weight an offspring has gained by the time it is weaned and at the one year mark. Typically the weaning weight is measured at the 205-day mark and the yearling weight is taken at the 365-day mark. Typically a larger number is favored for both of these traits.\n\nMilk EPDs give an estimate for the maternal portion of the weaning weight that determined by milk production of the dam. It is measured in pounds of weaning weight of a bull’s grandprogeny due to the milk production of the bull’s daughters.\n\nScrotal circumference is an indicator of a bull’s fertility. A larger circumference is preferred and is an indicator for his sons to have a larger scrotal circumference and his daughters to reach puberty sooner and therefore have calves sooner.\n\nGestation length is an indicator of the probability of dystocia. The longer a calf is in utero the larger it will be at birth and the greater the chance of it having dystocia. It also gives the cow a larger postpartum interval between pregnancies. A shorter gestation length is usually preferred because of this.\n\nStayability is an indicator EPD of longevity of a bull’s daughter in a cow herd. The higher the EPD value the greater chance a cow will stay in a herd over six years and continue producing quality offspring.\n\nCarcass weight predicts what an animal’s total retail product will be compared to other animals of that breed. It does not however predict percent retail value, or the actual amount of sellable meat that can be produced from the carcass.\n\nFat thickness determined the expected external and seam fat the animal will contain. These two factors contribute to the greatest waste in an animal and best way to reduce economic loss. Fat thickness EPDs can help producers reduce this loss, by using animals with mid range EPDs, so as not to have too little or too much fat.\n\nMarbling EPDs are also important in the beef industry for predicting palatability in a beef carcass. They show the estimated USDA Quality grading System and marbling score an animal would receive if it were slaughtered. This EDP is different in that it is measured in units of marbling score, and not in weight gained by an animal. Higher values indicate the presence of genes that will produce more intramuscular fat.\n\nEPDs are used in both scientific research and in typical farm usage. Through mathematical equations and computing power EPDs can be generated for use in both situations. Through EPDs certain phenotypes can be chosen for over others. Use of EPSs and DNA markers can help when choosing certain traits over others. By using certain EPDs of an animal one can rapidly improve genetics of a herd, especially for highly heritable traits such as marbling. In addition, by using a combination of EPDs and DNA markers, EPDs can be made more accurate. When new DNA markers are used, they can help adjust the EPD according to the genotype and therefore produce a more accurate measurement.\n\nTo determine how accurate EPDs are for an individual, samples of all of a bull’s offspring are looked at and compared to what their expected outcome should have been. The EPDs can then be changed based on the values that are gathered. The higher the accuracy rate, the closer most of the progeny will be to the EPD values listed. Accuracy is not an indicator of beneficial EPDs, but rather shows how close the EPD is to the true genetic potential of the animal. The closer an accuracy value is to 1, the more accurate the EPDs can be thought to be. Other factors can affect the progeny as well, such as non genetic effects including feed, weather, environment, as well as random Mendelian sampling.\n\nIn addition to the EPD and accuracy shown in a chart, often the percent rank is also given. This shows what percentile the animal ranks for the given EPD. The higher the percentile, the better the EPD is for that characteristic. The better the EPDs for a given bull, the higher chance its progeny will have a given characteristic.\n"}
{"id": "8747074", "url": "https://en.wikipedia.org/wiki?curid=8747074", "title": "Food-feed system", "text": "Food-feed system\n\nA food-feed system is an integrated livestock-crop production system where crops grown on farms are harvested for human consumption and the crop-residues or by-products are used as feed for livestock. For example, intercropping of cowpeas between rows of cassava. The legumes can enrich soil fertility and enhance the crop productivity. Further the cassava hay can be used as dairy feed.\n"}
{"id": "35201597", "url": "https://en.wikipedia.org/wiki?curid=35201597", "title": "Force matching", "text": "Force matching\n\nForce matching is a research method consisting of test subjects attempting to produce a force that is equal to a set reference force.\n\nA subject’s maximum voluntary contraction (MVC) is recorded and used to normalize both reference forces and results between subjects. During the test subjects are assisted in producing a reference force using various types of feedback (static weight or visual display of force generated). This is followed by an attempt of the subject to generate the reference force without assistance. The duration for both reference and matching tasks is usually four seconds. Results are taken as a mean value of force generated over a time interval set by the researcher. Time intervals are generally one second long and near the end of the attempt. Reference forces are typically set as a percentage of a subject’s MVC while error is typically reported as a percentage of a subject’s MVC.\n\nForce matching has been used by researchers do describe the accuracies of muscle contractions under various conditions. It has been observed that the thumb is more accurate in force matching than fingers are. Impairment of the extensor pollicis longus has not produced a decrease in force matching accuracy of the flexor pollicis longus.\n\n"}
{"id": "2434424", "url": "https://en.wikipedia.org/wiki?curid=2434424", "title": "Forty Six &amp; 2", "text": "Forty Six &amp; 2\n\n\"Forty Six & 2\" is a song by the American progressive metal band Tool. It was released as the fourth single from their third major label release \"Ænima\" in 1997 and received radio airplay.\n\nPopular belief dictates that the song title references an idea first conceived by Carl Jung and later expounded upon by Drunvalo Melchizedek concerning the possibility of reaching a state of evolution at which the body would have two more than the normal 46 total chromosomes and leave a currently disharmonious state. The premise is that humans would deviate from the current state of human DNA which contains 44 autosomes and 2 sex chromosomes. The next step of evolution would likely result in human DNA being reorganized into 46 autosomes and 2 sex chromosomes, according to Melchizedek. \n\nFurthermore, it is believed the song references a wish to experience change through the \"shadow\"; an idea which represents the parts of one's identity that one hates, fears, and represses, this exists as a recurring theme in the work of Carl Jung. \n\nThe song is mostly in 4/4 time with some sections of 7/8 in between. In the intro, Danny Carey plays 4 measures of 7/8 on his ride cymbal over the rest of the band playing in 4/4, and they all meet up on the downbeat of the 5th measure in 4/4. During the bridge there are 3 measures of 7/8 followed by one measure of 4/4. During a particular quad fill, the drums are in 3/8, the guitar plays one measure of 9/8 followed by one in 5/8 all while the bass keeps time in 7/8.\n"}
{"id": "513466", "url": "https://en.wikipedia.org/wiki?curid=513466", "title": "George Graham (clockmaker)", "text": "George Graham (clockmaker)\n\nGeorge Graham (7 July 1673 – 20 November 1751) was an English clockmaker, inventor, and geophysicist, and a Fellow of the Royal Society.\n\nHe was born in Kirklinton, Cumberland. A Friend (Quaker) like his mentor Thomas Tompion, Graham left Cumberland in 1688 for London to work with Tompion. He later married Tompion's niece, Elizabeth Tompion.\n\nGraham was partner to the influential English clockmaker Thomas Tompion during the last few years of Tompion's life. Graham is credited with inventing several design improvements to the pendulum clock, inventing the mercury pendulum and also the orrery. He was made Master of the Worshipful Company of Clockmakers in 1722.\n\nBetween 1730 and 1738, Graham had as an apprentice Thomas Mudge, who went on to be an eminent watchmaker in his own right, and invented the lever escapement, an important development for pocket watches.\n\nHe was widely acquainted with practical astronomy, invented many valuable astronomical instruments, and improved others. Graham made for Edmond Halley the great mural quadrant at Greenwich Observatory, and also the fine transit instrument and the zenith sector used by James Bradley in his discoveries. He supplied the French Academy with the apparatus used for the measurement of a degree of the meridian, and constructed the most complete planetarium known at that time, in which the motions of the celestial bodies were demonstrated with great accuracy. This was made in cabinet form, at the desire of Charles Boyle, 4th Earl of Orrery.\n\nGraham was introduced to John Harrison on the latter's arrival in London, and became a longtime advisor and supporter of Harrison's work on a marine chronometer. Graham and Harrison spent many hours discussing clockwork when first introduced, and Graham gave Harrison an unsecured and interest-free loan to continue his work at this first meeting. Graham later presented Harrison to the Board of Longitude, speaking on his behalf and securing additional funding from the Board.\n\nThe deadbeat escapement is often erroneously credited to George Graham who introduced it around 1715 in his precision regulator clocks. However it was actually invented around 1675 by astronomer Richard Towneley, and first used by Graham's mentor Thomas Tompion in a clock built for Sir Jonas Moore, and in the two precision regulators he made for the new Greenwich Observatory in 1676, mentioned in correspondence between Astronomer Royal John Flamsteed and Towneley\n\nHis major contribution to geophysics was the discovery of the diurnal variation of the terrestrial magnetic field in 1722/23. He was also one of the first to notice long-term secular change in the direction of the compass needle. The compass needles he produced as an instrument-maker were used by many contemporary magneticians. Around 1730, George loaned approximately £200 to John Harrison so that he could start work on his marine timekeeper known later as H1. George was commonly known in the trade as 'Honest George Graham'.\n\n\nHe died at his home in Fleet Street, London and was buried in the same tomb as his friend and mentor Thomas Tompion in Westminster Abbey.\n"}
{"id": "21163118", "url": "https://en.wikipedia.org/wiki?curid=21163118", "title": "Harry Henry", "text": "Harry Henry\n\nHarry Henry (11 March 1916 – 22 November 2008) was born in London on 11 March 1916, as the elder son of an accountant, who died in 1924. His mother brought up and provided for her two sons by working as a dressmaker. Henry was one of Britain's market research pioneers whose contributions to its acceptance and understanding won him an international reputation. He was the last survivor of the 23 founders of the Market Research Society (MRS), which he helped establish in 1947, and which now has 8,000 members in 50 countries. He married Mary Anstey, daughter of Vera Anstey, in 1938. She died in 1989, a year after they had celebrated their golden wedding anniversary.\n\nIn publishing, he was Marketing Director of the Thomson Organisation for nine years, where he also became a Deputy Managing Director. His influence was significant in pulling an old-fashioned conservative industry towards the appreciation and practice of marketing in newspapers and magazines, a benefit inherited by radio and television. He was the first to use the computer in a market research study. In 1988, he received the Market Research Society's rarely awarded Gold Medal. In 2004, he received the Advertising Association's prestigious Mackintosh Medal, awarded for outstanding personal and public service to the industry (in this case, only the third time in the previous fifteen years, and also in recognition of forty years as chairman of its Advertising Statistics Committee).\n\nHe attended two primary schools, one in Finchley and the other in Hackney and at the age of eleven was transferred to Upton House London County Council Central School, in Homerton. This type of school, an early experiment in state secondary education, had just been created for students up to the age of sixteen, who would otherwise be leaving school at fourteen, to extend their general education with the addition of some commercial subjects. He obtained the London Matriculation with Honours, and was awarded a Wedgewood Scholarship to City of London College in 1932. In 1934, he was awarded a Bursary that enabled him to enter the London School of Economics (LSE), where he read Economics, with Statistics as a subsidiary subject.\n\nIn those days, the LSE had less than 3,000 students. Henry served as Senior Treasurer of the Students' Union, Editor of the Clare Market Review, the Union's official magazine (after having edited and largely written Felix, an extremely unofficial one), Chairman of the Social Committee, Chairman of the Literary Society, Business Manager of the Review of Economic Studies (an academic journal), and Vice-Chairman of the Labour Society.\n\nTowards the end of this, he was involved in a libel action arising from an article he wrote in the University of London Union Magazine, pointing out that the gerrymandering involved in the election of the M.P. for London University gave control to the commercial correspondence colleges, whose interests were commercial rather than academic. Pressure from the judge (in chambers) led to a reluctant withdrawal, but there were no damages and he was convinced that Higher Authority had organised a cover-up.\n\nHe obtained a B.Sc. (Economics) in 1937, which qualified him for his first job. Colman Prentis & Varley, a relatively new and extremely small advertising agency, having gained two minor advertising accounts from Procter & Gamble (just beginning to make a presence felt in the UK, under the name of Thomas Hedley) learnt that it would need a market research department. They went to LSE to find somebody to fill this role, in the mistaken belief that what was required was a statistician. Henry was appointed to this post in 1938, and began to learn about market research, largely by experiment. He supplemented this as the London correspondent of the Boston Globe, providing a weekly column on the various social and other activities of the family of Joseph Kennedy, the then US ambassador to London.\n\nThe outbreak of war terminated these apprenticeships, and he was called up in 1940. After training in the ranks of the Royal Artillery, he was commissioned in 1941, and served as a regimental officer in various anti-aircraft batteries until 1943, when he transferred to the newly formed statistical branch of the Army as a Staff Officer. He was eventually posted to the Headquarters staff of Montgomery's 21st Army Group, where he played a modest part in the preparations for the invasion of Europe and in subsequent operations in France, Belgium and Germany.\n\nDuring the latter part of this period, he found the opportunity to use his market research skills in surveys of radio listening among the troops in that theatre of war, and was also responsible for the introduction of a number of what would later be referred to as Information Systems. One of these involved the production of a weekly abstract of administrative statistics for circulation throughout the theatre, and another, the devising and operating of 'TOCCI' (Theatre Officers Central Card Index), a system making use of Hollerith punched-card equipment to facilitate and speed up the demobilisation of officers.\n\nHenry was demobilised with the rank of Major in 1946. He returned to Colman Prentis & Varley, which had begun to move into the big league, first as Market Research Manager and shortly thereafter as Director and General Manager of Market Information Services Ltd, a subsidiary company that he created from scratch. By the time he left Market Information Services in 1954, it had grown into one of the top ten research companies in the country. These years were a period of rapid expansion in the field of market research as a whole, and he took his part in the development of many of the procedures which subsequently came into common use.\n\nHe was one of the 23 people who, in 1946, founded the Market Research Society, and played a major role both in the development of research techniques and in the promotion of market research as a tool of management. His particular interest in the media of communication was stimulated by the fact that he was responsible for the planning, conduct and production from 1947 to 1953 of the Hulton Readership Surveys, which were the predecessors of what became the National Readership Surveys. In this same area, he was responsible for the Hulton Tables of Advertisement Attention Value (1949), the development of the first system for using punched-card equipment to compute net press coverage (1949), and the first national surveys of readership among farmers (1947), among retailers (1948) and among children (1950). He was also Editor and joint author of The Rural Market (1948) and Patterns of British Life (1950).\n\nIn 1954, he joined McCann-Erickson Advertising Ltd. as Director of Research, and there built up a very sizeable research department, more closely integrated with the overall workings of the agency than was usual in those days. In particular, it became agency policy to persuade clients (particularly new clients) that full-scale research into their markets would result in more effective advertising. In 1959, he launched Marplan Ltd., a subsidiary company, which developed along these lines into what was one of the leading market research agencies in Great Britain. He organised the establishment of sister Marplan companies in Belgium, France and Germany, and was Chairman of the McCann-Erickson European Research Committee, while continuing as a senior member of the board of McCann-Erickson Ltd.\n\nDuring the mid-1950s, he had come to the conclusion that the existing techniques of market research, though adequate to the measurement of consumer behaviour, could not offer a reliable framework for marketing decisions. This led to the publication of his 1958 book, \"Motivation Research: Its Practice and Uses\", the first on the subject to be published in Europe. In this book, he drew on his experience to produce a practical guide to the rigorous scientific use of indirect techniques to produce insights which, he argued, could not be validly or even reliably derived from direct questioning or discussion.\n\nIn 1961, he was invited to join the Board of The Thomson Organisation Ltd, then principally (apart from its ownership of Scottish Television) the publisher of the Sunday Times and a raft of regional newspapers, as Director of Marketing, bringing marketing into newspaper publishing for the first time.\n\nIn 1962, he established the annual Thomson Medals and Awards for Advertising Research, which, by stimulating fresh thinking on media research problems, were responsible over the following ten years for the formulation and dissemination of a number of fundamental contributions to the theory and practice of advertising. Also in 1962, he took advantage of the relaxation by Nikita Khrushchev of the rigidity of Soviet rule to mount an unprecedented promotion exercise for the recently launched Sunday Times Colour Magazine, which did not immediately return satisfactory advertising revenue. This involved hiring a Tupulev 114 from Aeroflot in which to fly a plane-load of British business tycoons for the week-end, to meet and talk to their opposite numbers in Moscow.\n\nIn 1965, after successfully tendering for the contract from Post Office Telecommunications for the introduction into the UK of 'yellow pages' telephone directories, he set up and was the first Chairman of Thomson Yellow Pages Ltd.\nIn 1968, he and Mick Shields (of Associated Newspapers) were asked by Cecil King, then Chairman of the Newspaper Publishers' Association, to examine the practicability of the introduction into the Association of a marketing function as a means of improving the efficiency of the national newspaper industry and of enabling it better to withstand competition. This led to the establishment of a consortium, the 'NPA Marketing Executive' (which he chaired) and the recruitment of some specialist staff, but which petered out because, as he observed then and at intervals over the next thirty years, the national newspaper publishers would always prefer to hang separately than to hang together. But what might be regarded as its successor was launched in 2002 as the Newspaper Marketing Agency. He left the Thomson Organisation in 1970, setting up his own consultancy in 1971.\n\nAs Consultant Editor and subsequently as Editor-in-Chief of Admap, he consolidated that publication's reputation for intellectual stringency and authority, which sets it aside from the generality of newsy, even gossipy, advertising periodicals. In 1983, he joined Mike Waterson in setting up a modest venture to launch the quarterly Food & Drink Forecast. This developed rapidly, and had grown by 1998 into Information Sciences Ltd and become one of the world's largest private sector suppliers of economic information through its operating arm NTC Research Ltd, and one of the world's major publisher of advertising and media knowledge via Admap, many other trade journals and books, and the World Advertising Research Center.\n\nHis whole career was marked by innovation. He was the first to use a computer LEO, the first UK computer dedicated to business use) to analyse market research data (1959) and the first to do so in a readership survey (for the Western Mail in 1961). He carried out the first national study of public attitudes towards advertising (1954), the first survey of the poster audience (1948), the first readership survey among retailers (1948), the first national readership survey among children(1950), and the first qualitative study of regional newspaper readership (1963). In 1956, he acted as technical consultant to the first Italian national readership survey.\n\nNot surprisingly, he has been much in demand by industry and academic bodies. He was at various times (and sometimes simultaneously) Visiting Professor of Marketing at the University of Bradford and the Cranfield Institute of Technology (now University), and Professor of Marketing and Media Policy at the International Management Centres. He was for twelve years Chairman of the Marketing Communication Research Centre, an ongoing operation funded by a number of major marketing companies which was founded in Bradford in 1972, and subsequently moved to Cranfield when he did. He had chaired the technical sub-committee of the National Readership Survey, and the Research Committees of the Regional Newspaper Advertising Bureau and of British Posters, and was for forty years Chairman of the Research Committee of the Advertising Association. He was a Governor of the History of Advertising Trust, and a member of the editorial boards of the Journal of Advertising History and the International Journal of Advertising.\n\nAs a by-product of his business and academic activities, and in various managerial capacities, he wrote and lectured extensively, in a number of countries, on management, marketing, research and related topics.\nBooks published in earlier years, include \"Patterns of British Life\" (1948) and \"The Rural Market\" (1950).\n\nHis first book, written when he was 21 years old, \"The Insurance Man and his Trade\" (1938), in the 'Fact' library published monthly by Raymond Postgate (1/- cloth board, 6d. paper covers), suggested that burial insurance was 'the grossest form of systematic exploitation ever practised in this country'\n\nA selection of his papers up to 1971 having been published in that year under the title of \"Perspectives in Management, Marketing and Research\".\n\nLater publications included \"Behind the Headlines: Readings in the Economics of the Press\" (1978) and \"The Dynamics of the British Press, 1961 to 1984\" (1986) and he edited the massive Proceedings of the four International Readership Research Symposia of 1981, 1983, 1985 and 1988 (published as \"Readership Research: Theory and Practice\").\n\n"}
{"id": "600342", "url": "https://en.wikipedia.org/wiki?curid=600342", "title": "Hiwi (volunteer)", "text": "Hiwi (volunteer)\n\nThe term Hiwi () is a German abbreviation of the word , meaning \"voluntary assistant\", or more literally, \"willing helper\". During World War II, the term Hiwis gained broad popularity in reference to auxiliary forces recruited from the indigenous Soviet populations in the areas acquired by Nazi Germany in Eastern Europe. Hitler reluctantly agreed to allow recruitment of Soviet citizens in the Rear Areas during Operation Barbarossa. In a short period of time, many of them were moved to combat units. In late 1942, Hiwis comprised 50 percent of the 2nd Panzer Army's 134 Infantry Division, while the 6th Army at the Battle of Stalingrad was composed of 25 percent Hiwis. By 1944, their numbers had grown to 600,000. Both men and women of the Soviet Union were recruited. Veteran Hiwis were practically indistinguishable from the regular German troops, and often served in entire company strengths.\n\nBetween September 1941 and July 1944 the \"SS\" employed thousands of collaborationist auxiliary police recruited as Hiwis directly from the Soviet POW camps. After training, they were deployed for service with Nazi Germany in the General Government and the occupied East.\n\nIn one instance, the German \"SS\" and police inducted, processed, and trained 5,082 Hiwi guards before the end of 1944 at the \"SS\" training camp division of the Trawniki concentration camp set up in the village of Trawniki southeast of Lublin. They were known as the \"Trawniki men\" () and were former Soviet citizen, mostly Ukrainians. Trawnikis were sent to all major killing sites of the \"Final Solution\", which was their primary purpose of training. They took an active role in the executions of Jews at Bełżec, Sobibór, Treblinka II, Warsaw (three times), Częstochowa, Lublin, Lvov, Radom, Kraków, Białystok (twice), Majdanek as well as Auschwitz, and Trawniki itself.\n\nThe term 'Hiwis' acquired a thoroughly negative meaning during World War II when it entered into several other languages in reference to \"Ostlegionen\" as well as volunteers enlisted from occupied territories for service in a number of roles including hands-on shooting actions and guard duties at Extermination camps on top of regular military service, drivers, cooks, hospital attendants, ammunition carriers, messengers, sappers, etc.\n\nIn the context of World War II the term has clear connotations of collaborationism, and in the case of the occupied Soviet territories also of anti-Bolshevism (widely presented as such by the Germans). \n\nGerman historian wrote that there were many different reasons why Soviet citizens volunteered. He argues that the issue has to be seen first and foremost with the German \"Vernichtungskrieg\" (war of annihilation) policy in mind. For example, volunteering allowed Soviet POWs to get out of the barbaric German POW camp system, giving them a much higher chance of survival. During World War II, Nazi Germany engaged in a policy of deliberate maltreatment of Soviet POWs, in contrast to their treatment of British and American POWs. This resulted in some 3.3 to 3.5 million deaths, or 57% of all Soviet POWs. Therefore it becomes very difficult to differentiate between a genuine desire to volunteer, and seeming to volunteer in the hope of a better chance of surviving the war.\n\nA captured Hiwi told his People's Commissariat for Internal Affairs (\"\", or NKVD) interrogators:\n\nSoviet authorities referred to the Hiwis as \"former Russians\" regardless of the circumstances of their joining or their fate at the hands of the NKVD secret police. After the war, thousands attempted to return to their homes in the USSR. Hundreds were captured and prosecuted, charged with treason and therefore guilty of enlistment from the start of judicial proceedings. Most were sentenced to the Gulags, and released under the Khrushchev amnesty of 1955.\n\nThe reliance upon Hiwis exposed a gap between Nazi ideologues and pragmatic German Army commanders. Nazi leaders including Adolf Hitler regarded all Slavs as \"Untermenschen\" and therefore of limited value as volunteers also. On the other hand, the manpower was needed, and German Intelligence had recognised the need to divide the Soviet nationals. The contradiction was sometimes disguised by reclassification of Slavs as Cossacks. Colonel Helmuth Groscurth (Chief of Staff, XI Corps) wrote to General Beck: \"\"It is disturbing that we are forced to strengthen our fighting troops with Russian prisoners of war, who are already being turned into gunners. It's an odd state of affairs that the \"Beasts\" we have been fighting against are now living with us in closest harmony.\"\" The Hiwis may have constituted one quarter of 6th Army's front-line strength, amounting to over 50,000 Slavic auxiliaries serving with the German troops.\n\n\n"}
{"id": "20454705", "url": "https://en.wikipedia.org/wiki?curid=20454705", "title": "Hugh Couchman", "text": "Hugh Couchman\n\nHugh Couchman is a Canadian astronomer and professor at McMaster University. He is a computational astrophysicist who studies the growth of structure in the universe via gravitational N-body simulations.\n\nCouchman received both his undergraduate and graduate degrees from Cambridge University. In his final year as an undergraduate, he took a course in astronomy taught by Martin Reese, which sparked his interest in cosmology. In 1986, he completed his Ph.D. from the Institute of Astronomy at Cambridge.\n\nAfter finishing his PhD, Couchman moved to Canada to begin a postdoctoral fellowship at the Canadian Institute for Theoretical Astrophysics. He spent some time as an assistant professor in the department of Astronomy at the University of Toronto, before beginning a position at the department of physics and astronomy at the University of Western Ontario in 1991.\n\nIn 1999, he moved to the department of physics and astronomy at McMaster University. There he helped found SHARCNET, a consortium of universities in South Western Ontario that have joined high performance computers together by optical fiber. He is also a member of the Virgo Consortium for Cosmological Supercomputer Simulations, an international collaboration which performs state-of-the-art cosmological simulations of the large scale structure of the Universe.\n\n\n\n"}
{"id": "56224952", "url": "https://en.wikipedia.org/wiki?curid=56224952", "title": "Karen Masters", "text": "Karen Masters\n\nKaren Masters (born 1979) is an Associate Professor of Astrophysics interested in galaxy formation who works in Haverford College, Pennsylvania. She is project scientist for the citizen science project Galaxy Zoo, and uses the classifications to study the evolution of galaxies.\n\nMasters was born in Birmingham and attended King Edward VI College, Nuneaton. She completed a BSc in Physics at the University of Oxford in 2000. She received a PhD in Astronomy from Cornell University in 2005, entitled \"Galaxy flows in and around the Local Supercluster\", under the supervision of Martha Haynes and Riccardo Giovanelli.\n\nIn 2005 Masters moved to Harvard University to work as a postdoctoral researcher with John Huchra on a project to make the most complete map of the local Universe. Masters \"unveiled the most complete 3-D map of the local universe (out to a distance of 380 million light-years) ever created\" in 2011 at the 218th meeting of the American Astronomical Society. The map was created using data from the Two-Micron All-Sky Survey.\n\nShe moved to the Institute of Cosmology and Gravitation at the University of Portsmouth in October 2008. She was appointed the Gruber Foundation IAU Fellow in 2008. In 2010 Masters was awarded a Leverhulme Early Career Fellowship, for a project entitled \"Do bars kill spiral galaxies?\". She was promoted to Senior Lecturer in 2014 and Associate Professor in 2015. She has been working on extragalactic astronomy, and in 2018 was appointed as Associate Professor at Haverford College in Pennsylvania.\n\nMasters is the Scientific Spokesperson and Director for Education and Public Engagement for the Sloan Digital Sky Survey.\n\nMasters coordinates the research scientists for Galaxy Zoo, a crowd-sourced galaxy classification project. She has appeared on the BBC Sky At Night.\n\nShe coordinated the She's An Astronomer page for Galaxy Zoo, collating the stories of women from astronomy. In 2014 Masers won the Women of the Future Award for Science. That year she was listed as one of the BBC's top 100 women.\n"}
{"id": "50123105", "url": "https://en.wikipedia.org/wiki?curid=50123105", "title": "Kosmos 57", "text": "Kosmos 57\n\nKosmos 57 was an unmanned Soviet Spacecraft launched on 22 February 1965. The craft was essentially an unmanned version of Voskhod 2. Its primary mission was to test the Volga airlock. The test was successful, but the craft was lost shortly after. The Spaceflight is designated under the Kosmos system, placed with many other Soviet scientific and military satellites.\n\n\nThe unmanned craft was launched three weeks before Voskhod 2. The primary objective of Voskhod 2 was to conduct a spacewalk, which relied on the inflatable Volga airlock. Kosmos 57 was to test the performance of the airlock. The airlock opened and closed successfully and the craft was re-pressurized without flaw.\n\nThe unmanned spacecraft was destroyed on its third orbit around Earth. Two ground control stations, one in Klyuchi and the other in Yelisovo, sent simultaneous commands instructing the craft to depressurize its airlock. The craft interpreted this as an order to begin descent and a propulsion error put the craft into a tumble. Approximately twenty-nine minutes later the craft's automatic self-destruct function activated. The craft was completely destroyed to prevent sensitive information from literally falling into enemy hands. Over 100 pieces of the spacecraft were tracked, falling into the ocean between 31 March and 6 April 1965.\n\n"}
{"id": "764072", "url": "https://en.wikipedia.org/wiki?curid=764072", "title": "Lewis and Clark River", "text": "Lewis and Clark River\n\nThe Lewis and Clark River is a tributary of Youngs River, approximately long, in northwest Oregon in the United States. It drains of the Northern Oregon Coast Range in the extreme northwest corner of the state, entering Youngs River just above its mouth on the Columbia River at Youngs Bay. Near the river's mouth is the site of former Fort Clatsop of the Lewis and Clark Expedition. The river is named for Meriwether Lewis and William Clark.\n\nThe river was called the Netul River by Lewis and Clark and the Native American Clatsop people who were living in the area at the time. It continued to be known as the Netul River until 1925, when it was renamed to honor Lewis and Clark. The river flows through Lewis and Clark National and State Historical Parks, which were collectively designated as a National Historical Park in 2004.\n\nThe river is home to bottom-feeding sturgeon, which is a popular sport fish in the area. It is also home to an extensive salmon repopulation program, just outside Astoria, Oregon proper, that is currently run by the Oregon Department of Fish and Wildlife. The salmon fry, called \"fingerlings\" can be seen writhing and jumping within their net-lined pens along the river's eastern banks. The river also supports runs of wild steelhead and cutthroat trout.\n\n"}
{"id": "2293891", "url": "https://en.wikipedia.org/wiki?curid=2293891", "title": "List of Canadian flags", "text": "List of Canadian flags\n\nThis is a list of flags used in Canada. The Department of Canadian Heritage lays out protocol guidelines for the display of flags, including an order of precedence; these instructions are only conventional, however, and are generally intended to show respect for what are considered important symbols of the state or institutions. The Queen's personal standard is supreme in the order of precedence, followed by those for the monarch's representatives (depending on jurisdiction), the personal flags of other members of the Royal Family, and then the national flag and provincial flags.\n\nMany museums across Canada display historic flags in their exhibits. The Canadian Museum of History, in Hull, Quebec has many culturally important flags in their collections. Settlers, Rails & Trails Inc., in Argyle, Manitoba holds the 2nd largest exhibit - known as the Canadian Flag Collection.\n\nA number of private corporations also use their own flags, but they are often used alongside the Maple Leaf.\n\nA list of corporations with corporate flags:\n\n\n\n"}
{"id": "58847886", "url": "https://en.wikipedia.org/wiki?curid=58847886", "title": "List of botanical gardens in Tamil Nadu", "text": "List of botanical gardens in Tamil Nadu\n\nThis article lists the botanical gardens found in the state of Tamil Nadu, India.\n\n\n"}
{"id": "46670894", "url": "https://en.wikipedia.org/wiki?curid=46670894", "title": "List of countries by iron-ore exports", "text": "List of countries by iron-ore exports\n\nThe following is a list of countries by iron ore exports. Data is for 2012 & 2016, in millions of United States dollars, as reported by The Observatory of Economic Complexity. Currently the top twenty countries (as of 2016) are listed.\n\n"}
{"id": "2794892", "url": "https://en.wikipedia.org/wiki?curid=2794892", "title": "Logan Sapphire", "text": "Logan Sapphire\n\nThe Logan Sapphire is a flawless specimen from Sri Lanka, a cushion-cut stone which possesses a rich deep blue color and is the second largest (blue) sapphire known, weighing 422.99 carats (84.6 g). \n\nThe cushion-cut stone is one of the world's largest and most famous sapphires (its size about that of an egg) that was cut from a crystal mined in Sri Lanka. The Logan Sapphire is named after Mrs. Polly Logan, who donated the gemstone to the Smithsonian Institution in 1960.\n\nThe Logan Sapphire is set in a brooch surrounded by 20 round brilliant cut diamonds weighing, in total, 16 carats (3.2 g). It is currently on display at the National Museum of Natural History in Washington, D.C., alongside the Bismarck Sapphire Necklace and the Hall Sapphire and Diamond Necklace.\n\n"}
{"id": "13595409", "url": "https://en.wikipedia.org/wiki?curid=13595409", "title": "Loop performance", "text": "Loop performance\n\nLoop performance in control engineering indicates the performance of control loops, such as a regulatory PID loop. Performance refers to the accuracy of a control system's ability to track (output) the desired signals to regulate the plant process variables in the most beneficial and optimised way, without delay or overshoot.\n\nRegulatory control loops are critical in automated manufacturing and utility industries like refining, paper and chemicals manufacturing, power generation, among others. They are used to control a particular parameter within a process. The parameter that is being controlled could be temperature, pressure, flow or level of some process. For example, temperature controllers are used in boilers which are used in production of gasoline.\n\nThere are many software applications that help in measuring and analysing the performance of control loops in industrial plants. Benchmarking the loop performance and identifying opportunities for improvement are key drivers for improving plant reliability, production throughput and safe operation.\n"}
{"id": "12795695", "url": "https://en.wikipedia.org/wiki?curid=12795695", "title": "Magic Quadrant", "text": "Magic Quadrant\n\nMagic Quadrant (MQ) refers to a series published by IT consulting firm Gartner of market research reports that rely on proprietary qualitative data analysis methods to demonstrate market trends, such as direction, maturity and participants. Their analyses are conducted for several specific technology industries and are updated every 1–2 years.\n\nGartner rates vendors upon two criteria: \"completeness of vision\" and \"ability to execute\". Using a methodology which Gartner does not disclose, these component scores lead to a vendor position in one of four quadrants:\n\nIt has been pointed out that the criteria for the Magic Quadrant cater more towards investors and large vendors than towards buyers.\n\nMuch of the criticism is focused on the lack of disclosure of the money received from the vendors it rates, raising conflict of interest issues. Also a source of criticism is the lack of disclosure on the vendor's component scores and the lack of transparency in Gartner's methodology used to derive the vendor's position on the MQ map.\n\nThe Magic Quadrant, and analysts in general, also skew the market: according to research conducted by a Scottish lecturer, by applying their methodologies to describe a market, they change that marketplace to fit their tools.\n\nAnother criticism is that open source vendors are not considered sufficiently by analysts like Gartner, as has been published in an online discussion between a VP from Talend and a German Research VP from Gartner.\n\nGartner was the target of a federal lawsuit (filed May 29, 2009) from software vendor, ZL Technologies, challenging the “legitimacy” of Gartner’s Magic Quadrant rating system. Gartner filed a motion to dismiss by claiming First Amendment protection since it contends that its MQ reports contain \"pure opinion,\" which legally means opinions which are not based on fact. The court threw out the ZL case because it lacked a specific complaint. That decision was upheld on appeal.\n\nAnalyzed markets include:\n\n\n"}
{"id": "18962436", "url": "https://en.wikipedia.org/wiki?curid=18962436", "title": "Ohno's law", "text": "Ohno's law\n\nOhno's law was proposed by a Japanese-American biologist Susumu Ohno, saying that the gene content of the mammalian species has been conserved over species not only in the DNA content but also in the genes themselves. That is, nearly all mammalian species have conserved the X chromosome from their primordial X chromosome of a common ancestor.\n\nAs a cytological evidence, in first, mammalian X chromosomes in various species, including human and mouse, have nearly the same size, with the content of about 5% of the genome. Second, for individual gene loci, a number of X-linked genes are common through mammalian species. Examples are found in glucose-6-phosphate dehydrogenase (G6PD), a gene for polypeptide of antihemophilic globulin (AHG or Factor VIII) in hemophilia A and B, and plasma thromboplastin component gene (PTC or Factor IX). Moreover, no instances were found where an X-linked gene in one species was located on an autosome in the other species.\n\nThe content of a chromosome would be changed mainly by mutation after duplication of the chromosome and translocation with other chromosomes. However, in mammals, since the chromosomal sex-determination mechanism would have been established in their earlier stages of evolution, polyploidy would have not occurred due to its incompatibility with the sex-determining mechanism. Moreover, X-autosome translocation would have been prohibited because it might have resulted in detrimental effects for survival to the organism. Thus in mammals, the content of X chromosomes has been conserved after typical 2 round duplication events at early ancestral stages of evolution, at the fish or amphibia (2R hypothesis).\n\nGenes on the long arm of the human X are contained in the monotreme X and genes on the short arm of the human X are distributed on the autosomes of marsupials. Ohno commented to the result that monotremes and marsupials were not considered to be ancestors of true mammals, but they have diverged very early from the main line of mammals. Chloride channel gene (\"CLCN4\") was mapped to the human X but on chromosome 7 of C57BL/6 mice, species of \"Mus musculus\", though the gene is located on X of \"Mus spretus\" and rat.\n\n"}
{"id": "3478532", "url": "https://en.wikipedia.org/wiki?curid=3478532", "title": "On the Sizes and Distances (Aristarchus)", "text": "On the Sizes and Distances (Aristarchus)\n\nOn the Sizes and Distances (of the Sun and Moon) (Περὶ μεγεθῶν καὶ ἀποστημάτων [ἡλίου καὶ σελήνης], \"Peri megethon kai apostematon\") is widely accepted as the only extant work written by Aristarchus of Samos, an ancient Greek astronomer who lived circa 310–230 BCE. This work calculates the sizes of the Sun and Moon, as well as their distances from the Earth in terms of Earth's radius.\n\nThe book was presumably preserved by students of Pappus of Alexandria's course in mathematics, although there is no evidence of this. The \"editio princeps\" was published by John Wallis in 1688, using several medieval manuscripts compiled by Sir Henry Savile. The earliest Latin translation was made by Giorgio Valla in 1488. There is also a 1572 Latin translation and commentary by Frederico Commandino.\n\nThe work's method relied on several observations:\n\nThe rest of the article details a reconstruction of Aristarchus' method and results. The reconstruction uses the following variables:\nAristarchus began with the premise that, during a half moon, the moon forms a right triangle with the Sun and Earth. By observing the angle between the Sun and Moon, \"φ\", the ratio of the distances to the Sun and Moon could be deduced using a form of trigonometry.\n\nFrom the diagram and trigonometry, we can calculate that\n\nThe diagram is greatly exaggerated, because in reality, \"S = 390 L\", and \"φ\" is extremely close to 90°. Aristarchus determined \"φ\" to be a thirtieth of a quadrant (in modern terms, 3°) less than a right angle: in current terminology, 87°. Trigonometric functions had not yet been invented, but using geometrical analysis in the style of Euclid, Aristarchus determined that\n\nIn other words, the distance to the Sun was somewhere between 18 and 20 times greater than the distance to the Moon. This value (or values close to it) was accepted by astronomers for the next two thousand years, until the invention of the telescope permitted a more precise estimate of solar parallax.\n\nAristarchus also reasoned that as the angular size of the Sun and the Moon were the same, but the distance to the Sun was between 18 and 20 times further than the Moon, the Sun must therefore be 18-20 times larger.\n\nAristarchus then used another construction based on a lunar eclipse:\n\nBy similarity of the triangles, formula_3 and formula_4\n\nDividing these two equations and using the observation that the apparent sizes of the Sun and Moon are the same, formula_5, yields\n\nThe rightmost equation can either be solved for \"ℓ/t\"\n\nor \"s/t\"\n\nThe appearance of these equations can be simplified using \"n\" = \"d/ℓ\" and \"x\" = \"s/ℓ\".\n\nThe above equations give the radii of the Moon and Sun entirely in terms of observable quantities.\n\nThe following formulae give the distances to the Sun and Moon in terrestrial units:\n\nwhere \"θ\" is the apparent radius of the Moon and Sun measured in degrees.\n\nIt is unlikely that Aristarchus used these exact formulae, yet these formulae are likely a good approximation to those of Aristarchus.\n\nThe above formulae can be used to reconstruct the results of Aristarchus. The following table shows the results of a long-standing (but dubious) reconstruction using \"n\" = 2, \"x\" = 19.1 (\"φ\" = 87°) and \"θ\" = 1°, alongside the modern day accepted values.\n\nThe error in this calculation comes primarily from the poor values for \"x\" and \"θ\". The poor value for \"θ\" is especially surprising, since Archimedes writes that Aristarchus was the first to determine that the Sun and Moon had an apparent diameter of half a degree. This would give a value of \"θ\" = 0.25, and a corresponding distance to the moon of 80 Earth radii, a much better estimate. The disagreement of the work with Archimedes seems to be due to its taking an Aristarchus statement that the lunisolar diameter is 1/15 of a \"meros\" of the zodiac to mean 1/15 of a zodiacal sign (30°), unaware that the Greek word \"meros\" meant either \"portion\" or 7°1/2; and 1/15 of the latter amount is 1°/2, in agreement with Archimedes' testimony.\n\nA similar procedure was later used by Hipparchus, who estimated the mean distance to the moon as 67 Earth radii, and Ptolemy, who took 59 Earth radii for this value.\n\nSome interactive illustrations of the propositions in \"On Sizes\" can be found here:\n\n\n"}
{"id": "2411838", "url": "https://en.wikipedia.org/wiki?curid=2411838", "title": "Peter Dollond", "text": "Peter Dollond\n\nPeter Dollond (24 February 1731 – 2 July 1820 born Kensington, England) was an English maker of optical instruments, the son of John Dollond. He is known for his successful optics business, and for the invention of the apochromat.\n\nWorking together with his father and subsequently with his younger brother and nephew (George Dollond) he successfully designed and manufactured a number of optical instruments. He is particularly credited with the invention of the triple achromatic lens - i.e., apochromatic lens - in 1763, still in wide use today, though known as the Cooke triplet after a much later 1893 patent.\n\nPeter Dollond worked at first silk weaving with his father, but his father's passion for optics inspired him so much that in 1750 Peter quit the silk business and opened an optical instruments shop in Kennington, London. After two years, his father gave up silk, too, and joined him.\n\nDollond telescopes, for sidereal or terrestrial use, were amongst the most popular in both Great Britain and abroad for a period of over one and half centuries. Admiral Lord Nelson himself owned one. Another had sailed with Captain Cook in 1769 to observe the Transit of Venus.\n\nThe Peter Dollond compound chest microscope is based on improvements to the Cuff-style microscope introduced by British scientific instrument designers Edward Nairne and Thomas Blunt around 1780. Another design was for the Peter Dollond compound monocular Eriometer around 1790 used to accurately measure the thickness and size of wool fibres.\n\nAfter successfully defending a legal challenge to the patent he held for the achromatic lens the business prospered and he successfully sued his rivals for patent infringement. Dollond's reputation, especially with his father being a Fellow of the Royal Society as a result of his development and patenting of the achromat, provided the company with the de facto right of refusal on the best optical flint glass. This privilege permitted Dollond to maintain an edge in quality over competitor's telescopes and optical instruments for many years.\n\nNotable customers also included:\n\n\nDollond & Co merged with Aitchison & Co in 1927 to form Dollond & Aitchison, the well-known British high street chain of opticians.\n\nPeter Dollond's wife was Ann Phillips and they had two daughters, Louise and Anne.\n"}
{"id": "50399682", "url": "https://en.wikipedia.org/wiki?curid=50399682", "title": "Predictive engineering analytics", "text": "Predictive engineering analytics\n\nPredictive engineering analytics (PEA) is a development approach for the manufacturing industry that helps with the design of complex products (for example, products that include smart systems). It concerns the introduction of new software tools, the integration between those, and a refinement of simulation and testing processes to improve collaboration between analysis teams that handle different applications. This is combined with intelligent reporting and data analytics. The objective is to let simulation drive the design, to predict product behavior rather than to react on issues which may arise, and to install a process that lets design continue after product delivery.\n\nIn a classic development approach, manufacturers deliver discrete product generations. Before bringing those to market, they use extensive verification and validation processes, usually by combining several simulation and testing technologies. But this approach has several shortcomings when looking at how products are evolving. Manufacturers in the automotive industry, the aerospace industry, the marine industry or any other mechanical industry all share similar challenges: they have to re-invent the way they design to be able to deliver what their customers want and buy today.\n\nProducts include, besides the mechanics, ever more electronics, software and control systems. Those help to increase performance for several characteristics, such as safety, comfort, fuel economy and many more. Designing such products using a classic approach, is usually ineffective. A modern development process should be able to predict the behavior of the complete system for all functional requirements and including physical aspects from the very beginning of the design cycle.\n\nIn view of cost or fuel economy, manufacturers need to consider ever more new materials and corresponding manufacturing methods. That makes product development more complex, as engineers cannot rely on their decades of experience anymore, like they did when working with traditional materials, such as steel and aluminium, and traditional manufacturing methods, such as casting. New materials such as composites, behave differently when it comes to structural behavior, thermal behavior, fatigue behavior or noise insulation for example, and require dedicated modeling.\n\nOn top of that, as design engineers do not always know all manufacturing complexities that come with using these new materials, it is possible that the \"product as manufactured\" is different from the \"product as designed\". Of course all changes need to be tracked, and possibly even an extra validation iteration needs to be done after manufacturing.\n\nToday's products include lots of sensors that allow them to communicate with each other, and to send feedback to the manufacturer. Based on this information, manufacturers can send software updates to continue optimizing behavior, or to adapt to a changing operational environment. Products will create the internet of things, and manufacturers should be part of it. A product \"as designed\" is never finished, so development should continue when the product is in use. This evolution is also referred to as Industry 4.0, or the fourth industrial revolution. It challenges design teams, as they need to react quickly and make behavioral predictions based on an enormous amount of data.\n\nThe ultimate intelligence a product can have, is that it remembers the individual behavior of its operator, and takes that into consideration. In this way, it can for example anticipate certain actions, predict failure or maintenance, or optimize energy consumption in a self-regulating manner. That requires a predictive model inside the product itself, or accessible via cloud. This one should run very fast and should behave exactly the same as the actual product. It requires the creation of a digital twin: a replica of the product that remains in-sync over its entire product lifecycle.\nConsumers today can get easy access to products that are designed in any part of the world. That puts an enormous pressure on the time-to-market, the cost and the product quality. It's a trend which has been going on for decades. But with people making ever more buying decisions online, it has become more relevant than ever. Products can easily be compared in terms of price and features on a global scale. And reactions on forums and social media can be very grim when product quality is not optimal. This comes on top of the fact that in different parts of the world, consumer have different preferences, or even different standards and regulations are applicable. \nAs a result, modern development processes should be able to convert very local requirements into a global product definition, which then should be rolled out locally again, potentially with part of the work being done by engineers in local affiliates. That calls for a firm globally operating product lifecycle management system that starts with requirements definition. And the design process should have the flexibility to effectively predict product behavior and quality for various market needs.\n\nDealing with these challenges is exactly the aim of a predictive engineering analytics approach for product development. It refers to a combination of tools deployment and a good alignment of processes. Manufacturers gradually deploy the following methods and technologies, to an extent that their organization allows it and their products require it:\n\nIn this multi-disciplinary simulation-based approach, the global design is considered as a collection of mutually interacting subsystems from the very beginning. From the very early stages on, the chosen architecture is virtually tested for all critical functional performance aspects simultaneously. These simulations use scalable modeling techniques, so that components can be refined as data becomes available. Closing the loop happens on 2 levels:\nClosed-loop systems driven product development aims at reducing test-and-repair. Manufacturers implement this approach to pursue their dream of designing right the first time.\n\n1D system simulation, also referred to as 1D CAE or mechatronics system simulation, allows scalable modeling of multi-domain systems. The full system is presented in a schematic way, by connecting validated analytical modeling blocks of electrical, hydraulic, pneumatic and mechanical subsystems (including control systems). It helps engineers predict the behavior of concept designs of complex mechatronics, either transient or steady-state. \nManufacturers often have validated libraries available that contain predefined components for different physical domains. Or if not, specialized software suppliers can provide them. Using those, the engineers can do concept predictions very early, even before any Computer-aided Design (CAD) geometry is available. During later stages, parameters can then be adapted.\n1D system simulation calculations are very efficient. The components are analytically defined, and have input and output ports. Causality is created by connecting inputs of a components to outputs of another one (and vice versa). Models can have various degrees of complexity, and can reach very high accuracy as they evolve. Some model versions may allow real-time simulation, which is particularly useful during control systems development or as part of built-in predictive functionality.\n\n3D simulation or 3D CAE usually comes in somewhat later stages of product development than 1D system simulations, and can account for phenomena that cannot be captured in 1D models. The models can evolve into highly detailed representations that are very application-specific. Those can be very computationally intensive.\n\n3D simulation or 3D CAE technologies were already essential in classic development processes for verification and validation. They often proved their value by speeding up development and avoiding late-stage changes. Now moving into predictive engineering analytics, 3D simulation or 3D CAE is still indispensable. But it evolves, and takes up a role as development driving force. Software suppliers put great effort in making revolutionary improvements, by adding new capabilities and increasing performance on modeling, process and solver side. They try to capture industry knowledge and best practices in application verticals. And they try to bundle solutions for various functional performance aspects in a common platform. Those improvements should keep 3D simulation or 3D CAE in-sync with ever shorter design cycles.\n\nAs the closed-loop systems-driven product development approach requires concurrent development of the mechanical system and controls, strong links must exist between 1D simulation, 3D simulation and control algorithm development. Software suppliers achieve this through offering co-simulation capabilities for (MiL), Software-in-the-Loop (SiL) and Hardware-in-the-Loop (HiL) processes.\n\nAlready when evaluating potential architectures, 1D simulation should be combined with models of control software, as the electronic control unit (ECU) will play a crucial role in achieving and maintaining the right balance between functional performance aspects when the product will operate. During this phase, engineers cascade down the design objectives to precise targets for subsystems and components. They use multi-domain optimization and design trade-off techniques. The controls need to be included in this process. By combining them with the system models in MiL simulations, potential algorithms can be validated and selected. \nIn practice, MiL involves co-simulation between virtual controls from dedicated controller modeling software and scalable 1D models of the multi-physical system. This provides the right combination of accuracy and calculation speed for investigation of concepts and strategies, as well as controllability assessment.\n\nAfter the conceptual control strategy has been decided, the control software is further developed while constantly taking the overall global system functionality into consideration. The controller modeling software can generate new embedded C-code and integrate it in possible legacy C-code for further testing and refinement.\nUsing SiL validation on a global, full-system multi-domain model helps anticipate the conversion from floating point to fixed point after the code is integrated in the hardware, and refine gain scheduling when the code action needs to be adjusted to operating conditions.\n\nSiL is a closed-loop simulation process to virtually verify, refine and validate the controller in its operational environment, and includes detailed 1D and/or 3D simulation models.\n\nDuring the final stages of controls development, when the production code is integrated in the ECU hardware, engineers further verify and validate using extensive and automated HiL simulation. The real ECU hardware is combined with a downsized version of the multi-domain global system model, running in real time. This HiL approach allows engineers to complete upfront system and software troubleshooting to limit the total testing and calibration time and cost on the actual product prototype.\n\nDuring HiL simulation, the engineers verify if regulation, security and failure tests on the final product can happen without risk. They investigate interaction between several ECUs if required. And they make sure that the software is robust and provides quality functionality under every circumstance. When replacing the global system model running in real-time with a more detailed version, engineers can also include pre-calibration in the process. These detailed models are usually available anyway since controls development happens in parallel to global system development.\n\nEvolving from verification and validation to predictive engineering analytics means that the design process has to become more simulation-driven. But that does not at all reduce the importance of physical testing. On the contrary. First of all will final prototype testing for product sign-off always remain the ultimate and crucial step before bringing a product to market. The scale of this task will become even bigger than before, as more conditions and parameters combinations will need to be tested, in a more integrated and complex measurement system that can combine multiple physical aspects, as well as control systems.\n\nBesides, also in other development stages, combining test and simulation in a well aligned process will be essential for successful predictive engineering analytics.\n\nModal testing or experimental modal analysis (EMA) was already essential in verification and validation of pure mechanical systems. It is a well-established technology that has been used for many applications, such as structural dynamics, vibro-acoustics, vibration fatigue analysis, and more, often to improve finite element models through correlation analysis and model updating. The context was however very often trouble-shooting. \nAs part of predictive engineering analytics, modal testing has to evolve, delivering results that increase simulation realism and handle the multi-physical nature of the modern, complex products. Testing has to help to define realistic model parameters, boundary conditions and loads. Besides mechanical parameters, different quantities need to be measured. And testing also needs to be capable to validate multi-body models and 1D multi-physical simulation models. In general a whole new range of testing capabilities (some modal-based, some not) in support of simulation becomes important, and much earlier in the development cycle than before.\n\nAs the number of parameters and their mutual interaction explodes in complex products, testing efficiency is crucial, both in terms of instrumentation and definition of critical test cases. A good alignment between test and simulation can greatly reduce the total test effort and boost productivity.\n\nSimulation can help to analyze upfront which locations and parameters can be more effective to measure a certain objective. And it also allows to investigate the coupling between certain parameters, so that the amount of sensors and test conditions can be minimized.\n\nOn top of that, simulation can be used to derive certain parameters that cannot be measured directly. Here again, a close alignment between simulation and testing activities is a must. Especially 1D simulation models can open the door to a large number of new parameters that cannot directly accessed with sensors.\n\nAs complex products are in fact combinations of subsystems which are not necessarily concurrently developed, systems and subsystems development requires ever more often setups that include partially hardware, partially simulation models and partially measurement input. These hybrid modeling techniques will allow realistic real-time evaluation of system behavior very early in the development cycle. Obviously this requires dedicated technologies as a very good alignment between simulation (both 1D and 3D) and physical testing.\n\nTomorrow's products will live a life after delivery. They will include predictive functionalities based on system models, adapt to their environment, feed information back to design, and more. From this perspective, design and engineering are more than turning an idea into a product. They are an essential part of the digital thread through the entire product value chain, from requirements definition to product in use.\n\nClosing the loop between design and engineering on one hand, and product in use on the other, requires that all steps are tightly integrated in a product lifecycle management software environment. Only this can enable traceability between requirements, functional analysis and performance verification, as well as analytics of use data in support of design. It will allow models to become digital twins of the actual product. They remain in-sync, undergoing the same parameter changes and adapting to the real operational environment.\n"}
{"id": "40193081", "url": "https://en.wikipedia.org/wiki?curid=40193081", "title": "Roseiflexus castenholzii", "text": "Roseiflexus castenholzii\n\nRoseiflexus castenholzii is a thermophilic, filamentous, photosynthetic bacterium that lacks chlorosomes.\n\nThe cell diameter is of 0.8-1.0 micrometres. The bacterium is red to reddish-brown in colour and formed a distinct red bacterial mat in the natural environment. It is able to grow photoheterotrophically under anaerobic light conditions and also chemoheterotrophically under aerobic dark conditions. Optimal growth conditions for this organism are 50 degrees C and pH 7.5-8.0. Its type strain is HLO8T (= DSM 13941T = JCM 11240T).\n\n\n"}
{"id": "48322065", "url": "https://en.wikipedia.org/wiki?curid=48322065", "title": "SHERPA/RoMEO", "text": "SHERPA/RoMEO\n\nSHERPA/RoMEO is a service run by SHERPA to show the copyright and open access self-archiving policies of academic journals.\n\nThe database uses a colour-coding scheme to classify publishers according to their self-archiving policy. This shows authors whether the journal allows preprint or postprint archiving in their copyright transfer agreements. It currently hold records for over 22,000 journals.\n\n"}
{"id": "5575192", "url": "https://en.wikipedia.org/wiki?curid=5575192", "title": "Social psychology (sociology)", "text": "Social psychology (sociology)\n\nIn sociology, social psychology, also known as sociological social psychology or microsociology, is an area of sociology that focuses on social actions and on interrelations of personality, values, and mind with social structure and culture. Some of the major topics in this field are social status, structural \npower, sociocultural change, social inequality and prejudice, leadership and intra-group behavior, social exchange, group conflict, impression formation and management, conversation structures, socialization, social constructionism, social norms and deviance, identity and roles, and emotional labor. The primary methods of data collection are sample surveys, field observations, vignette studies, field experiments, and controlled experiments.\n\nSociological social psychology was born in 1902 with the landmark study by sociologist Charles Horton Cooley, \"Human Nature and the Social Order\", which presented Cooley's concept of the looking glass self. The first textbook in social psychology by a sociologist appeared in 1908—\"Social Psychology\" by Edward Alsworth Ross. The area's main journal was founded as \"Sociometry\" by Jacob L. Moreno in 1937. The journal's name changed to \"Social Psychology\" in 1978, and to \"Social Psychology Quarterly\" in 1979.\n\nIn the 1920s W. I. Thomas contributed the notion of the definition of the situation, with the proposition that became a basic tenet of sociology and sociological social psychology: \"If men define situations as real, they are real in their consequences.\"\n\nOne of the major currents of theory in this area sprang from work by philosopher and sociologist George Herbert Mead at the University of Chicago from 1894 forward. Mead generally is credited as the founder of symbolic interactionism. Mead's colleague and disciple at Chicago, sociologist Herbert Blumer, coined the name of the framework in 1937.\n\nSociologist Talcott Parsons, at Harvard University from 1927 forward, developed a cybernetic theory of action which was adapted to small group research by Parsons' student and colleague, Robert Freed Bales, resulting in a body of observational studies of social interaction in groups using Bales' behavior coding scheme, Interaction Process Analysis. During his 41-year tenure at Harvard, Bales mentored a distinguished group of sociological social psychologists concerned with group processes and other topics in sociological social psychology.\n\nContemporary symbolic interactionism originated out of ideas of George Herbert Mead and Max Weber. In this framework meanings are constructed during social interaction, and constructed meanings influence the process of social interaction. Many symbolic interactionists see the self as a core meaning constructed through social relations, and influencing social relations.\n\nThe structural school of symbolic interactionism uses shared social knowledge from a macro-level culture, natural language, social institution, or organization to explain relatively enduring patterns of social interaction and psychology at the micro-level, typically investigating these matters with quantitative methods. Identity theory, affect control theory, and the Iowa School are major programs of research in this tradition. Identity Theory and Affect Control Theory both focus on how actions control mental states, thereby manifesting the underlying cybernetic nature of the approach, evident in Mead's writings Affect Control Theory provides a mathematical model of role theory and of labeling theory.\n\nProcess symbolic interactionism stems from the Chicago School and considers the meanings underlying social interactions to be situated, creative, fluid, and often contested. Researchers in this tradition frequently use qualitative and ethnographic methods. A journal, \"Symbolic Interaction\", was founded in 1977 by the Society for the Study of Symbolic Interaction as a central outlet for the empirical research and conceptual studies produced by scholars in this area.\n\nPostmodern symbolic interactionists understand the notions of self and identity to be increasingly fragmented and illusory, and consider attempts at theorizing to be meta-narratives with no more authority than other conversations. The approach is presented in detail by \"The SAGE Handbook of Qualitative Research\".\n\nSocial exchange theory emphasizes the idea that social action is the result of \"personal choices\" made in order to maximize benefits and minimize costs. A key component of this theory is the postulation of the \"comparison level of alternatives\", which is the actor's sense of the best possible alternative (i.e. the choice with the highest net benefits or lowest net costs).\n\nTheories of social exchange share many essential features with classical economic theories like rational choice theory. However, social exchange theories differ from economic theories by making predictions about the relationships between persons, and not just the evaluation of goods. For example, social exchange theories have been used to predict human behaviour in romantic relationships by taking into account each actor's subjective sense of costs (i.e., volatility, economic dependence), benefits (i.e., attraction, chemistry, attachment), and comparison level of alternatives (i.e., if any viable alternative mates are available).\n\nExpectation states theory and its popular \"sub-theory\", status characteristics theory, proposes that individuals use available social information to form expectations for themselves and others. Group members use stereotypes about competence to attempt to determine who will be comparatively more skilled in any given task, indicating to whom the group should listen and accord status. Group members use known ability on the task at hand, membership in social categories (race, gender, age, education, etc.), and observed dominance behaviors (glares, rate of speech, interruptions, etc.) to determine everyone's relative ability and assign rank accordingly. While exhibiting dominant behavior or being of a certain race, for instance, has no direct connection to actual ability, implicit cultural beliefs about who is relatively more or less socially valued drive group members to \"act as if\" they believe some people have more useful contributions than others. As such, the theory has been used to explain the rise, persistence, and enactment of status hierarchies.\n\nThis research perspective deals with relationships between large-scale social systems and individual behaviors and mental states including feelings, attitudes and values, and mental faculties. Some researchers focus on issues of health and how social networks bring useful social support to the ill. Another line of research deals with how education, occupation, and other components of social class impact values. Some studies assess emotional variations, especially in happiness versus alienation and anger, among individuals in different structural positions.\n\nSocial influence is a factor in every individual's life. Social influence takes place when one's thoughts, actions and feelings are affected by other people. It is a way of interaction that affects individual behavior and can occur within groups and between groups. It is a fundamental process that affects ways of socialization, conformity, leadership and social change.\n\nAnother aspect of microsociology aims to focus on individual behavior in social settings. One specific researcher in the field, Erving Goffman, claims that humans tend to believe that they are actors on a stage. He explains his theories in his book The Presentation of Self in Everyday Life. He argues that as a result, individuals will further proceed with their actions based on the response of that individual's 'audience' or in other words, the people to whom he is speaking. Much like a play, Goffman believes that rules of conversing and communication exist: to display confidence, display sincerity, and avoid infractions which are otherwise known as embarrassing situations. Breaches of such rules are what make social situations awkward.\n\nFrom the sociological perspective, group processes scholars study how power, status, justice, and legitimacy impact the structure and interactions that take place within groups. Group processes scholars study how group size affects the type and quality of interactions that take place between group members, an area of study initiated by the work of the German social theorist, Georg Simmel. Dyads consist of two people and triads consist of three people, and the fundamental difference is that one person who leaves a dyad dissolves that group whereas the same is not true of a triad. The difference between these two types of groups also indicates the fundamental nature of group size, which is that every additional member of a group increases the group's stability but also decreases the possible amount of intimacy or interactions between any two members. Groups are also distinguished in terms of how and why the members know each other, and this stems from whether they are members of primary groups consisting of close friends and family held together by expressive ties; secondary groups consisting of coworkers, colleagues, classmates, etc. held together by instrumental ties; or reference groups consisting of people who do not necessarily know or interact with each other but who use each other for standards of comparison for appropriate behaviors. Group processes researchers also study interactions between groups, such as in the case of Muzafer Sherif's Robbers Cave Experiment.\n\n\n"}
{"id": "373212", "url": "https://en.wikipedia.org/wiki?curid=373212", "title": "Social research", "text": "Social research\n\nSocial research is a research conducted by social scientists following a systematic plan. Social research methodologies can be classified as quantitative and qualitative.\n\n\nWhile methods may be classified as quantitative or qualitative, most methods contain elements of both. For example, qualitative data analysis often involves a fairly structured approach to coding the raw data into systematic information, and quantifying intercoder reliability. Thus, there is often a more complex relationship between \"qualitative\" and \"quantitative\" approaches than would be suggested by drawing a simple distinction between them.\n\nSocial scientists employ a range of methods in order to analyse a vast breadth of social phenomena: from census survey data derived from millions of individuals, to the in-depth analysis of a single agent's social experiences; from monitoring what is happening on contemporary streets, to the investigation of ancient historical documents. Methods rooted in classical sociology and statistics have formed the basis for research in other disciplines, such as political science, media studies, program evaluation and market research.\n\nSocial scientists are divided into camps of support for particular research techniques. These disputes relate to the historical core of social theory (positivism and antipositivism; structure and agency). While very different in many aspects, both qualitative and quantitative approaches involve a systematic interaction between theory and data. The choice of method often depends largely on what the researcher intends to investigate. For example, a researcher concerned with drawing a statistical generalization across an entire population may administer a survey questionnaire to a representative sample population. By contrast, a researcher who seeks full contextual understanding of an individuals' social actions may choose ethnographic participant observation or open-ended interviews. Studies will commonly combine, or \"triangulate\", quantitative \"and\" qualitative methods as part of a multi-strategy design.\n\nTypically a population is very large, making a census or a complete enumeration of all the values in that population infeasible. A sample thus forms a manageable subset of a population. In positivist research, statistics derived from a sample are analysed in order to draw inferences regarding the population as a whole. The process of collecting information from a sample is referred to as \"sampling\". Sampling methods may be either random (random sampling, systematic sampling, stratified sampling, cluster sampling) or non-random/nonprobability (convenience sampling, purposive sampling, snowball sampling). The most common reason for sampling is to obtain information about a population. Sampling is quicker and cheaper than a complete census of a population.\n\nSocial research is based on logic and empirical observations. Charles C. Ragin writes in his \"Constructing Social Research\" book that \"Social research involved the interaction between ideas and evidence. Ideas help social researchers make sense of evidence, and researchers use evidence to extend, revise and test ideas.\" Social research thus attempts to create or validate theories through data collection and data analysis, and its goal is exploration, description, explanation, and prediction. It should never lead or be mistaken with philosophy or belief. Social research aims to find social patterns of regularity in social life and usually deals with social groups (aggregates of individuals), not individuals themselves (although science of psychology is an exception here). Research can also be divided into pure research and applied research. Pure research has no application on real life, whereas applied research attempts to influence the real world.\n\nThere are no laws in social science that parallel the laws in natural science. A law in social science is a universal generalization about a class of facts. A fact is an observed phenomenon, and observation means it has been seen, heard or otherwise experienced by researcher. A theory is a systematic explanation for the observations that relate to a particular aspect of social life. Concepts are the basic building blocks of theory and are abstract elements representing classes of phenomena. Axioms or postulates are basic assertions assumed to be true. Propositions are conclusions drawn about the relationships among concepts, based on analysis of axioms. Hypotheses are specified expectations about empirical reality derived from propositions. Social research involves testing these hypotheses to see if they are true.\n\nSocial research involves creating a theory, operationalization (measurement of variables) and observation (actual collection of data to test hypothesized relationship). Social theories are written in the language of variables, in other words, theories describe logical relationships between variables. Variables are logical sets of attributes, with people being the \"carriers\" of those variables (for example, gender can be a variable with two attributes: male and female). Variables are also divided into independent variables (data) that influences the dependent variables (which scientists are trying to explain). For example, in a study of how different dosages of a drug are related to the severity of symptoms of a disease, a measure of the severity of the symptoms of the disease is a dependent variable and the administration of the drug in specified doses is the independent variable. Researchers will compare the different values of the dependent variable (severity of the symptoms) and attempt to draw conclusions.\n\nWhen social scientists speak of \"good research\" the guidelines refer to how the science is mentioned and understood. It does not refer to how what the results are but how they are figured. Glenn Firebaugh summarizes the principles for good research in his book \"Seven Rules for Social Research\". The first rule is that \"There should be the possibility of surprise in social research.\" As Firebaugh (p. 1) elaborates: \"Rule 1 is intended to warn that you don't want to be blinded by preconceived ideas so that you fail to look for contrary evidence, or you fail to recognize contrary evidence when you do encounter it, or you recognize contrary evidence but suppress it and refuse to accept your findings for what they appear to say.\"\n\nIn addition, good research will \"look for differences that make a difference\" (Rule 2) and \"build in reality checks\" (Rule 3). Rule 4 advises researchers to replicate, that is, \"to see if identical analyses yield similar results for different samples of people\" (p. 90). The next two rules urge researchers to \"compare like with like\" (Rule 5) and to \"study change\" (Rule 6); these two rules are especially important when researchers want to estimate the effect of one variable on another (e.g. how much does college education actually matter for wages?). The final rule, \"Let method be the servant, not the master,\" reminds researchers that methods are the means, not the end, of social research; it is critical from the outset to fit the research design to the research issue, rather than the other way around.\n\nExplanations in social theories can be idiographic or nomothetic. An idiographic approach to an explanation is one where the scientists seek to exhaust the idiosyncratic causes of a particular condition or event, i.e. by trying to provide all possible explanations of a particular case. Nomothetic explanations tend to be more general with scientists trying to identify a few causal factors that impact a wide class of conditions or events. For example, when dealing with the problem of how people choose a job, idiographic explanation would be to list all possible reasons why a given person (or group) chooses a given job, while nomothetic explanation would try to find factors that determine why job applicants in general choose a given job.\n\nResearch in science and in social science is a long, slow and difficult process that sometimes produces false results because of methodological weaknesses and in rare cases because of fraud, so that reliance on any one study is inadvisable.\n\nThe ethics of social research are shared with those of medical research. In the United States, these are formalized by the Belmont report as:\n\nThe principle of respect for persons holds that (a) individuals should be respected as autonomous agents capable of making their own decisions, and that (b) subjects with diminished autonomy deserve special considerations. A cornerstone of this principle is the use of informed consent.\n\nThe principle of beneficence holds that (a) the subjects of research should be protected from harm, and, (b) the research should bring tangible benefits to society. By this definition, research with no scientific merit is automatically considered unethical.\n\nThe principle of justice states the benefits of research should be distributed fairly. The definition of fairness used is case-dependent, varying between \"(1) to each person an equal share, (2) to each person according to individual need, (3) to each person according to individual effort, (4) to each person according to societal contribution, and (5) to each person according to merit.\"\n\n\"The following list of research methods is not exhaustive:\"\n\n\nThe origin of the survey can be traced back at least early as the Domesday Book in 1086, while some scholars pinpoint the origin of demography to 1663 with the publication of John Graunt's \"Natural and Political Observations upon the Bills of Mortality\". Social research began most intentionally, however, with the positivist philosophy of science in the early 19th century.\nStatistical sociological research, and indeed the formal academic discipline of sociology, began with the work of Émile Durkheim (1858–1917). While Durkheim rejected much of the detail of Auguste Comte's philosophy, he retained and refined its method, maintaining that the social sciences are a logical continuation of the natural ones into the realm of human activity, and insisting that they may retain the same objectivity, rationalism, and approach to causality. Durkheim set up the first European department of sociology at the University of Bordeaux in 1895, publishing his \"Rules of the Sociological Method\" (1895). In this text he argued: main goal is to extend scientific rationalism to human conduct. ... What has been called our positivism is but a consequence of this rationalism.\"\n\nDurkheim's seminal monograph, \"Suicide\" (1897), a case study of suicide rates among Catholic and Protestant populations, distinguished sociological analysis from psychology or philosophy. By carefully examining suicide statistics in different police districts, he attempted to demonstrate that Catholic communities have a lower suicide rate than that of Protestants, something he attributed to social (as opposed to individual or psychological) causes. He developed the notion of objective \"suis generis\" \"social facts\" to delineate a unique empirical object for the science of sociology to study. Through such studies he posited that sociology would be able to determine whether any given society is \"healthy\" or \"pathological\", and seek social reform to negate organic breakdown or \"social anomie\". For Durkheim, sociology could be described as the \"science of institutions, their genesis and their functioning\".\n\nIn the early 20th century innovation in survey methodology were developed that are still dominant. In 1928, the psychologist Louis Leon Thurstone developed a method to select and score multiple items with which to measure complex ideas, such as attitudes towards religion. In 1932, the psychologist Rensis Likert developed the Likert scale where participants rate their agreement with statement using five options from totally disagree to totally agree. Likert like scales remain the most frequently used items in survey. \n\nIn the mid-20th century there was a general—but not universal—trend for American sociology to be more scientific in nature, due to the prominence at that time of action theory and other system-theoretical approaches. Robert K. Merton released his \"Social Theory and Social Structure\" (1949). By the turn of the 1960s, sociological research was increasingly employed as a tool by governments and businesses worldwide. Sociologists developed new types of quantitative and qualitative research methods. Paul Lazarsfeld founded Columbia University's Bureau of Applied Social Research, where he exerted a tremendous influence over the techniques and the organization of social research. His many contributions to sociological method have earned him the title of the \"founder of modern empirical sociology\". Lazarsfeld made great strides in statistical survey analysis, panel methods, latent structure analysis, and contextual analysis. Many of his ideas have been so influential as to now be considered self-evident.\n\n\n\n"}
{"id": "51291798", "url": "https://en.wikipedia.org/wiki?curid=51291798", "title": "The Kingdom of Speech", "text": "The Kingdom of Speech\n\nThe Kingdom of Speech is a critique of Charles Darwin and Noam Chomsky written by Tom Wolfe. The book's criticisms of Chomsky are outlined in an article in \"Harper's Magazine\".\n\nIn the book, Wolfe criticises Darwin and his colleagues for taking partial credit from Alfred Wallace for the theory of evolution and ignoring Wallace's later work on the theory. Wolfe then criticises Noam Chomsky for dismissing Daniel Everett, who disputes Chomsky's claim that all languages are based ultimately on a hard-wired mechanism known as the language acquisition device (LAD). Wolfe argues that speech, not evolution, sets humans apart from animals and is responsible for all of humanity's complex achievements.\n\nIn \"The Guardian\", Steven Poole criticises Wolfe's whole approach to Darwin and dismisses his suggestion that Darwin had no evidence for his theory of evolution of human speech, saying that Darwin 'adduced a lot of evidence at the time, including the geographical distribution of species, comparative anatomy, fossils and the existence of vestigial organs. Today, of course, evolution is observed in real time in the laboratory, among microbes or insects'.\n\nIn \"The Washington Post\", Jerry Coyne agrees that Wolfe 'grossly distorts the theory of evolution'. He also notes that 'Everett didn't slay [Chomsky's theory of] universal grammar: Later linguists found that the Pirahã language indeed had recursion (e.g., \"I want the same hammock you just showed me\"). Finally, the technical notion of \"recursion\" was never the totality of Chomsky's theory anyway. He highlighted the idea in a brief paper in 2003, but his theory always consisted of operations for merging words into bigger and bigger phrases, something no one disputes.' In concluding his review, Coyne states that 'I'm not sure why Wolfe bears such animus against evolution and the use of evidence rather than bluster to support claims about reality. Perhaps his social conservatism has bred such a discomfort with the implications of modern science - that the universe works by natural rather than supernatural or divine laws - that he's compelled to snicker at one of the foundations of modern science: He's called another one, the big bang, \"the nuttiest theory I've ever heard.\"'\n\nIn \"The Times\", Oliver Kamm is equally critical, pointing out that Wolfe doesn't appreciate that Chomsky himself 'is sceptical that the \"language organ\" is a product of natural selection' and that, indeed, some 'scholars believe that Chomsky underestimates the explanatory power of evolutionary theory.' Harry Ritchie in \"The Spectator\" says 'Wolfe is at his best when describing Chomsky's almost religiously cultish, charismatic hold over linguistics', but that Wolfe's 'version of Chomsky's downfall is as wrong as Chomsky certainly is.'. David Z. Morris's in the \"Washington Independent\" points out that Wolfe 'has proven his enduring ability to choose the right moment. Our views of language and human nature are shifting radically and quickly ... \"The Kingdom of Speech\" is traversing the right territory' but he then concludes that the book 'is too loose, too glib, and, in a few places, too glaringly flawed.'\n\nIn some contrast to these opinions, Peter York in \"The Sunday Times\", claims that the geneticist Steve Jones admires Wolfe's grasp of both the Darwin literature and the \"real weaknesses\" of Chomsky's view of language origins. While Everett himself has said Wolfe's book is 'the opinion of someone who has looked carefully at the field for years. Some mistakes are likely his fault. Others are the fault of the field for having been unsuccessful in making itself understandable to the public.' (Although Everett has also tweeted that: 'Chomsky's view of [language] origins is nearly identical to Wolfe's view of evol[ution]. Both simplistic.')\n\nIn \"The Chronicle of Higher Education\", Tom Bartlett interviews both Wolfe and Chomsky, and compares and contrasts Wolfe's book with anthropologist Chris Knight's more 'in-depth' investigation, \"Decoding Chomsky: Science and revolutionary politics\". In Bartlett's interview, Chomsky criticises Wolfe saying his 'errors are so extraordinary that it would take an essay to review them.'\n\nIn his review in the journal \"Counterpunch\", Chris Knight writes that although Wolfe's book is 'awash with screaming errors' , it is beautifully written and the author is correct to point out that where language is concerned, evolutionary scientists still have little idea as to how it could possibly have evolved. On the other hand, Knight continues, Wolfe gets things bizarrely wrong when he describes Noam Chomsky as a ‘heavyweight evolutionary theorist’. Wolfe seems unaware that throughout his career, Chomsky has consistently opposed Darwin’s view that language gradually evolved. According to Knight, this elementary mistake is a sobering example of Wolfe's hopeless grasp of who adopts which side and why in the various scientific debates about language which he attempts to cover. \n\nJohn McWhorter observed in his \"Vox\" review that Wolfe revealed a fundamental misunderstanding of the Chomsky-Everett controversy, and concluded that the author \"ultimately misses the essence of the debate from various angles.\" According to McWhorter's account, Wolfe misidentified both the topic of the discussion (which doesn't revolve around the origin of language, but cognitive mechanisms of language production) and its still inconclusive outcome by wrongly depicting Chomskyan linguists as clear losers and Everett as a \"victorious gladiator in this scholarly clash\".\n\n\n"}
{"id": "2058685", "url": "https://en.wikipedia.org/wiki?curid=2058685", "title": "Thomas theorem", "text": "Thomas theorem\n\nThe Thomas theorem is a theory of sociology which was formulated in 1928 by William Isaac Thomas and Dorothy Swaine Thomas (1899–1977) :\n\nIn other words, the interpretation of a situation causes the action. This interpretation is not objective. Actions are affected by subjective perceptions of situations. Whether there even is an objectively correct interpretation is not important for the purposes of helping guide individuals' behavior. \n\nThe Thomas theorem is not a theorem in the mathematical sense. \n\nIn 1923, W. I. Thomas stated more precisely that any definition of a situation would influence the present. In addition, after a series of definitions in which an individual is involved, such a definition would also \"gradually [influence] a whole life-policy and the personality of the individual himself\". Consequently, Thomas stressed societal problems such as intimacy, family, or education as fundamental to the role of the situation when detecting a social world \"in which subjective impressions can be projected on to life and thereby become real to projectors\".\n\n"}
{"id": "47337977", "url": "https://en.wikipedia.org/wiki?curid=47337977", "title": "Variant of uncertain significance", "text": "Variant of uncertain significance\n\nA variant of uncertain (or unknown) significance (VUS) is an allele, or variant form of a gene, which has been identified through genetic testing, but whose significance to the function or health of an organism is not known. Two related terms are \"Gene of uncertain significance\" (GUS), which refers to a gene which has been identified through genome sequencing, but whose connection to a human disease has not been established, and \"Insignificant mutation\", referring to a gene variant that has no impact on the health or function of an organism. The term \"variant' is favored over \"mutation\" because it can be used to describe an allele more precisely. When the variant has no impact on health it is called a \"benign variant\". When it is associated with a disease it is called a \"pathogenic variant\". A \"pharmacogenomic variant\" has an effect only when an individual takes a particular drug and therefore it is neither benign nor pathogenic.\n\nA VUS is most commonly encountered by people when they get the results of a lab test looking for a variant form - a mutation in a particular gene. For example, many people know that mutations in the BRCA1 gene are involved in the development of breast cancer because of the publicity surrounding Angelina Jolie's preventative treatment. Few people are aware of the immense number of other genetic variants in and around BRCA1 and other genes that may predispose to hereditary breast and ovarian cancer. A recent study of the genes ATM, BRCA1, BRCA2, CDH1, CHEK2, PALB2, and TP53 found 15,311 DNA sequence variants in only 102 patients.\n\nMany of those 15,311 variants have no significant phenotypic effect. That is, a difference can be seen in the DNA sequence, but the differences have no effect on the growth or health of the person. Identifying variants that are significant or likely to be significant is a difficult task, which may require expert human and \"in silico\" analysis, laboratory experiments, and even information theory. In spite of those efforts, many people may be worried about their particular VUS even though it has not been determined to be significant or likely to be significant. Most VUS notices will not be supported .\n\nSickle cell anemia is widely considered to be the first \"molecular disease\". From earlier protein biochemistry, it was known that the disease was caused by a mutation in the β-globin gene. In 1977, in the third of a series of 3 research papers published in \"The Journal of Biological Chemistry\", this mutation was identified as a single base transversion of adenosine to uridine.\n\nIn 2001, an initial draft of the human genome was published by the International Human Genome Sequencing Consortium. With the development of next-generation sequencing, the cost of sequencing has plummeted and the number of human genomes and exomes sequenced each year is increasing dramatically. , the cost of a quality whole genome sequence is $1,000 or less. If the ratio of approximately 20 DNA sequence variants per gene holds over the entire genome (with approximately 20,000 genes) that means that every person who elects to have their genome sequenced will be provided with almost half a million Variants of Unknown Significance. To assist people to understand the meaning of all these variants, classification is a first step.\n\nA consistent variant classification system is central to the use of genomics in patient care. However, in the USA (and probably the rest of the world) there are no federal laws governing the classification of variants or how they are presented to clients. , US government agencies have limited involvement in regulating genomic testing. A publication from May 2015, based on a workgroup from 2013, lists 5 recommended classification categories based on expert opinion. Those categories are \"pathogenic\", \"likely pathogenic\", \"uncertain significance\", \"likely benign\", and \"benign\".\n\nThis category is for variants that are identical to previously-described variants for which there is excellent data indicating it causes a particular disease. This is the type of variant Angelina Jolie has.\n\nThis category is for variants that have never been found before. However, they are in a gene that is known to cause disease, and appear to affect important structure or function of the encoded protein.\n\nThis category is for variants that have not been reported previously. These variants change an amino acid residue that is conserved in the corresponding protein in other mammals.\n\nThis category is for variants that have been seen before, are not exceedingly rare, and for which \"in silico\" analysis predicts a benign effect on the encoded protein.\n\nThis category is for variants that have been seen previously at a higher frequency, \"in silico\" analysis predicts a benign effect on the encoded protein, and a sibling of the patient with the same disease symptoms does not have the variant.\n\nLess than 5% of the human genome encodes proteins, and the rest is associated with non-coding RNA molecules, regulatory DNA sequences, LINEs, SINEs, introns, and sequences for which as yet no function has been determined. Thus, only a small fraction of the almost half-million VUS's that are expected to be identified by whole genome sequencing can be categorized into the 5 categories above, leaving the patient nearly as uninformed about their variants as they would have been without this information.\n\nMost of the base sequences regulating gene expression are found outside of protein-coding sequences, either within introns or outside of genes in intergenic regions. Changes in those regulatory regions can lead to dysfunction of a gene(s) and produce phenotypic effects that can be relevant to health and function.\n\nAn example of a variant in an intergenic enhancer is one that is associated with blond hair color in northern Europeans. The variant in an enhancer of the KITLG gene causes only a 20% change in gene expression, yet causes hair lightening.\n\nAn example of an intronic VUS controlling gene expression is the SNP found in an intron of the FTO gene. The FTO gene encodes the fat mass and obesity-associated protein, and the SNP (or VUS) found in its intron was shown by genome-wide association studies to be associated with an increased risk for obesity and diabetes. The initial assumption was that this mutation was misregulating FTO to cause the disease risk. However, it was later shown that the intronic variant was in fact regulating the distant IRX3 gene and not the FTO gene. That is just one example of how difficult it can be to determine the significance of a VUS even when many research labs are focused on it, and it illustrates that clinicians cannot reliably interpret genetic results that have not been fully clarified by prior research.\n\nThe number of VUS reports makes it impossible to mention all such reports. To give a flavor for some applications in one field, it is perhaps of most interest to focus on breast cancer. Remember, this is only a fraction of the information available world-wide about VUS reports related to breast cancer, and as always, your results may vary.\n\nIn a 2009 US study of over 200 women who received BRCA VUS reports and were surveyed for one year thereafter, distress over the result persisted for the year.\n\nA 2012 survey of patient outcomes in the Netherlands found that, after genetic counseling for BRCA VUS, patients perceived themselves to have different cancer risks than what had been explained to them by genetic counselors, and that this misperception influenced decisions about radical medical procedures.\n\nIn a 2015 study in the UK, where BRCA VUSs occur in 10-20% of tests, 39% of breast cancer specialists taking part in the study did not know how to explain a VUS report to a patient with no family history, and 71% were unsure about the clinical implications of the test reports.\n\n"}
{"id": "2983480", "url": "https://en.wikipedia.org/wiki?curid=2983480", "title": "Walrus HULA", "text": "Walrus HULA\n\nThe Walrus HULA (Hybrid Ultra Large Aircraft) project was a DARPA-funded experiment to create an airship capable of traveling up to 12,000 nautical miles (about 22,000 km) in range, while carrying 500-1000 tons of air cargo. In distinct contrast to earlier generation airships, the Walrus HULA would be a heavier-than-air vehicle and would generate lift through a combination of aerodynamics, thrust vectoring, and gas buoyancy generation and management.\n\nDARPA said advances in envelope and hull materials, buoyancy and lift control, drag reduction and propulsion combined to make this concept feasible. Technologies to be investigated in the initial study phase included vacuum/air buoyancy compensator tanks, which provide buoyancy control without ballast, and electrostatic atmospheric ion propulsion.\n\nThe WALRUS could potentially expand and speed the strategic airlift capability of the United States substantially while simultaneously reducing costs. A smaller scale demonstration was scheduled for 2008, when a small scale version of the WALRUS designed to carry only the capacity of a C-130 Hercules (i.e., 18,000 kg or about 40,000 lbs) was expected to fly.\n\nThe project was cancelled in 2010.\n\n\n\n"}
{"id": "39038931", "url": "https://en.wikipedia.org/wiki?curid=39038931", "title": "World polity theory", "text": "World polity theory\n\nWorld polity theory (also referred to as world society theory, global Neo-institutionalism, and the \"Stanford school\" of global analysis) was developed mainly as an analytical frame for interpreting global relations, structures, and practices. It was developed partly in response to the application of world systems theory. The theory views the world system as a social system with a cultural framework called world polity, which encompasses and influences the actors, such as nations, international organizations, and individuals under it. In other words, according to John Boli and George M. Thomas, \"the world polity is constituted by distinct culture – a set of fundamental principles and models, mainly ontological and cognitive in character, defining the nature and purposes of social actors and action.\" The World polity theory views the primary component of the world society as \"world polity\", which provides a set of cultural norms or directions in which the actors of the world society follow in dealing with problems and general procedures. In contrast to other theories such as neo-realism or liberalism, the theory considers other actors such as the states and institutions to be under the influence of global norms. Although it closely resembles constructivism, world polity theory is to be distinguished from it because \"world-polity theorists have been far more resolute in taking the “cultural plunge” than their constructivism counterparts\". In other words, world polity theory puts more of an emphasis on homogenization than the other. Through globalization, world polity and culture trigger the formation of enactable cultures and organizations while in return cultures and organizations elaborate the world society further.\n\nBeginning in the 1970s with its initiation by John W. Meyer of Stanford University, world polity analysis initially revolved around examining inter-state relations. Simultaneously in the 1970s and also in the 1980s, a significant amount of work was done on international education environment. However, in the 1980s and 1990s due to the noticeable influence of globalization on world culture, the direction of the study shifted towards analyzing the transnational social movement that may amount to a global polity while at the same time attempting to better understand how global polity ideas are implemented through global actors.\n\nThrough a series of empirical studies, Meyer and others observed that new states organize themselves in a significantly similar manner despite their differing needs and background to give strength to their explanation that there is a set norm of forming a new state under the bigger umbrella of world polity.\n\nOther instances suggest a definite presence of world polity:\n\nCritics point to the fact that world polity theory assumes a rather flawless and smooth transfer of norms of world polity to the global actors, which might not always be really plausible. Also, its tendency to focus on the homogenizing effect brings criticisms. World culture theory differs in this aspect from world polity theory because it recognizes that actors find their own identities in relation to the greater global cultural norm instead of simply following what is suggested by the world polity.\n\nAlso, an instance of \"glocalization\" cannot fully be explained by world polity theory. It is a phenomenon by which local values and global cultures converge to create something new.\n\n\n"}
{"id": "40077258", "url": "https://en.wikipedia.org/wiki?curid=40077258", "title": "Władysław Szajnocha", "text": "Władysław Szajnocha\n\nWładysław Szajnocha (1857-1928) was a Polish geologist and paleontologist; son of historian Karol Szajnocha. Rector of the Jagiellonian University (1911-1912 and 1916-1917).\n"}
