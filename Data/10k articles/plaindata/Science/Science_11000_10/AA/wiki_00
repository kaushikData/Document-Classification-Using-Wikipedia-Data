{"id": "54973127", "url": "https://en.wikipedia.org/wiki?curid=54973127", "title": "American Eclipse (book)", "text": "American Eclipse (book)\n\nAmerican Eclipse is a book by David Baron about the popular impression of the 1878 solar eclipse as observed across the United States. Its full subtitle is, \"A Nation's Epic Race to Catch the Shadow of the Moon and Win the Glory of the World\".\n\n\n"}
{"id": "24962404", "url": "https://en.wikipedia.org/wiki?curid=24962404", "title": "Andrew Bell (minister)", "text": "Andrew Bell (minister)\n\nAndrew Bell (5 September 1803 – 27 September 1856) was a Presbyterian minister who was born in London, England, moved to Scotland and emigrated to Upper Canada with his family in 1817.\n\nBell and his family settled at Perth where his father was a minister. He studied under his father until 1823 and then pursued divinity courses in Scotland. He returned to Upper Canada in 1826, his formal studies not completed.\n\nBell had a successful career within the Presbyterian Church. However, some of his most important contributions to Canada came through his hobby, geology. He served on an advisory committee of the Legislative Assembly of Upper Canada where he helped shape the work of the Geological Survey of Canada and, more particularly, the work of William Edmond Logan, who had helped establish that body. His son, Robert Bell, became the director of the survey at a later date.\n\n"}
{"id": "98275", "url": "https://en.wikipedia.org/wiki?curid=98275", "title": "Aphotic zone", "text": "Aphotic zone\n\nThe aphotic zone (aphotic from Greek prefix + \"without light\") is the portion of a lake or ocean where there is little or no sunlight. It is formally defined as the depths beyond which less than 1% of sunlight penetrates. Consequently, bioluminescence is essentially the only light found in this zone. Most food in this zone comes from dead organisms sinking to the bottom of the lake or ocean from overlying waters.\n\nThe depth of the aphotic zone can be greatly affected by such things as turbidity and the season of the year. The aphotic zone underlies the photic zone, which is that portion of a lake or ocean directly affected by sunlight.\n\nDepending on how aphotic zone is defined, the aphotic zone of the ocean begins between depths of roughly to , and extends to the ocean floor. Temperatures can range from roughly to . Unusual and unique creatures dwell in this expanse of pitch black water, such as the gulper eel, the giant squid, the anglerfish, and the vampire squid.\n\nThe aphotic zone is further divided into additional zones: the bathyal zone, the abyssal zone, and the hadal zone. The bathyal zone extends from to . The abyssal zone extends from to . The hadal zone spans from depths of to the ocean floor. Creatures in these areas must be able to live in complete darkness.\n\n"}
{"id": "1951313", "url": "https://en.wikipedia.org/wiki?curid=1951313", "title": "Arthur Erich Haas", "text": "Arthur Erich Haas\n\nArthur Erich Haas (April 30, 1884 in Brno – February 20, 1941 in Chicago) was an Austrian physicist, noted for a 1910 paper he submitted in support of his habilitation as \"Privatdocent\" at the University of Vienna that outlined a treatment of the hydrogen atom involving quantization of electronic orbitals, thus anticipating the Bohr model (1913) by three years.\n\nHaas’ paper, however, was initially rejected and even ridiculed. As noted in his autobiography, Haas recalls \"When I lectured to the Chemical-Physical Society of Vienna ... Lecher ... referred to the presentation during open discussion as a carnival joke\" (the lecture was held during carnival time in Austria, February 1910). Soon thereafter, however, by September 1911 at a physical science convention in Karlsruhe, former detractors of Haas' work acknowledged it with greater enthusiasm as noted in a footnote: \"We do not know what caused [a] change of mind in 1911 and can merely suggest the general trend of thinking at the time: 1910 saw the beginning of a universal shift of opinion of the quantum concept.\"\n\nThe significance of Haas' work lay in the establishment of a relationship between Planck's constant and atomic dimensions, having been first to correctly estimate the magnitude of what is today known as the Bohr radius.\n\n\n\n"}
{"id": "31815801", "url": "https://en.wikipedia.org/wiki?curid=31815801", "title": "Ball Glacier (James Ross Island)", "text": "Ball Glacier (James Ross Island)\n\nBall Glacier is a small glacier separating Redshaw Point from Hamilton Point, flowing north-east to Markham Bay on the south-east side of James Ross Island. It was named by the UK Antarctic Place-names Committee in 1995 after H. William Ball (b. 1926), Keeper of Paleontology, British Museum (Natural History), 1966–86, and author of Falklands Islands Dependencies Survey Scientific Report No. 24 on fossils from the James Ross Island area.\n\n"}
{"id": "15380494", "url": "https://en.wikipedia.org/wiki?curid=15380494", "title": "Batoni (title)", "text": "Batoni (title)\n\nBatoni () is a Georgian word for \"lord\", or \"master\". It is derived from \"patroni\" (პატრონი), the earlier term of similar meaning, and appears in common usage in the 15th century.\n\n\n"}
{"id": "2338629", "url": "https://en.wikipedia.org/wiki?curid=2338629", "title": "Bimkom", "text": "Bimkom\n\nBimkom - Planners for Planning Rights () is an Israeli non-profit organization established in May 1999 by planners and architects seeking to address human rights concerns in their spatial and urban designs. Bimkom's goals include retroactively legalizing illegal construction in Arab neighborhoods and promoting greater public involvement in the planning process. Its strategies include community-planning activities, educational activities, and public out-reach activities.\n\nBimkom has undertaken projects in the Negev Desert of southern Israel to aid Bedouin living in unrecognized villages. A plan for building 1,900 housing units in the Arab village of Isawiya, northwest of the road to Ma'ale Adumim was approved by the Jerusalem Municipality in 2005. According to then Jerusalem mayor Uri Lupolianski, it is one of 50 plans under way for improving the status of East Jerusalem residents and providing a solution for the acute housing shortage. \n\n\n"}
{"id": "7461486", "url": "https://en.wikipedia.org/wiki?curid=7461486", "title": "Brian Wansink", "text": "Brian Wansink\n\nBrian Wansink is an American former researcher and professor who worked in the fields of consumer behavior and marketing research. He is the former executive director of the USDA's Center for Nutrition Policy and Promotion (CNPP) (2007–2009) and held the John S. Dyson Endowed Chair in the Applied Economics and Management Department at Cornell University, where he directed the Cornell Food and Brand Lab.\n\nWansink's lab researched people's food choices and ways to improve those choices. Starting in 2017, problems with Wansink's papers and presentations were brought to wider public scrutiny. These problems included conclusions not supported by the data presented, data and figures duplicated across papers, questionable data including impossible values and incorrect and inappropriate statistical analyses, and data manipulation via \"p-hacking\". The lab had thirteen papers retracted (one twice) and had 15 corrections issued. On September 20, 2018, Cornell determined that Wansink had committed scientific misconduct and removed him from research and teaching, limiting his activity to cooperating with further investigation of his papers; he resigned effective June 30, 2019.\n\nBrian Wansink was born in Sioux City, Iowa. He was raised in a blue-collar family and is the older brother of Craig Wansink, a professor and chair of the Department of Religious Studies at Virginia Wesleyan. Wansink received a B.S. in Business Administration from Wayne State College (Nebraska) in 1982 and an M.A. in Journalism and Mass Communication from Drake University in 1984, followed by a Ph.D. in Marketing (Consumer Behavior) in 1990 from Stanford Graduate School of Business.\n\nWansink is married and has three daughters. His wife is a trained chef.\n\nWansink's first academic appointment was to the faculty of the Tuck School of Business at Dartmouth College (1990–1994). He taught at the Wharton Graduate School of Business at the University of Pennsylvania (1995–1997), and was a marketing, nutritional science, advertising, and agricultural economics professor at the University of Illinois at Urbana-Champaign (1997–2005) before moving to the Department of Applied Economics and Management in the College of Agriculture and Life Sciences at Cornell University in 2005.\n\nHe set up a nonprofit foundation to support his work in 1999.\n\nWansink's research focuses on ways people make choices – for example, how portion sizes affect food intake. Some of his work led to the introduction of mini-size packaging. Another of his papers found that people who eat with someone who is fat will make worse food choices, which the UK NHS described as \"not wholly convincing and does not prove this phenomenon exists in the general population.\" \n\nIn 2005, Wansink's lab published experimental findings in a paper titled, \"Bottomless bowls: why visual cues of portion size may influence intake\". In the \"bottomless bowls study\", an apparatus was built which contained a tube that pumped soup into the bottom of a bowl at a steady rate as the participant ate. Participants who ate from the bottomless bowl consumed more soup than those whose bowls were filled manually (thus were aware of the amount they consumed). In 2007, Wansink received the Ig Nobel Prize in Nutrition for the \"bottomless bowls\" study. The experiment's data and analysis were challenged as part of the review of Wansink's body of work that started in 2017.\nIn 2006, Wansink published \"Mindless Eating: Why We Eat More Than We Think\". It was described as a popular science book combined with a self-help diet book, as each chapter ends with brief advice. The book details Wansink's research into what, how much, and when people eat. The book was cited by the National Action Against Obesity as being helpful in effort to curb obesity in the United States.\n\nWansink was appointed as the executive director of the USDA's Center for Nutrition Policy and Promotion from November 2007 through January 2009. He was responsible for oversight of the 2010 Dietary Guidelines for Americans, MyPyramid.gov, and various other food-related programs administered by the USDA. In 2011, Wansink was elected to a one-year-term as president of the Society for Nutrition Education. \n\nIn a 2009 paper, a team led by Wansink described their finding that calorie counts in \"The Joy of Cooking\" had gone up around 44% since the cookbook's first edition in 1936, and related this to the obesity epidemic. Over time this finding became a shorthand way to refer to the unhealthy western pattern diet. The publisher of \"Joy of Cooking\", John Becker, noticed that Wansink's sample size was small, consisting of only 18 recipes out of about 4500 that were published during the study time interval, and did his own analysis of changes in calories in the recipes. In 2017, after news of Wansink's research practices became widely discussed in the media, Becker sent his results to several statisticians, including James Heathers, a behavioral scientist at Northeastern University. Heathers found Wansink's conclusions to be invalid, and found a number of problems with Wansink's paper, including counting a whole cake as a \"serving\" and comparing a recipe for a clear chicken soup with one for gumbo. \n\nWansink's second book, \"Slim by Design\" was released in 2014. During 2014 he ran a Kickstarter campaign and raised around $10,000 to fund a coaching program based on the book; as of February 2018 the program had not been produced.\n\nWansink's research came under scrutiny in 2017, leading to several retractions and an investigation by Cornell into the integrity of his work. In September 2018, Cornell determined that Wansink had committed scientific misconduct, and removed him from all teaching and research positions; he was restricted solely to cooperating with people investigating his publications. He also resigned from the university, effective June 30, 2019. After the announcement of the findings and his resignation, in an email exchange with \"Buzzfeed\", Wansink acknowledged some problems with his published work and also wrote: \"There was no fraud, no intentional misreporting, no plagiarism, or no misappropriation.\"\n\nIn January 2017, the validity of research from Wansink's labs was called into question by Jordan Anaya, Nicholas J L Brown, and Tim van der Zee, after Wansink had written a blog post about asking a graduate student to \"salvage\" conclusions from a study which had null results, subsequently producing five papers from it, all published with Wansink as co-author. Van der Zee, Anaya, and Brown analysed four of the five papers (referred to as \"the pizza papers\"), and found conclusions not supported by the data presented, and a total of 150 questionable numbers, such as impossible values, incorrect ANOVA results, and dubious p-values. According to the critics, requests for access to the original data were denied by Wansink, who cited privacy issues over the anonymity of the participants. A February 2017 article in \"New York Magazine\" described the pizza papers as \"shockingly unprofessional\" and expressed concern over the journals that published them. \n\nIn response, Wansink announced an in-depth review of the four disputed papers, after locating some of the original datasets, and published a detailed response in March 2017. A few days later, Cornell released a statement confirming that the university administration had conducted a preliminary investigation of Wansink's four pizza papers, and had not found evidence of scientific misconduct. The investigation did find multiple cases of self-plagiarism and confirmed \"numerous instances of inappropriate data handling and statistical analysis\", requiring Wansink to hire independent, external statistical experts to check and reanalyze his own review of the papers.\n\nLater in 2017, Anaya and his colleagues found errors in six other papers published by members of the Cornell Food and Brand Lab. As of December 2017, six papers had been retracted and 14 corrections had been issued. \n\nBy March 2018, two more papers had been retracted and an additional correction made, bringing the totals to 7 (one of them retracted twice, so technically 8) and 15, respectively. In April 2018, \"JAMA (Journal of the American Medical Association)\" issued a \"Notice of Expression of Concern\" about all six articles authored by Wansink in JAMA and JAMA network specialty journals, to alert the scientific community of concerns about the validity of Wansink's research; the notice included a request for Cornell to have the validity of the papers independently assessed. In September 2018 JAMA retracted six papers by Wansink.\n\nThe following papers were retracted:\n\n\n\n"}
{"id": "16800222", "url": "https://en.wikipedia.org/wiki?curid=16800222", "title": "Buck (crater)", "text": "Buck (crater)\n\nBuck is a crater in the Navka region of Venus. It has the terraced walls, flat radar-dark floor, and central peak that are characteristic of craters classified as \"complex\". The central peak on its floor is unusually large. Flow-like deposits extend beyond the limits of the coarser rim deposits on its west and southwest. Like about half of the craters mapped by \"Magellan\" to date, it is surrounded by a local, radar-dark halo.\n"}
{"id": "56516309", "url": "https://en.wikipedia.org/wiki?curid=56516309", "title": "Chanda Prescod-Weinstein", "text": "Chanda Prescod-Weinstein\n\nChanda Prescod-Weinstein is an American cosmologist, science writer and equality activist based at the University of Washington. She is the Principal Investigator on a Foundational Questions Institute (FQXI) grant titled \"Epistemological Schemata of Astro | Physics: A Reconstruction of Observers\".\n\nPrescod-Weinstein was born in El Sereno in East Los Angeles, California, and went to school in the Los Angeles Unified School District. She earned a Bachelor of Arts degree in Physics and Astronomy at Harvard College in 2003. Her thesis, \"A study of winds in active galactic nuclei\", was completed under the supervision of Martin Elvis. She then earned a Master's degree in Astronomy at the University of California, Santa Cruz, working with Anthony Aguirre. In 2010, Prescod-Weinstein completed her Ph.D. dissertation, titled \"Acceleration as Quantum Gravity Phenomenology\", under the supervision of Lee Smolin and Niayesh Afshordi at University of Waterloo, while conducting her research at the Perimeter Institute for Theoretical Physics. In doing so, she became the 63rd African-American woman in history to earn a Ph.D. in Physics.\n\nPrescod-Weinstein's research has focused on various topics in cosmology and theoretical physics, including the axion as a dark matter candidate, inflation, and classical and quantum fields in the early universe. \n\nFrom 2004 to 2007 Prescod-Weinstein was a named National Science Foundation Graduate Research Fellow.\n\nAfter Prescod-Weinstein's Ph.D., she was a NASA Postdoctoral Fellow in the Observational Cosmology Lab at Goddard Space Flight Center. In 2011, she won a Dr. Martin Luther King, Jr. Postdoctoral Fellowship at the Massachusetts Institute of Technology, where she was jointly appointed to the Kavli Institute for Astrophysics and Space Research and the Department of Physics. At MIT, Prescod-Weinstein worked in Alan Guth's group in the Centre for Theoretical Physics.\n\nIn 2016, she became the Principal Investigator on a $100,522 FQXI grant to study “Epistemological Schemata of Astro | Physics: A Reconstruction of Observers” seeking to answer questions regarding how to re-frame who is an \"observer\", to acknowledge those existing outside of the European Enlightenment framework, and how that might change knowledge production in science.\n\nShe is working on the NASA STROBE-X experiment.\n\nPrescod-Weinstein earned the Barbados House Canada Inc. Gordon C Bynoe Scholarship in 2007. In 2013 she won the MIT \"Infinite Kilometer Award\". In March 2017, Prescod-Weinstein won the LGBT+ Physicists Acknowledgement of Excellence Award \"\"For Years of Dedicated Effort in Changing Physics Culture to be More Inclusive and Understanding Toward All Marginalised Peoples\".\" \n\nPrescod-Weinstein is an advocate for increasing the diversity within science by considering intersectionality and proper celebration of the underrepresented groups who contribute to scientific knowledge production. She has been a member of the executive committee of the National Society of Black Physicists. In 2017 she was a plenary speaker at the Women in Physics Canada meeting.\n\nPrescod-Weinstein has contributed popular science articles for \"Slate\", \"American Scientist\", \"Nature Astronomy\", \"Bitch media\", and \"Physics World\". She is on the Book Review Board of \"Physics Today\" and editor-in-chief of The Offing. The American Physical Society described her as a \"vocal presence on Twitter\". Prescod-Weinstein maintains a \"Decolonising Science Reading List. Prescod-Weinstein has given several interviews and public talks.\n\nIn 2018, Prescod-Weinstein published \"Particles for Justice\", a statement condemning Alessandro Strumia's controversial comments on women in physics at CERN.\n\nPrescod-Weinstein is queer and agender. She is married to a lawyer. \n\n"}
{"id": "47776395", "url": "https://en.wikipedia.org/wiki?curid=47776395", "title": "Child Abuse Review", "text": "Child Abuse Review\n\nChild Abuse Review is a bimonthly peer-reviewed academic journal with a focus on child protection, particularly: research findings, practice developments, training initiatives and policy issues. It is also the official journal of British Association for the Study and Prevention of Child Abuse and Neglect (BASPCAN).\n\nIt is co-edited by Jane V. Appleton (Oxford Brookes University), and Peter Sidebotham (University of Warwick).\n\nAccording to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 1.543, ranking it 9th out of 42 journals in the category \"Social Work\" and 19th out of 43 journals in the category \"Family Studies\".\n"}
{"id": "19276893", "url": "https://en.wikipedia.org/wiki?curid=19276893", "title": "Clearing factor", "text": "Clearing factor\n\nIn centrifugation the clearing factor or k factor represents the relative pelleting efficiency of a given centrifuge rotor at maximum rotation speed. It can be used to estimate the time formula_1 (in hours) required for sedimentation of a fraction with a known sedimentation coefficient formula_2 (in svedbergs):\n\nThe value of the clearing factor depends on the maximum angular velocity formula_4 of a centrifuge (in rad/s) and the minimum and maximum radius formula_5 of the rotor:\n\nAs the rotational speed of a centrifuge is usually specified in RPM, the following formula is often used for convenience:\n\nCentrifuge manufacturers usually specify the minimum, maximum and average radius of a rotor, as well as the formula_8 factor of a centrifuge-rotor combination.\n\nFor runs with a rotational speed lower than the maximum rotor-speed, the formula_8 factor has to be adjusted:\n\nThe K-factor is related to the sedimentation coefficient formula_11 by the formula:\n\nformula_12\n\nWhere formula_13 is the time to pellet a certain particle in hours. Since formula_11 is a constant for a certain particle, this relationship can be used to interconvert between different rotors.\n\nformula_15\n\nWhere formula_16 is the time to pellet in one rotor, and formula_17 is the K-factor of that rotor. formula_18 is the K-factor of the other rotor, and formula_19, the time to pellet in the other rotor, can be calculated. In this manner, one does not need access to the exact rotor cited in a protocol, as long as the K-factor can be calculated. Many online calculators are available to perform the calculations for common rotors.\n\n"}
{"id": "53604540", "url": "https://en.wikipedia.org/wiki?curid=53604540", "title": "Colin Eden", "text": "Colin Eden\n\nColin Eden was a professor in management science and operations research at Strathclyde University.\n\nEden worked as an operational researcher in engineering upon bagging a doctorate. He then proceeded to academia, first working at the University of Bath Management School as a senior lecturer, then, as a reader.\n\nHe then assumed a professorship at Strathclyde University in management science. He subsequently became head of department at the Strathclyde Business School. And then Director of the Graduate School of Business at Strathclyde, and then finally Vice Dean.\n\nHis research interests is in business strategy, project management, and operations research.\n\nHe is widely published in respected management, project management and management science journals such as: Journal of Management Studies; and European Journal of Operational Research. His significant contributions are i) in the development of SODA as a problem structuring method; setting out a clear view of the nature Action Research in the study of management and organisations; iii) the development and theoretical grounding for strategy development as negotiation.\n\nHe has been awarded many accolades including lifetime achievement awards from the OR Society (Beale Medal 2007)and from INFORMS GND Section (2008).\n\n\n"}
{"id": "20524832", "url": "https://en.wikipedia.org/wiki?curid=20524832", "title": "David M. Jones", "text": "David M. Jones\n\nDavid M. Jones (December 18, 1913 – November 25, 2008) was a United States Air Force pilot and general officer who served with distinction during World War II. He was one of the Doolittle Raiders whose exploits in April 1942 were dramatized in the film \"Thirty Seconds Over Tokyo\". He then flew combat missions over North Africa, where he was shot down. He was a German prisoner of war for two and a half years — helping with the April 1944 mass escape at Stalag Luft III . \n\nIn his last assignment with the Air Force, Jones was the commander of the Air Force Eastern Test Range in Cape Kennedy, Florida as well as the Department of Defense Manager for Manned Space Flight Support Operations. He retired from the Air Force in 1973.\n\nDavid Mudgett Jones was born on December 18, 1913 in Marshfield, Oregon, the son of David Arthur (Dade) Jones and Grace Mudgett. His father Dade Jones, the son of Welsh immigrants, homesteaded land in Meadow Lake Township, North Dakota in 1896 before marrying Grace and moving to Oregon.\n\nDavid Jones enlisted in the Arizona National Guard while he was attending the University of Arizona at Tucson, from 1932 to 1936. His later education included three major Armed Forces schools: Command and General Staff School in Fort Leavenworth, Kansas, 1946; Armed Forces Staff College in Norfolk, Virginia, 1948; and the National War College in Washington, D.C., 1956.\n\nJones was commissioned a second lieutenant in the U.S. Cavalry arm of the Arizona Army National Guard, where he served one year of active duty before transferring to the Army Air Corps and entering pilot training in June 1937. After earning his wings in June 1938, he served as a Northrop A-17 pilot with the 95th Attack Squadron of the 17th Attack Group, based at March Field, California. \n\nIn 1939 the 17th was re-designated a medium bombardment group and transferred to McChord Field, Washington. By September 1941 it became the first unit of the Army Air Forces to be fully equipped with the new B-25 Mitchell medium bomber.\n\nIn early 1942, Jones volunteered for the Doolittle Project — a secret bombing raid to be launched on Japan in retaliation for the December 1941 attack on Pearl Harbor. During the training phase of this project, he acted as navigation and intelligence officer for the ad hoc squadron of B-25 bombers. On April 18, 1942, the Doolittle Raid launched from the United States Navy's aircraft carrier , dropping their bombs on Tokyo and four other Japanese cities. This raid was the first good news that the Americans had from the Pacific front.\n\nLacking the fuel to make a safe landing after the raid, Jones bailed out over China, where he was assisted by the Chinese people in evading capture. He received the Distinguished Flying Cross for his participation as a flight commander in the planning, training and completion of the mission. After escaping capture, Jones was flown to India, where he spent three months with the 22nd Bomb Squadron flying further B-25 missions against the Japanese.\n\nIn September 1942, Jones was assigned to the new 319th Bombardment Group, preparing for combat in North Africa. He was assigned to develop low-level bombing tactics and techniques due to his experience with the Doolittle project and his belief in low-level bombing tactics. On December 4, 1942, he was shot down over Bizerte, North Africa, and spent two and a half years as a prisoner of war in Stalag Luft III. As a result of his constant agitation and harassment of the enemy, he was selected for the \"escape committee\" by fellow prisoners. The committee reviewed escape plans and directed escapes. Jones led the digging team on tunnel \"Harry\" used in the \"Great Escape\", chronicled in the Paul Brickhill's autobiographical book, and portrayed in the 1963 film. After his liberation in April 1945, Jones was commended for leadership among his fellow prisoners.\n\nIn July 1946, Jones was assigned as an air inspector at Headquarters Air Training Command. He followed this with attendance at the Armed Forces Staff College, which he completed in 1948. During this time, the U.S. Army Air Forces was disestablished and his commission was transferred to the newly established United States Air Force. He then served as Director of War Plans at Headquarters Tactical Air Command at Langley AFB, Virginia, followed by an assignment as Director of Combat Operations for the Ninth Air Force; then as Commander of the 47th Bombardment Group, until February 1952.\n\nHe was then the commander of the 47th Bombardment Wing (a B-45 Tornado jetbomber wing) at Langley Air Force Base, Virginia and at RAF Sculthorpe, England until July 1955.\n\nJones began working in research and development in 1956 when he was the Deputy Chief of Staff for Operations for the Air Proving Ground Command at Eglin Air Force Base, Florida. His experience in bombardment-type aircraft and previous command staff assignments in research and development resulted in his being selected director of the B-58 Test Force, organized in February 1958 at Carswell Air Force Base, Texas. The B-58 Hustler was the first operational jet bomber capable of Mach 2 supersonic flight. During this time, Jones continued to maintain his flight status in the B-58, TF-102, and T-33 aircraft; participating in design speed dashes, low-level penetrations, night, weather, formation and inflight refueling missions. He had more supersonic time testing the B-58 than any senior U.S. Air Force pilot.\n\nIn September 1960, he became vice commander of the Wright Air Development Division at Wright-Patterson Air Force Base, Ohio. In October 1961, he was named program manager of the GAM-87 \"Skybolt\" at Air Force Systems Command's Aeronautical Systems Division. When that project was cancelled, he became ASD deputy for systems management and later vice commander.\n\nIn August 1964, he became deputy chief of staff for systems at Headquarters Air Force Systems Command at Andrews Air Force Base, Maryland, which had responsibility for all USAF research, development, and weapon system acquisition.\n\nIn December 1964, Jones became Deputy Assistant Administrator for Manned Space Flight with the National Aeronautics and Space Administration (NASA). In July 1965, he was given responsibility for development of the S-IVB Orbital Workshop and spent-stage experiment support module (SSESM) — a concept of \"in-orbit\" conversion of a spent S-IVB stage to a shelter. In August 1965, he took on the additional duties as of the Saturn/Apollo Applications (SAA) Acting Director.\nThen, in May 1967, he assumed duties as commander of the Air Force Eastern Test Range, Patrick AFB and Cape Kennedy AFS, Florida.\n\nJones retired at the rank of Major General on May 31, 1973. He was one of five Doolittle Raiders who later became general officers; the others are James H. Doolittle, John A. Hilger, Everett W. Holstrom, and Richard A. Knobloch.\n\nJones died on November 25, 2008, at age 94, at his home in Tucson, Arizona. He was preceded in death by his first wife Anita Maddox Jones, and survived by his wife Janna-Neen Johnson-Dingell-Cunningham-Jones, daughter (Jere Jones Yeager), two sons (David Jones and Jim Jones) and a stepdaughter (Ann-Eve Grace Dingell-Cunningham) . At the time of his death, out of the eighty men who participated in the Doolittle Raid, there were ten survivors.\n\nMajor General Jones' military decorations and awards include:\n\n\n\n"}
{"id": "3140673", "url": "https://en.wikipedia.org/wiki?curid=3140673", "title": "Dielectric breakdown model", "text": "Dielectric breakdown model\n\nDielectric breakdown model (DBM) is a macroscopic mathematical model combining the diffusion-limited aggregation model with electric field. It was developed by Niemeyer, Pietronero, and Weismann in 1984. It describes the patterns of dielectric breakdown of solids, liquids, and even gases, explaining the formation of the branching, self-similar Lichtenberg figures.\n\n"}
{"id": "2292623", "url": "https://en.wikipedia.org/wiki?curid=2292623", "title": "Docking (molecular)", "text": "Docking (molecular)\n\nIn the field of molecular modeling, docking is a method which predicts the preferred orientation of one molecule to a second when bound to each other to form a stable complex. Knowledge of the preferred orientation in turn may be used to predict the strength of association or binding affinity between two molecules using, for example, scoring functions.\n\nThe associations between biologically relevant molecules such as proteins, peptides, nucleic acids, carbohydrates, and lipids play a central role in signal transduction. Furthermore, the relative orientation of the two interacting partners may affect the type of signal produced (e.g., agonism vs antagonism). Therefore, docking is useful for predicting both the strength and type of signal produced.\n\nMolecular docking is one of the most frequently used methods in structure-based drug design, due to its ability to predict the binding-conformation of small molecule ligands to the appropriate target binding site. Characterisation of the binding behaviour plays an important role in rational design of drugs as well as to elucidate fundamental biochemical processes.\n\nOne can think of molecular docking as a problem of \"“lock-and-key”\", in which one wants to find the correct relative orientation of the \"“key”\" which will open up the \"“lock”\" (where on the surface of the lock is the key hole, which direction to turn the key after it is inserted, etc.). Here, the protein can be thought of as the “lock” and the ligand can be thought of as a “key”. Molecular docking may be defined as an optimization problem, which would describe the “best-fit” orientation of a ligand that binds to a particular protein of interest. However, since both the ligand and the protein are flexible, a \"“hand-in-glove”\" analogy is more appropriate than \"“lock-and-key”\". During the course of the docking process, the ligand and the protein adjust their conformation to achieve an overall \"best-fit\" and this kind of conformational adjustment resulting in the overall binding is referred to as \"induced-fit\".\n\nMolecular docking research focuses on computationally simulating the molecular recognition process. It aims to achieve an optimized conformation for both the protein and ligand and relative orientation between protein and ligand such that the free energy of the overall system is minimized.\n\nTwo approaches are particularly popular within the molecular docking community. One approach uses a matching technique that describes the protein and the ligand as complementary surfaces. The second approach simulates the actual docking process in which the ligand-protein pairwise interaction energies are calculated. Both approaches have significant advantages as well as some limitations. These are outlined below.\n\nGeometric matching/ shape complementarity methods describe the protein and ligand as a set of features that make them dockable. These features may include molecular surface / complementary surface descriptors. In this case, the receptor’s molecular surface is described in terms of its solvent-accessible surface area and the ligand’s molecular surface is described in terms of its matching surface description. The complementarity between the two surfaces amounts to the shape matching description that may help finding the complementary pose of docking the target and the ligand molecules. Another approach is to describe the hydrophobic features of the protein using turns in the main-chain atoms. Yet another approach is to use a Fourier shape descriptor technique. Whereas the shape complementarity based approaches are typically fast and robust, they cannot usually model the movements or dynamic changes in the ligand/ protein conformations accurately, although recent developments allow these methods to investigate ligand flexibility. Shape complementarity methods can quickly scan through several thousand ligands in a matter of seconds and actually figure out whether they can bind at the protein’s active site, and are usually scalable to even protein-protein interactions. They are also much more amenable to pharmacophore based approaches, since they use geometric descriptions of the ligands to find optimal binding.\n\nSimulating the docking process is much more complicated. In this approach, the protein and the ligand are separated by some physical distance, and the ligand finds its position into the protein’s active site after a certain number of “moves” in its conformational space. The moves incorporate rigid body transformations such as translations and rotations, as well as internal changes to the ligand’s structure including torsion angle rotations. Each of these moves in the conformation space of the ligand induces a total energetic cost of the system. Hence, the system's total energy is calculated after every move.\n\nThe obvious advantage of docking simulation is that ligand flexibility is easily incorporated, whereas shape complementarity techniques must use ingenious methods to incorporate flexibility in ligands. Also, it more accurately models reality, whereas shape complimentary techniques are more of an abstraction.\n\nClearly, simulation is computationally expensive, having to explore a large energy landscape. Grid-based techniques, optimization methods, and increased computer speed have made docking simulation more realistic.\n\nTo perform a docking screen, the first requirement is a structure of the protein of interest. Usually the structure has been determined using a biophysical technique such as x-ray crystallography or NMR spectroscopy, but can also derive from homology modeling construction. This protein structure and a database of potential ligands serve as inputs to a docking program. The success of a docking program depends on two components: the search algorithm and the scoring function.\n\nThe search space in theory consists of all possible orientations and conformations of the protein paired with the ligand. However, in practice with current computational resources, it is impossible to exhaustively explore the search space—this would involve enumerating all possible distortions of each molecule (molecules are dynamic and exist in an ensemble of conformational states) and all possible rotational and translational orientations of the ligand relative to the protein at a given level of granularity. Most docking programs in use account for the whole conformational space of the ligand (flexible ligand), and several attempt to model a flexible protein receptor. Each \"snapshot\" of the pair is referred to as a pose.\n\nA variety of conformational search strategies have been applied to the ligand and to the receptor. These include:\n\n\nConformations of the ligand may be generated in the absence of the receptor and subsequently docked or conformations may be generated on-the-fly in the presence of the receptor binding cavity, or with full rotational flexibility of every dihedral angle using fragment based docking. Force field energy evaluation are most often used to select energetically reasonable conformations, but knowledge-based methods have also been used.\n\nPeptides are both highly flexible and relatively large-sized molecules, which makes modeling their flexibility a challenging task. A number of methods were developed to allow for efficient modeling of flexibility of peptides during protein-peptide docking.\n\nComputational capacity has increased dramatically over the last decade making possible the use of more sophisticated and computationally intensive methods in computer-assisted drug design. However, dealing with receptor flexibility in docking methodologies is still a thorny issue. The main reason behind this difficulty is the large number of degrees of freedom that have to be considered in this kind of calculations. Neglecting it, however, in some of the cases may lead to poor docking results in terms of binding pose prediction.\n\nMultiple static structures experimentally determined for the same protein in different conformations are often used to emulate receptor flexibility. Alternatively rotamer libraries of amino acid side chains that surround the binding cavity may be searched to generate alternate but energetically reasonable protein conformations.\n\nDocking programs generate a large number of potential ligand poses, of which some can be immediately rejected due to clashes with the protein. The remainder are evaluated using some scoring function, which takes a pose as input and returns a number \nindicating the likelihood that the pose represents a favorable binding interaction and ranks one ligand relative to another.\n\nMost scoring functions are physics-based molecular mechanics force fields that estimate the energy of the pose within the binding site. The various contributions to binding can be written as an additive equation:\n\nformula_1\n\nThe components consist of solvent effects, conformational changes in the protein and ligand, free energy due to protein-ligand interactions, internal rotations, association energy of ligand and receptor to form a single complex and free energy due to changes in vibrational modes. A low (negative) energy indicates a stable system and thus a likely binding interaction.\n\nAn alternative approach is to derive a knowledge-based statistical potential for interactions from a large database of protein-ligand complexes, such as the Protein Data Bank, and evaluate the fit of the pose according to this inferred potential.\n\nThere are a large number of structures from X-ray crystallography for complexes between proteins and high affinity ligands, but comparatively fewer for low affinity ligands as the later complexes tend to be less stable and therefore more difficult to crystallize. Scoring functions trained with this data can dock high affinity ligands correctly, but they will also give plausible docked conformations for ligands that do not bind. This gives a large number of false positive hits, i.e., ligands predicted to bind to the protein that actually don't when placed together in a test tube.\n\nOne way to reduce the number of false positives is to recalculate the energy of the top scoring poses using (potentially) more accurate but computationally more intensive techniques such as Generalized Born or Poisson-Boltzmann methods.\n\nThe interdependence between sampling and scoring function affects the docking capability in predicting plausible poses or binding affinities for novel compounds. Thus, an assessment of a docking protocol is generally required (when experimental data is available) to determine its predictive capability. Docking assessment can be performed using different strategies, such as:\n\nDocking accuracy represents one measure to quantify the fitness of a docking program by rationalizing the ability to predict the right pose of a ligand with respect to that experimentally observed.\n\nDocking screens can be also evaluated by the enrichment of annotated ligands of known binders from among a large database of presumed non-binding, “decoy” molecules. In this way, the success of a docking screen is evaluated by its capacity to enrich the small number of known active compounds in the top ranks of a screen from among a much greater number of decoy molecules in the database. The area under the receiver operating characteristic (ROC) curve is widely used to evaluate its performance.\n\nResulting hits from docking screens are subjected to pharmacological validation (e.g. IC affinity or potency measurements). Only prospective studies constitute conclusive proof of the suitability of a technique for a particular target.\n\nThe potential of docking programs to reproduce binding modes as determined by X-ray crystallography can be assed by a range of docking benchmark sets.\n\nFor small molecules, several benchmark data sets for docking and virtual screening exist e.g. \"Astex Diverse Set\" consisting of high quality protein−ligand X-ray crystal structures or the \"Directory of Useful Decoys\" (DUD) for evaluation of virtual screening performance.\n\nAn evaluation of docking programs for their potential to reproduce peptide binding modes can be assessed by \"Lessons for Efficiency Assessment of Docking and Scoring\" (LEADS-PEP).\n\nA binding interaction between a small molecule ligand and an enzyme protein may result in activation or inhibition of the enzyme. If the protein is a receptor, ligand binding may result in agonism or antagonism. Docking is most commonly used in the field of drug design — most drugs are small organic molecules, and docking may be applied to:\n\n"}
{"id": "2703671", "url": "https://en.wikipedia.org/wiki?curid=2703671", "title": "Eberhard Köllner", "text": "Eberhard Köllner\n\nEberhard Köllner (born 29 September 1939 in Stassfurt, Germany) was selected for Soyuz 31 as the backup for Sigmund Jähn.\n\nHe later became the Director of the Airforce Academy of the German Democratic Republic in the rank of an \"Oberst\" (\"Colonel\"), following the reunion of Germany he refused to be transferred to the (West) German \"Bundeswehr\". He is currently working in private industries.\n\n"}
{"id": "39297814", "url": "https://en.wikipedia.org/wiki?curid=39297814", "title": "Equatorial Vortex Experiment", "text": "Equatorial Vortex Experiment\n\nThe Equatorial Vortex Experiment (EVEX) is a NASA-funded sounding rocket mission to better understand and predict the electrical storms in Earth's upper atmosphere. As part of this experiment, two rockets were launched for a twelve-minute journey through the equatorial ionosphere above the South Pacific. These rockets were launched from Kwajalein Atoll in the Marshall Islands during a period of April 27 to May 10, 2013.\n\nThe principal investigator for this mission is Erhan Kudeki of the University of Illinois. The purpose of this experiment is to study what disrupts Radio waves.\n\nA NASA Terrier Oriole sounding rocket was launched at 3:39 a.m. EDT on 7 May 2013 from Roi Namur, Republic of the Marshall Islands. Ninety seconds later a Terrier-Improved Malemute sounding rocket also was launches successfully.\n\nThis experiment will help scientists better understand and predict the electrical storms in Earth's upper atmosphere. The electrical storms can interfere with satellite communication and global positioning signals. Payload for each rocket included two canisters of samarium and a dual frequency RF Beacon (NRL CERTO).\n\nThese two rockets released vapor clouds of lithium (trimethyl aluminum),and were observed from various locations in the area. All scientific instruments on the rockets worked as planned. These two rockets were the second and third rockets of four planned for launch during 2013's campaign in the Marshall Islands.\n\nIn June 2017, a Terrier-Improved Malemute two-stage sounding rocket was launched from NASA's Wallops Flight Facility in Virginia. The artificial clouds released by this rocket helped scientists track the movement of Earth's Ionosphere. \n"}
{"id": "53577368", "url": "https://en.wikipedia.org/wiki?curid=53577368", "title": "Eyach virus", "text": "Eyach virus\n\nEyach virus (EYAV) is a viral infection (genus Coltivirus) in the Reoviridae family transmitted by a tick vector. It has been isolated from \"Ixodes ricinus\" and \"I. ventalloi\" ticks in Europe. It is closely related to Colorado tick fever virus, the type virus of Coltivirus.\n\nEyach virus is acquired by tick bite. The tick gets infected after a blood meal from a vertebrate host, which is suspected to be the European rabbit \"O. cunniculus.\" Eyach virus has been linked to tick-borne encephalitis, as well as polyradiculoneuritis and meningopolyneuritis, based on serological samples of patients with these neurological disorders.\n"}
{"id": "12020536", "url": "https://en.wikipedia.org/wiki?curid=12020536", "title": "Farm Animal Welfare Committee", "text": "Farm Animal Welfare Committee\n\nThe Farm Animal Welfare Committee (FAWC) is an independent advisory body established by the Government of Great Britain in 2011. It replaced the Farm Animal Welfare Council which was an independent advisory body established in 1979. The Council published its Final Report before its closure and replacement on 31 March 2011.\n\nThe Farm Animal Welfare Council terms of reference were to keep under review the welfare of farm animals on agricultural land, at market, in transit and at the place of slaughter and advise Government of any changes that may be necessary. The council comprised various Standing Committees and Working Groups that consulted widely and openly about the issues FAWC considered relevant to the welfare of farmed animals and to prepare recommendations for the Council's consideration. Once agreed, the recommendations formed the basis for advice given to Government. Copies of FAWC's Reports and other advice are available on the FAWC website.\n\nThe Council's major strength lay in its independence to investigate any topic falling within its remit and to communicate freely with outside bodies, including the European Commission and the public, while maintaining the independence to publish its advice.\n\nThe council also established the Five Freedoms for farm animals. These freedoms serve as a basic outline for regulations concerning livestock, poultry, etc.\n\n\n"}
{"id": "15753039", "url": "https://en.wikipedia.org/wiki?curid=15753039", "title": "Flaminio Baudi di Selve", "text": "Flaminio Baudi di Selve\n\nFlaminio Baudi di Selve (7 July 1821, Savigliano – 26 June 1901, Genoa) was an Italian entomologist who specialised in Coleoptera but also Heteroptera.\n\nHe wrote Europae et circummediterraneae Faunae Tenebrionidum specierum, quae Comes Dejean in suo Catalogo, editio 3, consignavit, ejusdem collectione in R. Taurinensi Musaeo asservata, cum auctorum hodierne recepta denominatione collatio. Pars tertia. \"Dtsch. Entomol. Z\". 20: 225-267 (1876), \"Catalogo dei coleotteri del Piemonte\" Torino, Tip. e. Lit. Camilla E. Bertolero (1889) and very many shorter works on beetles.He described many new species.\n\nHis insect collection, mainly Palearctic is shared between Museo Civico di Storia Naturale di Genova, Turin Museum of Natural History and Museo Regionale di Scienze Naturali Regione Piemonte\n\n\n"}
{"id": "49335273", "url": "https://en.wikipedia.org/wiki?curid=49335273", "title": "Generations and Gender Survey", "text": "Generations and Gender Survey\n\nThe Generations and Gender Survey (GGS) is a series of surveys administered by the Generations and Gender Programme to improve demographic and social developments among several countries in Europe as well as Australia and Japan. The programme has collected least one wave of surveys in more than 19 countries, with an average of 9,000 respondents per country. The resultant data has generated over 1,200 peer-reviewed publications. It was launched by the United Nations Economic Commission for Europe, as a successor to its previous Fertility and Family Survey in the 1990s.\n\nThe participating countries are Australia, Austria, Belarus, Belgium, Bulgaria, Czech Republic, Estonia, France, Georgia, Germany, Hungary, Italy, Japan, Lithuania, Netherlands, Norway, Poland, Romania, Russian Federation and Sweden. It does not include the United Kingdom, where, on the other hand, the study has a similar scope.\n\nThe core questionnaire contains over 1,000 questions or items, broadly classified as follows:\n\n\n"}
{"id": "20769891", "url": "https://en.wikipedia.org/wiki?curid=20769891", "title": "Halvor Heyerdahl Rasch", "text": "Halvor Heyerdahl Rasch\n\nHalvor Heyerdahl Rasch (8 January 1805 – 26 August 1883) was a Norwegian zoologist and educator.\n\nHe was born at Eidsberg in Østfold, Norway. Rasch studied botany and zoology at the University of Christiania (now University of Oslo).\n\nAn avid hunter and sportsman, he published the book \"Jagten i Norge\" (1845) as well as works about livestock, oyster cultivation and beekeeping. He was among the founders of the \"Centralforeningen for Udbredelse af Legemsøvelser og Vaabenbrug\", a precursor to the Norwegian Confederation of Sports, in 1861.\n\nRasch was decorated as a Knight of the Royal Norwegian Order of St. Olav in 1863 and received the Silver Medal of the Société Impériale Zoologique d'Acclimatation \nat Paris in 1866.\n"}
{"id": "6710209", "url": "https://en.wikipedia.org/wiki?curid=6710209", "title": "Herbert E. Walter", "text": "Herbert E. Walter\n\nHerbert Eugene Walter, (1867 – 1945), was a prominent biologist, author, Professor at Brown University and researcher.\n\nHerbert Walter was born in Burke, Vermont in 1867. He attended the Lyndon Institute, and then graduated from Bates College in Maine in 1892. He next received a M.A. from Brown University in 1893 and then studied at the University of Freiburg. From 1894 to 1904 he taught biology. In 1906 He received a Ph.D. from Harvard University. Walter came to Brown as assistant professor of biology. He was promoted to associate professor in 1913 and professor in 1923. Walter published many books, including, \"Studies in Animal Life\", (1901), \"Genetics: An Introduction To the Study of Heredity\" (1913), \"The Human Skeleton\" (1913), and \"Biology of the Vertebrates\". With his wife, Alice Hall Lyndon, he wrote \"Wild Birds in City Parks\". Walter was director of research for the Federal Bureau of Fisheries at Woods Hole, and conducted a course in field zoology for teachers of biology at the Marine Biological Institute of the Brooklyn Institute of Arts and Sciences at Cold Spring Harbor, Long Island. He was assistant director of the Institute from 1917 to 1926. Walter Hall was named after the Professor in 1959 it is now home to the department of Ecology and Evolutionary Biology on the Brown campus at 80 Waterman Street.\n\n\n"}
{"id": "47845518", "url": "https://en.wikipedia.org/wiki?curid=47845518", "title": "ISRA International Journal of Islamic Finance", "text": "ISRA International Journal of Islamic Finance\n\nISRA International Journal of Islamic Finance\n\nISRA International Journal of Islamic Finance (IIJIF) is a reputable academic journal dedicated to publish high quality research in all the relevant fields of Islamic economics and finance. It is published by the International Shari’ah Research Academy for Islamic Finance (ISRA), which has been vested the task to promote applied Shari’ah research in the niche area of Islamic finance. As part of its fervent devotion to Islamic finance, the publication of IIJIF is yet another endeavor of ISRA to further the excellence in leading scholastic as well as practical research.\n\nIIJIF has been published since December 2009. It is a fully peer-reviewed and refereed journal, aiming at a wider readership interested to learn more about Islamic economics, finance and other relevant disciplines. It aspires to publish original and unpublished work within the areas of, but not limited to, Sharia (Islamic law), Islamic economics, banking, capital markets, takaful (Islamic Insurance), etc.\n\nIIJIF considers the research with academic rigour as a fuelling-factor for the development and growth of the Islamic finance industry. This consideration is reflected in the IIJIF’s section of ‘Academic Articles’. However, beside academic research, IIJIF also appreciates the significance of hands-on experience of the Islamic finance industry professionals and is keen to offer them an opportunity to share their ideas and views through its dedicated section of ‘Practitioners’ Article’. This section is devoted to publish the articles that are written by various industry players. In this way, each issue of IIJIF represents a fine blend of research from both academic and practical aspects. Moreover, it also introduces research work which is currently carried on by Islamic finance researchers in the section titled ‘Research Notes’.\n\nIIJIF is published semi-annually (in June and December) in both printed and electronic forms. Each issue of IIJIF consists of 4 academic articles, 1 practitioners’ article and 5 research notes.\n\nIIJIF has set high standards of publication ethics since its inception and has been observing the policy of ‘no compromise’ on ethics in publication. It conscientiously encourages integrity in research publication.\n\nIIJIF enjoys the privilege of having a highly reputable International Editorial Advisory Board comprising renowned Shari’ah scholars and distinguished academics. It greatly benefits from those esteemed members in terms of their vast knowledge and experience. Those members contribute in IIJIF by refereeing the submitted articles; offering advice to the Editorial Committee; and contributing articles to IIJIF.\n\nJournal Name: ISRA International Journal of Islamic Finance (IIJIF)\n\nISRA International Journal of Islamic Finance is indexed in:\n"}
{"id": "38438395", "url": "https://en.wikipedia.org/wiki?curid=38438395", "title": "Index of physics articles", "text": "Index of physics articles\n\nPhysics (Greek: physis–φύσις meaning \"nature\") is the natural science which examines basic concepts such as mass, charge, matter and its motion and all that derives from these, such as energy, force and spacetime. More broadly, it is the general analysis of nature, conducted in order to understand how the world and universe behave.\n\nThe index of physics articles is split into multiple pages due to its size.\n\nTo navigate by individual letter use the table of contents below.\n\n"}
{"id": "38448168", "url": "https://en.wikipedia.org/wiki?curid=38448168", "title": "Index of physics articles (A)", "text": "Index of physics articles (A)\n\nThe index of physics articles is split into multiple pages due to its size.\n\nTo navigate by individual letter use the table of contents below.\n\n"}
{"id": "7895070", "url": "https://en.wikipedia.org/wiki?curid=7895070", "title": "K correction", "text": "K correction\n\nK correction is a correction to an astronomical object's magnitude (or equivalently, its flux) that allows a measurement of a quantity of light from an object at a redshift \"z\" to be converted to an equivalent measurement in the rest frame of the object. If one could measure all the light from an object at all wavelengths (a bolometric flux), a K correction would not be required. If one measures the light emitted in an emission line, a K-correction is not required. The need for a K-correction arises because an astronomical measurement through a single filter or a single bandpass only sees a fraction of the total spectrum, redshifted into the frame of the observer. So if the observer wants to compare the measurements through a red filter of objects at different redshifts, the observer will have to apply estimates of the K corrections to these measurements to make a comparison.\n\nOne claim for the origin of the term \"K correction\" is Edwin Hubble, who supposedly arbitrarily chose formula_1 to represent the reduction factor in magnitude due to this effect. Yet Kinney et al., in footnote 7 on page 48 of their article, note an earlier origin from Carl Wilhelm Wirtz (1918), who referred to the correction as a \"Konstante\" (German for \"constant\"), hence K-correction.\n\nThe K-correction can be defined as follows\n\nI.E. the adjustment to the standard relationship between absolute and apparent magnitude required to correct for the redshift effect. Here, D is the luminosity distance measured in parsecs.\n\nThe exact nature of the calculation that needs to be applied in order to perform a K correction depends upon the type of filter used to make the observation and the shape of the object's spectrum. If multi-color photometric measurements are available for a given object thus defining its spectral energy distribution (SED), K corrections then can be computed by fitting it against a theoretical or empirical SED template. It has been shown that K corrections in many frequently used broad-band filters for low-redshift galaxies can be precisely approximated using two-dimensional polynomials as functions of a redshift and one observed color. This approach is implemented in the K corrections calculator web-service.\n\n"}
{"id": "51899244", "url": "https://en.wikipedia.org/wiki?curid=51899244", "title": "Kumeyaay astronomy", "text": "Kumeyaay astronomy\n\nKumeyaay astronomy or cosmology (Kumeyaay: \"My Uuyow\", \"sky knowledge\") comprises the astronomical knowledge of the Kumeyaay people. A deeply rooted cosmological belief system was developed and followed by the Kumeyaay civilization based on this knowledge including the computing of time (Kumeyaay Mat’taam).\n\nThe first evidence of astronomical observations and visual registration was discovered in the El Vallecito archeological zone. The \"Men in a square\" rupestric painting located at El Diablito area of El Vallecito depicted a square that aligns with sunlight on the Fall equinox. These paintings were made by the nomadic(?) Kumeyaay people. Kumeyaay sand paintings and rock art modeled the passage of the sun, moon, and constellations.\n\nObservation areas were made by the Kumeyaay to watch and register astronomical events. However many were destroyed by vandals before protection measures were instituted.\n\n\nConstellations:\n\n\n"}
{"id": "14589710", "url": "https://en.wikipedia.org/wiki?curid=14589710", "title": "List of Atlantic–Pacific crossover hurricanes", "text": "List of Atlantic–Pacific crossover hurricanes\n\nAn Atlantic–Pacific crossover hurricane is a tropical cyclone that develops in the Atlantic Ocean and moves into the Pacific Ocean, or vice versa. Since reliable records began in 1851, a total of eighteen tropical cyclones have done this. It is more common for the remnants of a North Atlantic hurricane to redevelop into a different storm in the Pacific; in such a scenario, they are not considered the same system.\n\nThere were several other tropical cyclones that formed in one basin, dissipated, and re-developed. In addition, there were tropical cyclones that developed and entered another basin briefly or at a weak intensity, however, they were not recognized as an Atlantic-Pacific crossover hurricane. In chronological order from most recent to earliest, they are:\n\nPrior to 2000, storms were renamed after crossing from the Gulf of Mexico into the Eastern Pacific. At the 22nd hurricane committee in 2000 it was decided that tropical cyclones that moved from the Atlantic to the Eastern Pacific basin and vice versa would no longer be renamed. Hurricane Otto in 2016 was the first storm to cross from one basin to another to apply under this rule.\n\n"}
{"id": "56134", "url": "https://en.wikipedia.org/wiki?curid=56134", "title": "List of U.S. state songs", "text": "List of U.S. state songs\n\nForty-nine of the fifty U.S. states that make up the United States of America have one or more state songs, which are selected by each state legislature, and/or state governor, as a symbol (or emblem) of that particular U.S. state. New Jersey has no official state song, while Virginia's previous state song, \"Carry Me Back to Old Virginny\", adopted in 1940, was rescinded due to its racist language by the Virginia General Assembly. In 2015, \"Our Great Virginia\" was made the new state song of Virginia.\n\nSome U.S. states have more than one official state song, and may refer to some of their official songs by other names; for example, Arkansas officially has two state songs, plus a state anthem, and a state historical song. Tennessee has the most state songs, with 9 official state songs and an official bicentennial rap.\n\nArizona has a song that was written specifically as a state anthem in 1915, as well as the 1981 country hit \"Arizona\", which it adopted as the alternate state anthem in 1982.\n\nTwo individuals, Stephen Foster, and John Denver, have written or co-written two state songs. Foster's two state songs, \"Old Folks at Home\" (better known as \"Swanee Ribber\" or \"Suwannee River\") (for adopted by Florida), and \"My Old Kentucky Home\" are among the best-known songs in the U.S. On March 12, 2007, the Colorado Senate passed a resolution to make Denver's trademark 1972 hit \"Rocky Mountain High\" one of the state's two official state songs, sharing duties with its predecessor, \"Where the Columbines Grow\". On March 7, 2014, the West Virginia Legislature approved a resolution to make Denver's \"Take Me Home, Country Roads\" one of four official state songs of West Virginia. Governor Earl Ray Tomblin signed the resolution into law on March 8, 2014.\nOther well-known state songs include \"Yankee Doodle\", \"You Are My Sunshine\", \"Rocky Top\", and \"Home on the Range\"; a number of others are popular standards, including \"Oklahoma!\" (from the Rodgers and Hammerstein musical), Hoagy Carmichael's \"Georgia on My Mind\", \"Tennessee Waltz\", \"Missouri Waltz\", and \"On the Banks of the Wabash, Far Away\". Many of the others are much less well-known, especially outside the state.\n\nMaryland (\"Maryland, My Maryland\") and Iowa (\"The Song of Iowa\") use the tune from the song \"O Tannenbaum\" as the melody to their official state songs.\n\nSome American overseas territories, although not U.S. states, have songs and marches of their own.\n"}
{"id": "554991", "url": "https://en.wikipedia.org/wiki?curid=554991", "title": "List of effects", "text": "List of effects\n\nThis is a list of names for observable phenomena that contain the word effect, amplified by reference(s) to their respective fields of study.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "49538727", "url": "https://en.wikipedia.org/wiki?curid=49538727", "title": "List of inventions and discoveries by women", "text": "List of inventions and discoveries by women\n\nThis page aims to list inventions and discoveries in which women played a major role. \n\n\n\n\n\n\n\n\n\nThe first disposable diaper was invented in 1946 by Marion Donovan, a professional-turned-housewife who wanted to ensure her children's cloth diapers remained dry while they slept. Donovan patented her design (called \n'Boaters') in 1951. She also invented the first paper diapers, but executives did not invest in this idea and it was consequently scrapped for over ten years, until Procter & Gamble used Donovan's design ideas to create Pampers.\n\nAnother diaper design was created by Valerie Hunter Gordon (née de Ferranti), who patented it in 1948 . \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIngrid Daubechies introduced the Daubechies wavelet and contributed to the development of the CDF wavelet, important tools in image compression. \n\n\nIn 1966 Mark Kac asked whether the shape of a drum could be determined by the sound it makes (whether a Riemannian manifold\nis determined by the spectrum of its Laplace-Beltrami operator). John Milnor observed that a theorem due to Witt implied the existence of a pair of 16-dimensional tori that have the same spectrum but different shapes. However, the problem in two dimensions remained open until 1992, when Carolyn S. Gordon with coauthors Webb and Wolpert, constructed a pair of regions in the Euclidean plane that have different shapes but identical eigenvalues (see figure on right). \n\n\nIn mathematics, the Cauchy–Kowalevski theorem (also written as the Cauchy–Kovalevskaya theorem) is the main local existence and uniqueness theorem for analytic partial differential equations associated with Cauchy initial value problems. A special case was proven by Augustin Cauchy (1842), and the full result by Sophia Kovalevskaya (1875). \n\n\nIn classical mechanics, the precession of a rigid body such as a top under the influence of gravity is not, in general, an integrable problem. There are however three (or four) famous cases that are integrable, the Euler, the Lagrange, and the Kovalevskaya top. The Kovalevskaya top is a special symmetric top with a unique ratio of the moments of inertia which satisfy the relation\n\nThat is, two moments of inertia are equal, the third is half as large, and the center of gravity is located in the plane perpendicular to the symmetry axis (parallel to the plane of the two equal points).\n\n\nIn numerical linear algebra, the QR algorithm is an eigenvalue algorithm: that is, a procedure to calculate the eigenvalues and eigenvectors of a matrix. The QR algorithm was developed in the late 1950s by John G. F. Francis and by Vera N. Kublanovskaya, working independently. The basic idea is to perform a QR decomposition, writing the matrix as a product of an orthogonal matrix and an upper triangular matrix, multiply the factors in the reverse order, and iterate.\n\n\nOlga Ladyzhenskaya provided the first rigorous proofs of the convergence of a finite difference method for the Navier–Stokes equations. Ladyzhenskaya was on the shortlist for potential recipients for the 1958 Fields Medal, ultimately awarded to Klaus Roth and René Thom. \n\n\nRuth Lawrence's 1990 paper, \"Homological representations of the Hecke algebra\", in \"Communications in Mathematical Physics\", introduced, among other things, certain novel linear representations of the braid group — known as Lawrence–Krammer representation. In papers published in 2000 and 2001, Daan Krammer and Stephen Bigelow established the faithfulness of Lawrence's representation. This result goes by the phrase \"braid groups are linear.\"\n\n\nRózsa Péter was one of the founders of recursion theory, a branch of mathematical logic, of computer science, and of the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability. In these areas, recursion theory overlaps with proof theory and effective descriptive set theory. \n\n\nHilbert's tenth problem is the tenth on the list of mathematical problems that the German mathematician David Hilbert posed in 1900. It is the challenge to provide a general algorithm which, for any given Diophantine equation (a polynomial equation with integer coefficients and a finite number of unknowns) can decide whether the equation has a solution with all unknowns taking integer values. \n\nFor example, the Diophantine equation formula_2 has an integer solution: formula_3. By contrast, the Diophantine equation formula_4 has no such solution.\n\nHilbert's tenth problem has been solved, and it has a negative answer: such a general algorithm does not exist. This is the result of combined work of Martin Davis, Yuri Matiyasevich, Hilary Putnam and Julia Robinson which spans 21 years, with Yuri Matiyasevich completing the theorem in 1970. The theorem is now known as Matiyasevich's theorem or the MRDP theorem.\n\nIn the design of experiments, optimal designs (or optimum designs) are a class of experimental designs that are optimal with respect to some statistical criterion. The creation of this field of statistics has been credited to Danish statistician Kirstine Smith.\n\nThe three-gap theorem states that if one places \"n\" points on a circle, at angles of \"θ\", 2\"θ\", 3\"θ\" ... from the starting point, then there will be at most three distinct distances between pairs of points in adjacent positions around the circle. When there are three distances, the larger of the three always equals the sum of the other two. Unless \"θ\" is a rational multiple of , there will also be at least two distinct distances.\n\nThis result was conjectured by Hugo Steinhaus, and proved in the 1950s by Vera T. Sós, , and Stanisław Świerczkowski. Its applications include the study of plant growth and musical tuning systems, and the theory of Sturmian words. \n\n\nThe Noether normalization lemma is a result of commutative algebra, introduced by Emmy Noether in 1926. It states that for any field \"k\", and any finitely generated commutative \"k\"-algebra \"A\", there exists a nonnegative integer \"d\" and algebraically independent elements \"y\", \"y\", ..., \"y\" in \"A\" \nsuch that \"A\" is a finitely generated module over the polynomial ring \"S\":=\"k\"[\"y\", \"y\", ..., \"y\"].\n\nThe theorem has a geometric interpretation. Suppose \"A\" is integral. Let \"S\" be the coordinate ring of the \"d\"-dimensional affine space formula_5, and \"A\" as the coordinate ring of some other \"d\"-dimensional affine variety \"X\". Then the inclusion map \"S\" → \"A\" induces a surjective finite morphism of affine varieties formula_6. The conclusion is that any affine variety is a branched covering of affine space.\n\nThe Noether normalization lemma is an important step to proving Hilbert's Nullstellensatz.\n\n\nNoether's (first) theorem states that every differentiable symmetry of the action of a physical system has a corresponding conservation law. The theorem was proven by mathematician Emmy Noether in 1915 and published in 1918, although a special case was proven by E. Cosserat & F. Cosserat in 1909. The action of a physical system is the integral over time of a Lagrangian function (which may or may not be an integral over space of a Lagrangian density function), from which the system's behavior can be determined by the principle of least action.\n\nNoether's theorem is used in theoretical physics and the calculus of variations. A generalization of the formulations on constants of motion in Lagrangian and Hamiltonian mechanics (developed in 1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian alone (e.g. systems with a Rayleigh dissipation function). In particular, dissipative systems with continuous symmetries need not have a corresponding conservation law.\n\nNoether's theorem can be stated informally\n\n\nIn mathematics and theoretical physics, Noether's second theorem relates symmetries of an action functional with a system of differential equations. The action \"S\" of a physical system is an integral of a so-called Lagrangian function \"L\", from which the system's behavior can be determined by the principle of least action.\n\n\nIn mathematics, specifically abstract algebra, the isomorphism theorems are three theorems that describe the relationship between quotients, homomorphisms, and subobjects. Versions of the theorems exist for groups, rings, vector spaces, modules, Lie algebras, and various other algebraic structures. In universal algebra, the isomorphism theorems can be generalized to the context of algebras and congruences.\n\nThe isomorphism theorems were formulated in some generality for homomorphisms of modules by Emmy Noether in her paper \"Abstrakter Aufbau der Idealtheorie in algebraischen Zahl- und Funktionenkörpern\" which was published in 1927 in Mathematische Annalen. Less general versions of these theorems can be found in work of Richard Dedekind and previous papers by Noether.\n\nThree years later, B.L. van der Waerden published his influential \"Algebra,\" the first abstract algebra textbook that took the groups-rings-fields approach to the subject. Van der Waerden credited lectures by Noether on group theory and Emil Artin on algebra, as well as a seminar conducted by Artin, Wilhelm Blaschke, Otto Schreier, and van der Waerden himself on ideals as the main references. The three isomorphism theorems, called \"homomorphism theorem\", and \"two laws of isomorphism\" when applied to groups, appear explicitly.\n\n\nIn mathematics, the Lasker–Noether theorem states that every Noetherian ring is a Lasker ring, which means that every ideal can be decomposed as an intersection, called primary decomposition, of finitely many \"primary ideals\" (which are related to, but not quite the same as, powers of prime ideals). The theorem was first proven by for the special case of polynomial rings and convergent power series rings, and was proven in its full generality by .\n\nThe Lasker–Noether theorem is an extension of the fundamental theorem of arithmetic, and more generally the fundamental theorem of finitely generated abelian groups to all Noetherian rings. The Lasker–Noether theorem plays an important role in algebraic geometry, by asserting that every algebraic set may be uniquely decomposed into a finite union of irreducible components.\n\n\nIn algebraic number theory, the Albert–Brauer–Hasse–Noether theorem states that a central simple algebra over an algebraic number field \"K\" which splits over every completion \"K\" is a matrix algebra over \"K\". The theorem is an example of a local-global principle in algebraic number theory and \nleads to a complete description of finite-dimensional division algebras over algebraic number fields in terms of their local invariants. It was proved independently by Richard Brauer, Helmut Hasse, and Emmy Noether and by Abraham Adrian Albert.\n\n\nFields medalist Maryam Mirzakhani proved the long-standing conjecture that William Thurston's earthquake flow on Teichmüller space is ergodic.\n\n\n\n\n\nGerty Cori, together with Carl Ferdinand Cori, discovered the Cori cycle, the metabolic pathway in which lactate produced by anaerobic glycolysis in the muscles moves to the liver and is converted to glucose, which then returns to the muscles and is metabolized back to lactate.\n\nRosalyn Sussman Yalow developed the radioimmunoassay, an immunoassay that uses radiolabeled molecules in a stepwise formation of [immune complexes at the Veterans Administration Hospital in the Bronx, New York. This technique is used to accurately measure levels of substances such as hormones which are found in small concentrations in the body. \n\nBarbara McClintock discovered transposable elements (also known as transposons and jumping genes), DNA sequences which change their position within the genome. Transposons make up a large fraction of the DNA in eukaryotic cells (44% if the human genome and 90% of the maize genome ) and play an important role in genome function and evolution. . In \"Oxytricha\", which has a unique genetic system, these elements play a critical role in development.\n\nRita Levi-Montalcini and colleague Stanley Cohen discovered nerve growth factor, a neurotrophic factor and neuropeptide primarily involved in the regulation of growth, maintenance, proliferation, and survival of certain target neurons. This discovery was recognized with the Nobel Prize in Physiology or Medicine in 1986. \n\nChristiane Nüsslein-Volhard and colleague Eric Wieschaus were the first to describe gap genes, genes involved in the development of segmentation in Drosophila embryogenesis. This work was foundational to our understanding of the genetic control of embryonic development.\n\n\nElizabeth Blackburn, Carol W. Greider, and Jack W. Szostak co-discovered the enzyme telomerase, which replenishes the telomere, a structure found at the ends of chromosomes which protects the DNA in the rest of the chromosome from damage.\n\nMay-Britt Moser, together with Edvard Moser and their students Torkel Hafting, Marianne Fyhn and Sturla Molden, discovered grid cells, cells which contribute to the brain's positioning and navigation system. The grid cells of a freely moving animal fire when the animal is near the vertices of a hexagonal grid in the environment.\n\n"}
{"id": "19646692", "url": "https://en.wikipedia.org/wiki?curid=19646692", "title": "List of probability journals", "text": "List of probability journals\n\nThis is a list of peer-reviewed scientific journals published in the field of probability.\n\n"}
{"id": "36961844", "url": "https://en.wikipedia.org/wiki?curid=36961844", "title": "List of verrucous carcinoma subtypes", "text": "List of verrucous carcinoma subtypes\n\nVerrucous carcinoma is a type of squamous cell carcinoma that may be associated with HPV infection (may be subtypes 16 or 18, but types 6 and 11 have also been reported, as have HPV negative variants). Several subtypes of verrucous carcinoma have been described. Treatment of verrucous carcinoma with radiation therapy should be avoided due to the risk of anaplastic transformation.\n\n\n"}
{"id": "336917", "url": "https://en.wikipedia.org/wiki?curid=336917", "title": "M2 motorway (Great Britain)", "text": "M2 motorway (Great Britain)\n\nThe M2 is a motorway in Kent, England. It is 25.7 miles (41.4 km) long and acts as a bypass of the section of the A2 road which runs through the Medway Towns, Sittingbourne and Faversham. It provides an alternative route to the port of Dover, supplementing that of the M20.\n\nThe M2 starts west of Strood, Kent at Three Crutches, diverging southeastwards from the A2 road that heads ESE from Central London, one of five roads of dual carriageway width or greater reaching into the southern half of Greater London. From Junction 1 it has four lanes each way that slope into the Medway Valley south of Rochester. On the west bank of the River Medway is Junction 2 intersecting the A228 between Strood and West Malling, a junction where the master exit roundabout passes under the HS1 track and which retains, by footbridge and tunnel, the North Downs Way (a public footpath).\n\nBy this point the road is mounted on the Medway Viaduct, passing over the Medway Valley railway and the river. On the east (right) bank are Wouldham marshes, south, and north are the elevated suburbs of the three conjoined Medway Towns including Borstal, a village nationally synonymous with its prototype 1902-founded Young Offenders' Institution.\n\nThe M2 ascends a steep stream valley to Blue Bell Hill (under which HS1 runs in tunnel) using split-levels to reach Junction 3 (for Medway Towns & Maidstone) by Walderslade. It takes the north of the escarpment of the North Downs becoming a conventional three lanes, and runs northeast across Cossington Fields, Westfield Sole, Lidsing, and Bredhurst towards Junction 4 where the road becomes two lanes to Junction 7.\n\nContinuing east, passing Medway Service area, it crosses the A249 over the Stockbury Viaduct at Junction 5 (for Sheerness and Maidstone East). It then takes the unusually gentle coastal lower slopes of the North Downs, below which before its end, is Faversham north of Junction 6. It ends at Junction 7, allowing traffic to continue on either of two dual carriageways: the A299 for six coastal towns including four on Thanet or the upgraded A2 towards Canterbury and Dover as far as Lydden, from the edge of Dover which then mainly reduces to one operational lane each way.\n\nThe initial section of the motorway (junctions 2 to 5) was opened by the then Transport Minister Ernest Marples on 29 May 1963, with the remainder being constructed in 1965. It was opened in three stages:\n\nIt was planned to extend the M2 to London and Dover, making it the main route between London and the channel ports, but this extension never materialised due to a lack of traffic demand. Instead the A2 was dualled and improved from Brenley Corner to Dover.\n\nThe M2 was originally to be designated as the A2(M), but as a result of the \"Daily Telegraph\" reporting it as the M2, the Ministry of Transport adopted this, and later decided upon the M20 designation for the main London-Channel Ports link.\n\nAside from retrofitting central crash barriers, like all early motorways, the alignment of the M2 did not significantly change until the late 1990s. Traffic using it decreased when the M20 was completed from London to Folkestone in May 1991, while the M2 continued to Canterbury and the North Kent ports of Sheerness and Ramsgate. Junction 1 was altered when the A289 \"Wainscott Northern bypass\" was built in the late 1990s.\n\nThe M2 was still busy between Junctions 1 and 4, and suffered from HGVs blocking the outside lane. In 2000 work began on widening the M2 from two lanes to four lanes. A joint venture between Costain, Skanska and Mowlem (CSM) created the company that would undertake the project. The project required the redesign of Junction 2 and Junction 3, and a second Medway Bridge. The existing bridge was converted to a four lane eastbound carriageway (including a hard shoulder). The new bridge formed the westbound carriageway. The entire stretch was lit with streetlights (the old section was not lit). The old Medway Bridge was physically narrowed by removing part of the footpath. High-pressure water cutting equipment was used to cut the concrete into manageable sections for disposal. There is only one path open to the public now.\n\nSpoil from the North Downs Tunnel was used to form the new embankment for the London bound traffic between Junction 2 and the Nashenden Valley.\n\nThe widening was completed in July 2003.\n\nThe M2 opened with a single service area between Junctions 4 and 5, named Farthing Corner Services and operated by Top Rank. Today the services are known as Medway services and are operated under the Moto brand with a Travelodge hotel.\n\nThe services have an access road to the local network for service and delivery vehicles that is not, like some motorway service areas, restricted with a gate or barrier. This has led to local businesses using the services as an unofficial exit from the motorway.\n\nData from driver location signs are used to provide distance (in kilometres) and carriageway identifier information. Where junctions extend over several hundred metres and the data are available, values are given for the start and end points of the junction.\n\n!scope=col|miles\n!scope=col|km\n!scope=col abbr=\"Westbound\"|Westbound exits (B carriageway)\n!scope=col|Junction\n!scope=col abbr=\"Eastbound\"|Eastbound exits (A carriageway)\n!scope=col|Coordinates\n\n\n"}
{"id": "54301620", "url": "https://en.wikipedia.org/wiki?curid=54301620", "title": "Master of IT in Business", "text": "Master of IT in Business\n\nThe Master of IT in Business (MITB) is a master's degree emphasizing on the application of information technology to create business impact and value. The MITB degree originated in the early 21st century as the digital transformation of businesses took place leaving industries to compete with each other on their technological capacity and advancement. The MITB is typically a sandwich course of two distinct horizontals targeted at providing a holistic view and understanding of both technology and management alike. The MITB offers a more specialized and niche course selection for technology and business education aspirants alike. Students on completion of such degrees typically pursue careers in various verticals leveraging information technology.\n\nThe degree is offered under various names by:\n\nand by many other universities worldwide.\n\nMany programs base their admission decisions in a very similar fashion to the MBA programs, on a combination of undergraduate grade point average, academic transcripts, entrance exam scores, a résumé containing significant work experience, essays, letters of recommendation, and personal interviews. Some schools are also interested in extracurricular activities, community service activities or volunteer work, and how the student can improve the school's diversity and contribute to the student body as a whole. The Graduate Management Admission Test (GMAT) is the most prominently used entrance exam for admissions into MITB programs. The Graduate Record Examination (GRE) is also accepted by almost all MITB programs in order to fulfill any entrance exam requirement they may have. Some schools do not weigh entrance exam scores as heavily as other criteria, and some programs do not require entrance exam scores for admission.\n"}
{"id": "1444198", "url": "https://en.wikipedia.org/wiki?curid=1444198", "title": "Neelanjali Ruby", "text": "Neelanjali Ruby\n\nThe Neelanjali Ruby, at 1,370 carats (274 g), is the world's largest double-star ruby.\n\nA ruby is known as a star ruby if it contains an asterism (distinctive star-shaped light refraction) in the gem. The Neelanjali Ruby is the world's largest star ruby with a 12-point asterism, which is commonly denoted as a double-star ruby.\n\nThe \"Neelanjali\" ruby, along with the Rajarathna ruby\",\" was used as a Shiva lingam and was worshipped for centuries in the home of the family which owned it. The ruby belongs to G. Vidyaraj, the scion of the Aravidu dynasty which was the last of the four imperial lineages to rule the Vijayanagara empire. It is reported to be in safe keeping in Bangalore, India.\n"}
{"id": "54571711", "url": "https://en.wikipedia.org/wiki?curid=54571711", "title": "Panguana", "text": "Panguana\n\nPanguana is a biological research station, founded in 1968 and since 2011 it is also a private conservation area extending over almost 10 km² of tropical primary forest in Peru.\n\nPanguana is located in the low land rainforest at the western foothill of the El Sira mountain range east to the Andes. It was named after a common native tinamou species (\"Crypturellus undulatus\"). The station is located at 230 m asl next to the Rio Yuyapichis, a tributary to the Rio Pachitea. It can be accessed by crossing the Pachitea at the village of Yuyapichis and a 1 1/2h foot march over pastures and through forests or a 2h boat ride upstream the Rio Yuyapichis.\nThe research area terrain is slightly hilly and contains various water bodies, non-flooded terra firme, swamp, alluvial and secondary forests, and also some plantations and pastures in the western border areas. \nIn the east, the area borders the territory of the indigenous people of the Asháninka. Their area stretches to the Sira mountains, which are about 40 km away and almost 2500 m high. It is only extensively used and largely covered by primary rainforest. About 4 km east of the station is a central village of the people with a school, where the Asháninka children from the area go to. Panguana supports this school to teach the local people the value of their rainforest.\nThe annual average temperature is 24.5 °C, but temperatures of over 40 °C or more are quite frequent during the dry season (May - September). Annual precipitation amounts to 2,000 to 3,000 mm with around 180 rain days. The rainy season usually extends from October to April, followed by a dry season. Within the forest, there is a constant humidity of about 90% throughout the year.\nLarge parts of the Panguana conservation area are still covered by primary Amazon rainforest, and thus show a very high biodiversity, which has been explored only fragmentarily. So far, 500 tree species and 16 species of palm trees were identified on an area of 2 square kilometers, and over 670 different vertebrates, including 360 bird, 115 mammalian, 78 reptile, 76 amphibian and 34 fish species were documented. In the 1980s, the head of the station studied the 52 bat species found in Panguana. Meanwhile, 57 species are known. For comparison, there are only about 27 bat species documented for Europe, and only around 254 breeding birds live all over Germany with an area of around 357,000 km². The insect fauna is extraordinarily rich in diversity and only known in the beginning. Manfred Verhaagh from the State Museum of Natural History in Karlsruhe was able to find about 500 species of ants in Panguana, which is one of the highest number of species recorded worldwide. About 250 butterfly species have been found. Moths and small butterflies that have so far hardly been explored are estimated to be between 10,000 and 12,000 species.\n\nThe aim of the research station Panguana is to explore the biodiversity of flora and fauna, and study their ecological relationships. In addition, a unique ecosystem is protected and preserved.\nThrough continuous scientific work, the diverse flora and fauna can be explored, systematically assigned and the different ways of life and biological relationships documented. Numerous diploma and doctoral studies were made in Panguana and many international expeditions were held there. To date, more than 180 scientific publications have been published on the research results in this area. After consultation with the head of the station, Juliane Diller, scientists can work at the research station Panguana.\nFor better observation, over 20 km of paths were cut into the jungle. The station has three guest houses with a laboratory for about 14 people, a round house for dinning and workshops, several boats and a photovoltaic system for electricity and the operation of a well pump. By now, even an internet connection via wifi is available. It is guarded and supervised by the owner of the neighboring farm Carlos Vásquez \"Moro\" Módena, and his family.\n\nThe biological research station Panguana was founded in 1968 by the biologists Maria Koepcke and Hans-Wilhelm Koepcke and was originally only designed for a period of 5 years of field research. As accommodation originally served a wooden cabin, which was abandoned by locals. It was standing on stilts, with a roof of palm leaves (see picture) and an additional kitchen hut next to it. Later it was replaced by a larger house and another was added. Since 2000, her daughter, Juliane Diller, has been running the station. In her absence, the manager, Moro, is her representative. Since 2003, there has been a co-operation between the Zoological State Collection Munich, where Juliane Diller works, and the Natural History Museum in Lima, Peru. Through sponsoring by the \"Hofpfisterei\" in Munich, the area of the station has been enlarged several times over the past few years by the acquisition of adjacent lands.\nIn the 1970s, Hans-Wilhelm Koepcke's unsuccessfully tried to promote Panguana as a nature conservation area and in 1972 the area became an official scientific research area (zona de estudio científico del Ministerio de Agricultura, Direccíon forestal, de caza y Tierras). Finally at the end of 2011, the newly created Peruvian Ministry of Environment declared Panguana as a private conservation area (Area de Conservación Privada) in order to protect the area permanently from clearing, hunting and colonization.\n\n\n"}
{"id": "53057", "url": "https://en.wikipedia.org/wiki?curid=53057", "title": "Pareto distribution", "text": "Pareto distribution\n\n</math>\n\nThe Pareto distribution, named after the Italian civil engineer, economist, and sociologist Vilfredo Pareto, is a power-law probability distribution that is used in description of social, scientific, geophysical, actuarial, and many other types of observable phenomena. Originally applied to describing the distribution of wealth in a society, fitting the trend that a large portion of wealth is held by a small fraction of the population, the Pareto distribution has colloquially become known and referred to as the Pareto principle, or \"80-20 rule\", and is sometimes called the \"Matthew principle\". This rule states that, for example, 80% of the wealth of a society is held by 20% of its population. However, the Pareto distribution only produces this result for a particular power value, formula_12 (\"α\" = log5 ≈ 1.16). While formula_12 is variable, empirical observation has found the 80-20 distribution to fit a wide range of cases, including natural phenomena and human activities.\n\nIf \"X\" is a random variable with a Pareto (Type I) distribution, then the probability that \"X\" is greater than some number \"x\", i.e. the survival function (also called tail function), is given by\n\nwhere \"x\" is the (necessarily positive) minimum possible value of \"X\", and \"α\" is a positive parameter. The Pareto Type I distribution is characterized by a scale parameter \"x\" and a shape parameter \"α\", which is known as the \"tail index\". When this distribution is used to model the distribution of wealth, then the parameter \"α\" is called the Pareto index.\n\nFrom the definition, the cumulative distribution function of a Pareto random variable with parameters \"α\" and \"x\" is\n\nIt follows (by differentiation) that the probability density function is\n\nWhen plotted on linear axes, the distribution assumes the familiar J-shaped curve which approaches each of the orthogonal axes asymptotically. All segments of the curve are self-similar (subject to appropriate scaling factors). When plotted in a log-log plot, the distribution is represented by a straight line.\n\n\n\n\n\nThe conditional probability distribution of a Pareto-distributed random variable, given the event that it is greater than or equal to a particular number formula_23 exceeding formula_24, is a Pareto distribution with the same Pareto index formula_12 but with minimum formula_23 instead of formula_24.\n\nSuppose formula_28 are independent identically distributed random variables whose probability distribution is supported on the interval formula_29 for some formula_30. Suppose that for all formula_31, the two random variables formula_32 and formula_33 are independent. Then the common distribution is a Pareto distribution.\n\nThe geometric mean (\"G\") is\n\nThe harmonic mean (\"H\") is\n\nThere is a hierarchy of Pareto distributions known as Pareto Type I, II, III, IV, and Feller–Pareto distributions. Pareto Type IV contains Pareto Type I–III as special cases. The Feller–Pareto distribution generalizes Pareto Type IV.\nThe Pareto distribution hierarchy is summarized in the next table comparing the survival functions (complementary CDF).\n\nWhen \"μ\" = 0, the Pareto distribution Type II is also known as the Lomax distribution.\n\nIn this section, the symbol \"x\", used before to indicate the minimum value of \"x\", is replaced by \"σ\".\n\nThe shape parameter \"α\" is the tail index, \"μ\" is location, \"σ\" is scale, \"γ\" is an inequality parameter. Some special cases of Pareto Type (IV) are\n\nThe finiteness of the mean, and the existence and the finiteness of the variance depend on the tail index \"α\" (inequality index \"γ\"). In particular, fractional \"δ\"-moments are finite for some \"δ\" > 0, as shown in the table below, where \"δ\" is not necessarily an integer.\n\nFeller defines a Pareto variable by transformation \"U\" = \"Y\" − 1 of a beta random variable \"Y\", whose probability density function is\n\nwhere \"B\"( ) is the beta function. If\n\nthen \"W\" has a Feller–Pareto distribution FP(\"μ\", \"σ\", \"γ\", \"γ\", \"γ\").\n\nIf formula_41 and formula_42 are independent Gamma variables, another construction of a Feller–Pareto (FP) variable is\n\nand we write \"W\" ~ FP(\"μ\", \"σ\", \"γ\", \"δ\", \"δ\"). Special cases of the Feller–Pareto distribution are\n\nPareto originally used this distribution to describe the allocation of wealth among individuals since it seemed to show rather well the way that a larger portion of the wealth of any society is owned by a smaller percentage of the people in that society. He also used it to describe distribution of income. This idea is sometimes expressed more simply as the Pareto principle or the \"80-20 rule\" which says that 20% of the population controls 80% of the wealth. However, the 80-20 rule corresponds to a particular value of \"α\", and in fact, Pareto's data on British income taxes in his \"Cours d'économie politique\" indicates that about 30% of the population had about 70% of the income. The probability density function (PDF) graph at the beginning of this article shows that the \"probability\" or fraction of the population that owns a small amount of wealth per person is rather high, and then decreases steadily as wealth increases. (Note that the Pareto distribution is not realistic for wealth for the lower end. In fact, net worth may even be negative.) This distribution is not limited to describing wealth or income, but to many situations in which an equilibrium is found in the distribution of the \"small\" to the \"large\". The following examples are sometimes seen as approximately Pareto-distributed:\n\n\n\nThe Pareto distribution is related to the exponential distribution as follows. If \"X\" is Pareto-distributed with minimum \"x\" and index \"α\", then\n\nis exponentially distributed with rate parameter \"α\". Equivalently, if \"Y\" is exponentially distributed with rate \"α\", then\n\nis Pareto-distributed with minimum \"x\" and index \"α\".\n\nThis can be shown using the standard change-of-variable techniques:\n\nThe last expression is the cumulative distribution function of an exponential distribution with rate \"α\".\n\nThe Pareto distribution and log-normal distribution are alternative distributions for describing the same types of quantities. One of the connections between the two is that they are both the distributions of the exponential of random variables distributed according to other common distributions, respectively the exponential distribution and normal distribution.\n\nThe Pareto distribution is a special case of the generalized Pareto distribution, which is a family of distributions of similar form, but containing an extra parameter in such a way that the support of the distribution is either bounded below (at a variable point), or bounded both above and below (where both are variable), with the Lomax distribution as a special case. This family also contains both the unshifted and shifted exponential distributions.\n\nThe Pareto distribution with scale formula_51 and shape formula_12 is equivalent to the generalized Pareto distribution with location formula_53, scale formula_54 and shape formula_55. Vice versa one can get the Pareto distribution from the GPD by formula_56 and formula_57.\n\nThe Pareto distribution is continuous probability distribution. Zipf's law, also sometimes called the zeta distribution, is a discrete distribution, separating the values into a simple ranking. Both are a simple power law with a negative exponent, scaled so that their cumulative distributions equal 1. Zipf's can be derived from the Pareto distribution if the formula_58 values (incomes) are binned into formula_59 ranks so that the number of people in each bin follows a 1/rank pattern. The distribution is normalized by defining formula_51 so that formula_61 where formula_62 is the generalized harmonic number. This makes Zipf's probability density function derivable from Pareto's.\n\nformula_63\n\nwhere formula_64 and formula_58 is an integer representing rank from 1 to N where N is the highest income bracket. So a randomly selected person (or word, website link, or city) from a population (or language, internet, or country) has formula_66 probability of ranking formula_58.\n\nThe \"80-20 law\", according to which 20% of all people receive 80% of all income, and 20% of the most affluent 20% receive 80% of that 80%, and so on, holds precisely when the Pareto index is \"α\" = log(5) = log(5)/log(4), approximately 1.161. This result can be derived from the Lorenz curve formula given below. Moreover, the following have been shown to be mathematically equivalent:\n\nThis does not apply only to income, but also to wealth, or to anything else that can be modeled by this distribution.\n\nThis excludes Pareto distributions in which 0 < \"α\" ≤ 1, which, as noted above, have infinite expected value, and so cannot reasonably model income distribution.\n\nPrice's square root law is sometimes offered as a property of or as similar to the Pareto distribution. However, the law only holds in the case that formula_68. Note that in this case, the total and expected amount of wealth are not defined, and the rule only applies asymptotically to random samples. The extended Pareto Principle mentioned above is a far more general rule.\n\nThe Lorenz curve is often used to characterize income and wealth distributions. For any distribution, the Lorenz curve \"L\"(\"F\") is written in terms of the PDF \"f\" or the CDF \"F\" as\n\nwhere \"x\"(\"F\") is the inverse of the CDF. For the Pareto distribution,\n\nand the Lorenz curve is calculated to be\n\nFor formula_72 the denominator is infinite, yielding \"L\"=0. Examples of the Lorenz curve for a number of Pareto distributions are shown in the graph on the right.\n\nAccording to Oxfam (2016) the richest 62 people have as much wealth as the poorest half of the world's population. We can estimate the Pareto index that would apply to this situation. Letting ε equal formula_73 we have:\nor\nThe solution is that \"α\" equals about 1.15, and about 9% of the wealth is owned by each of the two groups. But actually the poorest 69% of the world adult population owns only about 3% of the wealth.\n\nThe Gini coefficient is a measure of the deviation of the Lorenz curve from the equidistribution line which is a line connecting [0, 0] and [1, 1], which is shown in black (\"α\" = ∞) in the Lorenz plot on the right. Specifically, the Gini coefficient is twice the area between the Lorenz curve and the equidistribution line. The Gini coefficient for the Pareto distribution is then calculated (for formula_76) to be\n\n(see Aaberge 2005).\n\nThe likelihood function for the Pareto distribution parameters \"α\" and \"x\", given an independent sample \"x\" = (\"x\", \"x\", ..., \"x\"), is\n\nTherefore, the logarithmic likelihood function is\n\nIt can be seen that formula_80 is monotonically increasing with \"x\", that is, the greater the value of \"x\", the greater the value of the likelihood function. Hence, since \"x\" ≥ \"x\", we conclude that\n\nTo find the estimator for \"α\", we compute the corresponding partial derivative and determine where it is zero:\n\nThus the maximum likelihood estimator for \"α\" is:\n\nThe expected statistical error is:\n\nMalik (1970) gives the exact joint distribution of formula_85. In particular, formula_86 and formula_87 are independent and formula_86 is Pareto with scale parameter \"x\" and shape parameter \"nα\", whereas formula_87 has an inverse-gamma distribution with shape and scale parameters \"n\" − 1 and \"nα\", respectively.\n\nThe characteristic curved 'long tail' distribution when plotted on a linear scale, masks the underlying simplicity of the function when plotted on a log-log graph, which then takes the form of a straight line with negative gradient: It follows from the formula for the probability density function that for \"x\" ≥ \"x\",\n\nSince \"α\" is positive, the gradient −(\"α\" + 1) is negative.\n\nRandom samples can be generated using inverse transform sampling. Given a random variate \"U\" drawn from the uniform distribution on the unit interval (0, 1], the variate \"T\" given by\n\nis Pareto-distributed. If \"U\" is uniformly distributed on [0, 1), it can be exchanged with (1 − \"U\").\n\n | cdf =formula_92\n\nThe bounded (or truncated) Pareto distribution has three parameters: \"α\", \"L\" and \"H\". As in the standard Pareto distribution \"α\" determines the shape. \"L\" denotes the minimal value, and \"H\" denotes the maximal value.\n\nThe probability density function is\n\nwhere \"L\" ≤ \"x\" ≤ \"H\", and \"α\" > 0.\n\nIf \"U\" is uniformly distributed on (0, 1), then applying inverse-transform method \n\nis a bounded Pareto-distributed.\nThe symmetric Pareto distribution can be defined by the probability density function:\n\nIt has a similar shape to a Pareto distribution for \"x\" > \"x\" and is mirror symmetric about the vertical axis.\n\nThe univariate Pareto distribution has been extended to a multivariate Pareto distribution.\n\n\n\n"}
{"id": "1722775", "url": "https://en.wikipedia.org/wiki?curid=1722775", "title": "Pure sociology", "text": "Pure sociology\n\nLike rational choice theory, conflict theory, or functionalism, pure sociology is a sociological paradigm — a strategy for explaining human behavior. Developed by Donald Black as an alternative to individualistic and social-psychological theories, pure sociology was initially used to explain variation in legal behavior. Since then, Black and other pure sociologists have used the strategy to explain terrorism, genocide, lynching, and other forms of conflict management as well as science, art, and religion.\n\nPure sociology explains social life with its social geometry. Social life refers to any instance of human behavior—such as law, suicide, gossip, or art — while the social geometry of a behavior, also called its social structure, refers to the social characteristics of those involved—such as their degree of past interaction or their level of wealth. To some extent this approach draws from aspects of earlier sociological work, ranging from Durkheim's emphasis on social explanations for individual behavior to later work in the variation of police (and other legal) behavior.\n\nVirtually all sociology explains the behavior of people—whether groups or individuals—with some reference to their mental constructs (psychology) or the purposes of their action (teleology). But pure sociology reconceptualizes human behavior as social life—something that does not exist in the mind, is not explainable by the aims of actions, and is supraindividual. Pure sociology, then, can be distinguished from other sociological paradigms by what is absent from it: psychology, teleology, and even people as such. Pure sociology's focus on a unique social reality may sound Durkheimian, but Black views the approach as \"more Durkheimian than Durkheim.\"\n\nIn \"The Behavior of Law, published in 1976, Donald Black introduced the first example of pure sociology—a general theory of law, or governmental social control. This theory seeks to explain variation in law, and one aspect of legal variation is the amount of law attracted to a case of conflict. A conflict is a situation where one person has a grievance against another, such as where an assault has occurred or a contract has been broken, and the offended parties may or may not appeal to the police or to the civil courts to resolve it. Cases may attract law or not, then, and when they do attract law, there may be more or less of it. When the police make an arrest in an assault case, for instance, there is more law when there is merely a call to the police, and when someone is convicted and sentenced there is more law than when there is merely an arrest. The pure sociology of law explains this variation by identifying a number of sociological variables that are associated with variation in the quantity of law. These include various forms of social status (such as wealth, integration, culture, conventionality, organization, and respectability) as well as various forms of social distance (such as relational distance and cultural distance). These are aspects of the social structures of cases, then, and so cases where the disputants are both high in status have different social structures—and are handled differently—than cases involving low-status disputants. Whether the disputants are socially close to or distant from one another also determines the amount of law the case attracts. For example, one of the theory’s predictions is that within a society, law varies directly with relational distance. Relational distance refers to the amount and intensity of interaction between the parties, so the theory predicts that there is more law in conflicts between strangers than in those between intimates. This aspect of the theory explains numerous facts, such as why those who kill strangers are punished more severely than those who kill intimates and why women who are raped by strangers are more likely to report it to the police.\n\nSince the publication of \"The Behavior of Law\", Black and other pure sociologists have applied the theoretical strategy to numerous other subjects. Most notably, Black has developed a general theory of social control that goes beyond law to explain more generally the handling of all human conflicts. Most conflicts are handled without appealing to the legal system, and the theory thus explains not just law but avoidance, gossip, therapy, feuding, and numerous other forms of non-governmental social control. In addition to extending the subject matter, this later work also extends the theory to focus not just on the social characteristics of the initial disputants in a conflict, but also of third parties (all those with knowledge of a conflict). For example, Mark Cooney examines how third party behavior shapes violence. Whether and how third parties involve themselves in a conflict can determine not only the likelihood of violence, but also the form the violence takes. For example, social configurations characterized by close and distant group ties are conducive to feud-like behavior where violence occurs back and forth between groups over a long period of time. In this situation, third parties are members of groups, and they are relationally close to fellow group members but distant from others. When conflicts between groups occur, they thus support one side and oppose the other, and they may join in retaliatory violence against members of rival groups. Other social configurations are conducive to other forms of violence or even to peace. For example, where there are cross-cutting ties, such as where people are relationally close to members of other groups, third parties are more likely to promote peace.\n\nRecently, Black has moved beyond the study of how conflicts are handled to examine the origin of conflict itself. \"Moral Time\" identifies the causes of clashes of right and wrong in human relationships. In doing so, this theory invokes a new explanatory concept—the idea of movement in \"social time\" — and thus extends the pure sociological approach.\n\nBlack and others have also moved beyond conflict and social control to develop explanations of ideas, predation, welfare, research, and other forms of social life. For example, Black’s theory of ideas explains the content of ideas with their social structures. Just as each conflict has a social structure that consists of the social characteristics of the disputants and third parties, every idea—every statement about reality—has a social structure consisting of the characteristics of the source, subject, and audience. For example, the subject of an idea may be intimate or distant from the source: People have ideas about family members and friends as well as strangers. The subject may also be high or low in social status: People have ideas about senators and businessmen as well as skid row vagrants. But ideas vary depending on their social structures. Black’s explanation of voluntarism and determinism, for example, states that ideas about high status subjects are more likely to be voluntaristic (to invoke free will). The theory would predict, then, that people would offer voluntaristic explanations of senators and businessmen and deterministic explanations of skid row vagrants.\n\nA number of sociologists have used at least some elements of Black's theoretical strategy in their work, including Professors M.P. Baumgartner, Marian Borg, Bradley Campbell, Mark Cooney, Ellis Godard, Allan Horwitz, Scott Jacques, Marcus Kondkar, Jason Manning, Joseph Michalski, Calvin Morrill, Scott Phillips, Roberta Senechal de la Roche, and James Tucker.\n\nWhile prominent sociologists such as Randall Collins, Karen A. Cerulo, David Sciulli, and Jonathan H. Turner have praised aspects of pure sociology, the approach has also been criticized. Kam C. Wong criticizes pure sociology’s scientism, David F. Greenberg its use of covering-law explanations, and Thomas J. Scheff its attempt at disciplinary purity. In a 2008 symposium, Douglas A. Marshall offers an extended critique of the system. Marshall argues that, contrary to Black’s stated goal of making sociology more scientific, his approach is actually antithetical to modern scientific values and practices—a theme reiterated by Stephen Turner in the same symposium.\n\nMark Cooney, Allan Horwitz, and Joseph Michalski have responded to some specific criticisms of pure sociology, while Donald Black, in \"The Epistemology of Pure Sociology\" as well as other writings, has responded generally to critics' claims and provided an extensive defense of the pure sociological approach.\n\nNoting the ideological nature of many of the attacks, Black says that his theory is in fact \"politically and morally neutral.\" But according to Black, it nonetheless attracts politicized hostility due to its unconventionality:\n\"My work is shocking not because it is politically incorrect, but because it is epistemologically incorrect. It violates conventional conceptions of social reality in general and legal and moral reality in particular. It therefore shocks — epistemologically shocks — many on whom it is inflicted. If I disturb your universe I may be worthy of contempt. I may appear to be your favorite political enemy, a conservative if you are radical, a radical if you are conservative.\"\nBlack also discusses the aims of the approach. While it is unconventional sociology, it is conventional science, striving to provide simple, general, testable, valid, and original explanations of reality. And it is by these criteria alone, Black maintains, that it should be judged:\n\"If you wish to criticize my work, tell me you can predict and explain legal and related behavior better than I can. Tell me my work is not as testable as something else, tell me it is not as general as something else, tell me it is less elegant than something else, tell me that it has already been published, or just tell me it is wrong. Tell me something relevant to what I am trying to accomplish — something scientific.\"\nBaumgartner, M.P.\n\n\nBlack, Donald\n\n\nBlack, Donald and M.P. Baumgartner\n\nBorg, Marian J.\n\nBorg, Marian J. and William P. Arnold III\n\nBorg, Marian J. and Karen F. Parker\n\nCampbell, Bradley\n\nCooney, Mark\n\nCooney, Mark and Scott Phillips\n\nGeiger-Oneto, Stephanie and Scott Phillips\n\nGodard, Ellis\n\nHawdon, James and John Ryan\n\nHembroff, Larry A.\n\nHoffmann, Heath C.\n\nHorwitz, Allan V.\n\nJacques, Scott, and Richard Wright\n\nKan, Yee W. and Scott Phillips\n\nKruttschnitt, Candace\n\nKuan, Ping-Yin\n\nLally, William E. and Alfred DeMaris\nLee, Catherine\n\nManning, Jason\n\nMarshall, Douglas A.\n\nMichalski, Joseph H.\n\nMileski, Maureen\n\nMorrill, Calvin\n\nMullis, Jeffrey\n\nPeterson, Elicka S.\n\nPhillips, Scott\n\nPhillips, Scott and Mark Cooney\n\nSenechal de la Roche, Roberta\n\nSilberman, Matthew\n\nTucker, James\n\nTucker, James and Susan Ross\n\nWong, Kam C.\n\nWong, Siu Kwong\n"}
{"id": "7714627", "url": "https://en.wikipedia.org/wiki?curid=7714627", "title": "Reproducing kernel particle method", "text": "Reproducing kernel particle method\n\nIn applied mathematics, the reproducing kernel particle method is a meshfree computational method introduced in Liu et al. \n"}
{"id": "39107062", "url": "https://en.wikipedia.org/wiki?curid=39107062", "title": "Richard Reeve", "text": "Richard Reeve\n\nRichard Reeve (\"fl.\" 1640 – 1680) was an instrument-maker in London in the 17th century. He worked with Christopher Wren and Robert Hooke. His son was also Richard Reeve (fl. 1680).\n\nReeve's \"telescopes and microscopes had a worldwide reputation for accuracy. Hooke worked with him in a technical advisory capacity\". Richard Reeve, or Reeves, of Long Acre, was the foremost fashioner of optical instruments between 1641 and 1679, and \"perspective-glass maker to the King\".\n\nHe was James Gregory's optician.\nIn August 1664 Pepys purchased a microscope from him, \"the best he knows in England, and he makes the best in the world.\" 5 pounds 10 shillings is \"a great price,\" but Reeve throws in a Scotoscope (camera obscura), \"and a curious curiosity it is to [see] objects in a dark room with.\"\n\nReeve's son, also an instrument maker was known as Richard Reeve Jr (\"fl\". 1680). The man referred to as \"Young\" Reeve in Pepys' entry of 23 March 1659/60, would be the older Richard's son John, who took over the family business in 1679 and ran it until about 1710. The older Richard was arrested in 1664 for murdering his wife, but secured a royal pardon, probably at great cost.\n"}
{"id": "21172290", "url": "https://en.wikipedia.org/wiki?curid=21172290", "title": "Skunkworks project", "text": "Skunkworks project\n\nA skunkworks project is a project developed by a small and loosely structured group of people who research and develop a project primarily for the sake of radical innovation. The term originated with Lockheed's World War II \"Skunk Works\" project.\n\nEverett Rogers defined \"skunkworks\" as an \"enriched environment that is intended to help a small group of individuals design a new idea by escaping routine organizational procedures.\"\n\nThe term originated during World War II when the P-80 Shooting Star was designed by Lockheed’s Advanced Development Projects Division in Burbank, California, under similar circumstances. A closely guarded incubator was set up in a circus tent next to a plastics factory in Burbank. The strong smells that wafted into the tent made the Lockheed R&D workers think of the foul-smelling “Skonk Works” factory in Al Capp’s \"Li'l Abner\" comic strip. \n\nSince its origination with Skunk Works, the term was generalized to apply to similar high-priority R & D projects at other large organizations which feature a small team removed from the normal working environment and given freedom from management constraints.\n\nThe term typically refers to technology projects developed in semi-secrecy, such as Google X Lab. Other famous skunkworks were Microsoft Research, special teams at Boeing, and the lab of about 50 people established by Steve Jobs to develop the Macintosh computer, located behind the Good Earth Restaurant in Cupertino.\n\n\"The Economist\" notes that the expectations for the products developed by skunkworks have changed in the 21st century from \"something that makes their competitors say 'Wow'\" to \"something that makes their competitors' customers say 'Wow'\". Rather than sequestering skunkworks, the companies now tend to promote communication between them and marketing, design, and accounting departments.\n\n"}
{"id": "52952271", "url": "https://en.wikipedia.org/wiki?curid=52952271", "title": "The Etches Collection", "text": "The Etches Collection\n\nThe Etches Collection (formerly known as the Museum of Jurassic Marine Life) is a fossil museum in the village of Kimmeridge, Purbeck, Dorset, England. It is based on the lifetime collection of Steve Etches from the local area on the Jurassic Coast, a World Heritage Site, especially around Kimmeridge Bay and the Kimmeridge Ledges.\n\nThe museum building was opened in 2016 at a cost of £5 million to house a collection of over 2,000 fossil specimens. Steve Etches had been collecting for over 30 years prior to the museum opening, and in this time he has amassed an important collection of fossils that form the basis of the collection. The collection includes examples of ammonite eggs. The museum is considered world-class.\n\nThe museum uses social media such as Facebook and Twitter.\n\n\n"}
{"id": "36802383", "url": "https://en.wikipedia.org/wiki?curid=36802383", "title": "The Passions of the Mind", "text": "The Passions of the Mind\n\nThe Passions of the Mind is a 1971 novel by American author Irving Stone. It is a biographical novel about the psychiatrist Sigmund Freud and covers his life from when he was a student to when he is forced to leave Austria to escape the growing influence of the Nazis. It covers many aspects of the subject's life, including his hospital work, his relationship with his parents, his marriage to Martha Bernays, and his support for his successor, Carl Jung. The book is notable for going into great detail of Freud's theories, especially the Oedipus Complex.\n\nIrving Stone is best renowned for his several biographical novels, the best known being \"Lust for Life\" and \"The Agony and the Ecstasy\" (about the artists Vincent Van Gogh and Michelangelo, respectively), which were both adapted into major Hollywood productions. Though less well known, \"Passions of the Mind\" was an American bestseller upon its release, spending 13 weeks at the top of the \"New York Times\" Bestseller List (fiction) in the spring of 1971, and nearly 30 weeks in the top 15.\n"}
{"id": "33520809", "url": "https://en.wikipedia.org/wiki?curid=33520809", "title": "Theano (software)", "text": "Theano (software)\n\nTheano is a Python library and optimizing compiler for manipulating and evaluating mathematical expressions, especially matrix-valued ones.\nIn Theano, computations are expressed using a NumPy-esque syntax and compiled to run efficiently on either CPU or GPU architectures.\n\nTheano is an open source project primarily developed by a Montreal Institute for Learning Algorithms (MILA) at the Université de Montréal.\n\nOn 28 September 2017, Pascal Lamblin posted a message from Yoshua Bengio, \nHead of MILA: major development would cease after the 1.0 release due to competing offerings by strong industrial players. Theano 1.0.0 was then released on 15 November 2017.\n\nThe following code is the original Theano's example. It defines a computational graph with 2 scalars \"a\" and \"b\" of type \"double\" and an operation between them (addition) and then creates a python function \"f\" that do the actual computation.\nimport theano\nfrom theano import tensor\n\na = tensor.dscalar()\nb = tensor.dscalar()\n\nc = a + b\n\nf = theano.function([a,b], c)\n\nassert 4.0 == f(1.5, 2.5)\n\n"}
{"id": "35599585", "url": "https://en.wikipedia.org/wiki?curid=35599585", "title": "Warranting theory", "text": "Warranting theory\n\nWarranting theory is a theory adapted by Joseph B. Walther and Malcolm Parks from the works of Stone.\n\nThe adapted construct of warranting suggests that in the presence of anonymity, a person may potentially misrepresent information about his or her self. The greater the potential for such misrepresentation, the more likely observers are to be skeptical of the presented information. Warrants in this manner are cues that an observer uses to gauge the accuracy of a person's given information or profile.\n\nWalther and Parks (2002) observed a phenomenon in which people met offline after having first met online. Sometimes these experiences were positive, and other times they were negative. Walther and Parks (2002) were dissatisfied with existing theories' ability to explain these phenomena. To fill in the theoretical gap, Walther and Parks (2002) adapted the original concept of warranting presented by Stone (1995), describing connections between one's self and self-presentation as a continuum rather than a binary, moderated by anonymity. They suggested that the potential for anonymity resulted in the potential for a discrepancy along this continuum. The greater this potential discrepancy, the more compelling it is for observers to be skeptical of information provided by the individual about the self (Walther & Parks, 2002). Warrants, as described by Walther and Parks (2002), are perceived reliable cues that observers use to gauge how one's true identity matches that which is presented online. \n\nHowever, not all of these cues are weighted equally; rather, warrants possess a warranting value (Walther & Parks, 2002). This value is defined as the extent to which the cue is perceived to be unaltered by the target. Warrants that are very difficult to manipulate by the user are considered high in warranting value, while those that are easily changed have a low warranting value and are therefore much more questionable in terms of accuracy (Walther & Parks, 2002). For example, an article written about an individual has a higher warranting value than a social profile created by the same individual.\n\nWalther and Parks (2002) speculated that being able to obtain information from a partner's social network would increase the warrants within an online relationship. Because information from others is of high warranting value, it stands to reason that those invested in a potential online relationship would use available resources, in this case social networks, to alleviate any skepticism about the accuracy of claims made by a relational partner.\n\nWarrants do not necessarily have to be provided or controlled by others. Walther and Parks (2002) introduced the concept of partial warranting. This is information that, though provided by the user, contains easily verifiable facts. For example, the presentation of one's given name is a partial warrant, as this information can be used to look up public records or link to other profiles the user may possess. Providing numerical information, such as height, weight, age, or address, also constitutes as partial warranting, as these figures are easily checked and provide little room for gray area.\n\nMuch of the existing research regarding warranting examined how perception and judgments about an individual are influenced by others-generated information. Walther, Van Der Heide, Kim, Westerman, and Tong (2008), used fake Facebook profiles to assess if the attractiveness of friends, as well as what these friends said on an individual's profile, had an effect on social attraction. Profiles that were neutral in content displayed two comments from friends. The small profile pictures of commenting friends were either attractive or unattractive, and the comments suggested either socially desirable or socially undesirable behaviors (Walther et al., 2008).\n\nIt was found that social attractiveness was positively correlated with the physical attractiveness of commenting friends (Walther et al., 2008). This indicates that the simple observable presence of others in one's social network may be enough to make social judgments. Mixed results were found regarding actual content of friend's comments (Walther et al., 2008). A significant effect was found, although these effects depended on the gender of the profile owner. This confirms the assertion in warranting theory that comments made by others do indeed warrant judgments, but adds that these judgments may be moderated by the initial claims made by the target.\n\nThis study admittedly left a gap that required further research. While it confirmed that perceptions are influenced by others, the claims made by a user were very limited and purposefully neutral (Walther et al., 2008). Therefore, the authors were unable to make assertions about the power of others-generated cues over self-generated claims. A follow-up study addressed this issue.\n\nWalther, Van Der Heide, Hamel, and Shulman, (2009) tested the effects of self-generated information against information generated by others. Walther et al. (2009) compared subjects reactions to fake Facebook profiles and their judgments of extroversion and introversion. Profiles contained either self-generated information suggesting the profile owner was introverted or extroverted, and others-generated statements suggesting the owner was introverted or extroverted. Information suggesting introversion was considered negative while information suggestion extroversion was considered to be positive. Walther et al. (2009) found that while others-generated statements do indeed have an effect on observer judgments, the effect did not override self-generated information or negativity effects.\n\nTaking a further look at the effects of positive and negative statements with self versus others-generated information, Walther, Van Der Heide, Hamel, and Shulman (2009) replaced statements of introversion and extroversion with physical attractiveness. Subjects judged physical attractiveness of subjects based on information in the owner's profile and comments made by the owner's friends; pictures of owners were either male or female and were pre-tested to be at a neutral level of attractiveness.\n\nWalther et al. (2009) found that when asked to make judgments of attractiveness, results were strongly in line with predictions made by warranting theory. Walther et al. (2009) explain the discrepancies in results between the two experiments by posing that perhaps more is going on when making judgments of internal versus external characteristics. It is possible that friend's judgments about personality are deemed less accurate than ones made by the person his or herself. This is likely due to the assumption that a person would know his or herself best, and may still perform activities associated with another temperament while still claiming to be the opposite; for example, a true introvert may still elect\nto go out with friends on occasion (Walther et al., 2009).\n\nDespite the introduction of partial warranting in the seminal article, very little research focusing on this construct exists. Gibbs, Ellison, and Lai (2010) were the closest to studying partial warranting, or verifiable claims provided by the target, in isolation. Gibbs et al. (2010) studied warranting in the context of online dating profiles.\n\nThe study found that people in online dating situations tended to utilize web resources outside of the dating website to confirm partial warrants. For example, if a real full name was provided by a potential relational partner, users typed this information into search engines in order to retrieve public records or links to additional social-network site (SNS) profiles (Gibbs et al., 2010). This type of fact-checking was introduced as \"social triangulation\" (Gibbs et al., 2010). The authors speculated that this strategy would be common when \"true warrants\", or information uncontrolled by the user, are not available (Gibbs, et al., 2010).\n\nSince the introduction of warranting theory, two studies have examined how deceptive a person may be in terms of information they may present and the degree of verifiability of their warrants.\n\nParks and Archey-Ladas (2003) examined a sample of 200 personal home pages, and coded the information provided by the users. To examine the data, Parks and Archey-Ladas (2003) looked at warrants (easily verified information) and constraining information (information that is not easily verified but restricts identity. Items include political activity and hobbies). Parks and Archey-Ladas (2003) found that despite the freedom afforded to users to create identities online, people quite frequently had many linkages between their online and offline lives on their personal home pages. While it is suggested by Parks and Archey-Ladas (2003) that people present verifiable information online, it is assumed that the\ninformation is honest, as Parks and Archey-Ladas did not actually follow up with an attempt to contact owners of pages. As such, this study only demonstrates people present verifiable information, but did not actually find out to what extent the present information was actually true.\n\nWarkentin, Woodsworth, Hancock, and Cormier (2010) wondered if warranting potential in a medium influenced the presence of deceptive practices. They suspected that users of online media would be more truthful if others could easily catch them in a lie. A simple correlational study was conducted in which self-report survey answers regarding deception across mediums were compared to the warranting potential of each medium (Warkentin et al., 2010).\n\nThe hypotheses were confirmed. SNSs, which were found to have the highest warranting potential, also featured the least amount of self-reported deception. Synchronous chat, which scored the lowest in warranting potential, was reported to be the largest source of deceptive practices (Warkentin et al., 2010). From this study, it appears that people have an awareness of warranting by others, and act accordingly. Because this was a correlational study, these findings could have an alternate explanation.\n\nParks (2011) presented three boundary conditions that must be present for\ntrue warranting to exist:\n\n\nWarranting capacity is not medium-specific, but instead may occur anywhere where these three conditions are met. Still, it is often assumed that certain types of computer-mediated communication (CMC) lend themselves more easily to these\ncriteria than others. For example, SNSs have been at the epicenter of studies on warranting, suggesting that they are the ideal avenue for the examination of the theory. To explore this notion, Parks (2011) used a longitudinal study to explore whether a popular SNS, Myspace, met the three requirements.\n\nOverwhelmingly, Myspace failed to satisfy even one of the boundary conditions (Parks, 2011). Members posted very little about their identity, made few social connections, and received few to no comments on their profiles (Parks, 2011). Although it could be argued that Myspace is becoming more of a business avenue than SNSs (Parks, 2011), this study still cautions against a medium-based understanding of warranting. Instead, individual cases should be measured against qualifying criterion.\n\n"}
{"id": "7657737", "url": "https://en.wikipedia.org/wiki?curid=7657737", "title": "Wet lab", "text": "Wet lab\n\nA wet lab is a type of laboratory where it is necessary to handle various types of chemicals and potential \"wet\" hazards, so the room has to be carefully designed, constructed, and controlled to avoid spillage and contamination. \n\nA dry lab might have large experimental equipment but minimal chemicals, or instruments for analyzing data produced elsewhere.\n\nA wet lab is a type of laboratory in which a wide range of experiments are performed, for example, characterizing of enzymes in biology, titration in chemistry, diffraction of light in physics, etc. - all of which may sometimes involve dealing with hazardous substances. Due to the nature of these experiments, the proper ventilation and appropriate arrangement of safety equipment are of great importance. \n\nThe safety of occupants must be ensured by placing eye washers/baths and personnel showers in easily reachable positions, installing fire alarms, having fire extinguishers and blankets near the door, having vinyl floors and a ceiling which is inert, and paint on the wall that is easy to clean. There should also be view panels on doors to ensure that the risk of accident or injury to occupants whilst entering/leaving the lab is mitigated. \n\nThe researchers (the occupants) are required to know basic laboratory techniques including safety procedures and techniques related to the experiments that they perform.\n\nAt the present, lab design tends to focus on increasing the interactions between researchers through the use of open plans, allowing the space and opportunity for researchers exchange ideas, share equipment, and share storage space; increasing productivity and efficiency of experiments. This style of design has been proposed to support team-based work, though more compartmentalised or individual spaces are still important for some types of processes which require separate/isolated space such as electron microscopes, tissue cultures, work/workers that may be disturbed by noise levels, etc. \n\nFlexibility of laboratory design should also be promoted, for example, the wall and ceiling should be removable in case of expansion or contraction, the pipes, tubes and fume hoods should also be removable for future expansion, reallocation and change of use. A well thought-through design will ensure that a lab can be adjusted for any future use. The sustainability of resources is also a concern, so the amount of resources and energy used in the lab should be reduced where possible to save the environment, but still yield the same products.\n\nAs a laboratory consists of many areas such as wet lab, dry lab and office areas, wet labs should be separated from other spaces using controlling devices or dividers to prevent cross-contamination or spillage.\n\nDue to the nature of processes undergone in wet labs, the environmental conditions may need to be carefully considered and controlled using a cleanroom system. \n"}
