{"id": "14858883", "url": "https://en.wikipedia.org/wiki?curid=14858883", "title": "3C 215", "text": "3C 215\n\n3C 215 is a Seyfert galaxy / Quasar located in the constellation Cancer.\n\n"}
{"id": "20926419", "url": "https://en.wikipedia.org/wiki?curid=20926419", "title": "3rd meridian east", "text": "3rd meridian east\n\nThe meridian 3° east of Greenwich is a line of longitude that extends from the North Pole across the Arctic Ocean, the Atlantic Ocean, Europe, Africa, the Southern Ocean, and Antarctica to the South Pole.\n\nThe 3rd meridian east forms a great circle with the 177th meridian west.\n\nStarting at the North Pole and heading south to the South Pole, the 3rd meridian east passes through:\n"}
{"id": "6158574", "url": "https://en.wikipedia.org/wiki?curid=6158574", "title": "Aircraft Meteorological Data Relay", "text": "Aircraft Meteorological Data Relay\n\nAircraft Meteorological Data Relay (AMDAR) is a program initiated by the World Meteorological Organization. \nAMDAR is used to collect meteorological data worldwide by using commercial aircraft.\n\nData is collected by the aircraft navigation systems and the onboard standard temperature and static pressure probes.\nThe data is then preprocessed before linking them down to the ground either via VHF communication (ACARS)\nor via satellite link ASDAR.\n\nA detailed description is given in the AMDAR Reference Manual (WMO-No 958) available from the World Meteorological Organization, \nGeneva, Switzerland.\n\nAMDAR transmissions are most commonly used in forecast models as a supplement to radiosonde data, to aid in the plotting of upper-air data between the standard radiosonde soundings at 00Z and 12Z.\n\n"}
{"id": "24892088", "url": "https://en.wikipedia.org/wiki?curid=24892088", "title": "Alv Egeland", "text": "Alv Egeland\n\nAlv Egeland (born 22 March 1932) is a Norwegian physicist.\n\nHe was born in Kvinesdal. He took the cand.real. degree in 1959, and his doctorate at Stockholm University in 1963. He was a professor at the University of Oslo from 1972. His fields are space research and aurora borealis research. He is a member of the Norwegian Academy of Science and Letters.\n"}
{"id": "28913249", "url": "https://en.wikipedia.org/wiki?curid=28913249", "title": "Amphitheatre Glacier", "text": "Amphitheatre Glacier\n\nAmphitheatre Glacier () is a moraine-covered glacier that flows north from The Amphitheatre into Roaring Valley, in the Royal Society Range of Antarctica. It was named by a New Zealand Geographical Society field party in the area, 1977–78, in association with The Amphitheatre.\n\n"}
{"id": "2749087", "url": "https://en.wikipedia.org/wiki?curid=2749087", "title": "Anchialine pool", "text": "Anchialine pool\n\nAn anchialine pool or pond (pronounced \"AN-key-ah-lin\", from Greek \"ankhialos\", \"near the sea\") is a landlocked body of water with a subterranean connection to the ocean. Anchialine pools are a feature of coastal aquifers which are density stratified, with the water near the surface being fresh or brackish, and saline water intruding from the coast below at some depth. Depending on the site, it is sometimes possible to access the deeper saline water directly in the anchialine pool or sometimes it may be accessible by cave diving.\n\nWater levels in anchialine pools often fluctuate with tidal changes due to the coastal location and the connection with the ocean. The range in water levels fluctuations will be decreased (damped) and delayed compared to the range and time observed for the adjacent tide. The primary controls on the damping and lag are the distance from the coast, and the hydraulic conductivity of the geological materials.\n\nAnchialine pools are extremely common worldwide especially along neo-tropical coastlines where the geology and aquifer system are relatively young, and there is minimal soil development. Such conditions occur notably where the bedrock is limestone or recently formed volcanic lava. Many anchialine pools are found on the coastlines of the island of Hawaii, and on the Yucatán Peninsula, where they are locally called cenotes, as well as Christmas Island. The Sailor's Hat crater created by an explosives test in 1965 is an anchialine pool.\n\nEcological studies of anchialine pools frequently identify regionally rare and sometimes endemic species. In Hawaii, the pools are home to the ʻōpaeʻula (Hawaiian shrimp, \"Halocaridina rubra\"). In karst anchialine pools and the caves these may be connected to, the fauna are diverse and include crustaceans, including remipedia and copepods, and among the vertebrates are several species of blind cave fish.\n\n\n"}
{"id": "39589883", "url": "https://en.wikipedia.org/wiki?curid=39589883", "title": "Arago (Martian crater)", "text": "Arago (Martian crater)\n\nArago is a small impact crater in the Arabia quadrangle on Mars at 10.22 N and 29.93° E. and is 152 km in diameter and is in the northernmost of Terra Sabaea. Its name was approved in 1973 and is named after the French astronomer François Arago. Most of the crater is flat.\n\nNearby named prominent craters include Cassini to the north, the large Tikhonravov to the east, Janssen to the southeast, Tuscaloosa to the south, Henry to the west and Pasteur to the north-northwest.\n\nJust south of the crater rim is Naktong Vallis and north of the crater is Scamander Vallis.\n\n"}
{"id": "5761650", "url": "https://en.wikipedia.org/wiki?curid=5761650", "title": "Attachment in adults", "text": "Attachment in adults\n\nIn psychology, the theory of attachment can be applied to adult relationships including friendships, emotional affairs, adult romantic or platonic relationships and in some cases relationships with inanimate objects (\"transitional objects\"). Attachment theory, initially studied in the 1960s and 1970s primarily in the context of children and parents, was extended to adult relationships in the late 1980s.\n\nFour main styles of attachment have been identified in adults:\n\nInvestigators have explored the organization and the stability of mental \"working models\" that underlie these attachment styles. They have also explored how attachment impacts relationship outcomes and how attachment functions in relationship dynamics.\n\nMary Ainsworth and John Bowlby founded modern attachment theory on studies of children and their caregivers. Children and caregivers remained the primary focus of attachment theory for many years. Then, in the late 1980s, Cindy Hazan and Phillip Shaver applied attachment theory to adult relationships. Hazan and Shaver noticed that interactions between adults shared similarities to interactions between children and caregivers. For example, romantic or platonic partners desire to be close to one another. Adults feel comforted when their attachments are present and anxious or lonely when they are absent. Romantic relationships, for example, serve as a secure base that help people face the surprises, opportunities, and challenges life presents. Similarities such as these led Hazan and Shaver to extend attachment theory to adult relationships.\n\nRelationships between adults differ in many ways from relationships between children and caregivers. The claim is not that these two kinds of relationships are identical. The claim is that the core principles of attachment theory apply to both kinds of relationships.\n\nInvestigators tend to describe the core principles of attachment theory in light of their own theoretical interests. Their descriptions seem quite different on a superficial level. For example, Fraley and Shaver describe the \"central propositions\" of attachment in adults as follows:\n\nCompare this with the five \"core propositions\" of attachment theory listed by Rholes and Simpson:\n\nWhile these two lists clearly reflect the theoretical interests of the investigators who created them, a closer look reveals a number of shared themes. The shared themes claim that:\n\nNo doubt these themes could be described in a variety of ways (and other themes added to the list). Regardless of how one describes the core principles of attachment theory, the key insight is that the same principles of attachment apply to close relationships throughout the lifespan. The principles of attachment between children and caregivers are fundamentally the same as the principles of attachment between adults.\n\nAdults are described as having 4 attachment styles: Secure, Anxious-preoccupied, Dismissive-avoidant, and Fearful-avoidant.\n\nThe secure attachment style in adults corresponds to the secure attachment style in children. The anxious–preoccupied attachment style in adults corresponds to the anxious-ambivalent attachment style in children. However, the dismissive-avoidant attachment style and the fearful-avoidant attachment style, which are distinct in adults, correspond to a single avoidant attachment style in children. The descriptions of adult attachment styles offered below are based on the relationship questionnaire devised by Bartholomew and Horowitz and on a review of studies by Pietromonaco and Barrett.\n\nThere are several attachment-based treatment approaches that can be used with adults. In addition, there is an approach to treating couples based on attachment theory.\n\nSecurely attached people tend to agree with the following statements: \"It is relatively easy for me to become emotionally close to others. I am comfortable depending on others and having others depend on me. I don't worry about being alone or others not accepting me.\" This style of attachment usually results from a history of warm and responsive interactions with their attachments. Securely attached people tend to have positive views of themselves and their attachments. They also tend to have positive views of their relationships. Often they report greater satisfaction and adjustment in their relationships than people with other attachment styles. Securely attached people feel comfortable both with intimacy and with independence. \n\nSecure attachment and adaptive functioning are promoted by a caregiver who is emotionally available and appropriately responsive to his or her child's attachment behavior, as well as capable of regulating both his or her positive and negative emotions.\n\nPeople with anxious-preoccupied attachment type tend to agree with the following statements: \"I want to be completely emotionally intimate with others, but I often find that others are reluctant to get as close as I would like\", and \"I am uncomfortable being without close relationships, but I sometimes worry that others don't value me as much as I value them.\" People with this style of attachment seek high levels of intimacy, approval, and responsiveness from their attachment figure. They sometimes value intimacy to such an extent that they become overly dependent on the attachment figure. Compared with securely attached people, people who are anxious or preoccupied with attachment tend to have less positive views about themselves. They may feel a sense of anxiousness that only recedes when in contact with the attachment figure. They often doubt their worth as a person and blame themselves for the attachment figure's lack of responsiveness. People who are anxious or preoccupied with attachment may exhibit high levels of emotional expressiveness, emotional dysregulation, worry, and impulsiveness in their relationships.\n\nPeople with a dismissive style of avoidant attachment tend to agree with these statements: \"I am comfortable without close emotional relationships\", \"It is important to me to feel independent and self-sufficient\", and \"I prefer not to depend on others or have others depend on me.\" People with this attachment style desire a high level of independence. The desire for independence often appears as an attempt to avoid attachment altogether. They view themselves as self-sufficient and invulnerable to feelings associated with being closely attached to others. They often deny needing close relationships. Some may even view close relationships as relatively unimportant. Not surprisingly, they seek less intimacy with attachments, whom they often view less positively than they view themselves. Investigators commonly note the defensive character of this attachment style. People with a dismissive-avoidant attachment style tend to suppress and hide their feelings, and they tend to deal with rejection by distancing themselves from the sources of rejection (e.g. their attachments or relationships).\n\nPeople with losses or other trauma, such as sexual abuse in childhood and adolescence may often develop this type of attachment and tend to agree with the following statements: \"I am somewhat uncomfortable getting close to others. I want emotionally close relationships, but I find it difficult to trust others completely, or to depend on them. I sometimes worry that I will be hurt if I allow myself to become too close to other people.\"\nThey tend to feel uncomfortable with emotional closeness, and the mixed feelings are combined with sometimes unconscious, negative views about themselves and their attachments. They commonly view themselves as unworthy of responsiveness from their attachments, and they don't trust the intentions of their attachments. Similar to the dismissive-avoidant attachment style, people with a fearful-avoidant attachment style seek less intimacy from attachments and frequently suppress and deny their feelings. Because of this, they are much less comfortable expressing affection.\n\nBowlby observed that children learn from their interactions with caregivers. Over the course of many interactions, children form expectations about the accessibility and helpfulness of their caregivers. These expectations reflect children's thoughts about themselves and about their caregivers:\nChildren's thoughts about their caregivers, together with thoughts about how deserving they are themselves of good care from their caregivers, form \"working models\" of attachment. Working models help guide behavior by allowing children to anticipate and plan for caregiver responses. Bowlby theorized that once formed, working models remain relatively stable. Children usually interpret experiences in the light of their working models, rather than change their working models to fit new experiences. However, when experiences cannot be interpreted in the light of their working models children may then modify their working models.\n\nWhen Hazen and Shaver extended attachment theory to romantic relationships in adults, they also included the idea of working models. Research into adult working models has focused on two issues. First, how are the thoughts that form working models organized in the mind? Second, how stable are working models across time? These questions are briefly discussed below.\n\nBartholomew and Horowitz have proposed that working models consist of two parts. One part deals with thoughts about the self. The other part deals with thoughts about others. They further propose that a person's thoughts about self are generally positive or generally negative. The same applies to a person's thoughts about others. In order to test these proposals, Bartholomew and Horowitz have looked at the relationship between attachment styles, self-esteem, and sociability. The diagram below shows the relationships they observed:\n\nThe secure and dismissive attachment styles are associated with higher self-esteem compared with the anxious and fearful attachment styles. This corresponds to the distinction between positive and negative thoughts about the self in working models. The secure and anxious attachment styles are associated with higher sociability than the dismissive or fearful attachment styles. This corresponds to the distinction between positive and negative thoughts about others in working models. These results suggested working models indeed contain two distinct domains—thoughts about self and thoughts about others—and that each domain can be characterized as generally positive or generally negative.\n\nBaldwin and colleagues have applied the theory of relational schemas to working models of attachment. Relational schemas contain information about the way the attachment figure regularly interact with each other. For each pattern of interaction that regularly occurs between partners, a relational schema is formed that contains:\n\nFor example, if a person regularly asks his or her partner for a hug or kiss, and the partner regularly responds with a hug or kiss, the person forms a relational schema representing the predictable interaction. The schema contains information about the self (e.g., \"I need lots of physical affection\"). It also contains information about the partner (e.g., \"My partner is an affectionate person\"). And it contains information about the way the interaction usually unfolds, which can be summarized by an if–then statement (e.g., \"\"If\" I ask my partner for a hug or kiss, \"then\" my partner will respond with a hug or kiss and comfort me\"). Relational schemas help guide behavior in relationships by allowing people to anticipate and plan for partner responses.\n\nBaldwin and colleagues have proposed that working models of attachment are composed of relational schemas. The fact that relational schemas contain information about the self and information about others is consistent with previous conceptions of working models. The unique contribution of relational schemas to working models is the information about the way interactions with attachments usually unfold. Relational schemas add the if–then statements about interactions to working models. To demonstrate that working models are organized as relational schemas, Baldwin and colleagues created a set of written scenarios that described interactions dealing with trust, dependency and closeness. For example, the scenarios for closeness included:\n\nFollowing each scenario, people were presented with two options about how their attachments might respond. One option was \"he/she accepts you.\" The other option was \"he/she rejects you.\" People were asked to rate the likelihood of each response on a seven-point scale. Ratings of likely attachment responses corresponded to people's attachment styles. People with secure attachment styles were more likely to expect accepting responses from their attachments. Their relational schema for the third closeness scenario would be, \"\"If\" I tell my partner how deeply I feel for him or her, \"then\" my partner will accept me.\" People with other attachment styles were less likely to expect accepting responses from their attachments. Their relational schema for the third closeness scenario would be, \"\"If\" I tell my partner how deeply I feel for him or her, \"then\" my attachment will reject me.\" Differences in attachment styles reflected differences in relational schemas. Relational schemas may therefore be used to understand the organization of working models of attachment, as has been demonstrated in subsequent studies.\n\nThe relational schemas involved in working models are likely organized into a hierarchy. According to Baldwin:\nThe highest level of the hierarchy contains very general relational schemas that apply to all relationships. The next level of the hierarchy contains relational schemas that apply to particular kinds of relationships. The lowest level of the hierarchy contains relationship schemas that apply to specific relationships.\n\nIn fact, several theorists have proposed a hierarchical organization of working models. Pietromonaco and Barrett note:\nEvery hierarchy for working models includes both general working models (higher in the hierarchy) and relationship-specific working models (lower in the hierarchy). Studies have supported the existence of both general working models and relationship-specific working models. People can report a general attachment style when asked to do so, and the majority of their relationships are consistent with their general attachment style. A general attachment style indicates a general working model that applies to many relationships. Yet, people also report different styles of attachments to their friends, parents and lovers. Relationship-specific attachment styles indicate relationship-specific working models. Evidence that general working models and relationship-specific working models are organized into a hierarchy comes from a study by Overall, Fletcher and Friesen.\n\nIn summary, the mental working models that underlie attachment styles appear to contain information about self and information about others organized into relational schemas. The relational schemas are themselves organized into a three-tier hierarchy. The highest level of the hierarchy contains relational schemas for a general working model that applies to all relationships. The middle level of the hierarchy contains relational schemas for working models that apply to different types of relationships (e.g., friends, parents, lovers). The lowest level of the hierarchy contains relational schemas for working models of specific relationships.\n\nInvestigators study the stability of working models by looking at the stability of attachment styles. Attachment styles reflect the thoughts and expectations that constitute working models. Changes in attachment styles therefore indicate changes in working models.\n\nAround 70–80% of people experience no significant changes in attachment styles over time. The fact that attachment styles do not change for a majority of people indicates working models are relatively stable. Yet, around 20–30% of people do experience changes in attachment styles. These changes can occur over periods of weeks or months. The number of people who experience changes in attachment styles, and the short periods over which the changes occur, suggest working models are not rigid personality traits.\n\nWhy attachment styles change is not well understood. Waters, Weinfield and Hamilton propose that negative life experiences often cause changes in attachment styles. Their proposal is supported by evidence that people who experience negative life events also tend to experience changes in attachment styles. Davila, Karney and Bradbury have identified four sets of factors that might cause changes in attachment styles: (a) situational events and circumstances, (b) changes in relational schemas, (c) personality variables, and (d) combinations of personality variables and situational events. They conducted a study to see which set of factors best explained changes in attachment styles. The study found that all four sets of factors cause changes in attachment styles. Changes in attachment styles are complex and depend on multiple factors.\n\nAdult relationships vary in their outcomes. The participants of some relationships express more satisfaction than the participants of other relationships. The participants of some relationships stay together longer than the partners of other relationships. Does attachment influence the satisfaction and duration of relationships?\n\nSeveral studies have linked attachment styles to relationship satisfaction. People who have secure attachment styles usually express greater satisfaction with their relationships than people who have other attachment styles.\n\nAlthough the link between attachment styles and marital satisfaction has been firmly established, the mechanisms by which attachment styles influence marital satisfaction remain poorly understood. One mechanism may be communication. Secure attachment styles may lead to more constructive communication and more intimate self-disclosures, which in turn increase relationship satisfaction. Other mechanisms by which attachment styles may influence relationship satisfaction include emotional expressiveness, strategies for coping with conflict, and perceived support from partners. Further studies are needed to better understand how attachment styles influence relationship satisfaction.\n\nSome studies suggest people with secure attachment styles have longer-lasting relationships. This may be partly due to commitment. People with secure attachment styles tend to express more commitment to their relationships. People with secure attachment styles also tend to be more satisfied with their relationships, which may encourage them to stay in their relationships longer. However, secure attachment styles are by no means a guarantee of long-lasting relationships.\n\nNor are secure attachment styles the only attachment styles associated with stable relationships. People with anxious–preoccupied attachment styles often find themselves in long-lasting, but unhappy, relationships. Anxious–preoccupied attachment styles often involve anxiety about being abandoned and doubts about one's worth as a relationship. These kinds of feelings and thoughts may lead people to stay in unhappy relationships.\n\nAttachment plays a role in the way actors interact with one another. A few examples include the role of attachment in affect regulation, support, intimacy, and jealousy. These examples are briefly discussed below. Attachment also plays a role in many interactions not discussed in this article, such as conflict, communication and sexuality.\n\nBowlby, in studies with children, observed that certain kinds of events trigger anxiety, and that people try to relieve their anxiety by seeking closeness and comfort from caregivers. Three main sets of conditions trigger anxiety in children:\n\nThe anxiety triggered by these conditions motivates the individuals to engage in behaviors that bring them physically closer to caregivers. A similar dynamic occurs in adults in relationships where others care about them. Conditions involving personal well-being, conditions involving a relationship partner, and conditions involving the environment can trigger anxiety in adults. Adults try to alleviate their anxiety by seeking physical and psychological closeness to their partners.\n\nMikulincer, Shaver and Pereg have developed a model for this dynamic. According to the model, when people experience anxiety, they try to reduce their anxiety by seeking closeness with relationship partners. However, the partners may accept or reject requests for greater closeness. This leads people to adopt different strategies for reducing anxiety. People engage in three main strategies to reduce anxiety.\n\nThe first strategy is called the \"security-based\" strategy. The diagram below shows the sequence of events in the security-based strategy.\n\nA person perceives something that provokes anxiety. The person tries to reduce the anxiety by seeking physical or psychological closeness to her or his attachment. The attachment responds positively to the request for closeness, which reaffirms a sense of security and reduces anxiety. The person returns to her or his everyday activities.\n\nThe second strategy is called the \"hyperactivation\", or anxiety attachment, strategy. The diagram below shows the sequence of events in the hyperactivation strategy.\n\nThe events begin the same way. Something provokes anxiety in a person, who then tries to reduce anxiety by seeking physical or psychological closeness to their attachment. The attachment rebuffs the request for greater closeness. The lack of responsiveness increases feelings of insecurity and anxiety. The person then gets locked into a cycle with the attachment: the person tries to get closer, the attachment rejects the request for greater closeness, which leads the person to try even harder to get closer, followed by another rejection from the attachment, and so on. The cycle ends only when the situation shifts to a security-based strategy (because the attachment finally responds positively) or when the person switches to an attachment avoidant strategy (because the person gives up on getting a positive response from the attachment).\n\nThe third strategy is called the \"attachment avoidance\" strategy. The following diagram shows the sequence of events in the attachment avoidance strategy.\n\nThe events begin the same way as the security-based strategy. A person perceives something that triggers anxiety, and the person tries to reduce anxiety by seeking physical or psychological closeness to her or his attachment. But the attachment is either unavailable or rebuffs the request for closeness. The lack of responsiveness fuels insecurity and heightens anxiety. The person gives up on getting a positive response from the attachment, suppresses her or his anxiety, and distances herself or himself from the attachment.\n\nMikulincer, Shaver, and Pereg contend these strategies of regulating attachment anxiety have very different consequences. The security-based strategy leads to more positive thoughts, such as more positive explanations of why others behave in a particular way and more positive memories about people and events. More positive thoughts can encourage more creative responses to difficult problems or distressing situations. The hyperactivation and attachment avoidance strategies lead to more negative thoughts and less creativity in handling problems and stressful situations. It is notable that the security-based strategy is contingent on a positive response from their attachment. From this perspective, it would benefit people to have attachments who are willing and able to respond positively to the person's request for closeness, so that they can use security-based strategies for dealing with their anxiety.\n\nPeople feel less anxious when close to their attachments because their attachments can provide support during difficult situations. Support includes the comfort, assistance, and information people receive from their attachments.\n\nAttachment influences both the perception of support from others and the tendency to seek support from others. People who have attachments who respond consistently and positively to requests for closeness allow individuals to have secure attachments, and in return they seek more support, in a generally relaxed way, while people whose attachments are inconsistent in reacting positively or regularly reject requests for support find they need to use other attachment styles. People with secure attachment styles may trust their attachments to provide support because their attachments have reliably offered support in the past. They may be more likely to ask for support when it's needed. People with insecure attachment styles often do not have a history of supportive responses from their attachments. They may rely less on their attachments and be less likely to ask for support when it's needed, though there may be other factors involved, as well.\n\nChanges in the way people perceive attachment tend to occur with changes in the way people perceive support. One study looked at college students' perceptions of attachment to their mothers, fathers, same-sex friends, and opposite-sex friends and found that when students reported changes in attachment for a particular relationship, they usually reported changes in support for that relationship as well. Changes in attachment for one relationship did not affect the perception of support in other relationships. The link between changes in attachment and changes in support was relationship-specific.\n\nAttachment theory has always recognized the importance of intimacy. Bowlby writes:\nThe desire for intimacy has biological roots and, in the great majority of people, persists from birth until death. The desire for intimacy also has important implications for attachment. Relationships that frequently satisfy the desire for intimacy lead to more secure attachments. Relationships that rarely satisfy the desire for intimacy lead to less secure attachments.\n\nCollins and Feeney have examined the relationship between attachment and intimacy in detail. They define intimacy as a special set of interactions in which a person discloses something important about himself or herself, and their attachment responds to the disclosure in a way that makes the person feel validated, understood, and cared for. These interactions usually involve verbal self-disclosure. However, intimate interactions can also involve non-verbal forms of self-expression such as touching, hugging, kissing, and sexual behavior. From this perspective, intimacy requires the following:\n\nCollins and Feeney review a number of studies showing how each attachment style relates to the willingness to self-disclose, the willingness to rely on partners, and the willingness to engage in physical intimacy. The secure attachment style is generally related to more self-disclosure, more reliance on partners, and more physical intimacy than other attachment styles. However, the amount of intimacy in a relationship can vary due to personality variables and situational circumstances, and so each attachment style may function to adapt an individual to the particular context of intimacy in which they live.\n\nMashek and Sherman report some findings on the desire for less closeness with partners. Sometimes too much intimacy can be suffocating. People in this situation desire less closeness with their partners. On one hand, the relationship between attachment styles and desire for less closeness is predictable. People who have fearful-avoidant and anxious-preoccupied attachment styles typically want greater closeness with their partners. People who have dismissive–avoidant attachment styles typically want less closeness with their partners. On the other hand, the relatively large numbers of people who admit to wanting less closeness with their partners (up to 57% in some studies) far outnumbers the people who have dismissive-avoidant attachment styles. This suggests people who have secure, anxious–preoccupied, or fearful-avoidant attachment styles sometimes seek less closeness with their partners. The desire for less closeness is not determined by attachment styles alone.\n\nJealousy refers to the thoughts, feelings, and behaviors that occur when a person believes a valued relationship is threatened by a rival. A jealous person experiences anxiety about maintaining support, intimacy, and other valued qualities of her or his relationship. Given that attachment relates to anxiety regulation, support, and intimacy, as discussed above, it is not surprising that attachment also relates to jealousy.\n\nBowlby observed that attachment behaviors in children can be triggered by the presence of a rival:\nWhen children see a rival contending for a caregiver's attention, the children try to get close to the caregiver and capture the caregiver's attention. Attempts to get close to the caregiver and capture the caregiver's attention indicate the attachment system has been activated. But the presence of a rival also provokes jealousy in children. The jealousy provoked by a sibling rival has been described in detail. Recent studies have shown that a rival can provoke jealousy at very young ages. The presence of a rival can provoke jealousy in infants as young as six months old. Attachment and jealousy can both be triggered in children by the presence of a rival.\n\nAttachment and jealousy can be triggered by the same perceptual cues in adults, too. The absence of the attachment can trigger both a need for close proximity and jealousy when people believe the attachment is spending time with a rival. The presence of a rival can also trigger greater need for attachment and jealousy.\n\nDifferences in attachment styles influence both the frequency and the pattern of jealous expressions. People who have anxious–preoccupied or fearful-avoidant attachment styles experience jealousy more often and view rivals as more threatening than people who have secure attachment styles. People with different attachment styles also express jealousy in different ways. One study found that:\nA subsequent study has confirmed that people with different attachment styles experience and express jealousy in qualitatively different ways. Attachment thus plays an important role in jealous interactions by influencing the frequency and the manner in which attachments express jealousy.\n\nAfter dissolution of important romantic relationships people usually go through separation anxiety and grieving. Grief is a process which leads to the acceptance of loss and usually allows the person to move on. During this process people use different strategies to cope. Securely attached individuals tend to look for support, the most effective coping strategy. Avoidantly attached individuals tend to devalue the relationships and to withdraw. Anxiously attached individuals are more likely to use emotionally focused coping strategies and pay more attention to the experienced distress (Pistole, 1996). After the end of the relationships, securely attached individuals tend to have less negative overall emotional experience than insecurely attached individuals (Pistole, 1995).\n\nRidge & Feeney (1998) have studied a group of gays and lesbians in Australian universities. Results showed that the frequency of attachment styles in the gay and lesbian population was the same as in the heterosexual; at the same time attachment styles have predicted relationship variables in a similar way as in the heterosexual population. However, gay and lesbian adult attachment styles were not related to childhood experiences with parents. Contradicting this last result, Robinson (1999) has found that in the lesbian population there was a link between attachment styles and early parenting. However, unlike in heterosexual females, attachment style was related to participants' relationship with their fathers.\n\n\n\n"}
{"id": "6937695", "url": "https://en.wikipedia.org/wiki?curid=6937695", "title": "Band emission", "text": "Band emission\n\nBand emission, is the fraction of the total emission from a blackbody that is in a certain wavelength interval or band. For a prescribed temperature, T and the spectral interval from 0 to λ, is the ratio of the total emissive power of a black body from 0 to λ to the total emissive power over the entire spectrum.\n\n"}
{"id": "54168371", "url": "https://en.wikipedia.org/wiki?curid=54168371", "title": "Basement high", "text": "Basement high\n\nIn geology, a basement high is a portion of the basement in a sedimentary basin that is higher than its surroundings. Commonly, structures referred to as basement highs are hidden by the sedimentary fill of the basin. Usually basement highs are elongated features of tectonic origin.\n"}
{"id": "30185205", "url": "https://en.wikipedia.org/wiki?curid=30185205", "title": "Bender Glacier", "text": "Bender Glacier\n\nBender Glacier () is a glacier that flows from Mount Atkinson and Mount Craddock southwards between Chaplin Peak and Krusha Peak, and joins Nimitz Glacier just south of Gilbert Spur in the southern Sentinel Range, Ellsworth Mountains in Antarctica. Receiving ice influx from its left tributaries Severinghaus Glacier, Brook Glacier and Bolgrad Glacier.\n\nIt was named by the Advisory Committee on Antarctic Names (2006) after Professor Michael L. Bender at the Department of Geosciences (Geochemistry), Princeton University (earlier at the Graduate School of Oceanography, University of Rhode Island), whose paleoclimate research from 1984 centered on the glacial-interglacial climate change and the global carbon cycle.\n\n\n\n\n"}
{"id": "52048718", "url": "https://en.wikipedia.org/wiki?curid=52048718", "title": "Bibliometrix", "text": "Bibliometrix\n\nBibliometrix package is a R tool (http://www.bibliometrix.org) providing a set of tools for quantitative research in scientometrics and bibliometrics.\n\nBibliometrics is the application of quantitative analysis and statistics to publications such as journal articles and their accompanying citation counts. Quantitative evaluation of publication and citation data is now used in almost all science fields to evaluate growth, maturity, leading authors, conceptual and intellectual maps, trend of a scientific community. Bibliometrics is also used in research performance evaluation, especially in university and government labs, and also by policymakers, research directors and administrators, information specialists and librarians, and scholars themselves.\n\nBibliometrix is written in R-language. R is an open-source environment and ecosystem. The existence of substantial of good statistical algorithms, access to high-quality numerical routines, and integrated data visualization tools are perhaps the strongest qualities to prefer R to other languages for scientific computation.\n\nBibliometrix supports scholars in three key phases of analysis:\n\nThe following table lists the main functions of bibliometrix package:\n"}
{"id": "37720522", "url": "https://en.wikipedia.org/wiki?curid=37720522", "title": "Binning (metagenomics)", "text": "Binning (metagenomics)\n\nIn metagenomics, binning is the process of grouping reads or contigs and assigning them to operational taxonomic units. Binning methods can be based on either compositional features or alignment (similarity), or both.\n\nMetagenomic samples can contain reads from a huge number of organisms. For example, in a single gram of soil, there can be up to 18000 different types of organisms, each with its own genome. Metagenomic studies sample DNA from the whole community, and make it available as nucleotide sequences of certain length. In most cases, the incomplete nature of the obtained sequences makes it hard to assemble individual genes, much less recovering the full genomes of each organism. Thus, binning techniques represent a \"best effort\" to identify reads or contigs with certain groups of organisms designated as operational taxonomic units (OTUs).\n\nThe first studies that sampled DNA from multiple organisms used specific genes to assess diversity and origin of each sample. These marker genes had been previously sequenced from clonal cultures from known organisms, so, whenever one of such genes appeared in a read or contig from the metagenomic sample that read could be assigned to a known species or to the OTU of that species. The problem with this method was that only a tiny fraction of the sequences carried a marker gene, leaving most of the data unassigned.\n\nModern binning techniques use both previously available information independent from the sample and intrinsic information present in the sample. Depending on the diversity and complexity of the sample, their degree of success vary: in some cases they can resolve the sequences up to individual species, while in some others the sequences are identified at best with very broad taxonomic groups.\n\nBinning algorithms can employ previous information, and thus act as supervised classifiers, or they can try to find new groups, those act as unsupervised classifiers. Many, of course, do both. The classifiers exploit the previously known sequences by performing alignments against databases, and try to separate sequence based in organism-specific characteristics of the DNA, like GC-content.\n\nMande et al., (2012) provides a review of the premise, methodologies, advantages, limitations and challenges of various methods available for binning of metagenomic datasets obtained using the shotgun sequencing approach. Some of the prominent binning algorithms are described below.\n\nTETRA is a statistical classifier that uses tetranucleotide usage patterns in genomic fragments. There are four possible nucleotides in DNA, therefore there can be formula_1 different fragments of four consecutive nucleotides; these fragments are called tetramers. TETRA works by tabulating the frequencies of each tetramer for a given sequence. From these frequencies z-scores are then calculated, which indicate how over- or under-represented the tetramer is in contraposition with what would be expected by looking to individual nucleotide compositions. The z-scores for each tetramer are assembled in a vector, and the vectors corresponding to different sequences are compared pair-wise, to yield a measure of how similar different sequences from the sample are. It is expected that the most similar sequences belong to organisms in the same OTU.\n\nIn the DIAMOND+MEGAN approach, all reads are first aligned against a protein reference database, such as NCBI-nr, and then the resulting alignments are analyzed using the naive LCA algorithm, which places a read on the lowest taxonomic node in the NCBI taxonomy that lies above all taxa to which the read has a significant alignment. Here, an alignment is usually deemed \"significant\", if its bit score lies above a given threshold (which depends on the length of the reads) and is within 10%, say, of the best score seen for that read. The rationale of using protein reference sequences, rather than DNA reference sequences, is that current DNA reference databases only cover a small fraction of the true diversity of genomes that exist in the environment. \n\nPhylopythia is one supervised classifier developed by researchers at IBM labs, and is basically a support vector machine trained with DNA-kmers from known sequences.\n\nSOrt-ITEMS (Monzoorul et al., 2009) is an alignment-based binning algorithm developed by Innovations Labs of Tata Consultancy Services (TCS) Ltd., India. Users need to perform a similarity search of the input metagenomic sequences (reads) against the nr protein database using BLASTx search. The generated blastx output is then taken as input by the SOrt-ITEMS program. The method uses a range of BLAST alignment parameter thresholds to first identify an appropriate taxonomic level (or rank) where the read can be assigned. An orthology-based approach is then adopted for the final assignment of the metagenomic read. Other alignment-based binning algorithms developed by the Innovation Labs of Tata Consultancy Services (TCS) include DiScRIBinATE, ProViDE and SPHINX. The methodologies of these algorithms are summarized below.\n\nDiScRIBinATE (Ghosh et al., 2010) is an alignment-based binning algorithms developed by the Innovations Labs of Tata Consultancy Services (TCS) Ltd., India. DiScRIBinATE replaces the orthology approach of SOrt-ITEMS with a quicker 'alignment-free' approach. Incorporating this alternate strategy was observed to reduce the binning time by half without any significant loss in the accuracy and specificity of assignments. Besides, a novel reclassification strategy incorporated in DiScRIBinATE was seem to reduce the overall misclassification rate.\n\nProViDE (Ghosh et al., 2011) is an alignment-based binning approach developed by the Innovation Labs of Tata Consultancy Services (TCS) Ltd. for the estimation of viral diversity in metagenomic samples. ProViDE adopts the reverse orthlogy based approach similar to SOrt-ITEMS for the taxonomic classification of metagenomic sequences obtained from virome datasets. It a customized set of BLAST parameter thresholds, specifically suited for viral metagenomic sequences. These thresholds capture the pattern of sequence divergence and the non-uniform taxonomic hierarchy observed within/across various taxonomic groups of the viral kingdom.\n\nPCAHIER (Zheng et al., 2010), another binning algorithm developed by the Georgia Institute of Technology., employs n-mer oligonucleotide frequencies as the features and adopts a hierarchical classifier (PCAHIER) for binning short metagenomic fragments. The principal component analysis was used to reduce the high dimensionality of the feature space. The effectiveness of the PCAHIER was demonstrated through comparisons against a non-hierarchical classifier, and two existing binning algorithms (TETRA and Phylopythia).\n\nSPHINX (Mohammed et al., 2011), another binning algorithm developed by the Innovation Labs of Tata Consultancy Services (TCS) Ltd., adopts a hybrid strategy that achieves high binning efficiency by utilizing the principles of both 'composition'- and 'alignment'-based binning algorithms. The approach was designed with the objective of analyzing metagenomic datasets as rapidly as composition-based approaches, but nevertheless with the accuracy and specificity of alignment-based algorithms. SPHINX was observed to classify metagenomic sequences as rapidly as composition-based algorithms. In addition, the binning efficiency (in terms of accuracy and specificity of assignments) of SPHINX was observed to be comparable with results obtained using alignment-based algorithms.\n\nRepresent other composition-based binning algorithms developed by the Innovation Labs of Tata Consultancy Services (TCS) Ltd. These algorithms utilize a range of oligonucleotide compositional (as well as statistical) parameters to improve binning time while maintaining the accuracy and specificity of taxonomic assignments.\n\nThis list is not exhaustive:\n\n\nAll these algorithms employ different schemes for binning sequences, such as hierarchical classification, and operate in either a supervised or unsupervised manner. These algorithms provide a global view of how diverse the samples are, and can potentially connect community composition and function in metagenomes.\n\n"}
{"id": "30154962", "url": "https://en.wikipedia.org/wiki?curid=30154962", "title": "Bootstrap current", "text": "Bootstrap current\n\nIn a toroidal fusion power device, a plasma is confined within a donut-shaped cylinder. If the gas pressure of the plasma varies across the radius of the cylinder, a self-generated current will spontaneously arise within the plasma, due to collisions between trapped particles and passing particles. This current is called the bootstrap current, and is commonly found in tokamak fusion devices. The tokamak uses a combination of external magnets and a current driven in the plasma to create a stable confinement system. One goal of advanced tokamak designs is to maximize the bootstrap current, and thereby reduce or eliminate the need for an external current driver. This could dramatically reduce the cost and complexity of the device.\n\nFrom an engineering point of view the bootstrap current is the effect of trapped particles (which practically lie on poloidal plane) and density gradient: the poloidal current due to trapped particles motion is unbalanced since the density is not constant, therefore the bootstrap current is needed to \"close the circuit\".\n\n"}
{"id": "50899955", "url": "https://en.wikipedia.org/wiki?curid=50899955", "title": "Charles Whitwell", "text": "Charles Whitwell\n\nCharles Whitwell (ca. 1568 1611) was an English scientific instrument maker.\n\nAn English engraver and maker of mathematical instruments in the tradition of Humphrey Cole (c. 1530 1591) and active between 1591 and 1606, Whitwell engraved maps of English counties and built many instruments that were invented by Robert Dudley (1573 1649). These instruments, brought to Florence by Robert Dudley himself, were bequeathed to Ferdinand II de' Medici (1610 1670) and are now in the possession of the Museo Galileo of Florence.\n"}
{"id": "59227592", "url": "https://en.wikipedia.org/wiki?curid=59227592", "title": "Clinica 0-19", "text": "Clinica 0-19\n\nClinica 0-19 is a medical clinic in Monterrey, Mexico that has garnered media attention for its treatment of Diffuse Intrinsic Pontine Glioma (DIPG). While the clinic states that their treatment has resulted in patients no longer having any evidence of disease, they have refused to publish their treatment protocol or survival rates. Some patients declared to have no evidence of the disease by the clinic's doctors were found to have tumor growth a few months later. Other oncologists have criticized the clinic's lead doctors, Alberto Siller and Alberto Garcia, for their lack of transparency.\n\nClinica 0-19 treats DIPG with intra-arterial chemotherapy and immunotherapy. While consultation and follow-up appointments take place at Clinica 0-19, treatments are carried out at nearby hospitals, especially Hospital Angeles.\n\nFor the intra-arterial chemotherapy, a catheter is inserted into the femoral artery, guided to the patient's brainstem, and used to deliver chemotherapy drugs. The clinic has stated the drugs they use are FDA approved. Their website does not provide any examples of the kinds of drugs used. Drugs they have employed in the past have included unusually low doses of cisplatin, cetuximab, doxorubicin, and Avastin. None of these drugs have been shown to be effective against DIPG, and Avastin has not approved by the FDA for use in children. According to the parents of one patient interviewed in 2018, the doctors administer \"10 to 12 different types of medicines.\" Treatment is provided once every three weeks in some cases and once every six weeks in other cases for a total of ten or more times. \n\nThe immunotherapy provided by the clinic uses dendritic cells. As of July 2018, very little had been published on the use of dendritic cells to treat DIPG. \n\nSiller and Garcia have not published their protocol for treatment. They characterize their approach as tailoring the treatment to each child. Surgical oncologist and outspoken skeptic, David Gorski has criticized this approach, stating \"In other words, we make it up as we go along and can’t tell you how we do it in a sufficiently clear manner to make it possible to write a protocol to do it in a clinical trial.\" \n\nThe clinic has also been criticized for discouraging their patients from using radiation therapy. \n\nNo studies have been conducted on Clinica 0-19's approach to treating DIPG, and the clinic has not provided information on their patients' survival or recurrence rates.\n\nThe clinic has released scans appearing to show that patients' tumors have shrunk following treatment at the clinic. Dr. David Ziegler, an Australian pediatric oncologist, has stated the scans \"do do not accurately depict the tumor.\" Gorski has noted the apparent shrinking could be the abatement of inflammation around the tumor caused by previous treatment.\n\nMedia coverage of the clinic has highlighted cases of patients who, following treatment by Siller and Garcia, lived longer than was initially expected. Gorski has noted that children receiving care at the clinic may have been likely to live longer than average DIPG patients, regardless of treatment by Clinica 0-19, because the clinic only treats patients who can travel to the clinic, forgo treatment with steroids for several months prior to treatment, and who can afford the hundreds of thousands of dollars charged by the clinic for treatment. He notes that patients who are able to travel abroad and thrive without steroids are likely in better health than the average DIPG patient, and that families that can access large amounts of money for treatment may not be typical of DIPG patients in other ways that could impact their survival.\n\nThe cost of treatment for DIPG through Clinica 0-19 is about $30,000 per treatment, with total costs exceeding $300,000 in some reported cases.\n\nZiegler and Gorski have criticized Siller and Garcia for not publishing their results or treatment protocol.\n\nAustralia's Cure Brain Cancer Foundation (CBCF) has sought to visit Clinica 0-19 to research the treatment it provides. CBCF has also offered to assist the clinic in reporting their findings. A spokesperson for the clinic stated the doctors would need to develop a program to receive visiting doctors before they could work with the CBCF. As of July 2018, there was no evidence that such visits have been planned.\n\nIn February 2018, Siller stated the clinic had \"started to do some basic numbers\" with the intention of publishing their results \"in the next coming months.\"\n\nSeveral Clinica 0-19 patients have been the subject of media coverage.\n\n\n\n\n\n\n\n\n"}
{"id": "1530689", "url": "https://en.wikipedia.org/wiki?curid=1530689", "title": "Complementarity (physics)", "text": "Complementarity (physics)\n\nIn physics, complementarity is both a theoretical and an experimental result of quantum mechanics, also referred to as principle of complementarity. It holds that objects have certain pairs of complementary properties which cannot all be observed or measured simultaneously.\n\nThe complementarity principle was formulated by Niels Bohr, a leading founder of quantum mechanics. Examples of complementary properties that Bohr considered:\n\n\nFor example, the particle and wave aspects of physical objects are such complementary phenomena. Both concepts are borrowed from classical mechanics, where it is impossible to be a particle and wave at the same time. Therefore, it is impossible to measure the \"full\" properties of the wave and particle at a particular moment. Moreover, Bohr implies that it is not possible to regard objects governed by quantum mechanics as having intrinsic properties independent of determination with a measuring device, a viewpoint supported by the Kochen-Specker theorem. The type of measurement determines which property is shown. However the single and double-slit experiment and other experiments show that \"some\" effects of wave and particle can be measured in one measurement.\n\nAn aspect of complementarity is that it not only applies to measurability or knowability of some property of a physical entity, but more importantly it applies to the limitations of that physical entity’s very manifestation of the property in the physical world. All properties of physical entities exist only in pairs, which Bohr described as complementary or conjugate pairs. Physical reality is determined and defined by manifestations of properties which are limited by trade-offs between these complementary pairs. For example, an electron can manifest a greater and greater accuracy of its position only in even trade for a complementary loss in accuracy of manifesting its momentum. This means that there is a limitation on the precision with which an electron can possess (i.e., manifest) position, since an infinitely precise position would dictate that its manifested momentum would be infinitely imprecise, or undefined (i.e., non-manifest or not possessed), which is not possible. The ultimate limitations in precision of property manifestations are quantified by the Heisenberg uncertainty principle and Planck units. Complementarity and Uncertainty dictate that therefore all properties and actions in the physical world manifest themselves as non-deterministic to some degree.\n\nPhysicists F.A.M. Frescura and Basil Hiley have summarized the reasons for the introduction of the principle of complementarity in physics as follows:\nThe emergence of complementarity in a system occurs when one considers the circumstances under which one attempts to measure its properties; as Bohr noted, the principle of complementarity \"implies the impossibility of any sharp separation between the behaviour of atomic objects and the interaction with the measuring instruments that serve to define the conditions under which the phenomena appear.\" It is important to distinguish, as did Bohr in his original statements, the principle of complementarity from a statement of the uncertainty principle. For a technical discussion of contemporary issues surrounding complementarity in physics see, e.g., Bandyopadhyay (2000), from which parts of this discussion were drawn.\n\nIn his original lecture on the topic, Bohr pointed out that just as the finitude of the speed of light implies the impossibility of a sharp separation between space and time (relativity), the finitude of the quantum of action implies the impossibility of a sharp separation between the behavior of a system and its interaction with the measuring instruments and leads to the well known difficulties with the concept of 'state' in quantum theory; the notion of complementarity is intended to symbolize this new situation in epistemology created by quantum theory. Some people consider it a philosophical adjunct to quantum mechanics, while others consider it to be a discovery that is as important as the formal aspects of quantum theory. Examples of the latter include Leon Rosenfeld, who claimed that \"[C]omplementarity is not a philosophical superstructure invented by Bohr to be placed as a decoration on top of the quantal formalism, it is the bedrock of the quantal description.\", and John Wheeler, who opined that \"Bohr's principle of complementarity is the most revolutionary scientific concept of this century and the heart of his fifty-year search for the full significance of the quantum idea.\"\n\nThe quintessential example of wave–particle complementarity in the laboratory is the double slit. The crux of the complementary behavior is the question: \"What information exists – embedded in the constituents of the universe – that can reveal the history of the signal particles as they pass through the double slit?\" If information exists (even if it is not measured by a conscious observer) that reveals \"which slit\" each particle traversed, then each particle will exhibit no wave interference with the other slit. This is the particle-like behavior. But if \"no information\" exists about which slit – so that no conscious observer, no matter how well equipped, will ever be able to determine which slit each particle traverses – then the signal particles will interfere with themselves as if they traveled through both slits at the same time, as a wave. This is the wave-like behavior. These behaviors are complementary, according to the Englert–Greenberger duality relation, because when one behavior is observed the other is absent. Both behaviors \"can\" be observed at the same time, but each only as lesser manifestations of their full behavior (as determined by the duality relation). This superposition of complementary behaviors exists whenever there is partial \"which slit\" information. While there is some contention to the duality relation, and thus complementarity itself, the contrary position is not accepted by mainstream physics. Double slit experiments with single photons show clearly that photons are particles at the same time as they are waves. Photons impact the screen where they are detected in points, and when enough points have accumulated the wave aspect is clearly visible. Also the particle and wave aspect is seen at the same time in photons that are stationary.\n\nVarious neutron interferometry experiments demonstrate the subtlety of the notions of duality and complementarity. By passing through the interferometer, the neutron appears to act as a wave. Yet upon passage, the neutron is subject to gravitation. As the neutron interferometer is rotated through Earth's gravitational field a phase change between the two arms of the interferometer can be observed, accompanied by a change in the constructive and destructive interference of the neutron waves on exit from the interferometer. Some interpretations claim that understanding the interference effect requires one to concede that a single neutron takes both paths through the interferometer at the same time; a single neutron would \"be in two places at once\", as it were. Since the two paths through a neutron interferometer can be as far as to apart, the effect is hardly microscopic. This is similar to traditional double-slit and mirror interferometer experiments where the slits (or mirrors) can be arbitrarily far apart. So, in interference and diffraction experiments, neutrons behave the same way as photons (or electrons) of corresponding wavelength.\n\nNiels Bohr apparently conceived of the principle of complementarity during a skiing vacation in Norway in February and March 1927, during which he received a letter from Werner Heisenberg regarding the latter's newly discovered (and not yet published) uncertainty principle. Upon returning from his vacation, by which time Heisenberg had already submitted his paper on the uncertainty principle for publication, he convinced Heisenberg that the uncertainty principle was a manifestation of the deeper concept of complementarity. Heisenberg duly appended a note to this effect to his paper on the uncertainty principle, before its publication, stating:\n\nBohr has brought to my attention [that] the uncertainty in our observation does not arise exclusively from the occurrence of discontinuities, but is tied directly to the demand that we ascribe equal validity to the quite different experiments which show up in the [particulate] theory on one hand, and in the wave theory on the other hand.\n\nBohr publicly introduced the principle of complementarity in a lecture he delivered on 16 September 1927 at the International Physics Congress held in Como, Italy, attended by most of the leading physicists of the era, with the notable exceptions of Einstein, Schrödinger, and Dirac. However, these three were in attendance one month later when Bohr again presented the principle at the Fifth Solvay Congress in Brussels, Belgium. The lecture was published in the proceedings of both of these conferences, and was republished the following year in \"Naturwissenschaften\" (in German) and in \"Nature\" (in English).\n\nAn article written by Bohr in 1949 titled \"Discussions with Einstein on Epistemological Problems in Atomic Physics\" is considered by many to be a definitive description of the notion of complementarity.\n\n\n\n"}
{"id": "19172369", "url": "https://en.wikipedia.org/wiki?curid=19172369", "title": "Criticism of non-standard analysis", "text": "Criticism of non-standard analysis\n\nNon-standard analysis and its offshoot, non-standard calculus, have been criticized by several authors, notably Errett Bishop, Paul Halmos, and Alain Connes. These criticisms are analyzed below.\n\nThe evaluation of non-standard analysis in the literature has varied greatly. Paul Halmos described it as a technical special development in mathematical logic. Terence Tao summed up the advantage of the hyperreal framework by noting that it\n\nThe nature of the criticisms is not directly related to the logical status of the results proved using non-standard analysis. In terms of conventional mathematical foundations in classical logic, such results are quite acceptable. Abraham Robinson's non-standard analysis does not need any axioms beyond Zermelo–Fraenkel set theory (ZFC) (as shown explicitly by Wilhelmus Luxemburg's ultrapower construction of the hyperreals), while its variant by Edward Nelson, known as IST, is similarly a conservative extension of ZFC. It provides an assurance that the newness of non-standard analysis is entirely as a strategy of proof, not in range of results. Further, model theoretic non-standard analysis, for example based on superstructures, which is now a commonly used approach, does not need any new set-theoretic axioms beyond those of ZFC.\n\nControversy has existed on issues of mathematical pedagogy. Also non-standard analysis as developed is not the only candidate to fulfill the aims of a theory of infinitesimals (see Smooth infinitesimal analysis). Philip J. Davis wrote, in a book review of \"Left Back: A Century of Failed School Reforms\" by Diane Ravitch:\n\nNon-standard calculus in the classroom has been analysed in the study by K. Sullivan of schools in the Chicago area, as reflected in secondary literature at Influence of non-standard analysis. Sullivan showed that students following the NSA course were better able to interpret the sense of the mathematical formalism of calculus than a control group following a standard syllabus. This was also noted by Artigue (1994), page 172; Chihara (2007); and Dauben (1988).\n\nIn the view of Errett Bishop, classical mathematics, which includes Robinson's approach to nonstandard analysis, was non-constructive and therefore deficient in numerical meaning . Bishop was particularly concerned about the use of non-standard analysis in teaching as he discussed in his essay \"Crisis in mathematics\" . Specifically, after discussing Hilbert's formalist program he wrote:\n\nKatz & Katz (2010) note that a number of criticisms were voiced by the participating mathematicians and historians following Bishop's \"Crisis\" talk, at the AAAS workshop in 1974. However, not a word was said by the participants about Bishop's \"debasement\" of Robinson's theory. Katz & Katz point out that it recently came to light that Bishop in fact said not a word about Robinson's theory at the workshop, and only added his \"debasement\" remark at the galley proof stage of publication. This helps explain the absence of critical reactions at the workshop. Katz & Katz conclude that this raises issues of integrity on the part of Bishop whose published text does not report the fact that the \"debasement\" comment was added at galley stage and therefore was not heard by the workshop participants, creating a spurious impression that they did not disagree with the comments.\n\nThe fact that Bishop viewed the introduction of non-standard analysis in the classroom as a \"debasement of meaning\" was noted by J. Dauben. The term was clarified by Bishop (1985, p. 1) in his text \"Schizophrenia in contemporary mathematics\" (first distributed in 1973), as follows:\n\nThus, Bishop first applied the term \"debasement of meaning\" to classical mathematics as a whole, and later applied it to Robinson's infinitesimals in the classroom. In his \"Foundations of Constructive Analysis\" (1967, page ix), Bishop wrote:\n\nBishop's remarks are supported by the discussion following His lecture: \n\nBishop reviewed the book \"\" by Keisler, which presented elementary calculus using the methods of nonstandard analysis. Bishop was chosen by his advisor Paul Halmos to review the book. The review appeared in the \"Bulletin of the American Mathematical Society\" in 1977. This article is referred to by David O. Tall while discussing the use of non-standard analysis in education. Tall wrote:\n\nBishop's review supplied several quotations from Keisler's book, such as:\n\nand\n\nThe review criticized Keisler's text for not providing evidence to support these statements, and for adopting an axiomatic approach when it was not clear to the students there was any system that satisfied the axioms . The review ended as follows:\nThe technical complications introduced by Keisler's approach are of minor\nimportance. The real damage lies in [Keisler's] obfuscation and devitalization of those\nwonderful ideas [of standard calculus]. No invocation of Newton and Leibniz is going to justify\ndeveloping calculus using axioms V* and VI*-on the grounds that the usual\ndefinition of a limit is too complicated!\n\nand\n\nAlthough it seems to be futile, I always tell my calculus students that mathematics is not esoteric: It is common sense. (Even the notorious (ε, δ)-definition of limit is common sense, and moreover it is central to the important practical problems of approximation and estimation.) They do not believe me. In fact the idea makes them uncomfortable because it contradicts their previous experience. Now we have a calculus text that can be used to confirm their experience of mathematics as an esoteric and meaningless exercise in technique.\n\nIn his response in \"The\" \"Notices\", Keisler (1977, p. 269) asked:\n\nComparing the use of the law of excluded middle (rejected by constructivists) to wine, Keisler likened Halmos' choice with \"choosing a teetotaller to sample wine\".\n\nBishop's book review was subsequently criticized in the same journal by Martin Davis, who wrote on p. 1008 of :\n\nDavis added (p. 1008) that Bishop stated his objections\n\nPhysicist Vadim Komkov (1977, p. 270) wrote:\n\nWhether or not non-standard analysis can be done constructively, Komkov perceived a foundational concern on Bishop's part.\n\nPhilosopher of Mathematics Geoffrey Hellman (1993, p. 222) wrote:\n\nHistorian of Mathematics Joseph Dauben analyzed Bishop's criticism in (1988, p. 192). After evoking the \"success\" of nonstandard analysis\n\nDauben stated:\n\nDauben mentioned \"impressive\" applications in\n\nAt this \"deeper\" level of meaning, Dauben concluded,\n\nA number of authors have commented on the tone of Bishop's book review. Artigue (1992) described it as \"virulent\"; Dauben (1996), as \"vitriolic\"; Davis and Hauser (1978), as \"hostile\"; Tall (2001), as \"extreme\".\n\nIan Stewart (1986) compared Halmos' asking Bishop to review Keisler's book, to inviting Margaret Thatcher to review \"Das Kapital\".\n\nKatz & Katz (2010) point out that\n\nThey further note that\n\nG. Stolzenberg responded to Keisler's \"Notices\" criticisms of Bishop's review in a letter, also published in \"The Notices.\" Stolzenberg argues that the criticism of Bishop's review of Keisler's calculus book is based on the false assumption that they were made in a constructivist mindset whereas Stolzenberg believes that Bishop read it as it was meant to be read: in a classical mindset.\n\nIn \"Brisure de symétrie spontanée et géométrie du point de vue spectral\", Journal of Geometry and Physics 23 (1997), 206–234, Alain Connes wrote:\n\nIn his 1995 article \"Noncommutative geometry and reality\" Connes develops a calculus of infinitesimals based on operators in Hilbert space. He proceeds to \"explain why the formalism of nonstandard analysis is inadequate\" for his purposes. Connes points out the following three aspects of Robinson's hyperreals:\n\n(1) a nonstandard hyperreal \"cannot be exhibited\" (the reason given being its relation to non-measurable sets);\n\n(2) \"the practical use of such a notion is limited to computations in which the final result is independent of the exact value of the above infinitesimal. This is the way nonstandard analysis and ultraproducts are used [...]\".\n\n(3) the hyperreals are commutative.\n\nKatz & Katz analyze Connes' criticisms of non-standard analysis, and challenge the specific claims (1) and (2). With regard to (1), Connes' own infinitesimals similarly rely on non-constructive foundational material, such as the existence of a Dixmier trace. With regard to (2), Connes presents the independence of the choice of infinitesimal as a \"feature\" of his own theory.\n\nKanovei et al. (2012) analyze Connes' contention that non-standard numbers are \"chimerical\". They note that the content of his criticism is that \"ultrafilters\" are \"chimerical\", and point out that Connes exploited ultrafilters in an essential manner in his earlier work in functional analysis. They analyze Connes' claim that the hyperreal theory is merely \"virtual\". Connes' references to the work of Robert Solovay suggest that Connes means to criticize the hyperreals for allegedly not being definable. If so, Connes' claim concerning the hyperreals is demonstrably incorrect, given the existence of a definable model of the hyperreals constructed by Vladimir Kanovei and Saharon Shelah (2004). Kanovei et al. (2012) also provide a chronological table of increasingly vitriolic epithets employed by Connes to denigrate NSA over the period between 1995 and 2007, starting with \"inadequate\" and \"disappointing\" and culminating with \"the end of the road for being 'explicit'\".\n\nKatz & Leichtnam (2013) note that \"two-thirds of Connes' critique of Robinson's infinitesimal approach can be said to be incoherent, in the specific sense of not being coherent with what Connes writes (approvingly) about his own infinitesimal approach.\"\n\nPaul Halmos writes in \"Invariant subspaces\", \"American Mathematical Monthly\" 85 (1978) 182–183 as follows:\n\nHalmos writes in (Halmos 1985) as follows (p. 204):\n\nWhile commenting on the \"role of non-standard analysis in mathematics\", Halmos writes (p. 204):\n\nHalmos concludes his discussion of non-standard analysis as follows (p. 204):\n\nKatz & Katz (2010) note that\n\nLeibniz historian Henk Bos (1974) acknowledged that Robinson's hyperreals provide\n\nF. Medvedev (1998) further points out that\n\n\n\n"}
{"id": "2182514", "url": "https://en.wikipedia.org/wiki?curid=2182514", "title": "Dialogical self", "text": "Dialogical self\n\nThe dialogical self is a psychological concept which describes the mind's ability to imagine the different positions of participants in an internal dialogue, in close connection with external dialogue. The \"dialogical self\" is the central concept in the dialogical self theory (DST), as created and developed by the Dutch psychologist Hubert Hermans since the 1990s.\n\nDialogical Self Theory (DST) weaves two concepts, self and dialogue, together in such a way that a more profound understanding of the interconnection of self and society is achieved. Usually, the concept of self refers to something \"internal,\" something that takes place within the mind of the individual person, while dialogue is typically associated with something \"external,\" that is, processes that take place between people involved in communication.\n\nThe composite concept \"dialogical self\" goes beyond the self-other dichotomy by infusing the external to the internal and, in reverse, to introduce the internal into the external. As functioning as a \"society of mind\", the self is populated by a multiplicity of \"self-positions\" that have the possibility to entertain dialogical relationships with each other.\n\nIn Dialogical Self Theory (DST) the self is considered as \"extended,\" that is, individuals and groups in the society at large are incorporated as positions in the mini-society of the self. As a result of this extension, the self does not only include internal positions (e.g., I as the son of my mother, I as a teacher, I as a lover of jazz), but also external positions (e.g., my father, my pupils, the groups to which I belong).\n\nGiven the basic assumption of the extended self, the other is not simply outside the self but rather an intrinsic part of it. There is not only the actual other outside the self, but also the imagined other who is entrenched as the other-in-the-self. An important theoretical implication is that basic processes, like self-conflicts, self-criticism, self-agreements, and self-consultancy, are taking place in different domains in the self: within the internal domain (e.g., \"As an enjoyer of life I disagree with myself as an ambitious worker\"); between the internal and external (extended) domain (e.g., \"I want to do this but the voice of my mother in myself criticizes me\") and within the external domain (e.g., \"The way my colleagues interact with each other has led me to decide for another job\").\n\nAs these examples show, there is not always a sharp separation between the inside of the self and the outside world, but rather a gradual transition. DST assumes that the self as a society of mind is populated by internal and external self-positions. When some positions in the self silence or suppress other positions, monological relationships prevail. When, in contrast, positions are recognized and accepted in their differences and alterity (both within and between the internal and external domains of the self), dialogical relationships emerge with the possibility to further develop and renew the self and the other as central parts of the society at large.\n\nDST is inspired by two thinkers in particular, William James and Mikhail Bakhtin, who worked in different countries (USA and Russia, respectively), in different disciplines (psychology and literary sciences), and in different theoretical traditions (pragmatism and dialogism). As the composite term dialogical self suggests, the present theory finds itself not exclusively in one of these traditions but explicitly at their intersection. As a theory about the self it is inspired by William James, as a theory about dialogue it elaborates on some insights of Mikhail Bakhtin. The purpose of the present theory is to profit from the insights of founding fathers like William James, George Herbert Mead and Mikhail Bakhtin and, at the same time, to go beyond them.\nWilliam James (1890) proposed a distinction between the \"I\" and the \"Me\", which, according to Morris Rosenberg, is a classic distinction in the psychology of the self. According to James the I is equated with the self-as-knower and has three features: continuity, distinctness, and volition. The continuity of the self-as-knower is expressed in a sense of personal identity, that is, a sense of sameness through time. A feeling of distinctness from others, or individuality, is also characteristic of the self-as-knower. Finally, a sense of personal volition is reflected in the continuous appropriation and rejection of thoughts by which the self-as-knower manifests itself as an active processor of experience.\n\nOf particular relevance to DST is James's view that the \"Me\", equated with the self-as-known, is composed of the empirical elements considered as belonging to oneself. James was aware that there is a gradual transition between \"Me\" and \"mine\" and concluded that the empirical self is composed of all that the person can call his or her own, \"not only his body and his psychic powers, but his clothes and his house, his wife and children, his ancestors and friends, his reputation and works, his lands and horses, and yacht and bank-account\". According to this view, people and things in the environment belong to the self, as far as they are felt as \"mine\". This means that not only \"my mother\" belongs to the self but even \"my enemy\". In this way, James proposed a view in which the self is 'extended' to the environment. This proposal contrasts with a Cartesian view of the self which is based on a dualistic conception, not only between self and body but also between self and other. With his conception of the extended self, that defined as going beyond the skin, James has paved the way for later theoretical developments in which other people and groups, defined as \"mine\" are part of a dynamic multi-voiced self.\nIn the above quotation from William James, we see a constellation of characters (or self-positions) which he sees as belonging to the \"Me/mine\": my wife and children, my ancestors and friends. Such characters are more explicitly elaborated in Mikhail Bakhtin's metaphor of the polyphonic novel, which became a source of inspiration for later dialogical approaches to the self. In proposing this metaphor, he draws on the idea that in Dostoevsky's works there is not a single author at work—Dostoevsky himself—but several authors or thinkers, portrayed as characters such as Ivan Karamazov, Myshkin, Raskolnikov, Stavrogin, and the Grand Inquisitor.\n\nThese characters are not presented as obedient slaves in the service of one author-thinker, Dostoevsky, but treated as independent thinkers, each with their own view of the world. Each hero is put forward as the author of his own ideology, and not as the object of Dostoevsky's finalizing artistic vision. Rather than a multiplicity of characters within a unified world, there is a plurality of consciousnesses located in different worlds. As in a polyphonic musical composition, multiple voices accompany and oppose one another in dialogical ways. In bringing together different characters in a polyphonic construction, Dostoevsky creates a multiplicity of perspectives, portraying characters conversing with the Devil (Ivan and the Devil), with their alter egos (Ivan and Smerdyakov), and even with caricatures of themselves (Raskolnikov and Svidrigailov).\n\nInspired by the original ideas of William James and Mikhail Bakhtin, Hubert Hermans, Harry Kempen and Rens van Loon wrote the first psychological publication on the \"dialogical self\" in which they conceptualized the self in terms of a dynamic multiplicity of relatively autonomous \"I\"-positions in the (extended) landscape of the mind. In this conception, the \"I\" has the possibility to move from one spatial position to another in accordance with changes in situation and time. The \"I\" fluctuates among different and even opposed positions, and has the capacity to imaginatively endow each position with a voice so that dialogical relations between positions can be established. The voices function like interacting characters in a story, involved in processes of question and answer, agreement and disagreement. Each of them have a story to tell about their own experiences from their own stance. As different voices, these characters exchange information about their respective Me's and mines, resulting in a complex, narratively structured self.\n\nThe theory has led to the construction of different assessment and research procedures for investigating central aspects of the dialogical self. Hubert Hermans has constructed the \"Personal Position Repertoire\" (PPR) method, an idiographic procedure for assessing the internal and external domains of the self in terms of an organized position repertoire.\n\nThis is done by offering the participant a list of internal and external self-positions. The participants mark those positions that they feel as relevant in their lives. They are allowed to add extra internal and external positions to the list and phrase them in their own terms. The relationship between internal and external positions is then established by inviting the participants to fill out a matrix with the rows representing the internal positions and the columns the external positions. In the entries of the matrix, the participant fills in, on a scale from 0 to 5 the extent to which an internal position is prominent in the relation to an external position. The scores in the matrix allow for the calculation of a number of indices, such as sum scores representing the overall prominence of particular internal or external positions and correlations showing the extent to which internal (or external) positions have similar profiles. On the basis of the results of the quantitative analysis, some positions can be selected, by the client or assessor, for closer examination.\n\nFrom the selected positions the client can tell a story that reflects the specific experiences associated with that position and, moreover, assessor and client can explore which positions can be considered as a dialogical response to one or more other positions. In this way, the method combines both qualitative and quantitative analyses.\n\nThe psychometric aspects of the PPR method was refined a procedure proposed by A. Kluger, Nir, & Y. Kluger. The authors analyze clients' Personal Position Repertoires by creating a bi-plot of the factors underlying their internal and external positions. A bi-plot provides a clear and comprehensible visual map of the relations between all the meaningful internal and external positions within the self in such a way that both types of positions are simultaneously visible. Through this procedure clusters of internal and external positions and dominant patterns can be easily observed and analyzed.\n\nThe method allows researchers or practitioners to study the general deep structures of the self. There are multiple bi-plots technologies available today. The simplest approach, however, is to perform a standard principal component analysis (PCA). To obtain a bi-plot, a PCA is once performed on the external positions and once on the internal positions, with the number of components in both PCA's restricted to two. Next, a scatter of the two PCAs is plotted on the same plane, where results of the first components are projected to the X-axis and of the second components to Y-axis. In this way, an overview of the organization of the internal and external positions together is realized.\n\nAnother assessment method, the \"Personality Web\", is devised by Raggatt. This semi-structured method starts from the assumption that the self is populated by a number of opposing narrative voices, with each voice having its own life story. Each voice competes with other voices for dominance in thought and action and each is constituted by a different set of affectively-charged attachments, to people, events, objects and one's own body.\n\nThe assessment comprises two phases— \n\nThis method represents a combination of qualitative and quantitative procedures that provide insight in the content and organization of a multi-voiced self.\n\nDialogical relationships are also studied with an adapted version of the \"Self-Confrontation Method\" (SCM).\n\nTake the following example. A client, Mary, reported that she sometimes experienced herself as a witch, eager to murder her husband, particularly when he was drunk. She did a self-investigation in two parts, one from her ordinary position as Mary and another from the position of the witch. Then, she told from each of the positions a story about her past, present, and future. These stories were summarized in the form of a number of sentences. It appeared that Mary formulated sentences that were much more acceptable from a societal point of view than those from the witch. Mary formulated sentences like \"I want to try to see what my mother gives me: there's only one of me\" or \"For the first time in my life, I'm engaged in making a home (\"home\" is also coming at home, entering into myself)\", whereas the witch produced statements like \"With my bland, pussycat qualities I have vulnerable things in hand, from which I derive power at a later moment (somebody tells me things that I can use so that I get what I want)\" or \"I enjoy when I have broken him [husband]: from a power position entering the battlefield.\"\n\nIt was found that the sentences of the two positions were very different in content, style, and affective meaning. Moreover, the relationship between Mary and the witch seemed to be more monological than dialogical, that is, either the one or the other was in control of the self and the situation and there was not no exchange between them. After the investigation, Mary received a therapeutic supervision during which she started to keep a diary in which she learned to make fine discriminations between her own experiences as Mary and those of the witch. She became not only aware of the needs of the witch but learned also to give an adequate response as soon as she noticed that the energy of the witch was upcoming. In a second investigation, one year later, the intensely conflicting relationship between Mary and the witch was significantly reduced and, as a result, there was less tension and stress in the self. She reported that in some situations, she even could make good use of the energy of the witch (e.g., when applying for a job). Whereas in some situations she was in control of the witch, in other situations she could even cooperate with her. The changes that took place between investigation 1 and investigation 2 suggested that the initial monological relationship between the two positions changed clearly into a more dialogical direction.\n\nUnder the supervision of the Polish psychologist Piotr Oleś, a group of researchers constructed a questionnaire method, called the \"Initial Questionnaire\", for the measurement of three types of \"internal activity\" (a) change of perspective, (b) internal monologue and (c) internal dialogue. The purpose of this questionnaire is to induce the subject's self-reflection and determine which \"I\"-positions are reflected by the participant's interlocutors and which of them give new and different points of view to the person.\n\nThe method includes a list of potential positions. The participants are invited to choose some of them and can add their own to the list. The selected positions, both internal and external ones, are then assessed as belonging to the dialogue, monologue of perspective categories. Such a questionnaire is well-suited for the investigation of correlations with other questionnaires.\n\nFor example, correlating the Initial Questionnaire with the Revised NEO Personality Inventory (NEO PI-R), the researchers found that persons having inner dialogues scored significantly lower on Assertiveness and higher on Self-Consciousness, Fantasy, Aesthetics, Feelings and Openness than people having internal monologues. They concluded that \"people entering into imaginary dialogues in comparison with ones having mainly monologues are characterized by a more vivid and creative imagination (Fantasy), a deep appreciation of art and beauty (Aesthetics) and receptivity to inner feelings and emotions (Feelings). They are curious about both inner and outer worlds and their lives are experientially richer. They are willing to entertain novel ideas and unconventional values and they experience positive as well as negative emotions more keenly (Openness). At the same time these persons are more disturbed by awkward social situations, uncomfortable around others, sensitive to ridicule, and prone to feelings of inferiority (Self-Consciousness), they prefer to stay in the background and let others do the talking (Assertiveness)\".\n\nOther methods are developed in fields related to DST. Based on Stiles' assimilation model, \"Osatuke et al.\", describes a method that enables the researcher to compare what is said by a client (verbal content) and how it is said (speech sounds). With this method the authors are able to assess to what extent the vocal manifestations (how it is said) of different internal voices of the same client parallel, contradict or complement their written manifestations (what is said). This method can be used to study the non-verbal characteristics of different voices in the self in connection with verbal content.\n\nOn the basis of Mikhail Bakhtin's theory of utterances, Leiman devised a \"dialogical sequence analysis.\" This method starts from the assumption that every utterance has an addressee. The central question is: To whom is the person speaking?\n\nUsually, we think of one listener as the immediately observable addressee. However, the addressee is rather a multiplicity of others, a complex web of invisible others, whose presence can be traced in the content, flow and expressive elements of the utterance (e.g., I'm directly addressing you but while speaking I'm protesting to a third person who is invisibly present in the conversation). When there are more than one addressees present in the conversation, the utterance positions the author/speaker into more (metaphorical) locations. Usually, these locations form sequences, that can be examined and made explicit when one listens carefully not only to the content but also the expressive elements in the conversation. Leiman's method, which analyzes a conversation in terms of \"chains of dialogical patterns\", is theory-guided, qualitative and sensitive to the verbal and the non-verbal aspects of utterances.\n\nIt is not the main purpose of the presented theory to formulate testable hypotheses, but to generate new ideas. It is certainly possible to perform theory-guided research on the basis of the theory, as exemplified by a special issue on dialogical self research in the \"Journal of Constructivist Psychology\" (2008) and in other publications (further on in the present section). Yet, the primary purpose is the generation of new ideas that lead to continued theory, research, and practice on the basis of links between the central concepts of the theory.\n\nTheoretical advances, empirical research, and practical applications are discussed in the \"International Journal for Dialogical Science\" and at the biennial \"International Conferences on the Dialogical Self\" as they are held in different countries and continents: Nijmegen, Netherlands (2000), Ghent, Belgium (2002), Warsaw, Poland (2004), Braga, Portugal (2006), Cambridge, United Kingdom (2008), Athens, Greece (2010), Athens, Georgia, United States (2012), and The Hague, Netherlands (2014).The aim of the journal and the conferences is to transcend the boundaries of (sub)disciplines, countries, and continents and create fertile interfaces where theorists, researchers and practitioners meet in order to engage in innovative dialogue.\n\nAfter initial publication on DST, the theory has been applied in a variety of fields: cultural psychology psychotherapy; personality psychology;psychopathology; developmental psychology; experimental social psychology; autobiography; social work; educational psychology; brain science; Jungian psychoanalysis; history; cultural anthropology; constructivism; social constructionism; philosophy; the psychology of globalization cyberpsychology; media psychology, vocational psychology, and literary sciences.\n\nFields of applications are also reflected by several special issues that appeared in psychological journals. In \"Culture & Psychology\" (2001), DST, as a theory of personal and cultural positioning, was exposed and commented on by researchers from different cultures. In \"Theory & Psychology\" (2002), the potential contribution of the theory for a variety of fields was discussed: developmental psychology, personality psychology, psychotherapy, psychopathology, brain sciences, cultural psychology, Jungian psychoanalysis, and semiotic dialogism. A second issue of this journal published in 2010 was also devoted to DST. In the \"Journal of Constructivist Psychology\" (2003) researchers and practitioners focused on the implications of the dialogical self for personal construct psychology, on the philosophy of Martin Buber, on the rewriting of narratives in psychotherapy, and on a psycho-dramatic approach in psychotherapy. The topic of mediated dialogue in a global and digital age was at the heart of a special issue in \"Identity: An International Journal of Theory and Research\" (2004). In \"Counselling Psychology Quarterly\" (2006), the dialogical self was applied to a variety of topics, such as, the relationship between adult attachment and working models of emotion, paranoid personality disorder, narrative impoverishment in schizophrenia, and the significance of social power in psychotherapy. In the \"Journal of Constructivist Psychology\" (2008) and in \"Studia Psychologica\" (2008), groups of researchers addressed the question of how empirical research can be performed on the basis of DST. The relevance of the dialogical self to developmental psychology was discussed in a special issue of \"New Directions for Child and Adolescent Development\" (2012). The application of the dialogical self in educational settings was presented in a special issue of the \"Journal of Constructivist Psychology\" (2013).\n\nSince its first inception in 1992, DST is discussed and evaluated, particularly at the biennial \"International Conferences on the Dialogical Self\" and in the \"International Journal for Dialogical Science\". Some of the main positive evaluations and main criticisms are summarized here. On the positive side, many researchers appreciate the breadth and the integrative character of the theory. As the above review of applications demonstrates, there is a broad range of fields in psychology and other disciplines in which the theory has received interests from thinkers, researchers and practitioners. The breadth of interest is also reflected by the range of scientific journals that have devoted special issues to the theory and its implications.\n\nThe theory has the potential to bring together scientists and practitioners from a variety of countries, continents and cultures. The \"Fifth International Conference on the Dialogical Self\" in Cambridge, United Kingdom attracted 300 participants from 43 countries. The conference focused primarily on DST, and dialogism as a related field. However, by focusing on dialogue, dialogical self goes beyond the post-modernism idea of the decentralization of the self and the notion of fragmentation. Recent work by John Rowan has resulted in the publication of a book by him entitled - 'Personification: Using the Dialogical Self in Psychotherapy and Counselling' published by Routledge. The book shows how to apply the concepts by those working in the therapeutic field.\n\nThe theory and its applications have also received several criticism. Many researchers have noted a discrepancy between theory and research. Certainly, more than most post-modernist approaches, the theory has instigated a variety of empirical studies and some of its main tenets are confirmed in experimental social-psychological research. Yet, the gap between theory and research still exists.\n\nClosely related to this gap, there is the lack of connection between dialogical self research and mainstream psychology. Although the theory and its applications have been published in mainstream journals like \"Psychological Bulletin\" and the \"American Psychologist\", it has not yet led to the adoption of the theory as a significant development in mainstream (American) psychology. Apart from the theory-research gap, one of the additional reasons for the lacking connection with mainstream research may be the fact that interest in the notion of dialogue, central in the history of philosophy since Plato, is largely neglected in psychology and other social sciences. Another disadvantage of the theory is that it lacks a research procedure that is sufficiently common to allow for the exchange of research data among investigators. Although different research tools have been developed (see the above review of assessment and research methods), none of them are used by a majority of researchers in the field.\n\nInvestigators often use different research tools which lead to a considerable richness of information but, at the same time, create a stumbling block for the comparison of research data. It seems that the breadth of the theory and the richness of its applications have a shadowy side in the relative isolation of research in the DST subfields. Other researchers find the scientific work done thus far to be of a too verbal nature. While the theory explicitly acknowledges the importance of pre-linguistic, non-linguistic forms of dialogue, the actual research is typically taking place on the verbal level with the simultaneous neglect of the non-verbal level (for a notable exception cultural-anthropological research on shape-shifting). Finally, some researchers would like to see more emphasis on the bodily aspects of dialogue. Up till now the theory has focused almost exclusively on the transcendence of the self-other dualism, as typical of the modern model of the self. More work should be done on the embodied nature of the dialogical self (for the role of the body in connection with emotions).\n\n\n\n"}
{"id": "57614420", "url": "https://en.wikipedia.org/wiki?curid=57614420", "title": "Douglas Wahlsten", "text": "Douglas Wahlsten\n\nDouglas Leon Wahlsten (born October 13, 1943) is a Canadian neuroscientist, psychologist, and behavior geneticist. He is a professor emeritus of psychology at the University of Alberta. As of 2011, he was also a visiting professor at the University of North Carolina at Greensboro in North Carolina, United States. He is known for his laboratory research on the behavior of mice, and for his theoretical writings on a wide range of other topics. His laboratory research has included studies of the effects of different laboratory environments and experimenter characteristics on the results of mouse studies. He and his colleagues have also developed an altered form of the rotarod performance test involving wrapping sandpaper around the rod, to reduce the ability of mice to grip the rod and ride around on it. He has criticized some of his fellow behavior geneticists for trying to separate the effects of genes and the environment on human intelligence, an endeavor he considers futile. He also met and became friends with Leilani Muir, later helping to edit her autobiography, \"A Whisper Past\". He was the president of the International Behavioural and Neural Genetics Society from 2000 to 2001.\n\n"}
{"id": "32329916", "url": "https://en.wikipedia.org/wiki?curid=32329916", "title": "Dual-flashlight plot", "text": "Dual-flashlight plot\n\nIn statistics, a dual-flashlight plot is a type of scatter-plot in which the standardized mean (SMCV) is plotted against the mean of a contrast variable representing a comparison of interest\n. The commonly used dual-flashlight plot is for the difference between two groups in high-throughput experiments such as microarrays and high-throughput screening studies, in which we plot the SSMD versus average log fold-change on the \"y\"- and \"x\"-axes, respectively, for all genes or compounds (such as siRNAs or small molecules) investigated in an experiment. \nAs a whole, the points in a dual-flashlight plot look like the beams of a flashlight with two heads, hence the name dual-flashlight plot.\n\nWith the dual-flashlight plot, we can see how the genes or compounds are distributed into each category in effect sizes, as shown in the figure. Meanwhile, we can also see the average fold-change for each gene or compound. The dual-flashlight plot is similar to the volcano plot. In a volcano plot, the p-value (or q-value), instead of SMCV or SSMD, is plotted against average fold-change \n. The advantage of using SMCV over p-value (or q-value) is that, if there exist any non-zero true effects for a gene or compound, the estimated SMCV goes to its population value whereas the p-value (or q-value) for testing no mean difference (or zero contrast mean) goes to zero when the sample size increases \n. Hence, the value of SMCV is comparable whereas the value of p-value or q-value is not comparable in experiments with different sample size, especially when many investigated genes or compounds do not have exactly zero effects. The dual-flashlight plot bears the same advantage that the SMCV has, as compared to the volcano plot.\n\n\n"}
{"id": "43026600", "url": "https://en.wikipedia.org/wiki?curid=43026600", "title": "EMBRACE (telescope)", "text": "EMBRACE (telescope)\n\nEMBRACE (Electronic MultiBeam Radio Astronomy ConcEpt) is a prototype telescope for the phase 2 of the Square Kilometre Array (SKA) project. It's the first dense phased array for radioastronomy in the GHz frequency range (initially planned for covering the 0.5-1.5 GHz, mid-frequency band of SKA). It is composed of two sites, one at the Nançay radio telescope station in France, and one near the Westerbork Synthesis Radio Telescope antennas in Netherlands.\n\n\n"}
{"id": "3347310", "url": "https://en.wikipedia.org/wiki?curid=3347310", "title": "Ernst Friedrich Germar", "text": "Ernst Friedrich Germar\n\nErnst Friedrich Germar (3 November 1786 – 8 July 1853) was a German professor and director of the Mineralogical Museum at Halle. As well as being a mineralogist he was interested in entomology and particularly in the Coleoptera and Hemiptera. He monographed the heteropteran family Scutelleridae.\n\nIn 1845, he was elected a foreign member of the Royal Swedish Academy of Sciences.\nAmongst Germar’s publications are:\nHe was also editor of the entomological journal \"Zeitschrift für die Entomologie\", which was published from 1839 to 1844.\n\nNon Coleoptera Deutsches Entomologisches Institut (DEI)\n\n"}
{"id": "31039747", "url": "https://en.wikipedia.org/wiki?curid=31039747", "title": "Famine 1975! America's Decision: Who Will Survive?", "text": "Famine 1975! America's Decision: Who Will Survive?\n\nFamine 1975! America's Decision: Who Will Survive? is a best-selling 1967 book by William and Paul Paddock. The brothers describe the rapidly growing population of the world, and a situation in which they believe it would be impossible to feed the entire global population within the short-term future. They believed that widespread famine would be the inevitable result, by 1975.\n\nThe basic argument of the book is summarized in a 1969 review by Bruce Trumbo:\n\n\nIn response, they suggest a system of triage in which the United States must \"divide the underdeveloped nations into three categories: 1) Those so hopelessly headed for or in the grip of famine (whether because of overpopulation, agricultural insufficiency, or political ineptness) that our aid will be a waste; these \"can't-be-saved nations\" will be ignored and left to their fate; 2) Those who are suffering but who will stagger through without our aid, \"the walking wounded\"; and 3) Those who can be saved by our help.\"\n\nThe Paddocks were aware that their policy of abandoning food aid to the \"hopeless countries\" (India and Egypt for example) would lead to an immediate worsening of the situation there, but they wrote \"to send food is to throw sand in the ocean.\" Using the triage system they hoped to avoid a broader catastrophe and stabilize the global population.\n\nPaul R. Ehrlich, who wrote bestseller \"The Population Bomb\" along similar lines the following year, lavishly praised the book, calling it courageous for daring to address the problems of the age in a concrete way, and one of the most important books of our age. Others criticized the use of extrapolated, and sometimes incorrect, statistics and assumptions to make such drastic and consequential conclusions. The book is often cited as a classic example of the neo-Malthusian revival of the 1950s-1970s.\n\nThe argument of the book proved false. Through policies of public food distribution and work-for-food programs, as well as the use of technology and hybrid seeds to increase food production - the Green Revolution - the situation changed drastically over the final decades of the 20th century.\n"}
{"id": "883576", "url": "https://en.wikipedia.org/wiki?curid=883576", "title": "Gamma Ray Spectrometer (2001 Mars Odyssey)", "text": "Gamma Ray Spectrometer (2001 Mars Odyssey)\n\nThe Gamma Ray Spectrometer (GRS) is a gamma-ray spectrometer on the \"2001 Mars Odyssey\" spacecraft, a space probe orbiting the planet Mars since 2001. Part of NASA's Mars Surveyor 2001 program, it returns geological data about Mars's surface such as identifying elements and the location of water. It is maintained by the Lunar and Planetary Laboratory at the University of Arizona in the United States. This instrument has mapped the distribution surface hydrogen, thought to trace water in the surface layer of Martian soil. \n\nThe Gamma Ray Spectrometer weighs and uses 32 watts of power. Along with its cooler, it measures . The detector is a photodiode made of a germanium crystal, reverse biased to about 3 kilovolts, mounted at the end of a boom to minimize interferences from the gamma radiation produced by the spacecraft itself. Its spatial resolution is about .\n\nThe neutron spectrometer is .\n\nThe high-energy neutron detector measures . The instrument's central electronics box is .\n\n"}
{"id": "46299102", "url": "https://en.wikipedia.org/wiki?curid=46299102", "title": "Gareth V. Williams", "text": "Gareth V. Williams\n\nGareth Vaughan Williams (born 1965, in Windlesham, England) is an English-American astronomer, who is the associate director of the International Astronomical Union's Minor Planet Center (MPC). \n\nHe joined the MPC in January 1990, and as such is the longest-serving staff member presently there. He is an IAU member and is the MPC representative on various IAU committees and working groups, including the \"Working Group on Planetary System Nomenclature\" and is secretary of the \"Working Group on Small Body Nomenclature.\" Gareth got his undergraduate degree in astronomy at University College London, and his PhD in 2013 from the Open University. He is known for recovering the lost asteroids 878 Mildred in 1991 and 719 Albert in 2000.\n\nHe also identified the earliest known observation of a Jupiter trojan, when he linked A904 RD, an object seen on only a single night by E. E. Barnard, with . Barnard's observations, which he initially believed belonged to Saturn IX (Phoebe), were sufficient to show that the object was distant, but he did not follow it up. The first Jupiter Trojan to be recognized as such, 588 Achilles, was discovered in 1906.\n\nMinor planet 3202 Graff, a Hilda asteroid discovered by Max Wolf at Heidelberg in 1908, was named in his honor on 10 April 1990 ().\n\n\n"}
{"id": "34518735", "url": "https://en.wikipedia.org/wiki?curid=34518735", "title": "Geochemical modeling", "text": "Geochemical modeling\n\nGeochemical modeling is the practice of using chemical thermodynamics, chemical kinetics, or both, to analyze the chemical reactions that affect geologic systems, commonly with the aid of a computer. It is used in high-temperature geochemistry to simulate reactions occurring deep in the Earth's interior, in magma, for instance, or to model low-temperature reactions in aqueous solutions near the Earth's surface, the subject of this article.\n\nGeochemical modeling is used in a variety of fields, including environmental protection and remediation, the petroleum industry, and economic geology. Models can be constructed, for example, to understand the composition of natural waters; the mobility and breakdown of contaminants in flowing groundwater or surface water; the formation and dissolution of rocks and minerals in geologic formations in response to injection of industrial wastes, steam, or carbon dioxide; and the generation of acidic waters and leaching of metals from mine wastes.\n\nGarrels and Thompson (1962) first applied chemical modeling to geochemistry in 25 °C and one atmosphere total pressure. Their calculation, computed by hand, is now known as an \"equilibrium model\", which predicts species distributions, mineral saturation states, and gas fugacities from measurements of bulk solution composition. By removing small aliquots of solvent water from an equilibrated spring water and repeatedly recalculating the species distribution, Garrels and Mackenzie (1967) simulated the reactions that occur as spring water evaporated. This coupling of mass transfer with an equilibrium model, known as a \"reaction path model\", enabled geochemists to simulate reaction processes.\n\nHelgeson (1968) introduced the first computer program to solve equilibrium and reaction path models, which he and coworkers used to model geological processes like weathering, sediment diagenesis, evaporation, hydrothermal alteration, and ore deposition. Later developments in geochemical modeling included reformulating the governing equations, first as ordinary differential equations, then later as algebraic equations. Additionally, chemical components came to be represented in models by aqueous species, minerals, and gases, rather than by the elements and electrons which make up the species, simplifying the governing equations and their numerical solution.\n\nRecent improvements in the power of personal computers and modeling software have made geochemical models more accessible and more flexible in their implementation. Geochemists are now able to construct on their laptops complex reaction path or reactive transport models which previously would have required a supercomputer.\n\nAn aqueous system is uniquely defined by its chemical composition, temperature, and pressure. Creating geochemical models of such systems begins by choosing the basis, the set of aqueous species, minerals, and gases which are used to write chemical reactions and express composition. The number of basis entries required equals the number of components in the system, which is fixed by the phase rule of thermodynamics. Typically, the basis is composed of water, each mineral in equilibrium with the system, each gas at known fugacity, and important aqueous species. Once the basis is defined, a modeler can solve for the equilibrium state, which is described by mass action and mass balance equations for each component.\n\nIn finding the equilibrium state, a geochemical modeler solves for the distribution of mass of all species, minerals, and gases which can be formed from the basis. This includes the activity, activity coefficient, and concentration of aqueous species, the saturation state of minerals, and the fugacity of gases. Minerals with a saturation index (log Q/K) equal to zero are said to be in equilibrium with the fluid. Those with positive saturation indices are termed supersaturated, indicating they are favored to precipitate from solution. A mineral is undersaturated if its saturation index is negative, indicating that it is favored to dissolve.\n\nGeochemical modelers commonly create reaction path models to understand how systems respond to changes in composition, temperature, or pressure. By configuring the manner in which mass and heat transfer are specified (i.e., open or closed systems), models can be used to represent a variety of geochemical processes. Reaction paths can assume chemical equilibrium, or they can incorporate kinetic rate laws to calculate the timing of reactions. In order to predict the distribution in space and time of the chemical reactions that occur along a flowpath, geochemical models are increasingly being coupled with hydrologic models of mass and heat transport to form reactive transport models. Specialized geochemical modeling programs that are designed as cross-linkable re-entrant software objects enable construction of reactive transport models of any flow configuration.\n\nGeochemical models are capable of simulating many different types of reactions. Included among them are: \n\nSimple phase diagrams or plots are commonly used to illustrate such geochemical reactions. Eh-pH (Pourbaix) diagrams, for example, are a special type of activity diagram which represent acid-base and redox chemistry graphically.\n\nVarious sources can contribute to a range of simulation results. The range of the simulation results is defined as model uncertainty. One of the most important sources not possible to quantify is the conceptual model, which is developed and defined by the modeller. Further sources are the parameterization of the model regarding the hydraulic (only when simulating transport) and mineralogical properties. The parameters used for the geochemical simulations can also contribute to model uncertainty. These are the applied thermodynamic database and the parameters for the kinetic minerals dissolution. Differences in the thermodynamic data (i.e. equilibrium constants, parameters for temperature correction, activity equations and coefficients) can result in large uncertainties. Furthermore, the large spans of experimentally derived rate constants for minerals dissolution rate laws can cause large variations in simulation results. Despite this is well-known, uncertainties are not frequently considered when conducting geochemical modelling.\n\nReducing uncertainties can be achieved by comparison of simulation results with experimental data, although experimental data does not exist at every temperature-pressure condition and for every chemical system. Although such a comparison or calibration can not be conducted consequently the geochemical codes and thermodynamic databases are state-of-the-art and the most useful tools for predicting geochemical processes.\n\n\n\n"}
{"id": "1294888", "url": "https://en.wikipedia.org/wiki?curid=1294888", "title": "Glasser's choice theory", "text": "Glasser's choice theory\n\nThe term choice theory is the work of William Glasser, MD, author of the book so named, and is the culmination of some 50 years of theory and practice in psychology and counselling.\n\nChoice theory posits behaviours we choose are central to our existence. Our behaviour (choices) are driven by five genetically driven needs, survival, love and belonging, freedom, fun, and power.\nSurvival needs include\nand four fundamental psychological needs:\n\nChoice theory suggests the existence of a \"Quality World\". Glasser's idea of a \"Quality World\" restates the Jungian idea of archetypes but Glasser never acknowledged this. Nonetheless, Glasser's \"Quality World\" and what Jung would call healthy archetypes are indistinguishable.\n\nOur \"Quality World\" images are our role models of an individual's \"perfect\" world of parents, relations, possessions, beliefs, etc. How each person's \"Quality World\" is somewhat unusual, even in the same family of origin, is taken for granted.\n\nStarting from birth and continuing throughout our lives, each person places significant role models, significant possessions and significant systems of belief (religion, cultural values, and icons, etc.) into a mostly unconscious framework Glasser called our \"Quality World\". Glasser mostly ignores the issues of negative role models and stereotypes in choice theory.\n\nGlasser also posits a \"Comparing Place\" where we compare-contrast our perception of people, places, and things immediately in front of us against our ideal images (archetypes) of these in our Quality World framework. Our subconscious pushes us towards calibrating—as best we can—our real world experience with our Quality World (archetypes).\n\nBehavior (\"Total Behavior\" in Glasser's terms) is made up of these four components: acting, thinking, feeling, and physiology. Glasser suggests we have considerable control or choice over the first two of these; yet, little ability to directly choose the latter two as they are more deeply sub- and unconscious. These four components remain closely intertwined, the choices we make in our thinking and acting greatly affect our feeling and physiology.\n\nA big conclusion for Glasser, one he repeats often, is the source of much personal unhappiness is failing or failed relationships with people important to us: spouses, parents, children, friends and colleagues.\n\nThe symptoms of unhappiness are widely variable and are often seen as mental illness. Glasser believed that \"pleasure\" and \"happiness\" are related but are far from synonymous. Sex, for example, is a \"pleasure\" but may well be divorced from a \"satisfactory relationship\" which is a precondition for lasting \"happiness\" in life. Hence the intense focus on the improvement of relationships in counseling with choice theory—the \"new reality therapy\". Those familiar with both are likely to prefer choice theory, the more modern formulation.\n\nChoice theory posits most mental illness is, in fact, an expression of unhappiness. Glasser champions how we are able to learn and choose alternate behaviors resulting in greater personal satisfaction. Reality therapy is the choice theory-based counseling process focused on helping clients to learn to make those self-optimizing choices.\n\n\nWilliam Glasser's choice theory begins: behavior is not separate from choice; we all choose how to behave at any time. Second, we cannot control anyone's behavior but our own. Glasser also believed in the vitality of classroom meetings for the purpose of improving communication and solving real classroom problems. In the classroom, it is important for teachers to \"help students envision a quality existence in school and plan the choices that lead to it\".\n\nFor example, Johnny Waits is an 18-year-old high school senior and plans on attending college to become a computer programmer. Glasser suggests Johnny could be learning as much as he can about computers instead of reading Plato. This concept is called quality curriculum, which connects students with practical real world topics, chosen by the student according to their leanings. Topics with actual career potential are most encouraged. Under Glasser's strategy, teachers hold discussions with students when introducing new topics asking them to identify what they would like to explore in depth. As part of the process, students need to explain why the material is valuable in life.\n\nGlasser was no supporter of Summerhill. Most Quality Schools he supervised had very conventional curriculum topics. The main innovation was a deeper, humanistic approach to group process between teacher, student and learning.\n\nA typical example of choice theory and education are Sudbury Model schools, where students decide for themselves how to spend their days. In these schools, students of all ages determine what they will do, as well as when, how, and where they will do it. This freedom is at the heart of the school and it belongs to the students as their right, not to be violated. The fundamental premises of the school are: that all people are curious by nature; that the most efficient, long-lasting, and profound learning takes place when started and pursued by the learner; that all people are creative if they are allowed to develop their unique talents; that age-mixing among students promotes growth in all members of the group; and that freedom is essential to the development of personal responsibility. In practice this means that students initiate all their own activities and create their own environments. The physical plant, the staff, and the equipment are there for the students to use as the need arises. The school provides a setting in which students are independent, are trusted, and are treated as responsible people; and a community in which students are exposed to the complexities of life in the framework of a participatory democracy.\n\nSudbury schools are based on the premise that students are personally responsible for their acts, in opposition to virtually all schools today that deny it. The denial is threefold: schools do not permit students to choose their course of action fully; they do not permit students to embark on the course, once chosen; and they do not permit students to suffer the consequences of the course, once taken. Freedom of choice, freedom of action, freedom to bear the results of action—these are the three great freedoms that constitute personal responsibility. Thus, members of these schools learn democracy by experience, and enjoy the rights of individuals.\n\nSudbury schools do not perform and do not offer evaluations, assessments, or recommendations, asserting that they do not rate people, and that school is not a judge; comparing students to each other, or to some standard that has been set is for them a violation of the student's right to privacy and to self-determination. Students decide for themselves how to measure their progress as self-starting learners as a process of self-evaluation: real lifelong learning and the proper educational evaluation for the 21st Century, they adduce.\n\nGlasser's theories and teachings have not gone without criticism. In a book review, W. Clay Jackson writes, \"Dr. Glasser postulates that everything contained in the DSM-IV-TR is a result of an individual's brain creatively expressing its unhappiness. ... Dr. Glasser demonizes the entire profession as charlatans who have been brainwashed by their predecessors or who simply misrepresent many of the psychiatric illnesses to patients as having a biological basis. ... Despite claiming to have an appendix full of references demonstrating there is no evidence medications have a role in curing mental illness, the book simply relies on a core group of anti-establishment authors. ... However, what is noticeably absent from the book is a set of randomized clinical trials demonstrating the success of his teachings.\"\n\n\n"}
{"id": "2037155", "url": "https://en.wikipedia.org/wiki?curid=2037155", "title": "Group action (sociology)", "text": "Group action (sociology)\n\nIn sociology, a group action is a situation in which a number of agents take action simultaneously in order to achieve a common goal; their actions are usually coordinated.\n\nGroup action will often take place when social agents realize they are more likely to achieve their goal when acting together rather than individually. Group action differs from group behaviours, which are uncoordinated, and also from mass actions, which are more limited in place.\n\nGroup action is more likely to occur when the individuals within the group feel a sense of unity with the group, even in personally costly actions.\n"}
{"id": "23837915", "url": "https://en.wikipedia.org/wiki?curid=23837915", "title": "Helical Dirac fermion", "text": "Helical Dirac fermion\n\nA Helical Dirac fermion is a charge carrier that behaves as a massless relativistic particle with its intrinsic spin locked to its translational momentum.\n"}
{"id": "487943", "url": "https://en.wikipedia.org/wiki?curid=487943", "title": "Inverted sugar syrup", "text": "Inverted sugar syrup\n\nInvert(ed) sugar (syrup) is an edible mixture of two simple sugars—glucose and fructose—that is made by heating sucrose (table sugar) with water. Invert sugar is thought to be sweeter than table sugar, and foods that contain it retain moisture and crystallize less easily. Bakers, who call it \"invert syrup\", may use it more than other sweeteners.\n\nThough invert sugar syrup can be made by heating table sugar in water alone, the reaction can be sped up by adding lemon juice, cream of tartar or other catalysts often without changing the flavor noticeably.\n\nThe mixture of the two simple sugars is formed by a process of hydrolysis of sucrose. This mixture has the opposite direction of optical rotation as the original sugar, which is why it is called an \"invert\" sugar.\n\nTable sugar (sucrose) is converted to invert sugar by a chemical reaction called \"hydrolysis\". Heating a mixture or solution of table sugar and water breaks the chemical bond that links together the two simple-sugar components.\n\nThe balanced chemical equation for the hydrolysis of sucrose into glucose and fructose is:\n\nOnce a sucrose solution has had some of its sucrose turned into glucose and fructose the solution is no longer said to be pure. The gradual decrease in purity of a sucrose solution as it is hydrolyzed affects a chemical property of the solution called optical rotation that can be used to figure out how much of the sucrose has been hydrolyzed and therefore whether the solution has been inverted or not. \n\nA kind of light called plane polarized light can be shone through a sucrose solution as it is heated up for hydrolysis. Such light has an 'angle' that can be measured using a tool called a polarimeter. When such light is shone through a solution of pure sucrose it comes out the other side with a different angle than when it entered; its angle is therefore said to be 'rotated' and how many degrees the angle has changed (the degree of its rotation or its 'optical rotation') is given a letter name, formula_1 (alpha). When the rotation between the angle the light has when it enters and when it exits is in the clockwise direction, the light is said to be 'rotated right' and formula_1 is given to have a \"positive\" angle like formula_3. When the rotation between the angle the light has when it enters and when it exits is in the counterclockwise direction, the light is said to be 'rotated left' and formula_1 is given a \"negative\" angle like formula_5. \n\nWhen plane polarized light enters and exits a solution of \"pure\" sucrose its angle is rotated formula_6 (clockwise or to the right). As the sucrose is heated up and hydrolyzed the amount of glucose and fructose in the mixture increases and the optical rotation decreases. After formula_1 passes zero and becomes a negative optical rotation, meaning that the rotation between the angle the light has when it enters and when it exits is in the counter clockwise direction, it is said that the optical rotation has 'inverted' its direction. This leads to the definition of an 'inversion point' as the per cent amount sucrose that has to be hydrolyzed before formula_1 equals zero. Any solution which has passed the inversion point (and therefore has a negative value of formula_1) is said to be 'inverted'. \n\nAs the shapes of the molecules ('chemical structures') of sucrose, glucose and fructose are all asymmetrical the three sugars come in several different forms, called stereoisomers. The existence of these forms is what gives rise to these chemicals' optical properties. When plane polarized light passes through a pure solution of one of these \"forms\" of one of the sugars it is thought to hit and 'glance off' certain asymmetrical chemical bonds within the molecule of that form of that sugar. Because those particular bonds (which in cyclic sugars like sucrose, glucose and fructose include a kind of bond called an anomeric bond) are different in each form of the sugar, each form rotates the light to a different degree. \n\nWhen any one form of a sugar is purified and put in water, it rapidly takes other forms of the same sugar. This means that a solution of a pure sugar normally has all of its stereoisomers present in the solution in different amounts which usually do not change much. This has an 'averaging' effect on all of the optical rotation angles (formula_1 values) of the different forms of the sugar and leads to the pure sugar solution having its own 'total' optical rotation, which is called its 'specific rotation' or 'observed specific rotation' and which is written as formula_11.\nWater molecules do not have chirality, therefore they do not have any effect on the measurement of optical rotation. When plane polarized light enters a body of pure water its angle is no different than when it exits. Thus, for water, formula_12. Chemicals that, like water, have specific rotations that equal zero degrees are called 'optically inactive' chemicals and like water, they do not need to be considered when calculating optical rotation.\n\nThe overall optical rotation of a mixture of chemicals can be calculated if the proportion of the amount of each chemical in the solution is known. If there are formula_13-many optically active different chemicals ('chemical species') in a solution and the molar concentration (the number of moles of each chemical per Liter of liquid solution) of each chemical in the solution is known and written as formula_14 (where formula_15 is a number used to identify the chemical species); and if each species has a specific rotation (the optical rotation of that chemical were it made as a pure solution) written as formula_16, then the mixture has the overall optical rotationformula_17Where formula_18 is the mole fraction of the formula_19 species. \n\nAssuming no extra chemical products are formed by accident (\"that is\", there are no side reactions) a completely hydrolyzed sucrose solution no longer has any sucrose and is a half-and-half mixture of glucose and fructose. This solution has the optical rotationformula_20\n\nIf a sucrose solution has been partly hydrolyzed, then it contains sucrose, glucose and fructose and its optical rotation angle depends on the relative amounts of each for the solution;formula_21Where formula_22, formula_23, and formula_24 stand for sucrose, glucose, and fructose. \n\nThe particular values of formula_25 do not need to be known to make use of this equation as the inversion point (per cent amount of sucrose that must be hydrolyzed before the solution is inverted) can be calculated from the specific rotation angles of the pure sugars. The reaction stoichiometry (the fact that hydrolyzing one sucrose molecule makes one glucose molecule and one fructose molecule) shows that when a solution begins with formula_26 moles of sucrose and no glucose nor fructose and formula_27 moles of sucrose are then hydrolyzed the resulting solution has formula_28 moles of sucrose, formula_27 moles of glucose and formula_27 moles of fructose. The total number of moles of sugars in the solution is therefore formula_31and the reaction progress (per cent completion of the hydrolysis reaction) equals formula_32. It can be shown that the solution's optical rotation angle is a function of (explicitly depends on) this per cent reaction progress. When the quantity formula_33 is written as formula_34 and the reaction is formula_35 done, the optical rotation angle isformula_36By definition, formula_1 equals zero degrees at the 'inversion point'; to find the inversion point, therefore, alpha is set equal to zero and the equation is manipulated to find formula_34. This givesformula_39Thus it is found that a sucrose solution is inverted once at least formula_40 of the sucrose has been hydrolyzed into glucose and fructose. \n\nHolding a sucrose solution at temperatures of hydrolyzes no more than about 85 per cent of its sucrose. Finding formula_1 when formula_42 shows that the optical rotation of the solution after hydrolysis is done is formula_43; this reaction is said to invert the sugar because its final optical rotation is less than zero. A polarimeter can be used to figure out when the inversion is done by detecting whether the optical rotation of the solution at an earlier time in its hydrolysis reaction equals formula_43. \n\nCommon sugar can be inverted quickly by mixing sugar and citric acid or cream of tartar at a ratio of about 1000:1 by weight and adding water. If lemon juice which is about five per cent citric acid by weight is used instead then the ratio becomes 1:75. Such a mixture, heated to and added to another food, prevents crystallization without tasting sour. \n\nInvert sugar syrup can be made without acids or enzymes by heating it up alone: two parts granulated sugar and one part water, simmered for five to seven minutes, will be partly inverted.\n\nCommercially prepared enzyme-catalyzed solutions are inverted at . The optimum pH for inversion is 5.0. Invertase is added at a rate of about 0.15% of the syrup's weight, and inversion time will be about 8 hours. When completed the syrup temperature is raised to inactivate the invertase, but the syrup is concentrated in a vacuum evaporator to preserve color.\n\nCommercially prepared hydrochloric-acid catalysed solutions may be inverted at the relatively low temperature of . The optimum pH for acid-catalysed inversion is 2.15. As the inversion temperature is increased, the inversion time decreases. They are neutralized when the desired level of inversion is reached.\n\nIn confectionery and candy making, cream of tartar is commonly used as the acidulant, with typical amounts in the range of 0.15-0.25% of the sugar's weight. The use of cream of tartar imparts a honey-like flavor to the syrup. After the inversion is completed, it may be neutralized with baking soda using a weight of 45% of the cream of tartar's weight.\n\nWhen adding baking soda and whipping or mixing, the hot syrup will foam and bubble up, so some care is required. A much taller pan than otherwise needed will contain the foam. Make sure there is enough water remaining in the syrup to dissolve the baking soda. Alternatively, dissolve the baking soda in a little extra water, and ensure the syrup's temperature is somewhat below .\n\nThe amount of water can be increased to increase the time it takes to reach the desired final temperature, and increasing the time increases the amount of inversion that occurs. In general, higher final temperatures result in thicker syrups, and lower final temperatures, in thinner ones.\n\nAll constituent sugars (sucrose, glucose and fructose) support fermentation, so invert sugar solutions may be fermented as readily as sucrose solutions.\n\nFully inverted sugar's low water content improves the shelf lives of products that contain it. The shelf life of partly inverted sugar is about six months and varies by storage and climatic conditions. \n\nCrystallized invert sugar solutions can be restored to their liquid state by gently heating.\n\n\n\n"}
{"id": "11305248", "url": "https://en.wikipedia.org/wiki?curid=11305248", "title": "Joost Schymkowitz", "text": "Joost Schymkowitz\n\nJoost Schymkowitz is a Belgian molecular biologist and researcher at the KU Leuven (Brussels, Leuven). Together with Frederic Rousseau he is group leader at the VIB Switch Laboratory, KU Leuven.\n\nHis research interest is on essential cellular processes where functional regulation is governed by protein conformational switches that have to be actively controlled to ensure cell viability\n\nHe obtained a PhD at the University of Cambridge (Cambridge, United Kingdom) in 2001. He did a Postdoc at the EMBL in Heidelberg Germany from 2001 until 2003. Joost Schymkowitz is a VIB Group leader since 2003.\n\n"}
{"id": "54258215", "url": "https://en.wikipedia.org/wiki?curid=54258215", "title": "KBC Void", "text": "KBC Void\n\nThe KBC Void, named after astronomers Ryan Keenan, Amy Barger, and Lennox Cowie, who first inferred its existence in 2013, is an immense, comparatively empty region of space that contains the Milky Way itself, the Local Group and much of the Laniakea Supercluster. This void is roughly spherical, approximately 2 billion light-years (600 megaparsecs (Mpc)) in diameter, with the Milky Way within a few hundred million light-years of its centre. The KBC Void is the largest supervoid known to science. It is similar in size to the Eridanus supervoid. The existence of these voids have been shown to be consistent with the standard cosmological model. The KBC void has been used to explain the discrepancy between measurements of the Hubble constant using galactic supernovae and Cepheid variables (72–75 km/s/Mpc) and from the cosmic microwave background and baryon acoustic oscillation data (67–68 km/s/Mpc). Galaxies inside the void experience gravitational pull from matter outside the void, yielding a larger local value for the Hubble constant.\n\n"}
{"id": "32107594", "url": "https://en.wikipedia.org/wiki?curid=32107594", "title": "Konrad Thaler", "text": "Konrad Thaler\n\nKonrad Thaler (born 19 December 1940 in Innsbruck, Austria – died 11 June 2005) was an Austrian arachnologist.\n\nPeter J. Schwendinger, other Austrian arachnologist, studied with Konrad Thaler, in Innsbruck University.\n\nThe specific epithet of the spider species \"Palaeoperenethis thaleri\" was designated by P. Selden and D. Penney in honor of the late Dr. Konrad Thaler, past president of the International Society of Arachnology.\n\n"}
{"id": "9493992", "url": "https://en.wikipedia.org/wiki?curid=9493992", "title": "List of ATSC standards", "text": "List of ATSC standards\n\nBelow are the published ATSC standards for ATSC digital television service, issued by the Advanced Television Systems Committee.\n\n\nIn 2004, the main ATSC standard was amended to support Enhanced ATSC (A/112); this transmission mode is backwardly compatible with the original 8-Bit Vestigal Sideband modulation scheme, but provides much better error correction.\n\nATSC-M/H for mobile TV has been approved and added to some stations, though it is known that it uses MPEG-4 instead of MPEG-2 for encoding, and behaves as an MPEG-4-encoded subchannel, inheriting 8VSB from the remainder of the channel.\n\nATSC 3.0 is a non-backwards-compatible version of ATSC being developed (as of May 18, 2016) that uses OFDM instead of 8VSB and a much newer video codec (instead of ATSC 1 and 2's MPEG-2). At the time of this article's writing, it has several candidate and one finalized standard.\n\nOn March 28, 2016, the Bootstrap component of ATSC 3.0 (System Discovery and Signalling) was upgraded from candidate standard to finalized standard.\n\nOn May 4, 2016, the Audio Codec component of ATSC 3.0 was elevated to candidate standard, with two finalists remaining: Dolby AC-4 and MPEG-H Audio Alliance format from Fraunhofer IIS, Qualcomm and Technicolor SA. A third entry from DTS named (a successor to DTS-HD) was withdrawn before the standard was upgraded to candidate status.\n\nOn September 8, 2016, the Physical Layer Download (OFDM) component of ATSC 3.0 was upgraded from candidate standard to finalized standard.\n\nOn October 5, 2016, the Link Layer Protocol Standard (A/330) was elevated from Candidate to final standard, along with the Audio Watermark Emission Standard (A/334) and Video Watermark Emission Standard (A/335). ATSC Technology Group 3 (TG3) members have also begun voting on elevating the following Candidate Standards to Proposed Standard status (the final step before becoming an approved standard): Service Announcement (A/322), Service Usage Reporting (A/333) and Captions and Subtitles (A/343). TG3 members also are voting to elevate Security (A/360) to Candidate Standard status, joining Schedule and Studio-to-Transmitter Link Standard (A/324), which was recently elevated. On March 30, 2016, A/324 (Schedule and Studio-to-Transmitter Link) was upgraded from Proposed to Candidate Standard.\n\nOn January 3, 2017, ATSC announced the updated status of its standards, in time for its debut at the Consumer Electronics Show in Las Vegas. As a result, this update, Captions and Subtitles (A/343) was upgraded from Candidate to Finalized Standard; Security (A-360), Lab Performance Test Plan (A-325) and Field Test Plan (A-326) were upgraded to Candidate Standard from \"Under Consideration\".\n\nBy March 7, 2017, ATSC announced a further update to the status of its standards, with the following as Finalized: A/321 (System Discovery and Signaling); A/322 Physical Layer Protocl (COFDM); A/326 (Field Test Plan [Recommended Practice]); A/330 (Link Layer Protocol); A/333 (Service Usage Reporting); A/334 (Audio Watermark Emission); A/335 (Video Watermark Emission); A/336 (Content Recovery in Redistribution Scenarios [ATSC 3.0 over Cable and Satellite]); A/342 Part 1 (Audio Common Elements); A/342 Part 2 (Audio: Dolby AC-4 System); A/342 Part 3 (Audio MPEG-H System); and A/343 (Captions and Subtitles). The following are Proposed Standards: A/325 (Lab Performance Test Plan [Recommended Practice]); A/332 (Service Announcement); A/338 (Companion Device); A/341 (Video - H.265/HEVC). The following are Candidate Standards: A/300 (ATSC 3.0 System); A/324 (Scheduler/Studio-to-Transmitter Link); A/331 (Signalling, Delivery, Sync Error Protection); A/337 (Application Signalling); A/344 (Interactive Content); A/360 (Security and Service Protection). The following is a Draft Standard: A/323 (Physical Layer Uplink/Downlink).\n\n\n\n\n\n\n\n"}
{"id": "2674673", "url": "https://en.wikipedia.org/wiki?curid=2674673", "title": "List of compounds with carbon numbers 40–49", "text": "List of compounds with carbon numbers 40–49\n\nThis is a partial list of molecules that contain 40 to 49 carbon atoms.\n\n"}
{"id": "3469922", "url": "https://en.wikipedia.org/wiki?curid=3469922", "title": "List of metalworking occupations", "text": "List of metalworking occupations\n\nMetalworking occupations include:\n\n\n\n\n\n"}
{"id": "374684", "url": "https://en.wikipedia.org/wiki?curid=374684", "title": "List of mountain passes", "text": "List of mountain passes\n\nThis is a list of mountain passes.\n\n\n\n\n\n\n\n\nSee: List of mountain passes in Kyrgyzstan\n\n\n\n\n\n\n\n\n\"See also:\" Principal passes of the Alps, List of mountain passes in Switzerland.\n\n\n\n\n\n\n\n\"See:\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "54774161", "url": "https://en.wikipedia.org/wiki?curid=54774161", "title": "Maguari virus", "text": "Maguari virus\n\nMaguari virus, abbreviated MAGV, is a negative-sense, single-stranded RNA virus in the Bunyavirales order, genus \"Orthobunyavirus\", Bunyamwera serogroup, that has been shown to be capable of causing human disease. MAGV is related to Cache Valley virus and Tensaw virus.\n\nIn addition to humans, MAGV has been isolated from mosquitoes, horses, cattle, sheep, birds, and rodents. The mosquito species include \"Aedes fulvus\", \"Aedes scapularis\", \"Aedes serratus\", \"Culex taeniopus\", and species in the genera \"Anopheles\", \"Wyeomyia\", and \"Psorophora\".\n\nMAGV's geographic range includes Argentina, Brazil, Colombia, French Guiana, Guyana, and Peru.; it has not been isolated north of Trinidad. The presence of antibodies to Maguari virus in human residents of south Florida can be attributed to either cross-reactivity with Tensaw virus, or cross-reactivity to an antigenic subtype or variant of Tensaw virus, although it is possible that another, undescribed, Bunyamwera serogroup virus may exist in south Florida.\n"}
{"id": "51440095", "url": "https://en.wikipedia.org/wiki?curid=51440095", "title": "Mikko Tuomi", "text": "Mikko Tuomi\n\nMikko Tuomi is a Finnish astronomer from the University of Hertfordshire, most known for his contributions to the discovery of a number of exoplanets, among them the Proxima Centauri b which orbits the closest star to the Sun. Mikko Tuomi was the first to find indications of the existence of Proxima Centauri b in archival observation data. Other exoplanets to whose discovery or study Tuomi has contributed include HD 40307, HD 154857 c, Kapteyn c, Gliese 682 c, HD 154857, Gliese 221, Gliese 581 g and the planetary system orbiting Tau Ceti. He has led the development of new data analysis techniques for distinguishing observations caused by natural activity of the star and those caused by planets orbiting them.\n\nMikko Tuomi has contributed to many cosmologist research articles including:\n\nMikko Tuomi helped discover the Proxima Centauri b planet by observing the gravitational tug of Proxima Centauri's star, Proxima Centauri.\n\n"}
{"id": "17685051", "url": "https://en.wikipedia.org/wiki?curid=17685051", "title": "Multiple-alarm fire", "text": "Multiple-alarm fire\n\nOne-alarm, two-alarm, three-alarm fires, etc., are categories of fires indicating the level of response by local authorities. The term multiple-alarm is a quick way of indicating that a fire is severe and is difficult to contain. This system of classification is common in the United States and in Canada among both fire departments and news agencies.\n\nA common misconception is that a \"3-alarm fire,\" for example, means that three firehouses responded to the fire. This is not the rule behind the naming convention, although some cities may use the number of firehouses responding for multi-alarm designations because that is the simplest way to determine an alarm number.\n\nThe most widely used formula for multi-alarm designation is based on the number of units (firetrucks for example) and firefighters responding to a fire; the more vehicles and firefighters responding, the higher the alarm designation. (Note: In most cities, a \"unit\" can be anything from a tanker or ladder truck to rescue vehicles to even cars driven by the chief and deputies.)\n\nWith this unit/firefighter alarm designation, the initial dispatch is referred to as a \"first alarm\" and is typically the largest. Subsequent alarms are calls for additional units, usually because the fire has grown and additional resources are needed to combat it, or that the incident is persisting long enough that firefighters on scene need to be relieved.\n\nRequests for units and firefighters from outside jurisdictions do not normally occur in multi-firehouse urban areas until elevated alarms are reached (alarm three and above), but will depend on the location of the incident and the condition of the authority having jurisdiction at the time of the incident.\n\nThe system of classification comes from the old tradition of using pull stations to alert the local departments to a fire in their area. The \"box\" would send a message to all local stations by telegraph that there was a fire, indicating the location as a number: (station area)-(box number), e.g., 2-11. Fires are still dispatched as \"box alarms,\" following this tradition, with maps broken up into a grid of \"box areas.\"\n\nBelow is a list of the alarm levels used in the response policy of the New York City Fire Department. This is a basic example of how alarm levels are categorized in a fire department, how many fire apparatus or fire units respond to each alarm level, etc. In New York, however, additional special alarm levels are utilized, aside from the conventional 1st alarm, 2nd alarm, 3rd alarm, etc. Examples of such alarm levels are the signal 10-75 assignment, the signals 10-76 and 10-77 assignments, and the signal 10-60 assignment. A 10-75 is a working fire (i.e., there is fire visible from a building), the 10-76/10-77 assignments are the alarm levels separate from the 1st, 2nd, 3rd alarms, etc. that are the standard fire department responses to fires in high-rise buildings. The signal 10-60 is a separate response to major disasters. \n\nBelow is how alarm levels are categorized in order per protocol. Each apparatus count is in an addition per alarm (a five alarm assignment has 21 engine companies total). Each total is the total number of units on scene. \n\n1st alarm assignment / Box alarm assignment:\n\n1st alarm assignment / All hands box alarm assignment:\n\n2nd alarm assignment:\n\n3rd alarm assignment:\n\n4th alarm assignment:\n\n5th alarm assignment: \n\nIf the incident commander decides that the incident does not require a higher alarm level to be requested, they can specially request an additional unit to the scene without requesting a full alarm level assignment. For example, at a working fire, there are four engine companies, three ladder companies, one squad company, one rescue company, two battalion chiefs, and one division chief operating at the scene. If the fire is not large enough to require a 2nd alarm, but there is a need for more equipment and manpower, the commanding chief can request additional units to respond \"specially called\" to the scene.\n\nThus, at the scene of a 5th alarm fire in New York, there are a total of 20 engine companies, 11 ladder companies, one squad company, one rescue company, six battalion chiefs, one division chief, one deputy chief, one assistant chief, and the chief of operations, as well as multiple specialized units and or specially called units operating on the scene.\n\nAll of these companies come from many firehouses to the scene. Some companies, however, are quartered together at the same firehouses. So, it is not a matter of how many firehouses respond to a fire, as popularly believed, but rather, how many companies/units and how many firefighters are operating on scene.\n\n"}
{"id": "291928", "url": "https://en.wikipedia.org/wiki?curid=291928", "title": "Operator (physics)", "text": "Operator (physics)\n\nIn physics, an operator is a function over a space of physical states to another space of physical states. The simplest example of the utility of operators is the study of symmetry (which makes the concept of a group useful in this context). Because of this, they are a very useful tool in classical mechanics. Operators are even more important in quantum mechanics, where they form an intrinsic part of the formulation of the theory.\n\nIn classical mechanics, the movement of a particle (or system of particles) is completely determined by the Lagrangian formula_1 or equivalently the Hamiltonian formula_2, a function of the generalized coordinates \"q\", generalized velocities formula_3 and its conjugate momenta:\n\nIf either \"L\" or \"H\" is independent of a generalized coordinate \"q\", meaning the \"L\" and \"H\" do not change when \"q\" is changed, which in turn means the dynamics of the particle are still the same even when \"q\" changes, the corresponding momenta conjugate to those coordinates will be conserved (this is part of Noether's theorem, and the invariance of motion with respect to the coordinate \"q\" is a symmetry). Operators in classical mechanics are related to these symmetries.\n\nMore technically, when \"H\" is invariant under the action of a certain group of transformations \"G\":\n\nthe elements of \"G\" are physical operators, which map physical states among themselves.\n\nwhere formula_6 is the rotation matrix about an axis defined by the unit vector formula_7 and angle \"θ\".\n\nIf the transformation is infinitesimal, the operator action should be of the form\n\nwhere formula_9 is the identity operator, formula_10 is a parameter with a small value, and formula_11 will depend on the transformation at hand, and is called a generator of the group. Again, as a simple example, we will derive the generator of the space translations on 1D functions.\n\nAs it was stated, formula_12. If formula_13 is infinitesimal, then we may write\n\nThis formula may be rewritten as\n\nwhere formula_16 is the generator of the translation group, which in this case happens to be the \"derivative\" operator. Thus, it is said that the generator of translations is the derivative.\n\nThe whole group may be recovered, under normal circumstances, from the generators, via the exponential map. In the case of the translations the idea works like this.\n\nThe translation for a finite value of formula_17 may be obtained by repeated application of the infinitesimal translation:\n\nwith the formula_19 standing for the application formula_20 times. If formula_20 is large, each of the factors may be considered to be infinitesimal:\n\nBut this limit may be rewritten as an exponential:\n\nTo be convinced of the validity of this formal expression, we may expand the exponential in a power series:\n\nThe right-hand side may be rewritten as\n\nwhich is just the Taylor expansion of formula_26, which was our original value for formula_27.\n\nThe mathematical properties of physical operators are a topic of great importance in itself. For further information, see C*-algebra and Gelfand-Naimark theorem.\n\nThe mathematical formulation of quantum mechanics (QM) is built upon the concept of an operator.\n\nThe wavefunction represents the probability amplitude of finding the system in that state. The terms \"wavefunction\" and \"state\" in QM context are usually used interchangeably.\n\nPhysical pure states in quantum mechanics are represented as unit-norm vectors (probabilities are normalized to one) in a special complex Hilbert space. Time evolution in this vector space is given by the application of the evolution operator.\n\nAny observable, i.e., any quantity which can be measured in a physical experiment, should be associated with a self-adjoint linear operator. The operators must yield real eigenvalues, since they are values which may come up as the result of the experiment. Although traditionally physicists associated real eigenvalues with Hermiticity, in 1998 physicists realized that there also exist non-Hermitian operators with all-real spectra; namely, parity-time (PT) symmetric operators. For Hermitian operators, the probability of each eigenvalue is related to the projection of the physical state on the subspace related to that eigenvalue. See below for mathematical details about Hermitian operators.\n\nIn the wave mechanics formulation of QM, the wavefunction varies with space and time, or equivalently momentum and time (see position and momentum space for details), so observables are differential operators.\n\nIn the matrix mechanics formulation, the norm of the physical state should stay fixed, so the evolution operator should be unitary, and the operators can be represented as matrices. Any other symmetry, mapping a physical state into another, should keep this restriction.\n\nThe wavefunction must be square-integrable (see Lp spaces), meaning:\n\nand normalizable, so that:\n\nTwo cases of eigenstates (and eigenvalues) are:\n\nLet \"ψ\" be the wavefunction for a quantum system, and formula_36 be any linear operator for some observable \"A\" (such as position, momentum, energy, angular momentum etc.), then\n\nwhere:\n\n\nIf \"ψ\" is an eigenfunction of a given operator A, then a definite quantity (the eigenvalue a) will be observed if a measurement of the observable A is made on the state \"ψ\". Conversely, if \"ψ\" is not an eigenfunction of A, then it has no eigenvalue for A, and the observable does not have a single definite value in that case. Instead, measurements of the observable A will yield each eigenvalue with a certain probability (related to the decomposition of \"ψ\" relative to the orthonormal eigenbasis of A).\n\nIn bra–ket notation the above can be written;\n\nin which case formula_40 is an eigenvector, or eigenket.\n\nDue to linearity, vectors can be defined in any number of dimensions, as each component of the vector acts on the function separately. One mathematical example is the del operator, which is itself a vector (useful in momentum-related quantum operators, in the table below).\n\nAn operator in \"n\"-dimensional space can be written:\n\nwhere e are basis vectors corresponding to each component operator \"A\". Each component will yield a corresponding eigenvalue. Acting this on the wave function \"ψ\":\n\nin which\n\nIn bra–ket notation:\n\nIf two observables \"A\" and \"B\" have linear operators formula_45 and formula_46, the commutator is defined by,\n\nThe commutator is itself a (composite) operator. Acting the commutator on \"ψ\" gives:\n\nIf \"ψ\" is an eigenfunction with eigenvalues \"a\" and \"b\" for observables \"A\" and \"B\" respectively, and if the operators commute:\n\nthen the observables \"A\" and \"B\" can be measured simultaneously with infinite precision i.e. uncertainties formula_50, formula_51 simultaneously. \"ψ\" is then said to be the simultaneous eigenfunction of A and B. To illustrate this:\n\nIt shows that measurement of A and B does not cause any shift of state i.e. initial and final states are same (no disturbance due to measurement). Suppose we measure A to get value a. We then measure B to get the value b. We measure A again. We still get the same value a. Clearly the state (\"ψ\") of the system is not destroyed and so we are able to measure A and B simultaneously with infinite precision.\nIf the operators do not commute:\n\nthey can't be prepared simultaneously to arbitrary precision, and there is an uncertainty relation between the observables,\n\neven if \"ψ\" is an eigenfunction the above relation holds.. Notable pairs are position and momentum, and energy and time - uncertainty relations, and the angular momenta (spin, orbital and total) about any two orthogonal axes (such as \"L\" and \"L\", or \"s\" and \"s\" etc.).\n\nThe expectation value (equivalently the average or mean value) is the average measurement of an observable, for particle in region \"R\". The expectation value formula_55 of the operator formula_45 is calculated from:\n\nThis can be generalized to any function \"F\" of an operator:\n\nAn example of \"F\" is the 2-fold action of \"A\" on \"ψ\", i.e. squaring an operator or doing it twice:\n\nThe definition of a Hermitian operator is:\n\nFollowing from this, in bra–ket notation:\n\nImportant properties of Hermitian operators include:\n\n\nAn operator can be written in matrix form to map one basis vector to another. Since the operators are linear, the matrix is a linear transformation (aka transition matrix) between bases. Each basis element formula_62 can be connected to another, by the expression:\n\nwhich is a matrix element:\n\nA further property of a Hermitian operator is that eigenfunctions corresponding to different eigenvalues are orthogonal. In matrix form, operators allow real eigenvalues to be found, corresponding to measurements. Orthogonality allows a suitable basis set of vectors to represent the state of the quantum system. The eigenvalues of the operator are also evaluated in the same way as for the square matrix, by solving the characteristic polynomial:\n\nwhere \"I\" is the \"n\" × \"n\" identity matrix, as an operator it corresponds to the identity operator. For a discrete basis:\n\nwhile for a continuous basis:\n\nA non-singular operator formula_36 has an inverse formula_69 defined by:\n\nIf an operator has no inverse, it is a singular operator. In a finite-dimensional space, an operator is non-singular if and only if its determinant is nonzero:\n\nand hence the determinant is zero for a singular operator.\n\nThe operators used in quantum mechanics are collected in the table below (see for example,). The bold-face vectors with circumflexes are not unit vectors, they are 3-vector operators; all three spatial components taken together.\n\nThe procedure for extracting information from a wave function is as follows. Consider the momentum \"p\" of a particle as an example. The momentum operator in position basis in one dimension is:\n\nLetting this act on \"ψ\" we obtain:\n\nif \"ψ\" is an eigenfunction of formula_74, then the momentum eigenvalue \"p\" is the value of the particle's momentum, found by:\n\nFor three dimensions the momentum operator uses the nabla operator to become:\n\nIn Cartesian coordinates (using the standard Cartesian basis vectors e, e, e) this can be written;\n\nthat is:\n\nThe process of finding eigenvalues is the same. Since this is a vector and operator equation, if \"ψ\" is an eigenfunction, then each component of the momentum operator will have an eigenvalue corresponding to that component of momentum. Acting formula_79 on \"ψ\" obtains:\n"}
{"id": "34139758", "url": "https://en.wikipedia.org/wiki?curid=34139758", "title": "Orr's Circle of the Sciences", "text": "Orr's Circle of the Sciences\n\nOrr's Circle of the Sciences was a scientific encyclopedia of the 1850s, published in London by William Somerville Orr. \n\nWilliam S. Orr & Co. was a publisher in Paternoster Row, London. It put out the \"British Cyclopædia\" in ten volumes of the 1830s. It also was in business selling engravings (for example the Kenny Meadows illustrations to Shakespeare), and maps, such as a mid-century \"Cab Fare and Guide Map of London\" (c. 1853).\n\nThe firm was a general commercial publisher, with a specialist area of natural history, and also published periodicals. It was innovative in its use of wood engraving, in its 1838 edition of \"Paul et Virginie\". In children's literature, it published Christoph von Schmid's \"Basket of Flowers\" in an English translation of 1848, in partnership with J. B. Müller of Stuttgart.\n\nOrr himself was a publishers' agent from the 1830s, and was a close associate of Robert and William Chambers. He printed a London edition of \"Chambers's Edinburgh Journal\" by mid-1832. The arrangement used stereotype plates, and brought the circulation up to 50,000. By 1845 the circulation was declining from its peak, and Orr wrote to Chambers explaining that the market was changing. In 1846 Chambers terminated the arrangement with Orr.\n\n\"Punch\" magazine, set up in 1841, brought in Orr to help with distribution to booksellers and news agents. Orr died in 1873.\n\n\"Orr's Circle of the Sciences\" was announced first as a part publication, a series in weekly parts, price 2d. beginning 5 January 1854. The series editor was John Stevenson Bushnan, who also wrote the introductory section of the first volume.\n"}
{"id": "13561805", "url": "https://en.wikipedia.org/wiki?curid=13561805", "title": "Peter's tube-nosed bat", "text": "Peter's tube-nosed bat\n\nPeters's tube-nosed bat (\"Harpiola grisea\") is a species of vesper bat in the family Vespertilionidae, found in the Indian Subcontinent, mainly in the Western Himalayas. They have tube-shaped nostrils (hence the name) which assist them with their feeding. They are brown with white-yellow and underparts and have specks of orange around their neck. While they are roosting, their fur, which seems to appear as a dead plant, camouflages them from predators. They are 3.3-6.0 cm in length and have round heads, large eyes and soft fur. This bat is found in India. They are endangered due to clearing of the rain forests in which they live in and are not protected by the World Conservation Union. They feed on rain forest fruit and blossoms.\n\n"}
{"id": "81984", "url": "https://en.wikipedia.org/wiki?curid=81984", "title": "Pioneer 2", "text": "Pioneer 2\n\nPioneer 2 was the last of the three project Able space probes designed to probe lunar and space. Launch took place at 07:30:00 UTC on November 8, 1958. After \"Pioneer 1\" had failed due to guidance system deficiencies, the guidance system was modified with a Doppler command system to ensure more accurate commands and minimize trajectory errors. Once again, the first and second stage portion of the flight was uneventful, but the third stage of the launch vehicle failed to ignite, making it impossible for \"Pioneer 2\" to achieve orbital velocity. An attempt to fire the vernier engines on the probe was unsuccessful and the spacecraft attained a maximum altitude of before reentering Earth's atmosphere at 28.7 N, 1.9 E over NW Africa. A small amount of data was obtained during the short flight, including evidence that the equatorial region around Earth has higher flux and higher energy radiation than previously considered and that the micrometeorite density is higher around Earth than in space. The reason for the third stage failure was unclear, but it was suspected that the firing command from the second stage, which contained the guidance package for the entire launch vehicle, was never received, possibly due to damage to electrical lines during staging.\n\n\"Pioneer 2\" was nearly identical to \"Pioneer 1\". It consisted of a thin cylindrical midsection with a squat truncated cone frustum on each side. The cylinder was in diameter and the height from the top of one cone to the top of the opposite cone was . Along the axis of the spacecraft and protruding from the end of the lower cone was an solid propellant injection rocket and rocket case, which formed the main structural member of the spacecraft. Eight small low-thrust solid propellant velocity adjustment rockets were mounted on the end of the upper cone in a ring assembly which could be jettisoned after use. A magnetic dipole antenna also protruded from the top of the upper cone. The shell was composed of laminated plastic. The total mass of the spacecraft after vernier separation but before injection rocket firing was .\n\nThe scientific instrument package had a mass of 15.6 kg (34.4 lb) and consisted of an STL image-scanning television system (which replaced the NOTS image scanning infrared television system on Pioneer 1), a proportional counter for radiation measurements, an ionization chamber to measure radiation in space, a diaphragm/microphone assembly to detect micrometeorites, a spin-coil magnetometer to measure magnetic fields to 5 microgauss, and temperature-variable resistors to record spacecraft internal conditions. The spacecraft was powered by nickel-cadmium batteries for ignition of the rockets, silver cell batteries for the television system, and mercury batteries for the remaining circuits. Radio transmission was at 108.06 MHz through a magnetic dipole antenna for the television system, telemetry, and doppler. Ground commands were received at 115 MHz. The spacecraft was to be spin-stabilized at 1.8 revolutions per second, the spin direction approximately perpendicular to the geomagnetic meridian planes of the trajectory.\n\n"}
{"id": "40117483", "url": "https://en.wikipedia.org/wiki?curid=40117483", "title": "Six degrees of separation", "text": "Six degrees of separation\n\nSix degrees of separation is the idea that all living things and everything else in the world are six or fewer steps away from each other so that a chain of \"a friend of a friend\" statements can be made to connect any two people in a maximum of six steps. It was originally set out by Frigyes Karinthy in 1929 and popularized in an eponymous 1990 play written by John Guare. It is sometimes generalized to the average social distance being logarithmic in the size of the population.\n\nTheories on optimal design of cities, city traffic flows, neighborhoods, and demographics were in vogue after World War I. These conjectures were expanded in 1929 by Hungarian author Frigyes Karinthy, who published a volume of short stories titled \"Everything is Different.\" One of these pieces was titled \"Chains,\" or \"Chain-Links.\" The story investigated in abstract, conceptual, and fictional terms many of the problems that would captivate future generations of mathematicians, sociologists, and physicists within the field of network theory. Due to technological advances in communications and travel, friendship networks could grow larger and span greater distances. In particular, Karinthy believed that the modern world was 'shrinking' due to this ever-increasing connectedness of human beings. He posited that despite great physical distances between the globe's individuals, the growing density of human networks made the actual social distance far smaller.\n\nAs a result of this hypothesis, Karinthy's characters believed that any two individuals could be connected through at most five acquaintances. In his story, the characters create a game out of this notion. He wrote:\n\nA fascinating game grew out of this discussion. One of us suggested performing the following experiment to prove that the population of the Earth is closer together now than they have ever been before. We should select any person from the 1.5 billion inhabitants of the Earth – anyone, anywhere at all. He bet us that, using no more than \"five\" individuals, one of whom is a personal acquaintance, he could contact the selected individual using nothing except the network of personal acquaintances.\n\nThis idea both directly and indirectly influenced a great deal of early thought on social networks. Karinthy has been regarded as the originator of the notion of six degrees of separation.\nA related theory deals with the quality of connections, rather than their existence. The theory of three degrees of influence was created by Nicholas A. Christakis and James H. Fowler.\n\nMichael Gurevich conducted seminal work in his empirical study of the structure of social networks in his 1961 Massachusetts Institute of Technology PhD dissertation under Ithiel de Sola Pool. Mathematician Manfred Kochen, an Austrian who had been involved in urban design, extrapolated these empirical results in a mathematical manuscript, \"Contacts and Influences\", concluding that in a U.S.-sized population without social structure, \"it is practically certain that any two individuals can contact one another by means of at most two intermediaries. In a [socially] structured population it is less likely but still seems probable. And perhaps for the whole world's population, probably only one more bridging individual should be needed.\" They subsequently constructed Monte Carlo simulations based on Gurevich's data, which recognized that both weak and strong acquaintance links are needed to model social structure. The simulations, carried out on the relatively limited computers of 1973, were nonetheless able to predict that a more realistic three degrees of separation existed across the U.S. population, foreshadowing the findings of American psychologist Stanley Milgram.\n\nMilgram continued Gurevich's experiments in acquaintanceship networks at Harvard University in Cambridge, Massachusetts, U.S. Kochen and de Sola Pool's manuscript, \"Contacts and Influences\", was conceived while both were working at the University of Paris in the early 1950s, during a time when Milgram visited and collaborated in their research. Their unpublished manuscript circulated among academics for over 20 years before publication in 1978. It formally articulated the mechanics of social networks, and explored the mathematical consequences of these (including the degree of connectedness). The manuscript left many significant questions about networks unresolved, and one of these was the number of degrees of separation in actual social networks. Milgram took up the challenge on his return from Paris, leading to the experiments reported in \"The Small World Problem\" in popular science journal \"Psychology Today\", with a more rigorous version of the paper appearing in Sociometry two years later. The \"Psychology Today\" article generated enormous publicity for the experiments, which are well known today, long after much of the formative work has been forgotten.\n\nMilgram's article made famous his 1967 set of experiments to investigate de Sola Pool and Kochen's \"small world problem.\" Mathematician Benoit Mandelbrot, born in Warsaw, growing up in Poland then France, was aware of the Statist rule of thumb, and was also a colleague of de Sola Pool, Kochen and Milgram at the University of Paris during the early 1950s (Kochen brought Mandelbrot to work at the Institute for Advanced Study and later IBM in the U.S.). This circle of researchers was fascinated by the interconnectedness and \"social capital\" of human networks. Milgram's study results showed that people in the United States seemed to be connected by approximately three friendship links, on average, without speculating on global linkages; he never actually used the term \"six degrees of separation.\" Since the \"Psychology Today\" article gave the experiments wide publicity, Milgram, Kochen, and Karinthy all had been incorrectly attributed as the origin of the notion of six degrees; the most likely popularizer of the term \"six degrees of separation\" would be John Guare, who attributed the value '6' to Marconi.\n\nIn 2003, Columbia University conducted an analogous experiment on social connectedness amongst Internet email users. Their effort was named the Columbia Small World Project, and included 24,163 e-mail chains, aimed at 18 targets from 13 different countries around the world. Almost 100,000 people registered, but only 384 (0.4%) reached the final target. Amongst the successful chains, while shorter lengths were more common some reached their target after only 7, 8, 9 or 10 steps. Dodds et al. noted that participants (all of whom volunteers) were strongly biased towards existing models of Internet users and that connectedness based on professional ties was much stronger than those within families or friendships. The authors cite \"lack of interest\" as the predominating factor in the high attrition rate, a finding consistent with earlier studies.\n\nSeveral studies, such as Milgram's small world experiment, have been conducted to measure this connectedness empirically . The phrase \"six degrees of separation\" is often used as a synonym for the idea of the \"small world\" phenomenon.\n\nHowever, detractors argue that Milgram's experiment did not demonstrate such a link, and the \"six degrees\" claim has been decried as an \"academic urban myth\". Also, the existence of isolated groups of humans, for example the Korubo and other native Brazilian populations, would tend to invalidate the strictest interpretation of the hypothesis.\n\nIn 2001, Duncan Watts, a professor at Columbia University, attempted to recreate Milgram's experiment on the Internet, using an e-mail message as the \"package\" that needed to be delivered, with 48,000 senders and 19 targets (in 157 countries). Watts found that the average (though not maximum) number of intermediaries was around six.\nA 2007 study by Jure Leskovec and Eric Horvitz examined a data set of instant messages composed of 30 billion conversations among 240 million people. They found the average path length among Microsoft Messenger users to be 6.\n\nIt has been suggested by some commentators that interlocking networks of computer mediated lateral communication could diffuse single messages to all interested users worldwide as per the 6 degrees of separation principle via Information Routing Groups, which are networks specifically designed to exploit this principle and lateral diffusion.\n\nBakhshandeh \"et al.\" have addressed the search problem of identifying the degree of separation between two users in social networks such as Twitter. They have introduced new search techniques to provide optimal or near optimal solutions. The experiments are performed using Twitter, and they show an improvement of several orders of magnitude over greedy approaches. Their optimal algorithm finds an average degree of separation of 3.43 between two random Twitter users, requiring an average of only 67 requests for information over the Internet to Twitter. A near-optimal solution of length 3.88 can be found by making an average of 13.3 requests.\n\nNo longer limited strictly to academic or philosophical thinking, the notion of six degrees recently has become influential throughout popular culture. Further advances in communication technology – and particularly the Internet – have drawn great attention to social networks and human interconnectedness. As a result, many popular media sources have addressed the term. The following provide a brief outline of the ways such ideas have shaped popular culture.\n\nAmerican playwright John Guare wrote a play in 1990 and later released a film in 1993 that popularized it. It is Guare's most widely known work.\n\nThe play ruminates upon the idea that any two individuals are connected by at most five others. As one of the characters states:\nI read somewhere that everybody on this planet is separated by only six other people. Six degrees of separation between us and everyone else on this planet. The President of the United States, a gondolier in Venice, just fill in the names. I find it A) extremely comforting that we're so close, and B) like Chinese water torture that we're so close because you have to find the right six people to make the right connection... I am bound to everyone on this planet by a trail of six people.\nGuare, in interviews, attributed his awareness of the \"six degrees\" to Marconi. Although this idea had been circulating in various forms for decades, it is Guare's piece that is most responsible for popularizing the phrase \"six degrees of separation.\" Following Guare's lead, many future television and film sources would later incorporate the notion into their stories.\n\nJ. J. Abrams, the executive producer of television series \"Six Degrees\" and \"Lost\", played the role of Doug in the film adaptation of this play. Many of the play's themes are apparent in his television shows (see below).\n\nThe game \"Six Degrees of Kevin Bacon\" was invented as a play on the concept: the goal is to link any actor to Kevin Bacon through no more than six connections, where two actors are connected if they have appeared in a movie or commercial together. It was created by three students at Albright College in Pennsylvania, who came up with the concept while watching \"Footloose\". On September 13, 2012, Google made it possible to search for any given actor's 'Bacon Number' through their search engine.\n\nUpon the arrival of the 4G mobile network in the United Kingdom, Kevin Bacon appears in several commercials for the EE Network in which he links himself to several well known celebrities and TV shows in the UK.\n\nMusic critics have fun tracing the history and dissecting the popular song \"Der Kommissar\" which was first written and recorded by Austrian musician Falco with German vocals in 1981, then passed on and reworked in English by British band After The Fire in 1982, then lyrically rewritten and renamed \"Deep In The Dark\" by Laura Branigan in 1983 in the U.S., then rerecorded in its traditional style in 2007 by Dale Bozzio, who is the former lead singer of new wave band Missing Persons. In this situation, connections among six such diverse musical expressionists were generated through a song. In the beginning, the composer might never have thought that the song could spread so far. However, the six degree has demonstrated that \"the small world\" does exist.\n\nAn early version involved former world Heavyweight boxing champion, John L. Sullivan, in which people would ask others to \"shake the hand that shook the hand that shook the hand that shook the hand of 'the great John L.'\"\n\n\n\nIn late February 2018, the website www.SixDegreesOfWikipedia.com was published by Jacob Wenger. This site takes any two Wikipedia articles and finds the various hyperlink paths that interconnect the two in the least number of clicks. It then shows each of the steps that were taken to do so and also presents a graphical display of the connections. On March 14, 2018, the site stated that among searches up to that date (~half a million), there have been an average separation of 3.0190°. From these, the number of searches that required six or more degrees was 1.417 percent. It also stated that searches with no connection found was 1.07%, and this was attributed to certain articles being dead ends or having very few links. (Wenger's open source code is available on GitHub, and this enabled other sites to copy the concept, such as degreesofwikipedia.com.)\n\nA Facebook platform application named \"Six Degrees\" was developed by Karl Bunyan, which calculates the degrees of separation between different people. It had over 5.8 million users, as seen from the group's page. The average separation for all users of the application is 5.73 degrees, whereas the maximum degree of separation is 12. The application has a \"Search for Connections\" window to input any name of a Facebook user, to which it then shows the chain of connections. In June 2009, Bunyan shut down the application, presumably due to issues with Facebook's caching policy; specifically, the policy prohibited the storing of friend lists for more than 24 hours, which would have made the application inaccurate. A new version of the application became available at Six Degrees after Karl Bunyan gave permission to a group of developers led by Todd Chaffee to re-develop the application based on Facebook's revised policy on caching data.\n\nThe initial version of the application was built at a Facebook Developers Garage London hackathon with Mark Zuckerberg in attendance.\n\nYahoo! Research Small World Experiment has been conducting an experiment and everyone with a Facebook account can take part in it. According to the research page, this research has the potential of resolving the still unresolved theory of six degrees of separation.\n\nFacebook's data team released two papers in November 2011 which document that amongst all Facebook users at the time of research (721 million users with 69 billion friendship links) there is an average distance of 4.74. Probabilistic algorithms were applied on statistical metadata to verify the accuracy of the measurements. It was also found that 99.91% of Facebook users were interconnected, forming a large connected component.\nFacebook reported that the distance had decreased to 4.57 in February 2016, when it had 1.6 billion users (about 22% of world population).\n\nThe LinkedIn professional networking site operates the degree of separation one is away from a person with which he or she wishes to communicate. On LinkedIn, one's network is made up of 1st-degree, 2nd-degree, and 3rd-degree connections and fellow members of LinkedIn Groups. In addition, LinkedIn notifies the user how many connections they and any other user have in common.\n\nSixDegrees.com was an early social-networking website that existed from 1997 to 2001. It allowed users to list friends, family members and acquaintances, send messages and post bulletin board items to people in their first, second, and third degrees, and see their connection to any other user on the site. At its height, it had 3,500,000 fully registered members. However, it was closed in 2000.\nUsers on Twitter can follow other users creating a network. According to a study of 5.2 billion such relationships by social media monitoring firm Sysomos, the average distance on Twitter is 4.67. On average, about 50% of people on Twitter are only four steps away from each other, while nearly everyone is five steps or less away.\n\nIn another work, researchers have shown that the average distance of 1,500 random users in Twitter is 3.435. They calculated the distance between each pair of users using all the active users in Twitter.\n\nMathematicians use an analogous notion of \"collaboration distance\": two persons are linked if they are coauthors of an article. The collaboration distance with mathematician Paul Erdős is called the Erdős number. Erdős-Bacon numbers and Erdős-Bacon-Sabbath (EBS) numbers are further extensions of the same thinking.\n\nWatts and Strogatz showed that the average path length between two nodes in a random network is equal to , where = total nodes and = acquaintances per node. Thus if\n\nA 2007 article published in \"The Industrial-Organizational Psychologist\", by Jesse S. Michel from Michigan State University, applied Stanley Milgram's small world phenomenon (i.e., “small world problem”) to the field of I-O psychology through co-author publication linkages. Following six criteria, Scott Highhouse (Bowling Green State University professor and fellow of the Society of Industrial and Organizational Psychology) was chosen as the target. Co-author publication linkages were determined for (1) top authors within the I-O community, (2) quasi-random faculty members of highly productive I-O programs in North America, and (3) publication trends of the target. Results suggest that the small world phenomenon is alive and well with mean linkages of 3.00 to top authors, mean linkages of 2.50 to quasi-random faculty members, and a relatively broad and non-repetitive set of co-author linkages for the target. The author then provided a series of implications and suggestions for future research.\n\n\n"}
{"id": "13624853", "url": "https://en.wikipedia.org/wiki?curid=13624853", "title": "The Collapse of Chaos", "text": "The Collapse of Chaos\n\nThe Collapse of Chaos: Discovering Simplicity in a Complex World (1994) is a book about complexity theory and the nature of scientific explanation written by biologist Jack Cohen and mathematician Ian Stewart.\n\nIn this book Cohen and Stewart give their ideas on chaos theory, particularly on how the simple leads to the complex, and conversely, how the complex leads to the simple, and argue for a need for contextual explanation in science as a complement to reduction. This book dovetails with other books written by the Cohen-Stewart team, particularly \"Figments of Reality\".\n\nAs with other Cohen-Stewart books, topics are illustrated with humorous science fiction snippets dealing with a fictional alien intelligence, the Zarathustrians, whom Cohen and Stewart use as metaphors of the human mind itself.\n\n\"Next Generation\" commented, \"Although the book assumes you have zero knowledge of science (and thus is a little patronizing in the early chapters), it presents the concepts of Complexity Theory as well as anything we've seen.\"\n\n"}
{"id": "8219637", "url": "https://en.wikipedia.org/wiki?curid=8219637", "title": "Third-party technique", "text": "Third-party technique\n\nThird-party technique is a marketing strategy employed by public relations (PR) firms, that involves placing a premeditated message in the \"mouth of the media.\" Third-party technique can take many forms, ranging from the hiring of journalists to report the organization in a favorable light, to using scientists within the organization to present their perhaps prejudicial findings to the public.\n\nIndustry-sponsored groups used to relay these findings to the public are known as front groups. These groups claim to represent the general public’s agenda, when in reality they are facilitating the hidden interests of the organizations that are sponsoring them. Also related are astroturf groups, which are groups that have been formed by the industry, yet appear to have been formed by ordinary citizens.\n\nEdward Bernays, the “father of public relation,” is also said to be the founder of using front groups as marketing techniques. Bernays qualified the use of a reputable and prominent sponsoring groups in the statement, “The most useful method in a multiple society like ours to indicate the support of an idea of the many varied elements that make up our society. Opinion leaders and group leaders have an effect in a democracy and stand as symbols to their constituency.\"\n\nThe recent rise of the use of third-party technique has spurred the creation of groups such as PR Watch, an organization that investigates and combats manipulative and misleading PR messages.\nThird-party technique is often found within the drug and pharmaceutical industry. Journalists that may mistakenly report on these front groups' findings can unknowingly perpetuate the spread of biased information. The result is the perpetual exposure of doctors and patients to these carefully crafted messages.\n\nIn defense of the practice of third-party technique, public relations firms often qualify the use of a third-party to report on information as not putting words in the mouth of the media, but simply as mechanisms to display accurate information in a manner that the public will not see as biased towards the organization. \n\"Developing third party support and validation for the basic risk messages of the corporation is essential. This support should ideally come from medical authorities, political leaders, union officials, relevant academics, fire and police officials, environmentalists, regulators,” says Amanda Little from the world’s fifth largest PR firm, Burson-Marsteller.\n\n"}
