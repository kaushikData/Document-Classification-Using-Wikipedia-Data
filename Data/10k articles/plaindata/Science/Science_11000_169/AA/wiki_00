{"id": "13167630", "url": "https://en.wikipedia.org/wiki?curid=13167630", "title": "Accretion (coastal management)", "text": "Accretion (coastal management)\n\nAccretion is the process of coastal sediment returning to the visible portion of a beach or foreshore following a submersion event. A sustainable beach or foreshore often goes through a cycle of submersion during rough weather then accretion during calmer periods. If a coastline is not in a healthy sustainable state, then erosion can be more serious and accretion does not fully restore the original volume of the visible beach or foreshore leading to permanent beach loss.\n"}
{"id": "3164143", "url": "https://en.wikipedia.org/wiki?curid=3164143", "title": "Anala Mons", "text": "Anala Mons\n\nAnala Mons is a volcano on Venus. It is named after Anala, a Hindu fertility goddess. The feature was originally named Anala Corona. It is located at 11.0°N 14.1°E, in a region called the Sappho Patera quadrangle where numerous other volcanic features can be found.\n\n"}
{"id": "1975073", "url": "https://en.wikipedia.org/wiki?curid=1975073", "title": "Aquatic ecosystem", "text": "Aquatic ecosystem\n\nAn aquatic ecosystem is an ecosystem in a body of water. Communities of organisms that are dependent on each other and on their environment live in aquatic ecosystems. The two main types of aquatic ecosystems are marine ecosystems and freshwater ecosystems. \n\nMarine ecosystems, the largest of all ecosystems, cover approximately 71% of the Earth's surface and contain approximately 97% of the planet's water. They generate 32% of the world's net primary production. They are distinguished from freshwater ecosystems by the presence of dissolved compounds, especially salts, in the water. Approximately 85% of the dissolved materials in seawater are sodium and chlorine. Seawater has an average salinity of 35 parts per thousand of water. Actual salinity varies among different marine ecosystems.\n\nMarine ecosystems can be divided into many zones depending upon water depth and shoreline features. The oceanic zone is the vast open part of the ocean where animals such as whales, sharks, and tuna live. The benthic zone consists of substrates below water where many invertebrates live. The intertidal zone is the area between high and low tides; in this figure it is termed the littoral zone. Other near-shore (neritic) zones can include estuaries, salt marshes, coral reefs, lagoons and mangrove swamps. In the deep water, hydrothermal vents may occur where chemosynthetic sulfur bacteria form the base of the food web.\n\nClasses of organisms found in marine ecosystems include brown algae, dinoflagellates, corals, cephalopods, echinoderms, and sharks. Fishes caught in marine ecosystems are the biggest source of commercial foods obtained from wild populations.\n\nEnvironmental problems concerning marine ecosystems include unsustainable exploitation of marine resources (for example overfishing of certain species), marine pollution, climate change, and building on coastal areas.\n\nFreshwater ecosystems cover 0.78% of the Earth's surface and inhabit 0.009% of its total water. They generate nearly 3% of its net primary production. Freshwater ecosystems contain 41% of the world's known fish species.\n\nThere are three basic types of freshwater ecosystems:\n\nLake ecosystems can be divided into zones. One common system divides lakes into three zones (see figure). The first, the littoral zone, is the shallow zone near the shore. This is where rooted wetland plants occur. The offshore is divided into two further zones, an open water zone and a deep water zone. In the open water zone (or photic zone) sunlight supports photosynthetic algae, and the species that feed upon them. In the deep water zone, sunlight is not available and the food web is based on detritus entering from the littoral and photic zones. Some systems use other names. The off shore areas may be called the pelagic zone, the photic zone may be called the limnetic zone and the aphotic zone may be called the profundal zone. Inland from the littoral zone one can also frequently identify a riparian zone which has plants still affected by the presence of the lake—this can include effects from windfalls, spring flooding, and winter ice damage. The production of the lake as a whole is the result of production from plants growing in the littoral zone, combined with production from plankton growing in the open water.\n\nWetlands can be part of the lentic system, as they form naturally along most lake shores, the width of the wetland and littoral zone being dependent upon the slope of the shoreline and the amount of natural change in water levels, within and among years. Often dead trees accumulate in this zone, either from windfalls on the shore or logs transported to the site during floods. This woody debris provides important habitat for fish and nesting birds, as well as protecting shorelines from erosion.\n\nTwo important subclasses of lakes are ponds, which typically are small lakes that intergrade with wetlands, and water reservoirs. Over long periods of time, lakes, or bays within them, may gradually become enriched by nutrients and slowly fill in with organic sediments, a process called succession. When humans use the watershed, the volumes of sediment entering the lake can accelerate this process. The addition of sediments and nutrients to a lake is known as eutrophication.\n\nPonds are small bodies of freshwater with shallow and still water, marsh, and aquatic plants. They can be further divided into four zones: vegetation zone, open water, bottom mud and surface film. The size and depth of ponds often varies greatly with the time of year; many ponds are produced by spring flooding from rivers. Food webs are based both on free-floating algae and upon aquatic plants. There is usually a diverse array of aquatic life, with a few examples including algae, snails, fish, beetles, water bugs, frogs, turtles, otters and muskrats. Top predators may include large fish, herons, or alligators. Since fish are a major predator upon amphibian larvae, ponds that dry up each year, thereby killing resident fish, provide important refugia for amphibian breeding. Ponds that dry up completely each year are often known as vernal pools. Some ponds are produced by animal activity, including alligator holes and beaver ponds, and these add important diversity to landscapes.\n\nThe major zones in river ecosystems are determined by the river bed's gradient or by the velocity of the current. Faster moving turbulent water typically contains greater concentrations of dissolved oxygen, which supports greater biodiversity than the slow moving water of pools. These distinctions form the basis for the division of rivers into upland and lowland rivers. The food base of streams within riparian forests is mostly derived from the trees, but wider streams and those that lack a canopy derive the majority of their food base from algae. Anadromous fish are also an important source of nutrients. Environmental threats to rivers include loss of water, dams, chemical pollution and introduced species. A dam produces negative effects that continue down the watershed. The most important negative effects are the reduction of spring flooding, which damages wetlands, and the retention of sediment, which leads to loss of deltaic wetlands.\n\nWetlands are dominated by vascular plants that have adapted to saturated soil. There are four main types of wetlands: swamp, marsh, fen and bog (both fens and bogs are types of mire). Wetlands are the most productive natural ecosystems in the world because of the proximity of water and soil. Hence they support large numbers of plant and animal species. Due to their productivity, wetlands are often converted into dry land with dykes and drains and used for agricultural purposes. The construction of dykes, and dams, has negative consequences for individual wetlands and entire watersheds. Their closeness to lakes and rivers means that they are often developed for human settlement. Once settlements are constructed and protected by dykes, the settlements then become vulnerable to land subsidence and ever increasing risk of flooding. The Louisiana coast around New Orleans is a well-known example; the Danube Delta in Europe is another.\n\nAquatic ecosystems perform many important environmental functions. For example, they recycle nutrients, purify water, attenuate floods, recharge ground water and provide habitats for wildlife. Aquatic ecosystems are also used for human recreation, and are very important to the tourism industry, especially in coastal regions.\n\nThe health of an aquatic ecosystem is degraded when the ecosystem's ability to absorb a stress has been exceeded. A stress on an aquatic ecosystem can be a result of physical, chemical or biological alterations of the environment. Physical alterations include changes in water temperature, water flow and light availability. Chemical alterations include changes in the loading rates of biostimulatory nutrients, oxygen consuming materials, and toxins. Biological alterations include over-harvesting of commercial species and the introduction of exotic species. Human populations can impose excessive stresses on aquatic ecosystems. There are many examples of excessive stresses with negative consequences. Consider three. The environmental history of the Great Lakes of North America illustrates this problem, particularly how multiple stresses, such as water pollution, over-harvesting and invasive species can combine. The Norfolk Broadlands in England illustrate similar decline with pollution and invasive species. Lake Pontchartrain along the Gulf of Mexico illustrates the negative effects of different stresses including levee construction, logging of swamps, invasive species and salt water intrusion.\n\nAn ecosystem is composed of biotic communities that are structured by biological interactions and abiotic environmental factors. Some of the important abiotic environmental factors of aquatic ecosystems include substrate type, water depth, nutrient levels, temperature, salinity, and flow. It is often difficult to determine the relative importance of these factors without rather large experiments. There may be complicated feedback loops. For example, sediment may determine the presence of aquatic plants, but aquatic plants may also trap sediment, and add to the sediment through peat.\n\nThe amount of dissolved oxygen in a water body is frequently the key substance in determining the extent and kinds of organic life in the water body. Fish need dissolved oxygen to survive, although their tolerance to low oxygen varies among species; in extreme cases of low oxygen some fish even resort to air gulping. Plants often have to produce aerenchyma, while the shape and size of leaves may also be altered. Conversely, oxygen is fatal to many kinds of anaerobic bacteria.\n\nNutrient levels are important in controlling the abundance of many species of algae. The relative abundance of nitrogen and phosphorus can in effect determine which species of algae come to dominate. Algae are a very important source of food for aquatic life, but at the same time, if they become over-abundant, they can cause declines in fish when they decay. Similar over-abundance of algae in coastal environments such as the Gulf of Mexico produces, upon decay, a hypoxic region of water known as a dead zone.\n\nThe salinity of the water body is also a determining factor in the kinds of species found in the water body. Organisms in marine ecosystems tolerate salinity, while many freshwater organisms are intolerant of salt. The degree of salinity in an estuary or delta is an important control upon the type of wetland (fresh, intermediate, or brackish), and the associated animal species. Dams built upstream may reduce spring flooding, and reduce sediment accretion, and may therefore lead to saltwater intrusion in coastal wetlands.\n\nFreshwater used for irrigation purposes often absorbs levels of salt that are harmful to freshwater organisms.\n\nThe biotic characteristics are mainly determined by the organisms that occur. For example, wetland plants may produce dense canopies that cover large areas of sediment—or snails or geese may graze the vegetation leaving large mud flats. Aquatic environments have relatively low oxygen levels, forcing adaptation by the organisms found there. For example, many wetland plants must produce aerenchyma to carry oxygen to roots. Other biotic characteristics are more subtle and difficult to measure, such as the relative importance of competition, mutualism or predation. There are a growing number of cases where predation by coastal herbivores including snails, geese and mammals appears to be a dominant biotic factor.\n\nAutotrophic organisms are producers that generate organic compounds from inorganic material. Algae use solar energy to generate biomass from carbon dioxide and are possibly the most important autotrophic organisms in aquatic environments. The more shallow the water, the greater the biomass contribution from rooted and floating vascular plants. These two sources combine to produce the extraordinary production of estuaries and wetlands, as this autotrophic biomass is converted into fish, birds, amphibians and other aquatic species.\n\nChemosynthetic bacteria are found in benthic marine ecosystems. These organisms are able to feed on hydrogen sulfide in water that comes from volcanic vents. Great concentrations of animals that feed on these bacteria are found around volcanic vents. For example, there are giant tube worms (\"Riftia pachyptila\") 1.5 m in length and clams (\"Calyptogena magnifica\") 30 cm long.\n\nHeterotrophic organisms consume autotrophic organisms and use the organic compounds in their bodies as energy sources and as raw materials to create their own biomass. Euryhaline organisms are salt tolerant and can survive in marine ecosystems, while stenohaline or salt intolerant species can only live in freshwater environments.\n\n\n"}
{"id": "49204690", "url": "https://en.wikipedia.org/wiki?curid=49204690", "title": "Bernard Crespi", "text": "Bernard Crespi\n\nBernard J. Crespi is a professor of evolutionary biology at Simon Fraser University. His research focuses on social evolution across multiple scales, using genetic and ecological approaches. He is one of the initiators of the imprinted brain theory.\n\nIn 2010, he was elected as a fellow of the Royal Society of Canada.\n"}
{"id": "34001310", "url": "https://en.wikipedia.org/wiki?curid=34001310", "title": "Bibliography of science and technology in Canada", "text": "Bibliography of science and technology in Canada\n\nThis is a bibliography on the history of science and technology in Canada.\n\n\n\n\n\n"}
{"id": "1810201", "url": "https://en.wikipedia.org/wiki?curid=1810201", "title": "Biclustering", "text": "Biclustering\n\nBiclustering, block clustering\nThe term was first introduced by Boris Mirkin to name a technique introduced many years earlier, in 1972, by J. A. Hartigan.\n\nGiven a set of formula_1 samples represented by an formula_2-dimensional feature vector, the entire dataset can be represented as formula_1 rows in formula_2 columns (i.e., an formula_5 matrix). The biclustering algorithm generates biclusters – a subset of rows which exhibit similar behavior across a subset of columns, or vice versa.\n\nBiclustering was originally introduced by J. A. Hartigan in 1972. The term biclustering was later used by Mirkin. This algorithm was not generalized until 2000 when Y. Cheng and G. M. Church proposed a biclustering algorithm based on variance and applied it to biological gene expression data. Their paper is still the most important literature in the gene expression biclustering field.\n\nIn 2001 and 2003, I.S. Dhillon put forward two algorithms applying biclustering to files and words. One version was based on bipartite spectral graph partitioning. The other was based on information theory. Dhillon assumed the loss of mutual information during biclustering was equal to the Kullback–Leibler-distance (KL-distance) between P and Q. P means the distribution of files and feature words before biclustering. Q is the distribution after biclustering. KL-distance is for measuring the difference between two random distributions. KL = 0 when the two distributions are the same and KL increases as the difference increases. Thus, the aim of the algorithm was to find the minimum KL-distance between P and Q. In 2004, Arindam Banerjee used a weighted-Bregman distance instead of KL-distance to design a biclustering algorithm which was suitable for any kind of matrix, unlike the KL-distance algorithm.\n\nTo cluster more than two types of objects, in 2005, Bekkerman expanded the mutual information in Dhillon's theorem from a single pair into multiple pairs.\n\nThe complexity of the biclustering problem depends on the exact problem formulation, and particularly on the merit function used to evaluate the quality of a given bicluster. However most interesting variants of this problem are NP-complete. NP-complete have two conditions. In the simple case that there is only element \"a\" either 0 or 1 in the binary matrix A, a bicluster is equal to a biclique in the corresponding bipartite graph. The maximum size bicluster is equivalent to maximum edge biclique in bipartite graph. In the complex case, the element in matrix A is used to compute the quality of a given bicluster and solve the more restricted version of the problem. It requires either large computational effort or the use of lossy heuristics to short-circuit the calculation.\n\nDifferent biclustering algorithms have different definitions of bicluster.\n\nThey are:\n\n\n1.Bicluster with constant values\n\nWhen a biclustering algorithm tries to find a constant bicluster, the normal way for it is to reorder the rows and columns of the matrix so it can group together similar rows/columns and find biclusters with similar values. This method is OK when the data is tidy. But as the data can be noisy most of the times, so it can’t satisfy us. More sophisticated methods should be used.\nA perfect constant bicluster is a matrix(I,J) where all values a(i,j) are equal to μ. In real data, a(i,j) can be seen as n(i,j) +μ where n(i,j) is the noise. \nAccording to Hartigan’s algorithm, by splitting the original data matrix into a set of biclusters, variance is used to compute constant biclusters. So, a perfect bicluster is a matrix with variance zero. Also, in order to prevent the partitioning of the data matrix into biclusters with only one row and one column, Hartigan assumes that there are K biclusters within the data matrix. When the data matrix is partitioned into K biclusters, the algorithm ends.\n\n2.Biclusters with constant values on rows or columns\n\nThis kind of biclusters can’t be evaluated just by variance of its values. To finish the identification, the columns and the rows should be normalized at first. There are other algorithms, without normalization step, can find biclusters have rows and columns with different approaches.\n\n3.Biclusters with coherent values\n\nFor biclusters with coherent values on rows and columns, an overall improvement over the algorithms for biclusters with constant values on rows or on columns should be considered. \nThat means a sophisticated algorithm is needed. This algorithm may contain analysis of variance between groups, using co-variance between both rows and columns. In Cheng and Church's theorem, a bicluster is defined as a subset of rows and columns with almost the same score. The similarity score is used to measure the coherence of rows and columns.\n\nThe relationship between these cluster models and other types of clustering such as correlation clustering is discussed in.\n\nThere are many biclustering algorithms developed for bioinformatics, including: block clustering, CTWC (Coupled Two-Way Clustering), ITWC (Interrelated Two-Way Clustering), δ-bicluster, δ-pCluster, δ-pattern, FLOC, OPC, Plaid Model, OPSMs (Order-preserving submatrixes), Gibbs, SAMBA (Statistical-Algorithmic Method for Bicluster Analysis), Robust Biclustering Algorithm (RoBA), Crossing Minimization, cMonkey, PRMs, DCC, LEB (Localize and Extract Biclusters), QUBIC (QUalitative BIClustering), BCCA (Bi-Correlation Clustering Algorithm) BIMAX, ISA and FABIA (Factor Analysis for Bicluster Acquisition). Biclustering algorithms have also been proposed and used in other application fields under the names coclustering, bidimensional clustering, and subspace clustering.\n\nGiven the known importance of discovering local patterns in time-series data, recent proposals have addressed the biclustering problem in the specific case of time series gene expression data. In this case, the interesting biclusters can be restricted to those with contiguous columns. This restriction leads to a tractable problem and enables the development of efficient exhaustive enumeration algorithms such as CCC-Biclustering and \"e\"-CCC-Biclustering. \nThe approximate patterns in CCC-Biclustering algorithms allow a given number of errors, per gene, relatively to an expression profile representing the expression pattern in the bicluster. The e-CCC-Biclustering algorithm uses approximate expressions to find and report all maximal CCC-Biclusters by a discretized matrix A and efficient string processing techniques.\n\nThese algorithms find and report all maximal biclusters with coherent and contiguous columns with perfect/approximate expression patterns, in time linear/polynomial which is obtained by manipulating a discretized version of original expression matrix in the size of the time series gene expression matrix using efficient string processing techniques based on suffix trees. These algorithms are also applied to solve problems and sketch the analysis of computational complexity.\n\nSome recent algorithms have attempted to include additional support for biclustering rectangular matrices in the form of other datatypes, including cMonkey.\n\nThere is an ongoing debate about how to judge the results of these methods, as biclustering allows overlap between clusters and some algorithms allow the exclusion of hard-to-reconcile columns/conditions. Not all of the available algorithms are deterministic and the analyst must pay attention to the degree to which results represent stable minima. Because this is an unsupervised classification problem, the lack of a gold standard makes it difficult to spot errors in the results. One approach is to utilize multiple biclustering algorithms, with majority or super-majority voting amongst them deciding the best result. Another way is to analyse the quality of shifting and scaling patterns in biclusters. Biclustering has been used in the domain of text mining (or classification) where it is popularly known as co-clustering \n. Text corpora are represented in a vectorial form as a matrix D whose rows denote the documents and whose columns denote the words in the dictionary. Matrix elements D denote occurrence of word j in document i. Co-clustering algorithms are then applied to discover blocks in D that correspond to a group of documents (rows) characterized by a group of words(columns).\n\nTest clustering can solve the high-dimensional sparse problem, which means clustering text and words at the same time. When clustering text, we need to think about not only the words information, but also the information of words clusters that was composed by words. Then according to similarity of feature words in the text, will eventually cluster the feature words. This is called co-clustering. There are two advantages of co-clustering: one is clustering the test based on words clusters can extremely decrease the dimension of clustering, it can also appropriate to measure the distance between the tests. Second is mining more useful information and can get the corresponding information in test clusters and words clusters. This corresponding information can be used to describe the type of texts and words, at the same time, the result of words clustering can be also used to text mining and information retrieval.\n\nSeveral approaches have been proposed based on the information contents of the resulting blocks: matrix-based approaches such as SVD and BVD, and graph-based approaches. Information-theoretic algorithms iteratively assign each row to a cluster of documents and each column to a cluster of words such that the mutual information is maximized. Matrix-based methods focus on the decomposition of matrices into blocks such that the error between the original matrix and the regenerated matrices from the decomposition is minimized. Graph-based methods tend to minimize the cuts between the clusters. Given two groups of documents d and d, the number of cuts can be measured as the number of words that occur in documents of groups d and d.\n\nMore recently (Bisson and Hussain) have proposed a new approach of using the similarity between words and the similarity between documents to co-cluster the matrix. Their method (known as χ-Sim, for cross similarity) is based on finding document-document similarity and word-word similarity, and then using classical clustering methods such as hierarchical clustering. Instead of explicitly clustering rows and columns alternately, they consider higher-order occurrences of words, inherently taking into account the documents in which they occur. Thus, the similarity between two words is calculated based on the documents in which they occur and also the documents in which \"similar\" words occur. The idea here is that two documents about the same topic do not necessarily use the same set of words to describe it but a subset of the words and other similar words that are characteristic of that topic. This approach of taking higher-order similarities takes the latent semantic structure of the whole corpus into consideration with the result of generating a better clustering of the documents and words.\n\nIn text databases, for a document collection defined by a document by term D matrix (of size m by n, m: number of documents, n: number of terms) the cover-coefficient based clustering methodology yields the same number of clusters both for documents and terms (words) using a double-stage probability experiment. According to the cover coefficient concept number of clusters can also be roughly estimated by the following formula formula_6 where t is the number of non-zero entries in D. Note that in D each row and each column must contain at least one non-zero element.\n\nIn contrast to other approaches, FABIA is a multiplicative model that assumes realistic non-Gaussian signal distributions with heavy tails. FABIA utilizes well understood model selection techniques like variational approaches and applies the Bayesian framework. The generative framework allows FABIA to determine the information content of each bicluster to separate spurious biclusters from true biclusters.\n\n\n\n"}
{"id": "4458567", "url": "https://en.wikipedia.org/wiki?curid=4458567", "title": "Big Pharma (book)", "text": "Big Pharma (book)\n\nBig Pharma: How the World's Biggest Drug Companies Control Illness is a 2006 book by British journalist Jacky Law. The book examines how major pharmaceutical companies determine which health care problems are publicised and researched.\n\nOutlining the history of the pharmaceutical industry, Law identifies what she says is the failure of a regulatory framework that assumes pharmaceutical companies always produce worthwhile products that society will want.\n\nLaw has written about healthcare for 25 years, seven of them as associate editor of \"Scrip Magazine\", a monthly magazine for the drugs industry.\n\n\n"}
{"id": "31225533", "url": "https://en.wikipedia.org/wiki?curid=31225533", "title": "Clarence Petersen de la Motte", "text": "Clarence Petersen de la Motte\n\nClarence Petersen de la Motte (born 1892, date of death unknown), commonly C. P. de la Motte, was a sailor originally from Bulli, New South Wales. During his early career, he served aboard the New Zealand barque \"Northern Chief\" and the steamship \"Warrimoo\". From 1911 to 1914, during the Australasian Antarctic Expedition, he served as Third Officer aboard the SY \"Aurora\", under John King Davis. Expedition Commander Douglas Mawson named Cape De la Motte, in George V Land, after him. De la Motte joined the \"Aurora\" again in 1916, and served as First Officer during the rescue of the Ross Sea component of the Imperial Trans-Antarctic Expedition, for which he was later awarded the Polar Medal.\n"}
{"id": "492996", "url": "https://en.wikipedia.org/wiki?curid=492996", "title": "Cultural turn", "text": "Cultural turn\n\nThe cultural turn is a movement beginning in the early 1970s among scholars in the humanities and social sciences to make culture the focus of contemporary debates; it also describes a shift in emphasis toward \"meaning\" and away from a positivist epistemology. The cultural turn is described in 2005 by Lynette Spillman and Mark D. Jacobs as \"one of the most influential trends in the humanities and social sciences in the last generation\". A prominent historiographer argues that the cultural turn involved a \"wide array of new theoretical impulses coming from fields formerly peripheral to the social sciences\", especially post-structuralism, cultural studies, literary criticism, and various forms of linguistic analysis, which emphasized \"the causal and socially constitutive role of cultural processes and systems of signification\".\n\nThe cultural turn in the late 20th century is interpreted as referring to either a substantive shift in society or an analytical shift in academia. The former argues that culture plays a more significant role in advanced societies, which fits with the notion of post-modernity as an historical era in which people \"emphasizes the importance of art and culture for education, moral growth, and social criticism and change\". The latter is movement within academia to place the concept of culture, and the related notions of \"meaning\", \"cognition\", \"affect\", and \"symbols\" at the center of methodological and theoretical focus. Some argue that the analytical shift is endogenous to the substantive shift.\n\nCulture can be defined as \"the social process whereby people communicate meanings, make sense of their world, construct their identities, and define their beliefs and values\". Or, for Georg Simmel, culture refers to \"the cultivation of individuals through the agency of external forms which have been objectified in the course of history\". Thus culture can be interpreted on a spectrum from purely individualistic solipsism to objective forms of social organization and interaction.\n\nOne of the earliest works in which the term \"cultural turn\" showed up was Jeffrey C. Alexander's chapter \"The New Theoretical Movement\" in Neil Smelser's \"Handbook of Sociology\" (1988). Alexander, rightly pointed out that the origins of the cultural turn should be traced in the debate of idealism and materialism i.e. Marx and Hegel. Prior to the labeling of the movement, in the 1970s, \"foundational works underlying and facilitating the turn to cultural forms of analysis\" emerged: Hayden White's \"\" (1973), Clifford Geertz's \"The Interpretation of Cultures: Selected Essays\" (1973), Michel Foucault's \"Discipline and Punish\" (1977), and Pierre Bourdieu's \"Outline of a Theory of Practice\" (1977).\n\nWhile the earlier twentieth century experienced a linguistic turn, mostly brought about by the thought of Ludwig Wittgenstein and Ferdinand de Saussure, the cultural turn of the late twentieth century absorbed those criticisms and built on them.\n\nThe cultural turn has helped cultural studies to gain more respect as an academic discipline. With the shift away from high arts the discipline has increased its perceived importance and influence on other disciplines.\n\nBritish historian Heather Jones argues that the historiography of the First World War has been reinvigorated by the cultural turn in recent years. Scholars have raised entirely new questions regarding military occupation, radicalizion of politics, race, and the male body.\n\nThe cultural turn as an historical era that breaks substantively with the past is only tangentially related to cultural turn as analytical shift. Proponents of the former argue that:\n\nAdvertising, amateur photography, yellow journalism and an assortment of other forms of media arose after the politically charged 1960s. Moreover, this media was multicultural, and attempted to target all races, ethnicities and age groups, as opposed to more exclusive media prior to the 1960s. This \"new media\" of a postmodern America brought about an expansion and differentiation of culture, which has only been rapidly expanded by the Internet and social media.\n\nIn recent years, there has been something of a resurgence in rural studies, which has become somewhat more mainstream than previously in the academic space of social science. Increasing numbers of people have taken on important dualistic questions of society/space, nature/culture structure/agency and self/other from the perspective of rural studies. However, it is the 'cultural turn' in wider social science which has lent both respectability and excitement to the nexus with rurality, particularly with new foci on landscape, otherness and the spatiality of nature. With a conceptual fascination with difference, and a methodological fascination with ethnography, cultural studies have provided a significant palimpsestual overlay onto existing landscapes of knowledge.\n\n"}
{"id": "55268742", "url": "https://en.wikipedia.org/wiki?curid=55268742", "title": "Daimyo Clock Museum", "text": "Daimyo Clock Museum\n\nThe Daimyo Clock Museum (大名時計博物館) is a small community-run museum in Yanaka 2-chōme, Tokyo. The museum was established in 1972 to display Japanese clocks from the Edo period collected by Sakujiro (known as \"Guro\") Kamiguchi (1892–1970). It is the only museum in Japan exhibiting Japanese clocks.\n\nSakujiro Kamiguchi owned a highly-unusual log cabin shop which sold western clothing. The shop became known locally as \"Grotesque\", and this was the origin of Kamiguchi's nickname, \"Guro\". Kamiguchi had many interests, including pottery. He first became interested in Japanese clocks when he came across an English-made watch with an attached sundial in a local shop. Kamiguchi realised the unique cultural importance of daimyo clocks: \n\nIn 1951, Kamuguchi established the Kamiguchi Japanese Clock Preservation Society, and gifted his collection to it. After his death in 1970, his son Hitoshi Kamiguchi became President of the Society and opened the museum in April 1972.\n\n\"Daimyōs\" ('great lords') were the feudal aristocracy of Japan in the Edo period and they were the only people who could afford expensive timepieces. The museum displays mechanical clocks, sundials and incense clocks previously owned by daimyo families. There are around 50 pieces on display from the collection's total of some 200 items, in a single 83 square metre room. The museum's labels are all in Japanese only, though an English-language pamphlet explaining the traditional Japanese timekeeping system is also available.\n\nThe nearest metro station is Nezu on the Chiyoda line. The nearest Japan Rail station is Nippori.\n\n"}
{"id": "34288348", "url": "https://en.wikipedia.org/wiki?curid=34288348", "title": "Economics of science", "text": "Economics of science\n\nThe economics of science aims to understand the impact of science on the advance of \ntechnology, to explain the behavior of scientists, and to understand the efficiency or \ninefficiency of scientific institutions and markets.\n\nThe importance of the economics as science is substantially due to the importance of science as a driver of technology and technology as a driver of productivity and growth. Believing that science matters, economists have attempted to understand the behavior of scientists and the \noperation of scientific institutions.\n\nEconomists consider “science” as the search and production of knowledge using known starting conditions. Knowledge can be considered a public good, due to the fact that its utility to society is not diminished with additional consumption (non-rivalry), and once the knowledge is shared with the public it becomes very hard to restrict access to it or use of it (non-excludable). Traditional public economic theory asserts that competitive markets provide poor incentives for production of a public good because the producers cannot reap the benefits of use of their product, and thus costs will be higher than benefits. Economists have identified several possible reasons as to why producers of science might determine that the private costs they incur in the production process are larger than the benefits that they intend to reap, even though the benefits to society are greater than these costs. Firstly, the technological barriers to production are extremely high, which makes the market very risky. Technological barriers refer to the cost of research and development of new scientific knowledge, which becomes increasingly expensive as technology continues to play a more prominent role in this type of development. Secondly, due to the non-excludable nature of scientific knowledge, producers worry that they will be unable to enforce property rights on their produced goods. This will result in others being able to benefit from the scientific knowledge without having to bear the cost of the research and development, which would in turn make the potential return on investment too small to incentivize participation in the market. Therefore, science can be understood as the production of a public good, and can be studied within the framework of public economics.\n\nHowever, certain economists argue that a non-market mechanism has developed to correct the problem of indefinable property rights, such that scientists are incentivized to produce knowledge in a socially responsible way. Economist Paula Stephen refers to this mechanism as a reward system based primarily on a concept that she calls “priority of discovery.” Robert Merton argues that to goal of scientists is to establish “priority of discovery” by being the first to report a new discovery, which then results in the reward of recognition. The scientific community only bestows this reward on the person who discovers the new piece of knowledge first, and thus this sets up a winner-takes-all type of system that incentivizes producers to participate in the market of scientific knowledge. Stephen particularly notes that “Compensation in science is generally composed of two parts: one portion is paid regardless of the individual's success in races, the other is priority-based and reflects the value of the winner's contribution to science.” The first part that Stephens identifies corresponds to the salary that a professor in academia would expect to make over the course of his or her career; these salaries are notoriously flat, with one study noting that a full professor can expect to make only 70% more than a newly hired assistant professor. However, Stephen argues that the second part of compensation, that which is reaped when a scientist establishes priority of discovery, then the earnings profile becomes much less flat as the scientist gains prestige, journalistic citations, paid speaking invitations, and other such rewards. However, she notes that this theory had yet to be empirically tested at the time of writing. Furthermore, her analysis only applies to the world of academia, whereas industry is also a major source of scientific knowledge production.\n\nThe field of public economics posits that should market failures occur, the government might be able to intervene to correct these market failures. When speaking about the production of scientific knowledge, the government has several options for intervening in the market to attempt to correct the failure. In the United States, two of the most historically popular and most extensively studied options are the patent system and tax incentives.\n\nIn the United States, the Patent and Trademark Office issues patents that give the holder of the patent exclusive, defined property rights to their product for 20 years. From an economic perspective, the value of the patent is that it increases the marginal benefit of the firm that is producing the scientific knowledge. To graphically display this concept, the accompanying figure depicts the marginal benefit and marginal cost curves of a firm in the market for science. The vertical axis displays the marginal cost and marginal benefit of each additional dollar spent on research and development. The horizontal access displays the amount of money spent on research and development in total. Research and development is assumed to have diminishing returns. For simplicity’s sake, all curves are assumed to be linear, and the marginal cost curve is assumed to be constant. A firm will maximize their profits by producing where marginal cost intersects marginal benefit. In the absence of any government intervention, the firm will produce where at RD, where private marginal benefit (MB) intersects MC. However, if scientific knowledge is assumed to be a public good, then RD is too low a quantity to satisfy the social need. The optimal amount of R&D is at RD. The value of the introduction of the patent system is that it allows the marginal benefit curve for the firm to shift upward to MB so that the private benefit to the firm now produces the socially optimal quantity. The additional revenue is collected from society, as society now pays higher prices for the knowledge given the monopoly power of the producing firm.\n\nIn practice, patent law has been correlated with increased R&D expenditure, indicating that this form of government intervention is in fact incentivizing production. However, this type of government intervention does not allow particularly precise targeting of the optimal level of R&D production, and several economists argue that the benefit of 20 years of monopoly power is too high. This argument has particular relevance to current debates regarding the production of life-saving pharmaceuticals.\n\nIn 1954, the Internal Revenue Service incorporated an exemption for research costs such that firms could have research costs deducted from their yearly taxes. From an economic perspective, the value of the tax incentive is that it decreases the marginal cost of the firm that is producing the scientific knowledge. To graphically display this concept, the accompanying figure depicts the marginal benefit and marginal cost curves of a firm in the market for science. The vertical axis displays the marginal cost and benefit of each additional dollar spent on research and development. The horizontal access displays the amount of money spent on research and development in total. Research and development is assumed to have diminishing rate of return. For simplicity’s sake, all curves are assumed to be linear, and the marginal cost curve is assumed to be constant. A firm will maximize their profits by producing where marginal cost intersects marginal benefit. In the absence of any government intervention, the firm will produce where at RD0, where private marginal benefit (PMB0) intersects MC. However, if scientific knowledge is assumed to be a public good, then RD0 is too low a quantity to satisfy the social need. The optimal amount of R&D is at RD1, which is where the marginal cost curve intersects the social marginal benefit (not depicted on this graph). The value of the tax incentive is that it allows the marginal cost curve for the firm to shift downward so that the private cost to the firm now produces the socially optimal quantity. The rest of the cost is now borne by society, in the form of the lost tax revenue.\n\nTax incentives allow slightly more precise targeting than the patent system. However, the concern still remains that tax incentives exacerbate inequality by producing financial windfalls for firms that might already be very prosperous. Furthermore, empirical studies have been limited, although a 1996 report from the Congressional Office of Technological Assessment found that for every dollar lost in tax revenue, there was a dollar increasing in private R&D spending.\n\n"}
{"id": "871453", "url": "https://en.wikipedia.org/wiki?curid=871453", "title": "Explorer 3", "text": "Explorer 3\n\nExplorer 3 (international designation 1958 Gamma) was an artificial satellite of the Earth, nearly identical to the first United States artificial satellite Explorer 1 in its design and mission. It was the second successful launch in the Explorer program.\n\nThe satellite was launched from Cape Canaveral Air Force Station, Florida at 17:31:00 UTC on March 26, 1958, by the Juno I vehicle. \nThe Juno I had its origins in the United States Army's Project Orbiter in 1954. The project was canceled in 1955, however, when the decision was made to proceed with Project Vanguard.\n\nFollowing the launch of the Soviet Sputnik 1 on October 4, 1957, Army Ballistic Missile Agency (ABMA) was directed to proceed with the launching of a satellite using the Jupiter-C, which had already been flight-tested in nose-cone re-entry tests for the Jupiter IRBM (intermediate-range ballistic missile). Working closely together, ABMA and JPL completed the job of modifying the Jupiter-C to the Juno I and building the Explorer I in 84 days.\n\nExplorer 3 was launched in conjunction with the International Geophysical Year (IGY) by the U.S. Army (Ordnance) into an eccentric orbit. The objective of this spacecraft was a continuation of experiments started with Explorer 1. The payload consisted of a cosmic ray counter (a Geiger-Müller tube), and a micrometeorite detector (a wire grid array and acoustic detector). The Explorer 3 spacecraft was spin-stabilized and had an on-board tape recorder to provide a complete radiation history for each orbit. It was discovered soon after launch that the satellite was in a tumbling motion with a period of about 7 seconds. Explorer 3 decayed from orbit on June 27, 1958, after 93 days of operation.\n\nThe discovery of the Van Allen radiation belt by the Explorer satellites was considered to be one of the outstanding discoveries of the IGY.\n\nExplorer 3 was placed in an orbit with a perigee of 186 kilometers and an apogee of 2799 kilometers having a period of 115.7 minutes. Its total weight was 14.1 kilograms, of which 8.4 kg was instrumentation. The instrument section at the front end of the satellite and the empty scaled-down fourth-stage rocket casing orbited as a single unit, spinning around its long axis at 750 revolutions per minute.\n\nInstrumentation consisted of a cosmic ray detection package and a ring of micrometeorite erosion gauges. The Explorer 3 spacecraft was spin-stabilized and had an on-board tape recorder to provide a complete radiation history for each orbit. Data from these instruments was transmitted to the ground by a 60 milliwatt transmitter operating on 108.03 MHz and a 10 milliwatt transmitter operating on 108.00 MHz.\n\nTransmitting antennas consisted of two fiberglass slot antennas in the body of the satellite itself. The four flexible whip antennas of Explorer 1 were removed from the design.\n\nThe external skin of the instrument section was painted in alternate strips of white and dark green to provide passive temperature control of the satellite. The proportions of the light and dark strips were determined by studies of shadow-sunlight intervals based on firing time, trajectory, orbit, and inclination.\n\nElectrical power was provided by Mallory type RM Mercury cells that made up approximately 40 percent of the payload weight. These provided power that operated the high power transmitter for 31 days and the low-power transmitter for 105 days.\n\nBecause of the limited space available and the requirements for low weight, the Explorer 3 instrumentation was designed and built with simplicity and high reliability in mind. It was completely successful.\n\nExplorer 3 decayed from orbit on June 27, 1958, after 93 days of operation.\n\nA replica of the spacecraft is currently located in the Smithsonian Institution's National Air and Space Museum, Milestones of Flight Gallery.\n\n\n"}
{"id": "35828084", "url": "https://en.wikipedia.org/wiki?curid=35828084", "title": "Fourmarierite", "text": "Fourmarierite\n\nFourmarierite is a secondary uranium-lead mineral. It was named for the Belgian geologist Paul Fourmarier (1877–1970). Its chemical formula is Pb(UO)O(OH)•4HO.\n"}
{"id": "40503515", "url": "https://en.wikipedia.org/wiki?curid=40503515", "title": "Gauge vector–tensor gravity", "text": "Gauge vector–tensor gravity\n\nGauge vector–tensor gravity (GVT) is a relativistic generalization of Mordehai Milgrom's modified Newtonian dynamics (MOND) paradigm where gauge fields cause the MOND behavior. The former covariant realizations of MOND such as the Bekenestein's tensor–vector–scalar gravity and the Moffat's scalar–tensor–vector gravity attribute MONDian behavior to some scalar fields. GVT is the first example wherein the MONDian behavior is mapped to the gauge vector fields.\nThe main features of GVT can be summarized as follows:\nIts dynamical degrees of freedom are:\n\nThe physical geometry, as seen by particles, represents the Finsler geometry–Randers type:\n\nThis implies that the orbit of a particle with mass formula_4 can be derived from the following effective action:\n\nThe geometrical quantities are Riemannian. GVT, thus, is a bi-geometric gravity. \n\nThe metric's action coincides to that of the Einstein–Hilbert gravity:\n\nwhere formula_7 is the Ricci scalar constructed out from the metric. The action of the gauge fields follow:\n\nwhere L has the following MOND asymptotic behaviors\n\nand formula_10 represent the coupling constants of the theory while formula_11 are the parameters of the theory and formula_12\n\nMetric couples to the energy-momentum tensor. The matter current is the source field of both gauge fields. The matter current is\n\nwhere formula_14 is the density and formula_15 represents the four velocity.\n\nGVT accommodates the Newtonian and MOND regime of gravity; but it admits the post-MONDian regime. \nThe strong and Newtonian regime of the theory is defined to be where holds:\n\nThe consistency between the gravitoelectromagnetism approximation to the GVT theory and that predicted and measured by the Einstein–Hilbert gravity demands that \n\nwhich results in \n\nSo the theory coincides to the Einstein–Hilbert gravity in its Newtonian and strong regimes.\n\nThe MOND regime of the theory is defined to be\n\nSo the action for the formula_20 field becomes aquadratic. For the static mass distribution, the theory then converts to the AQUAL model of gravity with the critical acceleration of\n\nSo the GVT theory is capable of reproducing the flat rotational velocity curves of galaxies. The current observations do not fix formula_22 which is supposedly of order one. \nThe post-MONDian regime of the theory is defined where both of the actions of the formula_23 are aquadratic. The MOND type behavior is suppressed in this regime due to the contribution of the second gauge field. \n"}
{"id": "14535543", "url": "https://en.wikipedia.org/wiki?curid=14535543", "title": "Hiroshige (crater)", "text": "Hiroshige (crater)\n\nHiroshige is a crater on Mercury located at 13 S, 27 W. It is 138 km in diameter and was named after Andō Hiroshige.\n"}
{"id": "24897936", "url": "https://en.wikipedia.org/wiki?curid=24897936", "title": "Hydraulics Research Station", "text": "Hydraulics Research Station\n\nThe Hydraulics Research Station (HRS) was created by the UK Department of Scientific and Industrial Research in 1947. The Research Station was based in Wallingford, near Oxford. It was established to deal with “looser boundary” problems such as coastal erosion, flood protection and the silting and scouring of rivers, estuaries and harbours. The Hydraulics Research Station was housed at Howbery Park as a government establishment until 1982, when it was privatised from the Department of the Environment (DoE) to become Hydraulics Research Station Limited. It is now known as HR Wallingford. During its existence, HRS contributed to advance hydraulics research. It also worked on water-related projects in the UK and around the world.\n\n1945 - The Institution of Civil Engineers submitted a proposal to the Department of Scientific and Industrial Research on the need for a hydraulics research station in the UK.\n\n1947 – DSIR Hydraulic Research Organisation formed in London\n\n1951 – Hydraulics Research Station established in Wallingford\n\n1965 – Re-organisation into Ministry of Technology. Hydrological Research Unit transferred to the Natural Environment Research Council and later to become Institute of Hydrology and then Centre for Ecology and Hydrology\n\n1971 – Transfer to the Department of the Environment (DoE)\n\n1982 – Privatisation to create Hydraulics Research Station Limited - a company limited by guarantee.\n\n1983 – Hydraulics Research Limited\n\n1991 - HR Wallingford Limited\n\nHRS started doing research in the tidal Thames Estuary back in 1947. At this time HRO (Hydraulic Research Organisation) was based at the National Physical Laboratory at Teddington and had links with a large physical model set up by the Port of London Authority (PLA) in one of their disused warehouses on the Surrey Docks. This model was used to examine many hydrodynamic, sediment, water quality and morphological issues related to the Thames Estuary and the potential redevelopment of the Estuary following the considerable infrastructure damage that had been suffered during World War II. Many of the issues examined and the techniques developed in this pre-computer age formed a remarkably good base from which the modern range and scope of studies have been developed. This has determined the framework for an understanding of the many processes that operate within the tidal Thames Estuary.\nThe Thames barrier was designed by Rendel, Palmer and Tritton for the Greater London Council and tested at Hydraulics Research Station.\n\nHRS established the Hydrological Research Unit for the purpose of River catchment research and engineering and co-operation with other government offices such as the :\n- Soil Survey of England and Wales (JP Bell)\n- the Ministry of Agriculture Fisheries and Food,\n- the River Authority (1963 Act).\n\nThe work expanded greatly after the 1968 flood in Somerset from such actions as the Plynlimon Hafren and Gwy forest and grassland catchments of 1965 under the auspices of James McCulloch (civil engineering) and John C Rodda (hydrometerology and catchments), to operate several units Northumberland, Thetford, Plynlimon, and was moved to Crowmarsh Gifford as the Institute of Hydrology, in part concerning itself with a mass Flood analysis using existing River Authority data (1975). The Institute is now the Centre for Ecology and Hydrology, part of the Natural Environment Research Council.\n\nHydraulics\n"}
{"id": "17204562", "url": "https://en.wikipedia.org/wiki?curid=17204562", "title": "Indigen", "text": "Indigen\n\nIn general usage the word indigen is treated as a variant of the word indigene, meaning a native.\n\nHowever, it was used in a strictly botanical sense for the first time in 1918 by Liberty Hyde Bailey ((1858–1954) an American horticulturist, botanist and cofounder of the American Society for Horticultural Science) and described as a plant \n\n \" ... \"a species of which we know the nativity, - one that is somewhere recorded as indigenous\". \" The term was coined to contrast with cultigen which he defined in the 1923 paper as: \" ... \"the species, or its equivalent, that has appeared under domestication, – the plant is cultigenous.\"\"\n\n"}
{"id": "48822343", "url": "https://en.wikipedia.org/wiki?curid=48822343", "title": "John T. Dingle", "text": "John T. Dingle\n\nJohn T. Dingle is a British biologist and rheumatologist. He joined the staff of the Strangeways Research Laboratory in 1959 as a research assistant to then-director Honor Fell, and later himself served as director from 1979 to 1993, taking over the position after the death of Michael Abercrombie. His presence at Strangeways helped to move its research direction toward the original research interests of its founder, Thomas Strangeways, who sought to understand the physiology of arthritis and other rheumatic diseases, after many years in which the laboratory specialized more narrowly in tissue culture and cell biology. Dingle was the president of Hughes Hall from 1993 to 1998, and is an honorary fellow. He was the founding chairman of the British Connective Tissue Society (now the British Society for Matrix Biology), serving from 1980 to 1987. Among his notable trainees is University of California, San Francisco cell biologist Zena Werb, who was a postdoctoral fellow with Dingle and subsequently worked as a research assistant at Strangeways.\n"}
{"id": "1285524", "url": "https://en.wikipedia.org/wiki?curid=1285524", "title": "LHCb experiment", "text": "LHCb experiment\n\nThe LHCb (Large Hadron Collider beauty) experiment is one of seven particle physics detector experiments collecting data at the Large Hadron Collider at CERN. LHCb is a specialized b-physics experiment, designed primarily to measure the parameters of CP violation in the interactions of b-hadrons (heavy particles containing a bottom quark). Such studies can help to explain the matter-antimatter asymmetry of the Universe. The detector is also able to perform measurements of production cross sections, exotic hadron spectroscopy, charm physics and electroweak physics in the forward region. The LHCb collaboration, who built, operate and analyse data from the experiment, is composed of approximately 1260 people from 74 scientific institutes, representing 16 countries. As of 2017, the spokesperson for the collaboration is Giovanni Passaleva. The experiment is located at point 8 on the LHC tunnel close to Ferney-Voltaire, France just over the border from Geneva. The (small) MoEDAL experiment shares the same cavern.\n\nThe experiment has wide physics program covering many important aspects of heavy flavour (both beauty and charm), electroweak and quantum chromodynamics (QCD) physics. Six key measurements have been identified involving B mesons. These are described in a roadmap document that form the core physics programme for the first high energy LHC running in 2010–2012. They include:\n\nThe fact that the two b-hadrons are predominantly produced in the same forward cone is exploited in the layout of the LHCb detector. The LHCb detector is a single arm forward spectrometer with a polar angular coverage from 10 to 300 milliradians (mrad) in the horizontal and 250 mrad in the vertical plane. The asymmetry between the horizontal and vertical plane is determined by a large dipole magnet with the main field component in the vertical direction.\n\nThe vertex detector (VELO) is built around the proton interaction region. It is used to measure the particle trajectories close to the interaction point in order to precisely separate primary and secondary vertices.\n\nThe detector operates at from the LHC beam. This implies an enormous flux of particles; The VELO has been designed to withstand integrated fluences of more than 10 p/cm per year for a period of about three years. The detector operates in vacuum and is cooled to approximately using a biphase CO system. The data of the VELO detector are amplified and read out by the Beetle ASIC.\nThe RICH-1 detector (Ring imaging Cherenkov detector) is located directly after the vertex detector. It is used for particle identification of low-momentum tracks.\n\nThe main tracking system is placed before and after the dipole magnet. It is used to reconstruct the trajectories of charged particles and to measure their momenta. The tracker consists of three subdetectors:\n\nFollowing the tracking system is RICH-2. It allows the identification of the particle type of high-momentum tracks.\n\nThe electromagnetic and hadronic calorimeters provide measurements of the energy of electrons, photons, and hadrons. These measurements are used at trigger level to identify the particles with large transverse momentum (high-Pt particles).\n\nThe muon system is used to identify and trigger on muons in the events.\n\nDuring the 2011 proton-proton run LHCb recorded a luminosity of 1 fb at an energy of 7 TeV. In 2012 about 2 fb was collected at an energy of 8 TeV. These datasets allow the collaboration to carry out the physics program of precision Standard Model tests with many additional measurements. The analysis led to evidence for the flavour-changing neutral current decay B → μ μ. This measurement impacts the parameter space of supersymmetry. A combination with Compact Muon Solenoid (CMS) data from the completed 8 TeV run allowed for a precise measurement of the strange b-meson to dimuon branching fraction. CP violation was studied in various particle systems such as B, Kaons, and D. New Xi baryons were observed in 2014. Analysis of the decay of bottom lambda baryons (Λ) in the LHCb experiment also revealed the apparent existence of pentaquarks, in what was described as an \"accidental\" discovery.\n\n\n"}
{"id": "2290361", "url": "https://en.wikipedia.org/wiki?curid=2290361", "title": "Li Qinglong", "text": "Li Qinglong\n\nLi Qinglong (; born August 1962) is a Chinese pilot selected as part of the Shenzhou program.\n\nLi Qinglong was born in Dingyuan, Anhui province, China. In 1987 he graduated from the People's Liberation Army Air Force (PLAAF) Missile College and later the PLAAF Flight College. A fighter pilot in the PLAAF, he had accumulated 1230 flight-hours.\n\nIn November 1996, he and Wu Jie, started training at the Russian Yuri Gagarin Cosmonauts Training Center. When they returned to China a year later, they acted as the trainers for the first group of astronauts. In January 2003 it was reported by a Hong Kong newspaper that Chen Long would make the first manned Shenzhou flight. Then in March 2003 it was reported that Li Qinglong would make the first manned flight. It was thought that \"Chen Long\" was a misspell of \"Qinglong\", a fact confirmed by the newspaper a couple of days before the launch of \"Shenzhou 5\", which was flown by Yang Liwei.\n"}
{"id": "3299797", "url": "https://en.wikipedia.org/wiki?curid=3299797", "title": "Liquid mirror telescope", "text": "Liquid mirror telescope\n\nLiquid mirror telescopes are telescopes with mirrors made with a reflective liquid. The most common liquid used is mercury, but other liquids will work as well (for example, low melting alloys of gallium). The liquid and its container are rotated at a constant speed around a vertical axis, which causes the surface of the liquid to assume a paraboloidal shape, suitable for use as the primary mirror of a reflecting telescope. The rotating liquid assumes the paraboloidal shape regardless of the container's shape. To reduce the amount of liquid metal needed, and thus weight, a rotating mercury mirror uses a container that is as close to the necessary parabolic shape as possible. Liquid mirrors can be a low cost alternative to conventional large telescopes. Compared to a solid glass mirror that must be cast, ground, and polished, a rotating liquid metal mirror is much less expensive to manufacture.\n\nIsaac Newton noted that the free surface of a rotating liquid forms a circular paraboloid and can therefore be used as a telescope, but he could not actually build one because he had no way to stabilize the speed of rotation. The concept was further developed by Ernesto Capocci of the Naples Observatory (1850), but it was not until 1872 that Henry Skey of Dunedin, New Zealand constructed the first working laboratory liquid mirror telescope.\n\nAnother difficulty is that a liquid metal mirror can only be used in zenith telescopes, i.e., that look straight up, so it is not suitable for investigations where the telescope must remain pointing at the same location of inertial space (a possible exception to this rule may exist for a mercury mirror space telescope, where the effect of Earth's gravity is replaced by artificial gravity, perhaps by rotating the telescope on a very long tether, or propelling it gently forward with rockets). Only a telescope located at the North Pole or South Pole would offer a relatively static view of the sky, although the freezing point of mercury and the remoteness of the location would need to be considered. A very large telescope already exists at the South Pole, but the North Pole is located in the Arctic Ocean.\n\nThe mercury mirror of the Large Zenith Telescope in Canada was the largest liquid metal mirror ever built. It had a diameter of six meters, and rotated at a rate of about 8.5 revolutions per minute. It is now decommissioned. This mirror was a test, built for $1 million but it was not suitable for astronomy because of the test site's weather. They are now planning to build a larger 8 meter liquid mirror telescope ALPACA for astronomical use and a larger project called LAMA with 66 individual 6.15 meter telescopes with a total collecting power equal to a 55 meter telescope, resolving power of a 70 meter scope.\n\nIn the following discussion, formula_1 represents the acceleration due to gravity, formula_2 represents the angular speed of the liquid's rotation, in radians per second, formula_3 is the mass of an infinitesimal parcel of liquid material on the surface of the liquid, formula_4 is the distance of the parcel from the axis of rotation, and formula_5 is the height of the parcel above a zero to be defined in the calculation.\n\nThe force diagram (shown) represents a snapshot of the forces acting on the parcel, in a non-rotating frame of reference. The direction of each arrow shows the direction of a force, and the length of the arrow shows the force's strength. The red arrow represents the weight of the parcel, caused by gravity and directed vertically downward. The green arrow shows the buoyancy force exerted on the parcel by the bulk of the liquid. Since, in equilibrium, the liquid cannot exert a force parallel with its surface, the green arrow must be perpendicular to the surface. The short blue arrow shows the net force on the parcel. It is the vector sum of the forces of weight and buoyancy, and acts horizontally toward the axis of rotation. (It must be horizontal, since the parcel has no vertical acceleration.) It is the centripetal force that constantly accelerates the parcel toward the axis, keeping it in circular motion as the liquid rotates.\n\nThe buoyancy force (green arrow) has a vertical component which must equal the weight of the parcel (red arrow), which is formula_6, and the horizontal component of the buoyancy force must equal the centripetal force (blue arrow), which is formula_7. Therefore, the green arrow is tilted from the vertical by an angle whose tangent is the quotient of these forces. Since the green arrow is perpendicular to the surface of the liquid, the slope of the surface must be the same quotient of the forces:\n\nCancelling the formula_3's, integrating, and setting formula_10 when formula_11 leads to\n\nThis is of the form formula_13, where formula_14 is a constant, showing that the surface is, by definition, a paraboloid.\n\nThe equation of the paraboloid in terms of its focal length (see Parabolic reflector#Theory) can be written as:\n\nwhere formula_16 is the focal length, and formula_5 and formula_4 are defined as above.\n\nDividing this equation by the last one above it eliminates formula_5 and formula_4 and leads to:\n\nwhich relates the angular velocity of the rotation of the liquid to the focal length of the paraboloid that is produced by the rotation. Note that no other variables are involved. The density of the liquid, for example, has no effect on the focal length of the paraboloid. The units must be consistent, e.g. formula_16 may be in metres, formula_2 in radians per second, and formula_1 in metres per second-squared. The angle unit in formula_2 must be radians.\n\nIf we write formula_26 for the numerical value of the focal length in metres, and formula_27 for the numerical value of the rotation speed in revolutions per minute (RPM). then on the Earth's surface, where formula_1 is approximately 9.81 metres per second-squared, the last equation reduces to the approximation:\n\nIf the focal length is in feet, instead of metres, this approximation becomes: FS≈1467. The rotation speed is still in RPM.\n\nThese are made of liquid stored in a cylindrical container made of a composite material, such as Kevlar. The cylinder is spun until it reaches a few revolutions per minute. The liquid gradually forms a paraboloid, the shape of a conventional telescopic mirror. The mirror's surface is very precise and small imperfections in the cylinder's shape do not affect it. The amount of mercury used is small, less than a millimeter in thickness.\n\nLow temperature ionic liquids (below 130 kelvins) have been proposed\nas the fluid base for an extremely large diameter spinning liquid mirror telescope to be based on the Earth's moon. Low temperature is advantageous in imaging long wave infrared light which is the form of light (extremely red-shifted) that arrives from the most distant parts of the visible universe. Such a liquid base would be covered by a thin metallic film that forms the reflective surface.\n\nThe Rice liquid mirror telescope design is similar to conventional liquid mirror telescopes. It will only work in space; but in orbit, gravity will not distort the mirror's shape into a paraboloid. The design features a liquid stored in a flat-bottomed ring-shaped container with raised interior edges. The central focal area would be rectangular, but a secondary rectangular-parabolic mirror would gather the light to a focal point. Otherwise the optics are similar to other optical telescopes. The light gathering power of a Rice telescope is equivalent to approximately the width times the diameter of the ring, minus a percentage based on optics, superstructure design, etc.\n\nThe greatest advantage of a liquid mirror is its small cost, about 1% of a conventional telescope mirror. This cuts down the cost of the entire telescope at least 95%. The University of British Columbia’s 6 meter Large Zenith Telescope cost about a fiftieth as much as a conventional telescope with a glass mirror.\nThe greatest disadvantage is that the mirror can only be pointed straight up. Research is underway to develop telescopes that can be tilted, but currently if a liquid mirror were to tilt out of the zenith, it would lose its shape. Therefore, the mirror’s view changes as the Earth rotates and objects cannot be physically tracked. An object can be briefly electronically tracked while in the field of view by shifting electrons across the CCD at the same speed as the image moves; this tactic is called time delay and integration or drift scanning. Some types of astronomical research are unaffected by these limitations, such as long-term sky surveys and supernova searches. Since the universe is believed to be isotropic and homogeneous (this is called the Cosmological Principle), the investigation of its structure by cosmologists can also use telescopes which are highly reduced in their direction of view.\n\nSince mercury metal and its vapor are both toxic to humans and animals there remains a problem for its use in any telescope where it may affect its users and others in its area. In the Large Zenith Telescope, the mercury mirror and the human operators are housed in separately-ventilated rooms. At its location in the Canadian mountains, the ambient temperature is fairly low, which reduces the rate of evaporation of the mercury. The less toxic metal gallium may be used instead of mercury but has the disadvantage of high cost. Recently Canadian researchers have proposed the substitution of magnetically deformable liquid mirrors composed of a suspension of iron and silver nanoparticles in ethylene glycol. In addition to low toxicity and relatively low cost, such a mirror would have the advantage of being easily and rapidly deformable using variations of magnetic field strength.\nUsually, the mirror of a liquid mirror telescope is rotated around two axes simultaneously. For example, the mirror of a telescope on the surface of the Earth rotates at a speed of a few revolutions per minute about a vertical axis to maintain its parabolic shape, and also at a speed of one revolution per day about the Earth's axis because of the rotation of the Earth. Usually (except if the telescope is located at one of the Earth's poles), the two rotations interact so that, in a frame of reference that is stationary relative to the local surface of the Earth, the mirror experiences a torque about an axis that is perpendicular to both rotation axes, i.e. a horizontal axis aligned east-west. Since the mirror is liquid, it responds to this torque by changing its aim direction. The point in the sky at which the mirror is aimed is not exactly overhead; it is displaced slightly to the north or south. The amount of the displacement depends on the latitude, the rotation speeds, and the parameters of the telescope's design. On the Earth, the displacement is small, typically a few arcseconds, which can, nevertheless, be significant in astronomical observations. If the telescope were in space, rotating to produce artificial gravity, the displacement could be much larger, possibly many degrees. This would add complexity to the operation of the telescope.\n\nVarious prototypes exist historically. Following a resurgence of interest in the technology in the 1980s, several projects came to fruition.\n\n\n"}
{"id": "22280100", "url": "https://en.wikipedia.org/wiki?curid=22280100", "title": "List of DVD authoring applications", "text": "List of DVD authoring applications\n\nThe following applications can be used to create playable DVDs.\nFree software implementations often lack features such as encryption and region coding due to licensing restrictions issues, and depending on the demands of the DVD producer, may not be considered suitable for mass-market use. \n\n\n\n\n"}
{"id": "9025116", "url": "https://en.wikipedia.org/wiki?curid=9025116", "title": "List of UN numbers 2501 to 2600", "text": "List of UN numbers 2501 to 2600\n\nThe UN numbers from UN2501 to UN2600 as assigned by the United Nations Committee of Experts on the Transport of Dangerous Goods.\n\n"}
{"id": "2678379", "url": "https://en.wikipedia.org/wiki?curid=2678379", "title": "List of compounds with carbon number 18", "text": "List of compounds with carbon number 18\n\nThis is a partial list of molecules that contain 18 carbon atoms.\n\n"}
{"id": "47571281", "url": "https://en.wikipedia.org/wiki?curid=47571281", "title": "List of countries by average yearly temperature", "text": "List of countries by average yearly temperature\n\nAverage yearly temperature is calculated by averaging the minimum and maximum daily temperatures in the country, averaged for the years 1961–1990, based on gridded climatologies from the Climatic Research Unit elaborated in 2011.\n"}
{"id": "10665011", "url": "https://en.wikipedia.org/wiki?curid=10665011", "title": "List of quantum-mechanical potentials", "text": "List of quantum-mechanical potentials\n\nThis is a list of potential energy functions that are frequently used in quantum mechanics and have any meaning.\n\n\n\n"}
{"id": "6072401", "url": "https://en.wikipedia.org/wiki?curid=6072401", "title": "List of the largest Sites of Special Scientific Interest in England", "text": "List of the largest Sites of Special Scientific Interest in England\n\nThis is a list of the largest Sites of Special Scientific Interest in England in decreasing order of size. A lower threshold of 100 hectares or one square kilometre has been used.\n"}
{"id": "7119913", "url": "https://en.wikipedia.org/wiki?curid=7119913", "title": "List of volcanoes in Armenia", "text": "List of volcanoes in Armenia\n\nThis is a list of volcanoes in Armenia. \n"}
{"id": "7120120", "url": "https://en.wikipedia.org/wiki?curid=7120120", "title": "List of volcanoes in Martinique", "text": "List of volcanoes in Martinique\n\nThis is a list of active and extinct volcanoes.\n"}
{"id": "53125965", "url": "https://en.wikipedia.org/wiki?curid=53125965", "title": "List of yttrium compounds", "text": "List of yttrium compounds\n\nThis List of yttrium compounds shows compounds which do not yet have individual articles. \n"}
{"id": "805674", "url": "https://en.wikipedia.org/wiki?curid=805674", "title": "Little Joe II Qualification Test Vehicle", "text": "Little Joe II Qualification Test Vehicle\n\nQTV (Qualification Test Vehicle) of the Apollo Little Joe II rocket was the first test flight in 1963.\n\nThe Little Joe II Qualification Test Vehicle was launched on its first flight from White Sands Missile Range, New Mexico, Area # 3. The mission objectives were to prove the Little Joe II rocket's capability as an Apollo CSM test vehicle and to determine base pressures and heating on the rocket.\n\nLittle Joe II QTV was the first flight of the Little Joe II rockets. It was launched from White Sands Missile Range, New Mexico on August 28, 1963 from Launch Complex 36, Area number 3. The spacecraft consisted of a dummy launch escape system and a boilerplate command and service module. Most objectives were achieved. The lone failure was a malfunction in the destruct system.\n\n\n"}
{"id": "19646123", "url": "https://en.wikipedia.org/wiki?curid=19646123", "title": "Making a Real Killing", "text": "Making a Real Killing\n\nMaking a Real Killing: Rocky Flats and the Nuclear West is a 1999 book by Len Ackland. Ackland draws on information obtained from governmental sources, federal contractors, personal interviews, and newspaper articles to form a multi-layered history about the controversial Rocky Flats nuclear facility. The book also explores the creation and collapse of the nuclear weapons complex in the United States.\n\nReviews of \"Making a Real Killing\" have been published in \"Environmental History\" and \"Pacific Historical Review\".\n\nLen Ackland is the former editor of the \"Bulletin of the Atomic Scientists\" and director for environmental journalism at the University of Colorado at Boulder.\n\n"}
{"id": "40535205", "url": "https://en.wikipedia.org/wiki?curid=40535205", "title": "Mesoplasma", "text": "Mesoplasma\n\nMesoplasma is a genus of bacteria belonging to the class Mollicutes. Mesoplasma is related to the genus Mycoplasma but differ in several respects.\n\n\n"}
{"id": "11235396", "url": "https://en.wikipedia.org/wiki?curid=11235396", "title": "National Institute of Agricultural Botany", "text": "National Institute of Agricultural Botany\n\nThe National Institute of Agricultural Botany (NIAB) is a plant science research company based in Cambridge, UK.\n\n"}
{"id": "23021180", "url": "https://en.wikipedia.org/wiki?curid=23021180", "title": "Osmotic dehydration", "text": "Osmotic dehydration\n\nOsmotic dehydration is an operation used for the partial removal of water from plant tissues by immersion in a hyper-tonic (osmotic) solution.\n\nWater removal anuska \nis based on the natural and non-destructive phenomenon of osmosis across cell membranes. The driving force for the diffusion of water from the tissue into the solution is provided by the higher osmotic pressure of the hyper-tonic solution. The diffusion of water is accompanied by the simultaneous counter diffusion of solutes from the osmotic solution into the tissue. Since the cell membrane responsible for osmotic transport is not perfectly selective, solutes present in the cells (organic acids, reducing sugars, minerals, flavors and pigment compounds) can also be leaked into the osmotic solution, which affects the organoleptic and nutritional characteristics of the product.\n\nThe rate of diffusion of water from any material made up of such tissues depends upon factors such as temperature and concentration of the osmotic solution, the size and geometry of the material, the solution-to-material mass ratio and, to a certain level, agitation of the solution.\n"}
{"id": "18814533", "url": "https://en.wikipedia.org/wiki?curid=18814533", "title": "Pulse-swallowing counter", "text": "Pulse-swallowing counter\n\nA pulse-swallowing counter is a component in an all-digital feedback system. The overall pulse-swallowing system is used as part of a fractional-N frequency divider. The overall pulse-swallowing system cancels beatnotes created when switching between N, N+1, or N−1 in a fractional-N synthesizer.\n"}
{"id": "11864322", "url": "https://en.wikipedia.org/wiki?curid=11864322", "title": "Quasi-experiment", "text": "Quasi-experiment\n\nA quasi-experiment is an empirical interventional study used to estimate the causal impact of an intervention on target population without random assignment. Quasi-experimental research shares similarities with the traditional experimental design or randomized controlled trial, but it specifically lacks the element of random assignment to treatment or control. Instead, quasi-experimental designs typically allow the researcher to control the assignment to the treatment condition, but using some criterion other than random assignment (e.g., an eligibility cutoff mark). In some cases, the researcher may have control over assignment to treatment.\nQuasi-experiments are subject to concerns regarding internal validity, because the treatment and control groups may not be comparable at baseline. With random assignment, study participants have the same chance of being assigned to the intervention group or the comparison group. As a result, differences between groups on both observed and unobserved characteristics would be due to chance, rather than to a systematic factor related to treatment (e.g., illness severity). Randomization itself does not guarantee that groups will be equivalent at baseline. Any change in characteristics post-intervention is likely attributable to the intervention. With quasi-experimental studies, it may not be possible to convincingly demonstrate a causal link between the treatment condition and observed outcomes. This is particularly true if there are confounding variables that cannot be controlled or accounted for.\n\nThe first part of creating a quasi-experimental design is to identify the variables. The quasi-independent variable will be the x-variable, the variable that is manipulated in order to affect a dependent variable. “X” is generally a grouping variable with different levels. Grouping means two or more groups, such as two groups receiving alternative treatments, or a treatment group and a no-treatment group (which may be given a placebo - placebos are more frequently used in medical or physiological experiments). The predicted outcome is the dependent variable, which is the y-variable. In a time series analysis, the dependent variable is observed over time for any changes that may take place. Once the variables have been identified and defined, a procedure should then be implemented and group differences should be examined.\n\nIn an experiment with random assignment, study units have the same chance of being assigned to a given treatment condition. As such, random assignment ensures that both the experimental and control groups are equivalent. In a quasi-experimental design, assignment to a given treatment condition is based on something other than random assignment. Depending on the type of quasi-experimental design, the researcher might have control over assignment to the treatment condition but use some criteria other than random assignment (e.g., a cutoff score) to determine which participants receive the treatment, or the researcher may have no control over the treatment condition assignment and the criteria used for assignment may be unknown. Factors such as cost, feasibility, political concerns, or convenience may influence how or if participants are assigned to a given treatment conditions, and as such, quasi-experiments are subject to concerns regarding internal validity (i.e., can the results of the experiment be used to make a causal inference?).\n\nQuasi-experiments are also effective because they use the \"pre-post testing\". This means that there are tests done before any data are collected to see if there are any person confounds or if any participants have certain tendencies. Then the actual experiment is done with post test results recorded. This data can be compared as part of the study or the pre-test data can be included in an explanation for the actual experimental data. Quasi experiments have independent variables that already exist such as age, gender, eye color. These variables can either be continuous (age) or they can be categorical (gender). In short, naturally occurring variables are measured within quasi experiments.\n\nThere are several types of quasi-experimental designs, each with different strengths, weaknesses and applications. These designs include (but are not limited to):\n\nOf all of these designs, the regression discontinuity design comes the closest to the experimental design, as the experimenter maintains control of the treatment assignment and it is known to “yield an unbiased estimate of the treatment effects”. It does, however, require large numbers of study participants and precise modeling of the functional form between the assignment and the outcome variable, in order to yield the same power as a traditional experimental design.\n\nThough quasi-experiments are sometimes shunned by those who consider themselves to be experimental purists (leading Donald T. Campbell to coin the term “queasy experiments” for them), they are exceptionally useful in areas where it is not feasible or desirable to conduct an experiment or randomized control trial. Such instances include evaluating the impact of public policy changes, educational interventions or large scale health interventions. The primary drawback of quasi-experimental designs is that they cannot eliminate the possibility of confounding bias, which can hinder one’s ability to draw causal inferences. This drawback is often used to discount quasi-experimental results. However, such bias can be controlled for using various statistical techniques such as multiple regression, if one can identify and measure the confounding variable(s). Such techniques can be used to model and partial out the effects of confounding variables techniques, thereby improving the accuracy of the results obtained from quasi-experiments. Moreover, the developing use of propensity score matching to match participants on variables important to the treatment selection process can also improve the accuracy of quasi-experimental results.\nIn fact, data derived from quasi-experimental analyses has been shown to closely match experimental data in certain cases, even when different criteria were used. In sum, quasi-experiments are a valuable tool, especially for the applied researcher. On their own, quasi-experimental designs do not allow one to make definitive causal inferences; however, they provide necessary and valuable information that cannot be obtained by experimental methods alone. Researchers, especially those interested in investigating applied research questions, should move beyond the traditional experimental design and avail themselves of the possibilities inherent in quasi-experimental designs.\n\nA true experiment would, for example, randomly assign children to a scholarship, in order to control for all other variables. Quasi-experiments are commonly used in social sciences, public health, education, and policy analysis, especially when it is not practical or reasonable to randomize study participants to the treatment condition.\n\nAs an example, suppose we divide households into two categories: Households in which the parents spank their children, and households in which the parents do not spank their children. We can run a linear regression to determine if there is a positive correlation between parents' spanking and their children's aggressive behavior. However, to simply randomize parents to spank or to not spank their children may not be practical or ethical, because some parents may believe it is morally wrong to spank their children and refuse to participate.\n\nSome authors distinguish between a natural experiment and a \"quasi-experiment\". The difference is that in a quasi-experiment the criterion for assignment is selected by the researcher, while in a natural experiment the assignment occurs 'naturally,' without the researcher's intervention.\n\nQuasi-experiments have outcome measures, treatments, and experimental units, but do not use random assignment. Quasi-experiments are often the design that most people choose over true experiments. The main reason is that they can usually be conducted while true experiments can not always be. Quasi-experiments are interesting because they bring in features from both experimental and non experimental designs. Measured variables can be brought in, as well as manipulated variables. Usually Quasi-experiments are chosen by experimenters because they maximize internal and external validity.\n\nSince quasi-experimental designs are used when randomization is impractical and/or unethical, they are typically easier to set up than true experimental designs, which require random assignment of subjects. Additionally, utilizing quasi-experimental designs minimizes threats to ecological validity as natural environments do not suffer the same problems of artificiality as compared to a well-controlled laboratory setting. Since quasi-experiments are natural experiments, findings in one may be applied to other subjects and settings, allowing for some generalizations to be made about population. Also, this experimentation method is efficient in longitudinal research that involves longer time periods which can be followed up in different environments.\n\nOther advantages of quasi experiments include the idea of having any manipulations the experimenter so chooses. In natural experiments, the researchers have to let manipulations occur on their own and have no control over them whatsoever. Also, using self selected groups in quasi experiments also takes away to chance of ethical, conditional, etc. concerns while conducting the study.\n\nQuasi-experimental estimates of impact are subject to contamination by confounding variables. In the example above, a variation in the children's response to spanking is plausibly influenced by factors that cannot be easily measured and controlled, for example the child's intrinsic wildness or the parent's irritability. The lack of random assignment in the quasi-experimental design method may allow studies to be more feasible, but this also poses many challenges for the investigator in terms of internal validity. This deficiency in randomization makes it harder to rule out confounding variables and introduces new threats to internal validity. Because randomization is absent, some knowledge about the data can be approximated, but conclusions of causal relationships are difficult to determine due to a variety of extraneous and confounding variables that exist in a social environment. Moreover, even if these threats to internal validity are assessed, causation still cannot be fully established because the experimenter does not have total control over extraneous variables.\n\nDisadvantages also include the study groups may provide weaker evidence because of the lack of randomness. Randomness brings a lot of useful information to a study because it broadens results and therefore gives a better representation of the population as a whole. Using unequal groups can also be a threat to internal validity. If groups are not equal, which is sometimes the case in quasi experiments, then the experimenter might not be positive what the causes are for the results.\n\nInternal validity is the approximate truth about inferences regarding cause-effect or causal relationships. This is why validity is important for quasi experiments because they are all about causal relationships. It occurs when the experimenter tries to control all variables that could affect the results of the experiment. Statistical regression, history and the participants are all possible threats to internal validity. The question you would want to ask while trying to keep internal validity high is \"Are there any other possible reasons for the outcome besides the reason I want it to be?\" If so, then internal validity might not be as strong.\n\nExternal validity is the extent to which results obtained from a study sample can be generalized to the population of interest. When External Validity is high, the generalization is accurate and can represent the outside world from the experiment. External Validity is very important when it comes to statistical research because you want to make sure that you have a correct depiction of the population. When external validity is low, the credibility of your research comes into doubt. Reducing threats to external validity can be done by making sure there is a random sampling of participants and random assignment as well.\n\n\"Person-by-treatment\" designs are the most common type of quasi experiment design. In this design, the experimenter measures at least one independent variable. Along with measuring one variable, the experimenter will also manipulate a different independent variable. Because there is manipulating and measuring of different independent variables, the research is mostly done in laboratories. An important factor in dealing with person-by-treatment designs are that random assignment will need to be used in order to make sure that the experimenter has complete control over the manipulations that are being done to the study.\n\nAn example of this type of design was performed at the University of Notre Dame. The study was conducted to see if being mentored for your job led to increased job satisfaction. The results showed that many people who did have a mentor showed very high job satisfaction. However, the study also showed that those who did not receive the mentor also had a high number of satisfied employees. Seibert concluded that although the workers who had mentors were happy, he could not assume that the reason for it was the mentors themselves because of the numbers of the high number of non-mentored employees that said they were satisfied. This is why prescreening is very important so that you can minimize any flaws in the study before they are seen.\n\n\"Natural experiments\" are a different type of quasi experiment design used by researchers. It differs from person-by-treatment in a way that there is not a variable that is being manipulated by the experimenter. Instead of controlling at least one variable like the person-by-treatment design, experimenters do not use random assignment and leave the experimental control up to chance. This is where the name \"natural\" experiment comes from. The manipulations occur naturally, and although this may seem like an inaccurate technique, it has actually proven to be useful in many cases. These are the studies done to people who had something sudden happen to them. This could mean good or bad, traumatic or euphoric. An example of this could be studies done on those who have been in a car accident and those who have not. Car accidents occur naturally, so it would not be ethical to stage experiments to traumatize subjects in the study. These naturally occurring events have proven to be useful for studying posttraumatic stress disorder cases.\n\n"}
{"id": "5174853", "url": "https://en.wikipedia.org/wiki?curid=5174853", "title": "Roundness (object)", "text": "Roundness (object)\n\nRoundness is the measure of how closely the shape of an object approaches that of a mathematically perfect circle. Roundness applies in two dimensions, such as the cross sectional circles along a cylindrical object such as a shaft or a cylindrical roller for a bearing. In geometric dimensioning and tolerancing, control of a cylinder can also include its fidelity to the longitudinal axis, yielding cylindricity. The analogue of roundness in three dimensions (that is, for spheres) is sphericity. \n\nRoundness is dominated by the shape's gross features rather than the definition of its edges and corners, or the surface roughness of a manufactured object. A smooth ellipse can have low roundness, if its eccentricity is large. Regular polygons increase their roundness with increasing numbers of sides, even though they are still sharp-edged.\n\nIn geology and the study of sediments (where three-dimensional particles are most important), roundness is considered to be the measurement of surface roughness and the overall shape is described by sphericity.\n\nThe ISO definition of roundness is based on the ratio between the inscribed and the circumscribed circles, i.e. the maximum and minimum sizes for circles that are just sufficient to fit inside and to enclose the shape. \n\nHaving a constant diameter, measured at varying angles around the shape, is often considered to be a simple measurement of roundness. This is misleading.\n\nAlthough constant diameter is a necessary condition for roundness, it is not a sufficient condition for roundness: shapes exist that have constant diameter but are far from round. Mathematical shapes such as the Reuleaux triangle and, an everyday example, the British 50p coin demonstrate this.\n\nRoundness does not describe radial displacements of a shape from some notional centre point, merely the overall shape.\n\nThis is important in manufacturing, such as for crankshafts and similar objects, where not only the roundness of a number of bearing journals must be measured, but also their alignment on an axis. A bent crankshaft may have perfectly round bearings, yet if one is displaced sideways, the shaft is useless. Such measurements are often performed by the same techniques as for roundness, but also considering the centre position and its relative position along an additional axial direction.\n\nA single trace covering the full rotation is made and at each equally spaced angle, formula_1, a measurement, formula_2, of the radius or distance between the center of rotation and the surface point. A least-squares fit to the data gives the following estimators of the parameters of the circle:\n\nThe deviation is then measured as:\n\nRoundness measurement is very important in metrology. It includes measurement of a collection of points.\n\nFor this two fundamental methods are followed:\n\n\nThe intrinsic method is limited to small deformations only. For large deformations extrinsic method has to be followed. In this case the datum is not a point or set of points on the object, but is a separate precision bearing usually on the measuring instrument. The axis of the object or part of the object to be measured is aligned with the axis of the bearing. Then a stylus from the instrument is just made to touch the part to be measured. A touch sensor connected to the tip of the stylus makes sure that the stylus just touches the object. A minimum of three readings are taken and an amplified polar plot is drawn to get the required error.\n\n\n"}
{"id": "34866867", "url": "https://en.wikipedia.org/wiki?curid=34866867", "title": "STRAT-X", "text": "STRAT-X\n\nSTRAT-X, or Strategic-Experimental, was a U.S. government-sponsored study conducted during 1966 and 1967 that comprehensively analyzed the potential future of the U.S. nuclear deterrent force. At the time, the Soviet Union was making significant strides in nuclear weapons delivery, and also constructing anti-ballistic missile defenses to protect strategic facilities. To address a potential technological gap between the two superpowers, U.S. Secretary of Defense Robert McNamara entrusted the classified STRAT-X study to the Institute for Defense Analyses, which compiled a twenty-volume report in nine months. The report looked into more than one hundred different weapons systems, ultimately resulting in the MGM-134 Midgetman and LGM-118 Peacekeeper intercontinental ballistic missiles, the s, and the Trident submarine-launched ballistic missiles, among others. Journalists have regarded STRAT-X as a major influence on the course of U.S. nuclear policy.\n\nIn the mid-1960s, reports received by U.S. intelligence agencies indicated that the Soviets were planning to deploy large numbers of highly accurate and powerful intercontinental ballistic missiles (ICBMs). Later, the R-36 ICBM entered service. Possessing the greatest throw weight of any ICBM ever at , the R-36 was larger than the most modern ICBMs in the U.S. arsenal at the time. Due to its size, it was able to carry high-yield warheads capable of destroying Minuteman hardened silos (see Counterforce). This was considered a significant risk to American ICBMs and, as a result, to the United States' nuclear defense strategy by reducing the United States' ability to retaliate with nuclear weapons if attacked.\n\nAt the same time, the Soviets were designing and constructing increasingly sophisticated anti-ballistic missile defense systems to protect strategically important facilities around Moscow, reducing the threat posed by American ICBMs. These developments compelled the U.S. Secretary of Defense, Robert McNamara, to commission a study to look into ways of improving the survivability of the U.S. nuclear arsenal.\n\nAccording to Graham Spinardi in his book \"From Polaris to Trident\" (1994), STRAT-X was a response by the U.S. Department of Defense's Deputy Director of Defense Research and Engineering, Lloyd Wilson, to the U.S. Air Force; the service was demanding a large ICBM called the WS-120A. Spinardi suggests that STRAT-X was allowed to proceed so it could terminate the study for such a missile. Funding for the WS-120A would not be released by Secretary McNamara, and plans for such a missile were canceled in 1967.\n\nThe study was named \"STRAT-X\" in order not to reveal its intentions, and also to eliminate partiality towards sea-, air- or land-based systems. It was conducted by the Research and Engineering Support Division of the independent and non-profit Institute for Defense Analyses (IDA), which had conducted a study in early 1966 titled \"Pen-X\", upon which STRAT-X was based. STRAT-X was chaired by President of the IDA, General Maxwell D. Taylor, while the institution's Fred Payne presided over STRAT-X's \"working\" panel. The panel also included executives from major independent corporations and defense contractors such as Boeing, Booz Allen Hamilton, Thiokol and TRW. The Advisory Committee members were mostly military officers, including U.S. Navy Rear Admirals George H. Miller and Levering Smith.\nOn 1 November 1966, McNamara signed an order authorizing STRAT-X, officially initiating the study. During STRAT-X, the working panel was \"encouraged to examine system concepts unrestrained by considerations of potential management problems or political influences.\" The Secretary wanted new ideas about \"path-breaking\" weapons systems that were either offensive or defensive in nature, unhindered by defense bureaucracy, which had the potential to stifle innovation. Sea-, land- and air-based missile systems were investigated, but manned bombers and orbital systems were not. The group was also asked to consider the cost effectiveness of all systems, as well to predict possible Soviet responses. To meet this requirement, a series of documents were written from the perspective of the Soviet Minister of Defense General Andrei Grechko, complete with anti-capitalistic statements and a prediction of the eventual triumph of socialism. In the end, a twenty-volume report covered no fewer than 125 different ideas for missile systems, nine of which were reviewed in great detail.\n\nOf the nine prospective weapons systems, five were land-based. These were: \"Rock Silo\"—a system where missiles would be stored in hardened silos of granite bedrock in the Western and Northern United States; \"Soft Silo\"—a similar system but with easily and cheaply constructed silos; \"Rock Tunnel\"—a system where missiles would be transported around in deep underground networks before emerging at launch points; \"Soft Tunnel\"—a similar tunnel but built more cheaply and easily; and \"Land Mobile\"—a truck-based system where road-transporters traveled at speeds up to constantly around a dedicated and winding road system in of public land.\n\nOf the remaining four, three were sea-based. These were: \"Canal-Based\"—a systems where missiles would be sailed in canals to confuse Soviet military planners; \"Ship-Based\"—a system where ships carrying missile canisters would travel around the world, hiding among other traffic; and \"Submarine-Based\"—a system where ballistic missile submarines would roam the oceans while carrying missile canisters outside their pressure hulls. The single air-based consideration was the \"Air Launched ICBM\", which required large aircraft carrying standoff ballistic missiles to launch their payloads at the Soviet Union.\n\nDespite the numerous options investigated during the study, none were fully implemented. Although the STRAT-X \"Land Mobile\" option resulted in the MGM-134 Midgetman and LGM-118 Peacekeeper missiles, the fall of communism throughout the late 1980s and early 1990s resulted in the Midgetman being canceled while still a prototype, while only 50 out of the original 100 Peacekeeper missiles were ever fielded. Nevertheless, the study did inspire a number of developments in nuclear weapons delivery systems. In October 1974, the U.S. Air Force successfully conducted an air launch of a Minuteman missile from a C-5 Galaxy, demonstrating the credibility of the \"Air Launched ICBM\" option of STRAT-X.\n\nAlthough the U.S. Navy then had several classes of ballistic missile submarines and submarine-launched ballistic missile (SLBM) in service, the study placed a significant emphasis on the survivability of SLBMs. This resulted in the enormous \"Ohio\"-class submarine and the Trident SLBMs which the \"Ohio\" class carried. The study originally called for dedicated slow-moving missile-carrying submarines (instead of converted attack submarines) to embark missiles outside their hulls and rely primarily on stealth for survivability. However, Admiral Hyman Rickover, director of the Naval Reactors office, wanted a boat capable of a burst of high speed in order to affect a safe \"getaway\" after launching the boat's payload. As a result, the \"Ohio\" class was designed to accommodate enormous nuclear reactors to produce the required speed. \"Ohio\"-class submarines carry their missiles inside of their hulls, despite STRAT-X's recommendation. \"Ohio\"-class submarines and Trident missiles are still in service .\nSTRAT-X had far-reaching effects on the development and deployment of U.S. nuclear forces. It was the first time that the strategic requirements of the U.S. Armed Forces were addressed in a detailed and analytical manner. In a 2002 report by the RAND Corporation, STRAT-X was described as \"one of the most influential analyses ever conducted\" for the U.S. Department of Defense. Journalist Peter Grier, in his \"Air Force\" magazine article \"STRAT-X\", described the study as \"a wide-ranging look at the future of U.S. weapons that shaped the nuclear triad for decades, and remains a model for such efforts today\". In 2006, the Defense Science Board (DFS) noted STRAT-X's introduction of ideas and concepts that resulted in the \"Ohio\"-class submarines and small and mobile ICBMs. The DFS also attributed the use of air-launched cruise missiles, particularly those carried by the B-52 Stratofortress, to STRAT-X despite their lack of references in the study.\n\nNotes\nReferences\nBibliography\n"}
{"id": "958508", "url": "https://en.wikipedia.org/wiki?curid=958508", "title": "Science North", "text": "Science North\n\nScience North () is an interactive science museum in Greater Sudbury, Ontario, Canada.\n\nThe science centre, which is Northern Ontario's most popular tourist attraction, consists of two snowflake-shaped buildings on the southwestern shore of Ramsey Lake, just south of the city's downtown core, and a former ice hockey arena which includes the complex's entrance and an IMAX theatre. The snowflake buildings are connected by a rock tunnel, which passes through a billion-year-old geologic fault. This fault line was not known to be under the complex during the construction of the building in the early 1980s. Where the walkway reaches the larger snowflake, the Vale Cavern auditorium is frequently used for temporary exhibits, press conferences, and other gala events by Science North and the wider community.\n\nInside the main building, a 20-metre fin whale skeleton recovered from Anticosti Island hangs from the ceiling.\n\nThe complex also features a boat tour, the \"William Ramsey\", which offers touring cruises of the scenic Ramsey Lake. The Jim Gordon Boardwalk also extends from the facility to the city's Bell Park along the western shore of the lake.\n\nThe facility was designed by architect Raymond Moriyama, one of the founding partners of Moriyama & Teshima Architects, based in Toronto.\n\nAn agency of the provincial government of Ontario, Science North is overseen by the provincial Ministry of Culture.\n\n\n\nThe exhibits on the third floor are divided into four main areas:\n\n\n\nSpecial film and video exhibits which change over time; current exhibits include\n\n\nScience North, which was opened in 1984, also owns and operates Sudbury's Dynamic Earth facility, an earth sciences exhibition which is home to the Big Nickel, one of the city's most famous landmarks. From January 22, 2001 to May 10, 2003, the Big Nickel was temporarily located on the primary Science North grounds while Dynamic Earth was under construction.\n\nThe Science North Production Team produces object theatres, multi-media presentations and large format film productions for science museums and educational facilities around North America.\n\nScience North's former science director, Alan Nursall, is a correspondent for the Canadian science newsmagazine series \"Daily Planet\", which airs on Discovery Channel Canada and CTV. The institution's first science director, David Pearson, returned to the position in 2007.\n\nScience North has also worked extensively with the city's Laurentian University on scientific and environmental research and as a partner in the university's graduate program in science communication.\n\nScience North runs day camps in the summer, autumn, and winter.\n\n"}
{"id": "57734572", "url": "https://en.wikipedia.org/wiki?curid=57734572", "title": "Sneathia", "text": "Sneathia\n\nSneathia is a Gram-negative,rod-shaped, non-spore-forming and non-motile genus of bacteria from the family of Leptotrichiaceae. \"Sneathia\" is named of the microbiologist H. A. Snaeth.\n\n"}
{"id": "12162818", "url": "https://en.wikipedia.org/wiki?curid=12162818", "title": "Soyuz 7K-OK", "text": "Soyuz 7K-OK\n\nSoyuz 7K-OK was the first generation of Soyuz spacecraft in use from 1967 to 1971.\nThis first generation was used for the first ferry flights to the Salyut space station program; Soyuz spacecraft in their current generation are still in use to ferry crew to and from the ISS.\n\nThis generation is notable for the only fatalities of the Soyuz programme , with Soyuz 1 in 1967 (sole crew-member killed by parachute failure) and Soyuz 11 in 1971 (crew killed by depressurization during reentry).\n\nThe first unmanned automated docking in the history of spaceflight, between Kosmos 186 and Kosmos 188 in 1967, was achieved with this generation of Soyuz spacecraft. The generation encompasses furthermore the first docking between two manned spacecraft (Soyuz 4 and Soyuz 5), the longest manned flight involving only one spacecraft (the 18-day flight of Soyuz 9 in 1970) and the first successful manning of the first space station in the history of space flight (Soyuz 11 and Salyut 1 in 1971).\n\nThe Soyuz 7K-OK vehicles carried a crew of up to three without spacesuits. The craft can be distinguished from those following by their bent solar panels and their use of the Igla automatic docking navigation system, which required special radar antennas.\n\nThe 7K-OK was primarily intended as a variant of the 7K-LOK (the lunar mission Soyuz) for Earth orbital testing. Mostly the same vehicle, it lacked the larger antenna needed to communicate at lunar distance. The early Soyuz models also sported an external toroidal fuel tank surrounding the engines and meant to store extra propellant for lunar flights, but it was left empty on Soyuz 1-9. After the spacecraft was converted to a space station ferry, the tank was removed.\n\nSoyuz 7K-OK spacecraft had a \"probe and drogue\" docking mechanism to connect with other spacecraft in orbit, in order to gather engineering data as a preparation for the Soviet space station program. There were two variants of Soyuz 7K-OK: Soyuz 7K-OK(A) featuring an active \"probe\" docking port, and Soyuz 7K-OK(P) featuring a passive \"drogue\" docking target. The docking mechanisms of 7K-OK and 7K-LOK did not allow internal transfer (this feature was added on the 7K-OKS version), thus cosmonauts had to spacewalk between docked modules. This procedure was conducted successfully on the joint Soyuz 4 and Soyuz 5 missions, where Aleksei Yeliseyev and Yevgeny Khrunov transferred from their Soyuz 5 to the Soyuz 4 craft.\n\nThe first unmanned test of this version was Kosmos 133, launched on November 28, 1966.\n\nThe last two Soyuz space craft of this series were of the designation Soyuz 7K-OKS. In contrast to Soyuz 7K-OK, which docking mechanism lacked a hatch for internal crew transfer, the Soyuz 7K-OKS spacecraft were modified to utilize the new SSVP docking system that allowed internal crew transfer – this was performed for the first time with the manning of the Salyut 1 space station by Soyuz 11. The SSVP docking adapter is still in use until today at the ISS.\n\n\n\n"}
{"id": "12162956", "url": "https://en.wikipedia.org/wiki?curid=12162956", "title": "Soyuz 7K-T", "text": "Soyuz 7K-T\n\nThe second generation of the Soyuz spacecraft, the Soyuz Ferry or Soyuz 7K-T, comprised Soyuz 12 through Soyuz 40 (1973-1981). In the wake of the Soyuz 11 tragedy, the spacecraft was redesigned to accommodate two cosmonauts who would wear pressure suits at all times during launch, docking, undocking, and reentry. The place of the third cosmonaut was taken by extra life-support systems. Finally, the 7K-T, being intended purely as a space station ferry, had no solar panels, instead sporting two large whip antennas in their place. As a result, it relied on batteries which only provided enough power for two days of standalone flight. The idea was that the Soyuz would recharge while docked with a Salyut space station, but in the event of a docking or other mission failure (which ended up happening on several occasions), the crew was forced to power off everything except communications and life support systems until they could reenter.\n\nTwo test flights of the 7K-T were conducted prior to committing the redesigned Soyuz to a manned mission. Kosmos 496 was launched on June 26, 1972 and spent a week in space, part of it in powered-down mode. Then on September 2, an attempted launch of a Zenit reconnaissance satellite failed to orbit due to a malfunction of the vernier engines on the Blok A stage. The existing stock of Soyuz boosters had to be modified to prevent a recurrence of this failure mode on a manned mission, which delayed the next test until almost a year later when Kosmos 573 launched on June 15, 1973 and spent two days in space. With this done, the way was cleared for the first manned test, Soyuz 12, in September.\n\nIn addition, the standalone flights of Soyuz 13, Soyuz 16, Soyuz 19, and Soyuz 22 used a variant of the 7K-T with solar panels, and in the case of 13 and 22, special camera apparatus in place of the docking mechanism. A large Orion 2 astrophysical camera for imaging the sky and Earth were used on the former and an MKF-6 Zeiss camera on the latter.\n\nAnother modification was the \"Soyuz 7K-T/A9\" used for the flights to the military Almaz space station. This featured the ability to remote control the space station and a new parachute system and other still classified and unknown changes.\n\n\n\n"}
{"id": "45229676", "url": "https://en.wikipedia.org/wiki?curid=45229676", "title": "Standard Cost Coding System", "text": "Standard Cost Coding System\n\nA Standard Cost Coding System is a system of cost classification that can be used for benchmarking cost and quantities data. In the Norwegian oil and gas industry, NORSOK Z-014 developed as part of the NORSOK standards. ISO is also developing a Standard Cost coding System as an extension of NORSOK Z-014 under ISO 19008.\n\nTypically a Cost Classification system would be used to classify, activities, resources and product structure. In NORSOK Z-014, these classification taxonomies are called Standard Activity Breakdown (SAB), Code of Resources (COR) and Physical Breakdown Structure (PBS).\n"}
{"id": "6071361", "url": "https://en.wikipedia.org/wiki?curid=6071361", "title": "Sternalis muscle", "text": "Sternalis muscle\n\nThe sternalis muscle is an anatomical variation that lies in front of the sternal end of the pectoralis major parallel to the margin of the sternum. The sternalis muscle may be a variation of the pectoralis major or of the rectus abdominis.\n\nThe sternalis is a muscle that runs along the anterior aspect of the body of the sternum. It lies superficially and parallel to the sternum. Its origin and insertion are variable. The sternalis muscle often originates from the upper part of the sternum and can display varying insertions such as the pectoral fascia, lower ribs, costal cartilages, rectus sheath, aponeurosis of the abdominal external oblique muscle. There is still a great deal of disagreement about its innervation and its embryonic origin.\n\nIn a review, it was reported that the muscle was innervated by the external or internal thoracic nerves in 55% of the cases, by the intercostal nerves in 43% of the cases, while the remaining cases were supplied by both nerves. However, innervation by the pectoral nerves has also been reported. This appears to indicate that the sternalis is not always derived from the same embryonic origin.\n\nCadaveric studies showed that the sternalis muscle has a mean prevalence of around 7.8% in the population with the range from 0.5% to 23.5%. It has a slightly higher incidence in females. Though, It was proposed that a possible reason for the high prevalence may result from the existence of small, ill-defined or tendinous fibres, which could be misidentified for a sternalis muscle.\n\nA recent study classified the sternalis into three types depending on morphology.\nType I, the single head and single belly was seen in the majority of reported cases (58.5%), type II in 18.1%, and type III in 23.4%.\n\nIn addition to the above classification, triple-bellied/double-headed sternalis has also been reported.\n\nThe function of the sternalis muscle remains unknown. There are many theories for the physiological function of the sternalis. It may function as a proprioceptive sensor for thoracic wall movements. It may also take part in the movement of the shoulder joint or have an additional role in elevation of the chest wall.\n\nThe presence of the sternalis is asymptomatic but aesthetic complaints have been reported as it was reported to cause chest asymmetry or deviation of the nipple-areola complex. The presence of the sternalis may cause alterations in the electrocardiogram or confusion in mammography. However, there is a potential benefit of the muscle as it can be used as a flap in a reconstructive surgery of the head and neck and the anterior chest wall.\n\nThe sternalis was first reported by Carbolius in 1604 and the name was first given by Turner in 1867. Different terminologies have been given to the sternalis due to its highly varied morphology and the disagreement on its embryonic origin. The sternalis was referred to as the \"rectus sternalis,\" \"sternalis brutorum, musculus sternalis, episternalis, parasternalis, presternalis, rectus sterni, rectus thoracis, rectus thoracicus superficialis, superficial rectus abdominis, sternalis brutorum, japonicas, and thoracicus\" depending on studies\".\"\n"}
{"id": "26014806", "url": "https://en.wikipedia.org/wiki?curid=26014806", "title": "Swedish Radiation Safety Authority", "text": "Swedish Radiation Safety Authority\n\nThe Swedish Radiation Safety Authority () is the Swedish government authority responsible for radiation protection. It sorts under the Ministry of the Environment.\n\nIt was created on 1 July 2008 with the merging of the Swedish Nuclear Power Inspectorate and the Swedish Radiation Protection Authority. It employs 300 people and is located in Stockholm, with an annual budget of about 400 millions Swedish krona.\n\nIts Director-General is Mats Persson.\n\nOfficial Website in English\n"}
{"id": "10918014", "url": "https://en.wikipedia.org/wiki?curid=10918014", "title": "Willibald Jentschke", "text": "Willibald Jentschke\n\nWillibald Jentschke (Vienna, Austria-Hungary, 6 December 1911 – Göttingen, Germany, 11 March 2002) was an Austrian-German experimental nuclear physicist.\n\nDuring World War II, he made contributions to the German nuclear energy project.\n\nAfter World War II, he emigrated to the United States to work at Wright-Patterson Air Force Base, in Ohio, for the Air Force Materiel Command.\nIn 1950, he became a professor at the University of Illinois at Urbana–Champaign, where he became director of the Cyclotron Laboratory there in 1951.\n\nIn 1956, he became a professor of physics at the University of Hamburg and spearheaded the effort to build the 7.5 GeV electron synchrotron DESY, the foundation of which was in December 1959. He was director of DESY for 10 years. In 1971, he became Director General of CERN Laboratory I for the next five years. He retired from the University of Hamburg in 1980.\n\nJentschke studied physics at the University of Vienna, from 1930 to 1936. He received his doctorate under Georg Stetter in 1935.\n\nFrom 1937 to 1942, Jentschke was a teaching assistant to Georg Stetter at the University of Vienna. From 1942 to 1945, he was a lecturer at the University of Vienna. During World War II, Jentschke was also \"\" (Scientific Assistant) at the \"II. Physikalisches Institut der Universität, Wien\" (Second Physics Institute of the University of Vienna), where Georg Stetter was the director. One of Jentschke's colleagues there was Josef Schintlmeister. The Institute did research on transuranic elements and measurement of nuclear constants, in collaboration with the \"Institut für Radiumforschung\" (Institute for Radium Research) of the \"Österreichischen Adademie der Wissenschaften\" (Austrian Academy of Sciences). This work was done under the German nuclear energy project, also known as the \"Uranverein\" (Uranium Club); see, for example, the publications cited below under \"Internal Reports\".\n\nIn 1939, John Archibald Wheeler and Niels Bohr proposed the liquid-drop model of nuclear fission. Their work suggested that uranium 235 was responsible for thermal neutron fission. This was borne out by the work of Eugene T. Booth, John R. Dunning, A. V. Grosse, and Alfred O. C. Nier, which was submitted for publication in the spring of 1940. Jentschke, F. Prankl, and F. Hernegger also substantiated the Bohr-Wheeler claims shortly after the American work by observing the phenomenon in an isotope of thorium, thorium 230.\n\nFrom 1946 to 1947, Jentschke was a lecturer at the University of Innsbruck.\n\nNear the close and after the end of World War II in Europe, the Russians and the Western powers had programs to foster technology transfer and exploit German technical specialists. For example, the U.S. had Operation Paperclip and the Russians had trophy brigades advancing with their military forces. In the area of atomic technology, the U.S. had Operation Alsos and the Russians had their version. While operational aspects of the Russian operation were modeled after the trophy brigades, a more refined approach was warranted for the exploitation of German atomic related facilities, intellectual materials, and scientific personnel. This was rectified with a decree in late 1944 and the formation of specialize exploitation teams in early 1945 under the Russian Alsos, which had broader objectives, which included wholesale relocation of scientific facilities to the Soviet Union.\n\nJentschke emigrated to the United States under Operation Paperclip, where he worked at the Air Force Materiel Command (today, the Air Force Logistics Command after merger with the Air Force Systems Command in 1992), at Wright-Patterson Air Force Base, Ohio, from 1947 to 1948. On his way to the United States, Jentschke wrote to Walther Bothe that his reasons for going there was to do real scientific work, which then not possible in Austria and Germany.\n\nIn 1950, Jentschke became a resident assistant professor, and in 1955 resident professor, in the Department of Physics at the University of Illinois at Urbana–Champaign. In 1951, he became director of the Cyclotron Laboratory there.\n\nDuring 1956 and 1957, Jentschke was a member of the \"Arbeitskreis Kernphysik\" (Nuclear Physics Working Group) of the \"Fachkommission II \"Forschung und Nachwuchs\"\" (Commission II \"Research and Growth\") of the \"Deutschen Atomkommission\" (DAtK, German Atomic Energy Commission). Other members of the Nuclear Physics Working Group in both 1956 and 1957 were: Werner Heisenberg (chairman), Hans Kopfermann (vice-chairman), Fritz Bopp, Walther Bothe, Wolfgang Gentner, Otto Haxel, Heinz Maier-Leibnitz, Josef Mattauch, Wolfgang Riezler, Wilhelm Walcher, and Carl Friedrich von Weizsäcker. Wolfgang Paul was also a member of the group during 1957.\n\nIn 1956, Jentschke became an ordinarius Professor of Physics at the University of Hamburg. There, he found a positive climate, as well as funding, for his vision of building a new institute around a particle accelerator. An international particle accelerator conference at CERN in 1956 was helpful in the decision of which accelerator to build. His vision could not be supported by Hamburg alone, so negotiations took place to bring in support of the Federal Republic of Germany and the states of Germany (\"Länder\"). A financial agreement was signed on 18 December 1959, which founded the \"Deutsches Elektronen-Synchrotron\" (DESY), a 7.5 GeV electron synchrotron. Jentschke was chairman of the DESY Board of Directors from 1959 to 1970, and for many years also the director of the Second Institute of Experimental Physics at the University of Hamburg. While at DESY, Jentschke endorsed the electron-positron storage ring scheme for the DORIS accelerator, and promoted the use of synchrotron radiation for research purposes.\n\nIn 1971, Jentschke accepted the post as Director General of CERN Laboratory I (the Meyrin site); John Adams was Director General of the neighboring CERN Laboratory II (Prévessin), where the new SPS proton synchrotron was being constructed. They shared the directorship of CERN until the two laboratories were united in 1976. While Director General, Jentschke oversaw the exploitation of the new research tool, the Intersecting Storage Rings, which began operation in 1971.\n\nJentschke retired from the University of Hamburg in 1980.\n\n\nThe following reports were published in \"Kernphysikalische Forschungsberichte\" (\"Research Reports in Nuclear Physics\"), an internal publication of the German \"Uranverein\". The reports were classified Top Secret, they had very limited distribution, and the authors were not allowed to keep copies. The reports were confiscated under the Allied Operation Alsos and sent to the United States Atomic Energy Commission for evaluation. In 1971, the reports were declassified and returned to Germany. The reports are available at the Karlsruhe Nuclear Research Center and the American Institute of Physics.\n\n\n\n"}
{"id": "26772312", "url": "https://en.wikipedia.org/wiki?curid=26772312", "title": "Workplace listening", "text": "Workplace listening\n\nWorkplace listening is a type of active listening that is generally employed in a professional environment. Listening skills are imperative for career success, organizational effectiveness, and worker satisfaction. Workplace listening includes understanding the listening process (i.e. perception, interpretation, evaluation, and action) and its barriers that hamper the flow of that process. Like other skills, there are specific techniques for improving workplace listening effectiveness. Moreover, it is imperative to become aware of the role of nonverbal communication in communicating in the workplace, as understanding messages wholly entails more than simple verbal messages.\n\n\n\n"}
{"id": "37061819", "url": "https://en.wikipedia.org/wiki?curid=37061819", "title": "Wszechświat", "text": "Wszechświat\n\nWszechświat (\"The Universe\") is a Polish popular-science magazine, currently issued as quarterly by Polish Copernicus Society of Naturalists, supported by AGH University of Science and Technology and Polish Academy of Learning.\n\n\"Wszechświat\" was founded in 1882 as a biweekly, initiators were students and teachers of The Main School in Warsaw. First editor-in-chief of the magazine was chemist Bronisław Znatowicz. He was leading the journal for many years. In 1914, when World War I began, \"Wszechświat\" was closed and Znatowicz left.\n\nThe magazine was reactivated as a monthly in 1927. Since 1929 the editor-in-chief was a biologist Jan Bohdan Dembowski. In 1930, in the result of Dembowski's activity, Polish Copernicus Society of Naturalists took mastership over \"Wszechświat\" and in 1934 editorial office was moved to Vilnius. In 1939 the journal was closed again because of World War II.\n\nIt was brought back to life by geologist Kazimierz Maślankiewicz and zoologist Zygmunt Grodziński in 1945 in Kraków. Since 1981 until 2002 the editor-in-chief was pharmacologist and biochemist Jerzy Vetulani.\n\nNowadays the whole title is 'Wszechświat. Pismo Przyrodnicze' (the English translation: \"The Universe. Magazine of Nature\").\n\n\n"}
{"id": "31122905", "url": "https://en.wikipedia.org/wiki?curid=31122905", "title": "Yorks Islands", "text": "Yorks Islands\n\nYorks Islands, also known as \"Yorks 8 Islands\" or \"York's Islands\" or simply \"York Island(s)\" are a group of several islands in the flood plain of the Missouri River, in Broadwater County, Montana, about 4 miles south (up-river) from Townsend, Montana, along U.S. Highway 287. The islands were named by the Lewis and Clark Expedition (1803–1806) for Clark's servant/slave York, when the expedition passed this way in 1805 on their historic journey of exploration to the Pacific Ocean. The islands may be accessed from U.S.287, as a Montana Fishing Access site.\n\nThe islands were named for York (ca. 1770-1831), Captain William Clark's lifelong slave companion, and \"body servant\" who accompanied Clark on the expedition to the Pacific and back.\n\nThe expedition passed this point on 24 July 1805. However, the naming of \"Yorks 8 Islands\" is not found in the narrative journals of Lewis and Clark for that day. Instead it is found in Clark’s tabulations on his map drawings, and in his list of “Creeks and Rivers,”. Clark's map for this area has the entry, “Yorks 8 Islands,” and under related remarks in Clark's \"Creeks and Rivers\" is “W.C. on land York tired.”\n\nOn Wednesday, July 24, 1805 Meriwether Lewis made the following entry in his journal, which describes his thoughts about the formation of Yorks Islands. \"we saw many beaver and some otter today; the former dam up the small channels of the river between the islands and compell the river in these parts to make other channels; which as soon as it has effected that which was stopped by the beaver becomes dry and is filled up with mud sand gravel and driftwood. the beaver is then compelled to seek another spot for his habitation wher[e] he again erects his dam. thus the river in many places among the clusters of islands is constantly changing the direction of such sluices as the beaver are capable of stoping or of 20 yds in width. this anamal in that way I believe to be very instrumental in adding to the number of islands with which we find the river crouded.\"\n\nThe captains followed the practice of naming geographic features after prominent persons who somehow had been connected with the expedition, particularly the President and members of his cabinet, (viz Jefferson, Madison, and Gallatin Rivers) or attributes of President Jefferson (viz. Philanthropy, Philosophy, and Wisdom Rivers). In addition, as far as can be determined, a geographic feature was named for every Corps member, including Seaman, Lewis’s Newfoundland dog.\n\nIn 1806, on the return leg of the expedition down the Yellowstone River, Clark also named another geographical feature for York, \"York's Dry Creek\", a tributary of the Yellowstone River, in Custer County, Montana. This name did not stick and the creek became known as \"Custer Creek\".\n\nThough the name for \"Yorks 8 Islands\" appears on Clark's 1805 maps of the expeditions travel along the Missouri for July 24, 1805, the geographical designation for \"Yorks Islands\" was not officially confirmed until 2000, when the U.S. Board on Geographic Names approved the name Yorks Islands for the group of Missouri River islands in Broadwater County. The name was also approved by the Montana Board on Geographic Names and has been entered into the Nation’s official automated geographic names repository. The USGS (United States Geological Survey) Geographic Names Information System has settled on \"Yorks Islands\". However, the area is also called \"York's Islands\", or simply \"York Island\".\n\nThe island group are now incorporated into \"York's Islands Fishing Access Site\", maintained by the Montana Fish, Wildlife and Parks Department. This site is located on 22 acres along the east side of Missouri River 4 miles south of Townsend. The site is just off of U.S. Highway 287. The site is marked by a Montana Fish and Wildlife Department sign. It includes a boat launch, picnic tables, fishing access, camping sites (including some for small trailers) and toilets. There is a small fee. There is no water or any other services.\n\nGetting to Yorks Island: Utilize Google Earth for this area of Montana, or any map of the area, or simply drive 4 miles south of Townsend, Montana on U.S. Highway 287, and look for the fishing access sign on the west side of the highway.\n\n\n"}
