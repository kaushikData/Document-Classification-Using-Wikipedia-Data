{"id": "13613681", "url": "https://en.wikipedia.org/wiki?curid=13613681", "title": "Australasian Science", "text": "Australasian Science\n\nAustralasian Science is a bimonthly science magazine published in Australia. It contains a mixture of news items, feature articles and expert commentary.\n\n\"Australasian Science\" is Australia's longest-running scientific publication. It was first published in 1938 as \"The Australian Journal of Science\" by the Australian National Research Council, which was the forerunner of the Australian Academy of Science.\n\nIn 1954 the journal was transferred to ANZAAS – the \"Australian and New Zealand Association for the Advancement of Science\". Throughout this time the journal published the research of eminent Australian scientists, including Sir Douglas Mawson and Sir Frank Macfarlane Burnet, whose groundbreaking clonal selection theory was published in the journal in 1957.\n\nThe journal has evolved considerably over the following decades. Since merging in 1998 with a popular science magazine published by the University of Southern Queensland, the magazine has been published as \"Australasian Science\". Published by Control Publications and available in newsagents, it is the only magazine that is dedicated to Australian and New Zealand science.\n\n\"Australasian Science\"<nowiki>'</nowiki>s patrons are Nobel Laureate Prof Peter C. Doherty and ABC broadcaster Robyn Williams.\n\n\"Australasian Science\" also has a broad team of columnists covering astronomy, politics, biodiversity, ethics, scepticism, neuroscience and evidence-based medicine.\n\n\n"}
{"id": "40319313", "url": "https://en.wikipedia.org/wiki?curid=40319313", "title": "Biomesotherapy", "text": "Biomesotherapy\n\nBiomesotherapy is an alternative therapy practice that combines homotoxicology, mesotherapy, and acupuncture. Saline solution and homeopathic formulations are injected subcutaneously at specific acupuncture or trigger points, and homeopathic formulations are administered orally during treatment sessions. Biomesotherapy is used for pain management and general well-being.\n\n\"This article incorporates public domain text from the CDC as cited.\"\n"}
{"id": "56024424", "url": "https://en.wikipedia.org/wiki?curid=56024424", "title": "Biotic pump", "text": "Biotic pump\n\nThe theory of a biotic pump pertains to the importance of forests in the water cycle, specifically, in determining the levels of rainfall a region will receive. It states that an increased amount of evaporation or transpiration will cause a reduction in atmospheric pressure as clouds form, which will subsequently cause moist air to be drawn to regions where evaporation is at its highest. In a desert this will correspond to the sea whereas in a forest, moist air from the sea will be drawn inland. The theory predicts two different types of coast to contentinental rainfall patterns, first in a forested area one can expect no decrease in rainfall as one moves inland in contrast to a deforested region where one observes an exponential decrease in annual rainfall. Whereas it is true current global climate models fit these patterns as well, it is argued this is due to parametrization and not the veracity of the theories.\n\nThis theory is in contradiction of the more traditional view that surface winds are solely a direct product of differences in surface heating and heat released from condensation. The creators of the theory argue that phase changes in water play a greater role in atmospheric dynamics than currently acknowledged.\n\n"}
{"id": "7555", "url": "https://en.wikipedia.org/wiki?curid=7555", "title": "Casimir effect", "text": "Casimir effect\n\nIn quantum field theory, the Casimir effect and the Casimir–Polder force are physical forces arising from a quantized field. They are named after the Dutch physicist Hendrik Casimir who predicted them in 1948.\n\nThe Casimir effect can be understood by the idea that the presence of conducting metals and dielectrics alters the vacuum expectation value of the energy of the second quantized electromagnetic field. Since the value of this energy depends on the shapes and positions of the conductors and dielectrics, the Casimir effect manifests itself as a force between such objects.\n\nAny medium supporting oscillations has an analogue of the Casimir effect. For example, beads on a string as well as plates submerged in noisy water or gas illustrate the Casimir force.\n\nIn modern theoretical physics, the Casimir effect plays an important role in the chiral bag model of the nucleon; in applied physics, it is significant in some aspects of emerging microtechnologies and nanotechnologies.\n\nThe typical example is of the two uncharged conductive plates in a vacuum, placed a few nanometers apart. In a classical description, the lack of an external field means that there is no field between the plates, and no force would be measured between them. When this field is instead studied using the quantum electrodynamic vacuum, it is seen that the plates do affect the virtual photons which constitute the field, and generate a net force – either an attraction or a repulsion depending on the specific arrangement of the two plates. Although the Casimir effect can be expressed in terms of virtual particles interacting with the objects, it is best described and more easily calculated in terms of the zero-point energy of a quantized field in the intervening space between the objects. This force has been measured and is a striking example of an effect captured formally by second quantization.\n\nThe treatment of boundary conditions in these calculations has led to some controversy. In fact, \"Casimir's original goal was to compute the van der Waals force between polarizable molecules\" of the conductive plates. Thus it can be interpreted without any reference to the zero-point energy (vacuum energy) of quantum fields.\n\nBecause the strength of the force falls off rapidly with distance, it is measurable only when the distance between the objects is extremely small. On a submicron scale, this force becomes so strong that it becomes the dominant force between uncharged conductors. In fact, at separations of 10 nm – about 100 times the typical size of an atom – the Casimir effect produces the equivalent of about 1 atmosphere of pressure (the precise value depending on surface geometry and other factors).\n\nDutch physicists Hendrik Casimir and Dirk Polder at Philips Research Labs proposed the existence of a force between two polarizable atoms and between such an atom and a conducting plate in 1947; this special form is called the Casimir–Polder force. After a conversation with Niels Bohr, who suggested it had something to do with zero-point energy, Casimir alone formulated the theory predicting a force between neutral conducting plates in 1948 which is called the Casimir effect in the narrow sense.\n\nPredictions of the force were later extended to finite-conductivity metals and dielectrics, and recent calculations have considered more general geometries. Experiments before 1997 had observed the force qualitatively, and indirect validation of the predicted Casimir energy had been made by measuring the thickness of liquid helium films. However it was not until 1997 that a direct experiment by S. Lamoreaux quantitatively measured the force to within 5% of the value predicted by the theory. Subsequent experiments approach an accuracy of a few percent.\n\nThe causes of the Casimir effect are described by quantum field theory, which states that all of the various fundamental fields, such as the electromagnetic field, must be quantized at each and every point in space. In a simplified view, a \"field\" in physics may be envisioned as if space were filled with interconnected vibrating balls and springs, and the strength of the field can be visualized as the displacement of a ball from its rest position. Vibrations in this field propagate and are governed by the appropriate wave equation for the particular field in question. The second quantization of quantum field theory requires that each such ball-spring combination be quantized, that is, that the strength of the field be quantized at each point in space. At the most basic level, the field at each point in space is a simple harmonic oscillator, and its quantization places a quantum harmonic oscillator at each point. Excitations of the field correspond to the elementary particles of particle physics. However, even the vacuum has a vastly complex structure, so all calculations of quantum field theory must be made in relation to this model of the vacuum.\n\nThe vacuum has, implicitly, all of the properties that a particle may have: spin, or polarization in the case of light, energy, and so on. On average, most of these properties cancel out: the vacuum is, after all, \"empty\" in this sense. One important exception is the vacuum energy or the vacuum expectation value of the energy. The quantization of a simple harmonic oscillator states that the lowest possible energy or zero-point energy that such an oscillator may have is\n\nSumming over all possible oscillators at all points in space gives an infinite quantity. Since only \"differences\" in energy are physically measurable (with the notable exception of gravitation, which remains beyond the scope of quantum field theory), this infinity may be considered a feature of the mathematics rather than of the physics. This argument is the underpinning of the theory of renormalization. Dealing with infinite quantities in this way was a cause of widespread unease among quantum field theorists before the development in the 1970s of the renormalization group, a mathematical formalism for scale transformations that provides a natural basis for the process.\n\nWhen the scope of the physics is widened to include gravity, the interpretation of this formally infinite quantity remains problematic. There is currently no compelling explanation as to why it should not result in a cosmological constant that is many orders of magnitude larger than observed. However, since we do not yet have any fully coherent quantum theory of gravity, there is likewise no compelling reason as to why it should.\n\nThe Casimir effect for fermions can be understood as the spectral asymmetry of the fermion operator formula_2, where it is known as the Witten index.\n\nAlternatively, a 2005 paper by Robert Jaffe of MIT states that \"Casimir effects\ncan be formulated and Casimir forces can be computed without reference to zero-point energies. They are relativistic, quantum forces between charges and currents. The Casimir force (per unit\narea) between parallel plates vanishes as alpha, the fine structure constant, goes to zero, and the standard result, which appears to be independent of alpha, corresponds to the alpha approaching infinity limit,\" and that \"The Casimir force is simply the (relativistic, retarded) van der Waals force between the metal plates.\" Casimir and Polder's original paper used this method to derive the Casimir-Polder force. In 1978, Schwinger, DeRadd, and Milton published a similar derivation for the Casimir Effect between two parallel plates. In fact, the description in terms of van der Waals forces is the only correct description from the fundamental microscopic perspective, while other descriptions of Casimir force are merely effective macroscopic descriptions.\n\nCasimir's observation was that the second-quantized quantum electromagnetic field, in the presence of bulk bodies such as metals or dielectrics, must obey the same boundary conditions that the classical electromagnetic field must obey. In particular, this affects the calculation of the vacuum energy in the presence of a conductor or dielectric.\n\nConsider, for example, the calculation of the vacuum expectation value of the electromagnetic field inside a metal cavity, such as, for example, a radar cavity or a microwave waveguide. In this case, the correct way to find the zero-point energy of the field is to sum the energies of the standing waves of the cavity. To each and every possible standing wave corresponds an energy; say the energy of the \"n\"th standing wave is formula_3. The vacuum expectation value of the energy of the electromagnetic field in the cavity is then\n\nwith the sum running over all possible values of \"n\" enumerating the standing waves. The factor of 1/2 is present because the zero-point energy of the n'th mode is formula_5, where formula_3 is the energy increment for the n'th mode. (It is the same 1/2 as appears in the equation formula_7.) Written in this way, this sum is clearly divergent; however, it can be used to create finite expressions.\n\nIn particular, one may ask how the zero-point energy depends on the shape \"s\" of the cavity. Each energy level formula_3 depends on the shape, and so one should write formula_9 for the energy level, and formula_10 for the vacuum expectation value. At this point comes an important observation: the force at point \"p\" on the wall of the cavity is equal to the change in the vacuum energy if the shape \"s\" of the wall is perturbed a little bit, say by formula_11, at point \"p\". That is, one has\n\nThis value is finite in many practical calculations.\n\nAttraction between the plates can be easily understood by focusing on the one-dimensional situation. Suppose that a moveable conductive plate is positioned at a short distance \"a\" from one of two widely separated plates (distance \"L\" apart). With \"a\" « \"L\", the states within the slot of width \"a\" are highly constrained so that the energy \"E\" of any one mode is widely separated from that of the next. This is not the case in the large region \"L\", where there is a large number (numbering about \"L\" / \"a\") of states with energy evenly spaced between \"E\" and the next mode in the narrow slot – in other words, all slightly larger than \"E\". Now on shortening \"a\" by d\"a\" (< 0), the mode in the narrow slot shrinks in wavelength and therefore increases in energy proportional to −d\"a\"/\"a\", whereas all the \"L\" /\"a\" states that lie in the large region lengthen and correspondingly decrease their energy by an amount proportional to d\"a\"/\"L\" (note the denominator). The two effects nearly cancel, but the net change is slightly negative, because the energy of all the \"L\"/\"a\" modes in the large region are slightly larger than the single mode in the slot. Thus the force is attractive: it tends to make \"a\" slightly smaller, the plates attracting each other across the thin slot.\n\n\nIn the original calculation done by Casimir, he considered the space between a pair of conducting metal plates at distance formula_13 apart. In this case, the standing waves are particularly easy to calculate, because the transverse component of the electric field and the normal component of the magnetic field must vanish on the surface of a conductor. Assuming the plates lie parallel to the \"xy\"-plane, the standing waves are\n\nwhere formula_15 stands for the electric component of the electromagnetic field, and, for brevity, the polarization and the magnetic components are ignored here. Here, formula_16 and formula_17 are the wave numbers in directions parallel to the plates, and\n\nis the wave-number perpendicular to the plates. Here, \"n\" is an integer, resulting from the requirement that ψ vanish on the metal plates. The frequency of this wave is\n\nwhere \"c\" is the speed of light. The vacuum energy is then the sum over all possible excitation modes. Since the area of the plates is large, we may sum by integrating over two of the dimensions in \"k\"-space. The assumption of periodic boundary conditions yields,\n\nwhere \"A\" is the area of the metal plates, and a factor of 2 is introduced for the two possible polarizations of the wave. This expression is clearly infinite, and to proceed with the calculation, it is convenient to introduce a regulator (discussed in greater detail below). The regulator will serve to make the expression finite, and in the end will be removed. The zeta-regulated version of the energy per unit-area of the plate is\n\nIn the end, the limit formula_22 is to be taken. Here \"s\" is just a complex number, not to be confused with the shape discussed previously. This integral/sum is finite for \"s\" real and larger than 3. The sum has a pole at \"s\"=3, but may be analytically continued to \"s\"=0, where the expression is finite. The above expression simplifies to:\n\nwhere polar coordinates formula_24 were introduced to turn the double integral into a single integral. The formula_25 in front is the Jacobian, and the formula_26 comes from the angular integration. The integral converges if Re[\"s\"] > 3, resulting in\n\nThe sum diverges at \"s\" in the neighborhood of zero, but if the damping of large-frequency excitations corresponding to analytic continuation of the Riemann zeta function to \"s\"=0 is assumed to make sense physically in some way, then one has\n\nBut\n\nand so one obtains\n\nThe analytic continuation has evidently lost an additive positive infinity, somehow exactly accounting for the zero-point energy (not included above) outside the slot between the plates, but which changes upon plate movement within a closed system. The Casimir force per unit area formula_31 for idealized, perfectly conducting plates with vacuum between them is\n\nwhere\n\nThe force is negative, indicating that the force is attractive: by moving the two plates closer together, the energy is lowered. The presence of formula_33 shows that the Casimir force per unit area formula_31 is very small, and that furthermore, the force is inherently of quantum-mechanical origin.\n\nNOTE: In Casimir's original derivation , a moveable conductive plate is positioned at a short distance \"a\" from one of two widely separated plates (distance \"L\" apart). The 0-point energy on \"both\" sides of the plate is considered. Instead of the above \"ad hoc\" analytic continuation assumption, non-convergent sums and integrals are computed using Euler–Maclaurin summation with a regularizing function (e.g., exponential regularization) not so anomalous as formula_38 in the above.\n\nCasimir's analysis of idealized metal plates was generalized to arbitrary dielectric and realistic metal plates by Lifshitz and his students. Using this approach, complications of the bounding surfaces, such as the modifications to the Casimir force due to finite conductivity, can be calculated numerically using the tabulated complex dielectric functions of the bounding materials. Lifshitz's theory for two metal plates reduces to Casimir's idealized 1/\"a\" force law for large separations \"a\" much greater than the skin depth of the metal, and conversely reduces to the 1/\"a\" force law of the London dispersion force (with a coefficient called a Hamaker constant) for small \"a\", with a more complicated dependence on \"a\" for intermediate separations determined by the dispersion of the materials.\n\nLifshitz' result was subsequently generalized to arbitrary multilayer planar geometries as well as to anisotropic and magnetic materials, but for several decades the calculation of Casimir forces for non-planar geometries remained limited to a few idealized cases admitting analytical solutions. For example, the force in the experimental sphere–plate geometry was computed with an approximation (due to Derjaguin) that the sphere radius \"R\" is much larger than the separation \"a\", in which case the nearby surfaces are nearly parallel and the parallel-plate result can be adapted to obtain an approximate \"R\"/\"a\" force (neglecting both skin-depth and higher-order curvature effects). However, in the 2000s a number of authors developed and demonstrated a variety of numerical techniques, in many cases adapted from classical computational electromagnetics, that are capable of accurately calculating Casimir forces for arbitrary geometries and materials, from simple finite-size effects of finite plates to more complicated phenomena arising for patterned surfaces or objects of various shapes.\n\nOne of the first experimental tests was conducted by Marcus Sparnaay at Philips in Eindhoven (Netherlands), in 1958, in a delicate and difficult experiment with parallel plates, obtaining results not in contradiction with the Casimir theory, but with large experimental errors. Some of the experimental details as well as some background information on how Casimir, Polder and Sparnaay arrived at this point are highlighted in a 2007 interview with Marcus Sparnaay.\n\nThe Casimir effect was measured more accurately in 1997 by Steve K. Lamoreaux of Los Alamos National Laboratory, and by Umar Mohideen and Anushree Roy of the University of California, Riverside. In practice, rather than using two parallel plates, which would require phenomenally accurate alignment to ensure they were parallel, the experiments use one plate that is flat and another plate that is a part of a sphere with a large radius.\n\nIn 2001, a group (Giacomo Bressi, Gianni Carugno, Roberto Onofrio and Giuseppe Ruoso) at the University of Padua (Italy) finally succeeded in measuring the Casimir force between parallel plates using microresonators.\n\nIn order to be able to perform calculations in the general case, it is convenient to introduce a regulator in the summations. This is an artificial device, used to make the sums finite so that they can be more easily manipulated, followed by the taking of a limit so as to remove the regulator.\n\nThe heat kernel or exponentially regulated sum is\n\nwhere the limit formula_40 is taken in the end. The divergence of the sum is typically manifested as\n\nfor three-dimensional cavities. The infinite part of the sum is associated with the bulk constant \"C\" which \"does not\" depend on the shape of the cavity. The interesting part of the sum is the finite part, which is shape-dependent. The Gaussian regulator\n\nis better suited to numerical calculations because of its superior convergence properties, but is more difficult to use in theoretical calculations. Other, suitably smooth, regulators may be used as well. The zeta function regulator\n\nis completely unsuited for numerical calculations, but is quite useful in theoretical calculations. In particular, divergences show up as poles in the complex \"s\" plane, with the bulk divergence at \"s\"=4. This sum may be analytically continued past this pole, to obtain a finite part at \"s\"=0.\n\nNot every cavity configuration necessarily leads to a finite part (the lack of a pole at \"s\"=0) or shape-independent infinite parts. In this case, it should be understood that additional physics has to be taken into account. In particular, at extremely large frequencies (above the plasma frequency), metals become transparent to photons (such as X-rays), and dielectrics show a frequency-dependent cutoff as well. This frequency dependence acts as a natural regulator. There are a variety of bulk effects in solid state physics, mathematically very similar to the Casimir effect, where the cutoff frequency comes into explicit play to keep expressions finite. (These are discussed in greater detail in \"Landau and Lifshitz\", \"Theory of Continuous Media\".)\n\nThe Casimir effect can also be computed using the mathematical mechanisms of functional integrals of quantum field theory, although such calculations are considerably more abstract, and thus difficult to comprehend. In addition, they can be carried out only for the simplest of geometries. However, the formalism of quantum field theory makes it clear that the vacuum expectation value summations are in a certain sense summations over so-called \"virtual particles\".\n\nMore interesting is the understanding that the sums over the energies of standing waves should be formally understood as sums over the eigenvalues of a Hamiltonian. This allows atomic and molecular effects, such as the van der Waals force, to be understood as a variation on the theme of the Casimir effect. Thus one considers the Hamiltonian of a system as a function of the arrangement of objects, such as atoms, in configuration space. The change in the zero-point energy as a function of changes of the configuration can be understood to result in forces acting between the objects.\n\nIn the chiral bag model of the nucleon, the Casimir energy plays an important role in showing the mass of the nucleon is independent of the bag radius. In addition, the spectral asymmetry is interpreted as a non-zero vacuum expectation value of the baryon number, cancelling the topological winding number of the pion field surrounding the nucleon.\n\nThe dynamical Casimir effect is the production of particles and energy from an accelerated \"moving mirror\". This reaction was predicted by certain numerical solutions to quantum mechanics equations made in the 1970s. In May 2011 an announcement was made by researchers at the Chalmers University of Technology, in Gothenburg, Sweden, of the detection of the dynamical Casimir effect. In their experiment, microwave photons were generated out of the vacuum in a superconducting microwave resonator. These researchers used a modified SQUID to change the effective length of the resonator in time, mimicking a mirror moving at the required relativistic velocity. If confirmed this would be the first experimental verification of the dynamical Casimir effect.\nA similar analysis can be used to explain Hawking radiation that causes the slow \"evaporation\" of black holes (although this is generally visualized as the escape of one particle from a virtual particle-antiparticle pair, the other particle having been captured by the black hole).\n\nConstructed within the framework of quantum field theory in curved spacetime, the dynamical Casimir effect has been used to better understand acceleration radiation such as the Unruh effect.\n\nThere are few instances wherein the Casimir effect can give rise to repulsive forces between uncharged objects. Evgeny Lifshitz showed (theoretically) that in certain circumstances (most commonly involving liquids), repulsive forces can arise. This has sparked interest in applications of the Casimir effect toward the development of levitating devices. An experimental demonstration of the Casimir-based repulsion predicted by Lifshitz was carried out by Munday et al. Other scientists have also suggested the use of gain media to achieve a similar levitation effect, though this is controversial because these materials seem to violate fundamental causality constraints and the requirement of thermodynamic equilibrium (Kramers–Kronig relations). Casimir and Casimir-Polder repulsion can in fact occur for sufficiently anisotropic electrical bodies; for a review of the issues involved with repulsion see Milton et al.\n\nIt has been suggested that the Casimir forces have application in nanotechnology, in particular silicon integrated circuit technology based micro- and nanoelectromechanical systems, silicon array propulsion for space drives, and so-called Casimir oscillators.\n\nThe Casimir effect shows that quantum field theory allows the energy density in certain regions of space to be negative relative to the ordinary vacuum energy, and it has been shown theoretically that quantum field theory allows states where the energy can be \"arbitrarily\" negative at a given point. Many physicists such as Stephen Hawking, Kip Thorne, and others therefore argue that such effects might make it possible to stabilize a traversable wormhole.\n\nOn 4 June 2013 it was reported that a conglomerate of scientists from Hong Kong University of Science and Technology, University of Florida, Harvard University, Massachusetts Institute of Technology, and Oak Ridge National Laboratory have for the first time demonstrated a compact integrated silicon chip that can measure the Casimir force.\n\n\n\n\n\n"}
{"id": "1142324", "url": "https://en.wikipedia.org/wiki?curid=1142324", "title": "Climateprediction.net", "text": "Climateprediction.net\n\nClimateprediction.net (CPDN) is a distributed computing project to investigate and reduce uncertainties in climate modelling. It aims to do this by running hundreds of thousands of different models (a large climate ensemble) using the donated idle time of ordinary personal computers, thereby leading to a better understanding of how models are affected by small changes in the many parameters known to influence the global climate.\n\nThe project relies on the volunteer computing model using the BOINC framework where voluntary participants agree to run some processes of the project at the client-side in their personal computers after receiving tasks from the server-side for treatment.\n\nCPDN, which is run primarily by Oxford University in England, has harnessed more computing power and generated more data than any other climate modelling project. It has produced over 100 million model years of data so far. , there are more than 12,000 active participants from 223 countries with a total BOINC credit of more than 27 billion, reporting about 55 teraflops (55 trillion operations per second) of processing power.\n\nThe aim of the Climateprediction.net project is to investigate the uncertainties in various parameterizations that have to be made in state-of-the-art climate models. The model is run thousands of times with slight perturbations to various physics parameters (a 'large ensemble') and the project examines how the model output changes. These parameters are not known exactly, and the variations are within what is subjectively considered to be a plausible range. This will allow the project to improve understanding of how sensitive the models are to small changes and also to things like changes in carbon dioxide and sulphur cycle. In the past, estimates of climate change have had to be made using one or, at best, a very small ensemble (tens rather than thousands) of model runs. By using participants' computers, the project will be able to improve understanding of, and confidence in, climate change predictions more than would ever be possible using the supercomputers currently available to scientists.\n\nThe Climateprediction.net experiment should help to \"improve methods to quantify uncertainties of climate projections and scenarios, including long-term ensemble simulations using complex models\", identified by the Intergovernmental Panel on Climate Change (IPCC) in 2001 as a high priority. Hopefully, the experiment will give decision makers a better scientific basis for addressing one of the biggest potential global problems of the 21st century.\n\nAs shown in the graph above, the various models have a fairly wide distribution of results over time. For each curve, on the far right, there is a bar showing the final temperature range for the corresponding model version. As you can see and would expect, the further into the future the model is extended, the wider the variances between them. Roughly half of the variation depends on the future climate forcing scenario rather than uncertainties in the model. Any reduction in those variations whether from better scenarios or improvements in the models are wanted. Climateprediction.net is working on model uncertainties not the scenarios.\n\nThe crux of the problem is that scientists can run models and see that x% of the models warm y degrees in response to z climate forcings, but how do we know x% is a good representation of the probability of that happening in the real world? The answer is that scientists are uncertain about this and want to improve the level of confidence that can be achieved. Some models will be good and some poor at producing past climate when given past climate forcings and initial conditions (a hindcast). It does make sense to trust the models that do well at recreating the past more than those that do poorly. Therefore, models that do poorly will be downweighted.\n\nThe different models that Climateprediction.net has and will distribute are detailed below in chronological order. Therefore, anyone who has joined recently is likely to be running the Transient Coupled Model.\n\nMyles Allen first thought about the need for large Climate ensembles in 1997, but was only introduced to the success of SETI@home in 1999. The first funding proposal in April 1999 was rejected as utterly unrealistic.\n\nFollowing a presentation at the World Climate Conference in Hamburg in September 1999 and a commentary in Nature entitled Do it yourself climate prediction in October 1999, thousands signed up to this supposedly imminently available program. The Dot-com bubble bursting did not help and the project realised they would have to do most of the programming themselves rather than outsourcing.\n\nIt was launched September 12, 2003 and on September 13, 2003 the project exceeded the capacity of the Earth Simulator to become the world's largest climate modelling facility.\n\nThe 2003 launch only offered a Windows \"classic\" client. On 26 August 2004 a BOINC client was launched which supported Windows, Linux and Mac OS X clients. \"Classic\" will continue to be available for a number of years in support of the Open University course. BOINC has stopped distributing classic models in favour of sulfur cycle models. A more user friendly BOINC client and website called GridRepublic, which supports climateprediction and other BOINC projects, was released in beta in 2006.\n\nA thermohaline circulation slowdown experiment was launched in May 2004 under the classic framework to coincide with the film \"The Day After Tomorrow\". This program can still be run but is no longer downloadable. The scientific analysis has been written up in Nick Faull's thesis. A paper about the thesis is still to be completed. There is no further planned research with this model.\n\nA sulfur cycle model was launched in August 2005. They took longer to complete than the original models as a result of having five phases instead of three. Each timestep was also more complicated.\n\nBy November 2005, the number of completed results totalled 45,914 classic models, 3,455 thermohaline models, 85,685 BOINC models and 352 sulfur cycle models. This represented over 6 million model years processed.\n\nIn February 2006, the project moved on to more realistic climate models. A BBC Climate Change Experiment was launched, attracting around 23,000 participants on the first day. The transient climate simulation introduced realistic oceans. This allowed the experiment to investigate changes in the climate response as the climate forcings are changed, rather than an equilibrium response to a significant change like doubling the carbon dioxide level. Therefore, the experiment has now moved on to doing a hindcast of 1920 to 2000 as well as a forecast of 2000 to 2080. This model takes much longer.\n\nThe BBC gave the project publicity with over 120,000 participating computers in the first three weeks.\n\nIn March 2006, a high resolution model was released as another project, the Seasonal Attribution Project.\n\nIn April 2006, the coupled models were found to have a data input problem. The work was useful for a different purpose than advertised. New models had to be handed out.\n\nThe first results of the experiment were published in \"Nature\" in January 2005 and show that with only slight changes to the parameters within plausible ranges, the models can show climate sensitivities ranging from less than 2 °C to more than 11 °C (see and explanation). The higher climate sensitivities have been challenged as implausible. For example, by Gavin Schmidt (a climate modeler with the NASA Goddard Institute for Space Studies in New York).\n\nClimate sensitivity is defined as the equilibrium response of global mean temperature to doubling levels of carbon dioxide. Current levels of carbon dioxide are around 390 ppm and growing at a rate of 1.8 ppm per year compared with preindustrial levels of 280 ppm.\n\nClimate sensitivities of greater than 5 °C are widely accepted as being catastrophic. The possibility of such high sensitivities being plausible given observations had been reported prior to the Climateprediction.net experiment but \"this is the first time GCMs have produced such behaviour\".\n\nEven the models with very high climate sensitivity were found to be \"as realistic as other state-of-the-art climate models\". The test of realism was done with a root mean square error test. This does not check on realism of seasonal changes and it is possible that more diagnostic measures may place stronger constraints on what is realistic. Improved realism tests are being developed.\n\nIt is important to the experiment and the goal of obtaining a probability distribution function (pdf) of climate outcomes to get a very wide range of behaviours even if only to rule out some behaviours as unrealistic. Larger sets of simulations have more reliable pdfs. Therefore, models with climate sensitivities as high as 11 °C are included despite their limited accuracy. The sulfur cycle experiment is likely to extend the range downwards.\n\nPublished in Geophysical Review Letters, this paper concludes:\n\nWhen an internally consistent representation of the origins of model-data discrepancy is used to calculate the probability density function of climate sensitivity, the 5th and 95th percentiles are 2.2 K and 6.8 K respectively. These results are sensitive, particularly the upper bound, to the representation of the origins of model data discrepancy.\n\nThere is an Open University short course and teaching material available for schools to teach subjects relating to climate and climate modelling. There is also teaching material available for use in Key Stage 3/4 Science, A level Physics (Advanced Physics), Key Stage 3/4 Mathematics, Key Stage 3/4 Geography, 21st Century Science, Science for Public Understanding, Use of Mathematics, Primary.\n\nThe original experiment is run with HadSM3, which is the HadAM3 atmosphere from the HadCM3 model but with only a \"slab\" ocean rather than a full dynamic ocean. This is faster (and requires less memory) than the full model, but lacks dynamical feedbacks from the ocean, which are incorporated into the full coupled-ocean-atmosphere models used to make projections of climate change out to 2100.\n\nEach downloaded model comes with a slight variation in the various model parameters.\n\nThere is an initial \"calibration phase\" of 15 model years in which the model calculates the \"flux correction\"; extra ocean-atmosphere fluxes that are needed to keep the model ocean in balance (the model ocean does not include currents; these fluxes to some extent replace the heat that would be transported by the missing currents).\n\nThen there is a \"control phase\" of 15 years in which the ocean temperatures are allowed to vary. The flux correction ought to keep the model stable, but feedbacks developed in some of the runs. There is a quality control check, based on the annual mean temperatures, and models which fail this check are discarded.\n\nThen there is a \"double CO phase\" in which the CO content is instantaneously doubled and the model run for a further 15 years, which in some cases is not quite sufficient model time to settle down to a new (warmer) equilibrium. In this phase some models which produced physically unrealistic results were again discarded.\n\nThe quality control checks in the control and 2*CO phases were quite weak: they suffice to exclude obviously unphysical models but do not include (for example) a test of the simulation of the seasonal cycle; hence some of the models passed may still be unrealistic. Further quality control measures are being developed.\n\nThe temperature in the doubled CO phase is exponentially extrapolated to work out the equilibrium temperature. Difference in temperature between this and the control phase then gives a measure of the climate sensitivity of that particular version of the model.\n\nMost distributed computing projects have screensavers to visually indicate the activity of the application, but they do not usually show its results as they are being calculated. By contrast, climateprediction.net not only uses a built-in visualisation to show the climate of the world being modelled, but it is interactive which allows different aspects of climate (temperature, rainfall, etc.) to be displayed. In addition, there are other, more advanced visualisation programs that allow the user to see more of what the model is doing (usually by analysing previously generated results) and to compare different runs and models.\n\nThe real-time desktop visualisation for the model launched in 2003 was developed by Jeremy Walton at NAG, enabling users to track the progress of their simulation as the cloud cover and temperature changes over the surface of the globe. Other, more advanced visualisation programs in use include \"CPView\" and \"IDL Advanced Visualisation\". They have similar functionality. CPView was written by Martin Sykes, a participant in the experiment. The IDL Advanced Visualisation was written by Andy Heaps of the University of Reading (UK), and modified to work with the BOINC version by Tesella Support Services plc.\n\nOnly CPView allows you to look at unusual diagnostics, rather than the usual Temperature, Pressure, Rainfall, Snow, and Clouds. Up to 5 sets of data can be displayed on a map. It also has a wider range of functions like Max, Min, further memory functions, and other features.\n\nThe Advanced Visualisation has functions for graphs of local areas and over 1 day, 2 days, and 7 days, as well as the more usual graphs of season and annual averages (which both packages do). There are also Latitude - Height plots and Time - Height plots.\n\nThe download size is much smaller for CPView and CPView works with Windows 98.\nRunning the visualisation/screensaver may slow down the processing and is not recommended to be used continuously. \n\nAs of December 2008 there is no visualisation tool that works with the newer CPDN models. Neither CPView nor Advanced Visualisation have been updated to display data gathered from those models. So users can only visualize the data through the screensaver.\nThe BBC Climate Change Experiment was a BOINC project led by Oxford University with several partners including the UK Met Office, the BBC, the Open University and Reading University. It is the Transient coupled model of the Climateprediction.net project.\n\nMany participants joined the project with over 120,000 people signing up in teams.\n\nResults continued to be collected for some time with the follow-up television program being aired in January 2007. On 8 March 2009, Climateprediction.net officially declared that BBC Climate Change Experiment was finished, and then shut down this project.\n\n\n"}
{"id": "18960922", "url": "https://en.wikipedia.org/wiki?curid=18960922", "title": "Coot (software)", "text": "Coot (software)\n\nThe program Coot (Crystallographic Object-Oriented Toolkit) is used to display and manipulate atomic models of macromolecules, typically of proteins or nucleic acids, using 3D computer graphics. It is primarily focused on building and validation of atomic models into three-dimensional electron density maps obtained by X-ray crystallography methods, although it has also been applied to data from electron microscopy.\n\nCoot displays electron density maps and atomic models and allows model manipulations such as idealization, real space refinement, manual rotation/translation, rigid-body fitting, ligand search, solvation, mutations, rotamers, and Ramachandran idealization. The software is designed to be easy-to-learn for novice users, achieved by ensuring that tools for common tasks are 'discoverable' through familiar user interface elements (menus and toolbars), or by intuitive behaviour (mouse controls). Recent developments have enhanced the usability of the software for expert users, with customisable key bindings, extensions, and an extensive scripting interface.\n\nCoot is free software, distributed under the GNU GPL. It is available from the Coot web site originally at the University of York, and now at the MRC Laboratory of Molecular Biology. Pre-compiled binaries are also available for Linux and Windows from the web page and CCP4, and for Mac OS X through Fink and CCP4. Additional support is available through the Coot wiki and an active COOT mailing list.\n\nThe primary author is Paul Emsley (MRC-LMB at Cambridge). Other contributors include Kevin Cowtan, Bernhard Lohkamp and Stuart McNicholas (University of York), William Scott (University of California at Santa Cruz), and Eugene Krissinel (Daresbury Laboratory).\n\nCoot can be used to read files containing 3D atomic coordinate models of macromolecular structures in a number of formats, including pdb, mmcif, and Shelx files. The model may then be rotated in 3D and viewed from any viewpoint. The atomic model is represented by default using a stick-model, with vectors representing chemical bonds. The two halves of each bond are coloured according to the element of the atom at that end of the bond, allowing chemical structure and identity to be visualised in a manner familiar to most chemists.\n\nCoot can also display electron density, which is the result of structure determination experiments such as X-ray crystallography and EM reconstruction. The density is contoured using a 3D-mesh. The contour level controlled using the mouse wheel for easy manipulation - this provides a simple way for the user to get an idea of the 3D electron density profile without the visual clutter of multiple contour levels. Electron density may be read into the program from ccp4 or cns map formats, though it is more common to calculate an electron density map directly from the X-ray diffraction data, read from an mtz, hkl, fcf or mmcif file.\n\nCoot provides extensive features for model building and refinement (i.e. adjusting the model to better fit the electron density), and for validation (i.e. checking that the atomic model agrees with the experimentally derived electron density and makes chemical sense). The most important of these tools is the real space refinement engine, which will optimize the fit of a section of atomic model to the electron density in real time, with graphical feedback. The user may also intervene in this process, dragging the atoms into the right places if the initial model is too far away from the corresponding electron density.\n\nTools for general model building:\n\nTools for moving existing atoms:\n\nTools for adding atoms to the model:\n\nIn macromolecular crystallography, the observed data is often weak and the observation-to-parameter ratio near 1. As a result, it is possible to build an incorrect atomic model into the electron density in some cases. To avoid this, careful validation is required. Coot provides a range of validation tools, listed below. Having built an initial model, it is usual to check all of these and reconsider any parts of the model which are highlighted as problematic before deposition of the atomic coordinates with a public database.\n\n\nCoot is built upon a number of libraries. Crystallographic tools include the Clipper library for manipulating electron density and providing crystallographic algorithms, and the MMDB for the manipulation of atomic models. Other dependencies include FFTW, and the GNU Scientific Library.\n\nMuch of the program's functionality is available through a scripting interface, which provides access from both the Python and Guile scripting languages.\n\nThe CCP4mg molecular graphics software from Collaborative Computational Project Number 4 is a related project with which Coot shares some code. The projects are focused on slightly different problems, with CCP4mg dealing with presentation graphics and movies, whereas Coot deals with model building and validation.\n\nThe software has gained considerable popularity over the past 5 years, overtaking widely used packages such as 'O', XtalView, and Turbo Frodo. The primary publication has been cited in over 10,000 independent scientific papers since 2004.\n\n"}
{"id": "34567821", "url": "https://en.wikipedia.org/wiki?curid=34567821", "title": "Critical Assessment of Function Annotation", "text": "Critical Assessment of Function Annotation\n\nThe Critical Assessment of Functional Annotation (CAFA) is an experiment designed to provide a large-scale assessment of computational methods dedicated to predicting protein function. Different algorithms are evaluated by their ability to predict the Gene Ontology (GO) terms in the categories of Molecular Function, Biological Process, and Cellular Component.\n\nThe experiment consists of two tracks: (i) the eukaryotic track, \n(ii) the prokaryotic track. In each track, a set of targets is provided by the organizers. Participants are expected to submit their predictions by the submission deadline, after which they are assessed according to a set of specific metrics.\n\nThe genome of an organism may consist of hundreds to tens of thousands of genes, which encode for hundreds of thousands of different protein sequences. Due to the relatively low cost of genome sequencing, determining gene and protein sequences is fast and inexpensive. Thousands of species have been sequenced so far, yet many of the proteins are not well characterized. The process of experimentally determining the role of a protein in the cell, is an expensive and time consuming task. Further, even when functional assays are performed they are unlikely to provide complete insight into protein function. Therefore it has become important to use computational tools in order to functionally annotate proteins. There are several computational methods of protein function prediction that can infer protein function using a variety of biological and evolutionary data, but there is significant room for improvement. Accurate prediction of protein function can have longstanding implications on biomedical and pharmaceutical research.\n\nThe CAFA experiment is designed to provide unbiased assessment of computational methods, to stimulate research in computational function prediction, and provide insights into the overall state-of-the-art in function prediction.\n\nThe experiment consists of three phases:\n\nOrganizers provide protein sequences with unknown or incomplete function to community and set the deadline for the submission of predictions\n\n\nAfter all predictions are stored and the experiment enters a waiting period in which protein functions are expected to accumulate in public databases\n\n\nPredictors are ranked according to their performance. The results are publicly shared in scientific meetings and published after peer review.\n\nThe CAFA experiment is conducted by the Automated Function Prediction (AFP) Special Interest Group (AFP/SIG). An AFP/SIG meeting has been held alongside the Intelligent Systems for Molecular Biology conference in 2005, 2006, 2008, 2011, and 2012.\nThe first CAFA experiment was organized between fall 2010 and spring 2012. The organizers provided 48,000 sequences for the community with the task to prediction Gene Ontology annotations for each of these sequences. Of those 48,000 proteins, 866 were experimentally annotated during target accumulation phase. The results showed that current function prediction algorithms perform significantly better than a simple domain assignment or a straightforward use of BLAST package. However, they also revealed that accurate prediction of a protein's biological function is still an open and challenging problem.\n\nThe first CAFA experiment was organized between fall 2010 and spring 2012. The organizers provided 48,000 sequences for the community with the task to prediction Gene Ontology annotations for each of these sequences. Of those 48,000 proteins, 866 were experimentally annotated during target accumulation phase. The results showed that current function prediction algorithms perform significantly better than a simple domain assignment or a straightforward use of BLAST package. However, they also revealed that accurate prediction of a protein's biological function is still an open and challenging problem.\n\nThe second CAFA experiment kicked off in fall 2013. Starting in August, interested parties could download more than 100,000 target sequences in 27 species. Registered teams are challenged to annotate the sequences with Gene Ontology terms, with an additional challenge to annotate human sequences with Human Phenotype Ontology terms. The submission deadline was January 15, 2014. The assessment of predictions will take place in June 2014.\n\nCASP: Critical Assessment of protein Structure Prediction \nCAPRI: Critical Assessment of Prediction of Interactions\n\n"}
{"id": "28009780", "url": "https://en.wikipedia.org/wiki?curid=28009780", "title": "Fall protection", "text": "Fall protection\n\nFall protection is the use of controls designed to protect personnel from falling or in the event they do fall, to stop them without causing severe injury. Typically, fall protection is implemented when working at height, but may be relevant when working near any edge, such as near a pit or hole, or performing work on a steep surface.\n\nThere are four generally accepted categories of fall protection: fall elimination, fall prevention, fall arrest and administrative controls. According to the US Department of Labor, falls account for 8% of all work-related trauma injuries leading to death. Federal statutes, standards and regulations in the United States pertaining to the requirements for employers to provide fall protection are administered by OSHA.\n\nFalls from elevations were the fourth leading cause of workplace death from 1980 through 1994, with an average of 540 deaths per year accounting for 10% of all occupational fatalities.\n\nFalls are a concern for oil and gas workers, many of whom must work high on a derrick. A study of falls over the period 2005–2014 found that in 86% of fatal falls studied, fall protection was required by regulation, but it was not used, was used improperly, or the equipment failed. Many of the fatalities were because, although the workers were wearing harnesses, they neglected to attach them to an anchor point.\n\nIn most work-at-height environments, multiple fall protection measures are used concurrently.\n\nFall elimination is often the preferred way of providing fall protection. This entails finding ways of completing tasks without working at heights.\n\n\nFall arrest is the form of fall protection that stops a person who has fallen.\n\nAdministrative controls are used along with other measures, but they do not physically prevent a worker from going over an edge. Examples of administrative controls include placing a safety observer or warning line near an edge, or enforcing a safety policy which trains workers and requires them to adhere to other fall protection measures, or prohibiting any un-restrained worker from approaching an edge.\n"}
{"id": "13768895", "url": "https://en.wikipedia.org/wiki?curid=13768895", "title": "Fibrobacter succinogenes", "text": "Fibrobacter succinogenes\n\nFibrobacter succinogenes is a cellulolytic bacterium species in the genus \"Fibrobacter\". It is present in the rumen of cattle. Beta glucans are its substrate of choice in the rumen and its products after digestion include formate, acetate and succinate. Fibrobacter succinogenes forms characteristic extensive grooves in crystalline cellulose, and is also rather readily detached from its substrate during sample preparation.\n\nPhylogenetic studies based RpoC and Gyrase B protein sequences, indicate that \"Fibrobacter succinogenes\" is closely related to the species from the phyla \"Bacteroidetes\" and \"Chlorobi\". \"Fibrobacter succinogenes\" and the species from these two other phyla also branch in the same position based upon conserved signature indels in a number of important proteins. Lastly and most importantly, comparative genomic studies have identified two conserved signature indels (a 5-7 amino acid insert in the RpoC protein and a 13-16 amino acid insertion in serine hydroxymethyltransferase) and one signature protein (PG00081) that are uniquely shared by \"Fibrobacter succinogenes\" and all of the species from \"Bacteroidetes\" and \"Chlorobi\" phyla. All of these results provide compelling evidence that \"Fibrobacter succinogenes\" shared a common ancestor with \"Bacteroidetes\" and \"Chlorobi\" species exclusive of all other bacteria, and these species should be recognized as part of a single “FCB”superphylum.\n\n\n"}
{"id": "48209", "url": "https://en.wikipedia.org/wiki?curid=48209", "title": "Gas laws", "text": "Gas laws\n\nThe gas laws were developed at the end of the 18th century, when scientists began to realize that relationships between pressure, volume and temperature of a sample of gas could be obtained which would hold to approximation for all gases. Gases behave in a similar way over a wide variety of conditions because they all have molecules which are widely spaced, and the equation of state for an ideal gas is derived from kinetic theory. The earlier gas laws are now considered as special cases of the ideal gas equation, with one or more variables held constant.\n\nIn 1662 Robert Boyle studied the relationship between volume and pressure of a gas of fixed amount at constant temperature. He observed that volume of a given mass of a gas is inversely proportional to its pressure at a constant temperature.\nBoyle's law, published in 1662, states that, at constant temperature, the product of the pressure and volume of a given mass of an ideal gas in a closed system is always constant. It can be verified experimentally using a pressure gauge and a variable volume container. It can also be derived from the kinetic theory of gases: if a container, with a fixed number of molecules inside, is reduced in volume, more molecules will strike a given area of the sides of the container per unit time, causing a greater pressure.\n\nA statement of Boyle's law is as follows:\n\nThe concept can be represented with these formulae:\n\nCharles's law, or the law of volumes, was found in 1787 by Jacques Charles. It states that, for a given mass of an ideal gas at constant pressure, the volume is directly proportional to its absolute temperature, assuming in a closed system.\n\nThe statement of Charles's law is as follows:\nthe volume (V) of a given mass of a gas, at constant pressure (P), is directly proportional to its temperature (T).\nAs a mathematical equation, Charles's law is written as either:\n\nwhere \"V\" is the volume of a gas, \"T\" is the absolute temperature and \"k\" is a proportionality constant (which is not the same as the proportionality constants in the other equations in this article).\n\nGay-Lussac's law, Amontons' law or the pressure law was found by Joseph Louis Gay-Lussac in 1809. It states that, for a given mass and constant volume of an ideal gas, the pressure exerted on the sides of its container is directly proportional to its absolute temperature.\n\nAs a mathematical equation, Gay-Lussac's law is written as either:\n\nK=P divided by T\n\nAvogadro's law states that the volume occupied by an ideal gas is directly proportional to the number of molecules of the gas present in the container. This gives rise to the molar volume of a gas, which at STP (273.15 K, 1 atm) is about 22.4 L. The relation is given by\n\nThe Combined gas law or General Gas Equation is obtained by combining Boyle's Law, Charles's law, and Gay-Lussac's Law. It shows the relationship between the pressure, volume, and temperature for a fixed mass (quantity) of gas:\n\nThis can also be written as:\n\nWith the addition of Avogadro's law, the combined gas law develops into the ideal gas law:\n\nThese equations are exact only for an ideal gas, which neglects various intermolecular effects (see real gas). However, the ideal gas law is a good approximation for most gases under moderate pressure and temperature.\n\nThis law has the following important consequences:\n\n\nor\n\n\n\n\n"}
{"id": "50899473", "url": "https://en.wikipedia.org/wiki?curid=50899473", "title": "Giuseppe Ferrini", "text": "Giuseppe Ferrini\n\nGiuseppe Ferrini (18th century) was a Florentine model-maker.\n\nHe was commissioned by the Florentine surgeon and obstetrician Giuseppe Galletti (?-1819) to build a series of terracotta and wax obstetrical models for demonstrating different forms of childbirth to medical students, surgery students, and students at the School of Obstetrics. Ferrini worked in the renowned anatomical wax-modeling laboratory run by the abbot Felice Fontana (1730-1805). Grand Duke Peter Leopold (1747-1792) asked Fontana to set up the Museo di Fisica e di Storia Naturale of Florence.\n"}
{"id": "41583575", "url": "https://en.wikipedia.org/wiki?curid=41583575", "title": "Global value chain", "text": "Global value chain\n\nIn development studies, the concept of a value chain has been used to analyse international trade in global value chains which comprises “the full range of activities that are required to bring a product from its conception, through its design, its sourced raw materials and intermediate inputs, its marketing, its distribution and its support to the final consumer”. Specifically, when activities must be coordinated across geographies, the term global value chain (GVC) is used in the development literature. Simply put, the global value chain includes all of the people and activities involved in the production of a good or service and its global level supply, distribution and post sales activities (also known as the supply chain). GVC is similar to Industry Level Value Chain but encompasses operations at the global level.\n\nThe first references to the GVC concept date from the mid-1990s and were enthusiastic about the upgrading prospects for developing countries that joined them. In his early work based on research on East Asian garment firms, the pioneer in value chain analysis, Gary Gereffi, describes a process of almost ‘natural’ learning and upgrading for the firms that joined GVCs. This echoed the ‘export-led’ discourse of the World Bank in the ‘East Asian Miracle’ report based on the East Asian ‘Tigers’ success.\n\nThis encouraged the World Bank and other leading institutions to encourage developing firms to develop their indigenous capabilities through a process of upgrading technical capabilities to meet global standards with leading multinational enterprises (MNE) playing a key role in helping local firms through transfer of new technology, skills and knowledge.\n\nWider adoption of open source hardware technology used for digital fabrication such as 3D printers like the RepRap has the potential to partially reverse the trend towards global specialization of production systems into elements that may be geographically dispersed and closer to the end users (localization) and thus disrupt global value chains.\n\nGVC analysis views “upgrading” as a continuum starting with “process upgrading” (e.g. a producer adopts better technology to improve efficiency), then moves on to “product upgrading” where the quality or functionality of the product is upgraded by using higher quality material or a better quality management system (QMS), and then on to “functional upgrading” in which the firm begins to design its own product and develops marketing and branding capabilities and begins to supply to end markets/customers directly - often by targeting geographies or customers which are not served by its existing multinational clients). Subsequently, the process of upgrading might also cover inter-sectoral upgrading.\n\nThis upgrading process in GVCs has been challenged by other researchers – some of whom argue that insertion in global value chains does not always lead to upgrading. Some authors argue that the expected upgrading process might not hold for all types of upgrading. Specifically they argue upgrading into design, marketing and branding might be hindered by exporting under certain conditions because MNEs have no interest in transferring these core skills to their suppliers thus preventing them from accessing global markets (except as a supplier) for first world customer.\n\nThere are motivations behind renewed interest in global value chains and the opportunities that they may present for countries in South Asia. A 2013 report found that looking at the production chain, rather than the individual stages of production, is more helpful. Individual donors with their own priorities and expertise cannot be expected to provide comprehensive response to the needs identified, not to mention the legal responsibilities of many specialist agencies. The research suggests they adjust their priorities and modalities to the way production chains operate, and to coordinate with other donors to cover all trade needs. It calls for donors and governments to work together to assess how aid flows may affect power relationships.\n\nIn his 1994 paper, Gereffi identified two major types of governance. The first were buyer-driven chains, where the lead firms are final buyers such as retail chains and branded product producers such as non-durable final consumer products (e.g., clothing, footwear and food). The second governance type identified by Gereffi were producer-driven chains. Here the technological competences of the lead firms (generally upstream in the chain) defined the chain’s competitiveness.\n\nCurrent research suggests that GVCs exhibit a variety of characteristics and impact communities in a variety of ways. In a paper that emerged from the deliberations of the GVC Initiative, five GVC governance patterns were identified:\n\nAs capabilities in many low- and middle-income economies have grown, chain governance has tended to move away from quasi-hierarchical models toward modular type as this form of governance reduces the costs of supply chain management and allows chain governors to maintain a healthy level of competition in their supply chains. However, whilst it maintains short-term competition in the supply chain, it has allowed some leading intermediaries to develop considerable functional competences (e.g., design and branding). In the long term these have the potential to emerge as competitors to their original chain governor (Kaplinski, 2010).\n\nThe theoretical concepts often considered firms as operating in a single value chain (with a single customer). Whilst this was often the case in quasi-hierarchical chains (with considerable customer power) it has become apparent that some firms operate in multiple value chains (subject to multiple forms of governance) and serve both national and international markets and that this plays a role in the development of firm capabilities (Navas-Aleman, 2011; UNCTAD, 2013).\n\n\nIn 2013, UNCTAD published two reports on GVCs and their contribution to development. They concluded that:\n\nGender plays a prominent role in global value chains, because it influences consumption patterns within the United States, and thus affects production on a larger scale. In turn, specific roles within the value chain are also determined by gender, making gender a key component in the process as well. Far more women than men are found in the informal sector, as self-employed workers or subcontractors, while specific jobs and broader fields of work differ between men and women.\n\nWithin the global value chains, it is often women who work in the weakest or most disadvantaged chains.\n\nCarr et al. argue that “the vast majority of subcontract workers or industrial homeworkers, who earn some of the lowest wages worldwide, are women.”\n\n"}
{"id": "3430703", "url": "https://en.wikipedia.org/wiki?curid=3430703", "title": "Greater big-footed mouse", "text": "Greater big-footed mouse\n\nThe greater big-footed mouse or long-tailed big-footed mouse (\"Macrotarsomys ingens\") is a nocturnal rodent found only in north west Madagascar. It was first described by F. Petter in 1959. It is listed as an endangered species as a result of habitat loss.\n\nThe greater big-footed mouse grows to a head and body length of up to with an even longer tail of up to . Its weight is and it has large eyes and large oval ears. Its big feet are adapted for climbing among the branches of trees. The pelage is brownish-fawn with a greyish undercoat; the underparts and legs are creamy white.\n\nThe greater big-footed mouse is found only in Mahajanga Province in north-western Madagascar, in the Ankarafantsika forest which is now part of the Ankarafantsika National Park. This is a low rainfall area with typical dry tropical vegetation. The mouse is a nocturnal species. It spends the day in a burrow in the forest floor, the entrance of which is concealed under a rock or tree root and spends the night scrambling about in bushes and trees looking for food.\n\nThe greater big-footed mouse is believed to be herbivorous and probably eats berries, fruit, nuts, seeds, roots, and stems. It probably falls prey to such predators as snakes, birds and carnivorous mammals.\n\nThe greater big-footed mouse is listed as \"Endangered\" in the IUCN Red List of Threatened Species. This is because, although it is fairly common in the area in which it is found, its total range amounts to a single block of less than . Its habitat is subject to degradation by fire and logging and the mouse is at risk of predation by feral animals.\n"}
{"id": "41934701", "url": "https://en.wikipedia.org/wiki?curid=41934701", "title": "Guttenberg plagiarism scandal", "text": "Guttenberg plagiarism scandal\n\nGuttenberg plagiarism scandal refers to the German political scandal that led to the resignation of Karl-Theodor zu Guttenberg as Minister of Defence of Germany over the plagiarism of his doctoral dissertation. The first accusations of plagiarism in Guttenberg's dissertation were made public in February 2011. Guttenberg's doctoral dissertation, \"\" (\"Constitution and Constitutional Treaty\"), had been the basis of his 2007 Doctorate from the University of Bayreuth. Guttenberg at first denied intentional plagiarism, calling the accusations \"absurd,\" but acknowledged that he may have made errors in his footnotes. In addition, it emerged that Guttenberg had requested a report from the Bundestag's research department, which he had then inserted into his thesis without attribution. On 23 February 2011, Guttenberg apologized in parliament for flaws in his thesis, but denied intentional deception and denied the use of a ghostwriter.\n\nOn 23 February 2011, the University of Bayreuth withdrew Guttenberg's doctorate. In part due to the expressions of confidence by Angela Merkel, the scandal continued to evoke heavy criticism from prominent academics, legal scholars (who accused Guttenberg of intentional plagiarism), and politicians both in the opposition and in the governing coalition. On 1 March 2011, Guttenberg announced his resignation as Minister of Defense, from his seat in the Bundestag, and from all other political offices.\n\nIn May 2011, a University of Bayreuth commission tasked with investigating Guttenberg's dissertation came to the conclusion that Guttenberg had engaged in intentional deception in the writing of his dissertation, and had violated standards of good academic practice. The commission found that he had included borrowed passages throughout his thesis, without citation, and had modified those passages in order to conceal their origin.\n\nIn November 2011, the prosecution in Hof discontinued the criminal proceedings for copyright violations against Guttenberg on condition of Guttenberg paying €20,000 to a charity. The prosecutor found 23 prosecutable copyright violations in Guttenberg's dissertation, but estimated that the material damage suffered by the authors of those texts was marginal.\n\nGuttenberg studied law at the University of Bayreuth, where he passed the first legal state examination in 1999. In 2007, he was awarded a doctorate in law, under supervision of Peter Häberle, with a dissertation on the development of constitutional law in the United States and the European Union. The doctoral thesis was titled \"\" (translation: \"Constitution and Constitutional Treaties – Constitutional Steps of Development in the USA and the EU\"). The university awarded the dissertation with the highest honor \"“summa cum laude”\".\n\nOn 12 February 2011 Andreas Fischer-Lescano, professor of law at the University of Bremen, prepared a review of Guttenberg's thesis for the left-leaning German legal quarterly “Kritische Justiz”. Fischer-Lescano was co-editor of this publication. During a reference check he discovered an article of the Neue Zürcher Zeitung (NZZ), published in 2003, of which passages had been included in Guttenberg's thesis without citation. After this discovery Fischer-Lescano performed further searches and discovered seven more passages lacking proper citation. He discussed the findings with the other editors of “Kritische Justiz” and they decided that their publication, with only 1,800 subscribers, was not the appropriate forum to make the findings public. So Fisher-Lescano contacted the German newspaper Süddeutsche Zeitung in Munich.\n\nOn February 15, 2011 the newspaper contacted law professor Diethelm Klippel, the Bayreuth University ombudsman, and informed him of the plagiarism charges. Klippel had also been on the review committee for Guttenberg's doctoral thesis. On the same day the newspaper informed Guttenberg and gave him a few hours to respond to the allegations. Guttenberg was on an official visit in Poland that day. Fischer-Lescano informed Guttenberg's thesis supervisors Peter Häberle and Rudolf Streinz about the charges.\n\nOn 16 February 2011 the Süddeutsche Zeitung published an article reporting the allegations against Guttenberg. On the same day the German newspaper Frankfurter Allgemeine Zeitung (FAZ) reported that the introduction of his thesis was borrowed from an FAZ-article from 1997, written by the political scientist Barbara Zehnpfennig.\n\nIn a first statement Guttenberg, who still was in Poland, called the charge, his thesis being a plagiarism, “abstruse”. He insisted that the thesis was his own achievement and that none of his employees helped him to draft it. The University of Bayreuth delegated the allegations against Guttenberg to its Commission on Professional Self Regulation in Science.\nOn 16 February 2011, after his return from Poland, Guttenberg left for a surprise visit to German soldiers in Afghanistan. He spent the night in the military camp “OP North” in the war zone Baghlan province. Although the visit had been planned weeks in advance, allegations were raised, that this trip was some kind of “getaway” for Guttenberg.\n\nGuttenberg returned the next day, 17 February 2011, and had a meeting with German chancellor Angela Merkel. His offer to resign was refused by Merkel. The same day the University of Bayreuth sent a letter to Guttenberg conceding him two weeks to respond to the plagiarism allegations.\nThe crowd-source Guttenplag Wiki was launched by anonymous user PlagDoc as a platform for internet user to submit claims of unattributed work in Guttenberg's thesis.\nA scheduled speech at an election campaign in Saxony-Anhalt was cancelled by Guttenberg because of his meeting with Angela Merkel.\n\nThe district attorney's office in Hof, Bavaria, announced it would wait on the results of the Bayreuth University's examination of Guttenberg's thesis and confirmed that two criminal complaints had been filed against Guttenberg, one regarding possible copyright violations and another on a possible false statutory declaration. The second was immediately declined by the attorney due to apparent insubstantiality. The same day the newspaper Süddeutsche Zeitung reported its findings that Guttenberg had used texts of 19 authors without correct attribution.\n\nChancellor Angela Merkel declared: “He has my full confidence for his work, and this work is important. He has my support […] and we must wait until the university has completed its examination of the allegations”. In an interview the chancellor attested Guttenberg an “head-on approach” on the allegations.\n\nOn February 19, 2011, the German news magazine Der Spiegel reported that Guttenberg had used works of the German Parliamentary Research Service (PRS) without proper attribution. Any publication of PRS papers requires prior approval by the German Bundestag's departmental management. During the weekend of February 20, 2011, Guttenberg re-read his thesis to examine Guttenplag's and other's findings.\nThe next day in a letter Guttenberg asked the University of Bayreuth to revoke his title and declared that he had “lost track of the use of sources over the course of the seven years in which [he] worked on the thesis”, but “at no point made mistakes by intention”.\n\nAlso on 21 February 2011, chancellor Merkel reiterated her support for Guttenberg and told journalists “I appointed Guttenberg as Minister of Defence. I did not appoint him as an academic assistant or doctor. What is important to me is his work as Minister of Defence and he carries out these duties perfectly.”\nThis statement caused anger in the German academic community.\n\nAnnette Schavan, German minister of Education and Research, who two years later had to resign herself because of plagiarism-accusations related to her own thesis, declared, that she didn’t “consider the incident to be a trifle”, because “intellectual theft is not a small thing. The protection of intellectual property is a higher good.”\nThe same day it became known, that Guttenberg had used another PRS paper without proper citation.\nThat evening Guttenberg attended a campaign event in Kelkheim and stated that he would give up his doctorate title permanently.\n“Over the weekend I had another look at my doctorate thesis”, Guttenberg declared and continued \"I lost sight of the sources in one of two places. I wrote this piece of work myself and I stand by it, but I also stand by the rubbish I wrote\". Guttenberg admitted “serious mistakes”, which had not been intentionally but “do not meet the ethical code of science”, and apologized to people, who had been hurt by his work. Guttenberg called his decision to give up the title “painfull”. He rejected all speculation about his resignation and declared: “I will perform my duties with all my powers”.\nThe University of Bayreuth confirmed the same day that Guttenberg had asked them to revoke his title.\nChancellor Merkel approved his decision to give up his title, a spokesman declared.\n\nDistinct criticism on Guttenberg's crisis management was passed by Norbert Lammert, President of the German Bundestag. In an interview on 22 February 2013, Lammert also expressed his doubts about the reliability of the university's awarding process.\n\nThe following day Lammert presided over a parliamentary questioning in the Bundestag on Guttenberg's use of PRS papers in his thesis. During the questioning politicians from the opposition called for Guttenberg's resignation” and accused him of cheating, lying and intentions to deceive. In his reply Guttenberg again apologized and admitted: “I wrote a dissertation that was obviously flawed”. He described himself as “a man of mistakes and weaknesses” and stated “I did not deliberately cheat, but made serious errors”. Guttenberg declared: “I was certainly so arrogant as to believe that I could square the circle by trying to coordinate political passion and work, as well as academic and intellectual challenges, with being a young father” and apologized “for me this was overload, and today I regret to say that I couldn’t manage it.” Asked whether his call for the allegations as “abstruse” hadn’t been premature, Guttenberg confirmed this part of his previous statement as it was related to the accusation of his thesis as being a plagiarism. According to accusations of misusing the German Parliamentary Research Service (PRS) Guttenberg pointed out, that all studies had been related to his political work. He noted that all papers had been cited, but without the authors’ names, as they had been employers of the PRS. Guttenberg stated, that he couldn’t answer whether he had sought prior approval to use these reports for his thesis but that he had already apologized to Bundestag's President Lammert in case of a potential oversight.\n\nOn the same day the doctoral commission of Bayreuth University revoked Guttenberg's PhD. University's president Rüdiger Bormann declared, that Guttenberg had “objectively not conformed” to academic standards. According to Bormann this fast revocation was possible because of Guttenberg's statement. The doctoral commission made no judgment as to whether Guttenberg had acted intentionally, something that led to criticism. Such an investigation would “surely have been an extended process”, Bormann declared, which was unnecessary after Guttenberg's request for a withdrawal of his title. Further inquiries would be done by the Commission of Professional Self Conduct in Science, Bormann announced.\nChancellor Merkel commented on the revocation of Guttenberg's title as being “in line of what he had requested”.\n\nOn 24 February 2011, Bundestagspräsident Norbert Lammert declared that Guttenberg had used six reports by the parliament's research service for his doctoral thesis without prior approval. However, Lammert acknowledged the fact that it was widespread practice among Bundestag Members to use documents prepared by the parliamentary research service without first obtaining the necessary approval. The same day, an open letter to chancellor Merkel was published to be signed by doctoral students and researchers.\nThe successor of Guttenberg's thesis supervisor Häberle at the University of Bayreuth, Oliver Lepsius, alleged that the minister made the mistakes deliberately and accused him of fraud.\n\nMedia reported that between 1999 and 2006 a new chair of the University of Bayreuth had been sponsored with €747.000 by Rhön-Klinikum. Until 2002 Guttenberg's family held a major stake in the hospital and he had been a member of its supervisory board. In a statement the university denied any sponsoring by Guttenberg as the funding had been part of cooperation between the university, a health insurance, the state of Bavaria and Rhön-Klinikum.\n\nTwo days later, minister Schavan in an interview criticized Guttenberg for his thesis: “As someone who earned my doctorate 31 years ago and has worked with many doctoral candidates during my career, I’m embarrassed, and not just privately.” Bundestag's president Lammert called the affair a “nail in the coffin for trust in our democracy”.\n\nChancellor Merkel announced through her spokesman Steffen Seibert on 28 February 2011 that Guttenberg had still her full confidence. Asked for Merkel's opinion on the fraud accusation by Oliver Lepsius, Seibert declared, that “fraud requires intention. Any intention was denied by Guttenberg. The Chancellor believes him.”\nGuttenberg himself gained strong support at a CSU-meeting in Munich, while several CSU-politicians sharply criticized Lammert and Schavan on their comments.\n\nOn 1 March 2011 Guttenberg declared his resignation from all political offices at national level. He called this decision the “most painful step of my life” and declared \"I was always ready to fight, but I have reached the limits of my strength\". \"I must agree with my enemies who say that I was not appointed minister for self-defence, but defence-minister\" Guttenberg said in view of his ministerial office. As for the enquiry regarding his thesis he announced his full cooperation with the district attorney's investigations. Guttenberg thanked Angela Merkel for her support, trust and understanding.\n\nChancellor Merkel reacted to Guttenberg's decision by saying that “I deeply regret his resignation”. For Merkel he had “a unique and extraordinary ability” relating to people. For the future the Chancellor declared that \"I am convinced that we will have the opportunity to work together again in the future, in whatever form that may take\".\n\nRüdiger Bormann, president of University of Bayreuth, declared, that the inquiries of its Commission on Professional Self Regulation in Science would continue.\n\nOn 2 March 2011 the district attorney's office in Hof announced the launch of an investigation into potential copyright violations contained in Guttenberg's thesis as soon as his immunity would be withdrawn.\nWith his official resignation as Member of Parliament the following day, Guttenberg abandoned his parliamentary immunity, thus allowing the district attorney's investigations to proceed. Media reported on more than 80 charges, which had been filed. On 3 March 2011 Guttenberg received his dismissal certificate in a ceremony hosted by German President Christian Wulff.\n\nThe same day, media reported about an anonymous member of the Commission of Professional Self Regulation in Science allegedly accusing Guttenberg of deception. In contrast, Volker Rieble, law professor at Ludwig Maximilian University of Munich, himself a strong critic of Guttenberg's thesis, published an op-ed disputing the legal basis for further investigations in view of the fact that the university had already revoked Guttenberg's title and that he was no longer affiliated with the university.\n\nOn March 5, 2011, the thesis supervisors Peter Häberle and Rudolf Streinz issued a statement declaring that in 2006, when the thesis was finished, the plagiarism wasn’t detectable for lack of technical opportunities. Without knowledge of the copied passages, Guttenberg's thesis demonstrated “a high degree of analytical depths and penetrated the dissertation topic in all its facets”, Häberle emphasized. “It is important to note that the candidate [Guttenberg] was able to respond successfully to even intense questioning regarding the methodology and content of the thesis and was completely able to hold his own during the discussion as part of the oral PhD exam”, the supervisors declared on Guttenberg's doctoral examination procedure and its grading (summa cum laude).\n\nOn 7 March 2011 the district attorney's office in Hof announced the launch of a preliminary investigation into potential copyright violations by Guttenberg.\nThe University of Bayreuth, on 8 March 2011, announced the expansion of the commission investigating Guttenberg's dissertation to also include Wolfgang Löwer, professor of law at University of Bonn, and Jürgen Mittelstraß, professor of philosophy at University of Konstanz.\n\nA few days later, as a means of taking responsibility for his error, Guttenberg sent personal apology letters to all those authors who were not properly quoted in his thesis.\nGuttenberg resigned from his last political office, the Kulmbach county council, on 15 March 2011. At the same day the open letter, signed by 63,713 people, was delivered to the Chancellery in Berlin.\nOn 20 March 2011 the Zurich weekly NZZ published extracts of an apology Guttenberg had sent to Klara Obermüller for not quoting her in his dissertation.\n\nOn 7 April 2011, the Commission of Professional Self Regulation in Science sent its report of preliminary findings to Guttenberg asking for his response by 26 April 2011.\n\nOn 9 April 2011, there was a first leak of the commission's initial findings to the press, in which Guttenberg was accused of deliberate deception Guttenberg's lawyer sharply criticized the commission's leaks as an unfair \"prejudgment\" of his client given that the press reports were published both before the end of the investigation and before the 26 April deadline Guttenberg had been given by the university to respond to the commission's report before its public release. His lawyer also pointed out that the leaks were a violation of Guttenberg's personal rights. Critics accused Guttenberg of trying to prevent the public release of the commission's final report, an allegation that he rejected.\nSeveral representatives from universities and science demanded the release of the commission's report due to public interest. A spokesman of the university rejected the lawyer's charges and stated that there had been no official report accusing Guttenberg of deliberate deception.\nThe spokesman of chancellor Merkel declared on 11 April 2011 that she was expecting full clarification of the matter.\nGuttenberg's lawyers declared on 13 April 2011 that he was still standing by his commitment of full cooperation but that he was opposed to leaks to the press, which violated proper proceedings and caused prejudgment. Guttenberg had no objection against publishing the commission's report after end of proceedings, the lawyers said. The same day it was reported that the Bundestag would not press charges against Guttenberg.\n\nA few days later media published extracts from Guttenberg's initial response to the commission. Guttenberg denied any deliberate plagiarism. He described the working-method during the several years of his dissertation as often working in short periods and using various different data carriers. This led to his losing track of the PhD dissertation, Guttenberg stated.\nGuttenberg sent his response to the commission's draft report on 26 April 2011. In the following days media again published extracts of his response. On 10 May 2011 media reported that one author, whose work was copied, filed a complaint against Guttenberg with the district attorney's office in Hof.\n\nAfter an announcement on 6 May 2011, already citing the conclusions, the university released the report to the public on 11 May 2011. According to that document, he had \"grossly violated standard research practices and in so doing deliberately deceived\" and it further stated that it was \"obvious that plagiarism was involved\".\n\nThe report started with the elaboration of the commission's historical background, its internal duties within the Bayreuth University, also in relation to other committees like the university's promotion commission. These included institutional enhancements, the evaluation of academic misconduct corresponding to standards of scientific community.\nThe report then defined the criteria of academic misconduct: deliberately or grossly negligent use of “Falschangaben” (misrepresentations), the violation of other's intellectual property, and the obstruction of research. “Falschangaben” were defined as the fabrication or distortion of data and the “obstruction of research” was defined as unauthorized use of material with arrogation of its authorship.\nThe commission comprised as regular members Wiebke Putz-Osterloh, Nuri Aksel, Paul Rösch, Stephan Rixen as chairman, and Diethelm Klippel as a consulting member – all from the University of Bayreuth.\nOn 8 March 2011 Wolfgang Löwer, professor of law at University of Bonn, and Jürgen Mittelstraß, professor of philosophy at University of Konstanz joined the commission as consulting members.\nThe report listed four commission's meeting on Guttenberg's thesis: 16 February 2011, 8 and 23 March 2011, and the final meeting on 7 April 2011. The hearings of the thesis’ supervisors were on 23 March 2011. Then the report specified the used material including its communications with Guttenberg himself.\n\nThe commission reported missing citations and violations of the academic rules of citations. It reported numerous verbatim forms of plagiarism and plagiarism with regard to content where the use of other authors’ material was not clearly expressed. The commission exemplified this in detail on the basis of six Parliamentary Research Service (PRS) papers and listed the passages Guttenberg had used in his thesis, including all text modifications and extensions. Two of the PRS papers lacked any citation and four PRS papers were cited insufficiently, leading to misconceptions about the initial authorship of the passages.\nThe commission judged this as deception and described a pattern of creation (werkprägendes Bearbeitungsmuster). As for Guttenberg, the commission concluded on intentional actions and deliberate deception due to the numbers of violations of academic rules of citations. Both the use of verbatim text elements and the use with marginal text changes lacking proper citation or listing cited sources only in bibliography were evaluated as attempts to cover up.\nThe commission saw no indications that the thesis had been written by a ghost-writer.\n\nContrary to previous expectations, the commission denied any responsibility of the thesis supervisors due to a lack of semantic or other indicators for plagiarism and Guttenberg's “exceedingly convincing” oral PhD exam (\"rigorosum\"). Only the supervisors’ omission to ask for the PRS papers cited in the bibliography and the grading with “summa cum laude” was criticized by the commission.\n\nIn his final response, Guttenberg once again denied that he had deliberately deceived the university and instead blamed severe “errors in workmanship” for the grave deficiencies in his doctoral thesis. He described a high burden of professional commitments as a result of new political responsibilities during the years of his dissertation, which led to an ad-hoc and sometimes chaotic working-method with long intervals between working periods. Guttenberg reported on diverse collections of material such as books, loose sheets of papers, more than 80 data carriers, and several laptops spread across several domiciles, used for his thesis. This led to an overburdening by the thesis, which further increased during the years, Guttenberg described. Family expectations, namely that a started task had to be finished, his intention to not disappoint his supervisor, and his unwillingness to admit weakness, hindered him to quit the dissertation, Guttenberg explained.\nThe commission however, based on the view, that Guttenberg had continued his dissertation despite the feeling of being overburdened, denied negligence and concluded on intention.\n\nThe University of Bayreuth's handling of the matter was sharply criticized by its former Vice-President and law professor Walter Schmitt-Glaeser, who, while agreeing that revoking the doctoral degree was justified, described the additional measures taken by the institution as an attempt at character assassination (\"Treibjagd\"). As any affiliation of Guttenberg to the university was severed by the withdrawal of his title, the university therefore lacked legal standing for any further investigation, Schmitt-Glaeser argued. The leaking of information from the commission's confidential meetings to the press – according to Schmitt-Glaeser most-likely done by its members – he called “outrageous” and “more than embarrassing”.\n\nFor Schmitt-Glaeser especially the part of the report looking into a possible deception by Guttenberg should not have been published. Also, according to Schmitt-Glaeser, the fact that Guttenberg had to agree to the report's publication due to massive public pressure, including from University of Bayreuth, cast a shadow of suspicion on the entire proceedings. The professor, former president (CSU) of the Bavarian senate, also criticized the lack of conclusive evidence to prove the university's assertion that there was \"deliberate deception\" on the part of Guttenberg, Schmitt-Glaeser referred to the multiple instances of minor text changes – which the commission viewed as indicators of cheating – as a typical procedure with a text considered by an author as his own work. With the proceedings the university denied any solicitousness for its former student Guttenberg and damaged his social existence, the professor criticized. From the judicial proceeding on this case he expected a result solely based on facts without regard to the person concerned. In an interview, Schmitt-Glaeser described the university's intention as an attempt to “drag Guttenberg in front of a tribunal and find him guilty”. In the professor's conviction, knowing Guttenberg personally, he had not committed intentional cheating.\n\nOthers criticized the commission's final report for not decrying the university's own lack of due diligence in the matter and noted that the university had first awarded Guttenberg's degree with the highest possible distinction (\"summa cum laude\"). Furthermore, the commission's neutrality was disputed as the University of Bayreuth was party to the proceedings. Among these critics was Thomas Goppel (CSU), former Bavarian Minister of Education, who viewed the report as an attempt by the university to downplay its own responsibility and acquit itself. Others complained about the University's lack of control provisions.\n\nProfessor Volker Rieble approved the report of the University but saw the case as an expression of the public desire for “ritual punishment”. Rieble decried the widespread practice of academic publications being written by assistants but published by professors as much worse for academia than any plagiarism.\n\nGünther Beckstein, former Minister-President of Bavaria, referred to Guttenberg by saying that everyone deserves a second chance, after some period of time.\n\nIn August 2011, the authors of Guttenplag Wiki, which triggered similar initiatives on VroniPlag Wiki, were accused of running a partisan campaign after it emerged that the founder of VroniPlag was a member of the opposition SPD party.\n\nIn November 2011 the attorney's office in Hof dropped the charges against Guttenberg after having found 23 relevant copyright violations with only minor economic damage. This decision was criticized for being biased toward economic criteria. The court approved this arrangement after Guttenberg had agreed to make a donation of 20,000 Euro to a German charitable foundation.\n\nThe attorney's office saw no indications that Guttenberg had intentionally used other authors’ texts within his thesis without proper attribution and judged his explanation of losing track of sources as “comprehensible and irrefutable”. Contrary to University of Bayreuth, which had accused Guttenberg of deliberate deception, the prosecution concluded that Guttenberg had made the thesis’ errors only with contingent intent (dolus eventualis).\nThe attorney's office also stated that there was no criminal abuse of his PhD title nor fraud or breach of trust related to PRS papers either.\n"}
{"id": "1596317", "url": "https://en.wikipedia.org/wiki?curid=1596317", "title": "Habitat", "text": "Habitat\n\nIn ecology, a habitat is the type of natural environment in which a particular species of organism lives. It is characterized by both physical and biological features. A species' habitat is those places where it can find food, shelter, protection and mates for reproduction.\n\nThe physical factors are for example soil, moisture, range of temperature, and light intensity as well as biotic factors such as the availability of food and the presence or absence of predators. Every organism has certain habitat needs for the conditions in which it will thrive, but some are tolerant of wide variations while others are very specific in their requirements. A habitat is not necessarily a geographical area, it can be the interior of a stem, a rotten log, a rock or a clump of moss, and for a parasitic organism it is the body of its host, part of the host's body such as the digestive tract, or a single cell within the host's body.\n\nHabitat types include polar, temperate, subtropical and tropical. The terrestrial vegetation type may be forest, steppe, grassland, semi-arid or desert. Fresh water habitats include marshes, streams, rivers, lakes, ponds and estuaries, and marine habitats include salt marshes, the coast, the intertidal zone, reefs, bays, the open sea, the sea bed, deep water and submarine vents.\n\nHabitats change over time. This may be due to a violent event such as the eruption of a volcano, an earthquake, a tsunami, a wildfire or a change in oceanic currents; or the change may be more gradual over millennia with alterations in the climate, as ice sheets and glaciers advance and retreat, and as different weather patterns bring changes of precipitation and solar radiation. Other changes come as a direct result of human activities; deforestation, the ploughing of ancient grasslands, the diversion and damming of rivers, the draining of marshland and the dredging of the seabed. The introduction of alien species can have a devastating effect on native wildlife, through increased predation, through competition for resources or through the introduction of pests and diseases to which the native species have no immunity.\n\nThe word \"habitat\" has been in use since about 1755 and derives from the Latin \"habitāre\", to inhabit, from \"habēre\", to have or to hold. Habitat can be defined as the natural environment of an organism, the type of place in which it is natural for it to live and grow. It is similar in meaning to a biotope; an area of uniform environmental conditions associated with a particular community of plants and animals.\n\nThe chief environmental factors affecting the distribution of living organisms are temperature, humidity, climate, soil type and light intensity, and the presence or absence of all the requirements that the organism needs to sustain it. Generally speaking, animal communities are reliant on specific types of plant communities.\n\nSome plants and animals are generalists, and their habitat requirements are met in a wide range of locations. The small white butterfly (\"Pieris rapae\") for example is found on all the continents of the world apart from Antarctica. Its larvae feed on a wide range of \"Brassicas\" and various other plant species, and it thrives in any open location with diverse plant associations. The large blue butterfly is much more specific in its requirements; it is found only in chalk grassland areas, its larvae feed on \"Thymus\" species and because of complex lifecycle requirements it inhabits only areas in which \"Myrmica\" ants live.\n\nDisturbance is important in the creation of biodiverse habitats. In the absence of disturbance, a climax vegetation cover develops that prevents the establishment of other species. Wildflower meadows are sometimes created by conservationists but most of the flowering plants used are either annuals or biennials and disappear after a few years in the absence of patches of bare ground on which their seedlings can grow. Lightning strikes and toppled trees in tropical forests allow species richness to be maintained as pioneering species move in to fill the gaps created. Similarly coastal habitats can become dominated by kelp until the seabed is disturbed by a storm and the algae swept away, or shifting sediment exposes new areas for colonisation. Another cause of disturbance is when an area may be overwhelmed by an invasive introduced species which is not kept under control by natural enemies in its new habitat.\n\nTerrestrial habitat types include forests, grasslands, wetlands and deserts. Within these broad biomes are more specific habitats with varying climate types, temperature regimes, soils, altitudes and vegetation types. Many of these habitats grade into each other and each one has its own typical communities of plants and animals. A habitat may suit a particular species well, but its presence or absence at any particular location depends to some extent on chance, on its dispersal abilities and its efficiency as a coloniser.\nFreshwater habitats include rivers, streams, lakes, ponds, marshes and bogs. Although some organisms are found across most of these habitats, the majority have more specific requirements. The water velocity, its temperature and oxygen saturation are important factors, but in river systems, there are fast and slow sections, pools, bayous and backwaters which provide a range of habitats. Similarly, aquatic plants can be floating, semi-submerged, submerged or grow in permanently or temporarily saturated soils besides bodies of water. Marginal plants provide important habitat for both invertebrates and vertebrates, and submerged plants provide oxygenation of the water, absorb nutrients and play a part in the reduction of pollution.\n\nMarine habitats include brackish water, estuaries, bays, the open sea, the intertidal zone, the sea bed, reefs and deep / shallow water zones. Further variations include rock pools, sand banks, mudflats, brackish lagoons, sandy and pebbly beaches, and seagrass beds, all supporting their own flora and fauna. The benthic zone or seabed provides a home for both static organisms, anchored to the substrate, and for a large range of organisms crawling on or burrowing into the surface. Some creatures float among the waves on the surface of the water, or raft on floating debris, others swim at a range of depths, including organisms in the demersal zone close to the seabed, and myriads of organisms drift with the currents and form the plankton.\nA desert is not the kind of habitat that favours the presence of amphibians, with their requirement for water to keep their skins moist and for the development of their young. Nevertheless, some frogs live in deserts, creating moist habitats underground and hibernating while conditions are adverse. Couch's spadefoot toad (\"Scaphiopus couchii\") emerges from its burrow when a downpour occurs and lays its eggs in the transient pools that form; the tadpoles develop with great rapidity, sometimes in as little as nine days, undergo metamorphosis, and feed voraciously before digging a burrow of their own.\n\nOther organisms cope with the drying up of their aqueous habitat in other ways. Vernal pools are ephemeral ponds that form in the rainy season and dry up afterwards. They have their specially-adapted characteristic flora, mainly consisting of annuals, the seeds of which survive the drought, but also some uniquely adapted perennials. Animals adapted to these extreme habitats also exist; fairy shrimps can lay \"winter eggs\" which are resistant to desiccation, sometimes being blown about with the dust, ending up in new depressions in the ground. These can survive in a dormant state for as long as fifteen years. Some killifish behave in a similar way; their eggs hatch and the juvenile fish grow with great rapidity when the conditions are right, but the whole population of fish may end up as eggs in diapause in the dried up mud that was once a pond.\n\nMany animals and plants have taken up residence in urban environments. They tend to be adaptable generalists and use the town's features to make their homes. Rats and mice have followed man around the globe, pigeons, peregrines, sparrows, swallows and house martins use the buildings for nesting, bats use roof space for roosting, foxes visit the garbage bins and squirrels, coyotes, raccoons and skunks roam the streets. About 2,000 coyotes are thought to live in and around Chicago. A survey of dwelling houses in northern European cities in the twentieth century found about 175 species of invertebrate inside them, including 53 species of beetle, 21 flies, 13 butterflies and moths, 13 mites, 9 lice, 7 bees, 5 wasps, 5 cockroaches, 5 spiders, 4 ants and a number of other groups. In warmer climates, termites are serious pests in the urban habitat; 183 species are known to affect buildings and 83 species cause serious structural damage.\n\nA microhabitat is the small-scale physical requirements of a particular organism or population. Every habitat includes large numbers of microhabitats with subtly different exposure to light, humidity, temperature, air movement, and other factors. The lichens that grow on the north face of a boulder are different to those that grow on the south face, from those on the level top and those that grow on the ground nearby; the lichens growing in the grooves and on the raised surfaces are different from those growing on the veins of quartz. Lurking among these miniature \"forests\" are the microfauna, each species of invertebrate with its own specific habitat requirements.\n\nThere are numerous different microhabitats in a wood; coniferous forest, broad-leafed forest, open woodland, scattered trees, woodland verges, clearings and glades; tree trunk, branch, twig, bud, leaf, flower and fruit; rough bark, smooth bark, damaged bark, rotten wood, hollow, groove and hole; canopy, shrub layer, plant layer, leaf litter and soil; buttress root, stump, fallen log, stem base, grass tussock, fungus, fern and moss. The greater the structural diversity in the wood, the greater the number of microhabitats that will be present. A range of tree species with individual specimens of varying sizes and ages, and a range of features such as streams, level areas, slopes, tracks, clearings and felled areas will provide suitable conditions for an enormous number of biodiverse plants and animals. For example, in Britain it has been estimated that various types of rotting wood are home to over 1700 species of invertebrate.\n\nFor a parasitic organism, its habitat is the particular part of the outside or inside of its host on or in which it is adapted to live. The life cycle of some parasites involves several different host species, as well as free-living life stages, sometimes providing vastly different microhabitats. One such organism is the trematode (flatworm) \"Microphallus turgidus\", present in brackish water marshes in the southeastern United States. Its first intermediate host is a snail and the second, a glass shrimp. The final host is the waterfowl or mammal that consumes the shrimp.\n\nAlthough the vast majority of life on Earth lives in mesophyllic (moderate) environments, a few organisms, most of them microbes, have managed to colonise extreme environments that are unsuitable for most higher life forms. There are bacteria, for example, living in Lake Whillans, half a mile below the ice of Antarctica; in the absence of sunlight, they must rely on organic material from elsewhere, perhaps decaying matter from glacier melt water or minerals from the underlying rock. Other bacteria can be found in abundance in the Mariana Trench, the deepest place in the ocean and on Earth; marine snow drifts down from the surface layers of the sea and accumulates in this undersea valley, providing nourishment for an extensive community of bacteria.\n\nOther microbes live in habitats lacking in oxygen, and are dependent on chemical reactions other than photosynthesis. Boreholes drilled into the rocky seabed have found microbial communities apparently based on the products of reactions between water and the constituents of rocks. These communities have been little studied, but may be an important part of the global carbon cycle. Rock in mines two miles deep also harbour microbes; these live on minute traces of hydrogen produced in slow oxidizing reactions inside the rock. These metabolic reactions allow life to exist in places with no oxygen or light, an environment that had previously been thought to be devoid of life.\n\nThe intertidal zone and the photic zone in the oceans are relatively familiar habitats. However the vast bulk of the ocean is unhospitable to air-breathing humans, with scuba divers limited to the upper or so. The lower limit for photosynthesis is and below that depth the prevailing conditions include total darkness, high pressure, little oxygen (in some places), scarce food resources and extreme cold. This habitat is very challenging to research, and as well as being little studied, it is vast, with 79% of the Earth's biosphere being at depths greater than . With no plant life, the animals in this zone are either detritivores, reliant on food drifting down from surface layers, or they are predators, feeding on each other. Some organisms are pelagic, swimming or drifting in mid-ocean, while others are benthic, living on or near the seabed. Their growth rates and metabolisms tend to be slow, their eyes may be very large to detect what little illumination there is, or they may be blind and rely on other sensory inputs. A number of deep sea creatures are bioluminescent; this serves a variety of functions including predation, protection and social recognition. In general, the bodies of animals living at great depths are adapted to high pressure environments by having pressure-resistant biomolecules and small organic molecules present in their cells known as piezolytes, which give the proteins the flexibility they need. There are also unsaturated fats in their membranes which prevent them from solidifying at low temperatures.\nHydrothermal vents were first discovered in the ocean depths in 1977. They result from seawater becoming heated after seeping through cracks to places where hot magma is close to the seabed. The under-water hot springs may gush forth at temperatures of over and support unique communities of organisms in their immediate vicinity. The basis for this teeming life is chemosynthesis, a process by which microbes convert such substances as hydrogen sulfide or ammonia into organic molecules. These bacteria and Archaea are the primary producers in these ecosystems and support a diverse array of life. About 350 species of organism, dominated by molluscs, polychaete worms and crustaceans, had been discovered around hydrothermal vents by the end of the twentieth century, most of them being new to science and endemic to these habitats.\n\nBesides providing locomotion opportunities for winged animals and a conduit for the dispersal of pollen grains, spores and seeds, the atmosphere can be considered to be a habitat in its own right. There are metabolically active microbes present that actively reproduce and spend their whole existence airborne, with hundreds of thousands of individual organisms estimated to be present in a cubic metre of air. The airborne microbial community may be as diverse as that found in soil or other terrestrial environments, however these organisms are not evenly distributed, their densities varying spatially with altitude and environmental conditions. Aerobiology has been little studied, but there is evidence of nitrogen fixation in clouds, and less clear evidence of carbon cycling, both facilitated by microbial activity.\n\nThere are other examples of extreme habitats where specially adapted lifeforms exist; tar pits teeming with microbial life; naturally occurring crude oil pools inhabited by the larvae of the petroleum fly; hot springs where the temperature may be as high as and cyanobacteria create microbial mats; cold seeps where the methane and hydrogen sulfide issue from the ocean floor and support microbes and higher animals such as mussels which form symbiotic associations with these anaerobic organisms; salt pans harbour salt-tolerant microorganisms and also \"Wallemia ichthyophaga\", a basidomycotous fungus; ice sheets in Antarctica which support fungi \"Thelebolus\" spp., and snowfields on which algae grow.\n\nWhether from natural processes or the activities of man, landscapes and their associated habitats change over time. There are the slow geomorphological changes associated with the geologic processes that cause tectonic uplift and subsidence, and the more rapid changes associated with earthquakes, landslides, storms, flooding, wildfires, coastal erosion, deforestation and changes in land use. Then there are the changes in habitats brought on by alterations in farming practices, tourism, pollution, fragmentation and climate change.\n\nLoss of habitat is the single greatest threat to any species. If an island on which an endemic organism lives becomes uninhabitable for some reason, the species will become extinct. Any type of habitat surrounded by a different habitat is in a similar situation to an island. If a forest is divided into parts by logging, with strips of cleared land separating woodland blocks, and the distances between the remaining fragments exceeds the distance an individual animal is able to travel, that species becomes especially vulnerable. Small populations generally lack genetic diversity and may be threatened by increased predation, increased competition, disease and unexpected catastrophe. At the edge of each forest fragment, increased light encourages secondary growth of fast-growing species and old growth trees are more vulnerable to logging as access is improved. The birds that nest in their crevices, the epiphytes that hang from their branches and the invertebrates in the leaf litter are all adversely affected and biodiversity is reduced. Habitat fragmentation can be ameliorated to some extent by the provision of wildlife corridors connecting the fragments. These can be a river, ditch, strip of trees, hedgerow or even an underpass to a highway. Without the corridors, seeds cannot disperse and animals, especially small ones, cannot travel through the hostile territory, putting populations at greater risk of local extinction.\n\nHabitat disturbance can have long-lasting effects on the environment. \"Bromus tectorum\" is a vigorous grass from Europe which has been introduced to the United States where it has become invasive. It is highly adapted to fire, producing large amounts of flammable detritus and increasing the frequency and intensity of wildfires. In areas where it has become established, it has altered the local fire regimen to such an extant that native plants cannot survive the frequent fires, allowing it to become even more dominant. A marine example is when sea urchin populations \"explode\" in coastal waters and destroy all the macroalgae present. What was previously a kelp forest becomes an urchin barren that may last for years and this can have a profound effect on the food chain. Removal of the sea urchins, by disease for example, can result in the seaweed returning, with an over-abundance of fast-growing kelp.\n\nThe protection of habitats is a necessary step in the maintenance of biodiversity because if habitat destruction occurs, the animals and plants reliant on that habitat suffer. Many countries have enacted legislation to protect their wildlife. This may take the form of the setting up of national parks, forest reserves and wildlife reserves, or it may restrict the activities of humans with the objective of benefiting wildlife. The laws may be designed to protect a particular species or group of species, or the legislation may prohibit such activities as the collecting of bird eggs, the hunting of animals or the removal of plants. A general law on the protection of habitats may be more difficult to implement than a site specific requirement. A concept introduced in the United States in 1973 involves protecting the critical habitat of endangered species, and a similar concept has been incorporated into some Australian legislation.\n\nInternational treaties may be necessary for such objectives as the setting up of marine reserves. Another international agreement, the Convention on the Conservation of Migratory Species of Wild Animals, protects animals that migrate across the globe and need protection in more than one country. However, the protection of habitats needs to take into account the needs of the local residents for food, fuel and other resources. Even where legislation protects the environment, a lack of enforcement often prevents effective protection. Faced with food shortage, a farmer is likely to plough up a level patch of ground despite it being the last suitable habitat for an endangered species such as the San Quintin kangaroo rat, and even kill the animal as a pest. In this regard, it is desirable to educate the community on the uniqueness of their flora and fauna and the benefits of ecotourism.\n\nA monotypic habitat is one in which a single species of animal or plant is so dominant as to virtually exclude all other species. An example would be sugarcane; this is planted, burnt and harvested, with herbicides killing weeds and pesticides controlling invertebrates. The monotypic habitat occurs in botanical and zoological contexts, and is a component of conservation biology. In restoration ecology of native plant communities or habitats, some invasive species create monotypic stands that replace and/or prevent other species, especially indigenous ones, from growing there. A dominant colonization can occur from retardant chemicals exuded, nutrient monopolization, or from lack of natural controls such as herbivores or climate, that keep them in balance with their native habitats. The yellow starthistle, \"Centaurea solstitialis\", is a botanical monotypic-habitat example of this, currently dominating over in California alone. The non-native freshwater zebra mussel, \"Dreissena polymorpha\", that colonizes areas of the Great Lakes and the Mississippi River watershed, is a zoological monotypic-habitat example; the predators that control it in its home-range in Russia are absent and it proliferates abundantly. Even though its name may seem to imply simplicity as compared with habitats, the monotypic habitat can be complex. Aquatic habitats, such as exotic \"Hydrilla\" beds, support a similarly rich fauna of macroinvertebrates to a more varied habitat, but the creatures present may differ between the two, affecting small fish and other animals higher up the food chain.\n\n\n\n"}
{"id": "3302770", "url": "https://en.wikipedia.org/wiki?curid=3302770", "title": "HubMed", "text": "HubMed\n\nHubMed is an alternative, third-party interface to PubMed, the database of biomedical literature produced by the National Library of Medicine. Features include relevance-ranked search results, direct citation export, tagging and graphical display of related articles.\n"}
{"id": "1233572", "url": "https://en.wikipedia.org/wiki?curid=1233572", "title": "Johannes Abraham Bierens de Haan", "text": "Johannes Abraham Bierens de Haan\n\nJohan(nes) Abraham Bierens de Haan (March 17, 1883 – June 13, 1958) was a Dutch biologist and ethologist.\n\nHe was born in Haarlem, and died in Siena, Italy.\n"}
{"id": "6766814", "url": "https://en.wikipedia.org/wiki?curid=6766814", "title": "John Rothwell (physiologist)", "text": "John Rothwell (physiologist)\n\nJohn C. Rothwell (born 1954) is a Professor of neurophysiology at the UCL Institute of Neurology. His main area of interest is transcranial magnetic stimulation and motor control.\n\nRothwell was educated at the University of Cambridge. He completed his PhD at King's College London in 1980 which supervised by David Marsden.\n\nHis group has pioneered the use of the paired-pulse technique (Kujirai et al. 1992), interhemispheric studies (Ferbert et al. 1992).\n\nRothwell was appointed head of the Medical Research Council (MRC) Human Movement and Balance Unit after the untimely death of David Marsden. He has written over 400 papers and numerous chapters.\n"}
{"id": "21572370", "url": "https://en.wikipedia.org/wiki?curid=21572370", "title": "Kenneth H. Hunt", "text": "Kenneth H. Hunt\n\nKenneth Henderson Hunt (1920–2002) was Foundation Professor of Engineering at Monash University in Melbourne, Australia and an expert in kinematics.\n\nHunt was born in Seaford, East Sussex, in the United Kingdom, on 7 June 1920. He studied engineering at Balliol College, Oxford University and, during World War II, served in the Royal Engineers. After the war, he worked in the oil industry until 1949, when he took a lecturership at the University of Melbourne. He moved to Monash in 1960, at which time he was appointed Foundation Professor, and was dean of engineering there from 1961 to 1975. He is the author of \"Mechanisms and Motion\" (1949) and \"Kinematic Geometry of Mechanisms\" (1978).\n\n"}
{"id": "7096810", "url": "https://en.wikipedia.org/wiki?curid=7096810", "title": "Launch Complex 39 Press Site", "text": "Launch Complex 39 Press Site\n\nThe Launch Complex 39 Press Site is a news media facility at Launch Complex 39 at the John F. Kennedy Space Center (KSC) on Merritt Island, Florida where journalists have observed every U.S. manned space launch since Apollo 8 in 1968. The site is just south of the Vehicle Assembly Building (VAB); 3 miles (4.8 km) west-southwest of Pad A, and 3.4 miles (5.4 km) southwest of Pad B.\n\nThe site includes an elevated mound where news media facilities are located, as well as the KSC News Center and several smaller support buildings. The News Center is and contains 15 site support offices, media workspace, and a media library. Current media buildings include CBS, NBC, \"Florida Today\" and \"The Orlando Sentinel\"; and trailers for The Associated Press and Reuters.\n\nThe 100-seat auditorium in the audio-video support building, where pre- and post-launch news conferences are held, is named for former CNN correspondent John Holliman, who covered space exploration until his death in 1998. It was built in 1980.\n\nA large illuminated digital countdown clock and a flagpole flying an American flag on the edge of the turning basin have often been included in television coverage and launch photos. Before a launch, the clock counts down, showing the remaining time until T-zero in hours, minutes and seconds (–00:00:00). After launch, the clock counts forward in Mission Elapsed Time for several hours. The flagpole also flew a smaller Space Shuttle Orbiter-specific flag below the American flag on launch day during the final years of the Space Shuttle Program.\n\nThe site was ready for coverage of the first launch from KSC, the unmanned Apollo 4 flight on November 9, 1967, for which NASA received 510 requests for news media accreditation. The sound of this first Saturn V liftoff was sufficiently powerful at the Press Site to prompt CBS-TV anchor Walter Cronkite to exclaim, \"Our building's shaking here...the floor is shaking...this big glass window is shaking, we're holding it with our hands!\" A ceiling tile or two were shaken loose above his head.\n\nDuring the Apollo program, the NASA news center was located in Cocoa Beach. To provide on-site public affairs offices, a Charter-Sphere dome from the Third Century America exhibition near the VAB during the United States Bicentennial in 1976 was later moved to the mound. In 1983, it was replaced by a larger dome; and a permanent building, the current KSC News Center, replaced that dome in December 1995.\n\nDuring the first decade of Space Shuttle launches, NASA contractors provided reference materials to the media from the Joint Industry Press Center (JIPC, pronounced \"gypsy\"), housed in a semi-permanent trailer located near a large covered grandstand facing the two launch pads.\n\nThe grandstand, built in 1967, was torn down following damage from Hurricane Frances in September 2004. Several media trailers and buildings on the mound were also damaged, and were either removed or replaced with prefabricated structures.\n\nOn November 10, 2014, NASA powered on the clock for the last time for a final system test. The clock will be removed and replaced before the Orion EFT-1 Launch. The old clock is currently on display at the KSC Visitor Center.\n\nOn July 16, 1969, 3,493 journalists from the U.S. and 55 other countries attended the launch of Apollo 11. A plaque noting the event placed in 1975 by Sigma Delta Chi, the Society of Professional Journalists, designates the location as an Historic Site in Journalism for \"the largest corps of newsmen in history...to report fully and freely to the largest audience in history\". After Apollo 11, however, media attendance diminished. Apollo 17, the last in the lunar landing program and its only night launch, prompted a resurgence in attendance, as did the Apollo-Soyuz Test Project launch in 1975.\n\nThe STS-1 launch on April 12, 1981 had 2,707 accredited representatives present. The second-largest number, 2,468, was for the STS-26 launch on September 29, 1988. Most, however, covered the launch from a more distant causeway viewing site because the LC-39 Press Site was restricted to a limited number of journalists as part of safety precautions implemented after the 1986 \"Challenger\" explosion. The restriction was dropped for subsequent launches. Media attendance spiked again in October 1998 for John Glenn's launch aboard STS-95, and for the final shuttle launch, STS-135, on July 8, 2011 when 1,585 news people attended.\n\nOn July 16, 1974, a crowd gathered at the Press Site to dedicate the entire launch complex as a National Historic Site, which had been listed as of May 24, 1973. The countdown clock was set to reach zero at 9:32 a.m. ET, exactly five years after the Apollo 11 liftoff. Astronauts Neil Armstrong, Buzz Aldrin and Mike Collins then unveiled a plaque commemorating their flight. On January 21, 2000, the \"LC-39 Press Site–Clock and Flag Pole\" were specifically added to the U.S. National Register of Historic Places as part of a Multiple Property Submission.\n\n"}
{"id": "37968947", "url": "https://en.wikipedia.org/wiki?curid=37968947", "title": "List of Podicipediformes by population", "text": "List of Podicipediformes by population\n\nThis is a list of Podicipediformes species by global population. While numbers are estimates, they have been made by the experts in their fields. For more information on how these estimates were ascertained, see Wikipedia's articles on population biology and population ecology.\n\nThis list is not comprehensive, as not all Podicipediformes have had their numbers quantified.\n"}
{"id": "45568470", "url": "https://en.wikipedia.org/wiki?curid=45568470", "title": "List of TRS-80 and Tandy-branded computers", "text": "List of TRS-80 and Tandy-branded computers\n\nTandy Corporation released several computer product lines starting in 1977, under both \"TRS-80\" and \"Tandy\" branding.\n\n\"TRS-80\" was a brand associated with several desktop microcomputer lines sold by Tandy Corporation through their Radio Shack stores. It was first used on the original TRS-80 (later known as the \"Model I\"), one of the earliest mass-produced personal computers. However, Tandy later used the \"TRS-80\" name on a number of different computer lines, many of which were technically unrelated to (and incompatible with) the original Model I and its replacements.\n\nIn addition to these, Tandy released a number of computers using the \"Tandy\" name itself.\n\nThe original TRS-80 Micro Computer System (later known as the Model I to distinguish it from successors) was launched in 1977 and- alongside the Apple II and Commodore Pet- was one of the earliest mass-produced personal computers. The line won popularity with hobbyists, home users, and small-businesses.\n\nThe Model I included a full-stroke QWERTY keyboard, floating-point BASIC, a monitor, and a starting price of US$600.\n\nBy 1979, the TRS-80 had the largest selection of software in the microcomputer market.\n\nIn July 1980 the mostly-compatible TRS-80 Model III was launched, and the original Model I was discontinued.\n\nIn July 1980 Tandy released the Model III, a mostly-compatible replacement for the Model I.\n\nIts improvements over the Model I included built-in lower case, a better keyboard, elimination of the cable spaghetti, 1500-baud cassette interface, and a faster (2.03 MHz) Z-80 processor. With the introduction of the Model III, Model I production was discontinued as it did not comply with new FCC regulations as of January 1, 1981 regarding electromagnetic interference.\n\nThe Model III could run about 80% of Model I software, but used an incompatible disk format. It also came with the option of integrated disk drives.\n\nThe successor to the Model III was the Model 4, which included the capability to run CP/M, and had a faster Z80A 4 MHz CPU. It was completely compatible with Model III software. Prices started from $999 for the diskless version.\n\nEarly versions of the Model 4 mainboard were designed to accept a Zilog Z800 16 bit CPU upgrade board to replace the Z80 8 bit CPU but this option was never released.\n\nTandy's first design for the business market was a desk-based computer known as the Tandy 10 Business Computer System, which was released in 1978 but quickly discontinued.\n\nIn October 1979 Tandy began shipping the TRS-80 Model II, which was targeted to the small-business market. It was not an upgrade of the Model I, but an entirely different system with state-of-the-art hardware and numerous features not found in the primitive Model I. The Model II was not compatible with the Model I and never had the same breadth of available software. This was somewhat mitigated by the availability of the CP/M from third parties.\n\nThe Model II was replaced in 1982 by the \"TRS-80 Model 12\". This was essentially a Model 16B (described below) without the Motorola processor, and could be upgraded to a Model 16B.\n\nIn February 1982, Tandy released the TRS-80 Model 16, as the follow-on to the Model II; an upgrade kit was available for Model II systems. The Model 16 adds a 6 MHz, 16-bit Motorola 68000 processor and memory card.\n\nThe Model 16 sold poorly at first and was reliant on existing Model II software early on. In early 1983 Tandy switched from TRSDOS-16 to Xenix.\n\nThe Model 16 evolved into the Model 16B with 256 KB in July 1983, and later the Tandy 6000, gaining an internal hard drive along the way and switching to an 8 MHz 68000. \nThe 16B was the most popular Unix computer in 1984, with almost 40,000 units sold.\n\nTandy also produced the \"TRS-80 Color Computer\" (CoCo), based on the Motorola 6809 processor. This machine was clearly aimed at the home market, where the Model II and above were sold as business machines. It competed directly with the Commodore 64, Apple II, and Atari 8-bit family of computers. OS-9, a multitasking, multi-user operating system was supplied for this machine.\n\nIn addition to the above, Tandy produced the \"TRS-80 Model 100\" series of laptop computers. This series comprised the TRS-80 Model 100, Tandy 102, Tandy 200 and Tandy 600. The Model 100 was designed by the Japanese company Kyocera with software written by Microsoft. (The Model 100 firmware was the last Microsoft product to which Bill Gates was a major code contributor.) It was also marketed as the Micro Executive Workstation (MEWS).\n\nThe Model 100 had an internal 300 baud modem, built-in BASIC, and a limited text editor. It was possible to use the Model 100 with most phones in the world with the use of an optional acoustic coupler that fit over a standard telephone handset. The combination of the acoustic coupler, the machine's outstanding battery life (it could be used for days on a set of 4 AA cells), and its simple text editor made the Model 100/102 popular with journalists in the early 1980s. The Model 100 line also had an optional bar code reader, serial/RS-232 floppy drive and a Cassette interface.\n\nAlso available as an option to the Model 100 was an external expansion unit supporting video and a 5\" disk drive, connected via the 40-pin expansion port in the bottom of the unit.\n\nThe \"Tandy 200\" was introduced in 1984 as a higher-end complement to the Model 100. The Tandy 200 had 24 KB RAM expandable to 72 KB, a flip-up 16 line by 40 column display, and a spreadsheet (Multiplan) included. The Tandy 200 also included DTMF tone-dialing for the internal modem. Although less popular than the Model 100, the Tandy 200 was also particularly popular with journalists in the late 1980s and early 1990s.\n\n\"InfoWorld\" in 1985 disapproved of the computer's high cost of accessories (\"and you'll find that the Tandy 200 has more accessories than a Barbie doll\"), but called it \"a big step up from the Model 100 for someone who needs a note-taker or spreadsheet on the run\".\n\nThe MC-10 was a short-lived and little-known Tandy computer, similar in appearance to the Sinclair ZX81.\n\nIt was a small system based on the Motorola 6803 processor and featured 4 KB of RAM. A 16 KB RAM expansion pack that connected on the back of the unit was offered as an option as was a thermal paper printer. A modified version of the MC-10 was sold in France as the Matra Alice.\n\nPrograms loaded using a cassette which worked much better than those for the Sinclair. A magazine was published which offered programs for both the CoCo and MC-10 but very few programs were available for purchase. Programs for the MC-10 were not compatible with the CoCo.\n\nBoth the TRS-80 and Tandy brands were used for a range of \"Pocket Computers\" sold by Tandy. These were manufactured by Sharp or Casio, depending on the model.\n\nIn the early 1980s, Tandy began producing a line of computers that were \"DOS compatible\": able to run MS-DOS and certain applications, but not fully compatible with every nuance of the original IBM PC systems. The first of these was the \"Tandy 2000\", followed later by the less expensive \"Tandy 1000\". As margins decreased in PC clones, Tandy was unable to compete and stopped manufacturing their own systems, instead selling computers manufactured by a variety of companies, AST Research and Gateway 2000 among them.\n\nOriginally, Tandy offered computers manufactured by Tandon Corporation, and then started producing their own line of systems.\n\nThe Tandy 2000 system was similar to the Texas Instruments Professional Computer in that it offered better graphics, a faster processor (80186) and higher capacity disk drives (80 track double sided 800k 5.25 drives) than the original IBM PC.\n\nHowever, around the time of its introduction, the industry began moving away from MS-DOS compatible computers and towards fully IBM PC compatible clones; later Tandy offerings moved toward full PC hardware compatibility.\n\nThe later Tandy 1000 systems and follow-ons were also marketed by DEC, as Tandy and DEC had a joint manufacturing agreement.\n"}
{"id": "14485708", "url": "https://en.wikipedia.org/wiki?curid=14485708", "title": "List of members of the National Academy of Sciences (Cellular and molecular neuroscience)", "text": "List of members of the National Academy of Sciences (Cellular and molecular neuroscience)\n"}
{"id": "37019597", "url": "https://en.wikipedia.org/wiki?curid=37019597", "title": "List of photonics equations", "text": "List of photonics equations\n\nThis article summarizes equations in the theory of photonics, including geometric optics, physical optics, radiometry, diffraction, and interferometry.\n\nThere are different forms of the Poynting vector, the most common are in terms of the E and B or E and H fields.\n\nFor spectral quantities two definitions are in use to refer to the same quantity, in terms of frequency or wavelength.\n\nFar-field (Fraunhofer)\nformula_1\n!Huygen-Fresnel-Kirchhoff principle\n!Kirchhoff's diffraction formula\n\nIn astrophysics, \"L\" is used for \"luminosity\" (energy per unit time, equivalent to \"power\") and \"F\" is used for \"energy flux\" (energy per unit time per unit area, equivalent to \"intensity\" in terms of area, not solid angle). They are not new quantities, simply different names.\n\n\n"}
{"id": "175470", "url": "https://en.wikipedia.org/wiki?curid=175470", "title": "Magnetic monopole", "text": "Magnetic monopole\n\nIn particle physics, a magnetic monopole is a hypothetical elementary particle that is an isolated magnet with only one magnetic pole (a north pole without a south pole or vice versa). \nA magnetic monopole would have a net \"magnetic charge\". Modern interest in the concept stems from particle theories, notably the grand unified and superstring theories, which predict their existence.\n\nMagnetism in bar magnets and electromagnets is not caused by magnetic monopoles, and indeed, there is no known experimental or observational evidence that magnetic monopoles exist.\n\nSome condensed matter systems contain effective (non-isolated) magnetic monopole quasi-particles, or contain phenomena that are mathematically analogous to magnetic monopoles.\n\nMany early scientists attributed the magnetism of lodestones to two different \"magnetic fluids\" (\"effluvia\"), a north-pole fluid at one end and a south-pole fluid at the other, which attracted and repelled each other in analogy to positive and negative electric charge. However, an improved understanding of electromagnetism in the nineteenth century showed that the magnetism of lodestones was properly explained not by magnetic monopole fluids, but rather by a combination of electric currents, the electron magnetic moment, and the magnetic moments of other particles. Gauss's law for magnetism, one of Maxwell's equations, is the mathematical statement that magnetic monopoles do not exist. Nevertheless, it was pointed out by Pierre Curie in 1894 that magnetic monopoles \"could\" conceivably exist, despite not having been seen so far.\n\nThe \"quantum\" theory of magnetic charge started with a paper by the physicist Paul A. M. Dirac in 1931. In this paper, Dirac showed that if \"any\" magnetic monopoles exist in the universe, then all electric charge in the universe must be quantized (Dirac quantization condition). The electric charge \"is\", in fact, quantized, which is consistent with (but does not prove) the existence of monopoles.\n\nSince Dirac's paper, several systematic monopole searches have been performed. Experiments in 1975 and 1982 produced candidate events that were initially interpreted as monopoles, but are now regarded as inconclusive. Therefore, it remains an open question whether monopoles exist.\nFurther advances in theoretical particle physics, particularly developments in grand unified theories and quantum gravity, have led to more compelling arguments (detailed below) that monopoles do exist. Joseph Polchinski, a string-theorist, described the existence of monopoles as \"one of the safest bets that one can make about physics not yet seen\". These theories are not necessarily inconsistent with the experimental evidence. In some theoretical models, magnetic monopoles are unlikely to be observed, because they are too massive to create in particle accelerators (see below), and also too rare in the Universe to enter a particle detector with much probability.\n\nSome condensed matter systems propose a structure superficially similar to a magnetic monopole, known as a flux tube. The ends of a flux tube form a magnetic dipole, but since they move independently, they can be treated for many purposes as independent magnetic monopole quasiparticles. Since 2009, numerous news reports from the popular media have incorrectly described these systems as the long-awaited discovery of the magnetic monopoles, but the two phenomena are only superficially related to one another. These condensed-matter systems remain an area of active research. (See below.)\n\nAll matter ever isolated to date, including every atom on the periodic table and every particle in the standard model, has zero magnetic monopole charge. Therefore, the ordinary phenomena of magnetism and magnets have nothing to do with magnetic monopoles.\n\nInstead, magnetism in ordinary matter comes from two sources. First, electric currents create magnetic fields according to Ampère's law. Second, many elementary particles have an \"intrinsic\" magnetic moment, the most important of which is the electron magnetic dipole moment. (This magnetism is related to quantum-mechanical \"spin\".)\n\nMathematically, the magnetic field of an object is often described in terms of a multipole expansion. This is an expression of the field as the sum of component fields with specific mathematical forms. The first term in the expansion is called the \"monopole\" term, the second is called \"dipole\", then \"quadrupole\", then \"octupole\", and so on. Any of these terms can be present in the multipole expansion of an electric field, for example. However, in the multipole expansion of a \"magnetic\" field, the \"monopole\" term is always exactly zero (for ordinary matter). A magnetic monopole, if it exists, would have the defining property of producing a magnetic field whose \"monopole\" term is non-zero.\n\nA magnetic dipole is something whose magnetic field is predominantly or exactly described by the magnetic dipole term of the multipole expansion. The term \"dipole\" means \"two poles\", corresponding to the fact that a dipole magnet typically contains a \"north pole\" on one side and a \"south pole\" on the other side. This is analogous to an electric dipole, which has positive charge on one side and negative charge on the other. However, an electric dipole and magnetic dipole are fundamentally quite different. In an electric dipole made of ordinary matter, the positive charge is made of protons and the negative charge is made of electrons, but a magnetic dipole does \"not\" have different types of matter creating the north pole and south pole. Instead, the two magnetic poles arise simultaneously from the aggregate effect of all the currents and intrinsic moments throughout the magnet. Because of this, the two poles of a magnetic dipole must always have equal and opposite strength, and the two poles cannot be separated from each other.\n\nMaxwell's equations of electromagnetism relate the electric and magnetic fields to each other and to the motions of electric charges. The standard equations provide for electric charges, but they posit no magnetic charges. Except for this difference, the equations are symmetric under the interchange of the electric and magnetic fields. In fact, symmetric Maxwell's equations can be written when all charges (and hence electric currents) are zero, and this is how the electromagnetic wave equation is derived.\n\nFully symmetric Maxwell's equations can also be written if one allows for the possibility of \"magnetic charges\" analogous to electric charges. With the inclusion of a variable for the density of these magnetic charges, say , there is also a \"magnetic current density\" variable in the equations, .\n\nIf magnetic charges do not exist – or if they do exist but are not present in a region of space – then the new terms in Maxwell's equations are all zero, and the extended equations reduce to the conventional equations of electromagnetism such as (where is divergence and is the magnetic field).\n\nThe extended Maxwell's equations are as follows, in Gaussian cgs units:\n\nIn these equations is the \"magnetic charge density\", is the \"magnetic current density\", and is the \"magnetic charge\" of a test particle, all defined analogously to the related quantities of electric charge and current; is the particle's velocity and is the speed of light. For all other definitions and details, see Maxwell's equations. For the equations in nondimensionalized form, remove the factors of .\n\nIn SI units, there are two conflicting units in use for magnetic charge : webers (Wb) and ampere·meters (A·m). The conversion between them is , since the units are by dimensional analysis (H is the henry – the SI unit of inductance).\n\nMaxwell's equations then take the following forms (using the same notation above):\n\nMaxwell's equations in the language of tensors makes Lorentz covariance clear. The generalized equations are:\n\nwhere\n\nFor a particle having only electric charge, one can express its field using a four-potential, according to the standard covariant formulation of classical electromagnetism:\n\nHowever, this formula is inadequate for a particle that has both electric and magnetic charge, and we must add a term involving another potential .\n\nThis formula for the fields is often called the Cabibbo–Ferrari relation, though Shanmugadhasan proposed it earlier. The quantity is the Levi-Civita symbol, and the indices (as usual) behave according to the Einstein summation convention.\n\nThe generalized Maxwell's equations possess a certain symmetry, called a \"duality transformation\". One can choose any real angle , and simultaneously change the fields and charges everywhere in the universe as follows (in Gaussian units):\nwhere the primed quantities are the charges and fields before the transformation, and the unprimed quantities are after the transformation. The fields and charges after this transformation still obey the same Maxwell's equations. The matrix is a two-dimensional rotation matrix.\n\nBecause of the duality transformation, one cannot uniquely decide whether a particle has an electric charge, a magnetic charge, or both, just by observing its behavior and comparing that to Maxwell's equations. For example, it is merely a convention, not a requirement of Maxwell's equations, that electrons have electric charge but not magnetic charge; after a transformation, it would be the other way around. The key empirical fact is that all particles ever observed have the same ratio of magnetic charge to electric charge. Duality transformations can change the ratio to any arbitrary numerical value, but cannot change the fact that all particles have the same ratio. Since this is the case, a duality transformation can be made that sets this ratio at zero, so that all particles have no magnetic charge. This choice underlies the \"conventional\" definitions of electricity and magnetism.\n\nOne of the defining advances in quantum theory was Paul Dirac's work on developing a relativistic quantum electromagnetism. Before his formulation, the presence of electric charge was simply \"inserted\" into the equations of quantum mechanics (QM), but in 1931 Dirac showed that a discrete charge naturally \"falls out\" of QM. That is to say, we can maintain the form of Maxwell's equations and still have magnetic charges.\n\nConsider a system consisting of a single stationary electric monopole (an electron, say) and a single stationary magnetic monopole. Classically, the electromagnetic field surrounding them has a momentum density given by the Poynting vector, and it also has a total angular momentum, which is proportional to the product , and independent of the distance between them.\n\nQuantum mechanics dictates, however, that angular momentum is quantized in units of , so therefore the product must also be quantized. This means that if even a single magnetic monopole existed in the universe, and the form of Maxwell's equations is valid, all electric charges would then be quantized.\n\nWhat are the units in which magnetic charge would be quantized? Although it would be possible simply to integrate over all space to find the total angular momentum in the above example, Dirac took a different approach. This led him to new ideas. He considered a point-like magnetic charge whose magnetic field behaves as and is directed in the radial direction, located at the origin. Because the divergence of is equal to zero almost everywhere, except for the locus of the magnetic monopole at , one can locally define the vector potential such that the curl of the vector potential equals the magnetic field .\n\nHowever, the vector potential cannot be defined globally precisely because the divergence of the magnetic field is proportional to the Dirac delta function at the origin. We must define one set of functions for the vector potential on the \"northern hemisphere\" (the half-space above the particle), and another set of functions for the \"southern hemisphere\". These two vector potentials are matched at the \"equator\" (the plane through the particle), and they differ by a gauge transformation. The wave function of an electrically charged particle (a \"probe charge\") that orbits the \"equator\" generally changes by a phase, much like in the Aharonov–Bohm effect. This phase is proportional to the electric charge of the probe, as well as to the magnetic charge of the source. Dirac was originally considering an electron whose wave function is described by the Dirac equation.\n\nBecause the electron returns to the same point after the full trip around the equator, the phase of its wave function must be unchanged, which implies that the phase added to the wave function must be a multiple of :\n\nwhere is the vacuum permittivity, is the reduced Planck's constant, is the speed of light, and is the set of integers.\n\nThis is known as the Dirac quantization condition. The hypothetical existence of a magnetic monopole would imply that the electric charge must be quantized in certain units; also, the existence of the electric charges implies that the magnetic charges of the hypothetical magnetic monopoles, if they exist, must be quantized in units inversely proportional to the elementary electric charge.\n\nAt the time it was not clear if such a thing existed, or even had to. After all, another theory could come along that would explain charge quantization without need for the monopole. The concept remained something of a curiosity. However, in the time since the publication of this seminal work, no other widely accepted explanation of charge quantization has appeared. (The concept of local gauge invariance—see Gauge theory—provides a natural explanation of charge quantization, without invoking the need for magnetic monopoles; but only if the U(1) gauge group is compact, in which case we have magnetic monopoles anyway.)\n\nIf we maximally extend the definition of the vector potential for the southern hemisphere, it is defined everywhere except for a semi-infinite line stretched from the origin in the direction towards the northern pole. This semi-infinite line is called the Dirac string and its effect on the wave function is analogous to the effect of the solenoid in the Aharonov–Bohm effect. The quantization condition comes from the requirement that the phases around the Dirac string are trivial, which means that the Dirac string must be unphysical. The Dirac string is merely an artifact of the coordinate chart used and should not be taken seriously.\n\nThe Dirac monopole is a singular solution of Maxwell's equation (because it requires removing the worldline from spacetime); in more complicated theories, it is superseded by a smooth solution such as the 't Hooft–Polyakov monopole.\n\nA gauge theory like electromagnetism is defined by a gauge field, which associates a group element to each path in space time. For infinitesimal paths, the group element is close to the identity, while for longer paths the group element is the successive product of the infinitesimal group elements along the way.\n\nIn electrodynamics, the group is U(1), unit complex numbers under multiplication. For infinitesimal paths, the group element is which implies that for finite paths parametrized by , the group element is:\n\nThe map from paths to group elements is called the Wilson loop or the holonomy, and for a U(1) gauge group it is the phase factor which the wavefunction of a charged particle acquires as it traverses the path. For a loop:\n\nSo that the phase a charged particle gets when going in a loop is the magnetic flux through the loop. When a small solenoid has a magnetic flux, there are interference fringes for charged particles which go around the solenoid, or around different sides of the solenoid, which reveal its presence.\n\nBut if all particle charges are integer multiples of , solenoids with a flux of have no interference fringes, because the phase factor for any charged particle is . Such a solenoid, if thin enough, is quantum-mechanically invisible. If such a solenoid were to carry a flux of , when the flux leaked out from one of its ends it would be indistinguishable from a monopole.\n\nDirac's monopole solution in fact describes an infinitesimal line solenoid ending at a point, and the location of the solenoid is the singular part of the solution, the Dirac string. Dirac strings link monopoles and antimonopoles of opposite magnetic charge, although in Dirac's version, the string just goes off to infinity. The string is unobservable, so you can put it anywhere, and by using two coordinate patches, the field in each patch can be made nonsingular by sliding the string to where it cannot be seen.\n\nIn a U(1) gauge group with quantized charge, the group is a circle of radius . Such a U(1) gauge group is called compact. Any U(1) that comes from a grand unified theory is compact – because only compact higher gauge groups make sense. The size of the gauge group is a measure of the inverse coupling constant, so that in the limit of a large-volume gauge group, the interaction of any fixed representation goes to zero.\n\nThe case of the U(1) gauge group is a special case because all its irreducible representations are of the same size – the charge is bigger by an integer amount, but the field is still just a complex number – so that in U(1) gauge field theory it is possible to take the decompactified limit with no contradiction. The quantum of charge becomes small, but each charged particle has a huge number of charge quanta so its charge stays finite. In a non-compact U(1) gauge group theory, the charges of particles are generically not integer multiples of a single unit. Since charge quantization is an experimental certainty, it is clear that the U(1) gauge group of electromagnetism is compact.\n\nGUTs lead to compact U(1) gauge groups, so they explain charge quantization in a way that seems logically independent from magnetic monopoles. However, the explanation is essentially the same, because in any GUT that breaks down into a U(1) gauge group at long distances, there are magnetic monopoles.\n\nThe argument is topological:\n\n\nHence, the Dirac monopole is a topological defect in a compact U(1) gauge theory. When there is no GUT, the defect is a singularity – the core shrinks to a point. But when there is some sort of short-distance regulator on space time, the monopoles have a finite mass. Monopoles occur in lattice U(1), and there the core size is the lattice size. In general, they are expected to occur whenever there is a short-distance regulator.\n\nIn the universe, quantum gravity provides the regulator. When gravity is included, the monopole singularity can be a black hole, and for large magnetic charge and mass, the black hole mass is equal to the black hole charge, so that the mass of the magnetic black hole is not infinite. If the black hole can decay completely by Hawking radiation, the lightest charged particles cannot be too heavy. The lightest monopole should have a mass less than or comparable to its charge in natural units.\n\nSo in a consistent holographic theory, of which string theory is the only known example, there are always finite-mass monopoles. For ordinary electromagnetism, the upper mass bound is not very useful because it is about same size as the Planck mass.\n\nIn mathematics, a (classical) gauge field is defined as a connection over a principal G-bundle over spacetime. is the gauge group, and it acts on each fiber of the bundle separately.\n\nA \"connection\" on a -bundle tells you how to glue fibers together at nearby points of . It starts with a continuous symmetry group that acts on the fiber , and then it associates a group element with each infinitesimal path. Group multiplication along any path tells you how to move from one point on the bundle to another, by having the element associated to a path act on the fiber .\n\nIn mathematics, the definition of bundle is designed to emphasize topology, so the notion of connection is added on as an afterthought. In physics, the connection is the fundamental physical object. One of the fundamental observations in the theory of characteristic classes in algebraic topology is that many homotopical structures of nontrivial principal bundles may be expressed as an integral of some polynomial over any connection over it. Note that a connection over a trivial bundle can never give us a nontrivial principal bundle.\n\nIf spacetime is the space of all possible connections of the -bundle is connected. But consider what happens when we remove a timelike worldline from spacetime. The resulting spacetime is homotopically equivalent to the topological sphere .\n\nA principal -bundle over is defined by covering by two charts, each homeomorphic to the open 2-ball such that their intersection is homeomorphic to the strip . 2-balls are homotopically trivial and the strip is homotopically equivalent to the circle . So a topological classification of the possible connections is reduced to classifying the transition functions. The transition function maps the strip to , and the different ways of mapping a strip into are given by the first homotopy group of .\n\nSo in the -bundle formulation, a gauge theory admits Dirac monopoles provided is not simply connected, whenever there are paths that go around the group that cannot be deformed to a constant path (a path whose image consists of a single point). U(1), which has quantized charges, is not simply connected and can have Dirac monopoles while , its universal covering group, is simply connected, doesn't have quantized charges and does not admit Dirac monopoles. The mathematical definition is equivalent to the physics definition provided that—following Dirac—gauge fields are allowed that are defined only patch-wise, and the gauge field on different patches are glued after a gauge transformation.\n\nThe total magnetic flux is none other than the first Chern number of the principal bundle, and depends only upon the choice of the principal bundle, and not the specific connection over it. In other words, it is a topological invariant.\n\nThis argument for monopoles is a restatement of the lasso argument for a pure U(1) theory. It generalizes to dimensions with in several ways. One way is to extend everything into the extra dimensions, so that U(1) monopoles become sheets of dimension . Another way is to examine the type of topological singularity at a point with the homotopy group .\n\nIn more recent years, a new class of theories has also suggested the existence of magnetic monopoles.\n\nDuring the early 1970s, the successes of quantum field theory and gauge theory in the development of electroweak theory and the mathematics of the strong nuclear force led many theorists to move on to attempt to combine them in a single theory known as a Grand Unified Theory (GUT). Several GUTs were proposed, most of which implied the presence of a real magnetic monopole particle. More accurately, GUTs predicted a range of particles known as dyons, of which the most basic state was a monopole. The charge on magnetic monopoles predicted by GUTs is either 1 or 2 \"gD\", depending on the theory.\n\nThe majority of particles appearing in any quantum field theory are unstable, and they decay into other particles in a variety of reactions that must satisfy various conservation laws. Stable particles are stable because there are no lighter particles into which they can decay and still satisfy the conservation laws. For instance, the electron has a lepton number of one and an electric charge of one, and there are no lighter particles that conserve these values. On the other hand, the muon, essentially a heavy electron, can decay into the electron plus two quanta of energy, and hence it is not stable.\n\nThe dyons in these GUTs are also stable, but for an entirely different reason. The dyons are expected to exist as a side effect of the \"freezing out\" of the conditions of the early universe, or a symmetry breaking. In this scenario, the dyons arise due to the configuration of the vacuum in a particular area of the universe, according to the original Dirac theory. They remain stable not because of a conservation condition, but because there is no simpler \"topological\" state into which they can decay.\n\nThe length scale over which this special vacuum configuration exists is called the \"correlation length\" of the system. A correlation length cannot be larger than causality would allow, therefore the correlation length for making magnetic monopoles must be at least as big as the horizon size determined by the metric of the expanding universe. According to that logic, there should be at least one magnetic monopole per horizon volume as it was when the symmetry breaking took place.\n\nCosmological models of the events following the big bang make predictions about what the horizon volume was, which lead to predictions about present-day monopole density. Early models predicted an enormous density of monopoles, in clear contradiction to the experimental evidence. This was called the \"monopole problem\". Its widely accepted resolution was not a change in the particle-physics prediction of monopoles, but rather in the cosmological models used to infer their present-day density. Specifically, more recent theories of cosmic inflation drastically reduce the predicted number of magnetic monopoles, to a density small enough to make it unsurprising that humans have never seen one. This resolution of the \"monopole problem\" was regarded as a success of cosmic inflation theory. (However, of course, it is only a noteworthy success if the particle-physics monopole prediction is correct.) For these reasons, monopoles became a major interest in the 1970s and 80s, along with the other \"approachable\" predictions of GUTs such as proton decay.\n\nMany of the other particles predicted by these GUTs were beyond the abilities of current experiments to detect. For instance, a wide class of particles known as the X and Y bosons are predicted to mediate the coupling of the electroweak and strong forces, but these particles are extremely heavy and well beyond the capabilities of any reasonable particle accelerator to create.\n\nExperimental searches for magnetic monopoles can be placed in one of two categories: those that try to detect preexisting magnetic monopoles and those that try to create and detect new magnetic monopoles.\n\nPassing a magnetic monopole through a coil of wire induces a net current in the coil. This is not the case for a magnetic dipole or higher order magnetic pole, for which the net induced current is zero, and hence the effect can be used as an unambiguous test for the presence of magnetic monopoles. In a wire with finite resistance, the induced current quickly dissipates its energy as heat, but in a superconducting loop the induced current is long-lived. By using a highly sensitive \"superconducting quantum interference device\" (SQUID) one can, in principle, detect even a single magnetic monopole.\n\nAccording to standard inflationary cosmology, magnetic monopoles produced before inflation would have been diluted to an extremely low density today. Magnetic monopoles may also have been produced thermally after inflation, during the period of reheating. However, the current bounds on the reheating temperature span 18 orders of magnitude and as a consequence the density of magnetic monopoles today is not well constrained by theory.\n\nThere have been many searches for preexisting magnetic monopoles. Although there have been tantalizing events recorded, in particular the event recorded by Blas Cabrera on the night of February 14, 1982 (thus, sometimes referred to as the \"Valentine's Day Monopole\"), there has never been reproducible evidence for the existence of magnetic monopoles. The lack of such events places an upper limit on the number of monopoles of about one monopole per 10 nucleons.\n\nAnother experiment in 1975 resulted in the announcement of the detection of a moving magnetic monopole in cosmic rays by the team led by P. Buford Price. Price later retracted his claim, and a possible alternative explanation was offered by Alvarez. In his paper it was demonstrated that the path of the cosmic ray event that was claimed due to a magnetic monopole could be reproduced by the path followed by a platinum nucleus decaying first to osmium, and then to tantalum.\n\nHigh energy particle colliders have been used to try to create magnetic monopoles. Due to the conservation of magnetic charge, magnetic monopoles must be created in pairs, one north and one south. Due to conservation of energy, only magnetic monopoles with masses less than the center of mass energy of the colliding particles can be produced. Beyond this, very little is known theoretically about the creation of magnetic monopoles in high energy particle collisions. This is due to their large magnetic charge, which invalidates all the usual calculational techniques. As a consequence, collider based searches for magnetic monopoles cannot, as yet, provide lower bounds on the mass of magnetic monopoles. They can however provide upper bounds on the probability (or cross section) of pair production, as a function of energy.\n\nThe MoEDAL experiment, installed at the Large Hadron Collider, is currently searching for magnetic monopoles and large supersymmetric particles using nuclear track detectors and aluminum bars around LHCb's VELO detector. The particles it is looking for damage the plastic sheets that comprise the nuclear track detectors along their path, with various identifying features. Further, the aluminum bars can trap sufficiently slowly moving magnetic monopoles. The bars can then be analyzed by passing them through a SQUID.\n\nThe Russian astrophysicist Igor Novikov claims the fields of macroscopic black holes are potential magnetic monopoles, representing the entrance to an Einstein–Rosen bridge.\n\nSince around 2003, various condensed-matter physics groups have used the term \"magnetic monopole\" to describe a different and largely unrelated phenomenon.\n\nA true magnetic monopole would be a new elementary particle, and would violate Gauss's law for magnetism . A monopole of this kind, which would help to explain the law of charge quantization as formulated by Paul Dirac in 1931, has never been observed in experiments.\n\nThe monopoles studied by condensed-matter groups have none of these properties. They are not a new elementary particle, but rather are an emergent phenomenon in systems of everyday particles (protons, neutrons, electrons, photons); in other words, they are quasi-particles. They are not sources for the -field (i.e., they do not violate ); instead, they are sources for other fields, for example the -field, the \"-field\" (related to superfluid vorticity), or various other quantum fields. They are not directly relevant to grand unified theories or other aspects of particle physics, and do not help explain charge quantization—except insofar as studies of analogous situations can help confirm that the mathematical analyses involved are sound.\n\nThere are a number of examples in condensed-matter physics where collective behavior leads to emergent phenomena that resemble magnetic monopoles in certain respects, including most prominently the spin ice materials. While these should not be confused with hypothetical elementary monopoles existing in the vacuum, they nonetheless have similar properties and can be probed using similar techniques.\n\nSome researchers use the term magnetricity to describe the manipulation of magnetic monopole quasiparticles in spin ice, in analogy to the word \"electricity\".\n\nOne example of the work on magnetic monopole quasiparticles is a paper published in the journal \"Science\" in September 2009, in which researchers described the observation of quasiparticles resembling magnetic monopoles. A single crystal of the spin ice material dysprosium titanate was cooled to a temperature between 0.6 kelvin and 2.0 kelvin. Using observations of neutron scattering, the magnetic moments were shown to align into interwoven tubelike bundles resembling Dirac strings. At the defect formed by the end of each tube, the magnetic field looks like that of a monopole. Using an applied magnetic field to break the symmetry of the system, the researchers were able to control the density and orientation of these strings. A contribution to the heat capacity of the system from an effective gas of these quasiparticles was also described.\nThis research went on to win the 2012 Europhysics Prize for condensed matter physics.\n\nIn another example, a paper in the February 11, 2011 issue of \"Nature Physics\" describes creation and measurement of long-lived magnetic monopole quasiparticle currents in spin ice. By applying a magnetic-field pulse to crystal of dysprosium titanate at 0.36 K, the authors created a relaxing magnetic current that lasted for several minutes. They measured the current by means of the electromotive force it induced in a solenoid coupled to a sensitive amplifier, and quantitatively described it using a chemical kinetic model of point-like charges obeying the Onsager–Wien mechanism of carrier dissociation and recombination. They thus derived the microscopic parameters of monopole motion in spin ice and identified the distinct roles of free and bound magnetic charges.\n\nIn superfluids, there is a field , related to superfluid vorticity, which is mathematically analogous to the magnetic -field. Because of the similarity, the field is called a \"synthetic magnetic field\". In January 2014, it was reported that monopole quasiparticles for the field were created and studied in a spinor Bose–Einstein condensate. This constitutes the first example of a quasi-magnetic monopole observed within a system governed by quantum field theory.\n\nIn physics the phrase \"magnetic monopole\" usually denoted a Yang–Mills potential and Higgs field whose equations of motion are determined by the Yang–Mills action\n\nIn mathematics, the phrase customarily refers to a static solution to these equations in the Bogomolny–Parasad–Sommerfeld limit , which realizes, within topological class, the absolutes minimum of the functional\n\nThis means that it in a connection on a principal -bundle over (cf. also Connections on a manifold; principal -object) and a section of the associated adjoint bundle of Lie algebras such that the curvature and covariant derivative satisfy the Bogomolny equations\n\nand the boundary conditions.\n\nPure mathematical advances in the theory of monopoles from the 1980s onwards have often proceeded on the basis of physically motived questions.\n\nThe equations themselves are invariant under gauge transformation and orientation-preserving symmetries. When is large, defines a mapping from a 2-sphere of radius in to an adjoint orbit and the homotopy class of this mapping is called the magnetic charge. Most work has been done in the case SU(2), where the charge is a positive integer . The absolute minimum value of the functional is then and the coefficient in the asymptotic expansion of is .\n\nThe first SU(2) solution was found by E. B. Bogomolny, J. K. Parasad and C. M. Sommerfield in 1975. It is spherically symmetric of charge 1 and has the form\n\nIn 1980, C.H.Taubes showed by a gluing construction that there exist solutions for all large and soon after explicit axially symmetric solutions were found. The first exact solution in the general case was given in 1981 by R.S.Ward for in terms of elliptic functions.\n\nThere are two ways of solving the Bogomolny equations. The first is by twistor methods. In the formulation of N.J. Hitchin, an arbitrary solution corresponds to a holomorphic vector bundle over the complex surface , the tangent bundle of the projective line. This is naturally isomorphic to the space of oriented straight lines in .\n\nThe boundary condition show that the holomorphic bundle is an extension of line bundles determined by a compact algebraic curve of genus (the spectral curve) in , satisfying certain constraints.\n\nThe second method, due to W.Nahm, involves solving an eigen value problem for the coupled Dirac operator and transforming the equations with their boundary conditions into a system of ordinary differential equations, the Nahm equations.\n\nwhere is a -matrix valued function on .\n\nBoth constructions are based on analogous procedures for instantons, the key observation due to N. S. Manton being of the self-dual Yang–Mills equations (cf. also Yang–Mills field) in .\n\nThe equivalence of the two methods for SU(2) and their general applicability was established in (see also:). Explicit formulas for and are difficult to obtain by either method, despite some exact solutions of Nahm's equations in symmetric situations.\n\nMaximally imbedded spherically symmetric magnetic monopole solutions in the Bogolomony–Parasad–Sommerfield limit for the gauge group SU(\"n\") were exhibited by Bais. Gannoulis, Goddard and Olive, and Farwell and Minami showed that maximally imbedded spherically symmetric magnetic monopole solutions in the Bogolomony–Parasad–Sommerfield limit for an arbitrary simple gauge group corresponding to a Lie algebra with Cartan matrix and level vector , are solutions to the Toda molecule\nequation:\n\nNon-singular solutions have a magnetic field vanishes at the origin. Explicit finite energy solutions for the Lie algebras , and have been obtained using this method.\n\nThe case of a more general Lie group , where the stabilizer of at infinity is a maximal torus, was treated by M. K. Murray from the twistor point of view, where the single spectral curve of an SU(2)-monopole is replaced by a collection of curves indexed by the vertices of the Dynkin diagram of . The corresponding Nahm construction was designed by J. Hustubise and Murray.\n\nThe moduli space (cf. also Moduli theory) of all SU(2) monopoles of charge up to gauge equivalence was shown by Taubes to be a smooth non-compact manifold of dimension . Restricting to gauge transformations that preserve the connection at infinity gives a -dimensional manifold , which is a circle bundle over the true moduli space and carries a natural complete hyper-Kähler metric (cf. also Kähler–Einstein manifold). With suspected to any of the complex structures of the hyper-Kähler family, this manifold is holomorphically equivalent to the space of based rational mapping of degree from to itself.\n\nThe metric is known in twistor terms, and its Kähler potential can be written using the Riemann theta functions of the spectral curve, but only the case is known in a more conventional and usable form (as of 2000). This Atiyah–Hitchin manifold, the Einstein Taub-NUT metric and are the only 4-dimensional complete hyper-Kähler manifolds with a non-triholomorphic SU(2) action. Its geodesics have been studied and a programme of Manton concerning monopole dynamics put into effect. Further dynamical features have been elucidated by numerical and analytical techniques.\n\nA cyclic -fold conering of splits isometrically is a product , where is the space of strongly centred monopoles. This space features in an application of S-duality in theoretical physics, and in G. B. Segal and A. Selby studied its topology and the harmonic forms defined on it, partially confirming the physical prediction.\n\nMagnetic monopole on hyperbolic three-space were investigated from the twistor point of view by M. F. Atiyah (replacing the complex surface by the complement of the anti-diagonal in ) and in terms of discrete Nahm equations by Murray and M. A. Singer.\n\n\n"}
{"id": "3262898", "url": "https://en.wikipedia.org/wiki?curid=3262898", "title": "Metallome", "text": "Metallome\n\nIn biochemistry, the metallome distribution of free metal ions in every one of cellular compartments. The term was defined in analogy with proteome as metallomics is the study of metallome: the \"comprehensive analysis of the entirety of metal and metalloid species within a cell or tissue type\". Therefore, metallomics can be considered a branch of metabolomics, even though the metals are not typically considered as metabolites.\n\nAn alternative definition of \"metallomes\" as metalloproteins or any other metal-containing biomolecules, and \"metallomics\" as a study of such biomolecules. \n\nIn the study of metallomes the transcriptome, proteome and the metabolome constitutes the whole metallome. A study of the metallome is done to arrive at the metallointeractome.\n\nThe metallotranscriptome can be defined as the map of the entire transcriptome in the presence of biologically or environmentally relevant concentrations of an essential or toxic metal, respectively. The metallometabolome constitutes the complete pool of small metabolites in a cell at any given time. This gives rise to the whole metallointeractome and knowledge of this is important in comparative metallomics dealing with toxicity and drug discovery.\n\n\n"}
{"id": "24794587", "url": "https://en.wikipedia.org/wiki?curid=24794587", "title": "Nicolae Filip", "text": "Nicolae Filip\n\nNicolae Filip (March 3, 1926 – 15 May 2009) was born in the village of Sofia, Bălți to a family of farmers. He was an expert in the field of physics, and contributed significantly to the study of \"The spread of ultrashort radio waves.\" He was elected an honorary member of the Academy of Sciences of Moldova.\n\nHe was the Chancellor of Bălți's State University \"Alecu Russo\".\n\n\n\nBălți Municipal Council Decision Nr. 11/24 on 03.04.2001, the recorded in the Book of Honor by assigning him the title \"Honorary Citizen mun. Bălți.\"\n"}
{"id": "8559787", "url": "https://en.wikipedia.org/wiki?curid=8559787", "title": "Nucleogenic", "text": "Nucleogenic\n\nA nucleogenic isotope, or nuclide, is one that is produced by a natural terrestrial nuclear reaction, other than a reaction beginning with cosmic rays (the latter nuclides by convention are called by the different term cosmogenic). The nuclear reaction that produces nucleogenic nuclides is usually interaction with an alpha particle or the capture of fission or thermal neutron. Some nucleogenic isotopes are stable and others are radioactive.\n\nAn example of a nucleogenic nuclide would be neon-21 produced from neon-20 that absorbs a thermal neutron, to become stable neon-21 (some neon-21 is also primordial). Other nucleogenic reactions that produce heavy neon isotopes are (fast neutron capture, alpha emission) reactions, starting with magnesium-24 and magnesium-25, respectively. The source of the neutrons in these reactions is often secondary neutrons produced by alpha radiation from natural uranium and thorium in rock.\n\nBecause nucleogenic isotopes have been produced later than the birth of the solar system (and the nucleosynthetic events that preceded it), nucleogenic isotopes, by definition, are not primordial nuclides. However, nucleogenic isotopes should not be confused with much more common radiogenic nuclides that are also younger than primordial nuclides, but which arise as simple daughter isotopes from radioactive decay. Nucleogenic isotopes, as noted, are the result of a more complicated nuclear reaction, although such reactions may begin with a radioactive decay event.\n\nAlpha particles that produce nucleogenic reactions come from natural alpha particle emitters in uranium and thorium decay chains. Neutrons to produce nucleogenic nuclides may be produced by a number of processes, but due to the short half-life of free neutrons, all of these reactions occur on Earth. Among the most common are cosmic ray spallation production of neutrons from elements near the surface of the Earth. Alpha emission produced by some radioactive decay also produces neutrons by spallation knockout of neutron rich isotopes, such as the reaction of alpha particles with oxygen-18. Neutrons are also produced by the neutron emission type of radioactive decay, and also from spontaneous fission of fissile elements on Earth, particularly uranium-235.\n\nNucleogenesis (also known as nucleosynthesis) as a general phenomenon is a process usually associated with production of nuclides in the Big Bang or in stars, by nuclear reactions there. Some of these neutron reactions (such as the r-process and s-process) involve absorption by atomic nuclei of high-temperature (high energy) neutrons from the star. These processes produce most of the chemical elements in the universe heavier than zirconium (element 40), because nuclear fusion processes become increasingly inefficient and unlikely, for elements heavier than this. By convention, such heavier elements produced in normal elemental abundance, are \"not\" referred to as \"nucleogenic.\" Instead, this term is reserved for nuclides (isotopes) made on Earth from natural nuclear reactions.\n\nAlso, the term \"nucleogenic\" by convention excludes artificially produced radionuclides, for example tritium, many of which are produced in large amounts by a similar artificial processes, but using the copious neutron flux produced by conventional nuclear reactors.\n\n"}
{"id": "18654256", "url": "https://en.wikipedia.org/wiki?curid=18654256", "title": "Out of the Cradle (book)", "text": "Out of the Cradle (book)\n\nOut of the Cradle: Exploring the Frontiers beyond Earth is a book written and illustrated by planetary scientist William K. Hartmann, Ron Miller and Pamela Lee. Cradle describes potential manned space missions to the planets, moons and asteroids of the Solar System. The approximately 100 space art illustrations were in large part based on the plethora of new photographs from the unmanned space probes Pioneer 11, Voyager 1 and the Viking Lander, available at the time of publication, with scientific extrapolation of the likely appearance of various planetary surfaces. The title is derived from a quote from Konstantin Tsiolkovsky, which is included in the preface: \"Earth is the cradle of humanity, but one cannot live in the cradle forever.\"\n\n\nCradle has received critical academic reviews, including from the \"Journal of the Royal Astronomical Society of Canada\". It has been recommended as a teaching aid for science classes.\n\n\n"}
{"id": "28072082", "url": "https://en.wikipedia.org/wiki?curid=28072082", "title": "Pakistani missile research and development program", "text": "Pakistani missile research and development program\n\nThe missile research and development program was the Pakistan Ministry of Defence secretive program for the comprehensive research and the development of guided missiles. Initiatives began in 1987 in a direct response to equivalent program existed in India and was managed under the scrutiny of the Ministry of Defence in close coordination with the other related institutions.\n\nOn moral grounds, no chemical weapons were developed as the program focused towards developing the short to medium range missiles with a proper computer guidance system. The project started in 1987 and has since spawned several strategic missile systems capable of carrying both conventional and nonconventional payloads. In its early stage, the Hatf missiles were made feasible as well as developing the Ghauri missile program. Further development led to the introduction of ballistics and cruise missiles by different scientific organizations in the 2000s.\n\nPlanning and initiatives for the program began in 1987 based on an intelligence estimates on the existence of the missile program of India, which was taking place under the Indian DRDO. Memoirs written by former chief of army staff General Mirza Beg, the eventual planning of the program began in 1987, with many of the organizations associated with the Ministry of Defence. The program was delegated to Space Research Commission, DESTO, KRL, and PAEC, all individually working on the program under the MoD and the MoDP.\n\nPresident Zia-ul-Haq had held several national security meetings with the MoD and MoST officials to give crucial authorization for the launch of the program in 1987. The major motivation for this program, according to a military official, was to counter the India's ingeniously developed Prithvi system, first successfully tested in 1988. Only Hatf project was made it operational in 1987–88. Restriction and strict technology transfer monitored by the MTCL by the numbers of Western countries and the United States slowed the efforts for the program. In a direct technological race with India, the program was focused more towards ingenuity.\n\nThe program was aggressively pursued by Prime Minister Benazir Bhutto's government faced with the missile gap with India with apparent testing of Prithvi-I missile in 1990 and strongly advocated and lobbied for the program's feasibility in the 1990s. From 1993 to 1995, the program focused on developing the comprehensive short- to medium-range missiles systems to deter missile threat from India. The program picked up speed under the control of Prime Minister Benazir Bhutto, and its existence was kept under extreme secrecy. Crucial decisions were taken by Benazir government and technologies were developed under administration ultimately resulted in the successful development of both short– and medium-range systems.\n\nPrime Minister Benazir Bhutto is described as a political \"architect of Pakistan's missile technology\" by Emily MacFarquhar of Alicia Patterson Foundation. At the leftist convention held in 2014, former Prime Minister Yousaf Raza Gillani said, \"Benazir Bhutto gave this country the much-needed missile technology\".\n\nThe program eventually expended and diversified with the successful development of the cruise missiles and other strategic level arsenals in the early 2000s.\n\nSecretive codenames for the projects of concerning national security had been issued since the 1970s, and the military continued to do so, to promote the secrecy around the missile programme. The missile systems in this program were all given codenames by their respective organizations. However, all missiles were issued single codename series: \"Hatf\", for the surface to surface guided ballistic missiles. This codename was selected by the research and development committee at the GHQ of the Pakistan Army. In Arabic, \"hatf\" meaning \"death\" refers to the sword of the Prophet Muhammad which was used in many of his military conquests, and had the unique distinction of never missing its target.\n\nThe other unofficial names, such as Ghauri and Abdali, are given the names of historical figures in the Islamic conquest of South Asia. Pakistan's missile systems are named after the powerful Pashtun warlords who invaded India from the historical region of Greater Khorasan;present day Afghanistan and western Pakistan between the 11th and 18th centuries in an attempt to expand their empires.\n\nThe Hatf system (English tr.: \"Vengeance\") was the first project that was developed under this program in 1980s and the project went to the Pakistan Army. Designed by the SRC and developed by the KRL, the program was seen as to India's \"Prithvi\", with three variants developed for the use by the Pakistan Army. Classified under the BRBM class, the missile has been in services with Pakistan Army since 1992.\n\nThe Hatf–I is an unguided ballistic missile mounted on a TEL vehicle with a range of ; it has capability of both carrying the conventional and nuclear payloads of . The programme initially led by the SRC after developing the Hatf–IA, an improved version with same payload. Its final evolution led to the development of the Hatf–IB which includes a proper computer inertial guidance system with an extended range. The program evolved into final introduction of the Hatf–IV designation with a maximum range of with a payload of 1,000kg (2,200lb), equipped with a computer inertial navigation system. In 2011, the NDC developed the latest battle-field range system that seen as a compatible to Hatf–IV and widely believed to be a delivery system for small tactical nuclear weapons, which is codenamed as: Hatf–IX \"Nasr\".\n\nWith the development of BRBM type missiles, the program extended towards the creation of the both short–to–intermediate range system. Originally, Prime Minister Benazir Bhutto was initially interested in seeking the procurement of Chinese M–11 missiles but cancelled the talks due to international pressure in the 1990s. After convincing arguments, the project went to SRC in 1995 and the development soon began. Codename Ghaznavi, it is the first solid–fuel based short range system with a range of 600km with a payload of 500kg. The Ghaznavi system was tested in 1997 and is stated to have been a major break-through. The Ghaznavi is a two-stage solid fuel based missile and an advanced terminal guidance system with an onboard computer. The DESTO designed five different types of warheads for the Ghaznavi can be delivered with a CEP of 0.1% at 600 km. It is believed that the design of the Ghaznavi is influenced from the Chinese M-11 missile, but the military officials have claimed that the Ghaznavi was developed entirely in Pakistan.\n\nUnder the same category of SRBM, the second project which was codename Shaheen, was widely pursued and developed by the National Defence Complex (NDC)– a spinoff of PAEC. The Shaheen is a series of solid-fuelled missiles and expended well to SRC, NESCOM, DESTO, and Margalla Electronics. Despite many technological set backs and learning from India's developmental experience of the Agni-II, the project continues to evolve and produced the Shaheen-I which entered in the service in 1999. The Shaheen project produces three variants that are considered to be in the MRBM range. The Shaheen-II has a range of 2,500 km with a capacity of carrying payload of 1050kg. Its third variant, the Shaheen-III, rumored to be underdeveloped and is an IRBM range. No official confirmation of the project is known but only the media reports. In January 2017, the Ababeel, a development of the Shaheen-III with multiple independently targetable reentry vehicles (MIRV), was tested. This is a Shaheen-III airframe with an enlarged payload fairing and slightly shorter range of 2,200 km. The intention of the MIRV system is to counteract Indian BMD.\n\nWhile Shaheen was developed in 1995, another parallel project was being run under the KRL. Codename Ghauri, the project was aimed towards developing the liquid-fuel ballistic missile. The Ghauri was based on entirely on North Korea's Rodong-1 as its technology heavily reflected the Rodong-1. The project was supported by Benazir Bhutto who consulted for the project with North Korea and facilitated the technology transfer to KRL in 1993. According to the military officials, the original design was flawed and the missile burned up on re-entry during its first test flight. The KRL was forced to perform the heavy reverse engineering and had to redesign the entire missile. With scientific assistance from the DESTO, NDC, and the NESCOM, the first missile Ghauri–I was developed. It was successfully tested on 8 April 1998 and entered in the service. As Shaheen, the Ghauri evolves and produced Ghauri-II, also in the 1990s. The Ghauri-II has a maximum range of 2,000km (1,200mi) with a payload of 1,200kg.\n\nUnder the Ghauri, its third variant which was codenamed as Ghauri III was underdeveloped by KRL. The Ghauri III was cancelled in 2000 despite the project being completed its 50% of work.\n\nIn 2005 the Hatf VII \"Babur\" ground-launched cruise missile was revealed in a public test-firing. Early versions had a range of 500 km but later a 700 km variant was tested. In September 2012 a new launch vehicle was tested, as well as a new command and control system named the Strategic Command and Control Support System. It was stated that the SCCSS would give \"decision-makers at the National Command Centre robust command and control capability of all strategic assets with round the clock situational awareness in a digitised network-centric environment.\" It was also stated that the Babur's guidance system uses terrain contour matching (TERCOM) and digital scene matching and area co-relation (DSMAC) techniques to achieve accuracy described as \"pin-point\". The new launch vehicle, a MAZ transporter erector launcher, is armed with three missile rounds launched vertically.\n\nIn 2007 the Hatf VIII \"Ra'ad\", an air-launched cruise missile (ALCM) was revealed in a test by the Pakistan Air Force. It has a stated range of 350 km. A flight test on 31 May 2012 was stated to have validated integration with the new Strategic Command and Control Support System (SCCSS), stated to be capable of remotely monitoring the missile's flight path in real time. It can avoid radar detection due to its low altitude trajectory.\n\n"}
{"id": "54240011", "url": "https://en.wikipedia.org/wiki?curid=54240011", "title": "Patricia D'Amore", "text": "Patricia D'Amore\n\nPatricia Ann D’Amore is a professor at Harvard Medical School, where she is the Charles L. Schepens Professor of Ophthalmology, and Professor of Pathology.\n\nHer research focuses on the pathogenesis of eye disease, in particular in angiogenesis, and in the contribution of lipids and inflammation to the development of age related macular degeneration. She is one of a group of scientists that discovered the importance of Vascular endothelial growth factor (VEGF) in rapidly developing \"wet\" Macular Degeneration, which led to anti-VEGF therapy, which is widely used in western countries to slow down disease progression.\n\nD’Amore grew up in Everett, Massachusetts. After a 1973 BA at Regis College (Massachusetts), she received a PhD in biology at Boston University in 1977 under the direction David Shepro. She then became a postdoctoral fellow in Physiological Chemistry and Ophthalmology at Johns Hopkins Medical School, and became Assistant Professor of Ophthalmology there in 1980. In 1981 she moved to the Surgical Research Lab at Boston Children’s Hospital and joined the Harvard Medical School faculty as Assistant Professor of Surgery. She was appointed Associate Professor at Harvard in 1989, and full professor in 1998. In 2012 she was appointed Charles L. Schepens Professor of Ophthalmology, and in 2013, Professor of Pathology. She has been the Director of the Howe Laboratory at Massachusetts Eye and Ear since 2014, and its Associate Chief of Basic and Translational Research since 2014.\n\nShe also obtained an MBA from Northeastern University (1987).\n\nD’Amore's research group has authored over 150 original research papers. She is highly cited, with 4 papers cited more than 1000 times .\n\nShe is the founder of the Boston Angiogenesis Meeting, which started its annual meetings in 1998. She is the Editor-in-Chief of the journal \"Microvascular Research\", and has been an associate editor of \"The American Journal of Pathology\". She has also been on the editorial board of \"FASEB Journal\".\n\n1994: The Alcon Research Award and the Cogan Award<br>\n2004: elected to The Academy at Harvard Medical School<br>\n2006: Senior Scientific Investigator Award from Research to Prevent Blindness<br>\n2009: appointed an Association for Research in Vision and Ophthalmology (ARVO) Fellow<br>\n2010: invited lecturer for the 5th Annual Jeffrey M. Isner, M.D. Endowed Memorial Lectureship<br>\n2012: Rous-Whipple Award from the American Society of Investigative Pathology<br>\n2013: Everett Mendelsohn Excellence in Mentoring Award from Harvard University<br>\n2013: invited lecturer for the Hans Vilbertn Lecture, University of Regensburg, Germany<br>\n2013: Women Physicians Sector Mentorship Award from the American Medical Association<br>\n2014: Endre Balazs Prize from the International Society for Eye Research<br>\n2014: António Champalimaud Award<br>\n2015: Proctor Medal from the Association For Research & Vision Ophthalmology\n\n"}
{"id": "59033370", "url": "https://en.wikipedia.org/wiki?curid=59033370", "title": "Phycisphaeraceae", "text": "Phycisphaeraceae\n\nPhycisphaeraceae is a family of bacteria\n"}
{"id": "9783078", "url": "https://en.wikipedia.org/wiki?curid=9783078", "title": "Proteins@home", "text": "Proteins@home\n\nProteins@home (\"Proteins at home\") was one of many distributed computing projects that used the BOINC architecture. The project was run by the Department of Biology at École Polytechnique. The project began on December 28, 2006 and ended in June 2008.\n\nProteins@Home was a large-scale non-profit protein structure prediction project utilizing distributed computing to perform a lot of computations in a small amount of time. From their website:\n\n\"The amino acid sequence of a protein determines its three-dimensional structure, or 'fold'. Conversely, the three-dimensional structure is compatible with a large, but limited set of amino acid sequences. Enumerating the allowed sequences for a given fold is known as the 'inverse protein folding problem'. We are working to solve this problem for a large number of known protein folds (a representative subset: about 1500 folds). The most expensive step is to build a database of energy functions that describe all these structures. For each structure, we consider all possible sequences of amino acids. Surprisingly, this is computationally tractable, because our energy functions are sums over pairs of interactions. Once this is done, we can explore the space of amino acid sequences in a fast and efficient way, and retain the most favorable sequences. This large-scale mapping of protein sequence space will have applications for predicting protein structure and function, for understanding protein evolution, and for designing new proteins. By joining the project, you will help to build the database of energy functions and advance an important area of science with potential biomedical applications.\"\n\nOn March 25, 2007, Proteins@Home had 6040 of 9548 users actively participating with a total of 10405 of 15381 computers contributing. There were approximately 479 active teams spread across 100 countries.\n\nA number of other distributed computing projects are also contributing to the protein folding idea.\nSome of these projects are:\n\n\n"}
{"id": "21089139", "url": "https://en.wikipedia.org/wiki?curid=21089139", "title": "Pseudodeficiency alleles", "text": "Pseudodeficiency alleles\n\nA pseudodeficiency allele or pseudodeficiency mutation is a mutation that alters the protein product or changes the gene's expression, but without causing disease. For example, in the lysosomal storage diseases, patients with a pseudodeficiency allele show greatly reduced enzyme activity, yet they remain clinically healthy.\n\nIn medical genetics, a false positive result occurs in an enzyme assay test when test results are positive, but disease or morbidity is not present. One possible cause of false positive results is a pseudodeficiency allele. Disease may also be present, but at a subclinical level.\n\n\nA pseudodeficiency allele may indicate a deficiency of the enzyme assay method, or it may reflect incomplete understanding of the enzyme's activity.\n\n\nBecause of pseudodeficiency alleles, the results of enzyme assay testing in one population cannot be generalized to other populations. For example, while Tay-Sachs screening was able to nearly eliminate Tay-Sachs disease among Ashkenazi Jews, similar screening in the general population has proven less effective.\n\nFor some genetic diseases, especially those that have low penetrance or are easily treatable, carrier screening may be of questionable value when pseudodeficiency alleles are present at high frequency.\n"}
{"id": "159731", "url": "https://en.wikipedia.org/wiki?curid=159731", "title": "Quasi-empiricism in mathematics", "text": "Quasi-empiricism in mathematics\n\nQuasi-empiricism in mathematics is the attempt in the philosophy of mathematics to direct philosophers' attention to mathematical practice, in particular, relations with physics, social sciences, and computational mathematics, rather than solely to issues in the foundations of mathematics. Of concern to this discussion are several topics: the relationship of empiricism (see Maddy) with mathematics, issues related to realism, the importance of culture, necessity of application, etc.\n\nA primary argument with respect to quasi-empiricism is that whilst mathematics and physics are frequently considered to be closely linked fields of study, this may reflect human cognitive bias. It is claimed that, despite rigorous application of appropriate empirical methods or mathematical practice in either field, this would nonetheless be insufficient to disprove alternate approaches.\n\nEugene Wigner (1960) noted that this culture need not be restricted to mathematics, physics, or even humans. He stated further that \"The miracle of the appropriateness of the language of mathematics for the formulation of the laws of physics is a wonderful gift which we neither understand nor deserve. We should be grateful for it and hope that it will remain valid in future research and that it will extend, for better or for worse, to our pleasure, even though perhaps also to our bafflement, to wide branches of learning.\" Wigner used several examples to demonstrate why 'bafflement' is an appropriate description, such as showing how mathematics adds to situational knowledge in ways that are either not possible otherwise or are so outside normal thought to be of little notice. The predictive ability, in the sense of describing potential phenomena prior to observation of such, which can be supported by a mathematical system would be another example. \n\nFollowing up on Wigner, Richard Hamming (1980) wrote about applications of mathematics as a central theme to this topic and suggested that successful use can sometimes trump proof, in the following sense: where a theorem has evident veracity through applicability, later evidence that shows the theorem's proof to be problematic would result more in trying to firm up the theorem rather than in trying to redo the applications or to deny results obtained to date. Hamming had four explanations for the 'effectiveness' that we see with mathematics and definitely saw this topic as worthy of discussion and study. \n\n\nFor Willard Van Orman Quine (1960), existence is only existence in a structure. This position is relevant to quasi-empiricism, because Quine believes that the same evidence that supports theorizing about the structure of the world is the same as the evidence supporting theorizing about mathematical structures.\n\nHilary Putnam (1975) stated that mathematics had accepted informal proofs and proof by authority, and had made and corrected errors all through its history. Also, he stated that Euclid's system of proving geometry theorems was unique to the classical Greeks and did not evolve similarly in other mathematical cultures in China, India, and Arabia. This and other evidence led many mathematicians to reject the label of Platonists, along with Plato's ontology which, along with the methods and epistemology of Aristotle, had served as a foundation ontology for the Western world since its beginnings. A truly international culture of mathematics would, Putnam and others (1983) argued, necessarily be at least 'quasi'-empirical (embracing 'the scientific method' for consensus if not experiment).\n\nImre Lakatos (1976), who did his original work on this topic for his dissertation (1961, Cambridge), argued for 'Research Programs' as a means to support a basis for mathematics and considered thought experiments as appropriate to mathematical discovery. Lakatos may have been the first to use 'quasi-empiricism' in the context of this subject.\n\nSeveral recent works pertain to this topic. Chaitin's and Stephen Wolfram's work, though their positions may be considered controversial, apply. Chaitin (1997/2003) suggests an underlying randomness to mathematics and Wolfram (\"A New Kind of Science\", 2002) argues that undecidability may have practical relevance, that is, be more than an abstraction. \n\nAnother relevant addition would be the discussions concerning interactive computation, especially those related to the meaning and use of Turing's model (Church-Turing thesis, Turing machines, etc.). \n\nThese works are heavily computational and raise another set of issues. To quote Chaitin (1997/2003): \"Now everything has gone topsy-turvy. It's gone topsy-turvy, not because of any philosophical argument, not because of Gödel's results or Turing's results or my own incompleteness results. It's gone topsy-turvy for a very simple reason — the computer!\".\n\nThe collection of \"Undecidables\" in Wolfram (\"A New Kind of Science\", 2002) is another example. \n\nWegner's 2006 paper \"Principles of Problem Solving\" suggests that \"interactive computation\" can help mathematics form a more appropriate framework (empirical) than can be founded with rationalism alone. Related to this argument is that the function (even recursively related ad infinitum) is too simple a construct to handle the reality of entities that resolve (via computation or some type of analog) n-dimensional (general sense of the word) systems.\n\n"}
{"id": "43654412", "url": "https://en.wikipedia.org/wiki?curid=43654412", "title": "Rosenbrock system matrix", "text": "Rosenbrock system matrix\n\nIn applied mathematics, the Rosenbrock system matrix or Rosenbrock's system matrix of a linear time-invariant system is a useful representation bridging state-space representation and transfer function matrix form. It was proposed in 1967 by Howard H. Rosenbrock.\n\nConsider the dynamic system\nThe Rosenbrock system matrix is given by \nIn the original work by Rosenbrock, the constant matrix formula_4 is allowed to be a polynomial in formula_5.\n\nThe transfer function between the input formula_6 and output formula_7 is given by \nwhere formula_9 is the column formula_6 of formula_11 and formula_12 is the row formula_7 of formula_14.\n\nBased in this representation, Rosenbrock developed his version of the PHB test.\n\nFor computational purposes, a short form of the Rosenbrock system matrix is more appropriate and given by\nThe short form of the Rosenbrock system matrix has been widely used in H-infinity methods in control theory, where it is also referred to as packed form; see command pck in MATLAB. An interpretation of the Rosenbrock System Matrix as a Linear Fractional Transformation can be found in.\n\nOne of the first applications of the Rosenbrock form was the development of an efficient computational method for Kalman decomposition, which is based on the pivot element method. A variant of Rosenbrock’s method is implemented in the minreal command of Matlab and\nGNU Octave.\n"}
{"id": "25806816", "url": "https://en.wikipedia.org/wiki?curid=25806816", "title": "Self-expression values", "text": "Self-expression values\n\nSelf-expression values are part of a core value dimension in the modernization process. Self-expression is a cluster of values that include social toleration, life satisfaction, public expression and an aspiration to liberty. Ronald Inglehart, the University of Michigan professor who developed the theory of post-materialism, has worked extensively with this concept. On the Inglehart–Welzel Cultural Map \"self-expression values\" are contrasted with \"survival values\", illustrating the changes in values across countries and generations. The idea that the world is moving towards self-expression values was discussed at length in an article in the Economist. Self expression is to say something that you truly believe is important in a form of communication, such as art, speech and dance. It is to reveal yourself in a way that is special to yourself. \n\nThe emergence of the post-industrial society has brought about a wave of cultural change. In the United States, Canada, Western Europe, and a growing share of East Asia, a majority of the people are no longer employed in factories, but work in the service sector instead. There has been a shift from a mechanical environment to one where ever more people spend their days dealing with other people, symbols, and information, which means that workers in the knowledge sector must exercise their own judgment and choice.\n\nThis shift has had major consequences:\n\nThe destandardization of economic activities and social life diminishes social constraints in unprecedented ways. The shift in post-industrial societies is thus one of emancipation from authority.\n\nIndustrialization can lead to fascism, communism, theocracy or democracy. But post-industrial society brings socio-cultural changes that make truly effective democracy increasingly probable.\n\nKnowledge societies cannot function effectively without highly educated workers, who become articulate and accustomed to thinking for themselves. Furthermore, rising levels of economic security bring growing emphasis on self-expression values that give high priority to free choice. Mass publics become increasingly likely to want democracy, and increasingly effective in getting it. Repressing mass demands for liberalization becomes increasingly costly and detrimental to economic effectiveness. These changes link economic development with democracy.\n\nThe most thorough assessment of self-expression values is carried out in the World Values Survey. Five \"waves\" have been conducted so far, each adding additional countries to the survey.\n\nSubsequent data analysis by Inglehart revealed that a large percentage in the variability in the data could be explained by using a set of measures that tapped just two dimensions: a traditional to secular-rational axis, and a survival to self-expression axis. The factor scores were originally based on 22 variables, but this was reduced to only 10 (5 for each dimension) for the purposes of data availability.\n\nThe self-expression axis has the following factor loadings.\nDespite comprising only five variables, the correlates for this dimension across the WV survey are very strong. Below is a partial list. Positive answers indicate survival values, the opposite of self-expression values. \n"}
{"id": "16079015", "url": "https://en.wikipedia.org/wiki?curid=16079015", "title": "Small Astronomy Satellite 3", "text": "Small Astronomy Satellite 3\n\nThe Small Astronomy Satellite 3 (SAS 3, also known as SAS-C before launch) was a NASA X-ray astronomy space telescope. It functioned from May 7, 1975 to April 1979. It covered the X-ray range with four experiments on board. The satellite, built by the Johns Hopkins University Applied Physics Laboratory (APL), was proposed and operated by MIT's Center for Space Research (CSR). It was launched on a Scout vehicle from the Italian San Marco launch platform near Mombasa, Kenya, into a low-Earth, nearly equatorial orbit. It was also known as Explorer 53, as part of NASA's Explorer program.\n\nThe spacecraft was 3-axis stabilized with a momentum wheel that was used to establish stability about the nominal rotation, or z-axis. The orientation of the z-axis could be altered over a period of hours using magnetic torque coils that interacted with the Earth's magnetic field. Solar panels charged batteries during the daylight portion of each orbit, so that SAS 3 had essentially no expendables to limit its lifetime beyond the life of the tape recorders, batteries, and orbital drag. The spacecraft typically operated in a rotating mode, spinning at one revolution per 95-min orbit, so that the LEDs, tube and slat collimator experiments, which looked out along the y-axis, could view and scan the sky almost continuously. The rotation could also be stopped, allowing extended (up to 30 min) pointed observations of selected sources by the y-axis instruments. Data were recorded on board by magnetic tape recorders, and played back during station passes every orbit.\n\nSAS 3 was commanded from the NASA Goddard Space Flight Center (GSFC) in Greenbelt MD, but data were transmitted by modem to MIT for scientific analysis, where scientific and technical staff were on-duty 24 hours a day. The data from each orbit were subjected to quick-look scientific analysis at MIT before the next orbital station pass, so the science operational plan could be altered by telephoned instruction from MIT to GSFC in order to study targets in near real-time.\n\nThe major scientific objectives of the mission were:\n\nSAS 3 carried four experiments:\n\n\nSAS 3 was especially productive due to its flexibility and rapid responsiveness. Among its most important results were:\n\n\nLead investigators on SAS 3 were MIT professors George W. Clark, Hale V. Bradt, and Walter H. G. Lewin. Other major contributors were Profs Claude Canizares and Saul A. Rappaport, and Drs Jeffrey A. Hoffman, George Ricker, Jeff McClintock, Rodger E. Doxsey, Garrett Jernigan, John Doty, and many others, including numerous graduate students.\n\n\n"}
{"id": "12182603", "url": "https://en.wikipedia.org/wiki?curid=12182603", "title": "Social Science Library, Oxford", "text": "Social Science Library, Oxford\n\nThe Bodleian Social Science Library, Oxford (SSL) is the main teaching social sciences lending library at the University of Oxford, England. The library supports taught programmes for both undergraduates and postgraduates, and houses a dedicated research collection part of which contains legal deposit material out-housed from the Bodleian Library. The Social Science Library uses the Library of Congress classification scheme.\n\nSubject coverage includes:\n\nThe library is located in the Manor Road Building on Manor Road, Oxford. The Manor Road Building was designed by Foster & Partners.\n\nThe Social Science Library officially opened 1 October 2004, following a period in which subject collections housed within smaller units were amalgamated into a central library site.\nThe institution was preceded by the Social Studies Libraries; a grouping of three separate libraries:\nThese three libraries were the first to be combined, at the site of the existing Economics Library, between 2002 and 2003. Further integration followed with the Criminology Library and Socio-Legal Studies Library moving to the site in 2004. The collections housed at the International Development Centre (IDS) at Queen Elizabeth House joined the Social Science Library after its launch, in the summer of 2005. Material from the Slavonic and East European collections, previously kept within the Bodleian Library Slavonic reading room, also moved into the library later that year.\n\nIn March 2010 the Social Science Library was renamed the Bodleian Social Science Library. This was as result of the Oxford University Library Services - of which the Social Science Library was a part - changing its name to the Bodleian Libraries.\n\n"}
{"id": "9969372", "url": "https://en.wikipedia.org/wiki?curid=9969372", "title": "Solid fat index", "text": "Solid fat index\n\nSolid fat index (SFI) is a measure of the percentage of fat in crystalline (solid) phase to total fat (the remainder being in liquid phase) across a temperature gradient. The SFI of a fat is measured using a dilatometer that measures the expansion of a fat as it is heated; density measurements are taken at a series of standardized temperature check points. The resulting SFI/temperature curve is related to melting qualities and flavor. For example, butter has a sharp SFI curve, indicating that it melts quickly and that it releases flavor quickly. \n"}
{"id": "178182", "url": "https://en.wikipedia.org/wiki?curid=178182", "title": "Soyuz (spacecraft)", "text": "Soyuz (spacecraft)\n\nSoyuz () is a series of spacecraft designed for the Soviet space program by the Korolev Design Bureau (now RKK Energia) in the 1960s that remains in service today. The Soyuz succeeded the Voskhod spacecraft and was originally built as part of the Soviet manned lunar programs. The Soyuz spacecraft is launched on a Soyuz rocket, the most reliable launch vehicle in the world to date. The Soyuz rocket design is based on the Vostok launcher, which in turn was based on the 8K74 or R-7A Semyorka, a Soviet intercontinental ballistic missile. All Soyuz spacecraft are launched from the Baikonur Cosmodrome in Kazakhstan. \n\nThe first Soyuz flight was unmanned and started on November 28, 1966. The first Soyuz mission with a crew, Soyuz 1, launched on 23 April 1967 but ended with a crash due to a parachute failure, killing cosmonaut Vladimir Komarov. The following flight was unmanned. Soyuz 3, launched on October 26, 1968, became the program's first successful manned mission. The only other flight to suffer a fatal accident, Soyuz 11, killed its crew of three when the cabin depressurized prematurely just before reentry. These were the only humans to date to have died above the Kármán line. Despite these early incidents, Soyuz is widely considered the world's safest, most cost-effective human spaceflight vehicle, established by its unparalleled length of operational history. Soyuz spacecraft were used to carry cosmonauts to and from Salyut and later Mir Soviet space stations, and are now used for transport to and from the International Space Station (ISS). At least one Soyuz spacecraft is docked to ISS at all times for use as an escape craft in the event of an emergency. The spacecraft is intended to be replaced by the six-person Federation spacecraft.\n\nA Soyuz spacecraft consists of three parts (from front to back):\n\nThe orbital and service modules are single-use and are destroyed upon re-entry in the atmosphere. Though this might seem wasteful, it reduces the amount of heat shielding required for re-entry, saving mass compared to designs containing all of the living space and life support in a single capsule. This allows smaller rockets to launch the spacecraft or can be used to increase the habitable space available to the crew (6.2 m in Apollo CM vs 7.5 m in Soyuz) in the mass budget. The orbital and reentry portions are habitable living space, with the service module containing the fuel, main engines and instrumentation.\n\nSoyuz can carry up to three crew members and provide life support for about 30 person days. The life support system provides a nitrogen/oxygen atmosphere at sea level partial pressures. The atmosphere is regenerated through potassium superoxide (KO) cylinders, which absorb most of the carbon dioxide (CO) and water produced by the crew and regenerates the oxygen, and lithium hydroxide (LiOH) cylinders which absorb leftover CO.\n\nThe vehicle is protected during launch by a payload fairing, which is jettisoned along with the SAS at minutes into launch. It has an automatic docking system. The ship can be operated automatically, or by a pilot independently of ground control.\n\nThe Vostok spacecraft utilized an ejector seat to bail out the cosmonaut in the event of a low-altitude launch failure, as well as during reentry, however it would probably have been ineffective in the first 20 seconds after liftoff when the altitude would be too low for the parachute to deploy. Inspired by the Mercury LES, Soviet designers began work on a similar system in 1962. This included developing a complex sensing system to monitor various launch vehicle parameters and trigger an abort if a booster malfunction occurred. Based on data from R-7 launches over the years, engineers developed a list of the most likely failure modes for the vehicle and could narrow down abort conditions to premature separation of a strap-on booster, low engine thrust, loss of combustion chamber pressure, or loss of booster guidance. The Spacecraft Abort System (SAS; ) could also be manually activated from the ground, but unlike American spacecraft, there was no way for the cosmonauts to trigger it themselves.\n\nSince it turned out to be almost impossible to separate the entire payload shroud from the Soyuz service module cleanly, the decision was made to have the shroud split between the service module and descent module during an abort. Four folding stabilizers were added to improve aerodynamic stability during ascent. Two test runs of the SAS were carried out in 1966-67.\n\nThe basic design of the SAS has remained almost unchanged in 50 years of use and all Soyuz launches carry it. The only modification was in 1972 when the aerodynamic fairing over the SAS motor nozzles was removed for weight-saving reasons as the redesigned Soyuz 7K-T spacecraft carried extra life support equipment. The unmanned Progress resupply ferry has a dummy escape tower and removes the stabilizer fins from the payload shroud. There have been three failed launches of a manned Soyuz vehicle, Soyuz 18-1 in 1975, Soyuz T-10-1 in 1983 and Soyuz MS-10 in October 2018. The 1975 failure was aborted after escape tower jettison. In 1983, Soyuz T-10-1's SAS successfully rescued the cosmonauts from an on-pad fire and explosion of the launch vehicle. Most recently in 2018, the SAS sub-system in the payload shroud of Soyuz MS-10 successfully rescued the cosmonauts from a rocket failure 2 minutes and 45 second after liftoff after the escape tower had already been jettisoned.\n\nThe forepart of the spacecraft is the Orbital Module (), also known as habitation section. It houses all the equipment that will not be needed for reentry, such as experiments, cameras or cargo. The module also contains a toilet, docking avionics and communications gear. Internal volume is , living space . On the latest Soyuz versions (since Soyuz TM), a small window was introduced, providing the crew with a forward view.\n\nA hatch between it and the Descent Module can be closed so as to isolate it to act as an airlock if needed, crew members exiting through its side port (near the descent module). On the launch pad, the crew enter the spacecraft through this port.\n\nThis separation also lets the Orbital Module be customized to the mission with less risk to the life-critical descent module. The convention of orientation in a micro-g environment differs from that of the Descent Module, as crew members stand or sit with their heads to the docking port. Also the rescue of the crew whilst on the launch pad or with the SAS system is complicated because of the orbital module.\n\nSeparation of the Orbital Module is critical for a safe landing; without separation of the Orbital Module, it is not possible for the crew to survive landing in the Descent Module. This is because the Orbital Module would interfere with proper deployment of the Descent Module's parachutes, and the extra mass exceeds the capability of the main parachute and braking engines to provide a safe soft landing speed. In view of this, the Orbital Module was separated before the ignition of the return engine until the late 1980s. This guaranteed that the Descent Module and Orbital Module would be separated before the Descent Module was placed in a reentry trajectory. However, after the problematic landing of Soyuz TM-5 in September 1988 this procedure was changed and the Orbital Module is now separated after the return maneuver. This change was made as the TM-5 crew could not deorbit for 24 hours after they jettisoned their Orbital Module, which contained their sanitation facilities and the docking collar needed to attach to MIR. The risk of not being able to separate the Orbital Module is effectively judged to be less than the risk of needing the facilities in it, following a failed deorbit.\n\nThe Descent Module (), also known as a reentry capsule, is used for launch and the journey back to Earth. Half of the Descent Module is covered by a heat-resistant covering to protect it during reentry; this half faces the Earth during re-entry. It is slowed initially by the atmosphere, then by a braking parachute, followed by the main parachute which slows the craft for landing. At one meter above the ground, solid-fuel braking engines mounted behind the heat shield are fired to give a soft landing. One of the design requirements for the Descent Module was for it to have the highest possible volumetric efficiency (internal volume divided by hull area). The best shape for this is a sphere — as the pioneering Vostok spacecraft's Descent Module used — but such a shape can provide no lift, which results in a purely ballistic reentry. Ballistic reentries are hard on the occupants due to high deceleration and cannot be steered beyond their initial deorbit burn. That is why it was decided to go with the \"headlight\" shape that the Soyuz uses—a hemispherical forward area joined by a barely angled (seven degrees) conical section to a classic spherical section heat shield. This shape allows a small amount of lift to be generated due to the unequal weight distribution. The nickname was thought up at a time when nearly every headlight was circular. The small dimensions of the Descent Module led to it having only two-man crews after the death of the Soyuz 11 crew. The later Soyuz T spacecraft solved this issue. Internal volume of Soyuz SA is ; is usable for crew (living space).\n\nAt the back of the vehicle is the Service Module (). It has a pressurized container shaped like a bulging can (instrumentation compartment, ) that contains systems for temperature control, electric power supply, long-range radio communications, radio telemetry, and instruments for orientation and control. A non-pressurized part of the Service Module (propulsion compartment, ) contains the main engine and a liquid-fuelled propulsion system for maneuvering in orbit and initiating the descent back to Earth. The ship also has a system of low-thrust engines for orientation, attached to the intermediate compartment (). Outside the Service Module are the sensors for the orientation system and the solar array, which is oriented towards the Sun by rotating the ship. An incomplete separation between the Service and Reentry Modules led to emergency situations during Soyuz 5, Soyuz TMA-10 and Soyuz TMA-11, which led to an incorrect reentry orientation (crew ingress hatch first). The failure of several explosive bolts did not cut the connection between the Service/Reentry Modules on the latter two flights.\n\nThe Soyuz uses a method similar to the Apollo to deorbit itself. The spacecraft is turned engine-forward and the main engine is fired for deorbiting on the far side of Earth ahead of its planned landing site. This requires the least propellant for re-entry; the spacecraft travels on an elliptical Hohmann transfer orbit to the entry interface point where atmospheric drag slows it enough to fall out of orbit.\n\nEarly Soyuz spacecraft would then have the Service and Orbital Modules detach simultaneously from the Descent Module. As they are connected by tubing and electrical cables to the Descent Module, this would aid in their separation and avoid having the Descent Module alter its orientation. Later Soyuz spacecraft detached the Orbital Module before firing the main engine, which saved propellant. Since the Soyuz TM-5 landing issue, the Orbital Module is once again detached only after the re-enter firing, which led to (but did not cause) emergency situations of Soyuz TMA-10 and TMA-11. The Orbital Module cannot remain in orbit as an addition to a space station, as the airlock hatch between the Orbital and Reentry Modules is a part of the Reentry Module, and the Orbital Module therefore depressurizes after separation.\n\nRe-entry firing is usually done on the \"dawn\" side of the Earth, so that the spacecraft can be seen by recovery helicopters as it descends in the evening twilight, illuminated by the Sun when it is above the shadow of the Earth. The Soyuz craft is designed to come down on land, usually somewhere in the deserts of Kazakhstan in central Asia. This is in contrast to early US manned missions which \"splashed down\" in the ocean.\n\n\n\n\n\nThe Soyuz spacecraft has been the subject of continuous evolution since the early 1960s. Thus several different versions, proposals and projects exist.\n\nSergei Korolev initially promoted the Soyuz A-B-V circumlunar complex (\"7K-9K-11K\") concept (also known as L1) in which a two-man craft Soyuz 7K would rendezvous with other components (9K and 11K) in Earth orbit to assemble a lunar excursion vehicle, the components being delivered by the proven R-7 rocket.\n\nThe manned Soyuz spacecraft can be classified into design generations. Soyuz 1 through Soyuz 11 (1967–1971) were first-generation vehicles, carrying a crew of up to three without spacesuits and distinguished from those following by their bent solar panels and their use of the Igla automatic docking navigation system, which required special radar antennas. This first generation encompassed the original Soyuz 7K-OK and the Soyuz 7K-OKS for docking with the Salyut 1 space station. The probe and drogue docking system permitted internal transfer of cosmonauts from the Soyuz to the station.\n\nThe Soyuz 7K-L1 was designed to launch a crew from the Earth to circle the moon, and was the primary hope for a Soviet circumlunar flight. It had several test flights in the Zond program from 1967–1970 (Zond 4 to Zond 8), which produced multiple failures in the 7K-L1's re-entry systems. The remaining 7K-L1s were scrapped. The Soyuz 7K-L3 was designed and developed in parallel to the Soyuz 7K-L1, but was also scrapped. Soyuz 1 was plagued with technical issues, and cosmonaut Vladimir Komarov was killed when the spacecraft crashed during its return to Earth. This was the first in-flight fatality in the history of spaceflight.\n\nThe next manned version of the Soyuz was the Soyuz 7K-OKS. It was designed for space station flights and had a docking port that allowed internal transfer between spacecraft. The Soyuz 7K-OKS had two manned flights, both in 1971. Soyuz 11, the second flight, depressurized upon re-entry, killing its three-man crew.\nThe second generation, called \"Soyuz Ferry\" or Soyuz 7K-T, comprised Soyuz 12 through Soyuz 40 (1973–1981).\n\nIt was developed out of the military Soyuz concepts studied in previous years and was capable of carrying 2 cosmonauts with Sokol space suits (after the Soyuz 11 accident). Several models were planned, but none actually flew in space. These versions were named \"Soyuz P\", \"Soyuz PPK\", \"Soyuz R\", \"Soyuz 7K-VI\", and \"Soyuz OIS\" (Orbital Research Station).\n\nThe Soyuz 7K-T/A9 version was used for the flights to the military Almaz space station.\n\nSoyuz 7K-TM was the spacecraft used in the Apollo-Soyuz Test Project in 1975, which saw the first and only docking of a Soyuz spacecraft with an Apollo Command/Service Module. It was also flown in 1976 for the Earth-science mission, Soyuz 22. Soyuz 7K-TM served as a technological bridge to the third generation.\nThe third generation Soyuz-T (T: ) spacecraft (1976–1986) featured solar panels allowing longer missions, a revised Igla rendezvous system and new translation/attitude thruster system on the Service module. It could carry a crew of three, now wearing spacesuits.\n\nThe Soyuz-TM crew transports (M: ) were fourth generation Soyuz spacecraft, and were used from 1986 to 2003 for ferry flights to Mir and the International Space Station.\n\nSoyuz TMA (A: ) features several changes to accommodate requirements requested by NASA in order to service the International Space Station, including more latitude in the height and weight of the crew and improved parachute systems. It is also the first expendable vehicle to feature \"glass cockpit\" technology. Soyuz-TMA looks identical to a Soyuz-TM spacecraft on the outside, but interior differences allow it to accommodate taller occupants with new adjustable crew couches.\n\nThe Soyuz TMA-M was an upgrade of the baseline Soyuz-TMA, using a new computer, digital interior displays, updated docking equipment, and the vehicle's total mass was reduced by 70 kilograms. The new version debuted on 7 October 2010 with the launch of TMA-01M, carrying the ISS Expedition 25 crew.\n\nThe Soyuz TMA-08M mission set a new record for the fastest manned docking with a space station. The mission used a new six-hour rendezvous, faster than the previous Soyuz launches, which had, since 1986, taken two days.\n\nSoyuz MS is the final planned upgrade of the Soyuz spacecraft. Its maiden flight was in July 2016 with mission MS-01.\nMajor changes include:\n\nThe Soyuz MS-02 spacecraft had its maiden flight on October 19, 2016, to launch Expedition 49-50 from Baikonur with three crew members.\n\nThe unmanned Progress spacecraft were derived from Soyuz and are used for servicing space stations.\n\nWhile not being direct derivatives of Soyuz, the Chinese Shenzhou spacecraft uses Soyuz TM technology sold in 1984 and the Indian Orbital Vehicle follow the same general layout as that pioneered by Soyuz.\n\nSee List of Soviet manned space missions and List of Russian manned space missions, as well as the Zond program.\n\n\n"}
{"id": "2874981", "url": "https://en.wikipedia.org/wiki?curid=2874981", "title": "Standard algorithms", "text": "Standard algorithms\n\nIn elementary arithmetic, a standard algorithm or method is a specific method of computation which is conventionally taught for solving particular mathematical problems. These methods vary somewhat by nation and time, but generally include exchanging, regrouping, long division, and long multiplication using a standard notation, and standard formulas for average, area, and volume. Similar methods also exist for procedures such as square root and even more sophisticated functions, but have fallen out of the general mathematics curriculum in favor of calculators (or tables and slide rules before them).\n\nThe concepts of reform mathematics which the NCTM introduced in 1989 favors an alternative approach. It proposes a deeper understanding of the underlying theory instead of memorization of specific methods will allow students to develop individual methods which solve the same problems. Students' alternative algorithms are often just as correct, efficient, and generalizable as the standard algorithms, and maintain emphasis on the meaning of the quantities involved, especially as relates to place values (something that is usually lost in the memorization of standard algorithms). The development of sophisticated calculators has made manual calculation less important (see the note on square roots, above) and cursory teaching of traditional methods has created failure among many students. Greater achievement among all types of students is among the primary goals of mathematics education put forth by NCTM. Some researchers such as Constance Kamii have suggested that elementary arithmetic, as traditionally taught, is not appropriate in elementary school. Many first editions of textbooks written to the original 1989 standard such as TERC deliberately discouraged teaching of any particular method, instead devoting class and homework time to the solving of nontrivial problems, which stimulate students to develop their own methods of calculation, rooted in number sense and place value. This emphasis by no means excludes the learning of number facts; indeed, a major goal of early mathematical education is procedural fluency.\n\nThe NCTM in recent revisions has made more explicit this need for learning of basic math facts and correct, efficient methods. Many new editions of standards-based texts do present standard methods and basic skills. However, the original guidelines continue to draw fire from well-meaning parents and community members, some of whom advocate a return to traditional mathematics. Success of a particular text depends not only upon its content, but also on the willingness of a school community to allow new pedagogy and content and to commit to the recommended implementation of the materials.\n"}
{"id": "8524008", "url": "https://en.wikipedia.org/wiki?curid=8524008", "title": "Star Rigger", "text": "Star Rigger\n\nA Star rigger is a person in author Jeffrey Carver's Star Rigger universe books who pilots a spacecraft through the hyperspace realm known as \"the Flux.\"\n\nShips can be piloted by a solo rigger, or several riggers working as a team. Riggers must be very creative people, since navigating the Flux requires much imagination. In the Star Rigger books, it is suggested that riggers can \"burn out\", spending so much time in the Flux that they begin to live in their own dream world. Legends say that some become vaguely translucent, having seemingly left a part of themselves in the Flux. \n\nRiggers are members of the RiggerGuild, which protects them from exploitation and grants them certain rights. For example, a planet's Spacing Authority must provide all possible assistance and transportation to a ship's rigger crew and passengers from the point of first possible contact in the event of a FluxSpace accident. If the RiggerGuild is not satisfied that its members are being treated appropriately, it can call a general strike, paralyzing a planet's interstellar economy. \n\nThe first known rigger was Panglor Balef, in the Twelfth Century of Space.\n\nRiggers must navigate space by visualizing the intangible features of the Flux as an interactive landscape and adapting their flight patterns to the landscape. Although no two riggers will experience the same space in exactly the same way, a team of riggers must work together to create a shared vision of the Flux in order to successfully navigate their ship. The Flux has treacherous areas that can entrap a ship, manifesting themselves as whirlpools and eddies, or the more dangerous quantum flaws. Tales exist among the riggers of strange visions of dragons and ghost ships within the Flux.\n"}
{"id": "5521842", "url": "https://en.wikipedia.org/wiki?curid=5521842", "title": "Superghost", "text": "Superghost\n\nIn a supersymmetric quantum field theory, a superghost is a fermionic Faddeev–Popov ghost, which is used in the gauge fixing of a fermionic symmetry generator.\n"}
{"id": "7066598", "url": "https://en.wikipedia.org/wiki?curid=7066598", "title": "The Consumer's Guide to Effective Environmental Choices", "text": "The Consumer's Guide to Effective Environmental Choices\n\nThe Consumer's Guide to Effective Environmental Choices: Practical Advice from the Union of Concerned Scientists is a handbook printed by the nonprofit environmental group Union of Concerned Scientists.\n\nIn accordance with UCS's pledge to provide scientifically sound and nonbiased solutions to environmental problems, this book's main goal is to debunk myths associated with the environmental movement and reinforce realistic ways in which the average citizen can do his or her part in conservation.\n\nThe back cover of the book reads:\n\n\"Paper or plastic? Bus or car? Old house or new? Cloth diapers or disposables? Some choices have a huge impact on the environment; others are of negligible importance. To those of us who care about our quality of life and what is happening to the earth, this is a vastly important issue. In these pages, the Union of Concerned Scientists help inform consumers about everyday decisions that significantly affect the environment. For example, a few major decisions such as the choice of a house or vehicle have such a disproportionately large effect on the environment that minor environmental infractions shrink by comparison. This book identifies the 4 Most Significant Consumer Related Environmental Problems, 7 Most Damaging Categories, 11 Priority Actions, and 7 Rules for Responsible Consumption.\"\n\n\n1.Brower, Michael, and Warren Leon. \"The Consumer's Guide to Effective Environmental Choices : Practical Advice from the Union of Concerned Scientists\". New York: Three Rivers P, 1999.\n"}
{"id": "5160757", "url": "https://en.wikipedia.org/wiki?curid=5160757", "title": "The Symbolic Species", "text": "The Symbolic Species\n\nThe Symbolic Species is a 1997 book by biological anthropologist Terrence Deacon on the evolution of language. Combining perspectives from neurobiology, evolutionary theory, linguistics, and semiotics, Deacon proposes that language, along with the unique human capacity for symbolic thought, co-evolved with the brain.\n\n\"The Symbolic Species\" is a multi-disclipinary book that at the time of publishing was seen as groundbreaking. It is considered to have bound together a wide array of ideas in a way that advanced the understanding of professionals in several fields.\n\nThe reasons for the unique cognitive capacity of humans are explored, along with those for the large number human activities impossible for animals. The human use of language is said to be responsible for both.\n\nA chicken-and-egg problem is shown to exist between the emergence of symbolic thought and language: language is said to be the medium of symbolic thought, but it is reasoned that mastery of language would first require the ability to think symbolically. The solution of this chicken-and-egg problem, according to Deacon, is the subtle evolutionary process of co-evolution.\n"}
{"id": "22914706", "url": "https://en.wikipedia.org/wiki?curid=22914706", "title": "Tito Livio Burattini", "text": "Tito Livio Burattini\n\nTito Livio Burattini (, 8 March 1617 – 17 November 1681) was an inventor, architect, Egyptologist, scientist, instrument-maker, traveller, engineer, and nobleman. He was born in Agordo, Italy, and studied in Padua and Venice. In 1639, he explored the Great Pyramid of Giza with English mathematician John Greaves; both Burattini and Sir Isaac Newton used measurements made by Greaves in an attempt to accurately determine the circumference of the earth.\n\nFor Germany in 1641, the court of King Władysław IV invited him to Poland. In Warsaw, Burattini built a model aircraft with four fixed glider wings in 1647. Described as \"four pairs of wings attached to an elaborate 'dragon'\", it was said to have successfully lifted a cat in 1648 but not Burattini himself. According to Clive Hart's \"The Prehistory of Flight\", he promised that \"only the most minor injuries\" would result from landing the craft.\n\nHe later developed an early system of measurement based on time, similar to today's International System of Units; he published it in his book ' (lit. \"universal measure\") in 1675 at Wilno. His system includes the ' (lit. \"catholic [i.e. universal] metre\"), a unit of length equivalent to the length of a free seconds pendulum; it differs from the modern metre by half a centimetre. He is considered the first to recommend the name \"metre\" for a unit of length.\n\nAlong with two others he met at Kraków, Burattini \"performed optical experiments and contributed to the discovery of irregularities on the surface of Venus, comparable to those on the Moon\". He made lenses for microscopes and telescopes, and gave some of them to Cardinal Leopoldo de' Medici. He is also credited with building a calculating machine, which he donated to Grand Duke Ferdinando II, that borrows from both a Blaise Pascal machine and Napier's rods. He died in Kraków, aged 64.\n\n\n"}
{"id": "32645188", "url": "https://en.wikipedia.org/wiki?curid=32645188", "title": "Valeriy Chernyshev", "text": "Valeriy Chernyshev\n\nValeriy V. Chernyshev (born 25 September 1944 in Kemerovo, Soviet Union) is a Russian scientist, Doctor Sc. (Tech.), a specialist in nitrogen chemistry, Honourable Inventor of Russia. He works at the Department of Chemistry of the Moscow State University, Moscow.\n\nAmong his publications are:\n"}
{"id": "2393843", "url": "https://en.wikipedia.org/wiki?curid=2393843", "title": "Van der Waals constants (data page)", "text": "Van der Waals constants (data page)\n\nThe following table lists the van der Waals constants (from the van der Waals equation) for a number of common gases and volatile liquids.\n\nTo convert from formula_1 to formula_2, multiply by 100.\n\n1 J·m/mol = 1 m·Pa/mol = 10 L·bar/mol\n\n1 Latm/mol = .101325 J·m/mol = 0.101325 Pa·m/mol\n\n1 dm/mol = 1 L/mol = 1 m/kmol        (where kmol is kilomoles = 1000 moles)\n"}
{"id": "48922876", "url": "https://en.wikipedia.org/wiki?curid=48922876", "title": "Vasiliev equations", "text": "Vasiliev equations\n\nVasiliev equations are \"formally\" consistent gauge invariant nonlinear equations whose linearization over a specific vacuum solution describes free massless higher-spin fields on anti-de Sitter space. The Vasiliev equations are classical equations and no Lagrangian is known that starts from canonical two-derivative Fronsdal Lagrangian and is completed by interactions terms. There is a number of variations of Vasiliev equations that work in three, four and arbitrary number of space-time dimensions. Vasiliev's equations admit supersymmetric extensions with any number of super-symmetries and allow for Yang-Mills gaugings. Vasiliev's equations are background independent, the simplest exact solution being anti-de Sitter space. It is important to note that locality is not properly implemented and the equations give a solution of certain formal deformation procedure, which is difficult to map to field theory language. The higher-spin AdS/CFT correspondence is reviewed in Higher-spin theory article.\n\nThe Vasiliev equations are generating equations and yield differential equations in the space-time upon solving them order by order with respect to certain auxiliary directions. The equations rely on several ingredients: unfolded equations and higher-spin algebras.\n\nThe exposition below is organised in such a way as to split the Vasiliev's equations into the building blocks and then join them together. The example of the four-dimensional bosonic Vasiliev's equations is reviewed at length since all other dimensions and super-symmetric generalisations are simple modifications of this basic example. \n\nThree variations of Vasiliev's equations are known: four-dimensional, three-dimensional and d-dimensional. They differ by mild details that are discussed below.\nHigher-spin algebras are global symmetries of the higher-spin theory multiplet. The same time they can be defined as global symmetries of some conformal field theories (CFT), which underlies the kinematic part of the higher-spin AdS/CFT correspondence, which is a particular case of the AdS/CFT. Another definition is that higher-spin algebras are quotients of the universal enveloping algebra of the anti-de Sitter algebra formula_1 by certain two-sided ideals. Some more complicated examples of higher-spin algebras exist, but all of them can be obtained by tensoring the simplest higher-spin algebras with matrix algebras and then imposing further constraints. Higher-spin algebras originate as associative algebras and the Lie algebra can be constructed via the commutator.\n\nIn the case of the four-dimensional bosonic higher-spin theory the relevant higher-spin algebra is very simple thanks to formula_2 and can be built upon two-dimensional quantum Harmonic oscillator. In the latter case two pairs of creation/annihilation operators formula_3 are needed. These can be packed into the quartet \nformula_4 of operators obeying the canonical commutation relations\nwhere formula_6 is the formula_7 invariant tensor, i.e. it is anti-symmetric. As is well known, the bilinears provide an oscillator realization of formula_7:\nThe higher-spin algebra is defined as the algebra of all even functions formula_10 in formula_11. That the functions are even is in accordance with the bosonic content of the higher-spin theory as formula_11 will be shown to be related to the Majorana spinors from the space-time point of view and even powers of formula_11 correspond to tensors. It is an associative algebra and the product is conveniently realised by the Moyal star product:\nwith the meaning that the algebra of operators formula_15 can be replaced with the algebra of function formula_16 in ordinary commuting variables formula_17 (hats off) and the product needs to be replaced with the non-commutative star-product. For example, one finds\nand therefore formula_19 as it would be the case for the operators. Another representation of the same star-product is more useful in practice:\nThe exponential formula can be derived by integrating by parts and dropping the boundary terms. The prefactor is chosen as to ensure formula_21. In the Lorentz-covariant base we can split formula_22 and we also split formula_23. Then the Lorentz generators are formula_24, formula_25 and the translation generators are formula_26. The formula_27-automorphism can be realized in two equivalent ways: either as formula_28 or as formula_29. In both the cases it leaves the Lorentz generators untouched and flips the sign of translations.\n\nThe higher-spin algebra constructed above can be shown to be the symmetry algebra of the three-dimensional Klein-Gordon equation formula_30. Considering more general free CFT's, e.g. a number of scalars plus a number of fermions, the Maxwell field and other, one can construct more examples of higher-spin algebras.\n\nThe Vasiliev equations are equations in certain bigger space endowed with auxiliary directions to be solved for. The additional directions are given by the doubles of formula_17, called formula_32,\nwhich are furthermore entangled with Y. The star-product on the algebra of functions in formula_33 in formula_34-variables is\nThe integral formula here-above is a particular star-product that corresponds to the Weyl ordering among Y's and among Z's, with the opposite signs for the commutator:\nMoreover, the Y-Z star product is normal ordered with respect to Y-Z and Y+Z as is seen from \n\nThe higher-spin algebra is an associative subalgebra in the extended algebra. In accordance with the bosonic projection is given by formula_38.\n\nThe essential part of the Vasiliev equations relies on an interesting deformation of the Quantum harmonic oscillator, known as deformed oscillators. First of all, let us pack the usual creation and annihilation operators formula_39 in a doublet formula_40. The canonical commutation relations (the formula_41-factors are introduced to facilitate comparison with Vasiliev's equations)\ncan be used to prove that the bilinears in formula_43 form formula_44 generators\nIn particular, formula_46 rotates formula_43 as an formula_48-vector with formula_49 playing the role of the formula_48-invariant metric. The deformed oscillators are defined by appending the set of generators with an additional generating element formula_51 and postulating\nAgain, one can see that formula_46, as defined above, form formula_48-generators and rotate properly formula_43. At formula_56 we get back to the undeformed oscillators. In fact, formula_43 and formula_46 form the generators of the Lie superalgebra formula_59, where formula_43 should be viewed as odd generators. Then, formula_61 is the part of the defining relations of formula_59.\nOne (or two) copies of the deformed oscillator relations form a part of the Vasiliev equations where the generators are replaced with fields and the commutation relations are imposed as field equations.\n\nThe equations for higher-spin fields originate from the Vasiliev equations in the unfolded form.\nAny set of differential equations can be put in the first order form by introducing auxiliary fields to denote derivatives. Unfolded approach is an advanced reformulation of this idea that takes into account gauge symmetries and diffeomorphisms. Instead of just formula_63 the unfolded equations are written in the language of differential forms as\nwhere the variables are differential forms formula_65 of various degrees, enumerated by an abstract index formula_66; formula_67 is the exterior derivative formula_68. The structure function formula_69 is assumed to be expandable in exterior product Taylor series as\nwhere formula_71 has form degree formula_72 and the sum is over all forms whose form degrees add up to formula_73. The simplest example of unfolded equations are the zero curvature equations formula_74 for a one-form connection formula_75 of any Lie algebra formula_76. Here formula_66 runs over the base of the Lie algebra, and the structure function formula_78 encodes the structure constants of the Lie algebra.\n\nSince formula_79 the consistency of the unfolded equations requires \nwhich is the Frobenius integrability condition. In the case of the zero curvature equation this is just the Jacobi identity. Once the system is integrable it can be shown to have certain gauge symmetries. Every field formula_71 that is a form of non-zero degree formula_72 possesses a gauge parameter formula_83 that is a form of degree formula_84 and the gauge transformations are\n\nThe Vasiliev equations generate the unfolded equations for a specific field content, which consists of a one-form formula_75 and a zero-form formula_87, both taking values in the higher-spin algebra. Therefore, formula_88 and formula_89, formula_90. The unfolded equations that describe interactions of higher-spin fields are\nwhere formula_92 are the interaction vertices that are of higher and higher order in the formula_87-field. The product in the higher-spin algebra is denoted by formula_94. The explicit form of the vertices can be extracted from the Vasiliev equations. The vertices that are bilinear in the fields are determined by the higher-spin algebra. Automorphism formula_27 is induced by the automorphism of the anti-de Sitter algebra that flips the sign of translations, see below.\nIf we truncate away higher orders in the formula_87-expansion, the equations are just the zero-curvature condition for a connection formula_75 of the higher-spin algebra and the covariant constancy equation for a zero-form formula_87 that takes values in the twisted-adjoint representation (twist is by the automorphism formula_27).\n\nThe field content of the Vasiliev equations is given by three fields all taking values in the extended algebra of functions in Y and Z: \n\nAs to avoid any confusion caused by the differential forms in the auxiliary Z-space and to reveal the relation to the deformed oscillators the Vasiliev equations are written below in the component form. \nThe Vasiliev equations can be split into two parts. The first part contains only zero-curvature or covariant constancy equations:\nwhere the higher-spin algebra automorphism formula_112 is extended to the full algebra as \nthe latter two forms being equivalent because of the bosonic projection imposed on formula_114.\n\nTherefore, the first part of the equations implies that there is no nontrivial curvature in the x-space since formula_115 is flat. The second part makes the system nontrivial and determines the curvature of the auxiliary connection formula_108:\nwhere two Klein operators were introduced\nThe existence of the Klein operators is of utter importance for the system. They realise the formula_112 automorphism as an inner one \nIn other words, the Klein operator formula_121 behave as formula_122, i.e. it anti-commutes to odd functions and commute to even functions in y,z.\n\nThese 3+2 equations are the Vasiliev equations for the four-dimensional bosonic higher-spin theory. Several comments are in order.\n\nTo prove that the linearized Vasiliev equations do describe free massless higher-spin fields we need to consider the linearised fluctuations over the anti-de Sitter vacuum. First of all we take the exact solution where formula_150 is a flat connection of the anti-de Sitter algebra, formula_149 and formula_152 and add fluctuations \nThen, we linearize the Vasiliev equations \nAbove it was used several times that formula_155, i.e. the vacuum value of the S-field acts as the derivative under the commutator. It is convenient to split the four-component Y,Z into two-component variables as formula_156. Another trick that was used in the fourth equation is the invertibility of the Klein operators: \nThe fifth of the Vasiliev equations is now split into the last three equation above.\n\nThe analysis of the linearized fluctuations is in solving the equations one by one in the right order. Recall that one expects to find unfolded equations for two fields: one-form formula_158 and zero-form formula_104. From the fourth equation it follows that \nformula_160 does not depend on the auxiliary Z-direction. Therefore, one can identify formula_161. \nThe second equation then immediately leads to \nwhere formula_163 is the Lorentz covariant derivative\nwhere ... denote the term with formula_165 that is similar to the first one. The Lorentz covariant derivative comes from the usual commutator action of the spin-connection part of formula_143. The term with the vierbein results from the formula_112-automorphism that flips the sign of the AdS-translations and produces anti-commutator formula_168.\n\nTo read off the content of the C-equation one needs to expand it in Y and analyze the C-equation component-wise\nThen various components can be seen to have the following interpretation:\n\nThe last three equations can be recognized to be the equations of the form formula_179 where formula_180 is the exterior derivative on the space of differential forms in the Z-space. Such equations can be solved with the help of the Poincare Lemma. In addition one needs to know how to multiply by the Klein operator from the right, which is easy to derive from the integral formula for the star-product:\nI.e. the result is to exchange the half of the Y and Z variables and to flip the sign. The solution to the last three equations can be written as\nwhere a similar formula exists for formula_183.\nHere the last term is the gauge ambiguity, i.e. the freedom to add exact forms in the Z-space, and formula_184.\nOne can gauge fix it to have formula_185. Then, one plugs the solution to the third equation, which of the same type, i.e. a differential equation of the first order in the Z-space. Its general solution is again given by the Poincare Lemma\nwhere formula_187 is the integration constant in the Z-space, i.e. the de-Rham cohomology. It is this integration constant that is to be identified with the one-form formula_188 as the name suggests. After some algebra one finds\nwhere we again dropped a term with dotted and undotted indices exchanged. The last step is to plug the solution into the first equation to find\nand again the second term on the right is omitted. It is important that formula_191 is not a flat connection, while formula_192 is a flat connection. To analyze the formula_191-equations it is useful to expand formula_191 in Y\nThe content of the formula_191-equation is as follows:\n\nTo conclude, anti-de Sitter space is an exact solution of the Vasiliev equations and upon linearization over it one finds unfolded equations that are equivalent to the Fronsdal equations for fields with s=0,1,2,3... .\n\n\nso that the fields are now function of formula_208 and space-time coordinates. The components of the fields are required to have the right spin-statistic. The equations need to be slightly modified.\n\nThere also exist Vasiliev's equations in other dimensions:\nThe equations are very similar to the four-dimensional ones, but there are some important modifications in the definition of the algebra that the fields take values in and there are further constraints in the d-dimensional case.\nThere is a number of flaws/features of the Vasiliev equations that have been revealed over the last years. First of all, classical equations of motion, e.g. the Vasiliev equations, do not allow one to address the problems that require an action, the most basic one being quantization. Secondly, there are discrepancies between the results obtained from the Vasiliev equations and those from the other formulations of higher spin theories, from the AdS/CFT correspondence or from general field theory perspective. Most of the discrepancies can be attributed to the assumptions used in the derivation of the equations: gauge invariance is manifest, but locality was not properly imposed and the Vasiliev equations are a solution of a certain formal deformation problem. Practically speaking, it is not known in general how to extract the interaction vertices of the higher spin theory out of the equations.\n\nMost of the studies concern with the four-dimensional Vasiliev equations. \nThe correction to the free spin-2 equations due to the scalar field stress-tensor was extracted out of the four-dimensional Vasiliev equations and found to be\nwhere formula_211 are symmetrized derivatives with traces subtracted. The most important information is in the coefficients formula_212 and in the prefactor formula_213, where formula_214 is a free parameter that the equations have, see Other dimensions, extensions, and generalisations. It is important to note that the usual stress-tensor has no more than two derivative and the terms formula_215 are not independent (for example, they contribute to the same formula_216 AdS/CFT three-point function). This is a general property of field theories that one can perform nonlinear (and also higher derivative) field redefinitions and therefore there exist infinitely many ways to write the same interaction vertex at the classical level. The canonical stress-tensor has two derivatives and the terms with contracted derivatives can be related to it via such redefinitions.\n\nA surprising fact that had been noticed before its inconsistency with the AdS/CFT was realized is that the stress-tensor can change sign and, in particular, vanishes for formula_217. This would imply that the corresponding correlation function in the Chern-Simons matter theories vanishes, formula_218, which is not the case. \n\nThe most important and detailed tests were performed much later. It was first shown that some of the three-point AdS/CFT functions, as obtained from the Vasiliev equations, turn out to be infinite or inconsistent with AdS/CFT, while some other do agree. Those that agree, in the language of Unfolded equations correspond to formula_219 and the infinities/inconsistencies resulted from formula_220. The terms of the first type are local and are fixed by the higher spin algebra. The terms of the second type can be non-local (when solved perturbatively the master field formula_221 is a generating functions of infinitely many derivatives of higher spin fields). These non-localities are not present in higher spin theories as can be seen from the explicit cubic action. \n\nFurther infinities, non-localities or missing structures were observed. Some of these tests explore the extension of the Klebanov-Polyakov Conjecture to Chern-Simons matter theories where the structure of correlation functions is more intricate and certain parity-odd terms are present. Some of these structures were not reproduced by the Vasiliev equations. General analysis of the Vasiliev equations at the second order showed that for any three fixed spins the interaction term is an infinite series in derivatives (similar to formula_222-sum above); all of the terms in the series contribute to the same AdS/CFT three-point function and the contribution is infinite. All the problems can be attributed to the assumptions used in the derivation of the Vasiliev equations: restrictions on the number of derivatives in the interaction vertices or, more generally, locality was not imposed, which is important for getting meaningful interaction vertices, see e.g. Noether Procedure. The problem how to impose locality and extract interaction vertices out of the equations is now under active investigation.\n\nAs is briefly mentioned in Other dimensions, extensions, and generalisations there is an option to introduce infinitely many additional coupling constants that enter via phase factor formula_223. As was noted, the second such coefficient formula_224 will affect five-point AdS/CFT correlation functions, but not the three-point ones, which seems to be in tension with the results obtained directly from imposing higher spin symmetry on the correlation functions. Later, it was shown that the terms in the equations that result from \nformula_225 are too non-local and lead to an infinite result for the AdS/CFT correlation functions. \n\nIn three dimensions the Prokushkin-Vasiliev equations, which are supposed to describe interactions of matter fields with higher spin fields in three dimensions, are also affected by the aforementioned locality problem. For example, the perturbative corrections at the second order to the stress-tensors of the matter fields lead to infinite correlation functions. There is, however, another discrepancy: the spectrum of the Prokushkin-Vasiliev equations has, in addition to the matter fields (scalar and spinor) and higher spin fields, a set of unphysical fields that do not have any field theory interpretation, but interact with the physical fields.\n\nSince the Vasiliev equations are quite complicated there are few exact solutions known\n\n\n"}
