{"id": "531140", "url": "https://en.wikipedia.org/wiki?curid=531140", "title": "A35 road", "text": "A35 road\n\nThe A35 is a road in southern England, connecting Honiton in Devon and Southampton in Hampshire. It is a trunk road for some of its length. Most of its route passes through Dorset and the New Forest. It originally connected Exeter and Southampton, the original A35 ran along what is now the A3052 joining the present road at Charmouth.\n\nBeginning in Honiton off the A30 road, the A35 travels in a roughly south-easterly direction past Axminster, Charmouth and Bridport. After Bridport, there is a section of dual carriageway, before it reaches its bypass around Dorchester. After Dorchester, there are approximately of dual carriageway, including the Puddletown bypass, until it reaches its roundabout with the A31 road at Bere Regis. Continuing roughly south-easterly still, it becomes dual carriageway again near Upton, before returning to a single carriageway through Poole and Bournemouth, apart from a small section of dual carriageway on Wessex Way. On reaching Christchurch, there is a dual-carriageway bypass. It then heads in a north-easterly direction through the New Forest, passing through Lyndhurst where it meets the A337 road (to Lymington). It continues through Ashurst and Totton, meeting the A36 road and M271 motorway at grade separated junctions. It then turns north-east, acting as the western part of Southampton's ring road, with the A27 road making up the eastern part. It terminates at Swaythling, on the northern outskirts of Southampton.\n\nIn Bournemouth, it has been diverted around the Sovereign Centre of Boscombe along Centenary Way; much of its former road is now pedestrianised. Its route through Poole and Bournemouth passes through suburbs.\n\nThe A35 is a main route along England's south coast, and is congested. To help solve the congestion, sections of the A35 were upgraded between Honiton and Bere Regis. Some improvements have involved the construction of new dual carriageway bypasses such as the Puddletown bypass which opened in 1999, and others include new sections of single carriageway road, such as its straightening between Slepe and the Upton bypass in 2004, and the extension of the limit west of Slepe by in 2007. The Puddletown dual carriageway (together with the A30 Honiton-Exeter dualling) were financed under a Design, Build, Finance and Operate (DBFO) contract running from 1996-2026. \n\n"}
{"id": "32181776", "url": "https://en.wikipedia.org/wiki?curid=32181776", "title": "ACS Award for Encouraging Women into Careers in the Chemical Sciences", "text": "ACS Award for Encouraging Women into Careers in the Chemical Sciences\n\nThe award, sponsored by The Camille and Henry Dreyfus Foundation, was instituted in 1993 with the intention of recognizing \"significant accomplishments by individuals who have stimulated or fostered the interest of women in chemistry, promoting their professional development as chemists or chemical engineers.\" Recipients receive $5,000, a certificate, up to $1,500 for travel expenses, and a grant of $10,000. The deadline for nomination is 1 November every year.\n\n"}
{"id": "41087955", "url": "https://en.wikipedia.org/wiki?curid=41087955", "title": "Accelerator physics codes", "text": "Accelerator physics codes\n\nA charged particle accelerator is a complex machine that takes elementary charged particles and accelerates them to very high energies. Accelerator physics is a field of physics encompassing all the aspects required to design and operate the equipment and to understand the resulting dynamics of the charged particles. There are software packages associated with each such domain. There are a large number of such codes. The 1990 edition of the Los Alamos Accelerator Code Group's compendium provides summaries of more than 200 codes. Certain of those codes are still in use today although many are obsolete. Another index of existing and historical accelerator simulation codes is located at \n\nFor many applications it is sufficient to track a single particle through the relevant electric and magnetic fields.\nOld unmaintained codes include: BETA, AGS, ALIGN, COMFORT, DESIGN, DIMAD, GUINEA-PIG, HARMON, LEGO, LIAR, MAGIC, MARYLIE, PATRICIA, PETROS, PLACET, RACETRACK, SYNCH, TRACY and variants, TRANSPORT, TURTLE, and UAL.\nMaintained codes include:\n\n\n\n\n\n\nThe self interaction (e.g. space charge) of the charged particle beam can cause growth of the beam, such as with bunch lengthening, or intrabeam scattering. Additionally, space charge effects may cause instabilities and associated beam loss. Typically the Poisson equation is solved at intervals during the tracking using Particle-in-cell algorithms. Space charge effects lessen at higher energies so at higher energies the space charge effects may be modeled using simpler algorithms that are computationally much faster than the algorithms used at lower energies.\nCodes that handle low energy space charge effects, including computation of growth values and instability thresholds, include:\n\nAt higher energies, space charge effects include Touschek scattering and coherent synchrotron radiation (CSR). Codes that handle higher energy space charge include:\n\nAn important class of collective effects may be summarized in terms of the beams response to an \"impedance\". An important job is thus the computation of this impedance for the machine. Codes for this computation include\n\nTo control the charged particle beam, appropriate electric and magnetic fields must be created. There are software packages to help in the design and understanding of the magnets, RF cavities, and other elements that create these fields. Codes include\n\nGiven the variety of modelling tasks, there is not one common data format that has developed.\nFor describing the layout of an accelerator and the corresponding elements, one uses a so-called \"lattice file\".\nThere have been numerous attempts at unifying the lattice file formats used in different codes. One unification attempt is the Accelerator Markup Language, and the Universal Accelerator Parser. Another attempt at a unified approach to accelerator codes is the UAL or Universal Accelerator Library.\n\nThe file formats used in\nMAD may be the most common, with translation routines available to convert to an input form needed for a different code. \nAssociated with the Elegant code is a data format called SDDS, with an associated suite of tools. If one uses a Matlab-based code, such as Accelerator Toolbox, one has available all the tools within Matlab.\n\nThere are many applications of particle accelerators. For example, two important applications are elementary particle physics and synchrotron radiation production. When performing a modeling task for any accelerator operation, the results of charged particle beam dynamics simulations must feed into the associated application. Thus, for a full simulation, one must include the codes in associated applications. For particle physics, the simulation may be continued in a detector with a code such as Geant4. \n\nFor a synchrotron radiation facility, for example, the electron beam produces an x-ray beam that then travels down a beamline before reaching the experiment. Thus, the electron beam modeling software must interface with the x-ray optics modelling software such as SRW, Shadow, McXTrace, or Spectra. Bmad can model both X-rays and charged particle beams. The x-rays are used in an experiment which may be modeled and analyzed with various software, such as the DAWN science platform. OCELOT also includes both synchrotron radiation calculation and x-ray propagation models.\n\n"}
{"id": "32786118", "url": "https://en.wikipedia.org/wiki?curid=32786118", "title": "Alaskana", "text": "Alaskana\n\nAlaskana is a New Latin term meaning \"of Alaska\", used in taxonomy to denote species indigenous to or strongly associated with Alaska.\n\n\n\n\n\n"}
{"id": "647962", "url": "https://en.wikipedia.org/wiki?curid=647962", "title": "Antonio Damasio", "text": "Antonio Damasio\n\nAntonio Damasio () is a Portuguese-American neuroscientist. He is currently the David Dornsife Professor of Neuroscience, Psychology and Philosophy at the University of Southern California and an adjunct professor at the Salk Institute. Damasio heads the Brain and Creativity Institute, and has authored several books: his most recent work, \"Self Comes to Mind: Constructing the Conscious Brain\" (2010), explores the relationship between the brain and consciousness. Damasio's research in neuroscience has shown that emotions play a central role in social cognition and decision-making.\n\nDamasio studied medicine at the University of Lisbon Medical School, where he also did his neurological residency and completed his doctorate. For part of his studies, he researched behavioral neurology under the supervision of Norman Geschwind of the Aphasia Research Center in Boston.\n\nDamasio's main field is neurobiology, especially the neural systems which underlie emotion, decision-making, memory, language and consciousness. Damasio might believe that emotions play a critical role in high-level cognition—an idea counter to dominant 20th-century views in psychology, neuroscience and philosophy.\nDamasio formulated the somatic marker hypothesis, a theory about how emotions and their biological underpinnings are involved in decision-making (both positively and negatively, and often non-consciously). Emotions provide the scaffolding for the construction of social cognition and are required for the self processes which undergird consciousness. \"Damasio provides a contemporary scientific validation of the linkage between feelings and the body by highlighting the connection between mind and nerve cells ... this personalized embodiment of mind.\"\n\nThe somatic marker hypothesis has inspired many neuroscience experiments carried out in laboratories in the U.S. and Europe, and has had a major impact in contemporary science and philosophy. Damasio has been named by the Institute for Scientific Information as one of the most highly cited researchers in the past decade. Current work on the biology of moral decisions, neuro-economics, social communication, and drug-addiction, has been strongly influenced by Damasio's hypothesis.An article published in the Archives of Scientific Psychology in 2014 named Damasio one of the 100 most eminent psychologist of the modern era. (Diener et al. \"Archives of Scientific Psychology\", 2014, 2, 20–32). The June–July issue of \"Sciences Humaines\" included Damasio in its list of 50 key thinkers in the human sciences of the past two centuries.\n\nDamasio also proposed that emotions are part of homeostatic regulation and are rooted in reward/punishment mechanisms. He recovered William James' perspective on feelings as a read-out of body states, but expanded it with an \"as-if-body-loop\" device which allows for the substrate of feelings to be simulated rather than actual (foreshadowing the simulation process later uncovered by mirror neurons). He demonstrated experimentally that the insular cortex is a critical platform for feelings, a finding that has been widely replicated, and he uncovered cortical and subcortical induction sites for human emotions, e.g. in ventromedial prefrontal cortex and amygdala. He also demonstrated that while the insular cortex plays a major role in feelings, it is not necessary for feelings to occur, suggesting that brain stem structures play a basic role in the feeling process.\n\nHe has continued to investigate the neural basis of feelings and demonstrated that although the insular cortex is a major substrate for this process it is not exclusive, suggesting that brain stem nuclei are critical platforms as well. He regards feelings as the necessary foundation of sentience.\n\nIn another development, Damasio proposed that the cortical architecture on which learning and recall depend involves multiple, hierarchically organized loops of axonal projections that converge on certain nodes out of which projections diverge to the points of origin of convergence (the convergence-divergence zones). This architecture is applicable to the understanding of memory processes and of aspects of consciousness related to the access of mental contents.\n\nIn \"The Feeling of What Happens\", Damasio laid the foundations of the \"enchainment of precedences\": \"the nonconscious neural signaling of an individual organism begets the protoself which permits core self and core consciousness, which allow for an autobiographical self, which permits extended consciousness. At the end of the chain, extended consciousness permits conscience.\n\nDamasio's research depended significantly on establishing the modern human lesion method, an enterprise made possible by Hanna Damasio's structural neuroimaging/neuroanatomy work complemented by experimental neuroanatomy (with Gary Van Hoesen and Josef Parvizi), experimental neuropsychology (with Antoine Bechara, Ralph Adolphs, and Dan Tranel) and functional neuroimaging (with Kaspar Meyer, Jonas Kaplan, and Mary Helen Immordino-Yang). The experimental neuroanatomy work with Van Hoesen and Bradley Hyman led to the discovery of the disconnection of the hippocampus caused by neurofibrillary tangles in the entorhinal cortex of patients with Alzheimer's disease.\n\nAs a clinician, he and his collaborators have studied and treated disorders of behaviour and cognition, and movement disorders.\n\nDamasio's books deal with the relationship between emotions and their brain substrates. His 1994 book, \"Descartes' Error: Emotion, Reason and the Human Brain\", won the Science et Vie prize, was a finalist for the \"Los Angeles Times\" Book Award, and is translated in over 30 languages. It is regarded as one of the most influential books of the past two decades. His second book, \"The Feeling of What Happens: Body and Emotion in the Making of Consciousness\", was named as one of the ten best books of 2001 by the \"New York Times Book Review\", a \"Publishers Weekly\" Best Book of the Year, a \"Library Journal\" Best Book of the Year, and has over 30 foreign editions. Damasio's \"Looking for Spinoza: Joy, Sorrow, and the Feeling Brain\", was published in 2003. In it, Damasio suggested that Spinoza's thinking foreshadowed discoveries in biology and neuroscience views on the mind-body problem and that Spinoza was a protobiologist. Damasio's latest book is \"Self Comes to Mind: Constructing the Conscious Brain\". In it Damasio suggests that the self is the key to conscious minds and that feelings, from the kind he designates as primordial to the well-known feelings of emotion, are the basic elements in the construction of the protoself and core self. The book received the Corinne International Book Prize.\nDamasio is a member of the American Academy of Arts and Sciences, the National Academy of Medicine, the European Academy of Sciences and Arts. He is the recipient of several prizes, amongst them the Grawemeyer Award, the Honda Prize, the Prince of Asturias Award in Science and Technology and the Beaumont Medal from the American Medical Association, as well as honorary degrees from, most recently, the Sorbonne (Université Paris Descartes), shared with his wife Hanna Damasio. He has also received doctorates from the Universities of Aachen, Copenhagen, Leiden, Barcelona, Coimbra, Leuven and numerous others.\n\nIn 2013, the Escola Secundária António Damásio was dedicated in Lisbon.\n\nHe says he writes in the belief that \"scientific knowledge can be a pillar to help humans endure and prevail.\"\n\nHe is married to Hanna Damasio, a prominent neuroscientist and frequent collaborator and co-author, who is a professor of neuroscience at the University of Southern California and the director of the Dornsife Neuroimaging Center.\n\nIn 2017 he was designated member of the Council of State of Portugal, replacing Antonio Guterres, the 9th Secretary-General of the United Nations.\n\nDamasio additionally serves on the board of directors of the Berggruen Institute, and sits on the jury for the Berggruen Prize for Philosophy.\n\n\n\n\n"}
{"id": "7994349", "url": "https://en.wikipedia.org/wiki?curid=7994349", "title": "Bamboo blossom", "text": "Bamboo blossom\n\nBamboo blossom is a natural phenomenon in which the bamboos in a location blossom and become hung with bamboo seeds. In China, Myanmar and India, \"bamboo blossom\" was traditionally seen as a curse or an indication of a starvation coming.\n\nBamboos usually have a lifecycle around 40 to 80 years, varying among species. Normally, new bamboos grow up from bamboo shoots at the roots. At infrequent intervals for most species, they will start to blossom. After blossom, flowers produce fruit (called \"bamboo rice\" in parts of India and China, or colloquially called \"bambinos\" by some people of the internet). Following this, the bamboo forest dies out. Since a bamboo forest usually grows from a single bamboo, the death of bamboos occurs in a large area.\n\nMany bamboo species only flower at intervals as long as 65 or 120 years. These taxa exhibit mast flowering (or gregarious flowering), with all plants in a particular cohort flowering over a several-year period. Any plant derived through clonal propagation from this cohort will also flower regardless of whether it has been planted in a different location. The longest mass flowering interval known is 130 years, for the species \"Phyllostachys bambusoides\" (Sieb. & Zucc.). In this species, all plants of the same stock flower at the same time, regardless of differences in geographic locations or climatic conditions, and then die. The lack of environmental impact on the time of flowering indicates the presence of some sort of \"alarm clock\" in each cell of the plant which signals the diversion of all energy to flower production and the cessation of vegetative growth. This mechanism, as well as the evolutionary cause behind it, is still largely a mystery.\n\nOne hypothesis to explain the evolution of this semelparous mass flowering phenomenon is the predator satiation hypothesis, which argues that by fruiting at the same time, a population increases the survival rate of their seeds by flooding the area with fruit. Therefore, even if predators eat their fill, seeds will still be left over. By having a flowering cycle longer than the lifespan of the rodent predators, bamboos can regulate animal populations by causing starvation during the period between flowering events. Thus, the death of the adult clone is due to resource exhaustion, as it would be more effective for parent plants to devote all resources to creating a large seed crop than to hold back energy for their own regeneration.\n\nAnother hypothesis, called the fire cycle hypothesis, argues that periodic flowering followed by death of the adult plants has evolved as a mechanism to create disturbance in the habitat, thus providing the seedlings with a gap in which to grow. This argues that the dead culms create a large fuel load, and also a large target for lightning strikes, increasing the likelihood of wildfire. Because bamboos can be aggressive as early successional plants, the seedlings would be able to outstrip other plants and take over the space left by their parents.\n\nHowever, both have been disputed for different reasons. The predator satiation hypothesis does not explain why the flowering cycle is 10 times longer than the lifespan of the local rodents, something not predicted. The bamboo fire cycle hypothesis is considered by a few scientists to be unreasonable; they argue that fires only result from humans and no natural fires occur in India. This notion is considered wrong based on distribution of lightning strike data during the dry season throughout India. However, another argument against this is the lack of precedent for any living organism to harness something as unpredictable as lightning strikes to increase its chance of survival as part of natural evolutionary progress.\n\nThe mass fruiting also has direct economic and ecological consequences, however. The huge increase in available fruit in the forests often causes a boom in rodent populations, leading to increases in disease and famine in nearby human populations. For example, devastating consequences occur when the \"Melocanna bambusoides\" population flowers and fruits once every 30–35 years around the Bay of Bengal. The death of the bamboo plants following their fruiting means the local people lose their building material, and the large increase in bamboo fruit leads to a rapid increase in rodent populations. As the number of rodents increases, they consume all available food, including grain fields and stored food, sometimes leading to famine. These rats can also carry dangerous diseases, such as typhus, typhoid, and bubonic plague, which can reach epidemic proportions as the rodents increase in number. The relationship between rat populations and bamboo flowering was examined in a 2009 \"Nova\" documentary \"\".\n\nThe sudden death of large areas of bamboo puts pressure on animals that depend on bamboo as a food source, such as the endangered giant panda.\n\nFlowering produces large quantities of seeds, typically suspended from the ends of the branches. These seeds give rise to a new generation of plants that may be identical in appearance to those that preceded the flowering, or they may produce new cultivars with different characteristics, such as the presence or absence of striping or other changes in coloration of the culms.\n\nSeveral bamboo species are never known to set seed even when sporadically flowering has been reported. \"Bambusa vulgaris\", \"Bambusa balcooa\", and \"Dendrocalamus stocksii\" are common examples of such bamboo.\n\n"}
{"id": "7975240", "url": "https://en.wikipedia.org/wiki?curid=7975240", "title": "Behram (crater)", "text": "Behram (crater)\n\nBehram is an impact crater on the anti-Saturn hemisphere of Saturn's moon Enceladus. Behram was first observed in \"Cassini\" images during that mission's March 2005 flyby of Enceladus. It is located at 15.4° South Latitude, 181.0° West Longitude, and is 13.7 kilometers across. Behram's rim overlaps that of Shakashik, suggesting that Behram formed after Shakashik. Following formation, numerous criss-crossing fractures cut across Behram, forming canyons hundreds of meters deep along the crater's rim, as well as a region of disrupted terrain on the crater floor. The International Astronomical Union (IAU) adopted the S. Behram designation for feature ID 14238 in 2007.\n"}
{"id": "40911325", "url": "https://en.wikipedia.org/wiki?curid=40911325", "title": "Bio-duck", "text": "Bio-duck\n\nBio-duck is a mysterious quacking-like sound which was first reported in the open ocean by submarines in the 1960s. It is recorded frequently around the coasts of Australia, and in particular in the Perth Canyon.\n\nThe sounds were originally detected by sonar operators on . They are audible with frequencies from 50 to 300 Hz. The duration of the calls is between 1.6 and 3.1 seconds. The sounds occur many times per day from winter to October, and then taper off until December; they are not heard again until the next summer.\n\nIn 2014, it was claimed that the source of the sound had been identified as being vocalisations from the Antarctic minke whale. Although the reason for the vocalisations remains a mystery, they appear to be produced near the surface before deep-feeding dives. There are hopes that analysing the history, location, and frequency of the sounds will enable cetacean researchers to learn more about the life cycle of the minke.\n"}
{"id": "42165878", "url": "https://en.wikipedia.org/wiki?curid=42165878", "title": "Black Knight satellite conspiracy theory", "text": "Black Knight satellite conspiracy theory\n\nThe Black Knight satellite conspiracy theory claims that there is a spacecraft in near-polar orbit of the Earth that is of extraterrestrial origin, and that NASA is engaged in a cover-up regarding its existence and origin. This conspiracy theory combines several unrelated stories into one narrative.\n\nA 1998 NASA photo is believed by some to show the Black Knight satellite, but NASA has stated that this is likely space debris, specifically a thermal blanket lost during an EVA mission.\n\nAccording to some UFO conspiracists, the Black Knight is an artificial satellite of extraterrestrial origin which has orbited Earth for approximately 13,000 years; the \"satellite\" story is most likely a conflation of several disconnected stories about various objects and their interpretations, all of them well documented independently and none using the term \"Black Knight\" upon their first publication. According to senior education support officer Martina Redpath of Armagh Planetarium in Northern Ireland:\n\nThe origin of the Black Knight legend is often \"retrospectively dated\" back to natural extraterrestrial repeating sources supposedly heard during the 1899 radio experiments of Nikola Tesla and long delayed echoes first heard by amateur radio operator Jørgen Hals in Oslo, Norway, in 1928. Brian Dunning of the Skeptoid podcast attributes Tesla's 1899 radio signals to pulsars, which were not identified until 1968.\n\nIn 1954, UFO researcher Donald Keyhoe told newspapers that the U.S. Air Force had reported that two satellites orbiting Earth had been detected. At that time, no country had the technology to launch a satellite. Skeptics have noted that Keyhoe had been promoting a UFO book at the time, and the news stories were likely written \"tongue-in-cheek\" and not intended to be taken seriously.\n\nA British rocket called the Black Knight rocket was used in conjunction with the Blue Streak missile program between 1958 and 1965, to test re-entry vehicles. A \"Black Knight satellite launcher\" project announced in 1964 was considered a priority by the Ministry of Aviation. The program never put anything into orbit, and it is unrelated to the Black Knight satellite legend.\n\nIn February 1960, \"TIME\" reported that the U.S. Navy had detected a dark object thought to be a Soviet spy satellite in orbit. A follow-up article confirmed that the object was \"the remains of an Air Force Discoverer VIII satellite that had gone astray.\"\n\nIn 1963, astronaut Gordon Cooper supposedly reported a UFO sighting during his 15th orbit in \"Mercury 9\" that was confirmed by tracking stations, but there is no evidence that this happened. Neither NASA's mission transcripts nor Cooper's personal copies show any such report being made during the orbit.\n\nIn 1973, Scottish author Duncan Lunan analyzed the long delayed radio echoes received by Hals and others and speculated that they could possibly originate from a 13,000 year old alien probe located in an orbit around the Earth's Moon. He suggested that the probe may have originated from a planet located in the solar system of star Epsilon Boötis. Lunan later retracted his conclusions, saying that he had made \"outright errors\" and that his methods had been \"unscientific\".\n\nSpace debris photographed in 1998 during the STS-88 mission has been widely claimed to be the Black Knight satellite. Space journalist James Oberg considers it probable that the photographs are of a thermal blanket that was confirmed as lost during an EVA by Jerry L. Ross and James H. Newman.\n"}
{"id": "29332865", "url": "https://en.wikipedia.org/wiki?curid=29332865", "title": "Bond Glacier", "text": "Bond Glacier\n\nBond Glacier () is a steep, heavily crevassed glacier to the west of Ivanoff Head, flowing from the continental ice to Blunt Cove at the head of Vincennes Bay. It was mapped from air photos taken by U.S. Navy Operation Highjump (1946–47), and named by the Advisory Committee on Antarctic Names for Captain Charles A. Bond, U.S. Navy, commander of the expedition's Western Group.\n\n"}
{"id": "6462018", "url": "https://en.wikipedia.org/wiki?curid=6462018", "title": "Brake force", "text": "Brake force\n\nBrake force, also known as Brake Power, is a measure of braking power of a vehicle.\n\nIn the case of railways, it is important that staff are aware of the brake force of a locomotive so sufficient brake power will be available on trains, particularly heavy freight trains.\n\n\n"}
{"id": "56024845", "url": "https://en.wikipedia.org/wiki?curid=56024845", "title": "CIFAR-10", "text": "CIFAR-10\n\nThe CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly used to train machine learning and computer vision algorithms. It is one of the most widely used datasets for machine learning research. The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class.\n\nComputer algorithms for recognizing objects in photos often learn by example. CIFAR-10 is a set of images that can be used to teach a computer how to recognize objects. Since the images in CIFAR-10 are low-resolution (32x32), this dataset can allow researchers to quickly try different algorithms to see what works. Various kinds of convolutional neural networks tend to be the best at recognizing the images in CIFAR-10.\n\nCIFAR-10 is a labeled subset of the 80 million tiny images dataset. When the dataset was created, students were paid to label all of the images.\n\nThis is a table of some of the research papers that claim to have achieved state-of-the-art results on the CIFAR-10 dataset. Not all papers are standardized on the same pre-processing techniques, like image flipping or image shifting. For that reason, it is possible that one paper's claim of state-of-the-art could have a higher error rate than an older state-of-the-art claim but still be valid.\n\n"}
{"id": "4243084", "url": "https://en.wikipedia.org/wiki?curid=4243084", "title": "Carl Josef Bayer", "text": "Carl Josef Bayer\n\nCarl Josef Bayer (also Karl Bayer, March 4, 1847 – October 4, 1904) was an Austrian chemist who invented the Bayer process of extracting alumina from bauxite, essential to this day to the economical production of aluminium.\n\nBayer had been working in Saint Petersburg to develop a method to provide alumina to the textile industry that used it as a fixing agent in the dyeing of cotton. In 1887, he discovered that aluminium hydroxide precipitated from an alkaline solution which is crystalline and can be filtered and washed more easily than that precipitated from an acid medium by neutralization. In 1888, Bayer developed and patented his four-stage process of extracting alumina from bauxite ore.\n\nIn the mid-19th-century, aluminium was so precious that a bar of the metal was exhibited alongside the French Crown Jewels at the Exposition Universelle in Paris 1855. Along with the Hall-Héroult process, Bayer's solution caused the price of aluminum to drop about 80% in 1890 from what it had been in 1854.\n\n\n"}
{"id": "4144462", "url": "https://en.wikipedia.org/wiki?curid=4144462", "title": "Certified Funds", "text": "Certified Funds\n\nIn the United States and Canada, Certified Funds are a form of payment that is guaranteed to clear or settle by the company certifying the funds.\n\nWhen making certain types of transactions, such as purchasing real property, motor vehicles and other items that require title, the seller usually requires a guarantee that the payment method used will satisfy the obligations. To do this, the seller will require certified funds, usually in the form of:\n\n\nSpecifically, personal checks are not allowed, as the account may not have sufficient funds, and credit cards are not allowed, as the transaction may later be disputed or reversed.\n\nChecks sent by a bank bill payment service where funds are removed from the sender's account before the check is sent are in an ambiguous category since the recipient may not be notified that the funds are certified. An example is Chase Billpay service.\n\nSometimes steps may be taken to ensure that certified funds cannot easily be forged. These steps can include various unique stamps, inks and hole punchers, as well as the assistance of a machine such as a protectograph. Unfortunately these steps cannot prevent someone from erasing the payee's name and writing in their own name. Fraud is specifically not reimbursed by many issuers of money orders (e.g. Western Union), and so has to go through local police. The perpetrator can then claim \"identity theft\" to the investigating detective. Such money orders can be obtained from places like rent-drop boxes.\n"}
{"id": "34575952", "url": "https://en.wikipedia.org/wiki?curid=34575952", "title": "Cumbrian Coast Group", "text": "Cumbrian Coast Group\n\nThe Cumbrian Coast Group is a Permian lithostratigraphic group (a sequence of rock strata) which occurs in the western part of Cumbria in northern England.\n\nThe group outcrops near Whitehaven on the Cumbrian coast and beneath the Vale of Eden. It comprises the St Bees Evaporite and the overlying St Bees Shale Formation which are between 0 and 100m and 0 and 215m thick respectively. The lower formation sits atop the mixed lithology breccia known as Brockram. It is also found beneath the Irish Sea where the Barrowmouth Mudstone Formation is the equivalent of the St Bees Shale Formation. The Group can achieve thicknesses in excess of 300m here. \n"}
{"id": "36094506", "url": "https://en.wikipedia.org/wiki?curid=36094506", "title": "Developmentalist configuration", "text": "Developmentalist configuration\n\nThe developmentalist configuration is an socio-anthropological term used in development studies to describe the nature of \"development.\" The term was coined by Jean-Pierre Olivier de Sardan and is used by post-development theorists, postcolonialists, critical theorists and others. The term describes the paradigm of governments, NGOs, individuals and researchers who seek to progress the \"development\" of a country through cosmopolitan ideals which bring about social change.\n\nJean-Pierre Olivier de Sardan describes \"developmentalist configuration\" as:\n\n"}
{"id": "35205938", "url": "https://en.wikipedia.org/wiki?curid=35205938", "title": "Engineering studies", "text": "Engineering studies\n\nEngineering studies is an interdisciplinary branch of social sciences and humanities devoted to the study of engineers and their activities, often considered a part of science and technology studies (STS), and intersecting with and drawing from engineering education research. Studying engineers refers among other to the history and the sociology of their profession, its institutionalization and organization, the social composition and structure of the population of engineers, their training, their trajectory, etc. A subfield is for instance Women in engineering. Studying engineering refers to the study of engineering activities and practices, their knowledge and ontologies, their role into the society, their engagement.\n\nEngineering studies investigates how social, political, economical, cultural and historical dynamics affect technological research, design, engineering and innovation, and how these, in turn, affect society, economics, politics and culture.\n\n\n\n\n"}
{"id": "25277031", "url": "https://en.wikipedia.org/wiki?curid=25277031", "title": "Estavelle", "text": "Estavelle\n\nIn karst geology, estavelle or inversac is a ground orifice which, depending on weather conditions and season, can serve either as a sink or as a source of fresh water. It is a type of sinkhole.\n\n"}
{"id": "32478737", "url": "https://en.wikipedia.org/wiki?curid=32478737", "title": "Feminist security studies", "text": "Feminist security studies\n\nFeminist security studies is a subdiscipline of security studies that draws attention to gendered dimensions of security.\n\n"}
{"id": "14935313", "url": "https://en.wikipedia.org/wiki?curid=14935313", "title": "Franz Hübner", "text": "Franz Hübner\n\nFranz Hübner(18 November 1846 Drossen, near Frankfurt an der Oder – 31 December 1877) was a German entomologist\n\nBetween 1875 and 1877 he collected insects for the Museum Godeffroy in Samoa, Tonga and New Britain.\n\n"}
{"id": "57776148", "url": "https://en.wikipedia.org/wiki?curid=57776148", "title": "Ilyobacter insuetus", "text": "Ilyobacter insuetus\n\nIlyobacter insuetus is a mesophilic and anaerobic bacterium from the genus of \"Ilyobacter\" which has been isolated from marine anoxic sediments from Venice in Italy.\n"}
{"id": "18985062", "url": "https://en.wikipedia.org/wiki?curid=18985062", "title": "Information", "text": "Information\n\nInformation is any entity or form that provides the answer to a question of some kind or resolves uncertainty. It is thus related to data and knowledge, as data represents values attributed to parameters, and knowledge signifies understanding of real things or abstract concepts. As it regards data, the information's existence is not necessarily coupled to an observer (it exists beyond an event horizon, for example), while in the case of knowledge, the information requires a cognitive observer.\n\nInformation is conveyed either as the content of a message or through direct or indirect observation. That which is perceived can be construed as a message in its own right, and in that sense, information is always conveyed as the content of a message.\n\nInformation can be encoded into various forms for transmission and interpretation (for example, information may be encoded into a sequence of signs, or transmitted via a signal). It can also be encrypted for safe storage and communication.\n\nInformation reduces uncertainty. The uncertainty of an event is measured by its probability of occurrence and is inversely proportional to that. The more uncertain an event, the more information is required to resolve uncertainty of that event. The bit is a typical unit of information, but other units such as the nat may be used. For example, the information encoded in one \"fair\" coin flip is log(2/1) = 1 bit, and in two fair coin flips is\nlog(4/1) = 2 bits.\n\nThe concept that \"information is the message\" has different meanings in different contexts. Thus the concept of information becomes closely related to notions of constraint, communication, control, data, form, education, knowledge, meaning, understanding, mental stimuli, pattern, perception, representation, and entropy.\n\nThe English word apparently derives from the Latin stem (\"information-\") of the nominative (\"informatio\"): this noun derives from the verb \"informare\" (to inform) in the sense of \"to give form to the mind\", \"to discipline\", \"instruct\", \"teach\". \"Inform\" itself comes (via French \"informer\") from the Latin verb \"informare\", which means to give form, or to form an idea of. Furthermore, Latin itself already contained the word \"informatio\" meaning concept or idea, but the extent to which this may have influenced the development of the word \"information\" in English is not clear.\n\nThe ancient Greek word for \"form\" was (\"morphe\"; cf. morph) and also εἶδος (\"eidos\") \"kind, idea, shape, set\", the latter word was famously used in a technical philosophical sense by Plato (and later Aristotle) to denote the ideal identity or essence of something (see Theory of Forms). 'Eidos' can also be associated with thought, proposition, or even concept.\n\nThe ancient Greek word for \"information\" is , which transliterates (\"plērophoria\") from \nπλήρης (\"plērēs\") \"fully\" and φέρω (\"phorein\") frequentative of (\"pherein\") \"to carry through\". It literally means \"bears fully\" or \"conveys fully\". In modern Greek the word is still in daily use and has the same meaning as the word \"information\" in English. In addition to its primary meaning, the word as a symbol has deep roots in Aristotle's semiotic triangle. In this regard it can be interpreted to communicate information to the one decoding that specific type of sign. This is something that occurs frequently with the etymology of many words in ancient and modern Greek where there is a very strong denotative relationship between the signifier, e.g. the word symbol that conveys a specific encoded interpretation, and the signified, e.g. a concept whose meaning the interpreter attempts to decode.\n\nIn English, “information” is an uncountable mass noun.\n\nIn information theory, \"information\" is taken as an ordered sequence of symbols from an alphabet, say an input alphabet χ, and an output alphabet ϒ. Information processing consists of an input-output function that maps any input sequence from χ into an output sequence from ϒ. The mapping may be probabilistic or deterministic. It may have memory or be memoryless.\n\nOften information can be viewed as a type of input to an organism or system. Inputs are of two kinds; some inputs are important to the function of the organism (for example, food) or system (energy) by themselves. In his book \"Sensory Ecology\" Dusenbery called these causal inputs. Other inputs (information) are important only because they are associated with causal inputs and can be used to predict the occurrence of a causal input at a later time (and perhaps another place). Some information is important because of association with other information but eventually there must be a connection to a causal input. In practice, information is usually carried by weak stimuli that must be detected by specialized sensory systems and amplified by energy inputs before they can be functional to the organism or system. For example, light is mainly (but not only, e.g. plants can grow in the direction of the lightsource) a causal input to plants but for animals it only provides information. The colored light reflected from a flower is too weak to do much photosynthetic work but the visual system of the bee detects it and the bee's nervous system uses the information to guide the bee to the flower, where the bee often finds nectar or pollen, which are causal inputs, serving a nutritional function.\n\nThe cognitive scientist and applied mathematician Ronaldo Vigo argues that information is a concept that requires at least two related entities to make quantitative sense. These are, any dimensionally defined category of objects S, and any of its subsets R. R, in essence, is a representation of S, or, in other words, conveys representational (and hence, conceptual) information about S. Vigo then defines the amount of information that R conveys about S as the rate of change in the complexity of S whenever the objects in R are removed from S. Under \"Vigo information\", pattern, invariance, complexity, representation, and information—five fundamental constructs of universal science—are unified under a novel mathematical framework. Among other things, the framework aims to overcome the limitations of Shannon-Weaver information when attempting to characterize and measure subjective information.\n\nInformation is any type of pattern that influences the formation or transformation of other patterns. In this sense, there is no need for a conscious mind to perceive, much less appreciate, the pattern. Consider, for example, DNA. The sequence of nucleotides is a pattern that influences the formation and development of an organism without any need for a conscious mind. One might argue though that for a human to consciously define a pattern, for example a nucleotide, naturally involves conscious information processing.\n\nSystems theory at times seems to refer to information in this sense, assuming information does not necessarily involve any conscious mind, and patterns circulating (due to feedback) in the system can be called information. In other words, it can be said that information in this sense is something potentially perceived as representation, though not created or presented for that purpose. For example, Gregory Bateson defines \"information\" as a \"difference that makes a difference\".\n\nIf, however, the premise of \"influence\" implies that information has been perceived by a conscious mind and also interpreted by it, the specific context associated with this interpretation may cause the transformation of the information into knowledge. Complex definitions of both \"information\" and \"knowledge\" make such semantic and logical analysis difficult, but the condition of \"transformation\" is an important point in the study of information as it relates to knowledge, especially in the business discipline of knowledge management. In this practice, tools and processes are used to assist a knowledge worker in performing research and making decisions, including steps such as:\n\n\nStewart (2001) argues that transformation of information into knowledge is critical, lying at the core of value creation and competitive advantage for the modern enterprise.\n\nThe Danish Dictionary of Information Terms argues that information only provides an answer to a posed question. Whether the answer provides knowledge depends on the informed person. So a generalized definition of the concept should be: \"Information\" = An answer to a specific question\".\n\nWhen Marshall McLuhan speaks of media and their effects on human cultures, he refers to the structure of artifacts that in turn shape our behaviors and mindsets. Also, pheromones are often said to be \"information\" in this sense.\n\nInformation has a well-defined meaning in physics. In 2003 J. D. Bekenstein claimed that a growing trend in physics was to define the physical world as being made up of information itself (and thus information is defined in this way) (see Digital physics). Examples of this include the phenomenon of quantum entanglement, where particles can interact without reference to their separation or the speed of light. Material information itself cannot travel faster than light even if that information is transmitted indirectly. This could lead to all attempts at physically observing a particle with an \"entangled\" relationship to another being slowed down, even though the particles are not connected in any other way other than by the information they carry.\n\nThe mathematical universe hypothesis suggests a new paradigm, in which virtually everything, from particles and fields, through biological entities and consciousness, to the multiverse itself, could be described by mathematical patterns of information. By the same token, the cosmic void can be conceived of as the absence of material information in space (setting aside the virtual particles that pop in and out of existence due to quantum fluctuations, as well as the gravitational field and the dark energy). Nothingness can be understood then as that within which no matter, energy, space, time, or any other type of information could exist, which would be possible if symmetry and structure break within the manifold of the multiverse (i.e. the manifold would have tears or holes).\n\nAnother link is demonstrated by the Maxwell's demon thought experiment. In this experiment, a direct relationship between information and another physical property, entropy, is demonstrated. A consequence is that it is impossible to destroy information without increasing the entropy of a system; in practical terms this often means generating heat. Another more philosophical outcome is that information could be thought of as interchangeable with energy. Toyabe et al. experimentally showed in nature that information can be converted into work. Thus, in the study of logic gates, the theoretical lower bound of thermal energy released by an \"AND gate\" is higher than for the \"NOT gate\" (because information is destroyed in an \"AND gate\" and simply converted in a \"NOT gate\"). Physical information is of particular importance in the theory of quantum computers.\n\nIn thermodynamics, information is any kind of event that affects the state of a dynamic system that can interpret the information.\n\nThe information cycle (addressed as a whole or in its distinct components) is of great concern to information technology, information systems, as well as information science. These fields deal with those processes and techniques pertaining to information capture (through sensors) and generation (through computation, formulation or composition), processing (including encoding, encryption, compression, packaging), transmission (including all telecommunication methods), presentation (including visualization / display methods), storage (such as magnetic or optical, including holographic methods), etc. Information does not cease to exist, it may only get scrambled beyond any possibility of retrieval (within information theory, see lossy compression; in physics, the black hole information paradox gets solved with the aid of the holographic principle).\n\nInformation visualization (shortened as InfoVis) depends on the computation and digital representation of data, and assists users in pattern recognition and anomaly detection.\nInformation security (shortened as InfoSec) is the ongoing process of exercising due diligence to protect information, and information systems, from unauthorized access, use, disclosure, destruction, modification, disruption or distribution, through algorithms and procedures focused on monitoring and detection, as well as incident response and repair.nalysis is the process of inspecting, transforming, and modelling information, by converting raw data into actionable knowledge, in support of the decision-making process.\n\nInformation quality (shortened as InfoQ) is the potential of a dataset to achieve a specific (scientific or practical) goal using a given empirical analysis method.\n\nInformation communication represents the convergence of informatics, telecommunication and audio-visual media & content.\n\nIt is estimated that the world's technological capacity to store information grew from 2.6 (optimally compressed) exabytes in 1986 – which is the informational equivalent to less than one 730-MB CD-ROM per person (539 MB per person) – to 295 (optimally compressed) exabytes in 2007. This is the informational equivalent of almost 61 CD-ROM per person in 2007.\n\nThe world’s combined technological capacity to receive information through one-way broadcast networks was the informational equivalent of 174 newspapers per person per day in 2007.\n\nThe world's combined effective capacity to exchange information through two-way telecommunication networks was the informational equivalent of 6 newspapers per person per day in 2007.\n\nAs of 2007, an estimated 90% of all new information is digital, mostly stored on hard drives.\n\nRecords are specialized forms of information. Essentially, records are information produced consciously or as by-products of business activities or transactions and retained because of their value. Primarily, their value is as evidence of the activities of the organization but they may also be retained for their informational value. Sound records management ensures that the integrity of records is preserved for as long as they are required.\n\nThe international standard on records management, ISO 15489, defines records as \"information created, received, and maintained as evidence and information by an organization or person, in pursuance of legal obligations or in the transaction of business\". The International Committee on Archives (ICA) Committee on electronic records defined a record as, \"a specific piece of recorded information generated, collected or received in the initiation, conduct or completion of an activity and that comprises sufficient content, context and structure to provide proof or evidence of that activity\".\n\nRecords may be maintained to retain corporate memory of the organization or to meet legal, fiscal or accountability requirements imposed on the organization. Willis expressed the view that sound management of business records and information delivered \"...six key requirements for good corporate governance...transparency; accountability; due process; compliance; meeting statutory and common law requirements; and security of personal and corporate information.\"\n\nMichael Buckland has classified \"information\" in terms of its uses: \"information as process\", \"information as knowledge\", and \"information as thing\".\n\nBeynon-Davies explains the multi-faceted concept of information in terms of signs and signal-sign systems. Signs themselves can be considered in terms of four inter-dependent levels, layers or branches of semiotics: pragmatics, semantics, syntax, and empirics. These four layers serve to connect the social world on the one hand with the physical or technical world on the other.\n\nPragmatics is concerned with the purpose of communication. Pragmatics links the issue of signs with the context within which signs are used. The focus of pragmatics is on the intentions of living agents underlying communicative behaviour. In other words, pragmatics link language to action.\n\nSemantics is concerned with the meaning of a message conveyed in a communicative act. Semantics considers the content of communication. Semantics is the study of the meaning of signs - the association between signs and behaviour. Semantics can be considered as the study of the link between symbols and their referents or concepts – particularly the way that signs relate to human behavior.\n\nSyntax is concerned with the formalism used to represent a message. Syntax as an area studies the form of communication in terms of the logic and grammar of sign systems. Syntax is devoted to the study of the form rather than the content of signs and sign-systems.\n\nNielsen (2008) discusses the relationship between semiotics and information in relation to dictionaries. He introduces the concept of lexicographic information costs and refers to the effort a user of a dictionary must make to first find, and then understand data so that they can generate information.\n\nCommunication normally exists within the context of some social situation. The social situation sets the context for the intentions conveyed (pragmatics) and the form of communication. In a communicative situation intentions are expressed through messages that comprise collections of inter-related signs taken from a language mutually understood by the agents involved in the communication. Mutual understanding implies that agents involved understand the chosen language in terms of its agreed syntax (syntactics) and semantics. The sender codes the message in the language and sends the message as signals along some communication channel (empirics). The chosen communication channel has inherent properties that determine outcomes such as the speed at which communication can take place, and over what distance.\n\nKenett, R.S. and Shmueli, G. (2016) Information Quality: The Potential of Data and Analytics to Generate Knowledge, John Wiley and Sons, Chichester, UK\n\n\n"}
{"id": "39758073", "url": "https://en.wikipedia.org/wiki?curid=39758073", "title": "International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics", "text": "International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics\n\nThe International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB) is a preeminent yearly scientific conference focused on machine learning and computational intelligence applied to bioinformatics and biostatistics.\n\nThe CIBB conferences are typically organized by members of the IEEE Computational Intelligence Society (IEEE CIS) and International Neural Network Society (INNS), among others. Their main themes are machine learning, data mining and computational intelligence algorithms applied to biological and biostatistical problems.\n\nThe CIBB conference originally started by the Italian professors Francesco Masulli (University of Genoa), Antonina Starita (University of Pisa), and Roberto Tagliaferri (University of Salerno) as a special session within other international conferences held in Italy: the 14 Italian Workshop on neural networks (WIRN 2004), the 6 International Workshop on Fuzzy Logic and Applications (WILF 2005), the 7 International Fuzzy Logic and Intelligent Technologies in Nuclear Science Conference on Applied Artificial Intelligence (FLINS 2006), and the 7 International Workshop on Fuzzy Logic and Applications (WILF 2007). Because of the broad participation of researchers to the CIBB special session at WILF 2007, which included 26 papers, the CIBB steering committee decided to turn CIBB into an autonomous conference starting with the 2008 edition in Vietri sul Mare (Italy).\n\nDuring their first editions, the CIBB conferences were organized and attended mainly by Italian researchers at various academic locations throughout Italy (Genoa, Palermo, Gargnano). As international audience and importance of the conference grew up, following conference editions moved outside Italy. The 2012 CIBB conference was held in Houston, Texas, in the United States; the 2013 CIBB meeting took place in Nice, in the Côte d'Azur of Southern France; and the 2014 edition occurred at the University of Cambridge, in Great Britain. After these three editions, the 12 CIBB conference came back to Southern Italy and was organized in Naples in September 2015.\n\nThe 13 CIBB conference was organized at University of Stirling, in Scotland (United Kingdom) in the first days September 2016, while the following CIBB conference was held at University of Cagliari, in Sardinia (Italy) in September 2017.\n\nThe CIBB 2018 edition happened at Universidade Nova de Lisboa in Almada, Portugal, in September 2018, and the CIBB 2019 will take place at Università di Bergamo, in Bergamo, Italy.\n\nThe CIBB scientific conferences usually attract scientists from all over the world.\nThe conference is a single track meeting that includes invited talks as well as oral and poster presentations of refereed papers. It usually lasts three days in September.\nThe conference traditionally includes some special sessions about the application of computational intelligence to specific aspects of biology (for example, the \"Special session on machine learning in health informatics and biological systems\" at CIBB 2018), and occasionally some tutorials.\n\nIn the 2011 conference edition in Gargnano, the scientific committee assigned a \"young researcher best paper award\".\n\nThe proceedings for CIBB conferences are usually published in the Springer Lecture Notes in Computer Science (LNCS) series, whereas a selection of the best papers undergo extended publication in international peer-reviewed journals such as BMC Bioinformatics.\n\nFuture:\n\nPast:\n\n\nConference website and call for papers:\n\nProceedings books:\n\nJournal supplements on BMC Bioinformatics:\n\nMap of the locations of the CIBB conferences: \n"}
{"id": "19563691", "url": "https://en.wikipedia.org/wiki?curid=19563691", "title": "Large Lakes Observatory", "text": "Large Lakes Observatory\n\nThe Large Lakes Observatory, located in Duluth, Minnesota, studies the major lakes of the world. The researchers focus on a variety of sciences including aquatic chemistry, geochemistry and paleoclimatology.\nThe \"Blue Heron\" is the LLO's research vessel.\n"}
{"id": "11692445", "url": "https://en.wikipedia.org/wiki?curid=11692445", "title": "Lee Sung-yang", "text": "Lee Sung-yang\n\nLee Sung-yang (, born 29 May 1922) is a Taiwanese entomologist. He was the subject of the 1975 BBC documentary \"The Insect World of Dr. Lee\". In Taiwan, he is referred to as the \"Taiwanese Jean Henri Fabre.\"\n\n"}
{"id": "1916241", "url": "https://en.wikipedia.org/wiki?curid=1916241", "title": "List of Austrian scientists", "text": "List of Austrian scientists\n\nThis is a list of Austrian scientists and scientists from the Austria of Austria-Hungary.\n\n\n\n\n\n\n\n\n"}
{"id": "8805851", "url": "https://en.wikipedia.org/wiki?curid=8805851", "title": "List of Welsh inventors", "text": "List of Welsh inventors\n\nThis is a list of people of Welsh origin who are recognised as innovators or inventors who have made notable contributions to technical or theoretical world advancements.\n\n\n"}
{"id": "41652083", "url": "https://en.wikipedia.org/wiki?curid=41652083", "title": "List of scientific demonstrations", "text": "List of scientific demonstrations\n\nThis is a list of scientific demonstrations used in educational demonstrations and popular science lectures.\n\n\n\n"}
{"id": "33346197", "url": "https://en.wikipedia.org/wiki?curid=33346197", "title": "List of sequenced fungi genomes", "text": "List of sequenced fungi genomes\n\nThis list of sequenced fungi genomes contains all the fungal species known to have publicly available complete genome sequences that have been assembled, annotated and published; draft genomes are not included, nor are organelle only sequences.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChytridiomycota includes fungi with spores that have flagella (zoospores) and are a sister group to more advanced land fungi that lack flagella. Several chytrid species are pathogens, but have not had their genomes sequenced yet.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "7196522", "url": "https://en.wikipedia.org/wiki?curid=7196522", "title": "Lowest common ancestor", "text": "Lowest common ancestor\n\nIn graph theory and computer science, the lowest common ancestor (LCA) of two nodes and in a tree or directed acyclic graph (DAG) is the lowest (i.e. deepest) node that has both and as descendants, where we define each node to be a descendant of itself (so if has a direct connection from , is the lowest common ancestor).\n\nThe LCA of and in is the shared ancestor of and that is located farthest from the root. Computation of lowest common ancestors may be useful, for instance, as part of a procedure for determining the distance between pairs of nodes in a tree: the distance from to can be computed as the distance from the root to , plus the distance from the root to , minus twice the distance from the root to their lowest common ancestor . In ontologies, the lowest common ancestor is also known as the least common subsumer.\n\nIn a tree data structure where each node points to its parent, the lowest common ancestor can be easily determined by finding the first intersection of the paths from and to the root. In general, the computational time required for this algorithm is where is the height of the tree (length of longest path from a leaf to the root). However, there exist several algorithms for processing trees so that lowest common ancestors may be found more quickly. Tarjan's off-line lowest common ancestors algorithm, for example, preprocesses a tree in linear time to provide constant-time LCA queries. In general DAGs, similar algorithms exist, but with super-linear complexity.\n\nThe lowest common ancestor problem was defined by , but were the first to develop an optimally efficient lowest common ancestor data structure. Their algorithm processes any tree in linear time, using a heavy path decomposition, so that subsequent lowest common ancestor queries may be answered in constant time per query. However, their data structure is complex and difficult to implement. Tarjan also found a simpler but less efficient algorithm, based on the union-find data structure, for computing lowest common ancestors of an offline batch of pairs of nodes.\n\nFurther simplifications were made by and .\n\nA variant of the problem is the dynamic LCA problem in which the data structure should be prepared to handle LCA queries intermixed with operations that change the tree (that is, rearrange the tree by adding and removing edges) This variant can be solved using O(logN) time for all modifications and queries. This is done by maintaining the forest using the dynamic trees data structure with partitioning by size; this then maintains a heavy-light decomposition of each tree, and allows LCA queries to be carried out in logarithmic time in the size of the tree.\n\nWithout preprocessing you can also improve the naïve online algorithm's computation time to O(log \"h\") by storing the paths through the tree using skew-binary random access lists, while still permitting the tree to be extended in constant time (Edward Kmett (2012)). This allows LCA queries to be carried out in logarithmic time in the height of the tree.\n\nWhile originally studied in the context of trees, the notion of lowest common ancestors can be defined for directed acyclic graphs (DAGs), using either of two possible definitions. In both, the edges of the DAG are assumed to point from parents to children.\n\n\nIn a tree, the lowest common ancestor is unique; in a DAG of nodes, each pair of nodes may have as much as LCAs , while the existence of an LCA for a pair of nodes is not even guaranteed in arbitrary connected DAGs.\n\nA brute-force algorithm for finding lowest common ancestors is given by : find all ancestors of and , then return the maximum element of the intersection of the two sets. Better algorithms exist that, analogous to the LCA algorithms on trees, preprocess a graph to enable constant-time LCA queries. The problem of \"LCA existence\" can be solved optimally for sparse DAGs by means of an algorithm due to .\n\nThe problem of computing lowest common ancestors of classes in an inheritance hierarchy arises in the implementation of object-oriented programming systems . The LCA problem also finds applications in models of complex systems found in distributed computing .\n\n\n\n"}
{"id": "14684966", "url": "https://en.wikipedia.org/wiki?curid=14684966", "title": "MacDowell–Mansouri action", "text": "MacDowell–Mansouri action\n\nThe MacDowell–Mansouri action (named after S. W. MacDowell and Freydoon Mansouri) is an action that is used to derive Einstein's field equations of general relativity.\n\n"}
{"id": "35178539", "url": "https://en.wikipedia.org/wiki?curid=35178539", "title": "Max Kleiber", "text": "Max Kleiber\n\nMax Kleiber (4 January 1893 – 5 January 1976) was a Swiss agricultural biologist, born and educated in Zurich, Switzerland.\n\nKleiber graduated from the Federal Institute of Technology as an Agricultural Chemist in 1920, earned the ScD degree in 1924, and became a private \"dozent\" after publishing his thesis \"The Energy Concept in the Science of Nutrition\".\n\nKleiber joined the Animal Husbandry Department of UC Davis in 1929 to construct respiration chambers and conduct research on energy metabolism in animals. Among his many important achievements, two are especially noteworthy. In 1932 he came to the conclusion that the ¾ power of body weight was the most reliable basis for predicting the basal metabolic rate (BMR) of animals and for comparing nutrient requirements among animals of different size. He also provided the basis for the conclusion that total efficiency of energy utilization is independent of body size. These concepts and several others fundamental for understanding energy metabolism are discussed in Kleiber's book, \"The Fire of Life\" published in 1961 and subsequently translated into German, Polish, Spanish, and Japanese.\n\nHe is credited with the description of the ratio of metabolism to body mass, which became Kleiber's law.\n\n"}
{"id": "5808936", "url": "https://en.wikipedia.org/wiki?curid=5808936", "title": "NAICS 22", "text": "NAICS 22\n\nNAICS 22 (Utilities Sector) is the section of the NAICS system in Canada, the United States and Mexico which deals specifically with utilities in those countries.\n\nThe utilities sector consists of establishments engaged in the provision of the following utility services: electric power, natural gas, steam supply, water supply, and sewage removal. Within this sector, the specific activities associated with the utility services provided vary by utility: electric power includes generation, transmission, and distribution; natural gas includes distribution; steam supply includes provision and/or distribution; water supply includes treatment and distribution; and sewage removal includes collection, treatment, and disposal of waste through sewer systems and sewage treatment facilities. \n\nExcluded from this sector are establishments primarily engaged in waste management services classified in Subsector 562, Waste Management and Remediation Services. These establishments also collect, treat, and dispose of waste materials; however, they do not use sewer systems or sewage treatment facilities. \n\n"}
{"id": "2470362", "url": "https://en.wikipedia.org/wiki?curid=2470362", "title": "Odontolite", "text": "Odontolite\n\nOdontolite, also called bone turquoise or fossil turquoise or occidental turquoise, is fossil bone or ivory that has been traditionally thought to have been altered by turquoise or similar phosphate minerals such as vivianite.\n\n"}
{"id": "17565442", "url": "https://en.wikipedia.org/wiki?curid=17565442", "title": "Q.E.D. (UK TV series)", "text": "Q.E.D. (UK TV series)\n\nQ.E.D. (\"quod erat demonstrandum\", Latin for \"that which was to be demonstrated\") was the name of a strand of BBC popular science documentary films which aired in the United Kingdom from 1982 to 1999.\n\nRunning in a half-hour peak-time slot on the BBC's primary mass-audience channel BBC1, the series had a more populist and general interest agenda than the long-running \"Horizon\" series which aired on the more specialist channel BBC2.\n\n\"Horizon\" could often be difficult for a scientific novice, requiring a modicum of background knowledge beyond the reaches of many viewers, so \"Q.E.D\". was a more approachable way of introducing scientific stories.\n\n\n\n"}
{"id": "26196283", "url": "https://en.wikipedia.org/wiki?curid=26196283", "title": "Recruitment (biology)", "text": "Recruitment (biology)\n\nIn biology, especially marine biology, recruitment occurs when a juvenile organism joins a population, whether by birth or immigration, usually at a stage whereby the organisms are settled and able to be detected by an observer. \n\nThere are two types of recruitment: closed and open.\n\nIn the study of fisheries, recruitment is \"the number of fish surviving to enter the fishery or to some life history stage such as settlement or maturity\".\n"}
{"id": "54578929", "url": "https://en.wikipedia.org/wiki?curid=54578929", "title": "Research Foundation - Flanders (FWO)", "text": "Research Foundation - Flanders (FWO)\n\nThe Research Foundation - Flanders (FWO; \"- Vlaanderen\") is a Belgian public research council, based in Brussels. It aims to support ground-breaking scientific research, especially in association with the universities and institutes of the Flemish Community, which include, among others, Ghent University, University of Leuven, and Free University of Brussels.\n\nTogether with the Fund for Scientific Research - Wallonia (F.R.S.-FNRS; ) for the French-speaking region, the FWO is a successor to the Belgian National Fund for Scientific Research (NFWO/FNRS), which had been founded in 1928. A self-governing organization, the Research Foundation - Flanders is located in Brussels and financed by the Flemish government, the federal government, and the national lottery, with further support coming from partner institutes and companies.\n\nThe FWO supports research in science, engineering, and the humanities through a variety of frameworks. It offers funding for doctoral and postdoctoral fellowships as well as grants, projects, and infrastructure. Moreover, the FWO funds international mobility and collaboration.\n\nTogether with a number of partners, it also bestows scientific prizes.\n\nThe FWO organises a total of 31 expert panels, 30 specialist and one interdisciplinary. These panels advise the Senate and the Board of Trustees on matters of funding.\n\n\n\n\nThe NWO also organises special committees to advise on support for international collaboration, research infrastructure, and other special mandates.\n\nThe FWO is a member of Science Europe and collaborates in many European research organisations.\n\nIn addition, it has created partnerships with numerous counterparts across the globe, such as the French Centre national de la recherche scientifique (CNRS), Dutch Netherlands Organisation for Scientific Research (NWO), Japan Society for the Promotion of Science, Polish Academy of Sciences, and National Natural Science Foundation of China.\n\n"}
{"id": "4153791", "url": "https://en.wikipedia.org/wiki?curid=4153791", "title": "Similarity heuristic", "text": "Similarity heuristic\n\nThe similarity heuristic is a psychological heuristic pertaining to how people make judgments based on similarity. More specifically, the similarity heuristic is used to account for how people make judgments based on the similarity between current situations and other situations or prototypes of those situations.\n\nAt its most basic level, the similarity heuristic is an adaptive strategy. The goal of the similarity heuristic is maximizing productivity through favorable experience while not repeating unfavorable experiences. Decisions based on how favorable or unfavorable the present seems are based on how similar the past was to the current situation.\n\nFor example, a person may use the similarity heuristic when deciding on a book purchase. If a novel has a plot similar to that of novels read and enjoyed or the author has a writing style similar to that of favored authors, the purchasing decision will be positively influenced. A book with similar characteristics to previously pleasurable books is likely to also be enjoyed, causing the person to decide to obtain it.\n\nThe similarity heuristic directly emphasizes learning from past experience. For example, the similarity heuristic has been observed indirectly in experiments such as phonological similarity tests. These tests observe how well a person can distinguish similar sounds from dissimilar ones based on a comparison to previously heard sounds. While not involving a decision making process characteristic to heuristics in general, these studies show a reliance on past experience and comparison to the current experience. In addition, the similarity heuristic has become a valuable tool in the field of economics and consumerism.\n\nThe similarity heuristic is very easy to observe in the world of business, both from a marketing standpoint and from the position of the consumer. People tend to let past experience shape their world view; thus, if something presents itself as similar to a good experience had in the past, it is likely that the individual will partake in the current experience. The reverse holds true for situations that have proven unfavorable. A very basic example of this concept is a person deciding to get a meal at a particular restaurant because it reminds them of a similar establishment.\n\nCompanies often use the similarity heuristic as a marketing strategy. For example, companies will often advertise their services as something similar to a successful competitor, but better — such a concept is evident in the motion picture industry. Trailers for upcoming films will promote the latest movie as being made by a particular director, citing said director's past film credentials. In effect, a similarity heuristic is created in an audience's mind; creating a similarity between the coming attraction and past successes will likely make people decide to see the upcoming film.\n\nAutomotive parts companies and their distributors and dealers leverage similarity heuristics when they interchange the term, \"OEM\" (original equipment manufacturer), and \"OE\" (original equipment). For example, the OE design specifications may ask for a certain durability factor, corrosion resistance, and material composition. The OEM realizes they can produce the same part less expensively and with possibly greater profit, if they do not adhere to all or most of the OE design specifications. By marketing their product as \"OEM\" against a well-known brand or product (e.g., Mercedes-Benz), they predict that enough customers will purchase their OEM product vs. the OE product. The converse happens when the OE factory (e.g., Mercedes-Benz) promotes their brand of a commodity product (e.g., anti-freeze/coolant, spark plugs, etc.) as superior or better quality than the commodity product.\n\nIn addition, the use of a reverse similarity heuristic can be a highly valuable marketing tool. For example, when Nintendo wished to launch its Nintendo Entertainment System (NES) in the United States, it did so in the middle of a video game depression; Atari had managed to make video games one of the least popular American pastimes. Initial showing of the NES were met poorly — clearly, a similarity heuristic was in place, and people had created biases against anything relating to interactive television gaming. Nintendo's goal, then, became the differentiation of their system from the past examples. Employing a dissimilarity heuristic, Nintendo managed to create enough of a gap from the former video game industry and market a successful product.\n\nSome professions, such as software developers, regularly utilize the similarity heuristic. For software developers, the similarity heuristic is utilized when performing debugging tasks. A software bug exhibits a set of symptoms indicating the existence of a problem. In general, similar symptoms are caused by similar types of programming errors. By comparing these symptoms with those of previously corrected software flaws, a developer is able to determine the most probable cause and take an effective course of action. Over time, a developer’s past experiences will allow their use of the similarity heuristic to be highly effective, quickly choosing the debugging approach that will likely reveal the problem’s source.\n\nProblem solving in general is benefited by the similarity heuristic. When new problems arise similar to previous problems, the similarity heuristic selects an approach that previously yielded favorable results. Even if the current problem is novel, any similarity to previous issues will help choose a proper course of action.\n\n\n"}
{"id": "5823146", "url": "https://en.wikipedia.org/wiki?curid=5823146", "title": "Society (journal)", "text": "Society (journal)\n\nSociety is a scientific journal that publishes discussions and research findings in the social sciences and public policy.\n\nIt was founded as \"Transaction: Social Science and Modern SOCIETY\" by Irving Louis Horowitz in 1962. It was published by Transaction Publishers for decades before being purchased by Springer. Its chief editor is Jonathan Imber.\n\n\"Society\" is abstracted and indexed in the Social Sciences Citation Index. According to the \"Journal Citation Reports\", the journal has a 2015 impact factor of 0.188.\n\n"}
{"id": "5973210", "url": "https://en.wikipedia.org/wiki?curid=5973210", "title": "Solubility table", "text": "Solubility table\n\nThe table below provides information on the variation of solubility of different substances (mostly inorganic compounds) in water with temperature, at 1 atmosphere pressure. Units of solubility are given in grams per 100 millilitres of water (g/100 ml), unless shown otherwise. The substances are listed in alphabetical order.\n\n\n"}
{"id": "22614305", "url": "https://en.wikipedia.org/wiki?curid=22614305", "title": "Space science in Estonia", "text": "Space science in Estonia\n\nThe cornerstone of the Estonian cosmological research is the Tartu Observatory which was founded in 1812. The observatory itself has a long tradition of studying galaxies and theoretically modeling the structure of the universe and its formation. Till today this facility is Estonia’s main research centre for astronomy and atmospheric physics, with fundamental research focusing on physics of galaxies, stellar physics and remote sensing of the Earth’s atmosphere and ground surface. The observatory has also played a vital role in catapulting the career of Jaan Einasto, one of the most famous and eminent Estonian astrophysicists and one of the discoverers of \"Dark Matter\" and of the cellular structure of the Universe.\n\nDuring the Cold war Estonia was associated with and active in the extensive space program of the USSR. In the early 1970s The first Soviet Saljut type space station was equipped with the Estonian built Mikron a shining night clouds observer device. Several upgrades of the device were in service till the mid 1980s till the introduction of a more advanced technology. In the mid 1980s a telespectrometer \"FAZA\" (also known as \"Phasa\") was constructed in Estonia for the Soviet orbital space station Mir. The \"FAZA\" had a 10 arc-sec field-of-view and operated at 340-2850 nm and was fitted outside the Kvant-2 module. The device was used for study of the atmosphere and pollutants.\n\nThe first FAZA which was shipped on orbit at the Baikonur cosmodrome to enter service in the station Saljut 7, crashed down along with the station a year later in South America causing an international scandal for the Soviet Union in the region. Several years later in 1991 a joint space flight conducted by the Soviet Union and Austria ended the service of \"FAZA\" as the device was retired from service.\n\nEstonia was the first Baltic State to sign a cooperation treaty with the European Space Agency in 2007. The Estonian Space Office coordinates with ESA within the country.\n\nIn 2015, Estonia joined the European Space Agency.\n\nAfter re-gaining independence in 1991 its space research has mainly focused on cosmology. Since the 2000 the Estonian industry is again involved with the space sector where various specializing has taken place. Many Estonian companies are involved in the production of antennas for ground station for satellite communication, which have also contributed to the Mars Express mission. Furthermore, one of the Estonian companies built a large antenna reflector back structure for an ESA 35 metres radio telescope in Australia, which tracked Mars Express on its way to the red planet.\nFor several years Estonian scientists have collaborated with the ESA program Gaia, which plans to launch a space probe in 2011 to measure the brightness and exact coordinates of millions of space objects, both in the Milky Way galaxy as well as in more distant galaxies. Estonian scientists have offered their advice on how to measure these objects by using spectrophotometry.\n\n\n\n\n"}
{"id": "16968370", "url": "https://en.wikipedia.org/wiki?curid=16968370", "title": "Spin-destruction collision", "text": "Spin-destruction collision\n\nIn atomic physics, a spin-destruction (or spin-disorientation) collision is a physical impact where the spin angular momentum of an atom is irretrievably scrambled.\n\nThis type of collision can be a significant spin relaxation mechanism for polarized alkali metal vapor. In particular, the relaxation rate of alkali metal atoms in SERF atomic magnetometers is dominated by spin-destruction collisions. \n"}
{"id": "1743941", "url": "https://en.wikipedia.org/wiki?curid=1743941", "title": "Starch gelatinization", "text": "Starch gelatinization\n\nStarch gelatinization is a process of breaking down the intermolecular bonds of starch molecules in the presence of water and heat, allowing the hydrogen bonding sites (the hydroxyl hydrogen and oxygen) to engage more water. This irreversibly dissolves the starch granule in water. Water acts as a plasticizer. \nThree main processes happen to the starch granule: granule swelling, crystal or double helical melting, and amylose leaching.\n\n\nThe gelatinization temperature of starch depends upon plant type and the amount of water present, pH, types and concentration of salt, sugar, fat and protein in the recipe, as well as starch derivatisation technology are used. Some types of unmodified native starches start swelling at 55 °C, other types at 85 °C. The gelatinization temperature of modified starch depends on, for example, the degree of cross-linking, acid treatment, or acetylation.\nGel temperature can also be modified by genetic manipulation of starch synthase genes. Gelatinization temperature also depends on the amount of damaged starch granules; these will swell faster. Damaged starch can be produced, for example, during the wheat milling process, or when drying the starch cake in a starch plant. There is an inverse correlation between gelatinization temperature and glycemic index. High amylose starches require more energy to break up bonds to gelatinize into starch molecules.\n\nGelatinization improves the availability of starch for amylase hydrolysis.\nSo gelatinization of starch is used constantly in cooking to make the starch digestible or to thicken/bind water in roux, sauce, or soup.\n\nGelatinized starch, when cooled for a long enough period (hours or days), will thicken (or gel) and rearrange itself again to a more crystalline structure; this process is called retrogradation. During cooling, starch molecules gradually aggregate to form a gel. The following molecular associations can occur: amylose-amylose, amylose-amylopectin, and amylopectin-amylopectin. A mild association amongst chains come together with water still embedded in the molecule network.\n\nDue to strong associations of hydrogen bonding, longer amylose molecules (and starch which has a higher amylose content) will form a stiff gel. Amylopectin molecules with longer branched structure (which makes them more similar to amylose), increases the tendency to form strong gels. High amylopectin starches will have a stable gel, but will be softer than high amylose gels.\n\nRetrogradation restricts the availability for amylase hydrolysis to occur which reduces the digestibility of the starch.\n\nPregelatinized starch is starch which has been cooked and then dried in the starch factory on a drum dryer or in an extruder making the starch cold-water-soluble. Spray dryers are used to obtain dry starch sugars and low viscous pregelatinized starch powder.\n\nA simple technique to study starch gelation is by using a Brabender Viscoamylograph. It is a common technique used by food industries to determine the pasting temperature, swelling capacity, shear/thermal stability, and the extent of retrogradation. Under controlled conditions, starch and distilled water is heated at a constant heating rate in a rotating bowl and then cooled down. The viscosity of the mixture deflects a measuring sensor in the bowl. This deflection is measured as viscosity in torque over time vs. temperature and recorded on the computer. The viscoamylograph provides the audience with the beginning of gelatinization, gelatinization maximum, gelatinization temperature, viscosity during holding, and viscosity at the end of cooling.\n\nDifferential scanning calorimetry (DSC) is another method industries use to examine properties of gelatinized starch. As water is heated with starch granules, gelatinization occurs, involving an endothermic reaction.\n\nThe initiation of gelatinization is called the T-onset. T-peak is the position where the endothermic reaction occurs at the maximum. T-conclusion is when all the starch granules are fully gelatinized and the curve remains stable.\n\n\n"}
{"id": "40485358", "url": "https://en.wikipedia.org/wiki?curid=40485358", "title": "Synergistes jonesii", "text": "Synergistes jonesii\n\nSynergistes jonesii is a species of bacteria, the type species of its genus. It is a rumen bacterium that degrades toxic pyridinediols. It is obligately anaerobic, gram-negative and rod-shaped.\n\n\n"}
{"id": "5209123", "url": "https://en.wikipedia.org/wiki?curid=5209123", "title": "TerraMax", "text": "TerraMax\n\nTerraMax is the trademark for autonomous/unmanned ground vehicle technology developed by Oshkosh Defense. Primary military uses for the technology are seen as reconnaissance missions and freight transport in high-risk areas so freeing soldiers from possible attacks, ambushes or the threat of mines and IEDs. The technology could also be used in civilian settings, such as autonomous snow clearing at airports.\n\nThe original TerraMax vehicle was based on Oshkosh’s 6x6 Medium Tactical Vehicle Replacement (MTVR) and this was entered in the 2004 and 2005 DARPA Grand Challenges. A 4x4 variant was subsequently developed for, and entered in, the 2007 DARPA Urban Challenge. Since then, Oshkosh has continued developing the technology and in addition to four MTVRs has fitted the technology to its Palletized Load System (PLS), Family of Medium Tactical Vehicles (FMTV). and MRAP All Terrain Vehicle (M-ATV).\n\nOshkosh actively commenced development of the TerraMax in mid-2003. In the 2004 DARPA Grand Challenge (March 13–14, 2004) Team TerraMax was one of only seven teams to successfully navigate the qualifying course, going on to manage 1.2 miles on the race course before being ‘confronted’ by an impassable bush. The 2004 Team TerraMax consisted of Oshkosh Truck (now Oshkosh Defense), University of Parma's Artificial Vision and Intelligent Systems Laboratory (VisLab), Teledyne Scientific Company, Auburn University, IBEO and Caterpillar. \nAt the time, Don Verhoff, Oshkosh's executive vice president of technology explained that: \"although design development may continue for years, the idea of a driverless convoy of defense vehicles to deliver supplies to the front line, never jeopardizing the welfare of a single driver, is closer than one might imagine.\"\n\nIn October 2005 a second-generation TerraMax was one of five vehicles to complete the 2005 DARPA Grand Challenge 132-mile desert course. Unofficial run time was 12 hours and 51 minutes, this outside of the 10-hour limit to qualify for the $2 million prize money. The 2005 Team TerraMax consisted of Oshkosh Truck, Rockwell Collins, University of Parma's Artificial Vision and Intelligent Systems Laboratory (VisLab) and several financial sponsors.\n\nContinuing development of TerraMax technologies, in January 2006 Oshkosh unveiled an unmanned version of its Palletized Load System (PLS) truck at the U.S. Army Tactical Wheeled Vehicle Component Technology Demonstrations in Yuma, Arizona. East.\n\nIn 2007 Team TerraMax competed in the DARPA Urban Challenge with a 4x4 MTVR.\nIn June 2010, Oshkosh Defense was awarded the U.S. Marine Corps Cargo UGV (CUGV) initiative contract. The contract was awarded by US Marine Corps Warfighting Laboratory (MCWL) and the Joint Ground Robotics Enterprise Robotics Technology Consortium. The first evaluation for the cargo UGV was completed in May 2011 at Fort Pickett.\n\nIn October 2010 at the Association of the United States Army (AUSA) Annual Meeting and Exposition at the Washington Convention Center, Oshkosh displayed an FMTV Load Handling System (LHS) variant fitted with TerraMax technology.\n\nIn May 2014 Oshkosh announced that to demonstrate capabilities for route-clearance missions it had integrated its TerraMax unmanned ground vehicle (UGV) technology onto an Oshkosh MRAP All-Terrain Vehicle (M-ATV). The vehicle was demonstrated in June 2014 at the Eurosatory defence exhibition in Paris where it was equipped with a mine roller and autonomously navigated a course that simulated military route clearance missions.\n\nThe company was awarded a contract to evaluate the CUGV as a route clearing UGV, an effort that saw three vehicles working together. The trials took place at Fort A.P. Hill, Virginia, in April 2015. Also in 2015, the US Army worked on requirements documentation for the introduction of leader-follower technology for the Oshkosh 10×10 PLS logistics and support vehicles, which would be used initially as a driver safety mechanism for functions such as braking.\nIn 2016, TerraMax-equipped vehicles took part in the Autonomous Ground Resupply Programme. Under the programme, the vehicles were expected to demonstrate the carrying of cargo from the point of embarkation to a forward-operating base (FOB) and then to a patrol base. \n\nOshkosh is also involved in a number of other autonomous vehicle projects in conjunction with partners that include TARDEC, Lockheed Martin, Robotic Research, and DCS.\n\nIn 2016, Oshkosh PLS vehicles took part in the Autonomous Ground Resupply Programme. Under the programme, the vehicles were expected to demonstrate the carrying of cargo from the point of embarkation to a forward-operating base (FOB) and then to a patrol base.\nOn 27 June 2018, as part of the Expedient Leader Follower (ExLF) programme, Oshkosh received USD49 million to integrate its autonomous technology onto PLS vehicles. The TARDEC-led programme was awarded through an Other Transaction Authority (OTA) contract and is intended to facilitate the transition to an official Program of Record. The programme is expected to last until the end of its contract term in March 2021. \n\nTen vehicles will be integrated with autonomous technology and will undergo government qualification and safety testing in early 2019. In early 2019 there will be another integration on 60 vehicles which will then be sent for the operational technical demonstration (OTD), which will be in early 2020. \n\nOshkosh began integrating an initial 10 autonomy kits upon the award for the engineering test, verification, validation, and ATEC Safety Release, and will integrate another 60 kits for fiscal year 2019 for OTDs. The OTDs will occur in the second quarter of fiscal year 2020. The contract holds an option to procure up to 150 autonomy kits.\n\nTerraMax Unmanned Ground Vehicle (UGV) technology is modular and designed to be integrated into any tactical wheeled vehicle. The components of TerraMax are located so as not to impede the combat requirements of soldiers, while maintaining the utility of the vehicle. The technology is fully incorporated into the brakes, steering, engine and transmission. Fitted vehicles retain the ability to be driver-operated. Fitted vehicles can be operated fully autonomously in any position in a convoy, or semi-autonomously to follow the path of the lead vehicle.\n\nThe TerraMax UGV package consists of Oshkosh’s Command Zone electronics, a sensor suite and an advanced operator control unit (OCU).\n\nCommand Zone is an integrated control and diagnostics system that is computer-controlled, electronics technology that operates and diagnoses all major vehicle networks. The backbone of the Command Zone system is advanced multiplexing technology.\n\nThe multimodal sensor suite consists of a high definition LIDAR (light detection and ranging) system, a wide dynamic range camera, a short wave infrared camera, four situational awareness cameras, 12 short range radar systems (providing 360° close-view) and three long range radar systems. The military grade global navigation satellite system (GNSS) along with map registration software technology ensures the operation of the system without a satellite signal where a global positioning system (GPS) navigation system is blocked or denied.\n\nThe vehicle mounted with TerraMax UGV technology is autonomously controlled by an operator control unit (OCU). It enables the operators to manipulate route information and look out or manage internal operations and status of autonomous systems over tactical data links. The tele-operated OCU shows the necessary information according to the priority and facilitates the fast input of commands to maintain the pace in convoy operations. The screen of OCU displays overhead imagery, automated routes and driving parameters. It gives feedback if it identifies an obstacle to continue, or changes track accordingly. The OCU allows a single operator to control one or more UGVs. The operator can control an UGV with infrequent monitoring of the unit.\n\nFrom an earlier statement that it desired one third of its fleet to be autonomous by 2015, the U.S. Army now believes that the technology for autonomous vehicles may be ready for the field by 2025. At an AUVSI symposium in Arlington, Virginia in October 2015, Dr Paul Rogers, director of the army's Tank Automotive Research Development and Engineering Center (TARDEC), said \"Autonomous convoy technology for the US army remains about 10 years away, depending on how the acquisition process moves, due to requirements, testing, and budgeting processes.\"\n\nThe TerraMax was featured in Series 19 of BBC Television series \"Top Gear\". It was featured against presenter James May in the Range Rover in an off-road challenge in the Nevada Automotive Test Center, Nevada, USA.\n\n\n"}
{"id": "27907340", "url": "https://en.wikipedia.org/wiki?curid=27907340", "title": "The Sadeian Woman and the Ideology of Pornography", "text": "The Sadeian Woman and the Ideology of Pornography\n\nThe Sadeian Woman and the Ideology of Pornography is a 1978 non-fiction book by Angela Carter. The book is a feminist re-appraisal of the work of the Marquis de Sade, who had been criticized by earlier feminist theorists such as Andrea Dworkin. Carter sees de Sade as being the first writer to see women as more than mere breeding machines, as more than just their biology and, as such, finds him liberating.\n"}
{"id": "3219785", "url": "https://en.wikipedia.org/wiki?curid=3219785", "title": "The Science Alliance (TV series)", "text": "The Science Alliance (TV series)\n\nThe Science Alliance was an educational television show which was produced and broadcast by TVOntario in 1981-82. The hosts were Rex Hagon and Judy Haladay.\n\nThe typical episode would feature the hosts demonstrating various aspect of the subject of the episode. In addition, a largely unseen narrator named Bryant would interrupt at pertinent points with a vignette called \"Bryant's Giants of Science\" which would tell the story of a figure in the history of science and his contribution to scientific knowledge.\n\n11 episodes were produced. They were:\n\n\nAll episodes were 15 minutes in length.\n"}
{"id": "862253", "url": "https://en.wikipedia.org/wiki?curid=862253", "title": "Thomas Cooke (machinist)", "text": "Thomas Cooke (machinist)\n\n\"This page is about the scientific instrument maker. For other persons named Thomas Cooke, see Thomas Cooke (disambiguation)\"\n\nThomas Cooke (8 March 1807 – 19 October 1868) was a British scientific instrument maker based in York. He founded T. Cooke & Sons, the scientific instrument company.\n\nThomas Cooke was born in Allerthorpe, near Pocklington, East Riding of Yorkshire, the son of James Cook (a shoemaker).\nHis formal education consisted of two years at an elementary school (possibly the school of John Whitaker, also of Allerthorpe), but he continued learning after this and he taught himself navigation and astronomy with the intention of becoming a sailor. His mother dissuaded him from that career and he became a teacher. He made such a success of being an impromptu teacher to the farmers’ sons of the Pocklington district, that only a year later he was able to open a village school at Bielby. He continued to teach others by day and learn himself by night, and soon moved his school from Bielby to Skirpenbeck.\n\nAt Skirpenbeck he met his future wife, who was one of his pupils, and five years his junior. Fifty years on she spoke of how her husband developed his brief rudimentary education into becoming a schoolmaster: “He first learned mathematics by buying an old volume from a bookstall with a spare shilling. He also got odd sheets, and read books about geometry and mathematics, before he could buy them; for he had very little to spare”.\n\nBut Cooke’s interest in mathematics and science was practical as well academic. He had also retained his interest in navigation and instruments, and while at Skirpenbeck he made his own first rudimentary telescope – grinding a lens by hand out of the bottom of a glass whisky tumbler, then mounting in into a frame that he soldered together from a piece of tin. In 1829 he moved to York and worked as a mathematics schoolmaster at the Rev. Schackley's School in Ogleforth, near York Minster. He also taught in various ladies' schools to increase his income.\n\nHis marriage to Hannah was to produce seven children, five of whom were boys. Two of these Charles Frederick (1836–98) and Thomas (1839–1919) subsequently joined him in the business he founded in 1836 at number 50 (now renumbered to 18) Stonegate, close to York Minster with the assistance of a loan of £100 from his wife's uncle.\n\nCooke studied optics and became interested in making telescopes, the first of which was a refracting telescope with the base of a tumbler shaped to form its lens. This led to his friends including John Phillips encouraging him to make telescopes and other optical devices commercially.\n\nIn 1837 he established his first optical business in a small shop at 50 Stonegate, York, and later moved to larger premises in Coney Street. He built his first telescope for William Gray. At that time, the excise tax on glass discouraged the making of refracting telescopes, which were usually imported from abroad. Cooke was thus one of the pioneers of making such telescopes in Britain.\n\nHe made more instruments and built his reputation. He was not only an optician but had mechanical abilities as well, and among other things, manufactured turret clocks for church towers. He founded the firm T. Cooke & Sons. In 1855 he moved to bigger premises, the Buckingham Works at Bishophill in York, where factory methods of production were first applied to optical instruments. He exhibited at the York Exhibition in 1866 demonstrating his three-wheeled, steam powered car which he claimed could carry 15 people at 15 mph for a distance of 40 miles.\n\nOne of his finest achievements was the construction of the 25-inch 'Newall' refractor for Robert Stirling Newall; sadly, Thomas died before seeing it completed. For some years the Newall was the largest refracting telescope in the world. On Newall's death it was donated to the Cambridge Observatory and finally moved in 1959 to Mount Penteli observatory in Penteli, Greece. He made a telescope for the Royal Observatory, also Greenwich and another for Prince Albert. The firm amalgamated with Troughton & Simms (London) to become Cooke, Troughton & Simms in 1922 and this later became part of Vickers, but still run by his sons Thomas & Frederick.\n\nThomas Cooke was succeeded by his sons, Thomas and Frederick. He is buried in York Cemetery.\n\n\n\n"}
{"id": "35215077", "url": "https://en.wikipedia.org/wiki?curid=35215077", "title": "War Against War", "text": "War Against War\n\nIn political philosophy and international relations especially in peace and conflict studies the concept of a war against war also known as war on war refers to the reification of armed conflicts.\n\nIf a work of Edmond Potonié-Pierre from 1877 already discusses the idea of reifying conflicts under the title \"la guerre à la guerre\" (\"the war against war\") in its modern acceptation the concept is formally coined in 1906 by William James in his essay . Though also used as a political slogan, it was a cornerstone of the ideology of both the European pre World War I pacifist and anti-war movements in the 20th century especially in its German version of \"Kriege dem Kriege\" from the eponymous two volume 1925 pamphlet of Ernst Friedrich which was largely translated across Europe. Prior to World War I the French labour union \"Confédération Générale du Travail\" pleaded its pacifism under the slogan \"guerre à la guerre!\" which was also the title of a poster campaign by which the French \"Association de la Paix par le Droit\" recalled the commitments taken by European powers at the Hague Convention of 1907.\n\nAlthough the slogan and concept (as the reification of armed conflicts) of \"war against war\" was the subject of an anti-military pleading in 1916 at the military tribunal of Neuchâtel the concept will also be developed by the French military with General Alexandre Percin, then War Minister, dedicating a comprehensive study to the subject entitled \"Guerre à la Guerre\". The latter work, which especially explores the operational and strategic dimension of the concept from the angle of military doctrine would become a seminal contribution to its academic establishment. At times defended by the Socialist International as in the 1923 discourse of labour unionist Edo Fimmen at the international congress for peace at the Hague the concept will remain politically neutral overall in the 20th century, being also claimed by the Christian democratic movement with Gérard Marier and Jean Godin considering nuclear disarmament a central contribution to the war on war as a Christian mission\n\nThe United Nations Educational, Scientific and Cultural Organization which constitution declares \"That since wars begin in the minds of men, it is in the minds of men that the defences of peace must be constructed\" describing a memetical conception of war, brought together - in 1982 - thirteen poets for a collective work entitled \"War on war : The poet's cry\"\". \n\nMore recently the concept was discussed by scholars Joshua Goldstein and Idriss Aberkane. As Aberkane defines it \"\"it permits us to ask, before the war disease reaches its terminal phase, \"has man domesticated war, or has war domesticated man?\"\". Based on the paradigm of Humberto Maturana and Francisco Varela for the description of autonomy and the works of Idries Shah in group psychology Aberkane's discussion of the concept consists of considering armed conflicts autopoietic and therefore dissipative systems which are parasitic and comparable to Richard Dawkins' notion of selfish gene hence \"selfish wars\". Conceptually Aberkane argues for a prescriptive use of peace and conflict studies, especially in a study on the defusing of the tensions in Xinjiang by which he defends the logical soundness of the concept of a \"war against war\" in five points:\nWhat Aberkane thus describes as \"erenologic positivism\" is transcribed in game theoretical vocabulary as that in international relations \"it is also not proven that a Pareto-optimal situation at the world level (or at a less formal level of describing diplomatic reality, a situation that may be reminiscent to the latter without its mathematical precision) could not be also overlapping with the Nash equilibrium of interacting individual interests.\"\"\n\nAt least two approaches to the war against war may be distinguished, the frontal opposition to war or Anti-war movement on the one side and the transcendent, post-war conception of William James' 'Moral Equivalent of war' positing, in the way of the UNESCO, that the only way to end conflicts is to make Humanity busy with more fascinating endeavors than wars. In his Nobel acceptance speech Martin Luther King further underlined that idea which would become the basis of the transcendentist school (e.g. Aberkane):\n\nVarious scholars (Suter 1986, Aberkane 2012, Roberts 2009) and politicians (Seiberling 1972) have thus advocated that peace profiteering should simply be made palatably larger than any possible war profiteering, thus transcending the war against war. However, the first generations of Peace-Industrial scholars have advocated frontal opposition to the military–industrial complex rather than its transcendent, voluntary metamorphosis into a globally benevolent yet very profitable peace-industrial complex.\n\nIn 1986 Keith D. Suter defended his Ph.D dissertation on \"Creating the political will necessary for achieving multilateral disarmament : the need for a peace-industrial complex\", which was further cited in 1995. The concept of a Peace-industrial complex had already been introduced as early as in 1969 in the U.S. Senate Committee on Government Operations. The original quote affirmed direct opposition to the military-industrial complex and was therefore not transcendent in nature: \"It is time for the United States to break the Huge military-industrial complex and begin in its stead a people and peace-industrial complex\". It received further citation throughout the 1970s. The notion appeared in the United States Congress House Committee on Science and Astronautics, Subcommittee on Science, Research, and Development in 1972 in a congressional address by US Rep. J. F. Seiberling (1972) \n\nSuter further defended that \"each country create a national Ministry for Peace\", which was contemporary to the creation of the Ministry for Peace Australia (MFPA) initiative and the Global Alliance for Ministries & Infrastructures for Peace (GAMIP). An Education for a Peace Industrial Complex conference (EPIC) is also mentioned in a 1984 issue of the Nuclear Times.\n\nIf Kofi J. Roberts explicitly called for the substitution of a military-industrial complex by a peace industrial complex which would enable the focusing on federal spendings on construction rather than destruction, Idriss J. Aberkane further defended the transcendent approach to the peace-industrial complex by calling it the \"military-industrial complex 2.0\" and thus neither the enemy nor the political complement to the military industrial complex but rather its natural, inevitable evolution on the account that investors (either institutional or private) will inevitably realize the larger profitability of construction over destruction. Aberkane also advocates the political viability of a peace-industrial complex by declaring that in the 21st century, what he calls \"weapons of mass construction\" will grant much larger political leverage, leadership and soft power than weapons of mass destruction.\n\n\n"}
