{"id": "14705454", "url": "https://en.wikipedia.org/wiki?curid=14705454", "title": "3C 212", "text": "3C 212\n\n3C 212 is a quasar located in the constellation Cancer.\n\n"}
{"id": "2228460", "url": "https://en.wikipedia.org/wiki?curid=2228460", "title": "A390 road", "text": "A390 road\n\nThe A390 is a road in Cornwall and Devon, England. It runs from Tavistock to north west of the city of Truro. Starting in Tavistock, it heads south-westwards towards Liskeard, crossing over the River Tamar and into Cornwall, then through Gunnislake and Callington. Immediately before Liskeard, it merges with the A38 north of the town. It diverges from the A38 at Dobwalls, where it then runs in a south-westerly direction to Truro via Lostwithiel, St. Blazey, and St Austell. It then forms an upside down elongated square loop, and bypasses Truro City Centre. It then heads north west out of the city, where it forms the main corridor into Truro from the west. It passes the Royal Cornwall Hospital and skirts the village of Threemilestone, before linking up with the A30 road at Chiverton Cross, where the A390 terminates. Chiverton Cross is where the A390 trunk road from Truro and the B3277 to St Agnes meet the east-west A30 trunk road. Before the A390 was rerouted away from Chacewater it ended at its junction with the A30 at Scorrier.\n\nIt forms part of the main route (the Truro to Dobwalls section) from Truro to Plymouth, although after a severe accident in Lostwithiel in 1999 heavy goods vehicles are diverted from it at the top of Edgcumbe Road, Lostwithiel.\n\nBy far the busiest section of the road is between Truro City Centre and Chiverton Cross, where it carries through traffic to West Cornwall,as well as commuting traffic to and from Truro, and traffic for the Royal Cornwall Hospital, Truro College and the Richard Lander School. There are many residential new housing estates in this area which have created further traffic. The section from Truro College heading into Truro from the west also includes a Bus lane and some dual carriageway sections.\n"}
{"id": "18737402", "url": "https://en.wikipedia.org/wiki?curid=18737402", "title": "Agustín Fernández Mallo", "text": "Agustín Fernández Mallo\n\nAgustín Fernández Mallo (A Coruña, 1967) is a physicist and Spanish writer. He lives in Palma de Mallorca. He is a member of the so-called Nocilla Generation.\n\nAlthough he works as a physicist, he also collaborates with cultural magazines such as \"Lateral\", \"Contrastes\", \"La Bolsa de Pipas\", \"La fábrica\" and \"Anónima\".\n\n\n\n\n\n\"Nocilla Dream\" and \"Nocilla Experience\" are part of a trilogy called Nocilla Project, which will be completed with \"Nocilla Lab\".\n\n"}
{"id": "42502861", "url": "https://en.wikipedia.org/wiki?curid=42502861", "title": "Alternative explanations of the \"grandmother\" cell", "text": "Alternative explanations of the \"grandmother\" cell\n\nThe presumed grandmother cell responds to a seemingly impossible large number of stimuli. If the stimulus is a photo of your grandmother looking straight into the camera, the cell fires. If the stimulus is a painting of your grandmother in profile, the same cell fires. If the stimulus is a sketch of your grandmother walking away, drawn by a street artist, the same cell fires.\n\nThis neuron activates when a person identifies a specific entity, such as his or her grandmother. These neurons would activate not only when presented with a family member, but also when presented with celebrities such as Halle Berry. The term was coined by Jerry Lettvin around 1969. A similar concept had been proposed a few years earlier by Jerzy Konorski who called such cells gnostic units. Since grandmother cells are highly controversial it was very surprising when Kreiman announced in 2001 that he had discovered one. Since then, several explanations of such findings have been proposed which do not involve grandmother cells. One explanation is based on sparse coding and another on neural nets.\n\nA neuron normally fires when excited by one very specific feature of a stimulus. In a study performed by Logothetis and Pauls, this is seen with the spiking of a single macaque monkey neuron when presented with a paper clip bent in a certain way at a variety of angles. At a specific angle, the neuron fires rapidly. At different angles or shapes, it does not. A grandmother cell, by contrast, would seem to fire at many different shapes and angles. And it is very difficult to imagine what kind of mechanism \"will bind ... information from disparate sensory sources\" in a cell, which makes a grandmother cell \"a tremendous connectivity puzzle\".\n\nIn 2001, Kreiman announced that he had found a grandmother cell. This \"neuron in the human amygdala, for example, fires to a caricature, a portrait, and a group photo of President Clinton, but not to 47 other pictures of famous men, presidents, and a variety of objects\". In 2004, Koch affirmed \"such cells do exist\". In 2005, Quiroga and coworkers obtained similar results. However, in 2007, Quiroga, Kreiman, Koch and Fried rescinded their initial views, trying to explain their results with sparse coding.\n\nSparse coding starts with the activation of moderately small sets of neurons in a small region of the brain. In each region of the brain, related stimuli have a different subset of available neurons to process them together. This reduces the overlap of the representation of two items. Each item is individually represented by a different neuron or a small set of neurons. This helps represent data more efficiently.\n\nIn a recent article Quiroga, Kreiman, Koch and Fried admitted they had in fact not found grandmother cells, rather they had found sparse coding. The main question was how information was represented in the upper stages of the hierarchy, and still made accessible to perceptual, cognitive and mnemonic processes. This is why sparse coding became an alternative to grandmother cells. The idea of sparse coding is that very small numbers of neurons respond to specific features, objects or concepts in an obvious manner.\n\nBeing able to find one cell would be a difficult task. It seems implausible that grandmother cells have any extraordinary binding properties. They are the properties of sparse coding instead. It should also be noted that it is difficult to detect sparse firing neurons. Single electrode recordings with moveable probes are used to detect firing, but it is common to miss firing cells.\n\nWaydo and Koch conducted a study where a two layered network was exposed to 40 different face images of ten individuals. Each output unit was constrained to fire to the smallest possible number of inputs. Also, the smallest number of units represented each image. They found that most of the units responded to a single individual. This suggests that a sparse neuronal representation could emerge in the Medial Temporal Lobe (MTL) while using unsupervised learning.\n\nSparse coding often takes place in the hippocampus. The medial temporal lobe is critical for long term memory, particularly episodic memories. Both the hippocampus and the cortex may interact to produce sparse coding. This helps with numerous tasks, including word, object and facial identification. \nSparse representations are also associated with parallel distributed processing which proves to have great success with rapid learning. It does not have success with generalization, though. Compared to grandmother cells, an individual neuron in sparse coding is involved in coding two things, and two neurons code for a specific stimulus.\n\nOverall, visual information goes through the ventral pathway. Evidence that was obtained from recordings of single-neuron activity in humans suggests that a subset of MTL neurons presents an invariant representation of perceived objects. This means that the neurons respond to abstract concepts rather than more basic metric details. This information favors sparse coding over grandmother cells, because the neurons fire only to very few stimuli, and are mostly silent with the exception of their preferred stimuli.\n\nGrandmother cells can be explained in a much simpler way. As Munevar proposes, grandmother cells are in fact the output cells of neural nets rather than cells acting independently. The existence of grandmother cells clashes with the model of the brain as a distributive system and is improbable because such neurons would have powers of representation across visuals angles and contexts. The distributive explanation argues that the grandmother cell is nothing but the single-neuron output stage of a neural network trained to recognize a person. The hypothetical grandmother cell does not have any extraordinary binding properties. They are properties of the neural network instead.\n\nAlthough Kreiman's and Quiroga's results may seem to challenge the distributive model of the brain, the distributive theory offers a simple explanation of these phenomena. Such cells serve as the output neurons for neural networks trained to recognize the famous persons in question.\n\nPaul Churchland's accessible account of a network that distinguishes between sonar echoes of explosive underwater mines vs. those of rocks provides us with a better understanding of this idea. Churchland's work is based on the work of Gorman and Sejnowski. The network becomes trained as feedback is delivered to its hidden units. The feedback includes the error of the network in deciding whether every echo in the training set belongs to a mine or a rock.\n\nThe important factor here is that the recognition is done not by the output cell (\"mine\") but by the middle (hidden) layer. When that middle layer recognizes a mine echo, the single-cell output layer \"lights up\", which is exactly the sort of thing we find in those networks that recognize Clinton, Aniston or Berry.\n\nThis account extends beyond the discrimination between two alternatives (mines and rocks) to that between different faces. Therefore, the distributive brain may have a single-cell output layer fire whenever a specific person, e.g. your grandmother, is presented to the eye; although many famous people, such as Jennifer Aniston or Halle Berry, may be of greater interest. Neural networks, then, explain very plausibly the otherwise paradoxical results of the recent grandmother cell experiments. Neural nets are a form of parallel distributed processing.\n\nIn 1950 Frank Rosenblatt looked into a new approach in neural net technology. He used a circuit consisting of an array of input units. These units are connected through a set of intermediate neurons. In Rosenblatt's trials, he used binary words in both input and output, and he called his invention Perception. Perception was the most basic and revolutionary type of neural net. In 1969, the limitations of Perception were explained by Minsky and Papert. Neural nets have been expanded by a variety of neuroscientists and psychologists. Churchland discusses neural nets in the context of facial coding. The example consisted of a bank teller who could not remember the face of a bank robber to provide a description. However, Churchland says the teller would likely recognize and discriminate the robber's face when she sees him again. This example contains not only the idea of neural nets, but also of vector coding.\n\nCottrell, in research at the University of California at San Diego, developed the artificial neural network which had a 64 X 64 pixel grid with 256 activation levels. The network consisted of recognizable representations of real faces. Each input cell had a radiating set of axonal end branches to every cell in the middle layer. The middle layer is hidden and projects the coding from 80 cells to the eight output cells. These eight output cells could discriminate faces from non-faces just like grandmother cells are thought to do. Cottrell had no idea how synaptic connections should be configured in the artificial neural network. Cottrell overcame this issue by using a biologically realistic adjustment.\n\nThe Artificial Neural Network (ANN) was initially trained with 64 images, but was eventually pushed outside the training boundary. In a training session the facial recognition was performing at 100%. When the ANN performed facial recognition testing outside of training, the network was still performing at 100%. Even when one-fifth of the face was covered with a black strip the face was still 100% effective as long as the strip did not cover the forehead. The ANN proves another way in which a small number of cells can seem to do a big number of tasks. Previously, grandmother cells supposedly have a unique ability to complete these tasks. The input layer is variously activated by 80 holistic features which vector codes for the second layer. This vector code is an activation which allows the third layer to identify a known individual correctly. This is also the case in parallel distributed processing (PDP). PDP highlights the parallels between connectionist models and neural coding in the brain, while also dismissing localism.\n\nChurchland explains that neural nets contain three basic levels. The levels are the input layer, hidden layer and output layer. Munevar, following the PDP theory, believes that output cells can explain the presumed grandmother cells. PDP involves the performance of hundreds of millions of individual computation simultaneously, instead of in a lengthy sequence. It takes 10 milliseconds for a visual impulse to go across a sign layer to layer transformation, and reaches the 3rd layer where buried information has been made explicit.\n\nIn PDP, grandmother cells are a thing to ridicule. Finkel called them infamous grandmother cells, and Connor said no one wants to be accused of believing in grandmother cells. 50 years of studies involving PDP and grandmother cells have not really found any grandmother cells, but rather that the brain codes information with a form of distributed coding. Localist models ignore the neuroscience data taken to support distributed coding schemes, specifically single cell electrophysiological recording studies. Localist models are instead often supported by neuropsychological data. Biologically, the PDP model is superior and can be supported using the many different methods and studies. This includes studies of PDP in the brain, as discussed by Bowers.\n\n"}
{"id": "39639449", "url": "https://en.wikipedia.org/wiki?curid=39639449", "title": "Arquerite", "text": "Arquerite\n\nArquerite is a naturally occurring alloy of silver with mercury. It is a very rare mineral, consisting of a silver-rich variety of amalgam, containing about 87% silver and 13% mercury. Arquerite has been reported from only four localities worldwide, two are in Chile and the other two are in British Columbia, Canada. Other names for arquerite include, argental mercury, mercurian silver, and silver amalgam.\n\n"}
{"id": "576120", "url": "https://en.wikipedia.org/wiki?curid=576120", "title": "Balzan Prize", "text": "Balzan Prize\n\nThe International Balzan Prize Foundation awards four annual monetary prizes to people or organizations who have made outstanding achievements in the fields of humanities, natural sciences, culture, as well as for endeavours for peace and the brotherhood of man.\n\nEach year the foundation chooses the fields eligible for the next year's prizes, and determines the prize amount. These are generally announced in May, with the winners announced the September the following year. Since 2001 the prize money has increased to 1 million Swiss Francs per prize, on condition that half the money is used for projects involving young researchers.\n\nThe Balzan Prize committee comprises twenty members of the prestigious learned societies of Europe.\n\nThe assets behind the foundation were established by the Italian Eugenio Balzan (1874–1953), a part-owner of \"Corriere della Sera\" who had invested his assets in Switzerland and in 1933 had left Italy in protest against fascism. He left a substantial inheritance to his daughter Angela Lina Balzan (1892–1956), who at the time was suffering an incurable disease. Before her death, she left instructions for the foundation and since then it has two headquarters, the Prize administered from Milan, the Fund from Zurich.\n\nThe first award was in fact 1 million Swiss francs to the Nobel foundation in 1961. After 1962 a gap of 16 years followed when prizes recommenced with an award of half a million Swiss francs to Mother Teresa. Award ceremonies alternate between Bern and the Accademia dei Lincei in Rome, and frequently winners have later won a Nobel Prize.\n\nThe amount of each of the four Balzan Prizes is now 750,000 Swiss francs (approx. €700,000; $790,000; £500,000).\n\nAll awards are decided by a single committee.\n\nFour prizes have been awarded annually since 1978. The award fields vary each year and can be related to either a specific or an interdisciplinary field. The prizes go beyond the traditional subjects both in the humanities (literature, the moral sciences and the arts) and in the sciences (medicine and the physical, mathematical and natural sciences), with an emphasis on innovative research.\n\nEvery 3 to 7 years the foundation also awards the \"Prize for humanity, peace and brotherhood among peoples\". It was last awarded in 2014 to Vivre en Famille.\n\n\n"}
{"id": "45689615", "url": "https://en.wikipedia.org/wiki?curid=45689615", "title": "Bloomberg Distinguished Professorships", "text": "Bloomberg Distinguished Professorships\n\nBloomberg Distinguished Professorships (BDPs) were established as part of a $350 million investment by Michael Bloomberg, JHU Class of 1964, to Johns Hopkins University in 2013. Fifty faculty members, ten from Johns Hopkins University and forty recruited from institutions worldwide, will be chosen for these endowed professorships based on their research, teaching, service, and leadership records. The program is directed and managed by Johns Hopkins University Vice Provost for Research, Dr. Denis Wirtz.\n\nThe BDPs will create interdisciplinary connections and collaborations across Johns Hopkins University, train and mentor undergraduate and graduate students, and strengthen the university's leadership in research fields of international interest. Each of the BDPs will be appointed in at least two divisions or disciplines.\n\nAs of May 2018, thirty three of the fifty Bloomberg Distinguished Professorships have been announced.\n\nThe first cohort of scholars in 2014 included two Nobel Laureates, Peter Agre and Carol W. Greider, and poverty researcher, Kathryn Edin. Sociologist Stephen Morgan, neuroscientist Patricia Janak, and organization theorist Kathleen Sutcliffe, were announced as the second group of BDPs in June 2014. In March 2015, it was announced that biomedical informatics expert Christopher G. Chute of the Mayo Clinic and infectious disease specialist Arturo Casadevall of the Albert Einstein College of Medicine would be joining Johns Hopkins University as BDPs. Two internally selected professors were also named BDPs: big data scientist Alex Szalay and computational biologist Steven Salzberg. In July 2015, four new Bloomberg Distinguished Professors were named across five divisions of Johns Hopkins University: global food ethicist and nutritionist Jessica Fanzo, biophysicist Taekjip Ha, cell dynamics investigator Rong Li, and computer vision specialist Alan Yuille.\n\nIn October 2015, Paul Ferraro, an environmental economist was announced as a BDP and in November, it was announced that noted statistician Nilanjan Chatterjee would be joining Johns Hopkins as a BDP from the National Cancer Institute. Experimental astrophysicist and cosmologist Charles L. Bennett and epigeneticist Andrew Feinberg were named BDPs in December 2015. In February 2016, it was announced that diabetes and obesity expert Rexford Ahima of the University of Pennsylvania was joining Johns Hopkins as a BDP and in April 2016 Mauro Maggioni of Duke University announced that he was joining Johns Hopkins as a BDP. In May 2016, it was announced that computational biologist Michael Schatz of the Cold Spring Harbor Laboratory was named the 21st Bloomberg Professor at Johns Hopkins. Nobel Laureate in Physics Adam Riess, expert on chromatin biology and biochemistry Carl Wu, and neuroscientist Ulrich Mueller were announced as BDPs in summer 2016. In December 2016, heath equity expert was named the 25th BDP, marking the halfway point of the program.\n"}
{"id": "8145133", "url": "https://en.wikipedia.org/wiki?curid=8145133", "title": "Bordigism", "text": "Bordigism\n\nBordigism is a variant of left communism espoused by Marxist Amadeo Bordiga. He was a founder of the Communist Party of Italy and a prominent figure in the International Communist Party. Bordigists in the Italian Socialist Party would be the first to refuse on principle any participation in parliamentary elections.\n\n"}
{"id": "2461422", "url": "https://en.wikipedia.org/wiki?curid=2461422", "title": "Christoph Gottfried Andreas Giebel", "text": "Christoph Gottfried Andreas Giebel\n\nChristoph Gottfried Andreas Giebel (13 September 1820 – 14 November 1881) was a German zoologist and palaeontologist.\n\nGiebel was born on 13 September 1820 in Quedlinburg, Prussian Saxony, and educated at the University of Halle, where he graduated in 1845 with a Ph.D.. At Halle his instructors were Ernst Friedrich Germar and Hermann Burmeister. In 1858 he became professor of zoology and director of the museum there. He died on 14 November 1881 at Halle.\n\nGiebel's chief publications were \"Palaeozoologie\" (1846); \"Fauna der Vorwelt\" (1847-1856); \"Deutschlands Petrefacten\" (1852); \"Odontographie\" (1855); \"Lehrbuch der Zoologie\" (1857); \"Thesaurus ornithologiae\" (1872-1877). His 5-volume \"Naturgeschichte des Tierreichs\" (1859–1864) is considered to be a forerunner to \"Brehms Tierleben\". With Wilhelm Heinrich Heintz, he was editor of the \"Zeitschrift für Naturwissenschaften\".\n\nHe is the taxonomic author of the extinct fish genera \"Asima\" (1848), \"Elonichthys\" (1848), and \"Tharsis\" (1847).\n\n\n"}
{"id": "45067549", "url": "https://en.wikipedia.org/wiki?curid=45067549", "title": "Circe effect", "text": "Circe effect\n\nThe Circe effect is a phenomenon proposed by William Jencks seen in chemistry and biochemistry where in order to speed up a reaction, the ground state of the substrate is destabilized by an enzyme.\n\nHighly favourable binding of a substrate at a non-reactive site will force the reactive site of the substrate to be more reactive by putting it in a very unfavourable position. This effect was observed in orotidine 5'-phosphate decarboxylase. This can occur by positioning a charged amino acid group next to the charged substrate thus destabilizing it, thus making the reaction occur faster. Furthermore, the substrate is put into a very optimal position by the enzyme for the reaction to occur, thus decreasing the entropy greatly.\n\nThis process was named after Circe in Homer's \"Odyssey\", who lured men and turned them into pigs.\n"}
{"id": "1536286", "url": "https://en.wikipedia.org/wiki?curid=1536286", "title": "Depth conversion", "text": "Depth conversion\n\nDepth conversion is an important step of the seismic reflection method, which converts the acoustic wave travel time to actual depth, based on the acoustic velocity of subsurface medium (sediments, rocks, water).\n\nDepth conversion integrates several sources of information about the suburface velocity to derive a three-dimensional velocity model:\n\nThe conversion permits the production of depth and thickness maps that depict subsurface layers that are based on reflection data. These maps are crucial in hydrocarbon exploration because they permit the volumetric evaluation of gas or oil in place. In the example subsurface map presented below, depth increases from red to blue. The highest zone in red is an oilfield at approximately 3000 m below sea level.\n\n\n"}
{"id": "8930508", "url": "https://en.wikipedia.org/wiki?curid=8930508", "title": "EVA (benchmark)", "text": "EVA (benchmark)\n\nEVA was a continuously running benchmark project for assessing the quality and value of protein structure prediction and secondary structure prediction methods. Methods for predicting both secondary structure and tertiary structure - including homology modeling, protein threading, and contact order prediction - were compared to results from each week's newly solved protein structures deposited in the Protein Data Bank. The project aimed to determine the prediction accuracy that would be expected for non-expert users of common, publicly available prediction webservers; this is similar to the related LiveBench project and stands in contrast to the bi-yearly benchmark CASP, which aims to identify the maximum accuracy achievable by prediction experts.\n\n\n"}
{"id": "20845760", "url": "https://en.wikipedia.org/wiki?curid=20845760", "title": "Eilat stone", "text": "Eilat stone\n\nEilat Stone derives its name from the city of Eilat where it was once mined, it is a green-blue inhomogeneous mixture of several secondary copper minerals including malachite, azurite, turquoise, pseudomalachite, chrysocolla. The Eilat stone is the National stone of Israel, and is also known as the King Solomon Stone.\n"}
{"id": "6338699", "url": "https://en.wikipedia.org/wiki?curid=6338699", "title": "Embodied cognitive science", "text": "Embodied cognitive science\n\nEmbodied cognitive science is an interdisciplinary field of research, the aim of which is to explain the mechanisms underlying intelligent behavior. It comprises three main methodologies: 1) the modeling of psychological and biological systems in a holistic manner that considers the mind and body as a single entity, 2) the formation of a common set of general principles of intelligent behavior, and 3) the experimental use of robotic agents in controlled environments.\n\nEmbodied cognitive science borrows heavily from embodied philosophy and the related research fields of cognitive science, psychology, neuroscience and artificial intelligence. From the perspective of neuroscience, research in this field was led by Gerald Edelman of the Neurosciences Institute at La Jolla, the late Francisco Varela of CNRS in France, and J. A. Scott Kelso of Florida Atlantic University. From the perspective of psychology, research by Michael Turvey, Lawrence Barsalou and Eleanor Rosch. From the perspective of language acquisition, Eric Lenneberg and Philip Rubin at Haskins Laboratories. From the perspective of autonomous agent design, early work is sometimes attributed to Rodney Brooks or Valentino Braitenberg. From the perspective of artificial intelligence, see \"Understanding Intelligence\" by Rolf Pfeifer and Christian Scheier or \"How the body shapes the way we think\", also by Rolf Pfeifer and Josh C. Bongard. From the perspective of philosophy see Andy Clark, Shaun Gallagher, and Evan Thompson.\n\nTuring proposed that a machine may need a human-like body to think and speak:\n\nEmbodied cognitive science is an alternative theory to cognition in which it minimizes appeals to computational theory of mind in favor of greater emphasis on how an organism's body determines how and what it thinks. Traditional cognitive theory is based mainly around symbol manipulation, in which certain inputs are fed into a processing unit that produces an output. These inputs follow certain rules of syntax, from which the processing unit finds semantic meaning. Thus, an appropriate output is produced. For example, a human's sensory organs are its input devices, and the stimuli obtained from the external environment are fed into the nervous system which serves as the processing unit. From here, the nervous system is able to read the sensory information because it follows a syntactic structure, thus an output is created. This output then creates bodily motions and brings forth behavior and cognition. Of particular note is that cognition is sealed away in the brain, meaning that mental cognition is cut off from the external world and is only possible by the input of sensory information.\n\nEmbodied cognitive science differs from the traditionalist approach in that it denies the input-output system. This is chiefly due to the problems presented by the Homunculus argument, which concluded that semantic meaning could not be derived from symbols without some kind of inner interpretation. If some little man in a person's head interpreted incoming symbols, then who would interpret the little man's inputs? Because of the specter of an infinite regress, the traditionalist model began to seem less plausible. Thus, embodied cognitive science aims to avoid this problem by defining cognition in three ways.\n\nThe first aspect of embodied cognition examines the role of the physical body, particularly how its properties affect its ability to think. This part attempts to overcome the symbol manipulation component that is a feature of the traditionalist model. Depth perception for instance can be better explained under the embodied approach due to the sheer complexity of the action. Depth perception requires that the brain detect the disparate retinal images obtained by the distance of the two eyes. In addition, body and head cues complicate this further. When the head is turned in a given direction, objects in the foreground will appear to move against objects in the background. From this, it is said that some kind of visual processing is occurring without the need of any kind of symbol manipulation. This is because the objects appearing to move the foreground are simply appearing to move. This observation concludes then that depth can be perceived with no intermediate symbol manipulation necessary. \n\nA more poignant example exists through examining auditory perception. Generally speaking the greater the distance between the ears, the greater the possible auditory acuity. Also relevant is the amount of density in between the ears, for the strength of the frequency wave alters as it passes through a given medium. The brain's auditory system takes these factors into account as it process information, but again without any need for a symbolic manipulation system. This is because the distance between the ears for example does not need symbols to represent it. The distance itself creates the necessary opportunity for greater auditory acuity. The amount of density between the ears is similar, in that it is the actual amount itself that simply forms the opportunity for frequency alteration. Thus under consideration of the physical properties of the body, a symbolic system is unnecessary and an unhelpful metaphor.\n\nThe second aspect draws heavily from George Lakoff's and Mark Johnson's work on concepts. They argued that humans use metaphors whenever possible to better explain their external world. Humans also have a basic stock of concepts in which other concepts can be derived from. These basic concepts include spatial orientations such as up, down, front, and back. Humans can understand what these concepts mean because they can directly experience them from their own bodies. For example, because human movement revolves around standing erect and moving the body in an up-down motion, humans innately have these concepts of up and down. Lakoff and Johnson contend this is similar with other spatial orientations such as front and back too. As mentioned earlier, these basic stocks of spatial concepts are the basis in which other concepts are constructed. Happy and sad for instance are seen now as being up or down respectively. When someone says they are feeling down, what they are really saying is that they feel sad for example. Thus the point here is that true understanding of these concepts is contingent on whether one can have an understanding of the human body. So the argument goes that if one lacked a human body, they could not possibly know what up or down could mean, or how it could relate to emotional states.\n\n‘[I]magine a spherical being living outside of any gravitational field, with no\nknowledge or imagination of any other kind of experience. What could UP\npossibly mean to such a being?'\n\nWhile this does not mean that such beings would be incapable of expressing emotions in other words, it does mean that they would express emotions differently from humans. Human concepts of happiness and sadness would be different because human would have different bodies. So then an organism's body directly affects how it can think, because it uses metaphors related to its body as the basis of concepts.\n\nA third component of the embodied approach looks at how agents use their immediate environment in cognitive processing. Meaning, the local environment is seen as an actual extension of the body's cognitive process. The example of a personal digital assistant (PDA) is used to better imagine this. Echoing functionalism (philosophy of mind), this point claims that mental states are individuated by their role in a much larger system. So under this premise, the information on a PDA is similar to the information stored in the brain. So then if one thinks information in the brain constitutes mental states, then it must follow that information in the PDA is a cognitive state too. Consider also the role of pen and paper in a complex multiplication problem. The pen and paper are so involved in the cognitive process of solving the problem that it seems ridiculous to say they are somehow different from the process, in very much the same way the PDA is used for information like the brain. Another example examines how humans control and manipulate their environment so that cognitive tasks can be better performed. Leaving one's car keys in a familiar place so they aren't missed for instance, or using landmarks to navigate in an unfamiliar city. Thus, humans incorporate aspects of their environment to aid in their cognitive functioning.\n\nThe value of the embodiment approach in the context of cognitive science is perhaps best explained by Andy Clark. He makes the claim that the brain alone should not be the single focus for the scientific study of cognition\n\nIt is increasingly clear that, in a wide variety of cases, the individual brain should not be the sole locus of cognitive scientific interest. Cognition is not a phenomenon that can be successfully studied while marginalizing the roles of body, world and action.\n\nThe following examples used by Clark will better illustrate how embodied thinking is becoming apparent in scientific thinking.\n\n\"Thunnus\", or tuna, long baffled conventional biologists with its incredible abilities to accelerate quickly and attain great speeds. A biological examination of the tuna shows it is simply not capable of such feats. However, an answer can be found when taking the tuna's embodied state into account. The bluefin tuna is able to take advantage of and exploit its local environment by finding naturally occurring currents to increase its speed. The tuna also uses its own physical body for this end as well, by utilizing its tailfin to create the necessary vortices and pressure so it can accelerate and maintain high speeds. Thus, the bluefin tuna is actively using its local environment for its own ends through the attributes of its physical body.\n\nClark uses the example of the hopping robot constructed by Raibert and Hodgins to demonstrate further the value of the embodiment paradigm. These robots were essentially vertical cylinders with a single hopping foot. The challenge of managing the robot's behavior can be daunting because in addition to the intricacies of the program itself, there were also the mechanical matters regarding how the foot ought to be constructed so that it could hop. An embodied approach makes it easier to see that in order for this robot to function, it must be able to exploit its system to the fullest. That is, the robot's systems should be seen as having dynamic characteristics as opposed to the traditional view that it is merely a command center that just executes actions.\n\nClark distinguishes between two kinds of vision, animate and pure vision. Pure vision is an idea that is typically associated with classical artificial intelligence, in which vision is used to create a rich world model so that thought and reason can be used to fully explore the inner model. In other words, pure vision passively creates the external perceivable world so that the faculties of reason can be better used introspectively. Animate vision, by contrast, sees vision as the means by which real-time action can commence. Animate vision is then more of a vehicle by which visual information is obtained so that actions can be undertaken. Clark points to animate vision as an example of embodiment, because it uses both biological and local environment cues to create an active intelligent process. Consider the Clark's example of going to the drugstore to buy some Kodak film. In one's mind, one is familiar with the Kodak logo and its trademark gold color. Thus, one uses incoming visual stimuli to navigate around the drugstore until one finds the film. Therefore, vision should not be seen as a passive system but rather an active retrieval device that intelligently uses sensory information and local environmental cues to perform specific real-world actions.\n\nInspired by the work of the American psychologist James J. Gibson, this next example emphasizes the importance of action-relevant sensory information, bodily movement, and local environment cues. These three concepts are unified by the concept of affordances, which are possibilities of action provided by the physical world to a given agent. These are in turn determined by the agent's physical body, capacities, and the overall action-related properties of the local environment as well. Clark uses the example of an outfielder in baseball to better illustrate the concept of affordance. Traditional computational models would claim that an outfielder attempting to catch a fly-ball can be calculated by variables such as the running speed of the outfielder and the arc of the baseball. However, Gibson's work shows that a simpler method is possible. The outfielder can catch the ball so long as they adjust their running speed so that the ball continually moves in a straight line in their field of vision. Note that this strategy uses various affordances that are contingent upon the success of the outfielder, including their physical body composition, the environment of the baseball field, and the sensory information obtained by the outfielder. \n\nClark points out here that the latter strategy of catching the ball as opposed to the former has significant implications for perception. The affordance approach proves to be non-linear because it relies upon spontaneous real-time adjustments. On the contrary, the former method of computing the arc of the ball is linear as it follows a sequence of perception, calculation and performing action. Thus, the affordance approach challenges the traditional view of perception by arguing against the notion that computation and introspection are necessary. Instead, it ought to be replaced with the idea that perception constitutes a continuous equilibrium of action adjustment between the agent and the world. Ultimately Clark does not expressly claim this is certain but he does observe the affordance approach can explain adaptive response satisfactorily. This is because they utilize environmental cues made possible by perceptual information that is actively used in the real-time by the agent.\n\nIn the formation of general principles of intelligent behavior, Pfeifer intended to be contrary to older principles given in traditional artificial intelligence. The most dramatic difference is that the principles are applicable only to situated robotic agents in the real world, a domain where traditional artificial intelligence showed the least promise.\n\nPrinciple of Cheap Design and Redundancy: Pfeifer realized that implicit assumptions made by engineers often substantially influence a control architecture's complexity. This insight is reflected in discussions of the scalability problem in robotics. The internal processing needed for some bad architectures can grow out of proportion to new tasks needed of an agent. \nOne of the primary reasons for scalability problems is that the amount of programming and knowledge engineering that the robot designers have to perform grows very rapidly with the complexity of the robot's tasks. There is mounting evidence that pre-programming cannot be the solution to the scalability problem ... The problem is that programmers introduce too many hidden assumptions in the robot's code.\nThe proposed solutions are to have the agent exploit the inherent physics of its environment, to exploit the constraints of its niche, and to have agent morphology based on parsimony and the principle of Redundancy. Redundancy reflects the desire for the error-correction of signals afforded by duplicating like channels. Additionally, it reflects the desire to exploit the associations between sensory modalities. (See redundant modalities). In terms of design, this implies that redundancy should be introduced with respect not only to one sensory modality but to several. It has been suggested that the fusion and transfer of knowledge between modalities can be the basis of reducing the size of the sense data taken from the real world. This again addresses the scalability problem.\n\nPrinciple of Parallel, Loosely-coupled Processes: An alternative to hierarchical methods of knowledge and action selection. This design principle differs most importantly from the Sense-Think-Act cycle of traditional AI. Since it does not involve this famous cycle, it is not affected by the frame problem.\n\nPrinciple of Sensory-Motor Coordination: Ideally, internal mechanisms in an agent should give rise to things like memory and choice-making in an emergent fashion, rather than being prescriptively programmed from the beginning. These kinds of things are allowed to emerge as the agent interacts with the environment. The motto is, build fewer assumptions into the agent's controller now, so that learning can be more robust and idiosyncratic in the future. \n\nPrinciple of Ecological Balance: This is more a theory than a principle, but its implications are widespread. Its claim is that the internal processing of an agent cannot be made more complex unless there is a corresponding increase in complexity of the motors, limbs, and sensors of the agent. In other words, the extra complexity added to the brain of a simple robot will not create any discernible change in its behavior. The robot's morphology must already contain the complexity in itself to allow enough \"breathing room\" for more internal processing to develop.\n\nThe Value Principle: This was the architecture developed in the Darwin III robot of Gerald Edelman. It relies heavily on connectionism.\n\nA traditionalist may argue that objects may be used to aid in cognitive processes, but this does not mean they are part of a cognitive system. Eyeglasses are used to aid in the visual process, but to say they are a part of a larger system would completely redefine what is meant by a visual system. However, supporters of the embodied approach could make the case that if objects in the environment play the functional role of mental states, then the items themselves should not be counted among the mental states.\n\n\n"}
{"id": "45575339", "url": "https://en.wikipedia.org/wiki?curid=45575339", "title": "Equate Scotland", "text": "Equate Scotland\n\nEquate Scotland is an organisation established to promote and encourage the advancement of women in science, engineering, technology and the built environment. It is based at Edinburgh Napier University in Scotland.\n\nEquate began in 2006 as the \"Scottish Resource Centre for Women in Science Engineering and Technology\" and was the delivery agent in Scotland for the former \"UK Resource Centre for Women in Science Engineering and Technology\" [UKRC]. When the UKRC's contract with the UK Government expired in 2012, the Scottish Government agreed to fund the Scottish organisation.\n\nIn 2014, the Scottish Resource Centre was re-branded to \"Equate Scotland\", to \n\"increase awareness of their expertise in gender equality and diversity in these sectors. It will also lead to a clearly defined focus for supporting women and assist employers and academia in science, engineering and technology across Scotland recruit and retain talented female staff\" (Alex Salmond, then First Minister of Scotland).\n\nThe current Project Director of Equate Scotland is Talat Yaqoob. She took up this post in January 2016.\n\nEquate Scotland is funded by the Scottish Government, receiving £601,000 between 2012-15 to support the recruitment and retention of women in science engineering, technology and the built environment.\n\nOther funders have included the Construction Industry Training Board, Edinburgh Napier University and the Big Lottery Fund.\n\nEquate Scotland runs a variety of workshops and events, including:\n\nEquate also contributes to Scottish Government and Scottish Parliament consultations and inquiries.\n"}
{"id": "26405426", "url": "https://en.wikipedia.org/wiki?curid=26405426", "title": "Great Mambo Chicken and the Transhuman Condition", "text": "Great Mambo Chicken and the Transhuman Condition\n\nGreat Mambo Chicken and the Transhuman Condition is a non-fiction book copyright 1990 by Ed Regis, an American author and educator, that presents a lighthearted look at scientific visionaries planning for a future with \"post-biological\" people, space colonization, nanotechnology, and cryonics. The book emphasizes the personality and projects of Robert Truax, Eric Drexler, Gerard K. O'Neill, Chris Langton, Freeman Dyson, Hans Moravec, Ralph Merkle, Robert Forward, Keith Henson, Carolyn Meinel, Gary Hudson, Saul Kent, and a number of others.\n"}
{"id": "51085842", "url": "https://en.wikipedia.org/wiki?curid=51085842", "title": "HD 164595", "text": "HD 164595\n\nHD 164595 is a G-type star located in the constellation of Hercules, from Earth that is notably similar to the Sun. With an apparent magnitude of 7.075, the star can be found with binoculars or a small telescope in the constellation Hercules.\n\nHD 164595 has one known planet, , which orbits HD 164595 every 40 days. It was detected with the radial velocity technique with the SOPHIE echelle spectrograph. The planet has a minimal mass equivalent of 16 Earths.\n\nThe star has the same stellar classification as the Sun: G2V. It has a similar temperature, at compared with for the Sun. It has a lower logarithm of metallicity ratio, at −0.06 compared with 0.00, and a slightly younger age, at 4.5 versus 4.6 billion years.\n\nIn 2016, HD 164595 briefly attracted media attention after it was reported that a possible SETI signal had been detected from the direction of the star in the previous year. The signal was only heard once and never confirmed by other telescopes, and is thought to have been due to terrestrial interference.\n\nOn 15 May 2015, a brief, single radio signal at 11 GHz (2.7 cm wavelength) was observed in the direction of HD 164595 by a team led by N. N. Bursov involving Claudio Maccone at the RATAN-600 radio observatory. The signal may be due to terrestrial radio-frequency interference or gravitational lensing from a more distant source. It was observed only once (for two seconds), by a single team, at a single telescope, giving it a Rio Scale score of 1 (insignificant) or 2 (low). Discussions in the media from 29 August 2016 onwards featured speculation that the signal could be caused by an isotropic beacon from a Type II civilization.\n\nThe senior astronomer of the SETI Institute, Seth Shostak, stated that confirmation by another telescope is required. Astronomer Nicholas Suntzeff of Texas A&M University stated that the signal is in a military frequency band, and that it could have been a satellite downlink, implying that some such systems may be kept secret and therefore would be unknown to SETI scientists.\n\nSETI and METI studies followed with the Allen Telescope Array and the Boquete Optical SETI Observatory. Also, scientists at Berkeley SETI Research Center at the University of California, Berkeley observed HD 164595 using the Green Bank Telescope as part of the Breakthrough Listen program. No signal was detected at the position and frequency of the transient reported by the RATAN group.\n\nThe Special Astrophysical Observatory of the Russian Academy of Sciences has since released an official statement that the signal is of a \"most probable terrestrial origin\".\n\n"}
{"id": "9459497", "url": "https://en.wikipedia.org/wiki?curid=9459497", "title": "Henning Kagermann", "text": "Henning Kagermann\n\nHenning Kagermann (born 12 July 1947 in Braunschweig) is a German physicist and businessman. He was the former chairman of the Executive Board and Chief Executive Officer of SAP AG.\n\nKagermann studied physics in Braunschweig and Munich. He received his doctorate degree in theoretical physics in 1975 from the TU Braunschweig, Germany, and was promoted to professor there in 1980. He taught physics and computer science at TU Braunschweig and University of Mannheim, both in Germany, from 1980-1992. Kagermann also received an honorary doctorate from the University of Magdeburg, Germany, and is a trustee of the Technical University of Munich. He is also a member of the honorary senate of the Foundation Lindau Nobel Laureate Meetings.\n\nIn June 2009, Kagermann assumed the office of the president of acatech – German Academy of Science and Engineering.\n\nKagermann joined SAP in 1982 and was initially responsible for product development in the areas of cost accounting and controlling.\n\nTogether with Hasso Plattner, co-founder of SAP, he was co-chairman of the SAP Executive Board and CEO From 1998 to 2003. Following Plattner's election as chairman of the SAP Supervisory Board in May 2003, Kagermann became sole chairman of the SAP Executive Board and CEO until he retired in May 2009. Kagermann had overall responsibility for SAP's strategy, business development and also oversaw the areas of global communications, global intellectual property, internal audit, and talent management.\n\nKagermann is currently a member of the supervisory boards of Deutsche Bank AG and Münchener Rückversicherungs-Gesellschaft AG (Munich Re) in Germany, and in 2007 was made a member of the board of directors of Nokia Corporation. In October 2009, WIPRO also appointed Kagermann to its board of directors.\n\nIn May 2012, Kagermann became member of the Haniel board.\n\nSince May 2010, Kagermann has been chairing the German National Platform for Electric Mobility. \n\nKagermann is also known as one of the main proponents of the concept of Industry 4.0, which applies increased digitization and the Internet of Things and Services to industrial production. In 2012, a working group led by Kagerman proposed the idea to the Federal Government of Germany.\n\nIn 2016, Kagermann was appointed by Federal Minister of Transport and Digital Infrastructure Alexander Dobrindt to serve on the German government’s Ethics Commission on Autonomous Driving.\n\n"}
{"id": "38869556", "url": "https://en.wikipedia.org/wiki?curid=38869556", "title": "IAU (1976) System of Astronomical Constants", "text": "IAU (1976) System of Astronomical Constants\n\nThe International Astronomical Union at its XVIth General Assembly in Grenoble in 1976, accepted (Resolution No. 1\na whole new consistent set of astronomical constants \nrecommended for reduction of astronomical observations, and for computation of ephemerides. It superseded the IAU's previous recommendations of 1964 (see IAU (1964) System of Astronomical Constants), became in effect in the Astronomical Almanac from 1984 onward, and remained in use until the introduction of the IAU (2009) System of Astronomical Constants. In 1994\nthe IAU recognized that the parameters became outdated, but retained the 1976 set for sake of continuity, but also recommended to start maintaining a set of \"current best estimates\"\nthis \"sub group for numerical standards\" had published a list, which included new constants (like those for relativistic time scales)\n\nThe system of constants was prepared\nby Commission 4 on ephemerides led by P. Kenneth Seidelmann (after whom asteroid 3217 Seidelmann is named).\n\nAt the time, a new standard epoch (J2000.0) was accepted; followed later\nby a new reference system with fundamental catalogue (FK5), and expressions for precession of the equinoxes,\nand in 1979 by new expressions for the relation between Universal Time and sidereal time\n\n, and in 1979 and 1980 by a theory of nutation\n. There were no reliable rotation elements for most planets, but a joint working group on Cartographic Coordinates and Rotational Elements was installed to compile recommended values\n\nThe IAU(1976) system is based on the astronomical system of units:\n\nIAU commission 4: ,\n"}
{"id": "47908321", "url": "https://en.wikipedia.org/wiki?curid=47908321", "title": "ISO 16610", "text": "ISO 16610\n\nISO 16610: Geometrical product specifications (GPS) – Filtration is a standard series on filters for surface texture, and provides guidance on the use of these filters in various applications.\nFilters are used in surface texture in order reduce the bandwidth of analysis in order to obtain functional correlation with physical phenomena such as friction, wear, adhesion, etc. For example, filters are used to separate roughness and waviness from the primary profile, or to create a multiscale decomposition in order to identify the scale at which a phenomenon occurs. \nHistorically, the first roughness measuring instruments - stylus profilometer - used to have electronic filters made of capacitors and resistors that filtered out low frequencies in order to retain frequencies that represent roughness. Later, digital filters replaced analog filters and international standards such as ISO 11562 for the Gaussian filter were published.\n\nToday, a full set of filters is described in the ISO 16610 standard series. This standard is part of the GPS standards on Geometrical Product Specification and Verification, developed by ISO TC 213.\n\nISO 16610 is composed of two families of documents, one for profiles (open and closed) and one for surfaces. A general introduction is provided in:\n\nProfile filters are defined for open profiles, measured along a line by profilometers and expressed as z=f(x), as well as for closed profiles, measured around a circular component by roundness instruments and expressed as radius=f(angle). Most of these standards were first published as a Technical Specification (TS) and later converted to International Standards or withdrawn.\n\nParts related to profile filters are:\n\nNote: ISO/TS 16610-32 on robust spline filters was published as a technical specification in 2009 but was withdrawn in 2015 as it provides very similar results as the Robust Gaussian regression filter while being much more complex.\n\nAreal filters are defined for surfaces measured either by lateral scanning instruments or optical profilometers. \nParts related to areal filters are:\n\nThe following section describes which application is suitable for each filter. References to published papers or books are provided when available. Readers are encouraged to add below proven applications related to surface texture and tribology where a particular filter can be used alone or in conjunction with other treatments or analyses to provide significant results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "30775948", "url": "https://en.wikipedia.org/wiki?curid=30775948", "title": "Indian Economic and Social History Review", "text": "Indian Economic and Social History Review\n\nThe Indian Economic and Social History Review is an academic journal of Indian economic history. It is published by Sage Publications. The founding editor-in-chief was Tapan Raychaudhuri, who was succeeded by Dharma Kumar. The current editors-in-chief are Sunil Kumar and Sanjay Subrahmanyam.\n\nThe journal is a member of the Committee on Publication Ethics (COPE).\n\n\"Indian Economic and Social History Review\" is abstracted and indexed in:\n\n"}
{"id": "16344330", "url": "https://en.wikipedia.org/wiki?curid=16344330", "title": "Khazzoom–Brookes postulate", "text": "Khazzoom–Brookes postulate\n\nIn the 1980s, the economists Daniel Khazzoom and Leonard Brookes independently put forward ideas about energy consumption and behavior that argue that increased energy efficiency paradoxically tends to lead to increased energy consumption. In 1992, the US economist Harry Saunders dubbed this hypothesis the Khazzoom–Brookes postulate, and showed that it was true under neo-classical growth theory over a wide range of assumptions.\n\nIn short, the postulate states that \"energy efficiency improvements that, on the broadest considerations, are economically justified at the microlevel, lead to higher levels of energy consumption at the macrolevel.\" This idea is a more modern analysis of a phenomenon known as the Jevons Paradox. In 1865, William Stanley Jevons observed that England's consumption of coal increased considerably after James Watt introduced his improvements to the steam engine. Jevons argued that increased efficiency in the use of coal would tend to increase the demand for coal, and would not reduce the rate at which England's deposits of coal were running out.\n\nLike Jevons Paradox, the Khazzoom-Brookes Postulate is a deduction that is largely counter-intuitive as an efficiency paradox. When individuals change behavior and begin to use methods and devices that are more energy efficient, there are cases where, on a macro-economic level, energy usage actually increases.\" The effect of higher energy prices, either through taxes or producer-induced shortages, initially reduces demand but in the longer term encourages greater energy efficiency. This efficiency response amounts to a partial accommodation of the price rise and thus the reduction in demand is blunted. The end result is a new balance between supply and demand at a higher level of supply and consumption than if there had been no efficiency response.\"\n\nIncreased energy efficiency can increase energy consumption by three means. Firstly, increased energy efficiency makes the use of energy relatively cheaper, thus encouraging increased use. Secondly, increased energy efficiency leads to increased economic growth, which pulls up energy use in the whole economy. Thirdly, increased efficiency in any one bottleneck resource multiplies the use of all the companion technologies, products and services that were being restrained by it. One simple example is that suburban development limited by water use can be doubled if the houses adopt water efficiency measures that cut their water demand in half. That way a small efficiency can have large opposite multiplier effect. Similarly cars that use less fuel are likely to cause matching increases in the number of cars and trips and companion travel activities rather than a decrease in energy demand. It appears that these latent multipliers of opposite effects may be generally greater than the linear result of the original effect. As of late 2008 this appears to not have been factored into the general discussion of sustainability and global warming mitigation strategies.\n\nThe work done by Khazzoom and Brookes began after the OPEC oil crises of 1973 and 1979, when demand for more fuel-efficient automobiles began to rise. Although greater fuel efficiency was achieved for each automobile on average, overall consumption has continued to increase. \"The OPEC oil shocks spawned huge improvements in energy efficiency, particularly insofar as oil was concerned. But three decades later, we find that the net effect of all of those efficiency initiatives has been to increase the world’s appetite for crude. While oil per unit of GDP has fallen impressively in large energy-consuming economies like the United States, total oil consumption, and indeed, total energy consumption, continue to grow by leaps and bounds. The increase in energy usage has dwarfed the gains in economic efficiency. Hence, instead of capping energy demand, what we observe is that improvements in energy efficiency lead to ever and ever-greater levels of energy usage\", or, rather, that improvements in energy efficiency were associated with increased energy usage. Many of the increases that can be seen from empirical data might as well have taken place without efficiency gains, possibly leading to even larger increases.\n\nFurther important considerations are the potentials and limits of the efficiency multiplier effect, considering efficiency as a kind of complex system learning process. At the beginning of the learning curve efficiency and productivity improvements get physically easier to achieve and then later improvement slows as the difficulty of learning increases and the practically achievable level of efficiencies is reached. In market systems the investor choices may be driven by physical benefits or financial ones independently, so they may conflict. Promoting efficiencies that accelerate the depletion of resource necessities may raise their monetary value by increasing scarcity, and successively decreasing physical returns on investment EROEI. Accelerating toward terminal limits of resource utility is a form of tragedy of the commons following the equivalent of a maximum rate of depletion rather than a maximum longevity or utility principle.\n\nThe rebound effect will usually be larger if energy costs make up a large share of the total costs of a given product or its consumption, and will also depend on demand elasticities. For example, fuel efficiency of cars will lead to increased mileage to a larger extent than visits to restaurants would be increased by a better energy efficiency at restaurants (e.g. for cooking, fridges, heating): Energy costs make up a smaller amount of total costs for restaurants and will therefore influence their prices to a smaller extent, and thus also not the amount of visits to restaurants.\n\n"}
{"id": "23080169", "url": "https://en.wikipedia.org/wiki?curid=23080169", "title": "Lammas growth", "text": "Lammas growth\n\nLammas growth, also called Lammas leaves, Lammas flush, second shoots, or summer shoots, is a season of renewed growth in some trees in temperate regions put on in July and August (if in the northern hemisphere, January and February if in the southern), that is around Lammas day, August 1, which is the Celtic harvest festival. \n\nIt can occur in both hardwoods and softwoods. Examples of common trees which exhibit regrowth are oak, ash, beech, sycamore, yew, scots pine, sitka spruce and hawthorn. This secondary growth may be an evolutionary strategy to compensate for leaf damage caused by insects during the spring. It is not present in poplar, birch or willow.\n\nLammas growth declines with the age of the tree, being most vigorous and noticeable in young trees. It differs in nature from spring growth which is fixed when leaves and shoots are laid down in the bud the previous year. The lammas flush is free growth of newly made leaves/needles throughout the tree. One or more of the buds set in the Spring on the ends of terminal and lateral stems will break, and begin to grow, producing a new shoot.\n"}
{"id": "10496693", "url": "https://en.wikipedia.org/wiki?curid=10496693", "title": "List of impact craters in Africa", "text": "List of impact craters in Africa\n\nThis list of impact craters in Africa includes all 20 confirmed impact craters as listed in the Earth Impact Database. These features were caused by the collision of large meteorites or comets with the Earth. For eroded or buried craters, the stated diameter typically refers to an estimate of original rim diameter, and may not correspond to present surface features.\n\nThe following craters are officially considered \"unconfirmed\" because they are not listed in the Earth Impact Database. Due to stringent requirements regarding evidence and peer-reviewed publication, newly discovered craters or those with difficulty collecting evidence generally are known for some time before becoming listed. However, entries on the unconfirmed list could still have an impact origin disproven.\n\nMahas was anonymously added Jan 2015, but the coordinates do show a convincing impact-like structure.\n\n\n"}
{"id": "6186219", "url": "https://en.wikipedia.org/wiki?curid=6186219", "title": "List of national parks of Iceland", "text": "List of national parks of Iceland\n\nSince 2008, Iceland has three national parks. Prior to 2008 there were four national parks in Iceland; in that year Jökulsárgljúfur and Skaftafell were merged and incorporated into Vatnajökull National Park. \n\nVatnajökull National Park and Snæfellsjökull National Park are supervised by the Ministry for the Environment and Natural Resources, Þingvellir National Park is supervised by the Ministry for the Prime Minister.\n"}
{"id": "38957205", "url": "https://en.wikipedia.org/wiki?curid=38957205", "title": "List of things named after Leonardo da Vinci", "text": "List of things named after Leonardo da Vinci\n\nThis is a list of things named after Leonardo da Vinci.\n\n\n\n\n\n\n\n\n"}
{"id": "28960022", "url": "https://en.wikipedia.org/wiki?curid=28960022", "title": "List of university statistical consulting centers", "text": "List of university statistical consulting centers\n\nThis list of university statistical consulting centers (or centres) is a simple list of universities in which there is a specifically designated team providing statistical consultancy services. Often this service will be available only to enquirers from within the same university.\n\nThe following is a list of university Statistical Consulting Centers in the United States of America.\n\n\n"}
{"id": "7119998", "url": "https://en.wikipedia.org/wiki?curid=7119998", "title": "List of volcanoes in Ecuador", "text": "List of volcanoes in Ecuador\n\nThis is a list of active and extinct volcanoes in Ecuador.\n\nIn Ecuador, EPN monitors the volcanic activity in this Andean nation.\n\n\n"}
{"id": "17118551", "url": "https://en.wikipedia.org/wiki?curid=17118551", "title": "Lists of aquarium life", "text": "Lists of aquarium life\n\nIn fishkeeping, suitable species of aquarium fish, plants and other organisms vary with the size, water chemistry and temperature of the aquarium.\n\nFor aquatic species found in particular types of aquaria, see:\n"}
{"id": "52318777", "url": "https://en.wikipedia.org/wiki?curid=52318777", "title": "Matriphagy", "text": "Matriphagy\n\nMatriphagy is the consumption of the mother by her offspring. The behavior generally takes place within the first few weeks of life and has been documented in some species of insects, nematode worms, scorpions, and other arachnids as well as in caecilian amphibians.\nThe specifics of how matriphagy occurs varies among different species, but the process is best described in the Desert spider, \"Stegodyphus lineatus\", where the mother harbors nutritional resources for her young through food consumption. The mother is able to regurgitate small portions of food for her growing offspring, but between 1-2 weeks after hatching the progeny capitalize on this food source by eating her alive. Typically, offspring only feed on their biological mother as opposed to other females in the population. In other arachnid species, matriphagy occurs after the ingestion of nutritional eggs known as trophic eggs (e.g. Black lace-weaver \"Amaurobius ferox\", Crab spider \"Diaea ergandros\") and involves different techniques for killing the mother, such as transfer of poison via biting and sucking to cause a quick death (e.g. Black lace-weaver) or continuous sucking of the hemolymph, resulting in a more gradual death (e.g. Crab spider\").\" The behavior is less well described but follows a similar pattern in species such as the Hump earwig, pseudoscorpions, and caecilians.\n\nSpiders that engage in matriphagy produce offspring with higher weights, shorter and earlier moulting time, larger body mass at dispersal, and higher survival rates than clutches deprived of matriphagy. In some species, matriphagous offspring were also more successful at capturing large prey items and had a higher survival rate at dispersal. These benefits to offspring outweigh the cost of survival to the mothers and help ensure that her genetic material is passed to the next generation, thus perpetuating the behavior.\n\nOverall, matriphagy is an extreme form of parental care but is highly related to extended care in the Funnel-web spider, parental investment in caecilians, and gerontophagy in social spiders. The uniqueness of this phenomenon has led to several expanded analogies in human culture and contributed to the pervasive fear of spiders throughout society.\n\nMatriphagy can be broken down into two components:\n\n\nMatriphagy generally consists of offspring consuming their mother; however, different species exhibit different variations of this behavior.\n\nIn many Black lace-weavers, \"Amaurobius ferox\", offspring do not immediately consume their mother. A day after offspring emerge from their eggs, their mother lays a set of trophic eggs, which contain nutrition for the offspring to consume. Matriphagy commences days later when the mother begins communicating with her offspring through web vibrations, drumming, and jumping. Through these behaviors, offspring are able to detect when and where they can consume their mother. They migrate towards her and a couple of the spiderlings jump onto her back to consume her. In response, the mother jumps and drums more frequently to keep her offspring off of her, however, they relentlessly continue attempting to get onto her back. When the mother feels ready, she presses her body onto her offspring and allows them to consume her via sucking on her insides. As they consume her, they also release poison into her body, causing a quick death. The mother’s body is kept for a few weeks as a nutritional reserve. \n\nInterestingly, matriphagy in this species is dependent on the developmental stage that the offspring are currently at. If offspring, older than four days, are given to an unrelated mother, they refuse to consume her. However, if younger offspring are given to an unrelated mother, they readily consume her. Additionally, if a mother loses her offspring, she is able to produce another clutch of offspring.\n\nMothers of one particular species of the crab spider, \"Diaea ergandros,\" are only able to lay one clutch, unlike the Black lace-weaver. They invest a significant amount of time and energy into storing nutrients and food into large oocytes, known as trophic eggs, similar to the Black lace-weaver. However, these trophic eggs are too large to physically leave her body. Some of the nutrients from the trophic eggs are liquefied into haemolymph, which can be consumed through the mother’s leg joints by her offspring. She gradually shrinks until she becomes immobile and dies. \n\nIn this species, it has been shown that this behavior may contribute to reducing cannibalism by siblings.\n\nRight after hatching, desert spider, \"Stegodyphus lineatus,\" hatchlings rely solely upon their mother to provide them with food and nutrients. Their mother does this by regurgitating her bodily fluids, which contain a mixture of nutrients for them to feed on. \n\nThis behavior begins during mating. Mating causes an increase in the mother’s production of digestive enzymes to better digest her prey. Consequently, she is able to retain more nutrients for her offspring to consume later. The mother’s midgut tissues start to slowly degrade during the incubation period of her eggs. After her offspring hatch, she regurgitates food for them to feed on with the help of her already-liquefied midgut tissues. Meanwhile, her midgut tissues continue to degrade into a liquid state to maximize the amount of nutrients from the mother’s body that her offspring will be able to obtain. As degradation continues, nutritional vacuoles form within her abdomen to amass all of the nutrients. Consumption begins when her offspring puncture her abdomen to suck up the nutritional vacuoles. After approximately 2-3 hours, the mother’s bodily fluids are completely consumed, and only her exoskeleton remains. \n\nThis species is only able to have one clutch, which might explain why so much time and energy is spent on taking care of offspring. Furthermore, matriphagy can also occur between offspring and mothers who have recently laid eggs that are not related. \n\n\"Anechura harmandi\" is the only species of Earwigs that has been currently documented to exhibit matriphagy. Mothers in this particular species of Earwigs have been found to reproduce during colder temperatures. This is mainly for the purpose of avoiding predation and maximizing their offspring's survival, since females are unable to produce a second clutch. Due to the cold temperature, there is a scarcity of available nutrients when the offspring hatch, which is why the offspring end up consuming their mother. \n\nMatriphagy in this species of Pseudoscorpions is usually observed during times of food scarcity. After their offspring hatch, mothers exit their nests and await to be consumed. Offspring follow their mothers out of the nest where they grab onto her legs and proceed to feed through her leg joints, similar to that of \"Diaea ergandros\". \n\nFemales of this species are able to produce more than one clutch of offspring if their first clutch was unsuccessful. \n\nMatriphagy in this species has been predicted to prevent cannibalism between siblings as well.\n\nCaecilians are the only vertebrate animals with well-documented matriphagous behaviours. In viviparous caecilians, the young consume the mother's oviduct lining by scraping it off with their teeth. In at least two species, \"Boulengerula taitana\" and \"Siphonops annulatus\", the young feed on the mother's skin by tearing it off with their teeth, which then regenerates within a few days. Because neither are closely related, either this behaviour is more common than currently observed or it evolved independently.\n\nThe adaptive value of matriphagy is based on the benefits provided to the offspring and the costs borne by the mother . Functionally analyzing matriphagy in this manner sheds light on why this unusual and extreme form of care has evolved and been selected for. \n\n\nUnlike other milder forms of parental care, matriphagy ends with the life of the mother, the gravest of all costs. So, why has it evolved? In order to answer this, it is important to look at costs to the mother in terms of reproductive output, egg sac development, and number of young reared (i.e. are offspring more successful if the mother evades matriphagy and reproduces again or if she engages in matriphagy and produces only one clutch?). \n\n\nIn conclusion, offspring that engage in matriphagy benefit more than those that do not engage in this behavior. Furthermore, the progeny of females that escape matriphagy to lay a second brood are significantly less successful than those that ate their mother. Hence, enhanced fitness of the mother accounts for the evolution of this unusual and extreme form of parenting. \n\nMatriphagy is one of the most extreme forms of parental care observed in the animal kingdom. However, in some species such as the Funnel-web spider \"Coelotes terrestris\", matriphagy is only observed under certain conditions and extended maternal protection is the main method by which offspring receive care. In other organisms such as the African social velvet spider, \"Stegodyphus mimosarum\" and Caecilian amphibians, parental behavior closely related in form and function to matriphagy is used. \n\nThe ‘maternal social’ spider, \"Coelotes terrestris\" (Funnel-web spider) uses extended maternal care as a reproductive model for its offspring. Upon laying the egg sac, a C. terrestris mother stands guard and incubates the sac for 3 to 4 weeks. She stays with her young from the time of their emergence until dispersal approximately 5 to 6 weeks later. During the offsprings’ development, mothers will provide the spiderlings prey based on their levels of gregariousness.\n\nProtecting the egg sacs from predation and parasites yields a high benefit to cost ratio for the mothers. Fitness of the mother is highly correlated to offspring developmental state—a mother in better condition yields larger young that are better at surviving predation. The presence of the mother also protects the offspring against parasitism. In addition, the mother can keep feeding while guarding her progeny without any weight loss, allowing her to collect sufficient food for both herself and her offspring. \n\nOverall, costs of protecting the egg sac are low. Upon separation from egg sacs, 90% of females have the energy sustenance to lay new sacs, although it does induce a time loss of several weeks that could potentially affect reproductive success. \n\nIn experimental conditions, costs arose if maternal care was not provided, with egg sacs drying out and developing molds, thus illustrating that maternal care is essential for survival. Experimental food-deprived broods reared by the mother induced matriphagy, where 77% of offspring consumed their mother upon birth. This suggests that matriphagy can exist under nutrient-limited conditions, but the costs generally outweigh the benefits when mothers have sufficient access to resources.\n\nCaecilian amphibians are worm-like in appearance, and mothers have thick epithelial skin layers. The skin on a caecilian mother is used for a form of parent-offspring nutrient transfer. \n\nThe Taita African caecilian \"Boulengerula taitana\" is an oviparous (egg-laying) caecilian whose skin transforms in brooding females to supply nutrients to growing offspring. The offspring are born with specific dentition that they can use to peel and eat the outer epidermal later of their mother’s skin. Young move around their mother’s bodies, using their lower jaws to lift and peel the mother’s skin while vigorously pressing their heads against her abdomen. To account for this, the epidermis of brooding females can be up to twice the thickness of non-brooding females.\n\nViviparous (developing in the mother) caecilians on the other hand, have specialized fetal dentition which can be used for scraping lipid-rich secretions and cellular materials from the maternal oviduct lining. The ringed caecilian \"Siphonops annulatus\", an oviparous caecilian, exhibits characteristics similar to viviparous caecilians. Mothers have paler skin tones than non-attending females, suggesting that offspring feed on glandular secretions on the mother’s skin—a process that resembles mammalian lactation. This scraping method is different from the peeling actions performed by oviparous caecilians. \n\nFor both oviparous and viviparous caecilians, delayed investment is a common benefit. Providing nutrition through the skin allows for redirection of nutrients, yielding fewer and larger offspring than caecilians who only provide their offspring with yolk nutrients. Rather than the mother sacrificing herself and solely being used for the offspring’s nutrition, caecilian mothers supplement their offspring’s growth; they provide enough nutrients for the offspring to survive, but not at the cost of their own life. \n\n\"Stegodyphus\" mothers liquefy their inner organs and maternal tissue into food deposits. The African social velvet spider \"Stegodyphus mimosarum\" and the African social spider \"Stegodyphus dumicola\" are two social spider species that eat their mothers and other adult females, which is unique since social spiders do not tend to exhibit cannibalistic life history traits. In these specific spiders, deceased females are often found shriveled with shrunken abdomens. Offspring suck nutrients primarily from the dorsal part of the adult female's abdomen, and she may still be alive during this process.\n\nThis behavior is not quite the same as matriphagy because \"Stegodyphus\" spiderlings are perfectly tolerant to other offspring, healthy conspecifics, and members of other species, suggesting that ordinary cannibalism is suppressed. Instead, the parental care exhibited is known as \"gerontophagy\", or the “consumption of old individuals” (geron = old person, phagy = to feed on). Gerontophagy is the final act of care for the offspring, and some offspring are found larger than others. This implies that some young spiders are already able to feed on prey by themselves and gerontophagy as a source of nutrition is supplemental rather than necessary. Thus, there exists the ‘cannibal’s kin-dilemma’, which reveals a form of kin selection in social spiders. In this scenario, kin selection should counteract cannibalism of related individuals in social spiders, but any designated victim should prefer to be eaten by available close relatives.\n\nAlthough matriphagy is not a widely circulated term in society, there are specific instances where it has been used to represent or symbolize specific ideas. For instance, Dr. Luke Winslow from San Diego State University dubbed the concept \"rhetorical matriphagy\" in 2017 in connection with critiques of increasing online higher education offerings. In this analogy, higher education assumes the role of the mother, neoliberalism assumes the role of the hatchlings, and online education discourse assumes the role of the mother’s symbolic resources, which are used to attract hatchings and eventually lead them to engage in matriphagy for these resources. The theory behind the existence of matriphagy is similar to that of higher education. Matriphagy is a less than ideal survival mechanism in response to an environment with poor conditions and few natural resources, but it persists because of guaranteed survival of the mother’s genes via her hatchlings. Similarly, online education does not consider itself at the same level as traditional forms of higher education but does provide an equal opportunity for all to attain such education in a cost-effective way that satisfies most stakeholders in the process.\n\nThose who have been exposed to matriphagy may be frightened by such a seemingly strange and bizarre natural behavior, especially since it is mainly observed in already feared organisms. Thus, matriphagy is often posed as perpetuation of a long held fear of arachnids in human society.\n\nIn contrast, others may look to matriphagy as a leading example of purity, as it represents an instinctive form of altruism. Altruism in this case refers to an \"intentional action ultimately for the welfare of others that entails at least the possibility of either no benefit or a loss to the actor,\" and is a highly popularized and desirable concept in many human cultures. Matriphagy can be viewed as altruism, insofar as participating mothers \"sacrifice\" their survival for the welfare of their offspring. Although participation in matriphagy is not truly an intentional action, mothers are nevertheless driven by natural selection pressures based on offspring fitness to engage in such behavior. This in turn creates a cycle that perpetuates altruistic matriphagous behavior through generations. Such an example of altruism on a purely biological level differs severely from human standards of altruism, which are tainted by moral virtues such as rationality, trust, and reciprocity.\n\n\n\nPseudoscorpions\n\n\n"}
{"id": "3133892", "url": "https://en.wikipedia.org/wiki?curid=3133892", "title": "Matteo Campani-Alimenis", "text": "Matteo Campani-Alimenis\n\nMatteo or Mathieu Campani-Alimenis (born in Spoleto, Italy) was a mechanician and natural philosopher of the 17th century.\n\nHe held a curacy at Rome in 1661, but devoted himself principally to scientific pursuits. As an optician he is chiefly celebrated for, the manufacture of the large object-glasses with which Cassini discovered two of Saturn's satellites, and for an attempt to rectify chromatic aberration by using a triple eyeglass; and in clock-making, for his invention of the illuminated dial-plate, and that of noiseless clocks, as well as for an attempt to correct the irregularities of the pendulum which arise from variations of temperature. Campani published in 1678 a work on horology, and on the manufacture of lenses for telescopes.\n\nHis younger brother Giuseppe was also an ingenious optician (indeed the attempt to correct chromatic aberration has been ascribed to him instead of to Matteo), and is, besides, noteworthy as an astronomer, especially for his discovery, by the aid of a telescope of his own construction, of the spots in Jupiter, the credit of which was, however, also claimed by Eustachio Divini.\n\n"}
{"id": "53014376", "url": "https://en.wikipedia.org/wiki?curid=53014376", "title": "Matthias Maurer", "text": "Matthias Maurer\n\nDr. Matthias Maurer (born 18 March 1970 in St. Wendel, Saarland) is a European Space Agency astronaut and materials scientist, who was selected in 2015 to take part in space training.\n\nMaurer studied at the Saarland University, Germany, where he received a degree in materials sciences. He also studied at the University of Leeds and the Polytechnic University of Catalonia. He has been working as a researcher since 1999 and received his doctorate in engineering at the Institute of Materials Sciences of the RWTH Aachen University in 2004, with a dissertation on materials sciences. In his spare time, he enjoys traveling, photography, reading, politics, foreign languages, cycling and hiking.\n\nHe was selected as an astronaut in July 2015 by the European Space Agency. Before he joined the European Astronaut Corps, he worked for ESA as a Eurocom International Space Station flight controller.\n\nIn 2014, he took part in the ESA Cooperative Adventure for Valuing and Exercising Human Behaviour and Performance Skills program and in 2016 he was part of the NASA Extreme Environment Mission Operations 21 analog mission.\n\nIn May 2017, Maurer completed EVA training at the Neutral Buoyancy Lab alongside fellow ESA astronaut Tim Peake. In May 2018 he completed basic training and became certified to go to space.\n\n"}
{"id": "34115448", "url": "https://en.wikipedia.org/wiki?curid=34115448", "title": "Maximal information coefficient", "text": "Maximal information coefficient\n\nIn statistics, the maximal information coefficient (MIC) is a measure of the strength of the linear or non-linear association between two variables \"X\" and \"Y\".\n\nThe MIC belongs to the maximal information-based nonparametric exploration (MINE) class of statistics. In a simulation study, MIC outperformed some selected low power tests, however concerns have been raised regarding reduced statistical power in detecting some associations in settings with low sample size when compared to powerful methods such as distance correlation and Heller–Heller–Gorfine (HHG). Comparisons with these methods, in which MIC was outperformed, were made in Simon and Tibshirani and in Gorfine, Heller, and Heller. It is claimed that MIC approximately satisfies a property called \"equitability\" which is illustrated by selected simulation studies. It was later proved that no non-trivial coefficient can exactly satisfy the \"equitability\" property as defined by Reshef et al., although this result has been challenged. Some criticisms of MIC are addressed by Reshef et al. in further studies published on arXiv.\n\nThe maximal information coefficient uses binning as a means to apply mutual information on continuous random variables. Binning has been used for some time as a way of applying mutual information to continuous distributions; what MIC contributes in addition is a methodology for selecting the number of bins and picking a maximum over many possible grids.\n\nThe rationale is that the bins for both variables should be chosen in such a way that the mutual information between the variables be maximal. That is achieved whenever formula_1. Thus, when the mutual information is maximal over a binning of the data, we should expect that the following two properties hold, as much as made possible by the own nature of the data. First, the bins would have roughly the same size, because the entropies formula_2 and formula_3 are maximized by equal-sized binning. And second, each bin of \"X\" will roughly correspond to a bin in \"Y\".\n\nBecause the variables X and Y are reals, it is almost always possible to create exactly one bin for each (\"x\",\"y\") datapoint, and that would yield a very high value of the MI. To avoid forming this kind of trivial partitioning, the authors of the paper propose taking a number of bins formula_4 for \"X\" and formula_5 whose product is relatively small compared with the size N of the data sample. Concretely, they propose:\n\nformula_6\n\nIn some cases it is possible to achieve a good correspondence between formula_7 and formula_8 with numbers as low as formula_9 and formula_10, while in other cases the number of bins required may be higher. The maximum for formula_11 is determined by H(X), which is in turn determined by the number of bins in each axis, therefore, the mutual information value will be dependent on the number of bins selected for each variable. In order to compare mutual information values obtained with partitions of different sizes, the mutual information value is normalized by dividing by the maximum achieveable value for the given partition size. It is worth noting that a similar adaptive binning procedure for estimating mutual information had been proposed previously.\nEntropy is maximized by uniform probability distributions, or in this case, bins with the same number of elements. Also, joint entropy is minimized by having a one-to-one correspondence between bins. If we substitute such values in the formula\nformula_12, we can see that the maximum value achieveable by the MI for a given pair formula_13 of bin counts is formula_14. Thus, this value is used as a normalizing divisor for each pair of bin counts.\n\nLast, the normalized maximal mutual information value for different combinations of formula_4 and formula_5 is tabulated, and the maximum value in the table selected as the value of the statistic.\n\nIt is important to note that trying all possible binning schemes that satisfy formula_6 is computationally unfeasible even for small n. Therefore, in practice the authors apply a heuristic which may or may not find the true maximum.\n"}
{"id": "7767755", "url": "https://en.wikipedia.org/wiki?curid=7767755", "title": "Melanobatrachus", "text": "Melanobatrachus\n\nMelanobatrachus is a genus of narrow-mouthed frogs (family Microhylidae) that contains a single species, Melanobatrachus indicus. It is known under a number of common names, including Kerala Hills frog, black microhylid frog, and Malabar black narrow-mouthed frog. It is endemic to wet evergreen forests of southern Western Ghats in Kerala and Tamil Nadu states of India.\n\n\"Melanobatrachus indicus\" is a rare species that was only rediscovered in 1997. It lives amongst leaf-litter, rocks and other ground cover of moist evergreen tropical forests.\n\n\"Melanobatrachus indicus\" is an Evolutionarily Distinct and Globally Endangered (EDGE) species. It is the sole species in subfamily Melanobatrachinae, and it is classified as \"Endangered\" by the International Union for Conservation of Nature.\n\n\n"}
{"id": "32209316", "url": "https://en.wikipedia.org/wiki?curid=32209316", "title": "Morality of science", "text": "Morality of science\n\nThe relationship between morality and science manifests itself in many areas. These include the following:\n\n"}
{"id": "2592962", "url": "https://en.wikipedia.org/wiki?curid=2592962", "title": "Octant (instrument)", "text": "Octant (instrument)\n\nThe octant, also called reflecting quadrant, is a measuring instrument used primarily in navigation. It is a type of reflecting instrument.\n\nThe name \"octant\" derives from the Latin \"octans\" meaning \"eighth part of a circle\", because the instrument's arc is one eighth of a circle.\n\n\"Reflecting quadrant\" derives from the instrument using mirrors to reflect the path of light to the observer and, in doing so, doubles the angle measured. This allows the instrument to use a one-eighth of a turn to measure a quarter-turn or quadrant.\n\nIsaac Newton's reflecting quadrant was invented around 1699. A detailed description of the instrument was given to Edmond Halley, but the description was not published until after Halley's death in 1742. It is not known why Halley did not publish the information during his life, as this prevented Newton from getting the credit for the invention that is generally given to John Hadley and Thomas Godfrey.\n\nOne copy of this instrument was constructed by Thomas Heath (instrument maker) and may have been shown in Heath's shop window prior to its being published by the Royal Society in 1742.\n\nNewton's instrument used two mirrors, but they were used in an arrangement somewhat different from the two mirrors found in modern octants and sextants. The diagram on the right shows the configuration of the instrument.\n\nThe 45° arc of the instrument (P-Q), was graduated with 90 divisions of a half-degree each. Each such division was subdivided into 60 parts and each part further divided into sixths. This results in the arc being marked in degrees, minutes and sixths of a minute (10 seconds). Thus the instrument could have readings interpolated to 5 seconds of arc. This fineness of graduation is only possible due to the large size of the instrument - the sighting telescope alone was three to four feet long.\n\nA \"sighting telescope\" (A-B), three or four feet long, was mounted along one side of the instrument. A \"horizon mirror\" was fixed at a 45° angle in front of the telescope's objective lens (G). This mirror was small enough to allow the observer to see the image in the mirror on one side and to see directly ahead on the other. The index arm (C-D) held an index mirror (H), also at 45° to the edge of the index arm. The reflective sides of the two mirrors nominally faced each other, so that the image seen in the first mirror is that reflected from the second.\n\nWith the two mirrors parallel, the index reads 0°. The view through the telescope sees directly ahead on one side and the view from the mirror G sees the same image reflected from mirror H (see detail drawing to the right). When the index arm is moved from zero to a large value, the \"index mirror\" reflects an image that is in a direction away from the direct line of sight. As the index arm movement increases, the line of sight for the index mirror moves toward S (to the right in the detail image). This shows a slight deficiency with this mirror arrangement. The horizon mirror will block the view of the index mirror at angles approaching 90°.\n\nThe length of the sighting telescope seems remarkable, given the small size of the telescopes on modern instruments. This was likely Newton's choice of a way to reduce chromatic aberrations. Short–focal length telescopes, prior to the development of achromatic lenses, produced an objectionable degree of aberration, so much so that it could affect the perception of a star's position. Long focal lengths were the solution, and this telescope would likely have had both a long–focal length objective lens and a long–focal length eyepiece. This would decrease aberrations without excessive magnification.\n\nTwo men independently developed the octant around 1730: John Hadley (1682–1744), an English mathematician, and Thomas Godfrey (1704–1749), a glazier in Philadelphia. While both have a legitimate and equal claim to the invention, Hadley generally gets the greater share of the credit. This reflects the central role that London and the Royal Society played in the history of scientific instruments in the eighteenth century.\n\nTwo others who created octants during this period were Caleb Smith, an English insurance broker with a strong interest in astronomy (in 1734), and Jean-Paul Fouchy, a mathematics professor and astronomer in France (in 1732).\n\nHadley produced two versions of the reflecting quadrant. Only the second is well known and is the familiar octant.\n\nHadley's first reflecting quadrant was a simple device with a frame spanning a 45° arc. In the image at the right, from Hadley's article in the \"Philosophical Transactions\" of the Royal Society, you can see the nature of his design. A small sighting telescope was mounted on the frame along one side. One large \"index mirror\" was mounted at the point of rotation of the index arm. A second, smaller \"horizon mirror\" was mounted on the frame in the line of sight of the telescope. The horizon mirror allows the observer to see the image of the index mirror in one half of the view and to see a distant object in the other half. A shade was mounted at the vertex of the instrument to allow one to observe a bright object. The shade pivots to allow it to move out of the way for stellar observations.\n\nObserving through the telescope, the navigator would sight one object directly ahead. The second object would be seen by reflection in the horizon mirror. The light in the horizon mirror is reflected from the index mirror. By moving the index arm, the index mirror can be made to reveal any object up to 90° from the direct line of sight. When both objects are in the same view, aligning them together allows the navigator to measure the angular distance between them.\n\nVery few of the original reflecting quadrant designs were ever produced. One, constructed by Baradelle, is in the collection of the Musée de la Marine, Paris.\n\nHadley's second design had the form familiar to modern navigators. The image to the right, also taken from his Royal Society publication, shows the details.\n\nHe placed an \"index mirror\" on the index arm. Two \"horizon mirrors\" were provided. The upper mirror, in the line of the sighting telescope, was small enough to allow the telescope to see directly ahead as well as seeing the reflected view. The reflected view was that of the light from the index mirror. As in the previous instrument, the arrangement of the mirrors allowed the observer to simultaneously see an object straight ahead and to see one reflected in the index mirror to the horizon mirror and then into the telescope. Moving the index arm allowed the navigator to see any object within 90° of the direct view.\n\nThe significant difference with this design was that the mirrors allowed the instrument to be held vertically rather than horizontally and it provided more room for configuring the mirrors without suffering from mutual interference.\n\nThe second horizon mirror was an interesting innovation. The telescope was removable. It could be remounted so that the telescope viewed the second horizon mirror from the opposite side of the frame. By mounting the two horizon mirrors at right angles to each other and permitting the movement of the telescope, the navigator could measure angles from 0 to 90° with one horizon mirror and from 90° to 180° with the other. This made the instrument very versatile. For unknown reasons, this feature was not implemented on octants in general use.\n\nComparing this instrument to the photo of a typical octant at the top of the article, one can see that the only significant differences in the more modern design are:\n\nCaleb Smith, an English insurance broker with a strong interest in astronomy, had created an octant in 1734. He called it an \"Astroscope\" or \"Sea-Quadrant\". His used a fixed prism in addition to an index mirror to provide reflective elements. Prisms provide advantages over mirrors in an era when polished speculum metal mirrors were inferior and both the silvering of a mirror and the production of glass with flat, parallel surfaces was difficult.\n\nIn the drawing to the right, the horizon element (B) could be a mirror or a prism. On the index arm, the index mirror (A) rotated with the arm. A sighting telescope was mounted on the frame (C). The index did not use a vernier or other device at the scale (D). Smith called the instrument's index arm a \"label\", in the manner of Elton for his mariner's quadrant.\n\nVarious design elements of Smith's instrument made it inferior to Hadley's octant and it was not used significantly. For example, one problem with the Astroscope was that angle of the observer's line of sight. By looking down, he had greater difficulty in observing than an orientation with his head in a normal orientation.\n\nThe octant provided a number of advantages over previous instruments.\n\nThe sight was easy to align because the horizon and the star seem to move together as the ship pitched and rolled. This also created a situation where the error in observation was less dependent on the observer, as he could directly see both objects at once.\n\nWith the use of the manufacturing techniques available in the 18th century, the instruments were capable of reading very accurately. The size of the instruments was reduced with no loss of accuracy. An octant could be half the size of a Davis quadrant with no increase in error.\n\nUsing shades over the light paths, one could observe the sun directly, while moving the shades out of the light path allowed the navigator to observe faint stars. This made the instrument usable both night and day.\n\nBy 1780, the octant and sextant had almost completely displaced all previous navigational instruments.\n\nEarly octants were constructed primarily in wood, with later versions incorporating ivory and brass components. The earliest mirrors were polished metal, since the technology to produce silvered glass mirrors with flat, parallel surfaces was limited. As glass polishing techniques improved, glass mirrors began to be provided. These used coatings of mercury-containing tin amalgam; coatings of silver or aluminum were not available until the 19th century. The poor optical quality of the early polished speculum metal mirrors meant that telescopic sights were not practical. For that reason, most early octants employed a simple naked-eye sighting pinnula instead.\nEarly octants retained some of the features common to backstaves, such as transversals on the scale. However, as engraved, they showed the instrument to have an apparent accuracy of only two minutes of arc while the backstaff appeared to be accurate to one minute. The use of the vernier scale allowed the scale to be read to one minute, so improved the marketability of the instrument. This and the ease in making verniers compared to transversals, lead to adoption of the vernier on octants produced later in the 18th century.\n\nOctants were produced in large numbers. In wood and ivory, their relatively low price compared to an all-brass sextant made them a popular instrument. The design was standardized with many manufacturers using the identical frame style and components. Different shops could make different components, with woodworkers specializing in frames and others in the brass components. For example, Spencer, Browning and Rust, a manufacturer of scientific instruments in England from 1787 to 1840 (operating as \"Spencer, Browning and Co.\" after 1840) used a Ramsden dividing engine to produce graduated scales in ivory. These were widely used by others and the \"SBR\" initials could be found on octants from many other manufacturers.\n\nExamples of these very similar octants are in the photos in this article. The image at the top is essentially the same instrument as the one in the detail photos. However, they are from two different instrument makers - the upper is labelled \"Crichton - London, Sold by J Berry Aberdeen\" while the detail images are of an instrument from \"Spencer, Browning & Co. London\". The only obvious difference is the presence of horizon shades on the Crichton octant that are not on the other.\n\nThese octants were available with many options. A basic octant with graduations directly on the wood frame were least expensive. These dispensed with a telescopic sight, using a single- or double-holed sighting pinnula instead. Ivory scales would increase the price, as would the use of a brass index arm or a vernier.\n\nIn 1767 the first edition of the Nautical Almanac tabulated lunar distances, enabling navigators to find the current time from the angle between the sun and the moon. This angle is sometimes larger than 90°, and thus not possible to measure with an octant. For that reason, Admiral John Campbell, who conducted shipboard experiments with the lunar distance method, suggested a larger instrument and the sextant was developed.\n\nFrom that time onward, the sextant was the instrument that experienced significant development and improvements and was the instrument of choice for naval navigators. The octant continued to be produced well into the 19th century, though it was generally a less accurate and less expensive instrument. The lower price of the octant, including versions without telescope, made it a practical instrument for ships in the merchant and fishing fleets.\n\nOne common practice among navigators up to the late nineteenth century was to use both a sextant and an octant. The sextant was used with great care and only for lunars, while the octant was used for routine meridional altitude measurements of the sun every day. This protected the very accurate and pricier sextant, while using the more affordable octant where it performs well.\nFrom the early 1930s through the end of the 1950s, several types of civilian and military \"bubble octant\" instruments were produced for use aboard aircraft. All were fitted with an artificial horizon in the form of a bubble, which was centered to align the horizon for a navigator flying thousands of feet above the earth; some had recording features.\n\nUse and adjustment of the octant is essentially identical to the navigator's sextant.\n\nHadley's was not the first reflecting quadrant. Robert Hooke invented a reflecting quadrant in 1684 and had written about the concept as early as 1666. Hooke's was a single-reflecting instrument. Other octants were developed by Jean-Paul Fouchy and Caleb Smith in the early 1730s, however, these did not become significant in the history of navigation instruments.\n\n"}
{"id": "22340697", "url": "https://en.wikipedia.org/wiki?curid=22340697", "title": "Power resource theory", "text": "Power resource theory\n\nPower resource theory is a political theory which proposes the idea that the distribution of power between major classes is to some extent accountable for the successes and failure of various political ideologies. It argues that \"at its core, it asserts that working class power achieved through organisation by labor unions or left parties, produces more egalitarian distributional outcomes\".\n\n\nhvlhkkcihc\nहेलो.KUSHAGRA\n"}
{"id": "6101309", "url": "https://en.wikipedia.org/wiki?curid=6101309", "title": "Quantities of information", "text": "Quantities of information\n\nThe mathematical theory of information is based on probability theory and statistics, and measures information with several quantities of information. The choice of logarithmic base in the following formulae determines the unit of information entropy that is used. The most common unit of information is the bit, based on the binary logarithm. Other units include the nat, based on the natural logarithm, and the hartley, based on the base 10 or common logarithm.\n\nIn what follows, an expression of the form formula_1 is considered by convention to be equal to zero whenever \"p\" is zero. This is justified because formula_2 for any logarithmic base.\n\nShannon derived a measure of information content called the self-information or \"surprisal\" of a message \"m\":\n\nwhere formula_4 is the probability that message \"m\" is chosen from all possible choices in the message space formula_5. The base of the logarithm only affects a scaling factor and, consequently, the units in which the measured information content is expressed. If the logarithm is base 2, the measure of information is expressed in units of bits.\n\nInformation is transferred from a source to a recipient only if the recipient of the information did not already have the information to begin with. Messages that convey information that is certain to happen and already known by the recipient contain no real information. Infrequently occurring messages contain more information than more frequently occurring messages. This fact is reflected in the above equation - a certain message, i.e. of probability 1, has an information measure of zero. In addition, a compound message of two (or more) unrelated (or mutually independent) messages would have a quantity of information that is the sum of the measures of information of each message individually. That fact is also reflected in the above equation, supporting the validity of its derivation.\n\nAn example: The weather forecast broadcast is: \"Tonight's forecast: Dark. Continued darkness until widely scattered light in the morning.\" This message contains almost no information. However, a forecast of a snowstorm would certainly contain information since such does not happen every evening. There would be an even greater amount of information in an accurate forecast of snow for a warm location, such as Miami. The amount of information in a forecast of snow for a location where it never snows (impossible event) is the highest (infinity).\n\nThe entropy of a discrete message space formula_5 is a measure of the amount of uncertainty one has about which message will be chosen. It is defined as the average self-information of a message formula_7 from that message space:\n\nwhere\n\nAn important property of entropy is that it is maximized when all the messages in the message space are equiprobable (e.g. formula_10). In this case formula_11.\n\nSometimes the function \"H\" is expressed in terms of the probabilities of the distribution:\n\nAn important special case of this is the binary entropy function:\n\nThe joint entropy of two discrete random variables formula_16 and formula_17 is defined as the entropy of the joint distribution of formula_16 and formula_17:\n\nIf formula_16 and formula_17 are independent, then the joint entropy is simply the sum of their individual entropies.\n\nGiven a particular value of a random variable formula_17, the conditional entropy of formula_16 given formula_25 is defined as:\n\nwhere formula_27 is the conditional probability of formula_28 given formula_29.\n\nThe conditional entropy of formula_16 given formula_17, also called the equivocation of formula_16 about formula_17 is then given by:\n\nThis uses the conditional expectation from probability theory.\n\nA basic property of the conditional entropy is that:\n\nThe Kullback–Leibler divergence (or information divergence, information gain, or relative entropy) is a way of comparing two distributions, a \"true\" probability distribution \"p\", and an arbitrary probability distribution \"q\". If we compress data in a manner that assumes \"q\" is the distribution underlying some data, when, in reality, \"p\" is the correct distribution, Kullback–Leibler divergence is the number of average additional bits per datum necessry for compression, or, mathematically,\nIt is in some sense the \"distance\" from \"q\" to \"p\", although it is not a true metric due to its not being symmetric.\n\nIt turns out that one of the most useful and important measures of information is the mutual information, or transinformation. This is a measure of how much information can be obtained about one random variable by observing another. The mutual information of formula_16 relative to formula_17 (which represents conceptually the average amount of information about formula_16 that can be gained by observing formula_17) is given by:\n\nA basic property of the mutual information is that:\n\nThat is, knowing \"Y\", we can save an average of formula_43 bits in encoding \"X\" compared to not knowing \"Y\". Mutual information is symmetric:\n\nMutual information can be expressed as the average Kullback–Leibler divergence (information gain) of the posterior probability distribution of \"X\" given the value of \"Y\" to the prior distribution on \"X\":\nIn other words, this is a measure of how much, on the average, the probability distribution on \"X\" will change if we are given the value of \"Y\". This is often recalculated as the divergence from the product of the marginal distributions to the actual joint distribution:\n\nMutual information is closely related to the log-likelihood ratio test in the context of contingency tables and the multinomial distribution and to Pearson's χ test: mutual information can be considered a statistic for assessing independence between a pair of variables, and has a well-specified asymptotic distribution.\n\nThe basic measures of discrete entropy have been extended by analogy to continuous spaces by replacing sums with integrals and probability mass functions with probability density functions. Although, in both cases, mutual information expresses the number of bits of information common to the two sources in question, the analogy does \"not\" imply identical properties; for example, differential entropy may be negative. \n\nThe differential analogies of entropy, joint entropy, conditional entropy, and mutual information are defined as follows:\n\nwhere formula_52 is the joint density function, formula_53 and formula_54 are the marginal distributions, and formula_55 is the conditional distribution.\n\n"}
{"id": "19991024", "url": "https://en.wikipedia.org/wiki?curid=19991024", "title": "Social Science Research Laboratory", "text": "Social Science Research Laboratory\n\nThe Social Science Research Laboratory (SSRL) is a component of the College of Arts & Letters at San Diego State University (SDSU) in San Diego, California.\n\nThe SSRL provide comprehensive survey research and program evaluation services to university faculty, administration, students, and regional government and non-profit organizations, and to provide education and training in current survey research and program evaluation methods to SDSU students. The SSRL also provides information on current best practices in survey research methodology and program evaluation to SDSU students, faculty, staff, and the greater San Diego community.\nActivities are conducted in accordance with the American Association for Public Opinion Research (AAPOR) Code of Professional Ethics and Practices.\n\n"}
{"id": "124347", "url": "https://en.wikipedia.org/wiki?curid=124347", "title": "Spa town", "text": "Spa town\n\nA spa town is a resort town based on a mineral spa (a developed mineral spring). Patrons visit spas to \"take the waters\" for their purported health benefits. The word \"spa\" is derived from the name of Spa, a town in Belgium. \n\nThomas Guidott set up a medical practice in the English town of Bath in 1668. He became interested in the curative properties of the hot mineral waters there and in 1676 wrote \"A discourse of Bathe, and the hot waters there. Also, Some Enquiries into the Nature of the water\". This brought the purported health-giving properties of the waters to the attention of the aristocracy, who started to partake in them soon after.\n\nThe term \"spa\" is used for towns or resorts offering hydrotherapy, which can include cold water or mineral water treatments and geothermal baths.\n\n\nMost of the mineral springs in Australia are in the Central Highlands of Victoria, although there are a few springs in South Australia, Moree, New South Wales and Queensland. Most are within 30 km of Daylesford, Victoria: the Daylesford and Hepburn Springs call themselves 'Spa Country' and the 'Spa Centre of Australia'.\n\n\n\"See: List of spa towns in Bosnia and Herzegovina\"\n\nBanja Vrućica, Teslić\n\nBrazil has a growing number of spa towns. The traditional ones are: Águas de Lindoia, Serra Negra, Águas de São Pedro, Caxambu, Poços de Caldas, Caldas Novas, Araxá, and São Lourenço.\n\n\"See: List of spa towns in Bulgaria\"\n\nBulgaria is known for its more than 500 mineral springs, including the hottest spring in the Balkans at Sapareva Banya - 103 °C. Other famous spa towns include Sandanski, Hisarya, Bankya, Devin, Kyustendil, Varshets, Velingard.\n\nIn Bulgarian, the word for a spa is \"баня\" (transliterated \"banya\").\n\n\"See: List of spa towns in Canada\n\nHarrison Hot Springs is one of the oldest among 18 in British Columbia; there are also two in Alberta and one in Ontario.\n\n\"See: List of spa towns in Croatia\"\n\nIn Croatia, the word \"Toplice\" implies a spa town. The most famous spa towns in Croatia are Daruvar, Šibenik and Sisak.\n\n\"See: Spa towns in the Czech Republic\"\n\nIn the Czech Language, the word \"Lázně\" implies a spa town. The most famous spa towns in Czech Republic are Karlovy Vary, Teplice, Františkovy Lázně and Mariánské Lázně.\n\n\"See: List of spa towns in France\"\n\nIn France, the words \"bains, thermes,\" and \"eaux\" in city names often imply a spa town. There are more than 50 spa towns in France, including Vichy, Aix-les-Bains, Bagnoles-de-l'Orne, Dax, and Enghien-les-Bains.\n\n\"See: List of spa towns in Germany\"\n\nIn Germany, the word \"Bad\" implies a spa town. Among the many famous spa towns in Germany are Bad Aachen, Baden-Baden, Bad Brückenau, Bad Ems, Bad Homburg, Bad Honnef, Bad Kissingen, Bad Kreuznach, Bad Mergentheim, Bad Muskau, Bad Pyrmont, Bad Reichenhall, Bad Saarow, Bad Schandau, Bad Segeberg, Bad Soden, Bad Tölz, Bad Wildbad, Bad Wildstein, Berchtesgaden, Binz, Freudenstadt, Heiligendamm, Heringsdorf, Kampen, Königstein, Radebeul, Schwangau, St. Blasien, Titisee, Tegernsee, Travemünde and Zingst. Wiesbaden is the largest spa town in Germany.\n\n\"See: List of spa towns in Greece\"\n\nThe most famous spa towns in Greece are Aidipsos and Loutraki.\n\n\"See: List of spa towns in Hungary\"\n\nIn Hungary, the word \"fürdő\" or the more archaic \"füred\" (\"bath\"), \"fürdőváros\" (\"spa town\") or \"fürdőhely\" (\"bathing place\") implies a spa town. Hungary is rich in thermal waters with health benefits, and many spa towns are popular tourist destinations. Budapest has several spas, including Turkish style spas dating back to the 16th century. Eger also has a Turkish spa. Other famous spas include the ones at Hévíz, Harkány, Bük, Hajdúszoboszló, Gyula, Bogács, Bükkszék, Zalakaros, the Cave Bath at Miskolctapolca and the Zsóry-fürdő at Mezőkövesd.\n\n\n\"See: List of spa towns in Italy\"\n\nIn Italy, spa towns, called \"città termale\" (from Latin \"thermae\"), are very numerous all over the country because of the intense geological activity of the territory. These places were known and used since the Roman age.\n\n\n\n\n\n\"See: List of spa towns in Poland\"\n\nMost spa towns in Poland are located in the Lesser Poland and Lower Silesian Voivodeships. Some of them have an affix \"Zdrój\" in their name (written with hyphen or separately), meaning \"water spring\", to denote their spa status, but this is not a general rule (e.g. Ciechocinek and Inowrocław are spa towns, but do not use the affix).\n\nPortugal is well known by famous spa towns throughout of the country.\n\nDue to its high quality, as well as the landscape where are located, the most important ones are: \n\n\"See: List of spa towns in Romania\"\n\nIn Romania, the word \"Băile\" implies a spa town. The most famous spa towns in Romania are Băile Herculane, Băile Felix, Mangalia, Covasna, Călimănești & Borsec.\n\n\"See: List of spa towns in Serbia\"\n\nSerbia is known for its many spa cities. Some of the best known springs are the Vrnjačka Banja, Bukovička Banja, Vrujci, Sokobanja and Niška Banja. The hottest spring in Serbia is at Vranjska Banja (96°C)\n\nIn Serbia, the word \"Banja\" implies a spa town.\n\n\"See: Spa towns in Slovakia\"\n\nSlovakia is well known by its spa towns. The most famous is Piešťany. \nThe most important spa towns in Slovakia are:\n\n\nSpa towns in Slovenia include Rogaška Slatina, Radenci, Čatež ob Savi, Dobrna, Dolenjske Toplice, Šmarješke Toplice and Moravske Toplice. They offer accommodation in hotels, apartments, bungalows, and camp sites. The Slovenian words \"terme\" or \"toplice\" imply a spa town.\n\nSpa towns in Spain include:\n\n\nTaiwan is home to a number of towns and cities with tourism infrastructure centered on hot springs. These include:\n\n\nSome but not all UK spa towns contain \"Spa\", \"Wells\", or \"Bath\" in their names, e.g., Matlock Bath. Some towns are designated Spa Heritage Towns. Two out of three of the English towns granted the title \"Royal\", Royal Leamington Spa and Royal Tunbridge Wells, are spa towns.\n\n\n\"See: List of spa towns\"\n\n\n"}
{"id": "22293029", "url": "https://en.wikipedia.org/wiki?curid=22293029", "title": "St. Irvyne", "text": "St. Irvyne\n\nSt. Irvyne; or, The Rosicrucian: A Romance is a Gothic horror novel written by Percy Bysshe Shelley in 1810 and published by John Joseph Stockdale in December of that year, dated 1811, in London anonymously as \"by a Gentleman of the University of Oxford\" while the author was an undergraduate. The main character is Wolfstein, a solitary wanderer, who encounters Ginotti, an alchemist of the Rosicrucian or Rose Cross Order who seeks to impart the secret of immortality. The book was reprinted in 1822 by Stockdale and in 1840 in \"The Romancist and the Novelist's Library: The Best Works of the Best Authors, Vol. III\", edited by William Hazlitt. The novella was a follow-up to Shelley's first prose work, \"Zastrozzi\", published earlier in 1810. \"St. Irvyne\" was republished in 1986 by Oxford University Press as part of the World's Classics series along with \"Zastrozzi\" and in 2002 by Broadview Press.\n\nNicole Berry translated the novel in a French edition in 1999. A Spanish edition entitled \"St. Irvyne o el Rosacruz\", translated by Gregorio Cantera Chamorro, was published by Celeste in Madrid in 2002 with an introduction and notes by Roberto Cueto. The book was translated into Swedish by KG Johansson in 2013 in an edition by Vertigo.\n\n\nThe epigraph for chapter three is from \"Paradise Lost\" (1667) by John Milton, Book II, 681-683:\n\n\"Whence and what art thou, execrable shape,\nThat dar'st, though grim and terrible, advance\nThy miscreated Front athwart my way.\"\n\nThe novel opens amidst a raging thunderstorm. Wolfstein is a wanderer in the Swiss Alps who seeks cover from the storm. He is a disillusioned outcast from society who seeks to kill himself. A group of monks carrying a body for burial in a torch-light procession runs into him and saves his life. Bandits attack them and take Wolfstein to an underground hideout. He meets Megalena, whom the bandits have abducted after killing her father in an ambush. After Steindolph, one of the bandits, recites a ballad about the reanimation of the corpse of a nun named Rosa, Wolfstein manages to poison the leader of the bandits, Cavigni, in a second attempt. He is able to escape with Megalena. Ginotti, a member of the bandits, befriends Wolfstein.\n\nWolfstein and Megalena flee to Genoa where they live together. Olympia, a woman of the town, seduces Wolfstein. Megalena, enraged by the relationship, demands that Wolfstein kill Olympia. Armed with a dagger, Wolfstein is unable to kill her. Olympia kills herself.\n\nGinotti follows Wolfstein. Ginotti is a member of the Rosicrucian, or Rose Cross, Order. He is an alchemist who seeks the secret of immortality. He tells Wolfstein that he will give him the secret to immortality if he will renounce his faith and join the sect.\n\nEloise de St. Irvyne is the sister of Wolfstein who lives in Geneva, Switzerland. Ginotti, under his new identity of Frederic Nempere, travels to Geneva and seeks to seduce her.\n\nGinotti reveals his experiments in his lifelong quest to find the secret of eternal life: \"From my earliest youth, before it was quenched by complete\nsatiation, \"curiosity\", and a desire of unveiling the latent mysteries of nature, was the passion by which all the other emotions of my mind were intellectually organized. ... Natural philosophy at last became the peculiar science to which I directed my eager enquiries.\" He has studied science and the laws of nature to ascertain the mysteries of life and of being: \"I thought of \"death\"---... I cannot die.---'Will not this nature---will not the \"matter\" of which it is composed---exist to all eternity? Ah! I know it will; and, by the exertions of the energies with which nature has gifted me, well I know it shall.'\" Ginotti tells Wolfstein that he will reveal the \"secret of immortal life\" to him if he will take certain prescribed ingredients and \"mix them according to the directions which this book will communicate to you\" and meet him in the abbey at St. Irvyne.\n\nIn the final scene, which takes place at the abbey of St. Irvyne in France, Wolfstein finds the corpse of Megalena in the vaults. An emaciated Ginotti confronts Wolfstein. Wolfstein is asked if he will deny his Creator. Wolfstein refuses to renounce his faith. Lightning strikes the vaults as thunder and a sulphurous windstorm blast the abbey. Both men are struck dead. This is the penalty they pay for \"the delusion of the passions\", for tampering with forces that they neither can control nor understand in seeking \"endless life\".\n\nThe novel, originally intended as a much longer \"triple decker\" novel, circulated as part of the \"circulating libraries\" which were popular at that time. This was a source of revenue for the publisher of the novel. Shelley ended the novella abruptly, deciding not to develop or integrate the two strands. The result was a much shorter work.\n\nCritics attacked the novel, which received generally negative reviews. The conservative British periodical \"The Anti-Jacobin Review and Magazine\", in a January 1812 review, castigated \"the writer, who can outrage nature and common sense in almost every page of his book\". The reviewer sought to deter readers from \"the perusal of unprofitable and vicious productions.\"\n\nFrench author Maurice Sarfati adapted the novel as \"Wolfstein et Mégaléna, ou La Vengeance du Rosiccrucien\", or \"Wolfstein and Megalena, the Vengeance of the Rosicrucian\", in 1980.\n\nThe novel was popular enough, however, to be made into two chapbooks in 1822 and 1850. The first chapbook version was entitled \"Wolfstein; or, The Mysterious Bandit\" and was published and printed by John Bailey at 116, Chancery Lane in London in 1822 after the original novel was republished that year. The chapbook version was a condensed version of the novella in 28 pages meant for popular consumption, serving the same function as a paperback would. The chapbook sold for sixpence.\n\nThe story is described on the title page as \"A Terrific Romance\" with an epigraph by Ossian: \"A tale of horror, of murder, and of deeds done in darkness.\" Added to \"Wolfstein\" was the story \"The Bronze Statue, A Pathetic Tale\" by another author, Anna Jane Vardill. \"The Bronze Statue\" had appeared for the first time in print as part of the \"Annals of Public Justice\" in \"The European Magazine\" of May, 1820, signed \"V\", i.e., Anna Jane Vardill.\n\nAnother more condensed twelve page chapbook was published in 1850 by Thomas Redriffe in London entitled \"Wolfstein, The Murderer; or, The Secrets of a Robber's Cave\": A Terrific Romance. To which is Added, The Two Serpents, an Oriental Apologue. The Ossian epigraph appeared on the title page: \"A tale of horror, of murder, and of deeds done in darkness.\" Printed for Thomas Redriffe, Piccadilly. The price was \"Two-Pence\".\n\n\n"}
{"id": "26257152", "url": "https://en.wikipedia.org/wiki?curid=26257152", "title": "Steady state model", "text": "Steady state model\n\nIn cosmology, the steady state model is an alternative to the Big Bang theory of the evolution of the universe. In the steady state model, the density of matter in the expanding universe remains unchanged due to a continuous creation of matter, thus adhering to the perfect cosmological principle, a principle that asserts that the observable universe is basically the same at any time as well as at any place.\n\nWhile the steady state model enjoyed some popularity in the mid-20th century (though less popularity than the Big Bang theory), it is now rejected by the vast majority of cosmologists, astrophysicists and astronomers, as the observational evidence points to a hot Big Bang cosmology with a finite age of the universe, which the steady state model does not predict.\n\nIn the 13th century, Siger of Brabant authored the thesis \"The Eternity of the World\", which argued that there was no first man, and no first specimen of any particular: the physical universe is thus without any first beginning, and therefore eternal. Siger's views were condemned by the Pope in 1277.\n\nCosmological expansion was originally discovered through observations by Edwin Hubble. Theoretical calculations also showed that the static universe as modeled by Einstein (1917) was unstable and contradicted general relativity. The modern Big Bang theory is one in which the universe has a finite age and has evolved over time through cooling, expansion, and the formation of structures through gravitational collapse.\n\nThe steady state model asserts that although the universe is expanding, it nevertheless does not change its appearance over time (the perfect cosmological principle); the universe has no beginning and no end. This requires that matter be continually created in order to keep the universe's density from decreasing. Influential papers on steady state cosmologies were published by Hermann Bondi, Thomas Gold, and Fred Hoyle in 1948.\n\nIt is now known that Albert Einstein considered a steady state model of the expanding universe, as indicated in a 1931 manuscript, many years before Hoyle, Bondi and Gold. However, he quickly abandoned the idea.\n\nProblems with the steady state model began to emerge in the 1950s and 60s, when observations began to support the idea that the universe was in fact changing: bright radio sources (quasars and radio galaxies) were found only at large distances (therefore could have existed only in the distant past), not in closer galaxies. Whereas the Big Bang theory predicted as much, the steady state model predicted that such objects would be found throughout the universe, including close to our own galaxy. By 1961, statistical tests based on radio-source surveys had ruled out the steady state model in the minds of most cosmologists, although some proponents of the steady state insisted that the radio data were suspect.\n\nFor most cosmologists, the definitive refutation of the steady state model came with the discovery of the cosmic microwave background radiation in 1964, which was predicted by the Big Bang theory. The steady state model explained microwave background radiation as the result of light from ancient stars that has been scattered by galactic dust. However, the cosmic microwave background level is very even in all directions, making it difficult to explain how it could be generated by numerous point sources and the microwave background radiation shows no evidence of characteristics such as polarization that are normally associated with scattering. Furthermore, its spectrum is so close to that of an ideal black body that it could hardly be formed by the superposition of contributions from a multitude of dust clumps at different temperatures as well as at different redshifts. Steven Weinberg wrote in 1972,\n\nSince this discovery, the Big Bang theory has been considered to provide the best explanation of the origin of the universe. In most astrophysical publications, the Big Bang is implicitly accepted and is used as the basis of more complete theories.\n\nQuasi-steady state cosmology (QSS) was proposed in 1993 by Fred Hoyle, Geoffrey Burbidge, and Jayant V. Narlikar as a new incarnation of the steady state ideas meant to explain additional features unaccounted for in the initial proposal. The model suggests pockets of creation occurring over time within the universe, sometimes referred to as \"minibangs,\" \"mini-creation events,\" or \"little bangs\". After the observation of an accelerating universe, further modifications of the model were made.\n\nAstrophysicist and cosmologist Ned Wright has pointed out flaws in the model. These first comments were soon rebutted by the proponents. Wright and other mainstream cosmologists reviewing QSS have pointed out new flaws and discrepancies with observations left unexplained by proponents.\n\n"}
{"id": "34566625", "url": "https://en.wikipedia.org/wiki?curid=34566625", "title": "Tachyonic field", "text": "Tachyonic field\n\nA tachyonic field, or simply tachyon, is a field with an imaginary mass. Although tachyonic particles (particles that move faster than light) are a purely hypothetical concept that violate a number of essential physical principles, at least one field with imaginary mass is believed to exist. In general, tachyonic fields play an important role in physics and are discussed in popular books. Under no circumstances do any excitations of tachyonic fields ever propagate faster than light—the presence or absence of a tachyonic (imaginary) mass has no effect on the maximum velocity of signals, and so unlike faster-than-light particles there is no violation of causality.\n\nThe term \"tachyon\" was coined by Gerald Feinberg in a 1967 paper that studied quantum fields with imaginary mass. Feinberg believed such fields permitted faster than light propagation, but it was soon realized that Feinberg's model in fact did not allow for superluminal speeds. Instead, the imaginary mass creates an instability in the configuration: any configuration in which one or more field excitations are tachyonic will spontaneously decay, and the resulting configuration contains no physical tachyons. This process is known as tachyon condensation. A famous example is the condensation of the Higgs boson in the Standard Model of particle physics.\n\nIn modern physics, all fundamental particles are regarded as localized excitations of fields. Tachyons are unusual because the instability prevents any such localized excitations from existing. Any localized perturbation, no matter how small, starts an exponentially growing cascade that strongly affects physics everywhere inside the future light cone of the perturbation.\n\nAlthough the notion of a tachyonic imaginary mass might seem troubling because there is no classical interpretation of an imaginary mass, the mass is not quantized. Rather, the scalar field is; even for tachyonic quantum fields, the field operators at spacelike separated points still commute (or anticommute), thus preserving causality. Therefore, information still does not propagate faster than light, and solutions grow exponentially, but not superluminally (there is no violation of causality).\n\nThe \"imaginary mass\" really means that the system becomes unstable. The zero value field is at a local maximum rather than a local minimum of its potential energy, much like a ball at the top of a hill. A very small impulse (which will always happen due to quantum fluctuations) will lead the field to roll down with exponentially increasing amplitudes toward the local minimum. In this way, tachyon condensation drives a physical system that has reached a local limit and might naively be expected to produce physical tachyons, to an alternative stable state where no physical tachyons exist. Once the tachyonic field reaches the minimum of the potential, its quanta are not tachyons any more but rather are ordinary particles with a positive mass-squared, such as the Higgs boson.\n\nThere is a simple mechanical analogy that illustrates that tachyonic fields do not propagate faster than light, why they represent instabilities, and helps explain the meaning of imaginary mass (the mass squared being negative).\n\nConsider a long line of pendulums, all pointing straight down. The mass on the end of each pendulum is connected to the masses of its two neighbors by springs. Wiggling one of the pendulums will create two ripples that propagate in both directions down the line. As the ripple passes, each pendulum in its turn oscillates a few times about the straight down position. The speed of propagation of these ripples is determined in a simple way by the tension of the springs and the inertial mass of the pendulum weights. Formally, these parameters can be chosen so that the propagation speed is the speed of light. In the limit of an infinite density of closely spaced pendulums, this model becomes identical to a relativistic field theory, where the ripples are the analog of particles. Displacing the pendulums from pointing straight down requires positive energy, which indicates that the squared mass of those particles is positive.\n\nNow consider an initial condition where at time t=0, all the pendulums are pointing straight up. Clearly this is unstable, but at least in classical physics one can imagine that they are so carefully balanced they will remain pointing straight up indefinitely so long as they are not perturbed. Wiggling one of the upside-down pendulums will have a very different effect from before. The speed of propagation of the effects of the wiggle is identical to what it was before, since neither the spring tension nor the inertial mass have changed. However, the effects on the pendulums affected by the perturbation are dramatically different. Those pendulums that feel the effects of the perturbation will begin to topple over, and will pick up speed exponentially. Indeed, it is easy to show that any localized perturbation kicks off an exponentially growing instability that affects everything within its future \"ripple cone\" (a region of size equal to time multiplied by the ripple propagation speed). In the limit of infinite pendulum density, this model is a tachyonic field theory.\n\nTachyonic fields play an important role in modern physics. Perhaps the most famous example of a tachyon is the Higgs boson of the Standard model of particle physics. In its uncondensed phase, the square of the mass of the Higgs field is negative, and therefore, the associated particle is a tachyon.\n\nThe phenomenon of spontaneous symmetry breaking, which is closely related to tachyon condensation, plays a central part in many aspects of theoretical physics, including the Ginzburg–Landau and BCS theories of superconductivity.\n\nOther examples include the inflaton field in certain models of cosmic inflation (such as new inflation), and the tachyon of bosonic string theory.\n\nIn quantum field theory, a tachyon is a quantum of a field—usually a scalar field—whose squared mass is negative, and is used to describe spontaneous symmetry breaking: The existence of such a field implies the instability of the field vacuum; the field is at a local maximum rather than a local minimum of its potential energy, much like a ball at the top of a hill. A very small impulse (which will always happen due to quantum fluctuations) will lead the field (ball) to roll down with exponentially increasing amplitudes: it will induce tachyon condensation. Once the tachyonic field reaches the minimum of the potential, its quanta are not tachyons any more but rather have a positive mass-squared. The Higgs boson of the standard model of particle physics is an example.\n\nTechnically, the squared mass is the second derivative of the effective potential. For a tachyonic field the second derivative is negative, meaning that the effective potential is at a local maximum rather than a local minimum. Therefore, this situation is unstable and the field will roll down the potential.\n\nBecause a tachyon's squared mass is negative, it formally has an imaginary mass. This is a special case of the general rule, where unstable massive particles are formally described as having a complex mass, with the real part being their mass in usual sense, and the imaginary part being the decay rate in natural units.\n\nHowever, in quantum field theory, a particle (a \"one-particle state\") is roughly defined as a state which is constant over time; i.e., an eigenvalue of the Hamiltonian. An unstable particle is a state which is only approximately constant over time; If it exists long enough to be measured, it can be formally described as having a complex mass, with the real part of the mass greater than its imaginary part. If both parts are of the same magnitude, this is interpreted as a resonance appearing in a scattering process rather than particle, as it is considered not to exist long enough to be measured independently of the scattering process. In the case of a tachyon the real part of the mass is zero, and hence no concept of a particle can be attributed to it.\n\nEven for tachyonic quantum fields, the field operators at space-like separated points still commute (or anticommute), thus preserving the principle of causality. For closely related reasons, the maximum velocity of signals sent with a tachyonic field is strictly bounded from above by the speed of light. Therefore, information never moves faster than light regardless of the presence or absence of tachyonic fields.\n\nExamples for tachyonic fields are all cases of spontaneous symmetry breaking. In condensed matter physics a notable example is ferromagnetism; in particle physics the best known example is the Higgs mechanism in the standard model.\n\nIn string theory, tachyons have the same interpretation as in quantum field theory. However, string theory can, at least, in principle, not only describe the physics of tachyonic fields, but also predict whether such fields appear.\n\nTachyonic fields indeed arise in many versions of string theory. In general, string theory states that what we see as \"particles\" (electrons, photons, gravitons and so forth) are actually different vibrational states of the same underlying string. The mass of the particle can be deduced from the vibrations which the string exhibits; roughly speaking, the mass depends upon the \"note\" which the string sounds. Tachyons frequently appear in the spectrum of permissible string states, in the sense that some states have negative mass-squared, and therefore, imaginary mass. If the tachyon appears as a vibrational mode of an open string, this signals an instability of the underlying D-brane system to which the string is attached. The system will then decay to a state of closed strings and/or stable D-branes. If the tachyon is a closed string vibrational mode, this indicates an instability in spacetime itself. Generally, it is not known (or theorized) what this system will decay to. However, if the closed string tachyon is localized around a spacetime singularity, the endpoint of the decay process will often have the singularity resolved.\n\n\n"}
{"id": "20383842", "url": "https://en.wikipedia.org/wiki?curid=20383842", "title": "The Myth of Mars and Venus", "text": "The Myth of Mars and Venus\n\nThe Myth of Mars and Venus: Do Men and Women Really Speak Different Languages? is a 2008 book by Deborah Cameron, published by Oxford University Press.\n\nThe title refers to the central conceit of John Gray's 1992 book \"Men Are from Mars, Women Are from Venus\", which Cameron's book is partially a response to.\n\nCameron argues that \"what linguistic differences there are between men and women are driven by the need to construct and project personal meaning and identity.\" She challenges \"the idea that sex-differences might have biological rather than social causes\" as being more motivated by a reaction to politically correct attitudes than being derived from basic research.\n\nThe book argues that there is as much similarity and variation within each gender as between men and women. Cameron concludes that we have an urgent need to think about gender in more complex ways than the prevailing myths and stereotypes allow.\n"}
{"id": "36401496", "url": "https://en.wikipedia.org/wiki?curid=36401496", "title": "Theory of mediation", "text": "Theory of mediation\n\nThe theory of mediation, which is the principal referent of the research group of the Interdisciplinary Laboratory for Language Research (L.I.R.L.), is a theoretic model developed at Rennes (France) since the 1960' by Professor Jean Gagnepain, linguist and epistemologist. This model, whose principles Jean Gagnepain has methodically set forth in his three volume study \"On Meaning\" (\"Du Vouloir Dire\"), covers the whole field of the human sciences. One essential feature of the theory is that it seeks to find a kind of experimental verification of its theorems in the clinic of psychopathology. For this reason, the theory presents itself as a \"clinical anthropology\".\n\nThe theoretic model developed by Gagnepain and his research group at Rennes has inspired the work of professors and researchers in a number of European countries and in the United States in a wide variety of disciplinary fields, among them linguistics, literature, psychology, art history, archeology, psychoanalysis, theology. Its aim is deliberately trans-disciplinary - or, as Gagnepain humorously puts it, the theory of mediation cultivates \"in-discipline\".\n\nThis model, originally developed with respect to language, today takes for its object the entirety of what is called \"the cultural\", that is, the dimension that specifies human beings and distinguishes them from other living species. In other words, \"the cultural\" constitutes the specific order of reality in which only human beings participate. It is the cultural order that permits human beings, while remaining natural beings, to constantly transcend their natural being in abstracting themselves from it.\n\nThe theory of mediation understands the cultural order - more simply, culture - not as the totality of the essential works of a society, nor as the general state of a given civilization, but as the ensemble of properly human capacities which, absent pathological conditions, all human beings share regardless of their historical epoch or geographical setting. For the theory of mediation, culture and reason - the \"rationality\" which philosophers have discussed for centuries - are identical. The human sciences, understood as the theory of mediation understands them, take up in their own distinctive fashion the questions which philosophy has treated only speculatively.\n\nGagnepain's work shows, on the basis of what the clinic forces us to recognize, that human reason is diffracted. In other words, rationality in human beings has several different forms which the clinic requires us to dissociate. Reason is logical, to be sure, but it is equally and just as fundamentally technical, ethnical, and ethical. There is no hierarchy among these different \"planes\" or \"levels\" of rationality that constitute psychic life.\n\nOn each of these planes or levels, human being mediate their relations to the world and others (thus the term \"mediation\"). Unlike the other animals, human beings are not limited to what their immediate physiological capacities allow them to grasp. They can stand back from, or take a distance from, their natural insertion in the world and can elaborate those cultural mediations that are constitutive of a properly human reality.\n\nIn Kantian terms, it is a question of passing from a description of an already constituted reason to an explanation of a constituting reason. It is a question, therefore, of accounting for that in human beings which, without their knowing it, makes them capable of posing the world - and of posing it not only in one way, by knowing it, as the traditional analysis holds, but in four different ways on the basis of four different capacities.\n\nTo be sure, human beings manifest the world in and across the words they speak : with them, they designate the world and explain it to themselves. In so doing, they realize their logical capacity. Human beings also manifest the world in and across their tools : with them, they fabricate the world and, in doing so, they realize their technical capacity. Human beings likewise manifest the world in originating their histories and societies, realizations not of their logical or their technical capacities but of their ethnic capacity. Finally, human beings manifest the world in the norms and regulations to which they submit their desires. Here is it question of their ethical capacity.\n\nThe possible autonomisation of the four planes (or levels or modes) of our rationality is revealed by the clinic. Although \"normally\" the four modes of our rationality function together in such a way that it is hardly possible to distinguish them, pathologically it does become possible to distinguish them, as, for example, when one mode of rationality ceases to function in an afflicted person whereas the others continue to do so. Each plane of rationality has its specific pathology. The pathology specific to the logical plane is aphasia; the pathology specific to the technical plane is atechnia ; the pathology specific to the ethnical plane is psychosis (and perversion) ; the pathology specific to the ethical plane is neurosis (and psychopathic conditions).\n\nIn other words, pathology dissociates what normally cannot be distinguished and puts into evidence processes otherwise unseen. In this way, as Freud recognized, pathology provides a veritable analysis, that is, a breakdown, of the human psyche. Gagnepain therefore makes it a methodological rule \"to admit or to impute to a system only those dissociations that are pathologically verifiable.\"\n\n\n"}
{"id": "767316", "url": "https://en.wikipedia.org/wiki?curid=767316", "title": "Thomas Godfrey (inventor)", "text": "Thomas Godfrey (inventor)\n\nThomas Godfrey (December 1704 – December 1749) was an optician and inventor in the American colonies, who around 1730 invented the octant. At approximately the same time an Englishman, John Hadley, also invented the octant independently.\n\nGodfrey was born on his family's farm in Bristol Township, near Germantown, Philadelphia, Pennsylvania.\n\nBenjamin Franklin describes Godfrey at length in his \"Autobiography\", referring to him as a \"Great Mathematician\" who nevertheless was \"not a pleasing Companion\", demanding in conversations a \"universal Precision in every thing said.\"\n\nGodfrey's son, also Thomas Godfrey, died at only 26, but had already published several popular works, including The Prince of Parthia, a play that remains well known to this day.\n"}
{"id": "18918161", "url": "https://en.wikipedia.org/wiki?curid=18918161", "title": "Vale do Sinos Technology Park, VALETEC Park", "text": "Vale do Sinos Technology Park, VALETEC Park\n\nThe Vale do Sinos Regional Development Association, VALETEC, was established in 1998 as a non-profit private civil partnership. Its goal is to promote technological development in the Vale do Sinos region by stimulating regional integration, providing incentives for entrepreneurship, while creating, attracting, hosting, and developing companies. In order to achieve this goal, VALETEC develops and manages environments for technological innovation, such as incubators, entrepreneurial condominiums, technology parks and complexes, as well as other initiatives aligned with this focus. VALETEC's main commitment is to contribute for the improvement of the Vale do Sinos quality of life, while forging links between educational and research institutions and the society, governments, and companies. This contributes to generate jobs and wealth, boosting the sustainable development of the Vale do Sinos technopolitan project.\n\nOne of VALETEC's main initiatives was the creation of the Vale do Sinos Technology Park, VALETEC Park. The park currently works as a regional, multicampi and multisector enterprise, with its main axis following the path of highways RS-239 and BR-116, the Innovation Route. The first segment of the VALETEC Park was launched in the city of Campo Bom, in December, 2004. The second segment will consist of an urban park, to be established within the Hamburgo Velho's Historical Center, in the city of Novo Hamburgo. The park currently encompasses important educational institutions, research centers, labs, companies with intensive focus on knowledge, advanced services, incubators, entrepreneurial centers, facilities for the installation and expansion of companies, living areas and rooms for shared use.\n\nThe VALETEC Park is ready to host entrepreneurs, new or consolidated companies and institutions that develop or plan on developing clean technology, applied to the following priority areas: agriculture, animal husbandry, and agribusiness; automation and informatics; biotechnology; leather and footwear industry; design; energy; environment; telecommunications; and creative economy. In the city of Campo Bom, the park segment has capacity for over 120 companies, research centers, scientific and economic development organizations, and advanced service providers. The park's initially encompasses an area of 365,000 m², surrounded by lovely sceneries, a result of our deep respect for the environment. The surroundings have been keeping up with the park's growth. This is the result of the expansion and diversification of existing companies, along with the creation of spaces to host new business models and new areas for companies to operate in.\n\nCompanies can be installed in the Campo Bom segment of the VALETEC Park under three different modalities: inside the Feevale's Technology Incubator, ITEF, for pre-incubation and incubation; in business accelerators and business condominiums, such as the Alberto Santos Dumont Business Center and the Montserrat Business Condominium; and in individual facilities, which companies may choose to either purchase or rent. Feevale is a regional community university (public, yet not state-owned) which operates in the Campo Bom segment of the VALETEC Park through its Academic Extension Center. This is a space for learning, research and extension activities, with an auditorium, event hall, meeting rooms, restaurant and areas for partners and advanced service providers. In the surroundings of the Academic Extension Center, there are several green preservation areas which make perfect spots for relaxing outdoors, with plenty of shadow to contemplate nature and exercise creativity.\n\nThe Innovation Route represents the VALETEC Park's main axis. It is located along the path of highways RS-239 and BR-116, and its initial project encompasses eight cities. After the Campo Bom segment was established, an opportunity to expand the park was envisioned. This was motivated by the fact that the city's master plan has, since the 1960s, been preserving a minimum of 350 meters on each side of highway RS-239 exclusively for business use. The project received resources from the Research and Projects Financing, FINEP, the Brazilian Innovation Agency, a division of the Ministry of Science and Technology, in order to plan, develop, and implement the best practices identified by VALETEC during its international missions. The studies performed made it possible to identify several advantages that ensured the VALETEC Park's success as a regional enterprise.\n\nOne of the VALETEC's stylemarks is shared management through a network of alliances. Companies that associate to VALETEC also become members of the VALETEC Park, and have access to a wide range of advantages, such as a relationship network that enables them to develop cooperative projects and receive financial funding for innovation. In addition, members have access to the shared infrastructure and high added value services, along with the possibility of establishing relationships with associated educational and research institutions.\n"}
{"id": "20644288", "url": "https://en.wikipedia.org/wiki?curid=20644288", "title": "William Elvis Sloan", "text": "William Elvis Sloan\n\nWilliam Elvis Sloan I (October 1867 – June 25, 1961) invented the Flushometer flushing mechanism for toilets and urinals. It is installed in millions of commercial, institutional and industrial restrooms worldwide.\n\nHe was born in Liberty, Missouri in October 1867. He was an apprentice pipe fitter in Missouri then moved to Chicago, Illinois. He married Bertha Moore (1874-?) in 1898 in Chicago, Illinois and they had a child, Edith Marie Sloan (1913-?).\n\nIn 1906, he founded Sloan Valve Company, which is now headquartered in Franklin Park, Illinois.\n\nHe died on June 25, 1961 in Chicago, Illinois and was buried in Oak Park, Illinois.\n\nWith the exception of a period of time in the 1940s and early 1950s, Sloan Valve has been under the leadership of W.E. Sloan’s descendants. He had a grandson William Elvis Sloan II (1941–2001). \n"}
{"id": "35936877", "url": "https://en.wikipedia.org/wiki?curid=35936877", "title": "Wonderful life theory", "text": "Wonderful life theory\n\nIn biology, the wonderful life theory, also known as contingency theory, postulates that after hundreds of different phyla evolved during the Cambrian period, many of them subsequently became extinct, leaving the relatively few phyla that exist today. The theory was first suggested in 1989 by Stephen Jay Gould in his book \"Wonderful Life\".\n"}
