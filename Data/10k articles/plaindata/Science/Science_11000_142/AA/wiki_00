{"id": "32738418", "url": "https://en.wikipedia.org/wiki?curid=32738418", "title": "Acoustic membrane", "text": "Acoustic membrane\n\nAn acoustic membrane is a thin layer that vibrates and is used in acoustics to produce or transfer sound, such as a drum, microphone, or loudspeaker.\n\n"}
{"id": "58818207", "url": "https://en.wikipedia.org/wiki?curid=58818207", "title": "Adia Benton", "text": "Adia Benton\n\nAdia Benton is an American cultural and medical anthropologist. She won the 2017 Rachel Carson Prize for her book \"HIV Exceptionalism: Development Through Disease in Sierra Leone\" from the Society for Social Studies of Science. She is currently an Assistant Professor of Anthropology and African Studies at Northwestern University.\n"}
{"id": "37782673", "url": "https://en.wikipedia.org/wiki?curid=37782673", "title": "Airborne Launch Assist Space Access", "text": "Airborne Launch Assist Space Access\n\nAirborne Launch Assist Space Access, or DARPA ALASA is a canceled program of the US defense technology agency DARPA \"designed to produce a rocket capable of launching a 100-pound satellite into low Earth orbit for less than $1 million.\" The program was conceived, then announced in 2011, and funded development work began in 2012. The project was terminated in late 2015.\n\nTraditional launch methods of satellites are too costly to put small payloads into orbit without a larger payload accompanying it to make the launch worth the expense. Current launches of sub-100-pound satellites are performed as \"piggyback payloads\" on launches of much larger spacecraft, usually headed for geostationary orbit, and are released at the altitude of the primary payload. In addition, range costs of operating from ground-based infrastructure have escalated as they have aged, accounting for up to 35 percent of the launch cost. This restricts the number of light satellite launches to 10-12 per year, which could be increased if small payloads could be launched into space affordably and without ground range constraints. Air-launching satellites was first seriously considered during the 1950s and 1960s, but small payloads in the 100-pound class at the time did not have effective capabilities, so the method was overlooked. The first air-launched rocket to put a satellite into orbit was the Orbital Sciences Corporation Pegasus, which did it on 13 June 1990; however, it is currently deployed from a heavily modified and expensive Lockheed L-1011 airliner. The ALASA program's objective is to use an unmodified aircraft platform (except for software) that does not have to be dedicated to the mission to place a 100 lb satellite into orbit that requires only 24 hours notice to integrate and launch the payload, with the ability to re-plan the launch in flight and relocate the aircraft to any civilian airport or military airfield in a crisis situation, while using onboard GPS/inertial position reporting rather than ground-based radar tracking.\n\nA program solicitation was announced in 2011, and six companies were awarded contracts in July 2012. The six awardees who signed phase 1 contracts with DARPA included:\n\nIn the first phase, Boeing, Lockheed Martin and Virgin Galactic were funded to explore different ALASA system concepts while Northrop Grumman, Space Information Laboratories and Ventions were contracted to work on enabling technologies that could be used by any or all of the system teams.\n\nIn December 2012, DARPA announced that the ALASA program would provide the launch vehicle booster for another DARPA program that is intending to release a \"constellation of 24 micro-satellites (~ range) each with 1-meter imaging resolution.\"\n\nIn May 2013, DARPA requested for a second year of ALASA program funding in spring 2013.\n\nIn March 2014, Boeing won the large phase 2 ALASA contract from DARPA. Boeing will use their F-15E Strike Eagle fighter to carry the ALASA rocket up to , then release the rocket to ignite and carry itself into orbit. Using a modified fighter-jet to launch the rocket would increase satellite launch sites from four locations (Cape Canaveral Air Force Station, Florida; Vandenberg Air Force Base, California; Wallops Flight Facility, Virginia; and Kodiak Island, Alaska) to any available runway. The cost to put a microsatellite into orbit is targeted at $1 million, a decrease of 66 percent. A demonstration launch was hoped for in FY 2015.\n\nThe F-15E launch vehicle would have required no modifications to launch the ALASA payload, not even software, because the rocket will use the same communications protocols as a typically mounted weapons system. This enables the aircraft to continue flying other missions as a cost benefit over being specialized. The rocket will also feature new design technologies to lower complexity and costs. It will be powered by a monopropellant, a combination of nitrous oxide and acetylene, and mixed together in one propellant tank slightly below room temperature; the propellant choice is a dramatic simplification of the complexity of the rocket vehicle. Rocket design is also unconventional, mounting the four engines for the first stage at the front rather than rear. DARPA plans to develop a second, smaller launch system called the Small Air Launch Vehicle to Orbit (SALVO) to understand operations cost, demonstrate new technologies like battery-powered pumps for the rocket’s engines, and provide overall program before ALASA is launched. SALVO was planned to launch in spring 2015, six to nine months before the first ALASA flight in late 2015. 12 flights were to be conducted through mid-2016 from Eglin Air Force Base, Florida over the Atlantic Ocean.\n\nBy June 2015, DARPA and the Air Force had reportedly began SALVO flights, potentially having already commenced them to counter Chinese and Russian electronic and infrared surveillance; this could mean ALASA would give the U.S. a \"stealth satellite launch\" capability.\n\nThe program had a budget of \" for the 18-month first phase through September 2013, when [DARPA] planned another competition to select at least one team to conduct up to 36 launches in 2015 [in order to] to demonstrate [the Alasa system] at a persuasive scale.\" has been requested for the second year.\n\nAfter DARPA announced in December 2012 that the ALASA air-launched microsat launch vehicle would be chosen to launch the DARPA SeeMe program micro-satellites, some questions arose as to why other commercial options currently in development were not considered, such as the Virgin Galactic LauncherOne and the XCOR Aerospace Lynx.\n\nDARPA terminated the program in late 2015, due to safety concerns with the unique monopropellant, NA-7, which exploded in two ground tests. It was reported that development of the propellant would continue, as would efforts to apply technologies developed in the program.\n\n\n"}
{"id": "4153924", "url": "https://en.wikipedia.org/wiki?curid=4153924", "title": "Andromeda X", "text": "Andromeda X\n\nAndromeda X (And 10) is a dwarf spheroidal galaxy about 2.9 million light-years away from the Sun in the constellation Andromeda. Discovered in 2005, And X is a satellite galaxy of the Andromeda Galaxy (M31).\n\n"}
{"id": "7458308", "url": "https://en.wikipedia.org/wiki?curid=7458308", "title": "Chaos: Making a New Science", "text": "Chaos: Making a New Science\n\nChaos: Making a New Science is a debut non-fiction book by James Gleick that initially introduced the principles and early development of the chaos theory to the public. It was a finalist for the National Book Award and the Pulitzer Prize in 1987, and was shortlisted for the Science Book Prize in 1989. The book was published on October 29, 1987 by Viking Books.\n\nThe first popular book about chaos theory, it describes the Mandelbrot set, Julia sets, and Lorenz attractors without using complicated mathematics. It portrays the efforts of dozens of scientists whose separate work contributed to the developing field. The text remains in print and is widely used as an introduction to the topic for the mathematical layperson. An enhanced ebook edition was released by Open Road Media in 2011, adding embedded video and hyperlinked notes.\n\nRobert Sapolsky said that, \"Chaos is the first book since \"Baby Beluga\" where I've gotten to the last page and immediately started reading it over again from the front.\"\n\nFreeman Dyson critiqued the book for omitting the earlier work of Dame Mary L. Cartwright and J. E. Littlewood, which he credits as forming the foundation of chaos theory, but also praised it as a popular account.\n\n"}
{"id": "4490666", "url": "https://en.wikipedia.org/wiki?curid=4490666", "title": "Chukri System", "text": "Chukri System\n\nThe Chukri System is a debt bondage or forced labour system found in Kidderpore and other parts of West Bengal. Under this system a female can be coerced into prostitution in order to pay off debts. She generally works without pay for one year or longer in order to repay a supposed debt to the brothel owner for food, clothes, make-up, and living expenses.\n\nThe system creates a workforce of people virtually enslaved to their creditors, and constitutes one of the primary causes for women entering the sex trade. The system flourishes primarily in West Bengal or Calcutta. The name is used also in Bangladesh.\n\n"}
{"id": "4006709", "url": "https://en.wikipedia.org/wiki?curid=4006709", "title": "DeLong Star Ruby", "text": "DeLong Star Ruby\n\nThe DeLong Star Ruby, a oval cabochon star ruby, was discovered in Burma in the 1930s. It was sold by Martin Ehrmann to Edith Haggin DeLong for , who then donated it to the American Museum of Natural History in New York City in 1937.\n\nOn October 29, 1964, the DeLong star ruby was one of a number of precious gems stolen in a notorious jewelry heist by Jack Roland Murphy and two accomplices. Some of the stolen gems (notably, the Star of India and the Midnight Star) were recovered in a bus depot locker in January 1965; however, the DeLong ruby was not among them. After months of negotiation, the unknown holder of the ruby agreed, through third parties, to ransom it for $25,000. The ransom was paid by wealthy Florida businessman John D. MacArthur and he was present on September 2, 1965, when the ruby was recovered at the designated drop off site: a phone booth at a service plaza on the Sunshine State Parkway near Palm Beach, Florida.\n\n"}
{"id": "12076308", "url": "https://en.wikipedia.org/wiki?curid=12076308", "title": "EPOXI", "text": "EPOXI\n\nEPOXI is a compilation of NASA Discovery program missions led by the University of Maryland and principal investigator Michael A'Hearn, with co-operation from the Jet Propulsion Laboratory and Ball Aerospace. \"EPOXI\" uses the \"Deep Impact\" spacecraft in a campaign consisting two missions: the Deep Impact Extended Investigation (DIXI) and Extrasolar Planet Observation and Characterization (EPOCh). \"DIXI\" aimed to send the \"Deep Impact\" spacecraft on a flyby of another comet, after its primary mission was completed in July 2005, while \"EPOCh\" saw the spacecraft's photographic instruments as a space observatory, studying extrasolar planets.\n\n\"DIXI\" successfully sent the \"Deep Impact\" spacecraft on a flyby of comet Hartley 2 on November 4, 2010, revealing a \"hyperactive, small and feisty\" comet, after three gravity assists from Earth in December 2007, December 2008 and June 2010. The \"DIXI\" mission was not without problems, however; the spacecraft had initially been targeted for a December 5, 2008 flyby of comet Boethin, though, the comet could not be located, and was later declared a lost comet, prompting mission planners to reorganize a flyby of an alternative target, Hartley 2. After its flyby of Hartley 2, the spacecraft was also set to make a close flyby of the Apollo asteroid (163249) 2002 GT in 2020. The mission was suspended altogether, however, after contact with the spacecraft was suddenly lost in August 2013 and attempts to re-establish contact in the month following had failed. Mission scientists theorized that a Y2K-like problem had plagued the spacecraft's software.\n\nThe \"Deep Impact\" mission was finished with the visit to comet Tempel 1. But the spacecraft still had plenty of maneuvering fuel left, so NASA approved a second mission, called EPOXI (Extrasolar Planet Observation and Deep Impact Extended Investigation), which included a visit to a second comet (DIXI component) as well as observations of extrasolar planets (EPOCh component).\n\nOn July 21, 2005, \"Deep Impact\" executed a trajectory correction maneuver that placed the spacecraft on course to fly past Earth on December 31, 2007. The maneuver allowed the spacecraft to use Earth's gravity to begin a new mission in a path towards another comet. In January 2008 \"Deep Impact\" began studying the stars with several known extrasolar planets in an attempt to find other such stars nearby. The larger of the spacecraft's two telescopes attempts to find the planets using the transit method.\n\nThe initial plan was for a December 5, 2008 flyby of Comet Boethin, with the spacecraft coming within . The spacecraft did not carry a second impactor to collide with the comet and would observe the comet to compare it to various characteristics found on 9P/Tempel. A'Hearn, the \"Deep Impact\" team leader reflected on the upcoming project at that time: \"We propose to direct the spacecraft for a flyby of Comet Boethin to investigate whether the results found at Comet Tempel 1 are unique or are also found on other comets.\" He explained that the mission would provide only about half of the information collected during the collision with Tempel 1 but at a fraction of the cost. (EPOXI’s low mission cost of $40 million is achieved by reusing the existing Deep Impact spacecraft.) \"Deep Impact\" would use its spectrometer to study the comet's surface composition and its telescopes for viewing the surface features.\n\nHowever, as the Earth gravity assist approached, astronomers were unable to locate Comet Boethin, which is too faint to be observed. Consequently, its orbit could not be calculated with sufficient precision to permit a flyby. Instead, the team decided to send \"Deep Impact\" to comet 103P/Hartley requiring an extra two years. NASA approved the additional funding required and retargeted the spacecraft. Mission controllers at the Jet Propulsion Laboratory began redirecting EPOXI on November 1, 2007. They commanded the spacecraft to perform a three-minute rocket burn that changed the spacecraft's velocity. EPOXI’s new trajectory set the stage for three Earth flybys, the first on December 31, 2007. This placed the spacecraft into an orbital \"holding pattern\" so that it could encounter comet 103P/Hartley in 2010.\n\n\"It's exciting that we can send the \"Deep Impact\" spacecraft on a new mission that combines two totally independent science investigations, both of which can help us better understand how solar systems form and evolve,\" said in December 2007 \"Deep Impact\" leader and University of Maryland astronomer Michael A'Hearn who is principal investigator for both the overall EPOXI mission and its DIXI component.\n\nIn June 2009, EPOXI's spectrometer scanned the Moon on its way to Hartley, and discovered traces of \"water or hydroxyl\", confirming a Moon Mineralogy Mapper observation — a discovery announced in late September, 2009.\n\nBefore the 2008 flyby to re-orient for the comet 103P/Hartley encounter, the spacecraft used High Resolution Instrument, the larger of its two telescopes, to perform photometric observations of previously discovered transiting extrasolar planets from January to August 2008. The goal of photometric observations is to measure the quantity of light, not necessarily resolve an image. An aberration in the primary mirror of the HRI allowed the HRI to spread the light from observations over more pixels without saturating the CCD, effectively obtaining better data. A total of 198,434 images were exposed. EPOCh's goals were to study the physical properties of giant planets and search for rings, moons and planets as small as three Earth masses. It also looked at Earth as though it were an extrasolar planet to provide data that could characterize Earth-type planets for future missions, and it imaged the Earth over 24 hours to capture the Moon passing in front on 2008-05-29.\n\nThe spacecraft used Earth's gravity for the second gravity assist in December 2008 and made two distant flybys of Earth in June and December 2009. On May 30, 2010 it successfully fired its engines for an 11.3 second trajectory correction maneuver, for a velocity change (ΔV) of , in preparation for the third Earth flyby on June 27. Observations of 103P/Hartley began on September 5 and ended November 25, 2010. For a diagram of the EPOXI solar orbits see here.\n\nThe mission's closest approach to 103P/Hartley occurred at 10 am EDT on 4 November 2010, passing to within of this small comet. The flyby speed was 12.3 km/s. The spacecraft employed the same suite of three science instruments—two telescopes and an infrared spectrometer—that the Deep Impact spacecraft used during its prime mission to guide an impactor into comet Tempel 1 in July 2005 and observe the results.\n\nEarly results of the observations show that the comet is powered by dry ice, not water vapor as was previously thought.The images were clear enough for scientists to link jets of dust and gas with specific surface features.\n\n\"When comet Boethin could not be located, we went to our backup, which is every bit as interesting but about two years farther down the road,\" said Tom Duxbury, EPOXI project manager at NASA's Jet Propulsion Laboratory in Pasadena, California. \"Hartley 2 is scientifically just as interesting as comet Boethin because both have relatively small, active nuclei,\" said Michael A'Hearn, principal investigator for EPOXI at the University of Maryland, College Park.\n\nIn November 2010, EPOXI was used to make some test-training deep sky observations, using the MRI camera that is optimised for cometary imagery. Images were made of the Dumbbell Nebula (M27), the Veil Nebula (NGC6960) and the Whirlpool Galaxy (M51a).\n\n"}
{"id": "4217297", "url": "https://en.wikipedia.org/wiki?curid=4217297", "title": "Electromagnetic hypersensitivity", "text": "Electromagnetic hypersensitivity\n\nElectromagnetic hypersensitivity (EHS) is a claimed sensitivity to electromagnetic fields, to which negative symptoms are attributed. EHS has no scientific basis and is not a recognised medical diagnosis. Claims are characterized by a \"variety of non-specific symptoms, which afflicted individuals attribute to exposure to electromagnetic fields\".\nThose who are self-described with EHS report adverse reactions to electromagnetic fields at intensities well below the maximum levels permitted by international radiation safety standards. The majority of provocation trials to date have found that such claimants are unable to distinguish between exposure and non-exposure to electromagnetic fields. A systematic review in 2005 showed no convincing scientific evidence for symptoms being caused by electromagnetic fields. Since then, several double-blind experiments have shown that people who report electromagnetic hypersensitivity are unable to detect the presence of electromagnetic fields and are as likely to report ill health following a sham exposure as they are following exposure to genuine electromagnetic fields, suggesting the cause in these cases to be the nocebo effect.\nA 2005 review by the UK Health Protection Agency and a 2006 systematic review each evaluated the evidence for various medical, psychological, behavioral, and alternative treatments for EHS and each found that the evidence-base was limited and not generalizable, but that the best evidence favored cognitive behavioural therapy. As of 2005, WHO recommended that people presenting with claims of EHS be evaluated to determine if they have a medical condition that may be causing the symptoms the person is attributing to EHS, that they have a psychological evaluation, and that the person's environment be evaluated for issues like air or noise pollution that may be causing problems.\nSome people who feel they are sensitive to electromagnetic fields may seek to reduce their exposure or use alternative medicine. Government agencies have enforced false advertising claims against companies selling devices to shield against EM radiation.\n\nThere are no specific symptoms associated with claims of EHS and reported symptoms range widely between individuals. They include headache, fatigue, stress, sleep disturbances, skin prickling, burning sensations and rashes, pain and ache in muscles and many other health problems. In severe cases such symptoms can be a real and sometimes disabling problem for the affected person, causing psychological distress. There is no scientific basis to link such symptoms to electromagnetic field exposure.\n\nThe prevalence of some reported symptoms is geographically or culturally dependent and does not imply \"a causal relationship between symptoms and attributed exposure\". Many such reported symptoms overlap with other syndromes known as symptom-based conditions, functional somatic syndromes, and IEI (idiopathic environmental intolerance).\n\nThose reporting electromagnetic hypersensitivity will usually describe different levels of susceptibility to electric fields, magnetic fields, and various frequencies of electromagnetic waves. Devices implicated include fluorescent and low-energy lights, mobile, cordless/portable phones, and Wi-Fi. A 2001 survey found that people self-diagnosing as EHS related their symptoms most frequently to mobile phone base stations (74%), followed by mobile phones (36%), cordless phones (29%), and power lines (27%). Surveys of electromagnetic hypersensitivity sufferers have not been able to find any consistent pattern to these symptoms.\n\nMost blinded conscious provocation studies have failed to show a correlation between exposure and symptoms, leading to the suggestion that psychological mechanisms play a role in causing or exacerbating EHS symptoms. In 2010, Rubin et al. published a follow-up to their 2005 review, bringing the totals to 46 double-blind experiments and 1175 individuals with self-diagnosed hypersensitivity. Both reviews found no robust evidence to support the hypothesis that electromagnetic exposure causes EHS, as have other studies. They also concluded that the studies supported the role of the nocebo effect in triggering acute symptoms in those with EHS.\n\nSome other types of studies suggest evidence for symptoms at non-thermal levels of electromagnetic exposure. A review in 2010 of ten studies on neurobehavioral and cancer outcomes near cell phone base stations found eight with increased prevalence, including sleep disturbance and headaches. Since 1962, the microwave auditory effect or tinnitus has been shown from radio frequency exposure at levels below significant heating. Studies during the 1960s in Europe and Russia claimed to show effects on humans, especially the nervous system, from low energy RF radiation; the studies were disputed at the time.\n\nOther studies on sensitivity have looked at therapeutic procedures using non-thermal electromagnetic exposure, genetic factors, an alteration in mast cells, oxidative stress, protein expression and voltage-gated calcium channels. Mercury release from dental amalgam and heavy metal toxicity have also been implicated in exposure effects and symptoms. Another line of study has been the nature of hyper-sensitivity or intolerance and the range of environmental exposures which may be related to it. Some 80% of people with self-diagnosed electromagnetic intolerance also claim intolerance to low levels of chemical exposure.\n\nElectromagnetic hypersensitivity is not an accepted diagnosis; medically there is no case definition or clinical practice guideline and there is no specific test to identify it, nor is there an agreed-upon definition with which to conduct clinical research.\n\nComplaints of electromagnetic hypersensitivity may mask organic or psychiatric illness. Diagnosis of those underlying conditions involves investigating and identifying possible known medical causes of any symptoms observed. It may require both a thorough medical evaluation to identify and treat any specific conditions that may be responsible for the symptoms, and a psychological evaluation to identify alternative psychiatric/psychological conditions that may be responsible or contribute to the symptoms.\n\nSymptoms may also be brought on by imagining that exposure is causing harm, an example of the nocebo effect. Studies have shown that reports of symptoms are more closely associated with belief that one is being exposed than with any actual exposure.\n\nA 2006 systematic review and a 2005 review by the UK Health Protection Agency each evaluated the evidence for various medical, psychological, behavioral, and alternative treatments for EHS and each found that the evidence-base was limited and not generalizable. The conclusion of the 2006 review stated: \"The evidence base concerning treatment options for electromagnetic hypersensitivity is limited and more research is needed before any definitive clinical recommendations can be made. However, the best evidence currently available suggests that cognitive behavioural therapy is effective for patients who report being hypersensitive to weak electromagnetic fields.\"\n\nAs of 2005, WHO recommended that people presenting with claims of EHS be evaluated to determine if they have a medical condition that may be causing the symptoms the person is attributing to EHS, that they have a psychological evaluation, and that the person's environment be evaluated for issues like air or noise pollution that may be causing problems.\n\nThe prevalence of claimed electromagnetic hypersensitivity has been estimated as being between a few cases per million to 5% of the population depending on the location and definition of the condition.\n\nIn 2002, a questionnaire survey of 2,072 people in California found that the prevalence of self-reported electromagnetic hypersensitivity within the sample group was 3% (95% CI 2.8–3.68%), with electromagnetic hypersensitivity being defined as \"being allergic or very sensitive to getting near electrical appliances, computers, or power lines\" (response rate 58.3%).\n\nA similar questionnaire survey from the same year in Stockholm County (Sweden), found a 1.5% prevalence of self-reported electromagnetic hypersensitivity within the sample group, with electromagnetic hypersensitivity being defined as \"hypersensitivity or allergy to electric or magnetic fields\" (response rate 73%).\n\nA 2004 survey in Switzerland found a 5% prevalence of claimed electromagnetic hypersensitivity in the sample group of 2,048.\n\nIn 2007, a UK survey aimed at a randomly selected group of 20,000 people found a prevalence of 4% for symptoms self-attributed to electromagnetic exposure.\n\nA group of scientists also attempted to estimate the number of people reporting \"subjective symptoms\" from electromagnetic fields for the European Commission. In the words of a HPA review, they concluded that \"the differences in prevalence were at least partly due to the differences in available information and media attention around electromagnetic hypersensitivity that exist in different countries. Similar views have been expressed by other commentators.\"\n\nIn 2010, a cell tower operator in South Africa revealed at a public meeting that the tower that nearby residents were blaming for their current EHS symptoms had been turned off over six weeks prior to the meeting, thus making it a highly unlikely cause of EHS symptoms.\n\nIn February 2014, the UK Advertising Standards Authority found that claims of harm from electromagnetic radiation, made in a product advertisement, were unsubstantiated and misleading.\n\nPeople have filed lawsuits to try to win damages due to harm claimed from electromagnetic radiation. In 2012, a New Mexico judge dismissed a lawsuit in which one person sued his neighbor, claiming to have been harmed by EM radiation from his neighbor's cordless telephones, dimmer switches, chargers, Wi-Fi and other devices. The plaintiff brought the testimony of his doctor, who also believed she had EHS, and a person who represented himself as a neurotoxicologist; the judge found none of their testimony credible. In 2015, parents of a boy at a school in Southborough, Massachusetts alleged that the school's Wi-Fi was making the boy sick.\n\nIn November 2015, a depressed teenage girl in England committed suicide. Her suicide was attributed to EHS by her parents and taken up by tabloids and EHS advocates.\n\nSome people who feel they are sensitive to electromagnetic fields self-treat by trying to reduce their exposure to electromagnetic sources by avoiding sources of exposure, disconnecting or removing electrical devices, shielding or screening of self or residence, and alternative medicine. In Sweden, some municipalities provide disability grants to people who claim to have EHS in order to have abatement work done in their homes even though the public health authority does not recognize EHS as an actual medical condition; towns in Halland do not provide such funds and this decision was challenged and upheld in court.\n\nThe United States National Radio Quiet Zone is an area where wireless signals are restricted for scientific research purposes, and some people who believe they have EHS have relocated there seeking relief.\n\nGro Harlem Brundtland, former prime minister of Norway and Director general of the World Health Organization, claims to suffer from EHS. In 2015 she said that she had been sensitive for 25 years.\n\nIn the fictional television crime drama \"Better Call Saul\", the character Charles \"Chuck\" McGill is depicted as experiencing the symptoms of EHS. In the episode \"Alpine Shepherd Boy\", a skeptical doctor surreptitiously operates a switch controlling the electronics in Chuck's hospital bed. This does not affect his symptoms, suggesting that his electromagnetic hypersensitivity is not genuine. A similar instance of Chuck's symptoms being objectively psychosomatic is seen on the episode \"Chicanery\". Although a fully charged cellphone battery is planted on his person without his knowledge, Chuck experiences no adverse effects by having an electronic device on his body for close to two hours. When this fact is revealed to him, he is profoundly shaken, and comes to see \"beyond a shadow of a doubt\" that his symptoms are an indication of mental disease spurred on by past emotional trauma, rather than EHS.\n\n\n"}
{"id": "22603691", "url": "https://en.wikipedia.org/wiki?curid=22603691", "title": "Eobacteria", "text": "Eobacteria\n\nEobacteria is a proposed clade characterized by Cavalier-Smith. Species in this group lack lipopolysaccharide.\n\nThe clade includes Hadobacteria and Chlorobacteria.\n"}
{"id": "205490", "url": "https://en.wikipedia.org/wiki?curid=205490", "title": "Exeligmos", "text": "Exeligmos\n\nAn exeligmos ( — \"turning of the wheel\") is a period of 54 years, 33 days that can be used to predict successive eclipses with similar properties and location. For a solar eclipse, after every exeligmos a solar eclipse of similar characteristics will occur in a location close to the eclipse before it. For a lunar eclipse the same part of the earth will view an eclipse that is very similar to the one that occurred one exeligmos before it (see main text for visual examples). It is an eclipse cycle that is a triple saros, 3 saroses (or saroi) long, with the advantage that it has nearly an integer number of days so the next eclipse will be visible at locations and times near the eclipse that occurred one exeligmos earlier. In contrast, each saros, an eclipse occurs about 8 hours later in the day or about 120° to the west of the eclipse that occurred one saros earlier.\n\nThe Greeks had knowledge of the exeligmos by at latest 100 BC. A Greek astronomical clock called the Antikythera mechanism used epicyclic gearing to predict the dates of consecutive exeligmoses.\n\nThe exeligmos is 669 synodic months (every eclipse cycle must be an integer number of synodic months), almost exactly 726 draconic months (which ensures the sun and moon are in alignment during the new moon), and also almost exactly 717 anomalistic months (ensuring the moon is at the same point of its elliptic orbit). The first two factors make this a long lasting eclipse series. The latter factor is what makes each eclipse in an exeligmos so similar. The near integer number of anomalistic months ensures that the apparent diameter of the moon will be nearly the same with each successive eclipse. The fact that it is very nearly a whole integer of days ensures each successive eclipse in the series occurs very close to the previous eclipse in the series.\nFor each successive eclipse in an exeligmos series the longitude and latitude can change significantly because an exeligmos is over a month longer than a calendar year, and the gamma increases/decreases because an exeligmos is about three hours shorter than a draconic month. The sun's apparent diameter also changes significantly in one month, affecting the length and width of a solar eclipse.\n\nHere is a comparison of two total solar eclipses one exeligmos apart:\n\nHere is a comparison of two total lunar eclipses one exeligmos apart:\n\nExeligmos table of solar saros 136. Each eclipse occurs at roughly the same longitude but moves about 5-15 degrees in latitude with each successive cycle.\n\nHere is an animation of an exeligmos series. Note the similar paths of each total eclipse, and how they fall close to the same longitude of the earth.\n\nThis next animation is from the entire saros series of the exeligmos above. Notice how each eclipse falls on a different side of the earth (120 degrees apart).\n\n"}
{"id": "344907", "url": "https://en.wikipedia.org/wiki?curid=344907", "title": "Halley Research Station", "text": "Halley Research Station\n\nHalley Research Station is a research facility in Antarctica on the Brunt Ice Shelf operated by the British Antarctic Survey (BAS). The base was established in 1956 to study the Earth's atmosphere. Measurements from Halley led to the discovery of the ozone hole in 1985.\nThe current base is the sixth in a line of designs to overcome the challenges of building on a floating ice shelf where they become buried and crushed by snow. Despite moving the buildings 23km \"inland\", concern over the propagation of an ice crack resulted in the base being left unmanned for the winters of 2017 and 2018.\n\nHalley Bay base was founded in 1956, for the International Geophysical Year of 1957–1958, by an expedition from the Royal Society. The bay where the expedition decided to set up their base was named after the astronomer Edmond Halley. Taken over by FIDS (subsequently BAS), it was designated as Base Z. The name was changed to Halley in 1977 as the original bay had disappeared because of changes in the ice shelf. \n\nIn 2002, BAS realised that a calving event was possible which could destroy Halley V, so a competition was undertaken to design a replacement station. The current base, Halley VI officially opened in February 2013 after a test winter. It is the world's first fully relocatable terrestrial research station, and is distinguishable by its colourful modular structure built upon huge hydraulic skis.\n\nOn 30 July 2014, the station lost its electrical and heating supply for 19 hours. During the power cut, there were record low temperatures. Power was partially restored, but all science activities, apart from meteorological observations essential for weather forecasting, were suspended. Plans were made to evacuate some of the eight modules and to shelter in the remaining few that still had heat.\n\nAs with the German Neumayer-Station III, the base floats on an ice shelf in the Weddell Sea rather than being built on solid land of the continent of Antarctica. This ice shelf is slowly moving towards the open ocean and, if not relocated, each base would eventually calve off into a drifting iceberg.\n\nThere have been five previous bases at Halley. Various construction methods have been tried, from unprotected wooden huts to buildings within steel tunnels. The first four all became buried by snow accumulation and crushed until they were uninhabitable. The more recent structures have been designed to remain on the snow surface.\n\n\n\n\n\n\n\nHalley VI is a string of eight modules which, like Halley V, are jacked up on hydraulic legs to keep it above the accumulation of snow. Unlike Halley V, there are retractable giant skis on the bottom of these legs, which allow the building to be relocated periodically.\n\nThe Drewry summer accommodation building and the garage from Halley V were dragged to the Halley VI location and continue to be used. The Workshop and Storage Platform (WASP) provides storage for field equipment and a workshop for technical services. There are six external science cabooses which house scientific equipment for each experiment spread across the site and the Clean Air Sector Laboratory (CASLab) 1km from the station.\n\nAn architectural design competition was launched by RIBA Competitions and the British Antarctic Survey in June 2004 to provide a new design for Halley VI. The competition was entered by a number of architectural and engineering firms. The winning design, by Faber Maunsell and Hugh Broughton Architects was chosen in July 2005.\n\nHalley VI was built in Cape Town, South Africa by a South African consortium. A total of 26 modular accommodation pods were added in total, installed in eight modules, which provides fully serviced accommodation for 32 people. The first sections were shipped to Antarctica in December 2007. It was assembled next to Halley V, then dragged one-by-one 15 km and reconnected.\n\nHalley VI Station was officially opened in Antarctica on 5 February 2013. Kirk Watson, a filmmaker from Scotland, recorded the building of the station over a four-year period for a short film. A description of the engineering challenges and the creation of the consortium was provided by Adam Rutherford to coincide with an exhibition in Glasgow.\n\nA focus of the new architecture was the desire to improve the living conditions of the scientists and staff on the station. Solutions included consulting a colour psychologist to create a special colour palette to offset the more than 100 days of darkness each year, daylight simulation lamp alarm clocks to address biorhythm issues, the use of special wood veneers to imbue the scent of nature and address the lack of green growth, as well as lighting design and space planning to address social interaction needs and issues of living and working in isolation.\n\nAnother priority of the structure construction was to have the least environmental impact on the ice as possible.\n\nThe British Antarctic Survey announced that it intended to move Halley VI to a new site in summer 2016-2017. A large crack had been propagating through the ice and threatened to cut the station off from the main body of the ice shelf, prompting the decision to move. The planned move would see the station shifted from its previous site and would be the first time the station had been moved since it became operational in 2012. Horizon, the long-running BBC documentary series, sent film-maker Natalie Hewit to Antarctica for three months to document the move.\n\nWhilst the station was being relocated, concerns about a new crack which had been discovered on 31 October 2016 (dubbed the \"Halloween Crack\") led the BAS to announce that it would withdraw its staff from the base in March 2017. BAS completed the relocation of the base in February 2017. Staff returned after the Antarctic winter, in November 2017 and found the station in very good shape. The station was not manned in winter 2018.\n\nTemperatures at Halley rarely rise above 0 °C although temperatures around -10 °C are common on sunny summer days. Typical winter temperatures are below -20 °C with extreme lows of around -55 °C.\n\nWinds are predominantly from the east; strong winds often picking up the dusty surface snow reducing visibility to a few metres.\n\nOne of the reasons for the location of Halley is that it is under the auroral oval, making it ideally located for geospace research and resulting in frequent displays of the Aurora Australis overhead. These are easiest to see during the 105 days (29 Apr - 13 Aug) when the Sun does not rise above the horizon.\n\nDuring the winter months there are usually around 13 overwintering staff. In a typical winter the team is isolated from when the last aircraft leaves in early March until the first plane arrives in late October. In the peak summer period, from late December to late February, staff numbers increase to around 70.\n\nSometimes, none of the wintering team is a scientist. Most are the technical specialists required to keep the station and the scientific experiments running. The 2016 wintering team at Halley included a chef, a doctor, a communications manager, a vehicle mechanic, a generator mechanic, an electrician, a plumber, a field assistant, two electronics engineers, a meteorologist and a data manager. In addition there is a winter station leader who is sworn in as a magistrate prior to deployment and whose main role is to oversee the day-to-day management of the station.\n\n1996 saw the first female winterers at Halley. Since 2009 there are usually at least two women who winter each year.\n\nLife in Antarctica is dominated by the seasons, with a short, hectic summer and a long winter. In bases such as Halley that are resupplied by sea, the most significant event of the year is the arrival of the resupply ship (currently , before 1999, ) in late December. This is followed by intense activity to unload all supplies before the ship has to leave again; typically, this is done in less than two weeks.\n\nThe Halley summer season runs from as early as mid-October when the first plane lands, until early March when the ship has left and the last aircraft leaves transiting through Halley and on to Rothera Research Station before heading to South America.\n\nSignificant dates in the winter are sundown (last day when the Sun can be seen) on April 29th, midwinter on June 21st and sunrise (first day when the Sun rises after winter) on August 13th. Traditionally, the oldest person on base lowers the tattered flag on sundown and the youngest raises a new one on sunrise. Midwinter is a week long holiday, during which a member of the wintering team is chosen to keep the old flag.\n\n\n\n"}
{"id": "14161546", "url": "https://en.wikipedia.org/wiki?curid=14161546", "title": "Heat stabilization", "text": "Heat stabilization\n\nHeat stabilization is an additive-free preservation technology for tissue samples which stops degradation and changes immediately and permanently. Heat stabilization uses rapid conductive heating, under controlled pressure, to generate a fast, homogenous and irreversible thermal denaturation of proteins, resulting in a complete and permanent elimination of all enzymatic activity that would otherwise cause further biological changes to the tissue sample \"ex vivo\". Due to the permanent inactivation of enzymes, heat stabilization overcomes the drawbacks of conventional tissue sample preservation techniques, such as snap-freezing followed by inhibitors.\nUnderstanding the role of proteins, peptides and small molecules in normal and diseased tissue is crucial to defining their potential use as drugs, drug targets or disease biomarkers. Yet biological changes begin the moment tissue is removed from its native environment. Dramatic alterations at the molecular level occur within seconds e.g. changed metabolism, catabolic fragmentation of large molecules (such as ATP) occurs in order to release energy, leading to disrupted control mechanisms, phosphorylation states are altered and proteins begin to degrade. As a consequence vital information may be lost or distorted, leading to inter-sample variation, risk of incorrect data interpretation and potentially misleading conclusions.\n\nHeat stabilization offers significant advantages over conventional approaches to preventing biological change. It can be used to replace snap freezing followed by inhibitors, pH changes, organic solvents or cross-linking. It can also be used with frozen tissue, allowing stabilization of stored samples. Heat stabilization can be used for almost any kind of tissue sample, and has been verified to be compatible with many downstream analytical techniques such as mass spectrometry, phospho-shotgun, MALDI imaging, Western blot, 1D and 2D gels, reversed-phased protein arrays, RIA and ELISA. The method also allows samples collected and handled in bio safety level laboratories to be subsequently handled outside such labs after treatment.\n"}
{"id": "16899853", "url": "https://en.wikipedia.org/wiki?curid=16899853", "title": "History of polymerase chain reaction", "text": "History of polymerase chain reaction\n\nThe history of the polymerase chain reaction (PCR) has variously been described as a classic \"Eureka!\" moment, or as an example of cooperative teamwork between disparate researchers. Following is a list of events before, during, and after its development:\n\n\n\nBy 1980 all of the components needed to perform PCR amplification were known to the scientific community. The use of DNA polymerase to extend oligonucleotide primers was a common procedure in DNA sequencing and the production of cDNA for cloning and expression. The use of DNA polymerase for nick translation was the most common method used to label DNA probes for Southern blotting.\n\n\n\n\n\n"}
{"id": "3306498", "url": "https://en.wikipedia.org/wiki?curid=3306498", "title": "Hubble search for transition comets", "text": "Hubble search for transition comets\n\nHubble search for transition comets (Transition Comets—UV Search for OH Emissions in Asteroids) was a study involving amateur astronomers and the use of the Hubble Space Telescope. This was one of only six studies involving amateur astronomers that was approved by NASA.\n\nIn the beginning years of the Hubble Space Telescope (HST) project, NASA and Congress were interested in finding ways for amateur astronomers to participate in HST research. The director of the Space Telescope Science Institute (STScI), Riccardo Giacconi, decided to allocate some of his \"Director's Discretionary\" time to amateur observing programs. In December 1985, the leaders of seven national amateur astronomy organizations met at STScI in Baltimore to discuss the participation of amateur astronomers in the HST project.\n\nThe team used the Hubble Space Telescope to perform a spectroscopic search for OH emission from five asteroids. OH emission would indicate that the asteroids were once comets. 944 Hidalgo and 2201 Oljato move in elliptical, comet-like orbits. 182 Elsa, 224 Oceana, and 899 Jokaste are main-belt asteroids. The last three have been observed with coma (Kresak, 1977). Concurrently with the spectroscopic study, ground-based visual observations were carried out by 80 amateur astronomers in 22 countries.\n\nScientists suspect that some asteroids were once comets. A comet loses part of its mass with each passage around the Sun. It would appear that some would eventually use all of their volatiles, or perhaps cover these under a blanket of dust after repeated passages around the Sun. Such an object might then have an asteroid appearance.\n\nThe asteroid 944 Hidalgo is most frequently discussed as being in a comet-like orbit. In fact, Kresak (1977) identified it as an \"extinct comet nucleus\". In addition, comets tend to approach Jupiter closely while asteroids do not. Hidalgo approaches Jupiter at nearly the same distance as 3 comets: Comet Encke, Comet Arend-Rigaux, and Comet Neujmin I, all of which exhibit the low levels of activity expected of comets before they become extinct.\n\nThe Pioneer Venus Orbiter detected magnetic field disturbances that are correlated with 2201 Oljato. This could be caused by an outgassing of H at a rate of about 10 that for an active comet. Oljato's possible comet-like nature is supported by its unusual UV reflectance. It has been interpreted as Rayleigh scattering from a cloud of fine particles around it.\n\nOn December 13, 1923, the astronomer Josep Comas Solá observed the asteroid 224 Oceana with a coma. The asteroid was photographed with a faint halo 30 arc-seconds across. The asteroid's magnitude was determined to be 11.6, and at the asteroid's heliocentric distance of 167 million miles, this made the coma about 24,000 miles across.\n\nThe existence of volatiles on asteroids would be of great significance to future miners of the asteroid belt. Volatiles could supply water, fuel and oxygen for missions.\n\nFor the 1993 study involving amateur astronomers and the Hubble Space Telescope, 944 Hidalgo and 2201 Oljato were examined with the Hubble Space telescope's Faint Object Spectrograph for 3085 A emission of OH. These two asteroids were selected because of the nature of their orbits, their meteor-shower associations, and their other characteristics (see Weissman et al. (1989) for a full discussion of their comet-like features). The amber detector was used in the accumulation mode with spectral element G270H. This element covers wavelengths 2325-3225 A. The aperture was 1 arc-second.\n\nThe asteroids 182 Elsa, 224 Oceana, and 899 Jokaste were also observed with the Faint Object Spectrograph, and with the same spectral element G270H.\n\nThe team leaders placed advertisements in amateur astronomy publications for amateur observers who would be willing to observe these objects in the visible spectrum from the ground, while the HST was making studies in the UV. The ground-based observers were asked to check for evidence of a dust coma.\n\nThe spectra for 944 Hidalgo, and 2201 Oljato are essentially identical to the solar spectrum. No OH emission, or other emission lines were apparent.\n\nGround-based observations of these two objects were limited to several CCD images of the area in which 2201 Oljato was calculated to appear. Both 944 Hidalgo, and 2201 Oljato were near 19th magnitude. This placed them beyond the limit of most amateur telescopes.\n\nNone of the other three asteroids showed signs of the cometary emission that would be expected from a weakly active cometary nucleus. This should not be surprising, since the targets lie in stable main-belt orbits.\n\nGround-based observations of 182 Elsa, 224 Oceana, and 899 Jokaste were conducted visually, photographically, with VHS, and with CCD equipment. 224 Oceana was at 12th magnitude during the HST observations. 182 Elsa was also at 12th magnitude, and 899 Jokaste appeared at 15th magnitude.\n\nAll visual observations showed point images, with no sign of a dust coma. Photographic images gave the same result. A number of observers were able to accumulate CCD images in order to reach fainter magnitudes. Using this method, two observers reported signs of a possible short tail on the asteroid 899 Jokaste. This appeared around 17th magnitude. Studies of nearby stars on the original image suggest that this was due to a small amount of trailing during guiding. There was no further evidence of a coma around any of the three asteroids.\n\nObservations of 944 Hidalgo and 2201 Oljato had to be conducted before the Hubble Space Telescope servicing mission. As a result, the two asteroids were not in convenient positions to detect OH. Hidalgo was imaged at 5 Astronomical Units from the Sun, and moving away. This is approximately the same distance as SL-9 was before it impacted on Jupiter. The Hubble Space Telescope, using the same Faint Object Spectrograph and spectral element G270H, also failed to find any evidence of OH from SL-9.\n\n2201 Oljato was near aphelion when it was observed. From the distance of the asteroid belt, any OH signature may have been hidden by noise in the data of the 19th magnitude asteroid. Ideally, both objects should have been studied shortly after perihelion passage.\n\nThe main-belt asteroids 224 Oceana, 182 Elsa, and 899 Jokaste were observed near opposition, but did not show any evidence of OH emission. The last two were observed when the Hubble Space Telescope was having guidance problems, and had to present special orientation to the Sun due to the loss of one solar panel. The reports of previous coma may have been due to impacts.\n\nThe team had over 70 primary observers. Observers were located in 24 states of the U.S., and 22 different countries.\n\n"}
{"id": "33025924", "url": "https://en.wikipedia.org/wiki?curid=33025924", "title": "Interface: a journal for and about social movements", "text": "Interface: a journal for and about social movements\n\nInterface: a journal for and about social movements is an open access academic journal that covers original research and reviews of books concerned mainly with protests, social movements, and collective behavior.\n\nThe journal has multiple editors who are responsible for different world regions and is multilingual. It was established in 2009 and appears biannually.\n\n\"Interface\" attempts to engage not only social movement scholars, but also the activists themselves.\n\n\n"}
{"id": "49652113", "url": "https://en.wikipedia.org/wiki?curid=49652113", "title": "Iyoite", "text": "Iyoite\n\nIyoite is a very rare manganese copper chloride hydroxide mineral with the formula MnCuCl(OH). Iyoite is a new member of the atacamite group, and it an analogue of botallackite characterized in manganese and copper ordering. Iyoite is monoclinic (space group \"P\"2/\"m\"). It is chemically similar to misakiite. Both minerals come from the Ohku mine in the Ehime prefecture, Japan.\n"}
{"id": "44432406", "url": "https://en.wikipedia.org/wiki?curid=44432406", "title": "LeRoy E. Doggett Prize", "text": "LeRoy E. Doggett Prize\n\nThe LeRoy E. Doggett Prize is Awarded biennially by the Historical Astronomy Division of the American Astronomical Society for individuals who have significantly influenced the field of the history of astronomy by a career-long effort. The prize is a memorial to LeRoy Doggett, who was an active and highly regarded member of the Division and was serving as Secretary-Treasurer at the time of his untimely death.\n"}
{"id": "11393560", "url": "https://en.wikipedia.org/wiki?curid=11393560", "title": "Leibniz Institute for Baltic Sea Research", "text": "Leibniz Institute for Baltic Sea Research\n\nThe Leibniz Institute for Baltic Sea Research (, abbreviated IOW) is a research institution located in Warnemünde (Rostock), Germany.\n\nIt is part of the Leibniz-Association, cooperates with the University of Rostock and was founded in 1992. Employing about 160 people the main focus lies on interdisciplinary study of coastal oceans and marginal seas, especially on Baltic Sea related oceanography. The institute is a follow-up of the former Institute of Oceanography (\"Institut für Meereskunde\") which was part of the GDR Academy of Science.\n\nThe institute is divided in four departments: physical oceanography, marine chemistry, biological oceanography, and marine geology. Central task of the institute is fundamental research but also teaching at the universities of Rostock and Greifswald. IOW has direct access to the research vessel \"Maria S. Merian\" and can access by request a variety of other medium-sized vessels for longer trips and interdisciplinary tasks from the German research fleet. The institute's facilities are financed by the German Federal Ministry of Education and Research, and the Ministry of Education of Mecklenburg-Western Pomerania.\n\n"}
{"id": "57776292", "url": "https://en.wikipedia.org/wiki?curid=57776292", "title": "Leptotrichia goodfellowii", "text": "Leptotrichia goodfellowii\n\nLeptotrichia goodfellowii is a Gram-negative, non-spore-forming and non-motile bacterium from the genus of \"Leptotrichia\" which has been isolated from human blood of an endocarditis patient.\n"}
{"id": "106171", "url": "https://en.wikipedia.org/wiki?curid=106171", "title": "Levittown", "text": "Levittown\n\nLevittown is the name of seven large suburban housing developments created in the United States and Puerto Rico by William Levitt and his company Levitt & Sons. Built after World War II for returning veterans and their new families, the communities offered attractive alternatives to cramped central city locations and apartments. The Veterans Administration and the Federal Housing Administration (FHA) guaranteed builders that qualified veterans could buy housing for a fraction of rental costs.\n\nProduction was modeled on assembly lines in 27 steps with construction workers trained to perform one step. A house could be built in one day when effectively scheduled. This enabled quick and economical production of similar or identical homes with rapid recovery of costs. Standard Levittown houses included a white picket fence, green lawns, and modern appliances. Sales in the original Levittown began in March 1947. 1,400 homes were purchased during the first three hours.\n\n"}
{"id": "23204153", "url": "https://en.wikipedia.org/wiki?curid=23204153", "title": "List of ASTM International standards", "text": "List of ASTM International standards\n\nThis is a list of ASTM International standards.\nStandard designations usually consist of a letter prefix and a sequentially assigned number. This may optionally be followed by a dash and the last two digits of the year in which the standard was adopted. Prefix letters correspond to the following subjects:\nThis list may include either current or withdrawn standards. A withdrawn standard has been discontinued by its sponsoring committee. A standard may be withdrawn with or without replacement.\n\n\n\n\n\n\n\n\n"}
{"id": "20493323", "url": "https://en.wikipedia.org/wiki?curid=20493323", "title": "List of Georgia state symbols", "text": "List of Georgia state symbols\n\nThis is a list of state symbols for the U.S. state of Georgia\n\n"}
{"id": "38000555", "url": "https://en.wikipedia.org/wiki?curid=38000555", "title": "List of Tinamiformes by population", "text": "List of Tinamiformes by population\n\nThis is a list of Tinamiformes species by global population. While numbers are estimates, they have been made by the experts in their fields. For more information on how these estimates were ascertained, see Wikipedia's articles on population biology and population ecology.\n\nThis list is not comprehensive, as not all Tinamiformes have had their numbers quantified.\n"}
{"id": "30935057", "url": "https://en.wikipedia.org/wiki?curid=30935057", "title": "List of compounds with carbon number 12", "text": "List of compounds with carbon number 12\n\nThis is a partial list of molecules that contain 12 carbon atoms.\n\n"}
{"id": "42638285", "url": "https://en.wikipedia.org/wiki?curid=42638285", "title": "List of countries by Internet connection speeds", "text": "List of countries by Internet connection speeds\n\nThis list of countries by Internet connection speed lists the average data transfer rates for Internet access by end-users.\n\n\n"}
{"id": "49541051", "url": "https://en.wikipedia.org/wiki?curid=49541051", "title": "List of countries by body mass index", "text": "List of countries by body mass index\n\nThis page serves as a partial list of countries by adult mean body weight and incidence of obese and overweight populations as calculated by body mass index (BMI). \n\nThe data for 2014 was first published by the World Health Organization in 2015.\n\nMean body mass index (BMI) provides a simplified measure of the comparative weight of populations on a country by country basis. BMI calculates a person's mass (weight) divided by the square of their height. An individual with a BMI of 25 kg/m or more is considered overweight. An individual with a BMI of 30 kg/m or more is considered obese.\n\nThe data highlighted on this page comes from World Health Organization statistics for adult (18 years old and older) populations. Mean BMI data is shown separately for males and females, as well as a combined figure. Mean data highlights the central tendency of the population data and is but one method of calculating relative body weight between populations.\n\nThere are significant limitations to the usefulness of comparative BMI data cited by both the medical community and statisticians. BMI data has significant weaknesses in terms of scalability and in accounting for variations in physical characteristics. \n\nData published in 2015.\n\nData published in 2017.\n\nData published in 2015.\n"}
{"id": "38032581", "url": "https://en.wikipedia.org/wiki?curid=38032581", "title": "List of countries by total road tunnel length", "text": "List of countries by total road tunnel length\n\nThis is a list of the summed length of all the road tunnels in each country. As many countries are still adding tunnels at a rapid rate, care should be taken when comparing data from different years. Data from China are the time of end of 2017. \n"}
{"id": "19777655", "url": "https://en.wikipedia.org/wiki?curid=19777655", "title": "List of elements by atomic properties", "text": "List of elements by atomic properties\n\nThis is a list of chemical elements and their atomic properties, ordered by Atomic number.\n\nSince valence electrons are not clearly defined for the d-block and f-block elements, there not being a clear point at which further ionisation becomes unprofitable, a purely formal definition as number of electrons in the outermost shell has been used.\n\n"}
{"id": "463516", "url": "https://en.wikipedia.org/wiki?curid=463516", "title": "List of human anatomical parts named after people", "text": "List of human anatomical parts named after people\n\nThis is a list of human anatomical parts named after people.\n\nFor clarity, entries are listed by the name of the person associated with them, so Loop of Henle is listed under H not L.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "31319830", "url": "https://en.wikipedia.org/wiki?curid=31319830", "title": "List of people diagnosed with pancreatic cancer", "text": "List of people diagnosed with pancreatic cancer\n\nThis article is a list of notable people who have been diagnosed with pancreatic cancer.\n"}
{"id": "319875", "url": "https://en.wikipedia.org/wiki?curid=319875", "title": "List of tests", "text": "List of tests\n\nThe following is an alphabetized and categorized list of notable tests.\n\n\n\n\n\n"}
{"id": "49991105", "url": "https://en.wikipedia.org/wiki?curid=49991105", "title": "List of things named after Emmy Noether", "text": "List of things named after Emmy Noether\n\nEmmy Noether was a German mathematician who flourished in the 20th century. This article is dedicated to the things named after her achievements.\n\n\n\n"}
{"id": "44428758", "url": "https://en.wikipedia.org/wiki?curid=44428758", "title": "List of women psychologists", "text": "List of women psychologists\n\nThis is a list of women psychologists.\n"}
{"id": "46182552", "url": "https://en.wikipedia.org/wiki?curid=46182552", "title": "Localization and Urbanization Economies", "text": "Localization and Urbanization Economies\n\nLocalization and Urbanization Economies are two types of external economies of scale, or agglomeration economies. External economies of scale result from an increase in the productivity of an entire industry, region, or economy due to factors outside of an individual company. There are three sources of external economies of scale: input sharing, labor market pooling, and knowledge spillovers (Marshall, 1920) '\n\nLocalization economies occur when an increase in the size of an industry in a city leads to an increase in productivity of a particular activity. Alfred Marshall (1920) introduced the idea that the localization of industry can increase productivity in his book Principles of Economics. The highly concentrated high tech industry in Silicon Valley exemplifies industrial localization. Although the cost of labor and land in Silicon Valley is very high, high tech firms continue to locate there because of the added benefit they receive from their proximity to a high-skilled labor pool. The size of the high tech industry, creates positive externalities for each firm located in Silicon Valley.\n\nUrbanization economies arise when the size of the city leads to an increase in productivity. Los Angeles exemplifies urbanization economies in that it has no single dominant industry, yet continues to grow. Firms which locate in Los Angeles benefit from the common resources and large labor pool found in the city. Common resources such as roads, buildings and power supply benefit firms in cities regardless of their industry. Also, firms have better access to labor by locating in cities. The urban environment creates positive externalities that benefit several different industries. Jane Jacobs is often credited with the idea that urban diversity and a city’s size leads to agglomeration economies. However, Marshall’s (1920) discussion of urban diversity predates her work.\n"}
{"id": "20505600", "url": "https://en.wikipedia.org/wiki?curid=20505600", "title": "Maternity den", "text": "Maternity den\n\nA maternity den, in the animal kingdom, is a lair where the mother gives birth and nurtures the young, when they are in a vulnerable life stage. While such dens are typically subterranean, they may also be snow caves or simply beneath rock ledges. Characteristically there is an entrance, and optionally an exit corridor, in addition to a principal chamber.\n\nThe polar bear (\"Ursus maritimus\") creates a maternity den either in an earthen subterranean or in a snow cave. On the Hudson Bay Plain in Manitoba, Canada, many of these subterranean dens are situated in the Wapusk National Park, from which bears migrate to the Hudson Bay when the ice pack forms. The maternity den is the bear's shelter for most of the winter. \"When all the other polar bears are heading off to the openness of the ocean, the pregnant female polar bears begin looking for a maternity den. This maternity den is usually in a snow bank, or along an ice patch of ocean shore. It is here that the female polar bear will go into a hibernation type state. Female polar bears dig their own maternity den. It is important the female polar bears have fed enough in the spring and summer before fall, because of the scarcity of food on land when winter comes. While in the maternity den, the mother polar bear will not eat, drink or defecate. The female polar bear will stay in the maternity den and give birth to her cubs.\"\n\nPack members may guard the maternity den used by the alpha female; such is the case with the African wild dog, \"Lycaon pictus\".\n\nThe brown hyaena, \"Hyaena brunnea\", makes use of maternity dens as a means of nurturing and protecting their cubs. These dens are located in coastal or inland regions, most of them being caverns with narrow entrances. The brown hyaena, also, collects bones and stores them within or around the entrance of these dens.\n\nThe red fox also creates maternity dens. After mating, foxes make a maternity den for raising their offspring. Most often, the mother and father will find and enlarge an old woodchuck burrow. Sometimes, a hollow log, streambank, rockpile, cave, or dense shrub will play the role as a den.The den is usually chosen at a place where there is raised ground so the foxes can see all around. The main entrance will be approximately three feet wide, and the den will have one or two escape holes.The den is lined with grass and dry leaves.\n"}
{"id": "750654", "url": "https://en.wikipedia.org/wiki?curid=750654", "title": "Mercury-Atlas 2", "text": "Mercury-Atlas 2\n\nMercury-Atlas 2 (MA-2) was launched unmanned on February 21, 1961 at 14:10 UTC, from Launch Complex 14 at Cape Canaveral, Florida.\n\nTest objectives for this flight were concerned with the ability of the spacecraft to withstand reentry under the temperature-critical abort conditions and with the capability of the Atlas to meet the proper injection conditions. Convair had promised to deliver thicker-skinned Atlas vehicles for subsequent flights, however Missile 67D was the last of the thin-skinned model and so it had to be modified for the Mercury mission, incorporating a stainless steel reinforcing band installed around the vehicle between stations 502 and 510. A thin sheet of asbestos was installed between the reinforcing band and the tank skin. This modification was installed as a precaution against the type of failure which had occurred on the previous MA-1 flight. The booster's flight path was also modified somewhat from Mercury-Atlas 1, being placed on a more shallow trajectory so as to reduce aerodynamic loads.\n\nThe Atlas lifted into a clear blue February sky quite different from the cloudy, foggy weather of the MA-1 flight. Everyone in the blockhouse waited nervously for the vehicle to pass through the critical Max Q zone. When it did so successfully, there was \"enormous jubilation\" from the launch team. MA-2 flew a successful suborbital mission that lasted 17 minutes 56 seconds. Altitude reached was 114 miles (183 km), speed, 13,227 mph (21,287 km/h). All test objectives were fully met, the only problems being a bit of propellant slosh. The capsule was recovered 1,432 miles (2305 km) downrange. Peak acceleration was 15.9 \"g\" (156 m/s²). Mass 1,154 kg.\n\nMercury spacecraft #6 and Atlas #67-D were used in the Mercury-Atlas 2 mission. The Mercury capsule is currently displayed at the Houston Museum of Natural Science, Houston, TX.\n\n\n"}
{"id": "18200485", "url": "https://en.wikipedia.org/wiki?curid=18200485", "title": "Minnaert function", "text": "Minnaert function\n\nThe Minnaert function is a photometric function used to interpret astronomical observations and remote sensing data for the Earth. It was named after the astronomer Marcel Minnaert. This function expresses the radiance factor (RADF) as a function the phase angle (formula_1), the photometric latitude (formula_2) and the photometric longitude (formula_3).\n\nwhere formula_5 is the Minnaert albedo, formula_6 is an empirical parameter, formula_7 is the scattered radiance in the direction formula_8, formula_9 is the incident radiance, and\nThe phase angle is the angle between the light source and the observer with the object as the center.\n\nThe assumptions made are:\n\nMinnaert's contribution is the introduction of the parameter formula_6, having a value between 0 and 1, originally for a better interpretation of observations of the Moon. In remote sensing the use of this function is referred to as \"Minnaert topographic correction\", a necessity when interpreting images of rough terrain.\n"}
{"id": "3748162", "url": "https://en.wikipedia.org/wiki?curid=3748162", "title": "Multiple-conclusion logic", "text": "Multiple-conclusion logic\n\nA multiple-conclusion logic is one in which logical consequence is a relation, formula_1, between two sets of sentences (or propositions). formula_2 is typically interpreted as meaning that whenever each element of formula_3 is true, some element of formula_4 is true; and whenever each element of formula_4 is false, some element of formula_3 is false.\n\nThis form of logic was developed in the 1970s by D. J. Shoesmith and Timothy Smiley but has not been widely adopted.\n\nSome logicians favor a multiple-conclusion consequence relation over the more traditional single-conclusion relation on the grounds that the latter is asymmetric (in the informal, non-mathematical sense) and favors truth over falsity (or assertion over denial).\n\n"}
{"id": "42492354", "url": "https://en.wikipedia.org/wiki?curid=42492354", "title": "Navy oceanographic meteorological automatic device", "text": "Navy oceanographic meteorological automatic device\n\nThe Navy oceanographic meteorological automatic device (NOMAD) is an anchored automated weather station developed shortly after World War II and still used today.\n\nThe NOMAD has a boat-shaped hull made from aluminum, and it provides relatively high cost effectiveness and excellent long-term survivability in severe weather. These buoys are highly directional and have a quick rotational response and stability. There have been no known capsizings of 6-meter NOMAD hulls. The relatively small size of the NOMAD allows for easy transport across land.\n\nThe NOMAD hull was developed from the \"Roberts buoy\", which was a , boat-shaped buoy developed in the early 1940s, by the U.S. Coast and Geodetic Survey, to measure strong tidal currents. The buoy's performance was satisfactory, but its limited size significantly restricted the buoy's use in other areas.\n\nIn July 1946, the Bureau of Ships became involved in a program to develop automatic weather station buoys. As a perspective part of this program, they conducted a preliminary investigation into the feasibility of mooring a buoy. The investigation concluded that the buoy's hull size was of insufficient length to be moored in 3600 ft of water. To support such a mooring, a similar shaped hull had to be 20 ft (6 m) long and displace approximately 20,000 lb. This was to become the prototype of the buoy now known as the NOMAD.\n\nThe NOMAD was the first of such stations to be anchored successfully for a substantial period in more than 11,000 feet of water. It was also the first anchored automated station to detect formation of a hurricane and alert weather observers on land.\nThe station was developed as part of the ocean test and evaluation program, started in 1957, for the Bureau of Naval Weapons, with the National Bureau of Standards responsible for technical direction.\n\nToday, the NOMAD is used for monitoring meteorological, oceanographic and water quality parameters, all over the world. The U.S. National Weather Service currently has 17 NOMADs in operation. NOMADs have also been used by the Meteorological Service of Canada for over 25 years and there are now 16 NOMADs monitoring Canadian waters.\n"}
{"id": "386128", "url": "https://en.wikipedia.org/wiki?curid=386128", "title": "Outline of scientific method", "text": "Outline of scientific method\n\nThe following outline is provided as an overview of and topical guide to scientific method:\n\nScientific method – body of techniques for investigating phenomena and acquiring new knowledge, as well as for correcting and integrating previous knowledge. It is based on observable, empirical, reproducible, measurable evidence, and subject to the laws of reasoning.\n\nScientific method\n\nResearch\n\nObservation\n\nHypothesis\n\nExperiment\n\n\n\n\nEmpirical methods\n\n\n\nThe problem of induction questions the logical basis of scientific statements.\n\n\n\n\n\n\n\n\n\n"}
{"id": "27999806", "url": "https://en.wikipedia.org/wiki?curid=27999806", "title": "PLL multibit", "text": "PLL multibit\n\nA PLL multibit or multibit PLL is a phase-locked loop (PLL) which achieves improved performance compared to a unibit PLL by using more bits. Unibit PLLs use only the most significant bit (MSB) of each counter's output bus to measure the phase, while multibit PLLs use more bits. PLLs are an essential component in telecommunications.\n\nMultibit PLLs achieve improved efficiency and performance: better utilization of the frequency spectrum, to serve more users at a higher quality of service (QoS), reduced RF transmit power, and reduced power consumption in cellular phones and other wireless devices.\n\nA phase-locked loop is an electronic component or system comprising a closed loop for controlling the phase of an oscillator while comparing it with the phase of an input or reference signal. An indirect frequency synthesizer uses a PLL. In an all-digital PLL, a voltage-controlled oscillator (VCO) is controlled using a digital, rather than analog, control signal. The phase detector gives a signal proportional to the phase difference between two signals; in a PLL, one signal is the reference, and the other is the output of the controlled oscillator (or a divider driven by the oscillator).\n\nIn a unibit phase-locked loop, the phase is measured using only one bit of the reference and output counters, the most significant bit (MSB). In a multibit phase-locked loop, the phase is measured using more than one bit of the reference and output counters, usually including the most significant bit.\n\nIn unibit PLLs, the output frequency is defined by the input frequency and the modulo count of the two counters. In each counter, only the most significant bit (MSB) is used. The other output lines of the counters are ignored; this is wasted information.\n\nA PLL includes a phase detector, filter and oscillator connected in a closed loop, so the oscillator frequency follows (equals) the input frequency. Although the average output frequency equals the input frequency, the oscillator's frequency fluctuates or vibrates about that average value. The closed loop operates to correct such frequency deviations; higher performance PLL reduces these fluctuations to lower values, however these deviations can never be stopped. See Control theory. Phase noise, spurious emission, and jitter are results of the above phenomena.\n\n\nFrequency settling time is the time it takes the PLL to hop to another frequency. Frequency hopping is used in GSM, and still more in modern systems.\nIn CDMA, frequency hopping achieves better performance than phase coding.\n\nFine frequency resolution is the capability of a PLL to generate closely spaced frequencies. For example, a cellular network may require a mobile phone to set its frequency at any of a plurality of values, spaced 30 kHz or 10 kHz.\n\nThe \"performance envelope\" of a PLL defines the interrelation between the above essential criteria of performance - for example improving the frequency resolution will result in a slower PLL and higher phase noise, etc.\n\nThe PLL Multibit expands the performance envelope of the PLL - it enables to achieve faster settling time together with fine frequency resolution and with lower phase noise.\n\nAs one progresses from the MSB toward the least significant bit (LSB), the frequency increases. For a binary counter, each next bit is at twice the frequency of the previous one. For modulo counters, the relationship is more complicated.\n\nOnly the MSB of the two counters are at the same frequency. The other bits in one counter have different frequencies from those in the other counter.\n\nAll the bits at the output of one counter, together, represent a digital bus. Thus, in a PLL frequency synthesizer there are two buses, one for the reference counter, the other for the output (or VCO) counter. In a uni-bit PLL, of the two digital buses, only one bit (line) of each is used. All the rest of the information is lost.\n\nPLL design is an interdisciplinary task, difficult even for experts in PLLs. This - for the Unibit PLL, which is simpler than the Multibit PLL. The design should take into account:\n\n\nThe above PLL uses more of the bits in the two counters. There is a \"difficult problem\", of comparing signals at different frequencies, in two digital buses which count to a different final value.\n\nImproved performance is possible by using the faster bits of the counters, taking into account the additional available information.\n\nThe operation of the PLL is further disrupted by \"overflow in the counters\". This effect is only relevant in multibit PLLs; for Unibit PLL, there is only the one-bit signal MSB, therefore no overflow is possible.\n\nThe additional degree of freedom in Multibit PLLs allows to adapt each PLL to specific requirements. This can be effectively implemented with programmable logic devices (PLD), for example those manufactured by Altera Corp. Altera provides both digital components and advanced design tools for using and programming the components.\n\nEarly multibit PLLs used a microprocessor, a microcontroller or DSP to close the loop in a smart implementation.\n\nA multibit PLL offers fine frequency resolution and fast frequency hopping, together with lower phase noise and lower power consumption.\nIt thus enhances the overall performance envelope of the PLL.\n\nThe loop bandwidth can be optimized for phase noise performance and/or frequency settling speed; it depends less on the frequency resolution.\n\nImproving the PLL performance can make better use of the frequency spectrum and reduce transmit power. And indeed, PLL performance is being constantly improved.\n"}
{"id": "34271702", "url": "https://en.wikipedia.org/wiki?curid=34271702", "title": "Pearl vortex", "text": "Pearl vortex\n\nIn superconductivity, a Pearl vortex is a vortex of supercurrent in a thin film of type-II superconductor, first described in 1964 by Judea Pearl. A Pearl vortex is similar to Abrikosov vortex except for its magnetic field profile which, due to the dominant air-metal interface, diverges sharply as 1/formula_1 at short distances from the center, and decays slowly, like 1/formula_2 at long distances. Abrikosov's vortices, in comparison, have very short range interaction and diverge as formula_3 near the center.\n\nA transport current flowing through a superconducting film may cause these vortices to move with a constant velocity formula_4 proportional to, and perpendicular to the transport current. Because of their proximity to the surface, and their sharp field divergence at their centers, Pearl's vortices\ncan actually be seen by a scanning SQUID microscope.\nThe characteristic length governing the distribution of the magnetic field around the vortex center is given by the ratio formula_5/formula_6, also known as \"Pearl length,\" where formula_6 is the film thickness and\nformula_8 is London penetration depth.\nBecause this ratio can reach macroscopic dimensions (~1 mm) by making the film sufficiently thin, it can\nbe measured relatively easy and used to estimate the density of superconducting electrons.\n\nAt distances shorter than the Pearl's length, vortices behave like a Coulomb gas (1/formula_1 repulsive force).\n"}
{"id": "177456", "url": "https://en.wikipedia.org/wiki?curid=177456", "title": "Problem of induction", "text": "Problem of induction\n\nThe problem of induction is the philosophical question of whether inductive reasoning leads to knowledge understood in the classic philosophical sense, highlighting the apparent lack of justification for:\n\n\nThe problem calls into question all empirical claims made in everyday life or through the scientific method, and, for that reason, the philosopher C. D. Broad said that \"induction is the glory of science and the scandal of philosophy.\" Although the problem arguably dates back to the Pyrrhonism of ancient philosophy, as well as the Carvaka school of Indian philosophy, David Hume popularized it in the mid-18th century.\n\nIn inductive reasoning, one makes a series of observations and infers a new claim based on them. For instance, from a series of observations that a woman walks her dog by the market at 8am on Monday, it seems valid to infer that next Monday she will do the same, or that, in general, the woman walks her dog by the market every Monday. That next Monday the woman walks by the market merely adds to the series of observations, it does not prove she will walk by the market every Monday. First of all, it is not certain, regardless of the number of observations, that the woman always walks by the market at 8am on Monday. In fact, David Hume would even argue that we cannot claim it is \"more probable\", since this still requires the assumption that the past predicts the future.\n\nSecond, the observations themselves do not establish the validity of inductive reasoning, except inductively. Bertrand Russell illustrated this point in \"The Problems of Philosophy\":\n\nIn several publications it is presented as a story about a turkey, fed every morning without fail, who following the laws of induction concludes this will continue, but then his throat is cut on Thanksgiving Day.\n\nPyrrhonian skeptic Sextus Empiricus first questioned the validity of inductive reasoning, positing that a universal rule could not be established from an incomplete set of particular instances. He wrote:\nThe focus upon the gap between the premises and conclusion present in the above passage appears different from Hume's focus upon the circular reasoning of induction. However, Weintraub claims in The Philosophical Quarterly that although Sextus's approach to the problem appears different, Hume's approach was actually an application of another argument raised by Sextus:\nAlthough the criterion argument applies to both deduction and induction, Weintraub believes that Sextus's argument \"is precisely the strategy Hume invokes against induction: it cannot be justified, because the purported justification, being inductive, is circular.\" She concludes that \"Hume's most important legacy is the supposition that the justification of induction is not analogous to that of deduction.\" She ends with a discussion of Hume's implicit sanction of the validity of deduction, which Hume describes as intuitive in a manner analogous to modern foundationalism.\n\nThe Carvaka, a materialist and skeptic school of Indian philosophy, used the problem of induction to point out the flaws in using inference as a way to gain valid knowledge. They held that since inference needed an invariable connection between the middle term and the predicate, and further, that since there was no way to establish this invariable connection, that the efficacy of inference as a means of valid knowledge could never be stated.\n\nThe 9th century Indian skeptic, Jayarasi Bhatta, also made an attack on inference, along with all means of knowledge, and showed by a type of reductio argument that there was no way to conclude universal relations from the observation of particular instances.\n\nMedieval writers such as al-Ghazali and William of Ockham connected the problem with God's absolute power, asking how we can be certain that the world will continue behaving as expected when God could at any moment miraculously cause the opposite. Duns Scotus however argued that inductive inference from a finite number of particulars to a universal generalization was justified by \"a proposition reposing in the soul, 'Whatever occurs in a great many instances by a cause that is not free, is the natural effect of that cause. Some 17th-century Jesuits argued that although God could create the end of the world at any moment, it was necessarily a rare event and hence our confidence that it would not happen very soon was largely justified.\n\nFew philosophers are as associated with induction as David Hume. He described the problem in \"An Enquiry concerning Human Understanding\", §4, based on his epistemological framework. Here, \"reason\" refers to deductive reasoning and \"induction\" refers to inductive reasoning.\n\nFirst, Hume ponders the discovery of causal relations, which form the basis for what he refers to as \"matters of fact\". He argues that causal relations are found not by reason, but by induction. This is because for any cause, multiple effects are conceivable, and the actual effect cannot be determined by reasoning about the cause; instead, one must observe occurrences of the causal relation to discover that it holds. For example, when one thinks of \"a billiard ball moving in a straight line toward another\", one can conceive that the first ball bounces back with the second ball remaining at rest, the first ball stops and the second ball moves, or the first ball jumps over the second, etc. There is no reason to conclude any of these possibilities over the others. Only through previous observation can it be predicted, inductively, what will actually happen with the balls. In general, it is not necessary that causal relation in the future resemble causal relations in the past, as it is always conceivable otherwise; for Hume, this is because the negation of the claim does not lead to a contradiction.\n\nNext, Hume ponders the justification of induction. If all matters of fact are based on causal relations, and all causal relations are found by induction, then induction must be shown to be valid somehow. He uses the fact that induction assumes a valid connection between the proposition \"I have found that such an object has always been attended with such an effect\" and the proposition \"I foresee that other objects which are in appearance similar will be attended with similar effects\". One connects these two propositions not by reason, but by induction. This claim is supported by the same reasoning as that for causal relations above, and by the observation that even rationally inexperienced people can infer, for example, that touching fire causes pain. Hume challenges other philosophers to come up with a (deductive) reason for the connection. If a deductive justification for induction cannot be provided, then it appears that induction is based on an inductive assumption about the connection, which would be begging the question. Induction, itself, cannot validly explain the connection.\n\nIn this way, the problem of induction is not only concerned with the uncertainty of conclusions derived by induction, but doubts the very principle through which those uncertain conclusions are derived.\n\nNelson Goodman's \"Fact, Fiction, and Forecast\" presented a different description of the problem of induction in the chapter entitled \"The New Riddle of Induction\". Goodman proposed the new predicate \"grue\". Something is grue if and only if it has been (or will be, according to a scientific, general hypothesis) observed to be green before a certain time \"t\", or blue if observed after that time. The \"new\" problem of induction is, since all emeralds we have ever seen are both green and grue, why do we suppose that after time \"t\" we will find green but not grue emeralds? The problem here raised is that two different inductions will be true and false under the same conditions. In other words:\n\n– Given the observations of a lot of green emeralds, someone using a common language will inductively infer that all emeralds are green (therefore, he will believe that any emerald he will ever find will be green, even after time \"t\").\n\n– Given the same set of observations of green emeralds, someone using the predicate \"grue\" will inductively infer that all emeralds, which will be observed after \"t\", will be blue, despite the fact that he observed only green emeralds so far.\n\nGoodman, however, points out that the predicate \"grue\" only appears more complex than the predicate \"green\" because we have defined grue in terms of blue and green. If we had always been brought up to think in terms of \"grue\" and \"bleen\" (where bleen is blue before time \"t\", or green thereafter), we would intuitively consider \"green\" to be a crazy and complicated predicate. Goodman believed that which scientific hypotheses we favour depend on which predicates are \"entrenched\" in our language.\n\nW.V.O. Quine offers a practical solution to this problem by making the metaphysical claim that only predicates that identify a \"natural kind\" (i.e. a real property of real things) can be legitimately used in a scientific hypothesis. R. Bhaskar also offers a practical solution to the problem. He argues that the problem of induction only arises if we deny the possibility of a reason for the predicate, located in the enduring nature of something. For example, we know that all emeralds are green, not because we have only ever seen green emeralds, but because the chemical make-up of emeralds insists that they must be green. If we were to change that structure, they would not be green. For instance, emeralds are a kind of green beryl, made green by trace amounts of chromium and sometimes vanadium. Without these trace elements, the gems would be colourless.\n\nAlthough induction is not made by reason, Hume observes that we nonetheless perform it and improve from it. He proposes a descriptive explanation for the nature of induction in §5 of the \"Enquiry\", titled \"Skeptical solution of these doubts\". It is by custom or habit that one draws the inductive connection described above, and \"without the influence of custom we would be entirely ignorant of every matter of fact beyond what is immediately present to the memory and senses.\" The result of custom is belief, which is instinctual and much stronger than imagination alone.\n\nDavid Stove's argument for induction, based on the statistical syllogism, was presented in the \"Rationality of Induction\" and was developed from an argument put forward by one of Stove's heroes, the late Donald Cary Williams (formerly Professor at Harvard) in his book \"The Ground of Induction\". Stove argued that it is a statistical truth that the great majority of the possible subsets of specified size (as long as this size is not too small) are similar to the larger population to which they belong. For example, the majority of the subsets which contain 3000 ravens which you can form from the raven population are similar to the population itself (and this applies no matter how large the raven population is, as long as it is not infinite). Consequently, Stove argued that if you find yourself with such a subset then the chances are that this subset is one of the ones that are similar to the population, and so you are justified in concluding that it is likely that this subset \"matches\" the population reasonably closely. The situation would be analogous to drawing a ball out of a barrel of balls, 99% of which are red. In such a case you have a 99% chance of drawing a red ball. Similarly, when getting a sample of ravens the probability is very high that the sample is one of the matching or \"representative\" ones. So as long as you have no reason to think that your sample is an unrepresentative one, you are justified in thinking that probably (although not certainly) that it is.\n\nAn intuitive answer to Hume would be to say that a world inacessible to any inductive procedure would simply not be conceivable. This intuition was taking into account by Keith Campbell by considering that to be built a concept must be reapplied, what demands a certain continuity in its object of application and consequently some openness to induction . Recently, Claudio Costa has noted that a future can only be a future of its own past if it holds some identity with it. Moreover, the nearer a future is to the point of junction with its past, the greater are the similarities tendentially involved. Consequently - \"contra\" Hume - some form of principle of homogeneity (causal or structural) between future and past must be warranted, what would make some inductive procedure always possible..\n\nKarl Popper, a philosopher of science, sought to solve the problem of induction.\nHe argued that science does not use induction, and induction is in fact a myth. Instead, knowledge is created by conjecture and criticism. The main role of observations and experiments in science, he argued, is in attempts to criticize and refute existing theories.\n\nAccording to Popper, the problem of induction as usually conceived is asking the wrong question: it is asking how to justify theories given they cannot be justified by induction. Popper argued that justification is not needed at all, and seeking justification \"begs for an authoritarian answer\". Instead, Popper said, what should be done is to look to find and correct errors.\nPopper regarded theories that have survived criticism as better corroborated in proportion to the amount and stringency of the criticism, but, in sharp contrast to the inductivist theories of knowledge, emphatically as less likely to be true. Popper held that seeking for theories with a high probability of being true was a false goal that is in conflict with the search for knowledge. Science should seek for theories that are most probably false on the one hand (which is the same as saying that they are highly falsifiable and so there are lots of ways that they could turn out to be wrong), but still all actual attempts to falsify them have failed so far (that they are highly corroborated).\n\nWesley C. Salmon criticizes Popper on the grounds that predictions need to be made both for practical purposes and in order to test theories. That means Popperians need to make a selection from the number of unfalsified theories available to them, which is generally more than one. Popperians would wish to choose well-corroborated theories, in their sense of corroboration, but face a dilemma: either they are making the essentially inductive claim that a theory's having survived criticism in the past means it will be a reliable predictor in the future; or Popperian corroboration is no indicator of predictive power at all, so there is no rational motivation for their preferred selection principle.\n\nDavid Miller has criticized this kind of criticism by Salmon and others because it makes inductivist assumptions. Popper does not say that corroboration is an indicator of predictive power. The predictive power is in the theory itself, not in its corroboration. The rational motivation for choosing a well-corroborated theory is that it is simply easier to falsify: Well-corroborated means that at least one kind of experiment (already conducted at least once) could have falsified (but did not actually falsify) the one theory, while the same kind of experiment, regardless of its outcome, could not have falsified the other. So it is rational to choose the well-corroborated theory: It may not be more likely to be true, but if it is actually false, it is easier to get rid of when confronted with the conflicting evidence that will eventually turn up. Accordingly, it is wrong to consider corroboration as a reason, a justification for believing in a theory or as an argument in favor of a theory to convince someone who objects to it.\n\n\n\n"}
{"id": "56240950", "url": "https://en.wikipedia.org/wiki?curid=56240950", "title": "PyCBC", "text": "PyCBC\n\nPyCBC is an open source software package primarily written in the Python programming language which is designed for use in gravitational-wave astronomy and gravitational-wave data analysis. PyCBC contains modules for signal processing, FFT, matched filtering, gravitational waveform generation, among other tasks common in gravitational-wave data analysis.\n\nThe software is developed by the gravitational-wave community alongside LIGO and Virgo scientists to analyze gravitational-wave data, search for gravitational-waves, and to measure the properties of astrophysical sources. It has been used to analyze gravitational-wave data from the LIGO and Virgo observatories to detect gravitational-waves from the mergers of neutron stars and black holes and determine their statistical significance. PyCBC based analyses can integrate with the Open Science Grid for large scale computing resources. Software based on PyCBC has been used to rapidly analyze gravitational-wave data for astronomical follow-up.\n\n\n"}
{"id": "11158694", "url": "https://en.wikipedia.org/wiki?curid=11158694", "title": "Silence of the Songbirds", "text": "Silence of the Songbirds\n\nSilence of the Songbirds () is a book by bird lover and scientist Bridget Stutchbury about the rapid decline and loss of many species of songbirds. Some major threats covered include pesticides, sun-grown coffee, city lights, cowbirds, and global warming. The book was published by HarperCollins in 2007, and has 243 pages.\n\n\"Kirkus Reviews\" published a review of the book on June 1, 2007, and compared it to \"Silent Spring\" by Rachel Carson. Stutchbury describes the link between Latin American deforestation and the loss of food for migratory birds, and the impact of large amounts of pesticides. However, deforestation is minimal for shade-grown coffee. She mentions additional threats to songbirds: light pollution, tall buildings, and wind farms. Despite the diminishing populations of songbirds in recent decades, she provides advice for their survival.\n\n\n\n"}
{"id": "470648", "url": "https://en.wikipedia.org/wiki?curid=470648", "title": "Stephen Frick", "text": "Stephen Frick\n\nStephen Nathaniel Frick (born September 30, 1964) is an American astronaut and a veteran of two Space Shuttle missions. Raised in Pittsburgh, Pennsylvania, Frick graduated from Pine-Richland High School in 1982, earned a degree in aerospace engineering from the United States Naval Academy in 1986, was commissioned as a United States Navy officer, and trained as a F/A-18 fighter pilot. Stationed aboard the carrier , he flew combat missions during the Gulf War and then earned a master's degree in aeronautical engineering from the Naval Postgraduate School in 1994.\n\nFrick was selected as a NASA astronaut candidate in 1996 and was trained as a Space Shuttle pilot. He piloted mission STS-110, a docking mission with the International Space Station.\n\nIn July 2006, Frick was assigned to command the crew of STS-122. The 12-day mission delivered the European Space Agency's Columbus laboratory and returned Expedition 16 Flight Engineer Daniel M. Tani to Earth. The mission launched February 7, 2008, and touched down February 20, 2008. NASA announced his retirement in July 2015.\n\nDistinguished Flying Cross; Defense Meritorious Service Medal; 3 Navy Commendation Medals, one with Combat V; Air Medal with 2 Strike-Flight awards; 2 Southwest Asia Service Medals; NASA Outstanding Leadership Medal; NASA Spaceflight Medal\n\n"}
{"id": "32286570", "url": "https://en.wikipedia.org/wiki?curid=32286570", "title": "Teushen", "text": "Teushen\n\nThe Teushen or Tehues were an indigenous hunter-gatherer people of Patagonia in Argentina. They were considered \"foot nomads\", whose culture relied on hunting and gathering. Their territory was between the Tehuelche people to the south and the Puelche people to their north. \n\nBefore 1850, estimates claimed that there were 500 to 600 Teushen people. They were slaughtered in the Argentinian genocides of Patagonia, known as the Conquest of the Desert. By 1925, only ten to twelve Teushen survived. They are considered extinct as a tribe.\n\nThe Teushen language is almost entirely unknown. Linguists believe, from the limited data available, that it was closest to Tehuelche, the language of the people to the south of the Teushen.\n\n\n\n \n"}
{"id": "35715214", "url": "https://en.wikipedia.org/wiki?curid=35715214", "title": "The Body Electric (book)", "text": "The Body Electric (book)\n\nThe Body Electric: Electromagnetism and the Foundation of Life is a book by Robert O. Becker and Gary Selden in which Becker, an orthopedic surgeon at SUNY Upstate working for the Veterans Administration, described his research into \"our bioelectric selves\". \n\nThe book was first published by William Morrow and Company in 1985.\n\nThe first part of the book discusses regeneration, primarily in salamanders and frogs. Becker studied regeneration after lesions such as limb amputation, and hypothesized that electric fields played an important role in controlling the regeneration process. He mapped the electric potentials at various body parts during the regeneration, showing that the central part of the body normally was positive, and the limbs were negative. When a limb of a salamander or frog was amputated, the voltage at the cut (measured relative to the central part of the body) changed from about -10 mV (millivolts) to +20 mV or more the next day—a phenomenon called \"the current of injury\". In a frog, the voltage would simply change to the normal negative level in four weeks or so, and no limb regeneration would take place. In a salamander, however, the voltage would during the first two weeks change from the +20 mV to -30 mV, and then normalize (to -10 mV) during the next two weeks—and the limb would be regenerated.\n\nBecker then found that regeneration could be improved by applying electricity at the wound when there was a negative potential outside the amputation stub. He also found that bone has piezoelectric properties which would cause an application of force to generate a healing current, which stimulated growth at stress locations in accordance with Wolff's law.\n\nIn another part of the book Becker described potentials and magnetic fields in the nervous system, taking into account external influences like earth magnetism and solar winds. He measured the electrical properties along the skin surface, and concluded that at least the major parts of the acupuncture charts had an objective basis in reality. \n\nIn the last chapters of the book, Becker recounts his experiences as a member of an expert committee evaluating the physiological hazards of various electromagnetic pollutions. He presents research data which indicate that the deleterious effects are stronger than officially assumed. His contention is that the experts choosing the pollution limits are strongly influenced by the polluting industry.\n\nIn 1998 Becker filed a patent for an iontopheretic system for stimulation of tissue healing and regeneration (US 5814094 A).\n\nThe title of the book is a reference to the fiction anthology I Sing the Body Electric by Ray Bradbury, itself a reference to the poem of the same name by Walt Whitman.\n\n\"Library Journal\" called it \"a highly informative book ... for educated lay readers\". However \"Kirkus\" complained that \"speculative and heated\" conclusions \"vitiate much of the interesting, well-documented material\". \"The Sciences\" found that it was superficially well-told but with basic scientific errors and showing a lack of knowledge about recent biology.\n\n\n"}
{"id": "24644075", "url": "https://en.wikipedia.org/wiki?curid=24644075", "title": "Vernacular geography", "text": "Vernacular geography\n\nVernacular geography is the sense of place that is revealed in ordinary people's language. Current research by the Ordnance Survey is attempting to understand the landmarks, streets, open spaces, water bodies, landforms, fields, woods, and many other topological features. These commonly used descriptive terms do not necessarily use the official or current names for features; and often these concepts of places don't have clear, rigid boundaries. For example, sometimes the same name may refer to more than one feature, and sometimes people in a locality use more than one name for the same feature. When people refer to geographical regions in a vernacular form they are commonly referred to as imprecise regions. Regions can include large areas of a country such as the American Midwest, the British Midlands, the Swiss Alps, the south east of England and southern California; or smaller areas such as Silicon Valley in northern California. Commonly used descriptions of areas of cities such as a city's downtown district, New York's Upper East Side, London's square mile or the Latin Quarter of Paris can also be viewed as imprecise regions.\n\nBeyond \"vernacular geography,\" a \"vernacular region\" is a distinctive area where the inhabitants collectively consider themselves interconnected by a shared history, mutual interests, and a common identity. Such regions are \"intellectual inventions\" and a form of shorthand to identify things, people, and places. Vernacular regions reflect a \"sense of place,\" but rarely coincide with established jurisdictional borders.\n\nExamples of vernacular regions in the United States include Tidewater, also known as Hampton Roads, Siouxland, and the Tri-City area of Batavia, Geneva, and St. Charles, Illinois. Another can be the American South, since it can be non-officially considered as the confederate states of the U.S. Civil war, where the climate is warm and has limited snow, or by being below a certain latitude.\n\nThe World Wide Web is a major source of geographical information submitted by non-specialists. The British Ordnance Survey is sponsoring research at the Universities of Cardiff and Sheffield, the aim of which is to study the use of vernacular geography, and to investigate how information mined from the Web can be used to generate an approximate spatial boundary for an imprecise region.\nThe existing technology for accessing geographical data is not well adapted to the unstructured, largely text-based resources of the Web. Spatial information on the Web can be categorized geographically according to the textual content, but a major problem for GIS developers wanting to use this resource is the vague and imprecise nature of place names that are commonly employed within web documents.\n\nIn pursuit of delineating vernacular regions, trigger phrases are used by the researchers to capture regular linguistic patterns, which identify relationships between geographic locations. For example, the trigger phrase \"“X is located in Y”\" can be \"Birmingham is located in *\" or \"* is located in Birmingham\". The completed trigger phrase is then submitted to a search engine. For each search, up to 100 results are retrieved. Duplicate results are then removed based on the URL, and snippet text and the search result is used to find candidate region members.\n\nFrom these results, geo-references are extracted and assigned spatial coordinates. A bounding box is then applied and the bounding box is used to find coordinates of other regions and points, apparently lying outside the candidate region. Thus it is possible to compute a boundary for the imprecise region using the points inside and outside.\n\nCardiff University launched a web questionnaire together with mapping tools to capture people's perception of Vernacular Geography in Great Britain.\n\n"}
