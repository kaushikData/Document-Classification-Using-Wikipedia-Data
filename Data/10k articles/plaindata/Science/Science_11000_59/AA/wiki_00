{"id": "838016", "url": "https://en.wikipedia.org/wiki?curid=838016", "title": "Agricultural and Food Research Council", "text": "Agricultural and Food Research Council\n\nThe Agricultural and Food Research Council (AFRC) was a British Research Council responsible for funding and managing scientific and technological developments in farming and horticulture.\n\nThe AFRC was formed in 1983 from its predecessor, the Agricultural Research Council (ARC). It was replaced by the Biotechnology and Biological Sciences Research Council (BBSRC) as a result of government reorganisation in 1994. At that time Sir William Henderson who was secretary to the AFRC claimed that \"agriculture was a success story\" hence the AFRC could be closed and a new vision for research was envisaged in the creation of the BBSRC. With this shift in emphasis, there also followed the closure of several educational and research organisations as for example the internationally renowned Wye College.\n"}
{"id": "635379", "url": "https://en.wikipedia.org/wiki?curid=635379", "title": "Aleksandr Poleshchuk", "text": "Aleksandr Poleshchuk\n\nAleksandr Fyodorovich Poleshchuk (, born October 30, 1953) is a Russian cosmonaut.\n\nBorn in Cheremkhovo, Irkutsk region, he graduated from the Moscow Aviation Institute in 1977 with a mechanical engineering diploma. He then joined RSC Energia as a test engineer, where he was occupied with perfecting repair and assembly techniques performed during space flights. He has extensive experience in test work under simulated weightlessness conditions. In February 1989 he was selected as a test cosmonaut candidate (1989 Cosmonaut Candidates Class, Group 14, Civil Specialists). From September 1989 to January 1991 he underwent the complete course of general space training and was qualified as a test cosmonaut, and then till March 1992 he undertook advanced training for the Soyuz-TM transport vehicle and Mir station flight.\n\nIn 1992 he was selected as the backup flight engineer of the Soyuz TM-15 joint Russian-French mission, and consequently nominated as the flight engineer of the prime crew of Soyuz TM-16. In space from January 24 to July 22, 1993, he participated in a 179-day space flight with Gennady Manakov. During the flight he performed two EVAs totaling 9 hours and 58 minutes. Also testing of the androgynous peripheral docking subassembly of the Kristall module was performed.\n\nOctober 1994 to March 1995 he trained as back-up flight engineer for the Soyuz TM-21 transport vehicle and Mir Station 18th primary expedition flights.\n\nPoleshchuk is married and has one daughter.\n\n"}
{"id": "12441793", "url": "https://en.wikipedia.org/wiki?curid=12441793", "title": "Berthold Leibinger Zukunftspreis", "text": "Berthold Leibinger Zukunftspreis\n\nThe Berthold Leibinger Zukunftspreis (engl.: future prize) is an international award for excellent research on the application or generation of laser light. Since 2006 it is biennially awarded by the German non-profit foundation Berthold Leibinger Stiftung with an amount of 50,000 euros, not earmarked.\n\n\n\n"}
{"id": "33654482", "url": "https://en.wikipedia.org/wiki?curid=33654482", "title": "Bibliography of sociology", "text": "Bibliography of sociology\n\nThis bibliography of sociology is a list of works, organized by subdiscipline, on the subject of sociology. Some of the works are selected from general anthologies of sociology; other works are selected because they are notable enough to be mentioned in a general history of sociology or one of its subdisciplines.\n\nSociology studies society using various methods of empirical investigation to understand human social activity, from the micro level of individual agency and interaction to the macro level of systems and social structure.\n\n\n\"Demography\" is the statistical study of human population. It encompasses the study of the size, structure and distribution of these populations, and spatial and/or temporal changes in them in response to birth, migration, aging and death.\n\n\"Economic sociology\" attempts to explain economic phenomena. It overlaps with economics but concentrates on the roles of social relations and institutions.\n\n\"Industrial sociology\" is the sociology of technological change, globalization, labor markets, work organization, managerial practices and employment relations.\n\n\n\"Environmental sociology\" studies the relationship between society and environment, particularly the social factors that cause environmental problems, the societal impacts of those problems, and efforts to solve the problems.\n\n\n\"Sociology of knowledge\" is the study of the relationship between human thought and the social context within which it arises, and of the effects prevailing ideas have on societies.\n\n\"Political sociology\" was traditionally concerned with how social trends, dynamics, and structures of domination affect formal political processes, as well as exploring how various social forces work together to change political policies. Now it is also concerned with the formation of identity through social interaction, the politics of knowledge and other aspects of social relations.\n\nThe \"sociology of race and ethnic relations\" is the study of social, political, and economic relations between races and ethnicities at all levels of society. It encompasses racism and residential segregation. \n\nThe \"sociology of religion\" concerns the role of religion in society: practices, historical backgrounds, developments and universal themes. There is particular emphasis on the recurring role of religion in all societies and throughout recorded history. \n\n\"Sociological theories\" are complex theoretical and methodological frameworks used to analyze and explain objects of social study. They facilitate organizing sociological knowledge.\n\n\"Conflict theory\" emphasizes social conflict and related issues such as economic inequality, social inequality, oppression and crime.\n\n\"Rational choice theory\" models social behavior as the interaction of utility-maximizing individuals.\n\n\n\"Social network analysis\" is structural approach to sociology that views norms and behaviors as embedded in chains of social relations. Makes use of network theory.\n\n\"Sociocybernetics\" is the application of systems theory and cybernetics to sociology.\n\n\"Structural functionalism\" is a broad perspective that interprets society as a structure with interrelated parts.\n\n\"Urban sociology\" is the sociological study of social life and human interaction in metropolitan areas.\n\n\n\n\n"}
{"id": "21293835", "url": "https://en.wikipedia.org/wiki?curid=21293835", "title": "Cello scrotum", "text": "Cello scrotum\n\nCello scrotum is a hoax medical condition originally published as a brief case report in the \"British Medical Journal\" in 1974. As its name suggests, it was purportedly an affliction of the scrotum affecting male players of the cello.\n\nThe original letter was written by Dr Elaine Murphy but signed by her husband John as a joke to compare with a previous letter regarding 'guitar nipple', a condition reportedly occurring when some styles of guitar playing excessively irritate the player's nipple (a form of contact dermatitis similar to jogger's nipple), which Murphy and her husband believed was likely a joke.\n\nMurphy now points out that even a cursory study of the cellist's posture would show that the 'cello scrotum' complaint would not occur. The unlikelihood of a cellist's posture contributing to scrotal injury was raised back in 1974, but seems to have been overlooked.\n\nMurphy admitted the hoax in 2009 in another letter to the \"BMJ\" after an article in the 2008 Christmas edition of the \"BMJ\" made reference to the complaint. The truth of the case report had already been questioned in the medical literature in 1991. Others have cited it, although expressing scepticism.\n\nThe implications of this and other hoax medical letters for evidence-based medicine and public understanding of science were discussed by Séamus Mac Suibhne.\n\n"}
{"id": "49245321", "url": "https://en.wikipedia.org/wiki?curid=49245321", "title": "Chinese Chemical Society (Beijing)", "text": "Chinese Chemical Society (Beijing)\n\nThe Chinese Chemical Society (CCS; ) is a professional society of chemists headquartered in Beijing. It is part of the China Association for Science and Technology. Current membership is at around 55,000.\n\nThe CCS was founded in Nanjing on August 4, 1932. It merged with the Chinese Chemical Engineering Society in 1959. The organizations were separated again in 1963. CSS has been a member of the International Union of Pure and Applied Chemistry (IUPAC) since 1980 and of the Federation of Asian Chemical Societies (FACS) since 1984.\n\n\nThe CCS publishes many academic journals, including:\n\n\n"}
{"id": "2873458", "url": "https://en.wikipedia.org/wiki?curid=2873458", "title": "Civic Biology", "text": "Civic Biology\n\nA Civic Biology: Presented in Problems (usually referred to as just Civic Biology) was a biology textbook written by George William Hunter, published in 1914. It is the book which the state of Tennessee required high school teachers to use in 1925 and is best known for its section about evolution that was ruled by a local court to be in violation of the state Butler Act. It was for teaching from this textbook that John T. Scopes was brought to trial in Dayton, Tennessee in the Scopes \"Monkey\" Trial. The views espoused in the book about evolution, race, and eugenics were common to American Progressives (especially in the work of Charles Benedict Davenport, one of the most prominent American biologists of the early 20th century, whom Hunter cites in the book).\n\nExcerpts from the book give its general tone and approach to controversial topics regarding mankind:\n\nHunter was born in Mamaroneck New York, and was educated at Williams College, the University of Chicago, and New York University, where he obtained his doctorate. He later became chairman of the biology department at his alma mater, De Witt Clinton High School, a public secondary school for boys in Manhattan. During his time at Clinton, he wrote or co-authored 30 textbooks for college and high school biology courses, including \"Civic Biology\" in 1905. By working with educators at Columbia University's Teachers College and the geneticist, Thomas Hunt Morgan, Hunter developed \"Civic Biology\", a textbook that shaped the modern secondary-school biology curriculum.\n\n In the first edition of \"Civic Biology\", Hunter briefly discusses eugenics on one page of the 432 page textbook. Along with many other evolutionary biologists, Hunter embraced the idea of eugenics as a social doctrine. It was a popular idea in the early 20th century, and several states had enacted laws to compel the sexual segregation and sterilization of people deemed eugenically unfit. Hunter believed that society could perfect the human race by preventing intermarriage between people such as the mentally ill, criminals, and epileptics. Hunter also believed that the Caucasian race was the highest type of all the races.\n\n Darwin had published his evolutionary theories 60 years before the public controversy over \"Civic Biology\" with the Scopes Trial. Prior to the Scopes Trial and controversy over \"Civic Biology\", most science textbooks included Darwinian concepts in order to keep abreast with prevalent scientific ideas. In the early 20th century, there was a movement in the education sector to bring evolution into high school classrooms, in order to update and reshape how biology was taught. Once evolution began being taught in high schools, controversy over Darwin's theories developed. These efforts tried to incorporate progressive educational ideologies and apply the biological sciences to human society.\n\n\"Civic Biology\" was an example of the new approaches to scientific instruction emerging in the education sector. This modern education focused on applying scientific principles to human society by developing applicable and relevant content for students. This movement towards socially applicable biology was coupled with national efforts towards mandatory public education. Anti-evolution efforts and legislation were responding to the redesigned ideologies in the new biology curricula, and also to the centralized control and regulation of education. Concerns about public and standardized education were part of the public debate over \"Civic Biology.\"\n\n There were a variety of socio-cultural events that shaped the publication and reaction to \"Civic Biology.\" Following World War I, a cultural movement emerged that poised two ideological groups against each other: the traditionalists and the modernists. The modernists embraced intellectual experimentation by exploring new philosophies such as Freudian theories and advocated against alcohol prohibition in political spheres. In response to this modernist cultural movement, there was a religious revivalist movement to protest the emergence of these progressive ideologies. The traditionalists were distinguished by their religious zeal. This group favored alcohol prohibition laws and a literal interpretation of the Bible.\n\n The prosecution made arguments that fit the ideology of the traditionalists. The contents of \"Civic Biology\" were not specifically discussed in the trial, but rather the trial focused on the merits and problems with Darwin's theory of evolution. The Scopes Trial used \"Civic Biology\" as a tool to have a publicized and public debate on whether or not evolution should continue to be taught. \n\n The Scopes Trial and the controversy over \"Civic Biology\" was a major setback for anti-evolution groups because of the 15 states with anti-evolution legislation pending in 1925, only 2 enacted laws restricting the teaching of Darwin's evolutionary theories. While this public controversy had some political implications, it was not extensively influential in the education sector and the instruction of the biological sciences. During the 1970s, it was believed that pressure from Christian fundamentalists after the Scopes Trial forced textbook authors to limit the discussion of evolution. However, upon later examination of biology textbooks, scholars determined that textbook authors worked to develop biological curricula that differentiated and defended its content, in an attempt to reduce the influence of religious fundamentalism on the biological sciences. As a result, modern scholars now believe that overall, biology textbooks were not heavily influenced by anti-evolution rhetoric after the Scopes Trial.\n\nA revised edition of Hunter's book appeared the year after the Scopes Trial. This new edition no longer used the word \"evolution\", and removed most references to recognizably evolutionary concepts. A figure illustrating fossils of the precursors of horses in geological context and much of the associated text was eliminated. The section quoted above, \"Evolution of Man\", was renamed \"Development of Man\", and, rather than saying \"lower in mental organization\" said \"lower in civilization\" (page 250). The book did mention natural selection (page 383), homology (page 237), classification of plants and animals (Chapter XXI, pages 234–252) (including mention of \"man\" as a vertebrate, a mammal, and a primate, page 250), and had a short biography of Charles Darwin among the \"Great Names in Biology\" (pages 411–412).\n\nThis new edition contained an extended discussion of eugenics (Chapter XXXII \"The Improvement of the Human Race\", pages 394–404), but the section quoted above on \"The Remedy\" removed the words \"If such people were lower animals, we would probably kill them off to prevent them from spreading. Humanity will not allow this ...\". (page 400). The supposed history of the Jukes family and the Kallikak family was advanced (pages 398–399). The section \"Parasitism and its Cost to Society\" was unchanged but for the insertion of the sentence \"It is estimated that between 25% and 50% of all prisoners in penal institutions are feeble-minded.\"\n\nThis new edition retained the section \"The Races of Man\" as written, with two changes about \"caucasians\": They were no longer described as \"the highest type of all\", and \"the Hindus and Arabs of Asia\" were included among the enumerated caucasians (page 251).\n\n\n"}
{"id": "983668", "url": "https://en.wikipedia.org/wiki?curid=983668", "title": "Clay Riddell", "text": "Clay Riddell\n\nClayton Howard Riddell, OC (July 13, 1937 – September 15, 2018) was a Canadian billionaire businessman who was the founder, president and CEO of Paramount Resources, based in Calgary, Alberta.\n\nHe was born on a farm near Treherne, Manitoba on July 13, 1937, the youngest child of\nCecil Howard Riddell and Bertha Maude Riddell nee Taylor. Riddell earned with a bachelor's degree in geology from the University of Manitoba. \n\nHe also was part owner of the Calgary Flames and high-end Calgary restaurant Catch. With an estimated net worth of $US 2.5 billion (as of March 2011), he was ranked by Forbes as the 12th wealthiest Canadian and 459th in the world.\n\nRiddell was a president of the Canadian Society of Petroleum Geologists and Chair of the Canadian Association of Petroleum Producers.\n\nThe Clayton H. Riddell Faculty of Environment, Earth, and Resources at the University of Manitoba is named in his honour. He donated $10 million to create an endowment fund for the faculty, which combines the Department of Environment and Geography, the Department of Geological Sciences and the Natural Resources Institute.\n\nIn 2008, Riddell was made an Officer of the Order of Canada for his leadership and philanthropy. \n\nIn May 2010, Carleton University announced the creation of Canada's first graduate program in political management, Clayton H. Riddell Graduate Program in Political Management, made possible through a donation from Riddell that is the largest in Carleton's history. \n\nHe was married to Vi Thorarinson, a nurse for 49 years until her death from leukemia in 2012. They had three daughters and a son together, Lynne, Sue, Jim and Brenda.\n\nRiddell died on September 15, 2018, after a short illness.\n\n"}
{"id": "2944829", "url": "https://en.wikipedia.org/wiki?curid=2944829", "title": "Clinical study design", "text": "Clinical study design\n\nClinical study design is the formulation of trials and experiments, as well as observational studies in medical, clinical and other types of research (e.g., epidemiological) involving human beings. The goal of a clinical study is to assess the safety, efficacy, and / or the mechanism of action of an investigational medicinal product or procedure, or new drug or device that is in development, but potentially not yet approved by a health authority (e.g. Food and Drug Administration). It can also be to investigate a drug, device or procedure that has already been approved but is still in need of further investigation, typically with respect to long-term effects or cost-effectiveness.\n\nSome of the considerations here are shared under the more general topic of design of experiments but there can be others, in particular related to patient confidentiality and ethics.\n\n\n1. Descriptive\n\n2. Analytical\n\nWhen choosing a study design, many factors must be taken into account. Different types of studies are subject to different types of bias. For example, recall bias is likely to occur in cross-sectional or case-control studies where subjects are asked to recall exposure to risk factors. Subjects with the relevant condition (e.g. breast cancer) may be more likely to recall the relevant exposures that they had undergone (e.g. hormone replacement therapy) than subjects who don't have the condition.\n\nThe ecological fallacy may occur when conclusions about individuals are drawn from analyses conducted on grouped data. The nature of this type of analysis tends to overestimate the degree of association between variables.\n\nConducting studies in seasonal indications (such as allergies, Seasonal Affective Disorder, influenza, and others) can complicate a trial as patients must be enrolled quickly. Additionally, seasonal variations and weather patterns can affect a seasonal study.\n\n\n"}
{"id": "37619021", "url": "https://en.wikipedia.org/wiki?curid=37619021", "title": "Co-development", "text": "Co-development\n\nCodevelopment is a trend of thought and a development strategy in development studies\nwhich considers migrants to be a developing factor for their countries of origin.\n\nAlthough it is widely accepted that it was French scholar Sami Naïr who first coined the word codevelopment, it is believed this phenomenon has existed alongside migrations since they exist. Traditionally, immigrants (especially those who migrate for economic reasons) have, collectively or individually, supported their communities of origin.\n\nIn 1997 Sami Naïr, while directing the Interministerial Mission on Migration/Codevelopment, defined this last concept as \"a proposal for integrating immigration and development in a way that migration fluxes will benefit both the country of origin and the country of destination. This is, a consensual relationship between two countries that will allow migration to the country of destiny not to imply an equivalent loss in the country of origin\".\n\nAdherents to this model believe that it fosters mutual collaboration among countries in a way that traditional and hierarchical north-south development focus did not.\nIn this context, immigrant initiatives enrich the countries of origin both from a cultural and a human resources perspective. Their condition as transnational citizens would allow them a better understanding of the needs of both the communities of origin and destination.\nFor this reason, their participation in projects involving members of their communities, in both “their” countries would be more effective, as priorities and needs would be correctly identified.\nOn the other hand, this participation would also work as an integrating force in the destination countries, as they would be perceived as an enriching factor by the public opinion and institutions.\n\nIn countries receiving immigration, codevelopment has been implemented by the institutions in a different way. Since they were first implemented in France, codevelopment initiatives in Europe have been frequently linked to control of the migration fluxes, often promoting the return of immigrants.\n\nAt a European level, codevelopment was first mentioned during the Tampere Summit held in October 1999, when the European Council defined 5 guidelines for the new European migration policy aimed at a common space of \"\"Liberty, Security and Justice\".\"\nAnyhow, critics consider that Tampere quickly shifted towards a Fortress Europe mentality, limiting development aid to those countries willing to implement migration control measures and accepting repatriations.\n\nAn example of this tendency may be found in the imbalance between de 23 million euro budget that the EU plans on investing in the Schengen Information System and the Visa Information System (aimed at a further immigrant identification and control) with the 3 million euro budget (4 million in its second year) granted to codevelopment projects.\n\nSince the late nineties, codevelopment has been the subject of postgraduate studies, specialist courses, discussions and forums among multiple stakeholders, as well as calls for project grants by some local and regional administrations, beginning with the Municipality of Madrid. These authorities quickly became aware of the local impact of the presence of new immigrant communities, the importance of the links that bind them with their communities of origin, and the desirability of relating them in any future action to support the policies of cooperation. Codevelopment policies had a much stronger presence and development in the regional and local levels.\n\nAt the state level, in accordance with Tampere Summit on Migration, the GRECO Plan on migration management (Programa Global de Regulación y Coordinación de la Extranjería y la Inmigración 2000-2004), was launched by the Interior Ministry in 2001. This Plan devoted an important space for “joint development”, in line with the policies of Tampere and the French government. GRECO insisted on technical and educational cooperation, the voluntary return of migrants, the channelling of migrants’ remittances to development projects, and finally, on the cooperation with those governments that were ready to accept the returned illegal migrants, as well as to control the exit of their citizens with destination Spain. However, codevelopment was not even mentioned as a policy line in the Immigration law passed in 2000, and did not receive proper funding within the GRECO’s framework.\n\nSince ending 2004, codevelopment policies have been open to discussion, including the contents, limits, and plans. Two state agencies claimed their space in codevelopment actions: the State Secretary for International Cooperation, Ministry of Foreign Affairs, and the brand new General Directorate of Immigrants’ Integration, at the Ministry of Labour and Social Affairs.\n\nDuring 2005 there has been progress in the search for a consensus on competences and actors. A codevelopment line was mainstreamed in the Master Plans on Development Cooperation, and two years later, also in the new Integration Plan, called Strategic Plan on Citizenship and Integration 2007-2010, which was finally implemented in 2007, by the Ministry of Labour and Migration.\n\nRegional and local authorities soon adopted this win/win approach and consequently launched several codevelopment guidelines within their Integration Plans and Development Cooperation Strategies. Ending 2009, most of the Spanish Regions (Comunidades Autónomas) had considered codevelopment and had set up budget lines both for migrants' associations and Development NGOs. Graciela Malgesini argues that codevelopment and migrants' remittances partly redirect the decentralized cooperation funds to their countries of origin.\n\nIn the last ten years, academic research on codevelopment increased, as a result of the strong immigration process experienced by the Spanish society from 2000 onwards (which represented nearly 80% of Spain’s demographic growth). Many universities decided to open up new learning courses on the matter, following the steps of the Universidad Autónoma de Madrid. Several doctoral thesis are currently being carried out, focusing diverse aspects of codevelopment, including the impact of migration and remittances in the sending countries, mainly Ecuador, Colombia and Morocco. Migrants living in Spain remitted nearly EURO 10 thousands millions, in 2007 (while the overall Spanish OAD summed only half of this amount).\n\nAccording to Graciela Malgesini, spontaneous codevelopment could be defined as the win/win effect, the linkage between migration and development, which generates mostly positive impacts on both the society of origin of immigrants, and the host society. This definition presumes the role of immigrants as actors and vectors of development, in \"both sides\", and the understanding of the relations between host countries (North) and sending countries (South) in a horizontal way. Codevelopment is directly related to Transnationalism.\n\nCarlos Gimenez pinpointed other two characteristics of codevelopment: (1) The multiplicity of stakeholders (a network of participants that surpasses both quantitatively and qualitatively the traditional agents in the traditional development cooperation projects, as it includes authorities, social organizations, trade unions, universities, training institutes, businesses and immigrant associations). (2) Transnational citizenship (immigrants acting in codevelopment activities, embedded in a transnational dynamic, are also transnational citizens, to the extent that they have a dual presence. This dual space of belonging, in turn, encourages decision making, influences on the economic, political and social development, and allows the formation of a separate identity, based on two geographical areas, the country of origin and the country of destination).\n\nTransnationalism and codevelopment are spreading.In Latin America, covedelopment is a relatively new idea, but it has been embraced by grassroots organizations. For example, on November 25, 2010, FAMIGRANTES—the Federation of associations gathering migrants' family members in South America—gathered in their 4th Meeting on Migration and Codevelopment, in Rosario, Argentina. They stated that the codevelopment approach was the best way of understanding the contribution that their relatives were making both in the reception countries and in their homelands (www.famigrantes.org and www.famisur.org). At the same time, more than 100 migrants' associations launched FEDACOD (Federation of Associations for Codevelopment) in Valencia, Spain, on September 24, 2010 (www.fedacod.com).\n\n\n"}
{"id": "38982174", "url": "https://en.wikipedia.org/wiki?curid=38982174", "title": "Conceptualization (information science)", "text": "Conceptualization (information science)\n\nIn information science a conceptualization is an abstract simplified view of some selected part of the world, containing the objects, concepts, and other entities that are presumed of interest for some particular purpose and the relationships between them. An explicit specification of a conceptualization is an ontology, and it may occur that a conceptualization can be realized by several distinct ontologies. An \"ontological commitment\" in describing ontological comparisons is taken to refer to that subset of elements of an ontology shared with all the others. \"An ontology is \"language-dependent\"\", its objects and interrelations described within the language it uses, while a conceptualization is always the same, more general, its concepts existing \"independently of the language used to describe it\". The relation between these terms is shown in the figure to the right.\n\nNot all workers in knowledge engineering use the term ‘conceptualization’, but instead refer to the conceptualization itself, or to the ontological commitment of all its realizations, as an overarching ontology.\n\nAs a higher level abstraction, a conceptualization facilitates the discussion and comparison of its various ontologies, facilitating knowledge sharing and reuse. Each ontology based upon the same overarching conceptualization maps the conceptualization into specific elements and their relationships.\n\nThe question then arises as to how to describe the 'conceptualization' in terms that can encompass multiple ontologies. This issue has been called the 'Tower of Babel' problem, that is, how can persons used to one ontology talk with others using a different ontology? This problem is easily grasped, but a general resolution is not at hand. It can be a 'bottom-up' or a 'top-down' approach, or something in between.\n\nHowever, in more artificial situations, such as information systems, the idea of a 'conceptualization' and the 'ontological commitment' of various ontologies that realize the 'conceptualization' is possible. The formation of a conceptualization and its ontologies involves these steps:\n\nAn example of moving conception into a language leading to a variety of ontologies is the expression of a process in pseudocode (a strictly structured form of ordinary language) leading to implementation in several different formal computer languages like Lisp or Fortran. The pseudocode makes it easier to understand the instructions and compare implementations, but the formal languages make possible the compilation of the ideas as computer instructions. \n\nAnother example is mathematics, where a very general formulation (the analog of a conceptualization) is illustrated with 'applications' that are more specialized examples. For instance, aspects of a function space can be illustrated using a vector space or a topological space that introduce interpretations of the 'elements' of the conceptualization and additional relationships between them but preserve the connections required in the function space. \n\n\n"}
{"id": "4263285", "url": "https://en.wikipedia.org/wiki?curid=4263285", "title": "Cradle to Cradle: Remaking the Way We Make Things", "text": "Cradle to Cradle: Remaking the Way We Make Things\n\nCradle to Cradle: Remaking the Way We Make Things is a 2002 non-fiction book by German chemist Michael Braungart and U.S. architect William McDonough. It is a manifesto detailing how to achieve their Cradle to Cradle Design model. It calls for a radical change in industry: a switch from a cradle-to-grave pattern to a cradle-to-cradle pattern. It suggests that the \"reduce reuse recycle\" methods perpetuate this cradle-to-grave strategy, and that more changes need to be made. The book discourages downcycling, but rather encourages the manufacture of products with the goal of upcycling in mind. This vision of upcycling is based on a system of \"lifecycle development\" initiated by Braungart and colleagues at the Environmental Protection Encouragement Agency in the 1990s: after products have reached the end of their useful life, they become either \"biological nutrients\" or \"technical nutrients\". Biological nutrients are materials that can re-enter the environment. Technical nutrients are materials that remain within closed-loop industrial cycles.\n\nThe book uses historical examples such as the Industrial Revolution along with commentary on science, nature, and society.\n\nWilliam McDonough and Michael Braungart met at an Environmental Protection Encouragement Agency reception in New York City. They began discussing toxicity and design. They were immediately interested in working together to create a better world for each other, and in 1991 they coauthored \"The Hannover Principles\": a set of design guidelines for the 2000 World's Fair that were issued at the 1992 World Urban Forum and United Nations forum of the Earth Summit. In 1995 they founded McDonough Braungart Design Chemistry, a firm to assist companies in implementing sustainable design protocols.\n\nThe book was published in 2002 by North Point Press. The book itself is printed using DuraBook technology. The pages are not paper, but rather synthetics created from plastic resins and inorganic fillers. The books are more durable and strong than traditional paper books, waterproof, and upcyclable. It is considered a \"technical nutrient\" in the lifecycle development system.\n\nThe book has been translated into 12 languages.\n\n\n"}
{"id": "4864009", "url": "https://en.wikipedia.org/wiki?curid=4864009", "title": "Deformable mirror", "text": "Deformable mirror\n\nDeformable mirrors (DM) are mirrors whose surface can be deformed, in order to achieve wavefront control and correction of optical aberrations. Deformable mirrors are used in combination with wavefront sensors and real-time control systems in adaptive optics. In 2006 they found a new use in femtosecond pulse shaping.\n\nThe shape of a DM can be controlled with a speed that is appropriate for compensation of dynamic aberrations present in the optical system. In practice the DM shape should be changed much faster than the process to be corrected, as the correction process, even for a static aberration, may take several iterations.\n\nA DM usually has many degrees of freedom. Typically, these degrees of freedom are associated with the mechanical actuators and it can be roughly taken that one actuator corresponds to one degree of freedom.\n\nNumber of actuators determines the number of degrees of freedom (wavefront inflections) the mirror can correct. It is very common to compare an arbitrary DM to an ideal device that can perfectly reproduce wavefront modes in the form of Zernike polynomials. For predefined statistics of aberrations a deformable mirror with M actuators can be equivalent to an ideal Zernike corrector with N (usually N < M) degrees of freedom. For correction of the atmospheric turbulence, elimination of low-order Zernike terms usually results in significant improvement of the image quality, while further correction of the higher-order terms introduces less significant improvements. For strong and rapid wavefront error fluctuations such as shocks and wake turbulence typically encountered in high-speed aerodynamic flowfields, the number of actuators, actuator pitch and stroke determine the maximum wavefront gradients that can be compensated for.\n\nActuator pitch is the distance between actuator centers. Deformable mirrors with large actuator pitch and large number of actuators are bulky and expensive.\n\nActuator stroke is the maximum possible actuator displacement, typically in positive or negative excursions from some central null position. Stroke typically ranges from ±1 to ±30 micrometres. Free actuator stroke limits the maximum amplitude of the corrected wavefront, while the inter-actuator stroke limits the maximum amplitude and gradients of correctable higher-order aberrations.\n\nInfluence function is the characteristic shape corresponding to the mirror response to the action of a single actuator. Different types of deformable mirrors have different influence functions, moreover the influence functions can be different for different actuators of the same mirror. Influence function that covers the whole mirror surface is called a \"modal\" function, while localized response is called \"zonal\".\n\nActuator coupling shows how much the movement of one actuator will displace its neighbors. All \"modal\" mirrors have large cross-coupling, which in fact is good as it secures the high quality of correction of smooth low-order optical aberrations that usually have the highest statistical weight.\n\nResponse time shows how quickly the mirror will react to the control signal. Can vary from microseconds (MEMS and magnetics mirrors) to tens of seconds for thermally controlled DM's.\n\nHysteresis and creep are nonlinear actuation effects that decrease the precision of the response of the deformable mirror. For different concepts, the hysteresis can vary from zero (electrostatically-actuated mirrors) to tens of percent for mirrors with piezoelectric actuators. Hysteresis is a residual positional error from previous actuator position commands, and limits the mirror ability to work in a feedforward mode, outside of a feedback loop.\n\nSegmented concept mirrors are formed by independent flat mirror segments. Each segment can move a small distance back and forth to approximate the average value of the wavefront over the patch area. Advantageously, these mirrors have little or zero cross-talk between actuators. Stepwise approximation works poorly for smooth continuous wavefronts. Sharp edges of the segments and gaps between the segments contribute to light scattering, limiting the applications to those not sensitive to scattered light. Considerable improvement of the performance of the segmented mirror can be achieved by introduction of three degrees of freedom per segment: piston, tip and tilt. These mirrors require three times as many actuators compared to piston segmented mirrors. This concept was used for fabrication of large segmented primary mirrors for the Keck telescopes, JWST, and the future E-ELT. Numerous methods exist to accurately co-phase the segments and reduce the diffraction patterns introduced by the segment shapes and gaps. Future large space-based telescopes, such as the NASA ATLAST will also possess a segmented primary mirror. The development of robust methods to increase the contrast is key for the direct imaging and characterization of exoplanets.\n\nContinuous faceplate concept mirrors with discrete actuators are formed by the front surface of a thin deformable membrane. The shape of the plate is controlled by a number of discrete actuators that are fixed to its back side. The shape of the mirror depends on the combination of forces applied to the faceplate, boundary conditions (the way the plate is fixed to the mirror) and the geometry and the material of the plate. These mirrors allow smooth wavefront control with very large - up to several thousands - degrees of freedom.\n\nMagnetics concept mirrors are based on continuous reflective surface motioned by magnetics actuators. They feature large strokes, linearity and fast settling time.\n\nMEMS concept mirrors are fabricated using bulk and surface micromachining technologies. MEMS mirrors have a great potential to be cheap. They could break the high price threshold of conventional adaptive optics. MEMS mirrors typically have high response rates, limited hysteresis.\n\nMembrane concept mirrors are formed by a thin conductive and reflective membrane stretched over a solid flat frame. The membrane can be deformed electrostatically by applying control voltages to electrostatic electrode actuators that can be positioned under or over the membrane. If there are any electrodes positioned over the membrane, they are transparent. It is possible to operate the mirror with only one group of electrodes positioned under the mirror. In this case a bias voltage is applied to all electrodes, to make the membrane initially spherical. The membrane can move back and forth with respect to the reference sphere.\nBimorph concept mirrors are formed by two or more layers of different materials. One or more of (active) layers are fabricated from a piezoelectric or electrostrictive material. Electrode structure is patterned on the active layer to facilitate local response. The mirror is deformed when a voltage is applied to one or more of its electrodes, causing them to extend laterally, which results in local mirror curvature. Bimorph mirrors are rarely made with more than 100 electrodes.\n\nFerrofluid concept mirrors are liquid deformable mirrors made with a suspension of small (about 10 nm in diameter) ferromagnetic nanoparticles dispersed in a liquid carrier. In the presence of an external magnetic field, the ferromagnetic particles align with the field, the liquid becomes magnetized and its surface acquires a shape governed by the equilibrium between the magnetic, gravitational and surface tension forces. Using proper magnetic field geometries, any desired shape can be produced at the surface of the ferrofluid. This new concept offers a potential alternative for low-cost, high stroke and large number of actuators deformable mirrors.\n\n"}
{"id": "2900811", "url": "https://en.wikipedia.org/wiki?curid=2900811", "title": "Dippel's oil", "text": "Dippel's oil\n\nDippel's oil (sometimes known as bone oil) is a nitrogenous by-product of the destructive distillation of bones. A dark, viscous, tar-like liquid with an unpleasant smell, it is named after its inventor, Johann Conrad Dippel. The oil consists mostly of aliphatic chains, with nitrogen functionalities and includes species such as pyrroles, pyridines and nitriles, as well as other nitrogenous compounds.\n\nDippel's oil had a number of uses which are now mostly obsolete. Its primary use was as an animal and insect repellent. It saw limited use as a chemical warfare harassing agent during the desert campaign of World War II. The oil was used to render wells undrinkable and thus deny their use to the enemy. \nBy not being lethal, the oil was claimed to not be in breach of the Geneva Protocol. \n\n"}
{"id": "9560227", "url": "https://en.wikipedia.org/wiki?curid=9560227", "title": "E-research", "text": "E-research\n\nThe term e-Research (alternately spelled eResearch) refers to the use of information technology to support existing and new forms of research. E-research extends e-Science and cyberinfrastructure to other disciplines, including the humanities and social sciences.\n\nExamples of e-Research problems range across disciplines which include:\n\nE-Research includes research activities that use a spectrum of advanced information and communication technology (ICT) capabilities. It embraces new research methodologies emerging from increasing access to:\n\n\nSpecialist services, centres or programmes instituted to support Australian data and technology intensive research operate under the umbrella term: eResearch. In March 2012, representatives from these eResearch groups came together to discuss the need build a \"collaborative program to strengthen eResearch and address issues facing the sector nationally\". The Australian eResearch Organisation (AeRO) emerged from this forum as \"a collaborative organisation of national and state-based research organisations to advance eResearch implementation and innovation in Australia\". Professionals working in Australian eResearch annually convene a conference known as: eResearch Australasia.\n\n\n"}
{"id": "14101977", "url": "https://en.wikipedia.org/wiki?curid=14101977", "title": "Fernando Novas", "text": "Fernando Novas\n\nFernando Emilio Novas is an Argentine paleontologist working for the Comparative Anatomy Department of the Bernardino Rivadavia Natural Sciences Museum in Buenos Aires, Argentina. Novas holds a PhD in Natural sciences.\n\nWorking for the CONICET, he described or co-described many dinosaurs, among them \"Abelisaurus\", \"Aniksosaurus\", \"Aragosaurus\", \"Austroraptor\", \"Chilesaurus\", \"Megaraptor\", \"Neuquenraptor\", \"Orkoraptor\", \"Patagonykus\", \"Unenlagia\", \"Araucanoraptor\", \"Skorpiovenator\", \"Tyrannotitan\", \"Talenkauen\", and \"Puertasaurus\", most from the Patagonia region of Argentina. \"Chilesaurus diegosuarezi\" made the cover of Nature magazine on June 18, 2015. \n"}
{"id": "2695705", "url": "https://en.wikipedia.org/wiki?curid=2695705", "title": "Fluid pipe", "text": "Fluid pipe\n\nFluid pipes are a phenomenon driven by surface tension. When a pure water jet impinges on a reservoir, capillary waves are excited and propagate up the jet at the same speed that the jet falls.\n\nFluid pipe phenomenon may be observed with a kitchen faucet. When the diameter of the stream is 2–3 mm, placing an obstacle in the stream will give the desired effect. Contamination of the reservoir with a surfactant will eliminate the effect of capillary waves up a fluid pipe and results in the jet entering the reservoir as a rigid pipe.\n\n"}
{"id": "19095208", "url": "https://en.wikipedia.org/wiki?curid=19095208", "title": "Founders of statistics", "text": "Founders of statistics\n\nStatistics is the theory and application of mathematics to the scientific method including hypothesis generation, experimental design, sampling, data collection, data summarization, estimation, prediction and inference from those results to the population from which the experimental sample was drawn. This article lists statisticians who have been instrumental in the development of theoretical and applied statistics.\n\nThe role of a department of statistics is discussed in a 1949 article by Harold Hotelling, which helped to spur the creation of many departments of statistics.\n\n\n"}
{"id": "4779286", "url": "https://en.wikipedia.org/wiki?curid=4779286", "title": "Geoff Goodfellow", "text": "Geoff Goodfellow\n\nGeoff Goodfellow (born 1956 in California) associated with early wireless email ventures.\n\nIn 1982 he posted a message titled \"Electronic Mail for People on the Move\" in an arpanet mailing list called Telecom Digest. In the early 1990s Goodfellow attempted to commercialize this concept in a product called RadioMail. In 1992, Radiomail entered into a partnership with Research in Motion, RAM Mobile Data, and Ericsson. Goodfellow left the company in 1996.\n\nGoodfellow, a contributor to the \"Jargon File\" and participant in the early days of the Silicon Valley computer culture, did not believe in patenting his idea. He told \"The New York Times\", \"You don't patent the obvious...The way you compete is to build something that is faster, better, cheaper. You don't lock your ideas up in a patent and rest on your laurels.\" \n\nThe inventor, Thomas J. Campana Jr., was granted several patents covering his inventions related to the practical implementation of wireless e-mail. In 2006, after a protracted legal battle, (See NTP Inc.) Research in Motion had to pay $US 615 million to obtain rights to these patents.\n\nIn 2006 Goodfellow began researching the cause, nature and origin of what he regards as the critical state of disharmony on the planet.\n\n"}
{"id": "2081636", "url": "https://en.wikipedia.org/wiki?curid=2081636", "title": "Glasgow Science Centre", "text": "Glasgow Science Centre\n\nGlasgow Science Centre is a visitor attraction located in the Clyde Waterfront Regeneration area on the south bank of the River Clyde in Glasgow, Scotland. Queen Elizabeth II opened Glasgow Science Centre on 5 June 2001. It is one of Scotland's most popular paid-for visitor attractions. It is a purpose-built science centre composed of three principal buildings: Science Mall, Glasgow Tower and an IMAX cinema. The Scottish tourist board, VisitScotland, awarded Glasgow Science Centre a five star rating in the visitor attraction category. As well as its main location, Glasgow Science Centre also manages the visitor centre at Whitelee Wind Farm, which opened to the public in 2009.\n\nThe largest of the three main, titanium-clad buildings takes a crescent shape structure and houses a Science Mall. In architectural terms it represents the canted hull of a ship, a reference to the adjacent 'canting basin', where vessels were brought to have the marine growth removed from their hulls. Internally, there are three floors of over 250 science-learning exhibits. As is usual for science centres, the exhibits aim to encourage interaction, and can be used or played with as part of the informal learning experience the centre aims to deliver. The building was designed by BDP.\n\nOn Floor 1, amongst the many interactive exhibits that demonstrate scientific principles, visitors can access a Science Show Theatre and the Glasgow Science Centre Planetarium. The planetarium contains a Zeiss optical-mechanical projector that projects images of the night sky onto a 15m diameter dome. There is an area specifically aimed at young children, called The Big Explorer.\n\nOn Floor 2, visitors can explore opportunities in STEM careers in the My World of Work Live interactive exhibition space. There is also The Lab, primarily used as an educational workshop space.\n\nFloor 3 was refurbished in 2012 and reopened to the public on 28 March 2013. It now houses an interactive exhibition about human health and wellbeing in the 21st century, called BodyWorks. Visitors are invited to consider their bodies, health and lifestyle from a new perspective through 115 interactive exhibits, research capsules and live laboratory experiences.\n\nThe Ground Floor of the Science Mall contains the ticket desk, cafes, gift shop, and a cloakroom. There are a number of flexible room spaces on the Ground Floor that are used for a variety of educational and corporate purposes: an education space called The Egg; a lecture-theatre space called The Auditorium; and the Clyde Suite, a multi-purpose function space. Access to Glasgow Tower for the public is also via the Ground Floor.\n\nThe Glasgow Tower was designed to be the tallest freely-rotating tower in the world. It missed its opening date in 2001 and was plagued by problems since then. It has been closed for over 80% of its life, and was closed from August 2010 until July 2014.\n\nThe IMAX cinema was the first IMAX cinema to be built in Scotland. The single auditorium seats 370 in front of a rectangular screen measuring by and has the capability to show 3D films as well as standard 2D films in IMAX format. It opened to the public in October 2000, and premiered the first film, entitled \"Dolphins\", several months prior to the opening of the two other buildings. On 6 September 2013, Cineworld agreed a 10-year lease to operate the IMAX cinema and opened a Starbucks on site.\n\nOpened to the public in June 2001, Glasgow Science Centre is part of the ongoing redevelopment of Pacific Quay, an area which was once a cargo port known as Prince's Dock. The redevelopment started with the Glasgow Garden Festival in 1988. As with the other National Garden Festivals, the Glasgow site was intended to be sold off for housing development, but due to a housing slump in 1987, the developers were unable to develop the land as they intended, and the majority of the site remained derelict for years. Parts were finally redeveloped for the Science Centre and also Pacific Quay, including new headquarters for BBC Scotland and Scottish Television, opened in 2007. The Clydesdale Bank Tower was dismantled and re-erected in Rhyl in North Wales, however its spiritual successor came in the form of the Glasgow Tower as part of Science Centre complex, which stands on approximately the same spot.\n\nThe architects of the Glasgow Science Centre were Building Design Partnership, however the Glasgow Tower was originally designed by the architect Richard Horden with engineering design by Buro Happold. It was built at a cost of around £75 million, including £10 million for the Glasgow Tower, with over £37 million coming from the Millennium Commission.\n\nIn June 2004, it was announced that about a fifth of the workforce were to be made redundant following the creation of a funding deal with the Scottish Executive. In June 2008, the leader of the Scottish Liberal Democrats, Nicol Stephen, stated that Glasgow Science Centre was facing a 40% cut in government funding. Prime Minister Gordon Brown commented on this issue during Prime Minister's Questions saying, \"It's unfortunate in Glasgow that as a result of the SNP, funding has been cut, and they will live to regret that\". Although funding for the Scottish Science Centres as a whole has actually increased, it is now being split between four centres using a formula based on visitor numbers, and Glasgow is the only centre to face a reduction in budget. This led to the announcement in July 2008 that 28 full-time jobs were to be cut as a direct consequence of the cuts \"in order to secure Glasgow Science Centre's future\", according to the Chief Executive, Kirk Ramsay.\n\nGlasgow Science Centre is located in the Pacific Quay area, and as such, is surrounded by the media centres that form the Digital Media Quarter, a Scottish Enterprise development initiative, With the opening of the new STV headquarters in June 2006 and the beginning of broadcast programming from BBC Pacific Quay in the summer of 2004, it can be expected that more programming will be filmed in the area.\n\nIn the CBeebies television programme \"Nina and the Neurons\", the title character Nina is a neuroscientist who works at Glasgow Science Centre. In reality, Nina is played by the actress Katrina Bryan who is not a staff member at Glasgow Science Centre.\n\n\n"}
{"id": "26683400", "url": "https://en.wikipedia.org/wiki?curid=26683400", "title": "Global Energy Prize", "text": "Global Energy Prize\n\nThe Global Energy Prize is an international award which recognises outstanding scientific innovations and solutions in global energy research and its concurrent environmental challenges. Since its inception in 2002, the Global Energy Prize has grown to become a recognised global energy award. According to IREG Observatory on Academic Ranking and Excellence, the Global Energy Prize is one of TOP-99 international academic awards with the highest prestige and significance. It is the only award from Russia included in the IREG list. Moreover, the Global Energy Prize is included in the official list of the International Congress of Distinguished Awards (ICDA). In ICDA prestige rating the Global Energy Prize is in the category of “Mega Prizes” for its laudable goals, practices exemplary and the overall prize fund. Three leading Russian energy companies support and provide funding the prize: JSC Gazprom, JSC Surgutneftegaz and JSC Federal Grid Company UES.\n\nThe Global Energy Prize is awarded annually in Russia by the President of the Russian Federation or person on his behalf. Each laureate receives, amongst others, a commemorative medal and a monetary prize. In 2017 the Prize fund amounted to 39 million RUB. The award process is overseen by the Global Energy Prize International Award Committee, which consists of 20 scientists from 13 countries and is chaired by renowned British scientist Rodney John Allam.\n\nThe Global Energy Prize is managed by the Global Energy Association.\n\nThe Global Energy Prize was founded in October 2002 by leading Russian energy companies and endorsed by the President of the Russian Federation.\n\nSince the first awards ceremony in 2003, the award has been presented to 37 laureates from 12 countries: Australia, Canada, France, Germany, Iceland, Japan, Russia, Sweden, Switzerland, Ukraine, the UK and the US. The Global Energy Prize laureates include, amongst others, prominent scientists including Arthur Rosenfeld, awarded for his pioneering work in energy efficiency, and Gennady Mesyats, awarded for fundamental research and development in the field of powerful pulse energy.\n\nThe Global Energy Prize is managed by the Global Energy Association which is responsible for the organisational, financial and informational management of the Global Energy Prize. The Association is managed by the Board of Trustees, headed by Alexander Shokhin, Head of Russian Union of Industrialists and Entrepreneurs, and other representatives of Russian academia of Sciences, politics and energy sphere, including the Nobel Peace Prize winner Mikhail Gorbachev. The Board, formed by the Global Energy Association and renewed every 5 years, is responsible for general management of the nomination process and the awarding of the Prize. The members of the Board of Trustees do not take part in the nomination process, nor do they make a final decision on awarding.\n\nThe Global Energy Association manages a range of awards and projects related to the promotion of science and innovation in the field of energy:\n\n\nThe idea of the Global Energy Prize was developed in 2002 by a group of Russian scientists and was endorsed by Russian energy corporations, President of the Russian Federation, Vladimir Putin, and the scientific community. The Global Energy Fund was set up in October 2002, by three major Russian Energy companies: JSC Gazprom, JCS Federal Grid Company of the Unified Energy Systems (FGC UES, Former JSC Unified Energy Systems of Russia) and Yukos. In 2005, oil and gas company JSC Surgutneftegaz joined the group of funding companies.\n\nThe first Global Energy Prize awarding ceremony took place in June 2003 at the Konstantinovsky Palace, Strelna (St Petersburg district, Russia) and was attended by President Vladimir Putin. The award was presented to three scientists: Mr Nick Holonyak (USA), Chair Professor of Electrical and Computer Engineering and Physics at the University of Illinois, for his contribution to the development of power silicon electronics and the invention of the first semi-conducting light-emitting diodes, Mr Ian Douglas Smith (USA), Chief Manager and Senior Researcher in ‘Titan Pulse Sciences Division’, for his fundamental research and development in the field of powerful pulse energy, and a Russian scientist Mr Gennady Mesyats, then-Chairman of the State Commission for Academic Degrees and Titles of the Russian Federation, for his fundamental research and development in the field of powerful pulse energy.\n\nIn the history of the Global Energy Prize award, 37 scientists have been recognised for their work. The laureates represent 12 countries, including Australia, Canada, France, Germany, Iceland, Japan, Russia, Sweden, Switzerland, Ukraine, United Kingdom and the USA. Laureates since 2002 include:\n\nSergey Alexeenko (RUS) - For research and development in the field of thermal power engineering and heat transfer systems, enhancing resource potential of mankind\n\nMartin Green (Australia) - For research, development and educational activities in the field of photovoltaics that have revolutionized the efficiency and costs of solar photovoltaics, making this now the lowest cost option for bulk electricity supply\n\nMichael Grätzel (Switzerland) - For transcendent merits in development of low cost and efficient solar cells, known as “Graetzel cells”, aimed to creation of cost-efficient, large-scale engineering solutions for power generation.\n\nValentin Parmon (RUS) - For a breakthrough development of new catalysts in the area of petroleum refining and the renewable sources of energy as a principal contribution into the energy of the future.\n\nShuji Nakamura (USA) - For the invention, commercialization and development of energy-efficient white LED lighting technology.\n\nB. Jayant Baliga (USA) - For invention, development and commercialization of Insulated Gate Bipolar Transistor, which is one of the most important innovations for the control and distribution of energy.\n\nLars Gunnar Larsson (SWE) - Awarded for an outstanding contribution to nuclear safety enhancement and nuclear facility decommissioning.\n\nAshot Sarkisov (RUS) - Awarded for an outstanding contribution to nuclear safety enhancement and nuclear facility decommissioning.\n\nAkira Yoshino (JAP) - Awarded for the invention of the rechargeable lithium-ion battery, an essential element for mobile electronic devices and various types of electric vehicles, including hybrids Akira is also credited with the concept, development and commercialization of storage battery systems based on his invention.\n\nVladimir Fortov (RUS) - Awarded for research into thermodynamic, thermophysical, electrophysical, strength, optical, structural and electronic properties of fluids and construction materials in the previously unexplored field of the phase diagram. This includes extreme states of matter, which serves as the basis for perspective development of the energy sector.\n\nValery Kostuk (RUS) – Awarded for research and development in the field of high-efficiency liquid propellant rocket engines with cryogenic fuel for the purpose of the generation of energy in space.\n\nBoris Katorgin (RUS) – Awarded for research and development in the field of high-efficiency liquid propellant rocket engines with cryogenic fuel for the purpose of the generation of energy in space.\n\nRodney John Allam (UK) – Awarded for the development of new processes and equipment for production of gases and cryogenic liquids, as well as for the development and implementation of technology for production of electricity in power systems.\n\nPhilipp Rutberg (RUS) – Awarded for fundamental research and development of energy plasma technologies.\n\nArthur Rosenfeld (USA) – Awarded in recognition of his pioneering work in the field of energy efficiency.\n\nBoris Paton (UKR) – Awarded for his contribution to solving scientific and technical problems associated with the transportation by pipeline of energy resources.\n\nAlexander Leontiev (RUS) – Awarded for fundamental research in the field of the intensification of heat transfer in power plants.\n\nBrian Spalding (UK) – Awarded for numerous original concepts of heat-and-mass transfer processes, which formed the basis of practical calculations in fluid mechanics and computational fluid mechanics.\n\nAlexey Kontorovich (RUS) – Awarded for research on the implementation of new methods of surveying, prospecting and exploiting of hydrocarbon-bearing deposits.\n\nNikolai Laverov (RUS) – Awarded for fundamental research and large-scale implementation of new methods for the exploration and production of oil, gas and uranium deposits.\n\nEduard Volkov (RUS) – Awarded for the creation and implementation of synthetic oil production technologies.\n\nClement Bowman (CA) and Oleg Favorsky (RUS) – Awarded for the theoretical justification, creation and implementation of efficient technologies for synthetic fuel production from bituminous schist and oil sands.\n\nThorsteinn Ingi Sigfusson (IS) – Awarded for research and development in the implementation of hydrogen to power vehicles.\n\nGeoffrey Hewitt (UK) – Awarded for developing the idea of fuel generation on the basis of water power.\n\nVladimir Nakoryakov (RUS) - Awarded for the project \"Physicotechnical Bases of Power Technologies — Hydrodinamics and Heat Exchange, Non-Stationary and Wave Processes in Multiphase Mediums\".\n\nEvgeny Velikhov (RUS), Masaji Yoshikawa (JP) and Robert Aymar (FR) – Awarded for the development of a scientific and technical basis for the creation of the international thermonuclear experimental reactor (ITER Project).\n\nZhores Alferov (RUS) – Awarded for his contribution to the creation of semi-conductor energy converters for use in solar and electrical energy.\n\nKlaus Riedle (DE) – Awarded for the development and creation of high-temperature gas turbines for steam and gas power plants.\n\nFyodor Mitenkov (RUS) – Awarded for development of fast-neutron reactors.\n\nLeonard J. Koch (USA) and Alexander Sheindlin (RUS) – Awarded for fundamental research into the thermo physical properties of substances at extremely high temperatures.\n\nNick Holonyak (USA) – Awarded for his contribution to the development of power silicon electronics and the invention of the first semi-conducting light-emitting diodes in a visible part of the spectrum.\n\nGennady Mesyats (RUS) and Ian Douglas Smith (USA) – Awarded for fundamental research and development in the field of pulse power engineering.\n\nThe nomination process begins on 1 December and ends in July of the following year. Candidates for the Prize can by nominated by:\n\n\nNominators suggest candidates for the Prize by submitting pre-defined nomination forms. Once the nomination process is closed, the Commission of Experts provides a preliminary expert assessment of the nominees and their scientific achievements. The Commission of Experts consists of independent international experts from international energy organisations, scientific bodies and energy industry. The Commission of Experts’ recommendations – a list consisted of up to 5 nominees – is forwarded to the Global Energy Prize International Award Committee, which make the final selection of the winners.\n\nIn June, the Global Energy Prize International Award Committee makes a final decision on awarding the Global Energy Prize. There can be no more than three laureates in a given year.\n\nThe International Award Committee is responsible for choosing the laureates of the Global Energy Prize. This Committee, selected by the General Assembly of the Global Energy Association for a term of 4 years, includes representatives of the most reputable scientific and academic organizations (such as MIT Energy Initiative, Russian Academy of Science, IHS Cambridge Energy Research Associates), as well as governmental and public figures from Russia and abroad. The Committee is managed by the Chairman. As of today Rodney John Allam, Noble Prize and the Global Energy Prize laureate is elected Chairman of the Committee. The Committee consists of 20 members from 13 countries, including the Chairperson.\n\nThe Global Energy Prize is presented to laureates at an official award ceremony that takes place in Russia. In 2018, the ceremony will be held at the Russian Energy Week Interntional Forum. The laureate is recognised at the Forum for his or her contribution to global energy research at a ceremony attended by Russian government officials, leading scientists, directors of national and international scientific and public organizations, members of the Association's Board of Trustees, the Global Energy Prize International Award Committee and Association members. Laureates are presented a gold medal, a gold badge, a diploma and a monetary part (the Prize fund). \n\nThe statuette received by a Global Energy Prize laureate is made of gold and silver. The statuette conceptualises scientific contributions to the field of energy: the image on the front side of the medal is that of a rising star, symbolising discovery, whereas the back shows a star already risen, portraying the laureate's recognised contribution to global energy research.\n\nEach Global Energy Prize laureate is awarded a diploma recognising their contribution to the field of energy.\n\nAn honorary pin is provided to the Global Energy Prize laureates to reflect their scientific achievement.\n\nThe Global Energy Prize laureate are awarded a monetary prize. The total prize fund (which is 39 million RUB as of 2017) is split equally among the laureates.\n\nSince its establishment in 2002, the Global Energy Prize has grown to become a respected global energy award:\n\n\nThe Global Energy Prize laureates\nGlobal Energy Prize 2011 Annual Report\nThe Global Energy Prize Founders\nThe Global Energy Prize International Award Committee\nThe Global Energy Prize Greetings from the Heads of State & Government\n\nEcoGeneration Australia, US and Russian scientists awarded Global Energy Prize June, 2011\n\nEnergyAsia, President Medvedev awards Global Energy Prize 2011, June 2011\n\nPRNewswire, Global Energy Prize Summit Aims at Revolutionising Energy Thinking, October 2012\n\n"}
{"id": "25510416", "url": "https://en.wikipedia.org/wiki?curid=25510416", "title": "Gunung Padang Megalithic Site", "text": "Gunung Padang Megalithic Site\n\nThe existence of the site was mentioned in \"Rapporten van de Oudheidkundige Dienst\" (ROD, \"Report of the Department of Antiquities\") in 1914. The Dutch historian N. J. Krom also mentioned it in 1949. Employees of the National Archeology Research Centre visited the site in 1979 for a study of its archaeology, history, and geology.\n\nLocated at 885 metres above sea level, the site covers a hill in a series of terraces bordered by retaining walls of stone that are accessed by about 400 successive andesite steps rising about 95 metres. It is covered with massive rectangular stones of volcanic origin. The Sundanese people consider the site sacred and believe it was the result of King Siliwangi's attempt to build a palace in one night. The asymmetric Punden Berundak faces northwest, to Mount Gede and was constructed for the purpose of worship. Based on various dating techniques, the site was completed by 5000 BC and quite likely much earlier.\n\nThe villages closest to the site are Cimanggu, Ciwangun and Cipanggulakan. Two possible routes to access the site are\n\nAt the end of June 2014, the Education and Culture Ministry declared Gunung Padang Megalithic Site a National Site Area of 29 hectares area.\n\nOn October 1, 2014, surveyors halted excavation activities temporarily due to these facts and recommendations:\n\nThe 2014 excavation has been criticized for being improperly conducted.\n\nA survey conducted in 2012 showed the following:\n\nThirty-four Indonesian scientists signed a petition questioning the motives and methods of the Hilman-Arif team. Vulcanologist Sutikno Bronto states that the site is the neck of an ancient volcano and not a man-made pyramid. An unnamed archaeologist suggests that the Hilman-Arif team has \"created a civilisation around the period to explain their finding\".\n"}
{"id": "8554023", "url": "https://en.wikipedia.org/wiki?curid=8554023", "title": "Horizontal correlation", "text": "Horizontal correlation\n\nHorizontal correlation is a methodology for gene sequence analysis. Rather than referring to one specific technique, \"horizontal correlation\" instead encompasses a variety of approaches to sequence analysis that are unified by two specific themes:\n\n\nThe core ideas of the horizontal correlation approach were first presented in a year 2000 paper by Grosse, Herzel, Buldyrev, and Stanley (Grosse, et al. 2000). In this first formulation, Grosse and colleagues sought to characterize a large genetic sequence by dividing the sequence into coding and non-coding regions. Whereas traditional approaches to the coding-vs.-non-coding problem generally relied on sophisticated pattern recognition systems that were first trained on small inputs and then run over the entire sequence (Ohler, et al. 1999), the horizontal correlation approach of Grosse and colleagues worked instead by breaking the sequence into many relatively short sequence fragments, each only 500 base pairs in length. They then sought to characterize each of these fragments as either coding or non-coding. This was accomplished by comparing each size 3 window along the length of a fragment with the first size 3 window in that fragment, then measuring the value of the mutual information function between the two windows. Coding sequences were found to display a stylized pattern of 3-periodicity that non-coding sequences did not. Such a pattern was easy to recognize, and enabled significantly more rapid, more species-independent identification of coding regions (Grosse, et al. 2000).\n\nSince 2000, horizontal correlation methodologies emphasizing the measurement of information theoretic quantities along the length of a gene sequence have been put to widespread use, and have even found application in shotgun sequencing fragment assembly (Otu & Sayood, 2004).\n\n"}
{"id": "40178883", "url": "https://en.wikipedia.org/wiki?curid=40178883", "title": "INaturalist", "text": "INaturalist\n\niNaturalist is a citizen science project and online social network of naturalists, citizen scientists, and biologists built on the concept of mapping and sharing observations of biodiversity across the globe. iNaturalist may be accessed via its website or from its mobile applications. Observations recorded with iNaturalist provide valuable open data to scientific research projects, conservation agencies, other organizations, and the public. The project has been called \"a standard-bearer for natural history mobile applications.\"\n\niNaturalist.org began in 2008 as a UC Berkeley School of Information Master's final project of Nate Agrin, Jessica Kline, and Ken-ichi Ueda. Nate Agrin and Ken-ichi Ueda continued work on the site with Sean McGregor, a web developer. In 2011, Ueda began collaboration with Scott Loarie, a research fellow at Stanford University and lecturer at UC Berkeley. Ueda and Loarie are the current co-directors of iNaturalist.org. The organization merged with the California Academy of Sciences on April 24, 2014. In 2014, iNaturalist celebrated its one millionth observation. In 2017, iNaturalist became a joint initiative between the California Academy of Sciences and the National Geographic Society.\n\nThe iNaturalist platform is based on crowdsourcing of data. An iNaturalist observation records an encounter with an individual organism at a particular time and place. In addition to recording actual audio and photos of the organism, an iNaturalist observation may also record evidence of an organism, such as animal tracks, nests, and scat, but the scope of iNaturalist excludes natural but inert subjects such as geologic or hydrologic features. Users typically upload photos as evidence of their findings, though audio recordings are also accepted and such evidence is not a strict requirement. Users may share observation locations publicly, \"obscure\" them to display a less precise location, or make the locations private.\n\nOn iNaturalist, other users add identifications to each other’s observations in order to confirm or improve the \"community identification.\" Observations are classified as \"casual,\" \"needs ID\" (needs identification), or \"research grade\" based on the quality of the data provided and the community identification process. \"Research grade\" observations are incorporated into other online databases such as The Global Biodiversity Information Facility. Users have the option to license their observations, photos, and audio recordings in several ways, including for the public domain, Creative Commons, or with all rights reserved. \nIn addition to observations being identified by others in the community, iNaturalist includes an automated species identification computer vision tool, called \"Computer Vision.\" Images can be identified via an artificial intelligence model which has been trained on the large database of the \"research grade\" observations on iNaturalist. A broader taxon such as a genus or family is typically provided if the model cannot decide what the species is. If the image has poor lighting, is blurry, or contains multiple subjects, it can be difficult for the model to determine the species and it may decide incorrectly. Multiple species suggestions are typically provided, with the one the software believes the image is most likely of at the top of the list.\n\n, iNaturalist users contributed over 15,900,000 observations of plants, animals, and other organisms worldwide, with over 88,000 users active in the previous 30 days. iNaturalist is the preferred application for crowd-sourced biodiversity data in Mexico and southern Africa.\n\nUsers have created and contributed to thousands of different projects on iNaturalist. The platform is commonly used to record observations during bioblitzes, which are biological surveying events that attempt to record all the species that occur within a designated area, and a specific project type on iNaturalist. Other project types include collections of observations by location or taxon, or documenting specific types of observations such as animal tracks and signs, the spread of invasive species, roadkill, fishing catches, or discovering new species. In 2011, iNaturalist was used as a platform to power the Global Amphibian and Global Reptile BioBlitzes, in which observations were used to help monitor the occurrence and distribution of the world's reptiles and amphibian species. The US National Park Service partnered with iNaturalist to record observations from the 2016 National Parks BioBlitz. That project exceeded 100,000 observations in August 2016. In 2017, the United Nations Environment Programme teamed up with iNaturalist to celebrate World Environment Day.\n\nThe City Nature Challenge\n\nIn 2016, Lila Higgins from the Natural History Museum of Los Angeles County and Alison Young from the California Academy of Sciences co-founded the City Nature Challenge. In the first City Nature Challenge, naturalists in Los Angeles and the San Francisco Bay Area documented over 20,000 observations with the iNaturalist platform. In 2017, the challenge expanded to 16 cities across the United States and collected over 125,000 observations of wildlife in 5 days. \n\nIn 2018, the challenge expanded to a global audience, with 68 cities participating from 19 countries, with some cities using community science platforms other than iNaturalist to participate. In 4 days, over 17,000 people cataloged over 440,000 nature observations in urban regions around the world.\n\nSeek for kids/families\n\nIn spring of 2018, iNaturalist introduced Seek by iNaturalist for iOS mobile devices to help children and families learn about nearby nature, try to match a target species, and earn badges for observations. Billed as \"kid-safe\" and \"family-friendly,\" no registration is required and it collects no user data, meaning observations are not submitted to the iNaturalist database. An Android version is being considered.\n\n\n"}
{"id": "9586885", "url": "https://en.wikipedia.org/wiki?curid=9586885", "title": "Information seeking", "text": "Information seeking\n\nInformation seeking is the process or activity of attempting to obtain information in both human and technological contexts. Information seeking is related to, but different from, information retrieval (IR).\n\nTraditionally, IR tools have been designed for IR professionals to enable them to effectively and efficiently retrieve information from a source. It is assumed that the information exists in the source and that a well-formed query will retrieve it (and nothing else). It has been argued that laypersons' information seeking on the internet is very different from information retrieval as performed within the IR discourse. Yet, internet search engines are built on IR principles. Since the late 1990s a body of research on how casual users interact with internet search engines has been forming, but the topic is far from fully understood. IR can be said to be technology-oriented, focusing on algorithms and issues such as precision and recall. Information seeking may be understood as a more human-oriented and open-ended process than information retrieval. In information seeking, one does not know whether there exists an answer to one's query, so the process of seeking may provide the learning required to satisfy one's information need.\n\nMuch library and information science (LIS) research has focused on the information-seeking practices of practitioners within various fields of professional work. Studies have been carried out into the information-seeking behaviors of librarians, academics, medical professionals, engineers, lawyers and mini-publics(among others). Much of this research has drawn on the work done by Leckie, Pettigrew (now Fisher) and Sylvain, who in 1996 conducted an extensive review of the LIS literature (as well as the literature of other academic fields) on professionals' information seeking. The authors proposed an analytic model of professionals' information seeking behaviour, intended to be generalizable across the professions, thus providing a platform for future research in the area. The model was intended to \"prompt new insights... and give rise to more refined and applicable theories of information seeking\" (1996, p. 188). The model has been adapted by Wilkinson (2001) who proposes a model of the information seeking of lawyers.\n\nA variety of theories of information behavior – e.g. Zipf's Principle of Least Effort, Brenda Dervin's Sense Making, Elfreda Chatman's Life in the Round – seek to understand the processes that surround information seeking. In addition, many theories from other disciplines have been applied in investigating an aspect or whole process of information seeking behavior. \n\nA review of the literature on information seeking behavior shows that information seeking has generally been accepted as dynamic and non-linear (Foster, 2005; Kuhlthau 2006). People experience the information search process as an interplay of thoughts, feelings and actions (Kuhlthau, 2006). Donald O. Case (2007) also wrote a good book that is a review of the literature.\n\nInformation seeking has been found to be linked to a variety of interpersonal communication behaviors beyond question-asking, to include strategies such as candidate answers.\n\nRobinson’s (2010) research suggests that when seeking information at work, people rely on both other people and information repositories (e.g., documents and databases), and spend similar amounts of time consulting each (7.8% and 6.4% of work time, respectively; 14.2% in total). However, the distribution of time among the constituent information seeking stages differs depending on the source. When consulting other people, people spend less time locating the information source and information within that source, similar time understanding the information, and more time problem solving and decision making, than when consulting information repositories. Furthermore, the research found that people spend substantially more time receiving information passively (i.e., information that they have not requested) than actively (i.e., information that they have requested), and this pattern is also reflected when they provide others with information.\n\nThe concepts of information seeking, information retrieval, and information behaviour are objects of investigation of information science. Within this scientific discipline a variety of studies has been undertaken analyzing the interaction of an individual with information sources in case of a specific information need, task, and context. The research models developed in these studies vary in their level of scope. Wilson (1999) therefore developed a nested model of conceptual areas, which visualizes the interrelation of the here mentioned central concepts.\n\nWilson defines models of information behavior to be \"statements, often in the form of diagrams, that attempt to describe an information-seeking activity, the causes and consequences of that activity, or the relationships among stages in information-seeking behaviour\" (1999: 250).\n\n\n"}
{"id": "33254985", "url": "https://en.wikipedia.org/wiki?curid=33254985", "title": "Ionospheric Connection Explorer", "text": "Ionospheric Connection Explorer\n\nThe Ionospheric Connection Explorer (ICON) is a planned satellite designed to investigate changes in the Earth's ionosphere. \"ICON\" will study the interaction between Earth's weather systems and space weather driven by the Sun, and how this interaction drives turbulence in the upper atmosphere. It is hoped that a better understanding of this dynamic will mitigate its effects on communications, GPS signals, and technology in general. It is part of NASA Explorers program and will be operated by UC Berkeley's Space Sciences Laboratory.\n\nOn 12 April 2013, NASA announced that \"ICON\", along with Global-scale Observations of the Limb and Disk (\"GOLD\"), had been selected for development with the cost capped at , excluding launch costs. The principal investigator of \"ICON\" is Thomas Immel at the University of California, Berkeley.\n\n\"ICON\" was originally scheduled to launch in December 2017 and has been repeatedly delayed because of problems with its Pegasus XL rocket. Most recently, it was due to launch on 26 October 2018 but the launch was postponed to 7 November 2018. \n\nOnce launched, \"ICON\" will perform a two-year mission to observe conditions in both the thermosphere and ionosphere. \"ICON\" will be equipped with four instruments: a Michelson interferometer, built by the United States Naval Research Laboratory, will measure the winds and temperatures in the thermosphere; an ion drift meter, built by UT Dallas, will measure the motion of charged particles in the ionosphere; and two ultraviolet imagers built at UC Berkeley will observe the airglow layers in the upper atmosphere in order to determine both ionospheric and thermospheric density and composition.\n\nMany low-Earth orbiting satellites, including the International Space Station, fly through the ionosphere and can be affected by its changing electric and magnetic fields. The ionosphere also acts as a conduit for many communications signals, such as radio waves and the signals that make GPS systems work. The ionosphere is where space weather manifests, creating unpredicted conditions such as electric currents that can cause electrical charging of satellites, changing density that can affect satellite orbits, and shifting magnetic fields that can induce current in power systems, causing strain, disrupt communications and navigation or even blackouts. Improved understanding of this environment can help predict such events and improve satellite design.\n\n\"ICON\" was originally scheduled to launch in December 2017 and has been repeatedly delayed because of problems with its Pegasus XL rocket. Most recently, it was due to launch on 26 October 2018 but the launch was postponed indefinitely at that time. In 2017, ICON was going to be launched from Kwajalein Atoll in the Pacific. When that was launch was scrubbed, it was possible it to ferry the launch vehicle using the air-launch aircraft \"Stargazer\" back the United States where a new launch site was chosen, Cape Canaveral in Florida. The October 2018 launch was to be from Cape Canaveral, Florida before issues were detected with some new components. Whereas the delay in 2017 had to due concerns over the rocket-aircraft separation system;the launch system has a good track record launching small science payloads for NASA, and just had a successful launched in December 2016. The launch system has placed more than 80 satellites in low orbit since its first in 1990.\n\nLaunch windows:\n\nNorthrop Grumman's Pegasus XL rocket is carried aloft by the \"Stargazer\" aircraft to approximately 40,000 feet over the open ocean, where it is released and free-falls five seconds before igniting its first-stage rocket motor. The aircraft can also ferry the launch vehicle to different sites.\n\n\"ICON\" carries four scientific instruments designed to image even the faintest plasma or airglow to build up a picture of the ionosphere's density, composition and structure. The complete instrument payload has a mass of and are listed below: \n\n\nMIGHTI was developed at the United States Naval Research Laboratory, IVM at the University of Texas, and EUV and FUV were developed at the University of California. MIGHTI measures wind speed and temperature between 90 to 300 km in altitude. The velocity measurements are gathered by observing the doppler shift in the red and green lines of atomic oxygen. This is done with the Doppler Asymmetric Spatial Heterodyne (DASH) which uses échelle gratings. The temperature measurements are done by photometeric observations with a CCD. MIGHTI is designed to detect wind speeds as low as 10 mph even though the spacecraft would be traveling at over 14,000 mph (to stay in orbit).\n\nIVM collects \"in situ\" data about ions in the local environment around the spacecraft, whereas EUV and FUV are cameras. EUV is designed to observe height and density of the daytime ionosphere, and detect the glow of oxygen.\n\nThe solar panels produce 780 watts, but the instruments' power consumption ranges between 209-265 watts when in science mode.\n\n"}
{"id": "21513962", "url": "https://en.wikipedia.org/wiki?curid=21513962", "title": "Jack (Italian magazine)", "text": "Jack (Italian magazine)\n\nJack is a popular Italian-language technology magazine which has been published in Italy since October 2000 by Arnoldo Mondadori Editore in a joint venture with the German publisher Gruner + Jahr. It is the most popular technology magazine in Italy.\n\nIn 2007 the circulation of \"Jack\" was 129,729 copies.\n\nList of magazines published in Italy\n\n"}
{"id": "17553", "url": "https://en.wikipedia.org/wiki?curid=17553", "title": "Kepler's laws of planetary motion", "text": "Kepler's laws of planetary motion\n\nIn astronomy, Kepler's laws of planetary motion are three scientific laws describing the motion of planets around the Sun.\n\nMost planetary orbits are nearly circular, and careful observation and calculation are required in order to establish that they are not perfectly circular. Calculations of the orbit of Mars, whose published values are somewhat suspect, indicated an elliptical orbit. From this, Johannes Kepler inferred that other bodies in the Solar System, including those farther away from the Sun, also have elliptical orbits.\n\nKepler's work (published between 1609 and 1619) improved the heliocentric theory of Nicolaus Copernicus, explaining how the planets' speeds varied, and using elliptical orbits rather than circular orbits with epicycles.\n\nIsaac Newton showed in 1687 that relationships like Kepler's would apply in the Solar System to a good approximation, as a consequence of his own laws of motion and law of universal gravitation.\n\nKepler's laws improved the model of Copernicus. If the eccentricities of the planetary orbits are taken as zero, then Kepler basically agreed with Copernicus:\n\nThe eccentricities of the orbits of those planets known to Copernicus and Kepler are small, so the foregoing rules give fair approximations of planetary motion, but Kepler's laws fit the observations better than does the model proposed by Copernicus.\n\nKepler's corrections are not at all obvious:\n\nThe eccentricity of the orbit of the Earth makes the time from the March equinox to the September equinox, around 186 days, unequal to the time from the September equinox to the March equinox, around 179 days. A diameter would cut the orbit into equal parts, but the plane through the Sun parallel to the equator of the Earth cuts the orbit into two parts with areas in a 186 to 179 ratio, so the eccentricity of the orbit of the Earth is approximately\n\nwhich is close to the correct value (0.016710219) (see Earth's orbit).\n\nThe calculation is correct when perihelion, the date the Earth is closest to the Sun, falls on a solstice. The current perihelion, near January 3, is fairly close to the solstice of December 21 or 22.\n\nIt took nearly two centuries for the current formulation of Kepler's work to take on its settled form. Voltaire's \"Eléments de la philosophie de Newton\" (Elements of Newton's Philosophy) of 1738 was the first publication to use the terminology of \"laws\". The \"Biographical Encyclopedia of Astronomers\" in its article on Kepler (p. 620) states that the terminology of scientific laws for these discoveries was current at least from the time of Joseph de Lalande. It was the exposition of Robert Small, in \"An account of the astronomical discoveries of Kepler\" (1814) that made up the set of three laws, by adding in the third. Small also claimed, against the history, that these were empirical laws, based on inductive reasoning.\n\nFurther, the current usage of \"Kepler's Second Law\" is something of a misnomer. Kepler had two versions, related in a qualitative sense: the \"distance law\" and the \"area law\". The \"area law\" is what became the Second Law in the set of three; but Kepler did himself not privilege it in that way.\n\nJohannes Kepler published his first two laws about planetary motion in 1609, having found them by analyzing the astronomical observations of Tycho Brahe. Kepler's third law was published in 1619. Kepler had believed in the Copernican model of the solar system, which called for circular orbits, but he could not reconcile Brahe's highly precise observations with a circular fit to Mars' orbit—Mars coincidentally having the highest eccentricity of all planets except Mercury. His first law reflected this discovery.\n\nKepler in 1621 and Godefroy Wendelin in 1643 noted that Kepler's third law applies to the four brightest moons of Jupiter. The second law, in the \"area law\" form, was contested by Nicolaus Mercator in a book from 1664, but by 1670 his \"Philosophical Transactions\" were in its favour. As the century proceeded it became more widely accepted. The reception in Germany changed noticeably between 1688, the year in which Newton's \"Principia\" was published and was taken to be basically Copernican, and 1690, by which time work of Gottfried Leibniz on Kepler had been published.\n\nNewton was credited with understanding that the second law is not special to the inverse square law of gravitation, being a consequence just of the radial nature of that law; while the other laws do depend on the inverse square form of the attraction. Carl Runge and Wilhelm Lenz much later identified a symmetry principle in the phase space of planetary motion (the orthogonal group O(4) acting) which accounts for the first and third laws in the case of Newtonian gravitation, as conservation of angular momentum does via rotational symmetry for the second law.\n\nThe mathematical model of the kinematics of a planet subject to the laws allows a large range of further calculations.\n\nMathematically, an ellipse can be represented by the formula:\n\nwhere formula_3 is the semi-latus rectum, \"ε\" is the eccentricity of the ellipse, \"r\" is the distance from the Sun to the planet, and \"θ\" is the angle to the planet's current position from its closest approach, as seen from the Sun. So (\"r\", \"θ\") are polar coordinates.\n\nFor an ellipse 0 < \"ε\" < 1 ; in the limiting case \"ε\" = 0, the orbit is a circle with the sun at the centre (i.e. where there is zero eccentricity).\n\nAt \"θ\" = 0°, perihelion, the distance is minimum\n\nAt \"θ\" = 90° and at \"θ\" = 270° the distance is equal to formula_3.\n\nAt \"θ\" = 180°, aphelion, the distance is maximum (by definition, aphelion is – invariably – perihelion plus 180°)\n\nThe semi-major axis \"a\" is the arithmetic mean between \"r\" and \"r\":\n\nThe semi-minor axis \"b\" is the geometric mean between \"r\" and \"r\":\n\nThe semi-latus rectum \"p\" is the harmonic mean between \"r\" and \"r\":\n\nThe eccentricity \"ε\" is the coefficient of variation between \"r\" and \"r\":\n\nThe area of the ellipse is\n\nThe special case of a circle is \"ε\" = 0, resulting in \"r\" = \"p\" = \"r\" = \"r\" = \"a\" = \"b\" and \"A\" = \"πr\".\n\nThe orbital radius and angular velocity of the planet in the elliptical orbit will vary. This is shown in the animation: the planet travels faster when closer to the sun, then slower when farther from the sun. Kepler's second law states that the blue sector has constant area.\n\nIn a small time formula_12 the planet sweeps out a small triangle having base line formula_13 and height formula_14 and area formula_15 and so the constant areal velocity is formula_16\n\nThe area enclosed by the elliptical orbit is formula_17 So the period formula_18 satisfies\n\nand the mean motion of the planet around the Sun\n\nsatisfies\n\nThis captures the relationship between the distance of planets from the Sun, and their orbital periods.\n\nKepler enunciated in 1619 this third law in a laborious attempt to determine what he viewed as the \"music of the spheres\" according to precise laws, and express it in terms of musical notation.\nSo it was known as the \"harmonic law\".\n\nUsing Newton's Law of gravitation (published 1687), this relation can be found in the case of a circular orbit by setting the centripetal force equal to the gravitational force:\n\nThen, expressing the angular velocity in terms of the orbital period and then rearranging, we find Kepler's Third Law:\n\nA more detailed derivation can be done with general elliptical orbits, instead of circles, as well as orbiting the center of mass, instead of just the large mass. This results in replacing a circular radius, formula_24, with the elliptical semi-major axis, formula_25, as well as replacing the large mass formula_26 with formula_27. However, with planet masses being so much smaller than the sun, this correction is often ignored. The full corresponding formula is:\n\nwhere formula_26 is the mass of the sun, formula_30 is the mass of the planet, and formula_31 is the gravitational constant, formula_32 is the orbital period and formula_25 is the elliptical semi-major axis.\n\nThe following table shows the data used by Kepler to empirically derive his law: \nUpon finding this pattern Kepler wrote:\n\nFor comparison, here are modern estimates:\nIsaac Newton computed in his \"Philosophiæ Naturalis Principia Mathematica\" the acceleration of a planet moving according to Kepler's first and second law.\n\nThis implies that the Sun may be the physical cause of the acceleration of planets. However, Newton states in his Principia that he considers forces from a mathematical point of view, not a physical, thereby taking an instrumentalist view. Moreover, he does not assign a cause to gravity.\n\nNewton defined the force acting on a planet to be the product of its mass and the acceleration (see Newton's laws of motion). So:\n\nThe Sun plays an unsymmetrical part, which is unjustified. So he assumed, in Newton's law of universal gravitation:\n\nAs the planets have small masses compared to that of the Sun, the orbits conform approximately to Kepler's laws. Newton's model improves upon Kepler's model, and fits actual observations more accurately (see two-body problem).\n\nBelow comes the detailed calculation of the acceleration of a planet moving according to Kepler's first and second laws.\n\nFrom the heliocentric point of view consider the vector to the planet formula_34 where formula_35 is the distance to the planet and formula_36 is a unit vector pointing towards the planet. \n\nwhere formula_38 is the unit vector whose direction is 90 degrees counterclockwise of formula_36, and formula_40 is the polar angle, and where a dot on top of the variable signifies differentiation with respect to time.\n\nDifferentiate the position vector twice to obtain the velocity vector and the acceleration vector:\n\nSo\n\nwhere the radial acceleration is\n\nand the transversal acceleration is\n\nKepler's second law says that\n\nis constant.\n\nThe transversal acceleration formula_46 is zero:\n\nSo the acceleration of a planet obeying Kepler's second law is directed towards the sun.\n\nThe radial acceleration formula_48 is\n\nKepler's first law states that the orbit is described by the equation:\n\nDifferentiating with respect to time\n\nor\n\nDifferentiating once more\n\nThe radial acceleration formula_48 satisfies\n\nSubstituting the equation of the ellipse gives\n\nThe relation formula_57 gives the simple final result\n\nThis means that the acceleration vector formula_59 of any planet obeying Kepler's first and second law satisfies the inverse square law\n\nwhere\n\nis a constant, and formula_36 is the unit vector pointing from the Sun towards the planet, and formula_13 is the distance between the planet and the Sun.\n\nAccording to Kepler's third law, formula_64 has the same value for all the planets. So the inverse square law for planetary accelerations applies throughout the entire solar system.\n\nThe inverse square law is a differential equation. The solutions to this differential equation include the Keplerian motions, as shown, but they also include motions where the orbit is a hyperbola or parabola or a straight line. See Kepler orbit.\n\nBy Newton's second law, the gravitational force that acts on the planet is:\n\nwhere formula_66 is the mass of the planet and formula_64 has the same value for all planets in the solar system. According to Newton's third Law, the Sun is attracted to the planet by a force of the same magnitude. Since the force is proportional to the mass of the planet, under the symmetric consideration, it should also be proportional to the mass of the Sun, formula_68. So\n\nwhere formula_31 is the gravitational constant.\n\nThe acceleration of solar system body number \"i\" is, according to Newton's laws:\n\nwhere formula_72 is the mass of body \"j\", formula_73 is the distance between body \"i\" and body \"j\", formula_74 is the unit vector from body \"i\" towards body \"j\", and the vector summation is over all bodies in the world, besides \"i\" itself.\n\nIn the special case where there are only two bodies in the world, Earth and Sun, the acceleration becomes\n\nwhich is the acceleration of the Kepler motion. So this Earth moves around the Sun according to Kepler's laws.\n\nIf the two bodies in the world are Moon and Earth the acceleration of the Moon becomes\n\nSo in this approximation the Moon moves around the Earth according to Kepler's laws.\n\nIn the three-body case the accelerations are\n\nThese accelerations are not those of Kepler orbits, and the three-body problem is complicated. But Keplerian approximation is the basis for perturbation calculations. See Lunar theory.\n\nKepler used his two first laws to compute the position of a planet as a function of time. His method involves the solution of a transcendental equation called Kepler's equation.\n\nThe procedure for calculating the heliocentric polar coordinates (\"r\",\"θ\") of a planet as a function of the time \"t\" since perihelion, is the following four steps:\n\n\nThe Cartesian velocity vector can along be trivially calculated as formula_82.\n\nThe important special case of circular orbit, \"ε\" = 0, gives \"θ\" = \"E\" = \"M\". Because the uniform circular motion was considered to be \"normal\", a deviation from this motion was considered an anomaly.\n\nThe proof of this procedure is shown below.\n\nThe Keplerian problem assumes an elliptical orbit and the four points:\nand\n\nThe problem is to compute the polar coordinates (\"r\",\"θ\") of the planet from the time since perihelion, \"t\".\n\nIt is solved in steps. Kepler considered the circle with the major axis as a diameter, and\n\nThe sector areas are related by formula_91\n\nThe circular sector area formula_92\n\nThe area swept since perihelion, \n\nis by Kepler's second law proportional to time since perihelion. So the mean anomaly, \"M\", is proportional to time since perihelion, \"t\".\n\nwhere \"n\" is the mean motion.\n\nWhen the mean anomaly \"M\" is computed, the goal is to compute the true anomaly \"θ\". The function \"θ\" = \"f\"(\"M\") is, however, not elementary. Kepler's solution is to use\nas an intermediate variable, and first compute \"E\" as a function of \"M\" by solving Kepler's equation below, and then compute the true anomaly \"θ\" from the eccentric anomaly \"E\". Here are the details.\n\nDivision by \"a\"/2 gives Kepler's equation\n\nThis equation gives \"M\" as a function of \"E\". Determining \"E\" for a given \"M\" is the inverse problem. Iterative numerical algorithms are commonly used.\n\nHaving computed the eccentric anomaly \"E\", the next step is to calculate the true anomaly \"θ\".\n\nNote from the figure that\n\nso that\n\nDividing by formula_25 and inserting from Kepler's first law\n\nto get\n\nThe result is a usable relationship between the eccentric anomaly \"E\" and the true anomaly \"θ\".\n\nA computationally more convenient form follows by substituting into the trigonometric identity:\n\nGet\n\nMultiplying by 1 + \"ε\" gives the result\n\nThis is the third step in the connection between time and position in the orbit.\n\nThe fourth step is to compute the heliocentric distance \"r\" from the true anomaly \"θ\" by Kepler's first law:\n\nUsing the relation above between \"θ\" and \"E\" the final equation for the distance \"r\" is:\n\n\n\n"}
{"id": "1065874", "url": "https://en.wikipedia.org/wiki?curid=1065874", "title": "List of X-15 flights", "text": "List of X-15 flights\n\nA list of North American X-15 pilots and flights.\n\n\nOnly X-15 free flights are shown on the above list.\n\n\n"}
{"id": "768492", "url": "https://en.wikipedia.org/wiki?curid=768492", "title": "Little Joe 5B", "text": "Little Joe 5B\n\nLittle Joe 5B was an unmanned Launch Escape System test of the Mercury spacecraft, conducted as part of the US Mercury program. The mission used production Mercury spacecraft # 14A. The mission was launched April 28, 1961, from Wallops Island, Virginia. The Little Joe 5B flew to an apogee of 2.8 miles (4.5 km) and a range of 9 miles (14 km). The mission lasted 5 minutes 25 seconds. Maximum speed was 1,780 mph (2865 km/h) and acceleration was 10 g (98 m/s²). The mission was a success and Mercury spacecraft # 14A was recovered.\n\nMercury spacecraft #14A used in the Little Joe 5B mission, is currently displayed at the Virginia Air and Space Center in Hampton.\n\n\n"}
{"id": "8865843", "url": "https://en.wikipedia.org/wiki?curid=8865843", "title": "Loop modeling", "text": "Loop modeling\n\nLoop modeling is a problem in protein structure prediction requiring the prediction of the conformations of loop regions in proteins with or without the use of a structural template. Computer programs that solve these problems have been used to research a broad range of scientific topics from ADP to breast cancer. Because protein function is determined by its shape and the physiochemical properties of its exposed surface, it is important to create an accurate model for protein/ligand interaction studies. The problem arises often in homology modeling, where the tertiary structure of an amino acid sequence is predicted based on a sequence alignment to a \"template\", or a second sequence whose structure is known. Because loops have highly variable sequences even within a given structural motif or protein fold, they often correspond to unaligned regions in sequence alignments; they also tend to be located at the solvent-exposed surface of globular proteins and thus are more conformationally flexible. Consequently, they often cannot be modeled using standard homology modeling techniques. More constrained versions of loop modeling are also used in the data fitting stages of solving a protein structure by X-ray crystallography, because loops can correspond to regions of low electron density and are therefore difficult to resolve.\n\nRegions of a structural model that are predicted by non-template-based loop modeling tend to be much less accurate than regions that are predicted using template-based techniques. The extent of the inaccuracy increases with the number of amino acids in the loop. The loop amino acids' side chains dihedral angles are often approximated from a rotamer library, but can worsen the inaccuracy of side chain packing in the overall model. Andrej Sali's homology modeling suite MODELLER includes a facility explicitly designed for loop modeling by a satisfaction of spatial restraints method. All methods require an upload of the PDB file and some require the specification of the loop location.\n\nIn general, the most accurate predictions are for loops of fewer than 8 amino acids. Extremely short loops of three residues can be determined from geometry alone, provided that the bond lengths and bond angles are specified. Slightly longer loops are often determined from a \"spare parts\" approach, in which loops of similar length are taken from known crystal structures and adapted to the geometry of the flanking segments. In some methods, the bond lengths and angles of the loop region are allowed to vary, in order to obtain a better fit; in other cases, the constraints of the flanking segments may be varied to find more \"protein-like\" loop conformations. The accuracy of such short loops may be almost as accurate as that of the homology model upon which it is based. It should also be considered that the loops in proteins may not be well-structured and therefore have no one conformation that could be predicted; NMR experiments indicate that solvent-exposed loops are \"floppy\" and adopt many conformations, while the loop conformations seen by X-ray crystallography may merely reflect crystal packing interactions, or the stabilizing influence of crystallization co-solvents.\n\nAs mentioned above homology-based methods use a database to align the target protein gap with a known template protein. A database of known structures is searched for a loop that fits the gap of interest by similarity of sequence and stems (the edges of the gap created by the unknown loop structure). The success of this method largely depends on the quality of that alignment. Since the loop is the least conserved portion of a protein’s structure, the homology-based method cannot always find a known template that aligns with the target sequence. Fortunately, the template databases are always adding new templates so the problem of not being able to find an alignment is becoming less of an issue. Some programs that use this method are SuperLooper and FREAD.\n\nOtherwise known as an \"ab initio\" method, non-template based approaches use a statistical model to fill in the gaps created by the unknown loop structure. Some of these programs include MODELLER, Loopy, and RAPPER; but each of these programs approaches the problem in a different manner. For example, Loopy uses samples of torsion angle pairs to generate the initial loop structure then it revises this structure to maintain a realistic shape and closure, while RAPPER builds from one end of the gap to the other by extending the stem with different sampled angles until the gap is closed. Yet another method is the “divide and conquer” approach. This involves subdividing the loop into 2 segments and then repeatedly dividing and transforming each segment until the loop is small enough to be solved. Even with all these methods non-template based approaches are most accurate up to 12 residues (amino acids within the loop).\n\nThere are three problems that arise when using a non-template based technique. First, there are constraints that limit the possibilities for local region modeling. One such constraint is that loop termini are required to end at the correct anchor position. Also, the Ramachandran space cannot contain a backbone of dihedral angles. Second, a modeling program has to use a set procedure. Some programs use the “spare parts” approach as mentioned above. Other programs use a \"de novo\" approach that samples sterically feasible loop conformations and selects the best one. Third, determining the best model means that a scoring method must be created to compare the various conformations.\n\n\n"}
{"id": "1346871", "url": "https://en.wikipedia.org/wiki?curid=1346871", "title": "Lévy distribution", "text": "Lévy distribution\n\n~~\\frac{e^{-\\frac{c}{2(x-\\mu)}}}{(x-\\mu)^{3/2}}</math>|\nwhere formula_9 is Euler's constant|\nIn probability theory and statistics, the Lévy distribution, named after Paul Lévy, is a continuous probability distribution for a non-negative random variable. In spectroscopy, this distribution, with frequency as the dependent variable, is known as a van der Waals profile. It is a special case of the inverse-gamma distribution. It is a stable distribution.\n\nThe probability density function of the Lévy distribution over the domain formula_11 is\n\nwhere formula_13 is the location parameter and formula_14 is the scale parameter. The cumulative distribution function is\n\nwhere formula_16 is the complementary error function. The shift parameter formula_13 has the effect of shifting the curve to the right by an amount formula_13, and changing the support to the interval [formula_13, formula_2). Like all stable distributions, the Levy distribution has a standard form f(x;0,1) which has the following property:\n\nwhere \"y\" is defined as\n\nThe characteristic function of the Lévy distribution is given by\n\nNote that the characteristic function can also be written in the same form used for the stable distribution with formula_24 and formula_25:\n\nAssuming formula_4, the \"n\"th moment of the unshifted Lévy distribution is formally defined by:\n\nwhich diverges for all \"n\" > 0 so that the moments of the Lévy distribution do not exist. The moment generating function is then formally defined by:\n\nwhich diverges for formula_30 and is therefore not defined in an interval around zero, so that the moment generating function is not defined \"per se\". Like all stable distributions except the normal distribution, the wing of the probability density function exhibits heavy tail behavior falling off according to a power law:\n\n(This shows that Lévy is not just Heavy-tailed but also Fat-tailed.) \n\nThis is illustrated in the diagram below, in which the probability density functions for various values of \"c\" and formula_4 are plotted on a log-log scale.\n\nThe standard Lévy distribution satisfies the condition of being Stable\n\nwhere formula_35 are independent standard Lévy-variables with formula_24.\n\n\nRandom samples from the Lévy distribution can be generated using inverse transform sampling. Given a random variate \"U\" drawn from the uniform distribution on the unit interval (0, 1], the variate \"X\" given by\n\nis Lévy-distributed with location formula_13 and scale formula_14. Here formula_54 is the cumulative distribution function of the standard normal distribution.\n\n\n"}
{"id": "44065248", "url": "https://en.wikipedia.org/wiki?curid=44065248", "title": "M82 X-2", "text": "M82 X-2\n\nM82 X-2 is an X-ray pulsar located in the galaxy Messier 82, approximately 12 million light-years from Earth. It is exceptionally luminous, radiating energy equivalent to approximately ten million Suns. This object is part of a binary system: If the pulsar is of an average size, , then its companion is at least . On average, the pulsar rotates every 1.37 seconds, and revolves around its more massive companion every 2.5 days.\n\nM82 X-2 is an ultraluminous X-ray source (ULX), shining about 100 times brighter than theory suggests something of its mass should be able to. Its brightness is many times higher than the Eddington limit, a basic physics guideline that sets an upper limit on the brightness that an object of a given mass should be able to achieve. Possible explanations for violations of the Eddington limit include geometrical effects arising from the funneling of in-falling material along magnetic field lines.\n\nWhile M82 X-2 was previously known as an X-ray source, it was not until an observation campaign to study the newly discovered supernova SN 2014J in January 2014 that X-2's true nature was uncovered. Scientists looking at data from the NuSTAR spacecraft noticed a pulsing in the X-ray spectrum coming from near the supernova in Messier 82. Data from the Chandra and Swift spacecraft was used to verify the NuSTAR findings and provide the necessary spatial resolution to determine the exact source. After combining the NuSTAR and Chandra data, scientists were able to discern that M82 X-2 emitted both an X-ray beam and continuous broad X-ray radiation.\n"}
{"id": "5920170", "url": "https://en.wikipedia.org/wiki?curid=5920170", "title": "Manned Orbital Development System", "text": "Manned Orbital Development System\n\nThe Manned Orbital Development System was created by the US Air Force Space System Division (SSD) in June 1962. It was to begin working on plans to use Gemini hardware as the first step in a new US Air Force man-in-space program called MODS (Manned Orbital Development System), a type of military space station that used Gemini spacecraft as ferry vehicles. The term Blue Gemini first showed up in August 1962 as part of a more specific proposal to fly six Gemini missions with Air Force pilots in a preliminary orientation and training phase of MODS. MODS was effectively superseded when the Manned Orbital Laboratory was announced in December 1963.\n\n"}
{"id": "4458701", "url": "https://en.wikipedia.org/wiki?curid=4458701", "title": "Molinology", "text": "Molinology\n\nMolinology (from Latin: molīna, mill; and Greek λόγος, study) is the study of mills and other mechanical devices which use the energy of moving water or wind, or the strength of animal or human muscle to power machines for purposes such as hammering, grinding, pumping, sawing, pressing or fulling. More particularly, molinology aims to retain the knowledge of those traditional engines which have been rendered obsolete by modern technical and economic trends. \n\nThe term \"Molinology\" was coined in 1965 by the Portuguese industrial historian João Miguel dos Santos Simões. \n\nCultural and scientific interest in molinology is maintained by The International Molinological Society (TIMS), a non-profit organisation which brings together around five hundred members worldwide. It was founded in 1973 after earlier international symposia in 1965 and 1969.\n\n\n\n"}
{"id": "34438892", "url": "https://en.wikipedia.org/wiki?curid=34438892", "title": "NIH Intramural Research Program", "text": "NIH Intramural Research Program\n\nThe NIH Intramural Research Program (IRP) is the internal research program of the National Institutes of Health (NIH), known for its synergistic approach to biomedical science. With 1,200 Principal Investigators and over 4,000 Postdoctoral Fellows conducting basic, translational, and clinical research, the NIH Intramural Research Program is the largest biomedical research institution on earth. The unique funding environment of the IRP facilitates opportunities to conduct both long-term and high-impact science that would otherwise be difficult to undertake. With rigorous external reviews ensuring that only the most outstanding research secures funding, the IRP is responsible for many scientific accomplishments, including the discovery of fluoride to prevent tooth decay, the use of lithium to manage bipolar disorder, and the creation of vaccines against hepatitis, Hemophilus influenzae (HIB), and human papillomavirus (HPV). In addition, the IRP has also produced or trained 21 Nobel Prize-winning scientists.\n\nWithin the framework of the NIH mandate, the Intramural Research Program’s mission is to:\n\nThe NIH Intramural Research Program (IRP) traces its roots to 1887, when a one-room laboratory on Staten Island was created within the Marine Hospital Service, a predecessor agency to the U.S. Public Health Service. This laboratory evolved into the Hygienic Laboratory, which moved to Washington, D.C., in 1891 and, with the Ransdell Act of 1930, became the National Institute of Health. Several of the IRP’s initial Institutes were established over the next two decades and, after World War II, Vannevar Bush, director of the Office of Scientific Research and Development, outlined a program for postwar scientific research that affirmed the contributions of \"remote and unexpected fields of medicine and the underlying sciences\" in the progress against disease and the benefits of cooperative endeavors with industry and academia. \nThe disease orientation and categorical structure of the IRP had its genesis in the establishment of the National Cancer Institute (NCI) in 1944. In 1948, Congress passed the National Heart Act, which created the National Heart Institute, and soon after established institutes for research on mental health, oral diseases, neurological problems, and blindness. Today, the IRP consists of individual programs housed in 23 of the NIH Institutes and Centers, creating a network of multi-disciplinary, federally funded laboratories with an emphasis on translational research. The National Institutes of Health Clinical Center, the world’s largest clinical research hospital, is designed to foster smooth transitions between laboratory work, patient studies, and bedside cures, facilitating the translation of laboratory findings to new approaches for the prevention and cure of human diseases.\n\nThe National Institutes of Health (NIH) is composed of 27 Institutes and Centers, most of which include research programs led by a Scientific Director and conducted by federal researchers and their trainees at one of several NIH campus locations. Collectively, these research programs encompass the Intramural Research Program (IRP). The IRP includes the United States National Library of Medicine, an international resource for researchers, and the NIH Clinical Center, the world’s largest clinical research hospital.\n\nIntramural researchers are affiliated with individual Laboratories, Branches or Centers, which are typically organized around common thematic research goals and approaches, much like a department or center at an academic institution. Within these larger structures, Principal Investigators run Sections or Units devoted to their independent research goals. Core facilities, supported by staff scientists and clinicians, are among the shared resources available to IRP researchers.\n\nScientific interests are not bound by the organizational structure. There exists a full spectrum of scientific interest groups (called SIGs) that brings researchers from different Institutes and Centers together around common areas of scientific interest where ideas can be shared and collaborations initiated. In addition, institutes come together to work cooperatively on major initiatives focused on unraveling the complexities of disease. The Center for Human Immunology, Autoimmunity and Inflammation (CHI) is one example of this trans-NIH cooperative research approach. \n\nAs with all biomedical research, the scientific programs of the IRP’s 1,200 Principal Investigators are subject to periodic scientific review. Each Principal Investigator must be peer-reviewed at least once every four years by an external Board of Scientific Counselors (BSC). The BSC evaluates the quality of research, the resources that should be allocated to scientists, and the promise of tenure-track investigators for future success in their careers. These evaluations are based on the Principal Investigator’s past accomplishments, objectives met, and future plans. The review criteria mirror those used by extramural peer review with the addition of considering whether the investigator is taking advantage of the special features of the NIH intramural scientific environment and employing useful collaborative arrangements. As a result of these reviews, recommendations for altering allocated resources are prepared by the BSC for the Scientific Director, the Institute or Center Director, the NIH Deputy Director for Intramural Research, and the Institute or Center (IC) National Advisory Council or Board.\n\nAdditionally, each IRP as a whole is subject to periodic review by Blue Ribbon Panels. These panels, made up of expert external reviewers appointed by the NIH Director, ensure that the program’s overall objectives are current, relevant, distinctive, and appropriate to the unique research environment of the IRP. Blue Ribbon Panel reports are addressed to the NIH Director, the Advisory Committee of the Director, and the IC Director. \n\nIn addition to the external NIH Boards of Scientific Counselors and Blue Ribbon Panels, the NIH IRP receives regular and periodic independent evaluations by the following external bodies:\n\nMichael M. Gottesman, M.D. is the Deputy Director for Intramural Research and heads the Office of Intramural Research (OIR). In this role, he is responsible for oversight and coordination of all intramural research, training and technology transfer activities.\n\nThe National Institutes of Health Clinical Center, the world’s largest hospital entirely devoted to clinical research, is a national resource that enables the rapid translation of scientific observations and laboratory discoveries into new approaches for diagnosing, treating and preventing disease. Due to its position on the main campus in Bethesda, Maryland, the Institutes and Centers of the IRP are able to mobilize clinical resources quickly and effectively to respond to emerging scientific challenges and opportunities.\n\nThe NIH Clinical Center is the 2011 recipient of the Lasker-Bloomberg Public Service Award, given by the Albert and Mary Lasker Foundation. This award honors the Clinical Center for serving as a model institution that has, since 1953, transformed scientific advances into innovative therapies and provided high-quality care to patients and recognizes the Clinical Center’s rich history of medical discovery through clinical research.\n\nAt the NIH Clinical Center, clinical research participants—more than 480,000 since the hospital opened in 1953—are active partners in medical discovery. This partnership has resulted in a long list of medical milestones, including the development of chemotherapy; the first use of an immunotoxin to treat a malignancy; identification of the genes that cause kidney cancer, leading to the development of six new, targeted treatments for advanced kidney cancer; the discovery that lithium helps depression; the first gene therapy; the first AIDS treatment; and the development of tests to detect AIDS/HIV and hepatitis viruses in blood, which led to a safer blood supply. The NIH Clinical Center sees 10,000 new research participants a year from around the world.\n\n"}
{"id": "15460100", "url": "https://en.wikipedia.org/wiki?curid=15460100", "title": "Rabbit (nuclear engineering)", "text": "Rabbit (nuclear engineering)\n\nIn the field of nuclear engineering, a rabbit refers to a pneumatically controlled tool used to insert small samples of material inside the core of a nuclear reactor usually for the purpose studying the effect of irradiation on the material. Some rabbits have special linings to screen out certain types of neutrons. (For example, the Missouri University of Science and Technology research reactor uses a cadmium-lined rabbit to allow only high-energy neutrons through to samples in its core.)\n"}
{"id": "585935", "url": "https://en.wikipedia.org/wiki?curid=585935", "title": "Racial transformation", "text": "Racial transformation\n\nRacial transformation is the process by which a demographic region (e.g., a country, neighborhood, or a school) changes in racial composition.\n"}
{"id": "58725707", "url": "https://en.wikipedia.org/wiki?curid=58725707", "title": "Ralph (New Horizons)", "text": "Ralph (New Horizons)\n\nRalph is a science instrument aboard the unmanned \"New Horizons\" spacecraft, which was launched in 2006. Ralph is a visible and infrared imager and spectrometer to provide maps of relevant astronomical targets based on data from that hardware. Ralph has two major subinstruments, LEISA and MVIC. MVIC stands for \"Multispectral Visible Imaging Camera\" and is a color imaging device, while LEISA originally stood for \"Linear Etalon Imaging Spectral Array\" and is an infrared imaging spectrometer for spaceflight. LEISA observes 250 discrete wavelengths of infrared light from 1.25 to 2.5 micrometers. MVIC is a pushbroom scanner type of design with seven channels, including red, blue, near-infrared (NIR), and methane.\n\nRalph is one of seven major instruments aboard \"New Horizons\" which was launched in 2006 and flew-by the dwarf planet Pluto in 2015.\n\nAt Pluto, Ralph enables the observation of many aspects including:\n\nRalph and Alice were used to characterize the atmosphere of Pluto in 2015. Ralph was previously used to observer the planet Jupiter and its moons in 2006 and in 2007 when it flew-by en route out of the Solar System and past Pluto. Observations of Jupiter were taken with Ralph in February 2007, when \"New Horizons\" was about 6 million kilometers (nearly 4 million miles) from the giant.\n\nRalph will observe MU69 on the upcoming flyby, with closest approach planned for new year's day 2019. Ralph, in conjunction with the LORRI telescope, will be used to make a digital elevation map of the body. (see also (486958) 2014 MU69 ('Ultima Thule')\n\nRalph is named after a character in \"The Honeymooners\" (1950s television show), along with another \"New Horizons\" instrument, Alice.\n\nLEISA's acronym was retitled from \"Linear Etalon Imaging Spectral Array\" to \"Lisa Hardaway Infrared Mapping Spectrometer\" by NASA in June 2017, after Ralph's program manager.\n\nAn example of Ralph's abilities is shown by this detection of methane on the surface of Pluto(left), overlayed on an image from the reconnaissance camera instrument on the right:\n\nIn 2018 it was announced, based on \"New Horizons\" high resolution data, that some of the plains of Pluto have dunes made of methane ice granules. The dunes are thought to have been formed by the blowing winds of Pluto, which are not as dense as those of Earth, and were compared to Dunes elsewhere in the solar system such as on Saturn's moon Titan.\n\nSpecs:\n\nThe one telescope feeds light to both LEISA and MVIC channels, with light split by a dichroic beamsplitter.\n\n\nMVIC has seven CCD's that are wide but narrow, utilizing a time-delay integration method to scan over an imaging area. These channels have a resolution of 5024x32 pixels, with the larger direction providing the swath of the image. There are seven channels, with 6 used for imaging and the seventh with an array of 5024x128 for navigation of where the imager is looking . MVIC has a field of view that is 5.8 degrees wide\n\nMVIC Bands:\n\nLESIA achieved its highest resolution data of Pluto of about 3 km/pixel at New Horizon's closest approach to Pluto on July, 14, 2015, when it was 47,000 km distant.\n\nDuring the flyby of Pluto on July 14, 2015, Ralph was able to collect data on Pluto and its moons yielding various image products. In addition, the MVIC color channels are often the source of color on the otherwise panchromatic LORRI images\n\n\n"}
{"id": "533770", "url": "https://en.wikipedia.org/wiki?curid=533770", "title": "Schrödinger's Kittens and the Search for Reality", "text": "Schrödinger's Kittens and the Search for Reality\n\nSchrödinger's Kittens and the Search for Reality is a 1995 book by John Gribbin, in which the author attempts to explain the mysteries of modern quantum mechanics in a popular-scientific way. It is a sequel to his earlier book, \"In Search of Schrödinger's Cat\" (1984).\n\nIn his epilogue, Gribbin touches on what were then the most recent developments of string theory, and introduces the transactional interpretation of quantum mechanics as the new \"mythology\" of our time. His argument does not refute the theory, but demonstrates how all theories can be true and mythological (depending on one's perspective).\n"}
{"id": "4647532", "url": "https://en.wikipedia.org/wiki?curid=4647532", "title": "Shams al-Dīn Abū Abd Allāh al-Khalīlī", "text": "Shams al-Dīn Abū Abd Allāh al-Khalīlī\n\nShams al-Dīn Abū ʿAbd Allāh Muḥammad ibn Muḥammad al-Khalīlī (; 1320–1380) was a Mamluk-era Syrian astronomer who compiled extensive tables for astronomical use. He worked for most of his life as a religious timekeeper (\"muwaqqit\") at the Umayyad Mosque in Damascus. Little else is known about his life.\n\nAl-Khalili is known for two sets of mathematical tables he constructed, both totaling roughly 30,000 entries. He tabulated all the entries made by the celebrated Egyptian Muslim astronomer Ibn Yunus, except for the entries that al-Khalili made himself for the city of Damascus. He computed 13,000 entries into his 'Universal Tables' of different auxiliary functions which allowed him to generate the solutions of standard problems of spherical astronomy for any given latitude. In addition to this, he created a 3,000 entry table that gave the direction of the city of Mecca (the Qibla) for all latitudes and longitudes for all the Muslim countries of the 14th century. Knowledge of the direction of the Qibla is essential in Islam because Muslims pray in the direction of Mecca. The values present in al-Khalili’s tables have been determined to be accurate up to three or four significant decimal digits. Up to the present time, it is not known how exactly al-Khalili went about calculating each of his entries.\n\n"}
{"id": "32998958", "url": "https://en.wikipedia.org/wiki?curid=32998958", "title": "Social dualism", "text": "Social dualism\n\nIn sociology and economics, Social dualism is a theory developed by economist Julius Herman Boeke which characterizes a society in the economic sense by the social spirit, the organisational forms and the technique dominating it. According to Boeke, \"These three aspects are interdependent and in this connection typify a society, in this way that a prevailing social spirit and the prevailing forms of organisation and of technique give the society its style, its appearance, so that in their interrelation they may be called the social system, the social style or the social atmosphere of that society \".\n\nAccording to Boeke, it is not necessary that a society be dominated exclusively by one social system. If one social system does prevail, the society in question is a homogeneous society. When, on the contrary two (or more) social systems appear simultaneously, we have a dual society.\nBoeke qualifies the term dual society by using it only for societies \"showing a distinct cleavage of two synchronic and full grown social styles which in the normal, historical evolution of homogeneous societies are separated from each other by transitional forms, as for instance, pre-capitalism and high capitalism by early capitalism.\"\n\nThis qualification is necessary because every society going through the process of evolution or endogenic social progression shows besides the prevailing social systems, the remains of the preceding and the beginnings of its future social style. If, on the other hand, one social system is imported from abroad and this system fails to oust or assimilate the prevailing social system, a dual society obviously exists.\n\nOn this account Boeke defines a dual society as a society where \"one of the two prevailing social systems, as a matter of fact always the most advanced, will have been imported from abroad and have gained its existence in the new environment without being able to oust or assimilate the divergent social system that has grown up there, with the result that neither of them becomes general and characteristic for that society as a whole.\"\n\nThe first characteristic of dualistic economies pointed out by Boeke is the relatively greater importance of social needs as compared to western economies. Boeke states, \"Possessions in the share of cattle, land, clothes, and houses, the fulfilment of social duties in all the circumstances of likr, must be all regarded as largely the satisfaction of social needs. It is not their economic usefulness, not the individual services they render their possessor which determine the value of the goods. It is a matter of secondary importance whether the land produces reasonable profit in proportion to the money paid for it whether the cattle can be made reasonably useful to their owner in his own business,whether the clothing covers, protects,warms the wearer or affects him pleasantly in any way. For it is not the use of these objects to the subject himself that gives them their worth in his eyes; it is what the community thinks of them that sets the standard.\n\nThe social dualism theory has been criticised by Benjamin Higgins on the following grounds:\nHiggins argues that the main issue in dualistic economies is to provide employment opportunities and Boeke's theory fails to do it and has developed the theory of Technological dualism as a response.\n\n\n\n"}
{"id": "4088382", "url": "https://en.wikipedia.org/wiki?curid=4088382", "title": "Square-free word", "text": "Square-free word\n\nIn combinatorics, a square-free word is a word (a sequence of characters) that does not contain any subword twice in a row.\n\nThus a square-free word is one that avoids the pattern \"XX\".\n\nOver a two-letter alphabet {\"a, b\"} the only square-free words are the empty word and \"a\", \"b\", \"ab\", \"ba\", \"aba\", and \"bab\". However, there exist infinite square-free words in any alphabet with three or more symbols, as proved by Axel Thue.\n\nOne example of an infinite square-free word over an alphabet of size 3 is the word over the alphabet {0,±1} obtained by taking the first difference of the Thue–Morse sequence. That is, from the Thue–Morse sequence\none forms a new sequence in which each term is the difference of two consecutive terms of the Thue–Morse sequence. The resulting square-free word is\n\nAnother example found by John Leech is defined recursively over the alphabet {\"a, b, c\"}. Let formula_1 be any word starting with the letter \"a\". Define the words formula_2 recursively as follows: the word formula_3 is obtained from formula_4 by replacing each \"a\" in formula_4 with , each \"b\" with , and each \"c\" with . It is possible to check that the sequence converges to the infinite square-free word\n\nA cube-free word is one with no occurrence of \"www\" for a factor \"w\". The Thue-Morse sequence is an example of a cube-free word over a binary alphabet. This sequence is not square-free but is \"almost\" so: the critical exponent is 2. The Thue–Morse sequence has no overlap or \"overlapping square\", instances of 0\"X\"0\"X\"0 or 1\"X\"1\"X\"1: it is essentially the only infinite binary word with this property. Dejean's theorem characterizes the minimum possible critical exponents for each alphabet size.\n\nThe Thue number of a graph \"G\" is the smallest number \"k\" such that \"G\" has a \"k\"-coloring for which the sequence of colors along every simple path is squarefree.\n\nThe Kolakoski sequence is an example of a cube-free sequence.\n\nAn abelian \"p\"-th power is a subsequence of the form formula_6 where each formula_4 is a permutation of formula_1. There is no abelian-square-free infinite word over an alphabet of size three: indeed, every word of length eight over such an alphabet contains an abelian square. There is an infinite abelian-square-free word over an alphabet of size five.\n\n"}
{"id": "18624923", "url": "https://en.wikipedia.org/wiki?curid=18624923", "title": "Sustainable engineering", "text": "Sustainable engineering\n\nSustainable engineering is the process of designing or operating systems such that they use energy and resources sustainably, in other words, at a rate that does not compromise the natural environment, or the ability of future generations to meet their own needs.\n\n\nEvery engineering discipline is engaged in sustainable design, employing numerous initiatives, especially life cycle analysis (LCA), pollution prevention, design for the environment (DfE), design for disassembly (DfD), and design for recycling (DfR). These are replacing or at least changing pollution control paradigms. For example, concept of a \"cap and trade\" has been tested and works well for some pollutants. This is a system where companies are allowed to place a \"bubble\" over a whole manufacturing complex or trade pollution credits with other companies in their industry instead of a \"stack-by-stack\" and \"pipe-by-pipe\" approach, i.e. the so-called \"command and control\" approach. Such policy and regulatory innovations call for some improved technology based approaches as well as better quality-based approaches, such as leveling out the pollutant loadings and using less expensive technologies to remove the first large bulk of pollutants, followed by higher operation and maintenance (O&M) technologies for the more difficult to treat stacks and pipes. But, the net effect can be a greater reduction of pollutant emissions and effluents than treating each stack or pipe as an independent entity. This is a foundation for most sustainable design approaches, i.e. conducting a life-cycle analysis, prioritizing the most important problems, and matching the technologies and operations to address them. The problems will vary by size (e.g. pollutant loading), difficulty in treating, and feasibility. The most intractable problems are often those that are small but very expensive and difficult to treat, i.e. less feasible. Of course, as with all paradigm shifts, expectations must be managed from both a technical and an operational perspective. \nHistorically, sustainability considerations have been approached by engineers as constraints on their designs. For example, hazardous substances generated by a manufacturing process were dealt with as a waste stream that must be contained and treated. The hazardous waste production had to be constrained by selecting certain manufacturing types, increasing waste handling facilities, and if these did not entirely do the job, limiting rates of production. Green engineering recognizes that these processes are often inefficient economically and environmentally, calling for a comprehensive, systematic life cycle approach. Green engineering attempts to achieve four goals:\n\n\nGreen engineering encompasses numerous ways to improve processes and products to make them more efficient from an environmental and sustainable standpoint. Every one of these approaches depends on viewing possible impacts in space and time. Architects consider the sense of place. Engineers view the site map as a set of fluxes across the boundary. The design must consider short and long-term impacts. Those impacts beyond the near-term are the province of sustainable design. \nThe effects may not manifest themselves for decades. In the mid-twentieth century, designers specified the use of what are now known to be hazardous building materials, such as asbestos flooring, pipe wrap and shingles, lead paint and pipes, and even structural and mechanical systems that may have increased the exposure to molds and radon. Those decisions have led to risks to people inhabiting these buildings. It is easy in retrospect to criticize these decisions, but many were made for noble reasons, such as fire prevention and durability of materials. However, it does illustrate that seemingly small impacts when viewed through the prism of time can be amplified exponentially in their effects.\nSustainable design requires a complete assessment of a design in place and time. Some impacts may not occur until centuries in the future. For example, the extent to which we decide to use nuclear power to generate electricity is a sustainable design decision. The radioactive wastes may have half-lives of hundreds of thousands of years. That is, it will take all these years for half of the radioactive isotopes to decay. Radioactive decay is the spontaneous transformation of one element into another. This occurs by irreversibly changing the number of protons in the nucleus. Thus, sustainable designs of such enterprises must consider highly uncertain futures. For example, even if we properly place warning signs about these hazardous wastes, we do not know if the English language will be understood.\nAll four goals of green engineering mentioned above are supported by a long-term, life cycle point of view. A life cycle analysis is a holistic approach to consider the entirety of a product, process or activity, encompassing raw materials, manufacturing, transportation, distribution, use, maintenance, recycling, and final disposal. In other words, assessing its life cycle should yield a complete picture of the product.\nThe first step in a life cycle assessment is to gather data on the flow of a material through an identifiable society. Once the quantities of various components of such a flow are known, the important functions and impacts of each step in the production, manufacture, use, and recovery/disposal are estimated. Thus, in sustainable design, engineers must optimize for variables that give the best performance in temporal frames.\n\n\nIn 2013, the average annual electricity consumption for a U.S. residential utility customer was 10,908 kilowatt hours (kWh), an average of 909 kWh per month. Louisiana had the highest annual consumption at 15,270 kWh, and Hawaii had the lowest at 6,176 kWh. Residential sector itself uses 18% of the total energy generated and therefore, incorporating sustainable construction practices there can be significant reduction in this number. Basic Sustainable construction practices include : \n\n\n\n"}
{"id": "4148511", "url": "https://en.wikipedia.org/wiki?curid=4148511", "title": "Technology trajectory", "text": "Technology trajectory\n\nTechnology trajectory refers to a single branch in the evolution of a technological design of a product/service, with nodes representing separate designs.\n\nMovement along the technology trajectory is associated with research and development. Due to institutionalisation of ideas, markets and professions, a technology development can get 'stuck' within one trajectory, and firms and engineers unable to adopt to ideas and innovation from outside.\n\n\n"}
{"id": "50548864", "url": "https://en.wikipedia.org/wiki?curid=50548864", "title": "The Invention of Nature", "text": "The Invention of Nature\n\nThe Invention of Nature: Alexander von Humboldt's New World is a nonfiction book released in 2015, by the historian Andrea Wulf about the Prussian naturalist, explorer and geographer Alexander von Humboldt. The book follows Humboldt from his early childhood and travels through Europe as a young man to his journey through Latin America and his return to Europe. Wulf makes the case that Humboldt synthesized knowledge from many different fields to form a vision of nature as one interconnected system, that would go on to influence scientists, activists and the public.\n\nPart 1. Departure: Emerging Ideas\n\nWulf describes Humboldt's childhood with his emotionally distant mother. As a child his interests in nature and travel were not taken seriously. His mother, on whom he was financially dependent, insisted he become a civil servant. As a young man, Humboldt became friends with Goethe and other German intellectuals. His mother's death allowed him the freedom and financial independence needed to journey to the new world.\n\nPart 2 Arrival: Collecting Ideas\n\nHumboldt arrives in Cuba with his companion Bonpland and begins his journey through Central and South America. He brought with him a plethora of scientific instruments. He chronicles his travels and the measurements he obtained using scientific instruments in his journals. Humboldt climbs the Chimborazo Volcano, which was then believed to be highest mountain in the world. The trip concludes with his visit to the United States where he visited the White House to discuss science and politics with Thomas Jefferson before returning to Europe.\n\nPart 3 Return: Sorting Ideas\n\nHumboldt returns to Europe where he is greeted as a celebrity. He lives as an expat in Paris for a seven months as he finds the city and its scientific culture more stimulating than that of Berlin. While in France, he meets a young Simon Bolivar, who is impressed with Humboldt's knowledge and passion for his home country of Venezuela, and they discuss South American politics. Humboldt returns to Prussia, to earn a salary in the King's court before returning to Paris. At this point he begins to work several manuscripts based on his travels. The books are widely read. As Bolivar begins to plan and execute Revolutions in South America, Humboldt publishes a series of books on the politics of Latin America that criticize colonialism.\n\nPart 4 New Worlds: Spreading Ideas\n\nWulf discusses Humboldt's personal correspondence and influence on a young Charles Darwin. Darwin attributed the inspiration for his voyage on the Beagle to Humboldt. Humboldt's influence on the American poet and philosopher Henry David Thoreau is explored. Humboldt's magnum opus \"Cosmos\", where he talks of the interconnections of the natural world, is discussed.\n\nInvention of Nature became a New York Times bestseller. Some critics felt that the book could have covered Humboldt more thoroughly instead of focusing on people he influenced. Others found that the book provided a contemporary Humboldt. In September 2016, the book was awarded the Royal Society Insight Investment Science Book Prize \n"}
{"id": "29974", "url": "https://en.wikipedia.org/wiki?curid=29974", "title": "Total war", "text": "Total war\n\nTotal war is warfare that includes any and all civilian-associated resources and infrastructure as legitimate military targets, mobilizes all of the resources of society to fight the war, and gives priority to warfare over non-combatant needs. The American-English Dictionary defines total war as \"war that is unrestricted in terms of the weapons used, the territory or combatants involved, or the objectives pursued, especially one in which the laws of war are disregarded.\"\n\nIn the mid-19th century, scholars identified \"total war\" as a separate class of warfare. In a total war, to an extent inapplicable in less total conflicts, the differentiation between combatants and non-combatants diminishes and sometimes it even vanishes entirely because opposing sides can consider nearly every human resource, even that of non-combatants, to be a part of the war effort.\n\nActions that may characterize the post-19th century concept of total war include:\n\nThe phrase can be traced back to the 1935 publication of the World War I memoir of German General Erich Ludendorff, \"Der totale Krieg\" (\"The total war\"). Some authors extend the concept back as far as classic work of Carl von Clausewitz, \"On War\", as \"absoluter Krieg\" (absolute war); however, different authors interpret the relevant passages in diverging ways. Total war also describes the French \"guerre à outrance\" during the Franco-Prussian War.\n\nIn his letter to his Chief of Staff, Union General Henry Halleck on 24 December 1864 described that the Union was \"not only fighting hostile armies, but a hostile people, and must make old and young, rich and poor, feel the hard hand of war, as well as their organized armies,\" defending Sherman's March to the Sea, the operation that inflicted widespread destruction of infrastructure in Georgia.\n\nUnited States Air Force General Curtis LeMay updated the concept for the nuclear age. In 1949, he first proposed that a total war in the nuclear age would consist of delivering the entire nuclear arsenal in a single overwhelming blow, going as far as \"killing a nation\".\n\nDuring the Middle Ages, destruction under the Mongol Empire in the 13th century effectively exemplified total war. The military forces of Genghis Khan slaughtered whole populations and destroyed any city that resisted:\n\nAuthor and historian Mark van de Logt wrote: \"Although military historians tend to reserve the concept of 'total war' for conflicts between modern industrial nations, the term nevertheless most closely approaches the state of affairs between the Pawnees and the Sioux and Cheyennes. Both sides directed their actions not solely against warrior-combatants but against the people as a whole. Noncombatants were legitimate targets. Indeed, the taking of a scalp of a woman or child was considered honorable because it signified that the scalp taker had dared to enter the very heart of the enemy's territory.\"\n\nIn The American Revolution, also known as the American Revolutionary War, many basic tactics of Total War were created in a modern form, I.E. Scorched Earth. in 1779, The Sullivan Expedition began, marching through Western Pennsylvania and up through New York, burning Iroquois villages to the ground, leaving nothing behind but smoldering ruin and dead bodies. In the Southern theater of the American Revolutionary War, The British and American Militia did similar campaigns of Extermination, but to their neighbors. Whole towns where depopulated. The British Cavalry led by Banastre Tarleton join the militia in the creation of the inferno called \"South Carolina\".\n\nThe French Revolutionary Wars introduced to mainland Europe some of the first concepts of total war, such as mass conscription. The fledgling republic found itself threatened by a powerful coalition of European nations. The only solution, in the eyes of the Jacobin government, was to pour the entire nation's resources into an unprecedented war effort—this was the advent of the levée en masse. The following decree of the National Convention on August 23, 1793 demonstrates the immensity of the French war effort, when the French front line forces grew to some 800,000 with a total of 1.5 million in all services—the first time an army in excess of a million had been mobilized in Western history:\n\nThe wars merged into the Napoleonic Wars of the First French Empire from ca. 1803. Over the coming two decades of almost constant warfare it is estimated that somewhere in the vicinity of five million died—probably about half of them civilians—and France alone counted nearly a million (by some sources in excess of a million) deaths.\n\nIn the Russian campaign of 1812 the Russians resorted to destroying infrastructure and agriculture in their retreat in order to hamper the French and strip them of adequate supplies. In the campaign of 1813, Allied forces in the German theater alone amounted to nearly one million whilst two years later in the Hundred Days a French decree called for the total mobilization of some 2.5 million men (though at most a fifth of this was managed by the time of the French defeat at Waterloo). During the prolonged Peninsular War from 1808–1814 some 300,000 French troops were kept permanently occupied by, in addition to several hundred thousand Spanish, Portuguese and British regulars, an enormous and sustained guerrilla insurgency—ultimately French deaths would amount to 300,000 in the Peninsular War alone.\n\nThe Taiping Rebellion (1850–1864) was one of the deadliest wars in history. About 20 million people died, many due to disease and famine. It followed the secession of the Taiping Heavenly Kingdom from the Qing Empire. Almost every citizen of the Heavenly Kingdom was given military training and conscripted into the army to fight against the Imperial forces.\n\nDuring the American Civil War, Union Army General Philip Sheridan's stripping of the Shenandoah Valley, beginning on September 21, 1864 and continuing for two weeks, was considered \"total war\". Its purpose was to eliminate food and supplies vital to the South's military operations, as well as to strike a blow at Southern civilian morale. Sheridan took the opportunity when he realized opposing forces had become too weak to resist his army.\n\nUnion Army General William Tecumseh Sherman's 'March to the Sea' in November and December 1864 destroyed the resources required for the South to make war. General Ulysses S. Grant and President Abraham Lincoln initially opposed the plan until Sherman convinced them of its necessity.\n\nScholars taking issue with the notion that Sherman was employing \"total war\" include Noah Andre Trudeau. Trudeau believes that Sherman's goals and methods do not meet the definition of total war and to suggest as much is to \"misread Sherman's intentions and to misunderstand the results of what happened\".\n\nAlmost the whole of Europe and the European colonial empires mobilized to wage World War I. Young men were removed from production jobs to serve in military roles, and were replaced on the production line by women. Rationing occurred on the home fronts. Bulgaria went so far as to mobilize a quarter of its population or 800,000 people, a greater share of its population than any other country during the war.\nOne of the features of Total War in Britain was the use of government propaganda posters to divert all attention to the war on the home front. Posters were used to influence public opinion about what to eat and what occupations to take, and to change the attitude of support towards the war effort. Even the Music Hall was used as propaganda, with propaganda songs aimed at recruitment.\n\nAfter the failure of the Battle of Neuve Chapelle, the large British offensive in March 1915, the British Commander-in-Chief Field Marshal John French blamed the lack of progress on insufficient and poor-quality artillery shells. This led to the Shell Crisis of 1915 which brought down both the Liberal government and Premiership of H. H. Asquith. He formed a new coalition government dominated by Liberals and appointed David Lloyd George as Minister of Munitions. It was a recognition that the whole economy would have to be geared for war if the Allies were to prevail on the Western Front.\n\nAs young men left the farms for the front, domestic food production in Britain and Germany fell. In Britain the response was to import more food, which was done despite the German introduction of unrestricted submarine warfare, and to introduce rationing. The Royal Navy's blockade of German ports prevented Germany from importing food and hastened German capitulation by creating a food crisis in Germany.\nThe Second World War was the quintessential total war of modernity. The level of national mobilization of resources on all sides of the conflict, the battlespace being contested, the scale of the armies, navies, and air forces raised through conscription, the active targeting of non-combatants (and non-combatant property), the general disregard for collateral damage, and the unrestricted aims of the belligerents marked total war on an unprecedented and unsurpassed, multicontinental scale.\n\nDuring the first part of the Shōwa era, the government of Imperial Japan launched a string of policies to promote a total war effort against China and occidental powers and increase industrial production. Among these were the National Spiritual Mobilization Movement and the Imperial Rule Assistance Association.\n\nThe National Mobilization Law had fifty clauses, which provided for government controls over civilian organizations (including labor unions), nationalization of strategic industries, price controls and rationing, and nationalized the news media. The laws gave the government the authority to use unlimited budgets to subsidize war production, and to compensate manufacturers for losses caused by war-time mobilization. Eighteen of the fifty articles outlined penalties for violators.\n\nTo improve its production, Shōwa Japan used millions of slave labourers and pressed more than 18 million people in East Asia into forced labor.\n\nBefore the onset of the Second World War, the United Kingdom drew on its First World War experience to prepare legislation that would allow immediate mobilization of the economy for war, should future hostilities break out.\n\nRationing of most goods and services was introduced, not only for consumers but also for manufacturers. This meant that factories manufacturing products that were irrelevant to the war effort had more appropriate tasks imposed. All artificial light was subject to legal blackouts.\n\nNot only were men conscripted into the armed forces from the beginning of the war (something which had not happened until the middle of World War I), but women were also conscripted as Land Girls to aid farmers and the Bevin Boys were conscripted to work down the coal mines.\n\nEnormous casualties were expected in bombing raids, so children were evacuated from London and other cities en masse to the countryside for compulsory billeting in households. In the long term this was one of the most profound and longer-lasting social consequences of the whole war for Britain. This is because it mixed up children with the adults of other classes. Not only did the middle and upper classes become familiar with the urban squalor suffered by working class children from the slums, but the children got a chance to see animals and the countryside, often for the first time, and experience rural life.\n\nThe use of statistical analysis, by a branch of science which has become known as Operational Research to influence military tactics was a departure from anything previously attempted. It was a very powerful tool but it further dehumanised war particularly when it suggested strategies which were counter intuitive. Examples where statistical analysis directly influenced tactics include the work done by Patrick Blackett's team on the optimum size and speed of convoys and the introduction of bomber streams by the Royal Air Force to counter the night fighter defences of the Kammhuber Line.\n\nIn contrast, Germany started the war under the concept of Blitzkrieg. Officially, it did not accept that it was in a total war until Joseph Goebbels' Sportpalast speech of 18 February 1943.\n\nThe commitment to the doctrine of the short war was a continuing handicap for the Germans; neither plans nor state of mind were adjusted to the idea of a long war until the failure of the Operation Barbarossa. A major strategical defeat in the Battle of Moscow forced Albert Speer, who was appointed as Germany's armament minister in early 1942, to nationalize German war production and eliminate the worst inefficiencies.\n\nUnder Speer's direction a threefold increase in armament production occurred and did not reach its peak until late 1944. To do this during the damage caused by the growing strategic Allied bomber offensive, is an indication of the degree of industrial under-mobilization in the earlier years. It was because the German economy through most of the war was substantially under-mobilized that it was resilient under air attack. Civilian consumption was high during the early years of the war and inventories both in industry and in consumers' possession were high. These helped cushion the economy from the effects of bombing.\n\nPlant and machinery were plentiful and incompletely used, thus it was comparatively easy to substitute unused or partly used machinery for that which was destroyed. Foreign labour, both slave labour and labour from neighbouring countries who joined the Anti-Comintern Pact with Germany, was used to augment German industrial labour which was under pressure by conscription into the \"Wehrmacht\" (Armed Forces).\n\nThe Soviet Union (USSR) was a command economy which already had an economic and legal system allowing the economy and society to be redirected into fighting a total war. The transportation of factories and whole labour forces east of the Urals as the Germans advanced across the USSR in 1941 was an impressive feat of planning. Only those factories which were useful for war production were moved because of the total war commitment of the Soviet government.\n\nThe Eastern Front of the European Theatre of World War II encompassed the conflict in central and eastern Europe from June 22, 1941 to May 9, 1945. It was the largest theatre of war in history in terms of numbers of soldiers, equipment and casualties and was notorious for its unprecedented ferocity, destruction, and immense loss of life (see World War II casualties). The fighting involved millions of German, Hungarian, Romanian and Soviet troops along a broad front hundreds of kilometres long. It was by far the deadliest single theatre of World War II. Scholars now believe that at most 27 million Soviet citizens died during the war, including at least 8.7 million soldiers who fell in battle against Hitler's armies or died in POW camps. Millions of civilians died from starvation, exposure, atrocities, and massacres. The Axis lost over 5 million soldiers in the east as well as many thousands of civilians.\n\nDuring the Battle of Stalingrad, newly built T-34 tanks were driven—unpainted because of a paint shortage—from the factory floor straight to the front. This came to symbolise the USSR's commitment to the Great Patriotic War and demonstrated the government's total war policy.\n\nThe United States underwent an unprecedented mobilization of national resources for the Second World War. Conditions in the United States were not as strained as they were in the United Kingdom or as desperate as they were in the Soviet Union, but the United States greatly curtailed nearly all non-essential activities in its prosecution of the Second World War and redirected nearly all available national resources to the conflict, including reaching the point of diminishing returns by late 1944, where the U.S. military was unable to find any more males of the correct military age to draft into service.\n\nThe strategists of the U.S. military looked abroad at the storms brewing on the horizon in Europe and Asia, and began quietly making contingency plans as early as the mid-1930s; new weapons and weapons platforms were designed, and made ready. Following the outbreak of war in Europe and the ongoing aggression in Asia, efforts were stepped up significantly. The collapse of France and the airborne aggression directed at Great Britain unsettled the Americans, who had close relations with both nations, and a peacetime draft was instituted, along with Lend-Lease programs to aid the British, and covert aid was passed to the Chinese as well.\n\nAmerican public opinion was still opposed to involvement in the problems of Europe and Asia, however. In 1941, the Soviet Union became the latest nation to be invaded, and the U.S. gave her aid as well. American ships began defending aid convoys to the Allied nations against submarine attacks, and a total trade embargo against the Empire of Japan was instituted to deny its military the raw materials its factories and military forces required to continue its offensive actions in China.\n\nIn late 1941, Japan's Army-dominated government decided to seize by military force the strategic resources of South-East Asia and Indonesia since the Western powers would not give Japan these goods by trade. Planning for this action included surprise attacks on American and British forces in Hong Kong, the Philippines, Malaya, and the U.S. naval base and warships at Pearl Harbor. In response to these attacks, the U.K. and U.S. declared war on the Empire of Japan the next day. Nazi Germany declared war on the U.S. a few days later, along with Fascist Italy; the U.S. found itself fully involved in a second world war.\nAs the United States began to gear up for a major war, information and propaganda efforts were set in motion. Civilians (including children) were encouraged to take part in fat, grease, and scrap metal collection drives. Many factories making non-essential goods retooled for war production. Levels of industrial productivity previously unheard of were attained during the war; multi-thousand-ton convoy ships were routinely built in a month-and-a-half, and tanks poured out of the former automobile factories. Within a few years of the U.S. entry into the Second World War, nearly every man fit for service, between 18 and 30, had been conscripted into the military \"for the duration\" of the conflict, and unprecedented numbers of women took up jobs previously held by them. Strict systems of rationing of consumer staples were introduced to redirect productive capacity to war needs.\n\nPreviously untouched sections of the nation mobilized for the war effort. Academics became technocrats; home-makers became bomb-makers (massive numbers of women worked in heavy industry during the war); union leaders and businessmen became commanders in the massive armies of production. The great scientific communities of the United States were mobilized as never before, and mathematicians, doctors, engineers, and chemists turned their minds to the problems ahead of them.\n\nBy the war's end a multitude of advances had been made in medicine, physics, engineering, and the other sciences. Even the theoretical physicists, whose theories were not believed to have military applications (at the time), were sent far into the Western deserts to work at the Los Alamos National Laboratory on the Manhattan Project that culminated in the Trinity nuclear test and changed the course of history.\n\nIn the war, the United States lost 407,316 military personnel, but had managed to avoid the extensive level of damage to civilian and industrial infrastructure that other participants suffered. The U.S. emerged as one of the two superpowers after the war.\n\nAfter the United States entered World War II, Franklin D. Roosevelt declared at Casablanca conference to the other Allies and the press that unconditional surrender was the objective of the war against the Axis Powers of Germany, Italy, and Japan. Prior to this declaration, the individual regimes of the Axis Powers could have negotiated an armistice similar to that at the end of World War I and then a conditional surrender when they perceived that the war was lost.\n\nThe unconditional surrender of the major Axis powers caused a legal problem at the post-war Nuremberg Trials, because the trials appeared to be in conflict with Articles 63 and 64 of the Geneva Convention of 1929. Usually if such trials are held, they would be held under the auspices of the defeated power's own legal system as happened with some of the minor Axis powers, for example in the post World War II Romanian People's Tribunals. To circumvent this, the Allies argued that the major war criminals were captured after the end of the war, so they were not prisoners of war and the Geneva Conventions did not cover them. Further, the collapse of the Axis regimes created a legal condition of total defeat (\"debellatio\") so the provisions of the 1907 Hague Convention over military occupation were not applicable.\n\nSince the end of World War II, no industrial nation has fought such a large, decisive war. This is likely due to the availability of nuclear weapons, whose destructive power and quick deployment render a full mobilization of a country's resources such as in World War II unnecessary. Such weapons are developed and maintained with relatively modest peacetime defense budgets.\n\nBy the end of the 1950s, the ideological stand-off of the Cold War between the Western World and the Soviet Union had resulted in thousands of nuclear weapons being aimed by each side at the other. Strategically, the equal balance of destructive power possessed by each side situation came to be known as Mutually Assured Destruction (MAD), considering that a nuclear attack by one superpower would result in nuclear counter-strike by the other. This would result in hundreds of millions of deaths in a world where, in words widely attributed to Nikita Khrushchev, \"The living will envy the dead\".\n\nDuring the Cold War, the two superpowers sought to avoid open conflict between their respective forces, as both sides recognized that such a clash could very easily escalate, and quickly involve nuclear weapons. Instead, the superpowers fought each other through their involvement in proxy wars, military buildups, and diplomatic standoffs.\n\nIn the case of proxy wars, each superpower supported its respective allies in conflicts with forces aligned with the other superpower, such as in the Vietnam War and the Soviet invasion of Afghanistan.\n\nDuring the Yugoslav Wars, NATO conducted strikes against the electrical grid in enemy territory using graphite bombs. Some observers considered this to be an act of total war, owing to the fact that powerplants supported by the electrical grid were essential to water purification and thus the strike represented a direct attack on civilian resources. NATO claimed that the objective of their strikes was to disrupt military infrastructure and communications.\n\n\n"}
{"id": "57261916", "url": "https://en.wikipedia.org/wiki?curid=57261916", "title": "William Augustus Edmond Ussher", "text": "William Augustus Edmond Ussher\n\nWilliam Augustus Edmond Ussher (8 July 1849 – 19 March 1920) was a British geologist.\n\nUssher, born in County Galway, was the youngest of six children in an Irish Protestant family that could trace its ancestry back to Archbishop James Ussher. In April 1868 William Ussher joined the Geological Survey after passing a civil service examination. He retired from the Survey in 1909 after making major contributions to establishing the stratigraphic succession in the Devonian, Carboniferous, and Permian-Triassic rocks in southwestern England, especially Cornwall, Devon, and West Somerset. He contributed articles to the \"Geological Magazine\", the \"Quarterly Journal of the Geological Society of London\", and the \"Proceedings of the Somersetshire Archaeological and Natural History Society\", and several other learned journals. He was awarded the Murchison Medal in 1914.\n\nThe Ussher Society, named in his honour, was founded in 1962 to promote the study of geology and geomorphology in southwest England.\n"}
{"id": "4559334", "url": "https://en.wikipedia.org/wiki?curid=4559334", "title": "Worldwide Youth in Science and Engineering", "text": "Worldwide Youth in Science and Engineering\n\nThe Worldwide Youth in Science and Engineering (WYSE) Academic Challenge is a high school academic competition run in Illinois and Missouri by the University of Illinois at Urbana-Champaign and Missouri University of Science and Technology, respectively. The team competition consists of 14-person teams from multiple high schools each taking two exams. There are seven subject areas from which each student chooses their two tests: Biology, Chemistry, Computer Science, Engineering Graphics/Drafting, English, Mathematics, and Physics. Awards are given to both teams and individuals at three progressively harder levels; Regionals, Sectionals, and the State Finals.\n\nThe tests are 40 minute multiple choice tests. Each test has a different number of questions. Computer Science and Mathematics are 30 question exams; Physics 35; Biology 50; Chemistry, and Engineering Graphics 40; and English 80. These questions are divided into subcategories in each field; for instance, there are 14 Algebra questions, 7 Geometry questions, etc. on the Mathematics exam at the Regional Level.\n\nSchools competing in the WYSE Competition are divided into 4 divisions based on their IHSA classification enrollment; Division 300 features schools with a maximum of 300 enrolled students, Division 700 features schools with an enrollment larger than 300 and smaller than 700, Division 1500 from 700 to 1500, and Division Unlimited for schools larger than 1500 students. The tests are the same in each division.\n\nSchools are divided geographically into Sectionals; nine in Illinois, two in Missouri. Each Sectional is divided again into smaller Regionals, and Sectionals can have anywhere from two to five Regionals within it. Schools from different divisions can and do attend the same Regional and Sectional, even though they do not compete against each other. Some Regionals may not be represented in certain Divisions if no schools of that enrollment participate in the competition. \n\nTests are graded and ranked from highest score to lowest score. At Regionals and Sectionals, the top 3 scores in each test, including ties, are awarded medals. At the State Finals, the top 6 scores including ties are awarded medals.\n\nIndividual tests are graded and ranked from highest score to lowest score. The two highest scores in each subject for a school are added together to determine the school's Raw Score for that subject. If a school has only one score in a subject, the Raw Score is zero.\n\nThe highest Raw Score for a subject is considered the Normalizing Raw Score. This Raw Score is divided into 100 to find the Normalizing Constant. (100/Top Raw Score=Normalizing Constant) Every Raw Score for that subject in that division is then multiplied by the Normalizing Constant to find the school's normalized score. This is done for each subject in each division.\n\nOnce the normalized scores have been found, the school's score is determined by adding the normalized scores for five of the seven tests together. The five tests are English, Chemistry, Mathematics, and the two highest normalized scores of the remaining four tests (Biology, Computer Science, Engineering Graphics, and Physics).\n\nIndividuals and teams advance from Regionals to Sectionals to the State Finals based on their placement at the current level of competition.\n\nIndividuals who place 1st or 2nd, including ties, at either Regionals or Sectionals advance to the next level. Thus, if there is one 1st place individual and a four-way tie for 2nd, five individuals will advance to the next level for that subject.\n\nAt the Sectional level, individuals can also advance to the State Finals by scoring a pre-determined qualifying score on a test. This prevents Sectionals from advancing only the top two scores when \nthere are additional high scores below the 2nd place finisher(s).\n\nIf an individual qualifies to advance for one test, that individual still takes two tests at the next level, even if their school does not advance with the individual.\n\nTeams advance to the next level based on their finish compared to the number of schools competing.\nIf there are 1 or 2 teams competing, both advance.\nIf there are from 3-7 teams, the top 2 advance.\nIf there are from 8-12 teams, the top 3 advance.\nIf there are from 13-16 teams, the top 4 advance.\nIf there are more than 16 teams, the top 5 advance.\n\nWYSE Website\n"}
{"id": "15954824", "url": "https://en.wikipedia.org/wiki?curid=15954824", "title": "Zooillogix", "text": "Zooillogix\n\nZooillogix is a zoology blog on the ScienceBlogs network, created and edited by Andrew and Benny Bleiman. The site has been featured on ABC News, in Seed magazine, Mental Floss, FHM, and the Annals of Improbable Research, awarders of the Ig Nobel Prize. The site attracts a diverse readership from notable scientists, such as PZ Myers, to biology students to young children.\n\nZooillogix focuses on bizarre zoological news, covering research published in scientific journals, such as the Public Library of Science (PLoS), as well as stories reported in general news outlets. Typical items include the discovery of new species, newly documented animal behavior, zoo and aquarium industry news, and interviews with scientists and researchers. Content is written to be accessible to a non-scientific audience.\n\n\n"}
