{"id": "16526332", "url": "https://en.wikipedia.org/wiki?curid=16526332", "title": "Aitkenhead Glacier", "text": "Aitkenhead Glacier\n\nAitkenhead Glacier () is a long glacier flowing east-southeast from the Detroit Plateau, Graham Land, (south of Mancho Buttress and Baley Nunatak and north of Simpson Nunatak and Hitar Petar Nunatak) into Prince Gustav Channel (close north of Alectoria Island). It was mapped from surveys by the Falkland Islands Dependencies Survey (FIDS) (1960–61), and named by the United Kingdom Antarctic Place-Names Committee for Neil Aitkenhead, a FIDS geologist at Hope Bay (1959–60).\n\n\n\n"}
{"id": "2766", "url": "https://en.wikipedia.org/wiki?curid=2766", "title": "Ames test", "text": "Ames test\n\nThe Ames test is a widely employed method that uses bacteria to test whether a given chemical can cause mutations in the DNA of the test organism. More formally, it is a biological assay to assess the mutagenic potential of chemical compounds. A positive test indicates that the chemical is mutagenic and therefore may act as a carcinogen, because cancer is often linked to mutation. The test serves as a quick and convenient assay to estimate the carcinogenic potential of a compound because standard carcinogen assays on mice and rats are time-consuming (taking two to three years to complete) and expensive. However, false-positives and false-negatives are known.\n\nThe procedure was described in a series of papers in the early 1970s by Bruce Ames and his group at the University of California, Berkeley.\n\nThe Ames test uses several strains of the bacterium \"Salmonella typhimurium\" that carry mutations in genes involved in histidine synthesis. These strains are auxotrophic mutants, i.e. they require histidine for growth, but cannot produce it. The method tests the capability of the tested substance in creating mutations that result in a return to a \"prototrophic\" state, so that the cells can grow on a histidine-free medium.\n\nThe tester strains are specially constructed to detect either frameshift (e.g. strains TA-1537 and TA-1538) or point (e.g. strain TA-1531) mutations in the genes required to synthesize histidine, so that mutagens acting via different mechanisms may be identified. Some compounds are quite specific, causing reversions in just one or two strains. The tester strains also carry mutations in the genes responsible for lipopolysaccharide synthesis, making the cell wall of the bacteria more permeable, and in the excision repair system to make the test more sensitive.\n\nLarger organisms like mammals have metabolic processes that could potentially turn a chemical considered not mutagenic into one that is or one that is considered mutagenic into one that is not. Therefore, to more effectively test a chemical compound's mutagenicity in relation to larger organisms, rat liver enzymes can be added in an attempt to replicate the metabolic processes' effect on the compound being tested in the Ames Test. Rat liver extract is optionally added to simulate the effect of metabolism, as some compounds, like benzo[\"a\"]pyrene, are not mutagenic themselves but their metabolic products are.\n\nThe bacteria are spread on an agar plate with small amount of histidine. This small amount of histidine in the growth medium allows the bacteria to grow for an initial time and have the opportunity to mutate.\nWhen the histidine is depleted only bacteria that have mutated to gain the ability to produce its own histidine will survive. The plate is incubated for 48 hours. The mutagenicity of a substance is proportional to the number of colonies observed.\n\nMutagens identified via Ames test are also possible carcinogens, and early studies by Ames showed that 90% of known carcinogens may be identified via this test. Later studies however showed identification of 50–70% of known carcinogens. The test was used to identify a number of compounds previously used in commercial products as potential carcinogens. Examples include tris(2,3-dibromopropyl)phosphate, which was used as a flame retardant in plastic and textiles such as children's sleepwear, and furylfuramide which was used as an antibacterial additive in food in Japan in 1960s and 1970s. Furylfuramide in fact had previously passed animal test, but more vigorous tests after its identification in the Ames test showed it to be carcinogenic. Their positive tests resulted in those chemicals being withdrawn from use in consumer products.\n\nOne interesting result from the Ames test is that the dose response curve using varying concentrations of chemical is almost always linear, indicating that there is no threshold concentration for mutagenesis. It therefore suggests that, as with radiations, there may be no safe threshold for chemical mutagens or carcinogens. However some proposed that organisms can tolerate low level of mutagens due to protective mechanisms such as DNA repair, and threshold may exist for certain chemical mutagens. Bruce Ames himself argued against linear dose-response extrapolation from the high dose used in carcinogenesis tests in animal systems to the lower dose of chemicals normally encountered in human exposure, as the results may be false positives due to mitogenic response caused by the artificially high dose of chemicals used in such tests. He also cautioned against the \"hysteria over tiny traces of chemicals that may or may not cause cancer\", that \"completely drives out the major risks you should be aware of.\"\n\nThe Ames test is often used as one of the initial screens for potential drugs to weed out possible carcinogens, and it is one of the eight tests required under the Pesticide Act (USA) and one of six tests required under the Toxic Substances Control Act (USA).\n\n\"Salmonella typhimurium\" is a prokaryote, therefore it is not a perfect model for humans. Rat liver S9 fraction is used to mimic the mammalian metabolic conditions so that the mutagenic potential of metabolites formed by a parent molecule in the hepatic system can be assessed; however, there are differences in metabolism between human and rat that can affect the mutagenicity of chemicals being tested. The test may therefore be improved by the use of human liver S9 fraction; its use was previously limited by its availability, but it is now available commercially and therefore may be more feasible. An adapted \"in vitro\" model has been made for eukaryotic cells, for example yeast.\n\nMutagens identified in the Ames test need not necessarily be carcinogenic, and further tests are required for any potential carcinogen identified in the test. Drugs that contain the nitrate moiety sometimes come back positive for Ames when they are indeed safe. The nitrate compounds may generate nitric oxide, an important signal molecule that can give a false positive. Nitroglycerin is an example that gives a positive Ames yet is still used in treatment today. Nitrates in food however may be reduced by bacterial action to nitrites which are known to generate carcinogens by reacting with amines and amides. Long toxicology and outcome studies are needed with such compounds to disprove a positive Ames test.\n\nThe Ames test was initially developed using agar plates (the plate incorporation technique), as described above. Since that time, a popular alternative to performing the Ames test has been developed, which is known as the \"fluctuation method\". This technique is the same in concept as the agar-based method, with bacteria being added to a reaction mixture with a small amount of histidine, which allows the bacteria to grow and mutate, returning to synthesize their own histidine. By including a pH indicator, the frequency of mutation is counted in microplates as the number of wells which have changed color (caused by a drop in pH due to metabolic processes of reproducing bacteria). As with the traditional Ames test, the sample is compared to the natural background rate of reverse mutation in order to establish the genotoxicity of a substance.The fluctuation method is performed entirely in liquid culture and is scored by counting the number of wells that turn yellow from purple in 96-well or 384-well microplates. \nIn the 96-well plate method the frequency of mutation is counted as the number of wells out of 96 which have changed color. The plates are incubated for up to five days, with mutated (yellow) colonies being counted each day and compared to the background rate of reverse mutation using established tables of significance to determine the significant differences between the background rate of mutation and that for the tested samples.\n\nIn the more scaled-down 384-well plate microfluctuation method the frequency of mutation is counted as the number of wells out of 48 which have changed color after 2 days of incubation. A test sample is assayed across 6 dose levels with concurrent zero-dose (background) and positive controls which all fit into one 384-well plate. The assay is performed in triplicates to provide statistical robustness. It uses the recommended OECD Guideline 471 tester strains (histidine auxotrophs and tryptophan auxotrophs).\n\nThe fluctuation method is comparable to the traditional pour plate method in terms of sensitivity and accuracy, however, it does have a number of advantages: it needs less test sample, it has a simple colorimetric endpoint, counting the number of positive wells out of possible 96 or 48 wells is much less time consuming than counting individual colonies on an agar plate. Several commercial kits are available. Most kits have consumable components in a ready-to-use state, including lyophilized bacteria, and tests can be performed using multichannel pipettes. The fluctuation method also allows for testing higher volumes of aqueous samples (up to 75% v/v), increasing the sensitivity and extending its application to low-level environmental mutagens.\n\n"}
{"id": "16267127", "url": "https://en.wikipedia.org/wiki?curid=16267127", "title": "Annals of the Former World", "text": "Annals of the Former World\n\nAnnals of the Former World is a book on geology written by John McPhee and published in 1998 by Farrar, Straus and Giroux. It won the 1999 Pulitzer Prize for General Non-Fiction.\n\nThe book presents a geological history of North America, and was researched and written over the course of two decades beginning in 1978. It consists of a compilation of five books, the first four of which were previously published as \"Basin and Range\" (1981), \"In Suspect Terrain\" (1983), \"Rising from the Plains\" (1986), and \"Assembling California\" (1993), plus a final book, \"Crossing the Craton\". A narrative table of contents provides an overview of the project, which largely consisted of a series of road journeys by McPhee across the North American continent in the company of noted geologists.\n\n"}
{"id": "4323107", "url": "https://en.wikipedia.org/wiki?curid=4323107", "title": "Arrestee Drug Abuse Monitoring", "text": "Arrestee Drug Abuse Monitoring\n\nArrestee Drug Abuse Monitoring, or ADAM, was a survey conducted by the U.S. Department of Justice to gauge the prevalence of alcohol and illegal drug use among prior arrestees. It was a reformulation of the prior Drug Use Forecasting (DUF) program, focused on five drugs in particular: cocaine, marijuana, methamphetamine, opiates, and PCP.\n\nParticipants were randomly selected from arrest records in major metropolitan areas; because no personally identifying information is taken from each record chosen, the resulting data can be correlated to arrest rates, but not to the total population of persons charged.\n\nADAM began as the Drug Use Forecasting program in 1987, which tested arrestees in 13 (later 23) jurisdictions on a quarterly basis. In 1991, juvenile data was added for the first time, at select sites. In 1996, President Clinton requested that the program be expanded, as \"ADAM\", by tripling the size to 75 metropolitan areas and adding an outreach component for non-metropolitan offenders, as a scientific control. At most, 42 jurisdictions ever participated in the program at one time.\n\nInformation was obtained from personal interviews and urine analysis obtained voluntarily and confidentially, usually on the day of arrest and always within 48 hours of arrest.\n\nUnder DUF, both male and female subjects were selected on a random basis. When ADAM was fully implemented in 2000, a different methodology was adopted, whereby male subjects were chosen at random and female subjects were chosen where available (not all participating sites have sufficient numbers of women arrestees to be statistically sound; ADAM defined this as 25 women available to be interviewed). In addition, the interview portion of the ADAM program was expanded to cover behaviors and drug use that could not be tested for by urinalysis, such as alcohol abuse. The catchment areas were redefined, from metropolitan area/city limits to county boundaries, in order to help standardize the data.\n\nOn January 29, 2004, the ADAM program was halted due to funding concerns; government sources state that a revised program was to be implemented in late 2005. The program was reimplemented in 2007 funded by ONDCP and run by Abt Associates. ONDCP continued the program in 10 of the original site until 2014. Results can be seen on the ONDCP website.\n\nThese sites were used at some or all points of the DUF/ADAM studies as testing areas. Some of the smaller locations such as Woodbury County, Iowa and Rio Arriba, New Mexico were added late in the program's life, in an attempt to gain information about non-metropolitan areas. In 2007 tenof the original sites continued: NYC, Chicago, Washington DC, Denver, Charlotte, Indianapolis, Sacramento, Portland, Minneapolis, Atlanta. In the last two years of operation the program continued in five of these sites: Denver, Sacramento, NYC, Atlanta,Chicago.\n\n\n"}
{"id": "21671434", "url": "https://en.wikipedia.org/wiki?curid=21671434", "title": "Barlow's law", "text": "Barlow's law\n\nBarlow's law was an incorrect physical law proposed by Peter Barlow in 1825 to describe the ability of wires to conduct electricity. It said that the strength of the effect of electricity passing through a wire varies inversely with the square root of its length and directly with the square root of its cross-sectional area, or, in modern terminology:\n\nwhere \"I\" is electric current, \"A\" is the cross-sectional area of the wire, and \"L\" is the length of the wire. Barlow formulated his law in terms of the diameter \"d\" of a cylindrical wire. Since \"A\" is proportional to the square of \"d\" the law becomes formula_2 for cylindrical wires.\n\nBarlow undertook his experiments with the aim of determining whether long-distance telegraphy was feasible, and believed he proved that it was not. The publication of Barlow's law delayed research into telegraphy for several years, until 1831 when Joseph Henry and Philip Ten Eyck constructed a circuit 1,060 feet long, which used a large battery to activate an electromagnet. Importantly, Barlow did not investigate the dependence of the current strength on electric tension (that is, voltage). He endeavoured to keep this constant, so neglected the possibility of solutions such as a high-intensity battery or step-up voltage converters to allow long-distance telegraphy.\n\nIn 1827, Georg Ohm published a different law, stating that the current varies directly with voltage (\"V\") and inversely with the wire's length, not its square root; that is, formula_3. Ohm's law is now considered the correct law and Barlow's false.\n\nThe law Barlow proposed was not in error due to poor measurement; in fact, it fits Barlow's careful measurements quite well. Heinrich Lenz pointed out that Ohm took into account \"all the conducting resistances…of the circuit\" whereas Barlow did not. Ohm explicitly included a term for what we would now call the internal resistance of the battery. Barlow did not have this term and approximated the results with a power law instead. Ohm's law in modern usage is rarely stated with this explicit term but nevertheless an awareness of it is necessary for a full understanding of the current in a circuit.\n"}
{"id": "24882534", "url": "https://en.wikipedia.org/wiki?curid=24882534", "title": "Brian (software)", "text": "Brian (software)\n\nBrian is an open source Python package for developing simulations of networks of spiking neurons.\n\nBrian is aimed at researchers developing models based on networks of spiking neurons. The general design is aimed at maximising flexibility, simplicity and users' development time. Users specify neuron models by giving their differential equations in standard mathematical form as strings, create groups of neurons and connect them via synapses. This is in contrast to the approach taken by many neural simulators in which users select from a predefined set of neuron models.\n\nBrian is written in Python. Computationally, it is based around the concept of code generation: users specify the model in Python but behind the scenes Brian generates, compiles and runs code in one of several languages (including Python, Cython and C++). In addition there is a \"standalone\" mode in which Brian generates an entire C++ source code tree with no dependency on Brian, allowing models to be run on platforms where Python is not available.\n\nThe following code defines, runs and plots a randomly connected network of leaky integrate and fire neurons with exponential inhibitory and excitatory currents.\n\nBrian is primarily, although not solely, aimed at single compartment neuron models. Simulators focused on multi-compartmental models include Neuron, GENESIS, and its derivatives.\n\nThe focus of Brian is on flexibility and ease of use, and only supports simulations running on a single machine. The NEST simulator includes facilities for distributing simulations across a cluster.\n\n\n"}
{"id": "56078342", "url": "https://en.wikipedia.org/wiki?curid=56078342", "title": "Deep hot zone", "text": "Deep hot zone\n\nIn geology, a deep hot zone refers to the process where a number of mafic sills intrude into the \"moho\", the region between the Earth's crust and the mantle.\n"}
{"id": "12371652", "url": "https://en.wikipedia.org/wiki?curid=12371652", "title": "Discoglossus scovazzi", "text": "Discoglossus scovazzi\n\nDiscoglossus scovazzi (common name: Moroccan painted frog, in French \"discoglosse peint\" or \"discoglosse à ventre blanc\") is a species of frog in the Alytidae family. It is found in Morocco as well as in the Spanish North African enclaves Ceuta and Melilla.\n\n\"Discoglossus scovazzi\" lives near streams, cisterns and pools of either fresh or saline water, often in \"Quercus\" forest, \"Nerium oleander\" scrub, or near ruins. It is a relatively common species in Morocco.\n\nIncreasing pressure on agricultural land conversion and from surface water extraction to serve the expanding regional human population is placing downward pressure on this species (as well as other amphibians in the region). However, the species is not considered significantly threatened.\n\n\"Discoglossus scovazzi\" is listed among \"Top 100 EDGE Amphibians\". It is believed to have diverged from its closest relative some 5–10 million years ago.\n"}
{"id": "46599573", "url": "https://en.wikipedia.org/wiki?curid=46599573", "title": "Distant Retrograde Orbit", "text": "Distant Retrograde Orbit\n\nDistant Retrograde Orbit is a highly stable lunar orbit. It was a proposed orbit for the canceled Jupiter Icy Moons Orbiter.\n"}
{"id": "12825875", "url": "https://en.wikipedia.org/wiki?curid=12825875", "title": "Donald Cooksey", "text": "Donald Cooksey\n\nDonald Cooksey (May 15, 1892 – August 19, 1977), was a son of George Cooksey from Birmingham, England and Linda Dows from New York.\n\nAfter High School at the Thacher School in California, Donald Cooksey followed his brother Charlton Cooksey (a physics professor at Yale) and attended Yale and where he too became a physicist specializing in designing and building scientific instruments, especially detectors for measuring sub-atomic particles such as neutrons. When Ernest O. Lawrence was at Yale during the 1920s, Cooksey and Lawrence became friends. In 1932, after Lawrence had moved to Berkeley, California to set up the Radiation Laboratory there, Lawrence asked Cooksey to come to Berkeley to make detectors for use with Lawrence's cyclotrons. Cooksey continued to be a close associate of Lawrence and became associate director of the Lawrence Radiation Laboratory of the University of California at Berkeley.\n\nDonald Cooksey and his wife Milicent Sperry had a son Donald Dows Cooksey (born in 1944) and a daughter Helen Sperry Cooksey(born 1947) who became a surgeon.\n"}
{"id": "25394665", "url": "https://en.wikipedia.org/wiki?curid=25394665", "title": "Eco-costs value ratio", "text": "Eco-costs value ratio\n\nThe EVR model is a life cycle assessment based method to analyse consumption patterns, business strategies and design options in terms of eco-efficient value creation. Next to this it is used to compare products and service systems (e.g. benchmarking).\n\nThe eco-costs/value ratio (EVR) is an indicator to reveal sustainable and unsustainable consumption patterns of people. The eco-costs is an indicator for the environmental pollution of the products people buy, the value is the price they pay for it in our free market economy. Example: When somebody spends 1000 euro per month on housing (in Europe: EVR approx. 0,3) it is less harmful for the environment than when 1000 euro is spend on diesel (in Europe: EVR approx. 1,0). See section 3.1.\n\nThe EVR is also relevant for business strategies, because companies are facing the slow but inevitable internalization of environmental costs. At the moment the costs of products don't take into account the environmental damage caused by these products. This \"pollution is for free\" mentality is less and less accepted by communities.\n\nThe EVR makes companies aware of the relative importance of the environmental pollution of their products, and the relative risk they run that future production costs will increase because of this internalization of environmental costs. \nBy using the EVR, companies can make decisions for their product portfolio: abandon products with low value and high environmental costs and stimulate products with high value and low environmental costs. See sections 2.3 and 3.2.\n\nThe EVR model has been introduced in 1998 and published in 2000–2004 in the International Journal of LCA, and in the Journal of Cleaner Production. The concept of EVR is based on eco-costs. In 2007, 2012 and in 2017, the eco-costs system was updated. General databases of eco-costs are provided (open source) at www.ecocostsvalue.com of Delft University of Technology (the Netherlands). In 2010 a book named \"LCA-based assessment of sustainability: the Eco-costs/Value Ratio (EVR)\" was published containing the most important articles about the EVR.\n\nEVR = Eco-costs/value. The basic idea of the EVR model is to link the 'value chain' to the ecological product chain. In the value chain, the added value (in terms of money) and the added costs are determined for each step of the product 'from cradle to grave'. Similarly, the ecological impact of each step in the product chain is expressed in terms of money, the so-called 'eco-costs'. See Figure 1. Note that there exists also a Porter chain from the right to the left in Figure 1, starting with waste and adding value by recycling. In this way the Porter chain becomes circular.\n\nEco-costs express the amount of environmental burden of a product on basis of prevention of that burden. They are the marginal prevention costs (money) which should be made to reduce the environmental pollution and materials depletion in our world to a level which is in line with the carrying capacity of our earth.\n\nAs such, the eco-costs are virtual costs, since they are not yet integrated in the real life costs of current production chains (Life Cycle Costs). The eco-costs should be regarded as hidden obligations.\nFor example: for each 1000 kg emission, one should invest €135,- in offshore windmill parks (or other reduction systems at that price or less). When this is done consequently, the total emissions in the world will be reduced by 65% compared to the emissions in 2008. As a result global warming will stabilise. In short: \"the eco-costs of 1000kg are € 135,-\".\nSimilar calculations can be made on the environmental burden of acidification, eutrification, summer smog, fine dust, eco-toxicity, and the use of metals, fossil fuels and land (nature).\n\nEco-costs are used in Life Cycle Assessment, LCA, to assess the environmental performance of different materials, processes and End of Life methods.\n\nThe EVR combines eco-cost and value to see whether a product will be successful. The product should have low environmental impact in its lifecycle (low eco-costs) and an attractive value for consumers. The value here is the market value (perceived customer value, also called fair price). Figure 2 depicts the three dimensions of a product: the value, the costs and the eco-costs.\n\nIt is a trend in society that heavy pollution of industry is not accepted anymore by the inhabitants of a country. This results in stricter regulations by countries (e.g. tradable emission rights, enforcement of best available technologies, eco-taxes, etc.). Eco-costs will then become part of the internal production costs. This internalizing of eco-costs might be a threat to a company, but it might also be an opportunity: “When my product has less eco-burden than that of my competitor, my product can withstand stricter regulations of the government. So this characteristic of low eco-costs of my product is a competitive edge.” \nTo analyse the short term and the long term market prospects of a product or a product service combination (Product Service System, PSS), each product or PPS can be positioned in the portfolio matrix of Figure 3. The basic idea of the product portfolio matrix is the notion that a product, service or PSS is characterized by: \n\nIn terms of product strategy, the matrix results in 3 strategic directions:\n\n\nFor many 'green designs', the usual problem is that they have a low current value/costs ratio. In most of the cases the production costs are higher than the production costs of the classic solution, in some cases even the (perceived) quality is poor. There are two ways to do something about it:\n\na. enhance the (perceived) quality of the product\n\nb. attach to the product a service (create a PSS) in a way that the value of the bundle of the product and the service is more than the value of its components.\n\nFor a product which has a good present value/costs ratio, but high eco-costs, the product and the production process have to be redesigned to lower the eco-costs. This road towards sustainability is often far more promising than the strategy of enhancing the value/costs ratio of a green design. The reason is that the economies of scale for production and distribution are available and that the new product is marketed to an existing client base which is used to the brand name, the quality standards, the service system, etc.\n\nNote: The most common fear of business managers is that their new green products end up with a deteriorated value/costs ratio, and hence will have a cumbersome position in the market. The stability of the governmental policy plays an important role here. When governmental regulations which level the playing field are postponed or even abandoned, proactive companies with sound product strategies are harmed. This can cause severe damage to the transition process and may lead to reluctance of players to move proactively in the future.\nThe most successful design options are depicted in Figure 4. The best design strategy is: \n\nIn economics, de-linking (also known as decoupling) is often used in the context of economic production and environmental quality. In this context, it refers to the ability of an economy to grow without corresponding increases in environmental pressure. In many economies increasing production (GDP) would involve increased pressure on the environment. An economy that is able to sustain GDP growth, without also experiencing a worsening of environmental conditions, is said to be de-linked.\n\nThere is a consumer's side of the de-linking of economy and ecology. Under the assumption that most of the households spend in their life what they earn in their life, the total EVR of the spending of households is the key towards sustainability. Only when this total EVR of the spending gets lower, the eco-costs related to the total spending can be reduced even at a higher level of spending. There are two ways of achieving this: \n\nAt the production side, society is heading in the right direction: gradually, industrial production is achieving higher levels of the value/costs ratio and is at the same time becoming cleaner. At the consumer's side, however, society is suffering from the fact that the consumers preferences are heading in the wrong direction: towards products and services with an unfavourable EVR (like driving in SUVs, more kilometres, intercontinental flights for holidays). These unfavourable preferences can be concluded from Figure 5.\n\nFigure 5 shows that people in the Netherlands (and probably in the other EC countries as well) spend relatively more money on cars and holidays when they have more money available. Other studies show that people tend to have intercontinental holidays at the moment they can afford it. This shift in consumer spending will become a big problem in the near future, since the EVR of e.g. housing and health care is much lower than the EVR of transport and (inter)continental holidays by plane. \nFigure 6 shows the EVR (= ecocosts/price) on the Y/axis as a function of the cumulative expenditures of all products and services of all citizens in the EU 25 on the X-axis. The data is from the EIPRO study of the European Commission (EIPRO = environmental impact of products).\n\nThe area underneath the curve is proportional to the total eco-costs of the EU25. Basically there are two strategies to reduce the area under the curve: - ask industry to reduce the eco-costs of their products (this will shift the curve downward) - try to reduce expenditures of consumers in high end of the curve, and let them spend this money at the low end of the curve (this will shift the middle part of the curve to the right). \nThe question is now how designers and engineers can contribute to this required shift towards sustainability and what this means to product portfolio strategies of companies. The solution is Eco-efficient Value Creation.\n\nThe way towards sustainability requires a double aim in product innovation, see Figure 7: \n\nWe call this: eco-efficient value creation. \nThe reason we need value creation for eco-efficient products is threefold:\n\n\nBelow, an example of eco-efficient value creation is given, which is the introduction of the Lexus RX 400h in the USA: \n\nNote that the acceleration of a car is an interesting issue in terms of value. High acceleration is associated with expensive sports cars (Porsche, Ferrari). But people who buy these fast cars hardly use it. For these people acceleration is more part of the image of the product than it is part of the product qualities they use on a daily basis. So reducing the acceleration is the wrong strategy: it eliminates the extra value, and it hardly reduces the overall eco-costs in practice.\n\nLife cycle assessment (LCA) is the generally accepted method to compare two (or more) alternative products or services. A prerequisite for such a comparison is that the functionality ('functional unit') and the quality of the alternatives are the same (you cannot compare apples and oranges in the classical LCA). In cases of product design and architecture, however, this prerequisite seems to be a fundamental flaw in the application of LCA: the designer or architect is aiming at a better quality (in the broad sense of the word: including intangible aspects like beauty and image), so the new design never has the same quality. In some cases the functionality of the design is not the same, since the design solution is limited by a maximum budget, in some cases the functionality is the same, but the higher quality results in a higher price. In all these cases a single indicator in LCA (like the eco-costs) is not suitable for environmental benchmarking.\nIn these cases however, it does make sense to compare the design alternatives on the basis of the eco-costs/value ratio (EVR), where the value is the perceived customer value (the fair price). See section 3.1 on Delinking.\n\nExample 1. Different types of armchairs differ in terms of comfort, aesthetics, etc. rather than in terms of functionality. A classical LCA (with a single indicator like eco-costs, carbon footprint, etc.) does not make sense here. Selection on the basis of EVR, however, is the key to a sustainable consumption pattern. The chair with the lowest EVR is the best solution in terms of sustainability.\n\nExample 2. In LCA, the comparison of a new building and a renovated building is in the majority of cases not possible, since, in practice, both solutions differ in almost all quality aspects (tangible as well as intangible). However, the solution with lowest EVR is the best in terms of sustainable consumption.\n\nNote that the renovated building is the best solution in most of the cases, because it has the lowest EVR in the production phase. However, in some cases the renovated building is not the best solution, because of unfavourable energy consumption (high EVR) in the use phase.\n"}
{"id": "25587463", "url": "https://en.wikipedia.org/wiki?curid=25587463", "title": "Fallout: An American Nuclear Tragedy", "text": "Fallout: An American Nuclear Tragedy\n\nFallout: An American Nuclear Tragedy is a 1989 book by Philip L. Fradkin which was republished in a second edition in 2004. The book is about the radiation exposure of people and their livestock living downwind from the nuclear weapons testing at the Nevada Test Site in the 1950s. The case of \"Irene Allen et al. vs. the United States\" is used as a framework for the narrative. The court case \"resulted in an award of $2.66 million in damages to eight persons with leukemia, one with thyroid cancer, and another with breast cancer\".\n\nPhilip Fradkin is an American environmentalist historian, journalist and author. Fradkin shared a Pulitzer Prize awarded to the metropolitan staff of the \"Los Angeles Times\" for coverage of the Watts riots in 1965.\n\n\n"}
{"id": "34471894", "url": "https://en.wikipedia.org/wiki?curid=34471894", "title": "Felicity Aston", "text": "Felicity Aston\n\nFelicity Ann Dawn Aston is an English explorer and former climate scientist.\n\nOriginally from Birchington-on-Sea, Kent, Aston was educated at University College London (BSc) and Reading University (MSc in applied meteorology).\n\nBetween 2000 and 2003 Felicity Aston was the senior meteorologist at Rothera Research Station located on Adelaide Island off the Antarctic Peninsula operated by the British Antarctic Survey, monitoring climate and ozone. As was usual at the time for British Antarctic Survey staff, she spent three summers and two winters continuously at the station without leaving Antarctica.\n\nIn 2005 she joined a race across Arctic Canada to the 1996 position of the North Magnetic Pole, known as the Polar Challenge. She was part of the first all-female team to complete this race; they came in 6th place out of 16 teams.\n\nIn 2006 Aston was part of the first all-female British expedition across the Greenland ice sheet.\n\nIn 2009 she was the team leader of the Kaspersky Commonwealth Antarctic Expedition, which was a Commonwealth of Nations expedition in which seven women from six Commonwealth member countries skied to the South Pole in 2009 to celebrate the 60th anniversary of the founding of the Commonwealth. \"Call of the White: Taking the world to the South Pole\" is her account of this expedition. It was published by Summersdale in 2011 and was a finalist in the Banff Mountain Book Competition in that year.\n\nIn 2012 she became the first person to ski alone across the Antarctic land-mass using only personal muscle power, as well as the first woman to cross the Antarctic land-mass alone. Her journey began on 25 November 2011, at the Leverett Glacier and continued for 59 days and a distance of 1,084 miles (1,744 kilometres). She had two supply drops. \n\nAston has also walked across the ice of Lake Baikal, the world's deepest and oldest lake, and completed the Marathon des Sables.\n\nShe is an official ambassador for both the British Antarctic Monument Trust and the Equaladventure charity, and was awarded an honorary doctorate by Canterbury Christ Church University for her exploration achievements.\n\nAston was appointed Member of the Order of the British Empire (MBE) and awarded the Polar Medal in the 2015 New Year Honours for services to polar exploration.\n"}
{"id": "12387955", "url": "https://en.wikipedia.org/wiki?curid=12387955", "title": "Fixed expression", "text": "Fixed expression\n\nA fixed expression is a standard form of expression that has taken on a more specific meaning than the expression itself. It is different from a proverb in that it is used as a part of a sentence, and is the standard way of expressing a concept or idea.\n\nExamples include:\n\n"}
{"id": "2635032", "url": "https://en.wikipedia.org/wiki?curid=2635032", "title": "Fyodor Luzhin", "text": "Fyodor Luzhin\n\nFyodor Fyodorovich Luzhin (Russian: \"Федор Федорович Лужин\") (died 1727) was a Russian geodesist and cartographer.\n\nFyodor Luzhin was first a student at the School for Mathematical and Navigational Sciences in Moscow and then in a geodesic class of the Naval Academy in St. Petersburg (until 1718). In 1719–1721, Luzhin took part in drawing a map of Kamchatka and Kuril Islands together with Ivan Yevreinov. In 1723–1724, he made surveys of different parts of East Siberia. In 1725–1727, Luzhin participated in the First Kamchatka Expedition led by Vitus Bering.\n"}
{"id": "14084088", "url": "https://en.wikipedia.org/wiki?curid=14084088", "title": "International Code of Area Nomenclature", "text": "International Code of Area Nomenclature\n\nThe International Code of Area Nomenclature (ICAN) provides a universal naming system or nomenclature for areas of endemism used in biogeography and elsewhere. The ICAN also serves as the international standard rules for proposing and using area names.\n\nThe ICAN was ratified by the Systematic and Evolutionary Biogeographical Association (SEBA) in Paris during July 2007.\n\n"}
{"id": "1937921", "url": "https://en.wikipedia.org/wiki?curid=1937921", "title": "Kongu Nadu", "text": "Kongu Nadu\n\nKongu Nadu is a region and aspirant state of India comprising the western part of the Tamil Nadu. In the ancient Tamilakam, it was the seat of the Chera kings, bounded on the east by Tondai Nadu, on the south-east by Chola Nadu and on the south by Pandya Nadu regions. \n\nThe region was ruled by the Cheras during Sangam period between c. 1st and the 4th centuries CE and it served as the eastern entrance to the Palakkad Gap, the principal trade route between the west coast and Tamil Nadu. The Kosar people mentioned in the second century CE Tamil epic \"Silappathikaram\" and other poems in Sangam literature is associated with the Coimbatore region. The region was located along an ancient Roman trade route that extended from Muziris to Arikamedu.The medieval Cholas conquered the region in the 10th century CE. It came under the rule of the Vijayanagara Empire by the 15th century. After the Vijayanagara Empire fell in the 17th century, the Madurai Nayaks, who were the military governors of the Vijayanagara Empire established their state as an independent kingdom. In the latter part of the 18th century, the region came under the Kingdom of Mysore, following a series of wars with the Madurai Nayak dynasty. After the defeat of Tipu Sultan in the Anglo-Mysore Wars, the British East India Company annexed Kongunadu to the Madras Presidency in 1799. The region was hard hit during the Great Famine of 1876–78 resulting in nearly 200,000 famine related fatalities. The first three decades of the 20th century saw nearly 20,000 plague-related deaths and acute water shortage. The region played a significant role in the Indian independence movement.\n\nKongu Nadu is believed to have come from \"Kongadesam\", \"Konga\" a variant of the term \"Ganga\", meaning \"land of the Gangas\". Kongu may also mean nectar of flowers.\n\nKongu Nadu was one of the earliest territorial divisions and home of the ancient Tamil people. The river Kaveri flows in southeastern direction through the region. Archaeological data from Kodumanal, a village on the banks of the Noyyal River, suggests the beginning of civilization around 4th century BCE. Kodumanal was situated on the ancient trade route between across the Palghat gap in the Western Ghats and yielded remains belonging to the Sangam age. Tamil-Brahmi writings were found on coins, seals and rings obtained from Amaravathi river bed near Karur, the erstwhile capital of the Cheras. A musical inscription in Tamil Brahimi was found in a cave in Arachalur, dating from the 4th Century CE and Iravatham Mahadevan writes that these are syllables used in dance.\n\nThe region was ruled by the Cheras during Sangam period between c. 1st and the 4th centuries CE. The western part of the region was under the Cheras and the eastern regions were ruled by Pandyas. The medieval Cholas conquered most of the region in 10th century CE. After brief period under the Hoysalas and the Delhi sultanate, the region was captured by the Vijayanagara Empire in the 15th century. In the 1550s, Madurai Nayaks, who were the military governors of the Vijayanagara Empire, took control of the region. After the Vijayanagara Empire fell in the 17th century, the Nayaks established their state as an independent kingdom and they introduced the Palayakkarar system. In the latter part of the 18th century, the region came under the Kingdom of Mysore, following a series of wars with the Madurai Nayak dynasty. After the defeat of Tipu Sultan in the Anglo-Mysore Wars, the British East India Company annexed the region to the Madras Presidency in 1799. The region played a prominent role in the Second Poligar War (1801), when it was the area of operations of Dheeran Chinnamalai.Dheeran Chinnamalai was one of the freedom fighters who fought against the rule of British East India Company \n\nKongunadu comprises the modern day districts of Coimbatore district, Nilgiris district, Tirupur district, Erode district, Namakkal district, Karur district, Salem district, Dharmapuri district, Krishnagiri district and some parts of Dindigul district in the South Indian state of Tamil Nadu and also parts of South-western India including parts of Palakkad District in the Kerala state and parts of Chamarajanagar District in Karnataka state. The Western Ghats mountain range passes through the region with major rives Kaveri, Bhavani, Amravati and Noyyal flowing through the region. Palghat Gap, a mountain pass connects the neighbouring state of Kerala to the region. The Eastern Ghats mountain range, which consists of the hills Kollimalai of Namakkal district, Shevaroy (Shervarayan) and Mettur Hills of Salem district and Palamalai of Coimbatore district, also passes through the region. The Biligiriranga Hills of Chamarajanagar District is located at the confluence of Eastern and Western Ghats belongs to the region. The Kaveri river flows into Tamil Nadu from Karnataka through Dharmapuri, Salem, Erode, Namakkal and Karur districts.\n\nThe table below lists geographic and demographic parameters for districts that constitute the 'Kongu region' of Tamil Nadu.\n\"Kongu Tamil\" is the dialect of Tamil language that is spoken in Kongu Nadu, which is the western region of Tamil Nadu. It is originally known as \"Kangee\"` or \"Kongalam\" or \"Kongappechu\". \n\nKongu Nadu had a flourishing economy from ancient times and had trade contacts with foreign nations. Kodumanal was a 2,500-year-old industrial colony discovered by archaeologists. The region was located along an ancient Roman trade route that extended from Muziris to Arikamedu. A Chola highway called \"Rajakesari Peruvazhi\" ran through the region.\n\nKongu Nadu is amongst the most industrialised regions in the country. Agriculture and textile industries contribute majorly to the economy of the region. It is one of the major producers of textiles including cotton, apparels and knit wear, and hosieries, agricultural and allied products including milk, poultry, turmeric, sugar-cane, rice, white silk, coconut and plantain, industrial products including paper, auto parts, water pumps, wet grinders, jewellery, aluminium and steel and IT services in Tamil Nadu. Kovai Cora cotton sarees, Coimbatore Wet Grinders, Salem silk sarees, Bhavani Jamakkalam, Toda Embroidery and Nilgiri tea are recognized geographical Indications from the region. Coimbatore along with Tiruppur is called the \"Manchester of South India\" due to its extensive textile industry, fed by the surrounding cotton fields. With 43% share, Erode district is the top turmeric producer in Tamil Nadu. Namakkal district is the largest poultry production and export hub in Tamil Nadu. Salem district has one of the largest magnesite, bauxite and iron ore deposits in India. Salem and Namakkal districts are among a few regions in Asia where tapioca ( \"maravallikilangu\" ) productivity is high. TNPL Paper Plant in Pugalur of Karur district is the one of the largest producers of bagasse based paper in the world. Salem, Dharmapuri and Krishnagiri districts are leading producers of mango. \n\nKongu Nadu cuisine is predominantly south Indian with rice as its base and a collection of exotic recipes being created by the people residing in the Kongu region.\n\nAs it is also native to an arid area, the cuisine includes cereals like jowar (cholam), bajra (kambu), ragi (kezhvaragu), and different kinds of pulses and sesame. Food is served over a banana leaf. Eating on a banana leaf is an old custom and imparts a unique flavor to the food and is considered healthy. Idly, dosa, paniyaram and appam are popular dishes. Kongu Nadu cuisine does not involve marination of any raw material and as a result the food has a different taste and unique texture. People of this region does not add salt to cook rice. This is different to the culinary methods in the rest of Tamil Nadu.Turmeric is added into curries which gives the product a deep yellow colour and an aromatic substance.The traditional Kongu people were mostly vegetarians for religious reason. Opputtu is a variant of Puran poli made with rice, chickpea, palm or cane jaggery, cardamom and ghee.\n\nThere have been numerous claims that Kongu Nadu region has often been ignored by successive governments in spite of being the largest contributor to the state's economy. The entire region comprising 10 districts accounts for more than 40 percent of the revenue. There are demands for the creation of separate state of Kongu Nadu, comprising the regions of western districts of Tamil Nadu. A number of political outfits namely Kongunadu Makkal Desia Katchi, Kongunadu Munnetra Kazhagam, Kongu Vellala Goundergal Peravai, Tamil Nadu Kongu Ilaignar Peravai, Kongu Desa Makkal Katchi are active in the region claiming to fight for the rights of the region.\n"}
{"id": "3925832", "url": "https://en.wikipedia.org/wiki?curid=3925832", "title": "List of Macintosh models by case type", "text": "List of Macintosh models by case type\n\nThis list of Macintosh models by case type contains all case designs used by Apple Inc. for its Macintosh computers. The list is sectioned by general case layout, but inside the sections the order is chronological. Models that used multiple names (like most Performas) are listed only once. Where available, the \"form factor\" from Apple's datasheets has been used to determine the case designation and the computers that used it. For all-in-one models and notebooks that have the same basic case design, but differ in size, just one type is listed. Also, some models that differ only slightly or internally are listed together, like the two iMac G3s and the Power Mac G4s.\n\n\"All-in-one\" in the Macintosh sense means that the display is integrated in the computer case. The keyboard and mouse always remained detachable.\n\nThe Macintosh II series were the first Macintoshes designed to be placed under the monitor. Apple's most recent desktop offering, the Mac mini, is meant to be set alongside the monitor.\n\n\"Portable\" in this case means computers that are able to run on internal batteries - modern computers like the Mac mini are arguably more portable than the \"luggable\" Macintosh Portable, but have no integrated batteries. All modern portable Macintoshes use the notebook design, which the PowerBook series helped to establish.\n\n\"Rackmount\" computers use cases that are designed to be screwed into a standard 19-inch rack, a form factor that is mostly used for servers. The Xserve series is the only Macintosh ever specifically designed in this way, with all of the computer models having a height of only one rack unit.\n\n\n"}
{"id": "323428", "url": "https://en.wikipedia.org/wiki?curid=323428", "title": "List of craters on the Moon", "text": "List of craters on the Moon\n\nThis is a list of named lunar craters. The large majority of these features are impact craters. The crater nomenclature is governed by the International Astronomical Union, and this listing only includes features that are officially recognized by that scientific society.\n\nThe lunar craters are listed in the following subsections. Where a formation has associated satellite craters, these are detailed on the main crater description pages.\n\nLunar craters are listed alphabetically on the following partial lists:\n\nLocations and diameters of some prominent craters on the near side of the Moon:\n\n\nThe following sources were used as references on the individual crater pages.\n\nThe following reference sites were also used during the assembly of the crater information.\n"}
{"id": "14073506", "url": "https://en.wikipedia.org/wiki?curid=14073506", "title": "List of films about animals", "text": "List of films about animals\n\nThis is a list of notable films that are primarily about and/or feature animals. While films involving dinosaurs and other prehistoric animals are included on this list, those concerning mythical creatures, such as dragons or vampires, are not; however, films concerning anthropomorphized animals (such as Scooby-Doo), gigantized animals (such as King Kong), mutated forms of real animals (such as \"Anaconda\"), or fictional hybrids of real animals (such as \"Sharktopus\") are considered to be films about animals, and are thus featured on this list.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "56855846", "url": "https://en.wikipedia.org/wiki?curid=56855846", "title": "List of human transcription factors", "text": "List of human transcription factors\n\nThis list of manually curated human transcription factors is taken from Lambert, Jolma, Campitelli et al.\nIt was assembled by manual curation.\nMore detailed information is found in the manuscript and the web site accompanying the paper (Human Transcription Factors)\n"}
{"id": "1036107", "url": "https://en.wikipedia.org/wiki?curid=1036107", "title": "List of lakes, reservoirs, and dams in Kansas", "text": "List of lakes, reservoirs, and dams in Kansas\n\nThis is a listing of lakes, reservoirs, and dams located in the State of Kansas.\n\nThe shorelines of Kansas Lakes are mostly in government ownership and open to the public for hunting, fishing, camping, and hiking. Large areas of public land surround most of the lakes. \n\nSources: Army Corps of Engineers, Bureau of Reclamation, Kansas State Parks. Copan, Hulah, and Kaw lakes extend into Kansas but are mostly in Oklahoma.\n\n\n\n\n\n"}
{"id": "3716409", "url": "https://en.wikipedia.org/wiki?curid=3716409", "title": "List of streaming media systems", "text": "List of streaming media systems\n\nThis is a list of streaming media systems with articles. A more detailed comparison of streaming media systems is also available.\n\n\nPeer-to-peer video streaming solutions:\n\n\n"}
{"id": "36906256", "url": "https://en.wikipedia.org/wiki?curid=36906256", "title": "List of target antigens in pemphigus", "text": "List of target antigens in pemphigus\n\nCirculating auto-antibodies in the human body can target normal parts of the skin leading to disease. This is a list of antigens in the skin that may become targets of circulating auto-antibodies leading to the various types of pemphigus.\n\n"}
{"id": "748074", "url": "https://en.wikipedia.org/wiki?curid=748074", "title": "Little Joe 1", "text": "Little Joe 1\n\nLittle Joe 1 (LJ-1) was a failed launch of a Little Joe solid fuel rocket that was designed to test the Mercury spacecraft Launch Escape and Recovery systems. The vehicle was in height, weighed approximately , and was in diameter with a fin span of . The Little Joe booster consisted of four Pollux and four Recruit clustered, solid-fuel rockets, could develop a thrust of , and could lift a maximum payload of . The escape system, using a Grand Central 1KS52000 rocket motor, weighed .\n\nOn August 21, 1959, LJ-1 was being prepared for launch from the Wallops Flight Facility, Wallops Island, Virginia. Suddenly, about a half-hour before the scheduled launch, the escape rocket fired and pulled the Mercury spacecraft away from the launch pad. The spacecraft reached an apogee of and landed about 2000 feet away. The flight time was 20 seconds.\n\nAccording to the Sept. 18, 1959 accident report, the unexpected triggering of the launch escape system was caused by a transient or electrical leak; analysis showed it to be due to the rapid-abort system being wired directly into the destruct arming busbar. The batteries were shipped from England to the U.S. uncharged and shorted; on charging at the pad, the batteries, when enough charge was reached, actuated the sequencer for the abort system, and sensing insufficient altitude, fired the squibs in the abort motor. Insufficient power in the batteries then failed to initiate the tower jettison motor and capsule parachute recovery charge, and both crashed into the sea.\n\n"}
{"id": "40767189", "url": "https://en.wikipedia.org/wiki?curid=40767189", "title": "Margaret Buckingham", "text": "Margaret Buckingham\n\nMargaret Buckingham, , is a developmental biologist working in the fields of myogenesis and cardiogenesis. She is a professor emeritus at the Pasteur Institute in Paris and exceptional grade senior researcher emeritus at the Centre national de la recherche scientifique. She is a member of the European Molecular Biology Organization, the Academia Europaea and the French Academy of Sciences.\n\nAfter graduating from Oxford University, where her thesis was on histone modifications, she joined F. Gros's laboratory at the Pasteur Institute to work on mRNA regulation during skeletal muscle differentiation.\n\n"}
{"id": "967488", "url": "https://en.wikipedia.org/wiki?curid=967488", "title": "Messier 77", "text": "Messier 77\n\nMessier 77 (also known as NGC 1068) is a barred spiral galaxy about 47 million light-years away in the constellation Cetus. Messier 77 is an active galaxy with an Active Galactic Nucleus (AGN), which is obscured from view by astronomical dust at visible wavelengths. The diameter of the molecular disk and hot plasma associated with the obscuring material was first measured at radio wavelengths by the VLBA and VLA. The hot dust around the nucleus was subsequently measured in the mid-infrared by the MIDI instrument at the VLTI. It is the brightest Seyfert galaxy and is of type 2.\n\nMessier 77's diameter is estimated at 170,000 light-years.\n\nMessier 77 was discovered by Pierre Méchain in 1780, who originally described it as a nebula. Méchain then communicated his discovery to Charles Messier, who subsequently listed the object in his catalog. Both Messier and William Herschel described this galaxy as a star cluster. Today, however, the object is known to be a galaxy.\n\nX-ray source 1H 0244+001 in Cetus has been identified as Messier 77 (NGC 1068, M77).\n\nOnly one supernova has been detected in Messier 77. The supernova, named SN 2018 ivc, was discovered on 24 November 2018 by the DLT40 Survey. It is a type II supernova, and at discovery it was 15th magnitude and brightening.\n\n"}
{"id": "10985354", "url": "https://en.wikipedia.org/wiki?curid=10985354", "title": "Mood management theory", "text": "Mood management theory\n\nMood management theory posits that the consumption of messages, particularly entertaining messages, is capable of altering prevailing mood states, and that the selection of specific messages for consumption often serves the regulation of mood states (Zillmann, 1988a).\n\nThe idea of selecting media content in the interest of enhancing one's states has been proposed by Zillmann and Bryant (1985) and Zillmann (1988a). Initially, the assumptions were referred to as theory of affect-dependent stimulus arrangement, but subsequently gained more prominence under the label of mood management (Knobloch, 2006).\n\nMood management research can be traced back to Leon Festinger's (1957) cognitive dissonance theory. Festinger notes that the human organism tries to establish internal congruity among cognitions such as attitudes, beliefs, and knowledge about oneself and the environment. When a person holds two cognitions that are incompatible, dissonance is produced. But such dissonance can be reduced through selective exposure, that is, individuals will seek out information that will reduce the dissonance and avoid information that will increase the already existing dissonance.\n\nFestinger's theory was primarily laid out in cognitive terms, addressing exposure choices to persuasive messages. Zillmann and his colleagues thus proposed the mood management theory that attempts to cope with the broadest possible range of message choices such as news, documents, comedies, dramas, tragedies, music performances, and sports. It deals with all conceivable moods rather than a single, specific affective state, such as dissonance (Zillman, 1988b).\n\nBased on the hedonistic premise that individuals are motivated for pleasure and against pain, mood management theory states that, to the extent possible, individuals tend to arrange their environment so that good mood (commonly pleasure) is maximized or maintained, and bad mood (commonly pain) is diminished or alleviated. Environmental arrangement can take many forms, including psychically moving away from or avoiding situations that creates negative effect (such as avoiding a stressful traffic jam), or moving toward or selecting situations that result in gratification (such as strolling in a beautiful garden). Moreover, since entertainment provides its audience with the opportunity to symbolically arrange the environment, mood management theory states that people's entertainment choices should similarly serve the management of moods (Oliver, 2003).\n\nThe specific hypotheses of mood management theory have been summarized as follows by Zillmann (2000):\n\nThe indicated hedonistic objective is best served by selective exposure to material that (a) is excitationally opposite to prevailing states associated with noxiously experienced hypo- or hyperarousal, (b) has positive hedonic value above that of prevailing states, and (c) in hedonically negative states, has little or no semantic affinity with the prevailing states.\nAlthough mood management suggests that individuals' behaviors often conform to the hedonistic assumption, this theory also makes clear that individuals are not necessarily aware of their motivation. Rather, people are thought to initially arrange their environments in a random fashion, and arrangements that are incidentally made during good moods and that extend or enhance the hedonically positive state leave a memory trace that increases the likelihood for making similar stimulus arrangements under similar circumstances (Zillmann, 1988a, 1988b). In other words, the formation of these preferences is controlled by a mechanism called operant conditioning, which refers to the use of consequences to modify the occurrence and form of behavior.\n\nAlthough its principles relate to the broader realm of mood optimization, mood management theory has largely been applied to entertainment choices. Focusing on viewers' selection of television entertainment, for example, an experimental study by Bryant and Zillmann (1984) reveals that individuals can overcome boredom or stress through selective exposure to exciting or relaxing television programming respectively. In the context of music exposure, Knobloch and Zillmann (2002) demonstrate that individuals could improve negative moods by electing to listen to highly energetic-joyful music. Also, Wakshlag et al. (1983) reported that participants with increased fear levels preferred films with low victimization scores and with high justice scores. This demonstrates that individuals aim to minimize stimuli which are associated with the source of a negative mood.\n\nThe theoretical proposition of mood management theory has been faced with challenges, especially when studying (1) the role that negative moods and burdening feelings play within the entertainment experience; (2) the diversity of individual users, social and cultural situations, and media products on offer, and (3) the new, so-called interactive media and how entertainment can best be conceptualized within them (Vorderer, 2003).\n\n\n"}
{"id": "650327", "url": "https://en.wikipedia.org/wiki?curid=650327", "title": "Morning", "text": "Morning\n\nMorning is the period of time between midnight and noon, especially from sunrise to noon. There are no exact times for when morning begins and ends (also true for evening and night) because they can vary according to one's lifestyle and the hours of daylight at each time of year. However, morning strictly ends at noon, which is when afternoon starts. Morning can be defined as starting from midnight to noon, or from sunrise (which varies throughout the year) to noon, or from when one wakes up to noon.\n\nMorning precedes afternoon, evening, and night in the sequence of a day. Originally, the term referred to sunrise.\n\nThe Modern English words \"morning\" and \"tomorrow\" began in Middle English as \"morwening,\" developing into \"morwen,\" then \"morwe,\" and eventually \"morrow\". English, unlike some other languages, has separate terms for \"morning\" and \"tomorrow\", despite their common root. Other languages, like German, may use a single word – \"Morgen\" – to signify both \"morning\" and \"tomorrow\".\n\nAccording to Max Weber (\"General Economic History\" pp23), the German word \"Morgen\" also takes on another meaning, specifically, the size of land strip \"which an ox could plow in a day without giving out\".\nAs such, a \"good morning\" could mean \"a good day's plow\".\n\nSome languages that use the time of day in greeting have a special greeting for morning, such as the English good morning. The appropriate time to use such greetings, such as whether it may be used between midnight and dawn, depends on the culture's or speaker's concept of morning. Many people greet someone with the shortened 'morning' rather that 'good morning'. It is usually always used a greeting, never a farewell, unlike 'good night' which is used as a farewell. \n\nMorning typically encompasses the (mostly menial) prerequisites for full productivity and life in public, such as bathing, eating a meal such as breakfast, dressing, and so on. It may also include information activities, such as planning the day's schedule or reading a morning newspaper. The boundaries of such morning periods are by necessity idiosyncratic, but they are typically considered to have ended on reaching a state of full readiness for the day's productive activity. For some, the word \"morning\" may refer to the period immediately following waking up, irrespective of the current time of day. This modern sense of \"morning\" is due largely to the worldwide spread of electricity, and the concomitant independence from natural light sources.\n\nThe morning period may be a period of enhanced or reduced energy and productivity. The ability of a person to wake up effectively in the morning may be influenced by a gene called \"Period 3\". This gene comes in two forms, a \"long\" and a \"short\" variant. It seems to affect the person's preference for mornings or evenings. People who carry the long variant were over-represented as morning people, while the ones carrying the short variant were evening preference people.\n\nGood Morning Wishes Pictures\n"}
{"id": "37052063", "url": "https://en.wikipedia.org/wiki?curid=37052063", "title": "Moving horizon estimation", "text": "Moving horizon estimation\n\nMoving horizon estimation (MHE) is an optimization approach that uses a series of measurements observed over time, containing noise (random variations) and other inaccuracies, and produces estimates of unknown variables or parameters. Unlike deterministic approaches, MHE requires an iterative approach that relies on linear programming or nonlinear programming solvers to find a solution.\n\nMHE reduces to the Kalman filter under certain simplifying conditions. A critical evaluation of the extended Kalman filter and MHE found improved performance of MHE with the only cost of improvement being the increased computational expense. Because of the computational expense, MHE has generally been applied to systems where there are greater computational resources and moderate to slow system dynamics. However, in the literature there are some methods to accelerate this method.\n\nThe application of MHE is generally to estimate measured or unmeasured states of dynamical systems. Initial conditions and parameters within a model are adjusted by MHE to align measured and predicted values. MHE is based on a finite horizon optimization of a process model and measurements. At time the current process state is sampled and a minimizing strategy is computed (via a numerical minimization algorithm) for a relatively short time horizon in the past: formula_1. Specifically, an online or on-the-fly calculation is used to explore state trajectories that find (via the solution of Euler–Lagrange equations) a objective-minimizing strategy until time formula_2. Only the last step of the estimation strategy is used, then the process state is sampled again and the calculations are repeated starting from the time-shifted states, yielding a new state path and predicted parameters. The estimation horizon keeps being shifted forward and for this reason the technique is called moving horizon estimation. Although this approach is not optimal, in practice it has given very good results when compared with the Kalman filter and other estimation strategies.\n\nMoving horizon estimation (MHE) is a multivariable estimation algorithm that uses:\nto calculate the optimum states and parameters.\n\nThe optimization estimation function is given by:\n\nformula_3\n\nwithout violating state or parameter constraints (low/high limits)\n\nWith:\n\nformula_4 = \"i\" -th model predicted variable (e.g. predicted temperature)\n\nformula_5 = \"i\" -th measured variable (e.g. measured temperature)\n\nformula_6 = \"i\" -th estimated parameter (e.g. heat transfer coefficient)\n\nformula_7 = weighting coefficient reflecting the relative importance of measured values formula_5\n\nformula_9 = weighting coefficient reflecting the relative importance of prior model predictions formula_10\n\nformula_11 = weighting coefficient penalizing relative big changes in formula_6\n\nMoving horizon estimation uses a sliding time window. At each sampling time the window moves one step forward. It estimates the states in the window by analyzing the measured output sequence and uses the last estimated state out of the window, as the prior knowledge.\n\n\n\n\n"}
{"id": "55334434", "url": "https://en.wikipedia.org/wiki?curid=55334434", "title": "National Longitudinal Study of Adolescent to Adult Health", "text": "National Longitudinal Study of Adolescent to Adult Health\n\nThe National Longitudinal Study of Adolescent to Adult Health, also known as Add Health, is a multiwave longitudinal study of adolescents in the United States. It was begun in 1994 in response to a Congressional mandate to study adolescent health, and was initially called the National Longitudinal Study of Adolescent Health. The first wave of the study, funded by the Eunice Kennedy Shriver National Institute of Child Health and Human Development, involved administering a questionnaire to a nationally representative sample of 7th- through 12th-graders during the 1994-95 school year. In the first wave of the study, the questionnaire was administered to about 20,000 adolescents, making it the largest longitudinal survey of adolescents ever conducted. The participants were then re-interviewed in 1996 (wave II), 2001–02 (wave III), and 2008 (wave IV), with a fifth wave of data collection underway since 2016. The first three waves included, among other information, detailed and sensitive interviews, and the 11,500 participants in wave III also provided urine and saliva samples.\nThe purpose of Waves I and II was to examine factors associated with health behaviors among adolescents. The purpose of Wave III, which was conducted when almost all participants were aged 18 to 26, was to determine the relationship between behaviors and experiences during adolescence and behaviors during the adjustment to young adulthood. The purpose of Wave IV, conducted when almost all participants were aged 24 to 32, was to assess developmental and health trajectories over the lifespan.\n"}
{"id": "1835511", "url": "https://en.wikipedia.org/wiki?curid=1835511", "title": "Oceanographic Museum of Monaco", "text": "Oceanographic Museum of Monaco\n\nThe Oceanographic Museum (\"Musée océanographique\") is a museum of marine sciences in Monaco-Ville, Monaco.\nIt is home to the Mediterranean Science Commission. This building is part of the Oceanographic Institute which is committed to sharing its knowledge of the oceans.\n\nThe Oceanographic Museum was inaugurated in 1910 by Monaco's modernist reformer, Prince Albert I.\nJacques-Yves Cousteau was director from 1957 to 1988. The Museum celebrated its centenary in March 2010, after extensive renovations.\n\nThis monumental example of highly charged Baroque Revival architecture has an impressive façade above the sea, towering over the sheer cliff face to a height of 279 feet (85.04 m). It took 11 years to build, using 100,000 tons of stone from La Turbie.\n\nThe museum is home to exhibitions and collections of various species of sea fauna (starfish, seahorses, turtles, jellyfish, crabs, lobsters, rays, sharks, sea urchins, sea cucumbers, eels, cuttlefish etc.). The museum's holdings also include a great variety of sea related objects, including model ships, sea animal skeletons, tools, weapons etc., as well as a collection of material culture and ritual objects made from, or integrating materials such as pearls, molluscs and nacre.\n\nAt the first floor, \"A Sailor’s Career\" showcases the work of Prince Albert I. It includes the laboratory from \"L’Hirondelle\", the first of Prince Albert's research yachts. Observations made there led to an understanding of the phenomenon of anaphylaxis, for which Dr Charles Richet received the Nobel Prize in Physiology or Medicine in 1913.\n\nAn aquarium in the basement of the museum presents a wide array of flora and fauna. Four thousand species of fish and over 200 families of invertebrates can be seen. The aquarium also features a presentation of Mediterranean and tropical marine ecosystems.\n\nNumerous artists displayed their artworks in the museum, such as Damien Hirst and .\n\nIn 1989, French marine biologist Alexandre Meinesz discovered a patch of a giant, tropical seaweed \"Caulerpa taxifolia\" directly under the walls of the museum. Several documentaries point to this patch as the origin of one of the largest seaweed contaminations of the Mediterranean Sea in recent history. The role of the museum and its director, François Doumenge when the discovery was made public is still heavily debated.\n\n"}
{"id": "30871775", "url": "https://en.wikipedia.org/wiki?curid=30871775", "title": "Open Access Scholarly Publishers Association", "text": "Open Access Scholarly Publishers Association\n\nThe Open Access Scholarly Publishers Association (OASPA) is a non-profit trade association representing the interests of open access journal publishers globally in all scientific, technical and scholarly disciplines. Along with promoting open access publishers (particularly open access journals), OASPA sets best practices and provides a forum for the exchange of information on and experiences of open access. OASPA brings together the major open access publishers on the one hand and independent—often society-based or university-based—publishers on the other, along with some hybrid open access publishers. While having started out with an exclusive focus on open access journals, it is now expanding its activities to include matters pertaining to open access books too.\n\nThe mission of OASPA is to support and represent the interests of open access publishers globally in all scientific, technical, and scholarly disciplines, and to advocate for Open Access journals in general. To this end, it provides a forum for professional exchange on matters of open access publishing in scholarly contexts, it engages in standardization efforts and outreach, identifies and promotes best practices for scholarly communications by open access, and supports the continuous development of viable business and publishing models.\n\nWith the growth of the open access movement, the interactions between different open access publishers intensified, as they met each other at a multitude of trade or scientific conferences, workshops or similar events. Yet open access publishing and its peculiarities with respect to traditional publishing or scholarly communication were rarely in the focus of such gatherings, which brought about the need for a dedicated forum. With the intention to provide that, OASPA was launched on October 14, 2008 at an \"Open Access Day\" celebration in London hosted by the Wellcome Trust.\n\nThe following organizations are founding members:\n\nOASPA organizes an annual Conference on Open Access Scholarly Publishing. The conference covers the whole spectrum of open access publishing, including business models, publishing platforms, peer review modes, and distribution channels.\n\nOASPA encourages publishers to use Creative Commons licenses, particularly the Creative Commons Attribution License (CC-BY), which is in line with most definitions of \"open\", e.g. the Open Definition by the Open Knowledge Foundation. The organization also engages beyond Open Access journals, e.g. for free access to scholarly works that have been awarded Nobel Prizes.\n\nOASPA members fall into the following groups:\n\nProfessional publishing organisations – Organisations that include at least one full-time professional who manages the publication of OA scholarly journals or books. These organisations may be for-profit or nonprofit, and they may own journals or books or manage the publication on a contract basis for societies or other groups of scientists or scholars. Members of this class may also include organisations such as academic/research libraries, university presses, or other organisations in which the primary focus is other than publishing scholarly journals but still employ full-time professionals who manage the publication of OA scholarly journals.\n\nScholar publishers – Individuals or small groups of scientists/scholars that publish usually a single scholarly journal in their field of study. The publication process is often largely subsidised by volunteer effort.\n\nOther organisations – Other organisations who provide significant services and/or support for OA publishing.\n\nIn order to join OASPA as a member organization, a publisher must meet set criteria established to promote transparency and best practices in scholarly publishing. These criteria were set in 2013 and revised again in August 2018. There are seven categories of OASPA membership:\n\n\nAs of September 2018, OASPA has 134 members.\n\nCriticism has focused on OASPA's self-declared role as the \"stamp of quality for open access publishing\", because it is apparently at odds with OASPA's application of its own criteria for membership. Another voiced concern is the fact that OASPA has been founded by BioMed Central and other open access publishers, which would cause a conflict of interest in their \"seal of approval\". OASPA has also been criticized for promoting gold open access in a way that may be at the expense of green open access. One member organization, Frontiers Media, is included on Jeffrey Beall's list of predatory open access publishing companies; at least two members, Hindawi and MDPI, were once called predatory by Beall, but have since been removed from his list.\n\nAs a response to the \"Who's Afraid of Peer Review?\" investigation, OASPA formed a committee to investigate the circumstances that led to the acceptance of the fake paper by 3 of its members. On 11 November 2013, OASPA terminated the membership of two publishers (Dove Medical Press and Hikari Ltd.) who accepted the fake paper. Sage Press, which also accepted a fake paper, was put \"under review\" for 6 months. Sage announced in a statement that it was reviewing the journal that accepted the fake paper, but that it would not shut it down. Sage's membership was reinstated at the end of the review period following changes to the journal's editorial processes. Dove Medical Press were also reinstated in September 2015 after making a number of improvements to their editorial processes.\n\n\n"}
{"id": "6395328", "url": "https://en.wikipedia.org/wiki?curid=6395328", "title": "Prebiotic score", "text": "Prebiotic score\n\nA Prebiotic Score, also known as Prebiotic Activity Score, is a term sometimes used to estimate the health effects of prebiotics in humans or animals. The idea is that prebiotics may have many different effects in the human gut, some of these may be quantified and combined to an overall score. For example, an increase in the populations of bifidobacteria or lactobacilli coincides with a relative increase in prebiotic activity; accordingly, an increase in enteric bacteria strains such as \"Clostridium perfringens\" result in a decrease of prebiotic activity. Also, increases and reductions of certain enzymes may be used as factors in a prebiotic score.\n\nMeasure of the prebiotic effect (MPE) is a quantitative analysis that takes into an account a number of dominant bacterial groups and end products of fermentation such as short-chain fatty acids (SCFA) and substrate assimilation. MPE was developed by Jelena Vulevic in conjunction with Glenn R Gibson and Robert Rastall and sponsored by Novartis Consumer Health \n\nSeveral quantitative approaches have been developed to aid in the analysis of prebiotics and their individual and collective activity within the gut Microbiome. These approaches are in part used to calculate the Measure of the Prebiotic Effect or (MPE).\n\nOne of the first quantitative approaches, the Prebiotic index or (PI), is a quantitative tool used to compare the prebiotic effect of dietary oligosaccharides. The Prebiotic Index equation takes into account bifidobacteria (Bif), bacteroides (Bac), lactobacilli (Lac), and clostridia (Clos): \n<br>\n\"PI = (Bif/Total) - (Bac/Total) + (Lac/Total) - (Clos/Total)\"\n\nThe rate of assimilation is the measure of the substrate assimilation calculated by measuring the substrate concentration over time:\n<br>\nS = substrate concentration after the time interval, , e.g. in hours; S = initial substrate concentration and A = rate of substrate assimilation, e.g. per hour, e.g. during the exponential phase of bacterial population growth.\n<br>\nRate of Assimilation: S = S - At\n\nRate of growth is determined by an equation that is based on the rate of growth for the bacterial populations. \n<br>\nRate of growth uses the following equation:\n<br>\nln \"N\" = ln \"N\" + µ\nPIm = μmaxBif + μmaxLac + μmaxEub - μmaxBac - μmaxClos - μmaxEC - μmaxSRB\n\nTSCFA = A + B + P + L\n\nwhere A is acetate, B is butyrate, P is propionate and L is lactate\n\nRatio = dL/dTSCFA\n\nMPE=0.5(XY+YZ+ZX) \n\n\n"}
{"id": "12404241", "url": "https://en.wikipedia.org/wiki?curid=12404241", "title": "Pseudoeurycea nigromaculata", "text": "Pseudoeurycea nigromaculata\n\nPseudoeurycea nigromaculata (common names: black-spotted salamander, black-spotted false brook salamander) is a species of salamander in the family Plethodontidae. It is endemic to Veracruz, Mexico, and known from Cerro Chicahuaxtla ( asl)) in Cuatlalpan (the type locality, near Fortín de las Flores) and from Volcán San Martín at elevations of . These separate populations likely represent distinct species.\n\n\"Pseudoeurycea nigromaculata\" is a medium-sized plethodontid: females in the type series (collected by Hobart Muir Smith) measure in snout–vent length. The tail is longer than the snout–vent length, giving a maximum total length of about . The body is blackish (lighter in younger specimens), the tail has lighter coloration, and both carry black spots that have given the species its name.\n\nTwo observed egg clutches contained 19 and 25 eggs.\n\n\"Pseudoeurycea nigromaculata\" is an arboreal species living in bromeliads in cloud forest. Once relatively common, it now appears to be very rare. Most of its habitat has disappeared or is severely degraded. It is protected by Mexican law under the \"Special Protection\" category.\n"}
{"id": "14032539", "url": "https://en.wikipedia.org/wiki?curid=14032539", "title": "Recombinase-mediated cassette exchange", "text": "Recombinase-mediated cassette exchange\n\nRMCE (recombinase-mediated cassette exchange) is a procedure in reverse genetics allowing the systematic, repeated modification of higher eukaryotic genomes by targeted integration, based on the features of site-specific recombination processes (SSRs). For RMCE, this is achieved by the clean exchange of a preexisting gene cassette for an analogous cassette carrying the \"gene of interest\" (GOI).\n\nThe genetic modification of mammalian cells is a standard procedure for the production of correctly modified proteins with pharmaceutical relevance. To be successful, the transfer and expression of the transgene has to be highly efficient and should have a largely predictable outcome. Current developments in the field of gene therapy are based on the same principles. Traditional procedures used for transfer of GOIs are not sufficiently reliable, mostly because the relevant epigenetic influences have not been sufficiently explored: transgenes integrate into chromosomes with low efficiency and at \"loci\" that provide only sub-optimal conditions for their expression. As a consequence the newly introduced information may not be realized (expressed), the gene(s) may be lost and/or re-insert and they may render the target cells in unstable state. It is exactly this point where RMCE enters the field. The procedure was introduced in 1994 and it uses the tools yeasts and bacteriophages have evolved for the efficient replication of important genetic information:\n\nMost yeast strains contain circular, plasmid-like DNAs called \"two-micron circles\". The persistence of these entities is granted by a recombinase called \"flippase\" or \"Flp\". Four monomers of this enzyme associate with two identical short (48 bp) target sites, called \"FRT\" (\"flip-recombinase targets\"), resulting in their crossover. The outcome of such a process depends on the relative orientation of the participating FRTs leading to\n\n\nThis spectrum of options could be extended significantly by the generation of spacer mutants for extended 48 bp \"FRT\" sites (cross-hatched half-arrows in Figure 1). Each mutant Fn recombines with an identical mutant Fn with an efficiency equal to the wildtype sites (F x F). A cross-interaction (F x Fn) is strictly prevented by the particular design of these components. This sets the stage for the situation depicted in Figure 1A:\n\n\nFirst applied for the Tyr-recombinase Flp, this novel procedure is not only relevant to the rational construction of biotechnologically significant cell lines, but it also finds increasing use for the systematic generation of stem cells. Stem cells can be used to replace damaged tissue or to generate transgenic animals with largely pre-determined properties.\n\nIt has been previously established that coexpression of both Cre and Flp recombinases catalyzes the exchange of sequences flanked by single loxP and FRT sites integrated into the genome at a random location. However, these studies did not explore whether such an approach could be used to modify conditional mouse alleles carrying single or multiple loxP and FRT sites. dual RMCE (dRMCE; Osterwalder et al., 2010) was recently developed as a re-engineering tool applicable to the vast numbers of mouse conditional alleles that harbor wild-type loxP and FRT sites and therefore are not compatible with conventional RMCE. The general dRMCE strategy takes advantage of the fact that most conditional alleles encode a selection cassette flanked by FRT sites, in addition to loxP sites that flank functionally relevant exons ('floxed' exons). The FRT-flanked selection cassette is in general placed outside the loxP-flanked region, which renders these alleles directly compatible with dRMCE. Simultaneous expression of Cre and Flp recombinases induces cis recombination and formation of the deleted allele, which then serves as a 'docking site' at which to insert the replacement vector by trans recombination. The correctly replaced locus would encode the custom modification and a different drug-selection cassette flanked by single loxP and FRT sites. dRMCE therefore appears as a very efficient tool for targeted re-engineering of thousands of mouse alleles produced by the IKMC consortium.\n\nMultiplexing setups rely on the fact that each F-Fn pair (consisting of a wildtype \"FRT\" site and a mutant called \"n\") or each Fn-Fm pair (consisting of two mutants, \"m\" and \"n\") constitutes a unique \"address\" in the genome. A prerequisite are differences in four out of the eight spacer positions (see Figure 1B). If the difference is below this threshold, some cross-interaction between the mutants may occur leading to a faulty deletion of the sequence between the heterospecific (Fm/Fn or F/Fn) sites.\n\n13 FRT-mutants have meanwhile become available, which permit the establishment of several unique genomic addresses (for instance F-Fn and Fm-Fo). These addresses will be recognized by donor plasmids that have been designed according to the same principles, permitting successive (but also synchronous) modifications at the predetermined \"loci\". These modifications can be driven to completion in case the compatible donor plasmid(s) are provided at an excess (mass-action principles). Figure 2 illustrates one use of the multiplexing principle: the stepwise extension of a coding region in which a basic expression unit is provided with genomic insulators, enhancers, or other \"cis\"-acting elements.\n\nA recent variation of the general concept is based on PhiC31 (an integrase of the Ser-class), which permits introduction of another RMCE target at a secondary site the first RMCE-based modification has occurred. This is due to the fact that each phiC31-catalyzed exchange destroys the attP and attB sites it has addressed converting them to \"att\"R and \"att\"L product sites, respectively. While these changes permit the subsequent mounting of new (and most likely remote) targets, they do not enable addressing several RMCE targets , nor do they permit \"serial RMCE\", i.e. successive, stepwise modifications at a given genomic locus.\n\nIt should be noted that this is different for Flp-RMCE, for which the post-RMCE status of \"FRT\"s corresponds to their initial state. This property enables the intentional, repeated mobilization of a target cassette by the addition of a new donor plasmid with compatible architecture. These \"multiplexing-RMCE\" options open unlimited possibilities for serial- and parallel specific modifications of pre-determined RMCE-targets \n\nGeneration of transgenic knock-out/-in mice and their genetic modification by RMCE.\n\nInsertion of a target cassette in a mammalian host cell line (CHO DG44 in suspension culture) and exchang with an ER stress reporter construct via targeted integration (RMCE).\n\n\n\n"}
{"id": "1045004", "url": "https://en.wikipedia.org/wiki?curid=1045004", "title": "Rodolfo Amando Philippi", "text": "Rodolfo Amando Philippi\n\nRodolfo Amando (or Rudolph Amandus) Philippi (14 September 1808 – 23 July 1904) was a German–Chilean paleontologist and zoologist.\n\nHe left his native Germany as a young man because he thought he was gravely ill and preferred to die in the mild climate of the Mediterranean region. He recovered and did considerable work there, including \"Abbildungen\" (illustrated monographs). Then he was invited to Chile by his brother Bernhard Eunom Philippi who worked for the government there. He moved to Santiago, Chile in 1851. There, he became a professor of botany and zoology and the director of the natural-history museum, and was a regular collaborator with Christian Ludwig Landbeck. Philippi described three new species of South American lizards.\n\nHis youngest son, Federico Philippi (1838–1910), was also a zoologist and botanist.\n\nHis grandson, Rodulfo Amando Philippi Bañados (1905-1969), was also a zoologist. In zoological nomenclature, the elder is referred to as \"Philippi [Krumwiede]\" to distinguish him from his grandson \"Philippi [Bañados]\".\n\nMuseo de la Exploración Rudolph Amandus Philippi in Valdivia is named after him.\n\n\n"}
{"id": "2731755", "url": "https://en.wikipedia.org/wiki?curid=2731755", "title": "Scottish Science and Technology Roadshow", "text": "Scottish Science and Technology Roadshow\n\nThe Scottish Science and Technology Roadshow (SCI-FUN), is a non-profit science roadshow for schools run by the University of Edinburgh.\n\nThe organisation states its aim as 'increasing enjoyment of and participation in science and technology-based subjects in Scottish schools' and operates mainly at Standard Grade level. It operates a travelling roadshow which visits schools in Scotland and the north of England, and organises special events such as at the Edinburgh International Science Festival. Its mode of operation as a travelling roadshow enables SCI-FUN to provide a science centre experience to pupils in remote areas of Scotland, such as the Highlands and Islands.\n\nThe SCI-FUN Roadshow tours secondary schools around Scotland offering a combination of presentations and hands-on exhibits encompassing scientific subjects in the fields of biology, chemistry, physics, maths, engineering and technology. The Roadshow is presented to S1 and S2 pupils (11-14 years old) with assistance from senior pupils studying sciences at Higher or Advanced Higher level.\n\nThe 2010/11 Roadshow includes presentations focusing on the senses and research, along with a talk encouraging pupils to consider the study of science subjects at Standard Grade. Current research at the University of Edinburgh is also covered in collaboration with FUSION; information on research in stem cells, the Large Hadron Collider, the Highland midge and Carbon Capture and Storage technology is currently included. In previous years, talks have discussed Climate change, the chemistry of the stars and survival.\n\nThere are around forty-five hands-on exhibits included in the Roadshow to allow pupils to have a go at a number of scientific experiments in a short time.\n\nSCI-FUN Primer states its aims as 'explaining to upper primary school children that science is something that is all around them' and is aimed at primary school pupils P6 and P7 (age 9-12). SCI-FUN Primer is based on the hands-on part of the main SCI-FUN Roadshow programme, with activities scaled down so it fits within a classroom environment. This may be hosted by an associated secondary school.\n\nSCI-FUN have a presence at the Edinburgh International Science Festival which is held every year in Edinburgh over the Easter holidays. Elements of the SCI-FUN Roadshow are incorporated into the Science Festival programme, including exhibits and presentations. In 2011, as previous years, SCI-FUN was at Adam House on Chambers Street. They also presented their Senses show for the general public. \n\nSCI-FUN took part in the Hebridean Science Festival in 2011. The festival is run by ESTEEM (Engineering, Science, Technology, Employability, Enterprise, Mathematics), which is based in Stornaway and aims to inspire the next generation of scientists, engineers, technologists and mathematicians from the Western Isles. SCI-FUN ran their usual Roadshow for Hebridean schools, as well as hosting an evening session and a drop-in session for the general public. Other guests at the Hebridean Science Festival include the Astronomer Royal for Scotland Professor John C Brown, Johnny Ball, the University of St Andrews, the Institution of Engineering and Technology, Mr Boom and the Stornoway Astronomical Society.\n\nSCI-FUN has previous attended other Science Festivals around Scotland, including in Shetland and Orkney, and plan to attend more.\n\nFUSION \"(Focusing on University Science Interpretation and Outreach Needs)\" is the sister group of SCI-FUN that collaborates with research groups within the University to find ways to promote their science to a wider audience. FUSION projects also lead to new topical science materials and exhibits, some of which have already been incorporated within the SCI-FUN Roadshow.\n\nA major project of FUSION has been to produce the CCS Interactive (Carbon Capture and Storage Interactive) exhibit, with funding from the EPSRC. The CCS Interactive is a working model of Carbon Capture and Storage and has been developed alongside the Schools of Engineering and GeoSciences at the University of Edinburgh. The CCS Interactive can be used to demonstrate the process of Carbon Capture and Storage to school and public audiences. The CCS Interactive consists of a model of a fossil fuel burning power station with CO being introduced into the system using a gas cylinder. There are two carbon capture columns, filled with high surface area zeolites which CO adsorbs to. Directions of gas flow and CO content of gas emissions are indicated using LEDs, and water vapour is used to simulate the gas emissions from the power station.\n\nSCI-FUN started out as \"FUNdamental Physics\" in the early 1990s. This was a small scale, volunteer-run show focusing on talks about the physical sciences. This was expanded and in 1999 became SCI-FUN in its current form, incorporating the hands-on exhibits about all the sciences and touring the whole of Scotland. A pilot project involved the hiring of equipment from the National Science-Technology Roadshow Trust in New Zealand.\n\nThe Roadshow has evolved over time, increasing collaboration with researchers at the University of Edinburgh and responding to changes in the school curriculum.\n\nSince 2004, Alan Walker from the Department of Physics at the University of Edinburgh has led the PP4SS \"(Particle Physics for Scottish Schools)\" project, funded by the PPARC. This is a workshop activity aimed at senior secondary school pupils which has been used at school visits, University functions, the Edinburgh International Science Festival and at Scottish Parliament events. The PP4SS team visited CERN as part of its 50th Anniversary celebration, along with Professor Peter Higgs who theorised the existence of the Higgs boson.\n\nIn 2005 and 2006, SCI-FUN organised and ran the University of Edinburgh's part of the Europe-wide LERU Kids' University programme. This programme aimed to allow secondary school pupils to experience the University environment at an early stage. \n\nPreviously, SCI-FUN has taken on S5 and S6 pupils for work experience placements. One week work experience places were offered to pupils to either travel out to schools with the Roadshow, or to work in the office developing exhibits or shows. In the past, pupils have also been offered a chance to apply for a bursary from the Nuffield Foundation for a six-week placement to work on exhibits for PP4SS.\n"}
{"id": "25206807", "url": "https://en.wikipedia.org/wiki?curid=25206807", "title": "Steve Mandel", "text": "Steve Mandel\n\nSteve Mandel is an amateur astronomer and astrophotographer. He owns a small observatory, called Hidden Valley Observatory, in Soquel, California. He has been acknowledged especially for his wide-field photographs of the Milky Way nebulae and for public outreach, for which he has received Amateur Achievement Award of the Astronomical Society of the Pacific. Besides this he has also captured and published wildlife images of endangered animals. He works as an American communications coach for professional executives, and is the founder of the Mandel Communications Inc., which aims to teach effective communication and public speaking. \n\nSteve Mandel's interest in astronomy began when he was 11 years old. He built his first telescope, a 6\" reflector, at his home in Los Angeles. His interest in astrophotography grew in the 1970s and 1980s and been worked as a photographic stringer for \"Newsweek\" magazine. Later, his wide-field images became recognized by professional astronomers. In 2005, he took pictures of high latitude areas of the sky and experimented with different wavelengths using various photographic filters and managed to take pictures of very faint unexplored nebulae above the plane of the Milky Way. The pictures were investigated by Adolf Witt, an astronomer of the University of Toledo in Ohio, who found out that the nebulae surprisingly contained carbon. Subsequently, his images became a subject of several scientific papers. These interstellar structures, labelled by Mandel as the integrated flux nebulae, are illuminated by the light from the entire galaxy, which distinguishes them from the typical reflection nebulae, illuminated by a nearby star. In 2004 Mandel started to work on the Mandel-Wilson Unexplored Nebulae Project aimed at their discovering, cataloguing and photographing.\n\nIn 1984 Steve Mandel published his portrait of the Cygnus constellation in the Sky and Telescope magazine, and since that time his pictures have been introduced in various other periodicals, including the NASA web page Astronomy Picture of the Day. In 2006 he published some of his astronomy photographs in his book \"Light in the Sky: Photographs of the Universe\".\nSteve Mandel has also cooperated with the Kitt Peak National Observatory Visitor Center, presenting educational Nightly Observer Program and Advanced Observer Program to the public. In 2004 he founded the Advanced Imaging Conference in San Jose, California, where about 250 amateur astronomers and manufacturers of astronomical equipment and software meet annually to discuss technology, imaging techniques and possibilities of scientific contributions. He created the so-called Hubble Award, given at the conference to an astronomer who made significant contributions to astrophotography.\n\nBesides astrophotography Steve Mandel has captured and published also images of endangered animals. He specializes in using robotic devices to gain artistic wildlife photographs from unusual angles.\n\nSteve Mandel received two awards for his contributions to astronomy, both in 2008. The Astronomical Society of the Pacific awarded him with their international Amateur Achievement Award, especially for his CCD imaging achievements and public outreach. The American Astronomical Society awarded him with the Chambliss Amateur Achievement Medal, annually given to North-American amateur astronomers, again for his contributions to wide-field imaging.\n\n"}
{"id": "34706719", "url": "https://en.wikipedia.org/wiki?curid=34706719", "title": "The Cost of Knowledge", "text": "The Cost of Knowledge\n\nThe Cost of Knowledge is a protest by academics against the business practices of academic journal publisher Elsevier. Among the reasons for the protests were a call for lower prices for journals and to promote increased open access to information. The main work of the project was to ask researchers to sign a statement committing not to support Elsevier journals by publishing, performing peer review, or providing editorial services for these journals.\n\nBefore the advent of the Internet, it was difficult for scholars to distribute articles giving their research results. Historically, publishers performed services including proofreading, typesetting, copyediting, printing, and worldwide distribution. In modern times, all researchers became expected to give the publishers digital copies of their work which needed no further processing – in other words, the modern academic is expected to do, often for free, duties traditionally assigned to the publisher, and for which, traditionally, the publisher is paid in exchange. For digital distribution, printing was unnecessary, copying was free, and worldwide distribution happens online instantly. Internet technology, and with it the aforementioned significant decrease in overhead costs, enabled the four major scientific publishers – Elsevier, Springer, Wiley, and Informa — to cut their expenditures such that they could consistently generate gross margins on revenue of over 33%.\n\nIn 2006, the nine editorial board members of Oxford University's Elsevier-published mathematics journal \"Topology\" resigned because they agreed among themselves that Elsevier's publishing policies had \"a significant and damaging effect on \"Topology\" reputation in the mathematical research community.\" An Elsevier spokesperson disputed this, saying that \"this still constitutes a pretty rare occurrence\" and that the journal \"is actually available today to more people than ever before\". Journalists recognize this event as part of the precedent to The Cost of Knowledge campaign. In 2008, the \"Journal of Topology\" started independently of Elsevier, and \"Topology\" ended publication in 2009.\n\nOn 21 January 2012, the mathematician Timothy Gowers called for a boycott of Elsevier with a post on his personal blog. This blog post attracted enough attention that other media sources commented on it as being part of the start of a movement. The three reasons he cited for the boycott are high subscription prices for individual journals, bundling subscriptions to journals of different value and importance, and Elsevier's support for SOPA, the PROTECT IP Act, and the Research Works Act. The \"Statement of Purpose\" on the Cost of Knowledge website explains that Elsevier was chosen as an initial focus for discontent due to a \"widespread feeling among mathematicians that they are the worst offender.\" The statement further mentions \"scandals, lawsuits, lobbying, etc.\" as reasons for focusing on Elsevier.\n\nElsevier disputed the claims, arguing that their prices are below the industry average, and stating that bundling is only one of several different options available to buy access to Elsevier journals. The company also claimed that its considerable profit margins are \"simply a consequence of the firm's efficient operation\". Critics of Elsevier claim that in 2010, 36% of Elsevier's reported revenues of US$3.2 billion was profit. Elsevier claimed to have an operating margin of 25.7% in 2010.\n\nA 2016 study evaluating the boycott has questioned its impact, stating that in the past four years 38% of signatories had abandoned their \"won't publish in an Elsevier outlet\" commitment and that only around 5000 researchers were still clearly boycotting Elsevier by publishing elsewhere. It concludes \"Few researchers have signed the petition in recent years, thus giving the impression the boycott has run its course.\".\n\nIn February 2012, analysts of the Exane Paribas bank reported a financial impact on Elsevier with the company's stock prices falling due to the boycott. Dennis Snower criticised the monopoly of scientific publishers, but said at the same time that he did not support the boycott even though he himself is the editor-in-chief of an open-access journal on economics. He thinks that more competition among the various journals should instead be encouraged. The Senate of the Kansas University has been reported to consider joining the boycott of Elsevier.\n\nIn allusion to the revolutions of the Arab Spring, the German \"Frankfurter Allgemeine Zeitung\" daily newspaper called the movement the \"Academic Spring\" (). When the British Wellcome Trust made a commitment to open up science, \"The Guardian\" similarly called this the \"Academic Spring\". After the Wellcome Trust announcement, The Cost of Knowledge campaign was recognized by that newspaper as the start of something new.\n\nA website called \"The Cost of Knowledge\" appeared, inviting researchers and scholars to declare their commitment to not submit papers to Elsevier journals, not referee articles for Elsevier's journals, and not participate in the editorial boards.\n\nOn 8 February 2012, 34 prominent mathematicians who had signed The Cost of Knowledge released a joint statement of purpose explaining their reasons for supporting the protest. In addition to Tim Gowers, Ingrid Daubechies, Juan J. Manfredi,\nTerence Tao, Wendelin Werner,\nScott Aaronson, László Lovász, and John Baez are among the signatories. Many signatories are researchers in the fields of mathematics, computer science, and biology.\nOn 1 February 2012, the declaration had a thousand signatories. By November 2018, over 17000 researchers had signed the petition. The success of the petition has been debated.\n\nOn 27 February 2012, Elsevier issued a statement on its website that declared that it has withdrawn support from the Research Works Act. Although the Cost of Knowledge movement was not mentioned, the statement indicated the hope that the move would \"help create a less heated and more productive climate\" for ongoing discussions with research funders. Hours after Elsevier's statement, Representatives Darrell Issa and Carolyn Maloney, who were sponsors of the bill, issued a joint statement saying that they would not push the bill in Congress. Earlier, Mike Taylor of the University of Bristol accused Issa and Maloney of being motivated by large donations that they received from Elsevier in 2011.\n\nWhile participants in the boycott celebrated the dropping of support for the Research Works Act, Elsevier denied that their action was a result of the boycott and stated that they took this action at the request of those researchers who did not participate in the boycott.\n\nOn the same day, Elsevier released an open letter to the mathematics community, stating that its target is to reduce its prices to $11/article or less. Elsevier also opened the archives of 14 mathematics journals back to 1995 with a four-year moving wall. In late 2012, Elsevier made all of its \"primary mathematics\" journals open access up to 2008.\nThe boycott remains in effect.\n\n\n"}
{"id": "276396", "url": "https://en.wikipedia.org/wiki?curid=276396", "title": "The Sand Reckoner", "text": "The Sand Reckoner\n\nThe Sand Reckoner (, \"Psammites\") is a work by Archimedes in which he set out to determine an upper bound for the number of grains of sand that fit into the Universe. In order to do this, he had to estimate the size of the universe according to the contemporary model, and invent a way to talk about extremely large numbers. The work, also known in Latin as Archimedis Syracusani Arenarius & Dimensio Circuli, which is about 8 pages long in translation, is addressed to the Syracusan king Gelo II (son of Hiero II), and is probably the most accessible work of Archimedes; in some sense, it is the first research-expository paper.\n\nFirst, Archimedes had to invent a system of naming large numbers. The number system in use at that time could express numbers up to a myriad (μυριάς — 10,000), and by utilizing the word \"myriad\" itself, one can immediately extend this to naming all numbers up to a myriad myriads (10). Archimedes called the numbers up to 10 \"first order\" and called 10 itself the \"unit of the second order\". Multiples of this unit then became the second order, up to this unit taken a myriad-myriad times, 10·10=10. This became the \"unit of the third order\", whose multiples were the third order, and so on. Archimedes continued naming numbers in this way up to a myriad-myriad times the unit of the 10-th order, i.e., formula_1.\n\nAfter having done this, Archimedes called the orders he had defined the \"orders of the first period\", and called the last one, formula_2, the \"unit of the second period\". He then constructed the orders of the second period by taking multiples of this unit in a way analogous to the way in which the orders of the first period were constructed. Continuing in this manner, he eventually arrived at the orders of the myriad-myriadth period. The largest number named by Archimedes was the last number in this period, which is\nAnother way of describing this number is a one followed by (short scale) eighty quadrillion (80·10) zeroes.\n\nArchimedes' system is reminiscent of a positional numeral system with base 10, which is remarkable because the ancient Greeks used a very simple system for writing numbers, which employs 27 different letters of the alphabet for the units 1 through 9, the tens 10 through 90 and the hundreds 100 through 900.\n\nArchimedes also discovered and proved the law of exponents, formula_4, necessary to manipulate powers of 10.\n\nArchimedes then estimated an upper bound for the number of grains of sand required to fill the Universe. To do this, he used the heliocentric model of Aristarchus of Samos. The original work by Aristarchus has been lost. This work by Archimedes however is one of the few surviving references to his theory, whereby the Sun remains unmoved while the Earth revolves about the Sun. In Archimedes' own words:\n\nThe reason for the large size of this model is that the Greeks were unable to observe stellar parallax with available techniques, which implies that any parallax is extremely subtle and so the stars must be placed at great distances from the Earth (assuming heliocentrism to be true).\n\nAccording to Archimedes, Aristarchus did not state how far the stars were from the Earth. Archimedes therefore had to make the following assumptions:\nThis assumption can also be expressed by saying that the stellar parallax caused by the motion of the Earth around its orbit equals the solar parallax caused by motion around the Earth. Put in a ratio:\n\nformula_5\n\nIn order to obtain an upper bound, Archimedes made the following assumptions of their dimensions:\nArchimedes then concluded that the diameter of the Universe was no more than 10 stadia (in modern units, about 2 light years), and that it would require no more than 10 grains of sand to fill it. With these measurements, each grain of sand in Archimedes's thought-experiment would have been approximately 19µm (0.019mm) in diameter.\n\nArchimedes claims that forty poppy-seeds laid side by side would equal one Greek dactyl (finger-width) which was approximately 19mm (3/4 inch) in length. Since volume proceeds as the cube of a linear dimension (“For it has been proved that spheres have the triplicate ratio to one another of their diameters”) then a sphere one dactyl in diameter would contain (using our current number system) 40, or 64,000 poppy seeds.\n\nHe then claimed (without evidence) that each poppy seed could contain a myriad (10,000) grains of sand. Multiplying the two figures together he proposed 640,000,000 as the number of hypothetical grains of sand in a sphere one dactyl in diameter.\n\nTo make further calculations easier, he rounded off 640 million to one billion, noting only that the first number is smaller than the second, and that therefore the number of grains of sand calculated subsequently will exceed the actual number of grains.\n\nA Greek stadium had a length of 600 Greek feet, and each foot was 16 dactyls long, so there were 9,600 dactyls in a stadium. Archimedes rounded this number up to 10,000 (a myriad) to make calculations easier, noting again that the resulting number will exceed the actual number of grains of sand.\n\nThe cube of 10,000 is a trillion (10); and multiplying a billion (the number of grains of sand in a dactyl-sphere) by a trillion (number of dactyl-spheres in a stadium-sphere) yields 10, the number of grains of sand in a stadium-sphere.\n\nArchimedes had estimated that the Aristarchian Universe was 10 stadiums in diameter, so there would accordingly be (10) stadium-spheres in the universe, or 10. Multiplying 10 by 10 yields 10, the number of grains of sand in the Aristarchian Universe.\n\nArchimedes made some interesting experiments and computations along the way. One experiment was to estimate the angular size of the Sun, as seen from the Earth. Archimedes' method is especially interesting as it takes into account the finite size of the eye's pupil, and therefore may be the first known example of experimentation in psychophysics, the branch of psychology dealing with the mechanics of human perception, whose development is generally attributed to Hermann von Helmholtz. Another interesting computation accounts for solar parallax and the different distances between the viewer and the Sun, whether viewed from the center of the Earth or from the surface of the Earth at sunrise. This may be the first known computation dealing with solar parallax.\n\nThe total number of nucleons in the observable universe of roughly the Hubble radius is the Eddington number, currently estimated at 10. Archimedes' 10 grains of sand contain roughly 10 nucleons, making the two numbers relatively equal. Please note the liberal use of the word \"coincidental\" in this context.\n\n\n"}
{"id": "31774747", "url": "https://en.wikipedia.org/wiki?curid=31774747", "title": "Tricorder X Prize", "text": "Tricorder X Prize\n\nThe Qualcomm Tricorder XPRIZE was an inducement prize contest, that originally offered a US$7 million grand prize, US$2 million second prize, and US$1 million third prize to the best among the finalists offering an automatic non-invasive health diagnostics system in a single portable package that weighs no more than 5 pounds (2.3 kg), able to autonomously diagnose 13 medical conditions (12 diseases and the 'absence of conditions'), including anemia, atrial fibrillation, Chronic obstructive pulmonary disease (COPD), diabetes, leukocytosis, pneumonia, otitis media, sleep apnea, and urinary tract infection. The winning devices must also be able to continuously record and stream the 5 main vital signs: blood pressure, heart rate, oxygen saturation, respiratory rate and temperature. The name is taken from the tricorder device from the science fiction TV series \"Star Trek\" which can be used to instantly diagnose ailments. The prize was initially announced by the X PRIZE Foundation on 10 May 2011 and subsequently launched on 10 January 2012 at CES 2012. Devices were sent to the University of California San Diego to be independently tested on patients during the winter and spring of 2015, and again in late 2016 at the Altman Clinical and Translational Research Institute (ACTRI) at UCSD. Although no team successfully met all the requirements of the grand prize, the competition was concluded in April 2017 when the XPRIZE Foundation awarded reduced prizes to the strongest performing teams. Most notably, Final Frontier Medical Devices was awarded US$2.6 million and Dynamical Biomarkers won US$1 million. A third team, Cloud DX, was named \"Bold Epic Innovator\" and awarded US$100,000 for achieving the main milestones of the competition while missing a crucial deadline. Earlier in 2016, some of the funds from the original prize purse were awarded to semi-finalist teams for hitting technology milestones. For the first time at any XPRIZE, the leftover funds from the main prize purse have been earmarked for further development, consumer testing and commercialization of tricorder prototypes for the two finalists and four semi-finalist teams as part of the Post Prize Initiative.\n\nThe two teams to compete in consumer testing round: \n\n\nThe 10 teams to be selected to compete are: \n\nIn the end no team met all the requirements needed to win the full prize purse for an automatic non-invasive health diagnostics system packaged into a single portable device.\n\nIn April 2017 X PRIZE Foundation made the following awards for a total of $3.7 million:\n\nThe rest of the original $10 million prize purse was diverted to ongoing consumer testing to get tricorder technology into the hands of patients ($3.8 million) and adapting tricorders for use in hospitals in developing countries ($1.6 million).\n\n"}
{"id": "776713", "url": "https://en.wikipedia.org/wiki?curid=776713", "title": "Unified field theory", "text": "Unified field theory\n\nIn physics, a unified field theory (UFT) is a type of field theory that allows all that is usually thought of as fundamental forces and elementary particles to be written in terms of a pair of physical and virtual fields. According to the modern discoveries in physics, forces are not transmitted directly between interacting objects, but instead are described and interrupted by intermediary entities called fields.\n\nClassically, however, a duality of the fields is combined into a single physical field. For over a century, unified field theory remains an open line of research and the term was coined by Albert Einstein, who attempted to unify his general theory of relativity with electromagnetism. The \"Theory of Everything\" and Grand Unified Theory are closely related to unified field theory, but differ by not requiring the basis of nature to be fields, and often by attempting to explain physical constants of nature. Earlier attempts based on classical physics are described in the article on classical unified field theories.\n\nThe goal of a unified field theory has led to a great deal of progress for future theoretical physics and progress continues.\n\nGoverned by a global event formula_1 under the universal topology, an operational environment is initiated by the scalar fields formula_2 of a rank-0 tensor, a differentiable function of a complex variable in its domain at its zero derivative, where a scalar function formula_3 or formula_4 is characterized as a single magnitude with variable components of the respective coordinate sets formula_5 or formula_6.\n\nBecause a field is incepted or operated under either virtual or physical primacy of an formula_7 or formula_8 manifold respectively and simultaneously, each point of the fields is entangled with and appears as a conjugate function of the scalar field formula_9 or formula_10 in its opponent manifold. A field can be classified as a scalar field, a vector field, or a tensor field according to whether the represented physical horizon is at a scope of scalar, vector, or tensor potentials, respectively.\n\nTherefore, at the scalar potentials, the effects are stationary projected to and communicated from their reciprocal opponent, shown as the following conjugate pairs:\n\nwhere * denotes a complex conjugate. A conjugate field formula_15 of the formula_7 scalar potential is mapped to a field in the formula_8 manifold, and vice versa that a conjugate field formula_18 of the formula_8 scalar potential is mapped to a field in the\nformula_7 manifold. In mathematics, if f(z) is a holomorphic function restricted to the Real Numbers, it has the complex conjugate properties of\nf (z) = f *(z*), which leads to the above equation when formula_21 is satisfied.\n\nAll four of the known fundamental forces are mediated by fields, which in the Standard Model of particle physics result from exchange of gauge bosons. Specifically the four fundamental interactions to be unified are:\n\nModern unified field theory attempts to bring these four interactions together into a single framework.\n\nThe first successful classical unified field theory was developed by James Clerk Maxwell. In 1820 Hans Christian Ørsted discovered that electric currents exerted forces on magnets, while in 1831, Michael Faraday made the observation that time-varying magnetic fields could induce electric currents. Until then, electricity and magnetism had been thought of as unrelated phenomena. In 1864, Maxwell published his famous paper on a dynamical theory of the electromagnetic field. This was the first example of a theory that was able to encompass previously separate field theories (namely electricity and magnetism) to provide a unifying theory of electromagnetism. By 1905, Albert Einstein had used the constancy of the speed of light in Maxwell's theory to unify our notions of space and time into an entity we now call spacetime and in 1915 he expanded this theory of special relativity to a description of gravity, general relativity, using a field to describe the curving geometry of four-dimensional spacetime.\n\nIn the years following the creation of the general theory, a large number of physicists and mathematicians enthusiastically participated in the attempt to unify the then-known fundamental interactions. In view of later developments in this domain, of particular interest are the theories of Hermann Weyl of 1919, who introduced the concept of an (electromagnetic) gauge field in a classical field theory and, two years later, that of Theodor Kaluza, who extended General Relativity to five dimensions. Continuing in this latter direction, Oscar Klein proposed in 1926 that the fourth spatial dimension be curled up into a small, unobserved circle. In Kaluza–Klein theory, the gravitational curvature of the extra spatial direction behaves as an additional force similar to electromagnetism. These and other models of electromagnetism and gravity were pursued by Albert Einstein in his attempts at a classical unified field theory. By 1930 Einstein had already considered the Einstein–Maxwell–Dirac System [Dongen]. This system is (heuristically) the super-classical [Varadarajan] limit of (the not mathematically well-defined) quantum electrodynamics. One can extend this system to include the weak and strong nuclear forces to get the Einstein–Yang–Mills–Dirac System. The French physicist Marie-Antoinette Tonnelat published a paper in the early 1940s on the standard commutation relations for the quantized spin-2 field. She continued this work in collaboration with Erwin Schrödinger after World War II. In the 1960s Mendel Sachs proposed a generally covariant field theory that did not require recourse to renormalisation or perturbation theory. In 1965, Tonnelat published a book on the state of research on unified field theories.\n\nIn 1963 American physicist Sheldon Glashow proposed that the weak nuclear force, electricity and magnetism could arise from a partially unified electroweak theory. In 1967, Pakistani Abdus Salam and American Steven Weinberg independently revised Glashow's theory by having the masses for the W particle and Z particle arise through spontaneous symmetry breaking with the Higgs mechanism. This unified theory modeled the electroweak interaction as a force mediated by four particles: the photon for the electromagnetic aspect, and a neutral Z particle and two charged W particles for weak aspect. As a result of the spontaneous symmetry breaking, the weak force becomes short-range and the W and Z bosons acquire masses of 80.4 and , respectively. Their theory was first given experimental support by the discovery of weak neutral currents in 1973. In 1983, the Z and W bosons were first produced at CERN by Carlo Rubbia's team. For their insights, Glashow, Salam, and Weinberg were awarded the Nobel Prize in Physics in 1979. Carlo Rubbia and Simon van der Meer received the Prize in 1984.\n\nAfter Gerardus 't Hooft showed the Glashow–Weinberg–Salam electroweak interactions to be mathematically consistent, the electroweak theory became a template for further attempts at unifying forces. In 1974, Sheldon Glashow and Howard Georgi proposed unifying the strong and electroweak interactions into the Georgi–Glashow model, the first Grand Unified Theory, which would have observable effects for energies much above 100 GeV.\n\nSince then there have been several proposals for Grand Unified Theories, e.g. the Pati–Salam model, although none is currently universally accepted. A major problem for experimental tests of such theories is the energy scale involved, which is well beyond the reach of current accelerators. Grand Unified Theories make predictions for the relative strengths of the strong, weak, and electromagnetic forces, and in 1991 LEP determined that supersymmetric theories have the correct ratio of couplings for a Georgi–Glashow Grand Unified Theory.\n\nMany Grand Unified Theories (but not Pati–Salam) predict that the proton can decay, and if this were to be seen, details of the decay products could give hints at more aspects of the Grand Unified Theory. It is at present unknown if the proton can decay, although experiments have determined a lower bound of 10 years for its lifetime.\n\nTheoretical physicists have not yet formulated a widely accepted, consistent theory that combines general relativity and quantum mechanics to form a theory of everything. Trying to combine the graviton with the strong and electroweak interactions leads to fundamental difficulties and the resulting theory is not renormalizable. The incompatibility of the two theories remains an outstanding problem in the field of physics.\n\n"}
{"id": "31333376", "url": "https://en.wikipedia.org/wiki?curid=31333376", "title": "Upshot-Knothole Annie", "text": "Upshot-Knothole Annie\n\nUpshot–Knothole \"Annie\" was a nuclear weapons test conducted by the United States as part of Operation Upshot–Knothole. It took place at the Nevada Test Site on 17 March 1953, and was nationally televised. The live TV coverage was recorded on a kinescope, so it is a rare record of the sound an actual atomic bomb makes.\n\nOperation Doorstep was a civil defense study conducted by the Federal Civil Defense Administration in conjunction with \"Annie\". It studied the effect of the nuclear blast on two wooden frame houses, fifty automobiles and eight bomb shelters designed for residential use.\n\nThe administration concluded that if windows were left open to prevent the car collapsing on its occupants a car would be \"relatively safe\" from a small nuclear bomb if at least ten blocks away from the hypocenter. The homes in the study were constructed in such a way as to minimize the thermal effects of Annie, with an eye towards determining if, in the absence of fire, the basement of the closer home — from the hypocenter — might shelter its occupants, while the second — at — could remain standing. Both homes performed as expected under the conditions of their construction.\n"}
{"id": "49136553", "url": "https://en.wikipedia.org/wiki?curid=49136553", "title": "Vector generalized linear model", "text": "Vector generalized linear model\n\nIn statistics, the class of vector generalized linear models (VGLMs) was proposed to \nenlarge the scope of models catered for by generalized linear models (GLMs).\nIn particular, VGLMs allow for response variables outside the classical exponential family\nand for more than one parameter. Each parameter (not necessarily a mean) can be transformed by a \"link function\".\nThe VGLM framework is also large enough to naturally accommodate multiple responses; these are\nseveral independent responses each coming from a particular statistical distribution with\npossibly different parameter values.\n\nVector generalized linear models are described in detail in Yee (2015).\nThe central algorithm adopted is the iteratively reweighted least squares method,\nfor maximum likelihood estimation of usually all the model parameters. In particular, \nFisher scoring is implemented by such, which, for most models,\nuses the first and expected second derivatives of the log-likelihood function.\n\nGLMs essentially cover one-parameter models from the classical exponential family,\nand include 3 of the most important statistical regression models:\nthe linear model, Poisson regression for counts, and logistic regression\nfor binary responses.\nHowever, the exponential family is far too limiting for regular data analysis.\nFor example, for counts, zero-inflation, zero-truncation and overdispersion are regularly\nencountered, and the makeshift adaptations made to the binomial and\nPoisson models in the form of quasi-binomial and\nquasi-Poisson can be argued as being ad hoc and unsatisfactory.\nBut the VGLM framework readily handles models such as\nzero-inflated Poisson regression,\nzero-altered Poisson (hurdle) regression,\npositive-Poisson regression, and\nnegative binomial regression.\nAs another example, for the linear model,\nthe variance of a normal distribution is relegated \nas a scale parameter and it is treated\noften as a nuisance parameter (if it is considered as a parameter at all).\nBut the VGLM framework allows the variance to be modelled using covariates.\n\nAs a whole, one can loosely think of VGLMs as GLMs that handle many models \noutside the classical exponential family and are not restricted to estimating \na single mean.\nDuring estimation,\nrather than using weighted least squares \nduring IRLS, one uses generalized least squares to handle the \ncorrelation between the \"M\" linear predictors.\n\nWe suppose that the response or outcome or the dependent variable(s), formula_1, are assumed to be generated from a particular distribution. Most distributions are univariate, so that formula_2, and an example of formula_3 is the bivariate normal distribution.\n\nSometimes we write our data as formula_4\nfor formula_5. Each of the \"n\" observations are considered to be\nindependent.\nThen formula_6.\nThe formula_7 are known positive prior weights, and often formula_8.\n\nThe explanatory or independent variables are written formula_9, \nor when \"i\" is needed, as formula_10.\nUsually there is an \"intercept\", in which case\nformula_11 or formula_12.\n\nActually, the VGLM framework allows for \"S\" responses, each of dimension formula_13.\nIn the above \"S\" = 1. Hence the dimension of formula_14 is more generally formula_15. One handles \"S\" responses by code such\nas codice_1 for \"S\" = 3.\nTo simplify things, most of this article has \"S\" = 1.\n\nThe VGLM usually consists of four elements:\n\nEach \"linear predictor\" is a quantity which incorporates\ninformation about the independent variables into the model. \nThe symbol formula_18 (Greek \"eta\") \ndenotes a linear predictor and a subscript \"j\" is used to denote the \"j\"th one. \nIt relates the \"j\"th parameter to the explanatory variables, and\nformula_18 is expressed as linear combinations (thus, \"linear\") \nof unknown parameters formula_27\ni.e., of regression coefficients formula_28.\n\nThe \"j\"th parameter, formula_19, of the distribution depends on the \nindependent variables, formula_30 through\n\nLet formula_32 be the vector of\nall the linear predictors. (For convenience we always let formula_33\nbe of dimension \"M\").\nThus \"all\" the covariates comprising formula_34 potentially affect \"all\" the parameters through the linear predictors formula_18. Later, we will allow the linear predictors to be generalized to additive predictors, which is the sum of smooth functions of each formula_36 and each function is estimated from the data.\n\nEach link function provides the relationship between a linear predictor and a \nparameter of the distribution. \nThere are many commonly used link functions, and their choice can be somewhat arbitrary. It makes sense to try to match the domain of the link function to \nthe range of the distribution's parameter value.\nNotice above that the formula_21 allows a different link function for each parameter.\nThey have similar properties as with generalized linear models, for example,\ncommon link functions include the \"logit\" link for parameters in formula_38,\nand the log link for positive parameters. The codice_2 package has function codice_3 for parameters that can assume both positive and negative values.\n\nMore generally, the VGLM framework allows for any linear constraints between the regression coefficients formula_28 of each linear predictors. For example, we may want to set some to be equal to 0, or constraint some of them to be equal. We have\n\nwhere the formula_23 are the \"constraint matrices\".\nEach constraint matrix is known and prespecified, and has \"M\" rows, and between 1 and \"M\" columns. The elements of constraint matrices are finite-valued, and often they are just 0 or 1.\nFor example, the value 0 effectively omits that element while a 1 includes it.\nIt is common for some models to have a \"parallelism\" assumption, which means that\nformula_42 for formula_43, and\nfor some models, for formula_44 too.\nThe special case when formula_45 for \nall formula_46 is known as \"trivial constraints\"; all the\nregression coefficients are estimated and are unrelated.\nAnd formula_19 is known as an \"intercept-only\" parameter\nif the \"j\"th row of all the formula_48 are equal to formula_49 for formula_43, i.e., formula_51 equals an intercept only. Intercept-only parameters are thus modelled as simply as possible, as a scalar.\n\nThe unknown parameters, formula_52, \nare typically estimated by the method of maximum likelihood.\nAll the regression coefficients may be put into a matrix as follows:\n\nWith even more generally, one can allow the value of a variable formula_36\nto have a different value for each formula_18.\nFor example, if each linear predictor is for a different time point then\none might have a time-varying covariate.\nFor example,\nin discrete choice models, one has \n\"conditional\" logit models,\n\"nested\" logit models,\n\"generalized\" logit models,\nand the like, to distinguish between certain variants and\nfit a multinomial logit model to, e.g., transport choices.\nA variable such as cost differs depending on the choice, for example,\ntaxi is more expensive than bus, which is more expensive than walking.\nThe codice_4 facility of codice_2 allows one to\ngeneralize formula_56\nto formula_57.\n\nThe most general formula is\n\nHere the formula_59 is an optional \"offset\"; which translates\nto be a formula_60 matrix in practice. \nThe codice_2 package has an codice_4 argument that allows\nthe successive elements of the diagonal matrix to be inputted.\n\nYee (2015) describes an R package \nimplementation in the \ncalled VGAM.\nCurrently this software fits approximately 150 models/distributions.\nThe central modelling functions are codice_8 and codice_9.\nThe codice_10 argument is assigned a \"VGAM family function\",\ne.g., codice_11 for negative binomial regression,\ncodice_12 for Poisson regression,\ncodice_13 for the \"proportional odd model\" or\n\"cumulative logit model\" for ordinal categorical regression.\n\nWe are maximizing a log-likelihood\n\nwhere the formula_62 are positive and known \"prior weights\".\nThe maximum likelihood estimates can be found \nusing an iteratively reweighted least squares algorithm using \nFisher's scoring method, with updates of the form:\n\nwhere formula_64 is\nthe Fisher information matrix at iteration \"a\".\nIt is also called the \"expected information matrix\", or \"EIM\".\n\nFor the computation, the (small) \"model matrix\" constructed\nfrom the RHS of the formula in codice_8\nand the constraint matrices are combined to form a \"big\" model matrix.\nThe IRLS is applied to this big X. This matrix is known as the VLM\nmatrix, since the \"vector linear model\" is the underlying least squares\nproblem being solved. A VLM is a weighted multivariate regression where the\nvariance-covariance matrix for each row of the response matrix is not\nnecessarily the same, and is known.\n(In classical multivariate regression, all the errors have the\nsame variance-covariance matrix, and it is unknown).\nIn particular, the VLM minimizes the weighted sum of squares\n\nThis quantity is minimized at each IRLS iteration.\nThe \"working responses\" (also known as \"pseudo-response\" and \"adjusted\ndependent vectors\") are\n\nwhere the formula_67 are known as \"working weights\" or \"working weight matrices\". They are symmetric and positive-definite. Using the EIM helps ensure that they are all positive-definite (and not just the sum of them) over much of the parameter space. In contrast, using Newton–Raphson would mean the observed information matrices would be used, and these tend to be positive-definite in a smaller subset of the parameter space.\n\nComputationally, the Cholesky decomposition is used to invert the working weight matrices and to convert the overall generalized least squares problem into an ordinary least squares problem.\n\nOf course, all generalized linear models are a special cases of VGLMs.\nBut we often estimate all parameters by full maximum likelihood estimation rather\nthan using the method of moments for the scale parameter.\n\nIf the response variable is an ordinal measurement with \"M\" + 1 \"levels\", then one may fit a model function of the form:\n\nfor formula_20\nDifferent links \"g\" lead to proportional odds models or ordered probit models,\ne.g., the codice_2 family function codice_16 assigns a probit link to the cumulative\nprobabilities, therefore this model is also called the \"cumulative probit model\".\nIn general they are called \"cumulative link models\".\n\nFor categorical and multinomial distributions, the fitted values are an (\"M\" + 1)-vector of probabilities, with the property that all probabilities add up to 1. Each probability indicates the likelihood of occurrence of one of the \"M\" + 1 possible values.\n\nIf the response variable is a nominal measurement, \nor the data do not satisfy the assumptions of an ordered model, then one may fit a model of the following form:\n\nfor formula_20 The above link is sometimes called the \"multilogit\" link,\nand the model is called the multinomial logit model.\nIt is common to choose the first or the last level of the response as the\n\"reference\" or \"baseline\" group; the above uses the last level.\nThe codice_2 family function codice_18 fits the above model,\nand it has an argument called codice_19 that can be assigned\nthe level used for as the reference group.\n\nClassical GLM theory performs Poisson regression for count data. \nThe link is typically the logarithm, which is known as the \"canonical link\".\nThe variance function is proportional to the mean:\n\nwhere the dispersion parameter formula_74 is typically fixed at exactly one. When it is not, the resulting quasi-likelihood model is often described as Poisson with overdispersion, or \"quasi-Poisson\"; then formula_74 is commonly estimated by the method-of-moments and as such,\nconfidence intervals for formula_74 are difficult to obtain.\n\nIn contrast, VGLMs offer a much richer set of models to handle overdispersion with respect to the Poisson, e.g., the negative binomial distribution and several variants thereof. Another count regression model is the \"generalized Poisson distribution\". Other possible models are the \"zeta distribution\" and the \"Zipf distribution\".\n\nRR-VGLMs are VGLMs where a subset of \nthe B matrix is of a lower rank.\nWithout loss of generality, suppose that formula_77 is a partition of the covariate vector. Then the part of the B matrix corresponding to formula_78 is of the form formula_79 where formula_80 and\nformula_81 are thin matrices (i.e., with \"R\" columns), e.g., vectors if the rank \"R\" = 1. RR-VGLMs potentially offer several advantages when applied to certain\nmodels and data sets. Firstly, if \"M\" and \"p\" are large then the number of regression coefficients\nthat are estimated by VGLMs is large (formula_82). Then RR-VGLMs can reduce the number of estimated regression coefficients enormously if \"R\" is low, e.g., \"R\" = 1\nor \"R\" = 2. An example of a model where this is particularly useful is the RR-multinomial logit model, also known as the \"stereotype model\".\nSecondly, formula_83 \nis an \"R\"-vector of latent variables, and often these can be usefully interpreted.\nIf \"R\" = 1 then we can write formula_84\nso that the latent variable comprises loadings on the explanatory variables.\nIt may be seen that RR-VGLMs take optimal linear combinations of the formula_78\nand then a VGLM is fitted to the explanatory variables formula_86. Thirdly, a biplot can be produced if \"R' = 2 , and this allows the model to be visualized.\n\nIt can be shown that RR-VGLMs are simply VGLMs where the constraint matrices for\nthe variables in formula_78 are unknown and to be estimated.\nIt then transpires that formula_88 for\nsuch variables.\nRR-VGLMs can be estimated by an \"alternating\" algorithm which fixes formula_80 \nand estimates formula_90 and then fixes formula_91 and estimates formula_80, etc.\n\nIn practice, some uniqueness constraints are needed for formula_80\nand/or formula_91. In codice_2, the codice_21 function uses \"corner constraints\" by default, which means that the top \"R\" rows of formula_80 is set to formula_96. RR-VGLMs were proposed in 2003.\n\nA special case of RR-VGLMs is when \"R\" = 1 and \"M\" = 2. This is \"dimension reduction\" from 2 parameters to 1 parameter. Then it can be shown that\n\nwhere elements formula_98 and formula_99 are estimated. Equivalently,\n\nThis formula provides a coupling of formula_101 and formula_102. It induces a relationship between two parameters of a model that can be useful, e.g., for modelling a mean-variance relationship. Sometimes there is some choice of link functions, therefore it offers a little flexibility when coupling the two parameters, e.g., a logit, probit, cauchit or cloglog link for parameters in the unit interval. The above formula is particularly useful for the negative binomial distribution, so that the RR-NB has variance function\n\nThis has been called the \"NB-P\" variant by some authors. The formula_104 and formula_105 are estimated, and it is also possible to obtain approximate confidence intervals for them too.\n\nIncidentally, several other useful NB variants can also be fitted, with the help of selecting the right combination of constraint matrices. For example, \"NB\" − 1, \"NB\" − 2 (codice_22 default), \"NB\" − \"H\"; see Yee (2014) and Table 11.3 of Yee (2015).\n\nThe subclass of \"row-column interaction models\"\n(RCIMs) has also been proposed; these are a special type of RR-VGLM. \nRCIMs apply only to a matrix Y response and there are\nno explicit explanatory variables formula_34.\nInstead, indicator variables for each row and column are explicitly set up, and an order-\"R\"\ninteraction of the form formula_79 is allowed.\nSpecial cases of this type of model include the \"Goodman RC association model\"\nand the quasi-variances methodology as implemented by the codice_23 R package.\n\nRCIMs can be defined as a RR-VGLM applied to Y with\n\nFor the Goodman RC association model, we have formula_109 so that\nif \"R\" = 0 then it is a Poisson regression fitted to a matrix of counts with row effects and column effects; this has a similar idea to a no-interaction two-way ANOVA model.\n\nAnother example of a RCIM is if formula_110 is the identity link and the parameter is the median and the model corresponds to an asymmetric Laplace distribution; then a no-interaction RCIM is similar to a technique called \"median polish\".\n\nIn codice_2, codice_25 and codice_26 functions fit the above models.\nAnd also Yee and Hadi (2014)\nshow that RCIMs can be used to fit unconstrained quadratic ordination\nmodels to species data; this is an example of indirect gradient analysis in\nordination (a topic in statistical ecology).\n\n\"Vector generalized additive models\" (VGAMs) are a major \nextension to VGLMs in which the linear predictor formula_18 is not restricted to be \nlinear in the covariates formula_36 but is the \nsum of smoothing functions applied to the formula_36:\n\nwhere formula_115\nThese are \"M\" \"additive predictors\".\nEach smooth function formula_116 is estimated from the data.\nThus VGLMs are \"model-driven\" while VGAMs are \"data-driven\".\nCurrently, only smoothing splines are implemented in the codice_2 package.\nFor \"M\" > 1 they are actually \"vector splines\", which estimate the component functions\nin formula_117 simultaneously.\nOf course, one could use regression splines with VGLMs.\nThe motivation behind VGAMs is similar to\nthat of\nHastie and Tibshirani (1990)\nand\nWood (2017).\nVGAMs were proposed in 1996\n\nCurrently, work is being done to estimate VGAMs using \"P-splines\" \nof Eilers and Marx (1996)\nThis allows for several advantages over using smoothing splines and vector backfitting, such as the\nability to perform automatic smoothing parameter selection easier.\n\nThese add on a quadratic in the latent variable to the RR-VGLM class.\nThe result is a bell-shaped curve can be fitted to each response, as\na function of the latent variable.\nFor \"R\" = 2, one has bell-shaped surfaces as a function of the 2\nlatent variables---somewhat similar to a \nbivariate normal distribution.\nParticular applications of QRR-VGLMs can be found in ecology, \nin a field of multivariate analysis called ordination.\n\nAs a specific rank-1 example of a QRR-VGLM,\nconsider Poisson data with \"S\" species.\nThe model for Species \"s\" is the Poisson regression\n\nfor formula_119. The right-most parameterization which uses the symbols formula_120 formula_121 formula_122 has particular ecological meaning, because they relate to the species \"abundance\", \"optimum\" and \"tolerance\" respectively. For example, the tolerance is a measure of niche width, and a large value means that that species can live in a wide range of environments. In the above equation, one would need formula_123 in order\nto obtain a bell-shaped curve.\n\nQRR-VGLMs fit Gaussian ordination models by maximum likelihood estimation, and\nthey are an example of direct gradient analysis.\nThe codice_28 function in the codice_2 package currently\ncalls codice_30 to search for the optimal \nformula_91, and given that, it is easy to calculate\nthe site scores and fit a suitable generalized linear model to that.\nThe function is named after the acronym CQO, which stands for\n\"constrained quadratic ordination\": the \"constrained\" is for direct\ngradient analysis (there are environmental variables, and a linear combination\nof these is taken as the latent variable) and the \"quadratic\" is for the\nquadratic form in the latent variables formula_125\non the formula_33 scale.\nUnfortunately QRR-VGLMs are sensitive to outliers in both the response\nand explanatory variables, as well as being computationally expensive, and\nmay give a local solution rather than a global solution.\nQRR-VGLMs were proposed in 2004.\n\n\n"}
{"id": "8435544", "url": "https://en.wikipedia.org/wiki?curid=8435544", "title": "Walls and Mirrors", "text": "Walls and Mirrors\n\nWalls And Mirrors is a computer science textbook, for undergraduates taking a second computer science course (typically on the subject of data structures and algorithms), originally written by Paul Helman and Robert Veroff. The book attempts to strike a balance between being too mathematically rigorous and formal, and being so informal, practical, and hands-on that computer science theory is not taught.\n\nThe \"walls\" of the title refer to the abstract data type (ADT) which has a wall between its public interface and private implementation. Early languages like Pascal did not build this wall very high; later languages like Modula-2 did create a much stronger wall between the two; and object-oriented languages such as C++ and Java implement walls using the class concept.\n\nThe \"mirrors\" of the title refer to recursion. The idea is of looking at a reflection in two mirrors placed in opposition to one another, so a repeated image is reflected smaller and smaller in them.\n\nThe first edition, which used the language Pascal, was published in 1986.\n\nAn edition which used Modula-2 was published in 1988. Modula-2 had much better support for the sort of ADT the book taught than Pascal.\n\nLater editions from the mid 1990s and the 2000s used C++ and Java, reflecting a fundamental shift in how computer science was taught. The original authors' names have been removed from the most recent editions of the book. \n\n"}
{"id": "25656014", "url": "https://en.wikipedia.org/wiki?curid=25656014", "title": "Yamazaki-Teiichi Prize", "text": "Yamazaki-Teiichi Prize\n\nYamazaki-Teiichi Prize is an award given annually by the Foundation for Promotion of Material Science and Technology of Japan (MST) to people who have achieved outstanding, creative results, with practical effect, by publishing theses, acquiring patents, or developing methods, technologies and the like and/or people with strong future potential for achieving such results. Chairman of the selection committee is Professor Hideki Shirakawa, the winner of the 2000 Nobel Prize in Chemistry. The prize was established in commemoration of the late , the first chairman of the MST's Board of Directors, for his contributions to scientific, technological and industrial development and human resource cultivation.\n\nThe Yamazaki-Teiichi Prize is awarded in the following four fields. Prizewinner receives an award diploma, a gold medal, and cash award of JPY 3,000,000 cash (about USD 30 thousands) per full Prize in each area.\n\n\n\n\n\n\n"}
{"id": "11432266", "url": "https://en.wikipedia.org/wiki?curid=11432266", "title": "Youth studies", "text": "Youth studies\n\nYouth studies is an interdisciplinary academic field devoted to the study of the development, , culture, psychology, and politics of youth. The field studies not only specific cultures of young people, but also their relationships, roles and responsibilities throughout the larger societies which they occupy. The field includes scholars of education, literature, history, politics, religion, sociology, and many other disciplines within the humanities and social sciences.Youth studies encourages the understanding of experiences that are predominantly manifested among young people, generalized phenomenon and social change. The majority of 15- to 24-year-olds in 2008 lived in developing countries. The definition of youth varies across cultural contexts. The social experience and organization of time and space are important themes in youth studies. Scholars examine how neoliberalism and globalization affect how young people experience life, including in comparison to previous generations.\n\n\n\n\n\n\n"}
