{"id": "8224878", "url": "https://en.wikipedia.org/wiki?curid=8224878", "title": "A415 road", "text": "A415 road\n\nThe A415 is a British A road which runs from the A4074 at Berinsfield, Oxfordshire to Witney passing through Abingdon, Marcham and Kingston Bagpuize. It crosses the River Thames twice, at Abingdon Bridge and Newbridge.\n\nWhen UK roads were first classified in 1922, the A415 started at a junction with the A4 just west of Maidenhead in Berkshire. It ran through Henley-on-Thames and Benson to Shillingford where it joined the then A42. It left the A42 a mile west at Dorchester. In the 1930s the section from Maidenhead to Shillingford became part of the A423. It is now part of the A4130 and the A4074.\n\nIn the 1980s, when the Dorchester Bypass was opened, the start of the A415 was switched to the western end of the bypass, at Berinsfield.\n"}
{"id": "14073243", "url": "https://en.wikipedia.org/wiki?curid=14073243", "title": "A4230 road", "text": "A4230 road\n\nThe A4230 is a suburban main road in south Wales.\n\nThe A4230 is a single carriageway road for its whole length. It begins at the Peniel Green roundabout (junction with the M4 motorway and the A48) in the far east of Llansamlet. It travels east through the Lonlas area of Skewen before passing through central Skewen, after which it traverses the Neath Abbey area before connecting with the A474 at the Cwrt Herbert roundabout in Dyffryn Clydach. It then follows the same road as the A474 along Neath Abbey Road then though Cadoxton until it connects with the A465 just south of Aberdulais.\n"}
{"id": "46477850", "url": "https://en.wikipedia.org/wiki?curid=46477850", "title": "Abell 2162", "text": "Abell 2162\n\nAbell 2162 is a galaxy cluster in the Abell catalogue located in the constellation Corona Borealis. It is a member of the Hercules Superclusters, the redshifts of the member galaxies of which lie between 0.0304 and 0.0414. The cluster hosts a massive Type-cD galaxy called NGC 6086.\n"}
{"id": "16773926", "url": "https://en.wikipedia.org/wiki?curid=16773926", "title": "Achelous (crater)", "text": "Achelous (crater)\n\nAchelous is a relatively fresh crater on Ganymede adjacent to the similarly sized Gula. It has an outer lobate ejecta deposit extending about a crater radius from the rim. \n\nA characteristic feature of both craters, almost identical in size, is the \"pedestal\" - an outward-facing, relatively gently sloped scarp that terminates the continuous ejecta blanket. Similar features may be seen in ejecta blankets of Martian craters, suggesting impacts into a volatile (ice)-rich target material. Furthermore, both craters appear crisp and feature terraces. Gula has a prominent central peak; Achelous instead may show the remnant of a collapsed central peak or a central pit that is not fully formed. On lower-resolution images taken under higher sun illumination angle, both craters are shown to have extended bright rays, especially Achelous, which demonstrates that these two craters are younger than the respective surrounding landscape.\n"}
{"id": "15637220", "url": "https://en.wikipedia.org/wiki?curid=15637220", "title": "Alexander Kirilow Drenowski", "text": "Alexander Kirilow Drenowski\n\nAlexander Kirilow Drenowski (22 July 1879, Ruse – 24 April 1967, Sofia) was a Bulgarian entomologist who specialised in Lepidoptera.\n\nBetween 1904 and 1953 Drenowski wrote 77 scientific papers on the butterflies of Bulgaria.His most significant work was on the butterflies of the Bulgarian high mountains Rila, Pirin, Rhodope and Stara Planina.\n\n"}
{"id": "22951942", "url": "https://en.wikipedia.org/wiki?curid=22951942", "title": "Bacolor (crater)", "text": "Bacolor (crater)\n\nBacolor Crater is a crater in the Casius quadrangle of Mars, located at 33 North and 241.4 West. It is 20.8 km in diameter and was named after a town in the Philippines.\n\n"}
{"id": "24568072", "url": "https://en.wikipedia.org/wiki?curid=24568072", "title": "Biophysical chemistry", "text": "Biophysical chemistry\n\nBiophysical chemistry is a physical science that uses the concepts of physics and physical chemistry for the study of biological systems. The most common feature of the research in this subject is to seek explanation of the various phenomena in biological systems in terms of either the molecules that make up the system or the supra-molecular structure of these systems.\n\nBiophysical chemists employ various techniques used in physical chemistry to probe the structure of biological systems. These techniques include spectroscopic methods such as nuclear magnetic resonance (NMR) and X-ray diffraction. For example, the work for which Nobel Prize was awarded in 2009 to three chemists was based on X-ray diffraction studies of ribosomes. Some of the areas in which biophysical chemists engage themselves are protein structure and the functional structure of cell membranes. For example, enzyme action can be explained in terms of the shape of a pocket in the protein molecule that matches the shape of the substrate molecule or its modification due to binding of a metal ion. Similarly the structure and function of the biomembranes may be understood through the study of model supramolecular structures as liposomes or phospholipid vesicles of different compositions and sizes.\n\nThe oldest reputed institute for biophysical chemistry is the Max Planck Institute for Biophysical Chemistry in Göttingen.\n\nBiophysical chemistry journals include \"Biophysical Journal\", \"Archives of Biochemistry and Biophysics\" (published by Academic Press), \"Biochemical and Biophysical Research Communications\" (Academic Press), \"Biochimica et Biophysica Acta\" (Elsevier Science), \"Biophysical Chemistry\", \"An International Journal devoted to the Physics and Chemistry of Biological Phenomena\" (Elsevier), \"Journal of Biochemical and Biophysical Methods\" (Elsevier), \"Journal of Biochemistry\", \"Biology and Biophysics\" (Taylor & Francis), and \"Journal de Chimie Physique\", \"Physico-Chimie Biologique\" (EDP Sciences and the Société Française de Chimie).\n\n\n"}
{"id": "23520318", "url": "https://en.wikipedia.org/wiki?curid=23520318", "title": "Bradyspory", "text": "Bradyspory\n\nBradyspory is the gradual release of seed from a cone or fruit over a long period of time, as opposed to tachyspory, the more-or-less immediate release of seed as soon as they have matured.\n\nBradyspory may occur because seed release is spontaneous but very gradual, or because seed release does not occur until triggered to do so by some environmental event. The latter case is termed serotiny.\n"}
{"id": "12970633", "url": "https://en.wikipedia.org/wiki?curid=12970633", "title": "Cape Town Science Centre", "text": "Cape Town Science Centre\n\nThe Cape Town Science Centre is a not-for-profit science centre in Cape Town, South Africa. It forms part of a wide range of non-classroom initiatives to improve the quality of science understanding and science literacy in South Africa.\n\nUntil early 2010, the MTN Sciencentre was located in the Canal Walk shopping mall. It will reopen in Observatory later in 2011.\n\nThe MTN Sciencentre's Ericsson cell phone is in the Guinness Book of World Records as the world's largest working cell phone.\n"}
{"id": "5060884", "url": "https://en.wikipedia.org/wiki?curid=5060884", "title": "Cash-flow diagram", "text": "Cash-flow diagram\n\nA cash-flow diagram is a tool used by accountants and engineers, to represent the transactions of cash which will take place over the course of a given project. Transactions can include initial investments, maintenance costs, projected earnings or savings resulting from the project, as well as salvage and resale value of equipment at the end of the project. Cash flow diagrams may also be used to represent payment schedules for bonds, mortgages and other types of loans.It is often used with a break even sheet and a balance sheet in order to see where money is being made or lost. Also it is used to monitor results of different things. For instance when changing variables it will see if they are successful or not in causing future profit.\n"}
{"id": "767937", "url": "https://en.wikipedia.org/wiki?curid=767937", "title": "Consensus sequence", "text": "Consensus sequence\n\nIn molecular biology and bioinformatics, the consensus sequence (or canonical sequence) is the calculated order of most frequent residues, either nucleotide or amino acid, found at each position in a sequence alignment. It represents the results of multiple sequence alignments in which related sequences are compared to each other and similar sequence motifs are calculated. Such information is important when considering sequence-dependent enzymes such as RNA polymerase.\n\nA protein binding site, represented by a consensus sequence, may be a short sequence of nucleotides which is found several times in the genome and is thought to play the same role in its different locations. For example, many transcription factors recognize particular patterns in the promoters of the genes they regulate. In the same way, restriction enzymes usually have palindromic consensus sequences, usually corresponding to the site where they cut the DNA. Transposons act in much the same manner in their identification of target sequences for transposition. Finally, splice sites (sequences immediately surrounding the exon-intron boundaries) can also be considered as consensus sequences.\n\nThus a consensus sequence is a model for a putative DNA binding site: it is obtained by aligning all known examples of a certain recognition site and defined as the idealized sequence that represents the predominant base at each position. All the actual examples shouldn't differ from the consensus by more than a few substitutions, but counting mismatches in this way can lead to inconsistencies.\n\nAny mutation allowing a mutated nucleotide in the core promoter sequence to look more like the consensus sequence is known as an up mutation. This kind of mutation will generally make the promoter stronger, and thus the RNA polymerase forms a tighter bind to the DNA it wishes to transcribe and transcription is up-regulated. On the contrary, mutations that destroy conserved nucleotides in the consensus sequence are known as down mutations. These types of mutations down-regulate transcription since RNA polymerase can no longer bind as tightly to the core promoter sequence.\n\nDeveloping software for pattern recognition is a major topic in genetics, molecular biology, and bioinformatics. Specific sequence motifs can function as regulatory sequences controlling biosynthesis, or as signal sequences that direct a molecule to a specific site within the cell or regulate its maturation. Since the regulatory function of these sequences is important, they are thought to be conserved across long periods of evolution. In some cases, evolutionary relatedness can be estimated by the amount of conservation of these sites.\n\nThe conserved sequence motifs are called consensus sequences and they show which residues are conserved and which residues are variable. Consider the following example DNA sequence:\n\nIn this notation, A means that an A is always found in that position; [CT] stands for either C or T; N stands for any base; and {A} means any base except A. Y represents any pyrimidine, and R indicates any purine.\n\nIn this example, the notation [CT] does not give any indication of the relative frequency of C or T occurring at that position. An alternative method of representing a consensus sequence uses a sequence logo. This is a graphical representation of the consensus sequence, in which the size of a symbol is related to the frequency that a given nucleotide (or amino acid) occurs at a certain position. In sequence logos the more conserved the residue, the larger the symbol for that residue is drawn; the less frequent, the smaller the symbol. Sequence logos can be generated using WebLogo, or using the Gestalt Workbench, a publicly available visualization tool written by Gustavo Glusman at the Institute for Systems Biology.\n\nBioinformatics tools are able to calculate and visualize consensus sequences. Examples of the tools are JalView and UGENE.\n\n"}
{"id": "1751466", "url": "https://en.wikipedia.org/wiki?curid=1751466", "title": "Differential thermal analysis", "text": "Differential thermal analysis\n\nDifferential thermal analysis (or DTA) is a thermoanalytic technique that is similar to differential scanning calorimetry. In DTA, the material under study and an inert reference are made to undergo identical thermal cycles, (i.e., same cooling or heating programme) while recording any temperature difference between sample and reference. This differential temperature is then plotted against time, or against temperature (DTA curve, or thermogram). Changes in the sample, either exothermic or endothermic, can be detected relative to the inert reference. Thus, a DTA curve provides data on the transformations that have occurred, such as glass transitions, crystallization, melting and sublimation. The area under a DTA peak is the enthalpy change and is not affected by the heat capacity of the sample.\n\nA DTA consists of a sample holder, thermocouples, sample containers and a ceramic or metallic block; a furnace; a temperature programmer; and a recording system. The key feature is the existence of two thermocouples connected to a voltmeter. One thermocouple is placed in an inert material such as AlO, while the other is placed in a sample of the material under study. As the temperature is increased, there will be a brief deflection of the voltmeter if the sample is undergoing a phase transition. This occurs because the input of heat will raise the temperature of the inert substance, but be incorporated as latent heat in the material changing phase.\n\nIn today's market most manufacturers no longer make true DTA systems but rather have incorporated this technology into thermogravimetric analysis (TGA) systems, which provide both mass loss and thermal information. With today's advancements in software, even these instruments are being replaced by true TGA-DSC instruments that can provide the temperature and heat flow of the sample, simultaneously with mass loss. \n\nA DTA curve can be used only as a \"finger print\" for identification purposes but usually the applications of this method are the determination of phase diagrams, heat change measurements and decomposition in various atmospheres.\n\nDTA is widely used in the pharmaceutical and food industries.\n\nDTA may be used in cement chemistry, mineralogical research and in environmental studies.\n\nDTA curves may also be used to date bone remains or to study archaeological materials.\nUsing DTA one can obtain liquidus & solidus lines of phase diagrams.\n"}
{"id": "8617294", "url": "https://en.wikipedia.org/wiki?curid=8617294", "title": "EASE/ACCESS", "text": "EASE/ACCESS\n\nThe Experimental Assembly of Structures in EVA and the Assembly Concept for Construction of Erectable Space Structures, or EASE/ACCESS, were a pair of space shuttle flight experiments that were performed on STS-61-B, on November 29 and December 1, 1985. The purpose of the experiments was to study how quickly astronauts would become proficient at assembling space structures during extravehicular activity, and how quickly they would become fatigued, and to explore various construction and maintenance techniques. In particular, researchers studied the applied moments of inertia arising in the manual assembly of a large space structure.\n\nEASE was a project of NASA's Marshall Space Flight Center and the Space Systems Laboratory at the Massachusetts Institute of Technology (later at the University of Maryland), while ACCESS was developed by NASA's Langley Research Center.\n\nAstronauts Jerry L. Ross and Sherwood C. Spring repeatedly assembled a tetrahedral truss (EASE) and a triangular column truss (ACCESS) during two extra-vehicular activities (EVAs). The first EVA was devoted to studying human performance in assembly techniques, while the second was dedicated to supplementary experiments, including alternative construction techniques and maintenance scenarios.\n\nThe EASE structure consisted of six identical aluminum beams, each long and with a mass of , connected by four nodal joints. ACCESS consisted of 93 tubular aluminum struts, each in diameter—thirty-three struts, and sixty struts—connected by thirty-three nodal joints. While assembling the EASE structure, the astronauts moved about the structure under their own power. For the assembly of the ACCESS structure, the astronauts were secured to a mobile platform on the Remote Manipulator System, which was guided by astronaut Mary L. Cleave.\n\nA stereoscopic camera system recorded the movements of the structural beams during assembly. Taking into account the effects of inertia, drag, and virtual mass, researchers used this data to reconstruct the applied moments of inertia. The structure was also assembled in neutral buoyancy simulation, and the two environments were compared. The EVAs were also recorded by an IMAX camera mounted in the shuttle cargo bay.\n\nApplied moments of inertia during EVA were found to be on the order of . In neutral buoyancy simulation, the applied moments of inertia were around five times greater than those during EVA. Assembly time during EVA was around 20% less than in neutral buoyancy simulation. The learning curve was on the order of 78%, and was unaffected by the strength, coordination, or size of the astronaut, or the fit of the space suit. In both environments, moments of inertia were applied as short impulses, interspersed by several seconds of coasting.\n\nThe EASE/ACCESS experiments were deemed to be successful. The information gathered provided a basis for planning future manually assembled space structures, and in the process NASA accrued valuable EVA assembly experience. The team responsible for the EASE project was awarded a NASA Group Achievement Award.\n\n\n"}
{"id": "5357979", "url": "https://en.wikipedia.org/wiki?curid=5357979", "title": "Ectromelia virus", "text": "Ectromelia virus\n\nEctromelia virus (ECTV) is a virus of the family \"Poxviridae\" and the genus \"Orthopoxvirus\" that causes mousepox, a disease of mice. It has only been seen in mouse colonies kept for research purposes. Mousepox causes skin lesions and generalized disease, which can be fatal. It is the only poxvirus to cause disease naturally in mice.\n"}
{"id": "1531739", "url": "https://en.wikipedia.org/wiki?curid=1531739", "title": "Electric form factor", "text": "Electric form factor\n\nThe electric form factor is the Fourier transform of electric charge distribution in space.\n\nThe idea originated from young William Thomson. \n\n"}
{"id": "740540", "url": "https://en.wikipedia.org/wiki?curid=740540", "title": "Engineering physics", "text": "Engineering physics\n\nEngineering physics or engineering science refers to the study of the combined disciplines of physics, mathematics and engineering, particularly computer, nuclear, electrical, electronic, materials or mechanical engineering. By focusing on the scientific method as a rigorous basis, it seeks ways to apply, design, and develop new solutions in engineering.\n\nUnlike traditional engineering disciplines, engineering science/physics is not necessarily confined to a particular branch of science, engineering or physics. Instead, engineering science/physics is meant to provide a more thorough grounding in applied physics for a selected specialty such as optics, quantum physics, materials science, applied mechanics, electronics, nanotechnology, microfabrication, microelectronics, computing, photonics, mechanical engineering, electrical engineering, nuclear engineering, biophysics, control theory, aerodynamics, energy, solid-state physics, etc. It is the discipline devoted to creating and optimizing engineering solutions through enhanced understanding and integrated application of mathematical, scientific, statistical, and engineering principles. The discipline is also meant for cross-functionality and bridges the gap between theoretical science and practical engineering with emphasis in research and development, design, and analysis.\n\nIt is notable that in many languages the term for \"engineering physics\" would be directly translated into English as \"technical physics\". In some countries, both what would be translated as \"engineering physics\" and what would be translated as \"technical physics\" are disciplines leading to academic degrees, with the former specializing in nuclear power research, and the latter closer to engineering physics. In some institutions, an engineering (or applied) physics major is a discipline or specialization within the scope of engineering science, or applied science.\n\nIn many universities, engineering science programs may be offered at the levels of B.Tech, B.Sc., M.Sc. and Ph.D. Usually, a core of basic and advanced courses in mathematics, physics, chemistry, and biology forms the foundation of the curriculum, while typical elective areas may include fluid dynamics, quantum physics, economics, plasma physics, relativity, solid mechanics, operations research, quantitative finance, information technology and engineering, dynamical systems, bioengineering, environmental engineering, computational engineering, engineering mathematics and statistics, solid-state devices, materials science, electromagnetism, nanoscience, nanotechnology, energy, and optics. While typical undergraduate engineering programs generally focus on the application of established methods to the design and analysis of engineering solutions, undergraduate program in engineering science focuses on the creation and use of more advanced experimental or computational techniques where standard approaches are inadequate (i.e., development of engineering solutions to contemporary problems in the physical and life sciences by applying fundamental principles).\n\nQualified Engineering Physicists, with a degree in Engineering Physics, can work professionally as Engineers and/or Physicists in the high technology industries and beyond, becoming domain experts in multiple engineering and scientific fields.\n\n\n"}
{"id": "36625495", "url": "https://en.wikipedia.org/wiki?curid=36625495", "title": "Femto-photography", "text": "Femto-photography\n\nFemto-photography is a technique for recording the propagation of ultrashort pulses of light through a scene at a very high speed. A femto-photograph is equivalent to an optical impulse response of a scene and has also been denoted by terms such as a light-in-flight recording or transient image. Femto-photography of macroscopic objects was first demonstrated using a holographic process in the 1970s by Nils Abramsson at the Royal Institute of Technology (Sweden). A research team at the MIT Media Lab led by Ramesh Raskar, together with contributors from the Graphics and Imaging Lab at the Universidad de Zaragoza, Spain, more recently achieved a significant increase in image quality using a streak camera synchronized to a pulsed laser and modified to obtain 2D images instead of just a single scanline.\n\nIn their publications, Raskar's team claims to be able to capture exposures so short that light only traverses 0.6 mm (corresponding to 2 picoseconds, or 2 seconds) during the exposure period, a figure that is in agreement with the nominal resolution of the Hamamatsu streak camera model C5680, on which their experimental setup is based. Recordings taken using the setup have reached significant spread in the mainstream media, including a presentation by Raskar at TEDGlobal 2012. Furthermore, the team was able to demonstrate the reconstruction of unknown objects \"around corners\", i.e., outside the line of sight of light source and camera, from femto-photographs.\n\nIn 2013, researchers at the University of British Columbia demonstrated a computational technique that allows the extraction of transient images from time-of-flight sensor data without the need for ultrafast light sources or detectors.\n\nPrior to the aforementioned work, the term \"femto-photography\" had been used for certain proposed procedures in experimental nuclear physics.\n"}
{"id": "28336076", "url": "https://en.wikipedia.org/wiki?curid=28336076", "title": "GeneTree", "text": "GeneTree\n\nGeneTree was a family history website focused on using DNA testing to trace ancestry. A website account was free, and within their account users could order DNA tests, enter results from other testing companies, search the DNA database, create an online family tree, and correspond with family members – including sharing pictures.\n\nGenetree.com was closed on January 1, 2013 and its assets were transferred to Ancestry.com.\n\nGeneTree was founded in 1997 by a graduate of Wayne State University’s Center for Molecular Medicine and Genetics, Terrence Carmichael, who earned a master’s degree in molecular biology and genetics in 1995. Genetree did not offer multi-generational genealogy tests initially. Carmichael declared, “Over 95 percent of our first-year business was in paternity testing.” In fall 2001, GeneTree sold its assets to Salt Lake City-based Sorenson Molecular Genealogy Foundation (\"SMGF\") which originated in 1999.\n\nThe GeneTree website brought together elements of several existing Sorenson Companies: DNA testing by Sorenson Genomics, the DNA-genealogy database established by the SMGF research project, and the media sharing abilities of Sorenson Media.\n\nIn 2012, Ancestry.com announced to its customers, \"In March, Ancestry.com DNA, LLC acquired access to an extensive collection of DNA assets from SMGF.\" Genetree.com was discontinued on January 1, 2013 and access to family trees was no longer made available through the GeneTree site. Prior to that date, Ancestry.com offered their subscribers who were former GeneTree customers an opportunity to download their DNA test results and manually enter them at Ancestry.com. Beginning January 1, 2013, those who don't have a current subscription to Ancestry.com no longer had access to their GeneTree DNA results.\n\nGeneTree offered Y chromosome DNA testing (males only) that gives information about paternal ancestry, including 33-marker and a 46-marker test. YDNA is passed from father to son, so males with the same YDNA often have the same surname in addition to sharing a male ancestor.\n\nGeneTree offered mitochondrial DNA testing (males and females) that gives information about maternal ancestry. GeneTree's tests look at HVR1, HVR2, and HVR3 of the mtDNA. mtDNA is passed from mother to children, so people with the same mtDNA share a female ancestor.\n\nIn addition to DNA testing services, GeneTree sold consultation services to help customers further understand their test results.\n\n\n"}
{"id": "15218575", "url": "https://en.wikipedia.org/wiki?curid=15218575", "title": "Gordon Aylward", "text": "Gordon Aylward\n\nGordon Hillis Aylward is an Australian chemical author. He is known for writing the \"SI Chemical Data\" book.\n\nAylward graduated on 20 May 1952 with a BSc (Honours) in Applied Chemistry from the then-new University of New South Wales in Sydney, Australia. Later he received a MSc from the same university, and continued to teach Analytical Chemistry for 13 years there. During that period he organized the Approach to Chemistry summer schools, together with his co-teacher dr Tristan Findlay. To support the course, they wrote the book \"SI Chemical Data\" as the textbook.\n\nLater Aylward joined Macquarie University as Associate Professor and worked from 1970 till his retirement in 2005 in developing countries as a Science Education consultant for UNESCO, then for the World Bank and finally as a freelance Senior Science Education Advisor.\n\n"}
{"id": "51990624", "url": "https://en.wikipedia.org/wiki?curid=51990624", "title": "H band (infrared)", "text": "H band (infrared)\n\nIn infrared astronomy, the H band refers to an atmospheric transmission window centred on 1.65 micrometres with a Full width at half maximum of 0.35 micrometres\n(in the near-infrared).\n"}
{"id": "19254435", "url": "https://en.wikipedia.org/wiki?curid=19254435", "title": "Index of urban sociology articles", "text": "Index of urban sociology articles\n\nUrban sociology is the sociological study of social life and human interaction in metropolitan areas. It is a normative discipline of sociology seeking to study the structures, processes, changes and problems of an urban area and by doing so providing inputs for planning and policy making.\n\nabandonment — accessibility — Active Living — activity centre — adaptive reuse — Administration for Children and Families — Acid Rain Program(EPA) — achievement gap in the United States — affirmative action — African American — Aid to Families with Dependent Children(AFDC) — air quality(indoor) — Air Pollution Index — Air Quality Index — alienation — amalgamation — annexation — anomie — arcology — arson — asset-based community development — Asian American — Athens Charter — automobile — automobile dependency — autonomy\n\nbureaucracy — birth rate — block grant — budget — bus — business cycle — business park\n\ncapitalism — capital improvement plan — carpool — carsharing — central business district — central place theory — charter school — City Beautiful movement — City of Light Development — city rhythm — civil rights — class stratification — clean air act — communal garden — Communities Directory — community development — community land trust — community of place — Community Reinvestment Act — commuting — complete streets — concentric zone model — conservation easement — Context Sensitive Solutions — context theory — Copenhagenization (bicycling) — core frame model — corporation — cost of living(U.S.) — counter urbanization — crime — criminal justice — cultural bias — culture of poverty\n-The Coons Effect\n\nde facto segregation — de jure segregation — death rate — decentralization — devolution — disability — disinvestment — division of labour\n\neconomic development — economic growth — elitism — emission standard — employment — empowerment zone — enterprise zone — entertainment center — entrepôt — ethnic enclave\n\nFederal Housing Administration — FHA loan — fragmentation\n\ngang — gentrification — globalization — government — great depression — gridlock — growth management\n\nhabitability — highway — Hispanic Americans — historic preservation — Home Mortgage Disclosure Act — homelessness — homeowners' association — Housing Act of 1937 — Housing Act of 1949 — Housing and Economic Recovery Act of 2008 — HOPE VI — human ecology — Department of Housing and Urban Development(H.U.D.) — hyperghettoization\n\nimmigration — inclusionary zoning — income — indoor air pollution in developing nations — industrial ecology — industrialization — inequality — infrastructure — interest group\n\nkinship\n\nland use — landfill — leapfrogging\n\nmagnet school — methanol — middle class — migration — modernization — Moving to Opportunity — multiple nuclei model\n\nNational Ambient Air Quality Standards — neighborhood — Neo-Marxism — nuclear family\n\norganized crime — overcrowding\n\nparochialism — Personal Responsibility and Work Opportunity Act — Phase I Environmental Site Assessment — polarization — police brutality — pollution — poverty — poverty line — privatization — public transport — psychological stress — public housing — public school — public transport\n\nracial discrimination — racial integration — racism — rail system — recycling — regime theory — revenue sharing — rural\n\nsavings and loan crisis — scholarship — segregation — single parent — smart growth — social complexity — social disorganization theory — social housing — social solidarity — social work — social welfare provision — Socialism — solidarity — Soviet Union — steam engine — streetcar — street children — suburbanization — suburb — sun belt\n\ntaxes — technology — \"Times Square Red, Times Square Blue\" —TANF — third world\n\nunderemployment — underground economy — unemployment — Uniform Crime Report — unionization — urban decay — Urban Mass Transportation Act of 1964 — urban renewal — urban sprawl — urbanization\n\nVice Lords — violence — volunteer — voting bloc\n\nWage — war on poverty — waste disposal — water supply — welfare — welfare reform — white flight — white collar crime — workfare\n\nxenophobia\n\nzoning\n\nList of United States cities by population\n\nList of U.S. metropolitan areas with large African-American populations\n\nList of air-filtering plants\n\nFlanagan, William G. (2001). \"Urban Sociology : Images and Structure\", Prentice Hall, \nKeiser, R. Lincoln. (1969). \"The Vice Lords: Warriors of the Streets\", Holt, Rinehart and Winston, \n\nShannon, Thomas R. (2001). \"Urban Problems in Sociological Perspective\", Waveland Press Inc, \n\nSpradley, James P. (1999). \"You Owe Yourself a Drunk: An Ethnography of Urban Nomad\", Waveland Press Inc, \n\nVargas, Joao H. Costa. (2006). \"Catching Hell in the City of Angels: Life And Meanings of Blackness in South Central Los Angeles\", University of Minnesota Press, \n\nWilliams, Terry. (1992). \"Crackhouse: Notes from the End of the Line\", Penguin Group(USA), \n"}
{"id": "223898", "url": "https://en.wikipedia.org/wiki?curid=223898", "title": "Industrial ecology", "text": "Industrial ecology\n\nIndustrial ecology (IE) is the study of material and energy flows through industrial systems. The global industrial economy can be modelled as a network of industrial processes that extract resources from the Earth and transform those resources into commodities which can be bought and sold to meet the needs of humanity. Industrial ecology seeks to quantify the material flows and document the industrial processes that make modern society function. Industrial ecologists are often concerned with the impacts that industrial activities have on the environment, with use of the planet's supply of natural resources, and with problems of waste disposal. Industrial ecology is a young but growing multidisciplinary field of research which combines aspects of engineering, economics, sociology, toxicology and the natural sciences.\n\nIndustrial ecology has been defined as a \"systems-based, multidisciplinary discourse that seeks to understand emergent behaviour of complex integrated human/natural systems\". The field approaches issues of sustainability by examining problems from multiple perspectives, usually involving aspects of sociology, the environment, economy and technology. The name comes from the idea that the analogy of natural systems should be used as an aid in understanding how to design sustainable industrial systems.\n\nIndustrial ecology is concerned with the shifting of industrial process from linear (open loop) systems, in which resource and capital investments move through the system to become waste, to a closed loop system where wastes can become inputs for new processes.\n\nMuch of the research focuses on the following areas:\n\nIndustrial ecology seeks to understand the way in which industrial systems (for example a factory, an ecoregion, or national or global economy) interact with the biosphere. Natural ecosystems provide a metaphor for understanding how different parts of industrial systems interact with one another, in an \"ecosystem\" based on resources and infrastructural capital rather than on natural capital. It seeks to exploit the idea that natural systems do not have waste in them to inspire sustainable design.\n\nAlong with more general energy conservation and material conservation goals, and redefining commodity markets and product stewardship relations strictly as a service economy, industrial ecology is one of the four objectives of Natural Capitalism. This strategy discourages forms of amoral purchasing arising from ignorance of what goes on at a distance and implies a political economy that values natural capital highly and relies on more instructional capital to design and maintain each unique industrial ecology.\n\nIndustrial ecology was popularized in 1989 in a \"Scientific American\" article by Robert Frosch and Nicholas E. Gallopoulos. Frosch and Gallopoulos' vision was \"why would not our industrial system behave like an ecosystem, where the wastes of a species may be resource to another species? Why would not the outputs of an industry be the inputs of another, thus reducing use of raw materials, pollution, and saving on waste treatment?\" A notable example resides in a Danish industrial park in the city of Kalundborg. Here several linkages of byproducts and waste heat can be found between numerous entities such as a large power plant, an oil refinery, a pharmaceutical plant, a plasterboard factory, an enzyme manufacturer, a waste company and the city itself. Another example is the Rantasalmi EIP in Rantasalmi, Finland. While this country has had previous organically formed EIP's, the park at Rantasalmi is Finland's first planned EIP.\n\nThe scientific field Industrial Ecology has grown quickly in recent years. The Journal of Industrial Ecology (since 1997), the International Society for Industrial Ecology (since 2001), and the journal Progress in Industrial Ecology (since 2004) give Industrial Ecology a strong and dynamic position in the international scientific community. Industrial Ecology principles are also emerging in various policy realms such as the concept of the Circular Economy that is being promoted in China. Although the definition of the Circular Economy has yet to be formalized, generally the focus is on strategies such as creating a circular flow of materials, and cascading energy flows. An example of this would be using waste heat from one process to run another process that requires a lower temperature. The hope is that strategy such as this will create a more efficient economy with fewer pollutants and other unwanted by-products.\n\nOne of the central principles of Industrial Ecology is the view that societal and technological systems are bounded within the biosphere, and do not exist outside it. Ecology is used as a \"metaphor\" due to the observation that natural systems reuse materials and have a largely closed loop cycling of nutrients. Industrial Ecology approaches problems with the hypothesis that by using similar principles as \"natural systems, industrial systems\" can be improved to reduce their impact on the natural environment as well. The table shows the general metaphor.\nIE examines societal issues and their relationship with both technical systems and the environment. Through this \"holistic view \", IE recognizes that solving problems must involve understanding the connections that exist between these systems, various aspects cannot be viewed in isolation. Often changes in one part of the overall system can propagate and cause changes in another part. Thus, you can only understand a problem if you look at its parts in relation to the whole. Based on this framework, IE looks at environmental issues with a \"systems thinking\" approach. A good IE example with these societal impacts can be found at the Blue Lagoon in Iceland. The Lagoon uses super-heated water from a local geothermal power plant to fill mineral-rich basins that have become recreational healing centers. In this sense the industrial process of energy production uses its wastewater to provide a crucial resource for the dependent recreational industry.\n\nTake a city for instance. A city can be divided into commercial areas, residential areas, offices, services, infrastructures, and so forth. These are all sub-systems of the 'big city’ system. Problems can emerge in one sub-system, but the solution has to be global. Let’s say the price of housing is rising dramatically because there is too high a demand for housing. One solution would be to build new houses, but this will lead to more people living in the city, leading to the need for more infrastructure like roads, schools, more supermarkets, etc. This system is a simplified interpretation of reality whose behaviors can be ‘predicted’.\n\nIn many cases, the systems IE deals with are complex systems. Complexity makes it difficult to understand the behavior of the system and may lead to rebound effects. Due to unforeseen behavioral change of users or consumers, a measure taken to improve environmental performance does not lead to any improvement or may even worsen the situation.\n\nMoreover, \"life cycle thinking\" is also a very important principle in industrial ecology. It implies that all environmental impacts caused by a product, system, or project during its life cycle are taken into account. In this context life cycle includes\nThe transport necessary between these stages is also taken into account as well as, if relevant, extra stages such as reuse, remanufacture, and recycle.\nAdopting a life cycle approach is essential to avoid shifting environmental impacts from one life cycle stage to another. This is commonly referred to as problem shifting. For instance, during the re-design of a product, one can choose to reduce its weight, thereby decreasing use of resources. However, it is possible that the lighter materials used in the new product will be more difficult to dispose of. The environmental impacts of the product gained during the extraction phase are shifted to the disposal phase. Overall environmental improvements are thus null.\n\nA final important principle of IE is its \"integrated approach\" or \"multidisciplinarity\". IE takes into account three different disciplines: social sciences (including economics), technical sciences and environmental sciences. The challenge is to merge them into a single approach.\n\nThe Kalundborg industrial park is located in Denmark. This industrial park is special because companies reuse each other's waste (which then becomes by-products). For example, the Energy E2 Asnæs Power Station produces gypsum as a by-product of the electricity generation process; this gypsum becomes a resource for the BPB Gyproc A/S which produces plasterboards. This is one example of a system inspired by the biosphere-technosphere metaphor: in ecosystems, the waste from one organism is used as inputs to other organisms; in industrial systems, waste from a company is used as a resource by others.\n\nApart from the direct benefit of incorporating waste into the loop, the use of an eco-industrial park can be a means of making renewable energy generating plants, like Solar PV, more economical and environmentally friendly. In essence, this assists the growth of the renewable energy industry and the environmental benefits that come with replacing fossil-fuels.\n\nAdditional examples of industrial ecology include:\n\nThe ecosystem metaphor popularized by Frosch and Gallopoulos has been a valuable creative tool for helping researchers look for novel solutions to difficult problems. Recently, it has been pointed out that this metaphor is based largely on a model of classical ecology, and that advancements in understanding ecology based on complexity science have been made by researchers such as C. S. Holling, James J. Kay, and further advanced in terms of contemporary ecology by others. For industrial ecology, this may mean a shift from a more mechanistic view of systems, to one where sustainability is viewed as an emergent property of a complex system. To explore this further, several researchers are working with agent based modeling techniques\n\nExergy analysis is performed in the field of industrial ecology to use energy more efficiently. The term \"exergy\" was coined by Zoran Rant in 1956, but the concept was developed by J. Willard Gibbs. In recent decades, utilization of exergy has spread outside physics and engineering to the fields of industrial ecology, ecological economics, systems ecology, and energetics.\n\nAnother great example of industrial ecology both in practice and in potential is the Burnside Cleaner Production Centre in Burnside, Nova Scotia. They play a role in facilitating the 'greening' of over 1200 businesses that are located in Burnside, Eastern Canada's largest industrial park. The creation of waste exchange is a big part of what they work towards, which will promote strong industrial ecology relationships.\n\n\n\n\n\n"}
{"id": "52922331", "url": "https://en.wikipedia.org/wiki?curid=52922331", "title": "John Akeroyd", "text": "John Akeroyd\n\nJohn Robert Akeroyd (1952–) is a British botanist.\n\nEducated at St. Andrew's University, he proceeded to Cambridge University for his doctorate on the ecological genetics of weeds. His post-doctoral work was at Trinity College, Dublin (1979–1981), and then at the University of Reading, Plant Sciences Department as a post-doctoral fellow (1981–1999). At Reading he worked on the \"Flora Europaea\". He succeeded William Stearn as editor of the \"Annales Musei Goulandris\" in 1999, and also editor of \"Watsonia\". He has served as a vice-president of the Botanical Society of Britain and Ireland. there he was referee for Polygonaceae, and also served on the Meetings and Publications Committee. He frequently contributes to popular articles on conservation and botany and co-founded \"Plant Talk\" conservation magazine associated with the Eden Project. He is also known for his culinary skills.\n\nHe collected plants from the Mediterranean and Ireland, an contributed to the herbarium at Reading. John Akeroyd is the botanical authority, for a nearly twenty taxa, such as \"Arenaria serpyllifolia\" L. subsp. \"aegaea\" (Rech.f.) Akeroyd. He was elected a Fellow of the Linnean Society (FLS) in 1982.\n\n\n"}
{"id": "40319830", "url": "https://en.wikipedia.org/wiki?curid=40319830", "title": "Journal of Natural Philosophy, Chemistry, and the Arts", "text": "Journal of Natural Philosophy, Chemistry, and the Arts\n\nThe A Journal of Natural Philosophy, Chemistry, and the Arts, generally known as \"Nicholson's Journal\", was the first monthly scientific journal in Great Britain. William Nicholson began it in 1797 and was the editor until it merged with another journal at the end of 1813.\n\nNicholson's journal would accept short papers, written by new or anonymous authors, and decide whether to publish them relatively quickly. These attributes distinguished the new journal from the established scientific journal \"Philosophical Transactions of the Royal Society\". By one account this less-formal model was so appealing that the next year a similar startup launched, Alexander Tilloch's \"Philosophical Magazine\".\n\n\nBy one account, William Nicholson started the journal and made all editorial decisions in a \"pioneering and uncertain attempt\" to make a living from publishing it. Revenues came only from subscriptions. Tilloch's \"Philosophical Magazine\" was more successful as a popular science journal business than Nicholson's journal, according to one source, and another such journal appeared in 1813 (\"Annals of Philosophy\"). Possibly partly because of this competition, William Nicholson ended the journal. By some accounts Nicholson's journal simply ceased, and by others it merged in 1814 with the \"Philosophical Magazine\" to form \"The Philosophical Magazine and Journal\".\n\nThe \"Advertisement\", dated 31 December 1813, at the start of Volume 42 of \"The Philosophical Magazine\" states:\n\n\"Nearly seventeen years have elapsed since \"The Philosophical Journal\" was commenced by Mr. Nicholson, and sixteen since the appearance of the first number of \"The Philosophical Magazine\". [...] [T]he result of [...] deliberations [between the publishers of \"Nicholson's Philosophical Journal\" and \"The Philosophical Magazine\" in order to respond to readers' complaints regarding duplication of material in the two publications] has been that it would certainly be best that we should unite, and that the joint product of our exertions and our correspondence should be consolidated in one periodical work. [...] The Philosophical Journal will henceforth be discontinued; and The Philosophical Magazine will be conducted by William Nicholson and Alexander Tilloch, in the same manner as it has always been carried on.\"\n\nFor the duration of Volume 43 (January to June 1814) the joint publishers of the new merged journal provided duplicate title-pages for each number, ostensibly so that subscribers to \"Nicholson's Philosophical Magazine\" might be enabled to \"preserve their Series without a chasm.\" However, despite their intention to continue this scheme of two-fold numeration, they abandoned it at the end of this trial period in June 1814, because of the perceived \"confusion and risque of many errors\" when referring to future volumes; from July 1814 a single numeration was used, following the numbering of \"The Philosophical Magazine.\"\n\nComplete journal issues have been scanned and are available online at the Biodiversity Heritage Library and at archive.org thanks to the Natural History Museum Library, London, the New York Public Library and google books.\n\n"}
{"id": "4240930", "url": "https://en.wikipedia.org/wiki?curid=4240930", "title": "Kenneth Oakley", "text": "Kenneth Oakley\n\nKenneth Page Oakley (7 April 1911 – 2 November 1981) was an English physical anthropologist, palaeontologist and geologist.\n\nOakley, known for his work in the relative dating of fossils by fluorine content, was instrumental in the exposure in the 1950s of the Piltdown Man hoax.\n\nOakley was born and died in Amersham, Buckinghamshire.\n\n"}
{"id": "20589206", "url": "https://en.wikipedia.org/wiki?curid=20589206", "title": "Krackhardt E/I Ratio", "text": "Krackhardt E/I Ratio\n\nThe Krackhardt E/I Ratio (or variously the E-I Index) is a social network measure which the relative density of internal connections within a social group compared to the number of connections that group has to the external world. It was so described in a 1988 paper by David Krackhardt and Robert N. Stern noting the increased effectiveness in moments of crisis of organizations which had stronger informal networks that crossed formal internal group structures.\n\nThe E/I ratio is related to the concept of conductance, which measures the likelihood that a random walk on a subgraph will exit that subgraph.\n\n\n"}
{"id": "5715611", "url": "https://en.wikipedia.org/wiki?curid=5715611", "title": "Las Cumbres Observatory", "text": "Las Cumbres Observatory\n\nLas Cumbres Observatory (LCO) is a network of astronomical observatories run by a non-profit private operating foundation directed by the technologist Wayne Rosing. Its offices are in Goleta, California. The telescopes are located at both northern and southern hemisphere sites distributed in longitude around the Earth. For some astronomical objects, the longitudinal spacing of telescopes allows continuous observations over 24 hours or longer. The operating network currently consists of two 2 meter telescopes, nine 1 meter telescopes, and seven 40 cm telescopes, placed at six astronomical observatories. The network operates as a single, integrated, observing facility, using a software scheduler that continuously optimizes the planned observing schedule of each individual telescope.\n\nRosing incorporated Las Cumbres Observatory in 1993 with the goal of aiding universities, observatories, and individuals in the acquisition and improvement of telescopes, optics, and instrumentation. He also set the objective for the organization to build and implement a global telescope system. In 2005, Rosing established the global telescope version of Las Cumbres Observatory.\n\nLCO initially acquired the two Faulkes 2 meter telescopes. Faulkes Telescope North (FTN) located at Haleakala Observatory, on Maui, Hawaii, and Faulkes Telescope South (FTS) at Siding Spring Observatory (SSO), in eastern Australia. LCO also purchased the company that built the Faulkes telescopes, Telescope Technologies Limited of Liverpool, with the intent of installing additional 2-meter telescopes at different sites to form a robotically operated network. Over the next few years, Rosing and the LCO staff came to understand that a network composed of many smaller telescopes would provide greater observing capacity. The organization designed its own 1 meter telescope with a plan to locate several of these at each chosen site. An even smaller 40 cm telescope was also developed primarily for use in education projects.\n\nDuring 2012 and 2013, nine 1 meter telescopes were constructed and deployed to McDonald Observatory at Fort Davis, Texas; Cerro Tololo Interamerican Observatory (CTIO) in Chile; South African Astronomical Observatory (SAAO), near Sutherland, South Africa; and SSO in Australia. During 2015 and 2016, seven 40 cm telescopes were deployed to CTIO, Haleakala Observatory, SSO, and to Teide Observatory on Tenerife in the Canary Islands.\n\nAfter completion of the construction and installation of these telescopes, LCO began its transition to operating a global observatory. In 2013, a Board of Directors was established and a President was hired to lead the organization. Full science scheduling began on 1 May 2014, with the two 2 meter and nine 1 meter telescopes operating as a single, integrated, observatory. The 40 cm telescopes were added to this system as they were commissioned.\n\nTwo agreements were completed in 2015 that will add to the scope of the LCO network. An agreement between LCO and the National Astronomical Observatories of the Chinese Academy of Sciences (NAOC) will result in the deployment of two additional 1-meter telescopes to Ali Observatory in western Tibet. An agreement between LCO and the Israeli I-CORE center will result in the integration of the 1 meter telescope at Wise Observatory into the LCO network.\n\nThe National Science Foundation made an award to LCO in 2016 through its Mid-Scale Innovations Program, purchasing access to the LCO network for all astronomers at U.S. institutions. The goal of this program is to prepare this community to carry out effective research following discoveries being made by current and future time domain astronomy surveys.\n\nLCO operates its network at six sites, with plans to add two more in the next few years. The operating sites are all professional astronomical observatories.\n\nIn the southern hemisphere:\n\nIn the northern hemisphere:\n\nThe Ali Observatory in western Tibet and Wise Observatory in Israel will be added as the telescopes there become operational.\n\nLCO also operates an identical 1 meter telescope at its headquarters in Goleta for engineering development and a 0.8 meter telescope at Sedgwick Reserve near Santa Ynez, California.\n\nThe 2 meter telescopes are the two Faulkes telescopes built by Telescope Technologies Ltd. They are f/10 Ritchey-Chrétien optical configurations on alt-az mounts.\n\nThe 1 meter telescopes are f/7.95 Ritchey-Chrétien optical systems on equatorial mounts. They have a 50 arcminute-diameter fully corrected field of view.\n\nThe telescopes use the optics and tubes from Meade 16 inch RCX telescopes. The mount has been replaced by a scaled-down version of the LCO 1 meter telescope mount.\n\nThe 2 meter telescopes are instrumented with optical imagers and low-resolution optical spectrographs.\n\nThe 1 meter telescopes are instrumented with “Sinistro” optical imagers that have a 26 arcminute square field of view. During 2017, a set of high-resolution (R = 50,000), high-stability spectrographs (NRES) are being deployed to four of the LCO sites to be coupled by optical fibers to the 1 meter telescopes.\n\nThe telescopes are equipped with SBIG STX-6303 optical imagers.\n\nThe global telescope network operates as a single observatory. Users request observations only for a generic class of telescope/instrument and the software scheduler determines an optimum observing schedule for each telescope. The scheduler revises the observing schedules for all telescopes as necessary and updates can be provided within 15 minutes. The rapid-response request mode bypasses the scheduler and can begin an observation within a few minutes after submission. Each telescope carries out a nightly calibration program and adjusts its pointing and focus several times per night.\n\nThe telescopes are all instrumented uniformly to facilitate the combining of data from multiple telescopes or sites. Data are returned to LCO headquarters, where they are processed to remove instrumental signature and ingested into an archive. Users have immediate access to their observations and all data are made public after 12 months.\n\nThe network is available to researchers at institutions that are members of the LCO science collaboration. Institutions that operate the sites hosting the LCO telescopes and a few institutions that have contributed resources to help build the network are members of the collaboration. The entire U.S. astronomical community gained access to the LCO network in 2016 as a result of an award from the National Science Foundation’s Mid-Scale Innovation Program. The program is administered through a peer-review proposal process run by the National Optical Astronomy Observatory. Several science teams and individuals also purchase time on the LCO network.\n\nThe design and operation of the LCO global telescope network provide the unique capabilities required for time domain astronomy. The LCO network offers the ability to observe objects or events continuously and the ability to obtain data rapidly upon the discovery or announcement of an event.\n\nThe LCO network has been used to study supernovae and other explosive transients; exoplanets, through observations of both transits and microlensing; asteroids; and AGN variability. In 2017 LCO played a critical part in two major discoveries: first visible counterpart of a gravitational wave event, and a new type of supernova with successive explosions.\n\nSince the beginning of LCO, education has been one of its core missions. In 2017, for the first time in LCO's history it issued an open call for education partners—groups who could use their robotic telescopes to inspire diverse audiences with educational and outreach projects that they support. In 2018 there are 17 LCO education partners based in the USA, Europe, Sub-Saharan Africa, the Middle East, Australia, or running entirely online programs, for students, teachers, and the wider public.\n\nThe LCO education team also maintains in-house educational programs to trigger observations and make use of data from the LCO network. These programs are designed to inspire anyone with an interest in astronomy to explore science using robotic telescopes. Recent successful programs include Asteroid Tracker and Agent Exoplanet.\n\nIn 2017 LCO received a grant from the Heising-Simons Foundation for $1,000,000 to be used in the construction and support of an additional 1-meter at their McDonald Observatory, Texas node.\n\n\n"}
{"id": "54899655", "url": "https://en.wikipedia.org/wiki?curid=54899655", "title": "List of craters on minor planets", "text": "List of craters on minor planets\n\nThis is a list of all named craters on minor planets in the Solar System as named by IAU's Working Group for Planetary System Nomenclature. In addition tentatively named craters—such as those of Pluto—may also be referred to. The number of craters is given in parenthesis. For a full list of all craters, \"see list of craters in the Solar System\".\n"}
{"id": "26580339", "url": "https://en.wikipedia.org/wiki?curid=26580339", "title": "List of lakes in Oneida County, Wisconsin", "text": "List of lakes in Oneida County, Wisconsin\n\nThere are 428 named lakes in Oneida County, Wisconsin, along with 701 with no names. Together they make up 68,447 acres of surface area. Willow Flowage, at 6,306 acres, is the largest. Oneida County is the county with the second largest number of lakes in Wisconsin, after neighboring Vilas County.\n\nNamed lakes are listed below. Alternate names are indicated in parentheses.\n"}
{"id": "3054266", "url": "https://en.wikipedia.org/wiki?curid=3054266", "title": "List of national parks of Zambia", "text": "List of national parks of Zambia\n\nThis is a list of national parks in Zambia. There are twenty national parks in Zambia, although a few of them are not maintained and so contain no facilities and few animals. Others have high concentrations of animals and are popular with tourists, while two or three are world-famous.\n\n\nThe national parks are administered by the Zambia Wildlife Authority, an autonomous body responsible to the Ministry of Tourism, Environment and Natural Resources. This Authority took over from the former National Parks and Wildlife Service which suffered from chronic underfunding. Consequently of the 20 parks, five have never had management or facilities and have very little wildlife: Isangano, Lavushi Manda, Lusenga Plain, West Lunga, and Mweru Wantipa; three have substantial wildlife but have hitherto been left undeveloped as a matter of policy or because a neighbouring park has been favoured: North Luangwa, Luambe, and Lukusuzi; and three have wildlife but have been too remote to develop: Liuwa Plain, Sioma Ngwezi, and Nyika Plateau. Of the remainder, most are in quite good shape, except Nsumbu, which went through a decline due to a lack of transport infrastructure, and the zoological park section of Mosi-oa-Tunya National Park which needs rehabilitation.\n\nIn approximate order of importance in terms of wildlife resources, the eight main functioning parks, all with access and accommodation are:\n\n"}
{"id": "3408446", "url": "https://en.wikipedia.org/wiki?curid=3408446", "title": "List of protected areas of Sierra Leone", "text": "List of protected areas of Sierra Leone\n\nThis is a list of protected areas of Sierra Leone, including national parks, game reserves, conservation areas, wetlands, and those that are listed as proposed protected areas in the UN Environment Programme World Conservation Monitoring Centre (UNEP WCM) database.\n\nMarine Protected Areas (MPAs)\n\n\n"}
{"id": "8324498", "url": "https://en.wikipedia.org/wiki?curid=8324498", "title": "List of retired Pacific hurricane names", "text": "List of retired Pacific hurricane names\n\nWithin the Pacific Ocean, the name of any significant tropical cyclone can be retired from the tropical cyclone naming lists by the World Meteorological Organization if it is felt that a storm is so deadly or damaging that the future use of its name would be inappropriate. Storm names can also be retired for other reasons, such as being very similar to another retired name or because it might suggest an undesirable meaning in another language. Within the Eastern and Central Pacific basins, a total of eighteen names have been removed from the official lists. The deadliest system to have its name retired was Hurricane Pauline, which caused over 230 fatalities when it struck Mexico during October 1997, while the costliest hurricane was Hurricane Manuel which caused an economic impact of over in damage in September 2013. Patricia was the most recent Pacific tropical cyclone to have its name retired, due to its exceptional intensity.\n\nIn 1950 a tropical cyclone that affected Hawaii was named Able, after a tropical cyclone had not affected Hawaii for a number of years. The system subsequently became widely known as Hurricane Hiki, since Hiki is Hawaiian for Able. Typhoons Olive and Della of 1952 and 1957, respectively, developed within the Central Pacific, but were not named until they had crossed the International Dateline and moved into the Western Pacific basin. During 1957, two other tropical cyclones developed in the Central Pacific and were named Kanoa and Nina by the Hawaiian military meteorological offices. It was subsequently decided that future tropical cyclones would be named by borrowing names from the Western Pacific naming lists.\n\nWithin the Eastern Pacific basin the naming of tropical cyclones started in 1960, with four sets of female names initially designed to be used consecutively before being repeated. In 1965 after two lists of names had been used, it was decided to return to the top of the second list and to start recycling the sets of names on an annual basis. In 1977 after protests by various women's rights groups, NOAA made the decision to relinquish control over the name selection by allowing a regional committee of the WMO to select new sets of names. The WMO selected six lists of names which contained male names and rotated every six years. They also decided that the new lists of hurricane name would start to be used in 1978 which was a year earlier than the Atlantic. Since 1978 the same lists of names have been used, with names of significant tropical cyclones removed from the lists and replaced with new names.\n\nDuring 1979, after ten names had been borrowed from the Western Pacific naming lists, Hawaiian names were reinstated for tropical cyclones developing into tropical storms forming in the Central Pacific. Five sets of Hawaiian names, using only the 12 letters of the Hawaiian alphabet, were drafted with the intent being to use the sets of names on an annual rotation basis. However, after no storms had developed in this region between 1979 and 1981, the annual lists were scrapped and replaced with four sets of names and designed to be used consecutively. Ahead of the 2007 hurricane season, the Central Pacific Hurricane Center and the Hawaii State Civil Defense requested that the hurricane committee retire eleven names from the Eastern Pacific naming lists. However, the committee declined the request and noted that its criteria for the retirement of names was \"well defined and very strict.\" It was felt that while the systems may have had a significant impact on the Hawaiian Islands, none of the impacts were major enough to warrant the retirement of the names. It was also noted that the Committee had previously not retired names for systems that had a greater impact than those that had been submitted. The CPHC also introduced a revised set of Hawaiian names for the Central Pacific, after they had worked with the University of Hawaii Hawaiian Studies Department to ensure the correct meaning and appropriate historical and cultural use of the names.\n\nThe practice of retiring significant names was started during 1955 by the United States Weather Bureau in the Atlantic basin, after hurricanes Carol, Edna, and Hazel struck the Northeastern United States and caused a significant amount of damage in the previous year. Initially the names were only designed to be retired for ten years after which they might be reintroduced, however, it was decided at the 1969 Interdepartmental hurricane conference that any significant hurricane in the future would have its name permanently retired. Several names have been removed from the Pacific naming lists for various reasons other than for causing a significant amount of death/destruction, which include being pronounced in a very similar way to other names and for political reasons.\n\nWithin the Eastern Pacific basin - between the western coasts of the Americas and 140°W - fifteen names have been retired since naming started in the region in 1960. Prior to the start of the modern naming lists in 1978, the names Hazel and Adele were retired from the list of names for reasons that are not clear. The name Fico was subsequently retired after the system had affected Hawaii in 1978, while the name Knut was removed after being used in 1987 for unknown reasons having barely reaching tropical storm strength. In 1989 the name Iva was removed as it was pronounced very similarly to Hurricane Iwa, which was retired from the Central Pacific lists of names in 1982 after affecting Hawaii. In the early 1990s the names Fefa and Ismael were both retired after they affected Hawaii and Northern Mexico, respectively. Hurricane Pauline became the deadliest Eastern Pacific hurricane, and its name was retired after it affected Mexico in 1997.\n\nThe names Adolph and Israel were subsequently retired for political considerations, after a tiff brewed over the use of their names at the start of the 2001 season. The name Kenna was retired in 2003 after it became one of the most intense Pacific hurricanes ever recorded. The name Alma was retired in 2009 after it had become the first Eastern Pacific tropical cyclone on record to make landfall along the Pacific Coast of Central America. The name Manuel was retired in 2014, after it became the first Eastern Pacific tropical cyclone on record to make landfall in mainland Mexico, redevelop over water, and become a hurricane. At the 2015 hurricane committee meeting the name Odile was retired from the list of names after it became the first major hurricane to affect Baja California in 25 years.<ref name=\"Isis/Odile Retired\"></ref> The name Isis was also pre-emptively retired from the lists of names for 2016, as it was deemed inappropriate to be used because of the Islamic extremist rebel group.\n\nWithin the Central Pacific basin—between 140°W and the International Date Line at 180°—four names have been retired since the introduction of the modern naming list for the basin in 1979. Hurricanes Iwa and Iniki were retired after impacting Hawaii, while Paka and Ioke were retired after the affecting various islands in Micronesia.\n\n\n"}
{"id": "973344", "url": "https://en.wikipedia.org/wiki?curid=973344", "title": "List of rivers by discharge", "text": "List of rivers by discharge\n\nThis is a list of rivers by their average discharge, that is, their water flow rate. Here, only those rivers are shown whose discharge is more than 2000 cubic meters per second.\n\n"}
{"id": "38792761", "url": "https://en.wikipedia.org/wiki?curid=38792761", "title": "Marco Antonio Serna Díaz", "text": "Marco Antonio Serna Díaz\n\nHermano Marco Antonio Serna Díaz (11 July 1936 - 31 December 1991) was a herpetologist, ornithologist, and naturalist from Colombia.\n\nSerna was born to Marco Antonio Serna and Anna Judith Díaz in San Vicente Ferrer in Antioquia, Colombia. On 20 June 1950 he entered the Institute of the Brothers of the Christian Schools in San Pedro, Antioquia, where he discovered his love for natural sciences. From 1956 to 1958 he studied in San Antonio de Prado. On 10 January 1962 he made his perpetual profession at the Institute of the Brothers of the Christian Schools.\n\nHe was curator of birds, amphibians, and reptiles at the Museo de La Salle in Bogotá, and professor of ornithology at the University of Antioquia.\n\nSerna was both president of the Sociedad Antioqueña de Ornitología which he co-founded in 1984, and the Asociación Colombiana de Ornitología, two notable ornithological organisations in Colombia.\n\nTogether with Juan Arturo Rivero, a herpetologist from Puerto Rico, he described several new frog species, including \"Pristimantis dorsopictus\", \"Pristimantis johannesdei\", \"Hyloxalus breviquartus\", \"Pseudopaludicola ceratophryes\", and \"Colostethus ramirezi\".\n\nIn 1971, he collected the last known specimen of the Antioquia brush finch (\"Atlapetes blancae\").\n\nIn 2012, the newly described Antioquia wren (\"Thryophilus sernai\") was named in his honour. In 1984, Rivero described the frog species \"Eleutherodactylus sernai\" which was later synonymized with \"Eleutherodactylus cerastes\" (Lynch, 1975).\n"}
{"id": "58784222", "url": "https://en.wikipedia.org/wiki?curid=58784222", "title": "Margaret Werner-Washburne", "text": "Margaret Werner-Washburne\n\nMargaret (Maggie) Werner-Washburne is a molecular biologist and Regents' Professor Emeritus of Biology. at the University of New Mexico. She was previously the President (2013-2015) of the Society for the Advancement of Chicanos/Hispanics and Native Americans in Science (SACNAS) which holds the largest broadly multidisciplinary and multicultural STEM diversity conference in the U.S. A pioneer in the genomics of the stationary phase of yeast, she is known for her innovative programs to attract and retain underrepresented minorities in STEM.\n\nWerner-Washburne grew up near a Mexican village within Fort Madison. Her father, Dr. Harold Theodore Werner, was a general practitioner and volunteer prison doctor. Her mother, Marta Lucia (neé Brown y Morales), was born in Aguascalientes, Mexico. Werner-Washburne's mother and her mother's family fled Mexico for the United States during the Mexican Revolution. Werner-Washbune's mother was a prison reformer and community activist. Werner-Washburne earned her Bachelor of Arts degree in English studying poetry at Stanford University. After graduation, she traveled extensively throughout Mexico, Central and South America, Alaska, Samoa, and New Zealand. She subsequently obtained a Master's degree in Botany at the University of Hawaii—Honolulu with Sanford Siegel, a PhD in Botany at the University of Wisconsin—Madison with Kenneth Keegstra, and did postdoctoral work with National Academy of Sciences Member Elizabeth Craig. \n\nWerner-Washburne joined the University of New Mexico as a faculty member in 1988. In addition to running a research lab, Werner-Washburne served as a program director at the National Science Foundation (1998-1999) for which she was given the Director's Special Service Award (1999). Werner-Washburne created the Initiative to Maximize Student Diversity (IMSD) at the University of New Mexico. More than 300 students participated in the initiative, with >70% entering PhD programs. In 2009 she was recognized with a SAGE Women Making a Difference Award for her role in creating IMSD. She has mentored more than 100 underrepresented students who have received their PhDs or who are enrolled in PhD programs. She has been a member of the Southwest Hispanic Research Institute since 2009.. She has been the subject of a documentary, \"The Mystery of an Ancient Gene\", which described her discovery of the role of a gene called SNZ in a cell's metabolic pathway. She has written about the importance of psychosocial mentors in diversifying science and technology and institutional barriers to retaining underrepresented students in STEM.\n\nWerner-Washburne has received numerous honors and awards. These include: the National Science Foundation Presidential Young Investigator Award (1990), the Presidential Award for Excellence in Science, Mathematics and Engineering Mentoring (PAESMEM) (2003), the SACNAS Distinguished Scientist Award (2005), the Harvard Foundation Scientist of the Year (2011), and the AAAS Mentor Award for Lifetime Achievement (2017). She is an American Association for the Advancement of Science AAAS Fellow (2006). In 2017, in the first session of the fifty-third legislature, the House of Representatives of the New Mexico Legislature recognized her contributions to science and to the mentoring of underrepresented minorities.\n\nAs a postdoc in Elizabeth Craig's lab, Werner-Washburne was part of a team that discovered that a group of heat-shock proteins were chaperones. Werner-Washburne began studying the stationary phase of yeast when she moved to the University of New Mexico. She found new cell types in yeast stationary phase cultures (quiescent and non-quiescent). As part of her genomics research, she developed hyperspectral imaging to improve the signal-to-noise ratio of microarrays.\n\nWerner-Washburne plays in the band Holy Water and Whiskey, which has won three New Mexico Music Awards: Best Vocal for \"Mary Had A Baby\" and Best Western for \"Fancy Red Boots\" in 2011 and Best Vocal Performance for \"Night Hymn\" in 2016.\n\n"}
{"id": "12940349", "url": "https://en.wikipedia.org/wiki?curid=12940349", "title": "Mariner's astrolabe", "text": "Mariner's astrolabe\n\nThe mariner's astrolabe, also called sea astrolabe, was an inclinometer used to determine the latitude of a ship at sea by measuring the sun's noon altitude (declination) or the meridian altitude of a star of known declination. Not an astrolabe proper, the mariner's astrolabe was rather a graduated circle with an alidade used to measure vertical angles. They were designed to allow for their use on boats in rough water and/or in heavy winds, which astrolabes are ill-equipped to handle. In the sixteenth century, the instrument was also called a ring.\n\nMany dates can be found for the appearance of the first mariner's astrolabes. The earliest date, 1295, is offered by the Majorcan astronomer Ramon Llull. Later dates center around the late 15th century, with Samuel Purchas claiming that it was adapted for marine navigation by Martin Behaim, a mariner not considered a trustworthy source by some historians. In any event, the instrument was certainly known by the end of the 15th century. Nevertheless, the creation and perfecting of the mariner's astrolabe is attributed to Portuguese navigators during the beginning of Portuguese discoveries. The earliest known description of how to make and use a sea astrolabe comes from Martín Cortés de Albacar’s \"Arte de Navegar\" published in 1551, although the basic principle is the same as that of the archipendulum used in constructing the Egyptian pyramids.\n\nThere is strong evidence that the mariner's astrolabe was derived directly from the planispheric astrolabe, as the earliest examples retain some of the markings (e.g. \"umbra recta\" and \"umbra versa\") of the prior device without having the same components.\n\nThe mariner's astrolabe would have replaced or complemented instruments such as the cross staff or quadrant as a navigator's instrument. The mariner's astrolabe was used until the middle or, at the latest, the end of the 17th century. It was replaced by more accurate and easier-to-use instruments such as the Davis quadrant.\n\nAlthough their heavy brass construction permits their longevity in marine environments, mariner's astrolabes are very rare today. In 2017, only 108 were known to exist. The biggest collection remains in museums in Portugal. The Corpus Christi Museum of Science and History in Texas, United States, contains a mariner's astrolabe with a confirmed date of 1554, salvaged from the wreck of the \"San Esteban\". A disk-type astrolabe dated between 1496 and 1500, now the world's oldest, was discovered in 2014 by the marine scientist David L. Mearns on the wreck site of a Portuguese nau \"Esmeralda,\" which sank off the coast of Oman in 1503.\n\nMariner's astrolabes were made of brass. Since weight was advantageous when using the instrument on the heaving deck of a ship or in high winds, other materials, such as wood or ivory, were not desirable though some wood sea astrolabes were made. Early sea astrolabes were made from sheets of brass. Due to their light weight, they tended to perform poorly at sea. Heavier cast brass frames began to be made in the mid-sixteenth century and were considerably better. As the accuracy of the instrument is related to the radius of the divided circle, these were made as large as practical.\n\nSince the large plate form of the planispheric astrolabe makes it sensitive to the wind, the mariner's astrolabe is made with a frame form. The openings in the frame allow wind to pass through, inducing less motion in the instrument.\n\nThe essential function of the device was to measure angles. Thus the instrument featured a ring graduated in degrees. Early instruments were only graduated for 90°; later instruments were graduated for the full 360° circle around the limb. The sole purpose of the spokes was to support the pivot point for the alidade. In order to lower the centre of gravity of the device and thus increase its period of motion as a means of stabilizing it, extra brass was usually added to the bottom of the instrument inside the ring. This is clearly evident in the lower left instrument seen in the photograph above.\n\nThe alidade was free to rotate about a pin through the centre of the instrument. The vanes of the alidade were either slotted or pierced with a hole to allow the user to align the alidade.\n\nThe astrolabe had a ring attached to the top of the instrument to allow it to hang vertically.\n\nIn order to use the astrolabe, the navigator would hold the instrument by the ring at the top. This caused the instrument to remain in a vertical plane. The navigator would then align the plane of the astrolabe to the direction of the object of interest. The alidade was aligned to point at the object and the altitude was read off the outer degree scale.\n\nIf observing a dim object such as a star, the navigator would observe the object directly through the alidade. If observing the sun, it was both safer and easier to allow the shadow of one of the alidade's vanes to be cast onto the opposite vane.\n\nThe mariner's astrolabe needed to be suspended vertically in order to measure the altitude of the celestial object. This meant it could not be used easily on the deck in windy conditions. It could not easily be used to measure the angle between two objects, which was necessary for longitude calculations by the lunar distance method (though that technique was not used when the instrument was developed). Another limitation was that the instrument's angular accuracy was directly proportional to the length of the alidade, which was not very long.\n\n\n"}
{"id": "24591420", "url": "https://en.wikipedia.org/wiki?curid=24591420", "title": "National University Research Council", "text": "National University Research Council\n\nNational University Research Council (Consiliul Naţional al Cercetării Ştiinţifice) is the Romanian national research funding body, and the accrediting body for academic journals and academic publishers. It was established in late 1994, and is located in Bucharest.\n\nIt is a member of the European Science Foundation.\n\nIn 1999, the council established the National Center for Science Policy and Scientometrics (Centrul Naţional pentru Politica Ştiinţei şi Scientometrie; CENAPOSS) to develop and promote Romanian research.\n\nJournals are ranked as \"A\", \"B+\" or \"B\".\n\n"}
{"id": "43266598", "url": "https://en.wikipedia.org/wiki?curid=43266598", "title": "No Place to Hide (Bradley book)", "text": "No Place to Hide (Bradley book)\n\nNo Place to Hide is a 1948 book by American writer David J. Bradley published by Little, Brown and Company. The book is a Harvard Medical School graduate's autobiographical tale of his work in the Radiological Safety Section in the Pacific in the aftermath of the Bikini atomic bomb tests, Operation Crossroads. The book alerted the world to the dangers of radioactive fallout from nuclear weapon explosions. The book was marketed for Bantam by Judith Merril, who found Bradley's prose \"a man's book with little appeal for women\", leading her to later write her own nuclear war story \"Shadow on the Hearth\" from the homemaker's perspective. Bradley toured lecturing on the dangers of fallout, including a 1950 lecture at Ford Hall Forum.\n\nThe book was reissued with an epilogue in 1984.\n"}
{"id": "621925", "url": "https://en.wikipedia.org/wiki?curid=621925", "title": "Operation Quicksilver (1978)", "text": "Operation Quicksilver (1978)\n\nOperation Quicksilver was a series of 16 nuclear tests conducted by the United States in 1978-1979 at the Nevada Test Site. These tests followed the \"Operation Cresset\" series and preceded the \"Operation Tinderbox\" series.\n"}
{"id": "17676093", "url": "https://en.wikipedia.org/wiki?curid=17676093", "title": "Polychotomous key", "text": "Polychotomous key\n\nPolychotomous key refers to the number of alternatives which a decision point may have in a non-temporal hierarchy of independent variables. The number of alternatives are equivalent to the root or nth root of a mathematical or logical variable. Decision points or independent variables with two states have a binary root that is referred to as a dichotomous key whereas, the term polychotomous key refers to roots which are greater than one or unitary and usually greater than two or binary. Polychotomous keys are used in troubleshooting to build troubleshooting charts and in classification/identification schemes with characteristics that have more than one attribute and the order of characteristics is not inherently based on the progression of time. \n\n\n"}
{"id": "20754804", "url": "https://en.wikipedia.org/wiki?curid=20754804", "title": "RV Wecoma", "text": "RV Wecoma\n\nRV \"Wecoma\" is a research vessel owned by the National Science Foundation and operated by the College of Oceanic & Atmospheric Sciences at Oregon State University (OSU) as a member of the University-National Oceanographic Laboratory System (UNOLS) fleet. It is based in Newport in the U.S. state of Oregon near OSU's Hatfield Marine Science Center. Launched in 1975, it has a maximum displacement of .\n\nThe ship is equipped with of laboratory space to support up to 18 scientists at sea. It has a variety of equipment permanently installed, and optional additional equipment available on request, to measure and analyze navigational data; surface atmospheric conditions; sea surface temperature, salinity, fluorescence; bottom depth; dissolved oxygen titration; solar radiation; GPS time; bioacoustics; and geological sampling. The range of depths of submerged equipment varies from . The vessel can support diving operations, radioactive isotope materials, and explosive materials.\n\n\"Wecoma\" made her last operational cruise November 2011 and was scheduled to be retired with the interim replacement ship being the former Woods Hole Oceanographic Institution operated R/V \"Oceanus\".\n\n"}
{"id": "383420", "url": "https://en.wikipedia.org/wiki?curid=383420", "title": "Regional park", "text": "Regional park\n\nA regional park is an area of land preserved on account of its natural beauty, historic interest, recreational use or other reason, and under the administration of a form of local government.\nA regional park can be a special park district covering a region crossing several jurisdiction boundaries, or a park system of a single jurisdiction, such as a province, county, or city.\n\nThere are 101 regional parks in Saskatchewan. All parks are operated by volunteer boards.\n\nRegional parks in Italy are administered by each region in Italy, a government unit like a U.S. state.\n\nIn New Zealand, regional parks are administered by regional councils rather than the Department of Conservation or territorial authorities.\n\nIn the United States, a regional park is sometimes referred to as a 'Metropolitan Park (Metropark)' or as an open space reserve. The terms \"region\" and \"metropolitan\" can have different meanings in U.S. local government agencies. Regional parks can be administered by a regional park board, a state, county or other units of local government. A special authority can be set up, under the joint jurisdiction of two or more government bodies or as an independent park district to administer parks. Individual parks may or may not cross governmental boundaries. The park district holds the authority, similar to fire protection districts, to manage and raise taxes to cover park acquisition and management costs.\n\nIn Ohio, under Ohio Revised Code Chapter 1545, metro parks such as the Columbus and Franklin County Metro Parks can have their own sworn police forces (rangers); The Cleveland Metroparks and Dayton Five Rivers Metroparks are also in the state. Other examples of large regional park systems are the Huron-Clinton Metroparks in southeast Michigan; and the Three Rivers Park District in Minnesota.\n\nThe East Bay Regional Park District and Midpeninsula Regional Open Space District have extensive parklands in the San Francisco Bay Area, protecting habitat and offering recreation.\n\nIn Scotland, regional parks are defined to co-ordinate the management of areas of attractive countryside that are of importance for recreation due to their proximity to population centres. The parks have been defined and are managed by local authorities. Currently Scotland has three regional parks:\n\n"}
{"id": "55563843", "url": "https://en.wikipedia.org/wiki?curid=55563843", "title": "Renée C. Kraan-Korteweg", "text": "Renée C. Kraan-Korteweg\n\nProfessor Renée C. Kraan-Korteweg (born 1954) is a Dutch - South African scientist.\n\nShe is Head of the Department of Astronomy at the University of Cape Town as well as Founder and Co-director of the Astrophysics, Cosmology and Gravity Centre. She also serves as Vice-President of Executive Committee of the International Astronomical Union. She is a member of the Academy of Science of South Africa.\n\nHer research interests include the Large Scale Structure and streaming motions in the nearby universe. She also conducts research with the South African Large Telescope (SALT), including searching for black holes in the centre of dwarf elliptical galaxies and looking at how much dark matter there is in low surface brightness galaxies. She was one of the lead authors in the breakthrough paper “The Parkes HI Zone of Avoidance Survey”, which used the Parkes 60m Radio Telescope to discover nearby galaxies that were previously \"hidden\" by the gravitational anomaly known as the Great Attractor.\n"}
{"id": "11281115", "url": "https://en.wikipedia.org/wiki?curid=11281115", "title": "SWEEPS-10", "text": "SWEEPS-10\n\nSWEEPS-10 is an extrasolar planet that, from June 2007 to August 2011, was the planet candidate with the shortest orbital period yet found, until PSR J1719-1438 b was discovered in 2011 with an even shorter orbit. The planet orbits the star SWEEPS J175902.00−291323.7 located in the Galactic bulge at a distance of approximately 22,000 light years from Earth (based on a distance modulus of 14.1). \n\nIt completes an orbit of its star (designated SWEEPS J175902.00−291323.7) in just 10 hours, and is categorized as an ultra-short period planet (USPP). Located only 1.2 million kilometers from its star (roughly three times the distance between the Earth and the Moon), the planet is among the hottest ever detected; its estimated temperature is approximately 1,650 degrees Celsius. \"This star-hugging planet must be at least 1.6 times the mass of Jupiter, otherwise the star's gravitational muscle would pull the planet apart,\" said team leader Kailash Sahu of the Space Telescope Science Institute in Baltimore, Maryland. Such USPPs seem to occur only around dwarf stars.\n\nThe small star's relatively low temperature allows the planet to exist. \"USPPs occur preferentially around normal red dwarf stars that are smaller and cooler than our Sun,\" Sahu said.\n\n\n"}
{"id": "185529", "url": "https://en.wikipedia.org/wiki?curid=185529", "title": "Scalability", "text": "Scalability\n\nScalability is the capability of a system, network, or process to handle a growing amount of work, or its potential to be enlarged to accommodate that growth. For example, a system is considered scalable if it is capable of increasing its total output under an increased load when resources (typically hardware) are added. An analogous meaning is implied when the word is used in an economic context, where a company's scalability implies that the underlying business model offers the potential for economic growth within the company.\n\nScalability, as a property of systems, is generally difficult to define and in any particular case it is necessary to define the specific requirements for scalability on those dimensions that are deemed important. It is a highly significant issue in electronics systems, databases, routers, and networking. A system whose performance improves after adding hardware, proportionally to the capacity added, is said to be a scalable system.\n\nAn algorithm, design, networking protocol, program, or other system is said to \"scale\" if it is suitably efficient and practical when applied to large situations (e.g. a large input data set, a large number of outputs or users, or a large number of participating nodes in the case of a distributed system). If the design or system fails when a quantity increases, it \"does not scale\". In practice, if there are a large number of things () that affect scaling, then resource requirements (for example, algorithmic time-complexity) must grow less than as increases. An example is a search engine, which scales not only for the number of users, but also for the number of objects it indexes. Scalability refers to the ability of a site to increase in size as demand warrants.\nThe concept of scalability is desirable in technology as well as business settings. The base concept is consistent the ability for a business or technology to accept increased volume without impacting the contribution margin (= revenue − variable costs). For example, a given piece of equipment may have a capacity for 1–1000 users, while beyond 1000 users additional equipment is needed or performance will decline (variable costs will increase and reduce contribution margin).\n\nAnother example is the Incident Command System (ICS), the emergency management system used across response agencies in the United States. ICS can scale resource coordination from a single-engine roadside brushfire to an interstate wildland fire, for example. The first resource on scene establishes IC, with authority to order resources and delegate responsibility within the span of control (managing five to seven officers, who will again delegate to up to seven, and on as the incident grows). Senior officers assume command at the top as complexity warrants. This proven system is remarkably simple, fully scalable and has been saving lives and property for nearly half a century.\nScalability can be measured in various dimensions, such as:\n\n\n\nMethods of adding more resources for a particular application fall into two broad categories: horizontal and vertical scaling.\n\n\nThere are tradeoffs between the two models. Larger numbers of computers means increased management complexity, as well as a more complex programming model and issues such as throughput and latency between nodes; also, some applications do not lend themselves to a distributed computing model. In the past, the price difference between the two models has favored \"scale up\" computing for those applications that fit its paradigm, but recent advances in virtualization technology have blurred that advantage, since deploying a new virtual system over a hypervisor (where possible) is often less expensive than actually buying and installing a real one. Configuring an existing idle system has always been less expensive than buying, installing, and configuring a new one, regardless of the model.\n\nNote that NFV defines these terms differently: scaling out/in is the ability to scale by add/remove resource instances (e.g. virtual machine), whereas scaling up/down is the ability to scale by changing allocated resources (e.g. memory/CPU/storage capacity)\n\nA number of different approaches enable databases to grow to very large size while supporting an ever-increasing rate of transactions per second. The rapid pace of hardware advances in both the speed and capacity of mass storage devices, as well as similar advances in CPU and networking speed is also important.\n\nOne technique supported by most of the major database management system (DBMS) products is the partitioning of large tables, based on ranges of values in a key field. In this manner, the database can be \"scaled out\" across a cluster of separate database servers. Also, with the advent of 64-bit microprocessors, multi-core CPUs, and large SMP multiprocessors, DBMS vendors have been at the forefront of supporting multi-threaded implementations that substantially \"scale up\" transaction processing capacity.\n\nNetwork-attached storage (NAS) and Storage area networks (SANs) coupled with fast local area networks and Fibre Channel technology enable still larger, more loosely coupled configurations of databases and distributed computing power. The widely supported X/Open XA standard employs a global transaction monitor to coordinate distributed transactions among semi-autonomous XA-compliant database resources. Oracle RAC uses a different model to achieve scalability, based on a \"shared-everything\" architecture that relies upon high-speed connections between servers.\n\nWhile DBMS vendors debate the relative merits of their favored designs, some companies and researchers question the inherent limitations of relational database management systems. GigaSpaces, for example, contends that an entirely different model of distributed data access and transaction processing, space-based architecture, is required to achieve the highest performance and scalability. On the other hand, Base One makes the case for extreme scalability without departing from mainstream relational database technology. For specialized applications, NoSQL architectures such as Google's Bigtable can further enhance scalability. Google's horizontally distributed Spanner technology, positioned as an relational alternative to Bigtable, supports general-purpose database transactions and provides a more conventional SQL-based query language.\n\nIn the context of scale-out data storage, scalability is defined as the maximum storage cluster size which guarantees full data consistency, meaning there is only ever one valid version of stored data in the whole cluster, independently from the number of redundant physical data copies. Clusters which provide \"lazy\" redundancy by updating copies in an asynchronous fashion are called 'eventually consistent'. This type of scale-out design is suitable when availability and responsiveness are rated higher than consistency, which is true for many web file hosting services or web caches (\"if you want the latest version, wait some seconds for it to propagate\"). For all classical transaction-oriented applications, this design should be avoided.\n\nMany open source and even commercial scale-out storage clusters, especially those built on top of standard PC hardware and networks, provide eventual consistency only. Idem some NoSQL databases like CouchDB and others mentioned above. Write operations invalidate other copies, but often don't wait for their acknowledgements. Read operations typically don't check every redundant copy prior to answering, potentially missing the preceding write operation. The large amount of metadata signal traffic would require specialized hardware and short distances to be handled with acceptable performance (i.e. act like a non-clustered storage device or database).\n\nWhenever strong data consistency is expected, look for these indicators: \n\nIndicators for eventually consistent designs (not suitable for transactional applications!) are:\n\nIt is often advised to focus system design on hardware scalability rather than on capacity. It is typically cheaper to add a new node to a system in order to achieve improved performance than to partake in performance tuning to improve the capacity that each node can handle. But this approach can have diminishing returns (as discussed in performance engineering). For example: suppose 70% of a program can be sped up if parallelized and run on multiple CPUs instead of one. If formula_1 is the fraction of a calculation that is sequential, and formula_2 is the fraction that can be parallelized, the maximum speedup that can be achieved by using P processors is given according to Amdahl's Law:\n\nSubstituting the value for this example, using 4 processors we get\n\nIf we double the compute power to 8 processors we get\n\nDoubling the processing power has only improved the speedup by roughly one-fifth. If the whole problem was parallelizable, the speed would also double. Therefore, throwing in more hardware is not necessarily the optimal approach.\n\nIn the context of high performance computing there are two common notions of scalability:\n\n"}
{"id": "31594262", "url": "https://en.wikipedia.org/wiki?curid=31594262", "title": "Science &amp; Vie Junior", "text": "Science &amp; Vie Junior\n\nScience & Vie Junior is a French science magazine targeting children.The magazine is published by Mondadori France, a subsidiary of the Italian media company Mondadori.\n\n\"Science & Vie Junior\" was started in 1989. The magazine is published on a monthly basis. The magazine is a spin-off of Science & Vie made for teenagers. In 2010, it won the Grand prix des Médias. In 2012 the circulation of the monthly was 166,451 copies.\n\nThe magazine has 3 main parts.\n\nL'actu is the first section of the magazine. The main part of this section is news.\n\nCucaracha is a comic in the L'actu section of the magazine. It is the first part of the section. It is about lives of cockroaches in human society. It was made by Marino Degano, and Laurent Salles.\n\nCucaracha cockroach is endowed with reason. With his small telescope, she is passionate about the sky and stars. But sometimes strange shadows infest his field of vision; These are Zoms who are struggling with frenzy. So rather than wait for the view emerges the heroine begins to watch them with a look relevant and sometimes critical.\n\nThe moral of this story... cockroaches need only be patient. One day the other, and will destroy the Zoms that day, they become \"masters of the world\".\n\nCucaracha is published monthly since November 2001 on the first page in Science et Vie Junior. The website presents a small selection of these stories.\n\nTout en images (All in images) is a part of L'actu. It takes an event that happened, gets lots of pictures. Then there are captions of one medium sized paragraph. Tout en images takes 7 pages most of the time.\n\nAnother section explains news of different sciences. It also has 1 citation from a person, and a piece of statistics. That is called \"Le nombre\" (The number). There is also a place called textos with no images. Just text. Also. there is a fake interview that has as example anti-hydrogen.\n\n100% Sciences is another section of the magazine. It has what is similar to the \"Tout en images\" but with less images. After, there are many long articles with 1 comic, experiments, technology tips, and math magic.\n\nThis section that means \"My Science & Vie Junior\". In this section, there is:\n\n"}
{"id": "50312945", "url": "https://en.wikipedia.org/wiki?curid=50312945", "title": "Supeno Surija", "text": "Supeno Surija\n\nDr. Supeno Surija, Ph.D. is a member of The American Institute of Astronautics and Aeronautics. He earned Master and Ph.D. degrees from California Coast University, United States. Since 1980, Dr. Supeno has dedicated his life to research microorganism and nanotechnology. He participated in many workshops, symposiums, and short courses on microbiology and nanotechnology.\nWith consistent multitasking ability and strong determination, Dr. Supeno devoted himself to many areas of expertise and jobs. He is a writer, journalist, caricaturist, scientist, diplomat, inventor and economic and environmental professional. He has received many awards for his essays, findings and inventions, including the award from Indonesia World Records Museum (MURI).\n\nAfter graduated from high school, Dr. Supeno studied in Faculty of Economics, Universitas Nomensen HKBP, Medan using Beasiswa Bank Indonesia(scholarship from Bank of Indonesia) by the recommendation and guidance of his mother. With strong determination in studying, he earned Bachelor and Doctorandus degrees in accounting in 3.5 years, this record has not been broken since 35 years ago. Later, he earned his Master and Ph.D degree from California Coast University.\nDr. Supeno has also completed CPD (Continuing Professional Development) from distinguished universities, namely:\n\nIn 1983, he began his career as a freelance accountant in various small companies. From 1986 to 1991, Dr. Supeno served as Director of Finance and Accounting in PT. Singa Margana. Then, from 1990 to 1991, he served as President Director in BPR Disky Suryajaya and in 1991, he earned Brevet Tax Consultant from Ministry of Finance of Republic of Indonesia. From 1991 to 1997, he was appointed as General Manager in South East Asia Bank (SEAB). Later, he continued his career as an Operational Coordinator of SEAB Liquidation Team. In 2002, he obtained license to be Registered Mutual FundRepresentative. In the same year, he was appointed as CEO in Shamrock Group company until 2011.\n\nIn 1979, he received an award for his caricature from Yomiuri Shimbun, the largest newspaper in Japan. This achievement marked the beginning of his career in the field of press. Dr. Supeno then became a caricaturist, writer and journalist in BSF media (Bintang Sport Film). Furthermore, his essays were published in many daily newspapers in Medan, such as Harian Analisa, Bukit Barisan, Sinar Indonesia Baru (SIB) and BSF (Bintang Sport Film). He received numerous appreciations and awards for his essays. In 1981, he received an award from Ikatan Pers Mahasiswa (Students' Press Association)for student’s essays. In the next year, in 1982, Dr. Supeno received an award from Pusat Pembinaan dan Pengembangan Bahasa (Language Training and Development Center) for his literature essay writing. In 1987, Ministry of Finance of Republic of Indonesia, Radius Prawiro, acknowledge him for his scientific article and popular article on tax. In the next year, he received an award from Pusat Penerangan ABRI and PWI Jaya (Indonesian Press Association) for his essay on defense and security of Republic of Indonesia. Furthermore, in the same year, he received an award from Ministry of Communication and Informatics of Republic of Indonesia as an Indonesian film critic.\n\nHe began his career in environmental field by doing research on Dioxin-Free since 1994. As he continued his research, he successfully completed his research and invention which was written in the book entitled “Dioxin-Free System” for preservation of the environment. This invention has received many world awards including award from MURI (Indonesia World Records Museum) as the first Dioxin-Free system in the world.\nIn 2000, Dr. Supeno became an active environmental activist and pioneer of Zero Burning in many plantations in Indonesia. In 2008, he established Buah Nabar Conservation in Sibolangit, Sumatera Utara, Indonesia. This conservation aims to become water catchments and storage areas for underground water which fill the wells in Medan and its surroundings to fulfill the needs of marginalized society. This conservation implements several methods, such as building thousands of biopores, planting trees, and preserving ecosystem in Buah Nabar region and its surroundings. In 2012, he was appointed as Director of Lembaga Konservasi dan Restorasi Hutan Indonesia (Institution of Indonesian Forest Conservation and Restoration) and Director of Lembaga Komunitas Peduli Masyarakat Marginal (Community Care for Marginalized Society). In 2015, he also established Yayasan REAL (Rekonstruksi Ekosistem Alam Lestari).\n\nDr. Supeno began his career in Biology since he was in senior high school. His determination led him to join various courses and trainings in many countries. In 1998, he started his research on agriculture and organic palm plantation. Through the root analysis and nano particle implementation, he successfully produced more efficient organic fertilizers at lower cost and increased the production significantly. This implementation disproved the failure of the previous generation of organic fertilizers for perennials (especiallyoil palm). In 2008, he established PT. Propadu Konair Tarahubun (Plantation Key Technology/ PKT) which is the acronym for PROyek terPADU KONservasi AIR, TAnah, udaRA, HUtan, dan perkeBUNan (Integrated Project for Water, Earth, Air, Forests Conservation, and Plantations) as plantation professional and producer of organic fertilizers, biopesticides, bioherbicides. The implementation of nanotechnology is the foundation of the success of PT Propadu Konair Tarahubun (Plantation Key Technology/ PKT) as the pioneer of organic oil palm plant and CPO around the world and in Indonesia. He is also the first person in the world to invent a vaccine for Ganoderma disease in oil palm plantation. PKT is also the only company recognized by many parties for its success in preventing and handling basal stem rot disease (Ganoderma)in oil palm plantation.\n\nHe began his research career in prevention of cancer, diabetes, infertility, and DNA damage by implementing Dioxin-Free system for the production, packaging, distribution and consumption processes. Dioxin-Free system has earned numerous world awards and making it the principle to produce various foods and beverages which do not cause cancer (and other diseases). This mechanism is also implemented in PKT products.\n\nAs a professional member of Institute of Nanotechnology in England and a pioneer who recommended the idea to implement nanotechnology in agriculture and plantation, he created NanoBiomimicry technique for oil palm, successfully producing fertilizers suitable for peat and other types of soil. As a member of Society for General Microbiology based in London, for over 30 years, he has consistently done research in microbiology, nanotechnology, and antibiotic development for human and plant.\n\nDr. Supeno obtained numerous patents (intellectual property rights) for his research in health, agriculture and food and beverage processing.\n\n"}
{"id": "40485654", "url": "https://en.wikipedia.org/wiki?curid=40485654", "title": "Thermodesulfobacterium hydrogeniphilum", "text": "Thermodesulfobacterium hydrogeniphilum\n\nThermodesulfobacterium hydrogeniphilum is a species of Sulfate-reducing bacteria. It is thermophilic, chemolithoautotrophic, non-spore-forming, marine species, with type strain SL6 (=DSM 14290 =JCM 11239).\n\n"}
{"id": "11522136", "url": "https://en.wikipedia.org/wiki?curid=11522136", "title": "Visible Language", "text": "Visible Language\n\nVisible Language is an American journal presenting visual communication research. Founded in 1967 as \"The Journal of Typographical Research\" by Merald Wrolstad, occasional \"Visible Language\" issues are co-edited with a guest editor-author. \n\nThe journal was founded with the primary tenet of the journal being that reading and writing together form a new, separate, and autonomous language system. The journal has evolved to focus on research in visual communication. The journal has covered the subject of concrete poetry, the Fluxus art movement, painted text, textual criticism, the abstraction of symbols, articulatory synthesis and text, and the evolution of the page from print to on-screen display. Guest editor-authors have included Colin Banks, John Cage, Adrian Frutiger, Dick Higgins, Richard Kostelanetz, Craig Saper, and George Steiner.\n\nThe journal was edited for 26 years (1987-2012) by Sharon Poggenpohl of the Illinois Institute of Technology's Institute of Design, with administrative offices at the Rhode Island School of Design. It is currently edited by Mike Zender of the University of Cincinnati, which publishes and provides administrative offices for the journal.\n\n\n\n"}
