{"id": "19973035", "url": "https://en.wikipedia.org/wiki?curid=19973035", "title": "Alasdair A. K. White", "text": "Alasdair A. K. White\n\nAlasdair Antony Kenneth White (born May 24, 1952) is a British management theorist best known for his work on performance management from a behavioural perspective and in the field of deconcentrated and networked organizations. Along with John Fairhurst, White developed the White-Fairhurst Performance Hypothesis relating to the performance life-cycle. He is also the author of \"Continuous Quality Improvement\" and \"The Essential Guide to Developing Your Staff\".\n\nIn 2006, he worked closely with John Fairhurst and, following various observational studies, they formulated the White-Fairhurst Performance Hypothesis which states that \"all performance will initially trend towards a steady state, particularly after a period of performance uplift, and that steady state will then develop a downward curve leading to a significant performance decline\".\nIn a paper entitled \"From Comfort Zone to Performance Management\", White examines the hypothesis from a theoretical perspective starting with the Comfort Zone Theory and the work of Robert Yerkes and John Dodson, David McClelland et al., the Tuckman Model and Colin Carnall. This theoretical examination leads White to conclude that the White-Fairhurst Hypothesis broadly holds true and the performance curve is as demonstrated in the White-Fairhurst TPR Life-cycle Model (TPR stands for Transforming, Performing, Reforming). There has been some criticism of White’s approach, suggesting that more recent sources and work would have been more appropriate, but none have been able to offer a counter-argument or to adequately refute or dispute the hypothesis. The fact remains that most of the fundamentals of performance behaviour were established in the last century and that the more recent work is itself based on this earlier work.\n\nWhite argues that what is important now is to determine trend-change points on the performance curve so that the most appropriate performance management actions can be applied. He conducted research within a university environment in this relation to this and the result is recorded in his 2008 essay \"Managing Academic Performance\" the underlying theory of which underpins the approach being adopted by a number of academic institutions.\n\nIn addition to writing on management issues, White is also active in writing and editing fiction for White & MacLean Publishing. Among those writings include the a political and action novel \"Shadows\", written under the pen-name of Alex Hunter.\n\n\n\n"}
{"id": "43248028", "url": "https://en.wikipedia.org/wiki?curid=43248028", "title": "Ash Hollow Formation", "text": "Ash Hollow Formation\n\nThe Ash Hollow Formation of the Ogallala Group is a geological formation found in Nebraska and South Dakota. It preserves fossils dating back to the Neogene period. It was named after Ash Hollow, Nebraska and can be seen in Ash Hollow State Historical Park.\n\n\n"}
{"id": "20201297", "url": "https://en.wikipedia.org/wiki?curid=20201297", "title": "Autism's False Prophets", "text": "Autism's False Prophets\n\nAutism's False Prophets: Bad Science, Risky Medicine, and the Search for a Cure is a 2008 book by Paul Offit, a vaccine expert and chief of infectious diseases at Children's Hospital of Philadelphia. The book focuses on the controversy surrounding the now discredited link between vaccines and autism. The current scientific consensus is that no convincing scientific evidence supports these claims, and a 2011 journal article described the vaccine-autism connection as \"the most damaging medical hoax of the last 100 years\".\n\nOffit describes the origins and development of claims regarding the MMR vaccine and the vaccine preservative thiomersal, as well as subsequent scientific evidence which has disproved a link with autism. The book discusses possible explanations for the persistence of these claims in the face of scientific evidence to the contrary, as well as the proliferation of potentially risky and unproven treatments for autism. The author takes a critical view of several advocates of a vaccine–autism link, including Andrew Wakefield, David Kirby, Mark Geier, and Boyd Haley, raising scientific and, in some cases, ethical and legal concerns. The book also explores divisions within the autism community on the topic of vaccines, as some parents consider the ongoing narrow focus on vaccines a distraction from more scientifically promising avenues of research. In this vein, Offit interviews Kathleen Seidel, a mother of an autistic child who has published investigations critical of those who profit from promoting vaccine–autism claims.\n\nOffit also touches on the heated and bitter debate surrounding vaccine claims. He describes receiving death threats, hate mail, and threats against his children as a result of his advocacy for vaccination. Offit declined to do a book tour for \"Autism's False Prophets\", citing concerns about his physical safety and comparing the intensity of hatred and threats directed at him to that experienced by abortion providers. Author's royalties from the book are being donated to the Center for Autism Research at Children's Hospital of Philadelphia.\n\nThe book was the nucleus of profiles of Offit in \"Newsweek\" and \"The Philadelphia Inquirer\". The \"New York Post\" reviewed the book positively, concluding: \"Although arguably the most courageous and most knowledgeable scientist about vaccines in the United States, Offit lives in fear for his life and that of his family.\" \"The Wall Street Journal\" also praised the book as \"an invaluable chronicle that relates some of the many ways in which the vulnerabilities of anxious parents have been exploited.\"\n\n\"The Philadelphia Inquirer\" wrote that the book \"names names and calls nonsense nonsense\", and provides \"important insight into the fatal flaws of the key arguments of vaccine alarmists.\" The \"Inquirer\" applauded Offit's focus on slanted and sensationalist media coverage of the vaccine–autism issue, but faulted Offit for not holding scientists themselves sufficiently accountable for their failure to communicate the facts to the public.\n\nThe \"Rocky Mountain News\" noted that the book \"turned the tables\" on those who see a pharmaceutical-industry conspiracy behind vaccination, by pointing out that the advocates of the autism–vaccine link receive large sums of money from lawyers and lobbyists. The \"News\" applauded the book's deconstruction of \"misinformation\" from Don Imus, Jenny McCarthy, Joseph Lieberman, and Robert F. Kennedy, Jr., among others, but found Offit's \"sarcasm and brow-beating of those he disagrees with\" to be \"grating\".\n\n\"Salon\" reviewed the book as an \"enlightening, highly readable, and ... timely\" work which \"deconstruct[s] the anti-vaccine movement as one driven by bad science, litigious greed, hype and ego.\" \"Salon\" faulted Offit for minimizing the work that autism advocacy groups have done to raise awareness, create support networks, and obtain research funding; the review noted that Offit focuses instead on aggressive and scientifically \"slanted\" groups like Defeat Autism Now! and Generation Rescue. The review concluded that the book \"effectively pulls back the curtain on the anti-vaccine movement to reveal a crusade grounded less in fact and more in greed and opportunism\".\n\n\"Science\" called the book \"forceful\" and \"an easy-to-read medical thriller about the consequences of greed, hubris, and intellectual sloppiness.\" The review noted that Offit did not discuss the irrationality of human decision-making in the presence of relative risk and both anecdotal and empirical evidence, and mentioned that Offit did not carefully discuss the role of regression. In conclusion, the review observed that the book has emboldened the media to apply scientific principles, and called for using the book's momentum to shift resources from the autism–vaccination debate to research into causes and treatments.\n\nThe \"Journal of Autism and Developmental Disorders\" said the book \"makes an important contribution to popular debates about the etiology and treatment of autism spectrum disorders. The book is arguably the most detailed and thorough history available of the current anti-vaccine movement\". The review noted one possible weakness: the book gives light coverage to the public's fundamental misunderstanding of the epidemiology of autism, in that the public fears an \"autism epidemic\" that may not in fact be occurring. The review concluded with a call to scientists and physicians to follow Offit's lead in communicating to the public even uncomfortable truths about autism.\n\nFour months after its release, \"The New York Times\" reported that the book had been endorsed widely by pediatricians, autism researchers, vaccine companies, and medical journalists, and was \"galvanizing a backlash against the antivaccine movement in the United States.\" Many doctors are critical of \"false equivalence\" in media coverage of the vaccine issue, and now argue that reporters should treat the antivaccine lobby with the same level of indifference as AIDS denialism and other fringe theories.\n\nLater in 2009, the \"New England Journal of Medicine\" reported that the book effectively advocated for vaccines and refuted the vaccine–autism myth. It noted that a particular strength of the book is its outline of the scientific method and the basic principles of probability and causality, and its coverage of the difficulty of explaining science to the public, such as the difference between causality and coincidence. It noted as a weakness the book's several diversions into topics such as breast implants.\n\nOther largely favorable reviews appeared in \"BioScience\", in \"Health Affairs\", and in the \"Journal of Child Neurology\".\n\nIn a guest column for \"The Atlanta Journal-Constitution\", neurologist Jon Poling panned Offit's book as \"a novel of perceived good and evil\". Poling, whose daughter was federally compensated for vaccine injuries, criticized Offit for attacking those with whom he disagrees: \"In the story, Offit takes no prisoners, smearing characters in the vaccine-autism controversy as effortlessly as a rich cream cheese.\"\n\n\n"}
{"id": "2353883", "url": "https://en.wikipedia.org/wiki?curid=2353883", "title": "Bahá'í Faith and science", "text": "Bahá'í Faith and science\n\nA fundamental principle of the Bahá'í Faith is the stated harmony of religion and science. Whilst Bahá'í scripture asserts that true science and true religion can never be in conflict, critics argue that statements by the founders clearly contradict current scientific understanding. `Abdu'l-Bahá, the son of the founder of the religion, stated that \"when a religion is opposed to science it becomes mere superstition\". He also said that true religion must conform to the conclusions of science.\n\nThis latter aspect of the principle seems to suggest that the religion must always accept current scientific knowledge as authoritative, but some Bahá'í scholars have suggested that this is not always the case. On some issues, the Bahá'í Faith subordinates the conclusions of current scientific thought to its own teachings, which the religion takes as fundamentally true. This is because, in the Bahá'í understanding the present scientific view is not always correct, neither is truth said to be only limited to what science can explain. Instead, in the Bahá'í view, knowledge must be obtained through the interaction of the insights obtained from revelation from God and through scientific investigation.\n\n`Abdu'l-Bahá, the son of the founder of the religion, asserted that science and religion cannot be opposed because they are aspects of the same truth; he also affirmed that reasoning powers are required to understand the truths of religion. Shoghi Effendi, the head of the Bahá'í Faith in the first half of the 20th century, described science and religion as \"the two most potent forces in human life\".\n\nThe teachings state that whenever conflict arises between religion and science it is due to human error; either through misinterpretation of religious scriptures or the lack of a more complete understanding of science. `Abdu'l-Bahá explained that religious teachings which are at variance with science should not be accepted; he explained that religion has to be reasonable since God endowed humankind with reason so that they can discover truth.\nScience and religion, in the Bahá'í writings, are compared to the two wings of a bird upon which a person's intelligence can increase, and upon which a person's soul can progress. Furthermore, the Bahá'í writings state that science without religion would lead to a person becoming totally materialistic, and religion without science would lead to a person falling into superstitious practices. `Abdu'l-Bahá in one of his public talks said:\n\n`Abdu'l-Bahá is quoted as saying this:\n\nBahá’u’lláh taught that the universe has \"neither beginning nor ending\", and that the component elements of the material world have always existed and will continue to exist. In the \"Tablet of Wisdom\" (\"Lawh-i-Hikmat\", written 1873-1874). Bahá'u'lláh states: \"That which hath been in existence had existed before, but not in the form thou seest today. The world of existence came into being through the heat generated from the interaction between the active force and that which is its recipient. These two are the same, yet they are different.\" The terminology used here refers to ancient Greek and Islamic philosophy. Jean-Marc Lepain, Robin Mihrshahi, Dale E. Lehman and Julio Savi suggest a possible relation of this statement with the Big Bang theory.\n\nBahá'ís believe that the story of creation in Genesis is a rudimentary account that conveys the broad essential spiritual truths of existence without a level of detail and accuracy that was unnecessary and incomprehensible at the time. Likewise, `Abdu'l-Bahá said that literal story of Adam and Eve cannot be accepted, affirmed, or imagined, and that it \"must be thought of simply as a symbol\". And rather than accepting the idea of a Young Earth, Bahá'í theology accepts that the Earth is ancient.\n\nIn regards to evolution and the origin of man, `Abdu'l-Bahá gave extensive comments on the subject when he addressed western audiences in the beginning of the 20th century. Transcripts of these talks can be found in \"Some Answered Questions\", \"Paris Talks\" and the \"Promulgation of Universal Peace\". `Abdu'l-Bahá describes the human species as evolving from a primitive form to modern man, but that the capacity to form human intelligence was always in existence.\n\n`Abdu'l-Bahá's comments seem to differ from the standard evolutionary picture of human development, where Homo sapiens as one species along with the great apes evolved from a common ancestor living in Africa millions of years ago. \n\nWhile `Abdu'l-Bahá states that man progressed through many stages before reaching this present form, `Abdu'l-Bahá states that humans are a distinct species, and not an animal, and that in every stage of evolution through which humans progressed, they were \"potentially\" humans.\n\nMehanian and Friberg wrote a 2003 article describing their belief that `Abdu'l-Bahá's statements can be entirely reconciled with modern science. Mehanian and Friberg state that `Abdu'l-Bahá's departures from the conventional interpretation of evolution are likely due \"to disagreements with the metaphysical, philosophical, and ideological aspects of those interpretations, not with scientific findings.\" And to this end `Abdu'l-Bahá suggested that a missing link between human and apes would not be found. The idea of a missing link per se was abandoned by science in favor of the idea of evolutionary transitions.\n\nThere are some differences between `Abdu'l-Bahá's statements and current scientific thought. The Bahá’í perspective that religion must be in accordance with science seems to suggest that religion must accept current scientific knowledge as authoritative; but, according to Mehanian and Friberg, this is not necessarily always the case as in their view the present scientific point of view is not always correct, nor truth only limited to what science can explain.\n\nOskooi chose the subject of evolution and Bahá'í belief for his 2009 thesis, and in doing so reviewed other Bahá'í authors' works on the subject. He concluded that, \"The problem of disharmony between scripture and science is rooted in an unwarranted misattribution of scriptural inerrancy.\" In other words, he believes that `Abdu'l-Bahá made statements about biology that were later proved wrong, and that `Abdu'l-Bahá's infallibility should not be applied to scientific matters.\n\nSeveral authors have written on the subject of evolution and Bahá'í belief.\n\nAether, or ether, was a substance postulated in the late 19th century to be the medium for the propagation of light. The Michelson-Morley experiment of 1887 made an effort to find the aether, but its failure to detect it led Einstein to devise his Special theory of Relativity. Further developments in modern physics, including General Relativity, Quantum Field Theory, and String Theory all incorporate the non-existence of the aether, and today the concept is considered obsolete scientific theory.\n\n`Abdu'l-Bahá's use of the aether concept in one of his talks - his audience including scientists of the time - has been the source of some controversy. The chapter in `Abdu'l-Bahá's \"Some Answered Questions\" which mentions aether differentiates between things that are \"perceptible to the senses\" and those which are \"realities of the intellect\" and not perceptible to the senses. `Abdu'l-Bahá includes \"ethereal matter\" (also translated as \"etheric matter\"), heat, light and electricity among other things, in the second group of things which are not perceptible to the senses, and are concepts which are arrived at intellectually to explain certain phenomena. The Universal House of Justice referring to `Abdu'l-Bahá's use of the word state that, \"in due course, when scientists failed to confirm the physical existence of the 'ether' by delicate experiments, they constructed other intellectual concepts to explain the same phenomena\" which is consistent with `Abdu'l-Bahá's categorization of aether.\n\nRobin Mishrahi in his published paper on the issue titled \"Ether, Quantum Physics and the Bahá'í Writings\" wrote,\n\nBahá'u'lláh wrote:\nBahá'ís later pointed to this as a statement about the discovery of nuclear energy and the use of nuclear weapons.\n\nIn 1873 Bahá'u'lláh wrote:\n\nBahá'u'lláh stated:\nThe idea that every planet has \"its own creatures\" is controversial. While no direct evidence has been found of extraterrestrial life, theories range from the Rare Earth hypothesis, that the earth may be unique in hosting life, to the more common idea that it would be improbable for life \"not\" to exist somewhere other than Earth.\n\nVery few Bahá'í sources deal with this idea in detail. Shoghi Effendi wrote in a letter,\nOn the same subject, the Universal House of Justice wrote,\n\n\n"}
{"id": "30863905", "url": "https://en.wikipedia.org/wiki?curid=30863905", "title": "Behavior Genetics Association", "text": "Behavior Genetics Association\n\nThe Behavior Genetics Association (BGA) is a learned society established in 1970 and which promotes research into the connections between heredity and behavior, both human and animal. Its members support education and training in behavior genetics; and aid in the dissemination of knowledge concerning genetics and behavior, and its implications, such as in health.\n\nThe association's goal is \"to promote scientific study of the interrelationship of genetic mechanisms and behavior, both human and animal; to encourage and aid the education and training of research workers in the field of behavior genetics; and to aid in the dissemination and interpretation to the general public of knowledge concerning the interrelationship of genetics and behavior, and its implications for health and human development and education.\" To help attain these goals, the society organizes an annual meeting and publishes its official scientific journal entitled \"Behavior Genetics\". The first 12 annual meetings were held in different places within the United States. In 1983, the association held its first annual meeting in Europe (London) and since then meetings have been held in Australia, Canada, France, Korea, Spain, Sweden, Norway, the Netherlands, and the United Kingdom, as well as various US states.\n\nThe society has three classes of members: Regular consisting of persons who teach or perform research related to behavioral genetics, Associate Members – students in good standing at a recognized college or university and retired members. Members receive a complimentary subscription to the society's journal as well as discounted registration rates for the association's annual meetings.\n\nThe society's business are conducted by a board of directors, called the executive committee. The board consists of 9 members: president, president-elect, past-president, secretary, treasurer, an information officer, and three members-at-large (representing the general members, associate members, and members from outside North America). Members of the executive committee serve three-year terms. To ensure continuity, one member-at-large is elected every year.\n\nPresidents serve three-year terms. Upon election, they become president-elect and they serve as chair of the program committee for that year. After one year they become president and in the third year of their term they serve as past-president. The association's first president was Theodosius Dobzhansky. Other notable presidents include Irving I. Gottesman (1976), John C. Loehlin (1980), Steven G. Vandenberg (1984), Sandra Scarr (1985), Robert Plomin (1989), Thomas J. Bouchard, Jr. (1993), Glayde Whitney (1994), Nick Martin (1996–1997), and Dorret Boomsma (2008). Whitney's presidential address at the 1995 annual meeting in Richmond, Virginia, on the possible genetic roots of the relationship between race and crime, caused a controversy resulting in several resignations from the association's executive committee. The association subsequently declared that \"the Association has no official spokesman and that the presidential address does not represent official policy of the association\". In addition, it was stated that \"members are not encouraged to express their personal political and moral views\" in presentations given at the meeting, which should be strictly scientific.\n\nThe association gives several yearly awards for accomplishments in the field of behavioral genetics.The Dobzhansky Award, named after its first president, is given for lifetime accomplishments and is chosen by the three most recent past presidents. The Fuller & Scott Award is an early career award for accomplishments by researchers that are within seven years of receiving their terminal degree. The award is named after former presidents John L. Fuller and John Paul Scott and the award committee is the same as for the Dobzhansky Award. The Thompson and Rowe awards, are named after former president W. R. Thompson (1977), and David C. Rowe, a pioneer of Gene-Environment interaction research respectively. The Thompson is given for the oral presentation given by an associate (student) member, and judged as most outstanding during the annual meeting. The Rowe similarly recognises the most outstanding student poster presentation. The awards committee for these awards consists of the past president together with the three members-at-large of the executive committee. In addition, the society gives the annual Fulker Award for an outstanding paper published in \"Behavior Genetics\". This award is named after former president David Fulker (1983), who also was a previous editor-in-chief of the journal. The awards committee consists of the journal's editorial advisory board.\n\n"}
{"id": "2856256", "url": "https://en.wikipedia.org/wiki?curid=2856256", "title": "Bermuda Principles", "text": "Bermuda Principles\n\nThe Bermuda Principles set out rules for the rapid and public release of DNA sequence data. The Human Genome Project, a multinational effort to sequence the human genome, generated vast quantities of data about the genetic make-up of humans and other organisms. But, in some respects, even more remarkable than the impressive quantity of data generated by the Human Genome Project is the speed at which that data has been released to the public. At a 1996 summit in Bermuda, leaders of the scientific community agreed on a groundbreaking set of principles requiring that all DNA sequence data be released in publicly accessible databases within twenty-four hours after generation. These \"Bermuda Principles\" (also known as the \"Bermuda Accord\") contravened the typical practice in the sciences of making experimental data available only after publication. These principles represent a significant achievement of private ordering in shaping the practices of an entire industry and have established rapid pre-publication data release as the norm in genomics and other fields.\n\nThe three principles retained originally were:\n\n\n\n"}
{"id": "2453751", "url": "https://en.wikipedia.org/wiki?curid=2453751", "title": "Big Picture Science", "text": "Big Picture Science\n\nBig Picture Science (formerly titled Are We Alone?) is the SETI Institute's weekly science radio program, hosted by Senior Astronomer Seth Shostak and Molly Bentley, the executive producer of the radio show.\n\nThe show covers a broad range of topics in the sciences as they relate to our understanding of the origin and evolution of life on Earth, and what life elsewhere in the universe might be like. Each week's show has a theme and allows scientists to share their latest findings and speculate on the implications for SETI (the Search for Extra-Terrestrial Intelligence) and society in general.\nGuests include scientists and researchers from academic, public and private spheres (including NASA), popular science writers, cultural critics and ethicists focused on science. Phil Plait is a recurring guest, with an own segment called \"Brains on Vacation\". One show a month is devoted to critical thinking and debunkers of junk science and the paranormal: \"Skeptic Check\".\n\nThe show is broadcast on public radio stations KLIV in San Jose, WCMU-FM in Michigan, WVPE in Indiana, WHRV in Virginia, WIEC in Wisconsin as well as World FM in Tawa, Wellington, New Zealand. The show is also available for download via podcast and direct download from the show's website where archived shows from 2006 onwards can also be found.\n\nIn May 2011, Executive Producer Molly Bentley announced on the show's blog that its name would be changing to \"Big Picture Science\" in mid-July.\n\n"}
{"id": "36804196", "url": "https://en.wikipedia.org/wiki?curid=36804196", "title": "Bringelly Shale", "text": "Bringelly Shale\n\nBringelly Shale is a component of the Wianammatta group of sedimentary rocks in the Sydney Basin of eastern Australia. It was formed in the Triassic Period. It is most often seen in the western parts of the city. The shale has its greatest geographical extent at Bringelly, near the suburb of Liverpool. It is similar to Ashfield Shale, though differing in having a greater amount of sandstone and lacking sideritic mudstone bands. The average thickness is around 60 metres. The Bringelly Shale was deposited in a swampy alluvial plain with meandering streams flowing from the west forming discontinuous beds of sandstone.\n\nSydney Basin, Hawkesbury sandstone, Ashfield Shale, Wianamatta shale, Mittagong formation and Narrabeen Group.\n"}
{"id": "12419087", "url": "https://en.wikipedia.org/wiki?curid=12419087", "title": "Carl von Heyden", "text": "Carl von Heyden\n\nCarl Heinrich Georg(es) von Heyden (20 January 1793 Frankfurt – 7 July 1866) was a German senator and entomologist. He collected insects in all Orders but was especially interested in Coleoptera, Microlepidoptera, Hymenoptera, Diptera and fossil insects. His collections are divided between the German Entomological Institute and the Senckenberg Museum.\n\nHe studied forestry under Johann Matthäus Bechstein at the Dreißigacker Forest Academy near Meiningen, then continued his education at the University of Heidelberg. With his son, Lukas von Heyden, he conducted studies of fossil insects found in lignite. In addition to his entomological research, he performed investigations of reptile specimens collected by Eduard Rüppell in North Africa.\n\nIn 1817, he was co-founder of the \"Senckenbergischen Naturforschenden Gesellschaft\".\n\n"}
{"id": "23376765", "url": "https://en.wikipedia.org/wiki?curid=23376765", "title": "Ceremonial marriage", "text": "Ceremonial marriage\n\nCeremonial marriage is a common form of marriage in which a couple follows laws and procedures specified by the state in order to gain recognition of their marriage (ex. buying a marriage license, participating in a ceremony led by an authorized official, having witnesses at a ceremony). They are often accompanied by weddings, and have different forms, reflecting particular religious and philosophical views of the couple.\n\nCeremonial marriage is an opposite to common-law marriage.\n"}
{"id": "528808", "url": "https://en.wikipedia.org/wiki?curid=528808", "title": "Chess symbols in Unicode", "text": "Chess symbols in Unicode\n\nChess symbols are part of Unicode. Instead of using images, one can represent chess pieces by symbols that are defined in the Unicode character set. This makes it possible to:\n\nIn order to display or print these symbols, one has to have one or more fonts with good Unicode support installed on the computer, and the document (Web page, word processor document, etc.) must use one of these fonts.\n\nChess symbols are part of the Miscellaneous Symbols block.\n"}
{"id": "7243928", "url": "https://en.wikipedia.org/wiki?curid=7243928", "title": "Cyclops laser", "text": "Cyclops laser\n\nCyclops was a high-power laser built at the Lawrence Livermore National Laboratory (LLNL) in 1975. It was the second laser constructed in the lab's \"Laser\" program, which aimed to study inertial confinement fusion (ICF).\n\nThe Cyclops was a single-beam Neodymium glass (Nd:glass) laser. The Janus laser, a two-beam version of it, was also completed in 1975. The main scientific aims of its construction were for the study of nonlinear focusing effects in high power laser beams, novel amplification techniques (disks of Nd:glass at the brewster angle), spatial filtering techniques which would be used on subsequent higher powered lasers such as the Argus and Shiva lasers and for inertial confinement fusion (ICF) research.\n\nEven the earliest ICF laser experiments demonstrated that one of the main problems which needed to be addressed was poor focusing of the beams and damage caused to optics due to the beam's extreme intensities caused by the optical Kerr effect, where, because the beam is so intense, that during its passage through either air or glass the electric field of the light actually alters the index of refraction of the material and causes the beam at the most intense points to \"self focus\" down to filament like structures of extremely high intensity. When a beam collapses into extremely high intensity filaments like this, it can easily exceed the optical damage threshold of laser glass and other optics, severely damaging them by creating pits, cracks and grey tracks through the glass.\nThis novel problem only became obvious as the lasers were scaled up in power to where nonlinear phenomena occur with very intense beams of light. LLNL's Krupke stated: \nIf the intensity of the light gets high enough —as in fusion lasers— the electric field in the light perturbs the atoms of the glass so strongly that the glass responds in a nonlinear way.\nAt the time there was no strong theoretical understanding of these effects, and predicting them was difficult. However, LLNL researchers combined their own efforts with those of the commercial glass vendors and were able to develop a new predictive tool which explained the relationship between the nonlinear effect intensity to all types of glass. As Krupke noted:\nIt was like the Rosetta stone. With this quantitative correspondence, they were able to plot the nonlinear refractive performance of millions of glasses and find the one with the lowest possible value. We then worked with our industrial partners to make a composition with the characteristics we needed.\nAlthough using the proper glass was able to reduce the problem as much as possible, the problem still existed. For smaller experiments this would not be enough of an effect to worry about, but with the much larger and more powerful Shiva already under design, some way of further improving the beam smoothness of the laser needed to be studied.\n\nThe simplest way to eliminate these effects was to filter them out physically using what essentially amounts to a Fourier transform technique applied to the beam's spaitial intensity profile. Imaging spatial filters are, in effect, small inverted telescopes inserted in the laser beam to focus the light through a pinhole. Many modes of spatial anisotropy would result in a very low angle of diffraction off the centerline however, so to improve the smoothing performance, the spatial filter tube is extremely long, thereby maximizing the distance the filaments moved from the centerline. Such a laser had not previously been built, the earlier Janus laser, which explored the Nd:glass laser itself, was only a few meters long.\n\nIt was precisely the problems of building a long laser that Cyclops was built to study. Cyclops was effectively a single-beam of the larger Shiva design, one that could be completed as quickly as possible in order to identify potential problems and come up with the best arrangement for the filters. In this goal Cyclops was successful, and every major ICF effort since has used the spatial filtering technique, leading to ever-growing laser \"beamlines\" on the order of 100 m today.\n\nWhile Cyclops was still under construction, another LLNL laser was being built that also incorporated the spatial filtering technique, Argus. Argus passed its light through a series of amplifiers, with spatial filters between each stage and easily achieved terawatt beam powers.\n\n\n"}
{"id": "5943744", "url": "https://en.wikipedia.org/wiki?curid=5943744", "title": "Divisor summatory function", "text": "Divisor summatory function\n\nIn number theory, the divisor summatory function is a function that is a sum over the divisor function. It frequently occurs in the study of the asymptotic behaviour of the Riemann zeta function. The various studies of the behaviour of the divisor function are sometimes called divisor problems.\n\nThe divisor summatory function is defined as\n\nwhere \n\nis the divisor function. The divisor function counts the number of ways that the integer \"n\" can be written as a product of two integers. More generally, one defines \n\nwhere \"d\"(\"n\") counts the number of ways that \"n\" can be written as a product of \"k\" numbers. This quantity can be visualized as the count of the number of lattice points fenced off by a hyperbolic surface in \"k\" dimensions. Thus, for \"k\"=2, \"D\"(\"x\") = \"D\"(\"x\") counts the number of points on a square lattice bounded on the left by the vertical-axis, on the bottom by the horizontal-axis, and to the upper-right by the hyperbola \"jk\" = \"x\". Roughly, this shape may be envisioned as a hyperbolic simplex. This allows us to provide an alternative expression for \"D\"(\"x\"), and a simple way to compute it in formula_4 time:\n\nIf the hyperbola in this context is replaced by a circle then determining the value of the resulting function is known as the Gauss circle problem.\n\nSequence of D(n):\n0, 1, 3, 5, 8, 10, 14, 16, 20, 23, 27, 29, 35, 37, 41, 45, 50, 52, 58, 60, 66, 70, 74, 76, 84, 87, 91, 95, 101, 103, 111, ...\n\nFinding a closed form for this summed expression seems to be beyond the techniques available, but it is possible to give approximations. The leading behaviour of the series is not difficult to obtain. Peter Gustav Lejeune Dirichlet demonstrated that\n\nwhere formula_8 is the Euler–Mascheroni constant, and the non-leading term is\n\nHere, formula_10 denotes Big-O notation. The Dirichlet divisor problem, precisely stated, is to find the smallest value of formula_11 for which \n\nholds true, for any formula_13. As of today, this problem remains unsolved. Progress has been slow. Many of the same methods work for this problem and for Gauss's circle problem, another lattice-point counting problem. Section F1 of \"Unsolved Problems in Number Theory\"\n\nsurveys what is known and not known about these problems.\n\n\nSo, formula_27 lies somewhere between 1/4 and 131/416 (approx. 0.3149); it is widely conjectured to be 1/4. Theoretical evidence lends credence to this conjecture, since formula_28 has a (non-Gaussian) limiting distribution. The value of 1/4 would also follow from a conjecture on exponent pairs.\n\nIn the generalized case, one has\n\nwhere formula_30 is a polynomial of degree formula_31. Using simple estimates, it is readily shown that \n\nfor integer formula_33. As in the formula_34 case, the infimum of the bound is not known for any value of formula_35. Computing these infima is known as the Piltz divisor problem, after the name of the German mathematician Adolf Piltz (also see his German page). Defining the order formula_36 as the smallest value for which formula_37 holds, for any formula_38, one has the following results (note that formula_39 is the formula_11 of the previous section):\n\n\nBoth portions may be expressed as Mellin transforms:\n\nfor formula_46. Here, formula_47 is the Riemann zeta function. Similarly, one has\n\nwith formula_49. The leading term of formula_50 is obtained by shifting the contour past the double pole at formula_51: the leading term is just the residue, by Cauchy's integral formula. In general, one has \n\nand likewise for formula_53, for formula_33.\n\n"}
{"id": "42084535", "url": "https://en.wikipedia.org/wiki?curid=42084535", "title": "Emaravirus", "text": "Emaravirus\n\nEmaravirus is a genus of plant viruses. The genus has more than six species, including: \"European mountain ash ringspot-associated virus\" (type species), \"Fig mosaic virus\", \"Pigeonpea sterility mosaic virus\", \"Raspberry leaf blotch virus\", \"Rose rosette virus\" and Maize red stripe virus. \"European mountain ash ringspot-associated virus\" is associated with a leaf mottling and ringspot disease of European mountain ash \"Sorbus aucuparia\". It can be transmitted by grafting and possibly mites.\n\nThe virion particle is between 80-100 nm and consists of an enveloped ribonucleocapsid that exhibits helical symmetry. The genomes are segmented, consisting of four strands of negative-sense single-stranded RNA.\n\nEmaravirus has the closest phylogenetic relationship with members of the genera \"Tospovirus\" and \"Orthobunyavirus\". The 3' and 5' ends of the genomic RNAs are complementary (similar to viruses of the Bunyaviridae family), with the sequence 5'-AGUAGUGUUCUCC-3' at the 5' terminus and 5'-GGAGUUCACUACU-3' at the 3' terminus. However, the number of genome segments and gene sequences distinguishes emaraviruses from bunyaviruses and tenuiviruses.\n"}
{"id": "1626494", "url": "https://en.wikipedia.org/wiki?curid=1626494", "title": "Fuzzy measure theory", "text": "Fuzzy measure theory\n\nIn mathematics, fuzzy measure theory considers generalized measures in which the additive property is replaced by the weaker property of monotonicity. The central concept of fuzzy measure theory is the fuzzy measure (also \"capacity\", see ) which was introduced by Choquet in 1953 and independently defined by Sugeno in 1974 in the context of fuzzy integrals. There exists a number of different classes of fuzzy measures including plausibility/belief measures; possibility/necessity measures; and probability measures which are a subset of classical measures.\n\nLet formula_1 be a universe of discourse, formula_2 be a class of subsets of formula_1, and formula_4. A function formula_5 where\n\n\nis called a \"fuzzy measure\". \nA fuzzy measure is called \"normalized\" or \"regular\" if formula_8.\n\nFor any formula_9, a fuzzy measure is:\n\n\nUnderstanding the properties of fuzzy measures is useful in application. When a fuzzy measure is used to define a function such as the Sugeno integral or Choquet integral, these properties will be crucial in understanding the function's behavior. For instance, the Choquet integral with respect to an additive fuzzy measure reduces to the Lebesgue integral. In discrete cases, a symmetric fuzzy measure will result in the ordered weighted averaging (OWA) operator. Submodular fuzzy measures result in convex functions, while supermodular fuzzy measures result in concave functions when used to define a Choquet integral.\n\nLet \"g\" be a fuzzy measure, the Möbius representation of \"g\" is given by the set function \"M\", where for every formula_22, \nThe equivalent axioms in Möbius representation are:\n\n\nA fuzzy measure in Möbius representation \"M\" is called \"normalized\"\nif formula_28\n\nMöbius representation can be used to give an indication of which subsets of X interact with one another. For instance, an additive fuzzy measure has Möbius values all equal to zero except for singletons. The fuzzy measure \"g\" in standard representation can be recovered from the Möbius form using the Zeta transform:\n\nFuzzy measures are defined on a semiring of sets or monotone class which may be as granular as the power set of X, and even in discrete cases the number of variables can as large as 2. For this reason, in the context of multi-criteria decision analysis and other disciplines, simplification assumptions on the fuzzy measure have been introduced so that it is less computationally expensive to determine and use. For instance, when it is assumed the fuzzy measure is \"additive\", it will hold that formula_30 and the values of the fuzzy measure can be evaluated from the values on X. Similarly, a \"symmetric\" fuzzy measure is defined uniquely by |X| values. Two important fuzzy measures that can be used are the Sugeno- or formula_31-fuzzy measure and \"k\"-additive measures, introduced by Sugeno and Grabisch respectively.\n\nThe Sugeno formula_31-measure is a special case of fuzzy measures defined iteratively. It has the following definition:\n\nLet formula_33 be a finite set and let formula_34. A Sugeno formula_31-measure is a function formula_36 such that\n\n\nAs a convention, the value of g at a singleton set formula_42\nis called a density and is denoted by formula_43. In addition, we have that formula_31 satisfies the property\n\nTahani and Keller as well as Wang and Klir have showed that once the densities are known, it is possible to use the previous polynomial to obtain the values of formula_31 uniquely.\n\nThe \"k\"-additive fuzzy measure limits the interaction between the subsets formula_47 to size formula_48. This drastically reduces the number of variables needed to define the fuzzy measure, and as \"k\" can be anything from 1 (in which case the fuzzy measure is additive) to X, it allows for a compromise between modelling ability and simplicity.\n\nA discrete fuzzy measure \"g\" on a set X is called \"k-additive\" (formula_49) if its Möbius representation verifies formula_50, whenever formula_51 for any formula_26, and there exists a subset \"F\" with \"k\" elements such that formula_53.\n\nIn game theory, the Shapley value or Shapley index is used to indicate the weight of a game. Shapley values can be calculated for fuzzy measures in order to give some indication of the importance of each singleton. In the case of additive fuzzy measures, the Shapley value will be the same as each singleton.\n\nFor a given fuzzy measure \"g\", and formula_54, the Shapley index for every formula_55 is:\n\nThe Shapley value is the vector formula_57\n\n\n\n"}
{"id": "17118806", "url": "https://en.wikipedia.org/wiki?curid=17118806", "title": "Gallium indium arsenide antimonide phosphide", "text": "Gallium indium arsenide antimonide phosphide\n\nGallium indium arsenide antimonide phosphide ( or GaInPAsSb) is a semiconductor material.\n\nResearch has shown that GaInAsSbP can be used in the manufacture of mid-infrared light-emitting diodes and thermophotovoltaic cells.\n\nGaInAsSbP layers can be grown by heteroepitaxy on indium arsenide, gallium antimonide and other materials. The exact composition can be tuned in order to make it lattice matched. The presence of five elements in the alloy allows extra degrees of freedom, making it possible to fix the lattice constant while varying the bandgap. Eg GaInPAsSb is lattice matched to InAs.\n\n"}
{"id": "1205315", "url": "https://en.wikipedia.org/wiki?curid=1205315", "title": "Gist (computing)", "text": "Gist (computing)\n\nIn computing, Gist is a scientific graphics library written in C by David H. Munro of Lawrence Livermore National Laboratory. It supports three graphics output devices: X Window, PostScript, and Computer Graphics Metafiles (CGM). The library is promoted as being small (writing directly to Xlib), efficient, and full-featured. Portability is restricted to systems running X Window (essentially the Unix world).\n\nThere is a Python port called PyGist; it is used as one of several optional graphics front-ends of the scientific library SciPy. PyGist is also ported to Mac and MS Windows.\n"}
{"id": "8283008", "url": "https://en.wikipedia.org/wiki?curid=8283008", "title": "GoPubMed", "text": "GoPubMed\n\nGoPubMed was a knowledge-based search engine for biomedical texts. The\nGene Ontology (GO) and Medical Subject Headings (MeSH) served as \"Table of contents\" in order to structure the millions of articles in the MEDLINE database. MeshPubMed was at one point a separate project, but the two were merged.\n\nThe technologies used in GoPubMed were generic and could in general be applied to any kind of texts and any kind of knowledge bases. The system was developed at the Technische Universität Dresden by Michael Schroeder and his team at Transinsight.\n\nGoPubMed was recognized with the 2009 red dot: best of the best award in the category communication design – graphical user interfaces and interactive tool. Transinsight was recognized with the German Innovation Prize IT for its developments in Enterprise Semantic Intelligence at CeBIT 2011.\n\n"}
{"id": "1688179", "url": "https://en.wikipedia.org/wiki?curid=1688179", "title": "History of materials science", "text": "History of materials science\n\nMaterials science has shaped the development of civilizations since the dawn of mankind. Better materials for tools and weapons has allowed mankind to spread and conquer, and advancements in material processing like steel and aluminum production continue to impact society today. Historians have regarded materials as such an important aspect of civilizations such that entire periods of time have defined by the predominant material used (Stone Age, Bronze Age, Iron Age, etc.). For most of recorded history, control of materials had been through alchemy or empirical means at best. The study and development of chemistry and physics assisted the study of materials, and eventually the interdisciplinary study of materials science emerged from the fusion of these studies. The history of materials science is the study of how different materials were used and developed through the history of Earth and how those materials affected the culture of the peoples of the Earth.\n\nIn many cases different cultures leave their materials as the only records which anthropologists can use to define the existence of such cultures. The progressive use of more sophisticated materials allows archeologists to characterize and distinguish between peoples. This is partially due to the major material of use in a culture and to its associated benefits and drawbacks. Stone-Age cultures were limited by which rocks they could find locally and by which they could acquire by trading. The use of flint around 300,000 BCE is sometimes considered the beginning of the use of ceramics. The use of polished stone axes marks a significant advance because a much wider variety of rocks could serve as tools.\n\nThe innovation of smelting and casting metals in the Bronze Age started to change the way that cultures developed and interacted with each other. Starting around 5500 BCE, early smiths began to re-shape native metals of copper and gold - without the use of fire - for tools and weapons. The heating of copper and its shaping with hammers began around 5000 BCE. Melting and casting started around 4000 BCE. Metallurgy had its dawn with the reduction of copper from its ore around 3500 BCE. The first alloy, bronze came into use around 3000 BCE. Iron-working came into prominence from about 1200 BCE.\n\nIn the 10th century BCE glass production began in ancient Near East. In the 3rd century BCE people in ancient India developed wootz steel, the first crucible steel. In the 1st century BCE glassblowing techniques flourished in Phoenicia. In the 2nd century CE steel-making became widespread in Han Dynasty China. The 4th century CE saw the production of the Iron pillar of Delhi, the oldest surviving example of corrosion-resistant steel.\n\nWood, bone, stone, and earth are some of the materials which formed the structures of the Roman Empire. Certain structures were made possible by the character of the land upon which these structures are built; a volcanic peninsula with stone aggregates and conglomerates containing crystalline material will produce material which weathers differently from soft, sedimentary rock and silt. That is one of the reasons that the concrete Pantheon of Rome could last for 1850 years, and why the thatched farmhouses of Holland sketched by Rembrandt have long since decayed.\n\nAfter the thighbone daggers of the early hunter-gatherers were superseded by wood and stone axes, and then by copper, bronze and iron implements of the Roman civilization, more precious materials could then be sought, and gathered together. Thus the medieval goldsmith Benvenuto Cellini could seek and defend the gold which he had to turn into objects of desire for dukes and popes. \"The Autobiography of Benvenuto Cellini\" contains one of the first descriptions of a metallurgical process.\n\nThe use of materials begins in the stone age. Typically materials such as bone, fibers, feathers, shells, animal skin, and clay were used for weapons, tools, jewelry, and shelter. The earliest tools were in the paleolithic age, called Oldowan. These were tools created from chipped rocks that would be used for scavenging purpose. As history carried on into the Mesolithic age, tools became more complex and symmetrical in design with sharper edges. Moving into the Neolithic age, agriculture began to develop as new was to form tools for farming were discovered. Nearing the end of the stone age, humans began using copper, gold, and silver as a material. Due to these metals softness, the general use was for ceremonial purposes and to create ornaments or decorations and did not replace other materials for use in tools. The simplicity of the tools used reflected on the simple understanding of the human species of the time.\n\nThe use of copper had become very apparent to civilizations, such as its properties of elasticity and plasticity that allow it to be hammered into useful shapes, along with its ability to be melted and pored into intricate shapes. Although the advantages of copper were many, the materials was to soft to find large scale usefulness. Through experimentation or by chance, additions to copper lead to increased hardness of a new metal alloy, called bronze. Bronze was originally composed of copper and arsenic, forming arsenic bronze.\n\nProto-porcelain material has been discovered dating back to the Neolthic period, with shards of material found in archaeological sites from the Eastern Han period in China. These wares are estimated to have been fired from 1260 to 1300 °C. In the 8th century, porcelain was invented in Tang Dynasty China. Porcelain in china resulted in a methodical development of widely used kilns that increased the quality and quantity that procelain could be produced. Tin-glazing of ceramics is invented by Arabic chemists and potters in Basra, Iraq.\n\nIn the 9th century, stonepaste ceramics were invented in Iraq, and lustreware appeared in Mesopotamia.\n\nIn the 11th century, Damascus steel is developed in the Middle East. In the 15th century, Johann Gutenberg develops type metal alloy and Angelo Barovier invents cristallo, a clear soda-based glass.\n\nIn the 16th century, Vannoccio Biringuccio publishes his Pirotechnia, the first systematic book on metallurgy, Georg Agricola writes De Re Metallica, an influential book on metallurgy and mining, and glass lens are developed in the Netherlands and used for the first time in microscopes and telescopes.\n\nIn the 17th century, Galileo's \"Two New Sciences\" (strength of materials and kinematics) includes the first quantitative statements in the science of materials. In the 18th century, William Champion patents a process for the production of metallic zinc by distillation from calamine and charcoal, Bryan Higgins was issued a patent for hydraulic cement (stucco) for use as an exterior plaster, and Alessandro Volta makes a copper/zinc acid battery.\n\nIn the 19th century, Thomas Johann Seebeck invents the thermocouple, Joseph Aspin invents Portland cement, Charles Goodyear invents vulcanized rubber, Louis Daguerre and William Fox Talbot invent silver-based photographic processes, James Clerk Maxwell demonstrates color photography, and Charles Fritts makes the first solar cells using selenium waffles.\n\nBefore the early 1800s, aluminum had not been produced as an isolated metal. It wasn't until 1825 that Hans Christian Ørsted discovered how to create elemental aluminum via the reduction of aluminum chloride. Since aluminum is a light element with good mechanical properties, it was widely sought to replace heavier less functional metals like silver and gold. Napoleon III used aluminum plates and utensils for his honored guests, while the rest were given silver. However, this process was still expensive and was still not able to produce the metal in large quantities. In 1886, American Charles Martin Hall and Frenchman Paul Héroult invented a process completely independent of each other to produce aluminum from aluminum oxide via electrolysis. This process would allow aluminum to be manufactured cheaper than ever before, and laid the groundwork for turning the element from a precious metal into an easily obtainable commodity. Around the same time in 1888, Carl Josef Bayer was working in St Petersburg, Russia to develop a method to make pure alumina for the textile industry. This process involved dissolving the aluminum oxide out of the bauxite mineral to produce gibbsite, which can then be purified back into raw alumina. The Bayer process and the Hall-Héroult process are still used today to produce a majority of the world's alumina and aluminum.\n\nMost fields of studies have a founding father, such as Newton in physics and Lavoisier in chemistry. Materials science on the other hand has no central figure that set in motion materials studies. In the 1940s, wartime collaborations of multiple fields of study to produce technological advances became a structure to the future field of study that would become known as material science and engineering. During the Cold War in the 1950s, US President Science Advisory Committee (PSAC) made materials a priority when it realized that materials were the limiting factor for advances in space and military technology. The Department of defense signed a contract with 5 universities (Harvard, MIT, Brown, Stanford, and Chicago) providing over $13 million for material research. Several institutions departments changed titles from \"metallurgy\" to \"metallurgy and materials science\" in 1960's.\n\nIn the early part of the 20th century, most engineering schools had a department of metallurgy and perhaps of ceramics as well. Much effort was expended on consideration of the austenite-martensite-cementite phases found in the iron-carbon phase diagram that underlies steel production. The fundamental understanding of other materials was not sufficiently advanced for them to be considered as academic subjects. In the post-WWII era, the systematic study of polymers advanced particularly rapidly. Rather than create new polymer science departments in engineering schools, administrators and scientists began to conceive of materials science as a new interdisciplinary field in its own right, one that considered all substances of engineering importance from a unified point of view. Northwestern University instituted the first materials science department in 1955.\n\nDr. Richard E. Tressler was an international leader in the development of high temperature materials. He pioneered high temperature fiber testing and use, advanced instrumentation and test methodologies for thermostructural materials, and design and performance verification of ceramics and composites in high temperature aerospace, industrial and energy applications. He was founding director of the Center for Advanced Materials (CAM) which supported many faculty and students from the College of Earth and Mineral Science, the Eberly College of Science, the College of Engineering, the Materials Research Laboratory and the Applied Research Laboratories at Penn State on high temperature materials. His vision for interdisciplinary research played a key role in the creation of the Materials Research Institute. Tressler's contribution to materials science is celebrated with a Penn State lecture named in his honor.\n\nThe Materials Research Society (MRS) has been instrumental in creating an identity and cohesion for this young field. MRS was the brainchild of researchers at Penn State University and grew out of discussions initiated by Prof. Rustum Roy in 1970. The first meeting of MRS was held in 1973. As of 2006, MRS has grown into an international society that sponsors a large number of annual meetings and has over 13,000 members. MRS sponsors meetings that are subdivided into symposia on a large variety of topics as opposed to the more focused meetings typically sponsored by organizations like the American Physical Society or the IEEE. The fundamentally interdisciplinary nature of MRS meetings has had a strong influence on the direction of science, particularly in the popularity of the study of soft materials, which are in the nexus of biology, chemistry, physics and mechanical and electrical engineering.\nBecause of the existence of integrative textbooks, materials research societies and university chairs in all parts of the world, BA, MA and PhD programs and other indicators of discipline formation, it is fair to call materials science (and engineering) a discipline.\n\nIn 1958, President Dwight D. Eisenhower created the Advanced Research Project Agency (ARPA), referred to as the Defense Advanced Research Project Agency (DARPA) since 1996. In 1960, ARPA encouraged the establishment of interdisciplinary laboratories (IDL's) on university campuses, which would be dedicated to the research of materials, as well as to the education of students on how to conduct materials science research. ARPA offered 4-year IDL contracts to universities, originally to Cornell University, University of Pennsylvania, and Northwestern University, eventually granting 9 more contracts. Although ARPA is no longer in control of the IDL program (the National Science Foundation took over the program in 1972), the original establishment of IDL's marked a significant milestone in the United States' research and development of materials science.\n\n\n\n"}
{"id": "14372651", "url": "https://en.wikipedia.org/wiki?curid=14372651", "title": "Indian political philosophy", "text": "Indian political philosophy\n\nIndian political philosophy may be categorized into several distinct traditions, including: the Vedic (c. 1200 BCE - 10th century CE); the Jain-Buddhist-Hindu (6th century BCE - 2nd century CE); the Indo-Islamic (10th century CE-1857); the modern or Indo-British (c. 1857 - 1947); and the contemporary (post-independence - present).\n\nIn India as elsewhere, political philosophy involves on the one hand speculations on the relationships between individual, society and state, and detailed treatises on the mechanics of statecraft, state policy, war and diplomacy and international relations.\n\nContemporary Indian political philosophy is an emerging discipline garnering increasing interest with the rise of comparative political theory. The Indian political theorist Aakash Singh Rathore has been attempting to cultivate the discipline with the 2010 book \"Indian Political Thought--A Reader\" (Routledge), and the 2017 monograph \"Indian Political Theory: Laying the Groundwork for Svaraj\" (Routledge).\n\n"}
{"id": "10221795", "url": "https://en.wikipedia.org/wiki?curid=10221795", "title": "Integrative bioinformatics", "text": "Integrative bioinformatics\n\nIntegrative bioinformatics is a discipline of bioinformatics that focuses on problems of data integration for the life sciences.\n\nWith the rise of high-throughput (HTP) technologies in the life sciences, particularly in molecular biology, the amount of collected data has grown in an exponential fashion. Furthermore, the data are scattered over a plethora of both public and private repositories, and are stored using a large number of different formats. This situation makes searching these data and performing the analysis necessary for the extraction of new knowledge from the complete set of available data very difficult. Integrative bioinformatics attempts to tackle this problem by providing unified access to life science data.\n\nIn the Semantic Web approach, data from multiple websites or databases is searched via metadata. Metadata is machine-readable code, which defines the contents of the page for the program so that the comparisons between the data and the search terms are more accurate. This serves to decrease the number of results that are irrelevant or unhelpful. Some meta-data exists as definitions called ontologies, which can be tagged by either users or programs; these serve to facilitate searches by using key terms or phrases to find and return the data. Advantages of this approach include the general increased quality of the data returned in searches and with proper tagging, ontologies finding entries that may not explicitly state the search term but are still relevant. One disadvantage of this approach is that the results that are returned come in the format of the database of their origin and as such, direct comparisons may be difficult. Another problem is that the terms used in tagging and searching can sometimes be ambiguous and may cause confusion among the results. In addition, the semantic web approach is still considered an emerging technology and is not in wide-scale use at this time.\n\nOne of the current applications of ontology-based search in the biomedical sciences is GoPubMed, which searches the PubMed database of scientific literature. Another use of ontologies is within databases such as SwissProt, Ensembl and TrEMBL, which use this technology to search through the stores of human proteome-related data for tags related to the search term.\n\nSome of the research in this field has focused on creating new and specific ontologies. Other researchers have worked on verifying the results of existing ontologies. In a specific example, the goal of Verschelde, et al. was the integration of several different ontology libraries into a larger one that contained more definitions of different subspecialties (medical, molecular biological, etc.) and was able to distinguish between ambiguous tags; the result was a data-warehouse like effect, with easy access to multiple databases through the use of ontologies. In a separate project, Bertens, et al. constructed a lattice work of three ontologies (for anatomy and development of model organisms) on a novel framework ontology of generic organs. For example, results from a search of ‘heart’ in this ontology would return the heart plans for each of the vertebrate species whose ontologies were included. The stated goal of the project is to facilitate comparative and evolutionary studies.\n\nIn the data warehousing strategy, the data from different sources are extracted and integrated in a single database. For example, various 'omics' datasets may be integrated to provide biological insights into biological systems. Examples include data from genomics, transcriptomics, proteomics, interactomics, metabolomics. Ideally, changes in these sources are regularly synchronized to the integrated database. The data is presented to the users in a common format. Many programs aimed to aid in the creation of such warehouses are designed to be extremely versatile to allow for them to be implemented in diverse research projects. One advantage of this approach is that data is available for analysis at a single site, using a uniform schema. Some disadvantages are that the datasets are often huge and difficult to keep up to date. Another problem with this method is that it is costly to compile such a warehouse.\n\nStandardized formats for different types of data (ex: protein data) are now emerging due to the influence of groups like the Proteomics Standards Initiative (PSI). Some data warehousing projects even require the submission of data in one of these new formats.\n\nData mining uses statistical methods to search for patterns in existing data. This method generally returns many patterns, of which some are spurious and some are significant, but all of the patterns the program finds must be evaluated individually. Currently, some research is focused on incorporating existing data mining techniques with novel pattern analysis methods that reduce the need to spend time going over each pattern found by the initial program, but instead, return a few results with a high likelihood of relevance. One drawback of this approach is that it does not integrate multiple databases, which means that comparisons across databases are not possible. The major advantage to this approach is that it allows for the generation of new hypotheses to test.\n\n\n"}
{"id": "21980341", "url": "https://en.wikipedia.org/wiki?curid=21980341", "title": "International Space Olympics", "text": "International Space Olympics\n\nThe International Space Olympics (ISO) is an annual two-week competition for teenagers aged from 14 to 18, held in Korolyov, Russia. The competition includes examinations in Mathematics, Physics, Computer Science, and English Literature, in addition to presentation of a space related research project.\n\nOn days when participants are not competing, they are given tours of some of Russia's top space facilities and areas of cultural significance, and even have a chance to videochat with astronauts and cosmonauts on the ISS. Participants come from a wide range of countries, with each country represented as a team. In previous years, teams have attended from Germany, Greece, Israel, United Kingdom, the United States, Kazakhstan, Australia, Spain and Russia. Overall, in 2012 there were 130 participating students, in 2013 and 2014 - over 200. Over the years, the International Space Olympics has been attended by over 1,850 Russian students, as well as more than 800 students from other countries. \n\nThe Olympics are held on the initiative of S. P. Korolyov Energia Space and Rocket Corporation and has an international status. The Opening Ceremony is usually started by the heads of Korolyov City, the Moscow region, municipal heads of education, pilots, cosmonauts and representatives from RSC Energia and MCC (mission control center in Korolyov).\n\nSometimes, in the media, the Space Olympics is referred to as an \"international space olympics camp\" for students or \"an intellectual marathon\". When in Russia, the competitors usually stay at a hostel, and at the end of the week they are taken to the country to sum up the whole event, to hold the award ceremony, to celebrate, and to have a \"good-bye\" party.\n\nIn 2014 China and Venezuela voiced their desire to participate in Space Olympic Games for the first time. Moreover, it is this year that freshmen and sophomores were considered for participation.\n\nIn the 2012 competition, Alex Gagliano won 1st place in Astrophysics presentations and Austin Chung won 1st place in Space History and Policy.\n\nIn the 2013 competition, Josh Ting won 1st place in Astrophysics and Hema Narlapati won 1st place in Space Policy.\n\n"}
{"id": "23704110", "url": "https://en.wikipedia.org/wiki?curid=23704110", "title": "Jovan Hadži", "text": "Jovan Hadži\n\nJovan Hadži (22 November 1884 – 11 December 1972) was a Slovenian zoologist of Serbian origin.\n\nHadži was born in a Serbian family in Temišvar (today Timişoara, Romania) in what was then Austria-Hungary. He began his career in Zagreb. In 1920, he moved to Ljubljana where he became the head of zoological institute at the then established University of Ljubljana. Between 1951 and 1972, Hadži was the head of the Biological institute at Slovenian Academy of Sciences and Arts (SASA). In 1938, he became a full member of SASA.\n\nHadži was perhaps most widely known for his unique theories of animal evolution. He devised a system of classification in which he divided the animal kingdom into six phyla: Protozoa, Parazoa, \"Ameria\" (animals with no segments), \"Oligomeria\" (animals with few segments), \"Polymeria\" (animals with many segments) and Chordata. His choice of characters important for classification was generally discredited by his contemporaries, and the system was never accepted by zoologists. However, due to its simplicity, the system was widely used in science education in former Yugoslavia. His other major theory was that of the origin of metazoa - he developed an existing hypothesis stating that the first multicellular animals resembling today's flatworms evolved from multinucleate ciliates in which cell nuclei became separated by cellular membranes. Again, the theory emphasized similarities of structure while disregarding other important characters, so it was never generally accepted.\n\nHadži's faunistical work focused on the invertebrate fauna of caves and mountains where he described more than a hundred new species and genera. He was also an active cave explorer and acted as a president of the Slovenian society for cave exploration (Društvo za raziskovanje jam Slovenije) between 1927 and 1945.\n\nFor his contributions to zoology, Hadži received Prešeren Award in 1956. In 1969, he received the honorary doctorate by the University of Ljubljana. Several invertebrate species were named after him by other zoologists, such as \"Astagobius hadzii\", \"Cyclopina hadzii\", \"Isohypsibius hadzii\", and \"Niphargus hadzii\".\n"}
{"id": "20886466", "url": "https://en.wikipedia.org/wiki?curid=20886466", "title": "Levich equation", "text": "Levich equation\n\nThe Levich equation models the diffusion and solution flow conditions around a rotating disk electrode (RDE). It is named after Veniamin Grigorievich Levich who first developed an RDE as a tool for electrochemical research. It can be used to predict the current observed at an RDE, in particular, the Levich equation gives the height of the sigmoidal wave observed in rotating disk voltammetry. The sigmoidal wave height is often called the Levich current.\n\nThe Levich equation is written as:\n\nwhere \"I\" is the Levich current (A), \"n\" is the number of moles of electrons transferred in the half reaction (number), \"F\" is the Faraday constant (C/mol), \"A\" is the electrode area (cm), \"D\" is the diffusion coefficient (see Fick's law of diffusion) (cm/s), \"ω\" is the angular rotation rate of the electrode (rad/s), \"v\" is the kinematic viscosity (cm/s), \"C\" is the analyte concentration (mol/cm)\n\nTo use the equation as written above (with the leading 0.620), radians per second for angular rotation units must be used. If revolution (rotations) per minute (rpm) are used, a value of 0.201 should be used in place of 0.620.\n\nWhereas the Levich equation suffices for many purposes, improved forms based on derivations utilising more terms in the velocity expression are available.\n\nThe Levich equation is often simplified by defining a Levich constant \"B\" such that:\n\n"}
{"id": "51243310", "url": "https://en.wikipedia.org/wiki?curid=51243310", "title": "List of country subdivisions by Punjabi speakers", "text": "List of country subdivisions by Punjabi speakers\n\nThis is a list of first-level administrative country subdivisions by Punjabi speakers in the world.\n\n\n"}
{"id": "8741743", "url": "https://en.wikipedia.org/wiki?curid=8741743", "title": "List of freshwater aquarium amphibian species", "text": "List of freshwater aquarium amphibian species\n\nThere are a wide range of frogs, salamanders and newts that can be kept in an aquarium.\n\n\n"}
{"id": "10761636", "url": "https://en.wikipedia.org/wiki?curid=10761636", "title": "List of lakes of Nebraska", "text": "List of lakes of Nebraska\n\nThis is a list of lakes in Nebraska. \n\n\n"}
{"id": "1741516", "url": "https://en.wikipedia.org/wiki?curid=1741516", "title": "List of natural gas fields", "text": "List of natural gas fields\n\nThis list of natural gas fields includes major fields of the past and present.\n\nN.B. Some of the items listed are basins or projects that comprise many fields (e.g. Sakhalin has three fields: Chayvo, Odoptu, and Arkutun-Dagi).\n\n\"Table sources:\n\nGlobal Natural Gas Reserves – A Heuristic Viewpoint\n\nNotes: \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew finding in the Nile Delta Basin of )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSorted by size (*10 m³):\n\n"}
{"id": "53155398", "url": "https://en.wikipedia.org/wiki?curid=53155398", "title": "List of scattering experiments", "text": "List of scattering experiments\n\nThis is a list of scattering experiments.\n\n\n\n\n"}
{"id": "3494065", "url": "https://en.wikipedia.org/wiki?curid=3494065", "title": "Mario Benazzi", "text": "Mario Benazzi\n\nMario Benazzi (Cento, August 29, 1902 – Pisa, December 6, 1997) was an Italian zoologist, professor at the Istituto di Zoologia e Anatomia Comparata of the University of Pisa. He published work on platyhelminths and evolutionary cytogenetics.\n\nBenazzi is honoured in the polychaete name \"Diurodrilus benazzii\" Gerlach, 1952 and in the copepod name \"Colobomatus benazzii\" Delamare Deboutteville & Nunes Ruivo, 1958. In 1971, he was elected a national member of the Accademia dei Lincei.\n"}
{"id": "50955026", "url": "https://en.wikipedia.org/wiki?curid=50955026", "title": "NGC 142", "text": "NGC 142\n\nNGC 142 is a spiral galaxy in the constellation of Cetus. It was discovered by Frank Muller in 1886.\n"}
{"id": "1974943", "url": "https://en.wikipedia.org/wiki?curid=1974943", "title": "Nordic Cross flag", "text": "Nordic Cross flag\n\nThe Nordic Cross flag is any of certain flags bearing the design of the Nordic or Scandinavian cross, a cross symbol in a rectangular field, with the center of the cross shifted towards the hoist.\n\nAll of the Nordic countries except Greenland have adopted such flags in the modern period, and while the Scandinavian cross is named for its use in the national flags of the Scandinavian nations, the term is used universally by vexillologists, in reference not only to the flags of the Nordic countries.\n\nThe cross design represents Christianity, and the characteristic shift of the center to the hoist side is early modern, first described the Danish civil ensign (\"Koffardiflaget\") for merchant ships in a regulation of 11 June 1748, which specified the shift of the cross center towards the hoist as \"the two first fields must be square in form and the two outer fields must be lengths of those\". The Danish design was adopted for the flags of Norway (civil ensign 1821) and Sweden (1906), both derived from a common ensign used during the Union between Sweden and Norway 1818–1844, as well as Iceland (1915) and Finland (1917); some of the subdivisions of these countries used this as inspiration for their own flags. The Norwegian flag was the first Nordic cross flag with three colours. \nAll Nordic flags may be flown as gonfalons as well.\n\nNote that some of these flags are historical. Also, note that flag proportions may vary between the different flags and sometimes even between different versions of the same flag.\n\nThe Flag of Greenland is the only national flag of a Nordic country or territory without a Nordic Cross. When Greenland was granted home rule, the present flag - with a graphic design unique to Greenland - was adopted on June 1985, supported by fourteen votes against eleven who supported a proposed green-and-white Nordic cross.\nHistorical Flag of the Kalmar Union, which united Denmark, Sweden and Norway 1397 to 1523. No pictorial evidence survives of the Kalmar Union's Flag. The flag appearing here is a reconstruction based on references in 1430 letters by King Eric of Pomerania.\nThese flags either do not have official status or represent various private entities. They have not been officially adopted and their use remains limited.\n\nNordic flag designs very similar to Denmark's, Sweden's, and Norway's national flags were proposed as Germany's national flags in both 1919 and 1948, after World War I and World War II, respectively. Today, the Nordic cross is a feature in some city and district flags or coats of arms.\n\nA number of flags for localities in the United Kingdom (primarily Scotland) are based on Nordic cross designs, intended to reflect the Scandinavian heritage introduced to the British Isles during the Viking Age and through the High Middle Ages.\n\n\n"}
{"id": "1618377", "url": "https://en.wikipedia.org/wiki?curid=1618377", "title": "Oceanic basin", "text": "Oceanic basin\n\nIn hydrology, an oceanic basin may be anywhere on Earth that is covered by seawater but geologically ocean basins are large geologic basins that are below sea level. Geologically, there are other undersea geomorphological features such as the continental shelves, the deep ocean trenches, and the undersea mountain ranges (for example, the mid-Atlantic ridge and the Emperor Seamounts) which are not considered to be part of the ocean basins; while hydrologically, oceanic basins include the flanking continental shelves and shallow, epeiric seas.\n\nOlder references (e.g., Littlehales 1930) consider the oceanic basins to be the complement to the continents, with erosion dominating the latter, and the sediments so derived ending up in the ocean basins. More modern sources (e.g., Floyd 1991) regard the ocean basins more as basaltic plains, than as sedimentary depositories, since most sedimentation occurs on the continental shelves and not in the geologically-defined ocean basins.\n\nHydrologically some geologic basins are both above and below sea level, such as the Maracaibo Basin in Venezuela, although geologically it is not considered an oceanic basin because it is on the continental shelf and underlain by continental crust.\n\nEarth is the only known planet in the solar system where hypsography is characterized by different kinds of crust, oceanic crust and continental crust. Oceans cover 70% of the Earth's surface. Because oceans lie lower than continents, the former serve as sedimentary basins that collect sediment eroded from the continents, known as clastic sediments, as well as precipitation sediments. Ocean basins also serve as repositories for the skeletons of carbonate- and silica-secreting organisms such as coral reefs, diatoms, radiolarians, and foraminifera.\n\nGeologically, an oceanic basin may be actively changing size or may be relatively, tectonically inactive, depending on whether there is a moving plate tectonic boundary associated with it. The elements of an active - and growing - oceanic basin include an elevated mid-ocean ridge, flanking abyssal hills leading down to abyssal plains. The elements of an active oceanic basin often include the oceanic trench associated with a subduction zone.\n\nThe Atlantic ocean and the Arctic ocean are good examples of active, growing oceanic basins, whereas the Mediterranean Sea is shrinking. The Pacific Ocean is also an active, shrinking oceanic basin, even though it has both spreading ridge and oceanic trenches. Perhaps the best example of an inactive oceanic basin is the Gulf of Mexico, which formed in Jurassic times and has been doing nothing but collecting sediments since then. The Aleutian Basin is another example of a relatively inactive oceanic basin. The Japan Basin in the Sea of Japan which formed in the Miocene, is still tectonically active although recent changes have been relatively mild.\n\n\n"}
{"id": "501746", "url": "https://en.wikipedia.org/wiki?curid=501746", "title": "Of Moths and Men", "text": "Of Moths and Men\n\nOf Moths and Men is a book by journalist Judith Hooper about the Oxford University ecological genetics school led by E.B. Ford. The book specifically concerns Bernard Kettlewell's experiments on the peppered moth which were intended as experimental validation of evolution. She highlights concerns about the methodology of Kettlewell's experiments and suggests that these issues could invalidate the results obtained, ignoring or disparaging evidence supporting natural selection while repeatedly implying that Kettlewell and his colleagues committed fraud or made careless errors. Subject matter experts have described the book as presenting a \"conspiracy theory\" with \"errors, misrepresentations, misinterpretations and falsehoods\".\n\nHooper alleges several flaws in experimental methodology, including gluing the moths in place on parts of trees where they would not naturally settle, feeding birds heavily enough to condition them to expect feeding at that point, artificially boosting recapture rates, altering experiments (unconsciously) to favour the expected outcome, and errors in statistical analysis.\n\nHistorian of biology David Rudge has also carefully reexamined the records upon which Hooper's argument is based. His conclusions were that her historical research was poor and she had shown fundamental misunderstandings about the nature of science.\n\nThe book was described as well-written in reviews in the mainstream press, but it has been criticised in scientific publications. Writing in \"Nature\", Coyne (2002) attacked Hooper's \"flimsy conspiracy theory [of] ambitious scientists who will ignore the truth for the sake of fame and recognition [by which] she unfairly smears a brilliant naturalist\". In \"Science\", Grant (2002) critically summarised the book's content, saying \"What it delivers is a quasi-scientific assessment of the evidence for natural selection in the peppered moth (\"Biston betularia\"), much of which is cast in doubt by the author’s relentless suspicion of fraud\". Bryan Clarke, who worked alongside Kettlewell at Oxford, described Hooper's book as \"a treasury of insinuations worthy of an unscrupulous newspaper\".\n\nThe entomologist and expert on peppered moth evolution Michael Majerus described the book as \"littered with errors, misrepresentations, misinterpretations and falsehoods\" and published research specifically refuting some of Hooper's claims.\n\n"}
{"id": "4190476", "url": "https://en.wikipedia.org/wiki?curid=4190476", "title": "Phytogeography", "text": "Phytogeography\n\nPhytogeography (from Greek φυτό, \"phyto\" = \"plant\" and γεωγραφία, \"geografía\" = \"geography\" meaning also distribution) or botanical geography is the branch of biogeography that is concerned with the geographic distribution of plant species and their influence on the earth's surface. Phytogeography is concerned with all aspects of plant distribution, from the controls on the distribution of individual species ranges (at both large and small scales, see species distribution) to the factors that govern the composition of entire communities and floras. Geobotany, by contrast, focuses on the geographic space's influence on plants.\n\nPhytogeography is part of a more general science known as biogeography. Phytogeographers are concerned with patterns and process in plant distribution. Most of the major questions and kinds of approaches taken to answer such questions are held in common between phyto- and zoogeographers.\n\nPhytogeography in wider sense (or geobotany, in German literature) encompasses four fields, according with the focused aspect, environment, flora (taxa), vegetation (plant community) and origin, respectively:\n\nPhytogeography is often divided into two main branches: ecological phytogeography and historical phytogeography. The former investigates the role of current day biotic and abiotic interactions in influencing plant distributions; the latter are concerned with historical reconstruction of the origin, dispersal, and extinction of taxa.\n\nThe basic data elements of phytogeography are occurrence records (presence or absence of a species) with operational geographic units such as political units or geographical coordinates. These data are often used to construct phytogeographic provinces (floristic provinces) and elements.\n\nThe questions and approaches in phytogeography are largely shared with zoogeography, except zoogeography is concerned with animal distribution rather than plant distribution. The term phytogeography itself suggests a broad meaning. How the term is actually applied by practicing scientists is apparent in the way periodicals use the term. The \"American Journal of Botany\", a monthly primary research journal, frequently publishes a section titled \"Systematics, Phytogeography, and Evolution.\" Topics covered in the \"American Journal of Botany\"'s \"Systematics and Phytogeography\" section include phylogeography, distribution of genetic variation and, historical biogeography, and general plant species distribution patterns. Biodiversity patterns are not heavily covered.\n\nPhytogeography has a long history. One of the subjects earliest proponents was Prussian naturalist Alexander von Humboldt, who is often referred to as the \"father of phytogeography\". Von Humboldt advocated a quantitative approach to phytogeography that has characterized modern plant geography.\n\nGross patterns of the distribution of plants became apparent early on in the study of plant geography. For example, Alfred Russel Wallace, co-discoverer of the principle of natural selection, discussed the Latitudinal gradients in species diversity, a pattern observed in other organisms as well. Much research effort in plant geography has since then been devoted to understanding this pattern and describing it in more detail.\n\nIn 1890, the United States Congress passed an act that appropriated funds to send expeditions to discover the geographic distributions of plants (and animals) in the United States. The first of these was The Death Valley Expedition, including Frederick Vernon Coville, Frederick Funston, Clinton Hart Merriam, and others.\nResearch in plant geography has also been directed to understanding the patterns of adaptation of species to the environment. This is done chiefly by describing geographical patterns of trait/environment relationships. These patterns termed ecogeographical rules when applied to plants represent another area of phytogeography. Recently, a new field termed macroecology has developed, which focuses on broad-scale (in both time and space) patterns and phenomena in ecology. Macroecology focuses as much on other organisms as plants.\n\nFloristics is a study of the flora of some territory or area. Traditional phytogeography concerns itself largely with floristics and floristic classification, see floristic province.\n\n\n\n"}
{"id": "40465955", "url": "https://en.wikipedia.org/wiki?curid=40465955", "title": "Planetarium of Medellín", "text": "Planetarium of Medellín\n\nThe Jesús Emilio Ramírez González Planetarium of Medellín () is a planetarium located in Medellín, Colombia and established on October 10, 1984. It was originally conceived by the Astronomical Society of the College of San José, led at that time by Brother Daniel (Julián González Patiño), a renowned scientist, astronomer, and botanist. The purpose is that the planetarium is a space for the promotion of scientific and technological culture of citizens and mentality that encourages scientific and technological creativity. The museum is surrounded by a public space known as the Park of Wishes, which was designed by Felipe Uribe de Bedout to coordinate with the planetarium.\n\nMedellín was the first South American city to have a computer-controlled planetarium.\n\nThe planetarium is named after Jess Emilio Ramírez González, a famous Colombian geophysicist. The planetarium is located in an area called the \"North Zone\" (Zona Norte), which is an urban sprawl area north of the city's center. The great socio-political instability of the 1980s and 1990s had an enormous effect on area like these where people from different strata lived. Insecurity and vandalism drove the observatory, the Botanical Garden of Medellín, and even the University of Antioquia to consider relocation for a time. However, a series of urban interventions at the end of the 1990s—including a University stop on the Medellín Metro—prevented the closure of the three areas and helped to reconstitute the area.\n\nIt was renovated in 2006 and comprises the largest recreational area, science and technology city, in the North Zone. In its vicinity are located North Park, Parque Explora, the Park of Wishes and several major units of the University of Antioquia.\n\nSince 2011, with support from the Mayor of Medellin, Bancolombia, and various astronomical communities of the city, Parque Explora developed a project to renovate the planetarium. The core of this transformation was a new scientific visualization center.\n\nThe first floor, open access for visitors, consists of an auditorium with capacity for 200 people, a library specializing in science literacy, and a store with items and souvenirs on astronomy and a café.\n"}
{"id": "1840179", "url": "https://en.wikipedia.org/wiki?curid=1840179", "title": "Positive political theory", "text": "Positive political theory\n\nPositive political theory or explanatory political theory is the study of politics using formal methods such as social choice theory, game theory, and statistical analysis. In particular, social choice theoretic methods are often used to describe and (axiomatically) analyze the performance of rules or institutions. The outcomes of the rules or institutions described are then analyzed by game theory, where the individuals/parties/nations involved in a given interaction are modeled as \"rational\" agents playing a game, guided by self-interest. \nBased on this assumption, the outcome of the interactions can be predicted as an equilibrium of the game.\n\nThe founder of the field was William H. Riker. In his book \"The Theory of Political Coalitions\" (1962), he applied the principles of game theory to the study of politics.\n\n\n\n"}
{"id": "12891586", "url": "https://en.wikipedia.org/wiki?curid=12891586", "title": "Pseudoplankton", "text": "Pseudoplankton\n\nPseudoplanktonic organisms are those that attach themselves to planktonic organisms or other floating objects, such as drifting wood, buoyant shells of organisms such as \"Spirula\", or man-made flotsam. Examples include goose barnacles and the bryozoan \"Jellyella\". By themselves these animals cannot float, which contrasts them with true planktonic organisms, such as \"Velella\" and the Portuguese Man o' War, which are buoyant. Pseudoplankton are often found in the guts of filtering zooplankters.\n"}
{"id": "22751338", "url": "https://en.wikipedia.org/wiki?curid=22751338", "title": "Semantic feature-comparison model", "text": "Semantic feature-comparison model\n\nThe semantic feature comparison model is used \"to derive predictions about categorization times in a situation where a subject must rapidly decide whether a test item is a member of a particular target category\". In this semantic model, there is an assumption that certain occurrences are categorized using its features or attributes of the two subjects that represent the part and the group. A statement often used to explain this model is \"a robin is a bird\". The meaning of the words \"robin\" and \"bird\" are stored in the memory by virtue of a list of features which can be used to ultimately define their categories, although the extent of their association with a particular category varies.\n\nThis model was conceptualized by Edward Smith, Edward Shoben and Lance Rips in 1974 after they derived various observations from semantic verification experiments conducted at the time. Respondents merely have to answer \"true\" or \"false\" to given sentences. Out of these experiments, they observed that people respond faster when (1) statements are true, (2) nouns are members of smaller categories, (3) items are \"typical\" or commonly associated with the category (also called prototypes), and (4) items are primed by a similar item previously given (University of Alaska Anchorage, n.d.). In the latter item, respondents will respond faster to the latter statement since the category bird has been primed. Based on the previous observations, the proponents were able to come up with the semantic feature comparison model.\n\nThe cognitive approach consists of two concepts: information processing depends on internal representations, and that mental representations undergo transformations. For the first concept, we could describe an object in a number of ways, with drawings, equations, or verbal descriptions, but it is up to the recipient to have a background understanding of the context to which the object is being described in order to fully comprehend the deliverable. The second concept explains how memory can alter the way we perceive representations of something, by determining the sequence in which the information is processed based on previous experiences.\n\nThe main features of the model, as discussed by Smith et al. (1974), are the defining features and the characteristic features. Defining features refer to the characteristics that are essential elements of the category, the non-negotiable, so to speak. For example, the 'bird' category includes such defining features as 'they have wings', 'feathers', 'they lay eggs', etc. Characteristic features refer to the elements usually found or inherent to category members but are not found in all, or non-essentials. For example, birds 'fly', – that is characteristic because while most birds fly, there are some who cannot.\n\nThe model has two stages for decision making. First, all features of the two concepts (bird and robin, in our example) are compared to find out how alike they are. If the decision is that they are very similar or very dissimilar, then a true or false decision can be made. Second, if the characteristics/features are in-between then the focus shifts to the defining features in order to decide if the example possesses enough features of the category, thus, categorization depends on similarity and not on the size of the category.\n\n"}
{"id": "43526790", "url": "https://en.wikipedia.org/wiki?curid=43526790", "title": "Soft modes", "text": "Soft modes\n\nIn theoretical condensed matter physics, soft modes emerge as a consequence of the Goldstone theorem. Soft modes are excitations above the ground state whose energy vanishes in the limit of long wavelengths.\n"}
{"id": "8464879", "url": "https://en.wikipedia.org/wiki?curid=8464879", "title": "The Big Bang (TV series)", "text": "The Big Bang (TV series)\n\nThe Big Bang was a CITV science show broadcast from 15 April 1996 – 8 September 2004 and produced by Yorkshire Television. It is notable for being one of CITV's longest-running science programmes. The aim of the programme was to make science fun and interesting for children.\n\nStarting in 1996, \"The Big Bang\" was originally presented by Gareth Jones and Kate Bellingham. Kate Bellingham left after the second series to care for her new child. Gareth Jones's long-time friend and now partner Violet Berlin was her replacement for the third series in 1998. The show essentially stayed the same before a new logo and set design was introduced for the fifth and sixth series in 2000 and 2001. Both Gareth and Violet left the show after the sixth series as they were, as said by ITV at the time, \"concentrating on running their own company [WhizzBang] and looking after baby Indigo\". However, Gareth Jones later revealed on his website that they left because \"when we were asked to do the 2002 series it looked like many of the people who usually make the programme with us wouldn't be doing it any more. There were lots of changes planned for the Big Bang which Violet & I didn't agree with, so we said thank you for the last 6 series and wished them all the luck for the future.\" \n\nFor the seventh series the show had another rebrand, notably a new theme tune, the first since the beginning of the show, however the old theme tune was brought back for the final two series. There was also a new logo, and new presenters Kate McIntyre and Michael Underwood were introduced. The set stayed remained mainly unchanged, however the neon \"The Big Bang\" sign in the kitchen was not replaced with the new logo until series 8. Michael and Kate presented the show together for two series, until Michael left to present \"Ministry of Mayhem\", and for the ninth and last series he was replaced with Sam Pinkham.\n\nIn 2003, a shorter 5-minute version of the show, \"The Little Bang\", was introduced at weekends. It took one section from a normal Big Bang episode and displayed what was needed for the make at the beginning of the show.\n\nAfter the show ended, it was repeated daily on Discovery Kids UK, who started showing it with the first series and all the way up until the sixth before the channel closed in 2007. Afterwards, series 7, 8 and 9 were regularly repeated on the CITV channel, normally at weekends until the end of 2012.\n\nIn Vietnam from 2003, all series of the show were originally aired on HTV7, later on HTV1 (public) and HTV4 (education & science).\n"}
{"id": "32873991", "url": "https://en.wikipedia.org/wiki?curid=32873991", "title": "The Chilling Stars", "text": "The Chilling Stars\n\nThe Chilling Stars is a non-fiction book about the possible causes and effects of global climate change by Henrik Svensmark and Nigel Calder. The paperback version was published by Totem Books on March 19, 2003. An updated version titled The Chilling Stars: A New Theory of Climate Change was published in 2007. Svensmark is otherwise known as a Danish physicist and professor while Calder has worked as a science journalist.\n\nThe authors argue that cloud cover changes caused by variations in cosmic rays are a major contributor to global temperature increases, and they state that human influences have been exaggerated.\n\nThe authors describe a cross-disciplinary theory that takes in elements of cosmology, particle physics, paleo-climatology, and meteorology. They label their concept 'cosmoclimatology', and they attempt to look back through prior climate trends such as the Medieval Warm Period and the Little Ice Age. They detail what they view as a close correlation between the rate of cosmic rays reaching the earth, which vary based on electromagnetic fluctuation on the sun's surface, and earth's temperature.\n\nThey write how the solar magnetic field grew over twice as strong as before over the 20th century, and they peg this as a primary driver of the approximately 0.6 °C warming over that time. Specifically, they state that fewer cosmic rays cause fewer clouds to form and thus the climate becomes hotter, given that the individual water droplets that make up clouds collect when cosmic particles turn water into ions.\n\nThe online magazine londonbookreview.com remarked, \"For those who believe that the argument about the causes of climate change have been settled may find this a difficult book to read. But those who retain an open mind may find this an interesting read, even if it is only to confirm that the science is far from being settled.\"\n\nScientists have generally not found the published work of Svensmark et al. persuasive. For example Lockwood et al. find that \"The cloud-cosmic ray suggestion increasingly fails to match observations\". A joint Spanish/Japanese collaboration of solar ray/astrophysics experts found that the change in global cloud cover is closely correlated with El Niño–Southern Oscillation and uncorrelated with solar rays. \n\n\n"}
{"id": "6876234", "url": "https://en.wikipedia.org/wiki?curid=6876234", "title": "The Mysteryes of Nature and Art", "text": "The Mysteryes of Nature and Art\n\nThe Mysteries of Nature and Art is a book by John Bate written in 1634. The book acts as a practical guide for amateur scientific experiments, and is divided into four sections: Water Workes, Fyer Workes, Drawing, Colouring, Painting and Engraving, and Divers Experiments. It inspired Isaac Newton during his younger years, in particular the section on fire Drakes, kites with firecrackers tied to their tails. It contains one of the earliest depictions of fireworks and their preparation to be detailed in the English language, in a similar manner to the preceding \"De la pirotechnia\".\n\n"}
{"id": "5795234", "url": "https://en.wikipedia.org/wiki?curid=5795234", "title": "The Old Malthouse School", "text": "The Old Malthouse School\n\nThe Old Malthouse School (The OMH) was a preparatory school in the village of Langton Matravers near Swanage in the Isle of Purbeck, Dorset, United Kingdom.\n\nThe school was founded in 1906 by Rex Corbett, an ex-England football player, and started with ten pupils in a building that was formerly a brewery. Tom Pellatt, his brother-in-law who ran Durnford School at Durnford Court in the same village had blasted out a swimming bath in the rocks at Dancing Ledge, a mile and a half away on the coast and the pupils of both schools used this daily in the summer term. Durnford's most famous former pupil is Ian Fleming, author of the James Bond novels and \"Chitty Chitty Bang Bang\".\n\nIn 1939, the school was sold by Corbett to Victor Haggard (H) and Evan Hope-Gill (Hopper) who inherited 37 boys. Durnford was requisitioned by the army later that year and the Durnford boys transferred to the Old Malthouse. Durnford was acquired by the owners of the Old Malthouse when the army gave it up in 1948. The main buildings were variously pulled down or sold, leaving the OMH with the grounds, which were levelled for playing fields. A third joint headmaster Peter Mattinson (Mr Matt) joined after World War II and the triumvirate ruled until 1974 when the school, then with about 80 boys, was sold to a Trust under the headmastership of Quintin Ambler. Ill-health led to Mr Ambler's early departure to be replaced as headmaster by Patrick Jordan in 1975 who expanded the school by adding a pre-prep department in the early 1980s. In 1988, Jon Phillips took over as Headmaster, remaining for 15 years. During this time the school became fully co-educational. Richard Keeble became Headmaster in January 2004 and left the school in July 2006 handing over the reins to longtime deputy Dr Moira Laffey. Through the 1970s and early 80s the school expanded to about 100 pupils but declining enrollment and increasing losses led to the decision to close the school in 2007. \n\nIn April 2007, the local press reported that the school would close at the end of the 2007 summer term\n\nIn 2008, the property was acquired and reopened by the Cothill Educational Trust as a science-based centre, running week-long practical science courses for schoolchildren aged 10–13.\n\n"}
{"id": "47204911", "url": "https://en.wikipedia.org/wiki?curid=47204911", "title": "Tide dial", "text": "Tide dial\n\nA tide dial, also known as a mass or scratch dial, is a sundial marked with the canonical hours rather than or in addition to the standard hours of daylight. Such sundials were particularly common between the 7th and 14th centuries in Europe, at which point they began to be replaced by mechanical clocks. There are more than 3,000 surviving tide dials in England and at least 1,500 in France.\n\nThe name \"tide dial\" preserves the Old English term ', used for hours and canonical hours prior to the Norman Conquest of England, after which the Norman French \"hour\" gradually replaced it. The actual Old English name for sundials was ' or \"day-marker\".\n\nJews long recited prayers at fixed times of day. Psalm 119 in particular mentions praising God seven times a day, and the apostles Peter and John are mentioned attending afternoon prayers. Christian communities initially followed numerous local traditions with regard to prayer, but Charlemagne compelled his subjects to follow the Roman liturgy and his son Louis the Pious imposed the Rule of StBenedict upon their religious communities. \n\nThe canonical hours adopted by Benedict and imposed by the Frankish kings were the Night Office (Nocturns) said in the wee hours, the Morning Office (Matins or Lauds) in the dawn just prior to sunrise, Prime after the 1st hour of sunlight, Terce at the 3rd, Sext at the 6th, Nones at the 9th, Vespers at sunset, and Compline after a period of rest and reading. Monks were called to these hours by their abbot or by the ringing of the church bell, with the time between services organized in reading the Bible or other religious texts, in manual labor, or in sleep.\n\nThe need for these monastic communities and others to organize their times of prayer prompted the establishment of tide dials built into the walls of churches. They began to be used in England in the late 7th century and spread from there across continental Europe through copies of Bede's works and by the Saxon and Hiberno-Scottish missions. Within England, tide dials fell out of favor after the Norman Conquest. By the 13th century, some tide dials—like that at Strasbourg Cathedral—were constructed as independent statues rather than built into the walls of the churches. From the 14th century onwards, the cathedrals and other large churches began to use mechanical clocks and the canonical sundials lost their utility, except in small rural churches, where they remained in use until the 16th century.\nThere are more than 3,000 surviving tide dials in England and at least 1,500 in France, mainly in Normandy, Touraine, Charente, and at monasteries along the pilgrimage routes to Santiago de Compostella in northwestern Spain.\n\nWith Christendom confined to the Northern Hemisphere, the tide dials were often carved vertically onto the south side of the church chancel at eye level near the priest's door. In an abbey or large monastery, dials were carefully carved into the stone walls, while in rural churches they were very often just scratched onto the wall.\n\nSome tide dials have a stone gnomon, but many have a circular hole which is used to hold a more easily replaced or adjusted wooden gnomon. These gnomons were perpendicular to the wall and cast a shadow upon the dial, a semicircle divided into a number of equal sectors. Most dials have supplementary lines marking the other 8 daytime hours, but are characterized by their noting the canonical hours particularly. The lines for the canonical hours may be longer or marked with a dot or cross. The divisions are seldom numbered.\n\nDials often have holes along the circumference of their semicircle. As additional gnomons were needless and these holes are often quite shallow, Cole suggests they were used to quickly and easily reconstruct the tide dials following a fresh whitewash of the church walls with chalk or lime.\n\nThe oldest surviving English tide dial is on the 7th- or 8th-century Bewcastle Cross in the church graveyard of St Cuthbert's in Bewcastle, Cumbria. It is carved on the south face of a Celtic cross at some height from the ground and is divided by five principal lines into four tides. Two of these lines, those for 9am and noon, are crossed at the point. The four spaces are further subdivided so as to give the twelve daylight hours of the Romans. On one side of the dial, there is a vertical line which touches the semicircular border at the second afternoon hour. This may be an accident, but the same kind of line is found on the dial in the crypt of Bamburgh Church, where it marks a later hour of the day. The sundial may have been used for calculating the date of the spring equinox and hence Easter.\n\nNendrum Monastery in Northern Ireland, supposedly founded in the 5th century by St Machaoi, now has a reconstructed tide dial. The 9th-century tide dial gives the name of its sculptor and a priest.\n\nThe 1056 1065 tide dial at St Gregory's Minster in Kirkdale, North Yorkshire, has four principal divisions marked by five crossed lines, subdivided by single lines. One marking ¼ of the way between sunrise and noon is an incised cross that would indicate about 9 am at midwinter and 6 am at midsummer. It was dedicated to a \"Hawarth\". \n\nProper tide dials prominently displaying the canonical hours:\n\nOther ecclesiastical sundials (\"mass dials\") used to determine times for prayer and mass during the same period:\n\n\n\n"}
{"id": "30612842", "url": "https://en.wikipedia.org/wiki?curid=30612842", "title": "Wally Funk", "text": "Wally Funk\n\nMary Wallace \"Wally\" Funk (born February 1, 1939) is an American aviator and Goodwill Ambassador. She was one of the Mercury 13, and was the first female air safety investigator for the National Transportation Safety Board, the first civilian flight instructor at Fort Sill, Oklahoma, and the first Federal Aviation Agency inspector.\n\nFunk was born in Las Vegas, New Mexico in 1939 and grew up in Taos, New Mexico. Her parents owned a variety store. The family had a collection of artwork from artists at the Taos art colony. The artists would trade artwork to pay off their debt at the store. When Funk was four years old, she jumped off the roof of a barn in a Superman costume. She thought the costume would make her fly instead of falling into a haystack. She did her first solo flight at age 16 and enrolled in the aviation program at Stephens College, which she completed first in her class in 1958. Next, she completed a Bachelor of Science degree in Secondary Education at Oklahoma State University, along with additional aviation instrumentation and instruction ratings. At 20 years old, Funk became a professional aviator.\n\nFunk always wanted to be an astronaut. She volunteered to be a part of the Women in Space Program in 1961, becoming one of the Mercury 13. Like the other participants in the program, Funk was put through rigorous physical and mental testing. In one test, volunteers were placed in sensory deprivation tanks. Funk was in the tank, without hallucinating, for 10 hours and 35 minutes, a record. She passed her tests and was qualified to go into space. She scored higher than John Glenn did in his testing and was the third best in the Mercury 13 program. Despite this, the program was canceled before the women were to undergo their last test.\n\nAfter the Mercury 13 program was canceled, Funk became a Goodwill Ambassador, flying over 80,000 miles throughout the world. She earned the rating of flight instructor in the United States, from the Federal Aviation Administration (FAA). At Fort Sill, Oklahoma, she became the first non-military flight instructor. She was the first woman to finish the FAA General Aviation Operations Inspector Academy, which she finished in 1971. She became an Air Safety Investigator for the National Transportation Safety Board, in 1974. She was the first woman to be given the title. In 1995, Eileen Collins became the first woman to pilot a space shuttle into space; Funk was too old to qualify to become a space shuttle pilot by the time Collins became one.\n\nShe has over 19,000 flight hours. She has taught over 3,000 students how to fly. Funk has continued to dream of going to space. In 2012, she put money down to be one of the first people to fly into space via Virgin Galactic. The money for the flight came from Funk's own book and film royalties and family money. She learned how to fly a Black Hawk helicopter in 2012. She lives in Roanoke, Texas.\n\n"}
{"id": "25823199", "url": "https://en.wikipedia.org/wiki?curid=25823199", "title": "Website localization", "text": "Website localization\n\nWebsite localization is the process of adapting an existing website to local language and culture in the target market. It is the process of adapting a website into a different linguistic and cultural context— involving much more than the simple translation of text. This modification process must reflect specific language and cultural preferences in the content, images and overall design and requirements of the site – all while maintaining the integrity of the website. Culturally adapted web sites reduce the amount of required cognitive efforts from visitors of the site to process information, making navigation easier and attitudes toward the web site more favorable. The modification of the website must additionally take into consideration the stated purpose of the new website with a focus on the targeted audience/market in the new locale. Website localization aims to customize a website so that it seems \"natural\", to its viewers despite cultural differences between the creators and the audience.\nTwo factors are involved—programming expertise and linguistic/cultural knowledge.\n\nThe proliferation of website localization is the result of the popularity of computer and Internet users. People all over the world treat the Internet as their main location for information and services. These people do not all speak the same language. As a result, website localization has become one of the primary tools for business global expansion.\n\nDue to website communication across multiple cultures for multiple needs, the Internet has given way to non professional translation practices. Because website localization involves mixed strategies, organizations tend to maintain a global image while using website localization to appeal to local users. The challenge of website localization has become even more important as websites increasingly have the potential to both supplement and replace presence in foreign markets. As web design becomes more congruent with national culture, it will foster online consumer purchasing. Creators take into account the \"language, education level, belief and value systems, [and] traditions and habits\" of the target culture in order to optimize results.\n\nWebsite localization is more than mere translation. Translating only solves partial language problems. Measurement units must be converted; images and text are modified to appeal to the target culture.\n\nThe process of website localization is complex and involves three different levels of adaptation. First is translation. Website localization involves adapting any text being used into the language of the country. It is important that translation of information be “clear and understandable” to avoid cultural misunderstanding or offense. In order to translate, the \"target culture\" must be known. Second is the actual localization, which includes translation but also involves all other efforts and activities to ensure that the adaptation of textual materials, visual displays, illustrations and graphics are “linguistically and culturally appropriate for the target locale.” Target locale is understood as the “market segment defined by criteria including language, currency, and perhaps educational level or income bracket.” Among the many technical elements which can be localized are: date and time formats, currency formats, number formats, address and telephone number formats, units of measure and connection speed. In order to ensure effective communication during the localization process, it is important to consider the following items: information architecture, theme and navigation, graphics, photographs, audio, and visual. Third is internationalization, which involves making sure that the software being used is fully compatible with the technology of the country in question.\n\nThere are two important considerations to keep in mind during the process of website localization. The first is to focus on the demands of the user. The readers of the “localized version of the website” want to be able to read and understand the pages in a way that makes sense to them. A second consideration is to take into account the goals of the client, whether an institution, government or individual, for example.\n\nMany elements of a website that are different according to the locale of the client need only minor manual changes by a localizer, or none at all. For example, the system on which the website is created should automatically produce the correct currency symbol based on the country in which the client is located. \n\nUsing website localization to its best advantage is vital to any business seeking to move into international markets. With more and more companies seeking to tap into these lucrative markets, website localization has become quite profitable. The business side involves global and local coordination; production and operations, including finance; sales and marketing; language translation, including technology and linguistic coordination; software engineering, as well as design.\nThe higher the technological abilities of the target culture, the more likely it is for website localization to be implemented and used effectively.\n\n"}
{"id": "1199099", "url": "https://en.wikipedia.org/wiki?curid=1199099", "title": "Well to Hell hoax", "text": "Well to Hell hoax\n\nThe \"Well to Hell\" is an urban legend regarding a putative borehole in Russia which was purportedly drilled so deep that it broke through into Hell. This urban legend has been circulating on the Internet since at least 1995. It is first attested in English as a 1989 broadcast by a U.S. domestic religion-based TV broadcaster, Trinity Broadcasting Network.\n\nThe legend holds that a team of Russian engineers purportedly led by an individual named \"Mr. Azakov\" in an unnamed place in Siberia had drilled a hole that was deep before breaking through to a cavity. Intrigued by this unexpected discovery, they lowered an extremely heat-tolerant microphone, along with other sensory equipment, into the well. The temperature deep within was —heat from a chamber of fire from which (purportedly) the tormented screams of the damned could be heard. However, the recording was later found to be looped together from various sound effects, sometimes identified as the soundtrack of the 1972 movie \"Baron Blood\".\n\nThe Soviet Union had, in fact, drilled a hole more than deep, the Kola Superdeep Borehole, located not in Siberia but on the Kola Peninsula, which shares borders with Norway and Finland. Upon reaching the depth of in 1989, some interesting geological anomalies were found, although they reported no supernatural encounters.\n\nThe story was reported to first have been published by the Finnish newspaper \"Ammennusastia\", a journal published by a group of Pentecostal Christians from Leväsjoki, a village in the municipality of Siikainen in Western Finland. Rich Buhler, who interviewed the editors, found that the story had been based on recollections of a letter printed in the feature section of a newspaper called \"Etelä Soumen\" (possibly the \"Etelä-Suomen Sanomat\"). When contacting the letter's author, Buhler found that he had drawn from a story appearing in a Finnish Christian newsletter named \"Vaeltajat\", which had printed the story in July 1989. The newsletter's editor claimed that its origin had been a newsletter called \"Jewels of Jericho\", published by a group of Messianic Jews in California. Here, Buhler stopped tracing the origins any further.\n\nAmerican tabloids soon ran the story, and sound files began appearing on various sites across the Internet. Sensationalistic retellings of the legend can be found on YouTube, usually featuring the aforementioned \"Baron Blood\" sound effects.\n\nThe story eventually made its way to the American Christian Trinity Broadcasting Network (TBN), which broadcast it on the network, claiming it to be proof of the literal existence of hell.\n\nÅge Rendalen, a Norwegian teacher, heard the story on TBN while visiting the United States. Disgusted with what he perceived to be mass gullibility, Rendalen decided to augment the tale at TBN's expense.\n\nRendalen wrote to the network, originally claiming that he disbelieved the tale but, upon his return to Norway, supposedly read a factual account of the story. According to Rendalen, the story claimed not only that the cursed well was real, but that a bat-like apparition (a common pictorial representation of demons, such as in Michelangelo's \"The Torment of Saint Anthony\" or the more recent Bat Boy by \"Weekly World News\") had risen out of it before blazing a trail across the Russian sky. To perpetuate his hoax, Rendalen deliberately mistranslated a trivial Norwegian article about a local building inspector into the story, and submitted both the original Norwegian article and the English \"translation\" to TBN. Rendalen also included his real name, phone number and address, as well as those of a pastor friend who knew about the hoax and had agreed to expose it to anyone who called seeking verification.\n\nHowever, TBN did nothing to verify Rendalen's claims, and aired the story as proof of the validity of the original story.\n\nSince its publicity, many alternative versions of the Well to Hell story have been published. In 1992, the U.S. tabloid \"Weekly World News\" published an alternative version of the story, which was set in Alaska where 13 miners were killed after Satan came roaring out of hell.\n\n\n"}
{"id": "47101859", "url": "https://en.wikipedia.org/wiki?curid=47101859", "title": "Wilhelm Phillip Daniel Schulz", "text": "Wilhelm Phillip Daniel Schulz\n\nWilhelm Phillip Daniel Schulz (6 March 1805 – 1 August 1877), also known as Guillermo Schulz, was a German mine engineer and geologist who spent most of his professional life in Spain. He was born in Dörnberg and died in Aranjuez.\n\nIn 1826 Schulz went to Spain, and shortly was hired by the Spanish Government to enhance the mining industry in the country. In 1833 he was appointed Mining Inspector for Asturias and Galicia, and in 1844 General Mining Inspector for Asturias. It was in Asturias where, for many years, he carried out outstanding geological and industrial work which he made public in several books. In 1854 he moved to Madrid, where he taught in the Mining Engineering School and was vice-president of the Geological Institute.\n"}
{"id": "31422551", "url": "https://en.wikipedia.org/wiki?curid=31422551", "title": "Youla–Kucera parametrization", "text": "Youla–Kucera parametrization\n\nIn control theory the Youla–Kučera parametrization (also simply known as Youla parametrization) is a formula that describes all possible stabilizing feedback controllers for a given plant P, as function of a single parameter Q.\n\nThe YK parametrization is a general result. It is a fundamental result of control theory and launched an entirely new area of research and found application, among others, in optimal and robust control.\n\nFor ease of understanding and as suggested by Kučera it is best described for three increasingly general kinds of plant.\n\nLet formula_1 be a transfer function of a stable Single-input single-output system (SISO) system. Further, let Ω be a set of stable and proper functions of \"s\". Then, the set of all proper stabilizing controllers for the plant formula_1 can be defined as\n\nformula_3,\n\nwhere formula_4 is an arbitrary proper and stable function of \"s\". It can be said, that formula_4 parametrizes all stabilizing controllers for the plant formula_1.\n\nConsider a general plant with a transfer function formula_1. Further, the transfer function can be factorized as\n\nformula_8, where M(s), N(s) are stable and proper functions of \"s\".\n\nNow, solve the Bézout's identity of the form\n\nformula_9,\n\nwhere the variables to be found (X(s), Y(s)) must be also proper and stable.\n\nAfter proper and stable X, Y were found, we can define one stabilizing controller that is of the form formula_10. After we have one stabilizing controller at hand, we can define all stabilizing controllers using a parameter Q(s) that is proper and stable. The set of all stabilizing controllers is defined as\n\nformula_11,\n\nIn a multiple-input multiple-output (MIMO) system, consider a transfer matrix formula_12. It can be factorized using right coprime factors formula_13 or left factors formula_14. The factors must be proper, stable and doubly coprime, which ensures that the system P(s) is controllable and observable. This can be written by Bézout identity of the form\n\nformula_15.\n\nAfter finding formula_16 that are stable and proper, we can define the set of all stabilizing controllers K(s) using left or right factor, provided having negative feedback.\n\nformula_17\n\nwhere formula_18 is an arbitrary stable and proper parameter. \n\nLet formula_1 be the transfer function of the plant and let formula_20 be a stabilizing controller. Let their right coprime factorizations be:\nthen all stabilizing controllers can be written as\nwhere Q is stable and proper.\nThe engineering significance of the YK formula is that if one wants to find a stabilizing controller that meets some additional criterion, one can adjust Q such that the desired criterion is met.\n\n"}
