{"id": "19105502", "url": "https://en.wikipedia.org/wiki?curid=19105502", "title": "Alfred Kohn", "text": "Alfred Kohn\n\nAlfred Kohn (22 February 1867 – 15 January 1959) was the head of the Institute of Histology at the Medical Faculty of German University in Prague for 26 years. He entered the history of medicine by discovery of the nature and origin of parathyroid glands and by pioneering research into chromaffin cells and sympathetic paraganglia. Kohn's papers on the pituitary, interstitial cells of testes, and ovaries are also related to endocrinology. All his studies are based on descriptive and comparative histological and embryological observations. Kohn was twice the dean of German Medical Faculty, and a member or honorary member of many important scientific societies. He was repeatedly nominated for Nobel Prize for physiology and medicine. For his Jewish origin he was expelled from Deutsche Gesellschaft der Wissenschaften und Künste für die Tschechoslowakische Republik in 1939 and transported to Terezin (Theresienstadt) ghetto in 1943. After the war he lived in Prague. On the occasion of his 90th birthday he was elected honorary president of Anatomische Gesellschaft and awarded by the Czechoslovak Order of Labour.\nAlfred Kohn died in 1959. He was one of the outstanding personalities that Prague gave to the world of science.\n\n"}
{"id": "16670750", "url": "https://en.wikipedia.org/wiki?curid=16670750", "title": "Astroscan", "text": "Astroscan\n\nThe Astroscan was a wide-field 4⅛\" clear-inch (105mm) diameter reflecting telescope, originally produced by the Edmund Scientific Corporation, that was for sale from 1976 to 2013. \n\nThe Astroscan had a Newtonian reflector layout with a 4⅛\" clear-inch (105mm) diameter f/4.2 aluminized and overcoated borosilicate glass parabolic primary mirror with a focal length of 17½ inches (445mm). The telescope's secondary mirror was mounted on a flat optical window at the front of the tube. The telescope was designed by Norman Sperling and Mike Simmons to be used as an introductory telescope. Rather than using a more traditional equatorial or altazimuth mount the Astroscan features a spherical housing around the primary reflector which sat in a cast aluminum cradle. The design was durable and allowed for simple operation by novice amateur astronomers; it won an Industrial Design Award in 1976. The telescope body was made from high impact acrylonitrile butadiene styrene (ABS) plastic and was equipped with a carrying strap to aid portability. \n\nBecause it was targeted at the novice market the telescope had its limitations: The general design was for low power hand-held or wide-angle work, the short f/4 focal ratio did not allow for high magnification without the image degrading, and the primary mirror was factory aligned with no provisions for adjustment. The Astroscan came with 15 mm and 28 mm focal length Plössl eyepieces, giving it a magnification of 30X and 16X respectively, with a 3.0° field of view using the 28 mm eyepiece, and a 1.6° with the 15 mm.\n\nWhen Edmund Scientific introduced the telescope in 1976 they called it \"The Edmund Wide-Field Telescope\" with a Part Number \"2001\" Edmund had a public contest which ran until November 15, 1976 to come up with a name. The winning name was \"Astroscan 2001\". The \"2001\" part of the name was dropped over time.\n\nThe Astroscan continued to be available after Edmund Scientific was acquired by Science Kit and Boreal Laboratories in 2001 with the telescope for sale on the \"Edmund Scientific\" website. Production and sales of the telescope ceased in 2013 when the mold for the plastic body broke. In 2016 the Edmund Scientific website, now called Scientifics Direct, began offering a more common format altitude-azimuth mounted 4.5 inch table-top Newtonian telescope labeled the \"Astroscan Millennium\". There are reports of a Kickstarter campaign by one of the original Astroscan creators to fund production of a new version based on the original Astroscan telescope. Scientifics Direct's website stated they would reintroduce the original Astroscan design in late 2020.\n\n\n"}
{"id": "13543277", "url": "https://en.wikipedia.org/wiki?curid=13543277", "title": "Biskit", "text": "Biskit\n\nBiskit is an open source software package written in Python.\nThe package facilitates research in Structural bioinformatics and molecular modelling. Biskit falls into two parts:\n\n\nThe Biskit library delegates many calculations to more specialized third-party programs and currently wraps about 15 external applications. Examples are X-PLOR, Hex, T-Coffee, DSSP and MODELLER.\n\nThe latest version 2.4.0 was released on 4 Mar 2012 and the project is under active development. Biskit was originally developed at the Pasteur Institute and the name Biskit refers to the title of the research group: \"Unité de BioInformatique Structurale\".\n"}
{"id": "11828715", "url": "https://en.wikipedia.org/wiki?curid=11828715", "title": "Book of Optics", "text": "Book of Optics\n\nThe Book of Optics (; Latin: De Aspectibus or Perspectiva; Italian: Deli Aspecti) is a seven-volume treatise on optics and other fields of study composed by the medieval Arab scholar Ibn al-Haytham, known in the West as Alhazen or Alhacen (965– c. 1040 AD).\n\nThe \"Book of Optics\" presented experimentally founded arguments against the widely held extramission theory of vision (as held by Euclid in his \"Optica\") and in favor of intromission theory, as supported by thinkers such as Aristotle, the now accepted model that vision takes place by light entering the eye. Alhazen's work extensively affected the development of optics in Europe between 1260 and 1650.\n\nBefore the \"Book of Optics\" was written, two theories of vision existed. The extramission or emission theory was forwarded by the mathematicians Euclid and Ptolemy, who asserted that certain forms of radiation are emitted from the eyes onto the object which is being seen. When these rays reached the object they allowed the viewer to perceive its color, shape and size. The intromission theory, held by the followers of Aristotle and Galen, argued that sight was caused by agents, which were transmitted to the eyes from either the object or from its surroundings.\n\nAl-Haytham offered many reasons against the extramission theory, pointing to the fact that eyes can be damaged by looking directly at bright lights, such as the sun. He claimed the low probability that the eye can fill the entirety of space as soon as the eyelids are opened as an observer looks up into the night sky. Using the intromission theory as a foundation, he formed his own theory that an object emits rays of light from every point on its surface which then travel in all directions, thereby allowing some light into a viewer's eyes. According to this theory, the object being viewed is considered to be a compilation of an infinite amount of points, from which rays of light are projected.\n\nIn the \"Book of Optics\", al-Haytham claimed the existence of primary and secondary light, with primary light being the stronger or more intense of the two. The book describes how the essential form of light comes from self-luminous bodies and that accidental light comes from objects that obtain and emit light from those self-luminous bodies. According to Ibn al-Haytham, primary light comes from self-luminous bodies and secondary light is the light that comes from accidental objects. Accidental light can only exist if there is a source of primary light. Both primary and secondary light travel in straight lines. Transparency is a characteristic of a body that can transmit light through them, such as air and water, although no body can completely transmit light or be entirely transparent. Opaque objects are those through which light cannot pass through directly, although there are degrees of opaqueness which determine how much light can actually pass through. Opaque objects are struck with light and can become luminous bodies themselves which radiate secondary light. Light can be refracted by going through partially transparent objects and can also be reflected by striking smooth objects such as mirrors, traveling in straight lines in both cases.\n\nAl-Haytham presented many experiments in \"Optics\" that upheld his claims about light and its transmission. He also claimed that color acts much like light, being a distinct quality of a form and travelling from every point on an object in straight lines. Through experimentation he concluded that color cannot exist without air.\n\nAs objects radiate light in straight lines in all directions, the eye must also be hit with this light over its outer surface. This idea presented a problem for al-Haytham and his predecessors, as if this was the case, the rays received by the eye from every point on the object would cause a blurred image. Al-Haytham solved this problem using his theory of refraction. He argued that although the object sends an infinite amount of rays of light to the eye, only one of these lines falls on the eye perpendicularly: the other rays meet the eye at angles that aren't perpendicular. According to al-Haytham, this causes them to be refracted and weakened. He claimed that all the rays other than the one that hits the eye perpendicularly are not involved in vision.\n\nIn al-Haytham's structure of the eye, the crystalline humor is the part that receives light rays from the object and forms a visual cone, with the object being perceived as the base of the cone and the center of the crystalline humor in the eye as the vertex. Other parts of the eye are the aqueous humor in front of the crystalline humor and the vitreous humor at the back. These, however, do not play as critical of a role in vision as the crystalline humor. The crystalline humor transmits the image it perceives to the brain through an optic nerve.\n\n\nThe \"Book of Optics\" was most strongly influenced by Ptolemy's \"Optics\", while the description of the anatomy and physiology of the eye was based upon an account by Galen.\n\nThe \"Book of Optics\" was translated into Latin by an unknown scholar at the end of the 12th (or the beginning of the 13th) century. The work was influential during the Middle Ages. It was printed by Friedrich Risner in 1572, as part of his collection \"Opticae thesaurus\". This included a book on twilight falsely attributed to Alhazen, as well as a work on optics by Witelo.\n\n\n"}
{"id": "1651329", "url": "https://en.wikipedia.org/wiki?curid=1651329", "title": "Botanical name", "text": "Botanical name\n\nA botanical name is a formal scientific name conforming to the \"International Code of Nomenclature for algae, fungi, and plants\" (ICN) and, if it concerns a plant cultigen, the additional cultivar or Group epithets must conform to the \"International Code of Nomenclature for Cultivated Plants\" (ICNCP). The code of nomenclature covers \"all organisms traditionally treated as algae, fungi, or plants, whether fossil or non-fossil, including blue-green algae (Cyanobacteria), chytrids, oomycetes, slime moulds and photosynthetic protists with their taxonomically related non-photosynthetic groups (but excluding Microsporidia).\"\n\nThe purpose of a formal name is to have a single name that is accepted and used worldwide for a particular plant or plant group. For example, the botanical name \"Bellis perennis\" denotes a plant species which is native to most of the countries of Europe and the Middle East, where it has accumulated various names in many languages. Later, the plant was introduced worldwide, bringing it into contact with more languages. English names for this plant species include: daisy, English daisy, and lawn daisy. The cultivar \"Bellis perennis\" 'Aucubifolia' is a golden-variegated horticultural selection of this species.\n\nThe botanical name itself is fixed by a type, which is a particular specimen (or in some cases a group of specimens) of an organism to which the scientific name is formally attached. In other words, a type is an example that serves to anchor or centralize the defining features of that particular taxon.\n\nThe usefulness of botanical names is limited by the fact that taxonomic groups are not fixed in size; a taxon may have a varying circumscription, depending on the taxonomic system, thus, the group that a particular botanical name refers to can be quite small according to some people and quite big according to others. For example, the traditional view of the family Malvaceae has been expanded in some modern approaches to include what were formerly considered to be several closely related families. Some botanical names refer to groups that are very stable (for example Equisetaceae, Magnoliaceae) while for other names a careful check is needed to see which circumscription is being used (for example Fabaceae, Amygdaloideae, \"Taraxacum officinale\").\n\nDepending on rank, botanical names may be in one part (genus and above), two parts (various situations below the rank of genus) or three parts (below the rank of species). The names of cultivated plants are not necessarily similar to the botanical names, since they may instead involve \"unambiguous common names\" of species or genera. Cultivated plant names may also have an extra component, bringing a maximum of four parts:\n\n\n\n\n\nA botanical name in three parts, i.e., an infraspecific name (a name for a taxon below the rank of species) needs a \"connecting term\" to indicate rank. In the \"Calystegia\" example above, this is \"subsp.\", for subspecies. In botany there are many ranks below that of species (in zoology there is only one such rank, subspecies, so that this \"connecting term\" is not used in zoology). A name of a \"subdivision of a genus\" also needs a connecting term (in the \"Acacia\" example above, this is \"subg.\", subgenus). The connecting term is not part of the name itself.\n\nA taxon may be indicated by a listing in more than three parts: \"\"Saxifraga aizoon\" var. \"aizoon\" subvar. \"brevifolia\" f. \"multicaulis\" subf. \"surculosa\" Engl. & Irmsch.\" but this is a classification, not a formal botanical name. The botanical name is \"Saxifraga aizoon\" subf. \"surculosa\" Engl. & Irmsch. (\"ICN\" Art 24: Ex 1).\n\nGeneric, specific, and infraspecific botanical names are usually printed in italics. The example set by the \"ICN\" is to italicize all botanical names, including those above genus, though the \"ICN\" preface states: \"The \"Code\" sets no binding standard in this respect, as typography is a matter of editorial style and tradition not of nomenclature\". Most peer-reviewed scientific botanical publications do not italicize names above the rank of genus, and non-botanical scientific publications do not, which is in keeping with two of the three other kinds of scientific name: zoological and bacterial (viral names above genus are italicized, a new policy adopted in the early 1990s).\n\nFor botanical nomenclature, the \"ICN\" prescribes a two-part name or binary name for any taxon below the rank of genus down to, and including the rank of species. Taxa below the rank of species get a three part (infraspecific name).\n\nA binary name consists of the name of a genus and an epithet.\n\n\n\nIn the case of cultivated plants, there is an additional epithet which is an often non-Latin part, not written in italics. For cultivars, it is always given in single quotation marks. The cultivar, Group, or grex epithet may follow either the botanical name of the species, or the name of the genus only, or the unambiguous common name of the genus or species. The generic name, followed by a cultivar name, is often used when the parentage of a particular hybrid cultivar is not relevant in the context, or is uncertain.\n\n\n\n"}
{"id": "5597508", "url": "https://en.wikipedia.org/wiki?curid=5597508", "title": "Cape Canaveral Air Force Station Launch Complex 19", "text": "Cape Canaveral Air Force Station Launch Complex 19\n\nLaunch Complex 19 (LC-19) is a deactivated launch site on Cape Canaveral Air Force Station, Florida used by NASA to launch all of the Gemini manned spaceflights. It was also used by unmanned Titan I and Titan II missiles.\n\nLC-19 was in use from 1959 to 1966, during which time it saw 27 launches, 10 of which were manned. The first flight from LC-19 was on August 14, 1959 and ended in a pad explosion, extensively damaging the facility, which took a few months to repair. The first successful launch from LC-19 was also a Titan I, on February 2, 1960. After being converted for the Titan II ICBM program in 1962, LC-19 was later designated for the Gemini flights. After the program concluded in December 1966, LC-19 was closed down.\n\nThe Gemini white room from the top of the booster erector has been partially restored and is on display at the Air Force Space and Missile Museum located at Complex 26.\n\n\n"}
{"id": "22786710", "url": "https://en.wikipedia.org/wiki?curid=22786710", "title": "Carcel", "text": "Carcel\n\nThe Carcel is a former French unit for measuring the intensity of light. The unit was defined in 1860 as the intensity of a Carcel lamp with standard burner and chimney dimensions, which burnt colza oil\n\nIn modern terminology one carcel equals about 9.74 candelas.\n\n"}
{"id": "55409585", "url": "https://en.wikipedia.org/wiki?curid=55409585", "title": "Carina M. Schlebusch", "text": "Carina M. Schlebusch\n\nCarina Maria Schlebusch is an evolutionary biologist at the University of Uppsala in Sweden. She is a specialist in the population history of Africa. In 2017 she was the co-author of a paper that suggested that modern humans emerged more than 300,000 years ago.\n"}
{"id": "32523563", "url": "https://en.wikipedia.org/wiki?curid=32523563", "title": "Coats of arms of the U.S. states", "text": "Coats of arms of the U.S. states\n\nThe coats of arms of the U.S. states are coats of arms, adopted by those states that have chosen, that are an official symbol of the state, alongside their seal. Eighteen states have officially adopted coats of arms. The former independent Republic of Texas and Kingdom of Hawaii each had a separate national coat of arms, which are no longer used.\n\n"}
{"id": "26346644", "url": "https://en.wikipedia.org/wiki?curid=26346644", "title": "Development ethics", "text": "Development ethics\n\nDevelopment ethics is a field of enquiry that reflects on both the ends and the means of economic development. It typically takes a normative stance, asking and answering questions about the nature of ethically desirable development and what ethics means for achieving development, and discusses various ethical dilemmas that the practice of development has led to. Its aim is to ensure that \"value issues\" are an important part of the discourse of development.\n\nDevelopment ethics typically looks at development theories and practice and their relationships with:\n\nA major focus of the literature is on the \"ethics of the means\". This involves asking not only how to realize the goals of development but also what are ethical limits in their pursuit.\n\nDenis Goulet, one of the founding fathers of the discipline, argued in \"The Cruel Choice\" (1971) that \"Development ethics is useless unless it can be translated into public action. By public action is meant action taken by public authority, as well as actions taken by private agents by having important consequences for the life of the public community. The central question is: How can moral guidelines influence decisions of those who hold power?\"\n\n\n"}
{"id": "1001908", "url": "https://en.wikipedia.org/wiki?curid=1001908", "title": "Dichromatism", "text": "Dichromatism\n\nDichromatism (or polychromatism) is a phenomenon where a material or solution's hue is dependent on both the concentration of the absorbing substance and the depth or thickness of the medium traversed. In most substances which are not dichromatic, only the brightness and saturation of the colour depend on their concentration and layer thickness.\nExamples of dichromatic substances are pumpkin seed oil, bromophenol blue, and resazurin.\nWhen the layer of pumpkin seed oil is less than 0.7 mm thick, the oil appears bright green, and in layer thicker than this, it appears bright red.\n\nThe phenomenon is related to both the physical chemistry properties of the substance and the physiological response of the human visual system to colour. This combined physicochemical–physiological basis was first explained in 2007.\n\nDichromatic properties can be explained by the Beer-Lambert law and by the excitation characteristics of the three types of cone photoreceptors in the human retina. Dichromatism is potentially observable in any substance that has an absorption spectrum with one wide but shallow local minimum and one narrow but deep local minimum. The apparent width of the deep minimum may also be limited by the end of the visible range of human eye; in this case, the true full width may not necessarily be narrow. As the thickness of the substance increases, the perceived hue changes from that defined by the position of the wide-but-shallow minimum (in thin layers) to the hue of the deep-but-narrow minimum (in thick layers).\n\nThe absorbance spectrum of pumpkin seed oil has the wide-but-shallow minimum in the green region of the spectrum and deep local minimum in the red region. In thin layers, the absorption at any specific green wavelength is not as low as it is for the red minimum, but a broader band of greenish wavelengths are transmitted, and hence the overall appearance is green. The effect is enhanced by the greater sensitivity to green of the photoreceptors in the human eye, and the narrowing of the red transmittance band by the long-wavelength limit of cone photoreceptor sensitivity.\nAccording to the Beer-Lambert law, when viewing through the coloured substance (and thus ignoring reflection), the proportion of light transmitted at a given wavelength, \"T\", decreases exponentially with thickness \"t\", T = e, where \"a\" is the absorbance at that wavelength. Let \"G\" = e be the green transmittance and \"R\" = e be the red transmittance. The ratio of the two transmitted intensities is then (\"G\"/\"R\") = e. If the red absorbance is less than the green, then as the thickness \"t\" increases, so does the ratio of red to green transmitted light, which causes the apparent hue of the colour to switch from green to red.\n\nThe extent of dichromatism of material can be quantified by the Kreft's dichromaticity index (DI). It is defined as the difference in hue angle (Δh) between the colour of the sample at the dilution, where the chroma (colour saturation) is maximal and the colour of four times more diluted (or thinner) and four times more concentrated (or thicker) sample. The two hue angle differences are called dichromaticity index towards lighter (Kreft's DI) and\ndichromaticity index towards darker (Kreft's DI) respectively. Kreft's dichromaticity index DI and DI for pumpkin oil, which is one of the most dichromatic substances, are −9 and −44, respectively. This means that pumpkin oil changes its colour from green-yellow to orange-red (for 44 degrees in Lab colour space) when the thickness of the observed layer is increased from cca 0.5 mm to 2 mm; and it changes slightly towards green (for 9 degrees) if its thickness is reduced for 4-fold.\n\nA record by William Herschel (1738–1822), shows he observed dichromatism with a solution of ferrous sulphate and tincture of nutgall in 1801 when working on an early solar telescope, but he did not recognise the effect.\n"}
{"id": "32044626", "url": "https://en.wikipedia.org/wiki?curid=32044626", "title": "Ellis Island Special", "text": "Ellis Island Special\n\nAn Ellis Island Special is a family name that is perceived or labeled, incorrectly, as having been altered by immigration officials at the Ellis Island Immigration Station, when a family reached the United States, typically from Europe in the 19th and early 20th centuries. In popular thought, some family lore, and literary fiction, some family names were seen as having been shortened by immigration officials for ease of pronunciation or record-keeping, or lack of understanding of the true name—even though name changes were made by the immigrants themselves at other times. Among the family names that are perceived as being Ellis Island Specials are some that were supposedly more identifiably Jewish, resulting in last names that were not identifiably so.\n\nThe phrase \"Ellis Island Special\" has also been adopted by some food vendors and applied to sandwiches, among other foods.\n"}
{"id": "10100", "url": "https://en.wikipedia.org/wiki?curid=10100", "title": "Equinox", "text": "Equinox\n\nAn equinox is commonly regarded as the moment when the plane (extended indefinitely in all directions) of Earth's equator passes through the center of the Sun, which occurs twice each year: around 20 March and 22–23 September. In other words, it is the moment at which the center of the visible Sun is directly above the Equator.\n\nHowever, because the Moon (and to a lesser extent the other planets) causes the true motion of the Earth to vary from a perfect ellipse, the equinox is now officially defined by the Sun's more regular ecliptic longitude rather than declination. The instants of the equinoxes are currently defined to be when the longitude of the Sun is 0° and 180°. There are tiny (up to 1¼ arcsecond) variations in the Sun's latitude (discussed below), which means the Sun's center is rarely precisely over the equator under the official definition. The two understandings of the equinox can lead to discrepancies of up to 69 seconds.\n\nOn the day of an equinox, daytime and nighttime are of approximately equal duration all over the planet. They are not exactly equal, however, due to the angular size of the Sun, atmospheric refraction, and the rapidly changing duration of the length of day that occurs at most latitudes around the equinoxes. The word is derived from the Latin ', from ' (equal) and ' (genitive ') (night).\n\nThe equinoxes are the only times when the solar terminator (the \"edge\" between night and day) is perpendicular to the equator. As a result, the northern and southern hemispheres are equally illuminated. The word comes from Latin \"Aequus\", meaning \"equal\", and \"Nox\", meaning \"night\".\n\nIn other words, the equinoxes are the only times when the subsolar point is on the equator, meaning that the Sun is exactly overhead at a point on the equatorial line. The subsolar point crosses the equator moving northward at the March equinox and southward at the September equinox.\n\nThe equinoxes, along with solstices, are directly related to the seasons of the year. In the northern hemisphere, the vernal equinox (March) conventionally marks the beginning of spring in most cultures and is considered the start of the New Year in Hindu calendar and the Persian calendar or Iranian calendars as Nowruz (means new day), while the autumnal equinox (September) marks the beginning of autumn.\n\nWhen Julius Caesar established the Julian calendar in 45 BC, he set 25 March as the date of the spring equinox which was already the starting day of year in Persian and Indian calendars. Because the Julian year is longer than the tropical year by about 11.3 minutes on average (or 1 day in 128 years), the calendar \"drifted\" with respect to the two equinoxes — such that in AD 300 the spring equinox occurred on about 21 March, and by AD 1500 it had drifted backwards to 11 March.\n\nThis drift induced Pope Gregory XIII to create a modern Gregorian calendar. The Pope wanted to continue to conform with the edicts concerning the date of Easter of the Council of Nicaea of AD 325, which means he wanted to move the vernal equinox to the date on which it fell at that time (21 March is the day allocated to it in the Easter table of the Julian calendar). However, the leap year intervals in his calendar were not smooth (400 is not an exact multiple of 97). This causes the equinox to oscillate by about 53 hours around its mean position. This in turn raised the possibility that it could fall on 22 March, and thus Easter Day might theoretically commence before the equinox. The astronomers chose the appropriate number of days to omit so that the equinox would swing from 19 to 21 March but never fall on the 22nd (although it can in a handful of years fall early in the morning of that day in the Far East).\n\n\nDay is usually defined as the period when sunlight reaches the ground in the absence of local obstacles. On the day of the equinox, the center of the Sun spends a roughly equal amount of time above and below the horizon at every location on the Earth, so night and day are about the same length. Sunrise and sunset can be defined in several ways, but a widespread definition is the time that the top limb of the sun is level with the horizon. With this definition, the day is longer than the night at the equinoxes:\n\nIn sunrise/sunset tables, the assumed semidiameter (apparent radius) of the Sun is 16 arcminutes and the atmospheric refraction is assumed to be 34 arcminutes. Their combination means that when the upper limb of the Sun is on the visible horizon, its centre is 50 arcminutes below the geometric horizon, which is the intersection with the celestial sphere of a horizontal plane through the eye of the observer.\n\nThese effects make the day about 14 minutes longer than the night at the equator and longer still towards the poles. The real equality of day and night only happens in places far enough from the equator to have a seasonal difference in day length of at least 7 minutes, actually occurring a few days towards the winter side of each equinox.\n\nThe times of sunset and sunrise vary with the observer's location (longitude and latitude), so the dates when day and night are equal also depend upon the observer's location.\n\nA third correction for the visual observation of a sunrise (or sunset) is the angle between the apparent horizon as seen by an observer and the geometric (or sensible) horizon. This is known as the dip of the horizon and varies from 3 arcminutes for a viewer standing on the sea shore to 160 arcminutes for a mountaineer on Everest. The effect of a larger dip on taller objects (reaching over 2½° of arc on Everest) accounts for the phenomenon of snow on a mountain peak turning gold in the sunlight long before the lower slopes are illuminated.\n\nThe date on which the day and night are exactly the same is known as an \"equilux\"; the neologism, believed to have been coined in the 1980s, achieved more widespread recognition in the 21st century. (Prior to this, the word \"equilux\" was more commonly used as a synonym for isophot, and there was no generally accepted term for the phenomenon.) At the most precise measurements, there is no such thing as an equilux, because the lengths of day and night change more rapidly than any other time of the year around the equinoxes. In the mid-latitudes, daylight increases or decreases by about three minutes per day at the equinoxes, and thus adjacent days and nights only reach within one minute of each other. The date of the closest approximation of the equilux varies slightly by latitude; in the mid-latitudes, it occurs a few days before the spring equinox and after the fall equinox in each respective hemisphere.\n\nIn the half-year centered on the June solstice, the Sun rises north of east and sets north of west, which means longer days with shorter nights for the northern hemisphere and shorter days with longer nights for the southern hemisphere. In the half-year centered on the December solstice, the Sun rises south of east and sets south of west and the durations of day and night are reversed.\n\nAlso on the day of an equinox, the Sun rises everywhere on Earth (except at the poles) at about 06:00 and sets at about 18:00 (local solar time). These times are not exact for several reasons:\n\nSome of the statements above can be made clearer by picturing the day arc (i.e., the path along which the Sun appears to move across the sky). The pictures show this for every hour on equinox day. In addition, some 'ghost' suns are also indicated below the horizon, up to 18° below it; the Sun in such areas still causes twilight. The depictions presented below can be used for both the northern and the southern hemispheres. The observer is understood to be sitting near the tree on the island depicted in the middle of the ocean; the green arrows give cardinal directions.\n\nThe following special cases are depicted:\n\nThe March equinox occurs about when the Sun appears to cross the celestial equator northward. In the Northern Hemisphere, the term \"vernal point\" is used for the time of this occurrence and for the precise direction in space where the Sun exists at that time. This point is the origin of some celestial coordinate systems, which are usually rooted to an astronomical epoch since it gradually varies (precesses) over time:\n\nStrictly speaking, at the equinox, the Sun's ecliptic longitude is zero. Its latitude will not be exactly zero, since Earth is not exactly in the plane of the ecliptic. Its declination will not be exactly zero either. (The ecliptic is defined by the barycenter of Earth and the Moon combined.) The modern definition of equinox is the instants when the Sun's apparent geocentric longitude is 0° (northward equinox) or 180° (southward equinox). See the adjacent diagram.\n\nBecause of the precession of the Earth's axis, the position of the vernal point on the celestial sphere changes over time, and the equatorial and the ecliptic coordinate systems change accordingly. Thus when specifying celestial coordinates for an object, one has to specify at what time the vernal point and the celestial equator are taken. That reference time is called the equinox of date.\n\nThe autumnal equinox is at ecliptic longitude 180° and at right ascension 12h.\n\nThe upper culmination of the vernal point is considered the start of the sidereal day for the observer. The hour angle of the vernal point is, by definition, the observer's sidereal time.\n\nUsing the current official IAU constellation boundaries – and taking into account the variable precession speed and the rotation of the celestial equator – the equinoxes shift through the constellations as follows (expressed in astronomical year numbering when the year 0 = 1 BC, −1 = 2 BC, etc.):\n\nThe equinoxes are sometimes regarded as the start of spring and autumn. A number of traditional harvest festivals are celebrated on the date of the equinoxes.\n\nObservations of the equinox are frequently used in online debates between flat-earth conspiracy proponents and those who support the generally accepted heliocentric globe model. Wolfie6020, a well-known flat-earth debunker on YouTube, has a semi-annual Equinox Challenge with prizes available to any flat-earther who can show a functioning flat-earth model that can match observations on the equinox. , no flat-earther has succeeded in winning the prize. Modern flat-earth proponents typically cite an azimuthal equidistant projection map to explain the daily and annual motions of the Sun and Moon, each along a sort of circular path around the North Pole, oscillating between the tropics throughout the year. However, proponents of the globe model point out that worldwide observations of the azimuths of the equinox sunrise and sunset]] (at around 90° east and 270° west, respectively) match the globe model, but cannot be reconciled with a localized overhead Sun above a flat plane.\n\nOne effect of equinoctial periods is the temporary disruption of communications satellites. For all geostationary satellites, there are a few days around the equinox when the sun goes directly behind the satellite relative to Earth (i.e. within the beam-width of the ground-station antenna) for a short period each day. The Sun's immense power and broad radiation spectrum overload the Earth station's reception circuits with noise and, depending on antenna size and other factors, temporarily disrupt or degrade the circuit. The duration of those effects varies but can range from a few minutes to an hour. (For a given frequency band, a larger antenna has a narrower beam-width and hence experiences shorter duration \"Sun outage\" windows.)\n\nEquinoxes occur on any planet with a tilted rotational axis. A dramatic example is Saturn, where the equinox places its ring system edge-on facing the Sun. As a result, they are visible only as a thin line when seen from Earth. When seen from above – a view seen during an equinox for the first time from the \"Cassini\" space probe in 2009 – they receive very little sunshine, indeed more planetshine than light from the Sun. This phenomenon occurs once every 14.7 years on average, and can last a few weeks before and after the exact equinox. Saturn's most recent equinox was on 11 August 2009, and its next will take place on 6 May 2025.\n\nMars's most recent equinox was on 22 May 2018 (northern autumn), and the next will be on 23 March 2019 (northern spring).\n\n\n"}
{"id": "22544085", "url": "https://en.wikipedia.org/wiki?curid=22544085", "title": "Excellence in Research for Australia", "text": "Excellence in Research for Australia\n\nExcellence in Research for Australia (ERA) is a research management initiative of the Australian Rudd Government developed by the Australian Research Council (ARC).\n\nIt replaced the Research Quality Framework that was developed by the Howard Government.\n\nIn addition to the Higher Education Research Data Collection, which collects statistics about research in Australia, the ERA collected itemised data, with all research classified according to the Australian Bureau of Statistics' Field of Research (FOR) classification scheme.\n\nFor the 2010 data collection, the Field of Research codes are distributed into the following eight clusters:\n\nFor the 2012 data collection, the clusters were changed. The SBE cluster was split into two new EHS and EC clusters, and the BCH and PAH clusters were merged to form a \"Medical and Health Sciences\" cluster.\nThe Field of Research codes are distributed into the following eight clusters:\n\nThe following institutions are deemed eligible to submit data to the government as part of the ERA initiative:\nOn 6 December 2012 Senator the Hon Chris Evans, Minister for Tertiary Education, Skills, Jobs and Workplace Relations, announced the outcomes of the ERA 2012 process, with the release of the \"ERA 2012 National report\".\n\nIn 2009, two trials were conducted for the clusters \"Physical, Chemical and Earth Sciences\" (PCE) and \"Humanities and Creative Arts\" (HCA), and reports have been published using this data.\n\nIn 2010, institutions were required to submit data for all eight clusters between June 1–18.\n\nOn 25 October 2010 the government announced that ERA data will be collected again in 2012.\n\nThe ARC maintains a list of journals that are eligible for inclusion in the ERA. The ARC initially stated that these journals would be ranked using the following \"four tiers of quality rating\":\n\nAfter the publication of its draft rankings, ERA introduced a form aimed at all scholars who wished to put a journal forward for the list. There were just three conditions for such a proposal: that the journal be \"a scholarly, peer reviewed journal with an ISSN\", that the person making the proposal state whether he/she was a member of the editorial board, and that the decision remain at ARC's discretion. This consultation procedure led to a significant increase in the number of journals in the final list: for example, Social Sciences and Humanities (SSH) journals went from 10,241 to 12,976. The percentage distributions were not recalled and not adhered to in the final list which was released on 9 February 2010, though the proportion of A* and A journals did not correlate directly with the performance of different disciplines.\n\nThese journal rankings (A*, A, B, C) were discontinued for the 2012 ERA process.\n\nThe ARC has used Scopus as the citation and bibliometrics provider for the 2010 and 2012 ERA.\n\nThe list of conference rankings was released in December 2009. Conferences have only a three level ranking scheme: A, B, or C.\n\nConferences are only ranked within the following Australian and New Zealand Standard Research Classification Fields of Research:\n\nAs with journal rankings, a prescribed distribution has not been mandated by the ARC. The Deakin ERA Journal Rankings Access website has been expanded and renamed the ERA Outlets Rankings Access website.\n\n\n"}
{"id": "7663635", "url": "https://en.wikipedia.org/wiki?curid=7663635", "title": "Explorer 33", "text": "Explorer 33\n\nExplorer 33 (also known as AIMP-D, IMP-D, AIMP 1, Anchored IMP 1, Interplanetary Monitoring Platform-D) was a spacecraft in the Explorer program launched by NASA on July 1, 1966 on a mission of scientific exploration.\n\nOriginally intended for a lunar orbit, mission controllers worried that the spacecraft's trajectory was too fast to guarantee lunar capture. Consequently, mission managers opted for a backup plan of placing the craft into an eccentric Earth orbit with a perigee of 265,679 km and an apogee of 480,762 km — still reaching distances beyond the Moon's orbit.\n\nDespite not attaining the intended lunar orbit, the mission met many of its original goals in exploring solar wind, interplanetary plasma, and solar X-rays. Principal investigator James Van Allen used electron and proton detectors aboard the spacecraft to investigate charged particle and X-ray activity. Astrophysicists N. U. Crooker, Joan Feynman, and J. T. Gosling used data from \"Explorer 33\" to establish relationships between the Earth's magnetic field and the solar wind speed near Earth.\n\n\n"}
{"id": "986051", "url": "https://en.wikipedia.org/wiki?curid=986051", "title": "Fine-tuning", "text": "Fine-tuning\n\nIn theoretical physics, fine-tuning is the process in which parameters of a model must be adjusted very precisely in order to fit with certain observations. \n\nTheories requiring fine-tuning are regarded as problematic in the absence of a known mechanism to explain why the parameters happen to have precisely the observed values that they return. The heuristic rule that parameters in a fundamental physical theory should not be too fine-tuned is called naturalness. \n\nThe idea that Naturalness will explain fine tuning was brought into question by Nima Arkani-Hamed, a theoretical physicist, in his talk 'Why is there a Macroscopic Universe?', a lecture from the mini-series \"Multiverse & Fine Tuning\" from the \"Philosophy of Cosmology\" project, A University of Oxford and Cambridge Collaboration 2013.\nIn it he describes how naturalness has usually provided a solution to problems in physics; and that it had usually done so earlier than expected.\nHowever, in addressing the problem of the cosmological constant, naturalness has failed to provide an explanation though it would have been expected to have done so a long time ago.\n\nThe necessity of fine-tuning leads to various problems that do not show that the theories are incorrect, in the sense of falsifying observations, but nevertheless suggest that a piece of the story is missing. For example, the cosmological constant problem (why is the cosmological constant so small?); the hierarchy problem; and the strong CP problem, among others.\n\nAn example of a fine-tuning problem considered by the scientific community to have a plausible \"natural\" solution is the cosmological flatness problem, which is solved if inflationary theory is correct: inflation forces the universe to become very flat, answering the question of why the universe is today observed to be flat to such a high degree.\n\nAlthough fine-tuning was traditionally measured by ad hoc fine-tuning measures, such as the Barbieri-Giudice-Ellis measure, over the past decade many scientists recognized that fine-tuning arguments were a specific application of Bayesian statistics.\n\n"}
{"id": "2916607", "url": "https://en.wikipedia.org/wiki?curid=2916607", "title": "Force field (physics)", "text": "Force field (physics)\n\nIn physics a force field is a vector field that describes a non-contact force acting on a particle at various positions in space. Specifically, a force field is a vector field formula_1, where formula_2 is the force that a particle would feel if it were at the point formula_3.\n\n\nAs a particle moves through a force field along a path \"C\", the work done by the force is a line integral\nThis value is independent of the velocity/momentum that the particle travels along the path. For a conservative force field, it is also independent of the path itself, depending only on the starting and ending points. Therefore, if the starting and ending points are the same, the work is zero for a conservative field:\nIf the field is conservative, the work done can be more easily evaluated by realizing that a conservative vector field can be written as the gradient of some scalar potential function:\n\nThe work done is then simply the difference in the value of this potential in the starting and end points of the path. If these points are given by x = a and x = b, respectively:\n\n\n"}
{"id": "5890041", "url": "https://en.wikipedia.org/wiki?curid=5890041", "title": "Future Fantastic", "text": "Future Fantastic\n\nFuture Fantastic was a British documentary television series which premiered in 1996. This show looked at the how science and science fiction complement each other, and how ideas and technologies from the past are helping to shape our future. The series was narrated by Gillian Anderson and co-produced by the British Broadcasting Corporation, The Learning Channel and Pro Sieben.\n\n\nThe theme music to \"Future Fantastic\" was by HAL who later collaborated with Gillian Anderson on the track \"Extremis\" which was released by Virgin Records in 1997.\n"}
{"id": "59877", "url": "https://en.wikipedia.org/wiki?curid=59877", "title": "Gas constant", "text": "Gas constant\n\nThe gas constant is also known as the molar, universal, or ideal gas constant, denoted by the symbol or and is equivalent to the Boltzmann constant, but expressed in units of energy per temperature increment per \"mole\", i.e. the pressure-volume product, rather than energy per temperature increment per \"particle\". The constant is also a combination of the constants from Boyle's law, Charles's law, Avogadro's law, and Gay-Lussac's law. It is a physical constant that is featured in many fundamental equations in the physical sciences, such as the ideal gas law and the Nernst equation.\n\nPhysically, the gas constant is the constant of proportionality that happens to relate the energy scale in physics to the temperature scale, when a mole of particles at the stated temperature is being considered. Thus, the value of the gas constant ultimately derives from historical decisions and accidents in the setting of the energy and temperature scales, plus similar historical setting of the value of the molar scale used for the counting of particles. The last factor is not a consideration in the value of the Boltzmann constant, which does a similar job of equating linear energy and temperature scales.\n\nThe gas constant value is\n\nThe two digits in parentheses are the uncertainty (standard deviation) in the last two digits of the value. The relative uncertainty is .\nSome have suggested that it might be appropriate to name the symbol \"R\" the Regnault constant in honour of the French chemist Henri Victor Regnault, whose accurate experimental data were used to calculate the early value of the constant; however, the exact reason for the original representation of the constant by the letter \"R\" is elusive.\n\nThe gas constant occurs in the ideal gas law, as follows:\nwhere \"P\" is the absolute pressure (SI unit pascals), \"V\" is the volume of gas (SI unit cubic metres), \"n\" is the amount of gas (SI unit moles), \"m\" is the mass (SI unit kilograms) contained in \"V\", and \"T\" is the thermodynamic temperature (SI unit kelvins). \"R\" is the molar-weight-specific gas constant, discussed below. The gas constant is expressed in the same physical units as molar entropy and molar heat capacity.\n\nFrom the general equation \"PV\" = \"nRT\" we get:\nwhere \"P\" is pressure, \"V\" is volume, \"n\" is number of moles of a given substance, and \"T\" is temperature.\n\nAs pressure is defined as force per unit area, the gas equation can also be written as:\n\nArea and volume are (length) and (length) respectively. Therefore:\n\nSince force × length = work:\n\nThe physical significance of \"R\" is work per degree per mole. It may be expressed in any set of units representing work or energy (such as joules), other units representing degrees of temperature (such as degrees Celsius or Fahrenheit), and any system of units designating a mole or a similar pure number that allows an equation of macroscopic mass and fundamental particle numbers in a system, such as an ideal gas (see Avogadro's number).\n\nInstead of a mole the constant can be expressed by considering the normal cubic meter.\n\nOtherwise, we can also say that:\n\nTherefore, we can write \"R\" as:\n\nAnd so, in SI base units:\n\nThe Boltzmann constant \"k\" (often abbreviated \"k\") may be used in place of the gas constant by working in pure particle count, \"N\", rather than amount of substance, \"n\", since\nwhere \"N\" is the Avogadro constant.\nFor example, the ideal gas law in terms of Boltzmann's constant is\nwhere \"N\" is the number of particles (molecules in this case), or to generalize to an inhomogeneous system the local form holds:\nwhere \"n\" is the number density.\n\nAs of 2006, the most precise measurement of \"R\" is obtained by measuring the speed of sound \"c\"(\"P\", \"T\") in argon at the temperature \"T\" of the triple point of water at different pressures \"P\", and extrapolating to the zero-pressure limit \"c\"(0, \"T\"). The value of \"R\" is then obtained from the relation\nwhere:\n\nThe specific gas constant of a gas or a mixture of gases (\"R\") is given by the molar gas constant divided by the molar mass (\"M\") of the gas or mixture.\n\nJust as the ideal gas constant can be related to the Boltzmann constant, so can the specific gas constant by dividing the Boltzmann constant by the molecular mass of the gas.\n\nAnother important relationship comes from thermodynamics. Mayer's relation relates the specific gas constant to the specific heats for a calorically perfect gas and a thermally perfect gas.\nwhere \"c\" is the specific heat for a constant pressure and \"c\" is the specific heat for a constant volume.\n\nIt is common, especially in engineering applications, to represent the specific gas constant by the symbol \"R\". In such cases, the universal gas constant is usually given a different symbol such as \"\" to distinguish it. In any case, the context and/or units of the gas constant should make it clear as to whether the universal or specific gas constant is being referred to.\n\nThe U.S. Standard Atmosphere, 1976 (USSA1976) defines the gas constant \"R\"* as:\n\nNote the use of kilomole units resulting in the factor of 1,000 in the constant. The USSA1976 acknowledges that this value is not consistent with the cited values for the Avogadro constant and the Boltzmann constant. This disparity is not a significant departure from accuracy, and USSA1976 uses this value of \"R\"* for all the calculations of the standard atmosphere. When using the ISO value of \"R\", the calculated pressure increases by only 0.62 pascal at 11 kilometers (the equivalent of a difference of only 17.4 centimeters or 6.8 inches) and an increase of 0.292 Pa at 20 km (the equivalent of a difference of only 0.338 m or 13.2 in).\n\n"}
{"id": "26790339", "url": "https://en.wikipedia.org/wiki?curid=26790339", "title": "Girl Geek Dinners", "text": "Girl Geek Dinners\n\nGirl Geek Dinners is an informal organisation that promotes women in the Information technology industry, with 64 established chapters in 23 countries. The organization was founded in London, United Kingdom by Sarah Lamb (\"née\" Blow), who realized how under-represented women are at information technology events after attending a Geek Dinner in 2005.\n\nChapters organise local events featuring both female and male speakers with mostly female attendees. As the name suggests, it's like a Geek Dinner but with one significant rule: men can only attend as invited guests of women, ensuring that women will never be outnumbered by men at events.\n\nA typical event is an informal dinner, followed by one or more presentations by featured speakers.\n\n\n"}
{"id": "142131", "url": "https://en.wikipedia.org/wiki?curid=142131", "title": "Hornbostel–Sachs", "text": "Hornbostel–Sachs\n\nHornbostel–Sachs or Sachs–Hornbostel is a system of musical instrument classification devised by Erich Moritz von Hornbostel and Curt Sachs, and first published in the \"Zeitschrift für Ethnologie\" in 1914. An English translation was published in the \"Galpin Society Journal\" in 1961. It is the most widely used system for classifying musical instruments by ethnomusicologists and organologists (people who study musical instruments). The system was updated in 2011 as part of the work of the Musical Instrument Museums Online (MIMO) Project.\n\nHornbostel and Sachs based their ideas on a system devised in the late 19th century by Victor-Charles Mahillon, the curator of musical instruments at Brussels Conservatory. Mahillon divided instruments into four broad categories according to the nature of the sound-producing material: an air column; string; membrane; and body of the instrument. From this basis, Hornbostel and Sachs expanded Mahillon's system to make it possible to classify any instrument from any culture.\n\nFormally, the Hornbostel–Sachs is modeled on the Dewey Decimal Classification for libraries. It has five top-level classifications, with several levels below those, adding up to over 300 basic categories in all. The top five levels of the scheme are as follows:\n\nIdiophones primarily produce their sounds by means of the actual body of the instrument vibrating, rather than a string, membrane, or column of air. In essence, this group includes all percussion instruments apart from drums, as well as some other instruments. In the Hornbostel–Sachs classification, idiophones are first categorized according to the method used to play the instrument. The result is four main categories: struck idiophones (11), plucked idiophones (12), friction idiophones (13), and blown idiophones (14). These groups are subsequently divided through various criteria. In many cases these sub-categories are split in singular specimens and sets of instruments. The latter category includes the xylophone, the marimba, the glockenspiel, and the glass harmonica.\n\nThese idiophones are set in vibration by being struck, for example cymbals or xylophones.\n\nThe player himself/herself executes the movement of striking; whether by mechanical intermediate devices, beaters, keyboards, or by pulling ropes, etc. It is definitive that the player can apply clear, exact, individual strokes, and that the instrument itself is equipped for this kind of percussion.\n\nThe player himself/herself does not go through the movement of striking; percussion results indirectly through some other movement by the player.\n\nPlucked idiophones, or lamellaphones, are idiophones set in vibration by being plucked; examples include the jaw harp or mbira. This group is sub-divided in the following two categories:\n\nThe lamellae vibrate within a frame or hoop.\n\nThe lamellae are tied to a board or cut out from a board like the teeth of a comb.\n\nIdiophones which are rubbed, for example the nail violin, a bowed instrument with solid pieces of metal or wood rather than strings.\n\n\n\n\nBlown idiophones are idiophones set in vibration by the movement of air, for example the Aeolsklavier, an instrument consisting of several pieces of wood which vibrate when air is blown onto them by a set of bellows. The piano chanteur features plaques.\n\n\n\nMembranophones primarily produce their sounds by means of the vibration of a tightly stretched membrane. This group includes all drums and kazoos. List of membranophones by Hornbostel–Sachs number\n\nStruck drums are instruments which have a struck membrane. This includes most types of drums, such as the timpani, or kettle drum, and the snare drum.\n\nInstruments in which the membrane is struck directly, such as through bare hands, beaters or keyboards.\n\nInstruments which are shaken, the membrane being vibrated by objects inside the drum (rattle drums).\n\nInstruments with a string attached to the membrane, so that when the string is plucked, the membrane vibrates (plucked drums).<br>\nSome commentators believe that instruments in this class ought instead to be regarded as chordophones (see below).\n\nInstruments in which the membrane vibrates as a result of friction. These are drums which are rubbed, rather than being struck.\n\nInstruments in which the membrane is vibrated from a stick that is rubbed or used to rub the membrane\n\nInstruments in which a cord, attached to the membrane, is rubbed.\n\nInstruments in which the membrane is rubbed by hand\n\nThis group includes kazoos, instruments which do not produce sound of their own, but modify other sounds by way of a vibrating membrane.\n\nInstruments in which the membrane is vibrated by an unbroken column of wind, without a chamber\n\nInstruments in which the membrane is placed in a box, tube or other container\n\nChordophones primarily produce their sounds by means of the vibration of a string or strings that are stretched between fixed points. This group includes all instruments generally called string instruments in the west, as well as many (but not all) keyboard instruments, such as pianos and harpsichords. List of chordophones by Hornbostel–Sachs number\n\nInstruments which are in essence simply a string or strings and a string bearer. These instruments may have a resonator box, but removing it should not render the instrument unplayable, though it may result in quite a different sound being produced. They include the piano therefore, as well as other kinds of zithers such as the koto, and musical bows.\n\nThe string bearer is bar-shaped.\n\nThe string bearer is a vaulted surface.\n\nThe string bearer is composed of canes tied together in the manner of a raft.\n\nThe string bearer is a board.\n\nThe strings are stretched across the mouth of a trough.\n\nThe strings are stretched across an open frame.\n\nAcoustic and electro-acoustic instruments which have a resonator as an integral part of the instrument, and solid-body electric chordophones. This includes most western string instruments, including lute-type instruments such as violins and guitars, and harps.\n\nThe plane of the strings runs parallel with the resonator's surface.\n\nThe plane of the strings lies perpendicular to the resonator's surface.\n\nThe plane of the strings lies at right angles to the sound-table; a line joining the lower ends of the strings would be perpendicular to the neck. These have notched bridges.\n\nAerophones primarily produce their sounds by means of vibrating air. The instrument itself does not vibrate, and there are no vibrating strings or membranes. List of aerophones by Hornbostel–Sachs number\n\nInstruments in which the vibrating air is not contained within the instrument, for example sirens, or the bullroarer.\n\nThe air-stream meets a sharp edge, or a sharp edge is moved through the air. In either case, according to more recent views, a periodic displacement of air occurs to the alternate flanks of the edge. Examples are the swordblade or the whip.\n\nThe air-stream is interrupted periodically.\n\nThe sound is caused by a single compression and release of air. Examples include the botija, the gharha, the ghatam, and the udu.\n\nThe vibrating air is contained within the instrument. This group includes most of the instruments called wind instruments in the west, such as the flute or French horn, as well as many other kinds of instruments such as conch shells.\n\nThe player makes a ribbon-shaped flow of air with his/her lips (421.1), or his/her breath is directed through a duct against an edge (421.2).\n\nThe player's breath is directed against a lamella or pair of lamellae which periodically interrupt the airflow and cause the air to be set in motion.\n\nThe player's vibrating lips set the air in motion.\n\n\nThe fifth top-level group, the electrophones category, was added by Sachs in 1940, to describe instruments involving electricity. Sachs broke down his 5th category into 3 subcategories: 51=electrically actuated acoustic instruments; 52=electrically amplified acoustic instruments; 53= instruments which make sound primarily by way of electrically driven oscillators, such as theremins or synthesizers, which he called radioelectric instruments. Francis William Galpin provided such a group in his own classification system, which is closer to Mahillon than Sachs–Hornbostel. For example, in Galpin's 1937 book \"A Textbook of European Musical Instruments,\" he lists electrophones with three second-level divisions for sound generation (\"by oscillation,\" \"electro-magnetic,\" and \"electro-static\"), as well as third-level and fourth-level categories based on the control method. Sachs himself proposed subcategories 51, 52, and 53, on pages 447–467 of his 1940 book \"The History of Musical Instruments.\" However, the original 1914 version of the system did not acknowledge the existence of his 5th category.\n\nPresent-day ethnomusicologists, such as Margaret Kartomi (page 173) and Ellingson (PhD dissertation, 1979, p. 544), suggest that, in keeping with the spirit of the original Hornbostel–Sachs classification scheme, of categorization by what first produces the initial sound in the instrument, that only subcategory 53 should remain in the electrophones category. Thus it has been more recently proposed that, for example, the pipe organ (even if it uses electric key action to control solenoid valves) remain in the aerophones category, and that the electric guitar remain in the chordophones category, etc..\n\nBeyond the top three groups are several further levels of classification, so that the xylophone, for example, is in the group labeled 111.212 (periods are usually added after every third digit to make long numbers easier to read). A long classification number does not necessarily indicate the instrument is a complicated one. The valveless bugle for instance, has the classification number 423.121.22, even though it is generally regarded as a relatively simple instrument (it is basically a bent conical tube which you blow down like a trumpet, but it does not have valves or finger-holes). The numbers in the bugle's classification indicate the following:\n\n423.121.22 does not uniquely identify the bugle, but rather identifies the bugle as a certain kind of instrument which has much in common with other instruments in the same class. Another instrument classified as 423.121.22 is the bronze lur, an instrument dating back to the Bronze Age.\n\nAfter the number described above, a number of suffixes may be appended. An 8 indicates that the instrument has a keyboard attached, while a 9 indicates the instrument is mechanically driven. In addition to these, there are a number of suffixes unique to each of the top-level groups indicating details not considered crucial to the fundamental nature of the instrument. In the membranophone class, for instance, suffixes can indicate whether the skin of a drum is glued, nailed or tied to its body; in the chordophone class, suffixes can indicate whether the strings are plucked with fingers or plectrum, or played with a bow.\n\nThere are ways to classify instruments with this system even if they have elements from more than one group. Such instruments may have particularly long classification numbers with colons and hyphens used as well as numbers. Hornbostel and Sachs themselves cite the case of various bagpipes where some of the pipes are single reed (like a clarinet) and others are double reed (like the oboe). A number of similar composite instruments exist.\n\n\n"}
{"id": "41452308", "url": "https://en.wikipedia.org/wiki?curid=41452308", "title": "I-TASSER", "text": "I-TASSER\n\nI-TASSER (Iterative Threading ASSEmbly Refinement) is a bioinformatics method for predicting three-dimensional structure model of protein molecules from amino acid sequences. It detects structure templates from the Protein Data Bank by a technique called fold recognition (or threading). The full-length structure models are constructed by reassembling structural fragments from threading templates using replica exchange Monte Carlo simulations. I-TASSER is one of the most successful protein structure prediction methods in the community-wide CASP experiments.\n\nI-TASSER has been extended for structure-based protein function predictions, which provides annotations on ligand binding site, gene ontology and enzyme commission by structurally matching structural models of the target protein to the known proteins in protein function databases. It has an on-line server built in the Yang Zhang Lab at the University of Michigan, Ann Arbor, allowing users to submit sequences and obtain structure and function predictions. A standalone package of I-TASSER is available for download at the I-TASSER website.\n\nI-TASSER (as 'Zhang-Server') has been consistently ranked as the top method in CASP, a community-wide experiment to benchmark the best structure prediction methods in the field of protein folding and protein structure prediction. CASP takes place every two years since 1994.\n\nI-TASSER is a template-based method for protein structure and function prediction. The pipeline consists of six consecutive steps:\n\nThe I-TASSER server allows users to generate automatically protein structure and function predictions.\n\nThe I-TASSER Suite is a downloadable package of standalone computer programs, developed by the Yang Zhang Lab for protein structure prediction and refinement, and structure-based protein function annotations. Through the I-TASSER License, researchers have access to the following standalone programs:\n\nHelp documents \n\n"}
{"id": "30679665", "url": "https://en.wikipedia.org/wiki?curid=30679665", "title": "Inherent bad faith model", "text": "Inherent bad faith model\n\nThe inherent bad faith model of information processing is a theory in political psychology that was first put forth by Ole Holsti to explain the relationship between John Foster Dulles' beliefs and his model of information processing. \n\nIt is the most widely studied model of one's opponent. A state is presumed to be implacably hostile, and contra-indicators of this are ignored. They are dismissed as propaganda ploys or signs of weakness. An example is John Foster Dulles' position regarding the Soviet Union.\n\n"}
{"id": "24952073", "url": "https://en.wikipedia.org/wiki?curid=24952073", "title": "International Society for Reef Studies", "text": "International Society for Reef Studies\n\nThe International Society for Reef Studies (ISRS) is an international, not-for profit, scientific society dedicated to the conservation of coral reefs through science and understanding. Founded in 1980, the primary objective of ISRS is the improvement of scientific knowledge and understanding of coral reefs, both living and fossil.\n\nTo achieve its objectives the ISRS prints and distributes the journal \"Coral Reefs\" as well as a Society newsletter, \"Reef Encounter\". The ISRS also holds annual meetings and co-sponsors other gatherings, symposia and conferences relating to coral reefs. \n\nISRS organizes the International Coral Reef Symposium (ICRS), which is held quadrennially. The last symposium was the Honolulu, Hawaii, in 2016.\n\nThe symposium has previously been held in Cairns, Queensland, Australia (2012); Fort Lauderdale, Florida, United States (2008), Okinawa, Japan (2004), Bali, Indonesia (2000), Panama City, Panama (1996), Guam (1992), Townsville, Queensland, Australia (1988), Tahiti, French Polynesia (1985), Manila, the Philippines (1980), Miami (1977), on board the M.V. \"Marco Polo\" in Australian waters (1974), and Mandapam Camp, India (1969, the first symposium). Published proceedings of the ICRS are available at \"ReefBase\".\n\nThe Darwin Medal, the most prestigious award given by the International Society for Reef Studies, is presented every four years at the International Coral Reef Symposium. It is awarded to a senior ISRS member who is recognized worldwide for major contributions throughout her or his career. The medal has been awarded seven times; recipients are David Stoddart, Peter Glynn, Ian Macintyre, Yossi Loya, Charlie Veron, Terry Hughes, and Jeremy Jackson.\n\n"}
{"id": "1136479", "url": "https://en.wikipedia.org/wiki?curid=1136479", "title": "Kinesics", "text": "Kinesics\n\nKinesics is the interpretation of body motion communication such as facial expressions and gestures, nonverbal behavior related to movement of any part of the body or the body as a whole. The equivalent popular culture term is body language, a term Ray Birdwhistell, considered the founder of this area of study, neither used nor liked (on the grounds that what can be conveyed with the body does not meet the linguist's definition of language).\n\nKinesics was first used in 1952 by an anthropologist named Ray Birdwhistell. Birdwhistell wished to study how people communicate through posture, gesture, stance and movement. His ideas over several decades were synthesized and resulted in the book \"Kinesics and Context.\" Interest in kinesics specifically and nonverbal behavior generally was popularized in the late 1960s and early 1970s by such popular mass market (nonacademic) publications as \"How to Read a Person Like a Book\". Part of Birdwhistell's work involved filming people in social situations and analyzing them to show elements of communication that were not clearly seen otherwise. One of his most important projects was \"The Natural History of an Interview,\" a long-term interdisciplinary collaboration including Gregory Bateson, Frieda Fromm-Reichmann, Norman A. McQuown, Henry W. Brosin and others.\n\nDrawing heavily on descriptive linguistics, Birdwhistell argued that all movements of the body have meaning and that nonverbal behavior has a grammar that can be analyzed in similar terms to spoken language. Thus, a \"kineme\" is \"similar to a phoneme because it consists of a group of movements which are not identical, but which may be used interchangeably without affecting social meaning.\"\n\nBirdwhistell estimated that no more than 30 to 35 percent of the social meaning of a conversation or an interaction is carried by the words. He also concluded that there were no universals in these kinesic displays, a claim that was disputed by Paul Ekman, who was interested in analysis of universals, especially in facial expression.\n\nIn a current application, kinesic behaviors are sometimes used as signs of deception by interviewers looking for clusters of movements to determine the veracity of the statement being uttered, although kinesics can be equally applied in any context and type of setting to construe innocuous messages whose carriers are indolent or unable to express verbally.\n\nRelevant concepts include:\n\nKinesic behaviors are an important part of nonverbal communication. Body movements convey information, but interpretations vary by culture. As many movements are carried out at a subconscious or at least a low-awareness level, kinesic movements carry a significant risk of being misinterpreted in an intercultural communication situation.\n\n\n"}
{"id": "6436951", "url": "https://en.wikipedia.org/wiki?curid=6436951", "title": "Latin American Public Opinion Project", "text": "Latin American Public Opinion Project\n\nThe Latin American Public Opinion Project (LAPOP) is a research institute specializing in the development, implementation, and analysis of public opinion surveys. Founded by Mitchell A. Seligson over two decades ago, its principal focus is on governance and democracy in Latin America. The AmericasBarometer is the best-known survey produced by LAPOP. It is the only survey of democratic public opinion and behavior that covers the Americas (North, Central, South, and the Caribbean). It measures democratic values and behaviors in the Americas using national probability samples of voting-age adults. Elizabeth Zechmeister is the director of LAPOP. Noam Lupu is associate director of LAPOP.\n\nLAPOP has its origins in studies of democratic values in Costa Rica. This pioneering public opinion research took place in the 1970s, a time in which much of the rest of Latin America was under the control of authoritarian regimes, prohibiting studies of public opinion. As democratization expanded in Latin America, LAPOP grew in scope and size. Today LAPOP regularly carries out public opinion surveys in nearly every country in Latin America, Canada, the United States, and much of Caribbean.\n\nLAPOP is housed at Vanderbilt University in Nashville, Tennessee. Vanderbilt is a research university that has been a leader in the study of Latin America and the Caribbean for over 60 years. At this host institution, a team of faculty, staff, post-doctoral researchers, graduate students, and undergraduate students designs and analyzes the public opinion surveys generated by the project. The group also edits and publishes the regular Insights Series reports, each one of which examines a one facet of public opinion. LAPOP’s network extends far beyond the Vanderbilt campus, to include partner institutions throughout the Americas.\n\nLAPOP functions as a consortium, working in partnership with numerous academic and non-governmental institutions in Latin America and the Caribbean. It collaborates with these institutions, sharing ideas for survey content and working together to disseminate the results of the public opinion surveys to the citizens of participating countries. This dissemination of results takes the form of systematic country reports, comparative studies, panel presentations, and media interviews.\n\nIn 2004, LAPOP established the AmericasBarometer as multi-country, regularly-conducted surveys of democratic values and behaviors in the Americas, organized by a consortium of academic and think-tank partners in the hemisphere. The first round included voting-age respondents from 11 countries. The second round of surveys took place in 2006 and included 22 countries from the hemisphere. The third round, 2008, included 24 countries in the Americas. The 2010 and 2012 round included 26 countries across North America, Latin America, and the Caribbean. The 2014/15 round consisted of over 50,000 interviews in 28 countries. The most recent round, 2016/17, comprises 29 national surveys and more than 43,000 interviews.\n\nCutting-edge methods and transparent practices ensure that data collected by LAPOP are of the highest quality. These methods and practices include the following:\n\n\n\n\nLAPOP’s resources and expertise allow it to conduct special projects requested by scholars, government institutions, and agencies concerned with democratic development. These have recently included novel experiments embedded within national surveys to assess issues of ethnicity and violence. In addition, these include an extensive new focus on randomized block experiments as a means of program evaluation.\n\nThe data are used by academic researchers; the United States Agency for International Development (USAID) in its efforts to promote democracy and good governance in Latin America and the Caribbean; the World Bank in its Governance Indicators series; the Inter-American Development Bank in its numerous research projects; the United Nations Development Programme and the Organization of American States in their democracy programs; and most significantly, by the governments of several Latin American countries as a source of independent information with which to assess public opinion and shape policy.\n\n"}
{"id": "25403526", "url": "https://en.wikipedia.org/wiki?curid=25403526", "title": "List of CBIR engines", "text": "List of CBIR engines\n\nThis is a list of publicly available Content-based image retrieval (CBIR) engines. These image search engines look at the content (pixels) of images in order to return results that match a particular query.\n"}
{"id": "23415304", "url": "https://en.wikipedia.org/wiki?curid=23415304", "title": "List of Indian satellites", "text": "List of Indian satellites\n\nIndia has been successfully launching satellites of many types since 1975. These satellites have been launched from various vehicles, including American, Russian and European rockets apart from Indian rockets. The organisation responsible for India's space program is Indian Space Research Organisation (ISRO) and it shoulders the bulk of the responsibility of designing, building, launching and operating these satellites.\n\nThis is a list of all Indian (wholly or partially owned, wholly or partially designed and/or manufactured) satellites, both operated by the Indian government (ISRO, Indian defence forces, other government agencies) or private (educational and research) entities. All satellite launches marked successful have completed at least one full orbital flight (no sub-orbital flights have been included in this list).\n\nIndian space missions began in the 1970s, with Soviet assistance in launching the first two satellites.\n\nIndia had three continuous successful satellite launches from its first generation rocket SLV. Although it had to meet failure with its second generation rocket, it avoided turning back irrespective of harsh circumstances, and kept developing heavy and powerful satellites.\n\nThis was the last decade when America and Russia enjoyed its involvement in launching Indian satellites. Henceforth, ISRO focused on building India's indigenous launch capabilities, leaving Europe with just launching its heavier satellites.\n\nISRO's workhorse, the PSLV, became the mainstay for successful launches of indigenous satellites from India during this decade. India successfully launched 11 geostationary or geosynchronous satellites during this period, which was equal to the total number of similar launches in the previous 2 decades put together. India's first inter-planetary mission was also successfully executed during this period.\n\nWhile India had to face failure in launching relatively heavier satellites early on in the decade, it did end up launching 10 geosynchronous/geostationary satellites (6 with indigenous, and 4 with European launchers) - in other words, in the first half of the current decade, it managed to launch almost as many geosynchronous/geostationary satellites as in the whole of the last decade. This period also saw India enter the exclusive club of nations capable of launching probes to Mars. ISRO also improved upon its student/university outreach by launching multiple pico-, nano- and mini-satellites from various Indian universities. This period was also marked by multiple bilateral collaborations with foreign universities and research organizations.\n\nThe following bar chart lists the number of satellites launched based on the origin of the launch vehicle\n\n\n"}
{"id": "34107136", "url": "https://en.wikipedia.org/wiki?curid=34107136", "title": "List of data-erasing software", "text": "List of data-erasing software\n\nThis is a list of utilities for performing data erasure.\n"}
{"id": "9731834", "url": "https://en.wikipedia.org/wiki?curid=9731834", "title": "List of genera of viruses", "text": "List of genera of viruses\n\nThis is an alphabetical list of genera of biological viruses. It includes all genera of virus listed by the International Committee on Taxonomy of Viruses (ICTV) 2018 report.\n\n\n"}
{"id": "28028899", "url": "https://en.wikipedia.org/wiki?curid=28028899", "title": "List of hoards in Great Britain", "text": "List of hoards in Great Britain\n\nThe list of hoards in Britain comprises significant archaeological hoards of coins, jewellery, precious and scrap metal objects and other valuable items discovered in Great Britain (England, Scotland and Wales). It includes both hoards that were buried with the intention of retrieval at a later date (personal hoards, founder's hoards, merchant's hoards, and hoards of loot), and also hoards of votive offerings which were not intended to be recovered at a later date, but excludes grave goods and single items found in isolation. The list is subdivided into sections according to archaeological and historical periods.\n\nHoards dating to Neolithic period, approximately 4000 to 2000 BC, comprise stone weapons and tools such as axeheads and arrowheads. Such hoards are very rare, and only a few are known from Britain.\n\nA large number of hoards associated with the British Bronze Age, approximately 2700 BC to 8th century BC, have been found in Great Britain. Most of these hoards comprise bronze tools and weapons such as axeheads, chisels, spearheads and knives, and in many cases may be founder's hoards buried with the intention of recovery at a later date for use in casting new bronze items. A smaller number of hoards include gold torcs and other items of jewellery. As coinage was not in use during the Bronze Age in Great Britain, there are no hoards of coins from this period.\n\nA large number of hoards associated with the British Iron Age, approximately 8th century BC to the 1st century AD, have been found in Britain. Most of the hoards comprise silver or gold Celtic coins known as staters, usually numbered in the tens or hundreds of coins, although the Hallaton Treasure contained over 5,000 silver and gold coins. In addition to hoards of coins, a number of hoards of gold torcs and other items of jewellery have been found, including the Snettisham Hoard and the Stirling Hoard.\n\nHoards associated with the period of Romano-British culture when part of Great Britain was under the control of the Roman Empire, from AD 43 until about 410, as well as the subsequent Sub-Roman period up to the establishment of Anglo-Saxon kingdoms are the most numerous type of hoard found in Great Britain, and Roman coin hoards are particularly well represented, with over 1,200 known examples. In addition to hoards composed largely or entirely of coins, a smaller number of hoards, such as the Mildenhall Treasure and the Hoxne Hoard, include items of silver or gold tableware such as dishes, bowls, jugs and spoons, or items of silver or gold jewellery.\n\nHoards associated with the Anglo-Saxon culture, from the 6th century to 1066, are relatively uncommon. Those that have been found include both hoards of coins and hoards of jewellery and metalwork such as sword hilts and crosses. The Staffordshire Hoard is the largest Anglo-Saxon hoard to have been found, comprising over 1,500 items of gold and silver. More Anglo-Saxon artefacts have been found in the context of grave burials than hoards in England. These include major finds from Sutton Hoo in Suffolk, Taplow in Buckinghamshire, Prittlewell, Mucking and Broomfield in Essex, and Crundale and Sarre in Kent.\n\nHoards associated with Pictish culture, dating from the end of Roman occupation in the 5th century until about the 10th century, have been found in eastern and northern Scotland. These hoards often contain silver brooches and other items of jewellery.\n\nHoards associated with the Viking culture in Great Britain, dating from the 9th to 11th centuries, are mostly found in northern England and Orkney, and frequently comprise a mixture of silver coins, silver jewellery and hacksilver that has been taken in loot, some coins originating from as far away as the Middle East.\n\nHoards dating to the later medieval period, from 1066 to about 1500, mostly comprise silver pennies, in some cases amounting to many thousands of coins, although the Fishpool Hoard contains over a thousand gold coins.\n\nMost hoards from the post-medieval period, later than 1500, date to the period of the English Civil War (1642–1651), from which time over 200 hoards are known.\n\n\n\n"}
{"id": "51848219", "url": "https://en.wikipedia.org/wiki?curid=51848219", "title": "List of international presidential trips made by George H. W. Bush", "text": "List of international presidential trips made by George H. W. Bush\n\nThis is a list of international presidential trips made by George H. W. Bush, the 41st president of the United States. George H. W. Bush made 26 international trips to 58 different countries on six continents—Africa, Asia, Australia, Europe, North America, and South America—during his presidency, which began on January 20, 1989 and ended on January 20, 1993.\n\nThe number of visits per country where he travelled are:\n\n"}
{"id": "2580333", "url": "https://en.wikipedia.org/wiki?curid=2580333", "title": "List of nerves of the human body", "text": "List of nerves of the human body\n\nThe following is a list of nerves in the human body:\n\n\n\n\n"}
{"id": "14686371", "url": "https://en.wikipedia.org/wiki?curid=14686371", "title": "List of oxidation states of the elements", "text": "List of oxidation states of the elements\n\nThis is a list of known oxidation states of the chemical elements within chemical compounds, excluding nonintegral values. The most common states appear in bold. The table is based on that of Greenwood and Earnshaw, with additions noted. Every element exists in oxidation state 0 when it is the pure non-ionized element in any phase, whether monatomic or polyatomic allotrope. The column for oxidation state 0 only shows elements know to exist in oxidation state 0 \"in compounds\". The format of the table, which was devised by Dmitri Mendeleev in 1889, shows the periodicity of the oxidation states of the elements.\n\nA figure with a similar format (shown below) was used by Irving Langmuir in 1919 in one of the early papers about the octet rule. The periodicity of the oxidation states was one of the pieces of evidence that led Langmuir to adopt the rule.\n"}
{"id": "42517482", "url": "https://en.wikipedia.org/wiki?curid=42517482", "title": "List of things named after Peter Debye", "text": "List of things named after Peter Debye\n\nThe article is a list of things named after the Dutchman P. J. W. Debye.\n\n\n\n\n"}
{"id": "24812601", "url": "https://en.wikipedia.org/wiki?curid=24812601", "title": "Magnetic braking", "text": "Magnetic braking\n\nMagnetic braking is a theory explaining the loss of stellar angular momentum due to material getting captured by the stellar magnetic field and thrown out at great distance from the surface of the star. It plays an important role in the evolution of binary star systems.\n\nThe currently accepted theory of the solar system's evolution states that the Solar System originates from a contracting gas cloud. As the cloud contracts, the angular momentum formula_1 must be conserved. Any small net rotation of the cloud will cause the spin to increase as the cloud collapses, forcing the material into a rotating disk. At the dense center of this disk a protostar forms, which gains heat from the gravitational energy of the collapse. As the collapse continues, the rotation rate can increase to the point where the accreting protostar can break up due to centrifugal force at the equator.\n\nThus the rotation rate must be braked during the first 100,000 years of the star's life to avoid this scenario. One possible explanation for the braking is the interaction of the protostar's magnetic field with the stellar wind. In the case of our own Sun, when the planets' angular momenta are compared to the Sun's own, the Sun has less than 1% of its supposed angular momentum. In other words, the Sun has slowed down its spin while the planets have not.\n\nIonized material captured by the magnetic field lines will rotate with the Sun as if it were a solid body. As material escapes from the Sun due to the solar wind, the highly ionized material will be captured by the field lines and rotate with the same angular velocity as the Sun, even though it is carried far away from the Sun's surface, until it eventually escapes. This effect of carrying mass far from the centre of the Sun and throwing it away slows down the spin of the Sun. The same effect is used in slowing the spin of a rotating satellite; here two wires spool out weights to a distance slowing the satellites spin, then the wires are cut, letting the weights escape into space and permanently robbing the spacecraft of its angular momentum.\n\nAs ionized material follows the Sun's magnetic field lines, due to the effect of the field lines being frozen in the plasma, the charged particles feel a force formula_2 of the magnitude:\nwhere formula_4 is the charge, formula_5 is the velocity and formula_6 is the magnetic field vector. This bending action forces the particles to \"corkscrew\" around the magnetic field lines while held in place by a \"magnetic pressure\"formula_7, or \"energy density\", while rotating together with the Sun as a solid body:\n\nSince magnetic field strength decreases with the cube of the distance there will be a place where the kinetic gas pressure formula_9 of the ionized gas is great enough to break away from the field lines:\n\nwhere n is the number of particles, m is the mass of the individual particle and v is the radial velocity away from the Sun, or the speed of the solar wind.\n\nThe magnetic field strength as a function of radius can be approximated as:\n\nThe critical distance where the material will break away from the field lines can then be calculated as the distance where the kinetic pressure and the magnetic pressure are equal, i.e.\n\nIf the solar mass loss is omni-directional then the mass loss formula_15; plugging this into the above equation and isolating the critical radius it follows that\n\nCurrently it is estimated that:\n\nThis leads to a critical radius formula_20. This means that the ionized plasma will rotate together with the Sun as a solid body until it reaches a distance of nearly 3.4 the radius of the Sun; from there the material will break off and stop affecting the Sun.\n\nThe amount of solar mass needed to be thrown out along the field lines to make the Sun completely stop rotating can then be calculated using the specific angular momentum:\nAbout 1/10 of the solar mass should thus have been lost by now due to this effect, but if the present mass loss rate was stable throughout the Sun's \"life\" only about 0.01% of the Sun's original mass would have been lost. This indicates that either the Sun's mass loss must have been higher in the past or that the Sun's magnetic field has been much stronger.\n\n"}
{"id": "665738", "url": "https://en.wikipedia.org/wiki?curid=665738", "title": "Magnetic shape-memory alloy", "text": "Magnetic shape-memory alloy\n\nMagnetic shape memory alloys (MSMAs), also called ferromagnetic shape memory alloys (FSMA), are particular shape memory alloys which produce forces and deformations in response to a magnetic field. The thermal shape memory effect has been obtained in these materials, too.\n\nMSM alloys are ferromagnetic materials that can produce motion and forces under moderate magnetic fields. Typically, MSMAs are alloys of Nickel, Manganese and Gallium (Ni-Mn-Ga).\n\nA \"magnetically induced deformation\" of about 0.2% was presented in 1996 by Dr. Kari Ullakko and co-workers at MIT. Since then, improvements on the production process and on the subsequent treatment of the alloys have led to deformations of up to 6% for commercially-available \"single crystalline\" Ni-Mn-Ga MSM elements, as well as up to 10-12 % and 20% for new alloys in R&D stage.\n\nThe large magnetically induced strain, as well as the short response times make the MSM technology very attractive for the design of innovative actuators to be used in pneumatics, robotics, medical devices and mechatronics. MSM alloys change their magnetic properties depending on the deformation. This companion effect, which co-exist with the actuation, can be useful for the design of displacement, speed or force sensors and mechanical energy harvesters.\n\nThe magnetic shape memory effect occurs in the low temperature martensite phase of the alloy, where the elementary cells composing the alloy have tetragonal geometry. If the temperature is increased beyond the martensite–austenite transformation temperature, the alloy goes to the austenite phase where the elementary cells have cubic geometry. With such geometry the magnetic shape memory effect is lost.\n\nThe transition from martensite to austenite produces force and deformation. Therefore, MSM alloys can be also activated thermally, like thermal shape memory alloys (see, for instance, Nickel-Titanium (Ni-Ti) alloys).\n\nThe mechanism responsible for the large strain of MSM alloys is the so-called \"magnetically induced reorientation\" (MIR), and is sketched in the figure. Like other ferromagnetic materials, MSM alloys exhibit a macroscopic magnetization when subjected to an external magnetic field, emerging from the alignment of elementary magnetizations along the field direction. However, differently from standard ferromagnetic materials, the alignment is obtained by the geometric rotation of the elementary cells composing the alloy, and not by rotation of the magnetization vectors within the cells (like in magnetostriction).\nA similar phenomenon occurs when the alloy is subjected to an external force. Macroscopically, the force acts like the magnetic field, favoring the rotation of the elementary cells and achieving elongation or contraction depending on its application within the reference coordinate system. The elongation and contraction processes are shown in the figure where, for example, the elongation is achieved magnetically and the contraction mechanically.\n\nThe rotation of the cells is a consequence of the large magnetic anisotropy of MSM alloys, and the high mobility of the internal regions. Simply speaking, an MSM element is composed by internal regions, each having a different orientation of the elementary cells (the regions are shown by the figure in green and blue colors). These regions are called twin-variants. The application of a magnetic field or of an external stress shifts the boundaries of the variants, called \"twin boundaries\", and thus favors one variant or the other. When the element is completely contracted or completely elongated, it is formed by only one variant and it is said to be in a \"single variant state\". The magnetization of the MSM element along a fixed direction differs if the element is in the contraction or in the elongation single variant state. The magnetic anisotropy is the difference between the energy required to magnetize the element in contraction single variant state and in elongation single variant state. The value of the anisotropy is related to the maximum work-output of the MSM alloy, and thus to the available strain and force that can be used for applications.\n\nThe main properties of the MSM effect for commercially available elements are summarized in (where other aspects of the technology and of the related applications are described):\n\nStandard alloys are Nickel-Manganese-Gallium (Ni-Mn-Ga) alloys, which are investigated since the first relevant MSM effect has been published in 1996. Other alloys under investigation are Iron-Palladium (Fe-Pd) alloys, Nickel-Iron-Gallium (Ni-Fe-Ga) alloys, and several derivates of the basic Ni-Mn-Ga alloy which further contain Iron (Fe), Cobalt (Co) or Copper (Cu). The main motivation behind the continuous development and testing of new alloys is to achieve improved thermo-magneto-mechanical properties, such as a lower internal friction, a higher transformation temperature and a higher Curie temperature, which would allow the use of MSM alloys in several applications. In fact, the actual temperature range of standard alloys is up to 50 °C. Recently, a 80 °C alloy has been presented.\n\nMSM actuator elements can be used where fast and precise motion is required. Possible application fields are robotics, manufacturing, medical surgery, valves, dampers, sorting.\n"}
{"id": "1722750", "url": "https://en.wikipedia.org/wiki?curid=1722750", "title": "Marine ecoregions", "text": "Marine ecoregions\n\nMarine ecoregions are ecoregions (ecological regions) of the oceans and seas identified and defined based on biogeographic characteristics.\n\nA more complete definition describes them as “Areas of relatively homogeneous species composition, clearly distinct from adjacent systems” dominated by “a small number of ecosystems and/or a distinct suite of oceanographic or topographic features”. Ecologically they “are strongly cohesive units, sufficiently large to encompass ecological or life history processes for most sedentary species.”\n\nThe global classification system Marine Ecoregions of the World—MEOW was devised by an international team, including major conservation organizations, academic institutions and intergovernmental organizations. The digital ecoregions layer is available for download as an ArcGIS Shapefile.\n\nThis system has a strong biogeographic basis, but was designed to aid in conservation activities for marine ecosystems. \n\nThe Marine Ecoregions of the World classification defines 232 marine ecoregions (e.g. Adriatic Sea, Cortezian, Ningaloo, Ross Sea) for the coastal and shelf waters of the world. \n\nThese marine ecoregions form part of a nested system and are grouped into 62 provinces (e.g. the South China Sea, Mediterranean Sea, Central Indian Ocean Islands).\n\nThe provinces in turn, are grouped into 12 major realms. The latter are considered analogous to the eight terrestrial ecozones, represent large regions of the ocean basins: \n\nOther classifications of marine ecoregions or equivalent areas have been widely developed at national and regional levels, as well as a small number of global schemes. \n\nEach of these systems, along with numerous regional biogeographic classifications, was used to inform the MEOW system. The WWF Global 200 work also identifies a number of major habitat types that correspond to the terrestrial biomes: polar, temperate shelves and seas, temperate upwelling, tropical upwelling, tropical coral, pelagic (trades and westerlies), abyssal, and hadal (ocean trench).\n\nOne of the most comprehensive early classifications was the system of 53 coastal provinces developed by Briggs in 1974. The near-global system of 64 large marine ecosystems has a partial biogeographic basis. \n\nThe World Wildlife Fund—WWF identified 43 priority marine ecoregions, as part of its Global 200 efforts.\n\n"}
{"id": "10464507", "url": "https://en.wikipedia.org/wiki?curid=10464507", "title": "Monoidal t-norm logic", "text": "Monoidal t-norm logic\n\nMonoidal t-norm based logic (or shortly MTL), the logic of left-continuous t-norms, is one of t-norm fuzzy logics. It belongs to the broader class of substructural logics, or logics of residuated lattices; it extends the logic of commutative bounded integral residuated lattices (known as Höhle's monoidal logic, Ono's FL, or intuitionistic logic without contraction) by the axiom of prelinearity.\n\nT-norms are binary functions on the real unit interval [0, 1] which are often used to represent a conjunction connective in fuzzy logic. Every \"left-continuous\" t-norm formula_1 has a unique residuum, that is, a function formula_2 such that for all \"x\", \"y\", and \"z\",\nThe residuum of a left-continuous t-norm can explicitly be defined as\nThis ensures that the residuum is the largest function such that for all \"x\" and \"y\",\nThe latter can be interpreted as a fuzzy version of the modus ponens rule of inference. The residuum of a left-continuous t-norm thus can be characterized as the weakest function that makes the fuzzy modus ponens valid, which makes it a suitable truth function for implication in fuzzy logic. Left-continuity of the t-norm is the necessary and sufficient condition for this relationship between a t-norm conjunction and its residual implication to hold.\n\nTruth functions of further propositional connectives can be defined by means of the t-norm and its residuum, for instance the residual negation formula_7 In this way, the left-continuous t-norm, its residuum, and the truth functions of additional propositional connectives (see the section \"Standard semantics\" below) determine the truth values of complex propositional formulae in [0, 1]. Formulae that always evaluate to 1 are then called \"tautologies\" with respect to the given left-continuous t-norm formula_8 or \"formula_9tautologies.\" The set of all formula_9tautologies is called the \"logic\" of the t-norm formula_8 since these formulae represent the laws of fuzzy logic (determined by the t-norm) which hold (to degree 1) regardless of the truth degrees of atomic formulae. Some formulae are tautologies with respect to \"all\" left-continuous t-norms: they represent general laws of propositional fuzzy logic which are independent of the choice of a particular left-continuous t-norm. These formulae form the logic MTL, which can thus be characterized as the \"logic of left-continuous t-norms.\"\n\nThe language of the propositional logic MTL consists of countably many propositional variables and the following primitive logical connectives:\nThe following are the most common defined logical connectives:\n\nWell-formed formulae of MTL are defined as usual in propositional logics. In order to save parentheses, it is common to use the following order of precedence:\n\nA Hilbert-style deduction system for MTL has been introduced by Esteva and Godo (2001). Its single derivation rule is modus ponens:\nThe following are its axiom schemata:\n\nThe traditional numbering of axioms, given in the left column, is derived from the numbering of axioms of Hájek's basic fuzzy logic BL. The axioms (MTL4a)–(MTL4c) replace the axiom of \"divisibility\" (BL4) of BL. The axioms (MTL5a) and (MTL5b) express the law of residuation and the axiom (MTL6) corresponds to the condition of prelinearity. The axioms (MTL2) and (MTL3) of the original axiomatic system were shown to be redundant (Chvalovský, 2012) and (Cintula, 2005). All the other axioms were shown to be independent (Chvalovský, 2012).\n\nLike in other propositional t-norm fuzzy logics, algebraic semantics is predominantly used for MTL, with three main classes of algebras with respect to which the logic is complete:\n\nAlgebras for which the logic MTL is sound are called \"MTL-algebras.\" They can be characterized as \"prelinear commutative bounded integral residuated lattices.\" In more detail, an algebraic structure formula_34 is an MTL-algebra if\n\nImportant examples of MTL algebras are \"standard\" MTL-algebras on the real unit interval [0, 1]. Further examples include all Boolean algebras, all linear Heyting algebras (both with formula_45), all MV-algebras, all BL-algebras, etc. Since the residuation condition can equivalently be expressed by identities, MTL-algebras form a variety.\n\nThe connectives of MTL are interpreted in MTL-algebras as follows:\n\nWith this interpretation of connectives, any evaluation \"e\" of propositional variables in \"L\" uniquely extends to an evaluation \"e\" of all well-formed formulae of MTL, by the following inductive definition (which generalizes Tarski's truth conditions), for any formulae \"A\", \"B\", and any propositional variable \"p\":\n\nInformally, the truth value 1 represents full truth and the truth value 0 represents full falsity; intermediate truth values represent intermediate degrees of truth. Thus a formula is considered fully true under an evaluation \"e\" if \"e\"(\"A\") = 1. A formula \"A\" is said to be \"valid\" in an MTL-algebra \"L\" if it is fully true under all evaluations in \"L\", that is, if \"e\"(\"A\") = 1 for all evaluations \"e\" in \"L\". Some formulae (for instance, \"p\" → \"p\") are valid in any MTL-algebra; these are called \"tautologies\" of MTL.\n\nThe notion of global entailment (or: global consequence) is defined for MTL as follows: a set of formulae Γ entails a formula \"A\" (or: \"A\" is a global consequence of Γ), in symbols formula_58 if for any evaluation \"e\" in any MTL-algebra, whenever \"e\"(\"B\") = 1 for all formulae \"B\" in Γ, then also \"e\"(\"A\") = 1. Informally, the global consequence relation represents the transmission of full truth in any MTL-algebra of truth values.\n\nThe logic MTL is sound and complete with respect to the class of all MTL-algebras (Esteva & Godo, 2001):\nThe notion of MTL-algebra is in fact so defined that MTL-algebras form the class of \"all\" algebras for which the logic MTL is sound. Furthermore, the \"strong completeness theorem\" holds:\n\nLike algebras for other fuzzy logics, MTL-algebras enjoy the following \"linear subdirect decomposition property\":\n\nIn consequence of the linear subdirect decomposition property of all MTL-algebras, the \"completeness theorem with respect to linear MTL-algebras\" (Esteva & Godo, 2001) holds:\n\n\"Standard\" are called those MTL-algebras whose lattice reduct is the real unit interval [0, 1]. They are uniquely determined by the real-valued function that interprets strong conjunction, which can be any left-continuous t-norm formula_37. The standard MTL-algebra determined by a left-continuous t-norm formula_37 is usually denoted by formula_61 In formula_62 implication is represented by the residuum of formula_63 weak conjunction and disjunction respectively by the minimum and maximum, and the truth constants zero and one respectively by the real numbers 0 and 1.\n\nThe logic MTL is complete with respect to standard MTL-algebras; this fact is expressed by the \"standard completeness theorem\" (Jenei & Montagna, 2002):\n\nSince MTL is complete with respect to standard MTL-algebras, which are determined by left-continuous t-norms, MTL is often referred to as the \"logic of left-continuous t-norms\" (similarly as BL is the logic of continuous t-norms).\n\n"}
{"id": "7415870", "url": "https://en.wikipedia.org/wiki?curid=7415870", "title": "Motion analysis", "text": "Motion analysis\n\nMotion analysis is used in computer vision, image processing, high-speed photography and machine vision that studies methods and applications in which two or more consecutive images from an image sequences, e.g., produced by a video camera or high-speed camera, are processed to produce information based on the apparent motion in the images. In some applications, the camera is fixed relative to the scene and objects are moving around in the scene, in some applications the scene is more or less fixed and the camera is moving, and in some cases both the camera and the scene are moving.\n\nThe motion analysis processing can in the simplest case be to detect motion, i.e., find the points in the image where something is moving. More complex types of processing can be to track a specific object in the image over time, to group points that belong to the same rigid object that is moving in the scene, or to determine the magnitude and direction of the motion of every point in the image. The information that is produced is often related to a specific image in the sequence, corresponding to a specific time-point, but then depends also on the neighboring images. This means that motion analysis can produce time-dependent information about motion.\n\nApplications of motion analysis can be found in rather diverse areas, such as surveillance, medicine, film industry, automotive crash safety, ballistic firearm studies, biological science, flame propagation, and navigation of autonomous vehicles to name a few examples.\n\nA video camera can be seen as an approximation of a pinhole camera, which means that each point in the image is illuminated by some (normally one) point in the scene in front of the camera, usually by means of light that the scene point reflects from a light source. Each visible point in the scene is projected along a straight line that passes through the camera aperture and intersects the image plane. This means that at a specific point in time, each point in the image refers to a specific point in the scene. This scene point has a position relative to the camera, and if this relative position changes, it corresponds to a \"relative motion in 3D\". It is a relative motion since it does not matter if it is the scene point, or the camera, or both, that are moving. It is only when there is a change in the relative position that the camera is able to detect that some motion has happened. By projecting the relative 3D motion of all visible points back into the image, the result is the \"motion field\", describing the apparent motion of each image point in terms of a magnitude and direction of velocity of that point in the image plane. A consequence of this observation is that if the relative 3D motion of some scene points are along their projection lines, the corresponding apparent motion is zero.\n\nThe camera measures the intensity of light at each image point, a light field. In practice, a digital camera measures this light field at discrete points, pixels, but given that the pixels are sufficiently dense, the pixel intensities can be used to represent most characteristics of the light field that falls onto the image plane. A common assumption of motion analysis is that the light reflected from the scene points does not vary over time. As a consequence, if an intensity \"I\" has been observed at some point in the image, at some point in time, the same intensity \"I\" will be observed at a position that is displaced relative to the first one as a consequence of the apparent motion. Another common assumption is that there is a fair amount of variation in the detected intensity over the pixels in an image. A consequence of this assumption is that if the scene point that corresponds to a certain pixel in the image has a relative 3D motion, then the pixel intensity is likely to change over time.\n\nOne of the simplest type of motion analysis is to detect image points that refer to moving points in the scene. The typical result of this processing is a binary image where all image points (pixels) that relate to moving points in the scene are set to 1 and all other points are set to 0. This binary image is then further processed, e.g., to remove noise, group neighboring pixels, and label objects. Motion detection can be done using several methods; the two main groups are differential methods and methods based on background segmentation.\n\nIn the areas of medicine, sports, video surveillance, physical therapy, and kinesiology, human motion analysis has become an investigative and diagnostic tool. See the section on motion capture for more detail on the technologies. Human motion analysis can be divided into three categories: human activity recognition, human motion tracking, and analysis of body and body part movement.\n\nHuman activity recognition is most commonly used for video surveillance, specifically automatic motion monitoring for security purposes. Most efforts in this area rely on state-space approaches, in which sequences of static postures are statistically analyzed and compared to modeled movements. Template-matching is an alternative method whereby static shape patterns are compared to pre-existing prototypes.\nHuman motion tracking can be performed in two or three dimensions. Depending on the complexity of analysis, representations of the human body range from basic stick figures to volumetric models. Tracking relies on the correspondence of image features between consecutive frames of video, taking into consideration information such as position, color, shape, and texture. Edge detection can be performed by comparing the color and/or contrast of adjacent pixels, looking specifically for discontinuities or rapid changes. Three-dimensional tracking is fundamentally identical to two-dimensional tracking, with the added factor of spatial calibration.\n\nMotion analysis of body parts is critical in the medical field. In postural and gait analysis, joint angles are used to track the location and orientation of body parts. Gait analysis is also used in sports to optimize athletic performance or to identify motions that may cause injury or strain. Tracking software that does not require the use of optical markers is especially important in these fields, where the use of markers may impede natural movement. \n\nMotion analysis is also applicable in the manufacturing process. Using high speed video cameras and motion analysis software, one can monitor and analyze assembly lines and production machines to detect inefficiencies or malfunctions. Manufacturers of sports equipment, such as baseball bats and hockey sticks, also use high speed video analysis to study the impact of projectiles. An experimental setup for this type of study typically uses a triggering device, external sensors (e.g., accelerometers, strain gauges), data acquisition modules, a high-speed camera, and a computer for storing the synchronized video and data. Motion analysis software calculates parameters such as distance, velocity, acceleration, and deformation angles as functions of time. This data is then used to design equipment for optimal performance.\n\nThe object and feature detecting capabilities of motion analysis software can be applied to count and track particles, such as bacteria, viruses, \"ionic polymer-metal composites\", micron-sized polystyrene beads, aphids, and projectiles.\n\n"}
{"id": "46607376", "url": "https://en.wikipedia.org/wiki?curid=46607376", "title": "Organizational-Activational Hypothesis", "text": "Organizational-Activational Hypothesis\n\nThe Organizational-Activational Hypothesis states that steroid hormones permanently organize the nervous system during early development, which is reflected in adult male or female typical behaviors. In adulthood, the same steroid hormones activate, modulate, and inhibit these behaviors. This idea was revolutionary when first published in 1959 because no other previous experiment had demonstrated that adult behaviors could be determined hormonally during early development. \n\nThe Phoenix et al. study sought to discover whether gonadal hormones given during the prenatal period had organizing effects on guinea pigs’ reproductive behavior It was found that when female controls, gonadectomized (removal of gonads) females, hermaphrodites, and castrated males were injected prenatally with testosterone proprionate, the mean number of mounts increased. This increase in male-typical reproductive behavior shows that prenatal androgens have a masculinizing effect. Moreover, the organizing effects of hormones can have permanent effects. Phoenix et al. found that females injected with testosterone propionate while pregnant, instead of neonatally, did not have any effect on lordosis. This demonstrates that when testosterone is given postnatally in females, there may not be lasting effects as compared to prenatally administered testosterone. The data from this study supports the organizational hypothesis that states when androgens are given prenatally there is an organizing effect on sexual behavior, permanently altering normal female mating behavior as adults. \n\nSexual behavior in rats is organized prenatally and activated with steroids hormones in adulthood. In males high levels of testosterone produced by testes and travel to the brain. Here, testosterone is aromatized to an estradiol and masculinizes and defeminizes the brain. Thus, estradiol is responsible for many male-typical behavior. In females, the ovaries produce large amounts of estrogen during gestation. Rats have alpha-fetoprotein that binds to the estrogen before it can reach the brain. The estrogen is eventually metabolized in the liver. This protein has a low affinity for androgens. Therefore, testosterone can reach the brain without being taken up by alpha-fetoprotein. Due to fact that males have different levels of androgens in the brain, this can lead to organizing effects from androgen exposure with the expression of masculine behaviour.\n\nThe organizational-activational theory has three main components.\n\n\nThere is evidence that organizational effects are not always permanent. In canaries, only the males produce song. Brain regions associated with bird song, including the hyperstriatum ventrale pars caudale (HVc) and nucleus robustus archistriatalis (RA), are larger in male canaries. However, the HVc and RA grow significantly larger in gonadectomized female canaries if given testosterone at 11 months of age and these females even begin to sing. These two brain regions change morphologically due to testosterone, indicating that neural tissue can be changed due to steroid hormones despite being organized to react to sex hormones in a female- or male-specific way.\n\nFinger ratio has been examined in relation to a number of physical traits that show sex differences and evidence suggests it is influenced by the prenatal environment, although there is no direct evidence for the latter. Studies in men have been motivated by two conflicting hypotheses. On the one hand, homosexual men were hypothesized to be exposed to high levels of testosterone in utero, which would be associated with a lower 2D:4D ratio than that found in heterosexual men. On the other hand, homosexual men have been hypothesized to have low prenatal testosterone exposure, and data from two studies are consistent with that hypothesis, showing homosexual men to have a higher finger ratio (on both hands) than heterosexual men. \nThe hormonal control of ovulation is also related to the organizational/activational hypothesis. Both males and females rats exhibit luteinizing hormone (LH) pulses in which LH is released from the anterior pituitary due to the secretion of gonadotropin releasing hormone (GnRH) from the hypothalamus. Females, however, show an increase in LH pulse frequency around ovulation due to the positive feedback mechanism. When estrogen is increased in the blood, the anteroventral periventricular nucleus (AVPV) of the hypothalamus causes the release of GnRH. The GnRH surge brings about a surge in LH and follicle stimulating hormone (FSH). Since females have a cyclic gonadal function, there may be a sexual dimorphism in the gonadal secretion. When female rats are injected with testosterone there is no positive feedback occurring and no LH surge. Moreover, castrated males will exhibit LH surges, similar to female cyclic gonadal behavior. \n\n"}
{"id": "46656", "url": "https://en.wikipedia.org/wiki?curid=46656", "title": "Radio telescope", "text": "Radio telescope\n\nA radio telescope is a specialized antenna and radio receiver used to receive radio waves from astronomical radio sources in the sky in radio astronomy. Radio telescopes are the main observing instrument used in radio astronomy, which studies the radio frequency portion of the electromagnetic spectrum emitted by astronomical objects, just as optical telescopes are the main observing instrument used in traditional optical astronomy which studies the light wave portion of the spectrum coming from astronomical objects. Radio telescopes are typically large parabolic (\"dish\") antennas similar to those employed in tracking and communicating with satellites and space probes. They may be used singly or linked together electronically in an array. Unlike optical telescopes, radio telescopes can be used in the daytime as well as at night. Since astronomical radio sources such as planets, stars, nebulas and galaxies are very far away, the radio waves coming from them are extremely weak, so radio telescopes require very large antennas to collect enough radio energy to study them, and extremely sensitive receiving equipment. Radio observatories are preferentially located far from major centers of population to avoid electromagnetic interference (EMI) from radio, television, radar, motor vehicles, and other manmade electronic devices.\n\nRadio waves from space were first detected by engineer Karl Guthe Jansky in 1932 at Bell Telephone Laboratories in Holmdel, New Jersey using an antenna built to study noise in radio receivers. The first purpose-built radio telescope was a 9-meter parabolic dish constructed by radio amateur Grote Reber in his back yard in Wheaton, Illinois in 1937. The sky survey he did with it is often considered the beginning of the field of radio astronomy.\n\nThe first radio antenna used to identify an astronomical radio source was one built by Karl Guthe Jansky, an engineer with Bell Telephone Laboratories, in 1932. Jansky was assigned the job of identifying sources of static that might interfere with radio telephone service. Jansky's antenna was an array of dipoles and reflectors designed to receive short wave radio signals at a frequency of 20.5 MHz (wavelength about 14.6 meters). It was mounted on a turntable that allowed it to rotate in any direction, earning it the name \"Jansky's merry-go-round\". It had a diameter of approximately and stood tall. By rotating the antenna, the direction of the received interfering radio source (static) could be pinpointed. A small shed to the side of the antenna housed an analog pen-and-paper recording system. After recording signals from all directions for several months, Jansky eventually categorized them into three types of static: nearby thunderstorms, distant thunderstorms, and a faint steady hiss of unknown origin. Jansky finally determined that the \"faint hiss\" repeated on a cycle of 23 hours and 56 minutes. This period is the length of an astronomical sidereal day, the time it takes any \"fixed\" object located on the celestial sphere to come back to the same location in the sky. Thus Jansky suspected that the hiss originated outside of the Solar System, and by comparing his observations with optical astronomical maps, Jansky concluded that the radiation was coming from the Milky Way Galaxy and was strongest in the direction of the center of the galaxy, in the constellation of Sagittarius.\n\nAn amateur radio operator, Grote Reber, was one of the pioneers of what became known as radio astronomy. He built the first parabolic \"dish\" radio telescope, in diameter, in his back yard in Wheaton, Illinois in 1937. He repeated Jansky's pioneering work, identifying the Milky Way as the first off-world radio source, and he went on to conduct the first sky survey at very high radio frequencies, discovering other radio sources. The rapid development of radar during World War II created technology which was applied to radio astronomy after the war, and radio astronomy became a branch of astronomy, with universities and research institutes constructing large radio telescopes.\n\nThe range of frequencies in the electromagnetic spectrum that makes up the radio spectrum is very large. This means that the types of antennas that are used as radio telescopes vary widely in design, size, and configuration. At wavelengths of 30 meters to 3 meters (10 MHz - 100 MHz), they are generally either directional antenna arrays similar to \"TV antennas\" or large stationary reflectors with moveable focal points. Since the wavelengths being observed with these types of antennas are so long, the \"reflector\" surfaces can be constructed from coarse wire mesh such as chicken wire.\nThe increasing use of radio frequencies for communication makes astronomical observations more and more difficult (see Open spectrum).\nNegotiations to defend the frequency allocation for parts of the spectrum most useful for observing the universe are coordinated in the Scientific Committee on Frequency Allocations for Radio Astronomy and Space Science.\n\nSome of the more notable frequency bands used by radio telescopes include:\n\nThe world's largest filled-aperture (i.e. full dish) radio telescope is the Five hundred meter Aperture Spherical Telescope (FAST) completed in 2016 by China. The dish with an area as large as 30 football fields is built into a natural Karst depression in the landscape in Guizhou province and cannot move; the feed antenna is in a cabin suspended above the dish on cables. The active dish is composed of 4450 moveable panels controlled by a computer. By changing the shape of the dish and moving the feed cabin on its cables, the telescope can be steered to point to any region of the sky up to 40° from the zenith. Although the dish is 500 meters in diameter, only a 300-meter circular area on the dish is illuminated by the feed antenna at any given time, so the actual effective aperture is 300 meters. Construction was begun in 2007 and completed July 2016 and the telescope became operational September 25, 2016.\n\nThe world's second largest filled-aperture telescope is the Arecibo radio telescope located in Arecibo, Puerto Rico. Another stationary dish telescope like FAST, whose dish is built into a natural depression in the landscape, the antenna is steerable within an angle of about 20° of the zenith by moving the suspended feed antenna. The largest individual radio telescope of any kind is the RATAN-600 located near Nizhny Arkhyz, Russia, which consists of a 576-meter circle of rectangular radio reflectors, each of which can be pointed towards a central conical receiver.\n\nThe above stationary dishes are not fully \"steerable\"; they can only be aimed at points in an area of the sky near the zenith, and cannot receive from sources near the horizon. The largest fully steerable dish radio telescope is the 100 meter Green Bank Telescope in West Virginia, United States, constructed in 2000. The largest fully steerable radio telescope in Europe is the Effelsberg 100-m Radio Telescope near Bonn, Germany, operated by the Max Planck Institute for Radio Astronomy, which also was the world's largest fully steerable telescope for 30 years until the Green Bank antenna was constructed. The third-largest fully steerable radio telescope is the 76-meter Lovell Telescope at Jodrell Bank Observatory in Cheshire, England, completed in 1957. The fourth-largest fully steerable radio telescopes are six 70-meter dishes: three Russian RT-70, and three in the NASA Deep Space Network. , the planned Qitai Radio Telescope will be the world's largest fully steerable single-dish radio telescope with a diameter of .\n\nA typical size of the single antenna of a radio telescope is 25 meters. Dozens of radio telescopes with comparable sizes are operated in radio observatories all over the world.\n\nSince 1965, humans have launched three space-based radio telescopes. In 1965, the Soviet Union sent the first one called Zond 3. In 1997, Japan sent the second, HALCA. The last one was sent by Russia in 2011 called Spektr-R.\n\nOne of the most notable developments came in 1946 with the introduction of the technique called astronomical interferometry, which means combining the signals from multiple antennas so that they simulate a larger antenna, in order to achieve greater resolution. Astronomical radio interferometers usually consist either of arrays of parabolic dishes (e.g., the One-Mile Telescope), arrays of one-dimensional antennas (e.g., the Molonglo Observatory Synthesis Telescope) or two-dimensional arrays of omnidirectional dipoles (e.g., Tony Hewish's Pulsar Array). All of the telescopes in the array are widely separated and are usually connected using coaxial cable, waveguide, optical fiber, or other type of transmission line. Recent advances in the stability of electronic oscillators also now permit interferometry to be carried out by independent recording of the signals at the various antennas, and then later correlating the recordings at some central processing facility. This process is known as Very Long Baseline Interferometry (VLBI). Interferometry does increase the total signal collected, but its primary purpose is to vastly increase the resolution through a process called Aperture synthesis. This technique works by superposing (interfering) the signal waves from the different telescopes on the principle that waves that coincide with the same phase will add to each other while two waves that have opposite phases will cancel each other out. This creates a combined telescope that is equivalent in resolution (though not in sensitivity) to a single antenna whose diameter is equal to the spacing of the antennas furthest apart in the array.\n\nA high-quality image requires a large number of different separations between telescopes. Projected separation between any two telescopes, as seen from the radio source, is called a baseline. For example, the Very Large Array (VLA) near Socorro, New Mexico has 27 telescopes with 351 independent baselines at once, which achieves a resolution of 0.2 arc seconds at 3 cm wavelengths. Martin Ryle's group in Cambridge obtained a Nobel Prize for interferometry and aperture synthesis. The Lloyd's mirror interferometer was also developed independently in 1946 by Joseph Pawsey's group at the University of Sydney. In the early 1950s, the Cambridge Interferometer mapped the radio sky to produce the famous 2C and 3C surveys of radio sources. An example of a large physically connected radio telescope array is the Giant Metrewave Radio Telescope, located in Pune, India. The largest array, the Low-Frequency Array (LOFAR), is currently being constructed in western Europe, consisting of about 20,000 small antennas in 48 stations distributed over an area several hundreds of kilometers in diameter, and operates between 1.25 and 30 m wavelengths. VLBI systems using post-observation processing have been constructed with antennas thousands of miles apart. Radio interferometers have also been used to obtain detailed images of the anisotropies and the polarization of the Cosmic Microwave Background, like the CBI interferometer in 2004.\n\nThe world's largest physically connected telescopes, the Square Kilometre Array (SKA), are planned to start operation in 2024.\n\nMany astronomical objects are not only observable in visible light but also emit radiation at radio wavelengths. Besides observing energetic objects such as pulsars and quasars, radio telescopes are able to \"image\" most astronomical objects such as galaxies, nebulae, and even radio emissions from planets.\n\n\n\n"}
{"id": "17908459", "url": "https://en.wikipedia.org/wiki?curid=17908459", "title": "Radioimmunoprecipitation assay buffer", "text": "Radioimmunoprecipitation assay buffer\n\nRadioimmunoprecipitation assay buffer (RIPA buffer) is a lysis buffer used to lyse cells and tissue for the radio immunoprecipitation assay (RIPA). This buffer is more denaturing than NP-40 or Triton X-100 because it contains the ionic detergents SDS and sodium deoxycholate as active constituents and is particularly useful for disruption of nuclear membranes in the preparation of nuclear extracts. The RIPA buffer gives low background but can denature kinases.\n\nAn approximate RIPA buffer recipe for lysing a normalized total protein extract is as follows:\n\n"}
{"id": "38179259", "url": "https://en.wikipedia.org/wiki?curid=38179259", "title": "SEVENDIP", "text": "SEVENDIP\n\nSEVENDIP, which stands for Search for Extraterrestrial Visible Emissions from Nearby Developed Intelligent Populations, was a project developed by the Berkeley SETI Research Center at the University of California, Berkeley that used visible wavelengths to search for extraterrestrial life's intelligent signals from outer space.\n\nBetween 1997 and 2007, SEVENDIP employed a 30-inch automated telescope located in Lafayette, California, to scan the sky for potential optical interstellar communications in the nanosecond time-scale laser pulses. Another instrument was mounted on Berkeley's 0.8-meter automated telescope at Leuschner Observatory. Their sensors have a rise time of 0.7 ns and are sensitive to 300 - 700 nm wavelengths.\n\nThe target list included mostly nearby F, G, K and M stars, plus a few globular clusters and galaxies. The Leuschner pulse search examined several thousand stars, each for approximately one minute or more.\n\n "}
{"id": "58786849", "url": "https://en.wikipedia.org/wiki?curid=58786849", "title": "Sabeth Verpoorte", "text": "Sabeth Verpoorte\n\nElisabeth MJ Verpoorte is a professor of microfluidics and miniaturized \"lab-on-a-chip\" systems in the Faculty of Science and Engineering at the University of Groningen, Netherlands.\n\nFrom 1990-1996, Verpoorte trained as an automation systems postdoctoral researcher in the Manz group at CIBA in Basel, Switzerland. She was then a Group Leader with Nico F. de Rooij at the Institute for Microtechnology in Neuchatel. Professor Verpoorte assumed her position in Groningen in 2003.\n\nVerpoorte's research explores simulating \"in vivo\" organismic biology onto microscopic external devices. This is achieved through fabrication and control of chemical detectors and separations modules onto silicon dioxide chips. This dramatically decreases the amount of analyte, solution, or cells required to perform a given analysis. Her specific interests involve electrokinetic control over movement of various substances on these chips.\n\nVerpoorte was the 2018 President for the Society of Laboratory Automation and Screening. She has also held leadership roles in the Dutch Pharmacy Association and multiple international conferences.\n"}
{"id": "38727847", "url": "https://en.wikipedia.org/wiki?curid=38727847", "title": "SciShow", "text": "SciShow\n\nSciShow is a series of science-related videos on YouTube. The program is mainly hosted by Hank Green of the VlogBrothers and Michael Aranda. \"SciShow\" was launched as an original channel.\n\nThough Green hosts the majority of episodes, the show has alternate hosts; Michael Aranda has been with the show since its inception, and Olivia Gordon of the Missoula Insectarium joined in June 2016. Prior to her move to Chicago, Emily Graslie of \"The Brain Scoop\", also occasionally hosted on the channel. There have also been guest appearances by Lindsey Doe, who hosts \"Sexplanations\", another channel launched by Green; and by longtime SciShow staffer Stefan Chin, who since 2018 has been a regular host. \"SciShow\" has grown since its 2012 launch; since then it has employed a full editorial, production, and operations staff.\n\n\"SciShow Space\" has three rotating hosts: Hank Green, Reid Reimers, and Caitlin Hofmeister.\n\n\"SciShow Kids\" is primarily hosted by Jessi Knudsen Castañeda.\n\n\"SciShow Psych\" is co-hosted by Hank Green and Brit Garner.\n\nThe channel was launched as an \"original channel\", which meant that YouTube funded the channel. The show's initial grant was projected to expire in 2014, and in response, on September 12, 2013 \"SciShow\" joined the viewer-funding site \"Subbable\", created in part by Green.\n\nIn 2014, the channel landed a national advertisement deal with YouTube. The educational program was featured on platforms such as billboards and television commercials, as a result. Green details that the advertisements had a positive effect on \"SciShow\", stating, \"My Twitter exploded, our followers and subscribers exploded.\"\n\nAfter Patreon acquired \"Subbable\", the channel switched over to Patreon where it continues to receive support in exchange for various perks. \"SciShow\" currently has over five thousand patrons.\n\nSeveral different scientific fields are covered by \"SciShow\", including chemistry, physics, biology, zoology, entomology, botany, meteorology, astronomy, medicine, psychology, anthropology, and computer science. The videos on \"SciShow\" have a vast variety of different topics, such as nutrition, and \"science superlatives\". As of February 2016, \"SciShow\" has released 820 videos.\n\nA spin-off channel, \"SciShow Space\", launched in April 2014 to specialize in space topics. A second spin-off, \"SciShow Kids\", launched in March 2015 to specialize in delivering science topics to children. A third spinoff channel was announced in February 2017, \"SciShow Psych\", which debuted in March 2017, specializing in psychology and neuroscience. A podcast, \"SciShow Tangents\", was launched in November 2018; it features entertaining exchanges of scientific facts among many of the shows' staffers, and is directed at a mature audience.\n\nAs \"SciShow\" has amassed a large following, the channel has been featured on several media outlets. \n\nAs for the channel's success on YouTube, it was documented that by October 2014, the channel amassed over two million subscribers and earned over 210 million video views.\nAs of May 2017, the channel has over four million subscribers and has over 662 million views. \n\n\"SciShow\" has been criticized by fellow science YouTuber Myles Power for presenting biased and incorrect information about genetic modification. Criticisms were later responded to in a separate response video.\n\nIn 2017, SciShow won Complexly's People's Voice award. \n\n"}
{"id": "4354949", "url": "https://en.wikipedia.org/wiki?curid=4354949", "title": "Science center", "text": "Science center\n\nScience center may refer to: \n\n"}
{"id": "12562471", "url": "https://en.wikipedia.org/wiki?curid=12562471", "title": "Sociological Forum", "text": "Sociological Forum\n\nSociological Forum is a quarterly peer-reviewed academic journal published by Wiley-Blackwell on behalf of the Eastern Sociological Society. It has been published since 1986. The current editor is Karen A. Cerulo (Rutgers University). The journal covers all areas of sociology and related fields, emphasizing innovative direction in sociological research.\n\nAccording to the \"Journal Citation Reports\", the journal has a 2011 impact factor of 0.908, ranking it 58th out of 137 journals in the category \"Sociology\".\n"}
{"id": "11271178", "url": "https://en.wikipedia.org/wiki?curid=11271178", "title": "Transient state", "text": "Transient state\n\nA system is said to be in a transient state when a process variable or variables have been changed and the system has not yet reached a steady state.The time taken for the circuit to change from one steady state to another steady state is called the transient time. Transient analysis KVL and KCL to circuits containing energy storage elements results in differential. \n\nA transient process is a process in which process variables change over time.\n\nTransient analysis L and to study circuits containing energy storage elements results in differential. study of transient processes.\n\nWhen a chemical reactor is being brought into operation, the concentrations, temperatures, species compositions, and reaction rates are changing with time until operation reaches its nominal process variables.\n\nWhen a switch is flipped in an appropriate electrical circuit containing a capacitor or inductor, the component draws out the resulting change in voltage or current (respectively), causing the system to take a substantial amount of time to reach a new steady state.\n\nWe can define a transient by saying that when a quantity is at rest or in uniform motion and a change in time takes place , changing the existing state , a transient has taken place.\n\nWhen a SCR (four-layer PNPN Device) is switched on we have the problem of transients occurring as a result of high values of current and voltage oscillating around the point before normal levels are obtained again. Filtering can prevent damage to SCR by means of LC filters, zener diodes, trans-zorps, and varistors.\n"}
{"id": "746646", "url": "https://en.wikipedia.org/wiki?curid=746646", "title": "Vorlesungen über Zahlentheorie", "text": "Vorlesungen über Zahlentheorie\n\nBased on Dirichlet's number theory course at the University of Göttingen, the were edited by Dedekind and published after Lejeune Dirichlet's death. Dedekind added several appendices to the , in which he collected further results of Lejeune Dirichlet's and also developed his own original mathematical ideas.\n\nThe cover topics in elementary number theory, algebraic number theory and analytic number theory, including modular arithmetic, quadratic congruences, quadratic reciprocity and binary quadratic forms.\n\nThe contents of Professor John Stillwell's 1999 translation of the are as follows\n\nThis translation does not include Dedekind's Supplements X and XI in which he begins to develop the theory of ideals.\n\nThe German titles of supplements X and XI are:\n\nChapters 1 to 4 cover similar ground to Gauss' , and Dedekind added footnotes which specifically cross-reference the relevant sections of the . These chapters can be thought of as a summary of existing knowledge, although Dirichlet simplifies Gauss' presentation, and introduces his own proofs in some places.\n\nChapter 5 contains Dirichlet's derivation of the class number formula for real and imaginary quadratic fields. Although other mathematicians had conjectured similar formulae, Dirichlet gave the first rigorous proof.\n\nSupplement VI contains Dirichlet's proof that an arithmetic progression of the form \"a\"+\"nd\" where \"a\" and \"d\" are coprime contains an infinite number of primes.\n\nThe can be seen as a watershed between the classical number theory of Fermat, Jacobi and Gauss, and the modern number theory of Dedekind, Riemann and Hilbert. Dirichlet does not explicitly recognise the concept of the group that is central to modern algebra, but many of his proofs show an implicit understanding of group theory.\n\nThe contains two key results in number theory which were first proved by Dirichlet. The first of these is the class number formulae for binary quadratic forms. The second is a proof that arithmetic progressions contains an infinite number of primes (known as Dirichlet's theorem); this proof introduces Dirichlet L-series. These results are important milestones in the development of analytic number theory.\n\nLeopold Kronecker's book was first published in 1901 in 2 parts and reprinted by Springer in 1978. It covers elementary and algebraic number theory, including Dirichlet's theorem.\n\nEdmund Landau's book \"Vorlesungen über Zahlentheorie\" was first published as a 3-volume set in 1927. The first half of volume 1 was published as \n\"Vorlesungen über Zahlentheorie. Aus der elementare Zahlentheorie\" in 1950, with an English translation in 1958 under the title \"Elementary number theory\". In 1969 Chelsea republished the second half of volume 1 together with volumes 2 and 3 as a single volume. \n\nVolume 1 on elementary and additive number theory includes the topics such as Dirichlet's theorem, Brun's sieve, binary quadratic forms, Goldbach's conjecture, Waring's problem, and the Hardy–Littlewood work on the singular series. Volume 2 covers topics in analytic number theory, such as estimates for the error in the prime number theorem, and topics in geometric number theory such as estimating numbers of lattice points. Volume 3 covers algebraic number theory, including ideal theory, quadratic number fields, and applications to Fermat's last theorem. Many of the results described by Landau were state of the art at the time but have since been superseded by stronger results.\n\nHelmut Hasse's book \"Vorlesungen über Zahlentheorie\" was published in 1950, and is different from and more elementary than his book \"Zahlentheorie\". It covers elementary number theory, Dirichlet's theorem, and quadratic fields.\n\n"}
