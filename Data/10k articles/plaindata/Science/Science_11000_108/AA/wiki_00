{"id": "28836636", "url": "https://en.wikipedia.org/wiki?curid=28836636", "title": "Aber Dinlle Fault", "text": "Aber Dinlle Fault\n\nThe Aber Dinlle Fault is a SW-NE trending fault in North Wales. It forms part of the Menai Strait Fault System, with the Berw and Dinorwic Faults. It lies close to the epicentre of the 1984 Llŷn Peninsula earthquake, although it is not thought to have been responsible for that event. The fault was active during deposition of Ordovician rocks in the Welsh Basin. It was reactivated as a normal fault in the Carboniferous with a downthrow to the northwest and again reactivated as a reverse fault during Late Carboniferous inversion associated with the Variscan Orogeny.\n\n"}
{"id": "51238945", "url": "https://en.wikipedia.org/wiki?curid=51238945", "title": "Advancing Secondary Science Education thru Tetrahymena", "text": "Advancing Secondary Science Education thru Tetrahymena\n\nAdvancing Secondary Science Education thru Tetrahymena (ASSET) is an organization at Cornell University that is dedicated to expanding the use of the protist \"Tetrahymena\" in K-12 classrooms. They are funded by the National Institutes of Health through the SEPA (Science Education Partnership Award) Program. Although their name includes the word \"secondary,\" they have worked in recent years to develop materials for students in elementary, middle and high schools. The group develops modules, which are stand-alone labs or lessons that can be inserted into the curriculum of a class at the discretion of the teacher.\n\nModules are designed to be stand-alone lessons that fit into and compliment a life science curriculum. The ASSET program ships all the equipment that is needed to complete the modules to the teacher in a reusable plastic container, at ASSET's expense. The teacher who requested the materials can use them for up to two weeks. At the end of the two weeks, the teacher uses a pre-paid return label to send the materials back in the same container. Some materials, such as live cells, may be sent separately to provide for a chance for the culture to be established in the teacher's classroom.\n\nASSET has created fifteen science modules, each of which addresses a particular topic relevant to life science education in United States school curricula.\n\nThis module utilizes two species of \"Tetrahymena\": \"Tetrahymena thermophilia\" and \"Tetrahymena vorax\". In the lab, an extract, called stomatin, is made from the thermophilia, then placed into the vorax culture. There, it induces a transformation from the microstome form to the macrostome form in T. vorax. This transformation is most notable by a marked increase in the size of the cell (doubling or sometimes more), the resorption of the microstomal oral apparatus and the construction of a much larger macrostomal oral apparatus. This transformation allows the macrostomal T. vorax cells to prey on T. thermophilia, but also to cannibalize the microstomes of their own species.\n\nASSET has also created five science and society modules, which are designed to integrate social studies and science education into one unit.\n\n\n"}
{"id": "18407188", "url": "https://en.wikipedia.org/wiki?curid=18407188", "title": "Agequake", "text": "Agequake\n\nAgequake: Riding the Demographic Rollercoaster Shaking Business, Finance and our World is a book written by Paul Wallace and published in 1999, that investigates what possible ramifications are likely as a significant and unprecedented portion of the human population age. The book argues that rising longevity and lower fertility is causing a seismic shift in the profile of populations worldwide, and will be a fundamental force to that will shake business and finance, along with lifestyles and attitudes. Wallace suggests the old bogey of overpopulation is being replaced by a population \"implosion\".\n\nThrough using dependency ratios (the ratio of non-working dependents to the working population) will lead to a point where workers will be burdened with the fiscal and practical responsibilities of supporting a ballooning population of aged retirees. Society and economy will be affected as the proportion of youth declines - typically the most entrepreneurial, creating and risk taking segment of society. Along with the liquidation of baby boomer assets to pay for their retirements, this is likely to halt economic growth in the future, and economic stagnation may be a more likely prospect. Housing prices will plummet, and the world may experience the greatest bear market in history.\n\nInternationally the relationship between youthful and aggressive developing world and the rich older Organisation for Economic Co-operation and Development (OECD) countries (where elderly women will become an influential constituency) will change.\n\n\n"}
{"id": "48779361", "url": "https://en.wikipedia.org/wiki?curid=48779361", "title": "Alfred Thomas Elwes", "text": "Alfred Thomas Elwes\n\nAlfred Thomas Elwes (bapt. 1 August 1841 – 1917) was a British artist and natural history illustrator of mammals and birds. For most of his life he lived and worked in England, illustrating various natural history books of the nineteenth century as well as for Illustrated London News.\n\nElwes was born in Leghorn, Italy, where he was baptised 1 August 1841. His parents were Alfred Elwes and Louisa Anne Elwes. His father was a professor of languages. From 1872 to 1877 he was employed by the Illustrated London News as the chief draftsman of natural history subjects. In 1882 Elwes wrote \"How to draw animals, birds and dogs\". He died sometime after 1911 probably around 1917 in Willesden, Middlesex.\n\nElwes was married in Gravesend, Kent on 15 October 1873 to Kate Barnard.\n\n"}
{"id": "28914191", "url": "https://en.wikipedia.org/wiki?curid=28914191", "title": "Barnett Glacier", "text": "Barnett Glacier\n\nBarnett Glacier () is a large glacier in the Anare Mountains that flows east along the south side of Tapsell Foreland into Smith Inlet, northern Victoria Land, Antarctica. It was mapped by the United States Geological Survey (USGS) from surveys and from U.S. Navy air photos, 1960–63, and named by the Advisory Committee on Antarctic Names after Donald C. Barnett, USGS topographic engineer, a member of USGS Topo East and West, 1962–63, in which the expedition extended geodetic control from the area of Cape Hallett to the Wilson Hills (Topo West) and from the foot of Beardmore Glacier through the Horlick Mountains (Topo East). The glacier lies on the Pennell Coast, a portion of Antarctica lying between Cape Williams and Cape Adare.\n\n"}
{"id": "26995550", "url": "https://en.wikipedia.org/wiki?curid=26995550", "title": "Bear Valley Strip Mine", "text": "Bear Valley Strip Mine\n\nThe Bear Valley Strip Mine is an abandoned coal strip mine located in Coal Township, Northumberland County, to the southwest of the town of Shamokin, Pennsylvania. It lies in the Western Middle Field of the Anthracite belt in the Ridge-and-Valley Appalachians, where the Pennsylvanian Llewellyn Formation is exposed. The property is owned by the Reading Anthracite Company.\nThe coal and other overlying rock has been removed by mining down to a resistant sandstone bed, revealing the three-dimensional structures of folding and faulting caused by the Alleghany Orogeny. Students of geology have visited the location for decades due to the quality of exposures.\n\nThe central anticline in the valley is often called the \"Whaleback\".\n\nThe sequence of structural deformation is outlined as follows:\n"}
{"id": "35284514", "url": "https://en.wikipedia.org/wiki?curid=35284514", "title": "Black Sky: The Race For Space", "text": "Black Sky: The Race For Space\n\nBlack Sky: The Race For Space is a 2005 Discovery Channel documentary about Space Ship One, and how a small team backed by Paul Allen achieved human suborbital spaceflight and won the Ansari X Prize. It contains insights about how the rocketplane was built, the challenges they faced when they flew it, the vision of Burt Rutan about the future of this technology (tier two and three), and his thoughts about NASA and government. It won a Peabody Award in 2004. \n\n"}
{"id": "51474724", "url": "https://en.wikipedia.org/wiki?curid=51474724", "title": "CL J1001+0220", "text": "CL J1001+0220\n\nCL J1001+0220 is, as of 2016, the most distant known galaxy cluster. Discovered in 2016 by the Chandra X-ray Observatory in conjunction with the ESO's UltraVISTA telescope and the Atacama Large Millimeter Array, it has a redshift of z=2.506, placing it at a distance of 11.1 billion light-years from Earth. The galaxy cluster appears to be undergoing the transformation from a galaxy cluster that is still forming, a proto-cluster, to a mature cluster, and it is the first such cluster observed in this stage of its evolution. The cluster consists of seventeen galaxies, and nine of the eleven massive galaxies closest to its centre are starburst galaxies, forming new stars at the rate of about 3,400 Suns per year. In contrast, the Milky Way produces only the equivalent of one Sun per year, so these galaxies at the core of the cluster are producing stars at a rate three hundred times greater than the Milky Way on average. Other galaxy clusters at ten billion light years and closer have far less star formation, and this suggests that star formation slows down in large galaxies within clusters after the galaxies have already come together during the development of a galaxy cluster.\n"}
{"id": "16795124", "url": "https://en.wikipedia.org/wiki?curid=16795124", "title": "Classical Mechanics (Goldstein book)", "text": "Classical Mechanics (Goldstein book)\n\nClassical Mechanics is a textbook about the subject of that name written by Herbert Goldstein. Intended for advanced undergraduate and beginning graduate students, it has been one of the standard references in its subject around the world since its first publication in 1951.\n\nBefore the death of its primary author in 2005, a new (third) edition of the book was released, with the collaboration of Charles P. Poole and John L. Safko. In the third edition, the book discusses at length various mathematically sophisticated reformations of Newtonian mechanics, namely analytical mechanics, as applied to particles, rigid bodies and continua. In addition, it covers in some detail classical electromagnetism, special relativity, classical and relativistic field theory, chaos theory and fractal geometry. A brief discussion on general relativity is also included. There is an appendix on group theory.\n\n\n\n"}
{"id": "1319882", "url": "https://en.wikipedia.org/wiki?curid=1319882", "title": "Danish Committees on Scientific Dishonesty", "text": "Danish Committees on Scientific Dishonesty\n\nThe Danish Committees on Scientific Dishonesty (, or UVVU) are a set of three committees under the Danish Ministry of Research and Information Technology: a committee for natural science, agricultural and veterinary science and technical science; a committee for health and medical science; and a committee for social science and the humanities. They have a common chairman.\n\nPreviously obscure, the DCSD became embroiled in controversy after its January 2003 decision that the 2001 book \"The Skeptical Environmentalist\" by Bjørn Lomborg was \"clearly contrary to the standards of good scientific practice\", due to the author's systematically biased choice of data, and \"objectively\" was scientifically irredeemable, but Lomborg himself could not be \"subjectively\" convicted of intentional or gross negligence. Lomborg had argued in his book that claims by environmentalists about global warming, overpopulation, deforestation, and other matters are not scientifically substantiated. The DCSD further held that because of Lomborg's lack of scientific expertise, he had not shown intentional or gross negligence, and acquitted him of the accusations of scientific dishonesty.\n\nIn February 2003, Lomborg filed a complaint with the Ministry, and in December 2003, the Ministry found that the DCSD's handling of the investigation in the case had been improper, and remitted it for re-examination. In March 2004, the DCSD stated that since its finding had been to acquit Lomborg of the charges of scientific dishonesty (although they had criticized his biased selection of data), there was no basis to re-open the investigation, and dismissed the case.\n\nThe original DCSD decision about Lomborg provoked a petition among Danish academics. 308 scientists, many of them from the social sciences, criticised the DCSD's methods in the case and called for the DCSD to be disbanded. The Danish Minister of Science, Technology, and Innovation then asked the Danish Research Agency to form an independent working group to review DCSD practices. In response to this, another group of Danish scientists collected over 600 signatures (primarily from the medical and natural sciences community) to support the continued existence of the DCSD and presented their petition to the Danish Research Agency.\n\nThe DCSD was involved in another controversy investigating a paper on sex and intelligence authored by Helmuth Nyborg. After the DCSD cleared Nyborg of the charges of scientific misconduct, two Aarhus University professors, Lise Togeby and Jens Mammen resigned from their positions in the DCSD, citing that the DCSD operated from too narrow of a framework. Togeby explained that \"Roughly speaking, these committees can only decide whether a researcher has cheated or not. We cannot consider the issue of academic quality, or decide whether research has been carried out in accordance with good academic standards\".\n\n"}
{"id": "40420402", "url": "https://en.wikipedia.org/wiki?curid=40420402", "title": "Electrochemical dualism", "text": "Electrochemical dualism\n\nElectrochemical dualism is an obsolete scientific theory in chemistry relevant between around 1800 and around 1830 and pioneered by Jöns Jacob Berzelius. The theory held that all molecules are salts composed of basic and acidic oxides. The compound potassium sulphate for example was viewed as a salt of KO and SO. Berzelius based his theory on investigations he conducted in collaboration with Wilhelm Hisinger on certain salts with the newly discovered voltaic pile. They observed that many compounds could be decomposed by an electric current in an acidic component at the positive pole and a basic component at the negative pole. The theory was ultimately challenged and made redundant by radical theory.\n"}
{"id": "72489", "url": "https://en.wikipedia.org/wiki?curid=72489", "title": "Electron energy loss spectroscopy", "text": "Electron energy loss spectroscopy\n\nIn electron energy loss spectroscopy (EELS) a material is exposed to a beam of electrons with a known, narrow range of kinetic energies. Some of the electrons will undergo inelastic scattering, which means that they lose energy and have their paths slightly and randomly deflected. The amount of energy loss can be measured via an electron spectrometer and interpreted in terms of what caused the energy loss. Inelastic interactions include phonon excitations, inter- and intra-band transitions, plasmon excitations, inner shell ionizations, and Cherenkov radiation. The inner-shell ionizations are particularly useful for detecting the elemental components of a material. For example, one might find that a larger-than-expected number of electrons comes through the material with 285 eV less energy than they had when they entered the material. This is approximately the amount of energy needed to remove an inner-shell electron from a carbon atom, which can be taken as evidence that there is a significant amount of carbon present in the sample. With some care, and looking at a wide range of energy losses, one can determine the types of atoms, and the numbers of atoms of each type, being struck by the beam. The scattering angle (that is, the amount that the electron's path is deflected) can also be measured, giving information about the dispersion relation of whatever material excitation caused the inelastic scattering.\n\nThe technique was developed by James Hillier and RF Baker in the mid-1940s but was not widely used over the next 50 years, only becoming more widespread in research in the 1990s due to advances in microscope instrumentation and vacuum technology. With modern instrumentation becoming widely available in laboratories worldwide, the technical and scientific developments from the mid-1990s have been rapid. The technique is able to take advantage of modern aberration-corrected probe forming systems to attain spatial resolutions down to ~0.1 nm, while with a monochromated electron source and/or careful deconvolution the energy resolution can be 0.1 eV or better. This has enabled detailed measurements of the atomic and electronic properties of single columns of atoms, and in a few cases, of single atoms.\n\nEELS is spoken of as being complementary to energy-dispersive x-ray spectroscopy (variously called EDX, EDS, XEDS, etc.), which is another common spectroscopy technique available on many electron microscopes. EDX excels at identifying the atomic composition of a material, is quite easy to use, and is particularly sensitive to heavier elements. EELS has historically been a more difficult technique but is in principle capable of measuring atomic composition, chemical bonding, valence and conduction band electronic properties, surface properties, and element-specific pair distance distribution functions. EELS tends to work best at relatively low atomic numbers, where the excitation edges tend to be sharp, well-defined, and at experimentally accessible energy losses (the signal being very weak beyond about 3 keV energy loss). EELS is perhaps best developed for the elements ranging from carbon through the 3d transition metals (from scandium to zinc). For carbon, an experienced spectroscopist can tell at a glance the differences between diamond, graphite, amorphous carbon, and \"mineral\" carbon (such as the carbon appearing in carbonates). The spectra of 3d transition metals can be analyzed to identify the oxidation states of the atoms. Cu(I), for instance, has a different so-called \"white-line\" intensity ratio than does Cu(II). This ability to \"fingerprint\" different forms of the same element is a strong advantage of EELS over EDX. The difference is mainly due to the difference in energy resolution between the two techniques (~1 eV or better for EELS, perhaps a few tens of eV for EDX).\n\nThere are several basic flavors of EELS, primarily classified by the geometry and by the kinetic energy of the incident electrons (typically measured in kiloelectron-volts, or keV). Probably the most common today is transmission EELS, in which the kinetic energies are typically 100 to 300 keV and the incident electrons pass entirely through the material sample. Usually this occurs in a transmission electron microscope (TEM), although some dedicated systems exist which enable extreme resolution in terms of energy and momentum transfer at the expense of spatial resolution.\n\nOther flavors include reflection EELS (including reflection high-energy electron energy-loss spectroscopy (RHEELS)), typically at 10 to 30 keV, and aloof EELS (sometimes called near-field EELS), in which the electron beam does not in fact strike the sample but instead interacts with it via the long-ranged Coulomb interaction. Aloof EELS is particularly sensitive to surface properties but is limited to very small energy losses such as those associated with surface plasmons or direct interband transitions.\n\nWithin transmission EELS, the technique is further subdivided into valence EELS (which measures plasmons and interband transitions) and inner-shell ionization EELS (which provides much the same information as x-ray absorption spectroscopy, but from much smaller volumes of material). The dividing line between the two, while somewhat ill-defined, is in the vicinity of 50 eV energy loss.\n\nThe EEL spectrum can be roughly split into two different regions: the low-loss spectrum (up until about 50eV in energy loss) and the high-loss spectrum. The low-loss spectrum contains the zero-loss peak as well as the plasmon peaks, and contains information about the band structure and dielectric properties of the sample. The high-loss spectrum contains the ionisation edges that arise due to inner shell ionisations in the sample. These are characteristic to the species present in the sample, and as such can be used to obtain accurate information about the chemistry of a sample. \n\nEELS allows quick and reliable measurement of local thickness in transmission electron microscopy. The most efficient procedure is the following:\n\nThe spatial resolution of this procedure is limited by the plasmon localization and is about 1 nm, meaning that spatial thickness maps can be measured in scanning transmission electron microscopy with ~1 nm resolution.\n\nThe intensity and position of low-energy EELS peaks are affected by pressure. This fact allows mapping local pressure with ~1 nm spatial resolution.\n\nScanning confocal electron energy loss microscopy (SCEELM) is a new analytical microscopy tool that enables a double corrected transmission electron microscope to achieve sub-10 nm depth resolution in depth sectioning imaging of nanomaterials. It was previously termed as energy filtered scanning confocal electron microscopy due to the lack to full spectrum acquisition capability (only a small energy window on the order of 5 eV can be used at a time). SCEELM takes advantages of the newly developed chromatic aberration corrector which allows electrons of more than 100 eV of energy spread to be focused to roughly the same focal plane. It has been demonstrated that a simultaneous acquisition of the zero loss, low-loss, and core loss signals up to 400 eV in the confocal geometry with depth discrimination capability.\n\n\n\n"}
{"id": "12567847", "url": "https://en.wikipedia.org/wiki?curid=12567847", "title": "Filius philosophorum", "text": "Filius philosophorum\n\nThe ' (Latin for \"the philosophers' child\", i.e. made by the true students of philosophy) is a symbol in alchemy. In some texts it is equated with the philosopher's stone (), but in others it assumes its own symbolic meanings. Other terms for the include ' (\"child of wisdom\"), ' (\"our child\"), ' (\"sun child\"), ' (\"moon child\"), and ' (\"sun moon child\").\n\nThere are several images that have been used to represent the . Among these are the transformed hermaphroditic Hermes, the child of the Red King and the White Queen (the Sun and Moon), the child of the egg, and the three-fathered Orion.\n\nThe was also one of the Jungian archetypes analyzed by the Swiss psychologist.\n\n"}
{"id": "4549998", "url": "https://en.wikipedia.org/wiki?curid=4549998", "title": "Flower induction", "text": "Flower induction\n\nFlower induction is the physiological process in the plant by which the shoot apical meristem becomes competent to develop flowers. Biochemical changes at the apex, particularly those caused by cytokinins, accompany this process. Usually flower induction is followed by flower differentiation, with some notable exceptions such as in kiwifruit, where the two processes are separated. Flower induction can be reversed, but flower differentiation is irreversible, because anatomical changes are in place.\n"}
{"id": "5343250", "url": "https://en.wikipedia.org/wiki?curid=5343250", "title": "Giuseppa Barbapiccola", "text": "Giuseppa Barbapiccola\n\nGiuseppa Eleonora Barbapiccola (1702 – ca 1740) was an Italian natural philosopher, poet and translator. She is best known for her translation of René Descartes' \"Principles of Philosophy\" to Italian in 1722. In her preface to her translation of \"Principles of Philosophy\", Barbapiccola claimed that women, in contrast to the belief of her contemporaries, were not intellectually inferior out of nature, but because of their lack of education. Neapolitan scholars credited Barbapiccola as the individual who brought Cartesianism thought to Italy.\n\nBarbapiccola was probably born in Naples, and her family seemed to have originally come from Salerno. Her uncle was Tommaso Maria Alfani, an acclaimed Dominican preacher in Naples and a correspondent of Giambattista Vico. Although nothing is known of her parents, it is arguable that her uncle influenced her upbringing in education and learning.\n\nShe was a member of the Accademia degli Arcadi in Bologna under the name \"Myristic\". She often published her poems in collaboration with her friend, the poet Luisa Vico. \n\nThere is no known information on Barbapiccola’s formal education. However, it is suggested that much of her knowledge accumulated by means of conversations in Neapolitan salons. In particular, it was most likely in the home of Italian philosopher Giambattista Vico where she obtained most of her knowledge, as Vico was the father of her close friend, Luisa From her correspondence with Vico's daughter and with Vico himself it can be deducted that she was a close friend of the Vico family and a noted member of the Neapolitan intellectual circles.\n\nIn the lengthy preface to her translation of René Descartes' \"Principles of Philosophy\" to Italian, published in 1722, Barbapiccola defended women's intellectual ability, their right to meaningful education, and their claim to a voice in an intellectual discourse dominated by men. She starts with an apology and defends her translation against the arguments for women's intellectual inferiority, such as those advanced by Homer, Herodotus, and Claude Fleury. Barbapiccola subsequently provides an account of women's achievements throughout history, citing among others Daphne, Diotima, Queen Christina of Sweden and Anne Lefevre. She also seeks to disseminate the Cartesian philosophy of Descartes, who accorded intellectual authority to women. She dedicated the translation to Queen Elizabeth Stuart of Bohemia, with whom Descartes maintained an extended philosophical exchange.\n\nBarbapiccola identified women as the beneficiaries of her translation. As she seeks to counterbalance the deficiencies of women's traditional education in \"the Catechism, sewing, diverse little works, singing, dance, fashion dress, courteous behaviour, and polite speech\". Instead she seeks to impart the clear and coherent method of intellectual inquiry of Cartesian philosophy.\n\n"}
{"id": "30831956", "url": "https://en.wikipedia.org/wiki?curid=30831956", "title": "Horseshoe magnet", "text": "Horseshoe magnet\n\nA horseshoe magnet is a magnet made in the shape of a horseshoe. The magnet has two magnetic poles close together. This shape creates a strong magnetic field between the poles. \n\nIt is one type of permanent magnet, meaning that it stays magnetized, as opposed to an electromagnet, the magnetic field of which can be started and stopped.\n\nThe purpose of a horseshoe magnet's shape is to place the poles as close together as possible. The total magnetic flux is the same, but the field is greater, as it is spread over a smaller volume. A horseshoe is used, rather than a simpler C-shaped magnet, which is also used, because this places the maximum amount of magnetised material into the magnet, for given dimensions around the poles. A particularly large horseshoe magnet is U-shaped with long parallel sides, rather than the classical horseshoe.\n\nElectromagnets are also constructed as horseshoes. They may have either one or two coils wound on them. As most coils are wound by machine, the coil formers are straight. They are thus usually placed as two coils, one on each side of a U-shaped horseshoe.\n\nA horseshoe magnet can be created by bending a bar magnet into a horseshoe shape.\n"}
{"id": "4738373", "url": "https://en.wikipedia.org/wiki?curid=4738373", "title": "IC 4970", "text": "IC 4970\n\nIC 4970 is an unbarred lenticular galaxy of type \"\" in the constellation Pavo. It is from Earth and is interacting with the barred spiral galaxy . It was discovered on 21 September 1900 by American astronomer DeLisle Stewart.\n\nIC 4970 is located a few arcseconds away from the much larger barred spiral galaxy NGC 6872, and the two are known to be interacting with each other. Horrelou and Koribalski (2007) reported on a computer simulation used to determine how the two galaxies were interacting. The study concluded that approached nearly along the plane of its spiral disk, making a closest approach approximately 130 million years ago resulting in the latter's current highly elongated shape.\n\nAn ultraviolet-to-infrared study by Eufrasio, \"et al.\" (2013), using data from GALEX, Spitzer, and other resources found that the interaction between the two galaxies appears to have triggered significant star formation in the northeastern arm of beginning about from its nucleus. The same appears to have also occurred in the southwestern arm. A bright ultraviolet source was discovered at the end of the northeastern arm, around from the nucleus, which may be a tidal dwarf galaxy formed out of the interaction between and . The bright ultraviolet nature of this cluster indicates that it contains stars less than 200 million years old, which roughly coincides with the timeframe of the collision.\n\n"}
{"id": "11896485", "url": "https://en.wikipedia.org/wiki?curid=11896485", "title": "Jean Étienne Bercé", "text": "Jean Étienne Bercé\n\nJean-Etienne Bercé (24 April 1803 – 29 December 1879) was a French entomologist specialising in Lepidoptera. He wrote \"Faune Entomologique Française. Lépidoptères. Description de tous les Papillons qui se trouvent en France\" Paris, Chez Deyrolle Fils, 1867–1878.\n\nThis is an eight-volume work with 72 plates, 67 coloured by hand. It is divided:\n\nHe was elected president of the Société entomologique de France for the year 1868.\n\n"}
{"id": "1484932", "url": "https://en.wikipedia.org/wiki?curid=1484932", "title": "Johann Andreas Wagner", "text": "Johann Andreas Wagner\n\nJohann Andreas Wagner (21 March 1797 – 17 December 1861) was a German palaeontologist, zoologist and archaeologist who wrote several important works on palaeontology.\n\nWagner was a professor at the University of Munich, and curator of the Zoologische Staatssammlung (State Zoology Collection).\nHe was the author of \"Die Geographische Verbreitung der Säugethiere Dargestellt\" (1844–46).\n\nWagner was a Christian creationist.\n\nIn his travels to the fossil beds of Pikermi, Wagner discovered and described fossil remains of mastodon, \"Dinotherium\", \"Hipparion\", two species of giraffe, antelope and others. His collaboration with Johannes Roth on these fossils became a major textbook in palaeontology, known as \"Roth & Wagner\", in which the \"bones were much broken, and no complete skeleton was found with all the parts united\".\n\nWagner is commemorated in the scientific name of a species of South American snake, \"Diaphorolepis wagneri\".\n\n\n"}
{"id": "14339849", "url": "https://en.wikipedia.org/wiki?curid=14339849", "title": "Laboratory of Tree-Ring Research", "text": "Laboratory of Tree-Ring Research\n\nThe Laboratory of Tree-Ring Research (LTRR) was established in 1937 by A.E. Douglass, founder of the modern science of dendrochronology. The LTRR is a research unit in the College of Science at the University of Arizona in Tucson. Since its founding, visiting scholars and faculty at the lab have done notable work in the areas of climate change, fire history, ecology, archeology and hydrology.\n\n"}
{"id": "18009", "url": "https://en.wikipedia.org/wiki?curid=18009", "title": "Lift (force)", "text": "Lift (force)\n\nA fluid flowing past the surface of a body exerts a force on it. Lift is the component of this force that is perpendicular to the oncoming flow direction. It contrasts with the drag force, which is the component of the force parallel to the flow direction. Lift conventionally acts in an upward direction in order to counter the force of gravity, but it can act in any direction at right angles to the flow.\n\nIf the surrounding fluid is air, the force is called an aerodynamic force. In water or any other liquid, it is called a hydrodynamic force.\n\nDynamic lift is distinguished from other kinds of lift in fluids. Aerostatic lift or buoyancy, in which an internal fluid is lighter than the surrounding fluid, does not require movement and is used by balloons, blimps, dirigibles, boats, and submarines. Planing lift, in which only the lower portion of the body is immersed in a liquid flow, is used by motorboats, surfboards, and water-skis.\n\nA fluid flowing past the surface of a body exerts a force on it. It makes no difference whether the fluid is flowing past a stationary body or the body is moving through a stationary volume of fluid. Lift is the component of this force that is perpendicular to the oncoming flow direction. Lift is always accompanied by a drag force, which is the component of the surface force parallel to the flow direction.\n\nLift is most commonly associated with the wings of fixed-wing aircraft, although it is more generally generated by many other streamlined bodies such as propellers, kites, helicopter rotors, racing car wings, maritime sails, and wind turbines in air, and by sailboat keels, ship's rudders, and hydrofoils in water. Lift is also exploited in the animal world, especially by birds, bats, and insects, and even in the plant world by the seeds of certain trees.\n\nWhile the common meaning of the word \"lift\" assumes that lift opposes weight, lift can be in any direction with respect to gravity, since it is defined with respect to the direction of flow rather than to the direction of gravity. When an aircraft is cruising in straight and level flight, most of the lift opposes gravity. However, when an aircraft is climbing, descending, or banking in a turn the lift is tilted with respect to the vertical. Lift may also act as downforce in some aerobatic manoeuvres, or on the wing on a racing car. Lift may also be largely horizontal, for instance on a sailing ship.\n\nThe Lift discussed in this article is mainly in relation to airfoils, although marine hydrofoils and propellers share the same physical principles and work in the same way, despite differences between air and water such as density, compressibility, and viscosity.\n\nAn airfoil is a streamlined shape that is capable of generating significantly more lift than drag. A flat plate can generate lift, but not as much as a streamlined airfoil, and with somewhat higher drag.\n\nThere are several ways to explain how an airfoil generates lift. Some are more complicated or more physically rigorous than others; some have been shown to be incorrect. For example, there are explanations based directly on Newton’s laws of motion and explanations based on Bernoulli’s principle. Either can be used to explain lift.\n\nAn airfoil generates lift by exerting a downward force on the air as it flows past. According to Newton's third law, the air must exert an equal and opposite (upward) force on the airfoil, which is lift.\n\nThe airflow changes direction as it passes the airfoil and follows a path that is curved downward. According to Newton's second law, this change in flow direction requires a downward force applied to the air by the airfoil. Then Newton's third law requires the air to exert an upward force on the airfoil; thus a reaction force, lift, is generated opposite to the directional change. In the case of an airplane wing, the wing exerts a downward force on the air and the air exerts an upward force on the wing.\n\nThe downward turning of the flow is not produced solely by the lower surface of the airfoil, and the air flow above the airfoil accounts for much of the downward-turning action.\n\nBernoulli's principle states: \"In a flow where no energy is being added or taken away, the sum of its various energies is a constant: consequently where the velocity increases the pressure decreases and vice versa.\" Thus, there is a direct mathematical relationship between the pressure and the speed, so if one knows the speed at all points within the airflow one can calculate the pressure, and vice versa. For any airfoil generating lift, there must be a pressure imbalance, i.e. lower average air pressure on the top than on the bottom. Bernoulli's principle states that this pressure difference must be accompanied by a speed difference.\n\nStarting with the flow pattern observed in both theory and experiments, the increased flow speed over the upper surface can be explained in terms of streamtube pinching and conservation of mass.\n\nFor incompressible flow, the rate of volume flow (e.g. volume units per minute) must be constant within each streamtube since matter is not created or destroyed. If a streamtube becomes narrower, the flow speed must increase in the narrower region to maintain the constant flow rate, to satisfy the principle of conservation of mass.\n\nThe upper streamtubes constrict as they flow up and around the airfoil. Conservation of mass says that the flow speed must increase as the stream tube area decreases. Similarly, the lower streamtubes expand and their flowrate slows.\n\nFrom Bernoulli's principle, the pressure on the upper surface where the flow is moving faster is lower than the pressure on the lower surface where it is moving slower. This pressure difference creates a net aerodynamic force, pointing upward.\n\nAs explained below under A more comprehensive physical explanation, producing a lift force requires maintaining pressure differences in both the vertical and horizontal directions, and thus requires both downward turning of the flow and changes in flow speed consistent with Bernoulli's principle. The simplified explanations given above are therefore incomplete because they define lift in terms of only one or the other. And depending on the details, they have other shortcomings as well.\n\nThe explanation based on Flow deflection and Newton's laws is correct but is incomplete. It does not explain how the airfoil can impart downward turning to a much deeper swath of the flow than it actually touches. Further, it doesn't explain how the pressure differences in the horizontal direction are sustained. That is, it leaves out the Bernoulli part of the interaction.\n\nExplanations based on Increased flow speed and Bernoulli's principle first try to establish that there is higher flow speed over the upper surface, but they fail to explain correctly what causes the flow to speed up:\n\n\nBernoulli-only explanations imply that a speed difference arises from causes other than a pressure difference, and that the speed difference then leads to a pressure difference by Bernoulli’s principle. This implied one-way causation is a misconception. The real cause-and-effect relationship between pressure and velocity is reciprocal. Finally, Bernoulli-only explanations don't explain how the pressure differences in the vertical direction are sustained. That is, they leave out the downward-turning part of the interaction.\n\nMany alternative explanations for the generation of lift by an airfoil have been put forward, most intended to explain the phenomenon of lift to a general audience. Although the explanations may share features in common with the explanations above, additional assumptions and simplifications may be introduced. Some explanations introduce assumptions which proved to be wrong, such as \"equal transit-time\", and some used controversial terminology, such as \"Coanda effect\".\n\nBasic or popular sources often describe the \"equal transit-time\" theory of lift, which incorrectly assumes that the parcels of air that divide at the leading edge of an airfoil must rejoin at the trailing edge, forcing the air traveling along the longer upper surface to go faster. Bernoulli's principle is then cited to conclude that since the air moves slower along the bottom of the wing, the air pressure must be higher, pushing the wing up.\n\nHowever, there is no physical principle that requires equal transit time and experimental results show that this assumption is false. In fact, the air moving over the top of an airfoil generating lift moves \"much\" \"faster\" than the equal transit theory predicts. Further, the theory violates Newton's third law of motion, since it describes a force on the wing with no opposite force.\n\nThe assertion that the air must arrive simultaneously at the trailing edge is sometimes referred to as the \"equal transit-time fallacy\".\n\nIn its original sense, the \"Coandă effect\" refers to the tendency of a fluid jet to stay attached to an adjacent surface that curves away from the flow, and the resultant entrainment of ambient air into the flow. The effect is named for Henri Coandă, the Romanian aerodynamicist who exploited it in many of his patents.\n\nMore broadly, some consider the effect to include the tendency of any fluid boundary layer to adhere to a curved surface, not just the boundary layer accompanying a fluid jet. It is in this broader sense that the Coandă effect is used by some to explain why airflow remains attached to the top side of an airfoil. Jef Raskin, for example, describes a simple demonstration, using a straw to blow over the upper surface of a wing. The wing deflects upwards, thus demonstrating that the Coandă effect creates lift. This demonstration correctly demonstrates the Coandă effect as a fluid jet (the exhaust from a straw) adhering to a curved surface (the wing). However, the upper surface in this flow is a complicated, vortex-laden mixing layer, while on the lower surface the flow is quiescent. The physics of this demonstration are very different from that of the general flow over the wing. The usage in this sense is encountered in some popular references on aerodynamics. This is a controversial use of the term \"Coanda effect.\" The more established view in the aerodynamics field is that the Coandă effect is defined in the more limited sense above, and the flow following the upper surface simply reflects an absence of boundary-layer separation; thus it is not an example of the Coandă effect.\n\nLift is a result of pressure differences and depends on angle of attack, airfoil shape, air density, and airspeed.\n\nPressure is the normal force per unit area exerted by the air on itself and on surfaces that it touches. The lift force is transmitted through the pressure, which acts perpendicular to the surface of the airfoil. Thus, the net force manifests itself as pressure differences. The direction of the net force implies that the average pressure on the upper surface of the airfoil is lower than the average pressure on the underside.\n\nThese pressure differences arise in conjunction with the curved airflow. When a fluid follows a curved path, there is a pressure gradient perpendicular to the flow direction with higher pressure on the outside of the curve and lower pressure on the inside. This direct relationship between curved streamlines and pressure differences, sometimes called the streamline curvature theorem, was derived from Newton's second law by Leonhard Euler in 1754:\n\nThe left side of this equation represents the pressure difference perpendicular to the fluid flow. On the right hand side ρ is the density, v is the velocity, and R is the radius of curvature. This formula shows that higher velocities and tighter curvatures create larger pressure differentials and that for straight flow (R → ∞) the pressure difference is zero.\n\nThe angle of attack is the angle between the chord line of an airfoil and the oncoming airflow. A symmetrical airfoil will generate zero lift at zero angle of attack. But as the angle of attack increases, the air is deflected through a larger angle and the vertical component of the airstream velocity increases, resulting in more lift. For small angles a symmetrical airfoil will generate a lift force roughly proportional to the angle of attack.\n\nAs the angle of attack increases, the lift reaches a maximum at some angle; increasing the angle of attack beyond this critical angle of attack causes the upper-surface flow to separate from the wing; there is less deflection downward so the airfoil generates less lift. The airfoil is said to be stalled.\n\nThe lift force depends on the shape of the airfoil, especially the amount of camber (curvature such that the upper surface is more convex than the lower surface, as illustrated at right). Increasing the camber generally increases lift.\n\nCambered airfoils will generate lift at zero angle of attack. When the chord line is horizontal, the trailing edge has a downward direction and since the air follows the trailing edge it is deflected downward. When a cambered airfoil is upside down, the angle of attack can be adjusted so that the lift force is upwards. This explains how a plane can fly upside down.\n\nThe ambient flow conditions which affect lift include the fluid density, viscosity and speed of flow. Density is affected by temperature, and by the medium's acoustic velocity - i.e. by compressibility effects.\n\nLift is proportional to the density of the air and approximately proportional to the square of the flow speed. Lift also depends on the size of the wing, being generally proportional to the wing's area projected in the lift direction. In calculations it is convenient to quantify lift in terms of a \"Lift coefficient\" based on these factors.\n\nNo matter how smooth the surface of an airfoil seems, any surface is rough on the scale of air molecules. Air molecules flying into the surface bounce off the rough surface in random directions relative to their original velocities. The result is that when the air is viewed as a continuous material, it is seen to be unable to slide along the surface, and the air's velocity relative to the airfoil decreases to nearly zero at the surface (i.e., the air molecules \"stick\" to the surface instead of sliding along it), something known as the no-slip condition. Because the air at the surface has near-zero velocity but the air away from the surface is moving, there is a thin boundary layer in which air close to the surface is subjected to a shearing motion. The air's viscosity resists the shearing, giving rise to a shear stress at the airfoil's surface called skin-friction drag. Over most of the surface of most airfoils, the boundary layer is naturally turbulent, which increases skin-friction drag.\n\nUnder usual flight conditions, the boundary layer remains attached to both the upper and lower surfaces all the way to the trailing edge, and its effect on the rest of the flow is modest. Compared to the predictions of inviscid flow theory, in which there is no boundary layer, the attached boundary layer reduces the lift by a modest amount and modifies the pressure distribution somewhat, which results in a viscosity-related pressure drag over and above the skin-friction drag. The total of the skin-friction drag and the viscosity-related pressure drag is usually called the profile drag.\n\nAn airfoil's maximum lift at a given airspeed is limited by boundary-layer separation. As the angle of attack is increased, a point is reached where the boundary layer can no longer remain attached to the upper surface. When the boundary layer separates, it leaves a region of recirculating flow above the upper surface, as illustrated in the flow-visualization photo at right. This is known as the \"stall\", or \"stalling\". At angles of attack above the stall, lift is significantly reduced, though it does not drop to zero. The maximum lift that can be achieved before stall, in terms of the lift coefficient, is generally less than 1.5 for single-element airfoils and can be more than 3.0 for airfoils with high-lift slotted flaps and leading-edge devices deployed.\n\nThe flow around bluff bodies – i.e. without a streamlined shape, or stalling airfoils – may also generate lift, in addition to a strong drag force. This lift may be steady, or it may oscillate due to vortex shedding. Interaction of the object's flexibility with the vortex shedding may enhance the effects of fluctuating lift and cause vortex-induced vibrations. For instance, the flow around a circular cylinder generates a Kármán vortex street: vortices being shed in an alternating fashion from the cylinder's sides. The oscillatory nature of the flow produces a fluctuating lift force on the cylinder, even though the net (mean) force is negligible. The lift force frequency is characterised by the dimensionless Strouhal number, which depends on the Reynolds number of the flow.\n\nFor a flexible structure, this oscillatory lift force may induce vortex-induced vibrations. Under certain conditions – for instance resonance or strong spanwise correlation of the lift force – the resulting motion of the structure due to the lift fluctuations may be strongly enhanced. Such vibrations may pose problems and threaten collapse in tall man-made structures like industrial chimneys.\n\nIn the Magnus effect, a lift force is generated by a spinning cylinder in a freestream. Here the mechanical rotation acts on the boundary layer, causing it to separate at different locations on the two sides of the cylinder. The asymmetric separation changes the effective shape of the cylinder as far as the flow is concerned such that the cylinder acts like a lifting airfoil with circulation in the outer flow.\n\nAs described above under \"Simplified physical explanations of lift on an airfoil\", there are two main popular explanations: one based on downward deflection of the flow (Newton's laws), and one based on pressure differences accompanied by changes in flow speed (Bernoulli's principle). Either of these, by itself, correctly identifies some aspects of the lifting flow but leaves other important aspects of the phenomenon unexplained. A more comprehensive explanation involves both downward deflection and pressure differences (including changes in flow speed associated with the pressure differences), and requires looking at the flow in more detail.\n\nThe airfoil shape and angle of attack work together so that the airfoil exerts a downward force on the air as it flows past. According to Newton's third law, the air must then exert an equal and opposite (upward) force on the airfoil, which is the lift.\n\nThe net force exerted by the air occurs as a pressure difference over the airfoil's surfaces. Pressure in a fluid is always positive in an absolute sense, so that pressure must always be thought of as pushing, and never as pulling. The pressure thus pushes inward on the airfoil everywhere on both the upper and lower surfaces. The flowing air reacts to the presence of the wing by reducing the pressure on the wing's upper surface and increasing the pressure on the lower surface. The pressure on the lower surface pushes up harder than the reduced pressure on the upper surface pushes down, and the net result is upward lift.\n\nThe pressure difference which results in Lift acts directly on the airfoil surfaces; however, understanding how the pressure difference is produced requires understanding what the flow does over a wider area.\n\nAn airfoil affects the speed and direction of the flow over a wide area, producing a pattern called a \"velocity field\". When an airfoil produces lift, the flow ahead of the airfoil is deflected upward, the flow above and below the airfoil is deflected downward, and the flow behind the airfoil is deflected upward again, leaving the air far behind the airfoil in the same state as the oncoming flow far ahead. The flow above the upper surface is sped up, while the flow below the airfoil is slowed down. Together with the upward deflection of air in front and the downward deflection of the air immediately behind, this establishes a net circulatory component of the flow. The downward deflection and the changes in flow speed are pronounced and extend over a wide area, as can be seen in the flow animation on the right. These differences in the direction and speed of the flow are greatest close to the airfoil and decrease gradually far above and below. All of these features of the velocity field also appear in theoretical models for lifting flows.\n\nThe pressure is also affected over a wide area, in a pattern of non-uniform pressure called a \"pressure field\". When an airfoil produces lift, there is a diffuse region of low pressure above the airfoil, and usually a diffuse region of high pressure below, as illustrated by the isobars (curves of constant pressure) in the drawing. The pressure difference that acts on the surface is just part of this pressure field.\n\nThe non-uniform pressure exerts forces on the air in the direction from higher pressure to lower pressure. The direction of the force is different at different locations around the airfoil, as indicated by the block arrows in the \"pressure distribution with isobars\" figure. Air above the airfoil is pushed toward the center of the low-pressure region, and air below the airfoil is pushed outward from the center of the high-pressure region.\n\nAccording to \"Newton's second law\", a force causes air to accelerate in the direction of the force. Thus the vertical arrows in the \"pressure distribution with isobars\" figure indicate that air above and below the airfoil is accelerated, or turned downward, and that the non-uniform pressure is thus the cause of the downward deflection of the flow visible in the flow animation. To produce this downward turning, the airfoil must have a positive angle of attack or have its rear portion curved downward as on an airfoil with camber. Note that the downward turning of the flow over the upper surface is the result of the air being pushed downward by higher pressure above it than below it. Some explanations that refer to the \"Coandă effect\" suggest that viscosity plays a key role in the downward turning, but this is false. (see below under \"Controversy regarding the Coandă effect\").\n\nThe arrows ahead of the airfoil indicate that the flow ahead of the airfoil is deflected upward, and the arrows behind the airfoil indicate that the flow behind is deflected upward again, after being deflected downward over the airfoil. These deflections are also visible in the flow animation.\n\nThe arrows ahead of the airfoil and behind also indicate that air passing through the low-pressure region above the airfoil is sped up as it enters, and slowed back down as it leaves. Air passing through the high-pressure region below the airfoil sees the opposite - it is slowed down and then sped up. Thus the non-uniform pressure is also the cause of the changes in flow speed visible in the flow animation. The changes in flow speed are consistent with \"Bernoulli's principle\", which states that in a steady flow without viscosity, lower pressure means higher speed, and higher pressure means lower speed.\n\nThus changes in flow direction and speed are directly caused by the non-uniform pressure. But this cause-and-effect relationship is not just one-way; it works in both directions simultaneously. The air's motion is affected by the pressure differences, but the existence of the pressure differences depends on the air's motion. The relationship is thus a mutual, or reciprocal, interaction: Air flow changes speed or direction in response to pressure differences, and the pressure differences are sustained by the air's resistance to changing speed or direction. A pressure difference can exist only if something is there for it to push against. In aerodynamic flow, the pressure difference pushes against the air's inertia, as the air is accelerated by the pressure difference. This is why the air's mass is part of the calculation, and why lift depends on air density.\n\nSustaining the pressure difference that exerts the lift force on the airfoil surfaces requires sustaining a pattern of non-uniform pressure in a wide area around the airfoil. This requires maintaining pressure differences in both the vertical and horizontal directions, and thus requires both downward turning of the flow and changes in flow speed according to Bernoulli's principle. The pressure differences and the changes in flow direction and speed sustain each other in a mutual interaction. The pressure differences follow naturally from Newton's second law and from the fact that flow along the surface follows the predominantly downward-sloping contours of the airfoil. And the fact that the air has mass is crucial to the interaction.\n\nProducing a lift force requires both downward turning of the flow and changes in flow speed consistent with Bernoulli's principle. Each of the simplified explanations given above in Simplified physical explanations of lift on an airfoil falls short by trying to explain lift in terms of only one or the other, thus explaining only part of the phenomenon and leaving other parts unexplained. \n\nWhen the pressure distribution on the airfoil surface is known, determining the total lift requires adding up the contributions to the pressure force from local elements of the surface, each with its own local value of pressure. The total lift is thus the integral of the pressure, in the direction perpendicular to the farfield flow, over the airfoil surface.\n\nformula_2\n\nwhere:\n\nThe above lift equation neglects the skin friction forces, which are small compared to the pressure forces.\n\nBy using the streamwise vector i parallel to the freestream in place of k in the integral, we obtain an expression for the pressure drag \"D\" (which includes the pressure portion of the profile drag and, if the wing is three-dimensional, the induced drag). If we use the spanwise vector j, we obtain the side force \"Y\".\n\nThe validity of this integration generally requires the airfoil shape to be a closed curve that is piecewise smooth.\n\nLift depends on the size of the wing, being approximately proportional to the wing area. It is often convenient to quantify the lift of a given airfoil by its \"lift coefficient\" formula_4, which defines its overall lift in terms of a unit area of the wing.\n\nIf the value of formula_4 for a wing at a specified angle of attack is given, then the lift produced for specific flow conditions can be determined:\n\nwhere \n\nMathematical theories of lift are based on continuum fluid mechanics, assuming that air flows as a continuous fluid. Lift is generated in accordance with the fundamental principles of physics, the most relevant being the following three principles:\n\nBecause an airfoil affects the flow in a wide area around it, the conservation laws of mechanics are embodied in the form of partial-differential equations combined with a set of boundary condition requirements which the flow has to satisfy at the airfoil surface and far away from the airfoil.\n\nTo predict lift requires solving the equations for a particular airfoil shape and flow condition, which generally requires calculations that are so voluminous that they are practical only on a computer, through the methods of computational fluid dynamics (CFD). Determining the net aerodynamic force from a CFD solution requires \"adding up\" (integrating) the forces due to pressure and shear determined by the CFD over every surface element of the airfoil as described under \"pressure integration\".\n\nThe Navier-Stokes equations (NS) provide the potentially most accurate theory of lift, but in practice, capturing the effects of turbulence in the boundary layer on the airfoil surface requires sacrificing some accuracy, and requires use of the Reynolds-averaged Navier-Stokes equations (RANS). Simpler but less accurate theories have also been developed.\n\nThese equations represent conservation of mass, Newton's second law (conservation of momentum), conservation of energy, the Newtonian law for the action of viscosity, the Fourier heat conduction law, an equation of state relating density, temperature, and pressure, and formulas for the viscosity and thermal conductivity of the fluid.\n\nIn principle, the NS equations, combined with boundary conditions of no through-flow and no slip at the airfoil surface, could be used to predict lift in any situation in ordinary atmospheric flight with high accuracy. However, airflows in practical situations always involve turbulence in the boundary layer next to the airfoil surface, at least over the aft portion of the airfoil. Predicting lift by solving the NS equations in their raw form would require the calculations to resolve the details of the turbulence, down to the smallest eddy. This is not yet possible, even on the most powerful current computer. So in principle the NS equations provide a complete and very accurate theory of lift, but practical prediction of lift requires that the effects of turbulence be modeled in the RANS equations rather than computed directly.\n\nThese are the NS equations with the turbulence motions averaged over time, and the effects of the turbulence on the time-averaged flow represented by turbulence modeling (an additional set of equations based on a combination of dimensional analysis and empirical information on how turbulence affects a boundary layer in a time-averaged average sense). A RANS solution consists of the time-averaged velocity vector, pressure, density, and temperature defined at a dense grid of points surrounding the airfoil.\n\nThe amount of computation required is a minuscule fraction (billionths) of what would be required to resolve all of the turbulence motions in a raw NS calculation, and with large computers available it is now practical to carry out RANS calculations for complete airplanes in three dimensions. Because turbulence models are not perfect, the accuracy of RANS calculations is imperfect, but it is adequate for practical aircraft design. Lift predicted by RANS is usually within a few percent of the actual lift.\n\nThe Euler equations are the NS equations without the viscosity, heat conduction, and turbulence effects. As with a RANS solution, an Euler solution consists of the velocity vector, pressure, density, and temperature defined at a dense grid of points surrounding the airfoil. While the Euler equations are simpler than the NS equations, they do not lend themselves to exact analytic solutions.\n\nFurther simplification is available through potential flow theory, which reduces the number of unknowns to be determined, and makes analytic solutions possible in some cases, as described below.\n\nEither Euler or potential-flow calculations predict the pressure distribution on the airfoil surfaces roughly correctly for angles of attack below stall, where they might miss the total lift by as much as 10-20%. At angles of attack above stall, inviscid calculations do not predict that stall has happened, and as a result they grossly overestimate the lift.\n\nIn potential-flow theory, the flow is assumed to be irrotational, i.e. that small fluid parcels have no net rate of rotation. Mathematically, this is expressed by the statement that the curl of the velocity vector field is everywhere equal to zero. Irrotational flows have the convenient property that the velocity can be expressed as the gradient of a scalar function called a potential. A flow represented in this way is called potential flow.\n\nIn potential-flow theory, the flow is assumed to be incompressible. Incompressible potential-flow theory has the advantage that the equation (Laplace's equation) to be solved for the potential is linear, which allows solutions to be constructed by superposition of other known solutions. The incompressible-potential-flow equation can also be solved by conformal mapping, a method based on the theory of functions of a complex variable. In the early 20th century, before computers were available, conformal mapping was used to generate solutions to the incompressible potential-flow equation for a class of idealized airfoil shapes, providing some of the first practical theoretical predictions of the pressure distribution on a lifting airfoil.\n\nA solution of the potential equation directly determines only the velocity field. The pressure field is deduced from the velocity field through Bernoulli's equation.\n\nApplying potential-flow theory to a lifting flow requires special treatment and an additional assumption. The problem arises because lift on an airfoil in inviscid flow requires circulation in the flow around the airfoil (See \"Circulation and the Kutta-Joukowski theorem\" below), but a single potential function that is continuous throughout the domain around the airfoil cannot represent a flow with nonzero circulation. The solution to this problem is to introduce a branch cut, a curve or line from some point on the airfoil surface out to infinite distance, and to allow a jump in the value of the potential across the cut. The jump in the potential imposes circulation in the flow equal to the potential jump and thus allows nonzero circulation to be represented. However, the potential jump is a free parameter that is not determined by the potential equation or the other boundary conditions, and the solution is thus indeterminate. A potential-flow solution exists for any value of the circulation and any value of the lift. One way to resolve this indeterminacy is to impose the Kutta condition, which is that, of all the possible solutions, the physically reasonable solution is the one in which the flow leaves the trailing edge smoothly. The streamline sketches illustrate one flow pattern with zero lift, in which the flow goes around the trailing edge and leaves the upper surface ahead of the trailing edge, and another flow pattern with positive lift, in which the flow leaves smoothly at the trailing edge in accordance with the Kutta condition.\n\nThis is potential-flow theory with the further assumptions that the airfoil is very thin and the angle of attack is small. The linearized theory predicts the general character of the airfoil pressure distribution and how it is influenced by airfoil shape and angle of attack, but is not accurate enough for design work. For a 2D airfoil, such calculations can be done in a fraction of a second in a spreadsheet on a PC.\n\nWhen an airfoil generates lift, several components of the overall velocity field contribute to a net circulation of air around it: the upward flow ahead of the airfoil, the accelerated flow above, the decelerated flow below, and the downward flow behind.\n\nThe circulation can be understood as the total amount of \"spinning\" (or vorticity) of an inviscid fluid around the airfoil.\n\nThe Kutta–Joukowski theorem relates the lift per unit width of span of a two-dimensional airfoil to this circulation component of the flow. In particular, it requires the Kutta condition to be met, in which the rear stagnation point moves to the airfoil trailing edge and attaches there for the duration of flight.\n\nThe Kutta-Joukowski theorem is a key element in an explanation of lift that follows the development of the flow around an airfoil as the airfoil starts its motion from rest and a starting vortex is formed and left behind, leading to the formation of circulation around the airfoil. Lift is then inferred from the Kutta-Joukowski theorem. This explanation is largely mathematical, and its general progression is based on logical inference, not physical cause-and-effect.\n\nThe Kutta-Joukowski model does not predict how much circulation or lift a two-dimensional airfoil will produce. Calculating the lift per unit span using Kutta-Joukowski requires a known value for the circulation.\n\nThe lift generated by a conventional airfoil is dictated by both its design and the flight conditions, such as forward velocity, angle of attack and air density. Lift can be increased by artificially increasing the circulation, for example by boundary-layer blowing or the use of blown flaps. In the Flettner rotor the entire airfoil is circular and spins about a spanwise axis to create the circulation.\n\nThe flow around a three-dimensional wing involves significant additional issues, especially relating to the wing tips. For a wing of low aspect ratio, such as a typical delta wing, two-dimensional theories may provide a poor model and three-dimensional flow effects can dominate. Even for wings of high aspect ratio, the three-dimensional effects associated with finite span can affect the whole span, not just close to the tips.\n\nThe vertical pressure gradient at the wing tips causes air to flow sideways, out from under the wing then up and back over the upper surface. This reduces the pressure gradient at the wing tip, therefore also reducing lift. The lift tends to decrease in the spanwise direction from root to tip, and the pressure distributions around the airfoil sections change accordingly in the spanwise direction. Pressure distributions in planes perpendicular to the flight direction tend to look like the illustration at right. This spanwise-varying pressure distribution is sustained by a mutual interaction with the velocity field. Flow below the wing is accelerated outboard, flow outboard of the tips is accelerated upward, and flow above the wing is accelerated inboard, which results in the flow pattern illustrated at right.\n\nThere is more downward turning of the flow than there would be in a two-dimensional flow with the same airfoil shape and sectional lift, and a higher sectional angle of attack is required to achieve the same lift compared to a two-dimensional flow. The wing is effectively flying in a downdraft of its own making, as if the freestream flow were tilted downward, with the result that the total aerodynamic force vector is tilted backward slightly compared to what it would be in two dimensions. The additional backward component of the force vector is called lift-induced drag.\n\nThe difference in the spanwise component of velocity above and below the wing (between being in the inboard direction above and in the outboard direction below) persists at the trailing edge and into the wake downstream. After the flow leaves the trailing edge, this difference in velocity takes place across a relatively thin shear layer called a vortex sheet.\n\nThe wingtip flow leaving the wing creates a tip vortex. As the main vortex sheet passes downstream from the trailing edge, it rolls up at its outer edges, merging with the tip vortices. The combination of the wingtip vortices and the vortex sheets feeding them is called the vortex wake.\n\nIn addition to the vorticity in the trailing vortex wake there is vorticity in the wing's boundary layer, called 'bound vorticity', which connects the trailing sheets from the two sides of the wing into a vortex system in the general form of a horseshoe. The horseshoe form of the vortex system was recognized by the British aeronautical pioneer Lanchester in 1907.\n\nGiven the distribution of bound vorticity and the vorticity in the wake, the Biot-Savart law (a vector-calculus relation) can be used to calculate the velocity perturbation anywhere in the field, caused by the lift on the wing. Approximate theories for the lift distribution and lift-induced drag of three-dimensional wings are based on such analysis applied to the wing's horseshoe vortex system. In these theories, the bound vorticity is usually idealized and assumed to reside at the camber surface inside the wing.\n\nBecause the velocity is deduced from the vorticity in such theories, some authors describe the situation to imply that the vorticity is the cause of the velocity perturbations, using terms such as \"the velocity induced by the vortex,\" for example. But attributing mechanical cause-and-effect between the vorticity and the velocity in this way is not consistent with the physics. The velocity perturbations in the flow around a wing are in fact produced by the pressure field.\n\nThe flow around a lifting airfoil must satisfy Newton's second law regarding conservation of momentum, both locally at every point in the flow field, and in an integrated sense over any extended region of the flow. For an extended region, Newton's second law takes the form of the \"momentum theorem for a control volume\", where a control volume can be any region of the flow chosen for analysis. The momentum theorem states that the integrated force exerted at the boundaries of the control volume (a surface integral), is equal to the integrated time rate of change (material derivative) of the momentum of fluid parcels passing through the interior of the control volume. For a steady flow, this can be expressed in the form of the net surface integral of the flux of momentum through the boundary.\n\nThe lifting flow around a 2D airfoil is usually analyzed in a control volume that completely surrounds the airfoil, so that the inner boundary of the control volume is the airfoil surface, where the downward force per unit span formula_12 is exerted on the fluid by the airfoil. The outer boundary is usually either a large circle or a large rectangle. At this outer boundary distant from the airfoil, the velocity and pressure are well represented by the velocity and pressure associated with a uniform flow plus a vortex, and viscous stress is negligible, so that the only force that must be integrated over the outer boundary is the pressure. The free-stream velocity is usually assumed to be horizontal, with lift vertically upward, so that the vertical momentum is the component of interest.\n\nFor the free-air case (no ground plane), the force formula_12 exerted by the airfoil on the fluid is manifested partly as momentum fluxes and partly as pressure differences at the outer boundary, in proportions that depend on the shape of the outer boundary, as shown in the diagram at right. For a flat horizontal rectangle that is much longer than it is tall, the fluxes of vertical momentum through the front and back are negligible, and the lift is accounted for entirely by the integrated pressure differences on the top and bottom. For a square or circle, the momentum fluxes and pressure differences account for half the lift each. For a vertical rectangle that is much taller than it is wide, the unbalanced pressure forces on the top and bottom are negligible, and lift is accounted for entirely by momentum fluxes, with a flux of upward momentum that enters the control volume through the front accounting for half the lift, and a flux of downward momentum that exits the control volume through the back accounting for the other half.\n\nThe results of all of the control-volume analyses described above are consistent with the Kutta-Joukowski theorem described above. Both the tall rectangle and circle control volumes have been used in derivations of the theorem.\n\nAn airfoil produces a pressure field in the surrounding air, as explained under \"The wider flow around the airfoil\" above. The pressure differences associated with this field die off gradually, becoming very small at large distances, but never disappearing altogether. Below the airplane, the pressure field persists as a positive pressure disturbance that reaches the ground, forming a pattern of slightly-higher-than-ambient pressure on the ground, as shown on the right. Although the pressure differences are very small far below the airplane, they are spread over a wide area and add up to a substantial force. For steady, level flight, the integrated force due to the pressure differences is equal to the total aerodynamic lift of the airplane and to the airplane's weight. According to Newton's third law, this pressure force exerted on the ground by the air is matched by an equal-and-opposite upward force exerted on the air by the ground, which offsets all of the downward force exerted on the air by the airplane. The net force due to the lift, acting on the atmosphere as a whole, is therefore zero, and thus there is no integrated accumulation of vertical momentum in the atmosphere, as was noted by Lanchester early in the development of modern aerodynamics.\n\n\n"}
{"id": "28854774", "url": "https://en.wikipedia.org/wiki?curid=28854774", "title": "List of Russian scientists", "text": "List of Russian scientists\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "16127798", "url": "https://en.wikipedia.org/wiki?curid=16127798", "title": "List of South Dakota state symbols", "text": "List of South Dakota state symbols\n\nThis is a list of the official state symbols of the U.S. state of South Dakota.\n\n"}
{"id": "2117793", "url": "https://en.wikipedia.org/wiki?curid=2117793", "title": "List of economics journals", "text": "List of economics journals\n\nThe following is a list of scholarly journals in economics containing most of the prominent academic journals in economics.\n\nPopular magazines or other publications related to economics, finance, or business are not listed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "4659082", "url": "https://en.wikipedia.org/wiki?curid=4659082", "title": "List of formulae involving π", "text": "List of formulae involving π\n\nThe following is a list of significant formulae involving the mathematical constant . The list contains only formulae whose significance is established either in the article on the formula itself, the article Pi, or the article Approximations of.\n\nwhere is the circumference of a circle, is the diameter.\n\nwhere is the area of a circle and is the radius.\n\nwhere is the volume of a sphere and is the radius.\n\nwhere is the surface area of a sphere and is the radius.\n\n\n\n\n\n\n\n\nThe following are efficient for calculating arbitrary binary digits of :\n\nwhere formula_42 is the \"n\"-th Fibonacci number.\n\nSome infinite series involving pi are:\n\nwhere formula_54 is the Pochhammer symbol for the rising factorial. See also Ramanujan–Sato series.\n\nViète's formula:\n\nFor more on the third identity, see Euler's continued fraction formula.\n\n\n"}
{"id": "11838733", "url": "https://en.wikipedia.org/wiki?curid=11838733", "title": "List of information systems journals", "text": "List of information systems journals\n\nThe following is a list of information systems journals, containing academic journals that cover information systems. The list given here contains the most influential, currently publishing journals in the field.\n\nTo understand which are the best journals for a particular Information Systems (IS) field of study, one needs to understand that IS is a multidisciplinary research area and that the \"IS discipline draws on the social science as well as the engineering research traditions. The social science tradition is represented by the economics-based and behavioral research, whereas the engineering tradition is epitomized by the design science approach in IS research.\" \n\nThe following journals were selected by the Association for Information Systems Senior Scholars as a top basket of journals, (but only \"focused on behavioral, business-oriented IS research\").\n\n\n"}
{"id": "21942454", "url": "https://en.wikipedia.org/wiki?curid=21942454", "title": "List of lakes of Michigan", "text": "List of lakes of Michigan\n\nThis is a list of lakes in Michigan. The American state of Michigan borders four of the Great Lakes.\nThe number of inland lakes in Michigan depends on the minimum size: there are 62,798 lakes ≥ 0.1 acres, 26,266 lakes ≥ 1.0 acres, 6,537 lakes ≥ 10.0 acres, 1,148 lakes ≥ 100 acres, 98 lakes ≥ 1,000 acres, and 10 lakes ≥ 10,000 acres.\n\nMany lakes share names, some of the most common are Clear Lake, Long Lake, Mud Lake, Round Lake and Silver Lake.\n\n\n"}
{"id": "5971814", "url": "https://en.wikipedia.org/wiki?curid=5971814", "title": "List of mathematicians (J)", "text": "List of mathematicians (J)\n\n\n\n\n\n"}
{"id": "7119992", "url": "https://en.wikipedia.org/wiki?curid=7119992", "title": "List of volcanoes in Costa Rica", "text": "List of volcanoes in Costa Rica\n\nThis is a list of active and extinct volcanoes in Costa Rica.\n\n\n"}
{"id": "28203227", "url": "https://en.wikipedia.org/wiki?curid=28203227", "title": "Madagascar (software)", "text": "Madagascar (software)\n\nMadagascar is a software package for multidimensional data analysis and reproducible computational experiments. Its mission is to provide\nfor researchers working with digital image and data processing in geophysics and related fields. Technology developed using the Madagascar project management system is transferred in the form of recorded processing histories, which become \"computational recipes\" to be verified, exchanged, and modified by users of the system.\n\nThe Madagascar environment consists of:\n\nAn example SConstruct file is shown below\n\nNote that SConstruct by itself does not do any job other than setting rules for building different targets. The targets get built when one executes scons on the command line. Running scons produces\n\nMadagascar is free software and is licensed under the GPL.\n\nMadagascar was first publicly presented at the EAGE Workshop in Vienna in June 2006. The work on the package (previously named RSF) was started by Sergey Fomel in 2003. Since then, many people have contributed to it.\n\nWhile being written mostly from scratch, Madagascar borrows ideas from the design of SEPlib, an open-source package maintained by Bob Clapp at the Stanford Exploration Project (SEP). Generations of SEP students and researchers contributed to SEPlib. Most important contributions came from Rob Clayton, Jon Claerbout, Dave Hale, Stew Levin, Rick Ottolini, Joe Dellinger, Steve Cole, Dave Nichols, Martin Karrenbach, Biondo Biondi, and Bob Clapp.\n\nMadagascar also borrows ideas from Seismic Unix (SU), a package maintained by John Stockwell at the Center for Wave Phenomenon (CWP) at the Colorado School of Mines (Stockwell, 1997; Stockwell, 1999). Main contributors to SU included Einar Kjartansson, Shuki Ronen, Jack Cohen, Chris Liner, Dave Hale, and John Stockwell. SU adopted an open-source BSD-style license starting with release 40 (April 10, 2007).\n\nMadagascar Schools on Reproducible Computational Geophysics are annual events, where new users get introduced to the package, and project participants meet to discuss new developments.\n\nHere is the list of previous schools:\n\n"}
{"id": "46260158", "url": "https://en.wikipedia.org/wiki?curid=46260158", "title": "Malunion", "text": "Malunion\n\nA malunion is when a fractured bone doesn’t heal properly. Some ways that it shows is by having the bone being twisted, shorter, or bent. Malunions can occur by having the bones improperly aligned when immobilized, having the cast taken off too early, or never seeking medical treatment after the break.\n\nMalunions are painful and commonly produce swelling around the area, possible immobilization, and deterioration of the bone and tissue.\n\nMalunions are presented by excessive swelling, twisting, bending, and possibly shortening of the bone. Patients may have trouble placing weight on or near the malunion. However, most commonly the presentation of a bend in the bone exhibits the diagnosis of a malunion.\n\nAn X-ray is essential for the proper diagnosis of a malunion. The doctor will look into the patient’s history and the treatment process for the bone fracture. Oftentimes a CT scan and probably a MRI are also used in diagnosis. MRI are used to check of cartilage and ligament issues that developed due to the malunion and misalignment. CT scans are used to locate normal or abnormal structures within the body and to help during procedures to guide the placement of instruments and/or treatments.\n\nOnce diagnosed and located, surgery is the most common treatment for a malunion. The surgery consists of the surgeon re-breaking the bone and realigning it to the anatomically correct position. There are different types and levels of severity for malunions which helps determine the treatment. Most often, either screws, plates or pins are used secure the new alignment. In some cases, the bone may be trimmed to allow full orientation at the fractured spot. It is also possible that a bone graft could be used to help with healing.\n\nDuring follow ups an X-ray or a CT scan may be used to verify that the fracture is healing properly and is now in the anatomically correct position.\n\n"}
{"id": "50900513", "url": "https://en.wikipedia.org/wiki?curid=50900513", "title": "Michael Bümel", "text": "Michael Bümel\n\nMichael Bümel (17th century) was a German scientific instrument maker. He was a maker of surveying instruments, active in Nuremberg between 1613 and 1633.\n"}
{"id": "11341364", "url": "https://en.wikipedia.org/wiki?curid=11341364", "title": "Microturbulence", "text": "Microturbulence\n\nMicroturbulence is a form of turbulence that varies over small distance scales. (Large-scale turbulence is called macroturbulence.)\n\nMicroturbulence is one of several mechanisms that can cause broadening of the absorption lines in the stellar spectrum. Stellar microturbulence varies\nwith the effective temperature and the surface gravity.\n\nThe microturbulent velocity is defined as the microscale\nnon-thermal component of the gas velocity in the region of spectral\nline formation.\nConvection is the mechanism believed to be responsible for the observed turbulent velocity field, both in low mass stars and massive stars.\nWhen examined by a spectroscope, the velocity of the convective gas along the line of sight produces Doppler shifts in the absorption bands. It is the distribution of these velocities along the line of sight that produces the microturbulence broadening of the absorption lines in low mass stars that have convective envelopes. In massive stars convection can be present only in small regions below the surface; these sub-surface convection zones can excite turbulence at the stellar surface through the emission of acoustic and gravity waves.\nThe strength of the microturbulence (symbolized by ξ, in units of km s) can be determined by comparing the broadening of strong lines versus weak lines.\n\nMicroturbulence plays a critical role in energy transport during magnetic nuclear fusion experiments, such as the Tokamak.\n"}
{"id": "3630347", "url": "https://en.wikipedia.org/wiki?curid=3630347", "title": "Natural competence", "text": "Natural competence\n\nIn microbiology, genetics, cell biology, and molecular biology, competence is the ability of a cell to alter its genetics by taking up extracellular (\"naked\") DNA from its environment in the process called transformation. Competence may be differentiated between \"natural competence\", a genetically specified ability of bacteria which is thought to occur under natural conditions as well as in the laboratory, and \"induced\" or \"artificial competence\", which arises when cells in laboratory cultures are treated to make them transiently permeable to DNA. Competence allows for rapid adaptation and DNA repair of the cell. This article primarily deals with natural competence in bacteria, although information about artificial competence is also provided.\n\nNatural competence was discovered by Frederick Griffith in 1928, when he showed that a preparation of killed cells of a pathogenic bacterium contained something that could transform related non-pathogenic cells into the pathogenic type. In 1944 Oswald Avery, Colin MacLeod, and Maclyn McCarty demonstrated that this 'transforming factor' was pure DNA \n. This was the first compelling evidence that DNA carries the genetic information of the cell.\n\nSince then, natural competence has been studied in a number of different bacteria, particularly \"Bacillus subtilis\", \"Streptococcus pneumoniae\" (Griffith's \"pneumococcus\"), \"Neisseria gonorrhoeae\" and \"Haemophilus influenzae\". Areas of active research include the mechanisms of DNA transport, the regulation of competence in different bacteria, and the evolutionary function of competence.\n\nIn the natural world DNA usually becomes available by death and lysis of other cells, but in the laboratory it is provided by the researcher, often as a genetically engineered fragment or plasmid. During uptake, DNA is transported across the cell membrane(s), and the cell wall if one is present. Once the DNA is inside the cell it may be degraded to nucleotides, which are reused for DNA replication and other metabolic functions. Alternatively it may be recombined into the cell’s genome by its DNA repair enzymes. If this recombination changes the cell’s genotype the cell is said to have been transformed. Artificial competence and transformation are used as research tools in many organisms (\"see Transformation (genetics)\").\n\nIn almost all naturally competent bacteria components of extracellular filaments called type 4 pili (a type of fimbria) bind extracellular double stranded DNA. The DNA is then translocated across the membrane (or membranes for gram negative bacteria) through multi-component protein complexes driven by the degradation of one strand of the DNA. Single stranded DNA in the cell is bound by a well-conserved protein, DprA, which loads the DNA onto RecA, which mediates homologous recombination through the classic DNA repair pathway.\n\nIn laboratory cultures, natural competence is usually tightly regulated and often triggered by nutritional shortages or adverse conditions. However the specific inducing signals and regulatory machinery are much more variable than the uptake machinery, and little is known about the regulation of competence in the natural environments of these bacteria. Transcription factors have been discovered which regulate competence; an example is sxy (also known as tfoX) which has been found to be regulated in turn by a 5' non-coding RNA element. In bacteria capable of forming spores, conditions inducing sporulation often overlap with those inducing competence. Thus cultures or colonies containing sporulating cells often also contain competent cells. Recent research by Süel \"et al.\" has identified an excitable core module of genes which can explain entry into and exit from competence when cellular noise is taken into account.\n\nMost competent bacteria are thought to take up all DNA molecules with roughly equal efficiencies, but bacteria in the families Neisseriaceae and Pasteurellaceae preferentially take up DNA fragments containing short DNA sequences, termed DNA uptake sequence (DUS) in Neisseriaceae and uptake signal sequence (USS) in Pasteurellaceae, that are very frequent in their own genomes. Neisserial genomes contain thousands of copies of the preferred sequence GCCGTCTGAA, and Pasteurellacean genomes contain either AAGTGCGGT or ACAAGCGGT.\n\nMost proposals made for the primary evolutionary function of natural competence as a part of natural bacterial transformation fall into three categories: (1) the selective advantage of genetic diversity; (2) DNA uptake as a source of nucleotides (DNA as “food”); and (3) the selective advantage of a new strand of DNA to promote homologous recombinational repair of damaged DNA (DNA repair). A secondary suggestion has also been made, noting the occasional advantage of horizontal gene transfer.\n\nArguments to support genetic diversity as the primary evolutionary function of sex (including bacterial transformation) are given by Barton and Charleworth . and by Otto and Gerstein. However, the theoretical difficulties associated with the evolution of sex suggest that sex for genetic diversity is problematic. Specifically with respect to bacterial transformation, competence requires the high cost of a global protein synthesis switch, with, for example, more than 16 genes that are switched on only during competence of \"Streptococcus pneumoniae\". However, since bacteria tend to grow in clones, the DNA available for transformation would generally have the same genotype as that of the recipient cells. Thus, there is always a high cost in protein expression without, in general, an increase in diversity. Other differences between competence and sex have been considered in models of the evolution of genes causing competence; these models found that competence's postulated recombinational benefits were even more elusive than those of sex.\n\nThe second hypothesis, DNA as food, relies on the fact that cells that take up DNA inevitably acquire the nucleotides the DNA consists of, and, because nucleotides are needed for DNA and RNA synthesis and are expensive to synthesize, these may make a significant contribution to the cell's energy budget. Some naturally competent bacteria also secrete nucleases into their surroundings, and all bacteria can take up the free nucleotides these nucleases generate from environmental DNA. The energetics of DNA uptake are not understood in any system, so it is difficult to compare the efficiency of nuclease secretion to that of DNA uptake and internal degradation. In principle the cost of nuclease production and the uncertainty of nucleotide recovery must be balanced against the energy needed to synthesize the uptake machinery and to pull DNA in. Other important factors are the likelihoods that nucleases and competent cells will encounter DNA molecules, the relative inefficiencies of nucleotide uptake from the environment and from the periplasm (where one strand is degraded by competent cells), and the advantage of producing ready-to-use nucleotide monophosphates from the other strand in the cytoplasm. Another complicating factor is the self-bias of the DNA uptake systems of species in the family \"Pasteurellaceae\" and the genus \"Neisseria\", which could reflect either selection for recombination or for mechanistically efficient uptake.\n\nIn bacteria, the problem of DNA damage is most pronounced during periods of stress, particularly oxidative stress, that occur during crowding or starvation conditions. Under such conditions there is often only a single chromosome present. The finding that some bacteria induce competence under such stress conditions, supports the third hypothesis, that transformation exists to permit DNA repair. In experimental tests, bacterial cells exposed to agents damaging their DNA, and then undergoing transformation, survived better than cells exposed to DNA damage that did not undergo transformation (Hoelzer and Michod, 1991). In addition, competence to undergo transformation is often inducible by known DNA damaging agents (reviewed by Michod \"et al\"., 2008 and Bernstein \"et al\"., 2012). Thus, a strong short-term selective advantage for natural competence and transformation would be its ability to promote homologous recombinational DNA repair under conditions of stress. Such stress conditions might be incurred during bacterial infection of a susceptible host. Consistent with this idea, Li et al. reported that, among different highly transformable \"S. pneumoniae\" isolates, nasal colonization fitness and virulence (lung infectivity) depends on an intact competence system.\n\nA counter argument was made based on the 1993 report of Redfield who found that single-stranded and double-stranded damage to chromosomal DNA did not induce or enhance competence or transformation in \"B. subtilis\" or \"H. influenzae\", suggesting that selection for repair has played little or no role in the evolution of competence in these species\n\nHowever more recent evidence indicates that competence for transformation is, indeed, specifically induced by DNA damaging conditions. For instance, Claverys \"et al\". in 2006 showed that the DNA damaging agents mitomycin C (a DNA cross-linking agent) and fluoroquinolone (a topoisomerase inhibitor that causes double-strand breaks) induce transformation in \"Streptococcus pneumoniae\". In addition, Engelmoer and Rozen in 2011 demonstrated that in \"S. pneumoniae\" transformation protects against the bactericidal effect of mitomycin C. Induction of competence further protected against the antibiotics kanomycin and streptomycin. Although these aminoglycoside antibiotics were previously regarded as non-DNA damaging, recent studies in 2012 of Foti \"et al\". showed that a substantial portion of their bactericidal activity results from release of the hydroxyl radical and induction of DNA damages, including double-strand breaks.\n\nDorer \"et al\"., in 2010, showed that ciprofloxacin, which interacts with DNA gyrase and causes production of double-strand breaks, induces expression of competence genes in \"Helicobacter pylori\", leading to increased transformation. In 2011 studies of \"Legionella pneumophila\", Charpentier \"et al\". tested 64 toxic molecules to determine which ones induce competence. Only six of these molecules, all DNA damaging agents, strongly induced competence. These molecules were norfloxacin, ofloxacin and nalidixic acid (inhibitors of DNA gyrase that produce double strand breaks), mitomycin C (which produces inter-strand cross-links), bicyclomycin (causes single- and double-strand breaks), and hydroxyurea (causes oxidation of DNA bases). Charpentier \"et al\". also showed that UV irradiation induces competence in \"L. pneumophila\" and further suggested that competence for transformation evolved as a response to DNA damage.\n\nA long-term advantage may occasionally be conferred by occasional instances of horizontal gene transfer also called \"lateral gene transfer\", (which might result from non-homologous recombination after competence is induced), that could provide for antibiotic resistance or other advantages.\n\nRegardless of the nature of selection for competence, the composite nature of bacterial genomes provides abundant evidence that the horizontal gene transfer caused by competence contributes to the genetic diversity that makes evolution possible.\n\n"}
{"id": "2951410", "url": "https://en.wikipedia.org/wiki?curid=2951410", "title": "Notarikon", "text": "Notarikon\n\nNotarikon ( \"Noṭariqōn\") is a method of deriving a word, by using each of its initial (Hebrew: ) or final letters () to stand for another, to form a sentence or idea out of the words. Another variation uses the first \"and\" last letters, or the two middle letters of a word, in order to form another word. The word \"notarikon\" is borrowed from the Greek language (νοταρικόν)), and was derived from the Latin word \"notarius\" meaning \"shorthand writer.\"\n\nNotarikon is one of the three ancient methods used by the Kabbalists (the other two are gematria and temurah) to rearrange words and sentences. These methods were used in order to derive the esoteric substratum and deeper spiritual meaning of the words in the Bible. Notarikon was also used in alchemy.\n\nRashi uses notarikon seven times in his pirush (, \"explanation\") on Chumash:\n\nA common usage of Notarikon in the practice of Kabbalah, either for magic or ritual, was to form secret names of God derived from religious or biblical verses. Agla, an acronym for \"Atah Gibor Le-olam Adonai\", translated, \"You, O Lord, are mighty forever,\" is one of the most famous examples of Notarikon. Dozens of examples are found in the Berit Menuchah, as is referenced in the following passage:\n\nAnd it was discovered that the Malachim were created from the wind and the fine and enlightening air, and that the name of their origin עַמַרֻמְאֵליוְהָ was derived from the verse (Psalms 104:4): ‘Who makest the winds thy messengers, fire and flame thy ministers’ (...) And when the lights reach this Sefira, they unite and receive a name that is derived from the central letters of the following verse (Genesis 6:2): ‘The sons of God saw that the daughters of men were fair; and they took to wife such of them as they chose.’ And this valiant name, which is drawn in the Gevura, is .\n\nSefer Gematriot, is another example where many Notarikons for usage on talismans, are given from Biblical verses.\n\n"}
{"id": "48784674", "url": "https://en.wikipedia.org/wiki?curid=48784674", "title": "Paul Anton Cibis", "text": "Paul Anton Cibis\n\nPaul Anton Cibis, M.D. (26 June 1911 – 30 April 1965) was a clinical ophthalmologist, surgeon and pioneer of modern vitreoretinal surgery. As part of Operation Paperclip Cibis came to the United States and performed research for the U.S. Air Force and studied the effects of atomic weapons testing on the eye. He was an internationally recognized expert in retinal detachment surgery and pioneered the use of liquid silicon for this procedure.\n\nCibis was born in 1911 at Rybnik, Silesia, Germany. In 1921, this became part of Poland. Prior to entry into university, he studied in Rybnik and Racibórz Silesia. Cibis then studied medicine at the University of Breslau and at Munich. He attended University of Berlin for medical school. He interned at Berlin and completed residency at Heidelberg. At Heidelberg he met his future wife Lisa, a medical student who became an ophthalmologist. The couple married in 1939. Lisa held membership in the German Ophthalmological Society. Later, they had a daughter, Andrea and son, Gerhart and both children pursued careers in ophthalmology after studying medicine in the United States at Washington University, St. Louis. From 1940 to 1944, he held the position of research and clinical assistant in the University Eye Clinic at Heidelberg. Upon appointment to Heidelberg, and at the outbreak of World War II, Cibis was quickly drafted into the German Army (Wehrmacht). He served on the Russian front for two years, as an ophthalmologist and medical administrator. From 1944 to 1949, Cibis was Oberarzt (senior physician) and Docent for Ophthalmology at Heidelberg. During the years 1949 to 1955, he was a research ophthalmologist for the United States Air Force, School of Aviation Medicine at Randolph Field in Texas. From 1 June 1955 to 1 June 1956, Cibis was an instructor in ophthalmology at Washington University in Saint Louis, Missouri. From 1956 to 1965, he held the position of associate professor of ophthalmology at Washington University in Saint Louis, Missouri. In 1965, he died of a heart attack at St. Louis, Missouri shortly after his return from England where he had attended the Annual Meeting of the Ophthalmological Society of the United Kingdom.\n\nHis initial education was in Rybnik and then in Racibórz Silesia. In 1931, Cibis began studies in medicine at the University of Breslau and continued for two additional years in Munich. Upon completion, he attended medical school at the University of Berlin and graduated on 4 December 1936. His internship was at Berlin in 1937. In 1938, he went to Heidelberg for residency at the University Eye Clinic. In 1940, Cibis completed residency in ophthalmology at the University of Heidelberg Eye Clinic. Until 1949, he was chief assistant in ophthalmology at the Eye Clinic.\n\nIn June 1949, he came to the United States as a research ophthalmologist at the United States Air Force School of Aviation Medicine at Randolph Air Force Base in Texas under Operation Paperclip. The work involved the visual and physiological problems of aviation, space travel and atomic weapons testing, including flashblindness. Cibis worked with Werner K. Noell to study the ocular effects from high intensity x-radiation. He paired with David V.L. Brown to analyze the retinal changes as a result of ionizing radiation. The team of Cibis, Brown and John E. Pickering utilized Rhesus monkeys to study the effect of gamma radiation on the retina. The joint efforts from Byrnes, Brown, Rose and Cibis studied retinal burns, chorioretinal burns and flashblindness that resulted from atomic weapons tests. The group was interested in the biological effects of atomic weapons. Much of this work was performed as part of Operation Redwing. Cibis worked with the Air Force until 1955. In 1955, Dr. Bernard Becker offered Cibis a position in the Department of Ophthalmology at Washington University, St. Louis.\n\nIn 1955, he was recruited by Dr. Bernard Becker to become a member of the Department of Ophthalmology at Washington University, St. Louis. From 1 June 1955 to 1 June 1956, Cibis was an instructor in ophthalmology at Washington University in Saint Louis, Missouri. From 1956 until 1965, he was associate professor of ophthalmology at Washington University. Cibis became an international authority for the treatment of disease of the vitreous and retina. The surgical techniques he pioneered and developed, involved directly operating on the vitreous to repair retinal detachments. The techniques were innovative and ground breaking. In St. Louis, Cibis teamed with Bernard Becker, Michel Ter-Pogossian, M.A. Constant and M.R. Smith to continue work on the ocular effects from x-radiation.\n\nCibis was an internationally recognized expert in retinal detachment surgery. He pioneered this procedure. The most significant contribution to ophthalmologic surgery being: \n\"the demonstration of the technique of injecting liquid silicon into the vitreous chamber to replace lost or shrunken vitreous thus forcing the retina back into apposition with the choroid.\"\n\nCibis was an active member of several professional societies. These include the German Ophthalmological Society, the Optical Society of America, the Association for Research in Ophthalmology and Otolaryngology, the American Medical Association, the Jules Gonin Club, the German Medical Society of Chicago, the Pan-American Ophthalmological Association, and the Pan-Pacific Surgical Association. Shortly after his death, the American Ophthalmological Society elected Cibis as a member.\n\nIn 1949, Docent Dr. Paul Cibis was granted the Albrecht von Graefe Award by the German Ophthalmological Society of Heidelberg for the years 1940 to 1948 inclusive. He singly authored numerous papers, was coauthor on numerous other publications, and produced one textbook titled, \"Vitreoretinal pathology and surgery in retinal detachment\".\n\nThe reference work on ophthamology by Dr. Anand Shroff, \"An Eye on Numbers: A Ready Reckoner in Ophthalmology\", lists in the contributions section on vitreoretinal surgery, in 1962 Cibis is credited with utilizing silicon oil used for the repair of retinal detachments. In 1965, he was recognized as being the first to describe intraocular cryotherapy, in addition to being the first to cut vitreous adhesions and traction bands.\n\nIn 2002, the Paul A. Cibis Distinguished Professorship of Ophthalmology and Visual Sciences was established by an anonymous donor at the Washington University School of Medicine in St. Louis, Missouri.\n\n\n\n\n"}
{"id": "3423435", "url": "https://en.wikipedia.org/wiki?curid=3423435", "title": "Paul Douglas (meteorologist)", "text": "Paul Douglas (meteorologist)\n\nPaul Douglas (born June 12, 1958; real name Douglas Paul Kruhoeffer) is a meteorologist, author, entrepreneur, and software expert in Minneapolis-St.Paul, Minnesota. He has over 30 years of broadcast television and radio experience.\n\nDouglas worked at WCCO-TV in Minneapolis, Minnesota from December 1997 until he was laid off in April 2008 as part of nationwide cutbacks by CBS. He currently leads a number of companies that he founded or co-founded, including WeatherNation (as CEO), Broadcast Weather (as CEO) and Smart Energy (as President). Douglas regularly writes and speaks about global warming and is critical of those who say that it is not occurring or is not caused by human actions.\n\nDouglas wrote a daily weather column for the \"Star Tribune\" from 1997 until his replacement by the WCCO-TV weather team in February 2009. He provided forecasts for three local radio stations. He remains a reporter for the Twin Cities Public Television show Almanac.\n\nDouglas received a B.S. in Meteorology from Pennsylvania State University in 1980.\n\nHe founded EarthWatch Communications in 1990, which created weather visualizations for the films \"Jurassic Park\" and \"Twister\". He made a cameo appearance in a weather center scene in the latter. He also co-founded Digital Cyclone in 1998. The company creates weather applications and supplies content for wireless devices under the My-Cast brand name. Douglas sold Digital Cyclone to Garmin in 2007 for $45 million.\n\nDouglas has authored two books, \"Prairie Skies\" (1991) and \"Restless Skies\" (2004).\n\nIn 2007, he co-founded SingularLogic LLC, a patent holding company, and founded Broadcast Weather LLC and NoozMe LLC, which hope to capitalize on SingularLogic's patents.\n\nIn 2009, the \"St. Cloud Times\" took him on as the head of their meteorological team and Conservation Minnesota partnered with him to create MNWeatherCenter, a hub for Minnesota weather.\n\nIn 2010, the \"Star Tribune\" rehired him as a weather blogger.\n\n"}
{"id": "1347601", "url": "https://en.wikipedia.org/wiki?curid=1347601", "title": "Robonaut", "text": "Robonaut\n\nA robonaut is a humanoid robot, part of a development project conducted by the Dexterous Robotics Laboratory at NASA's Lyndon B. Johnson Space Center (JSC) in Houston, Texas. Robonaut differs from other current space-faring robots in that, while most current space robotic systems (such as robotic arms, cranes and exploration rovers) are designed to move large objects, Robonaut's tasks require more dexterity.\n\nThe core idea behind the Robonaut series is to have a humanoid machine work alongside astronauts. Its form factor and dexterity are designed such that Robonaut can use space tools and work in similar environments suited to astronauts.\n\nThe latest Robonaut version, R2, the first US-built robot on the ISS, delivered by STS-133 in Feb 2011, is a robotic torso designed to assist with crew EVA's and can hold tools used by the crew. However, Robonaut 2 does not have adequate protection needed to exist outside the space station and enhancements and modifications would be required to allow it to move around the station's interior. NASA states \"Robonauts are essential to NASA's future as we go beyond low Earth orbit\", and R2 will provide performance data about how a robot may work side-by-side with astronauts.\n\nRobonaut 1 (R1) was the first model. The two Robonaut versions (R1A and R1B) had many partners including DARPA. None were flown to space. Other designs for Robonaut propose uses for teleoperation on planetary surfaces, where Robonaut could explore a planetary surface while receiving instructions from orbiting astronauts above. Robonaut B was introduced in 2002, R1B is a portable version of R1. R1 had several lower bodies. One of these was the Zero-G Leg, which if Robonaut was working on the space station he would climb using the external handrails and then use his zero-g leg to latch onto the station using a WIF socket. Another was the Robotic Mobility Platform (RMP), developed in 2003, it is a base with two wheels using a Segway PT. And the four wheeled Centaur 1, which was developed in 2006.\nRobonaut has participated in NASA's Desert Research and Technology Studies field trials in the Arizona desert.\n\nIn 2006, the automotive company General Motors expressed interest in the project and proposed to team up with NASA. In 2007 a Space Act Agreement was signed that allowed GM and NASA to work together on the next generation of Robonaut.\n\nIn February 2010, Robonaut 2 (R2) was revealed to the public. R2 is capable of speeds more than four times faster than R1, is more compact, more dexterous, and includes a deeper and wider range of sensing. It can move its arms up to 2 m/s, has a 40 lb payload capacity and its hands have a grasping force of roughly 5 lbs. per finger. There are over 350 sensors and 38 PowerPC processors in the robot.\n\nStation crew members will be able to operate R2, as will controllers on the ground; both will do so using telepresence. One of the improvements over the previous Robonaut generation is that R2 doesn’t need constant supervision. In anticipation of a future destination in which distance and time delays would make continuous management problematic, R2 was designed to be set to tasks and then carry them through autonomously with periodic status checks. While not all human range of motion and sensitivity has been duplicated, the robot's hand has 12 degrees of freedom as well as 2 degrees of freedom in wrist. The R2 model also uses touch sensors at the tips of its fingers.\n\nR2 was designed as a prototype to be used on Earth but mission managers were impressed by R2 and chose to send it to the ISS. Various upgrades were made to qualify it for use inside the station. The outer skin materials were exchanged to meet the station’s flammability requirements, shielding was added to reduce electromagnetic interference, processors were upgraded to increase the robot’s radiation tolerance, the original fans were replaced with quieter ones to accommodate the station’s noise requirements, and the power system was rewired to run on the station’s direct current system rather than the alternating current used on the ground.\nRobonaut 2 was launched on STS-133 on February 24, 2011, and delivered to the ISS. On August 22, R2 was powered up for the first time while in low earth orbit. This was called a \"power soak\" which is a power system test only with no movement. On October 13, R2 moved for the first time while in space. The conditions aboard the space station provide a proving ground for robots to work shoulder to shoulder with people in microgravity. Once this has been demonstrated inside the station, software upgrades and lower bodies may be added, allowing R2 to move around the interior of the station and perform maintenance tasks, such as vacuuming or cleaning filters. A pair of legs were delivered to the ISS on SpX-3 in April 2014. The battery backpack was planned to be launched on a later flight in Summer/Fall 2014.\nIn the design of the R2 robot, a 3D time of flight imager will be used in conjunction with a stereo camera pair to provide depth information and visible stereo images to the system. This allows the R2 to \"see\", which is one of the basic preconditions to fulfill its tasks. To integrate the various sensor data types in a single development environment the image processing software Halcon 9.0 from MVTec Software (Munich, Germany ) is used.\n\nFurther upgrades could be added to allow R2 to work outside in the vacuum of space, where R2 could help space walkers perform repairs, make additions to the station or conduct scientific experiments. While there were initially no plans to return the launched R2 back to earth, NASA announced on 1 April 2018 that R2 would return to Earth in May 2018 with CRS-14 Dragon for repair and eventual relaunch in about a year's time.\nNASA's experience with R2 on the station will help them understand its capabilities for possible deep space missions.\n\nIn late 2009, a proposed mission called Project M was announced by Johnson Space Center that, if it had been approved, would have had the objective of landing an R2 robot on the Moon within 1,000 days.\n\n\nTechnical Papers\n\n\n"}
{"id": "22406489", "url": "https://en.wikipedia.org/wiki?curid=22406489", "title": "Ryōzō Kanehira", "text": "Ryōzō Kanehira\n\n\n"}
{"id": "11503256", "url": "https://en.wikipedia.org/wiki?curid=11503256", "title": "Science and Technology Education Innovation Center", "text": "Science and Technology Education Innovation Center\n\nThe Science and Technology Education Innovation Center, formerly known as the Science Center of Pinellas County, is an educational center in St. Petersburg, Florida, United States. It occupies of land located on 22nd Avenue North, in West St. Petersburg, located near the Tyrone Mall.\n\nThe Science Center is a non-profit organization founded in St. Petersburg in 1959 by William Guild and Nell Rodgers Croley. It was the first science center of its kind in the world , and it operates on donations and grants. Its mission is to inspire interest in and to promoting the understanding of all sciences.\n\nIn 1966 the present main building was completed. During the 1960s and early 70s, the center offered after-school and weekend classes for students from Kindergarten through middle school in such subjects as biology, chemistry, electronics and astronautics. Today about 22,000 children visit the building each school year during field trips. The Science Center frequently works with Pinellas County schools, and all summer camp classes and science camps taught at the Science Center meet FCAT guidelines. \n\nIn 2000 the Margaret Ewell Dickins Marine Room opened. The exhibit includes numerous aquariums filled with marine creatures such as a snowflake moray and a black sea bass. The most popular feature is a 600-gallon touch tank where visitors can touch starfish, horseshoe crabs, guitarfish, slipper lobsters, hermit crabs and sea urchins. In addition, the Science Center holds marine biology classes for children in this room during science summer camps and school breaks.\n\nIn 1997, the Carol Samuels Observatory opened. The observatory provides visitors with a view of cosmic events and features through a Meade telescope. The research-grade telescope is the only one available to the public in Pinellas County in a non-university setting. The St. Petersburg Astronomy Club holds a handful of events at the Science Center throughout the year, and helps the public use the Meade Telescope to view comets, eclipses, or planets.\n\nThe idea for the White Gardens came from Starley M. White, Chairman of the Board of the former National Bank. Surrounding the White Gardens is a \"Walk of States\", a mosaic tile walkway made up of more than 50 sections, one section for each state in the United States. A segment of the walk is dedicated to the Seminole tribe and Andrew Jackson. Each of the 50 states are displayed in order of their admission into the union. The walkway shows an image of the state, including the state's bird and flower. Beside each state's tile walk the state's rock is on display. The Walk of States was created by St. Petersburg artist Attillio Puglisi. It was moved to the Science Center in 1971.\n\nIn 1971 the Discovery Center was built. This building houses the Columbia Planetarium. In 2003 the Spirit of Columbia Theater and Planetarium opened. The Planetarium offers a showing daily which is included in the $5 admission fee. The Science Center's digital planetarium is the only one available to the public in Pinellas County. It can show the sky as it was on any specific date in history and when the Center is rented out for birthday parties or anniversaries, it is often set to show the night sky as it appeared the night of the event commemorated.\n\nThe Science Center houses the only optical lab for the public to grind telescope mirrors in the southeastern United States. The lab is operated by the St. Petersburg Astronomy Club. The club welcomes the public to visit the optical lab on Saturdays between 1 p.m. and 4 p.m. to build a homemade telescope from scratch.\n\nA small gift shop features science and space-themed toys which are available to purchase at reasonable prices.\n\n"}
{"id": "1359750", "url": "https://en.wikipedia.org/wiki?curid=1359750", "title": "Slime layer", "text": "Slime layer\n\nA slime layer in bacteria is an easily removable (e.g. by centrifugation), unorganized layer of extracellular material that surrounds bacteria cells. Specifically, this consists mostly of exopolysaccharides, glycoproteins, and glycolipids.\n\nThe slime layer is not to be confused with the S-layer, a separate and highly organised glycoprotein layer surrounding many bacterial cells.\n\nThe function of the slime layer is to protect the bacteria cells from environmental dangers such as antibiotics and desiccation. The slime layer also allows bacteria to adhere to smooth surfaces such as prosthetic medical devices and catheters. It may permit bacterial colonies to survive chemical sterilization with chlorine, iodine, and other chemicals, leaving autoclaving or flushing with boiling water as the only certain methods of decontaminating.\n\nA bacterial capsule is similar, but is an organized structure that does not wash off as easily as slime layers.\n\n"}
{"id": "12162794", "url": "https://en.wikipedia.org/wiki?curid=12162794", "title": "Soyuz-A", "text": "Soyuz-A\n\nSergei Korolev initially promoted the Soyuz A-B-V circumlunar complex (\"7K-9K-11K\") concept (also known as L1) in which a two-man craft Soyuz 7K would rendezvous with other components (9K and 11K) in Earth orbit to assemble a lunar excursion vehicle, the components being delivered by the proven R-7 rocket. \n\nBesides the Soyuz 7K spacecraft, the complex would feature a Soyuz 9K booster and a Soyuz 11K tanker with twin whip antennas.\n\nThe 7K would have been equipped with cameras and sensors to study the lunar surface during the flyby, at a distance of 1,000 to 20,000 km from the Moon's surface. Total flight time would have been 7 to 8 days.\n\n"}
{"id": "3720467", "url": "https://en.wikipedia.org/wiki?curid=3720467", "title": "Svalbard Global Seed Vault", "text": "Svalbard Global Seed Vault\n\nThe Svalbard Global Seed Vault () is a secure seed bank on the Norwegian island of Spitsbergen near Longyearbyen in the remote Arctic Svalbard archipelago, about from the North Pole. Conservationist Cary Fowler, in association with the Consultative Group on International Agricultural Research (CGIAR), started the vault to preserve a wide variety of plant seeds that are duplicate samples, or \"spare\" copies, of seeds held in gene banks worldwide. The seed vault is an attempt to insure against the loss of seeds in other genebanks during large-scale regional or global crises. The seed vault is managed under terms spelled out in a tripartite agreement between the Norwegian government, the Crop Trust and the Nordic Genetic Resource Center (NordGen).\n\nThe Norwegian government entirely funded the vault's approximately ( in 2008) construction. Storing seeds in the vault is free to end users, with Norway and the Crop Trust paying for operational costs. Primary funding for the Trust comes from organisations such as the Bill & Melinda Gates Foundation and from various governments worldwide.\n\nThe Nordic Gene Bank (NGB) has, since 1984, stored backup Nordic plant germplasm via frozen seeds in an abandoned coal mine at Svalbard. In January 2008, the Nordic Gene Bank merged with two other Nordic conservation groups to form NordGen. The Svalbard Global Seed Vault officially opened on 26 February 2008, although the first seeds arrived in January 2008. Five percent of the seeds in the vault, about 18,000 samples with 500 seeds each, came from the Centre for Genetic Resources of the Netherlands (CGN), part of Wageningen University, Netherlands.\n\nAs part of the vault's first anniversary, more than 90,000 food crop seed samples were placed into storage, bringing the total number of seed samples to 400,000. Among the new seeds are included 32 varieties of potatoes from Ireland's national gene banks and 20,000 new samples from the U.S. Agricultural Research Service. Other seed samples came from Canada and Switzerland, as well as international seed researchers from Colombia, Mexico and Syria. This shipment brought the total number of seeds stored in the vault to over 20 million. As of this anniversary, the vault contained samples from approximately one-third of the world's most important food crop varieties. Also part of the anniversary, experts on food production and climate change met for a three-day conference in Longyearbyen.\n\nJapanese sculptor Mitsuaki Tanabe () presented a work to the vault named \"The Seed 2009 / Momi In-Situ Conservation\". In 2010 a delegation of seven U.S. congressmen handed over a number of different varieties of chili pepper.\n\nBy 2013, approximately one-third of the genera diversity stored in gene banks globally was represented at the Seed Vault.\n\nIn October 2016, the seed vault experienced an unusually large degree of water intrusion due to higher than average temperatures and heavy rainfall. While it is common for some water to seep into the vault's entrance tunnel during the warmer spring months, in this case the water encroached into the tunnel before freezing. The vault was designed for water intrusion and as such the seeds were not at risk. As a result, however, Norwegian public works agency Statsbygg plans to make improvements to the tunnel to prevent any such intrusion in the future, including waterproofing the tunnel walls, removing heat sources from the tunnel, and digging exterior drainage ditches.\n\nFor the seed vault's 10th anniversary on 26 February 2018 a shipment of 70,000 samples was delivered to the facility, bringing the number of samples received to more than one million (not counting withdrawals). At this time, the total number of samples held at the vault was 967,216, representing over 13,000 years of agricultural history.\n\nNorway, Sweden, Finland, Denmark, and Iceland's prime ministers ceremonially laid \"the first stone\" on 19 June 2006.\n\nThe seedbank is inside a sandstone mountain on Spitsbergen Island, and employs robust security systems. Seeds are packaged in special three-ply foil packets and heat sealed to exclude moisture. The facility is managed by the Nordic Genetic Resource Center, though there are no permanent staff on-site.\n\nSpitsbergen was considered ideal because it lacked tectonic activity and had permafrost, which aids preservation. Its being above sea level will keep the site dry even if the ice caps melt. Locally mined coal provides power for refrigeration units that further cool the seeds to the internationally recommended standard of . If the equipment fails, at least several weeks will elapse before the facility rises to the surrounding sandstone bedrock's temperature of , and is estimated to take two centuries to warm to .\n\nA feasibility study prior to construction determined that the vault could preserve most major food crops' seeds for hundreds of years. Some, including those of important grains, could potentially remain viable for thousands of years.\nRunning the length of the facility's roof and down the front face to the entryway is an illuminated artwork named \"Perpetual Repercussion\" by Norwegian artist Dyveke Sanne that marks the location of the vault from a distance. In Norway, government-funded construction projects exceeding a certain cost must include artwork. , the Norwegian State agency overseeing art in public spaces, engaged the artist to install lighting that highlights the importance and qualities of Arctic light. The roof and vault entrance are filled with highly reflective stainless steel, mirrors, and prisms. The installation reflects polar light in the summer months, while in the winter, a network of 200 fibre-optic cables gives the piece a muted greenish-turquoise and white light.\n\nThe Svalbard Global Seed Vault's mission is to provide a safety net against accidental loss of diversity in traditional genebanks. While the popular press has emphasized its possible utility in the event of a major regional or global catastrophe, it will be more frequently accessed when genebanks lose samples due to mismanagement, accident, equipment failures, funding cuts, and natural disasters. These events occur with some regularity. War and civil strife have a history of destroying some genebanks. The national seed bank of the Philippines was damaged by flooding and later destroyed by a fire; the seed banks of Afghanistan and Iraq have been lost completely. According to \"The Economist\", \"the Svalbard vault is a backup for the world's 1,750 seed banks, storehouses of agricultural biodiversity.\" Norwegian law has prohibited the storing of genetically modified seeds at the vault.\n\nThe adjacent Arctic World Archive provides a similar service for data, which is etched as code into reels of film. Project lead Piql of Norway states that the film, when properly preserved, should last for 1,000 years.\n\nVault seed samples are copies of samples stored in the depositing genebanks. Researchers, plant breeders, and other groups wishing to access seed samples cannot do so through the seed vault; they must instead request samples from the depositing genebanks. The samples stored in the genebanks will, in most cases, be accessible in accordance with the terms and conditions of the International Treaty on Plant Genetic Resources for Food and Agriculture, approved by 118 countries or parties.\n\nThe seed vault functions like a safe deposit box in a bank. The bank owns the building and the depositor owns the contents of his or her box. The Government of Norway owns the facility and the depositing genebanks own the seeds they send. The deposit of samples in Svalbard does not constitute a legal transfer of genetic resources. In genebank terminology this is called a \"black box\" arrangement. Each depositor signs a Deposit Agreement with NordGen, acting on behalf of Norway. The Agreement makes clear that Norway does not claim ownership over the deposited samples and that ownership remains with the depositor, who has the sole right of access to those materials in the seed vault. No one has access to anyone else's seeds from the seed vault. The database of samples and depositors is maintained by NordGen.\n\nThe Syrian Civil War caused another seed bank, the International Center for Agricultural Research in the Dry Areas (ICARDA), to move its headquarters from Aleppo to Beirut. Due to difficulties by ICARDA in transferring its collection, in 2015 the Svalbard Vault authorized the first withdrawal of seeds in its history.\n\nThe seeds are stored in sealed three-ply foil packages, then placed into plastic tote containers on metal shelving racks. The storage rooms are kept at . The low temperature and limited access to oxygen will ensure low metabolic activity and delay seed aging. The permafrost surrounding the facility will help maintain the low temperature of the seeds if the electricity supply fails. In the years since its opening, the vault saw minor water intrusion at its entrance during the annual spring permafrost thawing; warmer temperatures and heavy rainfall in October 2016 caused significantly greater amounts of water to inundate the entrance, but the facility's design ensured that the water froze after several meters and the seeds were not endangered.\n\nThe Crop Trust, officially known as the Global Crop Diversity Trust, plays a key role in the planning of the seed vault and coordinating shipments of seed samples to the Vault in conjunction with the Nordic Genetic Resource Center. The Trust provides most of the annual operating costs for the facility, and has set aside endowment funds to do so, while the Norwegian government finances upkeep of the structure itself. With support from the Bill & Melinda Gates Foundation and other donors, the Crop Trust assists selected genebanks in developing countries as well as the international agricultural research centers in packaging and shipping seeds to the seed vault. An International Advisory Council provides guidance and advice. It includes representatives from the FAO, CGIAR, the International Treaty on Plant Genetic Resources and other institutions.\n\nSvalbard Global Seed Vault ranked at No. 6 on \"Time\"s Best Inventions of 2008. It was awarded the Norwegian Lighting Prize for 2009.\n\nEach seed sample consists of approximately 500 seeds sealed in an airtight aluminum bag. The facility has a storage capacity of 4.5 million seed samples.\n\n"}
{"id": "4551565", "url": "https://en.wikipedia.org/wiki?curid=4551565", "title": "Telexistence", "text": "Telexistence\n\nTelexistence is fundamentally a concept named for the general technology that enables a human being to have a real-time sensation of being at a place other than where he or she actually exists, and being able to interact with the remote environment, which may be real, virtual, or a combination of both. It also refers to an advanced type of teleoperation system that enables an operator at the control to perform remote tasks dexterously with the feeling of existing in a surrogate robot working in a remote environment. Telexistence in the real environment through a virtual environment is also possible. This concept was first proposed by Susumu Tachi in Japan in 1980 and 1981 as patents and the first report was published in Japanese in 1982 and in English in 1984.\n\n"}
{"id": "1030440", "url": "https://en.wikipedia.org/wiki?curid=1030440", "title": "Virino", "text": "Virino\n\nThe virino is a hypothetical infectious particle that was once theorized to be the cause of scrapie and other degenerative diseases of the central nervous system; it was thought to consist of nucleic acids in a protective coat of host cell proteins. The hypothesis was never widely accepted, and the causative agents responsible for these diseases are now widely accepted to be prions.\n\nThe virino was described partially to protect the central dogma of molecular biology, which was threatened by the existence of a series of degenerative neurological TSE diseases including kuru, CJD, scrapie in sheep, and BSE in cattle. The central dogma states that nucleic acids act as the information carriers, and DNA and RNA make proteins. Proteins alone cannot make DNA. However, studies searching for the transmission agent of scrapie and other TSEs have failed to culture bacteria, and tests attacking nucleic acids strands have little effect on the infectivity of TSE solutions. These failures largely rule out a virus as the infective agent. Experiments using electron beams designed to disrupt large molecules have been performed to investigate the size of the agent show that it is very small: much smaller than the smallest known virus.\n\nThe virino also has the benefit of explaining the traits of TSEs which resemble nucleic acids: for example, their occurrence in strains, which positively indicates the TSE agent is information carrying, and not merely a toxin.\n\nIn 1971, Dickinson, AG and Meikle, VM provided a hypothesis for the replication of the scrapie agent. This hypothesis was based on the discovery of a single autosomal gene controlling the scrapie incubation period in mice and on observations about strains of the scrapie agent. They dubbed the gene sinc for scrapie incubator. This hypothesis proposed that the gene products of each sinc allele contributed to a multimeric protein structure, which then formed a 'replication site' for the scrapie agent. The replication of the agent would depend on how the particular strain interacted with the replication site and of what the site was composed. The fact that different strains of scrapie were known had suggested the agent was similar to conventional viruses in that it carried a genome composed of nucleic acids. Thus, variants could arise during incubation, giving rise to new strains. No host-encoded properties were found to determine scrapie agent strain differences. This was thought to prove that the genome of the agent could vary independently and, although replicated by normal host mechanisms, was not coded by the host. The term 'virino' was coined to reflect the small size, immunological neutrality, and virus-like nature of the infectious particles.\n\nThus, in the nucleotide model proposed by Dickinson, AG, and Outram, GW in 1979, the lifecycle of the scrapie agent included a stage where the genome was bound to host protein, probably a multimeric protein complex (now commonly known as a prion), derived from the \"sinc\" gene. Recalling Enrico Fermi's word play on a neutron-like particle, Outram coined the term 'virino' to describe a small virus. In the virino model, the host protein protects the scrapie agent nucleic acids from degradation and prevents the host from raising an immune response, since the protein/nucleic acid complex is seen as a legitimate part of the host. However, the presumed scrapie-associated nucleic acid has not been identified, and physical or chemical evidence for its presence is lacking.\n\nStanley B. Prusiner also discovered a genetic control which he dubbed prii, for prion incubator. This discovery was later shown to be interchangeable with \"sinc\".\n"}
{"id": "57140881", "url": "https://en.wikipedia.org/wiki?curid=57140881", "title": "William King (GSI)", "text": "William King (GSI)\n\nWilliam King FGS (1834? - ?) was the son of the Irish geologist William King who also became a geologist and worked in India with the Geological Survey of India, serving as its director from 1887 to 1894.\n\nKing studied civil engineering at Queen's College, Galway and at Queen's University before joining the Geological Survey of India on 4th March 1857. His first work was in southern India with H.F.Blanford. He then worked in central India and surveyed western Chota Nagpur and took over the position of director at Calcutta, succeeding Medlicott in 1887. He was given six extensions from the normal age of retirement at 54 thus retiring at the age of 60 on 16 July 1894 after 37 years of service. He was succeeded by Carl Griesbach as director as the next senior officer T.W.H. Hughes was injured, losing his eyesight and being forced to retire.\nKing published numerous reports as part of his work in the Geological Survey of India and numerous notes in its Records and Memoirs. The key emphasis was on surveys for coal and mineral resources. A partial list of publications include:\n\n"}
{"id": "14082746", "url": "https://en.wikipedia.org/wiki?curid=14082746", "title": "Win–stay, lose–switch", "text": "Win–stay, lose–switch\n\nIn psychology, game theory, statistics, and machine learning, win–stay, lose–switch (also win–stay, lose–shift) is a heuristic learning strategy used to model learning in decision situations. It was first invented as an improvement over randomization in bandit problems. It was later applied to the prisoner's dilemma in order to model the evolution of altruism.\n\nThe learning rule bases its decision only on the outcome of the previous play. Outcomes are divided into successes (wins) and failures (losses). If the play on the previous round resulted in a success, then the agent plays the same strategy on the next round. Alternatively, if the play resulted in a failure the agent switches to another action.\n\nA large-scale empirical study of players of the game rock, paper, scissors shows that a variation of this strategy is adopted by real-world players of the game, instead of the Nash equilibrium strategy of choosing entirely at random between the three options.\n\n"}
{"id": "17590286", "url": "https://en.wikipedia.org/wiki?curid=17590286", "title": "Yoshiaki Arata", "text": "Yoshiaki Arata\n\nArata started researching and publishing in the field of cold fusion around 1998, together with his colleague Yue Chang Zhang.\n\n\n"}
