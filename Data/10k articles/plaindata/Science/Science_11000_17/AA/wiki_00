{"id": "8217381", "url": "https://en.wikipedia.org/wiki?curid=8217381", "title": "A3220 road", "text": "A3220 road\n\nThe A3220 is a primary A road in London. It runs north from Clapham Common to the A40 Westway at Ladbroke Grove. \n\nThe road crosses the River Thames at Battersea Bridge. Turning left at the northern end of the bridge, the A3220 follows the northern bank of the Thames for some 500 m before swinging northwestward and becoming two separate roads as part of a one-way system. 500 m after crossing the A315, the route returns to being a standard bidirectional single-carriageway road for some 750 m as far as the Holland Park roundabout, from where it forms the dual-carriageway West Cross Route, formerly the M41 motorway.\n"}
{"id": "8219546", "url": "https://en.wikipedia.org/wiki?curid=8219546", "title": "A466 road", "text": "A466 road\n\nThe A466, also known as the Wye Valley Road, is a road from Hereford, England to Chepstow, Wales via Monmouth, Tintern and the Wye Valley.\n\nThe road was largely developed during the late 18th and early 19th centuries by turnpike trusts in Herefordshire and Monmouthshire. It replaced the River Wye as the principal means of transport to Tintern before the construction of the Wye Valley Railway in the late 19th century. The A466 remains an important route for local residents and tourists, and now provides access to the old Severn Bridge on the M48 motorway.\n\nThe A466 is about long and runs through the counties of Herefordshire and Gloucestershire in England, and Monmouthshire in Wales. It runs south from Kingsthorne, on the A49 south east of Hereford to Monmouth, crossing the A40. After crossing the River Wye at the Wye Bridge, Monmouth and Bigsweir Bridge near Llandogo, it follows a picturesque route south down the Wye valley through Tintern and Chepstow to the M48 motorway at junction 2. The road crosses the border between England and Wales at three places along its length: at Buckholt, Redbrook, and Bigsweir. It runs through an Area of Outstanding Natural Beauty, and is popular with tourists, offering good views of the area around the Wye valley.\n\nMost of the road is maintained by the respective county councils except for the southernmost section from Chepstow to the M48, which is a trunk road and funded by the National Assembly for Wales.\n\nBetween Kingsthorne and Monmouth, the road passes through open countryside several miles to the west of the River Wye, and the villages of Wormelow Tump, St Weonards, Llancloudy and Welsh Newton. This part of the road was turnpiked incrementally in short stages in the late eighteenth century and became known as \"The Great Road to the Town of Monmouth\". The initial turnpike in 1730 ran as far as St Weonards, and was extended to Llancloudy in 1769. A tollhouse survives at Monkgate, Monmouth. The Wye Bridge at Monmouth has existed in one form or another since the Middle Ages. The current bridge was built in 1879 by Edwin Seward of Cardiff from red and bluff sandstone. \n\nAfter crossing the river at Monmouth, the road passes through Redbrook, Llandogo, Tintern and St Arvans, in the deeply incised river valley, before reaching Chepstow Racecourse and the town of Chepstow. Until the early nineteenth century, the road between Chepstow and Monmouth passed through St Arvans, Devauden and Trellech (the current B4293); the riverside villages of Llandogo, Brockweir and Tintern, with their associated tourism, metalworking and shipbuilding industries, were more easily accessed by river. The route between Monmouth and Redbrook was part of the main road between Monmouth and Coleford. \n\nThe road between Crossway Green, just north of Chepstow, and St Arvans was improved soon after 1760 by Valentine Morris, the owner of the adjoining Piercefield estate, and again around 1800 by its subsequent owner Mark Wood. In about 1809, proposals were made to cut a new road along the valley, to improve access to Tintern in particular. Little progress was made until 1824, when legislation was passed \"for Making a Turnpike Road from Redbrook to St Arvans\" and a bridge at Bigsweir, under the authority of the Monmouth Turnpike Trust. The new road was constructed between St Arvans and Tintern in 1825, and by 1829 the road had been opened along its whole length. Bigsweir Bridge was opened in 1827 as part of the new road. Two turnpike houses from this period are still standing; one at St Arvans is Grade II listed, while another is by Bigsweir Bridge. The trusts were wound up in 1873, with control ultimately passing to the relevant county councils. The turnpike was superseded in 1876 by the Wye Valley Railway, which led to a huge increase in tourist traffic to the valley and Tintern.\n\nThe southernmost section of the A466 is the Wye Valley Link Road, which was a project built in conjunction with the first Severn Bridge. It opened to traffic in 1963. The remainder of the road north of this is under local government control; the Ministry of Transport dismissed calls in the late 1960s for them to improve the road to deal with increasing congestion.\n\n"}
{"id": "44433562", "url": "https://en.wikipedia.org/wiki?curid=44433562", "title": "AAAS Philip Hauge Abelson Prize", "text": "AAAS Philip Hauge Abelson Prize\n\nThe AAAS Philip Hauge Abelson Prize is awarded by The American Association for the Advancement of Science for public servants, recognized for sustained exceptional contributions to advancing science or scientists, whose career has been distinguished both for scientific achievement and for other notable services to the scientific community. The award consists of an engraved medallion and an honorarium of $5,000.\n\n"}
{"id": "46736509", "url": "https://en.wikipedia.org/wiki?curid=46736509", "title": "Advanced Propulsion Physics Laboratory", "text": "Advanced Propulsion Physics Laboratory\n\nThe Advanced Propulsion Physics Laboratory at NASA's Johnson Space Center, also known as \"Eagleworks Laboratories\", is a small research group investigating a variety of theories regarding new forms of spacecraft propulsion. Their principal investigator is Dr. Harold G. White.\n\nThe group's research includes development of the White–Juday warp-field interferometer for observing small disturbances of spacetime, and testing small prototypes such as RF resonant cavity thrusters and quantum vacuum plasma thrusters.\n\n"}
{"id": "4489461", "url": "https://en.wikipedia.org/wiki?curid=4489461", "title": "Akihiro Kusumi", "text": "Akihiro Kusumi\n\n\n"}
{"id": "8369300", "url": "https://en.wikipedia.org/wiki?curid=8369300", "title": "All-or-none law", "text": "All-or-none law\n\nThe all-or-none law is the principle that the strength by which a nerve or muscle fibre responds to a stimulus is independent of the strength of the stimulus. If that stimulus exceeds the threshold potential, the nerve or muscle fibre will give a complete response; otherwise, there is no response. \n\nIt was first established by the American physiologist Henry Pickering Bowditch in 1871 for the contraction of heart muscle. According to him, describing the relation of response to stimulus,““An induction shock produces a contraction or fails to do so according to its strength; if it does so at all, it produces the greatest contraction that can be produced by any strength of stimulus in the condition of the muscle at the time.”The individual fibres of both skeletal muscle and nerve respond to stimulation according to the all-or-none principle.\n\nThe magnitude of the action potential set up in any single nerve fibre is independent of the strength of the exciting stimulus, provided the latter is adequate. An electrical stimulus below threshold strength fails to elicit a propagated spike potential. If it is of threshold strength or over, a spike (a nervous impulse) of maximum magnitude is set up. Either the single fibre does not respond with spike production, or it responds to the utmost of its ability under the conditions at the moment. This property of the single nerve fibre is termed the all-or-none relationship. This relationship holds only for the unit of tissue; for nervous tissue the unit is the nerve cell, for skeletal muscle the unit is the individual muscle fiber and for the heart the unit is the entire auricles or the entire ventricles.\n\nStimuli too weak to produce a spike do, however, set up a local electrotonus, the magnitude of the electronic potential progressively increasing with the strength of the stimulus, until a spike is generated. This demonstrates the all-or-none relationship in spike production.\n\nThe above account deals with the response of a single nerve fibre. If a nerve trunk is stimulated, then as the exciting stimulus is progressively increased above a threshold, a larger number of fibres respond. The minimal effective (i.e., threshold) stimulus is adequate only for fibres of high excitability, but a stronger stimulus excites all the nerve fibres. Increasing the stimulus further does increase the response of whole nerve.\n\nHeart muscle is excitable, i.e., it responds to external stimuli by contracting. If the external stimulus is too weak, no response is obtained; if the stimulus is adequate, the heart responds to the best of its ability. Accordingly, the auricles or ventricles behave as a single unit, so that an adequate stimulus normally produces a full contraction of either the auricles or ventricles. The force of the contraction obtained depends on the state in which the muscles fibres find themselves. In the case of muscle fibres, the individual muscle fibre does not respond at all if the stimulus is too weak. However, it responds maximally when the stimulus rises to threshold. The contraction is not increased if the stimulus strength is further raised. Stronger stimuli bring more muscle fibres into action and thus the tension of a muscle increases as the strength of the stimulus applied to it rises.\n"}
{"id": "9539347", "url": "https://en.wikipedia.org/wiki?curid=9539347", "title": "Allomerism", "text": "Allomerism\n\nAllomerism is the similarity in the crystalline structure of substances of different chemical composition.\n\n"}
{"id": "7396128", "url": "https://en.wikipedia.org/wiki?curid=7396128", "title": "Asset turnover", "text": "Asset turnover\n\nAsset turnover (ATO) is a financial ratio that measures the efficiency of a company's use of its assets in generating sales revenue or sales income to the company.\n\nCompanies with low profit margins tend to have high asset turnover, while those with high profit margins have low asset turnover. Companies in the retail industry tend to have a very high turnover ratio due mainly to cutthroat and competitive pricing.\n\nformula_1\n\n"}
{"id": "2223114", "url": "https://en.wikipedia.org/wiki?curid=2223114", "title": "Baryon asymmetry", "text": "Baryon asymmetry\n\nIn physics, the baryon asymmetry problem, also known as the matter asymmetry problem or the matter-antimatter asymmetry problem, is the observed imbalance in baryonic matter (the type of matter experienced in everyday life) and antibaryonic matter in the observable universe. Neither the standard model of particle physics, nor the theory of general relativity provides a known explanation for why this should be so, and it is a natural assumption that the universe be neutral with all conserved charges. The Big Bang should have produced equal amounts of matter and antimatter. Since this does not seem to have been the case, it is likely some physical laws must have acted differently or did not exist for matter and antimatter.\nSeveral competing hypotheses exist to explain the imbalance of matter and antimatter that resulted in baryogenesis. However, there is as of yet no consensus theory to explain the phenomenon. As remarked in a 2012 research paper, \"The origin of matter remains one of the great mysteries in physics.\"\n\nIn 1967, Andrei Sakharov proposed a set of three necessary conditions that a baryon-generating interaction must satisfy to produce matter and antimatter at different rates. These conditions were inspired by the recent discoveries of the cosmic background radiation and CP-violation in the neutral kaon system. The three necessary \"Sakharov conditions\" are:\n\nBaryon number violation is obviously a necessary condition to produce an excess of baryons over anti-baryons. But C-symmetry violation is also needed so that the interactions which produce more baryons than anti-baryons will not be counterbalanced by interactions which produce more anti-baryons than baryons. CP-symmetry violation is similarly required because otherwise equal numbers of left-handed baryons and right-handed anti-baryons would be produced, as well as equal numbers of left-handed anti-baryons and right-handed baryons. Finally, the interactions must be out of thermal equilibrium, since otherwise CPT symmetry would assure compensation between processes increasing and decreasing the baryon number.\n\nCurrently, there is no experimental evidence of particle interactions where the conservation of baryon number is broken perturbatively: this would appear to suggest that all observed particle reactions have equal baryon number before and after. Mathematically, the commutator of the baryon number quantum operator with the (perturbative) Standard Model hamiltonian is zero: formula_2. However, the Standard Model is known to violate the conservation of baryon number only non-perturbatively: a global U(1) anomaly. To account for baryon violation in baryogenesis, such events (including proton decay) can occur in Grand Unification Theories (GUTs) and supersymmetric (SUSY) models via hypothetical massive bosons such as the X boson.\n\nThe second condition for generating baryon asymmetry – violation of charge-parity symmetry – is that a process is able to happen at a different rate to its antimatter counterpart. In the Standard Model, CP violation appears as a complex phase in the quark mixing matrix of the weak interaction. There may also be a non-zero CP-violating phase in the neutrino mixing matrix, but this is currently unmeasured. CP violation was first observed in the 1964 Fitch-Cronin experiment with neutral kaons, which resulted in the 1980 Nobel Prize in physics (direct CP-violation, that is violation of CP-symmetry in a decay process, was discovered later, in 1999). Due to CPT symmetry, violation of CP-symmetry demands violation of time inversion symmetry, or T-symmetry. Despite the allowance for CP-violation in the Standard Model, it is insufficient to account for the observed baryon asymmetry of the universe given the limits on baryon number violation, meaning that beyond-Standard Model sources are needed.\n\nA possible new source of CP violation was found at the Large Hadron Collider (LHC) by the LHCb collaboration during the first three years of LHC operations (beginning March 2010). The experiment analyzed the decays of two particles, the bottom Lambda (Λ) and its antiparticle, and compared the distributions of decay products. The data showed an asymmetry of up to 20% of CP-violation sensitive quantities, implying a breaking of CP-symmetry. This analysis will need to be confirmed by more data from subsequent runs of the LHC.\n\nIn the out-of-equilibrium decay scenario, the last condition states that the rate of a reaction which generates baryon-asymmetry must be less than the rate of expansion of the universe. In this situation the particles and their corresponding antiparticles do not achieve thermal equilibrium due to rapid expansion decreasing the occurrence of pair-annihilation.\n\nAnother possible explanation of the apparent baryon asymmetry is that matter and antimatter are essentially separated into different, widely separated regions of the universe. The formation of antimatter galaxies was originally thought to explain the baryon asymmetry, as from a distance, antimatter atoms are indistinguishable from matter atoms; both produce light (photons) in the same way. Along the boundary between matter and antimatter regions, however, annihilation (and the subsequent production of gamma radiation) would be detectable, depending on its distance and the density of matter and antimatter. Such boundaries, if they exist, would likely lie in deep intergalactic space. The density of matter in intergalactic space is reasonably well established at about one atom per cubic meter. Assuming this is a typical density near a boundary, the gamma ray luminosity of the boundary interaction zone can be calculated. No such zones have been detected, but 30 years of research have placed bounds on how far they might be. On the basis of such analyses, it is now deemed unlikely that any region within the observable universe is dominated by antimatter.\n\nOne attempt to explain the lack of observable interfaces between matter and antimatter dominated regions is that they are separated by a Leidenfrost layer of very hot matter created by the energy released from annihilation. This is similar to the manner in which water may be separated from a hot plate by a layer of evaporated vapor, delaying the evaporation of more water.\n\nThe presence of an electric dipole moment (EDM) in any fundamental particle would violate both parity (P) and time (T) symmetries. As such, an EDM would allow matter and antimatter to decay at different rates leading to a possible matter-antimatter asymmetry as observed today. Many experiments are currently being conducted to measure the EDM of various physical particles. All measurements are currently consistent with no dipole moment. However, the results do place rigorous constraints on the amount of symmetry violation that a physical model can permit. The most recent EDM limit, published in 2014, was that of the ACME Collaboration, which measured the EDM of the electron using a pulsed beam of thorium monoxide (ThO) molecules.\n\nThe challenges to the physics theories are then to explain \"how\" to produce this preference of matter over antimatter, and also the \"magnitude\" of this asymmetry. An important quantifier is the \"asymmetry parameter\",\nThis quantity relates the overall number density difference between baryons and antibaryons (n and n, respectively) and the number density of cosmic background radiation photons n.\n\nAccording to the Big Bang model, matter decoupled from the cosmic background radiation (CBR) at a temperature of roughly kelvin, corresponding to an average kinetic energy of / () = . After the decoupling, the \"total\" number of CBR photons remains constant. Therefore, due to space-time expansion, the photon density decreases. The photon density at equilibrium temperature T per cubic centimeter, is given by\nwith k as the Boltzmann constant, ħ as the Planck constant divided by 2π and c as the speed of light in vacuum, and ζ(3) as\nApéry's constant. At the current CBR photon temperature of , this corresponds to a photon density n of around 411 CBR photons per cubic centimeter.\n\nTherefore, the asymmetry parameter η, as defined above, is \"not\" the \"good\" parameter. Instead, the preferred asymmetry parameter uses the entropy density s,\nbecause the entropy density of the universe remained reasonably constant throughout most of its evolution. The entropy density is\nwith p and ρ as the pressure and density from the energy density tensor T, and g as the effective number of degrees of freedom for \"massless\" particles (inasmuch as mc ≪ kT holds) at temperature T,\nfor bosons and fermions with g and g degrees of freedom at temperatures T and T respectively. At the present era, s = .\n\n"}
{"id": "5741744", "url": "https://en.wikipedia.org/wiki?curid=5741744", "title": "Bovine herpesvirus 5", "text": "Bovine herpesvirus 5\n\nBovine herpesvirus 5 is a virus species of the genus \"Varicellovirus\" and subfamily Alphaherpesvirinae. It causes meningoencephalitis and respiratory disease in cattle and sheep. As with all herpes viruses latent infection can occur, with recrudescence at times of stressed and/or immunosuppression. Sites of latency include the CNS and mucosae of the nose and trachea. The disease has been documented in South America, the United States, Australia, Germany and Hungary. Caused by: BHV-5 — Bovine Encephalitis Virus — Bovine Encephalitis Herpesvirus\n\nDisease is most common in calves up to ten months of age.\n\nSigns of respiratory disease include tachycardia and tachypnea with pyrexia, dyspnea, mucoid nasal discharge, hypersalivation and abnormal lung sounds. Systemic signs such as lethargy and anorexia are seen.\n\nNeurological signs are normally acute. These signs include opisthotonus, hyperaesthesia, abnormal behaviour, ataxia, head pressing, blindness, proprioceptive deficits, coma and seizures. Sudden death occurs in neonates. Subacute disease almost always fatal, causing depression, anorexia, ataxia and a pronounced dyspnea.\n\nAnimals that recover from the infection or become infected following BHV-1 infection become latent carriers.\n\nTo diagnose infection, the virus is identified using specific monoclonal antibodies, PCR or ELISA. Neurological lesions should be identifiable on postmortem examination.\n\nThere is currently no treatment or specific vaccine for BHV-5, but BHV-1 vaccines seem to provide some cross-protection.\n\n\n\n"}
{"id": "7346", "url": "https://en.wikipedia.org/wiki?curid=7346", "title": "Centimetre–gram–second system of units", "text": "Centimetre–gram–second system of units\n\nThe centimetre–gram–second system of units (abbreviated CGS or cgs) is a variant of the metric system based on the centimetre as the unit of length, the gram as the unit of mass, and the second as the unit of time. All CGS mechanical units are unambiguously derived from these three base units, but there are several different ways of extending the CGS system to cover electromagnetism.\n\nThe CGS system has been largely supplanted by the MKS system based on the metre, kilogram, and second, which was in turn extended and replaced by the International System of Units (SI). In many fields of science and engineering, SI is the only system of units in use but there remain certain subfields where CGS is prevalent.\n\nIn measurements of purely mechanical systems (involving units of length, mass, force, energy, pressure, and so on), the differences between CGS and SI are straightforward and rather trivial; the unit-conversion factors are all powers of 10 as and . For example, the CGS unit of force is the dyne which is defined as , so the SI unit of force, the newton (), is equal to 100,000 dynes.\n\nOn the other hand, in measurements of electromagnetic phenomena (involving units of charge, electric and magnetic fields, voltage, and so on), converting between CGS and SI is more subtle. Formulas for physical laws of electromagnetism (such as Maxwell's equations) need to be adjusted depending on which system of units one uses. This is because there is no one-to-one correspondence between electromagnetic units in SI and those in CGS, as is the case for mechanical units. Furthermore, within CGS, there are several plausible choices of electromagnetic units, leading to different unit \"sub-systems\", including Gaussian units, \"ESU\", \"EMU\", and Lorentz–Heaviside units. Among these choices, Gaussian units are the most common today, and \"CGS units\" often used specifically refers to CGS-Gaussian units.\n\nThe CGS system goes back to a proposal in 1832 by the German mathematician Carl Friedrich Gauss to base a system of absolute units on the three fundamental units of length, mass and time. Gauss chose the units of millimetre, milligram and second. In 1873, a committee of the British Association for the Advancement of Science, including British physicists James Clerk Maxwell and William Thomson recommended the general adoption of centimetre, gram and second as fundamental units, and to express all derived electromagnetic units in these fundamental units, using the prefix \"C.G.S. unit of ...\".\n\nThe sizes of many CGS units turned out to be inconvenient for practical purposes. For example, many everyday objects are hundreds or thousands of centimetres long, such as humans, rooms and buildings. Thus the CGS system never gained wide general use outside the field of science. Starting in the 1880s, and more significantly by the mid-20th century, CGS was gradually superseded internationally for scientific purposes by the MKS (metre–kilogram–second) system, which in turn developed into the modern SI standard.\n\nSince the international adoption of the MKS standard in the 1940s and the SI standard in the 1960s, the technical use of CGS units has gradually declined worldwide, in the United States more slowly than elsewhere. CGS units are today no longer accepted by the house styles of most scientific journals, textbook publishers, or standards bodies, although they are commonly used in astronomical journals such as \"The Astrophysical Journal\". CGS units are still occasionally encountered in technical literature, especially in the United States in the fields of material science, electrodynamics and astronomy. The continued usage of CGS units is most prevalent in magnetism and related fields because the B and H fields have the same units in free space and there is a lot of potential for confusion when converting published measurements from cgs to MKS.\n\nThe units gram and centimetre remain useful \"as prefixed units\" within the SI system, especially for instructional physics and chemistry experiments, where they match the small scale of table-top setups. However, where derived units are needed, the SI ones are generally used and taught instead of the CGS ones today. For example, a physics lab course might ask students to record lengths in centimetres, and masses in grams, but force (a derived unit) in newtons, a usage consistent with the SI system.\n\nIn mechanics, the CGS and SI systems of units are built in an identical way. The two systems differ only in the scale of two out of the three base units (centimetre versus metre and gram versus kilogram, respectively), while the third unit (second as the unit of time) is the same in both systems.\n\nThere is a one-to-one correspondence between the base units of mechanics in CGS and SI, and the laws of mechanics are not affected by the choice of units. The definitions of all derived units in terms of the three base units are therefore the same in both systems, and there is an unambiguous one-to-one correspondence of derived units:\n\nThus, for example, the CGS unit of pressure, barye, is related to the CGS base units of length, mass, and time in the same way as the SI unit of pressure, pascal, is related to the SI base units of length, mass, and time:\n\nExpressing a CGS derived unit in terms of the SI base units, or vice versa, requires combining the scale factors that relate the two systems:\n\nThe conversion factors relating electromagnetic units in the CGS and SI systems are made more complex by the differences in the formulae expressing physical laws of electromagnetism as assumed by each system of units, specifically in the nature of the constants that appear in these formulae. This illustrates the fundamental difference in the ways the two systems are built: \n\nElectromagnetic relationships to length, time and mass may be derived by several equally appealing methods. Two of them rely on the forces observed on charges. Two fundamental laws relate (seemingly independently of each other) the electric charge or its rate of change (electric current) to a mechanical quantity such as force. They can be written in system-independent form as follows:\n\n\nMaxwell's theory of electromagnetism relates these two laws to each other. It states that the ratio of proportionality constants formula_10 and formula_14 must obey formula_17, where \"c\" is the speed of light in vacuum. Therefore, if one derives the unit of charge from the Coulomb's law by setting formula_18 then Ampère's force law will contain a prefactor formula_19. Alternatively, deriving the unit of current, and therefore the unit of charge, from the Ampère's force law by setting formula_20 or formula_21, will lead to a constant prefactor in the Coulomb's law.\n\nIndeed, both of these mutually exclusive approaches have been practiced by the users of CGS system, leading to the two independent and mutually exclusive branches of CGS, described in the subsections below. However, the freedom of choice in deriving electromagnetic units from the units of length, mass, and time is not limited to the definition of charge. While the electric field can be related to the work performed by it on a moving electric charge, the magnetic force is always perpendicular to the velocity of the moving charge, and thus the work performed by the magnetic field on any charge is always zero. This leads to a choice between two laws of magnetism, each relating magnetic field to mechanical quantities and electric charge:\nThese two laws can be used to derive Ampère's force law above, resulting in the relationship: formula_25. Therefore, if the unit of charge is based on the Ampère's force law such that formula_26, it is natural to derive the unit of magnetic field by setting formula_27. However, if it is not the case, a choice has to be made as to which of the two laws above is a more convenient basis for deriving the unit of magnetic field.\n\nFurthermore, if we wish to describe the electric displacement field D and the magnetic field H in a medium other than vacuum, we need to also define the constants ε and μ, which are the vacuum permittivity and permeability, respectively. Then we have (generally) formula_28 and formula_29, where P and M are polarization density and magnetization vectors. The units of P and M are usually so chosen that the factors λ and λ′ are equal to the \"rationalization constants\" formula_30 and formula_31, respectively. If the rationalization constants are equal, then formula_32. If they are equal to one, then the system is said to be \"rationalized\": the laws for systems of spherical geometry contain factors of 4π (for example, point charges), those of cylindrical geometry – factors of 2π (for example, wires), and those of planar geometry contain no factors of π (for example, parallel-plate capacitors). However, the original CGS system used λ = λ′ = 4π, or, equivalently, formula_33. Therefore, Gaussian, ESU, and EMU subsystems of CGS (described below) are not rationalized.\n\nThe table below shows the values of the above constants used in some common CGS subsystems:\nThe constant \"b\" in SI system is a unit-based scaling factor defined as: formula_34.\n\nAlso, note the following correspondence of the above constants to those in Jackson and Leung:\n\nIn system-independent form, Maxwell's equations can be written as:\n\nformula_39\n\nNote that of all these variants, only in Gaussian and Heaviside–Lorentz systems formula_40 equals formula_41 rather than 1. As a result, vectors formula_42 and formula_43 of an electromagnetic wave propagating in vacuum have the same units and are equal in magnitude in these two variants of CGS.\n\nIn one variant of the CGS system, Electrostatic units (ESU), charge is defined via the force it exerts on other charges, and current is then defined as charge per time. It is done by setting the Coulomb force constant formula_44, so that Coulomb's law does not contain an explicit prefactor.\n\nThe ESU unit of charge, franklin (Fr), also known as statcoulomb or esu charge, is therefore defined as follows: Therefore, in electrostatic CGS units, a franklin is equal to a centimetre times square root of dyne:\nThe unit of current is defined as:\n\nDimensionally in the ESU CGS system, charge \"q\" is therefore equivalent to mLt. Hence, neither charge nor current is an independent physical quantity in ESU CGS. This reduction of units is the consequence of the Buckingham π theorem.\n\nAll electromagnetic units in ESU CGS system that do not have proper names are denoted by a corresponding SI name with an attached prefix \"stat\" or with a separate abbreviation \"esu\".\n\nIn another variant of the CGS system, electromagnetic units (EMU), current is defined via the force existing between two thin, parallel, infinitely long wires carrying it, and charge is then defined as current multiplied by time. (This approach was eventually used to define the SI unit of ampere as well). In the EMU CGS subsystem, this is done by setting the Ampere force constant formula_47, so that Ampère's force law simply contains 2 as an explicit prefactor (this prefactor 2 is itself a result of integrating a more general formulation of Ampère's law over the length of the infinite wire).\n\nThe EMU unit of current, biot (Bi), also known as abampere or emu current, is therefore defined as follows:\nThe unit of charge in CGS EMU is:\n\nDimensionally in the EMU CGS system, charge \"q\" is therefore equivalent to mL. Hence, neither charge nor current is an independent physical quantity in EMU CGS.\n\nAll electromagnetic units in EMU CGS system that do not have proper names are denoted by a corresponding SI name with an attached prefix \"ab\" or with a separate abbreviation \"emu\".\n\nThe ESU and EMU subsystems of CGS are connected by the fundamental relationship formula_17 (see above), where \"c\" = 29,979,245,800 ≈ 3⋅10 is the speed of light in vacuum in centimetres per second. Therefore, the ratio of the corresponding \"primary\" electrical and magnetic units (e.g. current, charge, voltage, etc. – quantities proportional to those that enter directly into Coulomb's law or Ampère's force law) is equal either to \"c\" or \"c\":\nand\nUnits derived from these may have ratios equal to higher powers of \"c\", for example:\n\nThe practical cgs system is a hybrid system that uses the volt and the ampere as the unit of voltage and current respectively. Doing this avoids the inconveniently large and small quantities that arise for electromagnetic units in the esu and emu systems. This system was at one time widely used by electrical engineers because the volt and amp had been adopted as international standard units by the International Electrical Congress of 1881. As well as the volt and amp, the farad (capacitance), ohm (resistance), coulomb (electric charge), and henry are consequently also used in the practical system and are the same as the SI units. However, intensive properties (that is, anything that is per unit length, area, or volume) will not be the same as SI since the cgs unit of distance is the centimetre. For instance electric field strength is in units of volts per centimetre, magnetic field strength is in oersteds and resistivity is in ohm-cm.\n\nSome physicists and electrical engineers in North America still use these hybrid units.\n\nThere were at various points in time about half a dozen systems of electromagnetic units in use, most based on the CGS system. These also include the Gaussian units and the Heaviside–Lorentz units.\n\nIn this table, \"c\" = 29,979,245,800 is the numeric value of the speed of light in vacuum when expressed in units of centimetres per second. The symbol \"↔\" is used instead of \"=\" as a reminder that the SI and CGS units are \"corresponding\" but not \"equal\" because they have incompatible dimensions. For example, according to the next-to-last row of the table, if a capacitor has a capacitance of 1 F in SI, then it has a capacitance of (10 \"c\") cm in ESU; \"but\" it is usually incorrect to replace \"1 F\" with \"(10 \"c\") cm\" within an equation or formula. (This warning is a special aspect of electromagnetism units in CGS. By contrast, for example, it is \"always\" correct to replace \"1 m\" with \"100 cm\" within an equation or formula.)\n\nOne can think of the SI value of the Coulomb constant \"k\" as:\nThis explains why SI to ESU conversions involving factors of \"c\" lead to significant simplifications of the ESU units, such as 1 statF = 1 cm and 1 statΩ = 1 s/cm: this is the consequence of the fact that in ESU system \"k\" = 1. For example, a centimetre of capacitance is the capacitance of a sphere of radius 1 cm in vacuum. The capacitance \"C\" between two concentric spheres of radii \"R\" and \"r\" in ESU CGS system is:\nBy taking the limit as \"R\" goes to infinity we see \"C\" equals \"r\".\n\nWhile the absence of explicit prefactors in some CGS subsystems simplifies some theoretical calculations, it has the disadvantage that sometimes the units in CGS are hard to define through experiment. Also, lack of unique unit names leads to a great confusion: thus “15 emu” may mean either 15 abvolts, or 15 emu units of electric dipole moment, or 15 emu units of magnetic susceptibility, sometimes (but not always) per gram, or per mole. On the other hand, SI starts with a unit of current, the ampere, that is easier to determine through experiment, but which requires extra multiplicative factors in the electromagnetic equations. With its system of uniquely named units, the SI also removes any confusion in usage: 1.0 ampere is a fixed value of a specified quantity, and so are 1.0 henry, 1.0 ohm, and 1.0 volt.\n\nA key virtue of the Gaussian CGS system is that electric and magnetic fields have the same units, 4\"πε\" is replaced by 1, and the only dimensional constant appearing in the Maxwell equations is \"c\", the speed of light. The Heaviside–Lorentz system has these desirable properties as well (with \"ε\" equaling 1), but it is a “rationalized” system (as is SI) in which the charges and fields are defined in such a way that there are many fewer factors of 4\"π\" appearing in the formulas, and it is in Heaviside–Lorentz units that the Maxwell equations take their simplest form.\n\nIn SI, and other rationalized systems (for example, Heaviside–Lorentz), the unit of current was chosen such that electromagnetic equations concerning charged spheres contain 4π, those concerning coils of current and straight wires contain 2π and those dealing with charged surfaces lack π entirely, which was the most convenient choice for applications in electrical engineering. However, modern hand calculators and personal computers have eliminated this \"advantage\". In some fields where formulas concerning spheres are common (for example, in astrophysics), it has been argued that the nonrationalized CGS system can be somewhat more convenient notationally.\n\nSpecialized unit systems are used to simplify formulas even further than \"either\" SI \"or\" CGS, by eliminating constants through some system of natural units. For example, in particle physics a system is in use where every quantity is expressed by only one unit of energy, the electronvolt, with lengths, times, and so on all converted into electronvolts by inserting factors of speed of light \"c\" and the Planck constant \"ħ\". This unit system is very convenient for calculations in particle physics, but it would be considered impractical in other contexts.\n\n\n"}
{"id": "27754656", "url": "https://en.wikipedia.org/wiki?curid=27754656", "title": "Clinical and Translational Science Award", "text": "Clinical and Translational Science Award\n\nClinical and Translational Science Award (CTSA) is a type of U.S. federal grant administered by the National Center for Advancing Translational Sciences, part of the National Institutes of Health. The CTSA program began in October 2006 under the auspices of the National Center for Research Resources with a consortium of 12 academic health centers. The program was fully implemented in 2012, comprising 60 grantee institutions and their partners.\n\nThe CTSA program helps institutions create an integrated academic home for clinical and translational science with the resources to support researchers and research teams working to apply new knowledge and techniques to patient care. The program is structured to encourage collaborations among researchers from different scientific fields.\n\nThe CTSA program has raised awareness of clinical and translational science as a discipline among academic and industry researchers, philanthropists, government officials and the broader public.\n\nCTSA consortium leaders have set five broad goals to guide their activities. These include building national clinical and translational research capability, providing training and improving career development of clinical and translational scientists, enhancing consortium-wide collaborations, improving the health of U.S. communities and the nation, and advancing T1 translational research to move basic laboratory discoveries and knowledge into clinical testing.\n\nInstitutions funded by the CTSA program are working with other research facilities to improve drug discovery and development. For example, several consortium institutions are collaborating with the Rat Resource and Research Center at the University of Missouri to increase the speed of drug screening so that drug research is translated into clinical uses more quickly.\nConsortium institutions also are creating new fields of study or new uses for technologies. For example, researchers at the University of Rochester are pioneering the field of lipidomics, exploring how lipids affect human disease. Their work has led to lipid research collaborations among experts in community and preventive medicine, proteomics, nutrition, and pharmaceutical research.\n\nSome CTSA institutions are collaborating with community-based organizations to ensure research is translated successfully into clinical practice. Researchers at Duke University are working to prevent strokes by partnering with a local health care program to build stroke awareness among Latino immigrants.\n\nOthers are pursuing public and private partnerships to speed innovation. For example, the Oregon Health and Science University and Intel are developing new wireless devices with sensors to detect symptoms in patients who have diabetes or those at high risk of stroke so they can be treated earlier.\n\nWith the most recent awards, announced in July 2011, the consortium comprises 60 institutions in 30 states and the District of Columbia. These include:\n\n\nOn the 20 December 2011, the OIG published a report critical of the NIH's administration of the Clinical and Translational Science Awards (CTSA) program. The report read in part:\n\n"}
{"id": "28751590", "url": "https://en.wikipedia.org/wiki?curid=28751590", "title": "Community-driven development", "text": "Community-driven development\n\nCommunity-driven development (CDD) is a development initiative that provides control of the development process, resources and decision making authority directly to groups in the community. The underlying assumption of CDD projects are that communities are the best judges of how their lives and livelihoods can be improved and, if provided with adequate resources and information, they can organize themselves to provide for their immediate needs. CDD projects work by providing poor communities with direct funding for development with the communities then deciding how to spend the money. Lastly, the community plans and builds the project and takes responsibility for monitoring its progress.\n\nCDD programmes are motivated by their trust in people (Naidoo and Finn, 2001) and hence it advocates people changing their own environment as a powerful force for development.\n\nBy treating poor people as assets and partners in the development process, studies have shown that CDD is responsive to local demands, inclusive, and more cost-effective compared to centrally-led NGO-based programmes. CDD can also be supported by strengthening and financing community groups, facilitating community access to information, and promoting an enabling environment through policy and institutional reform (Dongier, 2002).\n\nFollowing from this description, field practitioners at the World Bank have denoted five key characteristics of CDD projects.\n\nCommunity-driven development is derived from community-based development (CBD) which can include a much broader range of projects. For example, CBD projects can include everything from simple information sharing to social, economic and political empowerment of community groups. However, CDD projects fit on the empowerment end of CBD by actively engaging beneficiaries in the design, management and implementation of projects. The stress on actual control of decision-making and project resources at nearly all stages of a subproject cycle distinguishes CDD from the previous generation of CBD projects. In this continuum of community participation covered by CBD, new-generation CDD projects are located at the extreme right (Tanaka, 2006).\n\nSince community-driven development has only recently diverged from the broad community-based development there are a few contrasts visible in the five characteristics of CDD programmes. In essence, all five properties of CDD projects exist together only in the newer generation of CDD implementations. Nevertheless, the first attribute of community focus would apply to all CDD projects and CBD projects. In contrast, the second characteristic of participatory planning and design and the fourth property of community involvement are often visible among all CDD projects but very rarely in CBD projects. Moreover, community-based monitoring and evaluation which is the fifth aspect of CDD projects is only found in some of the newer projects. The fifth characteristic is what positions many of the newer CDD projects in the extreme right of the CDD cluster as diagrammatically demonstrated in Figure 1. As mentioned above, the third characteristic of community control of resources seems to be the key factor to conceptually distinguish between CDD and CBD projects. However, many of the early NGOs implementing CDD projects did not always interpret this factor rigorously (Tanaka, 2006). Thus, the distinction between CDD projects and CBD projects with CDD components was not always clear; however, this would be expected since there was a gradual evolution of CDD out of CBD.\n\nTo alleviate the earlier problems of over-reliance on central governments as the main service provider, CDD programs were launched by the World Bank to improve the accountability and services in key areas. However, NGOs quickly learned that well designed and implemented CDD programmes had ripple effects of promoting equity and inclusiveness, efficiency and good governance. By effectively targeting and including the vulnerable and excluded groups, as well as allowing communities to manage and control resources directly it was evident that CDD programs could allow poverty reduction projects to scale up quickly. Efficiency is gained through demand responsive allocation of resources, reduced corruption and misuse of resources, lower costs and better cost recovery, better quality and maintenance, greater utilization of resources, and the community‘s willingness to pay for goods and services. Good governance is promoted by greater transparency, accountability in allocation and use of resources because the community participates in project decision-making processes. Some of the principles of CDD—such as participation, empowerment, accountability, and nondiscrimination—are also worthy ends in themselves (Asian Development Bank, 2008).\n\nIt was as early as 1881 when T.H. Green who wrote about the maximum power for all members of human society alike to make the best of themselves (Zakaria, 1999). However, it was not until the 1970s with John Rawls’ book ―A Theory of Justice and in the 1990s with Amartya Sen‘s book ―Development as Freedom where the notions of substantive freedom and the multidimensional nature of poverty were made explicit to the multilateral development banks. This recognition of the multidimensional nature of poverty as well as the combined failures of both markets and governments and the socio-political complexity of ground level realities has made it clear that relying on traditional top-down, state-led, ―big development‖ strategies would not be effective to combat poverty. Moreover, this resurgence in participatory development and bottom-up approaches in the NGO and development sector has come in only the last two decades as explained above.\n\nSince the mid-1990s, community-driven development has emerged as one of the fastest-growing investments by NGOs, aid organizations and multilateral developments banks. This continued investment in CDD has been driven mostly by a demand from donor agencies and developing countries for large-scale, bottom-up and demand-driven, poverty reduction subprojects that can increase the institutional capacity of small communities for self-development. The success and scale of some CDD projects in the World Bank are especially notable. The World Bank supported approximately 190 lending projects amounting to $9.3 billion in 2000–2005 (Tanaka, 2006). Initiated by the International Development Association (IDA) at the World Bank, CDD projects have been instrumental in harnessing the energy and capacity of communities for poverty reduction. Since the start of this decade, IDA lending for CDD has averaged annually just over 50 operations, for an average total of US$1.3 billion per year (International Development Association, 2009).\n\nEven the Asian Development Bank (ADB) has funded 57 projects worth about $2.5 billion between 2001-2007 that included community-driven development approaches to enhance deliver of inputs and beneficiary participation. They constituted 14% of the total loans approved by the Asian Development Bank during this period. Over one-third of the projects were in the agriculture and natural resources sector, followed by a smaller proportion of water supply and sanitation, waste management, education and health projects. The projects were primarily in Southeast Asia, South Asia, and Central and West Asia, where the developing country governments were investing in rural development programs (Asian Development Bank, 2008).\n\nIn the last few years the International Fund for Agricultural Development has been working with the Agence française de développement (AFD), the African Development Bank (AfDB), the European Union (EU), the Food and Agriculture Organization of the United Nations (FAO), the UN Capital Development Fund (UNCDF) and the World Bank to create a platform for learning and sharing knowledge on community-driven development (International Fund for Agricultural Development, 2010). Intensive forms of community participation have been attempted in projects of several donors for many years. Bilateral donors, such as the Department for International Development (DFID) of the United Kingdom and the Canadian International Development Agency (CIDA), have used CDD-type approaches for a long time as part of their sustainable livelihoods and integrated basic needs development assistance in developing countries. The Swedish International Development Agency (SIDA) and Danish International Development Agency have used CDD principles in the mandate of a rights-based approach to the development projects they fund (FAO, 2010).\n\nMore than 80 countries have now implemented CDD projects. The breadth and activities funded by the CDD programs at the World Bank can be explained by providing a brief overview of a few of them.\n\nThe Second National Fadama Development Project II (NFDP-II) targets the development of small scale irrigation, especially in the low-lying alluvial floodplains or \"Fadama‖. NFDP-II increased the productivity, living standards and development capacity of the economically active rural communities while increasing the efficiency in delivering implementation services to an estimated four million rural beneficiary households and raising the real incomes of households by 45 percent (African Development Bank, 2003). The Social Fund for Development in Yemen provided support 7 million people of which 49 percent were female and generated 8,000 permanent jobs. It also increased the number of girls‘ schools from 502 to 554 and basic education enrollment rates from 63 percent to 68 percent. The program focuses on helping the poor to help themselves through providing income-generating activities and building community infrastructure rather than making cash transfers (El-Gammal, 2004). The Social Investment Fund Project V in Honduras benefited 2.5 million people with the implementation of 2,888 projects (1,446 rehabilitated schools, about 700 new schools, 163 new health centers, 347 small water/sanitation systems, and 461 latrines) resulting in all children in the targeted areas attending primary school. In addition the project communities were provided with better access to health care assistance and access to running water (Perez de Castillo, 1998). The Andhra Pradesh Rural Poverty Reduction Project (APRPRP) in India has help to organize 10.1 million rural poor women into community-based organizations that collectively save over US$770 million and leverage credit over $2.7 billion from commercial banks (World Bank, 2003). The Kecamatan Development Program (KDP) in Indonesia which is what the National Solidarity Program in Afghanistan is based on has benefitted 18 million people by providing better services which include more than 37,000 kilometers of local roads and 8,500 bridges, 9,200 clean water supply units, and 3,000 new or improved health posts. In addition, more than 1.3 million people obtained loans to start or complement local businesses through microfinancing (Guggenheim, 2004). Lastly, the National Solidarity Programme (NSP) in Afghanistan will be the focus of this research. In this implementation elected village-level community development councils, which include women, use grants and local labor to rebuild bridges and roads, fix schools and install water pumps to benefit 13 million people across Afghanistan thereby building state credibility and strengthening local democracy.\n\n"}
{"id": "25975488", "url": "https://en.wikipedia.org/wiki?curid=25975488", "title": "Course of Theoretical Physics", "text": "Course of Theoretical Physics\n\nThe Course of Theoretical Physics is a ten-volume series of books covering theoretical physics that was initiated by Lev Landau and written in collaboration with his student Evgeny Lifshitz starting in the late 1930s.\n\nIt is said that Landau composed much of the series in his head while in an NKVD prison in 1938-1939. However, almost all of the actual writing of the early volumes was done by Lifshitz, giving rise to the witticism, \"not a word of Landau and not a thought of Lifshitz\". The first eight volumes were finished in the 1950s, written in Russian and translated into English in the late 1950s by John Stewart Bell, together with John Bradbury Sykes, M. J. Kearsley, and W. H. Reid. The last two volumes were written in the early 1980s. Vladimir Berestetskii and Lev Pitaevskii also contributed to the series. The series is often referred to as \"Landau and Lifshitz\", \"Landafshitz\" (Russian: \"Ландафшиц\"), or \"Lanlifshitz\" (Russian: \"Ланлифшиц\") in informal settings.\n\nThe presentation of material is advanced and typically considered suitable for graduate-level study. Despite this specialized character, it is estimated that a million volumes of the \"Course\" were sold by 2005.\n\nThe series has been called \"renowned\" in \"Science\" and \"celebrated\" in \"American Scientist.\" A note in \"Mathematical Reviews\" states, \"The usefulness and the success of this course have been proved by the great number of successive editions in Russian, English, French, German and other languages.\" At a centenary celebration of Landau's career, it was observed that the \"Course\" had shown \"unprecedented longevity.\"\n\nIn 1962, Landau and Lifshitz were awarded the Lenin Prize for their work on the \"Course\". This was the first occasion on which the Lenin Prize had been awarded for the teaching of physics.\n\nNote that reprints and revised editions are not listed.\n\n\nCovers classical mechanics without special or general relativity, in the Lagrangian and Hamiltonian formalisms.\n\n\nCovers relativistic mechanics of particles, and classical field theory for fields, specifically special relativity and electromagnetism, general relativity and gravitation.\n\n\nCovers quantum mechanics without special relativity.\n\n\nThe original edition was two books, labelled part 1 and part 2. The first had general aspects of relativistic quantum mechanics and relativistic quantum field theory, leading onto quantum electrodynamics. The second continued on with quantum electrodynamics and what was then known about the strong and weak interactions. These books were published in the early 1970s, at a time when the strong and weak forces were still not well understood. In the second edition, the corresponding sections were scrapped and replaced with more topics in the well-established quantum electrodynamics, and the two parts were unified into one, thus providing a one-volume exposition on relativistic quantum field theory with the electromagnetic interaction as the prototype of a quantum field theory.\n\n\nCovers general statistical mechanics and thermodynamics and applications, including chemical reactions, phase transitions, and condensed matter physics.\n\n\nCovers fluid mechanics in a condensed but varied exposition, from ideal to viscous fluids, includes a chapter on relativistic fluid mechanics, and another on superfluids.\n\n\nCovers elasticity theory of solids, including viscous solids, vibrations and waves in crystals with dislocations, and a chapter on the mechanics of liquid crystals.\n\n\nCovers electromagnetism in materials, includes a variety of topics in condensed matter physics, a chapter on magnetohydrodynamics, and another on nonlinear optics.\n\n\nBuilds from the original statistical physics book; more applications to condensed matter theory.\n\n\nPresents various applications of kinetic theory to condensed matter theory, on metals, insulators, and phase transitions.\n\n"}
{"id": "1530988", "url": "https://en.wikipedia.org/wiki?curid=1530988", "title": "Created kind", "text": "Created kind\n\nIn Christian and Jewish creationism, a religious view based on the creation account of the book of Genesis, created kinds are purported to be the original forms of life as they were created by God. They are also referred to as kinds, original kinds, Genesis kinds, and baramin (a neologism coined by combining the Hebrew words \"bara\" [created] and \"min\" [kind], though the combination does not work syntactically in actual Hebrew). The idea is promulgated by young Earth creationist organizations and preachers as a means to support their belief in the literal veracity of the Genesis creation myth as well as their contention that the ancestors of all land-based life on Earth were housed on Noah's ark before a great flood. Old earth creationists also employ the concept, rejecting the idea of common descent. In contrast to young Earth creationists, old earth creationists do not necessarily believe all land-based life was housed on the ark, and some accept some evolutionary change within the given kinds has occurred. \n\nIn contrast to the scientific theory of common descent, these creationists argue that not all life on Earth is related, but that life was created by God in a finite number of discrete forms. This viewpoint claims that kinds cannot interbreed and have no evolutionary relationship to one another.\n\nThe concept of the \"kind\" originates from a literal reading of Genesis 1:12-24:\n\nThere is some uncertainty about what exactly the Bible means when it talks of \"kinds.\" Creationist Brian Nelson claimed \"While the Bible allows that new varieties may have arisen since the creative days, it denies that any new species have arisen.\" However, Russell Mixter, another creationist writer, said that \"One should not insist that \"kind\" means species. The word \"kind\" as used in the Bible may apply to any animal which may be distinguished in any way from another, or it may be applied to a large group of species distinguishable from another group ... there is plenty of room for differences of opinion on what are the kinds of Genesis.\"\n\nFrank Lewis Marsh coined the term \"baramin\" in his book \"Fundamental Biology\" (1941) and expanded on the concept in \"Evolution, Creation, and Science\" (c. 1944), in which he stated that the ability to hybridize and create viable offspring was a sufficient condition for being members of the same baramin. However, he said that it was not a necessary condition, acknowledging that observed speciation events among \"Drosophila\" fruitflies had been shown to cut off hybridization.\n\nMarsh also originated \"discontinuity systematics\", the idea that there are boundaries between different animals that cannot be crossed with the consequence that there would be discontinuities in the history of life and limits to common ancestry.\n\nIn 1990, Kurt Wise introduced baraminology as an adaptation of Marsh's and Walter ReMine's ideas that was more in keeping with young Earth creationism. Wise advocated using the Bible as a source of systematic data. Baraminology and its associated concepts have been criticized by scientists and creationists for lacking formal structure. Consequently, in 2003 Wise and other creationists proposed a refined baramin concept in the hope of developing a broader creationary model of biology.\nAlan Gishlick, reviewing the work of baraminologists in 2006, found it to be surprisingly rigorous and internally consistent, but concluded that the methods did not work.\n\nWalter ReMine specified four groupings: holobaramins, monobaramins, apobaramins, and polybaramins. These are, respectively, all things of one kind; some things of the same kind; groups of kinds; and any mixed grouping of things. These groups correspond to the concepts of holophyly, monophyly, paraphyly, and polyphyly used in cladistics.\n\nBaraminology employs many of the same methods used in evolutionary systematics, including cladistics and Analysis of Pattern (ANOPA). However, instead of identifying continuity between groups of organisms based on shared similarities, baraminology uses these methods to search for morphological and genetic gaps between groups. Baraminologists have also developed their own systematics software, known as BDIST, to measure distance between groups.\n\nThe methods of baraminology are not universally accepted among young-Earth creationists. Other creationists have criticized these methods as having the same problems as traditional cladistics, as well as for occasionally producing results that they feel contradict the Bible.\n\nBaraminology has been heavily criticized for its lack of rigorous tests and post-study rejection of data to make it better fit the desired findings.\n\nSome techniques employed in Baraminology have been used to demonstrate evolution, thereby calling baraminological conclusions into question.\n\n\n"}
{"id": "43385931", "url": "https://en.wikipedia.org/wiki?curid=43385931", "title": "Data exploration", "text": "Data exploration\n\nData exploration is an approach similar to initial data analysis, whereby a data analyst uses visual exploration to understand what is in a dataset and the characteristics of the data, rather than through traditional data management systems. These characteristics can include size or amount of data, completeness of the data, correctness of the data, possible relationships amongst data elements or files/tables in the data.\n\nData exploration is typically conducted using a combination of automated and manual activities. Automated activities can include data profiling or data visualization or tabular reports to give the analyst an initial view into the data and an understanding of key characteristics.\n\nThis is often followed by manual drill-down or filtering of the data to identify anomalies or patterns identified through the automated actions. Data exploration can also require manual scripting and queries into the data (e.g. using languages such as SQL or R) or using Excel or similar tools to view the raw data.\n\nAll of these activities are aimed at creating a clear mental model and understanding of the data in the mind of the analyst, and defining basic metadata (statistics, structure, relationships) for the data set that can be used in further analysis.\n\nOnce this initial understanding of the data is had, the data can be pruned or refined by removing unusable parts of the data, correcting poorly formatted elements and defining relevant relationships across datasets. This process is also known as determining data quality.\n\nData exploration can also refer to the ad hoc querying and visualization of data to identify potential relationships or insights that may be hidden in the data.\n\nTraditionally, this had been a key area of focus for statisticians, with John Tukey being a key evangelist in the field. . Today, data exploration is more widespread and is the focus of data analysts and data scientists; the latter being a relatively new role within enterprises and larger organizations.\n\nThis area of data exploration has become an area of interest in the field of machine learning. This is a relatively new field and is still evolving. As its most basic level, a machine-learning algorithm can be fed a data set and can be used to identify whether a hypothesis is true based on the dataset. Common machine learning algorithms can focus on identifying specific patterns in the data. Many common patterns include regression and classification or clustering, but there are many possible patterns and algorithms that can be applied to data via machine learning.\n\nBy employing machine learning, it is possible to find patterns or relationships in the data that would be difficult or impossible to find via manual inspection, trial and error or traditional exploration techniques.\n\n"}
{"id": "647199", "url": "https://en.wikipedia.org/wiki?curid=647199", "title": "Democratic capitalism", "text": "Democratic capitalism\n\nDemocratic capitalism, also known as capitalist democracy, is a political, economic and social ideology that involves the combination of a democratic political system with a capitalist economic system. It is based on a tripartite arrangement of a private sector-driven market economy based predominantly on a democratic policy, economic incentives through free markets, fiscal responsibility and a liberal moral-cultural system which encourages pluralism. This ideology supports a capitalist economy subject to control by a democratic political system that is supported by the majority. It stands in contrast to corporatism by limiting the influence of special interest groups, including corporate lobbyists, on politics.\n\nIt is argued that the coexistence of modern capitalism and democracy was the result of the creation of the modern welfare state in the post-war period, which enabled a relatively stable political atmosphere and widespread support for capitalism. This period of history is often referred to as the \"Golden Age of Capitalism\".\n\nThe ideology of \"democratic capitalism\" has been in existence since medieval times. It is based firmly on the principles of liberalism and Whig historiography, which include liberty and equality. Some of its most prominent promoters were the Founding Fathers of the United States and subsequent Jeffersonians.\n\n\n\n"}
{"id": "3154278", "url": "https://en.wikipedia.org/wiki?curid=3154278", "title": "Development theory", "text": "Development theory\n\nDevelopment theory is a collection of theories about how desirable change in society is best achieved. Such theories draw on a variety of social science disciplines and approaches. In this article, multiple theories are discussed, as are recent developments with regard to these theories. Depending on which theory that is being looked at, there are different explanations to the process of development and their inequalities.\n\nModernization theory is used to analyze the processes in which modernization in societies take place. The theory looks at which aspects of countries are beneficial and which constitute obstacles for economic development. The idea is that development assistance targeted at those particular aspects can lead to modernization of 'traditional' or 'backward' societies. Scientists from various research disciplines have contributed to modernization theory.\n\nThe earliest principles of modernization theory can be derived from the idea of progress, which stated that people can develop and change their society themselves. Marquis de Condorcet was involved in the origins of this theory. This theory also states that technological advancements and economic changes can lead to changes in moral and cultural values. The French sociologist Émile Durkheim stressed the interdependence of institutions in a society and the way in which they interact with cultural and social unity. His work ‘The Division of Labor in Society’ was very influential. It described how social order is maintained in society and ways in which primitive societies can make the transition to more advanced societies.\n\nOther scientists who have contributed to the development of modernization theory are: David Apter, who did research on the political system and history of democracy; Seymour Martin Lipset, who argued that economic development leads to social changes which tend to lead to democracy; David McClelland, who approached modernization from the psychological side with his motivations theory; and Talcott Parsons who used his pattern variables to compare backwardness to modernity.\n\nThe linear stages of growth model is an economic model which is heavily inspired by the Marshall Plan which was used to revitalize Europe’s economy after World War II. It assumes that economic growth can only be achieved by industrialization. Growth can be restricted by local institutions and social attitudes, especially if these aspects influence the savings rate and investments. The constraints impeding economic growth are thus considered by this model to be internal to society.\n\nAccording to the linear stages of growth model, a correctly designed massive injection of capital coupled with intervention by the public sector would ultimately lead to industrialization and economic development of a developing nation.\n\nThe Rostow's stages of growth model is the most well-known example of the linear stages of growth model. Walt W. Rostow identified five stages through which developing countries had to pass to reach an advanced economy status: (1) Traditional society, (2) Preconditions for take-off, (3) Take-off, (4) Drive to maturity, (5) Age of high mass consumption. He argued that economic development could be led by certain strong sectors; this is in contrast to for instance Marxism which states that sectors should develop equally. According to Rostow’s model, a country needed to follow some rules of development to reach the take-off: (1) The investment rate of a country needs to be increased to at least 10% of its GDP, (2) One or two manufacturing sectors with a high rate of growth need to be established, (3) An institutional, political and social framework has to exist or be created in order to promote the expansion of those sectors.\n\nThe Rostow model has serious flaws, of which the most serious are: (1) The model assumes that development can be achieved through a basic sequence of stages which are the same for all countries, a doubtful assumption; (2) The model measures development solely by means of the increase of GDP per capita; (3) The model focuses on characteristics of development, but does not identify the causal factors which lead development to occur. As such, it neglects the social structures that have to be present to foster development.\n\nEconomic modernization theories such as Rostow's stages model have been heavily inspired by the Harrod-Domar model which explains in a mathematical way the growth rate of a country in terms of the savings rate and the productivity of capital. Heavy state involvement has often been considered necessary for successful development in economic modernization theory; Paul Rosenstein-Rodan, Ragnar Nurkse and Kurt Mandelbaum argued that a big push model in infrastructure investment and planning was necessary for the stimulation of industrialization, and that the private sector would not be able to provide the resources for this on its own.\nAnother influential theory of modernization is the dual-sector model by Arthur Lewis. In this model Lewis explained how the traditional stagnant rural sector is gradually replaced by a growing modern and dynamic manufacturing and service economy.\n\nBecause of the focus on the need for investments in capital, the Linear Stages of Growth Models are sometimes referred to as suffering from ‘capital fundamentalism’.\n\nModernization theory observes traditions and pre-existing institutions of so-called \"primitive\" societies as obstacles to modern economic growth. Modernization which is forced from outside upon a society might induce violent and radical change, but according to modernization theorists it is generally worth this side effect. Critics point to traditional societies as being destroyed and slipping away to a modern form of poverty without ever gaining the promised advantages of modernization.\n\nStructuralism is a development theory which focuses on structural aspects which impede the economic growth of developing countries. The unit of analysis is the transformation of a country’s economy from, mainly, a subsistence agriculture to a modern, urbanized manufacturing and service economy. Policy prescriptions resulting from structuralist thinking include major government intervention in the economy to fuel the industrial sector, known as import substitution industrialization (ISI). This structural transformation of the developing country is pursued in order to create an economy which in the end enjoys self-sustaining growth. This can only be reached by ending the reliance of the underdeveloped country on exports of primary goods (agricultural and mining products), and pursuing inward-oriented development by shielding the domestic economy from that of the developed economies. Trade with advanced economies is minimized through the erection of all kinds of trade barriers and an overvaluation of the domestic exchange rate; in this way the production of domestic substitutes of formerly imported industrial products is encouraged. The logic of the strategy rests on the infant industry argument, which states that young industries initially do not have the economies of scale and experience to be able to compete with foreign competitors and thus need to be protected until they are able to compete in the free market. The Prebisch–Singer hypothesis states that over time the terms of trade for commodities deteriorate compared to those for manufactured goods, because the income elasticity of demand of manufactured goods is greater than that of primary products. If true, this would also support the ISI strategy.\n\nStructuralists argue that the only way Third World countries can develop is through action by the state. Third world countries have to push industrialization and have to reduce their dependency on trade with the First World, and trade among themselves.\n\nThe roots of structuralism lie in South America, and particularly Chile. In 1950, Raul Prebisch went to Chile to become the first director of the Economic Commission for Latin America. In Chile, he cooperated with Celso Furtado, Anibal Pinto, Osvaldo Sunkel, and Dudley Seers, who all became influential structuralists.\n\nDependency theory is essentially a follow up to structuralist thinking, and shares many of its core ideas. Whereas structuralists did not consider that development would be possible at all unless a strategy of delinking and rigorous ISI was pursued, dependency thinking could allow development with external links with the developed parts of the globe. However, this kind of development is considered to be \"dependent development\", i.e., it does not have an internal domestic dynamic in the developing country and thus remains highly vulnerable to the economic vagaries of the world market. Dependency thinking starts from the notion that resources flow from the ‘periphery’ of poor and underdeveloped states to a ‘core’ of wealthy countries, which leads to accumulation of wealth in the rich states at the expense of the poor states. Contrary to modernization theory, dependency theory states that not all societies progress through similar stages of development. Periphery states have unique features, structures and institutions of their own and are considered weaker with regards to the world market economy, while the developed nations have never been in this colonized position in the past. Dependency theorists argue that underdeveloped countries remain economically vulnerable unless they reduce their connections to the world market.\n\nDependency theory states that poor nations provide natural resources and cheap labor for developed nations, without which the developed nations could not have the standard of living which they enjoy. When underdeveloped countries try to remove the Core's influence, the developed countries hinder their attempts to keep control. This means that poverty of developing nations is not the result of the disintegration of these countries in the world system, but because of the way in which they are integrated into this system.\n\nIn addition to its structuralist roots, dependency theory has much overlap with Neo-Marxism and World Systems Theory, which is also reflected in the work of Immanuel Wallerstein, a famous dependency theorist. Wallerstein rejects the notion of a Third World, claiming that there is only one world which is connected by economic relations (World Systems Theory). He argues that this system inherently leads to a division of the world in core, semi-periphery and periphery. One of the results of expansion of the world-system is the commodification of things, like natural resources, labor and human relationships.\n\nThe basic needs model was introduced by the International Labour Organization in 1976, mainly in reaction to prevalent modernization- and structuralism-inspired development approaches, which were not achieving satisfactory results in terms of poverty alleviation and combating inequality in developing countries. It tried to define an absolute minimum of resources necessary for long-term physical well-being. The poverty line which follows from this, is the amount of income needed to satisfy those basic needs. The approach has been applied in the sphere of development assistance, to determine what a society needs for subsistence, and for poor population groups to rise above the poverty line. Basic needs theory does not focus on investing in economically productive activities. Basic needs can be used as an indicator of the absolute minimum an individual needs to survive.\n\nProponents of basic needs have argued that elimination of absolute poverty is a good way to make people active in society so that they can provide labor more easily and act as consumers and savers. There have been also many critics of the basic needs approach. It would lack theoretical rigour, practical precision, be in conflict with growth promotion policies, and run the risk of leaving developing countries in permanent.\n\nNeoclassical development theory has it origins in its predecessor: classical economics. Classical economics was developed in the 18th and 19th centuries and dealt with the value of products and on which production factors it depends. Early contributors to this theory are Adam Smith and David Ricardo. Classical economists argued – as do the neoclassical ones – in favor of the free market, and against government intervention in those markets. The 'invisible hand' of Adam Smith makes sure that free trade will ultimately benefit all of society. John Maynard Keynes was a very influential classical economist as well, having written his General Theory of Employment, Interest, and Money in 1936.\n\nNeoclassical development theory became influential towards the end of the 1970s, fired by the election of Margaret Thatcher in the UK and Ronald Reagan in the USA. Also, the World Bank shifted from its Basic Needs approach to a neoclassical approach in 1980. From the beginning of the 1980s, neoclassical development theory really began to roll out.\n\nOne of the implications of the neoclassical development theory for developing countries were the Structural Adjustment Programmes (SAPs) which the World Bank and the International Monetary Fund wanted them to adapt. Important aspects of those SAPs include:\n\nThese measures are more or less reflected by the themes which were identified by the Institute of International Economics which were believed to be necessary for the recovery of Latin America from the economic and financial crises of the 1980s. These themes are known as the Washington consensus, a termed coined in 1989 by the economist John Williamson.\n\nPostdevelopment theory is a school of thought which questions the idea of national economic development altogether. According to postdevelopment scholars, the goal of improving living standards leans on arbitrary claims as to the desirability and possibility of that goal. Postdevelopment theory arose in the 1980s and 1990s.\n\nAccording to postdevelopment theorists, the idea of development is just a 'mental structure' (Wolfgang Sachs) which has resulted in a hierarchy of developed and underdeveloped nations, of which the underdeveloped nations desire to be like developed nations. Development thinking has been dominated by the West and is very ethnocentric, according to Sachs. The Western lifestyle may neither be a realistic nor a desirable goal for the world's population, postdevelopment theorists argue. Development is being seen as a loss of a country's own culture, people's perception of themselves and modes of life. According to Majid Rahnema, another leading postdevelopment scholar, things like notions of poverty are very culturally embedded and can differ a lot among cultures. The institutes which voice the concern over underdevelopment are very Western-oriented, and postdevelopment calls for a broader cultural involvement in development thinking.\n\nPostdevelopment proposes a vision of society which removes itself from the ideas which currently dominate it. According to Arturo Escobar, postdevelopment is interested instead in local culture and knowledge, a critical view against established sciences and the promotion of local grassroots movements. Also, postdevelopment argues for structural change in order to reach solidarity, reciprocity, and a larger involvement of traditional knowledge.\n\nSustainable development is development that meets the needs of the present without compromising the ability of future generations to meet their own needs. (Brundtland Commission) There exist more definitions of sustainable development, but they all have to do with the carrying capacity of the earth and its natural systems and the challenges faced by humanity. Sustainable development can be broken up into environmental sustainability, economic sustainability and sociopolitical sustainability. The book 'Limits to Growth', commissioned by the Club of Rome, gave huge momentum to the thinking about sustainability. Global warming issues are also problems which are emphasized by the sustainable development movement. This led to the 1997 Kyoto Accord, with the plan to cap greenhouse-gas emissions.\n\nOpponents of the implications of sustainable development often point to the environmental Kuznets curve. The idea behind this curve is that, as an economy grows, it shifts towards more capital and knowledge-intensive production. This means that as an economy grows, its pollution output increases, but only until it reaches a particular threshold where production becomes less resource-intensive and more sustainable. This means that a pro-growth, not an anti-growth policy is needed to solve the environmental problem. But the evidence for the environmental Kuznets curve is quite weak. Also, empirically spoken, people tend to consume more products when their income increases. Maybe those products have been produced in a more environmentally friendly way, but on the whole the higher consumption negates this effect. There are people like Julian Simon however who argue that future technological developments will resolve future problems.\n\nHuman development theory is a theory which uses ideas from different origins, such as ecology, sustainable development, feminism and welfare economics. It wants to avoid normative politics and is focused on how social capital and instructional capital can be deployed to optimize the overall value of human capital in an economy.\n\nAmartya Sen and Mahbub ul Haq are the most well-known human development theorists. The work of Sen is focused on capabilities: what people can do and be. It is these capabilities, rather than the income or goods that they receive (as in the Basic Needs approach), that determine their well being. This core idea also underlies the construction of the Human Development Index, a human-focused measure of development pioneered by the UNDP in its Human Development Reports; this approach has become popular the world over, with indexes and reports published by individual counties, including the American Human Development Index and Report in the United States. The economic side of Sen's work can best be categorized under welfare economics, which evaluates the effects of economic policies on the well-being of peoples. Sen wrote the influential book 'Development as freedom' which added an important ethical side to development economics \n\n\n"}
{"id": "33055034", "url": "https://en.wikipedia.org/wiki?curid=33055034", "title": "Diederik Stapel", "text": "Diederik Stapel\n\nDiederik Alexander Stapel (born 19 October 1966 in Oegstgeest) is a Dutch former professor of social psychology at Tilburg University and before that at the University of Groningen in the Netherlands. In 2011 Tilburg University suspended Stapel for fabricating and manipulating data for his research publications. This scientific misconduct took place over a number of years and affected at least 55 publications.\n\nStapel obtained an M.A. in psychology and communications from the University of Amsterdam (UvA) in 1991. In 1997 he obtained his Ph.D. cum laude in social psychology from the UvA. He became professor at the University of Groningen in 2000 and moved to Tilburg University in 2006, where he founded TiBER, the Tilburg Institute for Behavioral Economics Research. In September 2010, Stapel became dean of the social and behavioral sciences faculty.\n\nStapel received the \"Career Trajectory Award\" from the Society of Experimental Social Psychology in 2009, which has since been retracted. He returned his Ph.D. title to the University of Amsterdam in November 2011, noting that his \"behavior of the past years are inconsistent with the duties associated with the doctorate\".\n\nIn October 2014 Dutch media reported that Stapel had returned to work, teaching social philosophy at the Fontys Academy for Creative Industries in Tilburg.\n\nIn September 2011, Tilburg University suspended Stapel due to his fabrication of data used in research publications. The university announced an investigation of his work.\n\nOn 31 October 2011, a committee entrusted with investigating \"the extent and nature of the breach of scientific integrity committed by Mr D.A. Stapel\", formed by the Rector Magnificus of Tilburg University and chaired by Willem (\"Pim\") Levelt, published an interim report regarding Stapel's activities at the three Dutch universities where he had worked. The interim report pointed to three unidentified \"young researchers\" as the whistleblowers for the case, and implies that these whistleblowers spent months making observations of Stapel and his work before they concluded that something actually was wrong. The report also cites two professors who claim they had previously seen examples of Stapel's data that were \"too good to be true\". The report concluded that Stapel made up data for at least 30 publications. His general method towards the end of his career was to develop a complete experiment at the level of theory, hypotheses, methods, stimuli, questionnaires, and even participants' rewards – and then pretend that he would run the experiments at schools to which only he had access. Instead of doing so, he would make up the data and send these to colleagues for further analysis. The report also stated that earlier in his career, going back at least to 2004, he appears to have manipulated data rather than faked them. In all cases he acted alone and the report did not find any indication that coauthors, PhD students, or others were aware even in instances where suspicion may have been reasonable. On page 6-7, the interim report names 19 Ph.D. theses prepared with data delivered by Stapel. Of those, seven have been cleared. There are various degrees of suspicion about the remaining 12. The report advised that the Ph.D. degrees of the students involved should not be retracted.\n\nIt became widely known that Stapel treated his graduate students unfairly, with most of them graduating without ever actually completing an experiment. Stapel controlled the data in his lab, and when students asked to see the raw data, they were often given excuses. According to the report, there were occasions when Stapel's data were given to an assistant to be entered into a computer. This assistant would then return the data file to Stapel. The researcher analyzing the data would then receive the file directly from Stapel. Stapel would apparently tell this researcher to \"Be aware that you have gold in your hands.\" The report also suggests that Stapel elected to present a list of publications that contained fictitious data.\n\nThe interim report stated that it was not possible to determine whether Stapel fabricated or manipulated data for his 1997 dissertation at the University of Amsterdam, because the data had been destroyed. The university announced that it would investigate whether it would be possible to retract Stapel's Ph.D. because of exceptionally unworthy scientific behavior. Stapel has since returned his degree himself (see above).\n\nThe interim report stated that Stapel had caused severe damage to young people at the beginning of their careers, as well as to the general confidence in science, in particular social psychology. The University of Tilburg announced that it would pursue criminal prosecution of Stapel.\n\nAn extensive report investigates all of Stapel's 130 articles and 24 book chapters. A website was set up on 27 March 2012 to publish intermediate findings. According to the first findings, on the first batch of 20 publications by Stapel, studied by the \"Levelt Committee\", 12 were falsified and three contributions to books were also fraudulent. \"de Volkskrant\" reported that the final report was due on 28 November 2012, and that a book by Stapel (\"Ontsporing\", \"Derailment\") was to be released around the same time.\n\"We have some 30 papers in peer-reviewed journals where we are actually sure that they are fake, and there are more to come,\" Pim Levelt, chair of the committee investigating Stapel's work stated.\n\nA month after Tilburg University announced that it had found evidence of fraud in Stapel's work, the journal \"Science\" posted a retraction notice on Stapel's co-authored paper entitled \"Coping with chaos: How disordered contexts promote stereotyping and discrimination\". The report from \"Science\" says:\n\nIn December 2011, Stapel retracted this paper, the first to be retracted. The journal expressed initial concern regarding the paper's validity on November 1. In a response to the retraction, coauthor of the \"Chaos\" paper Siegwart Lindenberg told the journal in an email, \"Stapel's doing had caught me as much by surprise as it did anybody else. I never had any suspicion. He was a very trusted man, dean of the faculty, brilliant, successful, no indications for me to be distrustful. In this, I was not the only one. I also had no trouble with the results of the experiments.\"\n\nThe research result, obtained by Stapel and co-workers Roos Vonk (Radboud University) and (Tilburg University), that meat eaters are more selfish than vegetarians, which was widely publicized in Dutch media, was suspected and later turned out to be based on falsified data. The research result had not yet been published in a scientific journal; only a press bulletin was released.\n\nResponding to the interim report, Stapel stated:\nIn his memoirs published in November 2012, Stapel admits his fraud, but protests against the accusation in the interim report that he was a cunning, manipulative fraud with a plan.\n\nOn 28 November 2012 the joint final report, from the three investigating committees, was published.\n\nIt has been suggested that Stapel was able to continue his fraud for so long because of his status. At Tilburg he was \"considered a star\" and was seen by his colleagues and students as \"charismatic, friendly and incredibly talented\". Many students became his personal friends. But the final Levelt report raises more controversial questions about the ways in which Stapel went unchallenged for so long. The report concludes that there was \"a more general failure of scientific criticism in the peer community and a research culture that was excessively oriented to uncritical confirmation of one's own ideas and to finding appealing but theoretically superficial ad hoc results\". It goes on to suggest that \"not infrequently reviews [of social psychology journal articles] were strongly in favour of telling an interesting, elegant, concise and compelling story, possibly at the expense of the necessary scientific diligence.\"\n\nThis aspect of the report has been criticised by the Social Psychology Section of the British Psychological Society. In a letter to the Times Higher Education Supplement, on behalf of the Section, Stephen Gibson at York St John University, points out \".. there are no grounds for concluding either that research fraud is any more common in social psychology than other disciplines or that its editorial processes are particularly poor at detecting it\", adding that: \"Our sub-discipline does not deserve the harm to its reputation that may be provoked by the careless implication of \"unique\" deficiencies.\" The Levelt report has also been criticised by the European Association of Social Psychology in an open letter to its members.\n\nIn the February 2013 issue of \"The Psychologist\", Willem Levelt, together with the chairs of the other two investigating committees, published a rejoinder to these and other criticisms. Drenth et al. acknowledge that they did not compare the situation in social psychology with other disciplines, but note that \"such a comparative investigation was not part of the Committees' commission.\"\n\nIn a review for the Association for Psychological Science, Stapel's 315-page memoirs, entitled \"Ontsporing\" (\"Derailed\"), is described by Dutch psychologists Denny Borsboom and Eric-Jan Wagenmakers as \"priceless and revealing\". Stapel recounts that his misdemeanours began when he was sitting alone in his office and changed \"an unexpected 2 into a 4\". The reviewers describe the final chapter of the book as \"unexpectedly beautiful\" but note that it is full of lines taken from the works of writers Raymond Carver and James Joyce.\n\nIn June 2013 Stapel agreed, in a settlement with the prosecutor, to perform 120 hours of community service and to lose the right to some benefits associated with his former job equivalent to 1.5 years of salary. In this way, he avoided further criminal prosecution.\n\nAs of December, 2015, Retraction Watch reported that Stapel had 58 retractions.\nThese include the following:\n\n\n\n\n\n"}
{"id": "28923910", "url": "https://en.wikipedia.org/wiki?curid=28923910", "title": "Electronic engineering", "text": "Electronic engineering\n\nElectronic engineering (also called electronics and communications engineering) is an electrical engineering discipline which utilizes nonlinear and active electrical components (such as semiconductor devices, especially transistors, diodes and integrated circuits) to design electronic circuits, devices, VLSI devices and their systems. The discipline typically also designs passive electrical components, usually based on printed circuit boards.\n\nElectronics is a subfield within the wider electrical engineering academic subject but denotes a broad engineering field that covers subfields such as analog electronics, digital electronics, consumer electronics, embedded systems and power electronics. Electronics engineering deals with implementation of applications, principles and algorithms developed within many related fields, for example solid-state physics, radio engineering, telecommunications, control systems, signal processing, systems engineering, computer engineering, instrumentation engineering, electric power control, robotics, and many others.\n\nThe Institute of Electrical and Electronics Engineers (IEEE) is one of the most important and influential organizations for electronics engineers.\n\nElectronics is a subfield within the wider electrical engineering academic subject. An academic degree with a major in electronics engineering can be acquired from some universities, while other universities use electrical engineering as the subject. The term electrical engineer is still used in the academic world to include electronic engineers. However, some people consider the term 'electrical engineer' should be reserved for those having specialized in power and heavy current or high voltage engineering, while others consider that power is just one subset of electrical engineering, as well as 'electrical distribution engineering'. The term 'power engineering' is used as a descriptor in that industry. Again, in recent years there has been a growth of new separate-entry degree courses such as 'systems engineering' and 'communication systems engineering', often followed by academic departments of similar name, which are typically not considered as subfields of electronics engineering but of electrical engineering.\n\nElectronic engineering as a profession sprang from technological improvements in the telegraph industry in the late 19th century and the radio and the telephone industries in the early 20th century. People were attracted to radio by the technical fascination it inspired, first in receiving and then in transmitting. Many who went into broadcasting in the 1920s were only 'amateurs' in the period before World War I.\n\nTo a large extent, the modern discipline of electronic engineering was born out of telephone, radio, and television equipment development and the large amount of electronic systems development during World War II of radar, sonar, communication systems, and advanced munitions and weapon systems. In the interwar years, the subject was known as radio engineering and it was only in the late 1950s that the term electronic engineering started to emerge.\n\nIn the field of electronic engineering, engineers design and test circuits that use the electromagnetic properties of electrical components such as resistors, capacitors, inductors, diodes and transistors to achieve a particular functionality. The tuner circuit, which allows the user of a radio to filter out all but a single station, is just one example of such a circuit.\n\nIn designing an integrated circuit, electronics engineers first construct circuit schematics that specify the electrical components and describe the interconnections between them. When completed, VLSI engineers convert the schematics into actual layouts, which map the layers of various conductor and semiconductor materials needed to construct the circuit. The conversion from schematics to layouts can be done by software (see electronic design automation) but very often requires human fine-tuning to decrease space and power consumption. Once the layout is complete, it can be sent to a fabrication plant for manufacturing.\n\nFor systems of intermediate complexity, engineers may use VHDL modeling for programmable logic devices and FPGAs.\n\nIntegrated circuits, FPGAs and other electrical components can then be assembled on printed circuit boards to form more complicated circuits. Today, printed circuit boards are found in most electronic devices including televisions, computers and audio players.\n\nElectronic engineering has many subfields. This section describes some of the most popular subfields in electronic engineering; although there are engineers who focus exclusively on one subfield, there are also many who focus on a combination of subfields.\n\nSignal processing deals with the analysis and manipulation of signals. Signals can be either analog, in which case the signal varies continuously according to the information, or digital, in which case the signal varies according to a series of discrete values representing the information.\n\nFor analog signals, signal processing may involve the amplification and filtering of audio signals for audio equipment or the modulation and demodulation of signals for telecommunications. For digital signals, signal processing may involve the compression, error checking and error detection of digital signals.\n\nTelecommunications engineering deals with the transmission of information across a channel such as a co-axial cable, optical fiber or free space.\n\nTransmissions across free space require information to be encoded in a carrier wave in order to shift the information to a carrier frequency suitable for transmission, this is known as modulation. Popular analog modulation techniques include amplitude modulation and frequency modulation. The choice of modulation affects the cost and performance of a system and these two factors must be balanced carefully by the engineer.\n\nOnce the transmission characteristics of a system are determined, telecommunication engineers design the transmitters and receivers needed for such systems. These two are sometimes combined to form a two-way communication device known as a transceiver. A key consideration in the design of transmitters is their power consumption as this is closely related to their signal strength. If the signal strength of a transmitter is insufficient the signal's information will be corrupted by noise.\n\nElectromagnetics is an in-depth study about the signals that are transmitted in a channel (Wired or Wireless). This includes Basics of Electromagnetic waves, Transmission Lines and Waveguides, Antennas, its types and applications with Radio-Frequency (RF) and Microwaves. Its applications are seen widely in other sub-fields like Telecommunication, Control and Instrumentation Engineering.\n\nControl engineering has a wide range of applications from the flight and propulsion systems of commercial airplanes to the cruise control present in many modern cars. It also plays an important role in industrial automation.\n\nControl engineers often utilize feedback when designing control systems. For example, in a car with cruise control, the vehicle's speed is continuously monitored and fed back to the system which adjusts the engine's power output accordingly. Where there is regular feedback, control theory can be used to determine how the system responds to such feedback.\n\nInstrumentation engineering deals with the design of devices to measure physical quantities such as pressure, flow and temperature. These devices are known as instrumentation.\n\nThe design of such instrumentation requires a good understanding of physics that often extends beyond electromagnetic theory. For example, radar guns use the Doppler effect to measure the speed of oncoming vehicles. Similarly, thermocouples use the Peltier–Seebeck effect to measure the temperature difference between two points.\n\nOften instrumentation is not used by itself, but instead as the sensors of larger electrical systems. For example, a thermocouple might be used to help ensure a furnace's temperature remains constant. For this reason, instrumentation engineering is often viewed as the counterpart of control engineering.\n\nComputer engineering deals with the design of computers and computer systems. This may involve the design of new computer hardware, the design of PDAs or the use of computers to control an industrial plant. Development of embedded systems—systems made for specific tasks (e.g., mobile phones)—is also included in this field. This field includes the micro controller and its applications.\nComputer engineers may also work on a system's software. However, the design of complex software systems is often the domain of software engineering, which is usually considered a separate discipline.\n\nVLSI design engineering VLSI stands for \"very large scale integration\". It deals with fabrication of ICs and various electronic components.\n\nElectronics engineers typically possess an academic degree with a major in electronic engineering. The length of study for such a degree is usually three or four years and the completed degree may be designated as a Bachelor of Engineering, Bachelor of Science, Bachelor of Applied Science, or Bachelor of Technology depending upon the university. Many UK universities also offer Master of Engineering (MEng) degrees at the graduate level.\n\nSome electronics engineers also choose to pursue a postgraduate degree such as a Master of Science, Doctor of Philosophy in Engineering, or an Engineering Doctorate. The master's degree is being introduced in some European and American Universities as a first degree and the differentiation of an engineer with graduate and postgraduate studies is often difficult. In these cases, experience is taken into account. The master's degree may consist of either research, coursework or a mixture of the two. The Doctor of Philosophy consists of a significant research component and is often viewed as the entry point to academia.\n\nIn most countries, a bachelor's degree in engineering represents the first step towards certification and the degree program itself is certified by a professional body. Certification allows engineers to legally sign off on plans for projects affecting public safety. After completing a certified degree program, the engineer must satisfy a range of requirements, including work experience requirements, before being certified. Once certified the engineer is designated the title of Professional Engineer (in the United States, Canada, and South Africa), Chartered Engineer or Incorporated Engineer (in the United Kingdom, Ireland, India, and Zimbabwe), Chartered Professional Engineer (in Australia and New Zealand) or European Engineer (in much of the European Union).\n\nA degree in electronics generally includes units covering physics, chemistry, mathematics, project management and specific topics in electrical engineering. Initially, such topics cover most, if not all, of the subfields of electronic engineering. Students then choose to specialize in one or more subfields towards the end of the degree.\n\nFundamental to the discipline are the sciences of physics and mathematics as these help to obtain both a qualitative and quantitative description of how such systems will work. Today most engineering work involves the use of computers and it is commonplace to use computer-aided design and simulation software programs when designing electronic systems. Although most electronic engineers will understand basic circuit theory, the theories employed by engineers generally depend upon the work they do. For example, quantum mechanics and solid state physics might be relevant to an engineer working on VLSI but are largely irrelevant to engineers working with embedded systems.\n\nApart from electromagnetics and network theory, other items in the syllabus are particular to \"electronics\" engineering course. \"Electrical\" engineering courses have other specialisms such as machines, power generation and distribution. This list does not include the extensive engineering mathematics curriculum that is a prerequisite to a degree.\n\nElements of vector calculus: divergence and curl; Gauss' and Stokes' theorems, Maxwell's equations: differential and integral forms. Wave equation, Poynting vector. Plane waves: propagation through various media; reflection and refraction; phase and group velocity; skin depth. Transmission lines: characteristic impedance; impedance transformation; Smith chart; impedance matching; pulse excitation. Waveguides: modes in rectangular waveguides; boundary conditions; cut-off frequencies; dispersion relations. Antennas: Dipole antennas; antenna arrays; radiation pattern; reciprocity theorem, antenna gain.\n\nNetwork graphs: matrices associated with graphs; incidence, fundamental cut set, and fundamental circuit matrices. Solution methods: nodal and mesh analysis. Network theorems: superposition, Thevenin and Norton's maximum power transfer, Wye-Delta transformation. Steady state sinusoidal analysis using phasors. Linear constant coefficient differential equations; time domain analysis of simple RLC circuits, Solution of network equations using Laplace transform: frequency domain analysis of RLC circuits. 2-port network parameters: driving point and transfer functions. State equations for networks.\n\nElectronic devices: Energy bands in silicon, intrinsic and extrinsic silicon. Carrier transport in silicon: diffusion current, drift current, mobility, resistivity. Generation and recombination of carriers. p-n junction diode, Zener diode, tunnel diode, BJT, JFET, MOS capacitor, MOSFET, LED, p-i-n and avalanche photo diode, LASERs. Device technology: integrated circuit fabrication process, oxidation, diffusion, ion implantation, photolithography, n-tub, p-tub and twin-tub CMOS process.\n\nAnalog circuits: Equivalent circuits (large and small-signal) of diodes, BJT, JFETs, and MOSFETs. Simple diode circuits, clipping, clamping, rectifier. Biasing and bias stability of transistor and FET amplifiers. Amplifiers: single-and multi-stage, differential, operational, feedback and power. Analysis of amplifiers; frequency response of amplifiers. Simple op-amp circuits. Filters. Sinusoidal oscillators; criterion for oscillation; single-transistor and op-amp configurations. Function generators and wave-shaping circuits, Power supplies.\n\nDigital circuits: Boolean functions (NOT, AND, OR, XOR...). Logic gates digital IC families (DTL, TTL, ECL, MOS, CMOS). Combinational circuits: arithmetic circuits, code converters, multiplexers and decoders. Sequential circuits: latches and flip-flops, counters and shift-registers. Sample and hold circuits, ADCs, DACs. Semiconductor memories. Microprocessor 8086: architecture, programming, memory and I/O interfacing.\n\nDefinitions and properties of Laplace transform, continuous-time and discrete-time Fourier series, continuous-time and discrete-time Fourier Transform, z-transform. Sampling theorems. Linear Time-Invariant (LTI) Systems: definitions and properties; causality, stability, impulse response, convolution, poles and zeros frequency response, group delay, phase delay. Signal transmission through LTI systems. Random signals and noise: probability, random variables, probability density function, autocorrelation, power spectral density, function analogy between vectors & functions.\n\nBasic control system components; block diagrammatic description, reduction of block diagrams — Mason's rule. Open loop and closed loop (negative unity feedback) systems and stability analysis of these systems. Signal flow graphs and their use in determining transfer functions of systems; transient and steady state analysis of LTI control systems and frequency response. Analysis of steady-state disturbance rejection and noise sensitivity.\n\nTools and techniques for LTI control system analysis and design: root loci, Routh-Hurwitz stability criterion, Bode and Nyquist plots. Control system compensators: elements of lead and lag compensation, elements of Proportional-Integral-Derivative controller (PID). Discretization of continuous time systems using Zero-order hold (ZOH) and ADCs for digital controller implementation. Limitations of digital controllers: aliasing. State variable representation and solution of state equation of LTI control systems. Linearization of Nonlinear dynamical systems with state-space realizations in both frequency and time domains. Fundamental concepts of controllability and observability for MIMO LTI systems. State space realizations: observable and controllable canonical form. Ackermann's formula for state-feedback pole placement. Design of full order and reduced order estimators.\n\nAnalog communication systems: amplitude and angle modulation and demodulation systems, spectral analysis of these operations, superheterodyne noise conditions.\n\nDigital communication systems: pulse-code modulation (PCM), differential pulse-code modulation (DPCM), delta modulation (DM), digital modulation – amplitude, phase- and frequency-shift keying schemes (ASK, PSK, FSK), matched-filter receivers, bandwidth consideration and probability of error calculations for these schemes, GSM, TDMA.\n\nProfessional bodies of note for electrical engineers include the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Electrical Engineers (IEE) (now renamed the Institution of Engineering and Technology or IET). Members of the Institution of Engineering and Technology (MIET) are recognized professionally in Europe, as Electrical and computer (technology) engineers. The IEEE claims to produce 30 percent of the world's literature in electrical/electronic engineering, has over 430,000 members, and holds more than 450 IEEE sponsored or cosponsored conferences worldwide each year. SMIEEE is a recognised professional designation in the United States.\n\nFor most engineers not involved at the cutting edge of system design and development, technical work accounts for only a fraction of the work they do. A lot of time is also spent on tasks such as discussing proposals with clients, preparing budgets and determining project schedules. Many senior engineers manage a team of technicians or other engineers and for this reason, project management skills are important. Most engineering projects involve some form of documentation and strong written communication skills are therefore very important.\n\nThe workplaces of electronics engineers are just as varied as the types of work they do. Electronics engineers may be found in the pristine laboratory environment of a fabrication plant, the offices of a consulting firm or in a research laboratory. During their working life, electronics engineers may find themselves supervising a wide range of individuals including scientists, electricians, computer programmers and other engineers.\n\nObsolescence of technical skills is a serious concern for electronics engineers. Membership and participation in technical societies, regular reviews of periodicals in the field and a habit of continued learning are therefore essential to maintaining proficiency. And these are mostly used in the field of consumer electronics products.\n\n\n"}
{"id": "9977526", "url": "https://en.wikipedia.org/wiki?curid=9977526", "title": "Emerging adulthood and early adulthood", "text": "Emerging adulthood and early adulthood\n\nEmerging adulthood is a phase of the life span between adolescence and full-fledged adulthood which encompasses late adolescence and early adulthood, proposed by Jeffrey Arnett in a 2000 article in the \"American Psychologist\". It primarily describes people living in developed countries, but it is also experienced by young people in urban wealthy families in the Global South. The term describes young adults who do not have children, do not live in their own home, or do not have sufficient income to become fully independent. Arnett suggests emerging adulthood is the distinct period between 18 and 25 years of age where adolescents become more independent and explore various life possibilities. Arnett argues that this developmental period can be isolated from adolescence and young adulthood. Emerging adulthood is a new demographic, is contentiously changing, and some believe that twenty-somethings have always struggled with \"identity exploration, instability, self-focus, and feeling in-between\". Arnett called this period \"roleless role\" because emerging adults do a wide variety of activities, but are not constrained by any sort of \"role requirements\". The developmental theory is highly controversial within the developmental field, and developmental psychologists argue over the legitimacy of Arnett's theories and methods.\n\nCoined by psychology professor Jeffrey Arnett, emerging adulthood has been known variously as \"transition age youth\", \"delayed adulthood\", \"extended adolescence\", \"youthhood\", \"adultolescence\", and \"the twixter years\". Of the various terms, \"emerging adulthood\" has become popular among sociologists, psychologists, and government agencies as a way to describe this period of life in between adolescence and young adulthood.\n\nCompared to other terms that have been used which give the impression that this stage is just a \"last hurrah\" of adolescence, \"emerging adulthood\" recognizes the uniqueness of this period of life. Currently, it is appropriate to define adolescence as the period spanning ages 12 to 18. This is because people in this age group in the United States typically live at home with their parents, are undergoing pubertal changes, attend middle schools and high schools and are involved in a \"school-based peer culture\". All of these characteristics are no longer normative after the age of 18, and it is, therefore, considered inappropriate to call young adults \"adolescence\" or \"late adolescence\". Furthermore, in the United States, the age of 18 is the age at which people are able to legally vote and citizens are granted full rights upon turning 21 years of age.\n\nAccording to Arnett, the term \"young adulthood\" suggests that adulthood has already been reached, but most people in the emerging adulthood stage no longer consider themselves adolescents, but do not see themselves entirely as adults either. In the past, milestones such as finishing secondary school, finding a job, and getting married clearly marked the entrance to adulthood, but in modern, post-industrialized nations, as positions requiring a college degree have become more common and the average age of marriage has become older the length of time between leaving adolescence and reaching these milestones has been extended, delaying the age at which many young people fully enter adulthood. If the years 18–25 are classified as \"young adulthood\", Arnett believes it is then difficult to find an appropriate term for the thirties. Emerging adults are still in the process of obtaining an education, are unmarried, and are childless. By age thirty, most of these individuals do see themselves as adults, based on the belief that they have more fully formed \"individualistic qualities of character\" such as self-responsibility, financial independence, and independence in decision-making. Arnett suggests that many of the individualistic characteristics associated with adult status correlate to, but are not dependent upon, the role responsibilities associated with a career, marriage, and/or parenthood.\n\nOne of the most important features of emerging adulthood is that this age period allows for exploration in love, work, and worldviews more than any other age period. The process of identity formation emerges in adolescence but mostly takes place in emerging adulthood. Regarding love, although adolescents in the United States usually begin dating between ages 12 and 14, they usually view this dating as recreational. It is not until emerging adulthood that identity formation in love becomes more serious. Emerging adults are considering their own developing identities as a reference point for a lifetime relationship partner, so they explore romantically and sexually as there is less parental control. While in the United States during adolescence dating usually occurs in groups and in situations such as parties and dances, in emerging adulthood, relationships last longer and often include sexual relations as well as cohabitation.\n\nAs far as work, the majority of working adolescents in the United States tend to see their jobs as a way to make money for recreational activities rather than preparing them for a future career. In contrast, 18- to 25-year-olds in emerging adulthood view their jobs as a way to obtain the knowledge and skills that will prepare them for their future adulthood careers. Because emerging adults have the possibility of having numerous work experiences, they are able to figure out what type of work they are good at as well find what type of work they want to pursue for the rest of their life. Undergoing changes in worldviews is a main division of cognitive development during emerging adulthood.\n\nPeople in emerging adulthood that choose to attend college often begin college or university with the worldview they were raised with and learned in childhood and adolescence. However, emerging adults who have attended college or university have been exposed to and have considered different worldviews, and eventually commit to a worldview that is distinct from the worldview with which they were raised by the end of their college or university career.\n\nWhen Americans between the ages of 18 and 25 are asked whether they believe they have reached adulthood, most do not answer with a \"no\" or a \"yes\", but answer with \"In some respects yes, in some respects no\". It is clear from this ambiguity that most emerging adults in the United States feel they have completed adolescence but not yet entered adulthood.\n\nA number of studies have shown that regarding people in their late teens and early twenties in the United States, demographic qualities such as completing their education, finding a career, getting married, and becoming parents are not the criteria used in determining whether they have reached adulthood. Rather, the criteria that determine whether adulthood has been reached are character qualities, such as being able to make independent decisions and taking responsibility for one's self. In America, these character qualities are usually experienced in the mid to late twenties, thus confirming that emerging adulthood is distinct subjectively.\n\nEmerging adulthood is the sole age period where there is nothing that is demographically consistent. At this time, adolescents in the United States up to age 20, over 95% live at home with at least one parent, 98% are not married, under 10% have become parents, and more than 95% attend school. Similarly, people in their thirties are also demographically normative: 75% are married, 75% are parents, and under 10% attend school. Residential status and school attendance are two reasons that the period of emerging adulthood is incredibly distinct demographically. Regarding residential status, emerging adults in the United States have very diverse living situations. About one third of emerging adults attend college and spend a few years living independently while partially relying on adults.\n\nContrastingly, 40% of emerging adults do not attend college but live independently and work full-time. Finally, around two-thirds of emerging adults in the United States cohabitate with a romantic partner. Regarding school attendance, emerging adults are extremely diverse in their educational paths (Arnett, 2000, p. 470-471). Over 60% of emerging adults in the United States enter college or university the year after they graduate from high school. However, the years that follow college are extremely diverse – only about 32% of 25- to 29-year-olds have finished four or more years of college.\n\nThis is because higher education is usually pursued non-continuously, where some pursue education while they also work, and some do not attend school for periods of time. Further contributing to the variance, about one third of emerging adults with bachelor's degrees pursue a postgraduate education within a year of earning their bachelor's degree. Thus, because there is so much demographic instability, especially in residential status and school attendance, it is clear that emerging adulthood is a distinct entity based on its demographically non-normative qualities, at least in the United States. Some emerging adults end up moving back home after college graduation, which tests the demographic of dependency. During college, they may be completely independent, but that could quickly change afterwards when they are trying to find a full-time job with little direction on where to start their career.\n\nEmerging adulthood and adolescence differ significantly with regard to puberty and hormonal development. While there is considerable overlap between the onset of puberty and the developmental stage referred to as adolescence, there are considerably fewer hormonal and physical changes taking place in individuals between the ages of 18–25. Emerging adults have reached a stage of full hormonal maturity and are fully, physically equipped for sexual reproduction.\n\nEmerging adulthood is usually thought of as a time of peak physical health and performance as individuals are usually less susceptible to disease and more physically agile during this period than later stages of adulthood. However, emerging adults are generally more likely to contract sexually transmitted infections, as well as to adopt unhealthy behavioral patterns and lifestyle choices.\n\nWhile many people believe that the brains of emerging adults are fully developed, they are in fact still developing into their adult forms. Many connections within the brain are strengthened and those that are unused are pruned away. Several brain structures develop that allow for greater processing of emotions and social information. Areas of the brain used for planning and for processing risk and rewards also undergo important developments during this stage. These developments in brain structure and the resulting implications are one factor that leads emerging adults to be considered more mature than adolescents. This is due to the fact that they make fewer impulsive decisions and rely more on planning and evaluating of situations.\n\nWhile brain structures continue to develop during emerging adulthood, the cognition of emerging adults is an area that receives the majority of attention. Arnett explains, \"Emerging adulthood is a critical stage for the emergence of complex forms of thinking required in complex societies.\" Crucial changes take place in their sense of self and capacity for self-reflection. At this stage, emerging adults often decide on a particular worldview and are able to recognize that other perspectives exist and are valid as well. While cognition generally becomes more complex, education level plays an important role in this development. Not all emerging adults reach the same advanced level in cognition because of the variety of education received during this age period.\n\nMuch research has been directed at studying the onset of lifetime DSM disorders to dispel the common thought that most disorders begin earlier in life. Because of this reasoning, many people that show signs of disorders do not seek help due to its stigmatization. The research shows that those with various disorders will not feel symptoms until emerging adulthood. Kessler and Merikangas reported that \"50% of emerging adults between the ages of 18 and 25 experience at least one psychiatric disorder.\" Not only is the emergence of various disorders prevalent in emerging adulthood, but the chance of developing a disorder drastically decreases at age 28.\n\nSeventy-five percent of any lifetime DSM-IV anxiety, mood, impulse-control and substance abuse disorder begins before age 24. Most onsets at this age will not be, or become, comorbid. The median onset interquartile range of substance use disorders is 18–27, while the median onset age is 20. The median onset age of mood disorders is 25.\n\nEven disorders that begin earlier, like schizophrenia spectrum diagnoses, can reveal themselves within the age range of emerging adulthood. Often, patients will not seek help until several years of symptoms have passed, if at all. For example, those diagnosed with social anxiety disorder will rarely seek treatment until age 27 or later. Typically, symptoms of more severe disorders, such as major depression, begin at age 25 as well.\n\nWith the exception of some phobias, symptoms of many disorders begin to appear and are diagnosable during emerging adulthood. Major efforts have been taken to educate the public and influence those with symptoms to seek treatment past adolescence. There is minimal but intriguing evidence that those who attend college appear to have less of a chance of showing symptoms of DSM-IV disorders. In one study, \"they were significantly less likely to have a diagnosis of drug use disorder or nicotine dependence\". In addition, \"bipolar disorder was less common in individuals attending college\". However, other research reports that chance of alcohol abuse and addiction is increased with college student status.\n\nEmerging adulthood is characterized by a reevaluation of the parent-child relationship, primarily in regard to autonomy. As a child switches from the role of a dependent to the role of a fellow adult, the family dynamic changes significantly. At this stage, it is important that parents acknowledge and accept their child's status as an adult. This process may include gestures such as allowing increased amounts of privacy and extending trust. Granting this recognition assists the increasingly independent offspring in forming a strong sense of identity and exploration at a time when it is most crucial.\n\nThere is varied evidence regarding the continuity of emerging adults' relationships with parents, although most of the research supports the fact that there is moderate stability. A parent-child relationship of higher quality often results in greater affection and contact in emerging adulthood. Attachment styles tend to remain stable from infancy to adulthood. An initial secure attachment assists in healthy separation from parents while still retaining intimacy, resulting in adaptive psychological function. Changes in attachment are often associated with negative life events, as described below.\n\nDivorce and remarriage of parents often result in a weaker parent-child relationship, even if no adverse effects were apparent during childhood. When parental divorce occurs in early adulthood, it has a strong, negative impact on the child's relationship with their father.\n\nHowever, if parents and children maintain a good relationship throughout the divorce process, it could act as a buffer and reduce the negative effects of the experience. A positive parent-child relationship after parental divorce may also be facilitated by the child's understanding of divorce. Understanding the complexity of the situation and not dwelling on the negative aspects may actually assist a young adult's adjustment, as well as their success in their own romantic relationships.\n\nDespite the increasing need for autonomy that emerging adults experience, there is also a continuing need for support from parents, although this need is often different and less dependent than that of children and earlier adolescents. Many people over the age of 18 still require financial support in order to further their education and career, despite an otherwise independent lifestyle. Furthermore, emotional support remains important during this transition period. Parental engagement with low marital conflict results in better adjustment for college students. This balance of autonomy and dependency may seem contradictory, but relinquishing control while providing necessary support may strengthen the bond between parents and offspring and may even provide space for children to be viewed as sources of support.\n\nParental support may come in the form of co-residence, which has varied effects on an emerging adult's adjustment. The proportion of young adults living with their parents has steadily increased in recent years, largely due to financial strain, difficulty finding employment, and the necessity of higher education in the job field. The economic benefit of a period of co-residence may assist an emerging adult in exploration of career options. In households with lower socioeconomic status, this arrangement may have the added benefit of the young adult providing support for the family, both financial and otherwise.\n\nCo-residence can also have negative effects on an emerging adult's adjustment and autonomy. This may hinder parents' ability to acknowledge their child as an adult, while home-leaving promotes psychological growth and satisfying adult-to-adult relationships with parents characterized by less confrontation. Living in physically separate households can help both a young adult and a parent acknowledge the changing nature of their relationship.\n\nThere are a wide variety of factors that influence sexual relationships during emerging adulthood; this includes beliefs about certain sexual behaviors and marriage. For example, among emerging adults in the United States, it is common for oral sex to not be considered \"real sex\". In the 1950s and 1960s, about 75% of people between the ages of 20–24 engaged in premarital sex. Today, that number is 90%. Unintended pregnancy and sexually transmitted infections and diseases (STIs/STDs) are a central issue. As individuals move through emerging adulthood, they are more likely to engage in monogamous sexual relationships and practice safe sex.\n\nAcross most OECD countries, marriage rates are falling, the age at first marriage is rising, and cohabitation among unmarried couples is increasing. The Western European marriage pattern has traditionally been characterised by marriage in the mid twenties, especially for women, with a generally small age difference between the spouses, a significant proportion of women who remain unmarried, and the establishment of a neolocal household after the couple has married.\n\nHousing affordability has been linked to home ownership rates, and demographic researchers have argued for a link between the rising age at first marriage and the rising age of first home ownership.\n\nDemographers distinguish between developing countries, which constitute more than 80% of the world's population, and the economically advanced, industrialized nations that form the Organization for Economic Co-Operation and Development (OECD). This includes countries like the United States, Canada, Western Europe, Japan, South Korea, and Australia, all of which have significantly higher median incomes and educational attainment and significantly lower rates of illness, disease, and early death.\n\nThe theory of emerging adulthood is specifically applicable to cultures within these OECD nations, and as a stage of development has only emerged over the past half century. It is specific to \"certain cultural-demographic conditions, specifically widespread education and training beyond secondary school and entry into marriage and parenthood in the late twenties (or early thirties) or beyond\".\n\nFurthermore, emerging adulthood occurs only within societies that allow for occupational shifts, with emerging adults often experiencing frequent job changes before settling on particular job by the age of 30. Arnett also argues that emerging adulthood happens in cultures that allow for a period of time between adolescence and marriage, the marker of adulthood. Such marital and occupational instability found among emerging adults can be attributed to the strong sense of individualization found in cultures that allow for this stage of development; in individualized cultures, traditional familial and institutional constraints have become less pronounced than in previous times or in unindustrialized/developing cultures, allowing for more personal freedom in life decisions. However, emerging adulthood even occurs in industrialized nations that do not value individualization, as is the case in some Asian countries discussed below.\n\nUp until the latter portion of the 20th century in OECD countries, and contemporarily in developing countries around the world, young people made the transition from adolescence to young adulthood around or by the age of 22, when they settled into long-lasting, obligation-filled familial and occupational roles. Therefore, in societies where this trend still prevails, emerging adulthood does not exist as a widespread stage of development.\n\nAmong OECD countries, there is a general \"one size fits all\" model in regards to emerging adulthood, having all undergone the same demographic changes that resulted in this new stage of development between adolescence and young adulthood. However, the shape emerging adulthood takes can even vary between different OECD countries, and researchers have only recently begun exploring such cross-national differences. For instance, researchers have determined that Europe is the area where emerging adulthood lasts the longest, with high levels of government assistance and median marriage ages nearing 30, compared to the U.S. where the median marriage age is 27.\n\nEmerging adult communities in East Asia may be most dissimilar from their European and American counterparts, for while they share the benefits of affluent societies with strong education and welfare systems, they do not share as strong a sense of individualization. Historically and currently, East Asian cultures have emphasized collectivism more so than those in the West. For instance, while Asian emerging adults similarly engage in individualistic identity exploration and personal development, they do so within more constrictive boundaries set by familial obligation. For example, European and American emerging adults consistently list financial independence as a key marker of adulthood, while Asian emerging adults consistently list capable of supporting parents financially as a marker with equal weight. Furthermore, while casual dating and premarital sex has become normative in the West, in Asia parents still discourage such practices, where they remain \"rare and forbidden\". In fact, about 75% of emerging adults in the U.S. and Europe report having had premarital sexual relations by the age of 20, whereas less than 20% in Japan and South Korea reported the same.\n\nWhile emerging adulthood exemplars are found mainly within the middle and upper classes of OECD countries, the stage of development still seems to occur across classes, with the main difference between different ones being length—on average, young people in lower social classes tend to enter adulthood two years before those in upper classes.\n\nWhile emerging adulthood occurs on a wide scale only in OECD countries, developing countries may also exhibit similar phenomena in certain population subgroups. In contrast to those in poor or rural parts of developing nations, who have no emerging adulthood and sometimes no adolescence due to comparatively early entry into marriage and adult-like work, young people in wealthier urban classes have begun to enter stages of development that resemble emerging adulthood, and the amount to do so is rising. Such individuals may develop a bicultural or hybrid identity, with part of themselves identifying with local culture and another part participating in the professional culture of the global economy. One finds examples of such a situation among the middle class young people in India, who lead the globalized economic sector while still, for the most part, preferring to have arranged marriages and taking care of their parents in old age. While it is more common for emerging adulthood to occur in OECD countries, it is not always true that all young people of those societies have the opportunity to experience these years of change and exploration.\n\nEmerging adulthood is not just an idea being talked about by psychologists, the media has propagated the concept as well. Hollywood has produced multiple movies where the main conflict seems to be a \"grown\" adult's reluctance to actually \"grow\" up and take on responsibility. \"Failure to Launch\" and \"Step Brothers\" are extreme examples of this concept. While most takes on emerging adulthood (and the problems that it can cause) are shown in a light-humored attempt to poke fun at the idea, a few films have taken a more serious approach to the plight. \"Adventureland\", \"Take Me Home Tonight\", \"Cyrus\" and \"Jeff, Who Lives at Home\" are comedy-dramas that exhibit the plight of today's emerging adult. Television also is capitalizing on the concept of emerging adulthood with sitcoms such as \"$h*! My Dad Says\" and \"Big Lake\".\n\nHowever, it is not just on television where society sees the world becoming aware of this trend. In spring 2010, \"The New Yorker\" magazine showcased a picture of a post-grad hanging his PhD on the wall of his bedroom as his parents stood in the doorway. People do not have to seek out these media sources to find documentation of the emerging adulthood phenomenon. News sources about the topic are abundant. Nationwide, it is being found that people entering their 20s are faced with multitudes of living problems creating problems that this age group has received a lot of attention for. The Occupy movement is an example of what has happened to the youth of today and exhibits the frustration of today's emerging adults. Other television shows and films showcasing emerging/early adulthood are \"Girls\", \"How I Met Your Mother\", and \"Less Than Zero.\"\n\nThe concept of emerging adulthood has not been without its criticisms. Sociologists have pinpointed that it neglects class differences. While it might be true that middle class children in Western societies are spoiled for choice and can afford to postpone life decisions, there are other young people who have no choices at all, and stay in the parental home not because they want to, but because they cannot afford a life of their own: They experience a period of \"arrested adulthood\".\n\nA more theoretical criticism comes from developmental psychologists, who regard all stage theories as outdated. They argue that development is a dynamic interactive process, which is different for every individual, because every individual has their own experiences. Inventing a stage that only describes (not explains) a time period in the life of a few individuals (mostly white middle class young people living in Western societies within this decade), and has nothing to say about people living in different conditions or different points in history is not a scientific approach.\n\nArnett has taken up some of these critical points in public discussion.\n\n\n"}
{"id": "52458762", "url": "https://en.wikipedia.org/wiki?curid=52458762", "title": "Estuarine turbidity maximum", "text": "Estuarine turbidity maximum\n\nAn estuarine turbidity maximum, or ETM, is the zone of highest turbidity resulting from turbulent resuspension of sediment and flocculation of particulate matter in an estuary. The turbulence is driven by tidal forces, waves, and density-drive currents that push a salt wedge upstream and beneath outflowing freshwater discharge.\n"}
{"id": "50302335", "url": "https://en.wikipedia.org/wiki?curid=50302335", "title": "Feathers: The Evolution of a Natural Miracle", "text": "Feathers: The Evolution of a Natural Miracle\n\nFeathers: The Evolution of a Natural Miracle is a 2011 natural history book by American conservation biologist Thor Hanson. Published by Basic Books and written for general audiences, the book discusses the significance of feathers, their evolution, and their history both in nature and in use by humans.\n\n\"Feathers\" is divided into five parts. In \"Evolution\", Hanson discusses the scientific debate over how feathers evolved, interviewing ornithologists Richard Prum and Alan Feduccia, as well as paleontologist Xing Xu. In \"Fluff\", Hanson discusses how feathers play a role in regulating a bird's body temperature and how feather insulation has also been utilized by humans. \"Flight\" discusses how flight might have evolved in birds, interviewing Prum, Feduccia, Xu, and also ornithologist Ken Dial, who described wing-assisted incline running. In \"Fancy\", Hanson discusses the role of feathers in sexual selection, as well as how humans have utilized feathers for fashion, interviewing costume and fashion designers on the Las Vegas Strip and New York City. \"Function\" discusses how feathers have been adapted for other purposes, such as waterproofing, fly fishing, and quill pens.\n\nHanson decided he would write \"Feathers\" after going on a run and noticing a feather dropped at his feet by a vulture; coincidentally, he had considered writing a story involving vulture feathers earlier that day. The book was published first in hardcover by Basic Books in 2011, then in paperback in 2012. Critical reception to \"Feathers\" has predominantly been positive, with praise for Hanson's enthusiasm and writing. In 2012, \"Feathers\" won the Pacific Northwest Booksellers Association Award and the \"SB&F\" Prize in the Young Adult Science Book category, presented by the American Association for the Advancement of Science and Subaru. In 2013, the book was also awarded the John Burroughs Medal.\n\nThor Hanson is a researcher with a variety of interests. In addition to writing for general audiences, his published research includes the impact of forest fragmentation on bird nest predation, the impact of human warfare on conservation policy, the behavior of Neotropical monkeys and birds, and others. Hanson received a Ph.D. from the University of Idaho in 2006; for his dissertation, he studied the impact of habitat fragmentation on the ecology of a type of tropical tree.\n\nHanson's interest in feathers began after observing vultures in Kenya. According to Hanson, when brainstorming ideas for a new book, he considered a story which involved vultures and their feathers, and later that day, as he went on a run, he noticed four turkey vultures keeping watch over a roadkill deer. One of the vultures began flapping and dropped a feather at Hanson's feet. Moved by this coincidence, Hanson decided then to write a book about feathers. Basic Books first published \"Feathers\" in hardcover in 2011. In 2012, Basic Books published it in paperback as well, and the book has also been published as an ebook and audiobook. An essay-length adaptation titled \"The Multiple Miracles of Bird Feathers\" appeared in the January–February 2012 edition of \"Audubon\", published by the National Audubon Society. \"Feathers\" is Hanson's second book, after \"The Impenetrable Forest: My Gorilla Years in Uganda\" (2008).\n\nFollowing a preface and introduction, \"Feathers\" is divided into five main parts: \"Evolution\" discusses the evolutionary history of feathers, \"Fluff\" explains the role of feathers in regulating body temperature, \"Flight\" discusses the origin of avian flight as well as its impact on human aviation, \"Fancy\" discusses the role of feathers in sexual selection and human fashion, and \"Function\" discusses the continuing evolution of feathers in both nature and human usage.\n\n\"Feathers\" begins with the 1861 discovery of the first \"Archaeopteryx\" fossil specimen and the resulting debate between English paleontologist Richard Owen, an opponent to evolution by natural selection, and Thomas Henry Huxley, an advocate for evolution. Hanson himself visits the Wyoming Dinosaur Center, a small museum in Thermopolis, Wyoming, which had acquired an \"Archaeopteryx\" specimen. Hanson interviews Richard Prum, the Coe Professor of Ornithology at Yale University, who proposed a developmental theory of feather evolution, which focuses \"on how feathers grow and not worrying about what they're used for.\" Hanson also interviews Alan Feduccia, a professor at the University of North Carolina, who disagrees with the scientific consensus that birds evolved from theropod dinosaurs. Later, Hanson interviews Xing Xu, a Chinese paleontologist who described fossil specimens that helped support Prum's developmental theory, such as \"Beipiaosaurus\" and \"Microraptor\". Hanson also discusses the development of feathers in thin-billed prions, as well as how feather growth is controlled by the Sonic hedgehog gene.\n\nIn \"Fluff\", Hanson describes his experience at Winter Ecology, a \"hands-on exploration of cold-weather ecosystems\" organized by Bernd Heinrich, a biologist at the University of Vermont. While staying in a log cabin in a remote location in western Maine, Hanson observed how golden-crowned kinglets and other birds were able to keep warm on nights when the temperature was and discusses the role of feathers in maintaining a comfortable body temperature for birds. To understand how down feathers have been adapted for human purposes, Hanson interviews Travis Stier at the Pacific Coast Feather Company, which manufactures pillows and comforters made with feathers. Later, Hanson discusses various strategies that birds employ to keep cool in warmer climates and during periods of muscular activity, such as flight and fast running, as well as the role of feathers in these strategies.\n\nScientists disagree on how feathered flight originally evolved, and Hanson describes various viewpoints on the subject. The \"ground-up\" view holds that flight originated from theropod dinosaurs running along the ground, whereas the \"tree-down\" view holds that flight originated from animals who lived in trees \"as a means to extend their hops from branch to branch\". Hanson discusses the issue with Feduccia, who argues for the \"tree-down\" view, noting that other vertebrates also developed flight from the tree down. Hanson also discusses with Prum and Xu, who state that while the origin of \"feathers\" might have been ground-based theropods, the origin of \"flight\" could have been theropods who climbed trees, especially considering four-winged feathered theropods like \"Microraptor\". Hanson interviews Ken Dial, an ornithologist who described wing-assisted incline running (WAIR), a behavior exhibited by baby birds learning how to fly. WAIR has been proposed as an alternative model for the origin of avian flight, as it addresses weaknesses in both the \"ground-up\" and \"tree-down\" views. Later, Hanson interviews Ken Franklin, who raised Frightful, a peregrine falcon whose dive was measured to be , making her the fastest flying animal on record. Franklin describes the role of feathers in improving the aerodynamics of a falcon's flight. Hanson concludes the \"Flight\" section with discussion of how feathered flight has influenced human aviation.\n\nHanson describes the behavior of birds-of-paradise and the role of their elaborate feathers in sexual selection. Hanson also visits the Las Vegas Strip and observes how feathers play a role in pageant shows like \"Jubilee!\". Hanson interviews Marios Ignadiou, the head of \"Jubilee!\"'s costume shop, as well as fashion designer Pete Menefree. Hanson then describes the history of the feather trade in fashion. In the period before World War I, feathers were highly valuable commodities. Wanting to capitalize on the economic strength of feathers, the government of South Africa sponsored the Trans-Saharan Ostrich Expedition, led by Russel William Thornton to find the Barbary ostrich. After much adversity, Thornton and his crew returned to South Africa in 1912 with 127 surviving Barbary ostriches. Unfortunately, demand for feathers in fashion decreased dramatically a few years later, once more women entered the workforce for the war effort. Hanson later interviews Leah Chalfen, a hat designer based in New York City who specializes in feathers. Hanson also interviews Jodi Favazzo, the owner of the Rainbow Feather Company, which dyes feathers.\n\nHanson rescues a grounded common murre by carrying it back to the ocean (takeoff is extremely difficult for murres if they are not on water). He then explains how the structure of feathers is waterproofing, keeping birds dry in wet conditions. Hanson interviews John Sullivan, an experienced fly fisher who explains the role of feathers in fly fishing. Later, Hanson explains the history and use of feathers in quill pens, which he uses as an example of how feathers have been adapted for other purposes beyond their natural evolutionary purposes. Hanson also describes his experience observing the behavior of vultures in Kenya, in which he discusses the lack of feathers on the heads of vultures. Hanson interviews Kimberly Bostwick, an ornithologist and curator at Cornell University Museum of Vertebrates, who discusses her research in the club-winged manakin, a bird whose feathers enable it to make sounds with its wings. In the National Museum of Natural History, Hanson interviews Carla Dove, who works in the museum's Feather Identification Lab, which identifies the species of bird which a feather originates from.\n\nIn 2012, \"Feathers\" received the Pacific Northwest Booksellers Association Award, an annual award which recognizes \"excellence in writing\" from five U.S. states in the Pacific Northwest. In the same year, \"Feathers\" also received the \"SB&F\" Prize in the Young Adult Science Book category, presented by the American Association for the Advancement of Science and Subaru. \"Feathers\" was listed among the top ten best books of 2011 in the Sci-Tech category by the \"Library Journal\". \"Feathers\" received the John Burroughs Medal in 2013, which is awarded annually by the John Burroughs Association to \"the author of a distinguished book of natural history\".\n\nReception to \"Feathers\" has been positive. Amanda Katz, in a review published by \"The New York Times\", wrote that because \"Feathers\" is a work of synthesis, bird enthusiasts will find the book's content already \"familiar\", but noted that \"as synthesis goes, it is gracious, funny, persuasive and wide ranging.\" Peter Forbes, in a review published by \"The Guardian\", stated that Hanson's \"enthusiasm is infectious\", particularly for a topic Forbes found \"alluring\". \"The Economist\" published a review which also described Hanson as having \"infectious enthusiasm\" and stated, \"Mr Hanson's unpretentious style makes what is essentially an excellent scientific work into an enjoyable read for the ignorant and uninitiated.\" Irene Wanner, in a review published by \"The Seattle Times\", highlighted Hanson's analogies, calling them \"apt\" and helping to \"simplify complex concepts\". \"Kirkus Reviews\" described \"Feathers\" as a \"delightful ramble through the byways of evolution and the wonderful world of birds.\"\n\n\"Scientific American\" published an online review of \"Feathers\" in their blog \"Tetrapod Zoology\" by Darren Naish. Naish commented that prior to Hanson's book, \"it doesn’t seem that any one book has ever been devoted to feathers and feathers alone. ... \"Feathers\" is thus a rather significant book, and very nice it is too.\" Naish also stated that \"\"Feathers\" is not the provincial view of someone only interested in ecology or conservation biology; on the contrary, this is a remarkably well-rounded review of the subject.\" In a review published in the news magazine \"Maclean's\", Brian Bethune praised Hanson's storytelling, writing, \"For all the intriguing science, what really livens up Hanson's passionate discussion of his 'natural miracle' are the stories he tells.\" Laurence A. Marschall, a physics professor at Gettysburg College, wrote in a review published in the magazine \"Natural History\", \"In sum, \"Feathers\" is an impressive blend of beauty, form, and function.\"\n\nFrances C. James, in a review published in \"The Condor\", described the book as \"scholarly and enjoyable\", commenting that \"Hanson has worked hard to summarize the science behind our current understanding of the form and function of feathers, their development and their evolution.\" Regarding Hanson's description of the origin of feathers and flight, James commented that Hanson could have taken \"an even more critical approach\" by stating that \"Prum's developmental model has not really been tested ... Confirmation of the predictions of a theory is not a test unless the predictions help discriminate among alternative hypotheses.\" James eventually concluded, \"I don't blame Hanson for not having delved more deeply into this subject. I just wish that ornithologists would evaluate alternative theories on the full weight of their evidence and stop misquoting Huxley.\"\n\nPepper W. Trail wrote a review published in \"The Quarterly Review of Biology\", commenting, \"Hanson has a gift for narrative and is an engaging companion as he leads readers through sometimes complex material.\" Trail also commented, \"There are, however, a few places where the author's zest for telling a good story led him astray.\" As an example, Trail observed that Hanson uses eight pages to discuss the Trans-Saharan Ostrich Expedition, but \"less than four on the science of feather coloration, a field burgeoning with new discoveries.\" In a review published in \"The Wilson Journal of Ornithology\", Kimberly S. Bostwick described Hanson's writing as \"engaging\", writing that there is \"something for everyone to learn\", from professional ornithologists to non-biologists. Bostwick warned scientific readers, however, that \"Feathers\" uses poetic license \"to adapt some of the historical accounts a little to make them flow as stories\" and that \"Hanson uses a few analogies to explain some of the more complex biological phenomenon that are not entirely accurate.\"\n\n\n"}
{"id": "40561059", "url": "https://en.wikipedia.org/wiki?curid=40561059", "title": "Feature geometry", "text": "Feature geometry\n\nFeature geometry is a phonological theory which represents distinctive features as a structured hierarchy rather than a matrix or a set. Feature geometry grew out of autosegmental phonology, which emphasizes the autonomous nature of distinctive features and the non-uniform relationships among them. Feature geometry recognizes that some sets of features often pattern together in phonological and phonotactic generalizations, while others rarely interact. Feature geometry thus formally encodes groups of features under nodes in a tree: features that commonly pattern together are said to share a parent node, and operations on this set can be encoded as operation on the parent node.\n\nOne node in feature geometries is the Laryngeal node. The Laryngeal node is an organizing node that dominates the features of the larynx, usually taken to be [voice], [constricted glottis], and [spread glottis]. It is common for these three features to pattern together in the phonology of the world's languages to the exclusion of every other feature, and in feature geometry, this follows from the tree representation. Similarly, feature geometries generally include a Place node that is the dominant node of the place features, which also often pattern together. Feature geometry is easily compatible with theories of underspecification and can represent incomplete segments by missing nodes.\n\nThe Root node is the topmost node of the feature tree and works as the formal organizing unit of the segment, and in some frameworks encodes the major class features such as [consonantal], [sonorant], and [approximant]. Some features such as [nasal] and [lateral] are sometimes dependent of the root node, or sometimes of a Supralaryngeal node along with Place. Other features such as [anterior] and [distributed] are usually dependent from the Coronal place feature.\n\nThe first formal model of feature geometry was introduced in print by George N. Clements in 1985, drawing on unpublished work by K.P. Mohanon and Joan Mascaró. Another precursor to feature geometry was proposed by Roger Lass in 1976, in which he proposed a laryngeal feature submatrix within a distinctive feature matrix. Other important models have been proposed by Elizabeth Sagey (1986), John J. McCarthy (1988), and Clements & Hume (1995). Models vary widely in the number of the hierarchical nodes and in how consonant and vowel features are treated.\n\nFeature geometry has attracted formal and conceptual criticism. In 2003 Charles Reiss argued that feature geometry is insufficiently powerful to account for a class of phonological rules that involve dependencies between segments, such as partial and total identity and nonidentity. Feature geometry is unable to encode these properties. In 2008 Jeff Mielke argued that feature geometry merely recapitulated physiological organization, and that since the influence of articulation on sound change will independently create patterns in the behavior of features, feature geometry recapitulates diachrony and is redundant as a theory of the mental organisation of phonology.\n\n\n"}
{"id": "1965396", "url": "https://en.wikipedia.org/wiki?curid=1965396", "title": "Fossa (animal)", "text": "Fossa (animal)\n\nThe fossa ( or ; Malagasy ; \"Cryptoprocta ferox\") is a cat-like, carnivorous mammal endemic to Madagascar. It is a member of the Eupleridae, a family of carnivorans closely related to the mongoose family (Herpestidae). Its classification has been controversial because its physical traits resemble those of cats, yet other traits suggest a close relationship with viverrids (most civets and their relatives). Its classification, along with that of the other Malagasy carnivores, influenced hypotheses about how many times mammalian carnivores have colonized Madagascar. With genetic studies demonstrating that the fossa and all other Malagasy carnivores are most closely related to each other (forming a clade, recognized as the family Eupleridae), carnivorans are now thought to have colonized the island once, around 18 to 20 million years ago.\n\nThe fossa is the largest mammalian carnivore on the island of Madagascar and has been compared to a small cougar. Adults have a head-body length of and weigh between , with the males larger than the females. It has semi-retractable claws (meaning it can extend but not retract its claws fully) and flexible ankles that allow it to climb up and down trees head-first, and also support jumping from tree to tree. The fossa is unique within its family for the shape of its genitalia, which share traits with those of cats and hyenas.\n\nThe species is widespread, although population densities are usually low. It is found solely in forested habitat, and actively hunts both by day and night. Over 50% of its diet consists of lemurs, the endemic primates found on the island; tenrecs, rodents, lizards, birds, and other animals are also documented as prey. Mating usually occurs in trees on horizontal limbs and can last for several hours. Litters range from one to six pups, which are born blind and toothless (altricial). Infants wean after 4.5 months and are independent after a year. Sexual maturity occurs around three to four years of age, and life expectancy in captivity is 20 years. The fossa is listed as a vulnerable species by the International Union for Conservation of Nature. It is generally feared by the Malagasy people and is often protected by their \"fady\" (taboo). The greatest threat to the species is habitat destruction.\n\nThe generic name \"Cryptoprocta\" refers to how the animal's anus is hidden by its anal pouch, from the Ancient Greek words \"crypto-\" \"hidden\", and \"procta\" \"anus\". The species name \"ferox\" is the Latin adjective \"fierce\" or \"wild\". Its common name is spelled fossa in English or \"fosa\" in Malagasy, the Austronesian language from which it was taken, but some authors have adopted the Malagasy spelling in English. The word is similar to \"posa\" (meaning \"cat\") in the Iban language (another Austronesian language) from Borneo, and both terms may derive from trade languages from the 1600s. However, an alternative etymology suggests a link to another word that comes from Malay: \"pusa\" refers to the Malayan weasel (\"Mustela nudipes\"). The Malay word \"pusa\" could have become \"posa\" for cats in Borneo, while in Madagascar the word could have become \"fosa\" to refer to the fossa.\n\nThe fossa was formally described by Edward Turner Bennett on the basis of a specimen from Madagascar sent by Charles Telfair in 1833. The common name is the same as the generic name of the Malagasy civet (\"Fossa fossana\"), but they are different species. Because of shared physical traits with civets, mongooses, and cats (Felidae), its classification has been controversial. Bennett originally placed the fossa as a type of civet in the family Viverridae, a classification that long remained popular among taxonomists. Its compact braincase, large eye sockets, retractable claws, and specialized carnivorous dentition have also led some taxonomists to associate it with the felids. In 1939, William King Gregory and Milo Hellman placed the fossa in its own subfamily within Felidae, the Cryptoproctinae. George Gaylord Simpson placed it back in Viverridae in 1945, still within its own subfamily, yet conceded it had many cat-like characteristics.\nIn 1993, Géraldine Veron and François Catzeflis published a DNA hybridization study suggesting that the fossa was more closely related to mongooses (family Herpestidae) than to cats or civets. However, in 1995, Veron's morphological study once again grouped it with Felidae. In 2003, molecular phylogenetic studies using nuclear and mitochondrial genes by Anne Yoder and colleagues showed that all native Malagasy carnivorans share a common ancestry that excludes other carnivores (meaning they form a clade, making them monophyletic) and are most closely related to Asian and African Herpestidae. To reflect these relationships, all Malagasy carnivorans are now placed in a single family, Eupleridae. Within Eupleridae, the fossa is placed in the subfamily Euplerinae along with the falanouc (\"Eupleres goudoti\") and Malagasy civet, but its exact relationships are poorly resolved.\n\nAn extinct relative of the fossa was described in 1902 from subfossil remains and recognized as a separate species, \"Cryptoprocta spelea\", in 1935. This species was larger than the living fossa (with a body mass estimate roughly twice as great), but otherwise similar. Across Madagascar, people distinguish two kinds of fossa—a large \"fosa mainty\" (\"black fossa\") and the smaller \"fosa mena\" (\"reddish fossa\")—and a white form has been reported in the southwest. It is unclear whether this is purely folklore or individual variation—related to sex, age or instances of melanism and leucism—or whether there is indeed more than one species of living fossa.\nThe fossa appears as a diminutive form of a large felid, such as a cougar, but with a slender body and muscular limbs, and a tail nearly as long as the rest of the body. It has a mongoose-like head, relatively longer than that of a cat, although with a muzzle that is broad and short, and with large but rounded ears. It has medium brown eyes set relatively wide apart with pupils that contract to slits. Like many carnivorans that hunt at night, its eyes reflect light; the reflected light is orange in hue. \nIts head-body length is and its tail is long. There is some sexual dimorphism, with adult males (weighing ) being larger than females (). Smaller individuals are typically found north and east on Madagascar, while larger ones to the south and west. Unusually large individuals weighing up to have been reported, but there is some doubt as to the reliability of the measurements. The fossa can smell, hear, and see well. It is a robust animal and illnesses are rare in captive fossas.\nBoth males and females have short, straight fur that is relatively dense and without spots or patterns. Both sexes are generally a reddish-brown dorsally and colored a dirty cream ventrally. When in rut, they may have an orange coloration to their abdomen from a reddish substance secreted by a chest gland, but this has not been consistently observed by all researchers. The tail tends to be lighter in coloration than the sides. Juveniles are either gray or nearly white.\n\nSeveral of the animal's physical features are adaptions to climbing through trees. It uses its tail to assist balance and has semi-retractable claws that it uses to climb trees in its search for prey. It has semiplantigrade feet, switching between a plantigrade-like gait (when arboreal) and a digitigrade-like one (when terrestrial). The soles of its paws are nearly bare and covered with strong pads. The fossa has very flexible ankles that allow it to readily grasp tree trunks so as to climb up or down trees head first or to leap to another tree. Captive juveniles have been known to swing upside down by their hindfeet from knotted ropes.\n\nThe fossa has several scent glands, although the glands are less developed in females. <onlyinclude></onlyinclude>\n\nOne of the more peculiar physical features of this species is its external genitalia. The male fossa has an unusually long penis and baculum (penis bone), reaching to between his forelegs when erect, with an average thickness of . The glans extends about halfway down the shaft and is spiny except at the tip. In comparison, the glans of felids is short and spiny, while that of viverrids is smooth and long. The female fossa exhibits transient masculinization, starting at about 1–2 years of age, developing an enlarged, spiny clitoris that resembles a male's penis. The enlarged clitoris is supported by an os clitoris, which decreases in size as the animal grows. The females do not have a pseudo-scrotum, but they do secrete an orange substance that colors their underparts, much like the secretions of males. Hormone levels (testosterone, androstenedione, dihydrotestosterone) do not seem to play a part in this transient masculinization, as those levels are the same in masculinized juveniles and non-masculinized adults. It is speculated that the transient masculinization either reduces sexual harassment of juvenile females by adult males, or reduces aggression from territorial females. While females of other mammal species (such as the spotted hyena) have a pseudo-penis, no other is known to diminish in size as the animal grows.\n\nOverall, the fossa has features in common with three different carnivoran families, leading researchers to place it and other members of Eupleridae alternatively in Herpestidae, Viverridae, and Felidae. Felid features are primarily those associated with eating and digestion, including tooth shape and facial portions of the skull, the tongue, and the digestive tract, typical of its exclusively carnivorous diet. The remainder of the skull most closely resembles skulls of genus \"Viverra\", while the general body structure is most similar to that of various members of Herpestidae. The permanent dentition is (three incisors, one canine, three or four premolars, and one molar on each side of both the upper and lower jaws), with the deciduous formula being similar but lacking the fourth premolar and the molar. The fossa has a large, prominent rhinarium similar to that of viverrids, but has comparatively larger, round ears, almost as large as those of a similarly sized felid. Its facial vibrissae (whiskers) are long, with the longest being longer than its head. Like some mongoose genera, particularly \"Galidia\" (which is now in the fossa's own family, Eupleridae) and \"Herpestes\" (of Herpestidae), it has carpal vibrissae as well. Its claws are retractile, but unlike those of Felidae species, they are not hidden in skin sheaths. It has three pairs of nipples (one inguinal, one ventral, and one pectoral).\n\nThe fossa has the most widespread geographical range of the Malagasy carnivores, and is generally found in low numbers throughout the island in remaining tracts of forest, preferring pristine undisturbed forest habitat. It is also encountered in some degraded forests, but in lower numbers. Although the fossa is found in all known forest habitats throughout Madagascar, including the western, dry deciduous forests, the eastern rainforests, and the southern spiny forests, it is seen more frequently in humid than in dry forests. This may be because the reduced canopy in dry forests provides less shade, and also because the fossa seems to travel more easily in humid forests. It is absent from areas with the heaviest habitat disturbance and, like most of Madagascar's fauna, from the central high plateau of the country.\n\nThe fossa has been found across several different elevational gradients in undisturbed portions of protected areas throughout Madagascar. In the Réserve Naturelle Intégrale d'Andringitra, evidence of the fossa has been reported at four different sites ranging from . Its highest known occurrence was reported at ; its presence high on the Andringitra Massif was subsequently confirmed in 1996. Similarly, evidence has been reported of the fossa at the elevational extremes of and in the Andohahela National Park. The presence of the fossa at these locations indicates its ability to adapt to various elevations, consistent with its reported distribution in all Madagascar forest types.\n\nThe fossa is active during both the day and the night and is considered cathemeral; activity peaks may occur early in the morning, late in the afternoon, and late in the night. The animal generally does not reuse sleeping sites, but females with young do return to the same den. The home ranges of male fossas in Kirindy Forest are up to large, compared to for females. These ranges overlap—by about 30 percent according to data from the eastern forests—but females usually have separated ranges. Home ranges grow during the dry season, perhaps because less food and water is available. In general, radio-collared fossas travel between per day, although in one reported case a fossa was observed moving a straight-line distance of in 16 hours. The animal's population density appears to be low: in Kirindy Forest, where it is thought to be common, its density has been estimated at one animal per in 1998. Another study in the same forest between 1994 and 1996 using the mark and recapture method indicated a population density of one animal per and one adult per .\n\nExcept for mothers with young and occasional observations of pairs of males, animals are usually found alone, so that the species is considered solitary. A 2009 publication, however, reported a detailed observation of cooperative hunting, wherein three male fossas hunted a sifaka (\"Propithecus verreauxi\") for 45 minutes, and subsequently shared the prey. This behavior may be a vestige of cooperative hunting that would have been required to take down larger recently extinct lemurs.\n\nFossas communicate using sounds, scents, and visual signals. Vocalizations include purring, a threatening call, and a call of fear, consisting of \"repeated loud, coarse inhalations and gasps of breath\". A long, high yelp may function to attract other fossas. Females mew during mating and males produce a sigh when they have found a female. Throughout the year, animals produce long-lasting scent marks on rocks, trees, and the ground using glands in the anal region and on the chest. They also communicate using face and body expression, but the significance of these signals is uncertain. The animal is aggressive only during mating, and males in particular fight boldly. After a short fight, the loser flees and is followed by the winner for a short distance. In captivity, fossas are usually not aggressive and sometimes even allow themselves to be stroked by a zookeeper, but adult males in particular may try to bite.\n\nThe fossa is a carnivore that hunts small to medium-sized animals. One of eight carnivorous species endemic to Madagascar, the fossa is the island's largest surviving endemic terrestrial mammal and the only predator capable of preying upon adults of all extant lemur species, the largest of which can weigh as much as 90 percent of the weight of the average fossa. Although it is the predominant predator of lemurs, reports of its dietary habits demonstrate a wide variety of prey selectivity and specialization depending on habitat and season; diet does not vary by sex. While the fossa is thought to be a lemur specialist in Ranomafana National Park, its diet is more variable in other rain forest habitats.\n\nThe diet of the fossa in the wild has been studied by analyzing their distinctive scats, which resemble gray cylinders with twisted ends and measure long by thick. Scat collected and analyzed from both Andohahela and Andringitra contained lemur matter and rodents. Eastern populations in Andringitra incorporate the widest recorded variety of prey, including both vertebrates and invertebrates. Vertebrates consumed ranged from reptiles to a wide variety of birds, including both understory and ground birds, and mammals, including insectivores, rodents, and lemurs. Invertebrates eaten by the fossa in the high mountain zone of Andringitra include insects and crabs. One study found that vertebrates comprised 94% of the diet of fossas, with lemurs comprising over 50%, followed by tenrecs (9%), lizards (9%), and birds (2%). Seeds, which comprised 5% of the diet, may have been in the stomachs of the lemurs eaten, or may have been consumed with fruit taken for water, as seeds were more common in the stomach in the dry season. The average prey size varies geographically; it is only in the high mountains of Andringitra, in contrast to in humid forests and over in dry deciduous forests. In a study of fossa diet in the dry deciduous forest of western Madagascar, more than 90% of prey items were vertebrates, and more than 50% were lemurs. The primary diet consisted of approximately six lemur species and two or three spiny tenrec species, along with snakes and small mammals. Generally, the fossa preys upon larger lemurs and rodents in preference to smaller ones.\n\nPrey is obtained by hunting either on the ground or in the trees. During the non-breeding season the fossa hunts individually, but during the breeding season hunting parties may be seen, and these may be pairs or later on mothers and young. One member of the group scales a tree and chases the lemurs from tree to tree, forcing them down to the ground where the other is easily able to capture them. The fossa is known to eviscerate its larger lemur prey, a trait that, along with its distinct scat, helps identify its kills. Long-term observations of the fossa's predation patterns on rainforest sifakas suggest that the fossa hunts in a subsection of their range until prey density is decreased, then moves on. The fossa has been reported to prey on domestic animals, such as goats and small calves, and especially chickens. Food taken in captivity includes amphibians, birds, insects, reptiles, and small- to medium-sized mammals.\n\nThis wide variety of prey items taken in various rainforest habitats is similar to the varied dietary composition noted occurring in the dry forests of western Madagascar, as well. As the largest endemic predator on Madagascar, this dietary flexibility combined with a flexible activity pattern has allowed it to exploit a wide variety of niches available throughout the island, making it a potential keystone species for the Madagascar ecosystems.\n\nFossas have a polyandrous mating system. Most of the details of reproduction in wild populations are from the western dry deciduous forests; determining whether or not certain of these details are applicable to eastern populations will require further field research. Mating typically occurs during September and October, although there are reports of its occurring as late as December, and can be highly conspicuous. In captivity in the Northern Hemisphere, fossas instead mate in the northern spring, from March to July. Intromission usually occurs in trees on horizontal limbs about off the ground. Frequently the same tree is used year after year, with remarkable precision as to the date the season commences. Trees are often near a water source, and have limbs strong enough and wide enough to support the mating pair, about wide. Some mating has been reported on the ground as well.\n\nAs many as eight males will be at a mating site, staying in close vicinity to the receptive female. The female seems to choose the male she mates with, and the males compete for the attention of the female with a significant amount of vocalization and antagonistic interactions. The female may choose to mate with several of the males, and her choice of mate does not seem to have any correlation to the physical appearance of the males. To stimulate the male to mount her, she gives a series of mewling vocalizations. The male mounts from behind, resting his body on her slightly off-center, a position requiring delicate balance; if the female were to stand, the male would have significant difficulty continuing. He places his paws on her shoulders or grasps her around the waist and often licks her neck. Mating may last for nearly three hours. This unusually lengthy mating is due to the physical nature of the male's erect penis, which has backwards-pointing spines along most of its length. Fossa mating includes a copulatory tie, which may be enforced by the male's spiny penis. The tie is difficult to break if the mating session is interrupted. Copulation with a single male may be repeated several times, with a total mating time of up to fourteen hours, while the male may remain with the female for up to an hour after the mating. A single female may occupy the tree for up to a week, mating with multiple males over that time. Also, other females may take her place, mating with some of the same males as well as others. This mating strategy, whereby the females monopolize a site and maximize the available number of mates, seems to be unique among carnivores. Recent research suggests that this system helps the fossa overcome factors which would normally impede mate-finding, such as low population density and lack of den use.\n\nThe birthing of the litter of one to six (typically two to four) takes place in a concealed location, such as an underground den, a termite mound, a rock crevice, or in the hollow of a large tree (particularly those of the genus \"Commiphora\"). Contrary to older research, litters are of mixed sexes. Young are born in December or January, making the gestation period 90 days, with the late mating reports indicating a gestational period of about six to seven weeks. The newborns are blind and toothless and weigh no more than . The fur is thin and has been described as gray-brown or nearly white. After about two weeks the cubs' eyes open, they become more active, and their fur darkens to a pearl gray. The cubs do not take solid food until three months old, and do not leave the den until they are 4.5 months old; they are weaned shortly after that. After the first year, the juveniles are independent of their mother. Permanent teeth appear at 18 to 20 months. Physical maturity is reached by about two years of age, but sexual maturity is not attained for another year or two, and the young may stay with their mother until they are fully mature. Lifespan in captivity is up to or past 20 years of age, possibly due to the slow juvenile development.\n\nThe fossa has been assessed as \"Vulnerable\" by the IUCN Red List since 2008, as its population size has probably declined by at least 30 percent between 1987 and 2008; previous assessments have included \"Endangered\" (2000) and \"Insufficiently Known\" (1988, 1990, 1994). The species is dependent on forest and thus threatened by the widespread destruction of Madagascar's native forest but is also able to persist in disturbed areas. A suite of microsatellite markers (short segments of DNA that have a repeated sequence) have been developed to help aid in studies of genetic health and population dynamics of both captive and wild fossas. Several pathogens have been isolated from the fossa, some of which, such as anthrax and canine distemper, are thought to have been transmitted by feral dogs or cats. \"Toxoplasma gondii\" was reported in a captive fossa in 2013.\n\nAlthough the species is widely distributed, it is locally rare in all regions, making fossas particularly vulnerable to extinction. The effects of habitat fragmentation increase the risk. For its size, the fossa has a lower than predicted population density, which is further threatened by Madagascar's rapidly disappearing forests and dwindling lemur populations, which make up a high proportion of its diet. The loss of the fossa, either locally or completely, could significantly impact ecosystem dynamics, possibly leading to over-grazing by some of its prey species. The total population of the fossa living within protected areas is estimated at less than 2,500 adults, but this may be an overestimate. Only two protected areas are thought to contain 500 or more adult fossas: Masoala National Park and Midongy-Sud National Park, although these are also thought to be overestimated. Too little population information has been collected for a formal population viability analysis, but estimates suggest that none of the protected areas support a viable population. If this is correct, the extinction of the fossa may take as much as 100 years to occur as the species gradually declines. In order for the species to survive, it is estimated that at least is needed to maintain smaller, short-term viable populations, and at least for populations of 500 adults.\n\nTaboo, known in Madagascar as \"fady\", offers protection for the fossa and other carnivores. In the Marolambo District (part of the Atsinanana region in Toamasina Province), the fossa has traditionally been hated and feared as a dangerous animal. It has been described as \"greedy and aggressive\", known for taking fowl and piglets, and believed to \"take little children who walk alone into the forest\". Some do not eat it for fear that it will transfer its undesirable qualities to anyone who consumes it. However, the animal is also taken for bushmeat; a study published in 2009 reported that 57 percent of villages (8 of 14 sampled) in the Makira forest consume fossa meat. The animals were typically hunted using slingshots, with dogs, or most commonly, by placing snare traps on animal paths. Near Ranomafana National Park, the fossa, along with several of its smaller cousins and the introduced small Indian civet (\"Viverricula indica\"), are known to \"scavenge on the bodies of ancestors\", which are buried in shallow graves in the forest. For this reason, eating these animals is strictly prohibited by \"fady\". However, if they wander into villages in search of domestic fowl, they may be killed or trapped. Small carnivore traps have been observed near chicken runs in the village of Vohiparara.\n\nFossas are occasionally held in captivity in zoos. They first bred in captivity in 1974 in the zoo of Montpellier, France. The next year, at a time when there were only eight fossas in the world's zoos, the Duisburg Zoo in Germany acquired one; this zoo later started a successful breeding program, and most zoo fossas now descend from the Duisburg population. Research on the Duisburg fossas has provided much data about their biology.\n\nThe fossa was depicted as an antagonist in the DreamWorks 2005 animated film \"Madagascar\", accurately shown as the lemurs' most feared predator.\n\n\n"}
{"id": "53839825", "url": "https://en.wikipedia.org/wiki?curid=53839825", "title": "George Csanak", "text": "George Csanak\n\nGeorge Csanak from the Los Alamos National Laboratory, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Division of Atomic, Molecular & Optical Physics in 1995, for \"development of many-body Green's function techniques of bound-state and scattering properties of atomic and molecular systems; significant contributions to the theoretical foundation and physical interpretation of electron-photon coincidence experiments, and for contributions to the understanding of electron scat\"\n"}
{"id": "12654604", "url": "https://en.wikipedia.org/wiki?curid=12654604", "title": "George F. Sternberg", "text": "George F. Sternberg\n\nGeorge Fryer Sternberg (1883–1969) was a paleontologist best known for his discovery in Gove County, Kansas of the \"fish-within-a-fish\" of \"Xiphactinus audax\" with a recently eaten \"Gillicus arcuatus\" within its stomach. He was the son of Charles Hazelius Sternberg and nephew of Brigadier General George M. Sternberg (1838–1915). The Sternberg Museum of Natural History at Fort Hays State University in Hays, Kansas is named for him and other members of his fossil collecting family like his father Charles Hazelius Sternberg (1850–1943), or his brother Charles Mortram Sternberg (1885–1981).\n"}
{"id": "2285868", "url": "https://en.wikipedia.org/wiki?curid=2285868", "title": "Hans Bøchmann Melchior", "text": "Hans Bøchmann Melchior\n\nHans Bøchmann Melchior (14 May 1773 – 11 September 1831) was a Danish Naturalist.\n\nHe was the author of \"Den danske Stats og Norges Pattedyr\" (\"The mammals of the Danish state and Norway\"), published posthumously in 1834.\n\n"}
{"id": "45322969", "url": "https://en.wikipedia.org/wiki?curid=45322969", "title": "Hausberg", "text": "Hausberg\n\nHausberg (lit.: \"house mountain\", plural: \"Hausberge\") is German for a prominent mountain or hill in the immediate vicinity of a village, town or city, usually located on its municipal territory, but outside the built up area. It means something like the \"local mountain\" or \"local hill\" closely associated with a settlement by its population. The \"Hausberg\" forms a backdrop to its home settlement and also offers a prominent viewing point looking over the settlement. As a result, many \"Hausberge\" have cable cars or gondola lifts to transport visitors to the top. \"Hausberg\" is also the proper name of numerous local mountains and hills in German-speaking countries.\n\nThe \"Hausberg\" does not have to lie within the town's municipal boundaries: The Pfänder, the \"Hausberg\" of the town of Bregenz in Austria, is in the municipality of Lochau and the highest summit of the Pilatus, the \"Hausberg\" of Lucerne, is even just outside the Canton of Lucerne.\n\nA hill within a town or city itself is usually referred to in German as a \"Stadtberg\".\n\nThe following list contains a selection of well known \"Hausberge\" (with heights):\n\n\n\n\n\n\n\n\n\n"}
{"id": "455622", "url": "https://en.wikipedia.org/wiki?curid=455622", "title": "Henry Hindley", "text": "Henry Hindley\n\nHenry Hindley (1701–1771) was an 18th-century clockmaker, watchmaker and maker of scientific instruments. He invented a screw-cutting lathe, a fusee-cutting engine and an improved wheel-cutting engine and made one of the first dividing engines, for the construction of accurately-graduated arcs on scientific instruments. He is thought to have made the world's first equatorially-mounted telescope, which can now be seen in Burton Constable Hall.\n\nHindley was a Roman Catholic, born in Wigan (Lancs) in 1701. He was apprenticed and made clocks in Wigan from 1726 to 1730 and moved to York in 1731, where he was established first in Petergate and then Stonegate from 1741 until his death in 1771. John Smeaton's cousin John Holmes was apprenticed to Hindley. He was succeeded by his son, who died in 1775.\n\nMost of his surviving clocks are high quality long-case clocks featuring long going and the use of deadbeat escapements, six spoke wheels, high count trains and repeating, enclosed movements. A year-going clock with a bolt-and-shutter maintaining power is in the Castle Museum York, together with four eight-day clocks. A further example, a Hindley movement of around 1740 fitted into a walnut marquetry case of ca. 1690, is at Temple Newsome House (Leeds). Others exist in private collections\n\nHe made turret clocks such as those for York Minster (much modified over the years) and Bar Convent. One of his bracket clocks may also be seen in York Minster. He made watches in some numbers: examples exist in the Science Museum, the Victoria & Albert Museum as well as the Castle Museum, York.\n\n"}
{"id": "55760582", "url": "https://en.wikipedia.org/wiki?curid=55760582", "title": "History of aluminium", "text": "History of aluminium\n\nAluminium or aluminum is a chemical element with symbol Al and atomic number 13. At standard conditions, aluminium forms a bright silvery metal; this metal is unusually light and resistant against corrosion. Chemically, aluminium is a main-group element that normally assumes the +3 oxidation state. Aluminium is the third most abundant element in the Earth's crust; as such, it is widespread in human-related activities. Aluminium is produced in tens of millions of metric tons; the metal is commonly alloyed to improve some characteristics, such as hardness. Aluminium has no biological role but is not particularly toxic.\n\nThe aluminium compound alum has been known since the 5th century BCE and was extensively used by ancients for dyeing and city defense; during the Middle Ages, the former use made alum a subject of international commerce. Scientists of the Renaissance believed alum was a salt of a new earth; during the Age of Enlightenment, it was established that the earth was an oxide of a new metal. Discovery of this metal was announced in 1825 by Danish physicist Hans Christian Ørsted, whose work was extended by German chemist Friedrich Wöhler.\n\nAluminium was difficult to refine and thus uncommon in actual usage. Soon after its discovery, the price of aluminium exceeded that of gold and was only reduced after the initiation of the first industrial production by French chemist Henri Étienne Sainte-Claire Deville in 1856. Aluminium became much more available to the general public with the Hall–Héroult process independently developed by French engineer Paul Héroult and American engineer Charles Martin Hall in 1886 and the Bayer process developed by Austrian chemist Carl Joseph Bayer in 1889. These processes have been used for aluminium production up to the present.\n\nIntroduction of these methods to mass production of aluminium led to the extensive use of the metal in industry and everyday lives. Aluminium has been used in aviation, engineering, construction, and packaging thanks to its lightness and resistance against corrosion. Its production grew exponentially in the 20th century and it became an exchange commodity in the 1970s. In 1900, production was 6,800 metric tons; in 2015, it was 57,500,000 tons.\n\nThe history of aluminium was shaped by the usage of its compound alum. The first written record of alum was made in the 5th century BCE by Greek historian Herodotus. Ancients used alum as dyeing mordants, in medicine, as a fire-resistant coating for wood, and in chemical milling. Aluminium metal was unknown. Roman historian Pliny the Elder recorded a story about a metal bright as silver but much lighter. The metal was presented to the Emperor Tiberius (reigned 14–37 CE), who had the discoverer killed so the metal would not diminish the value of his gold and silver assets. Some sources suggest this metal could be aluminium, but this has been disputed. It is possible that the Chinese produced aluminium-containing alloys during the reign of the first Jin dynasty (265–420).\n\nAfter the Crusades, alum was a subject of international commerce; it was indispensable in the European fabric industry. Small alum mines were worked in Catholic Europe but most alum came from the Middle East. Alum continued to be traded through the Mediterranean Sea until the mid-15th century, when the Ottomans greatly raised export taxes. In 1460, Giovanni da Castro, godson of the Pope Pius II, discovered a rich source of alum at Tolfa near Rome and reported excitedly to his godfather, \"today I bring you victory over the Turk\". This newly found alum long played an important role in European pharmacy but the high prices set by the papal government eventually made other states start their own production; large-scale alum mining came to other regions of Europe in the 16th century.\n\nAt the start of the Renaissance, the nature of alum remained unknown. Around 1530, Swiss physician Paracelsus recognized alum as separate from \"vitriole\" (sulfates) and suggested that it was a salt of an earth. In 1595, German doctor and chemist Andreas Libavius demonstrated alum and green and blue \"vitriole\" were formed by the same acid but different earths; for the undiscovered earth that formed alum, he proposed the name \"alumina\". In 1702, German chemist Georg Ernst Stahl stated the unknown base of alum was akin to lime or chalk; this mistaken view was shared by many scientists for half a century. In 1722, German chemist Friedrich Hoffmann suggested the base of alum was a distinct earth. In 1728, French chemist Étienne Geoffroy Saint-Hilaire claimed alum was formed by an unknown earth and sulfuric acid; he mistakenly believed burning of that earth yielded silica. In 1739, French chemist Jean Gello proved the earth in clay and the earth resulting from the reaction of an alkali on alum were identical. In 1746, German chemist Johann Heinrich Pott showed the precipitate obtained from pouring an alkali into a solution of alum was different from lime and chalk.\n\nIn 1754, German chemist Andreas Sigismund Marggraf synthesized the earth of alum by boiling clay in sulfuric acid and adding potash. He realized that adding soda, potash, or an alkali to a solution of the new earth in sulfuric acid yielded alum. He described the earth as alkaline, as he had discovered it dissolved in acids when dried. Marggraf also described salts of this earth: the chloride, the nitrate, and the acetate. In 1758, French chemist Pierre Macquer wrote that alumina resembled a metallic earth. In 1767, Swedish chemist Torbern Bergman synthesized alum by boiling alunite in sulfuric acid and adding potash to the solution. He also synthesized alum as a reaction product between sulfates of potassium and earth of alum, demonstrating that alum was a double salt. In 1776, German pharmaceutical chemist Carl Wilhelm Scheele demonstrated that both alum and silica originated from clay and alum did not contain silicon. Geoffroy's mistake was only corrected in 1785 by German chemist and pharmacist Johann Christian Wiegleb who determined the earth of alum could not be synthesized from silica and alkalis, contrary to contemporary belief.\n\nSwedish chemist Jöns Jacob Berzelius suggested in 1815 the formula AlO for alumina. The correct formula, AlO, was established by German chemist Eilhard Mitscherlich in 1821; this helped Berzelius determine the correct atomic weight of the metal, 27.\n\nIn 1760, French chemist Theodor Baron de Henouville suggested alumina was a metallic earth and attempted to reduce it to its metal, but with no success. He claimed he had tried every method of reduction known at the time, though his methods were not published. It is probable that he mixed alum with carbon or some organic substance, with salt or soda for flux, and heated it in a charcoal fire. In 1782, French chemist Antoine Lavoisier wrote he considered alumina was an oxide of a metal with an affinity for oxygen so strong that no known reducing agents could overcome it.\n\nIn 1790, Austrian chemists Anton Leopold Ruprecht and Matteo Tondi repeated Baron's experiments, significantly increasing the temperatures. They found small metallic particles they believed were the sought-after metal; but later experiments by other chemists showed these were iron phosphide from impurities in charcoal and bone ash. German chemist Martin Heinrich Klaproth commented in an aftermath, \"if there exists an earth which has been put in conditions where its metallic nature should be disclosed, if it had such, an earth exposed to experiments suitable for reducing it, tested in the hottest fires by all sorts of methods, on a large as well as on a small scale, that earth is certainly alumina, yet no one has yet perceived its metallization.\" Lavoisier in 1794 and French chemist Louis-Bernard Guyton de Morveau in 1795 melted alumina to a white enamel in a charcoal fire fed by pure oxygen but found no metal. American chemist Robert Hare in 1802 melted alumina with an oxyhydrogen blowpipe, also obtaining the enamel, but still found no metal.\n\nIn 1807, British chemist Humphry Davy successfully electrolyzed alumina with alkaline batteries, but the resulting alloy contained potassium and sodium, and Davy had no means to separate the desired metal from these. He then heated alumina with potassium, forming potassium oxide, but was unable to produce the sought-after metal. In 1808, Davy set up a different experiment on electrolysis of alumina, establishing that alumina decomposed in the electric arc, but formed metal alloyed with iron and he was unable to separate the two. Finally he tried yet another electrolysis experiment, seeking to collect the metal on iron, but was again unable to separate the coveted metal from it. Davy suggested the metal be named \"alumium\" in 1808 and \"aluminum\" in 1812, thus producing the modern name. Other scientists used the spelling \"aluminium\"; the former spelling regained usage in the United States in the following decades.\n\nIn 1813, American chemist Benjamin Silliman repeated Hare's experiment and obtained small granules of the sought-after metal, which almost immediately burned.\n\nIn 1824, Danish physicist and chemist Hans Christian Ørsted attempted the production of the metal. He reacted anhydrous aluminium chloride with potassium amalgam, yielding a lump of metal that looked similar to tin. He presented his results and demonstrated a sample of the new metal in 1825. In 1826, he wrote, \"aluminium has a metallic luster and somewhat grayish color and breaks down water very slowly\"; this suggests that he had obtained an aluminium–potassium alloy rather than pure aluminium. Ørsted gave little importance to his discovery. He did not notify either Davy or Berzelius, both of whom he knew, and published his work in a Danish magazine unknown to the general European public. As a result, he is often not credited as the discoverer of the element; some earlier sources went further and claimed Ørsted had not isolated aluminium.\n\nBerzelius tried to isolate the metal in 1825 by carefully washing the potassium analog of the base salt in cryolite in a crucible. Prior to the experiment, he had correctly identified the formula of this salt as KAlF. He found no metal, but his experiment came very close to succeeding and was successfully reproduced many times later. Berzelius's mistake was in using an excess of potassium, which made the solution too alkaline; the alkaline solution dissolved all the newly formed aluminium.\n\nIn 1827, German chemist Friedrich Wöhler visited Ørsted and received explicit permission to continue the aluminium research, which Ørsted \"did not have time\" for. Wöhler repeated Ørsted's experiments but did not identify any aluminium. (Wöhler later wrote to Berzelius, \"what Oersted assumed to be a lump of aluminium was certainly nothing but aluminium-containing potassium\".) He conducted a similar experiment, mixing anhydrous aluminium chloride with potassium, and produced a powder of aluminium. (After hearing about this, Ørsted suggested his own aluminium may have contained potassium.) Wöhler continued his research and in 1845 was able to produce small pieces of the metal and described some of its physical properties. Wöhler's description of the properties indicates that he had obtained impure aluminium. Other scientists also failed to reproduce Ørsted's experiment, and Wöhler was credited as the discoverer. While Ørsted was not concerned with the priority of the discovery, some Danes tried to demonstrate he had obtained aluminium, and in 1921, the reason for the inconsistency between Ørsted's and Wöhler's experiments was discovered by Danish chemist Johan Fogh, who demonstrated that Ørsted's experiment was successful thanks to use of a large amount of excess aluminium chloride and an amalgam with low potassium content. In 1936, scientists from American aluminium producing company Alcoa successfully recreated that experiment and Ørsted was recognized as the discoverer thereafter.\n\nSince Wöhler's method could not yield large amounts of aluminium, the metal remained uncommon; its cost had exceeded that of gold before a new method was devised. In 1852, aluminium cost US$545 per pound.\n\nFrench chemist Henri Étienne Sainte-Claire Deville announced an industrial method of aluminium production in 1854 at the Paris Academy of Sciences. Aluminium chloride could be reduced by sodium, a metal more convenient and less expensive than potassium used by Wöhler. Deville was able to produce an ingot of the metal. Deville's research, which cost about 20 times the annual income of an ordinary family, was subsidized by Napoleon III of France. While the metal was still not displayed to the public, Napoleon is reputed to have held a banquet where the most honored guests were given aluminium utensils while others made do with gold.\n\nBars of aluminium were subsequently exhibited for the first time to the general public at the Exposition Universelle of 1855. The metal was presented as \"the silver from clay\", and this name was soon widely used. It caught wide attention, including that of the avantgarde writers of the time—Charles Dickens, Nikolay Chernyshevsky, and Jules Verne—and the fair led to eventual commercialization of the metal. From 1855 to 1859, the price of aluminium fell from US$115 to $17 per pound. At the next fair in Paris in 1867, the visitors were presented with aluminium wire and foil.\n\nManufacturers did not wish to devote resources from producing well-known metals, such as iron and bronze, to experiment with a new one; moreover, produced aluminium was still not of great purity and differed in properties by sample. This led to the initial general reluctance to produce the new metal. World's first industrial production of aluminium was established at a smelter in Rouen in 1856 by Deville and partners. Deville's smelter moved that year to La Glacière and then Nanterre, and in 1857 to Salindres. The smelter was subsequently acquired by the French company Pechiney and the Compagnie d'Alais et de la Camargue, which later became world's largest in chemical production of aluminium. The factory's technology continued to improve, and the output grew from 2 kilograms in Nanterre to 1,800 kilograms in Salindres in 1872. The factory in Salindres used bauxite as the primary aluminium ore; some chemists, including Deville, sought to use cryolite, but with little success. British engineer William Gerhard set up a plant with cryolite as the primary raw material in Battersea, London, in 1856, but technical and financial difficulties forced the closure of the plant in three years.\n\nOther chemists also sought to industrialize production of aluminium. British ironmaster Isaac Lowthian Bell produced aluminium from 1860 to 1874. During the opening of his factory, he waved to the crowd with a unique and costly aluminium top hat. British engineer James Fern Webster launched the industrial production of aluminium by reduction with sodium in 1882; his aluminium was much purer than Deville's. Several other production sites were set up in the 1880s. World production of aluminium in 1869 equaled 2 metric tons; in 1884, it was 4 tons. In 1886, American engineer Alexander Castner devised a method of cheaper production of sodium, which decreased the price of aluminium to $8 per pound, but he did not have enough capital to construct a large factory like Deville's. Deville continued to improve his production method and it remained dominant until the late 1890s, when the much cheaper electrolytic production was fully commercialized; Deville was unable to reduce the price from $15 per pound, while electrolytic aluminium cost $0.54 per pound in 1897.\n\nAluminium was first synthesized electrolytically in 1854 independently by Deville and the German chemist Robert Wilhelm Bunsen. Their electrolysis methods did not become the basis for industrial production of aluminium because electrical supplies were inefficient at the time; this only changed with the invention of the dynamo by Belgian engineer Zénobe-Théophile Gramme in 1870 and the three-phase current by Russian engineer Mikhail Dolivo-Dobrovolsky in 1889.\n\nThe first large-scale production method was independently developed by French engineer Paul Héroult and American engineer Charles Martin Hall in 1886; it is now known as the Hall–Héroult process. Electrolysis of pure alumina is impractical given its very high melting point; both Héroult and Hall realized its melting point could be significantly lowered by presence of molten cryolite. Héroult could not find enough interest in his invention as demand for aluminium was still small and Deville's factory in Salindres did not wish to improve their process. In 1888, Héroult and his companions founded Aluminium Industrie Aktien Gesellschaft and started industrial production of aluminium bronze in Neuhausen am Rheinfall. This production was only active for a year, but during that time, Société électrométallurgique française was founded in Paris. The society purchased Héroult's patents and appointed him as the director of a smelter in Isère, which would produce aluminium bronze on a large scale at first and pure aluminium in a few months.\n\nAt the same time, Hall produced aluminium by the same process in his home at Oberlin, and successfully tested it at the smelter in Lockport. He then sought to employ it for a large-scale production. For that, the smelter owners would have to radically change their production methods, which they were not willing to do because a mass production of aluminium would immediately drop the price of the metal. The president of the company considered purchasing Hall's patent to ensure that the competitors would not make use of the it. Hall founded the Pittsburgh Reduction Company in 1888 and initiated the mass production of aluminium. In the coming years, this technology was improved and new factories were constructed.\n\nThe Hall–Héroult process converts alumina into the metal; Austrian chemist Carl Joseph Bayer discovered a way of purifying bauxite to yield alumina in 1889, now known as the Bayer process. Bayer sintered bauxite with alkali and leached it with water; after stirring the solution and introducing a seeding agent to it, he found a precipitate of pure aluminium hydroxide, which decomposed to alumina on heating. In a few years, he discovered that the aluminium contents of bauxite dissolved in the alkaline leftover from isolation of alumina solids; this was crucial for the industrial employment of this method.\n\nModern production of the aluminium metal is based around the Bayer and Hall–Héroult processes. The Hall–Héroult process was further improved in 1920 by a team led by Swedish chemist Carl Wilhelm Söderberg. Previously, anode cells had been made from pre-baked coal blocks, which quickly corrupted and required replacement; the team introduced continuous electrodes made from a coke and tar paste in a reduction chamber. This greatly increased the world output of aluminium.\n\nThe prices for aluminium declined, and by the early 1890s, the metal had become widely used in jewelry, eyeglass frames, optical instruments, and many everyday items. Aluminium tableware began to be produced in the late 19th century and gradually supplanted copper and cast iron tableware in the first decades of the 20th century. Aluminium foil was popularized at that time. Aluminium is soft and light, but it was soon discovered that alloying it with other metals could increase its hardness while preserving low density. Aluminium alloys found many uses in the late 19th and early 20th centuries. For instance, aluminium bronze is applied to make flexible bands, sheets, and wire, and is widely employed in the shipbuilding and aviation industries. Aviation used a new aluminium alloy, duralumin, invented in 1903. Aluminium recycling started in the early 20th century and has been used extensively since as aluminium is not impaired by recycling and thus can be recycled repeatedly. At this point, only the metal that had not been used by end-consumers was recycled. During World War I, major governments demanded large shipments of aluminium for light strong airframes. They often subsidized factories and the necessary electrical supply systems. Overall production of aluminium peaked during the war: world production of aluminium in 1900 was 6,800 metric tons; in 1916, annual production exceeded 100,000 tons. The war created a greater demand for aluminium, which the growing primary production was unable to fully satisfy, and recycling grew intensely as well. The peak in production was followed by a decline, then a swift growth.\n\nDuring the first half of the 20th century, the real price for aluminium continuously fell from $14,000 per metric ton in 1900 to $2,340 in 1948 (in 1998 United States dollars) with some exceptions such as the sharp price rise during World War I. Aluminium was plentiful and in 1919, Germany began to replace its silver coins with aluminium ones; more and more denominations were switched to aluminium coins as hyperinflation progressed in the country. By the mid-20th century, aluminium had become a part of everyday lives, becoming an essential component of houseware. Aluminium freight cars first appeared in 1931. Their lower mass allowed them to carry more cargo. During the 1930s, aluminium emerged as a civil engineering material, being used in both basic construction and building interiors, and advanced its use in military engineering for both airplanes and tank engines.\n\nAluminium obtained from recycling was considered inferior to primary aluminium because of poorer chemistry control as well as poor removal of dross and slags. Recycling grew overall but largely depended on the output of primary production: for instance, as electric energy prices went down in the United States in the late 1930s, more primary aluminium could be produced in the energy-expensive Hall–Héroult process, rendering recycling less needed, and thus aluminium recycling rates went down. By 1940, mass recycling of post-consumer aluminium had begun.\n\nDuring World War II, production peaked again, first exceeding 1,000,000 metric tons in 1941. Aluminium was heavily used in aircraft production and thus a strategic material of extreme importance; so much so that when Alcoa (successor of Hall's Pittsburgh Reduction Company and the aluminium production monopolist in the United States at the time) did not expand its production, U.S. Secretary of Interior proclaimed in 1941, \"If America loses the war, it can thank the Aluminum Corporation of America\". In 1939, Germany was world's leading producer of aluminium; the Germans thus saw aluminium as their edge in the war. Aluminium coins continued to be used but while they symbolized decline on introduction, by 1939, they had come to represent power. (In 1941, they began to be withdrawn from circulation.) After the United Kingdom was attacked in 1940, it started an ambitious program of aluminium recycling; the newly appointed Minister of Aircraft Production appealed to the public to donate any household aluminium for airplane building. The Soviet Union received 328,100 metric tons of aluminium from its co-combatants from 1941 to 1945; this aluminium would be used in aircraft and tank engines. Without these shipments, the output of the Soviet aircraft industry would have fallen by over a half. Production fell after the war but then rose again.\n\nEarth's first artificial satellite, launched in 1957, consisted of two joined aluminium hemispheres, and almost all subsequent spacecraft have been made of aluminium. The aluminium can was first manufactured in 1956 and employed as a container for drinks in 1958. In the 1960s, aluminium was employed for production of wires and cables. Since the 1970s, high-speed trains have commonly used aluminium for its lightness. For the same reason, the aluminium content of cars is growing.\n\nBy 1955, the world market had been mostly divided by the Six Majors: Alcoa, Alcan (originated as a part of Alcoa), Reynolds, Kaiser, Pechiney (successor of Pechiney and the Compagnie d'Alais et de la Camargue that bought Deville's smelter), and Alusuisse (successor of Héroult's Aluminium Industrie Aktien Gesellschaft); their combined share of the market equaled 86%. From 1945, aluminium consumption grew by almost 10% each year for nearly three decades, gaining ground in building applications, electric cables, basic foils, and the aircraft industry. In the early 1970s, an additional boost came from the development of aluminium beverage cans. The real price declined until the early 1970s because extraction and processing costs were lowered over technological progress and increased production of aluminium, which first exceeded 10,000,000 metric tons in 1971.\n\nIn the late 1960s, governments became aware of waste from the industrial production; they enforced a series of regulations favoring recycling and waste disposal. Söderberg anodes, which save capital and labor to bake the anodes but are more harmful to the environment, fell in disfavor, and production began to shift back to the pre-baked anodes. The aluminium industry started to promote recycling of aluminium cans in an attempt to avoid restrictions on them. This sparked recycling of aluminium previously used by end-consumers: for example, in the United States, levels of recycling of such aluminium increased 3.5 times from 1970 to 1980 and 7.5 times to 1990. Production costs for primary aluminium grew in the 1970s and 1980s, and this also contributed to the rise of aluminium recycling.\n\nIn the 1970s, the increased demand for aluminium made it an exchange commodity; it entered the London Metal Exchange, world's oldest industrial metal exchange, in 1978. Since then, aluminium has been traded for United States dollars and its price fluctuated along with the exchange rates of the currency. The need to exploit lower-grade poorer quality deposits and the use of fast increasing input costs (above all, energy) increased the net cost of aluminium; the real price began to grow in the 1970s.\n\nThe increase of the real price and changes of tariffs and taxes started redistribution of the world producers' shares: the United States, the Soviet Union, and Japan accounted for nearly 60% of world's primary production in 1972 (and their combined share of consumption of primary aluminium was also close to 60%), but their combined share only slightly exceeded 10% in 2012. Production moved from the United States, Japan, and Western Europe to Australia, Canada, the Middle East, Russia, and China, where production was cheaper. Production costs in the late 20th century changed because of advances in technology, lower energy prices, exchange rates of the United States dollar, and alumina prices. The BRIC countries' combined share grew in the first decade of the 21st century from 32.6% to 56.5% in primary production and 21.4% to 47.8% in primary consumption. China is accumulating an especially large share of world production, thanks to abundance of resources, cheap energy, and governmental stimuli; it also increased its share of consumption from 2% in 1972 to 40% in 2010. The only other country with a two-digit percentage was the United States with 11%; no other country exceeded 5%. In the United States, Western Europe, and Japan, most aluminium was consumed in transportation, engineering, construction, and packaging.\n\nThe world output continued to grow: in 2013, annual production of aluminium exceeded 50,000,000 metric tons. In 2015, it was a record 57,500,000 tons.\n\n\n"}
{"id": "12776125", "url": "https://en.wikipedia.org/wiki?curid=12776125", "title": "Index of epistemology articles", "text": "Index of epistemology articles\n\nEpistemology (from Greek ἐπιστήμη – \"episteme\"-, \"knowledge, science\" and λόγος, \"logos\") or theory of knowledge is the branch of philosophy concerned with the nature and scope (limitations) of knowledge. It addresses the questions \"What is knowledge?\", \"How is knowledge acquired?\", \"What do people know?\", \"How do we know what we know?\", and \"Why do we know what we know?\". Much of the debate in this field has focused on analyzing the nature of knowledge and how it relates to similar notions such as truth, belief, and justification. It also deals with the means of production of knowledge, as well as skepticism about different knowledge claims.\n\nArticles related to epistemology include:\n\n– \"A Defence of Common Sense\"\n– A posteriori\n– A priori and a posteriori\n– A Treatise Concerning the Principles of Human Knowledge\n– Abductive reasoning\n– Academic skepticism\n– Acatalepsy\n– Ad hoc hypothesis\n– Adaptive representation\n– Adolph Stöhr\n– Aenesidemus\n– \"Aenesidemus\" (book)\n– African Spir\n– Agnosticism\n– Agrippa the Skeptic\n– Alethiology\n– Alief (belief)\n– Alison Wylie\n– Alvin Goldman\n– An Enquiry Concerning Human Understanding\n– An Essay Concerning Human Understanding\n– Analytic–synthetic distinction\n– Anamnesis (philosophy)\n– Androcentrism\n– Android epistemology\n– Anthony Wilden\n– Anti-foundationalism\n– Anti-realism\n– Apperception\n– Arda Denkel\n– Argument from illusion\n– Aristotle's theory of universals\n– Arnór Hannibalsson\n– Ásta Kristjana Sveinsdóttir\n– Atli Harðarson\n– Atomism\n– Autoepistemic logic\n– Ayn Rand\n\n– Barry Stroud\n– Basic belief\n– Basic limiting principle\n– Belief\n– Bertrand Russell\n– Bertrand Russell's views on philosophy\n– Björn Kraus\n– Black swan theory\n– Blind men and an elephant\n– Body of knowledge\n– Brain in a vat\n– Brute fact\n\n– C. D. Broad\n– Carper's fundamental ways of knowing\n– Cartesian doubt\n– Cartesian Other\n– Cartesian Self\n– Catherine Elgin\n– Causal chain\n– Causal Theory of Knowing\n– Causality\n– Center Leo Apostel for Interdisciplinary Studies\n– Centre de Recherche en Epistémologie Appliquée\n– Certainty\n– Claudio Canaparo\n– Cogito ergo sum\n– Cognitive closure (philosophy)\n– Cognitive synonymy\n– Coherence theory of truth\n– Coherentism\n– Common sense\n– Compensationism\n– Composition of Causes\n– Computational epistemology\n– Concluding Unscientific Postscript to Philosophical Fragments\n– Condition of possibility\n– Consensus theory of truth\n– Constructivism (mathematics)\n– Constructivist epistemology\n– Contextualism\n– Contrastivism\n– Correspondence theory of truth\n– Counterintuitive\n– Crispin Wright\n– Criteria of truth\n– Critical rationalism\n– Critical realism\n– Critical thinking\n– Cynicism\n\n– Daniel M. Hausman\n– David Hume\n– Deductive closure\n– Defeasible reasoning\n– Defeater\n– Deflationary theory of truth\n– Descriptive knowledge\n– Dharmarāja Adhvarin\n– Dialetheism\n– Dianoia\n– Direct and indirect realism\n– Direct experience\n– Discourse on the Method\n– Disjunctivism\n– Dispositional and occurrent belief\n– Divine command theory– Daimonic\n– Dogma\n– Doubt\n– Doxa\n– Doxastic attitudes\n– Dream argument\n– Duck test\n\n– Eastern epistemology\n– Ecology of contexts\n– Edgar Morin\n– Editology\n– Edmund Gettier\n– Educology\n– Egocentric predicament\n– Elephant test\n– Emergence\n– Empirical evidence\n– Empirical method\n– Empirical relationship\n– Empirical research\n– Empiricism\n– Endoxa\n– Enneads\n– Epilogism\n– Episteme\n– Epistemic closure\n– Epistemic commitment\n– Epistemic community\n– Epistemic conservatism\n– Epistemic feedback\n– Epistemic minimalism\n– Epistemic possibility\n– Epistemic theories of truth\n– Epistemic theory of miracles\n– Epistemic virtue\n– Epistemicism\n– Epistemocracy\n– Epistemological anarchism\n– Epistemological idealism\n– Epistemological particularism\n– Epistemological pluralism\n– Epistemological psychology\n– Epistemological realism\n– Epistemological rupture\n– Epistemological solipsism\n– Epistemology\n– Epoché\n– Eristic\n– Ernst von Glasersfeld\n– Eureka effect\n– Everett W. Hall\n– Evidence\n– Evidentialism\n– Evil demon\n– Evolutionary argument against naturalism\n– Evolutionary epistemology\n– Exclusion principle (philosophy)\n– Existential phenomenology\n– Exoteric\n– Expectation (epistemic)\n– Experience\n– Experiential knowledge\n– Experientialism\n– Externism\n– Eyewitness testimony\n\n– Fa (concept)\n– Fact\n– Factual relativism\n– Fact–value distinction\n– Faith and rationality\n– Fallibilism\n– Falsifiability\n– Feminist epistemology\n– Fideism\n– Finitism\n– Fitch's paradox of knowability\n– Fooled by Randomness\n– Formal epistemology\n– Formative epistemology\n– Foundationalism\n– Foundherentism\n– Fragmentalism\n– Frame problem\n– Frank Cameron Jackson\n– Fred Dretske\n– Frederick Wilhelmsen\n– Freethought\n– Functional contextualism\n\n– G. E. Moore\n– Gaston Bachelard\n– Generativity\n– Genetic epistemology\n– George Berkeley\n– George Pappas\n– Gettier problem\n– Giambattista Vico\n– Gila Sher\n– Gilbert Harman\n– Gilbert Ryle\n– Giulio Giorello\n– Gnosiology\n– Gödel's incompleteness theorems\n\n– Harry Binswanger\n– Heinz von Foerster\n– Helmut Wautischer\n– Here is a hand\n– Hierarchical epistemology\n– Hilary Kornblith\n– Humanism\n– Hume's fork\n\n– I know it when I see it\n– I know that I know nothing\n– Ideological criticism\n– Ideology\n– Ignoramus et ignorabimus\n– Ignorance\n– Illuminationism\n– Immanuel Kant\n– Incorrigibility\n– Indeterminacy (philosophy)\n– Inductive reasoning\n– Inductivism\n– Infallibilism\n– Infallibility\n– Inference\n– Infinitism\n– Information source\n– Innatism\n– Insight\n– Intellectual responsibility\n– Internalism and externalism\n– Intersubjective verifiability\n– Intersubjectivity\n– Introduction to Objectivist Epistemology\n– Introspection\n– Intuition (Bergson)\n– Intuition (philosophy)\n– Intuition (psychology)\n– Intuitionism\n– Irrealism (philosophy)\n– Is logic empirical?\n– Islamization of knowledge\n\n– Jean Piaget\n– Jean-Louis Le Moigne\n– Jean-Michel Berthelot\n– John Greco (philosopher)\n– John Hick\n– John Locke\n– John Searle\n– Jonathan Dancy\n– Jonathan Kvanvig\n- Jules Vuillemin\n– Justified true belief\n\n– Karla Jessen Williamson\n– Katalepsis\n– Keith Lehrer\n– KK thesis\n– Knowing and the Known\n– Knowledge\n– \"Knowledge and Its Limits\"\n– Knowledge by acquaintance\n– Knowledge by description\n– Knowledge organization\n– Knowledge relativity\n\n– Laplace's demon\n– Larry Laudan\n– Larry Sanger\n– Latitudinarianism (philosophy)\n– Laurence BonJour\n– Law (principle)\n– Leap of faith\n– Leonard Peikoff\n– Levels of adequacy\n– List of epistemologists\n– Logical holism\n– Logical positivism\n– Lottery paradox\n\n– Maieutics\n– Map–territory relation\n– Margaret Elizabeth Egan\n– Mathematical proof\n– Meditations on First Philosophy\n– Memory\n– Meno\n– Meno's slave\n– Meta\n– Meta-epistemology\n– Metaphor in philosophy\n– Metaphysical naturalism\n– Metatheory\n– Methodical culturalism\n– Methodism (philosophy)\n– Methodological solipsism\n– Michel de Montaigne\n– Mind extension\n– Mioara Mugur-Schächter\n– Misotheism\n– Molyneux's problem\n– Moore's paradox\n– Moral rationalism\n– Multiperspectivalism\n– Mundane reason\n\n– Naïve empiricism\n– Naïve realism\n– Nassim Nicholas Taleb\n– Naturalism (philosophy)\n– Naturalized epistemology\n– Nayef Al-Rodhan\n– Neopragmatism\n– Neutrality (philosophy)\n– New realism (philosophy)\n– Nicholas Rescher\n– Niklas Luhmann\n– Nomothetic\n– Nomothetic and idiographic\n– Noogony\n– Norman Malcolm\n– Noumenon\n\n– Object (philosophy)\n– Objectivism (Ayn Rand)\n– Objectivity (philosophy)\n– Observation\n– Ontologism\n– Omphalos hypothesis\n– Opinion\n– Outline of epistemology\n– Overbelief\n\n– P. F. Strawson\n– Pancritical rationalism\n– Panrationalism\n– Paradigm\n– Paradigm shift\n– Participatory theory\n– Paul Churchland\n– Perception\n– Perceptual learning\n– Peripatetic axiom\n– Perspectivism\n– Pessimism\n– Peter Millican\n– Peter Unger\n– Phenomenal conservatism\n– Phenomenalism\n– Phillip H. Wiebe\n– Philosophic burden of proof\n– Philosophical Fragments\n– Philosophical Investigations\n– Philosophical problems of testimony\n– Philosophical skepticism\n– Philosophical theology\n– Philosophical zombie\n– Philosophy of color\n– Philosophy of perception\n– Philosophy of science\n– Plato's Problem\n– Platonic epistemology\n– Pluralism (philosophy)\n– Pluralist theories of truth\n– Positivism\n– Postfoundationalism\n– Postmodern philosophy\n– Postpositivism\n– Pragmatic theory of truth\n– Pramāṇa\n– Praxeology\n– Predictive power\n– Preface paradox\n– Preformation theory\n– Presentationism\n– Presupposition (philosophy)\n– Primary/secondary quality distinction\n– Principle of charity\n– Private language argument\n– Privileged access\n– Probabilism\n– Probability interpretations\n– Problem of induction\n– Problem of other minds\n– Problem of the criterion\n– Problem of universals\n– Procedural knowledge\n– Proof (truth)\n– Propensity probability\n– Propositional attitude\n– Pseudointellectual\n– Psychological nominalism\n– Pyrrho\n– Pyrrhonism\n\n– Ramification problem\n– Rational egoism\n– Rational fideism\n– Rational ignorance\n– Rationalism\n– Rationality\n– Reason\n– Reasonism\n– Redundancy theory of truth\n– Reformed epistemology\n– Regress argument\n– Relevant alternatives theory\n– Reliabilism\n– Religious epistemology\n– Robert Audi\n– Robert Nozick\n– Roderick Chisholm\n– Role of chance in scientific discoveries\n\n– Sally Haslanger\n– Salvino Azzopardi\n– Satya\n– Scepticism and Animal Faith\n– Scottish Common Sense Realism\n– Self-evidence\n– Semantic externalism\n– Semantic theory of truth\n– Sensualism\n– Sextus Empiricus\n– Sherrilyn Roush\n– Simulated reality\n– Simulation hypothesis\n– Skepticism\n– Sleeping Beauty problem\n– Social constructionism\n– Social epistemology\n– Social Epistemology (journal)\n– Sociology of knowledge\n– Socrates\n– Solipsism\n– Sophist (dialogue)\n– Speculative reason\n– Steve Fuller (sociologist)\n– Subjectivism\n– Swamping problem\n– Swampman\n– Systemography\n\n– Tabula rasa\n– Tarski's undefinability theorem\n– Techne\n– Telesis\n– Testimony\n– The Black Swan (Taleb book)\n– The Course in Positive Philosophy\n– \"The Extended Mind\"\n– The Postmodern Condition\n– \"The Republic (Plato)\"\n– The Roots of Reference\n– The Will to Believe\n– The World as Will and Representation\n– Theaetetus (dialogue)\n– Theomachist\n– Theory of Forms\n– Theory of justification\n– There are known knowns\n– Thick Black Theory\n– Thought experiment\n– Tractatus Logico-Philosophicus (6.5)\n– Transcendent truth\n– Transcendental idealism\n– Transcendental philosophy\n– Transcendental realism\n– Transparency (philosophy)\n– Trenton Merricks\n– Truth\n– Truth by consensus\n– Truth predicate\n– Truth-value link\n– Twin Earth thought experiment\n– \"Two Dogmas of Empiricism\"\n– Two truths doctrine\n\n– Uncertainty\n– Underdetermination\n– Understanding\n– Universal pragmatics\n– Unknown known\n– Unobservable\n– Upamāṇa\n\n– Vagueness\n– Vasily Seseman\n– Verification theory\n– Verificationism\n– Verisimilitude\n– Veritism\n– Vienna Circle\n– Virtue epistemology\n– Visual space\n– Voluntarism (metaphysics)\n\n– Walter Terence Stace\n– Ward Jones\n– What Engineers Know and How They Know It\n– Wilfrid Sellars\n– William Alston\n– William Crathorn\n– Word and Object\n– World Hypotheses\n– World view\n\n– Xenophanes\n\n– Yujian Zheng\n\n– Zen and the Art of Motorcycle Maintenance\n\n"}
{"id": "6982876", "url": "https://en.wikipedia.org/wiki?curid=6982876", "title": "International Union for Quaternary Research", "text": "International Union for Quaternary Research\n\nThe International Union for Quaternary Research (INQUA) was founded in 1928. It has members from a number of scientific disciplines who study the environmental changes that occurred during the glacial ages, the last 2.6 million years. One goal of these investigators is to document the timing and patterns in past climatic changes to help understand the causes of changing climates.\n\nINQUA is a Scientific Union member of the International Council for Science (ICSU). INQUA holds an international congress normally every four years. The congresses serve as an educational forum as well as the opportunity for the various commissions, committees, and working groups to conduct business in person. Past congresses have been held in Copenhagen (1928), Leningrad (1932), Vienna (1936), Rome (1953), Madrid (1957), Warsaw (1961), Boulder (1965), Paris (1969), Christchurch (1973), Birmingham (1977), Moscow (1982), Ottawa (1987), Beijing (1991), Berlin (1995), Durban (1999), Reno (2003), Cairns (2007), and Bern (2011). \n\nThe most recent INQUA Congress (XIX) was held in Nagoya, Japan, in July 2015. In 2019 the next INQUA Congress (XX) will take place in Dublin, Ireland. \n\nIn 2007, the union issued a statement on climate change in which they reiterated the conclusions of the Intergovernmental Panel on Climate Change (IPCC), and urged all nations to take prompt action in line with the UNFCCC principles:\nHuman activities are now causing atmospheric concentrations of greenhouse gases - including carbon dioxide, methane, tropospheric ozone, and nitrous oxide - to rise well above pre-industrial levels….Increases in greenhouse gasses are causing temperatures to rise…The scientific understanding of climate change is now sufficiently clear to justify nations taking prompt action….Minimizing the amount of this carbon dioxide reaching the atmosphere presents a huge challenge but must be a global priority.\n\n\n \n"}
{"id": "6639797", "url": "https://en.wikipedia.org/wiki?curid=6639797", "title": "Island growth", "text": "Island growth\n\nIsland growth is a physical model of deposited film growth and chemical vapor deposition. \n\nConsider a situation where atoms are being deposited onto a flat surface at a very slow rate. The first atom deposited undergoes a random walk on the surface. Eventually a second atom is deposited and can be expected to eventually meet the first atom, given enough time. Once the two atoms meet they may bond to form a particle with a higher mass and a lower random walk velocity. Because the bonded particles are now more stable and less mobile than before, they are called an \"island\". Subsequent atoms deposited on the substrate eventually meet and bond with the island, further increasing its size and stability. Eventually the island can grow to fill the entire substrate with a single large grain.\n\nIf atoms are being deposited at a faster rate, there will be many atoms present on the substrate before any large stable islands form. As these atoms meet, they will bond to their local neighbors before having the chance to migrate to a distant island. In this way a large number of separate islands are formed and can grow independently. Eventually the separate islands will grow to become separate grains in the final film.\n\nThe island growth model is used to explain how fast deposition techniques (such as sputter deposition) can produce films with many randomly oriented grains, whereas slow deposition techniques (such as MBE) tend to produce larger grains with more uniform structure.\n\nStranski–Krastanov growth\n"}
{"id": "1598551", "url": "https://en.wikipedia.org/wiki?curid=1598551", "title": "Leopold Fitzinger", "text": "Leopold Fitzinger\n\nLeopold Joseph Franz Johann Fitzinger (13 April 1802 – 20 September 1884) was an Austrian zoologist.\n\nFitzinger was born in Vienna and studied botany at the University of Vienna under Nikolaus Joseph von Jacquin. He worked at the Vienna Naturhistorisches Museum between 1817, when he joined as a volunteer assistant, and 1821, when he left to become secretary to the provincial legislature of Lower Austria; after a hiatus he was appointed assistant curator in 1844 and remained at the Naturhistorisches Museum until 1861. Later he became director of the zoos of Munich and Budapest.\n\nIn 1826 he published \"Neue Classification der Reptilien\", based partly on the work of his friends Friedrich Wilhelm Hemprich and Heinrich Boie. In 1843 he published \"Systema Reptilium\", covering geckos, chameleons and iguanas.\n\nFitzinger is commemorated in the scientific names of five reptiles: \"Algyroides fitzingeri\", \"Leptotyphlops fitzingeri\", \"Liolaemus fitzingerii\", \"Micrurus tener fitzingeri\", and \"oxyrhopus fitzingeri\".\n\nWorks\n"}
{"id": "5199461", "url": "https://en.wikipedia.org/wiki?curid=5199461", "title": "List of Chilean flags", "text": "List of Chilean flags\n\nList of Chilean flags\nThis is a list of flags used in Chile. For more information about the national flag, visit the article Flag of Chile.\n\n\"See\": Regions of Chile\n\n"}
{"id": "2675178", "url": "https://en.wikipedia.org/wiki?curid=2675178", "title": "List of compounds with carbon number 22", "text": "List of compounds with carbon number 22\n\nThis is a partial list of molecules that contain 22 carbon atoms.\n\n"}
{"id": "50638813", "url": "https://en.wikipedia.org/wiki?curid=50638813", "title": "List of geographical noses", "text": "List of geographical noses\n\nNose is used in the name of several geographical features and their associated settlements:\n\n\n"}
{"id": "47480824", "url": "https://en.wikipedia.org/wiki?curid=47480824", "title": "Mary Mulvihill", "text": "Mary Mulvihill\n\nMary Mulvihill (1 Sep 1959 – 11 June 2015) was an Irish scientist, radio television presenter, author and educator. She founded and served as the first chairperson of Women in Technology and Science (WITS), and is viewed as a pioneer of science communication in Ireland. She was featured in \"Silicon Republic's\" 100 Top Women in STEM list.\n\nMulvihill studied at Trinity College, Dublin, where she was elected a Scholar in 1979, and graduated in 1981 with a degree in genetics. She then went on to complete a master in statistics in 1982 at Trinity. Until 1987, she worked as a Research Officer for An Foras Taluntais (now Teagasc). She later attended Dublin City University to study journalism, earning a diploma in 1988.\n\nMulvihill worked primarily as a self-employed freelancer, as a writer, broadcaster, and developing the online resource of Ingenious Ireland with its accompanying walking tours. She served on the Irish Council for Bioethics, and as a council member of Industrial Heritage Association of Ireland.\n\nMulvihill was the creator and host of a number of popular science series for RTÉ Radio 1 and Lyric FM. Two of the radio series she developed centred on the collections of the National Botanic Gardens, \"Washed, Pressed and Dried\" (2007), and of the Natural History Museum, \"Chopped, Pickled, and Stuffed\" (2006).\n\nHer work in broadcasting led her to develop a series of walking tours of Dublin, which took in the scientific history. These tours were also available as podcasts. One of the trails she developed was \"Dublin by Numbers\", in conjunction with Institution of Engineers of Ireland, which focused on the places in Dublin relating to mathematics. The accompanying website maps places of historic interest linked to STEM in Ireland, as well as sites of ecological and archaeological interest. A similar set of audio tours were developed by Mulvihill, in collaboration with Matthew Jebb for the National Botanic Gardens.\n\nMulvihill was an advocate for science, technology, engineering, and mathematics (STEM), in particular the history and biographies of women involved in STEM. She founded the group Women in Technology and Science (WITS) in 1990, and served as the organisation's first chairperson. WITS is an advocacy and networking group for women in STEM fields in Ireland. One is the resources WITS provides is a register of Irish women in STEM interested in serving on boards and professional or conference panels.\n\nIn 2014, she launched the exhibition \"SeaScience\" and Exploration Zone at the Galway City Museum.\n\nMulvihill served as the co-editor of Enterprise Ireland’s bi-monthly magazine \"Technology Ireland\". She was also a regular contributor to \"The Irish Times\". She wrote a number of books, and edited two volumes of historical biographies of women in STEM for WITS. For her book, \"Ingenious Ireland: A County-by-County Exploration of Irish Mysteries and Marvels.\", she received the Irish National Science and Technology Journalist of the Year 2002-3, which the judges described as \"a meticulously researched and hugely impressive book.\" With this book she also won the IBM Science Journalist of the Year award.\n\n\nMulvihill was also a blogger, and was involved in \"Silicon Republic's\" Women Invent initiative and curated their list of Ireland's Greatest Women Inventors, in which younger people were encouraged to vote for their favourite.\n\nMulvihill was married to Scottish theoretical physicist Brian Dolan of Maynooth University. She died on 11 June 2015, aged 55. WITS celebrated its 25th Anniversary on 3 November 2015 with a lecture in her memory and a lecture at the 2015 Robert Boyle Summer School in Lismore, County Waterford was also dedicated to her.\n\nIn 2016 the family and friends of Mary Mulvihill established the Mary Mulvihill Memorial Award to commemorate her work in science journalism and science communication. The award will go to a student at an Irish higher education institution who best represents the \"curiosity, creativity and storytelling imagination\"\n\n"}
{"id": "2148329", "url": "https://en.wikipedia.org/wiki?curid=2148329", "title": "Mathematical universe hypothesis", "text": "Mathematical universe hypothesis\n\nIn physics and cosmology, the mathematical universe hypothesis (MUH), also known as the ultimate ensemble theory, is a speculative \"theory of everything\" (TOE) proposed by cosmologist Max Tegmark.\n\nTegmark's MUH is: \"Our external physical reality is a mathematical structure\". That is, the physical universe is not merely \"described by\" mathematics, but \"is\" mathematics (specifically, a mathematical structure). Mathematical existence equals physical existence, and all structures that exist mathematically exist physically as well. Observers, including humans, are \"self-aware substructures (SASs)\". In any mathematical structure complex enough to contain such substructures, they \"will subjectively perceive themselves as existing in a physically 'real' world\".\n\nThe theory can be considered a form of Pythagoreanism or Platonism in that it proposes the existence of mathematical entities; a form of mathematical monism in that it denies that anything exists except mathematical objects; and a formal expression of ontic structural realism.\n\nTegmark claims that the hypothesis has no free parameters and is not observationally ruled out. Thus, he reasons, it is preferred over other theories-of-everything by Occam's Razor. Tegmark also considers augmenting the MUH with a second assumption, the computable universe hypothesis (CUH), which says that the mathematical structure that is our external physical reality is defined by computable functions.\n\nThe MUH is related to Tegmark's categorization of four levels of the multiverse. This categorization posits a nested hierarchy of increasing diversity, with worlds corresponding to different sets of initial conditions (level 1), physical constants (level 2), quantum branches (level 3), and altogether different equations or mathematical structures (level 4).\n\nAndreas Albrecht of Imperial College in London, called it a \"provocative\" solution to one of the central problems facing physics. Although he \"wouldn't dare\" go so far as to say he believes it, he noted that \"it's actually quite difficult to construct a theory where everything we see is all there is\".\n\nJürgen Schmidhuber argues that \"Although Tegmark suggests that '... all mathematical structures are a priori given equal statistical weight,' there is no way of assigning equal non-vanishing probability to all (infinitely many) mathematical structures.\" Schmidhuber puts forward a more restricted ensemble which admits only universe representations describable by constructive mathematics, that is, computer programs. He explicitly includes universe representations describable by non-halting programs whose output bits converge after finite time, although the convergence time itself may not be predictable by a halting program, due to the undecidability of the halting problem.\n\nIn response, Tegmark notes (sec. V.E) that the measure over all universes has not yet been constructed for the string theory landscape either, so this should not be regarded as a \"show-stopper\".\n\nIt has also been suggested that the MUH is inconsistent with Gödel's incompleteness theorem. In a three-way debate between Tegmark and fellow physicists Piet Hut and Mark Alford, the \"secularist\" (Alford) states that \"the methods allowed by formalists cannot prove all the theorems in a sufficiently powerful system... The idea that math is 'out there' is incompatible with the idea that it consists of formal systems.\"\n\nTegmark's response in (sec VI.A.1) is to offer a new hypothesis \"that only Gödel-complete (fully decidable) mathematical structures have physical existence. This drastically shrinks the Level IV multiverse, essentially placing an upper limit on complexity, and may have the attractive side effect of explaining the relative simplicity of our universe.\" Tegmark goes on to note that although conventional theories in physics are Gödel-undecidable, the actual mathematical structure describing our world could still be Gödel-complete, and \"could in principle contain observers capable of thinking about Gödel-incomplete mathematics, just as finite-state digital computers can prove certain theorems about Gödel-incomplete formal systems like Peano arithmetic.\" In (sec. VII) he gives a more detailed response, proposing as an alternative to MUH the more restricted \"Computable Universe Hypothesis\" (CUH) which only includes mathematical structures that are simple enough that Gödel's theorem does not require them to contain any undecidable or uncomputable theorems. Tegmark admits that this approach faces \"serious challenges\", including (a) it excludes much of the mathematical landscape; (b) the measure on the space of allowed theories may itself be uncomputable; and (c) \"virtually all historically successful theories of physics violate the CUH\".\n\nStoeger, Ellis, and Kircher (sec. 7) note that in a true multiverse theory, \"the universes are then completely disjoint and nothing that happens in any one of them is causally linked to what happens in any other one. This lack of any causal connection in such multiverses really places them beyond any scientific support\". Ellis (p29) specifically criticizes the MUH, stating that an infinite ensemble of completely disconnected universes is \"completely untestable, despite hopeful remarks sometimes made, see, e.g., Tegmark (1998).\"\nTegmark maintains that MUH is testable, stating that it predicts (a) that \"physics research will uncover mathematical regularities in nature\", and (b) by assuming that we occupy a typical member of the multiverse of mathematical structures, one could \"start testing multiverse predictions by assessing how typical our universe is\" ( sec. VIII.C).\n\nThe MUH is based on the Radical Platonist view that math is an external reality ( sec V.C). However, Jannes argues that \"mathematics is at least in part a human construction\", on the basis that if it is an external reality, then it should be found in some other animals as well: \"Tegmark argues that, if we want to give a complete description of reality, then we will need a language independent of us humans, understandable for non-human sentient entities, such as aliens and future supercomputers\". Brian Greene ( p. 299) argues similarly: \"The deepest description of the universe should not require concepts whose meaning relies on human experience or interpretation. Reality transcends our existence and so shouldn't, in any fundamental way, depend on ideas of our making.\"\n\nHowever, there are many non-human entities, plenty of which are intelligent, and many of which can apprehend, memorise, compare and even approximately add numerical quantities. Several animals have also passed the mirror test of self-consciousness. But a few surprising examples of mathematical abstraction notwithstanding (for example, chimpanzees can be trained to carry out symbolic addition with digits, or the report of a parrot understanding a “zero-like concept”), all examples of animal intelligence with respect to mathematics are limited to basic counting abilities. He adds, \"non-human intelligent beings should exist that understand the language of advanced mathematics. However, none of the non-human intelligent beings that we know of confirm the status of (advanced) mathematics as an objective language.\" In the paper \"On Math, Matter and Mind\" the secularist viewpoint examined argues (sec. VI.A) that math is evolving over time, there is \"no reason to think it is converging to a definite structure, with fixed questions and established ways to address them\", and also that \"The Radical Platonist position is just another metaphysical theory like solipsism... In the end the metaphysics just demands that we use a different language for saying what we already knew.\" Tegmark responds (sec VI.A.1) that \"The notion of a mathematical structure is rigorously defined in any book on Model Theory\", and that non-human mathematics would only differ from our own \"because we are uncovering a different part of what is in fact a consistent and unified picture, so math is converging in this sense.\" In his 2014 book on the MUH, Tegmark argues that the resolution is not that we invent the language of mathematics, but that we discover the structure of mathematics.\n\nDon Page has argued (sec 4) that \"At the ultimate level, there can be only one world and, if mathematical structures are broad enough to include all possible worlds or at least our own, there must be one unique mathematical structure that describes ultimate reality. So I think it is logical nonsense to talk of Level 4 in the sense of the co-existence of all mathematical structures.\" Tegmark responds ( sec. V.E) that \"this is less inconsistent with Level IV than it may sound, since many mathematical structures decompose into unrelated substructures, and separate ones can be unified.\"\n\nAlexander Vilenkin comments (Ch. 19, p. 203) that \"the number of mathematical structures increases with increasing complexity, suggesting that 'typical' structures should be horrendously large and cumbersome. This seems to be in conflict with the beauty and simplicity of the theories describing our world\". He goes on to note (footnote 8, p. 222) that Tegmark's solution to this problem, the assigning of lower \"weights\" to the more complex structures ( sec. V.B) seems arbitrary (\"Who determines the weights?\") and may not be logically consistent (\"It seems to introduce an additional mathematical structure, but all of them are supposed to be already included in the set\").\n\nTegmark has been criticized as misunderstanding the nature and application of Occam's razor; Massimo Pigliucci reminds that \"Occam's razor is just a useful heuristic, it should never be used as the final arbiter to decide which theory is to be favored\".\n\n\n\n"}
{"id": "4908033", "url": "https://en.wikipedia.org/wiki?curid=4908033", "title": "Northrop Grumman X-47B", "text": "Northrop Grumman X-47B\n\nThe Northrop Grumman X-47B is a demonstration unmanned combat air vehicle (UCAV) designed for aircraft carrier-based operations. Developed by the American defense technology company Northrop Grumman, the X-47 project began as part of DARPA's J-UCAS program, and subsequently became part of the United States Navy's Unmanned Combat Air System Demonstration (UCAS-D) program. The X-47B is a tailless jet-powered blended-wing-body aircraft capable of semi-autonomous operation and aerial refueling.\n\nThe X-47B first flew in 2011, and , its two active demonstrators have undergone extensive flight and operational integration testing, having successfully performed a series of land- and carrier-based demonstrations. In August 2014, the US Navy announced that it had integrated the X-47B into carrier operations alongside manned aircraft, and by May 2015 the aircraft's primary test program was declared complete. The X-47B demonstrators themselves were intended to become museum exhibits after the completion of their flight testing, but the Navy later decided to maintain them in flying condition pending further development.\n\nThe US Navy did not commit to practical UCAS efforts until 2000, when the service awarded contracts of US$2 million each to Boeing and Northrop Grumman for a 15-month concept-exploration program. Design considerations for a naval UCAV included dealing with the corrosive saltwater environment, deck handling for launch and recovery, integration with command and control systems, and operation in an aircraft carrier's high-electromagnetic-interference environment. The Navy was also interested in procuring UCAVs for reconnaissance missions, penetrating protected airspace to identify targets for following attack waves. Northrop Grumman's proof-of-concept X-47A Pegasus, which provided the basis for the X-47B's development, first flew in 2003.\n\nThe J-UCAS program was terminated in February 2006 following the US military's Quadrennial Defense Review. The US Air Force and Navy proceeded with their own UAV programs. The Navy selected Northrop Grumman's X-47B as its unmanned combat air system demonstrator (UCAS-D) program. To provide realistic testing, the company built the demonstration vehicle to be the same size and weight as the projected operational craft, with a full-sized weapons bay capable of carrying existing missile systems.\n\nThe X-47B prototype rolled out from Air Force Plant 42 in Palmdale, California, on 16 December 2008. Its first flight was planned for November 2009, but the flight was delayed as the project fell behind schedule. On 29 December 2009, Northrop Grumman oversaw towed taxi tests of the aircraft at the Palmdale facility, with the aircraft taxiing under its own power for the first time in January 2010.\n\nThe first flight of the X-47B demonstrator, designated Air Vehicle 1 (AV-1), took place at Edwards Air Force Base, California, on 4 February 2011.\nThe aircraft first flew in cruise configuration with its landing gear retracted on 30 September 2011. A second X-47B demonstrator, designated AV-2, conducted its maiden flight at Edwards Air Force Base on 22 November 2011.\n\nThe two X-47B demonstrators were initially planned to have a three-year test program with 50 tests at Edwards AFB and NAS Patuxent River, Maryland, culminating in sea trials in 2013. However, the aircraft performed so consistently that the preliminary tests stopped after 16 flights. Thereafter, the Navy decided to use the aircraft to demonstrate carrier launches and recoveries, as well as autonomous inflight refueling with a probe and drogue. In November 2011, the Navy announced that aerial refuelling equipment and software would be added to one of the prototype aircraft in 2014 for testing. The Navy also affirmed that the demonstrator aircraft would never be armed. In 2012, Northrop Grumman tested a wearable remote control system, designed to allow ground crews to steer the X-47B while on the carrier deck.\n\nIn May 2012, AV-1 began high-intensity electromagnetic interference testing at Patuxent River, to test its compatibility with planned electronic warfare systems. In June 2012, AV-2 arrived at Patuxent River to begin a series of tests, including arrested landings and catapult launches, to validate the ability of the aircraft to conduct precision approaches to an aircraft carrier. The drones first land-based catapult launch was conducted successfully on 29 November 2012.\n\nOn 26 November 2012, the X-47B began its carrier-based evaluation aboard the at Naval Station Norfolk, Virginia. On 18 December 2012, the X-47B completed its first at-sea test phase. The system was remarked to have performed \"outstandingly\", having proved that it was compatible with the flight deck, hangar bays, and communication systems of an aircraft carrier. With deck testing completed, the X-47B demonstrator returned to NAS Patuxent River for further tests. On 4 May 2013, the demonstrator successfully performed an arrested landing on a simulated carrier deck at Patuxent River. The Navy launched the X-47B from the on the morning of 14 May 2013 in the Atlantic Ocean, marking the first time that an unmanned drone was catapulted off an aircraft carrier. On 17 May 2013, another first was achieved when the X-47B performed touch-and-go landings and take-offs on the flight deck of the USS \"George H.W. Bush\" while underway in the Atlantic Ocean.\n\nOn 10 July 2013, the X-47B launched from Patuxent River and landed on the deck of the \"George H.W. Bush\", conducting the first ever arrested landing of a UAV on an aircraft carrier at sea. The drone subsequently completed a second successful arrested landing on the \"Bush\", but a third attempt was diverted to the Wallops Flight Facility in Virginia after a technical problem was detected, aborting the planned carrier landing. One of the drone's three navigational sub-systems failed, which was identified by the other two sub-systems. The anomaly was indicated to the mission operator, who followed test plan procedures to abort the landing. The Navy stated that the aircraft's detection of a problem demonstrated its reliability and ability to operate autonomously.\n\nOn 15 July 2013, the second X-47B demonstrator, designated 501, was forced to abort another planned landing on the \"Bush\" due to technical issues. Officials asserted that only one successful at-sea landing was required for the program, though testers were aiming for three, and only two out of four were achieved. The Navy continued flying the two X-47B demonstrators through 2014, after the service was criticised for prematurely retiring the testbeds. The Navy subsequently deployed the aircraft to carriers for three further test phases between 2013 and 2015, with the intent of demonstrating that unmanned aircraft could seamlessly work with a 70-plane carrier air wing.\n\nOn 18 September 2013, the X-47B flew the 100th flight for the UCAS-D program. The objectives of the program were finally completed in July, which included a total of 16 precision approaches to the carrier flight deck, including five tests of X-47B wave-off functions, nine touch-and-go landings, two arrested landings, and three catapult launches. On 10 November 2013, flight testing for the X-47B continued on board the . During this phase, the X-47B's digitized carrier-controlled environment was tested; this involved the interface between the unmanned aircraft and carrier personnel during launching, recovering, and flight operations.\n\nSea trials on the USS \"Theodore Roosevelt\" in 2014 were intended to test the X-47B's ability to swiftly take off, land, and hold in a pattern among manned aircraft without disruption to carrier operations. The X-47B also operated with a jet-blast deflector on deck for the first time, allowing it to conduct takeoffs without disrupting operations taking place behind it. On 10 April 2014, the X-47B performed its first night flight.\n\nLater that year on 17 August the aircraft took off and landed on the USS \"Theodore Roosevelt\" alongside an F/A-18 Hornet, marking the first time an unmanned aircraft operated in conjunction with manned aircraft aboard an aircraft carrier. The Hornet was launched from the carrier, followed by the X-47B. Both flew around the ship for 8 minutes, then the X-47B touched down and then immediately took off again to verify that all systems were working correctly. After 24 minutes, the X-47B landed on the flight deck and was then taxied away to give the Hornet room to land. All test objectives were met in the demonstration. The trials marked the X-47B's fifth test period at sea, having completed eight catapult launches from a carrier, 30 touch-and-goes, and seven arrested landings aboard the \"George H.W. Bush\" and \"Roosevelt\". The testing was successfully completed on 24 August 2014, with the X-47B completing five catapult launches, four arrestments, and nine touch-and-go landings. Nighttime taxi and deckhandling operations on the flight deck were also performed for the first time. The X-47B met its objective of performing launches and recoveries at 90-second intervals with manned Hornet planes. In April 2015, the X-47B successfully conducted the world's first fully autonomous aerial refuelling, rendezvousing with an Omega Air KC-707 tanker over the coast of Maryland. This marked the effective completion of the X-47B's development, as it had completed all the primary demonstration tasks required of it.\n\nIn February 2016 the US Navy has decided to morph the X-47B from a surveillance and strike aircraft into a reconnaissance and aerial refuelling drone with “limited strike capability”. The about-turn follows a top-level review and restructuring of the now-defunct unmanned carrier-launched airborne surveillance and strike (UCLASS) project, with the service’s latest budget instead funding the MQ-25 Stingray CBARS, or carrier-based aerial refuelling system. \n\nThe project was initially funded under a US$635.8-million contract awarded by the Navy in 2007. By January 2012, the X-47B's total program cost had grown to an estimated $813 million. Government funding for the X-47B UCAS-D program was to run out at the end of September 2013, with the close of the fiscal year. However, in June 2014 the Navy provided an additional $63 million for \"post-demonstration\" development of the X-47B.\n\nIn February 2015, the Navy stated that the competition for private tenders for constructing the UCLASS fleet would begin in 2016, with the aircraft expected to enter service in the early 2020s. Reportedly, despite the X-47B's success in test flights, Navy officials were concerned that it would be too costly and insufficiently stealthy for the needs of the UCLASS project. In April 2015, it was reported that the X-47B demonstrators would become museum exhibits upon the completion of their flight testing. In June 2015, United States Secretary of the Navy Ray Mabus stated that the X-47B test program should continue but that Northrop-Grumman should not gain an unfair advantage in the competition for the UCLASS contract. In July 2015, the Navy stated that the X-47B demonstrators would remain in flying condition rather than being converted to museum exhibits, allowing for a variety of follow-on evaluations.\n\nIn January 2017 the first X-47B departed NAS Patuxent River, Md. for a cross country trip back to Northrop Grumman's manufacturing facility in Palmdale, Calif. \nIn August 2017 Aviation Week published photos of a modified X-47B as testbed for Northrop Grumman's MQ-25 bid.\n\nIn March 2014, the X-47B won the 57th Annual Laureate Award for “extraordinary achievements” in aeronautics and propulsion hosted by \"Aviation Week\". On 9 April 2014, the National Aeronautic Association selected Northrop Grumman, the United States Navy, and the X-47B's development team as the joint recipients of the 2013 Collier Trophy for excellence in aeronautic technology.\n\nThe Navy used software from the X-47B to demonstrate unmanned aerial refueling capabilities. On 28 August 2013, a contractor-flown Learjet 25 refueled from a Boeing 707 tanker while flying autonomously as a surrogate aircraft uploaded with the X-47B's technology. The test was to demonstrate that unmanned and optionally manned aircraft can have an automated aerial refueling capability, significantly increasing their range, persistence, and flexibility. Plans to further demonstrate autonomous aerial refueling were reportedly cut in the Navy’s fiscal 2014 budget, but the X-47B nonetheless conducted a successful autonomous refuelling demonstration in April 2015.\n\nOriginal proof-of-concept prototype with a wingspan, first flown in 2003.\n\nDemonstrator aircraft with a wingspan, first flown in 2011.\n\nProposed larger version intended for the Navy's UCLASS project.\n\n"}
{"id": "7299171", "url": "https://en.wikipedia.org/wiki?curid=7299171", "title": "Novette laser", "text": "Novette laser\n\nNovette was a two beam neodymium glass (phosphate glass) testbed laser built at Lawrence Livermore National Laboratory in about 15 months throughout 1981 and 1982 and was completed in January 1983. Novette was made using recycled parts from the dismantled Shiva and Argus lasers and borrowed parts from the future Nova laser. Its main intended purpose was to validate the proposed design and expected performance of the then planned Nova laser. In addition to being used for the further study of enhanced laser to target plasma energy coupling utilizing frequency tripled light and examining its benefits with respect to inertial confinement fusion, Novette was also used in the world's first laboratory demonstration of an x-ray laser in 1984.\n\n\n"}
{"id": "28434", "url": "https://en.wikipedia.org/wiki?curid=28434", "title": "Outline of space science", "text": "Outline of space science\n\nThe following outline is provided as an overview of and topical guide to space science:\n\nSpace science encompasses all of the scientific disciplines that involve space exploration and study natural phenomena and physical bodies occurring in outer space, such as space medicine and astrobiology.\n\nThe following outline is an overview of and topical guide to space science:\n\n\n\nSee astronomical object for a list of specific types of entities which scientists study. See Earth's location in the universe for an orientation.\n\nAstronautics – science and engineering of spacefaring and spaceflight, a subset of Aerospace engineering (which includes atmospheric flight)\n\n\n"}
{"id": "26180636", "url": "https://en.wikipedia.org/wiki?curid=26180636", "title": "Pioneers of Science", "text": "Pioneers of Science\n\nPioneers of Science () () is a book by Cemal Yıldırım which has run to 22 editions. The book explains the scientific method with anecdotes from significant figures in scientific history such as Albert Einstein, Niels Bohr and Marie Curie.\n"}
{"id": "424202", "url": "https://en.wikipedia.org/wiki?curid=424202", "title": "Rotational invariance", "text": "Rotational invariance\n\nIn mathematics, a function defined on an inner product space is said to have rotational invariance if its value does not change when arbitrary rotations are applied to its argument. \n\nFor example, the function \n\nis invariant under rotations of the plane around the origin, because for a rotated set of coordinates through any angle \"θ\"\n\nthe function, after some cancellation of terms, takes exactly the same form \n\nThe rotation of coordinates can be expressed using matrix form using the rotation matrix,\n\nor symbolically x′ = Rx. Symbolically, the rotation invariance of a real-valued function of two real variables is\n\nIn words, the function of the rotated coordinates takes exactly the same form as it did with the initial coordinates, the only difference is the rotated coordinates replace the initial ones. For a real-valued function of three or more real variables, this expression extends easily using appropriate rotation matrices. \n\nThe concept also extends to a vector-valued function f of one or more variables;\n\nIn all the above cases, the arguments (here called \"coordinates\" for concreteness) are rotated, not the function itself.\n\nFor a function\n\nwhich maps elements from a subset \"X\" of the real line ℝ to itself, rotational invariance may also mean that the function commutes with rotations of elements in \"X\". This also applies for an operator that acts on such functions. An example is the two-dimensional Laplace operator \n\nwhich acts on a function \"f\" to obtain another function ∇\"f\". This operator is invariant under rotations.\n\nIf \"g\" is the function \"g\"(\"p\") = \"f\"(\"R\"(\"p\")), where \"R\" is any rotation, then (∇\"g\")(\"p\") = (∇\"f\")(\"R\"(\"p\")); that is, rotating a function merely rotates its Laplacian. \n\nIn physics, if a system behaves the same regardless of how it is oriented in space, then its Lagrangian is rotationally invariant. According to Noether's theorem, if the action (the integral over time of its Lagrangian) of a physical system is invariant under rotation, then angular momentum is conserved.\n\nIn quantum mechanics, rotational invariance is the property that after a rotation the new system still obeys Schrödinger's equation. That is\n\nfor any rotation \"R\". Since the rotation does not depend explicitly on time, it commutes with the energy operator. Thus for rotational invariance we must have [\"R\", \"H\"] = 0.\n\nFor infinitesimal rotations (in the \"xy\"-plane for this example; it may be done likewise for any plane) by an angle \"dθ\" the (infinitesimal) rotation operator is\n\nthen\n\nthus\n\nin other words angular momentum is conserved.\n\n\n"}
{"id": "206386", "url": "https://en.wikipedia.org/wiki?curid=206386", "title": "What If the Moon Didn't Exist", "text": "What If the Moon Didn't Exist\n\nWhat if the Moon Didn’t Exist is a collection of speculative articles about different versions of Earth, published in book form in 1993. They were originally published in \"Astronomy\" magazine. The individual scenarios are:\n\n\n\n"}
{"id": "9973744", "url": "https://en.wikipedia.org/wiki?curid=9973744", "title": "William C. Schneider", "text": "William C. Schneider\n\nWilliam Charles Schneider () served in the United States Navy Reserve 1942-1946 as an Aviation Machinists Mate, 1st Class Petty Officer. He joined NASA in June 1963 and served as the Gemini mission director for seven of the ten piloted Gemini missions. From 1967 to 1968, he served as Apollo mission director and the Apollo program’s deputy director for missions. He then served from 1968 to 1974 as the Skylab program’s director. From 1974 to 1978, he worked as the Deputy Associate Administrator for Space Transportation Systems. From 1978 to 1980, he served as the Associate Administrator for Space Tracking and Data systems. He received a Ph.D. in engineering from Catholic University of America.\n\nSchneider received the NASA Distinguished Service Medal, NASA's highest award, in 1969.\nGerald R. Ford presented the 1973 Collier Trophy to Skylab, with special recognition to Schneider and the three crews, \"For proving beyond question the value of man in future explorations of space and the production of data of benefit to all the people on Earth.\"\n\nNational Space Club awarded Dr. Schneider the 1974 award in aeronautical engineering.\n\nThe American Astronautical Society presented Schneider the 1974 Space Flight Award.\n"}
{"id": "25874393", "url": "https://en.wikipedia.org/wiki?curid=25874393", "title": "Working group", "text": "Working group\n\nA working group or working party is a group of experts working together to achieve specified goals. The groups are domain-specific and focus on discussion or activity around a specific subject area. The term can sometimes refer to an interdisciplinary collaboration of researchers working on new activities that would be difficult to sustain under traditional funding mechanisms (e.g., federal agencies).\n\nThe lifespan of a working group can last anywhere between a few months and several years. Such groups have the tendency to develop a \"quasi-permanent existence\" when the assigned task is accomplished; hence the need to disband (or phase out) the working group when it has achieved its goal(s).\n\nA working group’s performance is made up of the individual results of all its individual members. A team’s performance is made up of both individual results and collective results.\nIn large organisations, working groups are prevalent, and the focus is always on individual goals, performance and accountabilities. Working group members do not take responsibility for results other than their own.\nOn the other hand, teams require both individual and mutual accountability. There is more information sharing, more group discussions and debates to arrive at a group decision.\n\nExamples of common goals for working groups include:\n\nWorking groups are also referred to as \"task groups\", \"workgroups\", or \"technical advisory groups\".\n\nThe nature of the working group may depend on the group's \"raison d’être\" – which may be technical, artistic (specifically musical), or administrative in nature.\n\nThese working groups are established by decision makers at higher levels of the organization for the following purposes:\n\nFor example, the Interagency Working Group on Youth Programs is a group of twelve federal agencies within the executive branch of the U.S. government, and is responsible for promoting achievement of positive results for at-risk youth. This working group was formally established by Executive Order 13459, \"Improving the Coordination and Effectiveness of Youth Programs\", on February 7, 2008.\n\nQuality circles are an alternative to the dehumanizing concept of the division of labor, where workers or individuals are treated like robots. Quality circles can help enrich the lives of workers or students and aid in creating harmony and high performance. Typical topics are improving occupational safety and health, improving product design, and improvement in the workplace and manufacturing processes.\n\nAlthough any artisan or artist can benefit from being part of a working group, it is especially of great import for session players. Musicians face a variety of challenges that can impede the formation of musical working groups, such as touring and studio recording sessions. Such activities make it that much more difficult to concentrate on the developing the cohesiveness that is required to maintain a working group.\n\nHowever, working groups have been shown to be rewarding to the stakeholders, as it fosters innovation. By working with the same people frequently, members become familiar with the répertoire of other members, which develops trust and encourages spontaneity.\n\nSome of the more notable musical working groups include:\n\nIn many technical organizations, for example Standards organizations, the groups that meet and make decisions are called \"working groups\". Examples include:\n\n\nIn some cases, like the Printer Working Group, an entire consortium uses the term \"working group\" for itself.\n\nThe rules for who can be a part of the working groups, and how a working group makes decisions, varies considerably between organizations.\n\nIn problem-solving contexts, the random-word creativity technique is perhaps the simplest method. A person confronted with a problem is presented with a randomly generated word, in the hopes of a solution arising from any associations between the word and the problem. A random image, sound, or article can be used instead of a random word as a kind of creativity goad or provocation.\n\nTools and methodologies to support creativity.\n\n\nIt is imperative for the participants to appreciate and understand that the working group is intended to be a forum for cooperation and participation. Participants represent the interests and views of stakeholders from disparate sectors of the community which happen to have a vested interest in the results of the WG. Therefore, maintaining and strengthening communication lines with all parties involved is essential (this responsibility cuts both ways — stakeholders are expected to share what information, knowledge and expertise they have on the issue).\n\nProgrammes developed should be evaluated by encouraging community input and support; this will ensure that such programmes meet the community's vision for its future. The WG should also regularly seek community feedback on their projects. Apropos questions to be asked during such meetings include:\n\nDepending on the lifespan of the WG, involved parties (at the very least) convene annually. However, such meetings may happen as often as once every semester or trimester.\n\nThe managers are constantly called upon to make decisions in order to solve problems. Decision making and problem solving are ongoing processes of evaluating situations or problems, considering alternatives, making choices, and following them up with the necessary actions and now with this managed to reach a continuous improvement.\n\n"}
{"id": "33947300", "url": "https://en.wikipedia.org/wiki?curid=33947300", "title": "World tube", "text": "World tube\n\nIn physics, a world tube is the path of an object which occupies a nonzero region of space (nonzero volume) at every moment in time, as it travels through 4-dimensional spacetime. That is, as it propagates in spacetime, a world tube traces out a three-dimensional volume for every moment in time. The world tube is analogous to the one-dimensional world line in that it describes the time evolution of an object in space, with the difference that a world line represents the path of a point particle (of nonzero volume), whereas a world tube occupies finite space at all moments in time.\n\nThe concept of world tube is particularly relevant for special relativity, where a world tube is embedded in Minkowski space.\n\n"}
{"id": "352711", "url": "https://en.wikipedia.org/wiki?curid=352711", "title": "Zoom lens", "text": "Zoom lens\n\nA zoom lens is a mechanical assembly of lens elements for which the focal length (and thus angle of view) can be varied, as opposed to a fixed focal length (FFL) lens (see prime lens).\n\nA true zoom lens, also called a parfocal lens, is one that maintains focus when its focal length changes. A lens that loses focus during zooming is more properly called a varifocal lens. Despite being marketed as zoom lenses, virtually all consumer lenses with variable focal lengths use varifocal design.\n\nThe convenience of variable focal length comes at the cost of complexity - and some compromises on image quality, weight, dimensions, aperture, autofocus performance, and cost. For example, all zoom lenses suffer from at least slight, if not considerable, loss of image resolution at their maximum aperture, especially at the extremes of their focal length range. This effect is evident in the corners of the image, when displayed in a large format or high resolution. The greater the range of focal length a zoom lens offers, the more exaggerated these compromises must become.\n\nZoom lenses are often described by the ratio of their longest to shortest focal lengths. For example, a zoom lens with focal lengths ranging from 100 mm to 400 mm may be described as a 4:1 or \"4×\" zoom. The term superzoom or hyperzoom is used to describe photographic zoom lenses with very large focal length factors, typically more than 5× and ranging up to 19× in SLR camera lenses and 83× in amateur digital cameras. This ratio can be as high as 300× in professional television cameras. As of 2009, photographic zoom lenses beyond about 3× cannot generally produce imaging quality on par with prime lenses. Constant fast aperture zooms (usually 2.8 or 2.0) are typically restricted to this zoom range. Quality degradation is less perceptible when recording moving images at low resolution, which is why professional video and TV lenses are able to feature high zoom ratios. Digital photography can also accommodate algorithms that compensate for optical flaws, both within in-camera processors and post-production software.\n\nSome photographic zoom lenses are long-focus lenses, with focal lengths longer than a normal lens, some are wide-angle lenses (wider than \"normal\"), and others cover a range from wide-angle to long-focus. Lenses in the latter group of zoom lenses, sometimes referred to as \"normal\" zooms, have displaced the fixed focal length lens as the popular one-lens selection on many contemporary cameras. The markings on these lenses usually say W and T for \"Wide\" and \"Telephoto\". Telephoto is designated because the longer focal length supplied by the negative diverging lens is longer than the overall lens assembly (the negative diverging lens acting as the \"telephoto group\").\n\nSome digital cameras allow cropping and enlarging of a captured image, in order to emulate the effect of a longer focal length zoom lens (narrower angle of view). This is commonly known as digital zoom and produces an image of lower optical resolution than optical zoom. Exactly the same effect can be obtained by using digital image processing software on a computer to crop the digital image and enlarge the cropped area. Many digital cameras have both, combining them by first using the optical, then the digital zoom.\n\nZoom and superzoom lenses are commonly used with still, video, motion picture cameras, projectors, some binoculars, microscopes, telescopes, telescopic sights, and other optical instruments. In addition, the afocal part of a zoom lens can be used as a telescope of variable magnification to make an adjustable beam expander. This can be used, for example, to change the size of a laser beam so that the irradiance of the beam can be varied.\n\nEarly forms of zoom lenses were used in optical telescopes to provide continuous variation of the magnification of the image, and this was first reported in the proceedings of the Royal Society in 1834. Early patents for telephoto lenses also included movable lens elements which could be adjusted to change the overall focal length of the lens. Lenses of this kind are now called varifocal lenses, since when the focal length is changed, the position of the focal plane also moves, requiring refocusing of the lens after each change.\n\nThe first true \"zoom\" lens, which retained near-sharp focus while the effective focal length of the lens assembly was changed, was patented in 1902 by Clile C. Allen (). An early use of the zoom lens in cinema can be seen in the opening shot of the movie \"It\" starring Clara Bow, from 1927. The first industrial production was the Bell and Howell Cooke \"Varo\" 40–120 mm lens for 35mm movie cameras introduced in 1932. The most impressive early TV Zoom lens was the VAROTAL III, from Rank Taylor Hobson from UK built in 1953. The Kilfitt 36–82 mm/2.8 Zoomar introduced in 1959 was the first varifocal lens in regular production for still 35mm photography. The first modern film zoom lens, the Pan-Cinor, was designed around 1950 by Roger Cuvillier, a French engineer working for SOM-Berthiot. It had an optical compensation zoom system. In 1956, Pierre Angénieux introduced the mechanical compensation system, enabling precise focus while zooming, in his 17-68mm lens for 16mm released in 1958. The same year a prototype of the 35mm version of the Angénieux 4x zoom, the 35-140mm was first used by cinematographer Roger Fellous for the production of Julie La Rousse. Angénieux received a 1964 technical award from the academy of motion pictures for the design of the 10 to 1 zoom lenses, including the 12-120mm for 16mm film cameras and the 25-250mm for 35mm film cameras.\n\nSince then advances in optical design, particularly the use of computers for optical ray tracing, has made the design and construction of zoom lenses much easier, and they are now used widely in professional and amateur photography.\n\nThere are many possible designs for zoom lenses, the most complex ones having upwards of thirty individual lens elements and multiple moving parts. Most, however, follow the same basic design. Generally they consist of a number of individual lenses that may be either fixed or slide axially along the body of the lens. While the magnification of a zoom lens changes, it is necessary to compensate for any movement of the focal plane to keep the focused image sharp. This compensation may be done by mechanical means (moving the complete lens assembly while the magnification of the lens changes) or optically (arranging the position of the focal plane to vary as little as possible while the lens is zoomed).\n\nA simple scheme for a zoom lens divides the assembly into two parts: a focusing lens similar to a standard, fixed-focal-length photographic lens, preceded by an \"afocal zoom system\", an arrangement of fixed and movable lens elements that does not focus the light, but alters the size of a beam of light travelling through it, and thus the overall magnification of the lens system.\nIn this simple optically compensated zoom lens, the afocal system consists of two positive (converging) lenses of equal focal length (lenses \"L\" and \"L\") with a negative (diverging) lens (\"L\") between them, with an absolute focal length less than half that of the positive lenses. Lens \"L\" is fixed, but lenses \"L\" and \"L\" can be moved axially in a particular non-linear relationship. This movement is usually performed by a complex arrangement of gears and cams in the lens housing, although some modern zoom lenses use computer-controlled servos to perform this positioning.\n\nWhile the negative lens \"L\" moves from the front to the back of the lens, the lens \"L\" moves forward and then backward in a parabolic arc. In doing so, the overall angular magnification of the system varies, changing the effective focal length of the complete zoom lens. At each of the three points shown, the three-lens system is afocal (neither diverging or converging the light), and hence does not alter the position of the focal plane of the lens. Between these points, the system is not exactly afocal, but the variation in focal plane position can be small enough (about ±0.01 mm in a well-designed lens) not to make a significant change to the sharpness of the image.\nAn important issue in zoom lens design is the correction of optical aberrations (such as chromatic aberration and, in particular, field curvature) across the whole operating range of the lens; this is considerably harder in a zoom lens than a fixed lens, which needs only to correct the aberrations for one focal length. This problem was a major reason for the slow uptake of zoom lenses, with early designs being considerably inferior to contemporary fixed lenses and usable only with a narrow range of f-numbers. Modern optical design techniques have enabled the construction of zoom lenses with good aberration correction over widely variable focal lengths and apertures.\n\nWhereas lenses used in cinematography and video applications are required to maintain focus while the focal length is changed, there is no such requirement for still photography and for zoom lenses used as projection lenses. Since it is harder to construct a lens that does not change focus with the same image quality as one that does, the latter applications often use lenses that require refocusing once the focal length has changed (and thus strictly speaking are varifocal lenses, not zoom lenses). As most modern still cameras are autofocusing, this is not a problem.\n\nDesigners of zoom lenses with large zoom ratios often trade one or more aberrations for higher image sharpness. For example, a greater degree of barrel and pincushion distortion is tolerated in lenses that span the focal length range from wide angle to telephoto with a focal ratio of 10× or more than would be acceptable in a fixed focal length lens or a zoom lens with a lower ratio. Although modern design methods have been continually reducing this problem, barrel distortion of greater than one percent is common in these large-ratio lenses. Another price paid is that at the extreme telephoto setting of the lens the effective focal length changes significantly while the lens is focused on closer objects. The apparent focal length can more than halve while the lens is focused from infinity to medium close-up. To a lesser degree, this effect is also seen in fixed focal length lenses that move internal lens elements, rather than the entire lens, to effect changes in magnification.\n\nMany so-called \"zoom\" lenses, particularly in the case of fixed-lens cameras, are actually \"varifocal lenses\", which gives lens designers more flexibility in optical design trade-offs (focal length range, maximal aperture, size, weight, cost) than true parfocal zoom, and which is practical because of autofocus, and because the camera processor can move the lens to compensate for the change in the position of the focal plane while changing magnification (\"zooming\"), making operation essentially the same as a true parfocal zoom.\n\n\n"}
